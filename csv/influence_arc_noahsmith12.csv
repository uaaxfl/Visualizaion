2008.amta-papers.18,2008.amta-papers.18,1,0.106103,"Missing"
2008.amta-papers.18,W06-3105,0,0.0471548,"Missing"
2008.amta-papers.18,P08-1115,0,0.150282,"Missing"
2008.amta-papers.18,P08-1112,0,0.107632,"Missing"
2008.amta-papers.18,N03-1017,0,0.0707978,"Missing"
2008.amta-papers.18,W02-1018,0,0.0820035,"Missing"
2008.amta-papers.18,W06-1606,0,0.0455186,"Missing"
2008.amta-papers.18,P08-1023,0,0.111838,"Missing"
2008.amta-papers.18,J93-2003,0,0.0142579,"Missing"
2008.amta-papers.18,A00-2018,0,0.0833625,"Missing"
2008.amta-papers.18,P05-1033,0,0.0186015,"Missing"
2008.amta-papers.18,P03-1021,0,0.063759,"Missing"
2008.amta-papers.18,P02-1040,0,0.103367,"Missing"
2008.amta-papers.18,W06-1608,0,0.0844503,"Missing"
2008.amta-papers.18,N07-1063,1,0.905406,"Missing"
2008.amta-papers.18,J97-3002,0,0.0508368,"Missing"
2008.amta-papers.18,W06-3119,1,0.0891131,"Missing"
2008.amta-papers.18,C08-1144,1,0.680453,"Missing"
2008.amta-papers.18,J04-4002,0,\N,Missing
2008.amta-papers.18,J03-1002,0,\N,Missing
2008.amta-papers.18,2006.iwslt-evaluation.1,0,\N,Missing
2008.amta-papers.18,J07-2003,0,\N,Missing
2020.acl-main.178,N01-1021,0,0.295162,". By definition, realis events are claimed by the author to have taken place, which makes them more likely to be drawn from from autobiographical or episodic memory in diary-like stories. We train a realis event tagger (using BERT-base; Devlin et al., 2019) on the annotated literary events corpus by Sims et al. (2019), which slightly outperforms the original author’s models. We provide further training details in Appendix B.1. Semantic and Commonsense Knowledge We measure the amount of commonsense knowl3 Note that this is a sentence-level version of surprisal as defined by expectation theory (Hale, 2001; Levy, 2008) edge included explicitly in stories, as a proxy for semantic memory, a form of memory that is thought to encode general knowledge about the world (Tulving, 1972). While this includes facts about how events unfold (i.e., scripts or schemas; Schank and Abelson, 1977; van Kesteren et al., 2012), here we focus on commonsense knowledge, which is also encoded in semantic memory (McRae and Jones, 2013). Given the social focus of our stories, we use the social commonsense knowledge graph ATOMIC (Sap et al., 2019).4 For each story, we first match possible ATOMIC events to sentences by sel"
2020.acl-main.178,N15-1044,1,0.809472,"or commonsense inferences (e.g., “be very happy” “happy”; Figure 1). We describe this algorithm in further detail in Appendix B.2. In our analyses, the measure quantifies the number of story sentences with commonsense tuple matches in the two preceding and following sentences. 3.3 Lexical and Stylistic Measures To supplement our analyses, we compute several coarse-grained lexical counts for each story in H IPPOCORPUS. Such approaches have been used in prior efforts to investigate author mental states, temporal orientation, or counterfactual thinking in language (Tausczik and Pennebaker, 2010; Schwartz et al., 2015; Son et al., 2017). We count psychologically relevant word categories using the Linguistic Inquiry Word Count (Pennebaker et al., 2015, LIWC;), focusing only on the cognitive processes, positive emotion, negative emotion, and I-word categories, as well as the A NALYTIC and T ONE summary variables.5 Additionally, we measure the average concreteness level of words in stories using the lexicon by Brysbaert et al. (2014). 4 Imagining vs. Remembering We summarize the differences between imagined and recalled stories in H IPPOCORPUS in Table 2. For our narrative flow and lexicon-based analyses, 4 A"
2020.acl-main.178,P19-1353,0,0.14621,"and commonsense models to study how cognitive processes of recollection and imagination are engaged in storytelling. We rely on two key aspects of stories: narrative flow (how the story reads) and semantic vs. episodic knowledge (the types of events in the story). We propose as a measure of narrative flow the likelihood of sentences under generative language models conditioned on varying amounts of history. Then, we quantify semantic knowledge by measuring the frequency of commonsense events (from the ATOMIC knowledge graph; Sap et al., 2019), and episodic knowledge by counting realis events (Sims et al., 2019), both shown in Figure 1. 1970 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1970–1978 c July 5 - 10, 2020. 2020 Association for Computational Linguistics We introduce H IPPOCORPUS,1 a dataset of 6,854 diary-like short stories about salient life events, to examine the cognitive processes of remembering and imagining. Using a crowdsourcing pipeline, we collect pairs of recalled and imagined stories written about the same topic. By design, authors of recalled stories rely on their episodic memory to tell their story. We demonstrate that our measur"
2020.acl-main.178,P17-2103,0,0.026575,"es (e.g., “be very happy” “happy”; Figure 1). We describe this algorithm in further detail in Appendix B.2. In our analyses, the measure quantifies the number of story sentences with commonsense tuple matches in the two preceding and following sentences. 3.3 Lexical and Stylistic Measures To supplement our analyses, we compute several coarse-grained lexical counts for each story in H IPPOCORPUS. Such approaches have been used in prior efforts to investigate author mental states, temporal orientation, or counterfactual thinking in language (Tausczik and Pennebaker, 2010; Schwartz et al., 2015; Son et al., 2017). We count psychologically relevant word categories using the Linguistic Inquiry Word Count (Pennebaker et al., 2015, LIWC;), focusing only on the cognitive processes, positive emotion, negative emotion, and I-word categories, as well as the A NALYTIC and T ONE summary variables.5 Additionally, we measure the average concreteness level of words in stories using the lexicon by Brysbaert et al. (2014). 4 Imagining vs. Remembering We summarize the differences between imagined and recalled stories in H IPPOCORPUS in Table 2. For our narrative flow and lexicon-based analyses, 4 ATOMIC contains soci"
2020.acl-main.178,T75-2012,0,0.552144,"child. She and her husband were overwhelmed by emotions. Abstract RECALLED We investigate the use of NLP as a measure of the cognitive processes involved in storytelling, contrasting imagination and recollection of events. To facilitate this, we collect and release H IPPOCORPUS, a dataset of 7,000 stories about imagined and recalled events. # concrete events: 7 We introduce a measure of narrative flow and use this to examine the narratives for imagined and recalled events. Additionally, we measure the differential recruitment of knowledge attributed to semantic memory versus episodic memory (Tulving, 1972) for imagined and recalled storytelling by comparing the frequency of descriptions of general commonsense events with more specific realis events. IMAGINED We recently attended a family wedding. It was the first time in a decade we all got together. Our analyses show that imagined stories have a substantially more linear narrative flow, compared to recalled stories in which adjacent sentences are more disconnected. In addition, while recalled stories rely more on autobiographical events based on episodic memory, imagined stories express more commonsense knowledge based on semantic memory. Fina"
2020.acl-main.270,D19-1223,0,0.0301916,"Missing"
2020.acl-main.270,P19-1285,0,0.417687,"sublayers. When k = 0, we get the original transformer model, and when k = n − 1 (its maximal value) we get the previously mentioned s n f n model. We refer to k as the transformer’s sandwich coefficient. We train sandwich transformers for n = 16 (to remain within the same parameter budget as our baseline language model) and all values of k ∈ {0, . . . , 15}. Figure 5 shows the transformer’s performance as a function of the sandwich coefficient k. With the exception of k = 14, 15, all sandwich transformers achieve lower perplexities 2999 Test Baseline (Baevski and Auli, 2019) Transformer XL (Dai et al., 2019) kNN-LM (Khandelwal et al., 2019) 18.70 18.30 15.79 Baseline (5 Runs) Sandwich16 6 18.63 ± 0.26 17.96 19.00 18.75 Perplexity Model Table 3: Performance on the WikiText-103 test set. We compare the best sandwich transformer to the unmodified, interleaved transformer baseline (Baevski and Auli, 2019) trained over 5 random seeds and to other previously reported results. 5 One Reordering to Rule Them All? The sandwich transformer is a manually-crafted pattern motivated by the performance of random 18.25 18.00 17.75 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 Sandwich Coefficient Figure 5: The transformer’"
2020.acl-main.270,N19-1423,0,0.232145,"ded in order to unlock additional gains.1 (a) Interleaved Transformer s ss s s s s f sf s f s f sf s f s f s ff f f f f f (b) Sandwich Transformer Figure 1: A transformer model (a) is composed of interleaved self-attention (green) and feedforward (purple) sublayers. Our sandwich transformer (b), a reordering of the transformer sublayers, performs better on language modeling. Input flows from left to right. Introduction The transformer layer (Vaswani et al., 2017) is currently the primary modeling component in natural language processing, playing a lead role in recent innovations such as BERT (Devlin et al., 2019) and GPT-2 (Radford et al., 2019). Each transformer layer consists of a self-attention sublayer ( s ) followed by a feedforward sublayer ( f ), creating an interleaving pattern of self-attention and feedforward sublayers ( s f s f s f · · · ) throughout a multilayer transformer model. To the best of our knowledge, there is no reason to expect this particular pattern to be optimal. We conduct a series of explorations to obtain insights about the nature of transformer orderings that work well, and based on this, we 1 Our code is available at https://github.com/ ofirpress/sandwich_transformer des"
2020.acl-main.270,W18-6301,0,0.146902,"self-attention ( s ) and cross-attention ( c ) sublayers, and treat them as a single unit for reordering purposes ( s c ). For example, a three layer decoder ( s c f sc f s c f ) with a sandwiching coefficient of k = 1 would be: s c sc f s c f f . We apply the sandwich pattern to either the encoder or decoder separately, while keeping the other stack in its original interleaved pattern. Experiment Setting As a baseline, we use the large transformer model (6 encoder/decoder layers, embedding size of 1024, feedforward inner dimension of 4096, and 16 attention heads) with the hyperparameters of Ott et al. (2018). We also follow their setup for training and evaluation: we train on the WMT 2014 En-De dataset which contains 4.5M sentence pairs; we validate on newstest13 and test on newstest14. We use a vocabulary of 32K symbols based on a joint source and target byte pair encoding (Sennrich et al., 2016). For inference we use beam search with a beam width of 4 and length penalty of 0.6, following Vaswani et al. (2017) and Ott et al. (2018). As before, we do not modify our model’s hyperparameters or training procedure. Results Table 6 shows that reordering of either the encoder or decoder does not have a"
2020.acl-main.270,E17-2025,1,0.883093,"i and Auli (2019). Specifically, we use the Toronto Books Corpus (Zhu et al., 2015), which has previously been used to train GPT (Radford et al., 2018) and also BERT (Devlin et al., 2019) (combined with Wikipedia). The corpus contains roughly 700M tokens. We use the same train/validation/test split as Khandelwal et al. (2019), as well as their tokenization, which uses BERT’s vocabulary of 29K byte-pair encodings. Since the vocabulary is much smaller than WikiText-103’s, we replace the adaptive word embedding and softmax of Baevski and Auli (2019) with a tied word embedding and softmax matrix (Press and Wolf, 2017; Inan et al., 2017). Finally, we tune the sandwich coefficient on the development set for k ∈ {4, . . . , 8}, i.e., a neighborhood of 2 around the best value we found for WikiText-103 (k = 6). Table 4 shows that the sandwich transformer transfers well to the books domain, improving performance by 1.06 perplexity, achieving similar performance to the datastore-augmented kNN-LM (Khandelwal et al., 2019), which is the state of the art on WikiText-103 (see Section 4). 5.2 Character-level Language Modeling Modeling text as a stream of characters, rather than word or subword tokens, presents a diff"
2020.acl-main.270,P16-1162,0,0.0648078,"he encoder or decoder separately, while keeping the other stack in its original interleaved pattern. Experiment Setting As a baseline, we use the large transformer model (6 encoder/decoder layers, embedding size of 1024, feedforward inner dimension of 4096, and 16 attention heads) with the hyperparameters of Ott et al. (2018). We also follow their setup for training and evaluation: we train on the WMT 2014 En-De dataset which contains 4.5M sentence pairs; we validate on newstest13 and test on newstest14. We use a vocabulary of 32K symbols based on a joint source and target byte pair encoding (Sennrich et al., 2016). For inference we use beam search with a beam width of 4 and length penalty of 0.6, following Vaswani et al. (2017) and Ott et al. (2018). As before, we do not modify our model’s hyperparameters or training procedure. Results Table 6 shows that reordering of either the encoder or decoder does not have a significant impact on performance, across the board. We also find that using the most extreme sandwich decoder ( s c )6 f 6 performs almost exactly the same as the average baseline; this result is consistent with our observation from Section 4, where we show that the extreme sandwich language"
2020.acl-main.270,P19-1032,0,0.278645,"= 6). Table 4 shows that the sandwich transformer transfers well to the books domain, improving performance by 1.06 perplexity, achieving similar performance to the datastore-augmented kNN-LM (Khandelwal et al., 2019), which is the state of the art on WikiText-103 (see Section 4). 5.2 Character-level Language Modeling Modeling text as a stream of characters, rather than word or subword tokens, presents a different modeling challenge: long-range dependencies become critical, and the vocabulary takes on a more uniform distribution. We apply our sandwich reordering to the adaptive span model of Sukhbaatar et al. (2019), which is state of the art on the popular English-language benchmark text8 and is currently a close second on enwik8.3 The adaptive span 3 Both datasets are taken from http://mattmahoney. net/dc/textdata.html model learns to control each attention head’s maximal attention span, freeing up memory in the bottom layers (which typically need very short attention spans) and applying it to the top layers, allowing the top-level attention heads to reach significantly longer distances. The adaptive span model’s efficient use of attention also results in a significant speed boost. We tune the sandwich"
2020.acl-main.270,D19-1083,0,0.0315578,"Missing"
2020.acl-main.43,N18-1205,0,0.0296558,"port this conjecture. 1 Figure 1: Hierarchy of state expressiveness for saturated RNNs and related models. The y axis represents increasing space complexity. ∅ means provably empty. Models are in bold with qualitative descriptions in gray. Introduction While neural networks are central to the performance of today’s strongest NLP systems, theoretical understanding of the formal properties of different kinds of networks is still limited. It is established, for example, that the Elman (1990) RNN is Turing-complete, given infinite precision and computation time (Siegelmann and Sontag, 1992, 1994; Chen et al., 2018). But tightening these unrealistic assumptions has serious implications for expressive power (Weiss et al., 2018), leaving a significant gap between classical theory and practice, which theorems in this paper attempt to address. Recently, Peng et al. (2018) introduced rational RNNs, a subclass of RNNs whose internal state can be computed by independent weighted finite automata (WFAs). Intuitively, such models have a computationally simpler recurrent update than conventional models like long short-term memory networks (LSTMs; Hochreiter and Schmidhuber, 1997). Empirically, rational RNNs like th"
2020.acl-main.43,D19-1110,1,0.85501,"ower (Weiss et al., 2018), leaving a significant gap between classical theory and practice, which theorems in this paper attempt to address. Recently, Peng et al. (2018) introduced rational RNNs, a subclass of RNNs whose internal state can be computed by independent weighted finite automata (WFAs). Intuitively, such models have a computationally simpler recurrent update than conventional models like long short-term memory networks (LSTMs; Hochreiter and Schmidhuber, 1997). Empirically, rational RNNs like the quasirecurrent neural network (QRNN; Bradbury et al., 2016) and unigram rational RNN (Dodge et al., 2019) perform comparably to the LSTM, with a smaller computational budget. Still, the underlying simplicity of rational models raises the question of whether their expressive power is fundamentally limited compared to other RNNs. In a separate line of work, Merrill (2019) introduced the saturated RNN1 as a formal model for analyzing the capacity of RNNs. A saturated RNN is a simplified network where all activation functions have been replaced by step functions. The saturated network may be seen intuitively as a “stable” version of its original RNN, in which the in1 Originally referred to as the asy"
2020.acl-main.43,W18-2501,0,0.0432401,"Missing"
2020.acl-main.43,2020.tacl-1.11,0,0.111331,"Missing"
2020.acl-main.43,D14-1181,0,\N,Missing
2020.acl-main.43,D18-1152,1,\N,Missing
2020.acl-main.43,W19-3905,0,\N,Missing
2020.acl-main.486,D16-1129,0,0.0267646,"educating people on reducing unconscious biases in their language. 2 S OCIAL B IAS F RAMES Definition To better enable models to account for socially biased implications of language,2 we design a new pragmatic formalism that distinguishes several related but distinct inferences, shown in Figure 1. Given a natural language utterance, henceforth, post, we collect both categorical as well as free text inferences (described below), inspired by recent efforts in free-text annotations of commonsense knowledge (e.g., Speer and Havasi, 2012; Rashkin et al., 2018; Sap et al., 2019b) and argumentation (Habernal and Gurevych, 2016; Becker et al., 2017). The free-text explanations are crucial to our formalism, as they can both increase trust in predictions made by the machine (Kulesza et al., 2012; Bussone et al., 2015; Nguyen et al., 2018) and encourage a poster’s empathy towards a targeted group, thereby combating biases (CohenAlmagor, 2014). We base our initial frame design on social science literature of pragmatics (Lakoff, 1973; de Marneffe et al., 2012) and impoliteness (Kasper, 1990; Gabriel, 1998; Dynel, 2015; Vonasch and Baumeister, 2017). We then refine the frame structure (including number of possible answers"
2020.acl-main.486,N19-1169,0,0.0175417,"rated text candidate and variable probabilities.9 This can allow variables to be assigned an alternative value that is more globally optimal.10 4.1 Evaluation We evaluate performance of our models in the following ways. For classification, we report precision, recall, and F1 scores of the positive class. Following previous generative inference work (Sap et al., 2019b), we use automated metrics to evaluate model generations. We use BLEU2 and RougeL (F1 ) scores to capture word overlap between the generated inference and the references, which captures quality of generation (Galley et al., 2015; Hashimoto et al., 2019). We additionally compute word mover’s distance (WMD; Kusner et al., 2015), which uses distributed word representations to measure similarity between the generated and target text.11 4.2 Training Details As each post can contain multiple annotations, we define a training instance as containing one postgroup-statement triple (along with the five categorical annotations). We then split our dataset into train/dev./test (75:12.5:12.5), ensuring that no post is present in multiple splits. For evaluation (dev., test), we combine the categorical variables by averaging their binarized values and re-bi"
2020.acl-main.486,C92-2082,0,0.118608,"oke power dynamics between groups (e.g., “F*ck you” vs. “F*ck you, f*ggot”). This is a categorical variable with two possible answers: individualonly (no), group targeted (yes). Targeted group describes the social or demographic group that is referenced or targeted by the post. Here we collect free-text answers, but provide a seed list of demographic or social groups to encourage consistency. Implied statement represents the power dynamic or stereotype that is referenced in the post. We collect free-text answers in the form of simple Hearst-like patterns (e.g., “women are ADJ”, “gay men VBP”; Hearst, 1992). In-group language aims to capture whether the author of a post may be a member of the same social/demographic group that is targeted, as speaker identity changes how a statement is perceived (O’Dea et al., 2015). Specifically, in-group language (words or phrases that (re)establish belonging to a social group; Eble, 1996) can change the perceived offensiveness of a statement, such as reclaimed slurs (Croom, 2011; Galinsky et al., 2013) or self-deprecating language (Greengross and Miller, 2008). Note that we do not attempt to categorize the identity of the speaker. This variable takes three po"
2020.acl-main.486,D16-1230,0,0.0174402,"cs (Table 5). Overall, models do well at generating the targeted groups, likely because of the more limited generation space (there are only 1.4k possible groups in SBIC). Conversely, for implied statement generation (where output space is much larger), model performance is slightly worse. Similar to the classification tasks, SBF-GPT2 gdy shows a slight increase in RougeL score when using constrained decoding, but we see a slight drop in BLEU scores. Error analysis Since small differences in automated evaluation metrics for text generation sometimes only weakly correlate with human judgments (Liu et al., 2016), we manually perform an error analysis on a manually selected set of generated development-set examples from the SBFGPT2 -gdy-constr model (Table 6). Overall, the model seems to struggle with generating textual implications that are relevant to the post, instead generating very generic stereotypes about the demographic groups (e.g., in examples b and c). The model generates the correct stereotypes when there is high lexical overlap with the post (e.g., examples d and e). This is in line with previous research showing that large language models rely on correlational patterns in data (Sap et al"
2020.acl-main.486,J12-2003,0,0.0879155,"Missing"
2020.acl-main.486,D19-1474,0,0.0431593,"reased attention recently (Schmidt and Wiegand, 2017), and most dataset creation work has cast this detection problem as binary classification (Waseem and Hovy, 2016; Davidson et al., 2017; Founta et al., 2018). Moving beyond a single binary label, Wulczyn et al. (2017) and the PerspectiveAPI use a set of binary variables to annotate Wikipedia comments for several toxicityrelated categories (e.g., identity attack, profanity). Similarly, Zampieri et al. (2019) hierarchically annotate a dataset of tweets with offensiveness and whether a group or individual is targeted. Most related to our work, Ousidhoum et al. (2019) create a multilingual dataset of 13k tweets annotated for five different emotion- and toxicity-related aspects, including a 16-class variable representing social groups targeted. In comparison, S OCIAL B IAS F RAMES not only captures binary toxicity and hierarchical information about whether a group is targeted, but also free-text implications about 1.4k different targeted groups and the implied harm behind statements. Similar in spirit to this paper, recent work has tackled more subtle bias in language, such as microaggressions (Breitfeller et al., 2019) and condescension (Wang and Potts, 20"
2020.acl-main.486,P14-2056,0,0.0247509,"k different targeted groups and the implied harm behind statements. Similar in spirit to this paper, recent work has tackled more subtle bias in language, such as microaggressions (Breitfeller et al., 2019) and condescension (Wang and Potts, 2019). These types of biases are in line with the biases covered by S O CIAL B IAS F RAMES , but more narrowly scoped. 5484 Inference about social dynamics Various work has tackled the task of making inferences about power and social dynamics. Particularly, previous work has analyzed power dynamics about specific entities, either in conversation settings (Prabhakaran et al., 2014; Danescu-Niculescu-Mizil et al., 2012) or in narrative text (Sap et al., 2017; Field et al., 2019; Antoniak et al., 2019). Additionally, recent work in commonsense inference has focused on mental states of participants of a situation (e.g., Rashkin et al., 2018; Sap et al., 2019b). In contrast to reasoning about particular individuals, our work focuses on biased implications of social and demographic groups as a whole. 7 Ethical Considerations Risks in deployment Automatic detection of offensiveness or reasoning about harmful implications of language should be done with care. When deploying s"
2020.acl-main.486,D19-1482,0,0.0450103,"metric should be optimized (Corbett-Davies et al., 2017), as well as the fairness of the model on speech by different demographic groups or in different varieties of English (Mitchell et al., 2019). Additionally, deployment of such technology should discuss potential nefarious side effects, such as censorship (Ullmann and Tomalin, 2019) and dialect-based racial bias (Sap et al., 2019a; Davidson et al., 2019). Finally, offensiveness could be paired with promotions of positive online interactions, such as emphasis of community standards (Does et al., 2011) or counterspeech (Chung et al., 2019; Qian et al., 2019). Risks in annotation Recent work has highlighted various negative side effects caused by annotating potentially abusive or harmful content (e.g., acute stress; Roberts, 2016). We mitigated these by limiting the number of posts that one worker could annotate in one day, paying workers above minimum wage ($7–12), and providing crisis management resources to our annotators.13 Additionally, we acknowledge the implications of using data available on public forums for research (Zimmer, 2018) and urge researchers and practitioners to respect the privacy of the authors of posts in SBIC (Ayers et al.,"
2020.acl-main.486,P18-1043,1,0.933568,"tive analysis over large corpora can also be insightful for educating people on reducing unconscious biases in their language. 2 S OCIAL B IAS F RAMES Definition To better enable models to account for socially biased implications of language,2 we design a new pragmatic formalism that distinguishes several related but distinct inferences, shown in Figure 1. Given a natural language utterance, henceforth, post, we collect both categorical as well as free text inferences (described below), inspired by recent efforts in free-text annotations of commonsense knowledge (e.g., Speer and Havasi, 2012; Rashkin et al., 2018; Sap et al., 2019b) and argumentation (Habernal and Gurevych, 2016; Becker et al., 2017). The free-text explanations are crucial to our formalism, as they can both increase trust in predictions made by the machine (Kulesza et al., 2012; Bussone et al., 2015; Nguyen et al., 2018) and encourage a poster’s empathy towards a targeted group, thereby combating biases (CohenAlmagor, 2014). We base our initial frame design on social science literature of pragmatics (Lakoff, 1973; de Marneffe et al., 2012) and impoliteness (Kasper, 1990; Gabriel, 1998; Dynel, 2015; Vonasch and Baumeister, 2017). We th"
2020.acl-main.486,N16-3020,0,0.079531,"Missing"
2020.acl-main.486,P19-1163,1,0.544315,"a et al., 2016), and failure to do so can result in the deployment of harmful technologies (e.g., conversational AI systems turning sexist and racist; Vincent, 2016). Most previous approaches to understanding the implied harm in statements have cast this task as a simple toxicity classification (e.g., Waseem and Hovy, 2016; Founta et al., 2018; Davidson et al., 2017). However, simple classifications run the risk of discriminating against minority groups, due to high variation and identity-based biases in annotations (e.g., which cause models to learn associations between dialect and toxicity; Sap et al., 2019a; Davidson et al., 2019). In addition, detailed explanations are much more informative for people to understand and reason about why a statement is potentially harmful against other people (Gregor and Benbasat, 1999; Ribeiro et al., 2016). Thus, we propose S OCIAL B IAS F RAMES, a novel conceptual formalism that aims to model pragmatic frames in which people project social biases and stereotypes on others. Compared to semantic frames (Fillmore and Baker, 2001), the meanings projected by pragmatic frames are richer, and thus cannot be easily formalized using only categorical labels. Therefore,"
2020.acl-main.486,N19-1144,0,0.0277403,"(Sap et al., 2019c; Sakaguchi et al., 2020). 6 Related Work Bias and toxicity detection Detection of hateful, abusive, or other toxic language has received increased attention recently (Schmidt and Wiegand, 2017), and most dataset creation work has cast this detection problem as binary classification (Waseem and Hovy, 2016; Davidson et al., 2017; Founta et al., 2018). Moving beyond a single binary label, Wulczyn et al. (2017) and the PerspectiveAPI use a set of binary variables to annotate Wikipedia comments for several toxicityrelated categories (e.g., identity attack, profanity). Similarly, Zampieri et al. (2019) hierarchically annotate a dataset of tweets with offensiveness and whether a group or individual is targeted. Most related to our work, Ousidhoum et al. (2019) create a multilingual dataset of 13k tweets annotated for five different emotion- and toxicity-related aspects, including a 16-class variable representing social groups targeted. In comparison, S OCIAL B IAS F RAMES not only captures binary toxicity and hierarchical information about whether a group is targeted, but also free-text implications about 1.4k different targeted groups and the implied harm behind statements. Similar in spiri"
2020.acl-main.486,D17-1247,1,0.848658,"to this paper, recent work has tackled more subtle bias in language, such as microaggressions (Breitfeller et al., 2019) and condescension (Wang and Potts, 2019). These types of biases are in line with the biases covered by S O CIAL B IAS F RAMES , but more narrowly scoped. 5484 Inference about social dynamics Various work has tackled the task of making inferences about power and social dynamics. Particularly, previous work has analyzed power dynamics about specific entities, either in conversation settings (Prabhakaran et al., 2014; Danescu-Niculescu-Mizil et al., 2012) or in narrative text (Sap et al., 2017; Field et al., 2019; Antoniak et al., 2019). Additionally, recent work in commonsense inference has focused on mental states of participants of a situation (e.g., Rashkin et al., 2018; Sap et al., 2019b). In contrast to reasoning about particular individuals, our work focuses on biased implications of social and demographic groups as a whole. 7 Ethical Considerations Risks in deployment Automatic detection of offensiveness or reasoning about harmful implications of language should be done with care. When deploying such algorithms, ethical aspects should be considered including which performan"
2020.acl-main.486,D19-1454,1,0.861203,"a et al., 2016), and failure to do so can result in the deployment of harmful technologies (e.g., conversational AI systems turning sexist and racist; Vincent, 2016). Most previous approaches to understanding the implied harm in statements have cast this task as a simple toxicity classification (e.g., Waseem and Hovy, 2016; Founta et al., 2018; Davidson et al., 2017). However, simple classifications run the risk of discriminating against minority groups, due to high variation and identity-based biases in annotations (e.g., which cause models to learn associations between dialect and toxicity; Sap et al., 2019a; Davidson et al., 2019). In addition, detailed explanations are much more informative for people to understand and reason about why a statement is potentially harmful against other people (Gregor and Benbasat, 1999; Ribeiro et al., 2016). Thus, we propose S OCIAL B IAS F RAMES, a novel conceptual formalism that aims to model pragmatic frames in which people project social biases and stereotypes on others. Compared to semantic frames (Fillmore and Baker, 2001), the meanings projected by pragmatic frames are richer, and thus cannot be easily formalized using only categorical labels. Therefore,"
2020.acl-main.486,W17-1101,0,0.0384554,"to struggle with generating textual implications that are relevant to the post, instead generating very generic stereotypes about the demographic groups (e.g., in examples b and c). The model generates the correct stereotypes when there is high lexical overlap with the post (e.g., examples d and e). This is in line with previous research showing that large language models rely on correlational patterns in data (Sap et al., 2019c; Sakaguchi et al., 2020). 6 Related Work Bias and toxicity detection Detection of hateful, abusive, or other toxic language has received increased attention recently (Schmidt and Wiegand, 2017), and most dataset creation work has cast this detection problem as binary classification (Waseem and Hovy, 2016; Davidson et al., 2017; Founta et al., 2018). Moving beyond a single binary label, Wulczyn et al. (2017) and the PerspectiveAPI use a set of binary variables to annotate Wikipedia comments for several toxicityrelated categories (e.g., identity attack, profanity). Similarly, Zampieri et al. (2019) hierarchically annotate a dataset of tweets with offensiveness and whether a group or individual is targeted. Most related to our work, Ousidhoum et al. (2019) create a multilingual dataset"
2020.acl-main.486,speer-havasi-2012-representing,0,0.0296727,"In addition, the collective analysis over large corpora can also be insightful for educating people on reducing unconscious biases in their language. 2 S OCIAL B IAS F RAMES Definition To better enable models to account for socially biased implications of language,2 we design a new pragmatic formalism that distinguishes several related but distinct inferences, shown in Figure 1. Given a natural language utterance, henceforth, post, we collect both categorical as well as free text inferences (described below), inspired by recent efforts in free-text annotations of commonsense knowledge (e.g., Speer and Havasi, 2012; Rashkin et al., 2018; Sap et al., 2019b) and argumentation (Habernal and Gurevych, 2016; Becker et al., 2017). The free-text explanations are crucial to our formalism, as they can both increase trust in predictions made by the machine (Kulesza et al., 2012; Bussone et al., 2015; Nguyen et al., 2018) and encourage a poster’s empathy towards a targeted group, thereby combating biases (CohenAlmagor, 2014). We base our initial frame design on social science literature of pragmatics (Lakoff, 1973; de Marneffe et al., 2012) and impoliteness (Kasper, 1990; Gabriel, 1998; Dynel, 2015; Vonasch and Ba"
2020.acl-main.486,D19-1385,0,0.0579609,"houm et al. (2019) create a multilingual dataset of 13k tweets annotated for five different emotion- and toxicity-related aspects, including a 16-class variable representing social groups targeted. In comparison, S OCIAL B IAS F RAMES not only captures binary toxicity and hierarchical information about whether a group is targeted, but also free-text implications about 1.4k different targeted groups and the implied harm behind statements. Similar in spirit to this paper, recent work has tackled more subtle bias in language, such as microaggressions (Breitfeller et al., 2019) and condescension (Wang and Potts, 2019). These types of biases are in line with the biases covered by S O CIAL B IAS F RAMES , but more narrowly scoped. 5484 Inference about social dynamics Various work has tackled the task of making inferences about power and social dynamics. Particularly, previous work has analyzed power dynamics about specific entities, either in conversation settings (Prabhakaran et al., 2014; Danescu-Niculescu-Mizil et al., 2012) or in narrative text (Sap et al., 2017; Field et al., 2019; Antoniak et al., 2019). Additionally, recent work in commonsense inference has focused on mental states of participants of"
2020.acl-main.486,N16-2013,0,0.720936,"ly subtle) offensive implications about various demographic groups. recognize the implied demonizing stereotype that “Muslims are terrorists” (Figure 1). Understanding these biases with accurate underlying explanations is necessary for AI systems to adequately interact in the social world (Pereira et al., 2016), and failure to do so can result in the deployment of harmful technologies (e.g., conversational AI systems turning sexist and racist; Vincent, 2016). Most previous approaches to understanding the implied harm in statements have cast this task as a simple toxicity classification (e.g., Waseem and Hovy, 2016; Founta et al., 2018; Davidson et al., 2017). However, simple classifications run the risk of discriminating against minority groups, due to high variation and identity-based biases in annotations (e.g., which cause models to learn associations between dialect and toxicity; Sap et al., 2019a; Davidson et al., 2019). In addition, detailed explanations are much more informative for people to understand and reason about why a statement is potentially harmful against other people (Gregor and Benbasat, 1999; Ribeiro et al., 2016). Thus, we propose S OCIAL B IAS F RAMES, a novel conceptual formalis"
2020.acl-main.587,N18-1033,0,0.0501582,"achine Translation Datasets. We experiment with two machine translation datasets: • WMT14 EN-DE (Bojar et al., 2014).11 Following previous practice (Vaswani et al., 2017) we train on WMT14, and designate newstest2013 and newstest2014 as development and test data respectively. Our preprocessing follows that of Vaswani et al. (2017) and Ott et al. (2018). A shared source-target vocabulary is used, with 32k byte pair encoding types (BPE; Sennrich et al., 2016). • IWSLT14 DE-EN (Cettolo et al., 2014).12 It is based on TED talks, and is much smaller compared to WMT14. We use the preprocessing from Edunov et al. (2018). Following previous practice, we use separate vocabularies for the source and target, with around 9K and 7K BPE types respectively. Table 1 summarizes some statistics of the datasets. 10 Preliminary results show that mixing experts with fewer heads leads to underwhelming performance. We conjecture this is due to too strong a regularization effect (§3). 11 https://drive.google.com/a/ haopeng.name/uc?export=download&id=0B_ bZck-ksdkpM25jRUN2X2UxMm8 12 http://workshop2014.iwslt.org/. Data Train Dev. Test Vocab. WMT14 4.5M IWSLT14 160K 3K 7K 3K 7K 32K 9K/7K Table 1: Some statistics for WMT14 and"
2020.acl-main.587,K18-1056,0,0.102237,"CD training it suggests and benefits from. Concurrent to our work, (Fan et al., 2020) study structurally pruning transformer layers for more efficient inference. Another line of work aims to better understand the working of transformer models (Clark et al., 2019; Liu et al., 2019a; Tenney et al., 2019, inter alia). Mixture of experts. One of the most successful applications of MoE is ensemble learning (Caruana et al., 2004; Liu et al., 2018; Dutt et al., 2017, inter alia). Recent efforts also explore MoE in sequence learning (Shazeer et al., 2017), and to promote diversity in text generation (He et al., 2018; Shen et al., 2019; Cho et al., 2019, inter alia). 7 Conclusion We presented MAE. It is inspired by a mixture-ofexperts perspective of multi-head attention. With a learned gating function, MAE activates different experts on different inputs. MAE is trained using a block coordinate descent algorithm, which alternates between updating the responsibilities of 6573 the experts and their parameters. Our experiments show that MAE outperforms the transformer baselines on machine translation and language modeling benchmarks. The analysis shows that MAE learns to activate different experts. The code i"
2020.acl-main.587,P19-2030,0,0.0202487,"etuning methods (§5.2). The last two columns indicate the number of parameters to update, and the number of gradient steps needed to achieve the best development performance. 6 Related Work Multi-head attention. An increasing amount of effort has been devoted into developing better attention mechanisms (Malaviya et al., 2018; Deng et al., 2018; Sukhbaatar et al., 2019; Correia et al., 2019; Maruf et al., 2019, inter alia), and improving transformer architectures (Shaw et al., 2018; Dehghani et al., 2019; Hao et al., 2019; Correia et al., 2019; Yang et al., 2019a, inter alia). Closely related, Iida et al. (2019) applies another attention mechanism over the attention heads, allowing a learned reweighting of them. Our work focuses on the connection between multi-head attention and MoE, and the BCD training it suggests and benefits from. Concurrent to our work, (Fan et al., 2020) study structurally pruning transformer layers for more efficient inference. Another line of work aims to better understand the working of transformer models (Clark et al., 2019; Liu et al., 2019a; Tenney et al., 2019, inter alia). Mixture of experts. One of the most successful applications of MoE is ensemble learning (Caruana e"
2020.acl-main.587,D19-1308,0,0.0935488,"from. Concurrent to our work, (Fan et al., 2020) study structurally pruning transformer layers for more efficient inference. Another line of work aims to better understand the working of transformer models (Clark et al., 2019; Liu et al., 2019a; Tenney et al., 2019, inter alia). Mixture of experts. One of the most successful applications of MoE is ensemble learning (Caruana et al., 2004; Liu et al., 2018; Dutt et al., 2017, inter alia). Recent efforts also explore MoE in sequence learning (Shazeer et al., 2017), and to promote diversity in text generation (He et al., 2018; Shen et al., 2019; Cho et al., 2019, inter alia). 7 Conclusion We presented MAE. It is inspired by a mixture-ofexperts perspective of multi-head attention. With a learned gating function, MAE activates different experts on different inputs. MAE is trained using a block coordinate descent algorithm, which alternates between updating the responsibilities of 6573 the experts and their parameters. Our experiments show that MAE outperforms the transformer baselines on machine translation and language modeling benchmarks. The analysis shows that MAE learns to activate different experts. The code is publicly available at https://githu"
2020.acl-main.587,J90-1003,0,0.0989812,"less clear. gating weights and ignore the rest, instead of linearly combining them as in Eq. 6. We see from Table 5 a 0.3 BLEU decrease under this setting. In comparison, NO BCD has a larger performance decrease of 0.7 BLEU. NO BCD’s performance drop is similar to that of UNI -M AE -7, for which we randomly select an expert at each layer and average the performance over 5 runs. These results support the proposition that MAE specializes better when trained with BCD. Finally, we search for the tokens that are more likely to activate each expert. We compute the pointwise mutual information (PMI; Church and Hanks, 1990) between tokens and experts: PMI(tokeni , expertj ) = log p(tokeni , expertj ) . p(tokeni )p(expertj ) Table 6 lists the most indicative tokens of each expert, for the first layer. While some of the terms for some experts seem loosely related (e.g., bell, reuters, and computing for expert 2, it is hard to find clear patterns in most of them. 5.2 MAE’s Potential in Transfer Learning: A Case Study We now turn to evaluate another property of MAE: its potential for data-efficient transfer learning, by only updating the gating functions, freezing the experts. We consider the pretrain-then-finetune"
2020.acl-main.587,N19-1112,1,0.919486,"of h − 1 attention heads. MAE learns an input-dependent distribution of the experts (g). At each training step, a single expert is selected and updated (solid line); during the evaluation, experts’ outputs are linearly combined with weights produced by g. Introduction The transformer architecture and its variants achieve state-of-the-art performance across a variety of NLP tasks, including machine translation (Vaswani et al., 2017; Ott et al., 2018), language modeling (Radford et al., 2018; Baevski and Auli, 2019), semantic role labeling (Strubell et al., 2018), and more (Devlin et al., 2019; Liu et al., 2019b; Yang et al., 2019b). Under the hood, multihead attention provides the driving force: multiple separately parameterized attention functions act in parallel to contextualize the input representations; their outputs are then gathered by an affine transformation, and fed to onward computation. 1 Our implementation is publicly available at https:// github.com/Noahs-ARK/MAE. Recent efforts by Voita et al. (2019) and Michel et al. (2019) suggest that typical transformer networks are overparameterized, in the sense that at test time, many of the heads, or even a full layer (Fan et al., 2020), can b"
2020.acl-main.587,W19-4828,0,0.0231298,"ormer architectures (Shaw et al., 2018; Dehghani et al., 2019; Hao et al., 2019; Correia et al., 2019; Yang et al., 2019a, inter alia). Closely related, Iida et al. (2019) applies another attention mechanism over the attention heads, allowing a learned reweighting of them. Our work focuses on the connection between multi-head attention and MoE, and the BCD training it suggests and benefits from. Concurrent to our work, (Fan et al., 2020) study structurally pruning transformer layers for more efficient inference. Another line of work aims to better understand the working of transformer models (Clark et al., 2019; Liu et al., 2019a; Tenney et al., 2019, inter alia). Mixture of experts. One of the most successful applications of MoE is ensemble learning (Caruana et al., 2004; Liu et al., 2018; Dutt et al., 2017, inter alia). Recent efforts also explore MoE in sequence learning (Shazeer et al., 2017), and to promote diversity in text generation (He et al., 2018; Shen et al., 2019; Cho et al., 2019, inter alia). 7 Conclusion We presented MAE. It is inspired by a mixture-ofexperts perspective of multi-head attention. With a learned gating function, MAE activates different experts on different inputs. MAE"
2020.acl-main.587,2021.ccl-1.108,0,0.106237,"Missing"
2020.acl-main.587,D19-1223,0,0.0162426,"with F T G+ can be viable in lowresource transfer learning. 18 Here we reverse the translation direction of IWSLT14: §4.2 experimented with DE-EN, here we use EN-DE. Table 7: IWSLT14 development set performance of different finetuning methods (§5.2). The last two columns indicate the number of parameters to update, and the number of gradient steps needed to achieve the best development performance. 6 Related Work Multi-head attention. An increasing amount of effort has been devoted into developing better attention mechanisms (Malaviya et al., 2018; Deng et al., 2018; Sukhbaatar et al., 2019; Correia et al., 2019; Maruf et al., 2019, inter alia), and improving transformer architectures (Shaw et al., 2018; Dehghani et al., 2019; Hao et al., 2019; Correia et al., 2019; Yang et al., 2019a, inter alia). Closely related, Iida et al. (2019) applies another attention mechanism over the attention heads, allowing a learned reweighting of them. Our work focuses on the connection between multi-head attention and MoE, and the BCD training it suggests and benefits from. Concurrent to our work, (Fan et al., 2020) study structurally pruning transformer layers for more efficient inference. Another line of work aims t"
2020.acl-main.587,P18-2059,0,0.0191773,"T G+ outperforms F TA LL. These results suggest that finetuning MAE with F T G+ can be viable in lowresource transfer learning. 18 Here we reverse the translation direction of IWSLT14: §4.2 experimented with DE-EN, here we use EN-DE. Table 7: IWSLT14 development set performance of different finetuning methods (§5.2). The last two columns indicate the number of parameters to update, and the number of gradient steps needed to achieve the best development performance. 6 Related Work Multi-head attention. An increasing amount of effort has been devoted into developing better attention mechanisms (Malaviya et al., 2018; Deng et al., 2018; Sukhbaatar et al., 2019; Correia et al., 2019; Maruf et al., 2019, inter alia), and improving transformer architectures (Shaw et al., 2018; Dehghani et al., 2019; Hao et al., 2019; Correia et al., 2019; Yang et al., 2019a, inter alia). Closely related, Iida et al. (2019) applies another attention mechanism over the attention heads, allowing a learned reweighting of them. Our work focuses on the connection between multi-head attention and MoE, and the BCD training it suggests and benefits from. Concurrent to our work, (Fan et al., 2020) study structurally pruning transforme"
2020.acl-main.587,N19-1313,0,0.0211404,"able in lowresource transfer learning. 18 Here we reverse the translation direction of IWSLT14: §4.2 experimented with DE-EN, here we use EN-DE. Table 7: IWSLT14 development set performance of different finetuning methods (§5.2). The last two columns indicate the number of parameters to update, and the number of gradient steps needed to achieve the best development performance. 6 Related Work Multi-head attention. An increasing amount of effort has been devoted into developing better attention mechanisms (Malaviya et al., 2018; Deng et al., 2018; Sukhbaatar et al., 2019; Correia et al., 2019; Maruf et al., 2019, inter alia), and improving transformer architectures (Shaw et al., 2018; Dehghani et al., 2019; Hao et al., 2019; Correia et al., 2019; Yang et al., 2019a, inter alia). Closely related, Iida et al. (2019) applies another attention mechanism over the attention heads, allowing a learned reweighting of them. Our work focuses on the connection between multi-head attention and MoE, and the BCD training it suggests and benefits from. Concurrent to our work, (Fan et al., 2020) study structurally pruning transformer layers for more efficient inference. Another line of work aims to better understand"
2020.acl-main.587,P19-1032,0,0.0238442,"ggest that finetuning MAE with F T G+ can be viable in lowresource transfer learning. 18 Here we reverse the translation direction of IWSLT14: §4.2 experimented with DE-EN, here we use EN-DE. Table 7: IWSLT14 development set performance of different finetuning methods (§5.2). The last two columns indicate the number of parameters to update, and the number of gradient steps needed to achieve the best development performance. 6 Related Work Multi-head attention. An increasing amount of effort has been devoted into developing better attention mechanisms (Malaviya et al., 2018; Deng et al., 2018; Sukhbaatar et al., 2019; Correia et al., 2019; Maruf et al., 2019, inter alia), and improving transformer architectures (Shaw et al., 2018; Dehghani et al., 2019; Hao et al., 2019; Correia et al., 2019; Yang et al., 2019a, inter alia). Closely related, Iida et al. (2019) applies another attention mechanism over the attention heads, allowing a learned reweighting of them. Our work focuses on the connection between multi-head attention and MoE, and the BCD training it suggests and benefits from. Concurrent to our work, (Fan et al., 2020) study structurally pruning transformer layers for more efficient inference. Anoth"
2020.acl-main.587,P19-1452,0,0.0184234,"Dehghani et al., 2019; Hao et al., 2019; Correia et al., 2019; Yang et al., 2019a, inter alia). Closely related, Iida et al. (2019) applies another attention mechanism over the attention heads, allowing a learned reweighting of them. Our work focuses on the connection between multi-head attention and MoE, and the BCD training it suggests and benefits from. Concurrent to our work, (Fan et al., 2020) study structurally pruning transformer layers for more efficient inference. Another line of work aims to better understand the working of transformer models (Clark et al., 2019; Liu et al., 2019a; Tenney et al., 2019, inter alia). Mixture of experts. One of the most successful applications of MoE is ensemble learning (Caruana et al., 2004; Liu et al., 2018; Dutt et al., 2017, inter alia). Recent efforts also explore MoE in sequence learning (Shazeer et al., 2017), and to promote diversity in text generation (He et al., 2018; Shen et al., 2019; Cho et al., 2019, inter alia). 7 Conclusion We presented MAE. It is inspired by a mixture-ofexperts perspective of multi-head attention. With a learned gating function, MAE activates different experts on different inputs. MAE is trained using a block coordinate desc"
2020.acl-main.587,W18-6301,0,0.183155,"Figure 1: Illustration of MAE: a mixture of attentive experts. Each Hi box is an attention head in a given layer; there are h of them in total. Experts are groups of h − 1 attention heads. MAE learns an input-dependent distribution of the experts (g). At each training step, a single expert is selected and updated (solid line); during the evaluation, experts’ outputs are linearly combined with weights produced by g. Introduction The transformer architecture and its variants achieve state-of-the-art performance across a variety of NLP tasks, including machine translation (Vaswani et al., 2017; Ott et al., 2018), language modeling (Radford et al., 2018; Baevski and Auli, 2019), semantic role labeling (Strubell et al., 2018), and more (Devlin et al., 2019; Liu et al., 2019b; Yang et al., 2019b). Under the hood, multihead attention provides the driving force: multiple separately parameterized attention functions act in parallel to contextualize the input representations; their outputs are then gathered by an affine transformation, and fed to onward computation. 1 Our implementation is publicly available at https:// github.com/Noahs-ARK/MAE. Recent efforts by Voita et al. (2019) and Michel et al. (2019)"
2020.acl-main.587,P02-1040,0,0.106804,"1 summarizes some statistics of the datasets. 10 Preliminary results show that mixing experts with fewer heads leads to underwhelming performance. We conjecture this is due to too strong a regularization effect (§3). 11 https://drive.google.com/a/ haopeng.name/uc?export=download&id=0B_ bZck-ksdkpM25jRUN2X2UxMm8 12 http://workshop2014.iwslt.org/. Data Train Dev. Test Vocab. WMT14 4.5M IWSLT14 160K 3K 7K 3K 7K 32K 9K/7K Table 1: Some statistics for WMT14 and IWSLT14 datasets. We use separate source and target vocabularies in IWSLT14 experiments. Evaluation. The models are evaluated using BLEU (Papineni et al., 2002). A beam search with beam size 5 is used. In the WMT14 experiments, we follow Vaswani et al. (2017), and apply a compound split postprocessing.13 Results. Table 2 summarizes WMT14 EN-DE translation test performance. The base and large sized transformer models are due to Vaswani et al. (2017). To control for compounding factors, we additionally compare to our implementation of the base sized model (BASE). It achieves slightly better performance than Vaswani et al. (2017), with a 0.3 BLEU edge. M AE -7 improves over the base transformer by 0.8 BLEU, obtaining similar performance to the large-siz"
2020.acl-main.587,N18-1202,0,0.110339,"Missing"
2020.acl-main.587,P19-1580,0,0.394215,"ion (Vaswani et al., 2017; Ott et al., 2018), language modeling (Radford et al., 2018; Baevski and Auli, 2019), semantic role labeling (Strubell et al., 2018), and more (Devlin et al., 2019; Liu et al., 2019b; Yang et al., 2019b). Under the hood, multihead attention provides the driving force: multiple separately parameterized attention functions act in parallel to contextualize the input representations; their outputs are then gathered by an affine transformation, and fed to onward computation. 1 Our implementation is publicly available at https:// github.com/Noahs-ARK/MAE. Recent efforts by Voita et al. (2019) and Michel et al. (2019) suggest that typical transformer networks are overparameterized, in the sense that at test time, many of the heads, or even a full layer (Fan et al., 2020), can be removed without significant loss in performance.2 In response to this observation, they propose to prune the unimportant attention heads in the model after it is trained, aiming for faster inference. In this paper, we ask whether, instead of reducing the model capacity, we can use it more effectively. We propose mixture of attentive experts (MAE). MAE retains all attention heads, and learns to activate diff"
2020.acl-main.587,N19-1407,0,0.220239,"heads. MAE learns an input-dependent distribution of the experts (g). At each training step, a single expert is selected and updated (solid line); during the evaluation, experts’ outputs are linearly combined with weights produced by g. Introduction The transformer architecture and its variants achieve state-of-the-art performance across a variety of NLP tasks, including machine translation (Vaswani et al., 2017; Ott et al., 2018), language modeling (Radford et al., 2018; Baevski and Auli, 2019), semantic role labeling (Strubell et al., 2018), and more (Devlin et al., 2019; Liu et al., 2019b; Yang et al., 2019b). Under the hood, multihead attention provides the driving force: multiple separately parameterized attention functions act in parallel to contextualize the input representations; their outputs are then gathered by an affine transformation, and fed to onward computation. 1 Our implementation is publicly available at https:// github.com/Noahs-ARK/MAE. Recent efforts by Voita et al. (2019) and Michel et al. (2019) suggest that typical transformer networks are overparameterized, in the sense that at test time, many of the heads, or even a full layer (Fan et al., 2020), can be removed without si"
2020.acl-main.587,P16-1162,0,0.091906,"• UNI -M AE -6 mixes 28 6-attention-head experts, and is otherwise the same as UNI M AE -7. We refer the readers to Appendix A for implementation details. 4.2 Machine Translation Datasets. We experiment with two machine translation datasets: • WMT14 EN-DE (Bojar et al., 2014).11 Following previous practice (Vaswani et al., 2017) we train on WMT14, and designate newstest2013 and newstest2014 as development and test data respectively. Our preprocessing follows that of Vaswani et al. (2017) and Ott et al. (2018). A shared source-target vocabulary is used, with 32k byte pair encoding types (BPE; Sennrich et al., 2016). • IWSLT14 DE-EN (Cettolo et al., 2014).12 It is based on TED talks, and is much smaller compared to WMT14. We use the preprocessing from Edunov et al. (2018). Following previous practice, we use separate vocabularies for the source and target, with around 9K and 7K BPE types respectively. Table 1 summarizes some statistics of the datasets. 10 Preliminary results show that mixing experts with fewer heads leads to underwhelming performance. We conjecture this is due to too strong a regularization effect (§3). 11 https://drive.google.com/a/ haopeng.name/uc?export=download&id=0B_ bZck-ksdkpM25jR"
2020.acl-main.587,N18-2074,0,0.0321796,"direction of IWSLT14: §4.2 experimented with DE-EN, here we use EN-DE. Table 7: IWSLT14 development set performance of different finetuning methods (§5.2). The last two columns indicate the number of parameters to update, and the number of gradient steps needed to achieve the best development performance. 6 Related Work Multi-head attention. An increasing amount of effort has been devoted into developing better attention mechanisms (Malaviya et al., 2018; Deng et al., 2018; Sukhbaatar et al., 2019; Correia et al., 2019; Maruf et al., 2019, inter alia), and improving transformer architectures (Shaw et al., 2018; Dehghani et al., 2019; Hao et al., 2019; Correia et al., 2019; Yang et al., 2019a, inter alia). Closely related, Iida et al. (2019) applies another attention mechanism over the attention heads, allowing a learned reweighting of them. Our work focuses on the connection between multi-head attention and MoE, and the BCD training it suggests and benefits from. Concurrent to our work, (Fan et al., 2020) study structurally pruning transformer layers for more efficient inference. Another line of work aims to better understand the working of transformer models (Clark et al., 2019; Liu et al., 2019a;"
2020.acl-main.587,D18-1548,0,0.02049,"ayer; there are h of them in total. Experts are groups of h − 1 attention heads. MAE learns an input-dependent distribution of the experts (g). At each training step, a single expert is selected and updated (solid line); during the evaluation, experts’ outputs are linearly combined with weights produced by g. Introduction The transformer architecture and its variants achieve state-of-the-art performance across a variety of NLP tasks, including machine translation (Vaswani et al., 2017; Ott et al., 2018), language modeling (Radford et al., 2018; Baevski and Auli, 2019), semantic role labeling (Strubell et al., 2018), and more (Devlin et al., 2019; Liu et al., 2019b; Yang et al., 2019b). Under the hood, multihead attention provides the driving force: multiple separately parameterized attention functions act in parallel to contextualize the input representations; their outputs are then gathered by an affine transformation, and fed to onward computation. 1 Our implementation is publicly available at https:// github.com/Noahs-ARK/MAE. Recent efforts by Voita et al. (2019) and Michel et al. (2019) suggest that typical transformer networks are overparameterized, in the sense that at test time, many of the head"
2020.acl-main.587,W14-3302,0,\N,Missing
2020.acl-main.587,N19-1423,0,\N,Missing
2020.acl-main.593,D15-1075,0,0.0320946,"natural language inference (NLI) tasks in English. NLI is a pairwise sentence classification task, where the goal is to predict whether a hypothesis sentence entails, contradicts or is neutral to a premise sentence (Dagan et al., 2005). Below we describe our datasets, our baselines, and our experimental setup. Datasets For text classification, we experiment with the AG news topic identification dataset (Zhang et al., 2015) and two sentiment analysis datasets: IMDB (Maas et al., 2011) and the binary Stanford sentiment treebank (SST; Socher et al., 2013).8 For NLI, we experiment with the SNLI (Bowman et al., 2015) and MultiNLI (MNLI; Williams et al., 2018) datasets. We use the standard train-development-test splits for all datasets except for MNLI, for which there is no public test set. As MNLI contains two validation sets (matched and mismatched), we use the matched validation set as our validation set and the mismatched validation set as our test set. See Table 1 for dataset statistics. Baselines We use two types of baselines: running BERT-large in the standard way, with a single output layer on top of the last layer, and three efficient baselines of increasing size (Figure 2). Each is a fine-tuned B"
2020.acl-main.593,2020.ngt-1.3,0,0.0718724,"Missing"
2020.acl-main.593,N10-1115,0,0.0467783,"uracy. In contrast, our model is overconfident in its prediction of some labels (business for AG, positive for SST), and underconfident in others (tech for AG, entailment for MNLI). These findings might indicate that while our method is designed to be globally calibrated, it is not necessarily calibrated for each label individually. Such observations relate to existing concerns regarding fairness when using calibrated classifiers (Pleiss et al., 2017). 7 Related Work Methods for making inference more efficient have received considerable attention in NLP over the years (Eisner and Satta, 1999; Goldberg and Elhadad, 2010, inter alia). As the field has converged on deep neural architecture solutions, most efforts focus on making models smaller (in terms of model parameters) in order to save space as well as potentially speed up inference. In model distillation (Hinton et al., 2014) a smaller model (the student) is trained to mimic the behavior or structure of the original, larger model (the teacher). The result is typically a student that is as accurate as the teacher, but smaller and faster (Kim and Rush, 2016; Jiao et al., 2019; Tang et al., 2019; Sanh et al., 2019). Pruning (LeCun et al., 1990) removes some"
2020.acl-main.593,N18-2017,1,0.889082,"Missing"
2020.acl-main.593,2020.emnlp-main.21,0,0.0574536,"r k is given as input a weighted sum of all the layers up to and including k, such that the weight is learned during fine-tuning (Peters et al., 2018).7 Calibration Classifiers’ confidence scores are not always reliable (Jiang et al., 2018). One way to mitigate this concern is to use calibration, which encourages the confidence level to correspond to the probability that the model is correct (DeGroot and Fienberg, 1983). In this paper we use temperature calibration, which is a simple technique that has been shown to work well in practice (Guo et al., 2017), in particular for BERT fine-tuning (Desai and Durrett, 2020). The method learns a single parameter, denoted temperature or T , and divides each of the logits {zi } by T before applying the softmax function: exp(zi /T ) pred = arg max P i j exp(zj /T ) We select T to maximize the log-likelihood of the development dataset. Note that temperature calibration is monotonic and thus does not influence predictions. It is only used in our model to make early-exit decisions. Discussion Our approach has several attractive properties. First, if mi is not sufficiently confident in its prediction, we reuse the computation and continue towards mi+1 without recomputin"
2020.acl-main.593,N19-1423,0,0.574981,"an early exit, avoiding the computation associated with successive (higher) layers (grayed out). Otherwise, the model continues to the next layer/classifier. The large increase in the size of artificial intelligence models often increases production costs (Amodei and Hernandez, 2018; Schwartz et al., 2019), and can also limit adoption on real-time devices. Compared to training, which is a one-time large investment, inference costs are incurred for every instance in production, and can thus add up ∗ No No Introduction 1 Prediction significantly. For instance, Microsoft reports that using BERT (Devlin et al., 2019) to process Bing queries requires more than 2,000 GPUs concurrently.2 We present a method to reduce the inference cost of today’s common models in NLP: fine-tuned contextual word representations. Our method exploits variation along two axes: models differ in size and cost, and instances vary in difficulty. Our method assesses the complexity of each test instance and matches it with the most efficient model in our “toolbelt.”3 As a result, some instances, which we refer to in this paper as “easy” or “simple,” can be solved by small models, leading to computational savings, while other instances"
2020.acl-main.593,D16-1139,0,0.0349806,"fficient have received considerable attention in NLP over the years (Eisner and Satta, 1999; Goldberg and Elhadad, 2010, inter alia). As the field has converged on deep neural architecture solutions, most efforts focus on making models smaller (in terms of model parameters) in order to save space as well as potentially speed up inference. In model distillation (Hinton et al., 2014) a smaller model (the student) is trained to mimic the behavior or structure of the original, larger model (the teacher). The result is typically a student that is as accurate as the teacher, but smaller and faster (Kim and Rush, 2016; Jiao et al., 2019; Tang et al., 2019; Sanh et al., 2019). Pruning (LeCun et al., 1990) removes some of the weights in the network, resulting in a smaller, potentially faster network. The basic pruning approach removes individual weights from the network (Swayamdipta et al., 2018; Gale et al., 2019). More sophisticated approaches induce structured sparsity, which removes full blocks (Michel et al., 2019; Voita et al., 2019; Dodge et al., 2019b). Liu et al. (2018) and Fan et al. (2020) pruned deep models by applying dropout to different layers, which allows dynamic control of 6647 Figure 5: In"
2020.acl-main.593,D19-1224,1,0.921999,"1, which is applied to each confidence score to decide whether to exit early. Lower thresholds result in earlier exits, with 0 implying the most efficient classifier is always used. A threshold of 1 always uses the most expensive and accurate classifier. 5 Results A better speed/accuracy tradeoff. Figure 3 presents our test results.10 The blue line shows our model, where each point corresponds to an increasingly large confidence threshold. The leftmost 9 Preliminary experiments with other configurations, including ones with more layers, led to similar results. 10 For increased reproduciblity (Dodge et al., 2019a), we also report validation results in Appendix B. 6643 Layer n Layer n Prediction Layer n Prediction No Layer l Layer l Layer l Layer k Layer k Layer k Is confident? Layer j Layer j Is confident? Layer i Layer i Layer i Layer 0 Layer 0 Layer 0 Input Input Input Yes Early exit prediction No Layer j Prediction (a) Efficient Baseline (b) Standard Baseline Yes Early exit prediction No Is confident? Yes Early exit prediction (c) Our approach Figure 2: Illustration of our baselines. (2a) Efficient baseline: adding a single output layer to an intermediate layer, while not processing the remaining"
2020.acl-main.593,D19-1110,1,0.923288,"1, which is applied to each confidence score to decide whether to exit early. Lower thresholds result in earlier exits, with 0 implying the most efficient classifier is always used. A threshold of 1 always uses the most expensive and accurate classifier. 5 Results A better speed/accuracy tradeoff. Figure 3 presents our test results.10 The blue line shows our model, where each point corresponds to an increasingly large confidence threshold. The leftmost 9 Preliminary experiments with other configurations, including ones with more layers, led to similar results. 10 For increased reproduciblity (Dodge et al., 2019a), we also report validation results in Appendix B. 6643 Layer n Layer n Prediction Layer n Prediction No Layer l Layer l Layer l Layer k Layer k Layer k Is confident? Layer j Layer j Is confident? Layer i Layer i Layer i Layer 0 Layer 0 Layer 0 Input Input Input Yes Early exit prediction No Layer j Prediction (a) Efficient Baseline (b) Standard Baseline Yes Early exit prediction No Is confident? Yes Early exit prediction (c) Our approach Figure 2: Illustration of our baselines. (2a) Efficient baseline: adding a single output layer to an intermediate layer, while not processing the remaining"
2020.acl-main.593,P99-1059,0,0.149133,"and solved with high accuracy. In contrast, our model is overconfident in its prediction of some labels (business for AG, positive for SST), and underconfident in others (tech for AG, entailment for MNLI). These findings might indicate that while our method is designed to be globally calibrated, it is not necessarily calibrated for each label individually. Such observations relate to existing concerns regarding fairness when using calibrated classifiers (Pleiss et al., 2017). 7 Related Work Methods for making inference more efficient have received considerable attention in NLP over the years (Eisner and Satta, 1999; Goldberg and Elhadad, 2010, inter alia). As the field has converged on deep neural architecture solutions, most efforts focus on making models smaller (in terms of model parameters) in order to save space as well as potentially speed up inference. In model distillation (Hinton et al., 2014) a smaller model (the student) is trained to mimic the behavior or structure of the original, larger model (the teacher). The result is typically a student that is as accurate as the teacher, but smaller and faster (Kim and Rush, 2016; Jiao et al., 2019; Tang et al., 2019; Sanh et al., 2019). Pruning (LeCu"
2020.acl-main.593,W18-2501,0,0.0401913,"Missing"
2020.acl-main.593,D18-1153,0,0.0765574,"riginal, larger model (the teacher). The result is typically a student that is as accurate as the teacher, but smaller and faster (Kim and Rush, 2016; Jiao et al., 2019; Tang et al., 2019; Sanh et al., 2019). Pruning (LeCun et al., 1990) removes some of the weights in the network, resulting in a smaller, potentially faster network. The basic pruning approach removes individual weights from the network (Swayamdipta et al., 2018; Gale et al., 2019). More sophisticated approaches induce structured sparsity, which removes full blocks (Michel et al., 2019; Voita et al., 2019; Dodge et al., 2019b). Liu et al. (2018) and Fan et al. (2020) pruned deep models by applying dropout to different layers, which allows dynamic control of 6647 Figure 5: Instances with different labels are predicted with different degrees of confidence. Figure 6: Comparing confidence levels and F1 scores of our most efficient classifier across datasets and labels. High confidence by the model is sometimes explained by “easy” classes that are predicted with high F1 (e.g., sports in AG). Other cases might stem from biases of the model which make it overconfident despite the label being harder than other labels (e.g., positive in SST)."
2020.acl-main.593,2020.acl-main.537,0,0.0823282,"y reducing training and/or inference time (Graves, 2016; Seo et al., 2018). Our method also puts less resources into some of the input, but does so at the document level rather than for individual tokens. A few concurrent works have explored similar ideas for dynamic early exits in the transformer model. Elbayad et al. (2020) and Dabre et al. (2020) introduced early stopping for sequence-tosequence tasks (e.g., machine translation). Bapna et al. (2020) modify the transformer architecture with “control symbols” which determine whether components are short-circuited to optimize budget. Finally, Liu et al. (2020) investigated several inference-time cost optimizations (including early stopping) in a multilingual setting. Several computer vision works explored similar ideas to the one in this paper. Wang et al. (2018) introduced a method for dynamically skipping convolutional layers. Bolukbasi et al. (2017) and Huang et al. (2018) learned early exit policies for computer vision architectures, observing substantial computational gains. 8 Conclusion We presented a method that improves the speed/accuracy tradeoff for inference using pretrained language models. Our method makes early exits for simple instan"
2020.acl-main.593,P11-1015,0,0.0160066,"ts, and the bottom set are NLI datasets. 4 Experiments To test our approach, we experiment with three text classification and two natural language inference (NLI) tasks in English. NLI is a pairwise sentence classification task, where the goal is to predict whether a hypothesis sentence entails, contradicts or is neutral to a premise sentence (Dagan et al., 2005). Below we describe our datasets, our baselines, and our experimental setup. Datasets For text classification, we experiment with the AG news topic identification dataset (Zhang et al., 2015) and two sentiment analysis datasets: IMDB (Maas et al., 2011) and the binary Stanford sentiment treebank (SST; Socher et al., 2013).8 For NLI, we experiment with the SNLI (Bowman et al., 2015) and MultiNLI (MNLI; Williams et al., 2018) datasets. We use the standard train-development-test splits for all datasets except for MNLI, for which there is no public test set. As MNLI contains two validation sets (matched and mismatched), we use the matched validation set as our validation set and the mismatched validation set as our test set. See Table 1 for dataset statistics. Baselines We use two types of baselines: running BERT-large in the standard way, with"
2020.acl-main.593,N18-1202,0,0.0415422,"s function, we sum the losses of all classification layers, such that lower layers are trained to both be useful as feature generators for the higher layers, and as input to their respective classifiers. This also means that every output layer is trained to perform well on all instances. Importantly, we do not perform early exits during training, but only during inference. To encourage monotonicity in performance of the different classifiers, each classifier at layer k is given as input a weighted sum of all the layers up to and including k, such that the weight is learned during fine-tuning (Peters et al., 2018).7 Calibration Classifiers’ confidence scores are not always reliable (Jiang et al., 2018). One way to mitigate this concern is to use calibration, which encourages the confidence level to correspond to the probability that the model is correct (DeGroot and Fienberg, 1983). In this paper we use temperature calibration, which is a simple technique that has been shown to work well in practice (Guo et al., 2017), in particular for BERT fine-tuning (Desai and Durrett, 2020). The method learns a single parameter, denoted temperature or T , and divides each of the logits {zi } by T before applying t"
2020.acl-main.593,S18-2023,0,0.0641465,"Missing"
2020.acl-main.593,P19-1580,0,0.0284801,"o mimic the behavior or structure of the original, larger model (the teacher). The result is typically a student that is as accurate as the teacher, but smaller and faster (Kim and Rush, 2016; Jiao et al., 2019; Tang et al., 2019; Sanh et al., 2019). Pruning (LeCun et al., 1990) removes some of the weights in the network, resulting in a smaller, potentially faster network. The basic pruning approach removes individual weights from the network (Swayamdipta et al., 2018; Gale et al., 2019). More sophisticated approaches induce structured sparsity, which removes full blocks (Michel et al., 2019; Voita et al., 2019; Dodge et al., 2019b). Liu et al. (2018) and Fan et al. (2020) pruned deep models by applying dropout to different layers, which allows dynamic control of 6647 Figure 5: Instances with different labels are predicted with different degrees of confidence. Figure 6: Comparing confidence levels and F1 scores of our most efficient classifier across datasets and labels. High confidence by the model is sometimes explained by “easy” classes that are predicted with high F1 (e.g., sports in AG). Other cases might stem from biases of the model which make it overconfident despite the label being harder t"
2020.acl-main.593,N18-1101,0,0.0507775,"n English. NLI is a pairwise sentence classification task, where the goal is to predict whether a hypothesis sentence entails, contradicts or is neutral to a premise sentence (Dagan et al., 2005). Below we describe our datasets, our baselines, and our experimental setup. Datasets For text classification, we experiment with the AG news topic identification dataset (Zhang et al., 2015) and two sentiment analysis datasets: IMDB (Maas et al., 2011) and the binary Stanford sentiment treebank (SST; Socher et al., 2013).8 For NLI, we experiment with the SNLI (Bowman et al., 2015) and MultiNLI (MNLI; Williams et al., 2018) datasets. We use the standard train-development-test splits for all datasets except for MNLI, for which there is no public test set. As MNLI contains two validation sets (matched and mismatched), we use the matched validation set as our validation set and the mismatched validation set as our test set. See Table 1 for dataset statistics. Baselines We use two types of baselines: running BERT-large in the standard way, with a single output layer on top of the last layer, and three efficient baselines of increasing size (Figure 2). Each is a fine-tuned BERT model with a single output layer after"
2020.acl-main.593,J81-4005,0,0.691775,"Missing"
2020.acl-main.593,D13-1170,0,0.0050123,"pproach, we experiment with three text classification and two natural language inference (NLI) tasks in English. NLI is a pairwise sentence classification task, where the goal is to predict whether a hypothesis sentence entails, contradicts or is neutral to a premise sentence (Dagan et al., 2005). Below we describe our datasets, our baselines, and our experimental setup. Datasets For text classification, we experiment with the AG news topic identification dataset (Zhang et al., 2015) and two sentiment analysis datasets: IMDB (Maas et al., 2011) and the binary Stanford sentiment treebank (SST; Socher et al., 2013).8 For NLI, we experiment with the SNLI (Bowman et al., 2015) and MultiNLI (MNLI; Williams et al., 2018) datasets. We use the standard train-development-test splits for all datasets except for MNLI, for which there is no public test set. As MNLI contains two validation sets (matched and mismatched), we use the matched validation set as our validation set and the mismatched validation set as our test set. See Table 1 for dataset statistics. Baselines We use two types of baselines: running BERT-large in the standard way, with a single output layer on top of the last layer, and three efficient ba"
2020.acl-main.740,N19-1054,0,0.0409608,"9.81.4 80.80.6 81.20.8 81.70.4 83.40.3 82.50.5 83.00.3 TAPT 50NN - TAPT 150NN - TAPT 500NN - TAPT Curated-TAPT DAPT DAPT + TAPT Table 9: Computational requirements for adapting to the RCT-500 task, comparing DAPT (§3) and the various TAPT modifications described in §4 and §5. benefit ratio in this comparison, one must also take into account the cost of curating large in-domain data. Automatic methods such as k NN - TAPT are much cheaper than DAPT. 6 Related Work Transfer learning for domain adaptation Prior work has shown the benefit of continued pretraining in domain (Alsentzer et al., 2019; Chakrabarty et al., 2019; Lee et al., 2019).5 We have contributed further investigation of the effects of a shift between a large, diverse pretraining corpus and target domain on task performance. Other studies (e.g., Huang et al., 2019) have trained language models (LMs) in their domain of interest, from scratch. In contrast, our work explores multiple domains, and is arguably more cost effective, since we continue pretraining an already powerful LM. Task-adaptive pretraining Continued pretraining of a LM on the unlabeled data of a given task (TAPT) has been show to be beneficial for endtask performance (e.g. in How"
2020.acl-main.740,P18-1031,0,0.468347,"d low-resource settings. Above, we consider domains defined around genres and forums, but it is also possible to induce a domain from a given corpus used for a task, such as the one used in supervised training of a model. This raises the question of whether pretraining on a corpus more directly tied to the task can further improve performance. We study how domainadaptive pretraining compares to task-adaptive pretraining, or TAPT, on a smaller but directly taskrelevant corpus: the unlabeled task dataset (§4), drawn from the task distribution. Task-adaptive pretraining has been shown effective (Howard and Ruder, 2018), but is not typically used with the most recent models. We find that TAPT provides a large performance boost for RO BERTA, with or without domain-adaptive pretraining. Finally, we show that the benefits from taskadaptive pretraining increase when we have additional unlabeled data from the task distribution that has been manually curated by task designers or annotators. Inspired by this success, we propose ways to automatically select additional task-relevant unlabeled text, and show how this improves performance in certain low-resource cases (§5). On all tasks, our results using adaptive pret"
2020.acl-main.740,N19-1213,0,0.0227867,"inue pretraining an already powerful LM. Task-adaptive pretraining Continued pretraining of a LM on the unlabeled data of a given task (TAPT) has been show to be beneficial for endtask performance (e.g. in Howard and Ruder, 2018; Phang et al., 2018; Sun et al., 2019). In the presence of domain shift between train and test data distributions of the same task, domain-adaptive pretraining (DAPT) is sometimes used to describe what we term TAPT (Logeswaran et al., 2019; Han and Eisenstein, 2019). Related approaches include language modeling as an auxiliary objective to task classifier fine-tuning (Chronopoulou et al., 2019; Radford et al., 2018) or consider simple syntactic structure of the input while adapting to task-specific 5 3 We deduplicated this set to limit computation, since different sentences can share neighbors. 4 We use a flat search index with cosine similarity between embeddings with the FAISS (Johnson et al., 2019) library. In contrast, Peters et al. (2019) find that the JensenShannon divergence on term distributions between BERT’s pretraining corpora and each M ULTI NLI domain (Williams et al., 2018) does not predict its performance, though this might be an isolated finding specific to the Mult"
2020.acl-main.740,D19-1383,1,0.905779,"Missing"
2020.acl-main.740,N19-1149,0,0.0258252,"se pretraining explored in this paper. data (Swayamdipta et al., 2019). We compare DAPT and TAPT as well as their interplay with respect to dataset size for continued pretraining (hence, expense of more rounds of pretraining), relevance to a data sample of a given task, and transferability to other tasks and datasets. See Table 11 in Appendix §A for a summary of multi-phase pretraining strategies from related work. Data selection for transfer learning Selecting data for transfer learning has been explored in NLP (Moore and Lewis, 2010; Ruder and Plank, 2017; Zhang et al., 2019, among others). Dai et al. (2019) focus on identifying the most suitable corpus to pretrain a LM from scratch, for a single task: NER, whereas we select relevant examples for various tasks in §5.2. Concurrent to our work, Aharoni and Goldberg (2020) propose data selection methods for NMT based on cosine similarity in embedding space, using D ISTIL BERT (Sanh et al., 2019) for efficiency. In contrast, we use VAMPIRE, and focus on augmenting TAPT data for text classification tasks. Khandelwal et al. (2020) introduced kNN-LMs that allows easy domain adaptation of pretrained LMs by simply adding a datastore per domain and no furt"
2020.acl-main.740,Q18-1028,0,0.0222408,"† R EVIEWS Train (Lab.) Train (Unl.) Dev. Test Classes relation classification abstract sent. roles 4169 18040 - 2427 30212 3469 30135 13 5 ACL-ARC S CI ERC citation intent relation classification 1688 3219 - 114 455 139 974 6 7 H YPER PARTISAN AGN EWS partisanship topic 515 115000 5000 - 65 5000 65 7600 2 4 † review helpfulness review sentiment 115251 20000 50000 5000 5000 25000 25000 2 2 † H ELPFULNESS IMDB Table 2: Specifications of the various target task datasets. † indicates high-resource settings. Sources: C HEM P ROT (Kringelum et al., 2016), RCT (Dernoncourt and Lee, 2017), ACL-ARC (Jurgens et al., 2018), S CI ERC (Luan et al., 2018), H YPER PARTISAN (Kiesel et al., 2019), AGN EWS (Zhang et al., 2015), H ELPFULNESS (McAuley et al., 2015), IMDB (Maas et al., 2011). RO BA . DAPT ¬DAPT C HEM P ROT 81.91.0 87.20.1 84.20.2 87.60.1 79.41.3 86.90.1 63.05.8 77.31.9 75.42.5 80.81.5 66.44.1 79.20.9 H Y P. N EWS † AGN EWS 86.60.9 93.90.2 88.25.9 93.90.2 76.44.9 93.50.2 † H ELPFUL . 65.13.4 95.00.2 66.51.4 95.40.2 65.12.8 94.10.4 Dom. Task BM † RCT ACL-ARC CS S CI ERC R EV. † IMDB Table 3: Comparison of RO BERTA (RO BA .) and DAPT to adaptation to an irrelevant domain (¬ DAPT ). Reported results are test"
2020.acl-main.740,I17-2052,0,0.0432778,"B IO M ED † C HEM P ROT RCT CS N EWS † R EVIEWS Train (Lab.) Train (Unl.) Dev. Test Classes relation classification abstract sent. roles 4169 18040 - 2427 30212 3469 30135 13 5 ACL-ARC S CI ERC citation intent relation classification 1688 3219 - 114 455 139 974 6 7 H YPER PARTISAN AGN EWS partisanship topic 515 115000 5000 - 65 5000 65 7600 2 4 † review helpfulness review sentiment 115251 20000 50000 5000 5000 25000 25000 2 2 † H ELPFULNESS IMDB Table 2: Specifications of the various target task datasets. † indicates high-resource settings. Sources: C HEM P ROT (Kringelum et al., 2016), RCT (Dernoncourt and Lee, 2017), ACL-ARC (Jurgens et al., 2018), S CI ERC (Luan et al., 2018), H YPER PARTISAN (Kiesel et al., 2019), AGN EWS (Zhang et al., 2015), H ELPFULNESS (McAuley et al., 2015), IMDB (Maas et al., 2011). RO BA . DAPT ¬DAPT C HEM P ROT 81.91.0 87.20.1 84.20.2 87.60.1 79.41.3 86.90.1 63.05.8 77.31.9 75.42.5 80.81.5 66.44.1 79.20.9 H Y P. N EWS † AGN EWS 86.60.9 93.90.2 88.25.9 93.90.2 76.44.9 93.50.2 † H ELPFUL . 65.13.4 95.00.2 66.51.4 95.40.2 65.12.8 94.10.4 Dom. Task BM † RCT ACL-ARC CS S CI ERC R EV. † IMDB Table 3: Comparison of RO BERTA (RO BA .) and DAPT to adaptation to an irrelevant domain (¬ D"
2020.acl-main.740,N19-1423,0,0.607495,"man-curated datasets, and a simple data selection strategy to automatically approach this performance. Our code as well as pretrained models for multiple domains and tasks are publicly available.1 pora. The word (or wordpiece; Wu et al. 2016) representations learned in the pretrained model are then reused in supervised training for a downstream task, with optional updates (fine-tuning) of the representations and network from the first stage. One such pretrained LM is RO BERTA (Liu et al., 2019), which uses the same transformerbased architecture (Vaswani et al., 2017) as its predecessor, BERT (Devlin et al., 2019). It is trained with a masked language modeling objective (i.e., cross-entropy loss on predicting randomly masked tokens). The unlabeled pretraining corpus for RO BERTA contains over 160 GB of uncompressed raw text from different English-language corpora (see Appendix §A.1). RO BERTA attains better performance on an assortment of tasks than its predecessors, making it our baseline of choice. Although RO BERTA’s pretraining corpus is derived from multiple sources, it has not yet been established if these sources are diverse enough to generalize to most of the variation in the English language."
2020.acl-main.740,D19-1224,1,0.895378,"Missing"
2020.acl-main.740,W18-2501,0,0.0345151,"Missing"
2020.acl-main.740,P19-1590,1,0.848457,"T (see Table 9 for details of computational requirements for different pretraining phases). We propose simple unsuper8348 vised methods to retrieve unlabeled text that aligns with the task distribution, from a large in-domain corpus. Our approach finds task-relevant data from the domain by embedding text from both the task and domain in a shared space, then selects candidates from the domain based on queries using the task data. Importantly, the embedding method must be lightweight enough to embed possibly millions of sentences in a reasonable time. Given these constraints, we employ VAMPIRE (Gururangan et al., 2019; Figure 3), a lightweight bag-of-words language model. We pretrain VAMPIRE on a large deduplicated3 sample of the domain (1M sentences) to obtain embeddings of the text from both the task and domain sample. We then select k candidates of each task sentence from the domain sample, in embeddings space. Candidates are selected (i) via nearest neighbors selection (k NN - TAPT)4 , or (ii) randomly (RAND - TAPT). We continue pretraining RO BERTA on this augmented corpus with both the task data (as in TAPT) as well as the selected candidate pool. Results Results in Table 8 show that k NN - TAPT outp"
2020.acl-main.740,L16-1572,0,0.0291599,"a selection methods for NMT based on cosine similarity in embedding space, using D ISTIL BERT (Sanh et al., 2019) for efficiency. In contrast, we use VAMPIRE, and focus on augmenting TAPT data for text classification tasks. Khandelwal et al. (2020) introduced kNN-LMs that allows easy domain adaptation of pretrained LMs by simply adding a datastore per domain and no further training; an alternative to integrate domain information in an LM. Our study of human-curated data §5.1 is related to focused crawling (Chakrabarti et al., 1999) for collection of suitable data, especially with LM reliance (Remus and Biemann, 2016). What is a domain? Despite the popularity of domain adaptation techniques, most research and practice seems to use an intuitive understanding of domains. A small body of work has attempted to address this question (Lee, 2001; Eisenstein et al., 2014; van der Wees et al., 2015; Plank, 2016; Ruder et al., 2016, among others). For instance, Aharoni and Goldberg (2020) define domains by implicit Conclusion We investigate several variations for adapting pretrained LMs to domains and tasks within those domains, summarized in Table 10. Our experiments reveal that even a model of hundreds of millions"
2020.acl-main.740,2020.acl-main.447,1,0.742364,"D) papers, computer science (CS) papers, newstext from R EAL N EWS, and A MAZON reviews. We choose these domains because they have been popular in previous work, and datasets for text classification are available in each. Table 1 lists the specifics of the unlabeled datasets in all four domains, as well as RO BERTA’s training corpus.1 3.1 1 Analyzing Domain Similarity For B IO M ED and CS, we used an internal version of S2ORC that contains papers that cannot be released due to copyright restrictions. 8343 Domain Pretraining Corpus B IO M ED CS N EWS R EVIEWS 2.68M full-text papers from S2ORC (Lo et al., 2020) 2.22M full-text papers from S2ORC (Lo et al., 2020) 11.90M articles from R EAL N EWS (Zellers et al., 2019) 24.75M A MAZON reviews (He and McAuley, 2016) RO BERTA (baseline) see Appendix §A.1 # Tokens Size LRO B. LDAPT 7.55B 8.10B 6.66B 2.11B 47GB 48GB 39GB 11GB 1.32 1.63 1.08 2.10 0.99 1.34 1.16 1.93 N/A 160GB 1.19 - ‡ Table 1: List of the domain-specific unlabeled datasets. In columns 5 and 6, we report RO BERTA’s masked LM loss on 50K randomly sampled held-out documents from each domain before (LRO B. ) and after (LDAPT ) DAPT (lower implies a better fit on the sample). ‡ indicates that th"
2020.acl-main.740,W16-6012,0,0.0303039,"LMs by simply adding a datastore per domain and no further training; an alternative to integrate domain information in an LM. Our study of human-curated data §5.1 is related to focused crawling (Chakrabarti et al., 1999) for collection of suitable data, especially with LM reliance (Remus and Biemann, 2016). What is a domain? Despite the popularity of domain adaptation techniques, most research and practice seems to use an intuitive understanding of domains. A small body of work has attempted to address this question (Lee, 2001; Eisenstein et al., 2014; van der Wees et al., 2015; Plank, 2016; Ruder et al., 2016, among others). For instance, Aharoni and Goldberg (2020) define domains by implicit Conclusion We investigate several variations for adapting pretrained LMs to domains and tasks within those domains, summarized in Table 10. Our experiments reveal that even a model of hundreds of millions of parameters struggles to encode the complexity of a single textual domain, let alone all of language. We show that pretraining the model towards a specific task or small corpus can provide significant benefits. Our findings suggest it may be valuable to complement work on ever-larger LMs with parallel effo"
2020.acl-main.740,P19-1335,0,0.0293521,"models (LMs) in their domain of interest, from scratch. In contrast, our work explores multiple domains, and is arguably more cost effective, since we continue pretraining an already powerful LM. Task-adaptive pretraining Continued pretraining of a LM on the unlabeled data of a given task (TAPT) has been show to be beneficial for endtask performance (e.g. in Howard and Ruder, 2018; Phang et al., 2018; Sun et al., 2019). In the presence of domain shift between train and test data distributions of the same task, domain-adaptive pretraining (DAPT) is sometimes used to describe what we term TAPT (Logeswaran et al., 2019; Han and Eisenstein, 2019). Related approaches include language modeling as an auxiliary objective to task classifier fine-tuning (Chronopoulou et al., 2019; Radford et al., 2018) or consider simple syntactic structure of the input while adapting to task-specific 5 3 We deduplicated this set to limit computation, since different sentences can share neighbors. 4 We use a flat search index with cosine similarity between embeddings with the FAISS (Johnson et al., 2019) library. In contrast, Peters et al. (2019) find that the JensenShannon divergence on term distributions between BERT’s pretraini"
2020.acl-main.740,D17-1038,0,0.0244279,"X (Subset) 7 Table 10: Summary of strategies for multi-phase pretraining explored in this paper. data (Swayamdipta et al., 2019). We compare DAPT and TAPT as well as their interplay with respect to dataset size for continued pretraining (hence, expense of more rounds of pretraining), relevance to a data sample of a given task, and transferability to other tasks and datasets. See Table 11 in Appendix §A for a summary of multi-phase pretraining strategies from related work. Data selection for transfer learning Selecting data for transfer learning has been explored in NLP (Moore and Lewis, 2010; Ruder and Plank, 2017; Zhang et al., 2019, among others). Dai et al. (2019) focus on identifying the most suitable corpus to pretrain a LM from scratch, for a single task: NER, whereas we select relevant examples for various tasks in §5.2. Concurrent to our work, Aharoni and Goldberg (2020) propose data selection methods for NMT based on cosine similarity in embedding space, using D ISTIL BERT (Sanh et al., 2019) for efficiency. In contrast, we use VAMPIRE, and focus on augmenting TAPT data for text classification tasks. Khandelwal et al. (2020) introduced kNN-LMs that allows easy domain adaptation of pretrained L"
2020.acl-main.740,D18-1360,0,0.0422704,"nl.) Dev. Test Classes relation classification abstract sent. roles 4169 18040 - 2427 30212 3469 30135 13 5 ACL-ARC S CI ERC citation intent relation classification 1688 3219 - 114 455 139 974 6 7 H YPER PARTISAN AGN EWS partisanship topic 515 115000 5000 - 65 5000 65 7600 2 4 † review helpfulness review sentiment 115251 20000 50000 5000 5000 25000 25000 2 2 † H ELPFULNESS IMDB Table 2: Specifications of the various target task datasets. † indicates high-resource settings. Sources: C HEM P ROT (Kringelum et al., 2016), RCT (Dernoncourt and Lee, 2017), ACL-ARC (Jurgens et al., 2018), S CI ERC (Luan et al., 2018), H YPER PARTISAN (Kiesel et al., 2019), AGN EWS (Zhang et al., 2015), H ELPFULNESS (McAuley et al., 2015), IMDB (Maas et al., 2011). RO BA . DAPT ¬DAPT C HEM P ROT 81.91.0 87.20.1 84.20.2 87.60.1 79.41.3 86.90.1 63.05.8 77.31.9 75.42.5 80.81.5 66.44.1 79.20.9 H Y P. N EWS † AGN EWS 86.60.9 93.90.2 88.25.9 93.90.2 76.44.9 93.50.2 † H ELPFUL . 65.13.4 95.00.2 66.51.4 95.40.2 65.12.8 94.10.4 Dom. Task BM † RCT ACL-ARC CS S CI ERC R EV. † IMDB Table 3: Comparison of RO BERTA (RO BA .) and DAPT to adaptation to an irrelevant domain (¬ DAPT ). Reported results are test macro-F1 , except for C HEM P"
2020.acl-main.740,P11-1015,0,0.63943,"n intent relation classification 1688 3219 - 114 455 139 974 6 7 H YPER PARTISAN AGN EWS partisanship topic 515 115000 5000 - 65 5000 65 7600 2 4 † review helpfulness review sentiment 115251 20000 50000 5000 5000 25000 25000 2 2 † H ELPFULNESS IMDB Table 2: Specifications of the various target task datasets. † indicates high-resource settings. Sources: C HEM P ROT (Kringelum et al., 2016), RCT (Dernoncourt and Lee, 2017), ACL-ARC (Jurgens et al., 2018), S CI ERC (Luan et al., 2018), H YPER PARTISAN (Kiesel et al., 2019), AGN EWS (Zhang et al., 2015), H ELPFULNESS (McAuley et al., 2015), IMDB (Maas et al., 2011). RO BA . DAPT ¬DAPT C HEM P ROT 81.91.0 87.20.1 84.20.2 87.60.1 79.41.3 86.90.1 63.05.8 77.31.9 75.42.5 80.81.5 66.44.1 79.20.9 H Y P. N EWS † AGN EWS 86.60.9 93.90.2 88.25.9 93.90.2 76.44.9 93.50.2 † H ELPFUL . 65.13.4 95.00.2 66.51.4 95.40.2 65.12.8 94.10.4 Dom. Task BM † RCT ACL-ARC CS S CI ERC R EV. † IMDB Table 3: Comparison of RO BERTA (RO BA .) and DAPT to adaptation to an irrelevant domain (¬ DAPT ). Reported results are test macro-F1 , except for C HEM P ROT and RCT, for which we report micro-F1 , following Beltagy et al. (2019). We report averages across five random seeds, with stan"
2020.acl-main.740,N18-1101,0,0.10509,"Missing"
2020.acl-main.740,N19-1189,0,0.0357309,"Summary of strategies for multi-phase pretraining explored in this paper. data (Swayamdipta et al., 2019). We compare DAPT and TAPT as well as their interplay with respect to dataset size for continued pretraining (hence, expense of more rounds of pretraining), relevance to a data sample of a given task, and transferability to other tasks and datasets. See Table 11 in Appendix §A for a summary of multi-phase pretraining strategies from related work. Data selection for transfer learning Selecting data for transfer learning has been explored in NLP (Moore and Lewis, 2010; Ruder and Plank, 2017; Zhang et al., 2019, among others). Dai et al. (2019) focus on identifying the most suitable corpus to pretrain a LM from scratch, for a single task: NER, whereas we select relevant examples for various tasks in §5.2. Concurrent to our work, Aharoni and Goldberg (2020) propose data selection methods for NMT based on cosine similarity in embedding space, using D ISTIL BERT (Sanh et al., 2019) for efficiency. In contrast, we use VAMPIRE, and focus on augmenting TAPT data for text classification tasks. Khandelwal et al. (2020) introduced kNN-LMs that allows easy domain adaptation of pretrained LMs by simply adding"
2020.acl-main.740,P10-2041,0,\N,Missing
2020.acl-main.740,P15-2092,0,\N,Missing
2020.acl-main.740,W18-5446,0,\N,Missing
2020.acl-main.740,W19-5034,1,\N,Missing
2020.acl-main.740,W19-1909,0,\N,Missing
2020.acl-main.740,P19-2057,0,\N,Missing
2020.acl-main.740,D19-1371,1,\N,Missing
2020.acl-main.740,D19-1433,0,\N,Missing
2020.acl-main.740,2020.acl-main.692,0,\N,Missing
2020.cl-2.1,2020.cl-2.3,0,0.0613442,"Missing"
2020.cl-2.1,2020.acl-main.447,0,0.0363097,"he 2010s; red line). The fraction of papers mentioning two or more languages (yellow line) and the average per year (green line) showed increases in the 1990s and 2000s, though these appear to have slowed recently.3 The other trend is a matter of increasing supply: The diversity of computational tools now available—from conceptual definitions of language meaning to operationalizations in downloadable models—has exploded in the past decade. The term “semantic representation” was, not long ago, one that referred to a range of linguistic abstractions. 1 We explored ACL Anthology papers in S2ORC (Lo et al. 2020) with publication years 1980–2019, a total of 40,402 papers. 2 The list is Ethnologue’s list of the 20 most spoken languages in 2019, with Mandarin and Wu Chinese mapped to the string chinese. See https://www.ethnologue.com/guides/ethnologue200. Less dominant languages are, of course, also interesting, but also more sparse in the data. 3 The leveling off of these last two trends is, we speculate, due to the emergence of new representation learning methods that work best with very large data sets. We expect increasing multilinguality of the largest data sets and pretrained representations will"
2020.cl-2.1,2020.cl-2.2,0,0.426794,"cept may help to obtain more robust embeddings at sense level as shown by one of the works presented here. The contributions to this special issue are summarized in Table 1. The papers selected cover the different points we wanted to emphasize in our call. Three of the contributions refer to representations at word level and the others at sentence level, but the breadth of the field is reflected in the range of specific topics addressed. This issue presents novel work and reviews on interlingual representations (Ranta et al. 2020); semantic representations learned through translation at word (Mohiuddin and Joty 2020) and sentence level (V´azquez et al. 2020); senses, ambiguity, and polysemy (Colla, Mensa, and Radicioni 2020); and evaluation (Sahin 2020). Multilinguality is clearly the aim for all of them, with systems that cover from 4 up to 40 languages. Some systems also have the virtue to deal with text in low-resource languages such as Macedonian, Nepali, and Telugu. 4 http://universaldependencies.org. 251 Computational Linguistics Volume 46, Number 2 Table 1 Summary of contributions to the special issue. Granularity Word Paper (Mohiuddin and Joty 2020) Technique Application Languages Unsupervised Adv"
2020.cl-2.1,2020.cl-2.6,0,0.07929,"entary might also be true, and realizations in different languages of the same concept may help to obtain more robust embeddings at sense level as shown by one of the works presented here. The contributions to this special issue are summarized in Table 1. The papers selected cover the different points we wanted to emphasize in our call. Three of the contributions refer to representations at word level and the others at sentence level, but the breadth of the field is reflected in the range of specific topics addressed. This issue presents novel work and reviews on interlingual representations (Ranta et al. 2020); semantic representations learned through translation at word (Mohiuddin and Joty 2020) and sentence level (V´azquez et al. 2020); senses, ambiguity, and polysemy (Colla, Mensa, and Radicioni 2020); and evaluation (Sahin 2020). Multilinguality is clearly the aim for all of them, with systems that cover from 4 up to 40 languages. Some systems also have the virtue to deal with text in low-resource languages such as Macedonian, Nepali, and Telugu. 4 http://universaldependencies.org. 251 Computational Linguistics Volume 46, Number 2 Table 1 Summary of contributions to the special issue. Granulari"
2020.cl-2.1,J82-2005,0,0.392652,"us lines of work that illustrate a range of creative advances exploring natural language meaning, specifically with a multilingual focus. In inviting submissions, we encouraged a broad reading of the term “representations,” in granularity (words, sentences, paragraphs, etc.) and in theoretical assumptions (symbolic, neural, hybrid, etc.). We anticipated breadth as well in the set of motivating applications and evaluation methods. Our deliberate reference to interlingual—not only multilingual—representations evokes recent re-imaginings of interlingual machine translation, a classical approach (Richens 1958). We explicitly encouraged submissions that consider less-commonly studied languages and that go beyond mere projection of representations from text in one language to another. Of particular interest to our editorial team is the potential for multilingual representations (of any kind) to help overcome challenges of polysemy in individual languages. It has been shown that translations into other languages can help at distinguishing senses monolingually (Resnik and Yarowsky 1999). But the complementary might also be true, and realizations in different languages of the same concept may help to ob"
2020.cl-2.1,2020.cl-2.4,0,0.0730249,"ummarized in Table 1. The papers selected cover the different points we wanted to emphasize in our call. Three of the contributions refer to representations at word level and the others at sentence level, but the breadth of the field is reflected in the range of specific topics addressed. This issue presents novel work and reviews on interlingual representations (Ranta et al. 2020); semantic representations learned through translation at word (Mohiuddin and Joty 2020) and sentence level (V´azquez et al. 2020); senses, ambiguity, and polysemy (Colla, Mensa, and Radicioni 2020); and evaluation (Sahin 2020). Multilinguality is clearly the aim for all of them, with systems that cover from 4 up to 40 languages. Some systems also have the virtue to deal with text in low-resource languages such as Macedonian, Nepali, and Telugu. 4 http://universaldependencies.org. 251 Computational Linguistics Volume 46, Number 2 Table 1 Summary of contributions to the special issue. Granularity Word Paper (Mohiuddin and Joty 2020) Technique Application Languages Unsupervised Adversarial Translation en, es, de, it, fi, ar, ms, he Linked Data Intrinsic/extrinsic evaluation Word Similarity POS, dependencies, SRL, NER,"
2020.cl-2.1,2020.cl-2.5,0,0.0495885,"Missing"
2020.cl-2.1,D18-1268,0,0.0204012,"ion. The challenges addressed include learning unsupervised representations, introducing priors and linguistic knowledge to compute the representations, and evaluating the quality of these representations, taking into account linguistic features. Unsupervised Word Translation with Adversarial Encoder (Mohiuddin and Joty 2020). Crosslingual word embeddings are becoming crucial in multilingual natural language processing tasks and, recently, several authors claim that unsupervised methods even outperform the supervised ones (see for instance Lample et al. 2018, Artetxe, Labaka, and Agirre 2018, Xu et al. 2018), making them appealing also in the low-resource setting. This is not true in all cases, and specifically, adversarial techniques for dictionary induction show stability and convergence issues for some language pairs (Zhang et al. 2017; Lample et al. 2018). In general, unsupervised adversarial bilingual embeddings are learned in two phases: (i) induction of an initial seed dictionary using an adversarial network and (ii) refinement of the initial mapping, and therefore, dictionary, until convergence. This paper tries to address those limitations by extending adversarial autoencoders. One of th"
2020.cl-2.1,D17-1207,0,0.0236204,"tic features. Unsupervised Word Translation with Adversarial Encoder (Mohiuddin and Joty 2020). Crosslingual word embeddings are becoming crucial in multilingual natural language processing tasks and, recently, several authors claim that unsupervised methods even outperform the supervised ones (see for instance Lample et al. 2018, Artetxe, Labaka, and Agirre 2018, Xu et al. 2018), making them appealing also in the low-resource setting. This is not true in all cases, and specifically, adversarial techniques for dictionary induction show stability and convergence issues for some language pairs (Zhang et al. 2017; Lample et al. 2018). In general, unsupervised adversarial bilingual embeddings are learned in two phases: (i) induction of an initial seed dictionary using an adversarial network and (ii) refinement of the initial mapping, and therefore, dictionary, until convergence. This paper tries to address those limitations by extending adversarial autoencoders. One of the main contributions is training the adversarial mapping in a latent space, with the hope that this will minimize the effect of a lack of isomorphism between the two original embedding spaces. In addition, the authors combine several l"
2020.cl-2.1,Q17-1010,0,\N,Missing
2020.cl-2.1,Q19-1038,0,\N,Missing
2020.emnlp-main.369,Q19-1038,0,0.033966,"age processing, and research in this area has been accelerated by the abundance of corpora across different domains (e.g., Twitter sentiment (Pak and Paroubek, 2010), movie ratings (Maas et al., 2011), textual entailment (Bowman et al., 2015), restaurant reviews (Yelp Inc., 2019), among many others). The construction of multilingual classification systems which handle inputs from different languages has been studied extensively in previous work (e.g., Bel et al., 2003; De Melo and Siersdorfer, 2007). More recently, researchers have observed ‘zero-shot’ cross-lingual behavior (Lu et al., 2018; Artetxe and Schwenk, 2019) where classification performance in one language can be transferred to the same task in another, without target language supervision, as long as the encoder was pretrained on a machine translation task. In addition, contextual embeddings have shown unexpected cross-lingual behavior in classification, NER, and dependency parsing tasks (Wu and Dredze, 2019; Keung et al., 2019; Conneau et al., 2019). As with all other areas in NLP, progress in multilingual research relies on the availability of highquality data. However, large-scale multilingual text classification datasets are surprisingly rare"
2020.emnlp-main.369,Q17-1010,0,0.0262762,"Missing"
2020.emnlp-main.369,D15-1075,0,0.0394104,"ing by fine-tuning a multilingual BERT model on reviews data. We propose the use of mean absolute error (MAE) instead of classification accuracy for this task, since MAE accounts for the ordinal nature of the ratings. 1 nasmith@cs.washington.edu } Figure 1: A hypothetical review from our corpus. Introduction Text classification is one of the fundamental tasks in natural language processing, and research in this area has been accelerated by the abundance of corpora across different domains (e.g., Twitter sentiment (Pak and Paroubek, 2010), movie ratings (Maas et al., 2011), textual entailment (Bowman et al., 2015), restaurant reviews (Yelp Inc., 2019), among many others). The construction of multilingual classification systems which handle inputs from different languages has been studied extensively in previous work (e.g., Bel et al., 2003; De Melo and Siersdorfer, 2007). More recently, researchers have observed ‘zero-shot’ cross-lingual behavior (Lu et al., 2018; Artetxe and Schwenk, 2019) where classification performance in one language can be transferred to the same task in another, without target language supervision, as long as the encoder was pretrained on a machine translation task. In addition,"
2020.emnlp-main.369,W18-6309,1,0.833995,"in natural language processing, and research in this area has been accelerated by the abundance of corpora across different domains (e.g., Twitter sentiment (Pak and Paroubek, 2010), movie ratings (Maas et al., 2011), textual entailment (Bowman et al., 2015), restaurant reviews (Yelp Inc., 2019), among many others). The construction of multilingual classification systems which handle inputs from different languages has been studied extensively in previous work (e.g., Bel et al., 2003; De Melo and Siersdorfer, 2007). More recently, researchers have observed ‘zero-shot’ cross-lingual behavior (Lu et al., 2018; Artetxe and Schwenk, 2019) where classification performance in one language can be transferred to the same task in another, without target language supervision, as long as the encoder was pretrained on a machine translation task. In addition, contextual embeddings have shown unexpected cross-lingual behavior in classification, NER, and dependency parsing tasks (Wu and Dredze, 2019; Keung et al., 2019; Conneau et al., 2019). As with all other areas in NLP, progress in multilingual research relies on the availability of highquality data. However, large-scale multilingual text classification da"
2020.emnlp-main.369,P11-1015,0,0.120682,"nd zero-shot crosslingual transfer learning by fine-tuning a multilingual BERT model on reviews data. We propose the use of mean absolute error (MAE) instead of classification accuracy for this task, since MAE accounts for the ordinal nature of the ratings. 1 nasmith@cs.washington.edu } Figure 1: A hypothetical review from our corpus. Introduction Text classification is one of the fundamental tasks in natural language processing, and research in this area has been accelerated by the abundance of corpora across different domains (e.g., Twitter sentiment (Pak and Paroubek, 2010), movie ratings (Maas et al., 2011), textual entailment (Bowman et al., 2015), restaurant reviews (Yelp Inc., 2019), among many others). The construction of multilingual classification systems which handle inputs from different languages has been studied extensively in previous work (e.g., Bel et al., 2003; De Melo and Siersdorfer, 2007). More recently, researchers have observed ‘zero-shot’ cross-lingual behavior (Lu et al., 2018; Artetxe and Schwenk, 2019) where classification performance in one language can be transferred to the same task in another, without target language supervision, as long as the encoder was pretrained o"
2020.emnlp-main.369,D19-1018,0,0.0370648,"Missing"
2020.emnlp-main.369,pak-paroubek-2010-twitter,0,0.0343106,"lts for supervised text classification and zero-shot crosslingual transfer learning by fine-tuning a multilingual BERT model on reviews data. We propose the use of mean absolute error (MAE) instead of classification accuracy for this task, since MAE accounts for the ordinal nature of the ratings. 1 nasmith@cs.washington.edu } Figure 1: A hypothetical review from our corpus. Introduction Text classification is one of the fundamental tasks in natural language processing, and research in this area has been accelerated by the abundance of corpora across different domains (e.g., Twitter sentiment (Pak and Paroubek, 2010), movie ratings (Maas et al., 2011), textual entailment (Bowman et al., 2015), restaurant reviews (Yelp Inc., 2019), among many others). The construction of multilingual classification systems which handle inputs from different languages has been studied extensively in previous work (e.g., Bel et al., 2003; De Melo and Siersdorfer, 2007). More recently, researchers have observed ‘zero-shot’ cross-lingual behavior (Lu et al., 2018; Artetxe and Schwenk, 2019) where classification performance in one language can be transferred to the same task in another, without target language supervision, as l"
2020.emnlp-main.369,P19-4007,0,0.0525945,"Missing"
2020.emnlp-main.369,P10-1114,0,0.0774386,"2019) contains reviews from international marketplaces, but the reviews from each marketplace can be written in multiple languages and the language identity is not provided. Furthermore, the Yelp corpus itself is refreshed from time to time, and previous versions are not made available for download, which affects the reproducibility of published results. Several versions of the Amazon reviews corpus exist today. Neither the version from Ni et al. (2019) nor Amazon Inc. (2015) provide training, development, and test splits, and neither version focuses on the multilingual aspect of the reviews. Prettenhofer and Stein (2010) provide Amazon reviews in 4 languages (i.e., 2,000 training and test reviews, along with a variable number of unlabeled reviews), but the dataset is small by modern standards. We address many of the above-mentioned limitations by releasing a subset of Amazon reviews specifically tailored for the task of multilingual text classification: • We provide 200,000 reviews in the training set for each of the languages in the corpus. • We apply language detection algorithms to ensure reviews are associated with the correct language with high probability. • We distribute the corpus on AWS Open Datasets"
2020.emnlp-main.369,D18-1269,0,0.035366,"tilingual text classification datasets are surprisingly rare, and existing multilingual datasets have some notable deficiencies. The proprietary Reuters RCV1 (Lewis et al., 2004) and RCV2 (Reuters Ltd., 2005) corpora and its derivatives like MLDoc (Schwenk and Li, 2018) are relatively small; in RCV2, each language has ∼37,000 training examples on average, and the smallest language only has 1,794 examples. RCV1 and 2 are not easily accessible; a researcher who wishes to acquire the data would need to work with an organization that has obtained legal approval from Reuters Ltd. The XNLI dataset (Conneau et al., 2018) was designed for evaluating zero-shot cross-lingual transfer and does not contain training data for non4563 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 4563–4568, c November 16–20, 2020. 2020 Association for Computational Linguistics Number of products Number of reviewers Average characters/review Average characters/review title En De Es Fr Ja Zh 196,745 185,541 178.8 24.2 189,148 171,620 207.9 21.8 179,076 150,938 151.3 19.2 183,345 157,922 159.4 19.1 185,436 164,776 101.4 9.5 164,540 132,246 51.0 7.6 Table 1: Training corpus statistics. We p"
2020.emnlp-main.369,N19-1423,0,0.242134,"on. We fine-tuned the model for 15 epochs with the Adam optimizer using a constant learning rate of 8 × 10−7 . We used minibatches of 32 reviews. Each experiment required ∼10 hours to complete with a single GPU on an AWS p3.8xlarge instance with the MXNet GluonNLP framework. We truncated the review body at 180 wordpieces if it exceeded 180 wordpieces. 4.2 Supervised Text Classification and yi , yˆi ∈ {1, 2, 3, 4, 5} are the true star rating and the predicted rating for the i-th review respectively. All of our baseline models are initialized with the cased multilingual BERT (mBERT) base model (Devlin et al., 2019), which has 110M parameters. Note that the star ratings for each review are ordinal, and a 2-star prediction for a 5-star review should be penalized more heavily than a 4-star prediction for a 5-star review. However, previous work on Amazon reviews classification (e.g., Yang et al., 2016) used the classification accuracy as the primary metric, which ignores the ordinal nature of the labels. We use MAE in our baselines as the primary metric instead. We also report the classification accuracy for completeness (Table 3), but we encourage the use of MAE in future work. In Table 2a, we report our M"
2020.emnlp-main.369,D19-1138,1,0.836854,"fferent languages has been studied extensively in previous work (e.g., Bel et al., 2003; De Melo and Siersdorfer, 2007). More recently, researchers have observed ‘zero-shot’ cross-lingual behavior (Lu et al., 2018; Artetxe and Schwenk, 2019) where classification performance in one language can be transferred to the same task in another, without target language supervision, as long as the encoder was pretrained on a machine translation task. In addition, contextual embeddings have shown unexpected cross-lingual behavior in classification, NER, and dependency parsing tasks (Wu and Dredze, 2019; Keung et al., 2019; Conneau et al., 2019). As with all other areas in NLP, progress in multilingual research relies on the availability of highquality data. However, large-scale multilingual text classification datasets are surprisingly rare, and existing multilingual datasets have some notable deficiencies. The proprietary Reuters RCV1 (Lewis et al., 2004) and RCV2 (Reuters Ltd., 2005) corpora and its derivatives like MLDoc (Schwenk and Li, 2018) are relatively small; in RCV2, each language has ∼37,000 training examples on average, and the smallest language only has 1,794 examples. RCV1 and 2 are not easily ac"
2020.emnlp-main.369,L18-1560,0,0.0270863,"nslation task. In addition, contextual embeddings have shown unexpected cross-lingual behavior in classification, NER, and dependency parsing tasks (Wu and Dredze, 2019; Keung et al., 2019; Conneau et al., 2019). As with all other areas in NLP, progress in multilingual research relies on the availability of highquality data. However, large-scale multilingual text classification datasets are surprisingly rare, and existing multilingual datasets have some notable deficiencies. The proprietary Reuters RCV1 (Lewis et al., 2004) and RCV2 (Reuters Ltd., 2005) corpora and its derivatives like MLDoc (Schwenk and Li, 2018) are relatively small; in RCV2, each language has ∼37,000 training examples on average, and the smallest language only has 1,794 examples. RCV1 and 2 are not easily accessible; a researcher who wishes to acquire the data would need to work with an organization that has obtained legal approval from Reuters Ltd. The XNLI dataset (Conneau et al., 2018) was designed for evaluating zero-shot cross-lingual transfer and does not contain training data for non4563 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 4563–4568, c November 16–20, 2020. 2020 Associ"
2020.emnlp-main.369,D19-1077,0,0.0374874,"handle inputs from different languages has been studied extensively in previous work (e.g., Bel et al., 2003; De Melo and Siersdorfer, 2007). More recently, researchers have observed ‘zero-shot’ cross-lingual behavior (Lu et al., 2018; Artetxe and Schwenk, 2019) where classification performance in one language can be transferred to the same task in another, without target language supervision, as long as the encoder was pretrained on a machine translation task. In addition, contextual embeddings have shown unexpected cross-lingual behavior in classification, NER, and dependency parsing tasks (Wu and Dredze, 2019; Keung et al., 2019; Conneau et al., 2019). As with all other areas in NLP, progress in multilingual research relies on the availability of highquality data. However, large-scale multilingual text classification datasets are surprisingly rare, and existing multilingual datasets have some notable deficiencies. The proprietary Reuters RCV1 (Lewis et al., 2004) and RCV2 (Reuters Ltd., 2005) corpora and its derivatives like MLDoc (Schwenk and Li, 2018) are relatively small; in RCV2, each language has ∼37,000 training examples on average, and the smallest language only has 1,794 examples. RCV1 and"
2020.emnlp-main.369,N16-1174,0,0.0237981,"e review body at 180 wordpieces if it exceeded 180 wordpieces. 4.2 Supervised Text Classification and yi , yˆi ∈ {1, 2, 3, 4, 5} are the true star rating and the predicted rating for the i-th review respectively. All of our baseline models are initialized with the cased multilingual BERT (mBERT) base model (Devlin et al., 2019), which has 110M parameters. Note that the star ratings for each review are ordinal, and a 2-star prediction for a 5-star review should be penalized more heavily than a 4-star prediction for a 5-star review. However, previous work on Amazon reviews classification (e.g., Yang et al., 2016) used the classification accuracy as the primary metric, which ignores the ordinal nature of the labels. We use MAE in our baselines as the primary metric instead. We also report the classification accuracy for completeness (Table 3), but we encourage the use of MAE in future work. In Table 2a, we report our MAE on the fully supervised classification task, where the languages of the training and evaluation data are the same (i.e., train on French reviews and test on French reviews, etc.). We distinguish between the ‘fine-grained’ classification task, where we predict on the 5-star scale, and t"
2020.emnlp-main.369,N16-1000,0,0.20091,"Missing"
2020.emnlp-main.407,N19-1423,0,0.158013,"beddings and the use of pretraining). For document X, a HAN builds a vector representation dX using the (given) structure of X: typically, the document vector is derived from sentence vectors, which are derived from (contextualized) word vectors. Working in the order that the computation proceeds, the encoding procedure is: 1. Each word in the document is mapped (by lookup) to its type embedding.2 2. Each word’s vector is contextualized, i.e., a new word token vector is derived from the 1 word and the other words in the sentence. In this work, we consider two contextualizers: pretrained BERT (Devlin et al., 2019) and a GRU (Cho et al., 2014) whose parameters are trained only for the end task. Transformers have emerged as a successful tool across NLP (Vaswani et al., 2017), but they are not yet well suited for long sequences without an hierarchical configuration because their costs scale quadratically with sequence length. When more efficient variants of transformers become available, they will be an appealing option to consider in this setting as well. 2 “Type embedding” refers to traditional word (subword) vectors; we use the term to contrast with contextualized embeddings associated with specific to"
2020.emnlp-main.407,E14-1049,0,0.0181486,"or trees (Liu et al., 2018), while Yang et al. (2018) cast the problem as latent graph learning to capture dependencies between pairs of words from unlabeled data. Orthogonal to these studies, we use attention to compare documents represented by hierarchical document encoders at multiple levels. Similarity Learning. There are three types of similarity learning in NLP. The supervised paradigm differs from typical supervised learning in that 5019 training examples are cast into pairwise constraints (Yang and Jin, 2006), as in cross-lingual word embedding learning based on word-level alignments (Faruqui and Dyer, 2014) and zero-shot utterance/document classification (Yazdani and Henderson, 2015; Nam et al., 2016; Pappas and Henderson, 2019) based on utterance/document-level annotations. The unsupervised paradigm aims to learn an underlying low-dimensional space where the relationships between most of the observed data are preserved, as in word embedding learning (Collobert et al., 2011; Mikolov et al., 2013; Pennington et al., 2014; Levy and Goldberg, 2014). The weakly supervised paradigm is the middle ground between the two, as in cross-lingual word embedding learning based on sentence-level alignments (He"
2020.emnlp-main.407,E17-2066,0,0.0415006,"Missing"
2020.emnlp-main.407,D14-1162,0,0.0888752,"vised learning in that 5019 training examples are cast into pairwise constraints (Yang and Jin, 2006), as in cross-lingual word embedding learning based on word-level alignments (Faruqui and Dyer, 2014) and zero-shot utterance/document classification (Yazdani and Henderson, 2015; Nam et al., 2016; Pappas and Henderson, 2019) based on utterance/document-level annotations. The unsupervised paradigm aims to learn an underlying low-dimensional space where the relationships between most of the observed data are preserved, as in word embedding learning (Collobert et al., 2011; Mikolov et al., 2013; Pennington et al., 2014; Levy and Goldberg, 2014). The weakly supervised paradigm is the middle ground between the two, as in cross-lingual word embedding learning based on sentence-level alignments (Hermann and Blunsom, 2014; Gouws et al., 2015). Our approach is weakly supervised and operates at the document-level, making use of structural correspondence between documents. Chandra Bhagavatula, Sergey Feldman, Russell Power, and Waleed Ammar. 2018. Content-based citation recommendation. In Proc. of NAACL. 7 Dipanjan Das and Noah A. Smith. 2009. Paraphrase identification as probabilistic quasi-synchronous recognition"
2020.emnlp-main.407,D19-1410,0,0.028577,"3). 3.2 3. Each sentence in the document is encoded by aggregating the contextualized word vectors. Letting xi denote the ith word vector and y denote the sentence vector, the layer that performs this aggregation has the form: attentioni z }|   { X exp u&gt; tanh affine(xi )   xi , P y= &gt; tanh affine(x ) exp u j j i (1) where i and j range over the words within the sentence. We suppress the parameters of the affine transformation but not the attention parameters u. Note that, when using pretrained BERT, we instead take the average of word token vectors to obtain sentence vectors, following Reimers and Gurevych (2019). Hierarchical Attention Networks Yang et al. (2016) introduced a family of document encoding models that are based on a word/sentence/document hierarchy, known as hierarchical attention networks (HANs). They have been shown superior to earlier hierarchical encoding models based on convolutional networks (Collobert et al., 2011; Kim, 2014; Zhang et al., 2016), they are competitive for tasks involving long documents (Choi et al., 2016; Pappas and Popescu-Belis, 2017; Sun et al., 2018; Miculicich et al., 2018; Liu and Lapata, 2019; Guo et al., 2019),1 and they can be used orthogonally to other d"
2020.emnlp-main.407,D15-1044,0,0.0442471,"of the plagiarism dataset. S2D performance is shown in Table 5 with attention alignment; here we see a consistent benefit from CDA across encoders and evaluation scores. 6 Other Related Work Latent Alignment. Attention has been previously used to align word sequences based on their inEnc CDA MRR Random – 0.4215 44.23 43.28 P@10 P@5 BERT-AVG – 0.7864 58.24 64.69 – 0.8072 60.36 68.94 BERT-HAN S HALLOW 0.8386 60.47 69.07 – 0.6205 50.72 51.90 GRU-HAN S HALLOW 0.6479 51.71 53.05 D EEP 0.6378 52.07 53.82 Table 5: Sentence-to-document plagiarism detection. termediate hidden states for summarization (Rush et al., 2015) and machine translation (Bahdanau et al., 2015). The alignment is typically softly learned and does not consider alternative alignments in a probabilistic sense. Hard attention (Luong et al., 2015) is an alternative approach which selects only one word at a time but it is non-differentiable and requires more complicated techniques such as reinforcement learning to train. Deng et al. (2018) considered an alternative attention network for learning latent variable alignment models based on amortized variational inference. Others modified attention to attend to partial segmentations and subtrees"
2020.emnlp-main.407,P06-1062,0,0.050225,"xample of aligning scientific documents at different levels. We consider citation recommendation (whether A cites B) and citation localization (which sentence in A cites) at the same time. The confidence of our model for citation localization is represented by the degree of blueness. Introduction Aligning texts and understanding their relationships is a common problem for NLP tasks such as citation recommendation (Bhagavatula et al., 2018; Jiang et al., 2019), comparable document mining (He et al., 2010; Peng et al., 2016; Bhagavatula et al., 2018; Guo et al., 2019), parallel sentence mining (Shi et al., 2006; Ture and Lin, 2012; Guo et al., 2018), plagiarism detection (Barr´on-Cede˜no et al., 2010; Forner et al., 2013; Ferrero et al., 2017), paraphrase identification (Wan et al., 2006; Das and Smith, 2009; Wang et al., 2016), and textual entailment (Dagan and Glickman, 2004; Androutsopoulos and Malakasiotis, 2010; Zhao et al., 2016). Longer texts make the problem more challenging due to the potential complexity of the underlying correspondence. Here, we develop a model to address this problem and demonstrate its applicability on three different tasks which require such understanding, namely on ci"
2020.emnlp-main.407,C18-1203,0,0.0393618,"trained BERT, we instead take the average of word token vectors to obtain sentence vectors, following Reimers and Gurevych (2019). Hierarchical Attention Networks Yang et al. (2016) introduced a family of document encoding models that are based on a word/sentence/document hierarchy, known as hierarchical attention networks (HANs). They have been shown superior to earlier hierarchical encoding models based on convolutional networks (Collobert et al., 2011; Kim, 2014; Zhang et al., 2016), they are competitive for tasks involving long documents (Choi et al., 2016; Pappas and Popescu-Belis, 2017; Sun et al., 2018; Miculicich et al., 2018; Liu and Lapata, 2019; Guo et al., 2019),1 and they can be used orthogonally to other design decisions (e.g., word embeddings and the use of pretraining). For document X, a HAN builds a vector representation dX using the (given) structure of X: typically, the document vector is derived from sentence vectors, which are derived from (contextualized) word vectors. Working in the order that the computation proceeds, the encoding procedure is: 1. Each word in the document is mapped (by lookup) to its type embedding.2 2. Each word’s vector is contextualized, i.e., a new wor"
2020.emnlp-main.407,N12-1079,0,0.0324846,"scientific documents at different levels. We consider citation recommendation (whether A cites B) and citation localization (which sentence in A cites) at the same time. The confidence of our model for citation localization is represented by the degree of blueness. Introduction Aligning texts and understanding their relationships is a common problem for NLP tasks such as citation recommendation (Bhagavatula et al., 2018; Jiang et al., 2019), comparable document mining (He et al., 2010; Peng et al., 2016; Bhagavatula et al., 2018; Guo et al., 2019), parallel sentence mining (Shi et al., 2006; Ture and Lin, 2012; Guo et al., 2018), plagiarism detection (Barr´on-Cede˜no et al., 2010; Forner et al., 2013; Ferrero et al., 2017), paraphrase identification (Wan et al., 2006; Das and Smith, 2009; Wang et al., 2016), and textual entailment (Dagan and Glickman, 2004; Androutsopoulos and Malakasiotis, 2010; Zhao et al., 2016). Longer texts make the problem more challenging due to the potential complexity of the underlying correspondence. Here, we develop a model to address this problem and demonstrate its applicability on three different tasks which require such understanding, namely on citation recommendatio"
2020.emnlp-main.407,U06-1019,0,0.0186073,"me time. The confidence of our model for citation localization is represented by the degree of blueness. Introduction Aligning texts and understanding their relationships is a common problem for NLP tasks such as citation recommendation (Bhagavatula et al., 2018; Jiang et al., 2019), comparable document mining (He et al., 2010; Peng et al., 2016; Bhagavatula et al., 2018; Guo et al., 2019), parallel sentence mining (Shi et al., 2006; Ture and Lin, 2012; Guo et al., 2018), plagiarism detection (Barr´on-Cede˜no et al., 2010; Forner et al., 2013; Ferrero et al., 2017), paraphrase identification (Wan et al., 2006; Das and Smith, 2009; Wang et al., 2016), and textual entailment (Dagan and Glickman, 2004; Androutsopoulos and Malakasiotis, 2010; Zhao et al., 2016). Longer texts make the problem more challenging due to the potential complexity of the underlying correspondence. Here, we develop a model to address this problem and demonstrate its applicability on three different tasks which require such understanding, namely on citation recommendation, citation localization, and plagiarism detection for general web documents. One key component of an NLP system for aligning documents is the encoding process."
2020.emnlp-main.407,C16-1127,0,0.0300062,"r citation localization is represented by the degree of blueness. Introduction Aligning texts and understanding their relationships is a common problem for NLP tasks such as citation recommendation (Bhagavatula et al., 2018; Jiang et al., 2019), comparable document mining (He et al., 2010; Peng et al., 2016; Bhagavatula et al., 2018; Guo et al., 2019), parallel sentence mining (Shi et al., 2006; Ture and Lin, 2012; Guo et al., 2018), plagiarism detection (Barr´on-Cede˜no et al., 2010; Forner et al., 2013; Ferrero et al., 2017), paraphrase identification (Wan et al., 2006; Das and Smith, 2009; Wang et al., 2016), and textual entailment (Dagan and Glickman, 2004; Androutsopoulos and Malakasiotis, 2010; Zhao et al., 2016). Longer texts make the problem more challenging due to the potential complexity of the underlying correspondence. Here, we develop a model to address this problem and demonstrate its applicability on three different tasks which require such understanding, namely on citation recommendation, citation localization, and plagiarism detection for general web documents. One key component of an NLP system for aligning documents is the encoding process. Present approaches for comparing documen"
2020.emnlp-main.407,N16-1174,0,0.509391,"., 2016). Longer texts make the problem more challenging due to the potential complexity of the underlying correspondence. Here, we develop a model to address this problem and demonstrate its applicability on three different tasks which require such understanding, namely on citation recommendation, citation localization, and plagiarism detection for general web documents. One key component of an NLP system for aligning documents is the encoding process. Present approaches for comparing documents rely on hierarchically structured document encoders such as hierarchical attention networks (HANs; Yang et al., 2016), which independently represent the two documents as fixed-length vectors. The vectors are fed to a classifier which makes a decision on the relation between them (Jiang et al., 2019; Guo et al., 2019). However, such methods do not provide insights about or leverage the underlying relationships across documents and are applicable only to a single, predefined level (Jiang et al., 2019; Yang et al., 2020). Importantly, when comparing documents, those methods ignore the structural cor5012 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 5012–5025, c No"
2020.emnlp-main.407,D15-1027,0,0.0235189,"latent graph learning to capture dependencies between pairs of words from unlabeled data. Orthogonal to these studies, we use attention to compare documents represented by hierarchical document encoders at multiple levels. Similarity Learning. There are three types of similarity learning in NLP. The supervised paradigm differs from typical supervised learning in that 5019 training examples are cast into pairwise constraints (Yang and Jin, 2006), as in cross-lingual word embedding learning based on word-level alignments (Faruqui and Dyer, 2014) and zero-shot utterance/document classification (Yazdani and Henderson, 2015; Nam et al., 2016; Pappas and Henderson, 2019) based on utterance/document-level annotations. The unsupervised paradigm aims to learn an underlying low-dimensional space where the relationships between most of the observed data are preserved, as in word embedding learning (Collobert et al., 2011; Mikolov et al., 2013; Pennington et al., 2014; Levy and Goldberg, 2014). The weakly supervised paradigm is the middle ground between the two, as in cross-lingual word embedding learning based on sentence-level alignments (Hermann and Blunsom, 2014; Gouws et al., 2015). Our approach is weakly supervis"
2020.emnlp-main.407,C16-1212,0,0.0151747,"g their relationships is a common problem for NLP tasks such as citation recommendation (Bhagavatula et al., 2018; Jiang et al., 2019), comparable document mining (He et al., 2010; Peng et al., 2016; Bhagavatula et al., 2018; Guo et al., 2019), parallel sentence mining (Shi et al., 2006; Ture and Lin, 2012; Guo et al., 2018), plagiarism detection (Barr´on-Cede˜no et al., 2010; Forner et al., 2013; Ferrero et al., 2017), paraphrase identification (Wan et al., 2006; Das and Smith, 2009; Wang et al., 2016), and textual entailment (Dagan and Glickman, 2004; Androutsopoulos and Malakasiotis, 2010; Zhao et al., 2016). Longer texts make the problem more challenging due to the potential complexity of the underlying correspondence. Here, we develop a model to address this problem and demonstrate its applicability on three different tasks which require such understanding, namely on citation recommendation, citation localization, and plagiarism detection for general web documents. One key component of an NLP system for aligning documents is the encoding process. Present approaches for comparing documents rely on hierarchically structured document encoders such as hierarchical attention networks (HANs; Yang et"
2020.emnlp-main.429,J86-3001,0,0.736379,"on Writing Strategies The goal of general science communication is to increase public awareness, enjoyment, interest, and understanding about science (Burns et al., 2003). Based on the idea of compositionality in discourse theory (Bender and Lascarides, 2019), we can think of the communicative intent of science writing as being made up of smaller communication goals rep5327 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 5327–5344, c November 16–20, 2020. 2020 Association for Computational Linguistics resented in particular passages of an article (Grosz and Sidner, 1986; Louis and Nenkova, 2013a). Our computational approach builds on this theoretical assumption by annotating sentences and letting an article inherit the attributes we find in its sentences. Past work on science communication has taken a similar view (Louis and Nenkova, 2013a) by using syntactic relations to characterize an article’s communicative goals, allowing them to emerge inductively rather than from a theory of science communication. Our complementary approach starts with science communication guides to construct theory-driven communicative goals (referred to as “writing strategies” and"
2020.emnlp-main.429,W10-1913,0,0.0212414,"literacy to fostering participation in science (Hetland, 2014). A growing body of research shows that scientific literacy is only one of many factors that influence public decision making and cannot be divorced from cultural values (Nisbet and Scheufele, 2009; Bubela et al., 2009). Scientific writing. There is a wealth of work exploring writing in scientific journals (i.e., when scientists communicate within their discipline). Because of the natural structure of scientific journal papers, much work has looked at ways of automatically identifying content in these papers (Liakata et al., 2010; Guo et al., 2010; Liakata et al., 2012). Kr¨oll et al. (2014) examined the use of 10 While these press releases were written in the U.K., we expect science writing to be more invariant to regional dialects than other genres of writing. Future Work We annotated writing strategies at the sentence level, but some strategies, such as STORY and ANALOGY , might be better annotated at the fragment level to account for longer or shorter use of strategies. Future work can explore more finegrained analysis of these strategies (e.g., with metaphor detectors; Gao et al., 2018). We also hope to build on these findings by"
2020.emnlp-main.429,liakata-etal-2010-corpora,0,0.0416845,"m improving scientific literacy to fostering participation in science (Hetland, 2014). A growing body of research shows that scientific literacy is only one of many factors that influence public decision making and cannot be divorced from cultural values (Nisbet and Scheufele, 2009; Bubela et al., 2009). Scientific writing. There is a wealth of work exploring writing in scientific journals (i.e., when scientists communicate within their discipline). Because of the natural structure of scientific journal papers, much work has looked at ways of automatically identifying content in these papers (Liakata et al., 2010; Guo et al., 2010; Liakata et al., 2012). Kr¨oll et al. (2014) examined the use of 10 While these press releases were written in the U.K., we expect science writing to be more invariant to regional dialects than other genres of writing. Future Work We annotated writing strategies at the sentence level, but some strategies, such as STORY and ANALOGY , might be better annotated at the fragment level to account for longer or shorter use of strategies. Future work can explore more finegrained analysis of these strategies (e.g., with metaphor detectors; Gao et al., 2018). We also hope to build on"
2020.emnlp-main.429,2021.ccl-1.108,0,0.0511174,"Missing"
2020.emnlp-main.491,2020.acl-main.23,0,0.341107,"Missing"
2020.emnlp-main.491,K18-1040,0,0.0230713,"uccess, these models are costly to train and require a large amount of parallel data. Yet parallel data is scarce for conditional text generation problems, necessitating unsupervised 1 We use this term to refer to text generation conditioned on textual input. solutions. Text autoencoders (Bowman et al., 2016) have proven useful for a particular subclass of unsupervised problems that can be broadly defined as style transfer, i.e., changing the style of a text in such a way that the content of the input is preserved. Examples include sentiment transfer (Shen et al., 2017), sentence compression (Fevry and Phang, 2018), and neural machine translation (Artetxe et al., 2018). Most existing methods specialize autoencoders to the task by conditioning the decoder on the style attribute of interest (Lample et al., 2019; Logeswaran et al., 2018), assuming the presence of labels during training of the autoencoder. The main drawback of this approach is that it cannot leverage pretraining on unlabeled data, which is probably the most important factor for widespread progress in supervised NLP models in recent years in text analysis (Peters et al., 2018; Radford et al., 2019; Devlin et al., 2019) and generation tasks ("
2020.emnlp-main.491,K16-1028,0,0.0613516,"Missing"
2020.emnlp-main.491,D19-1659,0,0.0262443,"2018; Logeswaran et al., 2018; Yang et al., 2018; Li et al., 2019), which hinders their employment in a plug and play fashion. Most methods either rely on adversarial objectives (Shen et al., 2017; Hu et al., 2017; Fu et al., 2018), retrieval (Li et al., 2018), or backtranslation (Lample et al., 2019; Logeswaran et al., 2018) to make the latent codes independent of the style attribute. Notable exceptions are Transformer-based (Dai et al., 2019; Sudhakar et al., 2019), use reinforcement learning for backtranslating through the discrete space (Liu and Liu, 2019), build pseudo-parallel corpora (Kruengkrai, 2019; Jin et al., 2019), or modify the latent-variable at inference time by following the gradient of a style classifier (Wang et al., 2019; Liu et al., 2020). Similar to our motivation, Li et al. (2019) aim at improving in-domain performance by incorporating out-of-domain data into training. However, because their model again conditions on the target data, they have to train the autoencoder jointly with the target corpus, defeating the purpose of large-scale pretraining. In contrast to previous methods, Emb2Emb can be combined with any pretrained autoencoder even if it was not trained with target"
2020.emnlp-main.491,D19-1325,0,0.0199346,"t be conditioned on input text, and are thus not applicable to style transfer. Figure 6: Sentiment transfer results for different model scenarios. Up and right is better. 4 Related Work Text Style Transfer The most common approach to text style transfer is to learn a disentangled shared latent space that is agnostic to the style of the input. Style transfer is then achieved by training the decoder conditioned on the desired style attribute, (Hu et al., 2017; Shen et al., 2017; Fu et al., 2018; Zhao et al., 2018; Lample et al., 2019; Li et al., 2018; Logeswaran et al., 2018; Yang et al., 2018; Li et al., 2019), which hinders their employment in a plug and play fashion. Most methods either rely on adversarial objectives (Shen et al., 2017; Hu et al., 2017; Fu et al., 2018), retrieval (Li et al., 2018), or backtranslation (Lample et al., 2019; Logeswaran et al., 2018) to make the latent codes independent of the style attribute. Notable exceptions are Transformer-based (Dai et al., 2019; Sudhakar et al., 2019), use reinforcement learning for backtranslating through the discrete space (Liu and Liu, 2019), build pseudo-parallel corpora (Kruengkrai, 2019; Jin et al., 2019), or modify the latent-variable"
2020.emnlp-main.491,N18-1202,0,0.0832923,"Missing"
2020.emnlp-main.491,W19-4302,1,0.874578,"Missing"
2020.emnlp-main.491,P19-1153,0,0.0239885,"etrain-and-plugin variational autoencoders (Duan Textual Autoencoders Autoencoders are a very active field of research, leading to constant progress through denoising (Vincent et al., 2010), variational (Kingma and Welling, 2014; Higgins et al., 2017; Dai and Wipf, 2019), adversarial (Makhzani et al., 2016; Zhao et al., 2018), and, more recently, regularized (Ghosh et al., 2020) autoencoders, to name a few. Ever since Bowman et al. (2016) adopted variational autoencoders for sentences by employing a recurrent sequence-to-sequence model, improving both the architecture (Semeniuta et al., 2017; Prato et al., 2019; Liu and Liu, 2019; Gagnon-Marchand et al., 2019) and the training objective (Zhao et al., 2018; Shen et al., 2020) have received considerable attention. The goal is typically to improve both the reconstruction and generation performance (C´ıfka et al., 2018). Our framework is completely agnostic to the type of autoencoder that is used, as long as it is trained to reconstruct the input. Hence, our framework directly benefits from any kind of modelling advancement in autoencoder research. 5 Conclusion In this paper, we present Emb2Emb, a framework that reduces conditional text generation tasks"
2020.emnlp-main.491,D15-1044,0,0.047061,"rms better than or comparable to strong baselines while being up to four times faster. 1 Figure 1: The manifold of a text autoencoder is the lowdimensional region of the high-dimensional embedding space where texts are actually embedded. The example shows the mapping of a source sequence x with embedding zx to zy , which is the embedding of target sequence y such that it reflects the target manifold. Introduction Conditional text generation1 encompasses a large number of natural language processing tasks such as text simplification (Nisioi et al., 2017; Zhang and Lapata, 2017), summarization (Rush et al., 2015; Nallapati et al., 2016), machine translation (Bahdanau et al., 2015; Kumar and Tsvetkov, 2019) and style transfer (Shen et al., 2017; Fu et al., 2018). When training data is available, the state of the art includes encoder-decoder models with an attention mechanism (Bahdanau et al., 2015; Vaswani et al., 2017) which are both extensions of the original sequence-to-sequence framework with a fixed bottleneck introduced by Sutskever et al. (2014). Despite their success, these models are costly to train and require a large amount of parallel data. Yet parallel data is scarce for conditional text"
2020.emnlp-main.491,D17-1066,0,0.011888,"It is also similar to pretrain-and-plugin variational autoencoders (Duan Textual Autoencoders Autoencoders are a very active field of research, leading to constant progress through denoising (Vincent et al., 2010), variational (Kingma and Welling, 2014; Higgins et al., 2017; Dai and Wipf, 2019), adversarial (Makhzani et al., 2016; Zhao et al., 2018), and, more recently, regularized (Ghosh et al., 2020) autoencoders, to name a few. Ever since Bowman et al. (2016) adopted variational autoencoders for sentences by employing a recurrent sequence-to-sequence model, improving both the architecture (Semeniuta et al., 2017; Prato et al., 2019; Liu and Liu, 2019; Gagnon-Marchand et al., 2019) and the training objective (Zhao et al., 2018; Shen et al., 2020) have received considerable attention. The goal is typically to improve both the reconstruction and generation performance (C´ıfka et al., 2018). Our framework is completely agnostic to the type of autoencoder that is used, as long as it is trained to reconstruct the input. Hence, our framework directly benefits from any kind of modelling advancement in autoencoder research. 5 Conclusion In this paper, we present Emb2Emb, a framework that reduces conditional t"
2020.emnlp-main.746,D15-1075,0,0.585049,"dence) corresponds to easy-to-learn examples, the bottomleft corner (low variability, low confidence) corresponds to hard-to-learn examples, and examples on the right (with high variability) are ambiguous; all definitions are with respect to the RO BERTA-large model. The modal group in the data is formed by the easy-to-learn regions. For clarity we only plot 25K random samples from the SNLI train set. Fig. 8b in App. §C shows the same map in greater relief. The creation of large labeled datasets has fueled the advance of AI (Russakovsky et al., 2015; Antol et al., 2015) and NLP in particular (Bowman et al., 2015; Rajpurkar et al., 2016). The common belief is that the more abundant the labeled data, the higher the likelihood of learning diverse phenomena, which in turn leads to models that generalize well. In practice, however, out-of-distribution Work done at the Allen Institute for AI. ambiguou 0.4 0.2 Introduction ∗ 0.6 correct. 0.0 0.2 0.3 0.5 0.7 0.8 1.0 (OOD) generalization remains a challenge (Yogatama et al., 2019; Linzen, 2020); and, while recent large pretrained language models help, they fail to close this gap (Hendrycks et al., 2020). This urges a closer look at datasets, where not all exa"
2020.emnlp-main.746,P17-1152,0,0.0393322,"scussion (with empirical justifications) on connections between training dynamics measures and dropout-based (Srivastava et al., 2014), first-principles uncertainty estimates. These relations are further supported by previous work, which showed that deep ensembles provide well-calibrated uncertainty estimates (Lakshminarayanan et al., 2017; Gustafsson et al., 2019; Snoek et al., 2019). Generally, such approaches ensemble models trained from scratch; while ensembles of training checkpoints lose some diversity (Fort et al., 2019), they offer a cheaper alternative capturing some of the benefits (Chen et al., 2017a). Future work will involve investigation of such alternatives for building data maps. 7 Related Work Our work builds data maps using training dynamics measures for scoring data instances. Loss landscapes (Xing et al., 2018) are similar to training dynamics, but also consider variables from the stochastic optimization algorithm. Toneva et al. (2018) also use training dynamics to find train examples which are frequently “forgotten”, i.e., mis9282 classified during a later epoch of training, despite being classified correctly earlier; our correctness metric provides similar discrete scores, and"
2020.emnlp-main.746,N19-1423,0,0.0734792,"Missing"
2020.emnlp-main.746,D19-1224,1,0.892059,"Missing"
2020.emnlp-main.746,N18-2017,1,0.861687,"Missing"
2020.emnlp-main.746,2020.acl-main.244,0,0.0262881,"Missing"
2020.emnlp-main.746,N13-1132,0,0.0221471,"e ability of simple linear classifiers to predict them correctly. While AFLite, among others (Li and Vasconcelos, 2019; Gururangan et al., 2018), advocate removing “easy” instances from the dataset, our work shows that easy-to-learn instances can be useful. Similar intuitions have guided other work such as curriculum learning (Bengio et al., 2009) and self-paced learning (Kumar et al., 2010; Lee and Grauman, 2011) where all examples are prioritized based on their “difficulty”. Other approaches have used training loss (Han et al., 2018; Arazo et al., 2019; Shen and Sanghavi, 2019), confidence (Hovy et al., 2013), and meta-learning (Ren et al., 2018), to differentiate instances within datasets. Perhaps our measures are the closest to those from Chang et al. (2017); they propose prediction variance and threshold closeness—which correspond to variability and confidence, respectively.18 However, they use these measures to reweight all instances, similar to sampling effective batches in online learning (Loshchilov and Hutter, 2016). Our work, instead, does a hard selection for the purpose of studying different groups within data. Our methods are also reminiscent of active learning methods (Settles, 2009;"
2020.emnlp-main.746,D17-1215,0,0.127475,"Missing"
2020.emnlp-main.746,W02-2015,0,0.109712,"oring data instances. Loss landscapes (Xing et al., 2018) are similar to training dynamics, but also consider variables from the stochastic optimization algorithm. Toneva et al. (2018) also use training dynamics to find train examples which are frequently “forgotten”, i.e., mis9282 classified during a later epoch of training, despite being classified correctly earlier; our correctness metric provides similar discrete scores, and results in models with better performance. Variants of such approaches address catastrophic forgetting, and are useful for analyzing data instances (Pan et al., 2020; Krymolowski, 2002). Prior work has proposed other criteria to score instances. AFLite (LeBras et al., 2020) is an adversarial filtering algorithm which ranks instances based on their “predictability”, i.e. the ability of simple linear classifiers to predict them correctly. While AFLite, among others (Li and Vasconcelos, 2019; Gururangan et al., 2018), advocate removing “easy” instances from the dataset, our work shows that easy-to-learn instances can be useful. Similar intuitions have guided other work such as curriculum learning (Bengio et al., 2009) and self-paced learning (Kumar et al., 2010; Lee and Grauman"
2020.emnlp-main.746,2020.acl-main.465,0,0.0320741,"map in greater relief. The creation of large labeled datasets has fueled the advance of AI (Russakovsky et al., 2015; Antol et al., 2015) and NLP in particular (Bowman et al., 2015; Rajpurkar et al., 2016). The common belief is that the more abundant the labeled data, the higher the likelihood of learning diverse phenomena, which in turn leads to models that generalize well. In practice, however, out-of-distribution Work done at the Allen Institute for AI. ambiguou 0.4 0.2 Introduction ∗ 0.6 correct. 0.0 0.2 0.3 0.5 0.7 0.8 1.0 (OOD) generalization remains a challenge (Yogatama et al., 2019; Linzen, 2020); and, while recent large pretrained language models help, they fail to close this gap (Hendrycks et al., 2020). This urges a closer look at datasets, where not all examples might contribute equally towards learning (Vodrahalli et al., 2018). However, the scale of data can make this assessment challenging. How can we automatically characterize data instances with respect to their role in achieving good performance in- and out-of- distribution? Answering this question may take us a step closer to bridging the gap between dataset collection and broader task 9275 Proceedings of the 2020 Conferenc"
2020.emnlp-main.746,2021.ccl-1.108,0,0.106443,"Missing"
2020.emnlp-main.746,N19-1262,0,0.0341628,"Missing"
2020.emnlp-main.746,N18-1101,0,0.32608,"constructed using the RO BERTA-large model (Liu et al., 2019). The map reveals three distinct regions in the dataset: a region with instances whose true class probabilities fluctuate frequently during training (high variability), and are hence ambiguous for the model; a region with easy-to-learn instances that the model predicts correctly and consistently (high confidence, low variability); and a region with hard-to-learn instances with low confidence, low variability, many of which we find are mislabeled during annotation .1 Similar regions are observed across three other datasets: MultiNLI (Williams et al., 2018), WinoGrande (Sakaguchi et al., 2020) and SQuAD (Rajpurkar et al., 2016), with respect to respective RO BERTA-large classifiers. We further investigate the above regions by training models exclusively on examples from each region (§3). Training on ambiguous instances promotes generalization to OOD test sets, with little or no effect on in-distribution (ID) performance.2 Our data maps also reveal that datasets contain a majority of easy-to-learn instances, which are not as critical for ID or OOD performance, but without any such instances, training could fail to converge (§4). In §5, we show th"
2020.emnlp-main.746,W18-5446,0,0.0616166,"Missing"
2020.emnlp-main.746,N13-1086,0,0.0146549,", 2016). Our work, instead, does a hard selection for the purpose of studying different groups within data. Our methods are also reminiscent of active learning methods (Settles, 2009; Peris and Casacuberta, 2018; P.V.S and Meyer, 2019), such as uncertainty sampling (Lewis and Gale, 1994) which selects (unlabeled) data points, which a model trained on a small labeled subset, has least confidence in, or predicts as farthest (in vector space, based on cosine similarity) (Sener and Savarese, 2018; Wolf, 2011). Our approach uses labeled data for selection, similar to core-set selection approaches (Wei et al., 2013). Active learning approaches could be used in conjunction with data maps to create better datasets, similar to approaches proposed in Mishra et al. (2020). For instance, creating datasets 18 They also consider confidence intervals; our preliminary experiments, with and without, yielded similar results. with more ambiguous examples (with respect to a given model) could make it beneficial for OOD generalization. Data error detection also involves instance scoring. Influence functions (Koh and Liang, 2017), forgetting events (Toneva et al., 2018), cross validation (Chen et al., 2019), Shapely val"
2020.emnlp-main.96,D18-1461,0,0.0209535,"word it did not see in training. This is a longstanding challenge of language modeling (Jelinek, 1997), but it becomes especially important when we adapt to new domains and tasks. One way to “open up” the vocabulary is to model sequences of bytes, characters, or “wordpieces” rather than the conventional word tokens (Sennrich et al., 2016; Radford et al., 2018; Ponti et al., 2019). While effective, this approach requires the LM to memorize subsequences if it is to treat them as words. These models appear to require greater network depth and show slower convergence than word-based alternatives (Cherry et al., 2018; Al-Rfou et al., 2019); the extra work comes at a cost. This is one of the reasons why the area of word-level language modeling is still very active (Baevski and Auli, 2019; Sukhbaatar et al., 2019; Khandelwal et al., 2020; Press et al., 2020). Interpolations between word- and character- or morphology-based LMs represent another class of solutions (Mielke and Eisner, 2018; Gerz et al., 2018; Ataman et al., 2020). These “hybrid” approaches combine benefits from both model types. However, they introduce complexity which makes them potentially more difficult to train, maintain, and analyze. Nota"
2020.emnlp-main.96,N19-1423,0,0.186914,"nd on the training vocabulary. We evaluate the model on conventional language modeling as well as challenging cross-domain settings with an open vocabulary, finding that it matches or outperforms previous state-of-theart output embedding methods and adaptation approaches. Our analysis attributes the improvements to sample efficiency: our model is more accurate for low-frequency words. 1 Noah A. Smith♣♦ Introduction Language models (LMs) are at the heart of natural language processing, especially following their recent success in the pretraining paradigm (Dai and Le, 2015; Peters et al., 2018; Devlin et al., 2019; Radford et al., 2019, inter alia). Continued advances in NLP rely on the adaptability of LMs to domains beyond their training data and to new domains and tasks, e.g., through domain adaptive pretraining followed by finetuning (Gururangan et al., 2020). Here, we focus on an important component of LMs, namely the output vocabulary— over which a LM’s probability distribution over the “next word” (given the history) ranges—and investigate the impact of the type of its representation on the adaptability of neural LMs. Today, LMs are typically trained with a closed output vocabulary derived from t"
2020.emnlp-main.96,E17-2025,0,0.399773,"IcKDUNPB0Z7akmq9l5n+1Xgz+hZuwMIqBhmT2kR9zC4SV5WENmaQE+FQDJpLpXS0yxhIT0KlVdAjO/MmL0K7XnNNa/fas2rgs4iijQ3SETpCDzlED3aAmaiGCHtEzekVvxpPxYrwbH7PWklHM7KM/Mj5/AKY5mD0=&lt;/latexit&gt; 2.2 Ein Choice of Output Representations Above we assumed an output embedding matrix Eout that independently parameterizes each word in the vocabulary with a separate d-dimensional vector. This approach requires d × |V |parameters, leading to concerns about cost and overparameterization. Prior work addressed this issue by tying parameters between the input and output embedding matrices (i.e., Eout = Ein ; Inan et al., 2017; Press and Wolf, 2017). However, the parameters for each word are still independent from each other, as displayed in Figure 1(a). An alternative, also considered here, is to share output parameters across words as well as with the input embeddings. Specifically, this involves making the output embedding a function of the input embedding using a shared parameterization across words, Eout = g(Ein ), as displayed in Figure 1(b). For example, Gulordava et al. (2018) used a linear transformation, while Baevski and Auli (2019) used a linear transformation for each frequency bin to dedicate parameters to words proportiona"
2020.emnlp-main.96,P16-1162,0,0.344505,"apted or deployed. This makes large pretrained language models struggle with rare words, despite being able to produce contextualized representations for them (Schick and Sch¨utze, 2020). More importantly, this means a generative LM can never give nonzero probability to a specific word it did not see in training. This is a longstanding challenge of language modeling (Jelinek, 1997), but it becomes especially important when we adapt to new domains and tasks. One way to “open up” the vocabulary is to model sequences of bytes, characters, or “wordpieces” rather than the conventional word tokens (Sennrich et al., 2016; Radford et al., 2018; Ponti et al., 2019). While effective, this approach requires the LM to memorize subsequences if it is to treat them as words. These models appear to require greater network depth and show slower convergence than word-based alternatives (Cherry et al., 2018; Al-Rfou et al., 2019); the extra work comes at a cost. This is one of the reasons why the area of word-level language modeling is still very active (Baevski and Auli, 2019; Sukhbaatar et al., 2019; Khandelwal et al., 2020; Press et al., 2020). Interpolations between word- and character- or morphology-based LMs repres"
2020.emnlp-main.96,N18-1202,0,0.630585,"ze that does not depend on the training vocabulary. We evaluate the model on conventional language modeling as well as challenging cross-domain settings with an open vocabulary, finding that it matches or outperforms previous state-of-theart output embedding methods and adaptation approaches. Our analysis attributes the improvements to sample efficiency: our model is more accurate for low-frequency words. 1 Noah A. Smith♣♦ Introduction Language models (LMs) are at the heart of natural language processing, especially following their recent success in the pretraining paradigm (Dai and Le, 2015; Peters et al., 2018; Devlin et al., 2019; Radford et al., 2019, inter alia). Continued advances in NLP rely on the adaptability of LMs to domains beyond their training data and to new domains and tasks, e.g., through domain adaptive pretraining followed by finetuning (Gururangan et al., 2020). Here, we focus on an important component of LMs, namely the output vocabulary— over which a LM’s probability distribution over the “next word” (given the history) ranges—and investigate the impact of the type of its representation on the adaptability of neural LMs. Today, LMs are typically trained with a closed output voca"
2020.emnlp-main.96,D19-1005,1,0.893084,"Missing"
2020.emnlp-main.96,D19-1288,0,0.0371727,"Missing"
2020.emnlp-main.96,P19-1032,0,0.0134189,"en up” the vocabulary is to model sequences of bytes, characters, or “wordpieces” rather than the conventional word tokens (Sennrich et al., 2016; Radford et al., 2018; Ponti et al., 2019). While effective, this approach requires the LM to memorize subsequences if it is to treat them as words. These models appear to require greater network depth and show slower convergence than word-based alternatives (Cherry et al., 2018; Al-Rfou et al., 2019); the extra work comes at a cost. This is one of the reasons why the area of word-level language modeling is still very active (Baevski and Auli, 2019; Sukhbaatar et al., 2019; Khandelwal et al., 2020; Press et al., 2020). Interpolations between word- and character- or morphology-based LMs represent another class of solutions (Mielke and Eisner, 2018; Gerz et al., 2018; Ataman et al., 2020). These “hybrid” approaches combine benefits from both model types. However, they introduce complexity which makes them potentially more difficult to train, maintain, and analyze. Notable for enabling adaptability are interpolated LMs based on copy mechanisms (Merity et al., 2017), dynamic evaluation (Krause et al., 1252 Proceedings of the 2020 Conference on Empirical Methods in"
2020.emnlp-main.96,2020.acl-main.270,1,0.682367,"es, characters, or “wordpieces” rather than the conventional word tokens (Sennrich et al., 2016; Radford et al., 2018; Ponti et al., 2019). While effective, this approach requires the LM to memorize subsequences if it is to treat them as words. These models appear to require greater network depth and show slower convergence than word-based alternatives (Cherry et al., 2018; Al-Rfou et al., 2019); the extra work comes at a cost. This is one of the reasons why the area of word-level language modeling is still very active (Baevski and Auli, 2019; Sukhbaatar et al., 2019; Khandelwal et al., 2020; Press et al., 2020). Interpolations between word- and character- or morphology-based LMs represent another class of solutions (Mielke and Eisner, 2018; Gerz et al., 2018; Ataman et al., 2020). These “hybrid” approaches combine benefits from both model types. However, they introduce complexity which makes them potentially more difficult to train, maintain, and analyze. Notable for enabling adaptability are interpolated LMs based on copy mechanisms (Merity et al., 2017), dynamic evaluation (Krause et al., 1252 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 1252–1267,"
2020.findings-emnlp.117,W07-2441,0,0.0137691,"1. UD Parsing Finally, we discuss dependency parsing in the universal dependencies (UD) formalism (Nivre et al., 2016). We look at dependency parsing to show that contrast sets apply not only to modern “high-level” NLP tasks but also to longstanding linguistic analysis tasks. We first chose a specific type of attachment ambiguity to target: the classic problem of prepositional phrase (PP) attachment (Collins and Brooks, 1995), e.g. We ate spaghetti with forks versus We ate spaghetti with meatballs. We use a subset of the English UD treebanks: GUM (Zeldes, 2017), the English portion of LinES (Ahrenberg, 2007), the English portion of ParTUT (Sanguinetti and Bosco, 2015), and the dependency-annotated English Web Treebank (Silveira et al., 2014). We searched these treebanks for sentences that include a potentially structurally ambiguous attachment from the head of a PP to either a noun or a verb. We then perturbed these sentences by altering one of their noun phrases such that the semantics of the perturbed sentence required a different attachment for the PP. We then re-annotated these perturbed sentences to indicate the new attachment(s). Summary While the overall process we recommend for constructi"
2020.findings-emnlp.117,D15-1075,0,0.0445289,"ow frequently does Tom drink? Candidate Answer: Every other night Label: Unlikely Table 1: We create contrast sets for 10 datasets and show instances from seven of them here. guage. We do not expect intuition, even of experts, to exhaustively reveal gaps. Nevertheless, the presence of these gaps is welldocumented (Gururangan et al., 2018; Poliak et al., 2018; Min et al., 2019), and Niven and Kao (2019) give an initial attempt at formally characterizing them. In particular, one common source is annotator bias from data collection processes (Geva et al., 2019). For example, in the SNLI dataset (Bowman et al., 2015), Gururangan et al. (2018) show that the words sleeping, tv, and cat almost never appear in an entailment example, either in the training set or the test set, though they often appear in contradiction examples. This is not because these words are particularly important to the phenomenon of entailment; their absence in entailment examples is a systematic gap in the data that can be exploited by models to achieve artificially high test accuracy. This is but one kind of systematic gap; there are also biases due to the writing styles of small groups of annotators (Geva et al., 2019), the distribut"
2020.findings-emnlp.117,W18-6433,0,0.0341655,"Missing"
2020.findings-emnlp.117,D19-1606,1,0.924571,"round a pivot (e.g., Ribeiro et al., 2018b, 2019). However, it is very challenging to come up with rules or other automated methods for pushing pivots across a decision boundary—in most cases this presupposes a model that can already perform the intended task. We recommend annotators spend their time constructing these types of examples; easier examples can be automated. Adversarial Construction of Contrast Sets Some recent datasets are constructed using baseline models in the data collection process, either to filter out examples that existing models answer correctly (e.g., Dua et al., 2019; Dasigi et al., 2019) or to generate adversarial inputs (e.g., Zellers et al., 2018, 2019; Wallace et al., 2019b; Nie et al., 2019). Unlike this line of work, we choose not to have a model in the loop because this can bias the data to the failures of a particular model (cf. Zellers et al., 2019), rather than generally characterizing the local decision boundary. We do think it is acceptable to use a model on a handful of initial perturbations to understand which phenomena are worth spending time on, but this should be separate from the actual annotation process—observing model outputs while perturbing data creates"
2020.findings-emnlp.117,N19-1423,0,0.0110664,"st sets in Table 1. For most datasets, the average time to perturb each example was 1–3 minutes, which translates to approximately 17–50 hours of work to create 1,000 examples. However, some datasets, particularly those with complex output structures, took substantially longer: each example for dependency parsing took an average of 15 minutes (see Appendix B for more details). 4.3 Models Struggle on Contrast Sets For each dataset, we use a model that is at or near state-of-the-art performance. Most models involve fine-tuning a pretrained language model (e.g., ELMo (Peters et al., 2018), BERT (Devlin et al., 2019), RO BERTA (Liu et al., 2019), XLNet (Yang et al., 2019), etc.) or applying a task-specific architecture on top of one (e.g., Hu et al. (2019) add a DROP-specific model on top of BERT). We train each model on the original training set and evaluate it on both the original test set and our contrast sets. Existing models struggle on the contrast sets (Table 2), particularly when evaluating contrast consistency. Model performance degrades differently across datasets; however, note that these numbers are not directly comparable due to differences in dataset size, model architecture, contrast set de"
2020.findings-emnlp.117,N19-1246,1,0.944307,"arts of the gaps around a pivot (e.g., Ribeiro et al., 2018b, 2019). However, it is very challenging to come up with rules or other automated methods for pushing pivots across a decision boundary—in most cases this presupposes a model that can already perform the intended task. We recommend annotators spend their time constructing these types of examples; easier examples can be automated. Adversarial Construction of Contrast Sets Some recent datasets are constructed using baseline models in the data collection process, either to filter out examples that existing models answer correctly (e.g., Dua et al., 2019; Dasigi et al., 2019) or to generate adversarial inputs (e.g., Zellers et al., 2018, 2019; Wallace et al., 2019b; Nie et al., 2019). Unlike this line of work, we choose not to have a model in the loop because this can bias the data to the failures of a particular model (cf. Zellers et al., 2019), rather than generally characterizing the local decision boundary. We do think it is acceptable to use a model on a handful of initial perturbations to understand which phenomena are worth spending time on, but this should be separate from the actual annotation process—observing model outputs while pe"
2020.findings-emnlp.117,D18-1407,1,0.898157,"uniform evaluation of new modeling developments. However, recent work shows a problem with this standard evaluation paradigm based on i.i.d. test sets: datasets often F Matt Gardner led the project. All other authors are listed in alphabetical order. have systematic gaps (such as those due to various kinds of annotator bias) that (unintentionally) allow simple decision rules to perform well on test data (Chen et al., 2016; Gururangan et al., 2018; Geva et al., 2019). This is strikingly evident when models achieve high test accuracy but fail on simple input perturbations (Jia and Liang, 2017; Feng et al., 2018; Ribeiro et al., 2018a), challenge examples (Naik et al., 2018), and covariate and label shifts (Ben-David et al., 2010; Shimodaira, 2000; Lipton et al., 2018). To more accurately evaluate a model’s true capabilities on some task, we must collect data that 1307 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 1307–1323 c November 16 - 20, 2020. 2020 Association for Computational Linguistics fills in these systematic gaps in the test set. To accomplish this, we expand on long-standing ideas of constructing minimally-constrastive examples (e.g. Levesque et al., 2011)"
2020.findings-emnlp.117,D19-1107,1,0.925439,"l language processing (NLP) has long been measured with standard benchmark datasets (e.g., Marcus et al., 1993). These benchmarks help to provide a uniform evaluation of new modeling developments. However, recent work shows a problem with this standard evaluation paradigm based on i.i.d. test sets: datasets often F Matt Gardner led the project. All other authors are listed in alphabetical order. have systematic gaps (such as those due to various kinds of annotator bias) that (unintentionally) allow simple decision rules to perform well on test data (Chen et al., 2016; Gururangan et al., 2018; Geva et al., 2019). This is strikingly evident when models achieve high test accuracy but fail on simple input perturbations (Jia and Liang, 2017; Feng et al., 2018; Ribeiro et al., 2018a), challenge examples (Naik et al., 2018), and covariate and label shifts (Ben-David et al., 2010; Shimodaira, 2000; Lipton et al., 2018). To more accurately evaluate a model’s true capabilities on some task, we must collect data that 1307 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 1307–1323 c November 16 - 20, 2020. 2020 Association for Computational Linguistics fills in these systematic gaps"
2020.findings-emnlp.117,P18-2103,0,0.0419316,"ded minimally contrastive examples on which to train models. Selsam et al. (2019), Tafjord et al. (2019), Lin et al. (2019), and Khashabi et al. (2020) designed their data collection process to include contrastive examples. Data augmentation methods have also been used to mitigate gender (Zhao et al., 2018), racial (Dixon et al., 1314 2018), and other biases (Kaushik et al., 2020) during training, or to introduce useful inductive biases (Andreas, 2020). Challenge Sets The idea of creating challenging contrastive evaluation sets has a long history (Levesque et al., 2011; Ettinger et al., 2017; Glockner et al., 2018; Naik et al., 2018; Isabelle et al., 2017). Challenge sets exist for various phenomena, including ones with “minimal” edits similar to our contrast sets, e.g., in image captioning (Shekhar et al., 2017), machine translation (Sennrich, 2017; Burlot and Yvon, 2017; Burlot et al., 2018), and language modeling (Marvin and Linzen, 2018; Warstadt et al., 2019). Minimal pairs of edits that perturb gender or racial attributes are also useful for evaluating social biases (Rudinger et al., 2018; Zhao et al., 2018; Lu et al., 2018). Our key contribution over this prior work is in grouping perturbed inst"
2020.findings-emnlp.117,N18-2017,1,0.922108,"uction Progress in natural language processing (NLP) has long been measured with standard benchmark datasets (e.g., Marcus et al., 1993). These benchmarks help to provide a uniform evaluation of new modeling developments. However, recent work shows a problem with this standard evaluation paradigm based on i.i.d. test sets: datasets often F Matt Gardner led the project. All other authors are listed in alphabetical order. have systematic gaps (such as those due to various kinds of annotator bias) that (unintentionally) allow simple decision rules to perform well on test data (Chen et al., 2016; Gururangan et al., 2018; Geva et al., 2019). This is strikingly evident when models achieve high test accuracy but fail on simple input perturbations (Jia and Liang, 2017; Feng et al., 2018; Ribeiro et al., 2018a), challenge examples (Naik et al., 2018), and covariate and label shifts (Ben-David et al., 2010; Shimodaira, 2000; Lipton et al., 2018). To more accurately evaluate a model’s true capabilities on some task, we must collect data that 1307 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 1307–1323 c November 16 - 20, 2020. 2020 Association for Computational Linguistics fills in th"
2020.findings-emnlp.117,D19-1170,0,0.0120239,"work to create 1,000 examples. However, some datasets, particularly those with complex output structures, took substantially longer: each example for dependency parsing took an average of 15 minutes (see Appendix B for more details). 4.3 Models Struggle on Contrast Sets For each dataset, we use a model that is at or near state-of-the-art performance. Most models involve fine-tuning a pretrained language model (e.g., ELMo (Peters et al., 2018), BERT (Devlin et al., 2019), RO BERTA (Liu et al., 2019), XLNet (Yang et al., 2019), etc.) or applying a task-specific architecture on top of one (e.g., Hu et al. (2019) add a DROP-specific model on top of BERT). We train each model on the original training set and evaluate it on both the original test set and our contrast sets. Existing models struggle on the contrast sets (Table 2), particularly when evaluating contrast consistency. Model performance degrades differently across datasets; however, note that these numbers are not directly comparable due to differences in dataset size, model architecture, contrast set design, etc. On IMDb and PERSPECTRUM, the model achieves a reasonably high consistency, suggesting that, while there is definitely still room fo"
2020.findings-emnlp.117,D17-1263,0,0.0339764,"h to train models. Selsam et al. (2019), Tafjord et al. (2019), Lin et al. (2019), and Khashabi et al. (2020) designed their data collection process to include contrastive examples. Data augmentation methods have also been used to mitigate gender (Zhao et al., 2018), racial (Dixon et al., 1314 2018), and other biases (Kaushik et al., 2020) during training, or to introduce useful inductive biases (Andreas, 2020). Challenge Sets The idea of creating challenging contrastive evaluation sets has a long history (Levesque et al., 2011; Ettinger et al., 2017; Glockner et al., 2018; Naik et al., 2018; Isabelle et al., 2017). Challenge sets exist for various phenomena, including ones with “minimal” edits similar to our contrast sets, e.g., in image captioning (Shekhar et al., 2017), machine translation (Sennrich, 2017; Burlot and Yvon, 2017; Burlot et al., 2018), and language modeling (Marvin and Linzen, 2018; Warstadt et al., 2019). Minimal pairs of edits that perturb gender or racial attributes are also useful for evaluating social biases (Rudinger et al., 2018; Zhao et al., 2018; Lu et al., 2018). Our key contribution over this prior work is in grouping perturbed instances into a contrast set, for measuring lo"
2020.findings-emnlp.117,D17-1215,0,0.560991,"rks help to provide a uniform evaluation of new modeling developments. However, recent work shows a problem with this standard evaluation paradigm based on i.i.d. test sets: datasets often F Matt Gardner led the project. All other authors are listed in alphabetical order. have systematic gaps (such as those due to various kinds of annotator bias) that (unintentionally) allow simple decision rules to perform well on test data (Chen et al., 2016; Gururangan et al., 2018; Geva et al., 2019). This is strikingly evident when models achieve high test accuracy but fail on simple input perturbations (Jia and Liang, 2017; Feng et al., 2018; Ribeiro et al., 2018a), challenge examples (Naik et al., 2018), and covariate and label shifts (Ben-David et al., 2010; Shimodaira, 2000; Lipton et al., 2018). To more accurately evaluate a model’s true capabilities on some task, we must collect data that 1307 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 1307–1323 c November 16 - 20, 2020. 2020 Association for Computational Linguistics fills in these systematic gaps in the test set. To accomplish this, we expand on long-standing ideas of constructing minimally-constrastive examples (e.g. Lev"
2020.findings-emnlp.117,D19-1423,0,0.0613359,"Missing"
2020.findings-emnlp.117,2020.emnlp-main.12,1,0.714022,"ple and intuitive geometric interpretation of “bias” in dataset collection, and showing that this long-standing idea of minimal data changes can be effectively used to solve this problem on a wide variety of NLP tasks. We additionally generalize the idea of a minimal pair to a set, and use a consistency metric, which we contend more closely aligns with what NLP researchers mean by “language understanding”. Training on Perturbed Examples Many previous works have provided minimally contrastive examples on which to train models. Selsam et al. (2019), Tafjord et al. (2019), Lin et al. (2019), and Khashabi et al. (2020) designed their data collection process to include contrastive examples. Data augmentation methods have also been used to mitigate gender (Zhao et al., 2018), racial (Dixon et al., 1314 2018), and other biases (Kaushik et al., 2020) during training, or to introduce useful inductive biases (Andreas, 2020). Challenge Sets The idea of creating challenging contrastive evaluation sets has a long history (Levesque et al., 2011; Ettinger et al., 2017; Glockner et al., 2018; Naik et al., 2018; Isabelle et al., 2017). Challenge sets exist for various phenomena, including ones with “minimal” edits simil"
2020.findings-emnlp.117,W17-5401,0,0.118381,"Missing"
2020.findings-emnlp.117,P19-1554,1,0.81477,"ctive Power Contrast sets only have negative predictive power: they reveal if a model does not align with the correct local decision boundary but cannot confirm that a model does align with it. This is because annotators cannot exhaustively label all inputs near a pivot and thus a contrast set will necessarily be incomplete. However, note that this problem is not unique to contrast sets—similar issues hold for the original test set as well as adversarial test sets (Jia and Liang, 2017), challenge sets (Naik et al., 2018), and input perturbations (Ribeiro et al., 2018a; Feng et al., 2018). See Feng et al. (2019) for a detailed discussion of how dataset analysis methods only have negative predictive power. Dataset-Specific Instantiations The process for creating contrast sets is dataset-specific: although we present general guidelines that hold across many tasks, experts must still characterize the type of phenomena each individual dataset is intended to capture. Fortunately, the original dataset authors should already have thought deeply about such phenomena. Hence, creating contrast sets should be well-defined and relatively straightforward. 3 How to Create Contrast Sets Here, we walk through our pr"
2020.findings-emnlp.117,D19-5808,1,0.8991,"n the contrast sets (not including the original example). We report percentage accuracy for NLVR2, IMDb, PERSPECTRUM, MATRES, and BoolQ; F1 scores for DROP and Q UOREF; Exact Match (EM) scores for ROPES and MC-TACO; and unlabeled attachment score on modified attachments for the UD English dataset. We also report contrast consistency: the percentage of the “# Sets” contrast sets for which a model’s predictions are correct for all examples in the set (including the original example). More details on datasets, models, and metrics can be found in §A and §B. • Quoref (Dasigi et al., 2019) • ROPES (Lin et al., 2019) • BoolQ (Clark et al., 2019) • MC-TACO (Zhou et al., 2019) We choose these datasets because they span a variety of tasks (e.g., reading comprehension, sentiment analysis, visual reasoning) and input-output formats (e.g., classification, span extraction, structured prediction). We include high-level tasks for which dataset artifacts are known to be prevalent, as well as longstanding formalism-based tasks, where data artifacts have been less of an issue (or at least have been less well-studied). 4.2 Contrast Set Construction The contrast sets were constructed by NLP researchers who were deeply"
2020.findings-emnlp.117,2021.ccl-1.108,0,0.144976,"Missing"
2020.findings-emnlp.117,P11-1015,0,0.07085,"ena they are most interested in studying and craft their contrast sets to explicitly test those phenomena. Care should be taken during contrast set construction to ensure that the phenomena present in contrast sets are similar to those present in the original test set; the purpose of a contrast set is not to introduce new challenges, but to more thoroughly evaluate the original intent of the test set. 4 4.1 Datasets and Experiments Original Datasets We create contrast sets for 10 NLP datasets (full descriptions are provided in Section A): • NLVR2 (Suhr et al., 2019) • IMDb sentiment analysis (Maas et al., 2011) • MATRES Temporal RE (Ning et al., 2018) • English UD parsing (Nivre et al., 2016) • PERSPECTRUM (Chen et al., 2019) • DROP (Dua et al., 2019) 1312 Dataset # Examples # Sets Model Original Test Contrast Consistency NLVR2 994 479 LXMERT 76.4 61.1 (–15.3) 30.1 IMDb 488 488 BERT 93.8 84.2 (–9.6) 77.8 MATRES 401 239 CogCompTime2.0 73.2 63.3 (–9.9) 40.6 UD English 150 150 Biaffine + ELMo 64.7 46.0 (–18.7) 17.3 PERSPECTRUM 217 217 RoBERTa 90.3 85.7 (–4.6) 78.8 DROP 947 623 MTMSN 79.9 54.2 (–25.7) 39.0 QUOREF 700 415 XLNet-QA 70.5 55.4 (–15.1) 29.9 ROPES 974 974 RoBERTa 47.7 32.5 (–15.2) 17.6 BoolQ"
2020.findings-emnlp.117,J93-2004,0,0.068933,"ation: Two similarly-colored and similarly-posed chow dogs are face to face in one image. Figure 1: An example contrast set for NLVR2 (Suhr and Artzi, 2019). The label for the original example is T RUE and the label for all of the perturbed examples is FALSE. The contrast set allows probing of a model’s decision boundary local to examples in the test set, which better evaluates whether the model has captured the relevant phenomena than standard metrics on i.i.d. test data. Introduction Progress in natural language processing (NLP) has long been measured with standard benchmark datasets (e.g., Marcus et al., 1993). These benchmarks help to provide a uniform evaluation of new modeling developments. However, recent work shows a problem with this standard evaluation paradigm based on i.i.d. test sets: datasets often F Matt Gardner led the project. All other authors are listed in alphabetical order. have systematic gaps (such as those due to various kinds of annotator bias) that (unintentionally) allow simple decision rules to perform well on test data (Chen et al., 2016; Gururangan et al., 2018; Geva et al., 2019). This is strikingly evident when models achieve high test accuracy but fail on simple input"
2020.findings-emnlp.117,D18-1151,0,0.0148944,"1314 2018), and other biases (Kaushik et al., 2020) during training, or to introduce useful inductive biases (Andreas, 2020). Challenge Sets The idea of creating challenging contrastive evaluation sets has a long history (Levesque et al., 2011; Ettinger et al., 2017; Glockner et al., 2018; Naik et al., 2018; Isabelle et al., 2017). Challenge sets exist for various phenomena, including ones with “minimal” edits similar to our contrast sets, e.g., in image captioning (Shekhar et al., 2017), machine translation (Sennrich, 2017; Burlot and Yvon, 2017; Burlot et al., 2018), and language modeling (Marvin and Linzen, 2018; Warstadt et al., 2019). Minimal pairs of edits that perturb gender or racial attributes are also useful for evaluating social biases (Rudinger et al., 2018; Zhao et al., 2018; Lu et al., 2018). Our key contribution over this prior work is in grouping perturbed instances into a contrast set, for measuring local alignment of decision boundaries, along with our new, related resources. Additionally, rather than creating new data from scratch, contrast sets augment existing test examples to fill in systematic gaps. Thus contrast sets often require less effort to create, and they remain grounded i"
2020.findings-emnlp.117,N19-1314,0,0.0179195,"the task definition than a random selection of input / output pairs. 2.3 Contrast sets in practice Given these definitions, we now turn to the actual construction of contrast sets in practical NLP settings. There were two things left unspecified in the definitions above: the distance function d to use in discrete input spaces, and the method for sampling from a local decision boundary. While there has been some work trying to formally characterize dis2 In this discussion we are talking about the true decision boundary, not a model’s decision boundary. tances for adversarial robustness in NLP (Michel et al., 2019; Jia et al., 2019), we find it more useful in our setting to simply rely on expert judgments to generate a similar but meaningfully different x0 given x, addressing both the distance function and the sampling method. Future work could try to give formal treatments of these issues, but we believe expert judgments are sufficient to make initial progress in improving our evaluation methodologies. And while expertcrafted contrast sets can only give us an upper bound on a model’s local alignment with the true decision boundary, an upper bound on local alignment is often more informative than a pot"
2020.findings-emnlp.117,P19-1416,1,0.832263,"dissolute alcoholic. Question: How frequently does Tom drink? Candidate Answer: Every other night Label: Likely Context: She renews in Ranchipur an acquaintance with a former lover, Tom Ransome, who keeps very healthy habits. Question: How frequently does Tom drink? Candidate Answer: Every other night Label: Unlikely Table 1: We create contrast sets for 10 datasets and show instances from seven of them here. guage. We do not expect intuition, even of experts, to exhaustively reveal gaps. Nevertheless, the presence of these gaps is welldocumented (Gururangan et al., 2018; Poliak et al., 2018; Min et al., 2019), and Niven and Kao (2019) give an initial attempt at formally characterizing them. In particular, one common source is annotator bias from data collection processes (Geva et al., 2019). For example, in the SNLI dataset (Bowman et al., 2015), Gururangan et al. (2018) show that the words sleeping, tv, and cat almost never appear in an entailment example, either in the training set or the test set, though they often appear in contradiction examples. This is not because these words are particularly important to the phenomenon of entailment; their absence in entailment examples is a systematic gap"
2020.findings-emnlp.117,C18-1198,0,0.264897,"nt work shows a problem with this standard evaluation paradigm based on i.i.d. test sets: datasets often F Matt Gardner led the project. All other authors are listed in alphabetical order. have systematic gaps (such as those due to various kinds of annotator bias) that (unintentionally) allow simple decision rules to perform well on test data (Chen et al., 2016; Gururangan et al., 2018; Geva et al., 2019). This is strikingly evident when models achieve high test accuracy but fail on simple input perturbations (Jia and Liang, 2017; Feng et al., 2018; Ribeiro et al., 2018a), challenge examples (Naik et al., 2018), and covariate and label shifts (Ben-David et al., 2010; Shimodaira, 2000; Lipton et al., 2018). To more accurately evaluate a model’s true capabilities on some task, we must collect data that 1307 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 1307–1323 c November 16 - 20, 2020. 2020 Association for Computational Linguistics fills in these systematic gaps in the test set. To accomplish this, we expand on long-standing ideas of constructing minimally-constrastive examples (e.g. Levesque et al., 2011). We propose that dataset authors manually perturb instances fro"
2020.findings-emnlp.117,D19-1642,1,0.874527,"Missing"
2020.findings-emnlp.117,P18-1122,1,0.846317,"and craft their contrast sets to explicitly test those phenomena. Care should be taken during contrast set construction to ensure that the phenomena present in contrast sets are similar to those present in the original test set; the purpose of a contrast set is not to introduce new challenges, but to more thoroughly evaluate the original intent of the test set. 4 4.1 Datasets and Experiments Original Datasets We create contrast sets for 10 NLP datasets (full descriptions are provided in Section A): • NLVR2 (Suhr et al., 2019) • IMDb sentiment analysis (Maas et al., 2011) • MATRES Temporal RE (Ning et al., 2018) • English UD parsing (Nivre et al., 2016) • PERSPECTRUM (Chen et al., 2019) • DROP (Dua et al., 2019) 1312 Dataset # Examples # Sets Model Original Test Contrast Consistency NLVR2 994 479 LXMERT 76.4 61.1 (–15.3) 30.1 IMDb 488 488 BERT 93.8 84.2 (–9.6) 77.8 MATRES 401 239 CogCompTime2.0 73.2 63.3 (–9.9) 40.6 UD English 150 150 Biaffine + ELMo 64.7 46.0 (–18.7) 17.3 PERSPECTRUM 217 217 RoBERTa 90.3 85.7 (–4.6) 78.8 DROP 947 623 MTMSN 79.9 54.2 (–25.7) 39.0 QUOREF 700 415 XLNet-QA 70.5 55.4 (–15.1) 29.9 ROPES 974 974 RoBERTa 47.7 32.5 (–15.2) 17.6 BoolQ 339 70 RoBERTa 86.1 71.1 (–15.0) 59.0 MC-"
2020.findings-emnlp.117,P19-1459,0,0.0145999,"uestion: How frequently does Tom drink? Candidate Answer: Every other night Label: Likely Context: She renews in Ranchipur an acquaintance with a former lover, Tom Ransome, who keeps very healthy habits. Question: How frequently does Tom drink? Candidate Answer: Every other night Label: Unlikely Table 1: We create contrast sets for 10 datasets and show instances from seven of them here. guage. We do not expect intuition, even of experts, to exhaustively reveal gaps. Nevertheless, the presence of these gaps is welldocumented (Gururangan et al., 2018; Poliak et al., 2018; Min et al., 2019), and Niven and Kao (2019) give an initial attempt at formally characterizing them. In particular, one common source is annotator bias from data collection processes (Geva et al., 2019). For example, in the SNLI dataset (Bowman et al., 2015), Gururangan et al. (2018) show that the words sleeping, tv, and cat almost never appear in an entailment example, either in the training set or the test set, though they often appear in contradiction examples. This is not because these words are particularly important to the phenomenon of entailment; their absence in entailment examples is a systematic gap in the data that can be e"
2020.findings-emnlp.117,N18-1202,1,0.523856,"es from the different contrast sets in Table 1. For most datasets, the average time to perturb each example was 1–3 minutes, which translates to approximately 17–50 hours of work to create 1,000 examples. However, some datasets, particularly those with complex output structures, took substantially longer: each example for dependency parsing took an average of 15 minutes (see Appendix B for more details). 4.3 Models Struggle on Contrast Sets For each dataset, we use a model that is at or near state-of-the-art performance. Most models involve fine-tuning a pretrained language model (e.g., ELMo (Peters et al., 2018), BERT (Devlin et al., 2019), RO BERTA (Liu et al., 2019), XLNet (Yang et al., 2019), etc.) or applying a task-specific architecture on top of one (e.g., Hu et al. (2019) add a DROP-specific model on top of BERT). We train each model on the original training set and evaluate it on both the original test set and our contrast sets. Existing models struggle on the contrast sets (Table 2), particularly when evaluating contrast consistency. Model performance degrades differently across datasets; however, note that these numbers are not directly comparable due to differences in dataset size, model a"
2020.findings-emnlp.117,S18-2023,0,0.0681783,"Missing"
2020.findings-emnlp.117,P19-1621,1,0.860914,"Missing"
2020.findings-emnlp.117,P18-1079,1,0.679043,"of new modeling developments. However, recent work shows a problem with this standard evaluation paradigm based on i.i.d. test sets: datasets often F Matt Gardner led the project. All other authors are listed in alphabetical order. have systematic gaps (such as those due to various kinds of annotator bias) that (unintentionally) allow simple decision rules to perform well on test data (Chen et al., 2016; Gururangan et al., 2018; Geva et al., 2019). This is strikingly evident when models achieve high test accuracy but fail on simple input perturbations (Jia and Liang, 2017; Feng et al., 2018; Ribeiro et al., 2018a), challenge examples (Naik et al., 2018), and covariate and label shifts (Ben-David et al., 2010; Shimodaira, 2000; Lipton et al., 2018). To more accurately evaluate a model’s true capabilities on some task, we must collect data that 1307 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 1307–1323 c November 16 - 20, 2020. 2020 Association for Computational Linguistics fills in these systematic gaps in the test set. To accomplish this, we expand on long-standing ideas of constructing minimally-constrastive examples (e.g. Levesque et al., 2011). We propose that data"
2020.findings-emnlp.117,N18-2002,0,0.0515743,"Missing"
2020.findings-emnlp.117,E17-2060,0,0.0150328,"ds have also been used to mitigate gender (Zhao et al., 2018), racial (Dixon et al., 1314 2018), and other biases (Kaushik et al., 2020) during training, or to introduce useful inductive biases (Andreas, 2020). Challenge Sets The idea of creating challenging contrastive evaluation sets has a long history (Levesque et al., 2011; Ettinger et al., 2017; Glockner et al., 2018; Naik et al., 2018; Isabelle et al., 2017). Challenge sets exist for various phenomena, including ones with “minimal” edits similar to our contrast sets, e.g., in image captioning (Shekhar et al., 2017), machine translation (Sennrich, 2017; Burlot and Yvon, 2017; Burlot et al., 2018), and language modeling (Marvin and Linzen, 2018; Warstadt et al., 2019). Minimal pairs of edits that perturb gender or racial attributes are also useful for evaluating social biases (Rudinger et al., 2018; Zhao et al., 2018; Lu et al., 2018). Our key contribution over this prior work is in grouping perturbed instances into a contrast set, for measuring local alignment of decision boundaries, along with our new, related resources. Additionally, rather than creating new data from scratch, contrast sets augment existing test examples to fill in system"
2020.findings-emnlp.117,2020.acl-main.468,0,0.0236124,"ing set or the test set, though they often appear in contradiction examples. This is not because these words are particularly important to the phenomenon of entailment; their absence in entailment examples is a systematic gap in the data that can be exploited by models to achieve artificially high test accuracy. This is but one kind of systematic gap; there are also biases due to the writing styles of small groups of annotators (Geva et al., 2019), the distributional biases in the data that was chosen for annotation, as well as numerous other biases that are more subtle and harder to discern (Shah et al., 2020). Completely removing these gaps in the initial data collection process would be ideal, but is likely impossible—language has too much inherent variability in a very high-dimensional space. Instead, we use contrast sets to fill in gaps in the test data to give more thorough evaluations than what the original data provides. 1309 2.2 Definitions We begin by defining a decision boundary as a partition of some space into labels.2 This partition can be represented by the set of all points in the space with their associated labels: {(x, y)}. This definition differs somewhat from the canonical defini"
2020.findings-emnlp.117,silveira-etal-2014-gold,0,0.0413118,"Missing"
2020.findings-emnlp.117,P19-1644,1,0.918073,"hanging questions asking for counts to questions asking for sets (How many countries. . . to Which countries. . . ). Finally, we changed the ordering of events. A large number of questions about war paragraphs ask which of two events happened first. We changed (1) the order the events were asked about in the question, (2) the order that the events showed up in the passage, and (3) the dates associated with each event to swap their temporal order. NLVR2 We next consider NLVR2, a dataset where a model is given a sentence about two provided images and must determine whether the sentence is true (Suhr et al., 2019). The data collection process encouraged highly compositional language, which was intended to require understanding the relationships between objects, properties of objects, and counting. We constructed NLVR2 contrast sets by modifying the sentence or replacing one of the images with freely-licensed images from web searches. For example, we might change The left image contains twice the number of dogs as the right image to The left image contains three times the number of dogs as the right image. Similarly, given an image pair with four dogs in the left and two dogs in the right, we can replac"
2020.findings-emnlp.117,D19-1608,1,0.822267,"esources that we have created, is giving a simple and intuitive geometric interpretation of “bias” in dataset collection, and showing that this long-standing idea of minimal data changes can be effectively used to solve this problem on a wide variety of NLP tasks. We additionally generalize the idea of a minimal pair to a set, and use a consistency metric, which we contend more closely aligns with what NLP researchers mean by “language understanding”. Training on Perturbed Examples Many previous works have provided minimally contrastive examples on which to train models. Selsam et al. (2019), Tafjord et al. (2019), Lin et al. (2019), and Khashabi et al. (2020) designed their data collection process to include contrastive examples. Data augmentation methods have also been used to mitigate gender (Zhao et al., 2018), racial (Dixon et al., 1314 2018), and other biases (Kaushik et al., 2020) during training, or to introduce useful inductive biases (Andreas, 2020). Challenge Sets The idea of creating challenging contrastive evaluation sets has a long history (Levesque et al., 2011; Ettinger et al., 2017; Glockner et al., 2018; Naik et al., 2018; Isabelle et al., 2017). Challenge sets exist for various pheno"
2020.findings-emnlp.117,D18-1009,0,0.0260643,"is very challenging to come up with rules or other automated methods for pushing pivots across a decision boundary—in most cases this presupposes a model that can already perform the intended task. We recommend annotators spend their time constructing these types of examples; easier examples can be automated. Adversarial Construction of Contrast Sets Some recent datasets are constructed using baseline models in the data collection process, either to filter out examples that existing models answer correctly (e.g., Dua et al., 2019; Dasigi et al., 2019) or to generate adversarial inputs (e.g., Zellers et al., 2018, 2019; Wallace et al., 2019b; Nie et al., 2019). Unlike this line of work, we choose not to have a model in the loop because this can bias the data to the failures of a particular model (cf. Zellers et al., 2019), rather than generally characterizing the local decision boundary. We do think it is acceptable to use a model on a handful of initial perturbations to understand which phenomena are worth spending time on, but this should be separate from the actual annotation process—observing model outputs while perturbing data creates subtle, undesirable biases towards the idiosyncrasies of that"
2020.findings-emnlp.117,P19-1472,0,0.0134417,"annotators spend their time constructing these types of examples; easier examples can be automated. Adversarial Construction of Contrast Sets Some recent datasets are constructed using baseline models in the data collection process, either to filter out examples that existing models answer correctly (e.g., Dua et al., 2019; Dasigi et al., 2019) or to generate adversarial inputs (e.g., Zellers et al., 2018, 2019; Wallace et al., 2019b; Nie et al., 2019). Unlike this line of work, we choose not to have a model in the loop because this can bias the data to the failures of a particular model (cf. Zellers et al., 2019), rather than generally characterizing the local decision boundary. We do think it is acceptable to use a model on a handful of initial perturbations to understand which phenomena are worth spending time on, but this should be separate from the actual annotation process—observing model outputs while perturbing data creates subtle, undesirable biases towards the idiosyncrasies of that model. 2.5 Limitations of Contrast Sets Solely Negative Predictive Power Contrast sets only have negative predictive power: they reveal if a model does not align with the correct local decision boundary but cannot"
2020.findings-emnlp.118,W19-1909,0,0.0492153,"Missing"
2020.findings-emnlp.118,2020.acl-main.421,0,0.339686,"monolingual models have been effectively adapted to specialized domains through additional pretraining on domain-specific corpora (Gururangan et al., 2020). We hypothesize that we can improve the performance of multilingual models on low-resource language varieties analogously, through additional pretraining on language-specific corpora. However, additional pretraining on more data in the target language does not ensure its full representation in the model’s vocabulary, which is constructed to maximally represent the model’s original pretraining data (Sennrich et al., 2016; Wu et al., 2016). Artetxe et al. (2020) find that target languages’ representation in the vocabulary affects these models’ transferability, suggesting that language varieties on the fringes of the vocabulary 1324 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 1324–1334 c November 16 - 20, 2020. 2020 Association for Computational Linguistics may not be sufficiently well-modeled. Can we incorporate vocabulary from the target language into multilingual models’ existing alignment? We introduce the use of additional languagespecific pretraining for multilingual CWRs in a low-resource setting, before use in"
2020.findings-emnlp.118,D19-1371,0,0.0182949,"ations at each M B ERT layer as the CWR for each token. We then pass the representations into the graph-based dependency parser of Dozat and Manning (2017). This parser, which is also used in related work (Kondratyuk and Straka, 2019; Mulcaire et al., 2019a; Schuster et al., 2019), uses a biaffine attention mechanism between word representations to score a parse tree. 3 Experiments We consider two variants of each M B ERT method: one in which the pretrained CWRs are frozen; and one where they are further finetuned during parser training (FT). Following prior work involving these two variants (Beltagy et al., 2019), FT variants perform biaffine attention directly on the outputs of M B ERT instead of first passing them through a BiLSTM, as in Dozat and Manning (2017). We perform additional pretraining for up to 20 epochs, selecting our final models based on average validation LAS downstream. Full training details are given in the Appendix. We report average scores and standard errors based on five random initializations. Code and data are publicly available (see footnote 2). 3.1 Languages and Datasets We perform experiments on four typologically diverse low-resource languages: Irish (GA), Maltese (MT), V"
2020.findings-emnlp.118,Q17-1010,0,0.179989,"Missing"
2020.findings-emnlp.118,N19-1054,0,0.0112498,"ity and parameter sharing. Artetxe et al. (2020) emphasize the importance of sufficiently representing the target language in the vocabulary. Unlike these studies, we primarily consider how to improve the performance of multilingual models for a given target language variety. Though our experiments do not directly probe the impact of vocabulary overlap, we contribute further evaluation of the importance of improved modeling of the target variety. Recent work has also proposed additional pretraining for general-purpose language models, especially with respect to domain (Alsentzer et al., 2019; Chakrabarty et al., 2019; Gururangan et al., 2020; Han and Eisenstein, 2019; Howard and Ruder, 2018; Logeswaran et al., 2019; Sun et al., 2019). Lakew et al. (2018) and Zoph et al. (2016) perform additional training on parallel data to adapt bilingual translation models to unseen target languages, while Mueller et al. (2020) improve a polyglot task-specific model by finetuning on labeled monolingual data in the target variety. To the best of our knowledge, our work is the first to demonstrate the effectiveness of additional pretraining for massively multilingual language models toward a target low-resource language v"
2020.findings-emnlp.118,2020.acl-main.747,0,0.114903,"Missing"
2020.findings-emnlp.118,2020.acl-main.536,0,0.332561,"rce language varieties is to finetune a large, multilingual language model that has been pretrained on the union of many languages’ data (Devlin et al., 2019; Lample 1 Sociolinguists define “language varieties” broadly to encompass any distinct form of a language. In addition to standard varieties (conventionally referred to as “languages”), this includes dialects, registers, and styles (Trudgill, 2003). and Conneau, 2019). This enables the model to transfer some of what it learns from high-resource languages to low-resource ones, demonstrating benefits over monolingual methods in some cases (Conneau et al., 2020a; Tsai et al., 2019), though not always (Agerri et al., 2020; R¨onnqvist et al., 2019). Specifically, multilingual models face the transfer-dilution tradeoff (Conneau et al., 2020a): increasing the number of languages during pretraining improves positive crosslingual transfer but decreases the model capacity allocated to each language. Furthermore, such models are only pretrained on a finite amount of data and may lack exposure to specialized domains of certain languages or even entire low-resource language varieties. The result is a challenge for these language varieties, which must rely on"
2020.findings-emnlp.118,N19-1423,0,0.15129,"tional language-specific pretraining and vocabulary augmentation to adapt multilingual models to low-resource settings. Using dependency parsing of four diverse lowresource language varieties as a case study, we show that these methods significantly improve performance over baselines, especially in the lowest-resource cases, and demonstrate the importance of the relationship between such models’ pretraining data and target language varieties. 1 Introduction Contextual word representations (CWRs) from pretrained language models have improved many NLP systems. Such language models include BERT (Devlin et al., 2019) and ELMo (Peters et al., 2018), which are conventionally “pretrained” on large unlabeled datasets before their internal representations are “finetuned” during supervised training on downstream tasks like parsing. However, many language varieties1 lack large annotated and even unannotated datasets, raising questions about the broad applicability of such data-hungry methods. One exciting way to compensate for the lack of unlabeled data in low-resource language varieties is to finetune a large, multilingual language model that has been pretrained on the union of many languages’ data (Devlin et a"
2020.findings-emnlp.118,D19-1224,1,0.879051,"Missing"
2020.findings-emnlp.118,W18-2501,0,0.0424412,"Missing"
2020.findings-emnlp.118,2020.acl-main.740,1,0.885825,"Missing"
2020.findings-emnlp.118,D19-1433,0,0.0536625,"phasize the importance of sufficiently representing the target language in the vocabulary. Unlike these studies, we primarily consider how to improve the performance of multilingual models for a given target language variety. Though our experiments do not directly probe the impact of vocabulary overlap, we contribute further evaluation of the importance of improved modeling of the target variety. Recent work has also proposed additional pretraining for general-purpose language models, especially with respect to domain (Alsentzer et al., 2019; Chakrabarty et al., 2019; Gururangan et al., 2020; Han and Eisenstein, 2019; Howard and Ruder, 2018; Logeswaran et al., 2019; Sun et al., 2019). Lakew et al. (2018) and Zoph et al. (2016) perform additional training on parallel data to adapt bilingual translation models to unseen target languages, while Mueller et al. (2020) improve a polyglot task-specific model by finetuning on labeled monolingual data in the target variety. To the best of our knowledge, our work is the first to demonstrate the effectiveness of additional pretraining for massively multilingual language models toward a target low-resource language variety, using only unlabeled data in the target var"
2020.findings-emnlp.118,P18-1031,0,0.0290048,"sufficiently representing the target language in the vocabulary. Unlike these studies, we primarily consider how to improve the performance of multilingual models for a given target language variety. Though our experiments do not directly probe the impact of vocabulary overlap, we contribute further evaluation of the importance of improved modeling of the target variety. Recent work has also proposed additional pretraining for general-purpose language models, especially with respect to domain (Alsentzer et al., 2019; Chakrabarty et al., 2019; Gururangan et al., 2020; Han and Eisenstein, 2019; Howard and Ruder, 2018; Logeswaran et al., 2019; Sun et al., 2019). Lakew et al. (2018) and Zoph et al. (2016) perform additional training on parallel data to adapt bilingual translation models to unseen target languages, while Mueller et al. (2020) improve a polyglot task-specific model by finetuning on labeled monolingual data in the target variety. To the best of our knowledge, our work is the first to demonstrate the effectiveness of additional pretraining for massively multilingual language models toward a target low-resource language variety, using only unlabeled data in the target variety. 6 Conclusion We ex"
2020.findings-emnlp.118,D19-1279,0,0.096388,"many more low-resource language varieties than those of other NLP tasks—are expected to interest researchers and practitioners facing low-resource situations for other tasks. To this end, we make our code, data, and hyperparameters publicly available.2 2 Overview We are chiefly concerned with the adaptation of pretrained multilingual models to a target language by optimally using available data. As a case study, we use the multilingual cased BERT model (M B ERT) of Devlin et al. (2019), a transformerbased (Vaswani et al., 2017) language model which has produced strong CWRs for many languages (Kondratyuk and Straka, 2019, inter alia). M B ERT is pretrained on the 104 languages with the most Wikipedia data and encodes input tokens using a fixed wordpiece vocabulary (Wu et al., 2016) learned from this data. Low-resource languages are slightly oversampled in its pretraining data, but high resource languages are still more prevalent, resulting in a language imbalance.3 2 https://github.com/ethch18/ parsing-mbert 3 Sampling is done based on an exponentially smoothed distribution of the amount of data in each language, which slightly increases the representation of low-resource languages. See https://github.com/goo"
2020.findings-emnlp.118,2021.ccl-1.108,0,0.149846,"Missing"
2020.findings-emnlp.118,P19-1335,0,0.0119187,"g the target language in the vocabulary. Unlike these studies, we primarily consider how to improve the performance of multilingual models for a given target language variety. Though our experiments do not directly probe the impact of vocabulary overlap, we contribute further evaluation of the importance of improved modeling of the target variety. Recent work has also proposed additional pretraining for general-purpose language models, especially with respect to domain (Alsentzer et al., 2019; Chakrabarty et al., 2019; Gururangan et al., 2020; Han and Eisenstein, 2019; Howard and Ruder, 2018; Logeswaran et al., 2019; Sun et al., 2019). Lakew et al. (2018) and Zoph et al. (2016) perform additional training on parallel data to adapt bilingual translation models to unseen target languages, while Mueller et al. (2020) improve a polyglot task-specific model by finetuning on labeled monolingual data in the target variety. To the best of our knowledge, our work is the first to demonstrate the effectiveness of additional pretraining for massively multilingual language models toward a target low-resource language variety, using only unlabeled data in the target variety. 6 Conclusion We explore additional language"
2020.findings-emnlp.118,2020.acl-main.720,0,0.0197557,"t directly probe the impact of vocabulary overlap, we contribute further evaluation of the importance of improved modeling of the target variety. Recent work has also proposed additional pretraining for general-purpose language models, especially with respect to domain (Alsentzer et al., 2019; Chakrabarty et al., 2019; Gururangan et al., 2020; Han and Eisenstein, 2019; Howard and Ruder, 2018; Logeswaran et al., 2019; Sun et al., 2019). Lakew et al. (2018) and Zoph et al. (2016) perform additional training on parallel data to adapt bilingual translation models to unseen target languages, while Mueller et al. (2020) improve a polyglot task-specific model by finetuning on labeled monolingual data in the target variety. To the best of our knowledge, our work is the first to demonstrate the effectiveness of additional pretraining for massively multilingual language models toward a target low-resource language variety, using only unlabeled data in the target variety. 6 Conclusion We explore additional language-specific pretraining and vocabulary augmentation for multilingual contextual word representations in low-resource settings and find them to be effective for dependency parsing, especially in the lowest"
2020.findings-emnlp.118,K19-1029,1,0.819124,"beddings of the 99 new wordpieces than for the other parameters. We expect this method to learn the embeddings more thoroughly without overfitting the model’s remaining parameters. Learning rate details are given in the Appendix. 2.2 Evaluation We perform evaluation on dependency parsing. Following Kondratyuk and Straka (2019), we take a weighted sum of the activations at each M B ERT layer as the CWR for each token. We then pass the representations into the graph-based dependency parser of Dozat and Manning (2017). This parser, which is also used in related work (Kondratyuk and Straka, 2019; Mulcaire et al., 2019a; Schuster et al., 2019), uses a biaffine attention mechanism between word representations to score a parse tree. 3 Experiments We consider two variants of each M B ERT method: one in which the pretrained CWRs are frozen; and one where they are further finetuned during parser training (FT). Following prior work involving these two variants (Beltagy et al., 2019), FT variants perform biaffine attention directly on the outputs of M B ERT instead of first passing them through a BiLSTM, as in Dozat and Manning (2017). We perform additional pretraining for up to 20 epochs, selecting our final mode"
2020.findings-emnlp.118,N19-1392,1,0.792316,"beddings of the 99 new wordpieces than for the other parameters. We expect this method to learn the embeddings more thoroughly without overfitting the model’s remaining parameters. Learning rate details are given in the Appendix. 2.2 Evaluation We perform evaluation on dependency parsing. Following Kondratyuk and Straka (2019), we take a weighted sum of the activations at each M B ERT layer as the CWR for each token. We then pass the representations into the graph-based dependency parser of Dozat and Manning (2017). This parser, which is also used in related work (Kondratyuk and Straka, 2019; Mulcaire et al., 2019a; Schuster et al., 2019), uses a biaffine attention mechanism between word representations to score a parse tree. 3 Experiments We consider two variants of each M B ERT method: one in which the pretrained CWRs are frozen; and one where they are further finetuned during parser training (FT). Following prior work involving these two variants (Beltagy et al., 2019), FT variants perform biaffine attention directly on the outputs of M B ERT instead of first passing them through a BiLSTM, as in Dozat and Manning (2017). We perform additional pretraining for up to 20 epochs, selecting our final mode"
2020.findings-emnlp.118,N18-1202,0,0.0711141,"ining and vocabulary augmentation to adapt multilingual models to low-resource settings. Using dependency parsing of four diverse lowresource language varieties as a case study, we show that these methods significantly improve performance over baselines, especially in the lowest-resource cases, and demonstrate the importance of the relationship between such models’ pretraining data and target language varieties. 1 Introduction Contextual word representations (CWRs) from pretrained language models have improved many NLP systems. Such language models include BERT (Devlin et al., 2019) and ELMo (Peters et al., 2018), which are conventionally “pretrained” on large unlabeled datasets before their internal representations are “finetuned” during supervised training on downstream tasks like parsing. However, many language varieties1 lack large annotated and even unannotated datasets, raising questions about the broad applicability of such data-hungry methods. One exciting way to compensate for the lack of unlabeled data in low-resource language varieties is to finetune a large, multilingual language model that has been pretrained on the union of many languages’ data (Devlin et al., 2019; Lample 1 Sociolinguis"
2020.findings-emnlp.118,P19-1493,0,0.0270547,"roduce additional improvement for languages with greater proportions of unseen wordpieces. Overall, our findings are promising for lowresource language varieties, demonstrating that large improvements in performance are possible with the help of a little unlabeled data, and that the performance discrepancy of multilingual models for low-resource languages (Wu and Dredze, 2020) can be overcome. 5 Further Related Work Our work builds on prior empirical studies on multilingual models, which probe the behavior and components of existing models to explain why they are effective. Cao et al. (2020), Pires et al. (2019), and Wu and Dredze (2019) note the importance of both vocabulary overlap and the relationship between languages in determining the effectiveness of multilingual models, but they primarily consider high-resource languages. On the other hand, Conneau et al. (2020b) and K et al. (2020) find vocabulary overlap to be less significant of a factor, instead attributing such models’ successes to typological similarity and parameter sharing. Artetxe et al. (2020) emphasize the importance of sufficiently representing the target language in the vocabulary. Unlike these studies, we primarily consider how"
2020.findings-emnlp.118,W19-6204,0,0.027685,"Missing"
2020.findings-emnlp.118,N19-1162,0,0.0224436,"ordpieces than for the other parameters. We expect this method to learn the embeddings more thoroughly without overfitting the model’s remaining parameters. Learning rate details are given in the Appendix. 2.2 Evaluation We perform evaluation on dependency parsing. Following Kondratyuk and Straka (2019), we take a weighted sum of the activations at each M B ERT layer as the CWR for each token. We then pass the representations into the graph-based dependency parser of Dozat and Manning (2017). This parser, which is also used in related work (Kondratyuk and Straka, 2019; Mulcaire et al., 2019a; Schuster et al., 2019), uses a biaffine attention mechanism between word representations to score a parse tree. 3 Experiments We consider two variants of each M B ERT method: one in which the pretrained CWRs are frozen; and one where they are further finetuned during parser training (FT). Following prior work involving these two variants (Beltagy et al., 2019), FT variants perform biaffine attention directly on the outputs of M B ERT instead of first passing them through a BiLSTM, as in Dozat and Manning (2017). We perform additional pretraining for up to 20 epochs, selecting our final models based on average valid"
2020.findings-emnlp.118,P16-1162,0,0.0612532,"domain adaptation, where general-purpose monolingual models have been effectively adapted to specialized domains through additional pretraining on domain-specific corpora (Gururangan et al., 2020). We hypothesize that we can improve the performance of multilingual models on low-resource language varieties analogously, through additional pretraining on language-specific corpora. However, additional pretraining on more data in the target language does not ensure its full representation in the model’s vocabulary, which is constructed to maximally represent the model’s original pretraining data (Sennrich et al., 2016; Wu et al., 2016). Artetxe et al. (2020) find that target languages’ representation in the vocabulary affects these models’ transferability, suggesting that language varieties on the fringes of the vocabulary 1324 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 1324–1334 c November 16 - 20, 2020. 2020 Association for Computational Linguistics may not be sufficiently well-modeled. Can we incorporate vocabulary from the target language into multilingual models’ existing alignment? We introduce the use of additional languagespecific pretraining for multilingual CWRs"
2020.findings-emnlp.118,D19-1374,0,0.0195213,"is to finetune a large, multilingual language model that has been pretrained on the union of many languages’ data (Devlin et al., 2019; Lample 1 Sociolinguists define “language varieties” broadly to encompass any distinct form of a language. In addition to standard varieties (conventionally referred to as “languages”), this includes dialects, registers, and styles (Trudgill, 2003). and Conneau, 2019). This enables the model to transfer some of what it learns from high-resource languages to low-resource ones, demonstrating benefits over monolingual methods in some cases (Conneau et al., 2020a; Tsai et al., 2019), though not always (Agerri et al., 2020; R¨onnqvist et al., 2019). Specifically, multilingual models face the transfer-dilution tradeoff (Conneau et al., 2020a): increasing the number of languages during pretraining improves positive crosslingual transfer but decreases the model capacity allocated to each language. Furthermore, such models are only pretrained on a finite amount of data and may lack exposure to specialized domains of certain languages or even entire low-resource language varieties. The result is a challenge for these language varieties, which must rely on positive transfer fro"
2020.findings-emnlp.118,D16-1163,0,0.0284279,"imarily consider how to improve the performance of multilingual models for a given target language variety. Though our experiments do not directly probe the impact of vocabulary overlap, we contribute further evaluation of the importance of improved modeling of the target variety. Recent work has also proposed additional pretraining for general-purpose language models, especially with respect to domain (Alsentzer et al., 2019; Chakrabarty et al., 2019; Gururangan et al., 2020; Han and Eisenstein, 2019; Howard and Ruder, 2018; Logeswaran et al., 2019; Sun et al., 2019). Lakew et al. (2018) and Zoph et al. (2016) perform additional training on parallel data to adapt bilingual translation models to unseen target languages, while Mueller et al. (2020) improve a polyglot task-specific model by finetuning on labeled monolingual data in the target variety. To the best of our knowledge, our work is the first to demonstrate the effectiveness of additional pretraining for massively multilingual language models toward a target low-resource language variety, using only unlabeled data in the target variety. 6 Conclusion We explore additional language-specific pretraining and vocabulary augmentation for multiling"
2020.findings-emnlp.253,2020.acl-main.656,0,0.0281686,"uistic explanations that are often provided as localized visual highlights on the image. The latter, while pertinent to what the vision component of the model was attending to, cannot provide the full scope of rationales for such complex reasoning tasks as illustrated in Figure 1. Indeed, explanations for higher-level conceptual reasoning can be best conveyed through natural language, as has been studied in recent literature on (visual) NLI (Do et al., 2020; Camburu et al., 2018), (visual) QA (Wu and Mooney, 2019; Rajani et al., 2019), playing arcade games (Ehsan et al., 2019), fact checking (Atanasova et al., 2020), image classification (Hendricks et al., 2018), motivation prediction (Vondrick et al., 2016), and self-driving cars (Kim et al., 2018). In this paper, we present the first focused study on generating natural language rationales across several complex visual reasoning tasks: visual commonsense reasoning, visual-textual entailment, and visual question answering. Our study aims to complement the more broadly studied lower-level explanations such as attention weights and gradients in deep neural networks (Simonyan et al., 2014; Zhang et al., 2017; Montavon et al., 2018, among others). Because fr"
2020.findings-emnlp.253,D15-1075,0,0.105543,"Missing"
2020.findings-emnlp.253,2020.findings-emnlp.301,1,0.825493,"Missing"
2020.findings-emnlp.253,N18-2017,1,0.893736,"Missing"
2020.findings-emnlp.253,K19-1079,0,0.0239016,"understanding. 2.1 Background: Conditional Text Generation The GPT-2’s backbone architecture can be described as the decoder-only Transformer (Vaswani et al., 2017) which is pretrained with the conventional language modeling (LM) likelihood objective.3 This makes it more suitable for generation tasks compared to models trained with the masked LM objective (BERT; Devlin et al., 2019).4 We build on pretrained LMs because their capabilities make free-text rationalization of complex reasoning tasks conceivable. They strongly condition on the preceding tokens, produce coherent and contentful text (See et al., 2019), and importantly, capture some commonsense and world knowledge (Davison et al., 2019; Petroni et al., 2019). To induce conditional text generation behavior, Radford et al. (2019) propose to add the context tokens (e.g., question and answer) before a special token for the generation start. But for visual-textual tasks, the rationale generation has to be conditioned not only on textual context, but also on an image. 3 Sometimes referred to as density estimation, or left-toright or autoregressive LM (Yang et al., 2019). 4 See Appendix §A.1 for other details of GPT-2. 2811 (a) Object Detection (b"
2020.findings-emnlp.253,2020.acl-main.704,0,0.0155319,"ation For evaluating our models, we follow Camburu et al. (2018) who show that BLEU (Papineni et al., 2002) is not reliable for evaluation of rationale generation, and hence use human evaluation.9 We believe that other automatic sentence similarity measures are also likely not suitable due to a similar reason; multiple rationales could be plausible, although not necessarily paraphrases of each other (e.g., in Figure 4 both generated and human rationales are plausible, but they are not strict paraphrases).10 Future work might consider newly emerging learned evaluation measures, such as BLEURT (Sellam et al., 2020), that could learn to capture non-trivial semantic similarities between sentences beyond surface overlap. We use Amazon Mechanical Turk to crowdsource human judgments of generated rationales according to different criteria. Our instructions are provided in the Appendix §A.6. For VCR, we randomly sample one QA pair for each movie in the development split of the dataset, resulting in 244 examples for human evaluation. For VQA and E SNLI - VE, we randomly sample 250 examples from their development splits.11 We did not use any of 9 This is based on a low inter-annotator BLEU-score between three hu"
2020.findings-emnlp.253,P16-1162,0,0.0434243,"Missing"
2020.findings-emnlp.253,P18-1238,0,0.0213889,"image understanding. Visual-Textual Language Models There is a surge of work that proposes visual-textual pretraining of LMs by predicting masked image regions and tokens (Tan and Bansal, 2019; Lu et al., 2019; 2817 Chen et al., 2019, to name a few). We construct input elements of our models following the VL - BERT architecture (Su et al., 2020). Despite their success, these models are not suitable for generation due to pretraining with the masked LM objective. Zhou et al. (2020) aim to address that, but they pretrain their decoder from scratch using 3M images with weakly-associated captions (Sharma et al., 2018). This makes their decoder arguably less powerful compared to LMs that are pretrained with remarkably more (diverse) data such as GPT-2. Ziegler et al. (2019b) augment GPT-2 with a feature vector for the entire image and evaluate this model on image paragraph captioning. Some work extend pretrained LM to learn video representations from sequences of visual features and words, and show improvements in video captioning (Sun et al., 2019a,b). Our work is based on fine-tuning GPT-2 with features that come from visual object recognition, grounded semantic frames, and visual commonsense graphs. The"
2020.findings-emnlp.253,D19-1339,0,0.020365,"Missing"
2020.findings-emnlp.253,P19-1472,1,0.87627,"c frames, i.e., the primary activity and entities engaged in it detected by a grounded situation recognizer (Fig. 2b; Pratt et al., 2020), and (ii) commonsense inferences inferred from an image and an optional event predicted from a visual commonsense graph (Fig. 2c; Park et al., 2020).1 We report comprehensive experiments with careful analysis using three datasets with human rationales: (i) visual question answering in VQA - E (Li et al., 2018), (ii) visual-textual entailment in E - SNLI - VE (Do et al., 2020), and (iii) an answer justification subtask of visual commonsense reasoning in VCR (Zellers et al., 2019a). Our empirical findings demonstrate that while free-text rationalization remains a challenging task, newly emerging state-of-the-art models support rationale generation as a promising research direction to complement model interpretability for complex visual-textual reasoning tasks. In particular, we find that integration of richer semantic and pragmatic visual knowledge is important for generating rationales with higher visual fidelity, especially for tasks that require higher-level concepts and richer background knowledge. Our code, model weights, and the templates used for human evaluati"
2020.findings-emnlp.253,D19-1221,0,0.0208995,"Missing"
2020.findings-emnlp.253,D19-1002,0,0.0249619,"by Herman (2017). This relates to the pipeline predict-thenexplain setting, where a predictor model and a posthoc explainer model are completely independent. However, there are other settings where generated rationales are intrinsic to the model by design (endto-end predict-then-explain, both end-to-end and pipeline explain-then-predict). As such, generated rationales are more associated with the reasoning process of the model. We recommend that future work develops rationale generation in these settings, and aims for sufficiently faithful models as recommended by Jacovi and Goldberg (2020), Wiegreffe and Pinter (2019). 6 Conclusions We present R ATIONALE VT T RANSFORMER, an integration of a pretrained text generator with semantic and pragmatic visual features. These features improve visual plausibility and fidelity of generated rationales for visual commonsense reasoning, visual-textual entailment, and visual question answering. This represents progress in tackling important, but still relatively unexplored research direction; rationalization of complex reasoning for which explanatory approaches based solely on highlighting parts of the input are not suitable. Acknowledgments The authors thank Sarah Pratt"
2020.findings-emnlp.253,W19-4812,0,0.0174583,"re informative and conceptually relevant explanation to the given QA problem compared to the non-linguistic explanations that are often provided as localized visual highlights on the image. The latter, while pertinent to what the vision component of the model was attending to, cannot provide the full scope of rationales for such complex reasoning tasks as illustrated in Figure 1. Indeed, explanations for higher-level conceptual reasoning can be best conveyed through natural language, as has been studied in recent literature on (visual) NLI (Do et al., 2020; Camburu et al., 2018), (visual) QA (Wu and Mooney, 2019; Rajani et al., 2019), playing arcade games (Ehsan et al., 2019), fact checking (Atanasova et al., 2020), image classification (Hendricks et al., 2018), motivation prediction (Vondrick et al., 2016), and self-driving cars (Kim et al., 2018). In this paper, we present the first focused study on generating natural language rationales across several complex visual reasoning tasks: visual commonsense reasoning, visual-textual entailment, and visual question answering. Our study aims to complement the more broadly studied lower-level explanations such as attention weights and gradients in deep neu"
2020.findings-emnlp.253,Q14-1006,0,0.147764,"Missing"
2020.findings-emnlp.301,D18-1389,0,0.0552657,"Missing"
2020.findings-emnlp.301,W19-3504,0,0.0254159,"ms and corpora exhibit biases against minorities and suffer from low agreement in annotations (Waseem, 2016; Ross et al., 2017), partially due to annotator identity influencing their perception of hate speech (Cowan and Khatchadourian, 2003) and differences in annotation task setup (Sap et al., 2019). Notably, recent work has found that systems are overestimating the prevalence of toxicity in text that contains a minority identity mention (e.g., “I’m a gay man”; Dixon et al., 2018; Hutchinson et al., 2020) or text by racial minorities (e.g., text in African American English; Sap et al., 2019; Davidson et al., 2019). This is partially due to detectors’ over-reliance on lexical cues of toxicity (including swearwords, slurs, and other “bad” words Dinan et al., 2019). We further discuss and examine the effect of these biases in the Appendix, by assessing that the racial bias in toxicity is invariant with respect to model choice (Appendix §C.1) and analyzing the presence of profanity and swearwords separately from toxicity (Appendix §C.2). 3 Out-of-the-Box Generation Toxicity We focus our investigation of toxic degeneration in five popular autoregressive Transformer-based (Vaswani et al., 2017) language mode"
2020.findings-emnlp.301,N19-1423,0,0.0295442,"ontent. Our work provides a test bed for evaluating toxic generations by LMs and stresses the need for better data selection processes for pretraining. 1 Figure 1: Non-toxic examples from R EALT OXICI a new testbed for evaluating neural generations and their toxicity. Despite not containing any toxic language as measured by P ERSPECTIVE API, these prompts cause several pretrained LMs to systematically generate highly toxic text (shown in Table 17 in Appendix §E). TY P ROMPTS, safe deployment (McGuffie and Newhouse, 2020). Introduction Although they are the backbone of many modern NLP systems (Devlin et al., 2019; Radford et al., 2019; Raffel et al., 2019), language models (LMs) pretrained on large web text corpora suffer from degenerate and biased behavior (Sheng et al., 2019; Wallace et al., 2019). As illustrated in Figure 1, they can easily degenerate into toxicity, even without explicitly toxic prompts, which hinders their We first introduce a framework to systematically measure the risk of toxic degeneration by pretrained LMs. We release R EALT OXICI TY P ROMPTS (§4), a set of 100K naturally occurring prompts (i.e., sentence prefixes; Figure 1) extracted from a large corpus of English web text an"
2020.findings-emnlp.301,2020.emnlp-main.23,0,0.0280048,"rge-scale, systematic evaluations of detoxification techniques for language models. However, the conclusions one can make about the effectiveness of a detoxification method are limited by the biases of the model used to detect toxicity (§2.2). To combat these issues, we encourage further work on detecting and controlling different types of toxicity and undesirable social biases in generation, e.g., rudeness (Danescu-Niculescu-Mizil et al., 2013), hate speech (Golbeck et al., 2017), or microaggressions (Breitfeller et al., 2019). Additionally, measures of bias could be multi-dimensional (e.g., Dinan et al., 2020), include explanations (e.g., Sap et al., 2020), or be evolving over time (e.g., using similarity to toxic online content). Limitations We describe several limitations of our study. First, as noted in §2.2, we use an imperfect measure of toxicity that could bias the toxicity towards lexical cues, failing to detect more subtle biases and incorrectly flagging non-toxic content. Second, our analyses are limited to the five language models considered (and their steered variants). Further work could extend our analyses to toxicity to masked language models (Wang and Cho, 2019), among others. Lastly"
2020.findings-emnlp.301,D19-1461,0,0.0508788,"r identity influencing their perception of hate speech (Cowan and Khatchadourian, 2003) and differences in annotation task setup (Sap et al., 2019). Notably, recent work has found that systems are overestimating the prevalence of toxicity in text that contains a minority identity mention (e.g., “I’m a gay man”; Dixon et al., 2018; Hutchinson et al., 2020) or text by racial minorities (e.g., text in African American English; Sap et al., 2019; Davidson et al., 2019). This is partially due to detectors’ over-reliance on lexical cues of toxicity (including swearwords, slurs, and other “bad” words Dinan et al., 2019). We further discuss and examine the effect of these biases in the Appendix, by assessing that the racial bias in toxicity is invariant with respect to model choice (Appendix §C.1) and analyzing the presence of profanity and swearwords separately from toxicity (Appendix §C.2). 3 Out-of-the-Box Generation Toxicity We focus our investigation of toxic degeneration in five popular autoregressive Transformer-based (Vaswani et al., 2017) language models: GPT-1, 5 T OXICITY4 1 ity detection tool. Accessed through an API, T OX ICITY corresponds to the prediction output of a CNN (Lecun et al., 1998) tr"
2020.findings-emnlp.301,D19-1224,1,0.835063,"anguage models grow in size (Brown et al., 2020), so does their need for larger corpora, often drawn from easily accessible and abundant web text. However, our analyses reveal toxicity in web text data that likely enable language models to generate even unprompted toxicity (§3.1). Our findings raise several practical and ethical concerns. First, analysis of pretraining data is a crucial first step towards understanding toxic, biased, or otherwise degenerate behavior of language models. Therefore, echoing calls for transparency in NLP research (Bender and Friedman, 2018; Mitchell et al., 2019; Dodge et al., 2019), we recommend researchers publicly release all relevant information during data collection (e.g., original text, source URLs, timestamps, platform-specific metadata) when building pretraining corpora. Second, using Reddit popularity as a curation heuristic introduces representational harm (Barocas et al., 2017) by biasing the populations whose language and perspectives are included in pretraining (e.g., Reddit users skew male; Barthel et al., 2016). This raises the question of who decides whose voices are going to be learned by the language model, and whose voices are excluded. Following Blod"
2020.findings-emnlp.301,W17-4912,0,0.0376314,"ity per-category, is bolded. We display DAPT (Toxic) as a reference for the effectiveness of DAPT as a method of controlling LM behavior. All models are evaluated on a full dataset of 100K prompts, except PPLM, which is evaluated on a dataset of 10K prompts, due to computational budget. Domain-Adaptive Pretraining (DAPT) Using the framework outlined in Gururangan et al. (2020), we perform an additional phase of pretraining on the non-toxic subset of a balanced corpus with GPT-2. For comparison, we also perform the experiment using the toxic subset. Attribute Conditioning (AT C ON) Inspired by Ficler and Goldberg (2017) and Keskar et al. (2019), we prepend a corresponding toxicity attribute token (<|toxic|>, <|nontoxic|>) to a random sample of documents and pretrain the GPT-2 language model further. In our generation experiments, we prepend the <|nontoxic|> token to our prompts. 5.2 Decoding-Based Detoxification Noting the additional cost of training language models further, we explore three detoxifying strategies that only rely on altering the decoding algorithm and are therefore more readily usable by many practitioners. Vocabulary Shifting (VOCAB -S HIFT) Inspired by Eisenstein et al. (2011) and Ghosh et"
2020.findings-emnlp.301,P17-1059,0,0.0406262,"rg (2017) and Keskar et al. (2019), we prepend a corresponding toxicity attribute token (<|toxic|>, <|nontoxic|>) to a random sample of documents and pretrain the GPT-2 language model further. In our generation experiments, we prepend the <|nontoxic|> token to our prompts. 5.2 Decoding-Based Detoxification Noting the additional cost of training language models further, we explore three detoxifying strategies that only rely on altering the decoding algorithm and are therefore more readily usable by many practitioners. Vocabulary Shifting (VOCAB -S HIFT) Inspired by Eisenstein et al. (2011) and Ghosh et al. (2017), we learn a 2-dimensional representation of toxicity and non-toxicity for every token in GPT-2’s vocabulary, which we then use to boost the likelihood of non-toxic tokens. Given the language model’s unnormalized probability (logits) over the vocabulary, we add the term W · t, where t 2 R2 encodes (non-)toxicity, and W 2 RV represents the associations between each token and (non-)toxicity, and is the boosting strength. We set = 3 for all experiments. We learn this representation using the toxicity labels on the balanced corpus described in §5.1 (See Appendix §B.3 for more details). Word Filter"
2020.findings-emnlp.301,2020.acl-main.740,1,0.886716,"Missing"
2020.findings-emnlp.301,P18-1152,1,0.889536,"Missing"
2020.findings-emnlp.301,2020.acl-main.487,0,0.217442,"ases in Toxic Language Detection Although widely used, the P ERSPECTIVE API and other hate speech detection systems and corpora exhibit biases against minorities and suffer from low agreement in annotations (Waseem, 2016; Ross et al., 2017), partially due to annotator identity influencing their perception of hate speech (Cowan and Khatchadourian, 2003) and differences in annotation task setup (Sap et al., 2019). Notably, recent work has found that systems are overestimating the prevalence of toxicity in text that contains a minority identity mention (e.g., “I’m a gay man”; Dixon et al., 2018; Hutchinson et al., 2020) or text by racial minorities (e.g., text in African American English; Sap et al., 2019; Davidson et al., 2019). This is partially due to detectors’ over-reliance on lexical cues of toxicity (including swearwords, slurs, and other “bad” words Dinan et al., 2019). We further discuss and examine the effect of these biases in the Appendix, by assessing that the racial bias in toxicity is invariant with respect to model choice (Appendix §C.1) and analyzing the presence of profanity and swearwords separately from toxicity (Appendix §C.2). 3 Out-of-the-Box Generation Toxicity We focus our investigat"
2020.findings-emnlp.301,W19-3514,0,0.0208833,"c degeneration by both out-of-the-box and controlled models using 100K naturally occurring prompts, including some that do not contain identity mentions (see Figure 1). Additionally, our work focuses on the broad phenomenon of toxicity in generations, whereas Sheng et al. (2019) study the sentiment and regard expressed by a model’s generation towards demographic identities. The creation of R EALT OXICITY P ROMPTS was partly inspired by work in detecting conversational patterns that can cause derailment into antisocial behavior in online conversations (Zhang et al., ˇ 2018; Stoop et al., 2019; Karan and Snajder, 2019). Our work also draws from a strong line of research into controlling the outputs of language models (Dathathri et al., 2020; Sudhakar et al., 2019; Ziegler et al.; Keskar et al., 2019, inter alia). 9 Conclusion We introduce R EALT OXICITY P ROMPTS, a testbed of 100K prompts for evaluating the toxic degeneration in pretrained language models. Under this framework, we quantify the toxicity of multiple pretrained language models and the effectiveness of methods for detoxifying generations. We then analyze toxicity in two large web text corpora, including the GPT-2 pretraining corpus, to better u"
2020.findings-emnlp.301,W19-3823,0,0.0294992,"e models considered (and their steered variants). Further work could extend our analyses to toxicity to masked language models (Wang and Cho, 2019), among others. Lastly, because O PENAI-WT does not have available metadata, and due to the imperfect coverage of our subreddit and news reliability data, we only provide lower bound estimates of toxicity in web text corpora. 8 Related Work A wealth of work has shown that toxicity and social biases in training data are acquired by large pretrained sentence encoders (e.g., gender bias in BERT; May et al., 2019; Zhao et al., 2019; Basta et al., 2019; Kurita et al., 2019). However, fewer studies have investigated toxicity in autoregressive language models, whose generations also suffer from incoherence, blandness, and repetitiveness (Holtzman et al., 2020; Welleck et al., 2019). Similar in spirit to R EALT OXICITY P ROMPTS, Wallace et al. (2019) find universal adversarial triggers, nonsensical prompts that trigger toxic generations in GPT-2. In this work, we find and release naturally occurring prompts from web text that trigger toxicity, and compare toxic output in several language models. Most closely related to this work, Sheng et al. (2019) use a set of 60"
2020.findings-emnlp.301,2020.emnlp-main.602,1,0.751142,"r steering introduce unwanted side effects in language model behavior after adaptation. Decoding with a Purpose Our analyses also highlight the promise of certain decoding methods, such as PPLM (Dathathri et al., 2020), which is among the most effective methods we tested at avoiding toxicity with toxic prompts. In addition to automated toxicity classifiers, future work could explore the use of handpicked toxic documents as “negative examples” to avoid toxicity in generation. Future work could also investigate infusing models with more sophisticated or nuanced representations of social biases (Ma et al., 2020). Choice of Pretraining Data As pretrained language models grow in size (Brown et al., 2020), so does their need for larger corpora, often drawn from easily accessible and abundant web text. However, our analyses reveal toxicity in web text data that likely enable language models to generate even unprompted toxicity (§3.1). Our findings raise several practical and ethical concerns. First, analysis of pretraining data is a crucial first step towards understanding toxic, biased, or otherwise degenerate behavior of language models. Therefore, echoing calls for transparency in NLP research (Bender"
2020.findings-emnlp.301,N19-1063,0,0.0316988,"ent. Second, our analyses are limited to the five language models considered (and their steered variants). Further work could extend our analyses to toxicity to masked language models (Wang and Cho, 2019), among others. Lastly, because O PENAI-WT does not have available metadata, and due to the imperfect coverage of our subreddit and news reliability data, we only provide lower bound estimates of toxicity in web text corpora. 8 Related Work A wealth of work has shown that toxicity and social biases in training data are acquired by large pretrained sentence encoders (e.g., gender bias in BERT; May et al., 2019; Zhao et al., 2019; Basta et al., 2019; Kurita et al., 2019). However, fewer studies have investigated toxicity in autoregressive language models, whose generations also suffer from incoherence, blandness, and repetitiveness (Holtzman et al., 2020; Welleck et al., 2019). Similar in spirit to R EALT OXICITY P ROMPTS, Wallace et al. (2019) find universal adversarial triggers, nonsensical prompts that trigger toxic generations in GPT-2. In this work, we find and release naturally occurring prompts from web text that trigger toxicity, and compare toxic output in several language models. Most clos"
2020.findings-emnlp.301,W17-3006,0,0.0640256,"Missing"
2020.findings-emnlp.301,P19-1163,1,0.906234,"ion.allenai.org/ 3 https://github.com/conversationai/ perspectiveapi 4 P ERSPECTIVE API defines T OXICITY as a “rude, disrespectful, or unreasonable comment; likely to make people leave a discussion.” Biases in Toxic Language Detection Although widely used, the P ERSPECTIVE API and other hate speech detection systems and corpora exhibit biases against minorities and suffer from low agreement in annotations (Waseem, 2016; Ross et al., 2017), partially due to annotator identity influencing their perception of hate speech (Cowan and Khatchadourian, 2003) and differences in annotation task setup (Sap et al., 2019). Notably, recent work has found that systems are overestimating the prevalence of toxicity in text that contains a minority identity mention (e.g., “I’m a gay man”; Dixon et al., 2018; Hutchinson et al., 2020) or text by racial minorities (e.g., text in African American English; Sap et al., 2019; Davidson et al., 2019). This is partially due to detectors’ over-reliance on lexical cues of toxicity (including swearwords, slurs, and other “bad” words Dinan et al., 2019). We further discuss and examine the effect of these biases in the Appendix, by assessing that the racial bias in toxicity is in"
2020.findings-emnlp.301,2020.acl-main.486,1,0.659692,"on techniques for language models. However, the conclusions one can make about the effectiveness of a detoxification method are limited by the biases of the model used to detect toxicity (§2.2). To combat these issues, we encourage further work on detecting and controlling different types of toxicity and undesirable social biases in generation, e.g., rudeness (Danescu-Niculescu-Mizil et al., 2013), hate speech (Golbeck et al., 2017), or microaggressions (Breitfeller et al., 2019). Additionally, measures of bias could be multi-dimensional (e.g., Dinan et al., 2020), include explanations (e.g., Sap et al., 2020), or be evolving over time (e.g., using similarity to toxic online content). Limitations We describe several limitations of our study. First, as noted in §2.2, we use an imperfect measure of toxicity that could bias the toxicity towards lexical cues, failing to detect more subtle biases and incorrectly flagging non-toxic content. Second, our analyses are limited to the five language models considered (and their steered variants). Further work could extend our analyses to toxicity to masked language models (Wang and Cho, 2019), among others. Lastly, because O PENAI-WT does not have available me"
2020.findings-emnlp.301,P16-1162,0,0.099673,"Missing"
2020.findings-emnlp.301,2020.lrec-1.298,0,0.0458399,"Missing"
2020.findings-emnlp.301,D19-1339,0,0.268334,"toxic examples from R EALT OXICI a new testbed for evaluating neural generations and their toxicity. Despite not containing any toxic language as measured by P ERSPECTIVE API, these prompts cause several pretrained LMs to systematically generate highly toxic text (shown in Table 17 in Appendix §E). TY P ROMPTS, safe deployment (McGuffie and Newhouse, 2020). Introduction Although they are the backbone of many modern NLP systems (Devlin et al., 2019; Radford et al., 2019; Raffel et al., 2019), language models (LMs) pretrained on large web text corpora suffer from degenerate and biased behavior (Sheng et al., 2019; Wallace et al., 2019). As illustrated in Figure 1, they can easily degenerate into toxicity, even without explicitly toxic prompts, which hinders their We first introduce a framework to systematically measure the risk of toxic degeneration by pretrained LMs. We release R EALT OXICI TY P ROMPTS (§4), a set of 100K naturally occurring prompts (i.e., sentence prefixes; Figure 1) extracted from a large corpus of English web text and paired with toxicity scores from a widely used and commercially deployed toxicity detector (P ERSPECTIVE API). We show that popular LMs produce toxic generations whe"
2020.findings-emnlp.301,W19-3503,0,0.0314034,"Missing"
2020.findings-emnlp.301,D19-1322,0,0.022498,"ns (see Figure 1). Additionally, our work focuses on the broad phenomenon of toxicity in generations, whereas Sheng et al. (2019) study the sentiment and regard expressed by a model’s generation towards demographic identities. The creation of R EALT OXICITY P ROMPTS was partly inspired by work in detecting conversational patterns that can cause derailment into antisocial behavior in online conversations (Zhang et al., ˇ 2018; Stoop et al., 2019; Karan and Snajder, 2019). Our work also draws from a strong line of research into controlling the outputs of language models (Dathathri et al., 2020; Sudhakar et al., 2019; Ziegler et al.; Keskar et al., 2019, inter alia). 9 Conclusion We introduce R EALT OXICITY P ROMPTS, a testbed of 100K prompts for evaluating the toxic degeneration in pretrained language models. Under this framework, we quantify the toxicity of multiple pretrained language models and the effectiveness of methods for detoxifying generations. We then analyze toxicity in two large web text corpora, including the GPT-2 pretraining corpus, to better understand the root cause of toxic generations. Finally, we provide recommendations for gathering pretraining data. The data, code, and interactive"
2020.findings-emnlp.301,D19-1221,0,0.202545,"R EALT OXICI a new testbed for evaluating neural generations and their toxicity. Despite not containing any toxic language as measured by P ERSPECTIVE API, these prompts cause several pretrained LMs to systematically generate highly toxic text (shown in Table 17 in Appendix §E). TY P ROMPTS, safe deployment (McGuffie and Newhouse, 2020). Introduction Although they are the backbone of many modern NLP systems (Devlin et al., 2019; Radford et al., 2019; Raffel et al., 2019), language models (LMs) pretrained on large web text corpora suffer from degenerate and biased behavior (Sheng et al., 2019; Wallace et al., 2019). As illustrated in Figure 1, they can easily degenerate into toxicity, even without explicitly toxic prompts, which hinders their We first introduce a framework to systematically measure the risk of toxic degeneration by pretrained LMs. We release R EALT OXICI TY P ROMPTS (§4), a set of 100K naturally occurring prompts (i.e., sentence prefixes; Figure 1) extracted from a large corpus of English web text and paired with toxicity scores from a widely used and commercially deployed toxicity detector (P ERSPECTIVE API). We show that popular LMs produce toxic generations when conditioned on our pr"
2020.findings-emnlp.301,W19-2304,0,0.029457,"ti-dimensional (e.g., Dinan et al., 2020), include explanations (e.g., Sap et al., 2020), or be evolving over time (e.g., using similarity to toxic online content). Limitations We describe several limitations of our study. First, as noted in §2.2, we use an imperfect measure of toxicity that could bias the toxicity towards lexical cues, failing to detect more subtle biases and incorrectly flagging non-toxic content. Second, our analyses are limited to the five language models considered (and their steered variants). Further work could extend our analyses to toxicity to masked language models (Wang and Cho, 2019), among others. Lastly, because O PENAI-WT does not have available metadata, and due to the imperfect coverage of our subreddit and news reliability data, we only provide lower bound estimates of toxicity in web text corpora. 8 Related Work A wealth of work has shown that toxicity and social biases in training data are acquired by large pretrained sentence encoders (e.g., gender bias in BERT; May et al., 2019; Zhao et al., 2019; Basta et al., 2019; Kurita et al., 2019). However, fewer studies have investigated toxicity in autoregressive language models, whose generations also suffer from incoh"
2020.findings-emnlp.301,W16-5618,0,0.0171159,"eural language models, and therefore use the term “neural toxic degeneration.” Future work could examine whether non-neural language models exhibit similar behavior. 2 http://toxicdegeneration.allenai.org/ 3 https://github.com/conversationai/ perspectiveapi 4 P ERSPECTIVE API defines T OXICITY as a “rude, disrespectful, or unreasonable comment; likely to make people leave a discussion.” Biases in Toxic Language Detection Although widely used, the P ERSPECTIVE API and other hate speech detection systems and corpora exhibit biases against minorities and suffer from low agreement in annotations (Waseem, 2016; Ross et al., 2017), partially due to annotator identity influencing their perception of hate speech (Cowan and Khatchadourian, 2003) and differences in annotation task setup (Sap et al., 2019). Notably, recent work has found that systems are overestimating the prevalence of toxicity in text that contains a minority identity mention (e.g., “I’m a gay man”; Dixon et al., 2018; Hutchinson et al., 2020) or text by racial minorities (e.g., text in African American English; Sap et al., 2019; Davidson et al., 2019). This is partially due to detectors’ over-reliance on lexical cues of toxicity (incl"
2020.findings-emnlp.301,P18-1125,0,0.0642734,"Missing"
2020.findings-emnlp.301,N19-1064,0,0.0618342,"Missing"
2020.findings-emnlp.418,P16-2085,0,0.0362699,"Missing"
2020.findings-emnlp.418,P91-1008,0,0.661724,"in order to come up with examples or counterarguments that may undermine it; by analogy, the generative task we introduce here requires a model to come up with (rather than simply verify) examples of circumstances that undermine the given hypothesis. 2 Background and Related Work Defeasible reasoning is soft inference based on default assumptions to account for unknown facts, for example, “Tweety is a bird” entails that “Tweety flies”, because birds usually fly. Such a conclusion is not deductively valid, and might be invalidated by new information such as “Tweety is a penguin” (Reiter, 1980; Lascarides and Asher, 1991). Defeasible reasoning is a type of nonmonotonic logic, as it contrasts the monotonicity property of classical logic, according to which valid inferences cannot be defeated by adding additional information (Kraus et al., 1990). Defeasible reasoning has been studied in a range of fields from logic, through linguistics and artificial intelligence. Classical AI. In early AI, defeasible reasoning was used as a solution to the “frame problem”: it is impossible to list all the potential effects of actions without describing mundane and obvious effects (McCarthy and Hayes, 1969). McDermott and Doyle"
2020.findings-emnlp.418,2020.acl-main.703,0,0.0591476,"Missing"
2020.findings-emnlp.418,W04-1013,0,0.0613121,"Missing"
2020.findings-emnlp.418,2021.ccl-1.108,0,0.0545281,"Missing"
2020.findings-emnlp.418,W17-1609,1,0.892575,"Missing"
2020.findings-emnlp.418,D17-1238,0,0.0285361,"Missing"
2020.findings-emnlp.418,P02-1040,0,0.106635,"jective to predict the next word. We use the Transformers package (Wolf et al., 2019) and train each model for a single epoch with a batch size of 64. Further training details are provided in the appendix. Automatic Evaluation. We follow the common practice of reporting automated generation evaluation metrics. We report the perplexity on the test set, as is often used to measure the performance of a language model.4 In addition, we generated predictions for the test set using beam search with 5 beams, and evaluated them using standard n-gram based metrics: the precision-oriented BLEU-4 score (Papineni et al., 2002), which considers n-grams up to n = 4, and the recall-oriented 4666 4 Micro and macro perplexities were identical. Premise Hypothesis Type Generated Update 1 A man just roaming on the streets during night. A man is roaming the streets at night, drunk. — It is rude to point out their weight problem. S W The man has a beer in his hand You are a nutritionist 2 PersonX pays PersonX’s debt — W S PersonX is in debt to the IRS You are in an emergency 3 4 5 6 7 8 9 Because PersonX wanted to be debt free It is rude to refuse help. — It is wrong to kill an animal. S You are trying to save the life of a"
2020.findings-emnlp.418,Q19-1043,0,0.0222982,"as a softer version of semantic entailment, doubly hedging it with “a human would typically think that the hypothesis is likely true” (see Section 3, Dagan et al., 2005). It gained tremendous popularity again 10 years later, with the release of the large-scale Stanford Natural Language Inference dataset (SNLI; Bowman et al., 2015), that facilitated training neural models, and which was followed by several other datasets in that nature (Williams et al., 2018; Nie et al., 2019). But—among other criticisms of the task—it has been shown that people generally don’t agree on entailment annotations (Pavlick and Kwiatkowski, 2019), and new variants of the task suggested to shift away from categorical labels to ordinal or numeric values denoting plausibility (Zhang et al., 2017; Sakaguchi and Van Durme, 2018; Chen et al., 2020). In this paper we focus on the defeasibility of textual entailments, a less well-studied phenomenon in this context. 3 Definition In this paper, we employ a working definition of defeasible inference that may be seen as an outgrowth of prior work. Dagan et al. (2005) introduced the following informal definition for the Recognizing Textual Entailment (RTE) task: ...textual entailment is defined as"
2020.findings-emnlp.418,S18-2023,1,0.911929,"Missing"
2020.findings-emnlp.418,D19-1509,1,0.887999,"Missing"
2020.findings-emnlp.418,P18-1020,0,0.0609249,"Missing"
2020.findings-emnlp.418,D19-1629,0,0.140498,"Missing"
2020.findings-emnlp.418,L18-1239,0,0.0442715,"Missing"
2020.findings-emnlp.418,N18-1101,0,0.0369249,"elations by defining defeasible rules based on commonsense knowledge of typical causes and effects. Natural Language Processing. Textual entailment was defined as a softer version of semantic entailment, doubly hedging it with “a human would typically think that the hypothesis is likely true” (see Section 3, Dagan et al., 2005). It gained tremendous popularity again 10 years later, with the release of the large-scale Stanford Natural Language Inference dataset (SNLI; Bowman et al., 2015), that facilitated training neural models, and which was followed by several other datasets in that nature (Williams et al., 2018; Nie et al., 2019). But—among other criticisms of the task—it has been shown that people generally don’t agree on entailment annotations (Pavlick and Kwiatkowski, 2019), and new variants of the task suggested to shift away from categorical labels to ordinal or numeric values denoting plausibility (Zhang et al., 2017; Sakaguchi and Van Durme, 2018; Chen et al., 2020). In this paper we focus on the defeasibility of textual entailments, a less well-studied phenomenon in this context. 3 Definition In this paper, we employ a working definition of defeasible inference that may be seen as an outgrow"
2020.nuse-1.6,P19-1163,1,0.824746,"ut story length (# tokens), writing time, and writing speed (key press time) are not. RQ2 How are rater gender, age, and personality traits associated with variation in story quality ratings? Ratings of stories are often only used to evaluate a story writing system’s output (e.g., Fan et al., 2018; Yao et al., 2019) or to develop automatic evaluation metrics (e.g., Hashimoto et al., 2019; Purdy et al., 2018), ignoring the rater’s identity. However, prior work has shown differences in crowdsourcing worker’s behavior or annotations based on task framing (Levin et al., 2002; August et al., 2018; Sap et al., 2019) or the annotator’s own identity or experiences (Breitfeller et al., 2019; Geva et al., 2019). We seek to confirm and characterize these differences in our story rating task. As a follow-up to RQ2, we also investigate the interaction between author and rater demographics on story ratings. ings (§3.2) to evaluate the effect of writing setup and author identity on story writing. 3.1 Crowdsourcing Stories To construct S TORIES I N T HE W ILD, we first collected 1,630 written stories using a volunteerbased online study platform, LabintheWild (Reinecke and Gajos, 2015).3 Following best practices in"
2020.tacl-1.53,D18-1549,0,0.0620634,"efore shard the dataset into chunks of 32,768 sentences and perform nearest-neighbor comparisons in chunks for each language pair. We use a simple mapreduce algorithm to merge the intermediate results back together. We follow the approach outlined in Sec. 2 for Wikipedia bitext mining. For each source sentence, we retrieve the four nearest target neighbors across the millions of sentences that we extracted from Wikipedia and compute the margin-based scores for each pair. 4.1 Unsupervised NMT We show that our pseudo-parallel text can complement existing techniques for unsupervised translation (Artetxe et al., 2018; Lample et al., 2018c). In line with existing work on UNMT, we evaluate our approach on the WMT’14 Fr-En and WMT’16 De-En test sets. Our UNMT experiments build upon the reference implementation7 of XLM (Lample and Conneau, 2019). The UNMT model is trained by alternating between two steps: a denoising autoencoder step and a backtranslation step (refer to Lample et al., 2018c for more details). The backtranslation step generates pseudo-parallel Bitext for Neural Machine Translation 5 h t t p s : / / g i t hub.com/facebookresearch /fastText/blob/master/wikifil.pl. 6 https://github.com/fnl/syntok"
2020.tacl-1.53,P05-1074,0,0.152887,"ng the previous stateof-the-art. Finally, we enrich the IWSLT’15 English-Vietnamese corpus with pseudoparallel Wikipedia sentence pairs, yielding a 1.2 BLEU improvement on the low-resource MT task. We demonstrate that unsupervised bitext mining is an effective way of augmenting MT datasets and complements existing techniques like initializing with pre-trained contextual embeddings. 1 Introduction Large corpora of parallel sentences are prerequisites for training models across a diverse set of applications, such as neural machine translation (NMT; Bahdanau et al., 2015), paraphrase generation (Bannard and Callison-Burch, 2005), and aligned multilingual sentence embeddings (Artetxe and Schwenk, 2019b). Systems that extract parallel corpora typically rely on various cross-lingual resources (e.g., bilingual lexicons, parallel cor1 By unsupervised, we mean that no cross-lingual resources like parallel text or bilingual lexicons are used. Unsupervised techniques have been used to bootstrap MT systems for low-resource languages like Khmer and Burmese (Marie et al., 2019). 828 Transactions of the Association for Computational Linguistics, vol. 8, pp. 828–841, 2020. https://doi.org/10.1162/tacl a 00348 Action Editor: Colli"
2020.tacl-1.53,W18-6317,0,0.257025,"(2019) explored bitext mining with mBERT in the supervised context and found that retrieval performance significantly varies with the mBERT layer used to create sentence embeddings. In particular, they found layer 8 embeddings gave the highest precision-at1. We also observe an improvement (Table 1) in unsupervised retrieval of another 13 to 20 points by using the 8th layer instead of the default final layer (12th). We include these results but do not consider them unsupervised, as we would not know a priori which layer was best to use. 3.3 Choosing Negative Sentence Pairs Other authors (e.g., Guo et al., 2018) have noted that the choice of negative examples has a considerable impact on metric learning. Specifically, using negative examples which are difficult to distinguish from the positive nearest neighbor is often beneficial for performance. We examine the impact of taking random sentences instead of the remaining k −1 nearest neighbors as the negatives during self-training. Our results are in Table 3. While self-training with random negatives still greatly improves the untuned baseline, the use of hard negative examples mined from the k -nearest neighborhood can make a significant difference to"
2020.tacl-1.53,D18-1217,0,0.0606492,"Missing"
2020.tacl-1.53,N19-1423,0,0.150462,") created sentence representations by mean-pooling BWEs over content words. To disambiguate semantically similar but non-parallel sentences, Hangya and Fraser (2019) additionally proposed parallel segment detection by searching for paired substrings with high similarity scores per word. However, using word embeddings to generate sentence embeddings ignores sentential context, which may degrade bitext retrieval performance. We describe a new unsupervised bitext mining approach based on contextual embeddings. We create sentence embeddings by mean-pooling the outputs of multilingual BERT (mBERT; Devlin et al., 2019), which is pre-trained on unaligned Wikipedia sentences across 104 languages. For a pair of source and target languages, we find candidate translations by using nearest-neighbor search with margin-based similarity scores between pairs of mBERT-embedded source and target sentences. We bootstrap a dataset of positive and negative sentence pairs from these initial neighborhoods of candidates, then self-train mBERT on its own outputs. A final retrieval step gives a corpus of pseudo-parallel sentence pairs, which we expect to be a mix of actual translations and semantically related non-translations"
2020.tacl-1.53,P19-1118,0,0.1296,"Missing"
2020.tacl-1.53,C18-1122,0,0.0480451,"Missing"
2020.tacl-1.53,W18-2703,0,0.0323004,"text is included during training. As in previous works on En-Vi (cf. Luong and Manning, 2015), we use tst2012 (1,553 pairs) and tst2013 (1,268 pairs) as our development and test sets respectively, we tokenize all data with Moses, and we report tokenized BLEU via multi-bleu.perl. The BLEU score increases monotonically with the size of the pseudo-parallel corpus and exceeds the state-of-the-art system’s BLEU by 1.2 points. This result is consistent with improvements observed with other types of monolingual data augmentation like pre-trained UNMT initialization, various forms of backtranslation (Hoang et al., 2018; Zhou and Keung, 2020), and cross-view training (CVT; Clark et al., 2018): Luong and Manning (2015) Clark et al. (2018) Clark et al. (2018), with CVT Xu et al. (2019) Nguyen and Salazar (2019) 26.4 28.9 29.6 31.4 32.8 (28.8) + top 100k mined pairs + top 200k mined pairs + top 300k mined pairs + top 400k mined pairs 33.2 (29.5) 33.9 (29.8) 34.0 (30.0) 34.1 (29.9) Table 5: Tokenized BLEU scores on tst2013 for the low-resource IWSLT’15 English-Vietnamese translation task using bitext mined with our method. Added pairs are sorted by their score. Development scores on tst2012 in parentheses. We de"
2020.tacl-1.53,D19-1138,1,0.805258,"mission batch: 4/2020; Revision batch: 8/2020; Published 12/2020. c 2020 Association for Computational Linguistics. Distributed under a CC-BY 4.0 license. We investigate whether contextualized sentence embeddings created with unaligned text are useful for unsupervised bitext retrieval. Previous work explored the use of multilingual sentence encoders taken from machine translation models (e.g., Artetxe and Schwenk, 2019b; Lu et al., 2018) for zero-shot cross-lingual transfer. Our work is motivated by recent success in tasks like zero-shot text classification and named entity recognition (e.g., Keung et al., 2019; Mulcaire et al., 2019) with multilingual contextual embeddings, which exhibit cross-lingual properties despite being trained without parallel sentences. We illustrate our method in Figure 1. We first retrieve the candidate translation pairs: We apply our technique on the BUCC 2017 parallel sentence mining task (Zweigenbaum et al., 2017). We achieve state-of-the-art F1 scores on unsupervised bitext mining, with an improvement of up to 24.5 points (absolute) on published results (Hangya and Fraser, 2019). Other work (e.g., Libovick´y et al., 2019) has shown that retrieval performance varies su"
2020.tacl-1.53,W18-6325,0,0.0191143,"monolingual Vietnamese Wikipedia text by a factor of ten (13.3k sentence pairs). We use the reference implementation8 for the state-of-the-art model (Nguyen and Salazar, 2019), which is a highly regularized 6+6-layer transformer with pre-norm residual connections, scale normalization, and normalized word embeddings. We use the same hyperparameters (except for the dropout rate) but train on our augmented datasets. To mitigate domain shift, we finetune the best checkpoint for 75k more steps using only the IWSLT training data, in the spirit of ‘‘trivial’’ transfer learning for low-resource NMT (Kocmi and Bojar, 2018). In Table 5, we show BLEU scores as more pseudo-parallel text is included during training. As in previous works on En-Vi (cf. Luong and Manning, 2015), we use tst2012 (1,553 pairs) and tst2013 (1,268 pairs) as our development and test sets respectively, we tokenize all data with Moses, and we report tokenized BLEU via multi-bleu.perl. The BLEU score increases monotonically with the size of the pseudo-parallel corpus and exceeds the state-of-the-art system’s BLEU by 1.2 points. This result is consistent with improvements observed with other types of monolingual data augmentation like pre-train"
2020.tacl-1.53,P19-1493,0,0.0683454,"Missing"
2020.tacl-1.53,W18-6309,1,0.694215,"l., 2019). 828 Transactions of the Association for Computational Linguistics, vol. 8, pp. 828–841, 2020. https://doi.org/10.1162/tacl a 00348 Action Editor: Collin Cherry. Submission batch: 4/2020; Revision batch: 8/2020; Published 12/2020. c 2020 Association for Computational Linguistics. Distributed under a CC-BY 4.0 license. We investigate whether contextualized sentence embeddings created with unaligned text are useful for unsupervised bitext retrieval. Previous work explored the use of multilingual sentence encoders taken from machine translation models (e.g., Artetxe and Schwenk, 2019b; Lu et al., 2018) for zero-shot cross-lingual transfer. Our work is motivated by recent success in tasks like zero-shot text classification and named entity recognition (e.g., Keung et al., 2019; Mulcaire et al., 2019) with multilingual contextual embeddings, which exhibit cross-lingual properties despite being trained without parallel sentences. We illustrate our method in Figure 1. We first retrieve the candidate translation pairs: We apply our technique on the BUCC 2017 parallel sentence mining task (Zweigenbaum et al., 2017). We achieve state-of-the-art F1 scores on unsupervised bitext mining, with an impr"
2020.tacl-1.53,P16-1009,0,0.0820164,"Missing"
2020.tacl-1.53,W18-6319,0,0.0153696,"we chose to report results on XLM because it has been validated on a wider range of tasks and languages. We also trained a standard 6-layer transformer encoder-decoder model directly on the pseudoparallel text. We used the standard implementation in Sockeye (Hieber et al., 2018) as-is, and trained models for French and German on 2.5 million Wikipedia sentence pairs. We withheld 10k pseudo-parallel pairs per language pair to serve as a development set. We achieved BLEU scores of 20.8, 21.1, 28.2, and 28.0 on En-De, DeEn, En-Fr, and Fr-En respectively. BLEU scores were computed with SacreBLEU (Post, 2018). training data, and we incorporate our bitext during UNMT training in the same way, as another set of pseudo-parallel sentences. We also use the same initialization as Lample and Conneau (2019), where the UNMT models have encoders and decoders that are initialized with contextual embeddings trained on the source and target language Wikipedia corpora with the masked language model (MLM) objective; no parallel data is used. We performed the exhaustive (Fr Wiki)-(En Wiki) and (De Wiki)-(En Wiki) nearest-neighbor comparison on eight V100 GPUs, which requires 3 to 4 days to complete per language p"
2020.tacl-1.53,resnik-1998-parallel,0,0.877145,"Missing"
2020.tacl-1.53,J03-3002,1,0.710074,"arallel pairs only, using the same Sockeye recipe in Sec. 4.2. This yielded a BLEU score of 27.5 on En-Vi, which is lower than the best XLM-based result (i.e., 28.9), which suggests that the XLM initialization improves unsupervised NMT. A similar outcome was also reported in Lample and Conneau (2019). 5 Related Work 5.1 Parallel Sentence Mining Approaches to parallel sentence (or bitext) mining have been historically driven by the data requirements of statistical machine translation. Some of the earliest work in mining the Web for large-scale parallel corpora can be found in Resnik (1998) and Resnik and Smith (2003). Recent interest in the field is reflected by new shared tasks on parallel extraction and filtering (Zweigenbaum et al., 2017; Koehn et al., 2018) and the creation of massively multilingual parallel corpora mined from the Web, like WikiMatrix (Schwenk et al., 2019a) and CCMatrix (Schwenk et al., 2019b). Existing parallel corpora have been exploited in many ways to create sentence representations for supervised bitext mining. One approach involves a joint encoder with a shared wordpiece vocabulary, trained as part of multiple encoder-decoder translation models on parallel corpora (Schwenk, 201"
2021.acl-long.166,P19-1470,0,0.0371587,"Missing"
2021.acl-long.166,2020.findings-emnlp.428,1,0.841649,"ous works. Unlike categorization techniques, we require no task-specific annotated data as we supervise with citing sentences that are readily available in scientific documents. In practice, citation classification is used to assist in suggesting relevant works to researchers; our work complements this goal by providing rationales for the recommendation and furthering progress toward explainable AI. Our work is also connected to a long history of research on summarizing scientific documents (Luhn, 1958; Paice, 1980). Work in this area has mostly used used abstracts or peer reviews as targets (Cachola et al., 2020; Cohan et al., 2018; Jaidka et al., 2017). In particular, Pilault et al. (2020) show that using a simple extractive summary as input for abstractive summarization of scholarly texts work well. Researchers have also used citing sentences as part of the input for summarization, recognizing the explanatory power of these texts (Nakov et al., 2004; Cohan and Goharian, 2017; Yasunaga et al., 2019). Ours is the first work to focus on learning to express the specific relationship between two documents from such sentences. The closest work to our own is Xing et al. (2020), who pilot a task of in-line"
2021.acl-long.166,N18-2097,0,0.0229218,"gorization techniques, we require no task-specific annotated data as we supervise with citing sentences that are readily available in scientific documents. In practice, citation classification is used to assist in suggesting relevant works to researchers; our work complements this goal by providing rationales for the recommendation and furthering progress toward explainable AI. Our work is also connected to a long history of research on summarizing scientific documents (Luhn, 1958; Paice, 1980). Work in this area has mostly used used abstracts or peer reviews as targets (Cachola et al., 2020; Cohan et al., 2018; Jaidka et al., 2017). In particular, Pilault et al. (2020) show that using a simple extractive summary as input for abstractive summarization of scholarly texts work well. Researchers have also used citing sentences as part of the input for summarization, recognizing the explanatory power of these texts (Nakov et al., 2004; Cohan and Goharian, 2017; Yasunaga et al., 2019). Ours is the first work to focus on learning to express the specific relationship between two documents from such sentences. The closest work to our own is Xing et al. (2020), who pilot a task of in-line citation generation"
2021.acl-long.166,D15-1045,0,0.0267093,"Consequently, researchers must devote significant energy to quickly understand how a new piece of research fits with a rapidly changing research landscape. Several lines of research seek to reduce this burden on scientists. Citation recommendation systems suggest references to relevant published work (McNee et al., 2002; Bhagavatula et al., 2018). Intent classification systems help determine the type and importance of a citation in a work (Valenzuela et al., 2015; Cohan et al., 2019). Summarization systems aim to help researchers more quickly understand the basic ideas in a piece of research (Cohan and Goharian, 2015; Yasunaga et al., 2019). We draw inspiration from these works as well as ∗ Equal contribution. broader challenges like explaining the connection between concurrent works or relating a new paper to those a reader is already familiar with. Automatically describing inter-document relationships could decrease the time researchers devote to literature review. For instance, explanations for a new paper can be personalized to a particular reader by relating the new work to ones they have read before. Further, such technology could be incorporated into writing assistance systems to help less experien"
2021.acl-long.166,P18-1082,0,0.0280026,"bed in this work, we train a version of S CI GPT2 only on documents appearing in the training data, so that the principal documents and target sentences in the test data are unseen by the language model. We provide this and a full-corpus version of S CI GPT2 as resources for future research.3 4.2 Retrieval with Approximate Nearest Neighbors While neural text generation techniques have advanced significantly in recent years, their outputs are still inferior to human authored texts. For some tasks, it is better to retrieve a relevant humanauthored text than to generate novel text automatically (Fan et al., 2018). Is this also the case when generating explanations? To answer this question, we use an information retrieval (IR) baseline. We adapt an approximate nearest neighbor search algorithm to find similar pairs of documents. The basic search procedure is as follows: Given a test instance input (S, C) for principal S and cited document C, we find the set NC , the nearest neighbors to C in the training data. For each document NC from NC , let NS be the set of documents that cite NC . This means that each NS ∈ NS contains at least one citing sentence t0 which cites NC . We use the t0 associated with t"
2021.acl-long.166,D18-1360,0,0.0280904,"f , w100 is used to represent the cited document to the S CI G EN model. 8 A quantitative analysis of this phenomenon is available in Appendix H. 2135 Candidate 1 Principal Intro SciGen Candidate 2 Candidate 2 Candidate 20 Cited Entity/Terms SciGEN input Intro x Entities/Terms Rank & Select Figure 3: Overview of S CI GEN using terms/entities. We generate a list of candidates and rank them according to mean reciprocal rank to the input entities. Entities We extract entities from abstracts with the DyGIE++ information extraction framework (Wadden et al., 2019) using the model trained on SciERC (Luan et al., 2018), a dataset of scientific document abstracts with entity and relation annotations.9 The extracted entities ei from the cited document are sorted by their tf-idf scores compared to all entities in the corpus. As above, a special token ξ e is used to concatenate entities and help the language model distinguish this list from conventional text. If there is additional room in the context window we append the unigrams with the highest tfidf to the end of the listed entities until the window is full. In that case, the cited document context X e is e1 , ξ e , e2 , ..., ξ e , en , ξ tf , w1 ξ tf , ..."
2021.acl-long.166,2020.acl-main.173,0,0.0238364,"The extracted entities ei from the cited document are sorted by their tf-idf scores compared to all entities in the corpus. As above, a special token ξ e is used to concatenate entities and help the language model distinguish this list from conventional text. If there is additional room in the context window we append the unigrams with the highest tfidf to the end of the listed entities until the window is full. In that case, the cited document context X e is e1 , ξ e , e2 , ..., ξ e , en , ξ tf , w1 ξ tf , ..., wm , where n is the number of entities and m is 100 − n. 6.1 Entity-Based Ranking Maynez et al. (2020) point out that summarization systems frequently struggle with factuality and generate hallucinations unfaithful to input documents. We observe this problem with some generated explanations as well: popular, topical terms like ‘CNN’ would appear in explanations of papers using LSTM models, for example. To combat hallucinations and promote factual accuracy we include a ranking mechanism that rewards generated explanations with higher coverage of important entities from the conditioning context.10 The process we use is as follows: first, we generate a large space of candidate explanations for a"
2021.acl-long.166,P03-1021,0,0.128331,"We measure the closeness of two pairs of documents using the cosine distances between vector representations of their abstracts. The abstract of each document is encoded as a single dense vector by averaging the contextualized embeddings provided by the SciBERT model of Beltagy et al. (2019) and normalizing. The distance between (S, C) and neighbors (NS , NC ) is computed as: α cos(S, NS ) + β cos(C, NC ) (2) where α and β control the relative contribution of the two document similarities. We explore setting both α and β to 1, or tuning them to optimize BLEU on the validation data using MERT (Och, 2003). 3 https://github.com/Kel-Lu/SciGen 5 Representing Documents with Sentence Selection Methods for the related task of citation recommendation have made use of abstracts, which perhaps act as sufficient summaries of document content. Building on this, we represent the principal and cited documents with the first 450 tokens of either their abstracts, introductions, or sentences randomly sampled from throughout the full document.4 In this section, we answer two questions: 1) do neural generation models with sentence-based context outperform the IR baseline and 2) does the type of sentence-based c"
2021.acl-long.166,P02-1040,0,0.110026,"ing human judgments in technical domains is relatively rare, we believe it to be an important step in evaluating our systems for this task. Thus, we conduct thorough human evaluations and analyses with expert judges. We make use of both larger scale expert evaluations yielding hundreds of judgements as well as smaller scale, deeper evaluations where we can effect a higher degree of quality control over fewer datapoints. Further, we make use of intermediate human evaluations in the development of our models, and supplement these evaluations with automatic metrics — BLEU 2132 Web Text Pretrain (Papineni et al., 2002) and ROUGE (Lin, 2004) that are established in other generation tasks. 4 SciGPT2 SciGEN Models We develop several models for explaining document relationships. Following current work in neural text generation, we finetune the predictions of a large pretrained language model to our task (Section 4.1). In order to bring the language model into the scientific text domain, we do additional language model pretraining over full scientific texts. We also investigate approximate nearest neighbor methods to retrieve plausible human-authored explanations from the training data as a baseline (Section 4.2"
2021.acl-long.166,D19-1005,1,0.831564,"the generated text and improved modeling of the cited document. Factual accuracy is difficult to enforce in language model-based text generation systems, especially where inference includes sampling procedures. The use of information extraction for contexts showed promise in Section 6; other methods of incorporating information like grounding to knowledge bases could help prune false or irrelevant statements. Combining knowledge graphs with language models and generation is an active research area that has shown promise in other domains (Bosselut et al., 2019; Koncel-Kedziorski et al., 2019; Peters et al., 2019). Applying this line of work to scientific text by modeling input documents as knowledge graphs of their content may help algorithms better understand the cited document, provide distant supervision for concurrent work, and result in better outputs. search groups for their participation in our study. We thank members of Noah’s ARK for their helpful comments and the anonymous reviewers for their feedback. 8 Arman Cohan, Waleed Ammar, Madeleine van Zuylen, and Field Cady. 2019. Structural scaffolds for citation intent classification in scientific publications. In NAACL-HLT. Conclusion We have de"
2021.acl-long.166,2020.emnlp-main.748,0,0.0290093,"ted data as we supervise with citing sentences that are readily available in scientific documents. In practice, citation classification is used to assist in suggesting relevant works to researchers; our work complements this goal by providing rationales for the recommendation and furthering progress toward explainable AI. Our work is also connected to a long history of research on summarizing scientific documents (Luhn, 1958; Paice, 1980). Work in this area has mostly used used abstracts or peer reviews as targets (Cachola et al., 2020; Cohan et al., 2018; Jaidka et al., 2017). In particular, Pilault et al. (2020) show that using a simple extractive summary as input for abstractive summarization of scholarly texts work well. Researchers have also used citing sentences as part of the input for summarization, recognizing the explanatory power of these texts (Nakov et al., 2004; Cohan and Goharian, 2017; Yasunaga et al., 2019). Ours is the first work to focus on learning to express the specific relationship between two documents from such sentences. The closest work to our own is Xing et al. (2020), who pilot a task of in-line citation generation. Their goal is a model which can insert a citing sentence i"
2021.acl-long.166,C08-1087,0,0.0739482,"Retrieve, Generate: A Simple Approach to Sentiment and Style Transfer The authors of Cited proposed a model that combines neural generation with user interaction to create an objectcentric reading experience. Table 5: Example explanations. The given texts are the document titles and the S CI G EN outputs. In the last example, the two documents do not cite each other. are missing in our training data. In addition to missing some relationships, not all citation sentences are useful as explanations. As pointed out by other work, citation sentences can often be simple summaries of the cited work (Qazvinian and Radev, 2008; Cohan and Goharian, 2017). Alternatively, they can be too specific to be useful, as seen in Output 1, where a higher-level summary might be more useful. Future work could focus on curating better training sets for our task. It is notable that the S CI G EN model usually out2137 puts syntactically correct and topical explanations, even given the difficulty of the vocabulary in this domain. This is consistent with many recent findings using domain-specific language models. The fluency and appropriateness of S CI G EN’s generations shows the promise of generating explanations which accurately c"
2021.acl-long.427,2020.acl-main.270,1,0.884529,"Sliding window, stride S = 1 . Here, after the first inference pass we ignore all outputs other than the last (§2). (c) Caching (§5.2) where each subsequence attends to representations of the previous one. (In the next iteration, tokens d, e and f become the cache, with P.E. 1, 2 and 3, the three new tokens get P.E. 4, 5, and 6.) Experimental Setup Our baseline is the Baevski and Auli (2018) model, henceforth B & A, trained and evaluated on WikiText-103 (Merity et al., 2016). We use this baseline because of its prominent role in recent language modeling developments (Khandelwal et al., 2020; Press et al., 2020). The training set contains 103.2 million tokens from English Wikipedia. The B & A model has 16 transformer layers of dimension 1,024, with 8 heads in each self-attention sublayer, and feedforward sublayers with an inner dimension of 4,096. This model ties the word embedding and softmax matrices (Press and Wolf, 2017; Inan et al., 2017) and uses sinusoidal position embeddings. It has a subsequence length of 3,072 tokens and achieves a perplexity of 18.65 ± 0.24 (std. dev.) on the development set. In our experiments, other than varying the subsequence length, we modify no other hyperparameters,"
2021.acl-long.427,E17-2025,1,0.764592,". 4, 5, and 6.) Experimental Setup Our baseline is the Baevski and Auli (2018) model, henceforth B & A, trained and evaluated on WikiText-103 (Merity et al., 2016). We use this baseline because of its prominent role in recent language modeling developments (Khandelwal et al., 2020; Press et al., 2020). The training set contains 103.2 million tokens from English Wikipedia. The B & A model has 16 transformer layers of dimension 1,024, with 8 heads in each self-attention sublayer, and feedforward sublayers with an inner dimension of 4,096. This model ties the word embedding and softmax matrices (Press and Wolf, 2017; Inan et al., 2017) and uses sinusoidal position embeddings. It has a subsequence length of 3,072 tokens and achieves a perplexity of 18.65 ± 0.24 (std. dev.) on the development set. In our experiments, other than varying the subsequence length, we modify no other hyperparameters, including the random seed and number of training epochs (205). 3 How Does Context Window Size Affect Transformers? Segmenting a corpus into subsequences results in different effective context windows for different timesteps depending on where they fall in a segment. Subsequence length L is an upper bound on the effe"
2021.acl-long.427,N18-2074,0,0.0309263,"aatar et al. (2019) learns the maximum effective context window sizes for each head at each layer independently. Like in our method, context window sizes are smaller at the start of training and lengthen as training progresses. We show that a simple approach of manually choosing two subsequence lengths is highly effective. In addition, keeping subsequence lengths equal across all heads and layers lets us save memory and runtime. Position-Infused Attention TransformerXL (Dai et al., 2019) caches and attends to previous representations using an attention sublayer that uses relative positioning (Shaw et al., 2018). It runs much slower than the unmodified attention sublayer, requires extra parameters, and requires internally modifying the self-attention sublayer, while our PIA method (§5) does not. In parallel with our work, Ke et al. (2020) compute attention coefficients by summing two attention matrices, one based on position-position interactions and the other on content-content interactions. As in PIA, they do not add position embeddings at the bottom of the model. They present results only for BERT, which uses much smaller subsequences than our models. 8 Conclusion Our results challenge the convent"
2021.acl-long.427,P19-1032,0,0.104515,"Missing"
2021.acl-long.522,2020.findings-emnlp.301,1,0.467449,"on both automatic and human evaluations. Moreover, because DE XPERTS operates only on the output of the pretrained LM, it is effective with (anti-)experts of smaller size, including when operating on GPT-3. Our work highlights the promise of tuning small LMs on text with (un)desirable attributes for efficient decoding-time steering. 1 Introduction Controlling the output of pretrained language models (LMs) is crucial for achieving useful and safe language generation applications, such as nonoffensive sentence completion or friendly conversation generation (See et al., 2019; Sheng et al., 2020; Gehman et al., 2020). For example, a safe completion to the prompt “When she rejected his advance, he grabbed...” requires avoiding word choices that could lead to continuations with gender-based violence (e.g., “her”; Figure 1). Without such steering, these language models risk generating mindless and offensive content (Sheng et al., 2019; Holtzman et al., 2020) which hinders their safe deployment (Brockman et al., 2020; Bender et al., 2021). Importantly, as the scale of pretrained LMs increases (e.g., 175B and 1.6T parameters; Brown et al., 2020; Fedus et al., Figure 1: Illustration of DE XPERTS, where a toxic"
2021.acl-long.522,P17-1059,0,0.023953,"α ą 0 indicates positive rewriting, and α ă 0 indicates negative rewriting. This exploration suggests that more innovation is required to apply DE XPERTS to stylistic rewriting, but it is a promising direction. We anticipate future work on the subject. 6 Related Work The task of controlling the output of a language generation model has been widely studied by previous work (for a review, see Prabhumoye et al., 2020). Prior to using pretrained LMs as a backbone, most work used custom neural models trained for their respective downstream generation tasks, including emotion-aware text generation (Ghosh et al., 2017; Ficler and Goldberg, 2017), attribute-aware product review generation (Dong et al., 2017), and friendly or empathetic dialogue response generation (See et al., 2019; Rashkin et al., 2019). Since pretrained LMs have shown impressive text generation ability (Radford et al., 2018, 2019), two directions have emerged to control their language generation: training approaches and decoding-time approaches. Training approaches include finetuning the pretrained LMs on datasets that contain the desired attributes (Gururangan et al., 2020) as well as creating a class-conditioned pretrained LM trained on"
2021.acl-long.565,2020.emnlp-main.525,0,0.461254,"Missing"
2021.acl-long.565,C18-1281,0,0.0513987,"Missing"
2021.acl-long.565,E06-1040,0,0.581383,"an (left) or a machine (right). The evaluators point to a wide range of text attributes to make their decisions, sometimes using the same aspect of the text to come to opposite conclusions. Introduction Human-quality text has long been a holy grail for the output of natural language generation (NLG) systems, serving as an upper bound on their performance. Since we lack a good way of encoding many aspects of what constitutes human-quality output in an automated method, we often must rely on human evaluation for our models. Though evaluations with end-users in an applied setting are encouraged (Belz and Reiter, 2006), in practice, most human evaluations instead ask people to rate generated text’s intrinsic quality (van der Lee et al., 2019; Howcroft et al., 2020). Sometimes the generated text is explicitly compared to human-authored text (e.g., Liu et al., 2016; Zellers et al., 2021; Zhang et al., 2020), but even when no human-authored text is evaluated, evaluators implicitly compare the generated text to their knowledge of language and norms within specific domains. Evaluators are often asked to assess a text holistically, e.g., based on its overall quality, naturalness, or humanlikeness (van der Lee et"
2021.acl-long.565,2020.inlg-1.24,0,0.0562853,"Missing"
2021.acl-long.565,2020.inlg-1.4,0,0.0993079,"Missing"
2021.acl-long.565,W07-0718,0,0.103846,"Zhang et al., 2020), but even when no human-authored text is evaluated, evaluators implicitly compare the generated text to their knowledge of language and norms within specific domains. Evaluators are often asked to assess a text holistically, e.g., based on its overall quality, naturalness, or humanlikeness (van der Lee et al., 2021; Howcroft et al., 2020), where the exact evaluation criteria is left to the discretion of the evaluator. Though other evaluations are broken down along specific dimensions of text quality (e.g., grammaticality, coherence, etc.), Novikova et al. (2017, 2018) and Callison-Burch et al. (2007) found that these dimensions are often correlated and may be conflated in some evaluation settings. This is con7282 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 7282–7296 August 1–6, 2021. ©2021 Association for Computational Linguistics cerning because, as NLG models improve, evaluators are asked to read longer passages of text conditioned on large amounts of context. In these cases, fluency-related aspects of quality (i.e., the ones that don’t require careful reading of"
2021.acl-long.565,2020.emnlp-demos.25,0,0.0614612,"Missing"
2021.acl-long.565,P18-1082,0,0.065584,"g texts are in the supplementary materials and at ark.cs.washington.edu/ human_evals_ACL21). While this setting is not typically how GPT2 is used in practice, we held this approach constant to directly compare how model quality changes evaluators’ ability to distinguish between texts. For each domain, each generated text was conditioned on the same set of priming texts. The texts were delimited with an hEOSi token and generated using the default GPT3 generation settings (i.e., sampling with temperature = 0.7). 2.2.1 Stories The human-authored texts came from the Reddit WritingPrompts dataset (Fan et al., 2018).4 We collected all the stories that began with Once upon 3 Using NLTK; www.nltk.org/ github.com/pytorch/fairseq/tree/ master/examples/stories News Articles We collected 2,111 recent local news articles from 15 different newspapers using Newspaper3k5 (details in Appendix A.1). After filtering out articles under 100 words, we manually filtered out articles that weren’t local news or that referenced the coronavirus pandemic. We randomly chose 50 articles to use as our human-authored news articles and another 50 to use as prompts for our generation models. We conditioned each generated text on th"
2021.acl-long.565,N19-1169,0,0.0468837,"Missing"
2021.acl-long.565,2020.inlg-1.23,0,0.0721598,"Missing"
2021.acl-long.565,2020.acl-main.164,0,0.264349,"of NLG models (§5). We also encourage practitioners to consider alternative evaluation frameworks that capture the usefulness of generated text in downstream settings rather than its humanlikeness. 2 How well can untrained evaluators identify machine-generated text? In our first study, we ask how well untrained evaluators can distinguish between human- and machinegenerated text. This task format, inspired by the Turing (1950) Test, is used to compare the quality of machine-generated text to human-authored text and, as models’ fluency improves, to analyze NLG models’ ability to “fool” readers (Ippolito et al., 2020; Brown et al., 2020). By asking evaluators to assess the humanlikeness of the text with only minimal instructions (see Figure 2), we observe how well untrained evaluators can detect state-of-the-art machine-generated text and which attributes evaluators focus on and think are important for detecting machine-generated text. 2.1 The Task We gave evaluators 5 text passages, some of which were written by people and some generated by a model. We asked them to rate the text on a 4-point scale (Ippolito et al., 2020): 1. 2. 3. 4. Definitely human-written Possibly human-written Possibly machine-gener"
2021.acl-long.565,D17-1238,0,0.0600028,"Missing"
2021.acl-long.565,N18-2012,0,0.0442741,"Missing"
2021.acl-long.565,2020.evalnlgeval-1.2,0,0.0603899,"Missing"
2021.acl-long.565,2020.emnlp-main.673,0,0.0926312,"Missing"
2021.acl-long.565,P19-1472,0,0.0514632,"Missing"
2021.acl-long.565,2021.naacl-main.386,1,0.893484,"l language generation (NLG) systems, serving as an upper bound on their performance. Since we lack a good way of encoding many aspects of what constitutes human-quality output in an automated method, we often must rely on human evaluation for our models. Though evaluations with end-users in an applied setting are encouraged (Belz and Reiter, 2006), in practice, most human evaluations instead ask people to rate generated text’s intrinsic quality (van der Lee et al., 2019; Howcroft et al., 2020). Sometimes the generated text is explicitly compared to human-authored text (e.g., Liu et al., 2016; Zellers et al., 2021; Zhang et al., 2020), but even when no human-authored text is evaluated, evaluators implicitly compare the generated text to their knowledge of language and norms within specific domains. Evaluators are often asked to assess a text holistically, e.g., based on its overall quality, naturalness, or humanlikeness (van der Lee et al., 2021; Howcroft et al., 2020), where the exact evaluation criteria is left to the discretion of the evaluator. Though other evaluations are broken down along specific dimensions of text quality (e.g., grammaticality, coherence, etc.), Novikova et al. (2017, 2018) and"
2021.eacl-main.274,2020.findings-emnlp.301,1,0.840062,"of social biases or toxicity (e.g., Social Bias Frames; Sap et al., 2020). Ethical Implications & Limitations The above synthetic setting is meant to illustrate the role of labeling quality on biases in annotations. We strongly caution against using this approach in real-world applications, such as building parallel datasets for dialects. First, due to how its training data was selected, GPT-3 has likely not been exposed to many African American English varieties during training (Jo and Gebru, 2020). Second, pretrained language models are known to generate toxic language at non-trivial rates (Gehman et al., 2020), which could cause differential toxicity in the translations. 7 Related Work Debiasing Toxicity Detection As the popularity of hate speech and toxic language detection sys3150 AAE GPT-3 WAE Translation RT @user I can’t stand a bad texter bruh like don’t be mad if I forget about yo ass RT @user Retweet if you fuck with this!!!! RT @user That nigga needs anger management RT @user oh fucking hell take a day off man RT @user I can’t stand a bad texter bro like don’t be mad if I forget about you RT @user Retweet if you like this! RT @user That guy needs anger management RT @user oh fuck take a day"
2021.eacl-main.274,2020.emnlp-main.473,0,0.377726,"Missing"
2021.eacl-main.274,N18-2017,1,0.849901,"asks that debiasing methods have been successful on, such as textual entailment (e.g., SNLI, MNLI; Bowman et al., 2015; Williams et al., 2018) or reading comprehension (e.g., SQuAD; Rajpurkar et al., 2016). First, compared to these NLU tasks where there is one correct label, the toxicity of language is inherently more nuanced, subjective, and contextual, which causes toxic language datasets to have lower agreement in general (Ross et al., 2017). Second, the dataset biases in NLU are predominantly artifacts introduced during data creation (e.g., negations, exaggerations; Schwartz et al., 2017; Gururangan et al., 2018), whereas those in toxic language detection are grounded in the social dynamics of the world (Spears, 1998; Technau, 2018). For example, viewing AAE as a more toxic or less proper variety of English is a form of linguistic discrimination that upholds racial hierarchies in the United States (Rosa and Flores, 2017). In this work, we consider two broad categories of toxic language dataset biases—lexical (§2.1) and dialectal (§2.2). Our experiments focus on a single, widely used dataset (§2.3) from Founta et al. (2018). 2.1 Lexical Biases (T OX T RIG) Current toxic language detection systems often"
2021.eacl-main.274,D19-6115,0,0.102916,"inan et al., 2019) and dialectal bias, where toxicity is correlated with surface markers of African American English (AAE; Davidson et al., 2019; Sap et al., 2019). When trained on biased datasets, models acquire and exacerbate these biases (e.g., flagging text by Black authors as more toxic than by white authors; Sap et al., 2019; Zhang et al., 2018). Concurrently, there has been elevated interest in developing debiasing methods for standard natural language understanding (NLU) tasks, i.e., methods that aim to decrease over-reliance on spurious correlations in NLU models (Clark et al., 2019; He et al., 2019; Karimi Mahabadi et al., 2020; Bras et al., 2020). This raises a natural question: are 3143 Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics, pages 3143–3155 April 19 - 23, 2021. ©2021 Association for Computational Linguistics current debiasing approaches effective for mitigating biases specific to toxic language detection? In this work, we address the above question by investigating two classes of debiasing approaches to mitigate lexical and dialectal biases—one that employs additional training objectives for bias removal, and anothe"
2021.eacl-main.274,2020.acl-main.769,0,0.207937,"ialectal bias, where toxicity is correlated with surface markers of African American English (AAE; Davidson et al., 2019; Sap et al., 2019). When trained on biased datasets, models acquire and exacerbate these biases (e.g., flagging text by Black authors as more toxic than by white authors; Sap et al., 2019; Zhang et al., 2018). Concurrently, there has been elevated interest in developing debiasing methods for standard natural language understanding (NLU) tasks, i.e., methods that aim to decrease over-reliance on spurious correlations in NLU models (Clark et al., 2019; He et al., 2019; Karimi Mahabadi et al., 2020; Bras et al., 2020). This raises a natural question: are 3143 Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics, pages 3143–3155 April 19 - 23, 2021. ©2021 Association for Computational Linguistics current debiasing approaches effective for mitigating biases specific to toxic language detection? In this work, we address the above question by investigating two classes of debiasing approaches to mitigate lexical and dialectal biases—one that employs additional training objectives for bias removal, and another that filters training instan"
2021.eacl-main.274,D19-1461,0,0.245324,"g majority identity mentions, as illustrated in Figure 1. At the core of the issue are dataset biases, i.e., spurious correlations between surface patterns and annotated toxicity labels (§2), which stem from the data creation process (Sap et al., 2019). Previous work has outlined two such biases for hate 1 We use hate speech and toxic language interchangeably in this work, though their definitions do not perfectly align. speech datasets (both shown in Figure 1): lexical bias which associates toxicity with the presence of certain words (e.g., profanities, identity mentions; Dixon et al., 2018; Dinan et al., 2019) and dialectal bias, where toxicity is correlated with surface markers of African American English (AAE; Davidson et al., 2019; Sap et al., 2019). When trained on biased datasets, models acquire and exacerbate these biases (e.g., flagging text by Black authors as more toxic than by white authors; Sap et al., 2019; Zhang et al., 2018). Concurrently, there has been elevated interest in developing debiasing methods for standard natural language understanding (NLU) tasks, i.e., methods that aim to decrease over-reliance on spurious correlations in NLU models (Clark et al., 2019; He et al., 2019; K"
2021.eacl-main.274,2021.ccl-1.108,0,0.0951961,"Missing"
2021.eacl-main.274,D18-1302,0,0.0149317,"ke a day off man Gold New A ¨ A ¨ A ¨ A A Table 6: Examples of AAE tweets with their GPT-3 based WAE translation, and original gold standard and new annotations based on AAE-relabeled. For the first three tweets, the (biased) gold labels are changed by models predicting the new labels on their WAE translations. indicates presence of toxicity, and ¨ represents nontoxic. We anonymize the usernames to protect user privacy. A tems has grown, several biases have been found in dataset and models, spurring various debiasing efforts to mitigate these individual biases (e.g., gender bias, racial bias; Park et al., 2018; Sap et al., 2019; Davidson et al., 2019). Some work tackles identity-based biases, e.g., using data re-balancing (Dixon et al., 2018), or adversarial feature learning (Vaidya et al., 2019). Less work has tackled racial or dialectal bias. Notably, Xia et al. (2020) use adversarial training to prevent the model from associating toxicity with AAE, showing only small improvements in fairness. Based on those results, we do not explore adversarial methods, opting instead for ensemble-based methods of predefined bias reduction. In contemporary work, Mozafari et al. (2020) use a re-weighting mechani"
2021.eacl-main.274,C18-1130,0,0.0655929,"Missing"
2021.eacl-main.274,D16-1264,0,0.0299616,"goal of moderating online communities (Roberts, 2019; Vidgen et al., 2019). 2 https://github.com/XuhuiZhou/Toxic_ Debias 3 Our definition of “bias” is specific to the social biases in toxic language detection datasets, grounded as lexical and dialectal biases; see Blodgett et al. (2020) for a detailed investigation of the term “bias”. This task differs in several ways from the natural language understanding (NLU) tasks that debiasing methods have been successful on, such as textual entailment (e.g., SNLI, MNLI; Bowman et al., 2015; Williams et al., 2018) or reading comprehension (e.g., SQuAD; Rajpurkar et al., 2016). First, compared to these NLU tasks where there is one correct label, the toxicity of language is inherently more nuanced, subjective, and contextual, which causes toxic language datasets to have lower agreement in general (Ross et al., 2017). Second, the dataset biases in NLU are predominantly artifacts introduced during data creation (e.g., negations, exaggerations; Schwartz et al., 2017; Gururangan et al., 2018), whereas those in toxic language detection are grounded in the social dynamics of the world (Spears, 1998; Technau, 2018). For example, viewing AAE as a more toxic or less proper v"
2021.eacl-main.274,N18-1101,0,0.0225781,"fensive, hateful, or toxic language on the internet, with the goal of moderating online communities (Roberts, 2019; Vidgen et al., 2019). 2 https://github.com/XuhuiZhou/Toxic_ Debias 3 Our definition of “bias” is specific to the social biases in toxic language detection datasets, grounded as lexical and dialectal biases; see Blodgett et al. (2020) for a detailed investigation of the term “bias”. This task differs in several ways from the natural language understanding (NLU) tasks that debiasing methods have been successful on, such as textual entailment (e.g., SNLI, MNLI; Bowman et al., 2015; Williams et al., 2018) or reading comprehension (e.g., SQuAD; Rajpurkar et al., 2016). First, compared to these NLU tasks where there is one correct label, the toxicity of language is inherently more nuanced, subjective, and contextual, which causes toxic language datasets to have lower agreement in general (Ross et al., 2017). Second, the dataset biases in NLU are predominantly artifacts introduced during data creation (e.g., negations, exaggerations; Schwartz et al., 2017; Gururangan et al., 2018), whereas those in toxic language detection are grounded in the social dynamics of the world (Spears, 1998; Technau, 2"
2021.eacl-main.274,P19-1163,1,0.893203,"or toxic language detection1 systems exhibit problematic and discriminatory behavior that causes them to have disparate negative impact on minority populations (Yasin, 2018; Guynn, 2020; Kim et al., 2020; Dias Oliva et al., 2020). Tweets simply containing a minority identity mention are commonly flagged as toxic by current systems, in contrast to those containing majority identity mentions, as illustrated in Figure 1. At the core of the issue are dataset biases, i.e., spurious correlations between surface patterns and annotated toxicity labels (§2), which stem from the data creation process (Sap et al., 2019). Previous work has outlined two such biases for hate 1 We use hate speech and toxic language interchangeably in this work, though their definitions do not perfectly align. speech datasets (both shown in Figure 1): lexical bias which associates toxicity with the presence of certain words (e.g., profanities, identity mentions; Dixon et al., 2018; Dinan et al., 2019) and dialectal bias, where toxicity is correlated with surface markers of African American English (AAE; Davidson et al., 2019; Sap et al., 2019). When trained on biased datasets, models acquire and exacerbate these biases (e.g., fla"
2021.eacl-main.274,2020.acl-main.486,1,0.881032,"Missing"
2021.eacl-main.274,2020.socialnlp-1.2,0,0.419147,"ctives for bias removal, and another that filters training instances likely exhibiting spurious biases (§3). Through comprehensive experiments, we show that both approaches face major challenges in mitigating biases from a model trained on a biased dataset (in our case, the dataset from Founta et al., 2018) for toxic language detection. While data filtering results in reduced bias associations in the data, models trained on filtered datasets still pick up on lexical (§4) and dialectal biases (§5). We find that dialectal biases are particularly challenging to address, as has also been shown by Xia et al. (2020). “Debiased” models still disproportionately flag text in certain dialects as toxic. Notably, mitigating dialectal bias through current debiasing methods does not mitigate a model’s propensity to label tweets by Black authors as more toxic than by white authors. We additionally explore an alternative proof-ofconcept study—relabeling supposedly toxic training instances whose automatic translations into a majority dialect are deemed non-toxic by the classifier. To this end, we create a synthetic dataset via few-shot dialect translation system built with GPT3 (Brown et al., 2020). While only an i"
2021.eacl-main.274,K17-1004,1,0.843201,"e understanding (NLU) tasks that debiasing methods have been successful on, such as textual entailment (e.g., SNLI, MNLI; Bowman et al., 2015; Williams et al., 2018) or reading comprehension (e.g., SQuAD; Rajpurkar et al., 2016). First, compared to these NLU tasks where there is one correct label, the toxicity of language is inherently more nuanced, subjective, and contextual, which causes toxic language datasets to have lower agreement in general (Ross et al., 2017). Second, the dataset biases in NLU are predominantly artifacts introduced during data creation (e.g., negations, exaggerations; Schwartz et al., 2017; Gururangan et al., 2018), whereas those in toxic language detection are grounded in the social dynamics of the world (Spears, 1998; Technau, 2018). For example, viewing AAE as a more toxic or less proper variety of English is a form of linguistic discrimination that upholds racial hierarchies in the United States (Rosa and Flores, 2017). In this work, we consider two broad categories of toxic language dataset biases—lexical (§2.1) and dialectal (§2.2). Our experiments focus on a single, widely used dataset (§2.3) from Founta et al. (2018). 2.1 Lexical Biases (T OX T RIG) Current toxic langua"
2021.eacl-main.274,2020.emnlp-main.746,1,0.849539,"Missing"
2021.eacl-main.274,2020.acl-main.770,0,0.0157437,"marily on the quality of the underlying data for hate speech detection, such as accounting for speaker identity and dialect. Indeed, such efforts could act as an important step towards making systems less discriminatory, and hence safe and usable. Other General Debiasing Methods Several approaches for debiasing NLU tasks have been proposed lately. Some approaches rely on adversarial training to remove protected attributes (e.g. gender or race), from a model’s internal representations (Zhang et al., 2018; Wang et al., 2019; Xia et al., 2020). Other approaches include confidence regularization (Utama et al., 2020), as well as other product of expert approaches (He et al., 2019; Karimi Mahabadi et al., 2020) similar to the debiased training approach from Clark et al. (2019), which is the only debiased training we employ due to its relatively strong performance. We thank the anonymous reviewers and Laura Vianna for helpful comments on this work. This research was supported in part by NSF grants 1813153 and 1714566. 8 Conclusion We investigate whether toxic language detection systems can be debiased using recently introduced methods for debiasing text classification in NLU Acknowledgments References Su Li"
2021.emnlp-main.133,2020.emnlp-main.576,0,0.239464,", 2019b). Similarly, Shibata et al. (2020) found that LSTM language models trained on natural language data acquire saturated representations approximating counters. Recent work extends saturation analysis to transformers (Merrill, 2019; Merrill et al., 2020). Saturated attention heads reduce to generalized hard attention, where the attention scores can tie. In the case of ties, the head output averages the positions with maximal scores.3 While their power is not fully understood, saturated transformers can implement a counting mechanism similarly to LSTMs (Merrill et al., 2020). In practice, Bhattamishra et al. (2020) show transformers can learn tasks requiring counting, and that they struggle when more complicated structural representations are required. Ebrahimi et al. (2020) find that attention patterns of certain heads can emulate bounded stacks, but that this ability falls off sharply for longer sequences. Thus, the abilities of trained LSTMs and transformers appear to be predicted by the classes of problems solvable by their saturated counterparts. Merrill et al. (2020) conjecture that the saturated capacity might represent a class of tasks implicitly learnable by GD, but it is unclear a priori why t"
2021.emnlp-main.133,D19-1223,0,0.0631386,"Missing"
2021.emnlp-main.133,J93-2004,0,0.0741644,"t of Fig. 1 breaks down the growth trend by layer. Generally, the norm grows more quickly in later layers than in earlier √ ones, although always at a rate proportional to t.5 Next, in the bottom row of Fig. 1, we plot the cosine similarity between each parameter checkpoint θt+1 and its predecessor θt . This rapidly approaches 1, suggesting the “direction” of the parameters (θt /kθt k) converges. The trend in directional convergence looks similar across layers. We also train smaller transformer language models with 38M parameters on Wikitext-2 (Merity et al., 2016) and the Penn Treebank (PTB; Marcus et al., 1993). We consider two variants of the transformer: pre-norm and post-norm, which vary in the relative order of layer normalization and residual connections (cf. Xiong et al., 2020). Every model exhibits norm growth over training.6 Combined, these results provide evidence that the parameter norm of transformers tends to grow over the course of training. In the remainder of this paper, we will discuss the implications of this phenomenon for the linguistic biases of transformers, and then discuss potential causes of the trend rooted in the optimization dynamics. 4 Effect of Norm Growth §3 empirically"
2021.emnlp-main.133,W19-3901,1,0.912325,"are understandable in terms of formal languages and automata. Empirically, we find that internal 1 Introduction representations of pretrained transformers approxiTransformer-based models (Vaswani et al., 2017) mate their saturated counterparts, but for randomly like BERT (Devlin et al., 2019), XLNet (Yang et al., initialized transformers, they do not. This suggests that the norm growth implicit in training guides 2019), RoBERTa (Liu et al., 2019), and T5 (Raffel transformers to approximate saturated networks, et al., 2019) have pushed the state of the art on an justifying studying the latter (Merrill, 2019) as a impressive array of NLP tasks. Overparameterized way to analyze the linguistic biases of NLP architransformers are known to be unversal approximators (Yun et al., 2020), suggesting their general- tectures and the structure of their representations. ization performance ought to rely on useful biases Past work (Merrill, 2019; Bhattamishra et al., or constraints imposed by the learning algorithm. 2020) reveals that saturation permits two useful Despite various attempts to study these biases in types of attention heads within a transformer: one 1766 Proceedings of the 2021 Conference on Empi"
2021.emnlp-main.133,2020.acl-main.43,1,0.830634,"f particular interest for Our main contribution is analyzing the effect NLP. We leverage the emergent discrete strucof norm growth on the representations within the ture in a saturated transformer to analyze the transformer (§4), which control the network’s gramrole of different attention heads, finding that matical generalization. With some light assumpsome focus locally on a small number of potions, we prove that any network where the paramesitions, while other heads compute global avter norm diverges during training approaches a saterages, allowing counting. We believe underurated network (Merrill et al., 2020): a restricted standing the interplay between these two capabilities may shed further light on the structure network variant whose discretized representations of computation within large transformers. are understandable in terms of formal languages and automata. Empirically, we find that internal 1 Introduction representations of pretrained transformers approxiTransformer-based models (Vaswani et al., 2017) mate their saturated counterparts, but for randomly like BERT (Devlin et al., 2019), XLNet (Yang et al., initialized transformers, they do not. This suggests that the norm growth implicit i"
2021.emnlp-main.133,W19-3905,0,0.0187455,"in its recurrent memory (Kirov and Frank, 2012). Stacks are useful for processing compositional structure in linguistic data 1767 2 The limit over f is taken pointwise. The range of sf is R. (Chomsky, 1956), e.g., for semantic parsing. However, a saturated LSTM does not have enough memory to simulate a stack (Merrill, 2019). Rather, saturated LSTMs resemble classical counter machines (Merrill, 2019): automata limited in their ability to model hierarchical structure (Merrill, 2020). Experiments suggest that LSTMs trained on synthetic tasks learn to implement counter memory (Weiss et al., 2018; Suzgun et al., 2019a), and that they fail on tasks requiring stacks and other deeper models of structure (Suzgun et al., 2019b). Similarly, Shibata et al. (2020) found that LSTM language models trained on natural language data acquire saturated representations approximating counters. Recent work extends saturation analysis to transformers (Merrill, 2019; Merrill et al., 2020). Saturated attention heads reduce to generalized hard attention, where the attention scores can tie. In the case of ties, the head output averages the positions with maximal scores.3 While their power is not fully understood, saturated tran"
2021.emnlp-main.133,P18-2117,1,0.834048,"TM encoding a stack in its recurrent memory (Kirov and Frank, 2012). Stacks are useful for processing compositional structure in linguistic data 1767 2 The limit over f is taken pointwise. The range of sf is R. (Chomsky, 1956), e.g., for semantic parsing. However, a saturated LSTM does not have enough memory to simulate a stack (Merrill, 2019). Rather, saturated LSTMs resemble classical counter machines (Merrill, 2019): automata limited in their ability to model hierarchical structure (Merrill, 2020). Experiments suggest that LSTMs trained on synthetic tasks learn to implement counter memory (Weiss et al., 2018; Suzgun et al., 2019a), and that they fail on tasks requiring stacks and other deeper models of structure (Suzgun et al., 2019b). Similarly, Shibata et al. (2020) found that LSTM language models trained on natural language data acquire saturated representations approximating counters. Recent work extends saturation analysis to transformers (Merrill, 2019; Merrill et al., 2020). Saturated attention heads reduce to generalized hard attention, where the attention scores can tie. In the case of ties, the head output averages the positions with maximal scores.3 While their power is not fully under"
2021.emnlp-main.135,N19-1300,0,0.0843142,"s us a principled way to decide between edit models). Many works have tried to remove data artifacts by making minimal changes to existing data (Shekhar 5.1 Local Edits in Practice et al., 2017; Sennrich, 2017; Zhao et al., 2018, inter alia). In this section we show that this kind of data We empirically investigate the effectiveness of loaugmentation can be effective with an appropri- cal edits for reducing single feature artifacts using 1806 Artifact statistics for BoolQ before and after local edits locally edited samples generated from two datasets: (1) the Boolean Questions dataset (BoolQ; Clark et al., 2019a), which consists of pairs of paragraphs (p) and questions (q), where each q has a binary answer that can be found by reasoning over p; and (2) IMDb (Maas et al., 2011), a sentiment classification dataset in the domain of movie reviews. We define each feature xi as the occurrence of a particular word within q for BoolQ, and within the text of the review for IMDb. Gardner et al. (2020) generated additional data for BoolQ and IMDb by making local edits to the question or review text and recording the updated binary label. 5.2 Local Edits and Boolean Sensitivity In the above discussion we used t"
2021.emnlp-main.135,D19-1418,0,0.115214,"s us a principled way to decide between edit models). Many works have tried to remove data artifacts by making minimal changes to existing data (Shekhar 5.1 Local Edits in Practice et al., 2017; Sennrich, 2017; Zhao et al., 2018, inter alia). In this section we show that this kind of data We empirically investigate the effectiveness of loaugmentation can be effective with an appropri- cal edits for reducing single feature artifacts using 1806 Artifact statistics for BoolQ before and after local edits locally edited samples generated from two datasets: (1) the Boolean Questions dataset (BoolQ; Clark et al., 2019a), which consists of pairs of paragraphs (p) and questions (q), where each q has a binary answer that can be found by reasoning over p; and (2) IMDb (Maas et al., 2011), a sentiment classification dataset in the domain of movie reviews. We define each feature xi as the occurrence of a particular word within q for BoolQ, and within the text of the review for IMDb. Gardner et al. (2020) generated additional data for BoolQ and IMDb by making local edits to the question or review text and recording the updated binary label. 5.2 Local Edits and Boolean Sensitivity In the above discussion we used t"
2021.emnlp-main.135,2020.findings-emnlp.272,0,0.46513,"Missing"
2021.emnlp-main.135,W95-0103,0,0.378446,"each appearOur null hypothesis is that the binomial proportion ance in an instance as a separate occurrence5 for pb (y |xi ) = 0.5 = p0 , or equivalently, that ri = 0. 4 The use of a z-statistic depends on the normal approxiOur alternative hypothesis is that pb (y |xi ) ≥ 0.5. mation to a binomial distribution, which holds for large n. 5 Let pˆ be the observed probability. We can compute We remove punctuation and tokenize on whitespace only. 1804 UD English Web Treebank Next we turn to dependency parsing. In particular, we focus on the classic problem of prepositional phrase (PP) attachment (Collins and Brooks, 1995), which involves determining whether a PP attaches to a verb (e.g., We ate spaghetti with forks) or a noun (e.g., We ate spaghetti with meatballs). We heuristically extract (verb, noun, prepositional phrase) constructions with ambiguous attachment from the UD English Web Treebank (EWT) training data.7 We treat (verb, preposition) tuples as features and attachment types (noun or verb) as labels, and we compute a z-statistic for each tuple. Figure 2 shows the z-statistic for each tuple that appears 10 or more times in the data. We labeled tuples that also appear in the locally edited samples fro"
2021.emnlp-main.135,W19-3504,0,0.0659607,"y problems, we need data that accurately reflects the competency assumption, both to evaluate systems and (presumably) to train them. However, humans suffer from blind spots, social bias, priming, and other psychological effects that make collecting data for competency problems challenging. Examples of these effects include instructions in a crowdsourcing task that prime workers to use particular language,3 or distributional effects in source material, such as the “amazing” examples above, or racial bias in face recognition (Buolamwini and Gebru, 2018) and abusive language detection datasets (Davidson et al., 2019; Sap et al., 2019). In order to formally analyze the impact of human bias on collecting data for competency problems, we need a plausible model of this bias. We represent bias as rejection sampling from the target competency distribution based on single feature values. Specifically, we assume the following dataset collection procedure. First, a person samples an instance from an unbiased distribution pu (x, y) where the competency assumption holds. The person examines this instance, and if feature xi = 1 appears with label y = 0, the person rejects the instance and samples a new one, with pro"
2021.emnlp-main.135,N19-1246,1,0.886483,"Missing"
2021.emnlp-main.135,D19-1107,0,0.159203,"Missing"
2021.emnlp-main.135,N18-2017,1,0.738595,"Missing"
2021.emnlp-main.135,2020.insights-1.13,0,0.0362039,"ses some additional empirical evidence for models’ reliance on these artifacts. 5 Mitigating Artifacts with Local Edits ately sensitive edit model, where sensitivity refers to how often a change to inputs results in the label changing. However, because humans are involved in making these changes, achieving appropriate sensitivity is challenging, and bias in this process can lead to the introduction of new artifacts. This suggests that care must be taken when performing edit-based data augmentation, as large edited training datasets are not likely to be artifact-free (cf. Tafjord et al., 2019; Huang et al., 2020). Imagine a new dataset De consisting of samples x0 , y 0 generated by making local edits according to the following repeated procedure: 1. Randomly sample an instance x from a dataset Db of n instances created under pb . 2. Make some changes to x to arrive at x0 . 3. Manually label y 0 and add hx0 , y 0 i to De . We examine the expected probability pe (y 0 |x0i ) under this edit process. To derive this probability, we will need to know the probability that a change to x changes y. We define the edit sensitivity s to be this probability, i.e., s = pb (y 0 = ¬y). The other quantity of interest"
2021.emnlp-main.135,D17-1215,0,0.0551645,"ability of the label given the presence of the word. The label associated with each point is marked by color and superscript. All features above the blue line have detectable correlation with class labels, using a very conservative Bonferronicorrected statistical test. Attempts by the natural language processing community to get machines to understand language or read text are often stymied in part by issues in our datasets (Chen et al., 2016; Sugawara et al., 2018). Many recent papers have shown that popular datasets are prone to shortcuts, dataset artifacts, bias, and spurious correlations (Jia and Liang, 2017; Rudinger et al., 2018; Costa-jussà et al., 2019). While these empirical demonstrations of Equal contribution 0.6 = 0.01/28k neutral contradict entailment 0.2 Introduction ∗ nobodyc catsc n vacation cat c sleepingc first n noc outdoorse competitionn animal e 1.0 deficiencies in the data are useful, they often leave unanswered fundamental questions of what exactly makes a correlation “spurious”, instead of a feature that is legitimately predictive of some target label. In this work we attempt to address this question theoretically. We begin with the assumption that in a language understanding"
2021.emnlp-main.135,2020.acl-main.769,0,0.0999329,"Missing"
2021.emnlp-main.135,D19-5808,1,0.823901,"ly differs from 0.5. In this section, we will show that an artifact emerges if there is a bias at dimension i in the sampling procedure, which is inevitable for some features in practice. We will formalize this bias in terms of a rejection sampling probability ri . For a single sample x, y, we first derive the joint and marginal probabilities pb (y, xi ) and pb (xi ), from which we can obtain pb (y|xi ). These formulas use a recurrence relation obtained from the rejection sampling procedure. 3 This is ubiquitous in crowdsourcing; see, e.g., common patterns in DROP (Dua et al., 2019) or ROPES (Lin et al., 2019) that ultimately derive from annotator instructions. 1803 1 1 pb (y, xi ) = fi + fi ri pb (y, xi ) 2 2 fi ∴ pb (y, xi ) = 2 − fi ri 1 1 1 pb (xi ) = fi + fi (1 − ri ) + fi ri pb (xi ) 2 2 2 2fi − fi ri ∴ pb (xi ) = 2 − fi ri pb (y, xi ) 1 ∴ pb (y |xi ) = = pb (xi ) 2 − ri With no bias (ri = 0), this probability is 0.5, as expected, and it rises to 1 as ri increases to 1. We define pˆ(y|xi ) as the empirical expectation of pb (y|xi ) over n samples containing xi , with different samples indexed by superscript j. pˆ(y|xi ) = 1 Pn j ˆ is a conditional binomial j=1 y . Note that p n random variabl"
2021.emnlp-main.135,P11-1015,0,0.0437068,"in Practice et al., 2017; Sennrich, 2017; Zhao et al., 2018, inter alia). In this section we show that this kind of data We empirically investigate the effectiveness of loaugmentation can be effective with an appropri- cal edits for reducing single feature artifacts using 1806 Artifact statistics for BoolQ before and after local edits locally edited samples generated from two datasets: (1) the Boolean Questions dataset (BoolQ; Clark et al., 2019a), which consists of pairs of paragraphs (p) and questions (q), where each q has a binary answer that can be found by reasoning over p; and (2) IMDb (Maas et al., 2011), a sentiment classification dataset in the domain of movie reviews. We define each feature xi as the occurrence of a particular word within q for BoolQ, and within the text of the review for IMDb. Gardner et al. (2020) generated additional data for BoolQ and IMDb by making local edits to the question or review text and recording the updated binary label. 5.2 Local Edits and Boolean Sensitivity In the above discussion we used the term sensitivity in an informal way to describe the probability that a local edit changes the label. This term also has a related formal definition in the study of bo"
2021.emnlp-main.135,2020.emnlp-demos.16,0,0.067072,"Missing"
2021.emnlp-main.135,W17-5525,0,0.0666648,"Missing"
2021.emnlp-main.135,2020.findings-emnlp.225,1,0.71033,"orrelated? This could happen due to societal biases, word usage frequencies, or priming effects from data collection instructions given to all annotators. Surely across any pool of annotators there will be some dimensions along which r values are correlated, and other dimensions along with they are not. Increasing the number of annotators thus helps mitigate the problem, but does not solve it completely. Data filtering A recent trend is to remove data from a training set that is biased in some way in order to get a model that generalizes better (Le Bras et al., 2020; Swayamdipta et al., 2020; Oren et al., 2020). While this method can be effective for very biased datasets, it is somewhat unsatisfying to 6 Other Mitigation Techniques remove entire instances because of bias in a single feature. In the extreme case where ri ≈ 1, such as In this section we briefly discuss the implications of with “nobody” in SNLI (Fig. 1), this process could our theoretical analysis for other artifact mitigation effectively remove xi from the observed feature techniques that have been proposed in the literature. space. Our analysis in this section is not rigorous and is To understand the effect of these automated meant o"
2021.emnlp-main.135,N18-2002,0,0.0362563,"Missing"
2021.emnlp-main.135,P19-1163,1,0.817395,"a that accurately reflects the competency assumption, both to evaluate systems and (presumably) to train them. However, humans suffer from blind spots, social bias, priming, and other psychological effects that make collecting data for competency problems challenging. Examples of these effects include instructions in a crowdsourcing task that prime workers to use particular language,3 or distributional effects in source material, such as the “amazing” examples above, or racial bias in face recognition (Buolamwini and Gebru, 2018) and abusive language detection datasets (Davidson et al., 2019; Sap et al., 2019). In order to formally analyze the impact of human bias on collecting data for competency problems, we need a plausible model of this bias. We represent bias as rejection sampling from the target competency distribution based on single feature values. Specifically, we assume the following dataset collection procedure. First, a person samples an instance from an unbiased distribution pu (x, y) where the competency assumption holds. The person examines this instance, and if feature xi = 1 appears with label y = 0, the person rejects the instance and samples a new one, with probability ri . If y"
2021.emnlp-main.135,E17-2060,0,0.0241905,"or less uniformly, then ei ≈ 0 in a high-dimensional space, so engineering s ≈ 0.5 should produce artifact-free additional samples with pe (y 0 |x0i ) ≈ 0.5. Furthermore, edit sensitivity and edit dimension are empirically measurable when constructing a dataset. This empirical measurement can give theoretical guarantees for the degree to which the local editing will alleviate artifacts (i.e., this gives us a principled way to decide between edit models). Many works have tried to remove data artifacts by making minimal changes to existing data (Shekhar 5.1 Local Edits in Practice et al., 2017; Sennrich, 2017; Zhao et al., 2018, inter alia). In this section we show that this kind of data We empirically investigate the effectiveness of loaugmentation can be effective with an appropri- cal edits for reducing single feature artifacts using 1806 Artifact statistics for BoolQ before and after local edits locally edited samples generated from two datasets: (1) the Boolean Questions dataset (BoolQ; Clark et al., 2019a), which consists of pairs of paragraphs (p) and questions (q), where each q has a binary answer that can be found by reasoning over p; and (2) IMDb (Maas et al., 2011), a sentiment classifi"
2021.emnlp-main.135,2020.acl-main.468,0,0.0189388,"orse n competition animal e 0.6 outsidee persone 0.4 peoplee 0.2 0.0 102 103 n 104 105 Figure 4: Statistical artifacts in ambiguous instances (Swayamdipta et al., 2020; above) versus a random (same-size) sample from the SNLI training set (below). The filtering done by ambiguous instance detection targets statistical artifacts across the whole range of the statistical test, not just high PMI values. importance of the competency assumption.12 7 Other Related Work Theoretical analysis of bias Several recent works explore sources and theoretical treatments of bias or spurious correlations in NLP (Shah et al., 2020a; Kaushik et al., 2020) or ML more broadly (Shah et al., 2020b). Our work differs by introducing a competency assumption and exploring its implications. The difference between our biased and unbiased distributions is an instance of covariate shift (Quionero-Candela et al., 2009). Competent models An interesting question is whether we can inject a “competency inductive bias” into models, i.e., discourage relying on individual features. The closest works we are aware of are methods that ensemble weak models together with strong models during training (Clark et al., 2020; Dagaev et al., 2021), o"
2021.emnlp-main.135,silveira-etal-2014-gold,0,0.042419,"Missing"
2021.emnlp-main.135,D18-1453,0,0.0164836,"ividual feature (here words) should give information about the class label, plotting the number of occurrences of each word against the conditional probability of the label given the presence of the word. The label associated with each point is marked by color and superscript. All features above the blue line have detectable correlation with class labels, using a very conservative Bonferronicorrected statistical test. Attempts by the natural language processing community to get machines to understand language or read text are often stymied in part by issues in our datasets (Chen et al., 2016; Sugawara et al., 2018). Many recent papers have shown that popular datasets are prone to shortcuts, dataset artifacts, bias, and spurious correlations (Jia and Liang, 2017; Rudinger et al., 2018; Costa-jussà et al., 2019). While these empirical demonstrations of Equal contribution 0.6 = 0.01/28k neutral contradict entailment 0.2 Introduction ∗ nobodyc catsc n vacation cat c sleepingc first n noc outdoorse competitionn animal e 1.0 deficiencies in the data are useful, they often leave unanswered fundamental questions of what exactly makes a correlation “spurious”, instead of a feature that is legitimately predictive"
2021.emnlp-main.135,2020.emnlp-main.746,1,0.882498,"Missing"
2021.emnlp-main.135,D19-1608,1,0.825157,"text. Section 6 discusses some additional empirical evidence for models’ reliance on these artifacts. 5 Mitigating Artifacts with Local Edits ately sensitive edit model, where sensitivity refers to how often a change to inputs results in the label changing. However, because humans are involved in making these changes, achieving appropriate sensitivity is challenging, and bias in this process can lead to the introduction of new artifacts. This suggests that care must be taken when performing edit-based data augmentation, as large edited training datasets are not likely to be artifact-free (cf. Tafjord et al., 2019; Huang et al., 2020). Imagine a new dataset De consisting of samples x0 , y 0 generated by making local edits according to the following repeated procedure: 1. Randomly sample an instance x from a dataset Db of n instances created under pb . 2. Make some changes to x to arrive at x0 . 3. Manually label y 0 and add hx0 , y 0 i to De . We examine the expected probability pe (y 0 |x0i ) under this edit process. To derive this probability, we will need to know the probability that a change to x changes y. We define the edit sensitivity s to be this probability, i.e., s = pb (y 0 = ¬y). The other"
2021.emnlp-main.135,D19-1221,1,0.953469,"ve high performance, but will necessarily fail if the learner is evaluated under the competency setting. With a hypothesis test in hand, we can examine existing datasets for evidence of statisticallysignificant feature bias, and then explore the extent to which this bias impacts models supervised with this data. Prior work has used pointwise mutual information (PMI) to find features that have high correlation with labels (e.g., Gururangan et al., 2018). This measure is useful for understanding why certain features might get used as deterministic decision rules by models (Ribeiro et al., 2018; Wallace et al., 2019). However, studies involving PMI have also intuitively understood that PMI by itself does not tell the whole story, as a strict ranking by PMI would return features that only appear once in the dataset. To account for this problem, they used arbitrary cutoffs and included information about feature occurrence in addition to their PMI ranking. A benefit of our approach to defining and detecting artifacts is that we have a single statistical test that takes into account both the number of times a feature appears and how correlated it is with a single label. We use this test to find features with"
2021.emnlp-main.135,2020.emnlp-main.105,1,0.827376,"Missing"
2021.emnlp-main.135,2020.emnlp-demos.6,0,0.0369909,"Missing"
2021.emnlp-main.135,N18-2003,0,0.020093,"y, then ei ≈ 0 in a high-dimensional space, so engineering s ≈ 0.5 should produce artifact-free additional samples with pe (y 0 |x0i ) ≈ 0.5. Furthermore, edit sensitivity and edit dimension are empirically measurable when constructing a dataset. This empirical measurement can give theoretical guarantees for the degree to which the local editing will alleviate artifacts (i.e., this gives us a principled way to decide between edit models). Many works have tried to remove data artifacts by making minimal changes to existing data (Shekhar 5.1 Local Edits in Practice et al., 2017; Sennrich, 2017; Zhao et al., 2018, inter alia). In this section we show that this kind of data We empirically investigate the effectiveness of loaugmentation can be effective with an appropri- cal edits for reducing single feature artifacts using 1806 Artifact statistics for BoolQ before and after local edits locally edited samples generated from two datasets: (1) the Boolean Questions dataset (BoolQ; Clark et al., 2019a), which consists of pairs of paragraphs (p) and questions (q), where each q has a binary answer that can be found by reasoning over p; and (2) IMDb (Maas et al., 2011), a sentiment classification dataset in t"
2021.emnlp-main.137,Q19-1038,0,0.0164113,"ing (Vincent et al., 2010), variational (Kingma and Welling, 2014; Higgins et al., 2017; Dai and Wipf, 2019), adversarial (Makhzani et al., 2016; Zhao et al., 2018), and regularized (Ghosh et al., 2020) autoencoders. They have been found especially useful in controlled text generation (Hu et al., 2017; Logeswaran et al., 2018; Bowman et al., 2016a), especially in sentiment style transfer (Mai et al., 2020; Shen et al., 2017). The encoder-decoder structure for obtaining representations has been used in pretraining (Lewis et al., 2019), sentence infilling (Huang et al., 2020), and multilingual (Artetxe and Schwenk, 2019) scenarios. In particular, Lewis et al. (2019) treat denoising as translation task to perform pretraining from scratch, but their approach does not induce a sentence representation space with generative properties. In contrast, our method makes use of a frozen pretrained transformer to learn a shallow, sentence bottleneck autoencoder on top. 5 Conclusion We proposed an approach that converts a pretrained transformer language model into a sentence-level autoencoder that is able to reconstruct its pretraining data. The resulting model improves the performance of the pretrained model on sentence-"
2021.emnlp-main.137,K16-1002,0,0.318992,"for varying multiples of the sentiment vector are plotted. Upper right is better. former models without hurting their performance. 1825 4 Related Work Reconstructing text with autoencoders is an active area of research that has lead to several advancements such as denoising (Vincent et al., 2010), variational (Kingma and Welling, 2014; Higgins et al., 2017; Dai and Wipf, 2019), adversarial (Makhzani et al., 2016; Zhao et al., 2018), and regularized (Ghosh et al., 2020) autoencoders. They have been found especially useful in controlled text generation (Hu et al., 2017; Logeswaran et al., 2018; Bowman et al., 2016a), especially in sentiment style transfer (Mai et al., 2020; Shen et al., 2017). The encoder-decoder structure for obtaining representations has been used in pretraining (Lewis et al., 2019), sentence infilling (Huang et al., 2020), and multilingual (Artetxe and Schwenk, 2019) scenarios. In particular, Lewis et al. (2019) treat denoising as translation task to perform pretraining from scratch, but their approach does not induce a sentence representation space with generative properties. In contrast, our method makes use of a frozen pretrained transformer to learn a shallow, sentence bottlenec"
2021.emnlp-main.137,D15-1075,0,0.0404423,"e sentence bottleneck and the decoder are learned; the encoder parameters are kept fixed. 3 Experiments To assess the quality of the sentence representations learned by our model we evaluate on sentence similarity (Section 3.2), classification (Section 3.3), and generation tasks (Section 3.4). 3.1 Settings Datasets Since the RoBERTa dataset is not publicly available, we use for pretraining the exact same dataset as BERT (Devlin et al., 2019), which is composed of BooksCorpus (Zhu et al., 2015) and English Wikipedia. For sentence similarity, we use the Natural Language Inference (NLI) dataset (Bowman et al., 2015) for finetuning and evaluate on the Semantic Textual Similarity (STS) dataset (Cer et al., 2017), following Conneau et al. (2017). For classification, we use mainly single-sentence datasets from the GLUE benchmark (Wang et al., 2018), namely Stanford Sentiment Treebank (SST) and Corpus of Linguistic Acceptability (CoLA) datasets, but we also report the average performance on the remaining datasets. For generation, we use the Yelp reviews dataset (Shen et al., 2017). token representation) then finetuning the whole pretrained model using Siamese networks on a combination of natural language infe"
2021.emnlp-main.137,2020.acl-main.747,0,0.0274236,"by directly manipulating sentence repre1 Introduction sentations using basic numerical operations (Shen Recent research has focused on devising new unsu- et al., 2020a). Yet, how to convert pretrained transformer language models to autoencoders with such pervised pretraining methods from unlabeled data that involves some form of language modeling, pri- properties still remains unexplored. marily autoregressive (Peters et al., 2018; Radford To fill in this gap, we introduce AUTOBOT, a et al., 2019), masked (Devlin et al., 2019; Liu et al., new autoencoder model for learning sentence “bot2019; Conneau et al., 2020) and generalized (Rad- tleneck” (i.e., fixed-size) representations from preford et al., 2019; Brown et al., 2020; Song et al., trained transformers that is useful for similarity, 2019), with much success on downstream tasks. generation, and classification, displayed in FigUnder the hood, most of these methods use trans- ure 1. Our model has two unique components: formers (Vaswani et al., 2017) for encoding text se- (i) a transformation that uses dot product attention quences, which allows them to learn powerful con- to dynamically pool semantic information from the textual word representations"
2021.emnlp-main.137,D17-1070,0,0.0275733,"the sentence representations learned by our model we evaluate on sentence similarity (Section 3.2), classification (Section 3.3), and generation tasks (Section 3.4). 3.1 Settings Datasets Since the RoBERTa dataset is not publicly available, we use for pretraining the exact same dataset as BERT (Devlin et al., 2019), which is composed of BooksCorpus (Zhu et al., 2015) and English Wikipedia. For sentence similarity, we use the Natural Language Inference (NLI) dataset (Bowman et al., 2015) for finetuning and evaluate on the Semantic Textual Similarity (STS) dataset (Cer et al., 2017), following Conneau et al. (2017). For classification, we use mainly single-sentence datasets from the GLUE benchmark (Wang et al., 2018), namely Stanford Sentiment Treebank (SST) and Corpus of Linguistic Acceptability (CoLA) datasets, but we also report the average performance on the remaining datasets. For generation, we use the Yelp reviews dataset (Shen et al., 2017). token representation) then finetuning the whole pretrained model using Siamese networks on a combination of natural language inference data. To compare with them on sentence similarity, we incorporate our model within their framework and follow their setting"
2021.emnlp-main.137,N19-1423,0,0.376545,"an large pretrained models.1 toencoder allows one to perform controlled text generation by directly manipulating sentence repre1 Introduction sentations using basic numerical operations (Shen Recent research has focused on devising new unsu- et al., 2020a). Yet, how to convert pretrained transformer language models to autoencoders with such pervised pretraining methods from unlabeled data that involves some form of language modeling, pri- properties still remains unexplored. marily autoregressive (Peters et al., 2018; Radford To fill in this gap, we introduce AUTOBOT, a et al., 2019), masked (Devlin et al., 2019; Liu et al., new autoencoder model for learning sentence “bot2019; Conneau et al., 2020) and generalized (Rad- tleneck” (i.e., fixed-size) representations from preford et al., 2019; Brown et al., 2020; Song et al., trained transformers that is useful for similarity, 2019), with much success on downstream tasks. generation, and classification, displayed in FigUnder the hood, most of these methods use trans- ure 1. Our model has two unique components: formers (Vaswani et al., 2017) for encoding text se- (i) a transformation that uses dot product attention quences, which allows them to learn pow"
2021.emnlp-main.137,2020.acl-main.226,0,0.0320958,"to several advancements such as denoising (Vincent et al., 2010), variational (Kingma and Welling, 2014; Higgins et al., 2017; Dai and Wipf, 2019), adversarial (Makhzani et al., 2016; Zhao et al., 2018), and regularized (Ghosh et al., 2020) autoencoders. They have been found especially useful in controlled text generation (Hu et al., 2017; Logeswaran et al., 2018; Bowman et al., 2016a), especially in sentiment style transfer (Mai et al., 2020; Shen et al., 2017). The encoder-decoder structure for obtaining representations has been used in pretraining (Lewis et al., 2019), sentence infilling (Huang et al., 2020), and multilingual (Artetxe and Schwenk, 2019) scenarios. In particular, Lewis et al. (2019) treat denoising as translation task to perform pretraining from scratch, but their approach does not induce a sentence representation space with generative properties. In contrast, our method makes use of a frozen pretrained transformer to learn a shallow, sentence bottleneck autoencoder on top. 5 Conclusion We proposed an approach that converts a pretrained transformer language model into a sentence-level autoencoder that is able to reconstruct its pretraining data. The resulting model improves the pe"
2021.emnlp-main.137,N18-1202,0,0.0974588,"Missing"
2021.emnlp-main.137,D19-1410,0,0.0984294,"ial Intelligence {ivamon,npappas,nasmith}@cs.washington.edu Abstract from pretrained transformer language models based on a special token or basic pooling operations. To Representation learning for text via pretrainthis end, representation learning methods have been ing a language model on a large corpus has designed to better capture semantic information become a standard starting point for building from pretrained transformer language models, e.g., NLP systems. This approach stands in contrast using Siamese networks trained with a triplet loss to autoencoders, also trained on raw text, but (Reimers and Gurevych, 2019) or transforming the with the objective of learning to encode each input as a vector that allows full reconstruction. desired sentence distribution to a Gaussian distriAutoencoders are attractive because of their bution through normalizing flows (Li et al., 2020). latent space structure and generative properExisting sentence representations directly deties. We therefore explore the construction of a rived from pretrained language models or learned sentence-level autoencoder from a pretrained, by specialized methods cannot guarantee perfect frozen transformer language model. We adapt reconstruc"
2021.emnlp-main.804,2020.emnlp-main.263,0,0.0426793,"t tokens selected by an attribution method. Figure 6: An illustration of how simulatability is calculated in §4. I→O model. For example, in Table 6, at σ 2 = 30, the rationale provides signal for an incorrect answer choice (“indigestion”) that does not exist at σ 2 = 35. Therefore, we consider rationales produced with σ larger than the value at which simulatability reaches the minimum as unstable (see further examples corroborating this in Appendix B, Tables 11–13). 4.2 Feature Importance Agreement (1) where the function f reduces the gradient to a scalar. Choices for f include L1 or L2 norm (Atanasova et al., 2020), or an element-wise sum (Wallace et al., 2019). Intuitively, the gradient measures how much an infinitesimally small change in the input changes the predicted class’ logit, using a first-order Taylor series approximation of the logit function. Such methods have been extended to sequence-output models such as neural machine translation (He et al., 2019; Ding et al., 2019; Li et al., 2020) by computing the sum of m decoded (k) logits {lp }m k=1 with respect to the input: a(x(i) ; {lp(k) }k ) = Pm k=1 a(x (i) ; l(k) ) p ∈ R. (2) The attribution of a sequence of n input token embeddings, X ∈ Rn×d"
2021.emnlp-main.804,P19-1284,0,0.0383355,"Missing"
2021.emnlp-main.804,D15-1075,0,0.305552,"s (Figure 2) were designed for information extraction (IE) tasks 1 Our code is available at https://github.com/ allenai/label_rationale_association. for which a rationale can be extracted as a subset of the input and is sufficient to make a prediction on its own, without the rest of the input (Lei et al., 2016). Such models approach faithfulness by construction (Jain et al., 2020). There is a growing interest in tasks that require world and commonsense “knowledge” and “reasoning”, such as commonsense question-answering (CommonsenseQA; Talmor et al., 2019) and natural language inference (SNLI; Bowman et al., 2015). Here, extractive rationales necessarily fall short—rationales must instead take the form of free-text natural language to fill in the reasoning or knowledge gap (Camburu et al., 2018; Rajani et al., 2019).2 In Table 1, for example, the highlighted extractive rationale of the first problem instance lacks at least one reasoning step to adequately justify the answer; the natural language rationale (which is not extractive) fills in the gap. 2 We use “free-text” and “natural language” rationales interchangeably. We additionally use the term “rationale” to also mean “explanation”; for a more deta"
2021.emnlp-main.804,D18-1128,0,0.0275391,"ionales R∗ I→R I→OR E-SNLI CoS-E v1.0 CoS-E v1.11 97.67 84.84 68.63 89.11 53.47 45.45 90.52 62.00 53.15 E-SNLI CoS-E v1.0 CoS-E v1.11 7.77 21.26 19.09 -1.63 -12.11 -12.77 -0.86 -6.21 -6.06 Table 3: Accuracy of the trained R→O model evaluated on ground-truth natural language rationales (R∗ ) and rationales generated from two model architectures: I→OR and I→R (see §2 for model descriptions). Human simulatability (Doshi-Velez and Kim, 2017) has a rich history in machine learning interpretability as a reliable measure of rationale quality from the lens of utility to an end-user (Kim et al., 2016; Chandrasekaran et al., 2018; Hase and Bansal, 2020; Yeung et al., 2020; PoursabziSangdeh et al., 2021; Rajagopal et al., 2021, i.a.). Rather than computing word-level overlap with a ground-truth explanation, simulatability measures the additional predictive ability a rationale provides over the input, computed as the difference between task performance when a rationale is given as input vs. when it is not (IR→O minus I→O). Historically, humans have served as the predictors, but recent work has shown that the computation of simulatability can be automated using trained models. Hase et al. (2020) demonstrate that automate"
2021.emnlp-main.804,P18-1175,0,0.0212535,"te having double the parameters), and T5Base outperforms pretrained models used in prior work. Self-Rationalizing Model (I→OR) A joint, Evaluation We do not report BLEU scores (Papself-rationalizing model (Melis and Jaakkola, ineni et al., 2002), because BLEU and related met2018), illustrated in Figure 3, predicts both a label rics do not measure plausibility (Camburu et al., and rationale. This is the most common approach 2018; Kayser et al., 2021; Clinciu et al., 2021) or to free-text rationalization (Hendricks et al., 2016; faithfulness (Jacovi and Goldberg, 2020). In addiKim et al., 2018; Hancock et al., 2018; Camburu tion to low correlation with human scores, there et al., 2018; Ehsan et al., 2018; Liu et al., 2019a; can be many valid rationales for a given instance Wu and Mooney, 2019; Narang et al., 2020; Do (Miller, 2019); metrics that compare generated raet al., 2020; Tang et al., 2020), but little is under- tionales to a single ground-truth do not address this stood about model internals. I→OR models are and are thus a poor measure of quality. 10268 Source of Rationales R∗ I→R I→OR Source of Rationales R∗ I→R I→OR E-SNLI CoS-E v1.0 CoS-E v1.11 97.67 84.84 68.63 89.11 53.47 45.45 90.52 62.00"
2021.emnlp-main.804,2020.acl-main.491,0,0.108943,"S-E v1.0 CoS-E v1.11 97.67 84.84 68.63 89.11 53.47 45.45 90.52 62.00 53.15 E-SNLI CoS-E v1.0 CoS-E v1.11 7.77 21.26 19.09 -1.63 -12.11 -12.77 -0.86 -6.21 -6.06 Table 3: Accuracy of the trained R→O model evaluated on ground-truth natural language rationales (R∗ ) and rationales generated from two model architectures: I→OR and I→R (see §2 for model descriptions). Human simulatability (Doshi-Velez and Kim, 2017) has a rich history in machine learning interpretability as a reliable measure of rationale quality from the lens of utility to an end-user (Kim et al., 2016; Chandrasekaran et al., 2018; Hase and Bansal, 2020; Yeung et al., 2020; PoursabziSangdeh et al., 2021; Rajagopal et al., 2021, i.a.). Rather than computing word-level overlap with a ground-truth explanation, simulatability measures the additional predictive ability a rationale provides over the input, computed as the difference between task performance when a rationale is given as input vs. when it is not (IR→O minus I→O). Historically, humans have served as the predictors, but recent work has shown that the computation of simulatability can be automated using trained models. Hase et al. (2020) demonstrate that automated metrics for simulatab"
2021.emnlp-main.804,2020.findings-emnlp.390,0,0.0262551,"im et al., 2016; Chandrasekaran et al., 2018; Hase and Bansal, 2020; Yeung et al., 2020; PoursabziSangdeh et al., 2021; Rajagopal et al., 2021, i.a.). Rather than computing word-level overlap with a ground-truth explanation, simulatability measures the additional predictive ability a rationale provides over the input, computed as the difference between task performance when a rationale is given as input vs. when it is not (IR→O minus I→O). Historically, humans have served as the predictors, but recent work has shown that the computation of simulatability can be automated using trained models. Hase et al. (2020) demonstrate that automated metrics for simulatability have moderate to high correlation with human scores in both an expert and a crowdsourced setting.4 We thus use simulatability score as a measure of rationale quality. 3 Shortcomings of Free-Text Pipelines We first analyze “faithful-by-construction” pipeline models (I→R;R→O) for free-text rationalization with respect to two properties: quality of generated rationales (§3.1) and appropriateness of the sufficiency assumption (§3.2). 3.1 Joint Model Rationales are More Indicative of Labels Rationales should be a function of the input and the p"
2021.emnlp-main.804,D19-1088,0,0.0199665,"ility reaches the minimum as unstable (see further examples corroborating this in Appendix B, Tables 11–13). 4.2 Feature Importance Agreement (1) where the function f reduces the gradient to a scalar. Choices for f include L1 or L2 norm (Atanasova et al., 2020), or an element-wise sum (Wallace et al., 2019). Intuitively, the gradient measures how much an infinitesimally small change in the input changes the predicted class’ logit, using a first-order Taylor series approximation of the logit function. Such methods have been extended to sequence-output models such as neural machine translation (He et al., 2019; Ding et al., 2019; Li et al., 2020) by computing the sum of m decoded (k) logits {lp }m k=1 with respect to the input: a(x(i) ; {lp(k) }k ) = Pm k=1 a(x (i) ; l(k) ) p ∈ R. (2) The attribution of a sequence of n input token embeddings, X ∈ Rn×d , is a vector a(X) = [a(x(1) ), . . . , a(x(n) )] ∈ Rn , where a(x(i) ) is shorthand for the value defined in Equation 2. By decomposing the term in Equation 2 into two parts, we obtain two attribution vectors over the input tokens; one for the predicted label logits L, and one for the predicted rationale logits R in the decoded output: If label predi"
2021.emnlp-main.804,N18-1023,0,0.0605146,"Missing"
2021.emnlp-main.804,2020.acl-main.386,0,0.207315,"t label than a pipeline (§3.1). Next, we show that sufficiency is not universally applicable: a natural language rationale on its own does not generally provide enough information to arrive at the correct answer (§3.2). These findings suggest that a faithful-by-construction pipeline is not an ideal approach for reasoning tasks, leading us to ask: is there is a way to achieve faithful freetext rationalization with self-rationalizing models? We note that there is currently no way to assess the relationship between a prediction and a freetext rationale within the same fully differentiable model. Jacovi and Goldberg (2020) argue for the development of evaluations that measure the extent and likelihood that a rationale is faithful in practice (illustrated in Figure 1). To do so, we propose two measurements to initiate testing the extent to which predicted labels and explanations are associated within the model that produces them. 2 Tasks, Datasets, and Models Before we turn to our analyses we introduce datasets and models used for our experiments. Tasks and Datasets We explore two large-scale datasets for textual reasoning tasks that contain human-written natural language rationales: ESNLI (Camburu et al., 2018)"
2021.emnlp-main.804,N19-1357,0,0.0448118,"Missing"
2021.emnlp-main.804,2020.acl-main.409,1,0.915601,"owards faithfulness is to introduce architectural modifications or constraints that produce rationales with desirable properties (Andreas et al., 2016; Schwartz et al., 2018; Jiang et al., 2019, inter alia). For example, pipeline models (Figure 2) were designed for information extraction (IE) tasks 1 Our code is available at https://github.com/ allenai/label_rationale_association. for which a rationale can be extracted as a subset of the input and is sufficient to make a prediction on its own, without the rest of the input (Lei et al., 2016). Such models approach faithfulness by construction (Jain et al., 2020). There is a growing interest in tasks that require world and commonsense “knowledge” and “reasoning”, such as commonsense question-answering (CommonsenseQA; Talmor et al., 2019) and natural language inference (SNLI; Bowman et al., 2015). Here, extractive rationales necessarily fall short—rationales must instead take the form of free-text natural language to fill in the reasoning or knowledge gap (Camburu et al., 2018; Rajani et al., 2019).2 In Table 1, for example, the highlighted extractive rationale of the first problem instance lacks at least one reasoning step to adequately justify the an"
2021.emnlp-main.804,2020.acl-main.771,0,0.0405628,"Missing"
2021.emnlp-main.804,S19-1020,0,0.0279174,"lence test for both CoS-E datasets. models cannot be treated as faithful explanations without further investigation. At minimum, rationales must be implicitly or explicitly tied to the model’s prediction. We present two metrics to analyze the association between the mechanisms that produce labels and rationales in a multi-task, I→OR model: robustness equivalence (§4.1) and feature importance agreement (§4.2). These experiments serve as a necessary sanity check for the reliability of I→OR models’ explanations. 4.1 Robustness Equivalence Method Following related work (Wang et al., 2019; Lakshmi Narayan et al., 2019; Liu et al., 2019b), we add zero-mean Gaussian noise N (0, σ 2 ) to each input embedding in the I→OR encoder at inference time. We measure changes in label prediction as the number of predicted test set labels that flip, i.e., change from their original prediction to something else, alongside changes in accuracy of the I→OR model. We measure changes in rationale quality using simulatability (§2). We illustrate details of the simulatability calculation in Figure 6a. We report metrics on rationales generated by I→OR under different levels of noise, controlled by σ 2 . An example of noisy output"
2021.emnlp-main.804,P19-1261,0,0.0274904,"for explaining reasoning tasks. Introduction Interpretable NLP aims to better understand predictive models’ internals for purposes such as debugging, validating safety before deployment, or revealing unintended biases and behavior (Molnar, 2019). These objectives require faithful rationales— explanations of the model’s behavior that are accurate representations of its decision process (Melis and Jaakkola, 2018). One way towards faithfulness is to introduce architectural modifications or constraints that produce rationales with desirable properties (Andreas et al., 2016; Schwartz et al., 2018; Jiang et al., 2019, inter alia). For example, pipeline models (Figure 2) were designed for information extraction (IE) tasks 1 Our code is available at https://github.com/ allenai/label_rationale_association. for which a rationale can be extracted as a subset of the input and is sufficient to make a prediction on its own, without the rest of the input (Lei et al., 2016). Such models approach faithfulness by construction (Jain et al., 2020). There is a growing interest in tasks that require world and commonsense “knowledge” and “reasoning”, such as commonsense question-answering (CommonsenseQA; Talmor et al., 20"
2021.emnlp-main.804,N19-1371,0,0.0513452,"Missing"
2021.emnlp-main.804,D19-1129,0,0.0376115,"Missing"
2021.emnlp-main.804,P18-1028,1,0.844625,"rchitectures and tests for explaining reasoning tasks. Introduction Interpretable NLP aims to better understand predictive models’ internals for purposes such as debugging, validating safety before deployment, or revealing unintended biases and behavior (Molnar, 2019). These objectives require faithful rationales— explanations of the model’s behavior that are accurate representations of its decision process (Melis and Jaakkola, 2018). One way towards faithfulness is to introduce architectural modifications or constraints that produce rationales with desirable properties (Andreas et al., 2016; Schwartz et al., 2018; Jiang et al., 2019, inter alia). For example, pipeline models (Figure 2) were designed for information extraction (IE) tasks 1 Our code is available at https://github.com/ allenai/label_rationale_association. for which a rationale can be extracted as a subset of the input and is sufficient to make a prediction on its own, without the rest of the input (Lei et al., 2016). Such models approach faithfulness by construction (Jain et al., 2020). There is a growing interest in tasks that require world and commonsense “knowledge” and “reasoning”, such as commonsense question-answering (CommonsenseQ"
2021.emnlp-main.804,P19-1282,1,0.855719,"ge vs. blue line in Figure 7, left). The two simulatability lines (Figure 7, right) for CoS-E v1.0 have an inflection point. We illustrate how simulatability is calculated in Fig5 Related Work ure 6b. Similar to §4.1, we expect this is due to Analysis of NLP Models Structural tests for anrationales so noisy that IR→O ignores them and alyzing models’ internals include probing (Tenney behaves like I→O. If an input attribution degrades rationale quality (as measured by simulatability) et al., 2019) and attention analysis (Jain and Walmore than a random attribution, then the line cor- lace, 2019; Serrano and Smith, 2019; Wiegreffe and Pinter, 2019; Tutek and Snajder, 2020). These, responding to that attribution (for values of k for which neither that attribution nor the random attri- along with behavioral tests such as challenge sets (McCoy et al., 2019) and checklists (Ribeiro et al., bution have reached the inflection point) has to be 2020), are conceptually similar to our experiments, below the “random” line. For values of k for which both attributions have passed the inflection point, but study different model properties. the “random” line should be below the attribution Although gradient-attribution has"
2021.emnlp-main.804,D13-1170,0,0.00558026,"Missing"
2021.emnlp-main.804,N19-1421,0,0.161742,"iang et al., 2019, inter alia). For example, pipeline models (Figure 2) were designed for information extraction (IE) tasks 1 Our code is available at https://github.com/ allenai/label_rationale_association. for which a rationale can be extracted as a subset of the input and is sufficient to make a prediction on its own, without the rest of the input (Lei et al., 2016). Such models approach faithfulness by construction (Jain et al., 2020). There is a growing interest in tasks that require world and commonsense “knowledge” and “reasoning”, such as commonsense question-answering (CommonsenseQA; Talmor et al., 2019) and natural language inference (SNLI; Bowman et al., 2015). Here, extractive rationales necessarily fall short—rationales must instead take the form of free-text natural language to fill in the reasoning or knowledge gap (Camburu et al., 2018; Rajani et al., 2019).2 In Table 1, for example, the highlighted extractive rationale of the first problem instance lacks at least one reasoning step to adequately justify the answer; the natural language rationale (which is not extractive) fills in the gap. 2 We use “free-text” and “natural language” rationales interchangeably. We additionally use the t"
2021.emnlp-main.804,2020.acl-srw.23,0,0.0274813,"ted in Figure 3, predicts both a label rics do not measure plausibility (Camburu et al., and rationale. This is the most common approach 2018; Kayser et al., 2021; Clinciu et al., 2021) or to free-text rationalization (Hendricks et al., 2016; faithfulness (Jacovi and Goldberg, 2020). In addiKim et al., 2018; Hancock et al., 2018; Camburu tion to low correlation with human scores, there et al., 2018; Ehsan et al., 2018; Liu et al., 2019a; can be many valid rationales for a given instance Wu and Mooney, 2019; Narang et al., 2020; Do (Miller, 2019); metrics that compare generated raet al., 2020; Tang et al., 2020), but little is under- tionales to a single ground-truth do not address this stood about model internals. I→OR models are and are thus a poor measure of quality. 10268 Source of Rationales R∗ I→R I→OR Source of Rationales R∗ I→R I→OR E-SNLI CoS-E v1.0 CoS-E v1.11 97.67 84.84 68.63 89.11 53.47 45.45 90.52 62.00 53.15 E-SNLI CoS-E v1.0 CoS-E v1.11 7.77 21.26 19.09 -1.63 -12.11 -12.77 -0.86 -6.21 -6.06 Table 3: Accuracy of the trained R→O model evaluated on ground-truth natural language rationales (R∗ ) and rationales generated from two model architectures: I→OR and I→R (see §2 for model descript"
2021.emnlp-main.804,P19-1452,0,0.0295892,"Missing"
2021.emnlp-main.804,N18-1074,0,0.058636,"Missing"
2021.emnlp-main.804,2020.repl4nlp-1.17,0,0.0154037,"bility lines (Figure 7, right) for CoS-E v1.0 have an inflection point. We illustrate how simulatability is calculated in Fig5 Related Work ure 6b. Similar to §4.1, we expect this is due to Analysis of NLP Models Structural tests for anrationales so noisy that IR→O ignores them and alyzing models’ internals include probing (Tenney behaves like I→O. If an input attribution degrades rationale quality (as measured by simulatability) et al., 2019) and attention analysis (Jain and Walmore than a random attribution, then the line cor- lace, 2019; Serrano and Smith, 2019; Wiegreffe and Pinter, 2019; Tutek and Snajder, 2020). These, responding to that attribution (for values of k for which neither that attribution nor the random attri- along with behavioral tests such as challenge sets (McCoy et al., 2019) and checklists (Ribeiro et al., bution have reached the inflection point) has to be 2020), are conceptually similar to our experiments, below the “random” line. For values of k for which both attributions have passed the inflection point, but study different model properties. the “random” line should be below the attribution Although gradient-attribution has been extenline, assuming that after this point, a noi"
2021.emnlp-main.804,D19-3002,0,0.0546194,"Missing"
2021.emnlp-main.804,D19-1002,1,0.88191,"e 7, left). The two simulatability lines (Figure 7, right) for CoS-E v1.0 have an inflection point. We illustrate how simulatability is calculated in Fig5 Related Work ure 6b. Similar to §4.1, we expect this is due to Analysis of NLP Models Structural tests for anrationales so noisy that IR→O ignores them and alyzing models’ internals include probing (Tenney behaves like I→O. If an input attribution degrades rationale quality (as measured by simulatability) et al., 2019) and attention analysis (Jain and Walmore than a random attribution, then the line cor- lace, 2019; Serrano and Smith, 2019; Wiegreffe and Pinter, 2019; Tutek and Snajder, 2020). These, responding to that attribution (for values of k for which neither that attribution nor the random attri- along with behavioral tests such as challenge sets (McCoy et al., 2019) and checklists (Ribeiro et al., bution have reached the inflection point) has to be 2020), are conceptually similar to our experiments, below the “random” line. For values of k for which both attributions have passed the inflection point, but study different model properties. the “random” line should be below the attribution Although gradient-attribution has been extenline, assuming th"
2021.emnlp-main.804,W19-4812,0,0.156006,"lf-rationalizing model (Melis and Jaakkola, ineni et al., 2002), because BLEU and related met2018), illustrated in Figure 3, predicts both a label rics do not measure plausibility (Camburu et al., and rationale. This is the most common approach 2018; Kayser et al., 2021; Clinciu et al., 2021) or to free-text rationalization (Hendricks et al., 2016; faithfulness (Jacovi and Goldberg, 2020). In addiKim et al., 2018; Hancock et al., 2018; Camburu tion to low correlation with human scores, there et al., 2018; Ehsan et al., 2018; Liu et al., 2019a; can be many valid rationales for a given instance Wu and Mooney, 2019; Narang et al., 2020; Do (Miller, 2019); metrics that compare generated raet al., 2020; Tang et al., 2020), but little is under- tionales to a single ground-truth do not address this stood about model internals. I→OR models are and are thus a poor measure of quality. 10268 Source of Rationales R∗ I→R I→OR Source of Rationales R∗ I→R I→OR E-SNLI CoS-E v1.0 CoS-E v1.11 97.67 84.84 68.63 89.11 53.47 45.45 90.52 62.00 53.15 E-SNLI CoS-E v1.0 CoS-E v1.11 7.77 21.26 19.09 -1.63 -12.11 -12.77 -0.86 -6.21 -6.06 Table 3: Accuracy of the trained R→O model evaluated on ground-truth natural language rati"
2021.emnlp-main.804,D08-1004,0,0.116443,"Missing"
2021.emnlp-main.830,2020.emnlp-main.19,0,0.043024,"ng two major strategies: compressing the attention context and sparsifying the attention patterns. Attention Context Compression This strand of methods compresses the context that is attended to, thereby reducing the time and memory overhead in the attention. RNN models that we converted pretrained transformers into compress the context into a recurrent state. Other approaches include low rank approximation of the attention computation (Wang et al., 2020; Tay et al., 2021) and adding a memory module that can access multiple tokens at once (Liu et al., 2018; Dai et al., 2019; Lee et al., 2019; Ainslie et al., 2020; Rae et al., 2020; Beltagy et al., 2020; Zaheer et al., 2020). Sparse Attention Patterns Another approach to reducing the time and memory overhead from the attention computation is to limit the tokens that are attended to by sparsifying the attention patterns. These patterns can be set in advance or learned during training (Tay et al., 2020b). For example, prior works introduced fixed patterns of blockwise attention (Qiu et al., 2020) and strided attention (Child et al., 2019; Beltagy et al., 2020; Zaheer et al., 2020). Other previous works presented methods to learn attention patterns from d"
2021.emnlp-main.830,W14-4012,0,0.111424,"Missing"
2021.emnlp-main.830,P19-1285,0,0.0265845,"we describe several prior works along two major strategies: compressing the attention context and sparsifying the attention patterns. Attention Context Compression This strand of methods compresses the context that is attended to, thereby reducing the time and memory overhead in the attention. RNN models that we converted pretrained transformers into compress the context into a recurrent state. Other approaches include low rank approximation of the attention computation (Wang et al., 2020; Tay et al., 2021) and adding a memory module that can access multiple tokens at once (Liu et al., 2018; Dai et al., 2019; Lee et al., 2019; Ainslie et al., 2020; Rae et al., 2020; Beltagy et al., 2020; Zaheer et al., 2020). Sparse Attention Patterns Another approach to reducing the time and memory overhead from the attention computation is to limit the tokens that are attended to by sparsifying the attention patterns. These patterns can be set in advance or learned during training (Tay et al., 2020b). For example, prior works introduced fixed patterns of blockwise attention (Qiu et al., 2020) and strided attention (Child et al., 2019; Beltagy et al., 2020; Zaheer et al., 2020). Other previous works presented me"
2021.emnlp-main.830,N19-1423,0,0.0171946,"parameters and the other network parameters. Our experiments in language modeling and machine translation show that the conversion can compress the context into a much smaller recurrent state than the sequence length (e.g., 1/16 of the sequence length in WikiText-103 language modeling) while retaining high accuracy. In addition, this conversion requires much less GPU time than training randomly initialized models from scratch. State-of-the-art models in many natural language tasks are increasingly dependent on large-scale pretrained transformer models (e.g., GPT-2, Radford et al., 2019; BERT, Devlin et al., 2019; RoBERTa, Liu et al., 2019; T5, Raffel et al., 2020; BART, Lewis et al., 2020; DeBERTa, He et al., 2021). Converting a large off-the-shelf transformer to a lightweight inference model without repeating the whole training procedure is particularly useful in many downstream applications. Our work focuses on text generation and presents a viable approach towards efficient inference with high accuracy. 2 Convert a Transformer into an RNN The transformer architecture consists of multihead attention, feedforward, and layer normalization modules (Vaswani et al., 2017). When a transformer is trained"
2021.emnlp-main.830,2020.acl-main.703,0,0.037742,"ng and machine translation show that the conversion can compress the context into a much smaller recurrent state than the sequence length (e.g., 1/16 of the sequence length in WikiText-103 language modeling) while retaining high accuracy. In addition, this conversion requires much less GPU time than training randomly initialized models from scratch. State-of-the-art models in many natural language tasks are increasingly dependent on large-scale pretrained transformer models (e.g., GPT-2, Radford et al., 2019; BERT, Devlin et al., 2019; RoBERTa, Liu et al., 2019; T5, Raffel et al., 2020; BART, Lewis et al., 2020; DeBERTa, He et al., 2021). Converting a large off-the-shelf transformer to a lightweight inference model without repeating the whole training procedure is particularly useful in many downstream applications. Our work focuses on text generation and presents a viable approach towards efficient inference with high accuracy. 2 Convert a Transformer into an RNN The transformer architecture consists of multihead attention, feedforward, and layer normalization modules (Vaswani et al., 2017). When a transformer is trained for a sequence generation task with teacher forcing (Williams and Zipser, 1989"
2021.emnlp-main.830,N19-4009,0,0.0526643,"Missing"
2021.emnlp-main.830,W18-6301,0,0.0235063,"H: Hassan et al., 2018). by more than 2.0 perplexity points in the pretrain We use the hyperparameters of the large sized trans- setting. Unlike the other linear transformer models, former (Vaswani et al., 2017): 6 layers, 16 attention T2R greatly benefits from pretraining (T2R + Preheads, 1024 model dimensions, and 4096 hidden train: 19.6 vs. T2R + Random Init.: 20.8 test perdimensions for both the encoder and decoder. We plexity points). We attribute this advantage of T2R apply dropout with 0.3 and label smoothing with to the fact that the MLP feature map is able to learn ε = 0.1. Following Ott et al. (2018), we use an attention patterns that are similar to those of the preincreased batch size of approximately 460K to- trained transformer, as evidenced in §4. Notice also kens. Each randomly initialized model is trained that the T2R conversion is ∼5x faster (measured for 30K (60K for the large EN-FR dataset) steps in GPU hours) than training a model from scratch. using Adam with a learning rate of 5 ⋅ 10−4 and These results illustrate that a lightweight model β = (0.9, 0.98) (Kingma and Ba, 2015). We ob- can be obtained without repeating the expensive served that convergence of the T2R conversion"
2021.emnlp-main.830,P02-1040,0,0.109321,"Missing"
2021.emnlp-main.830,W18-2715,0,0.168723,"already discussed, we highlight related methods from prior work that make transformer models efficient. 5.1 Knowledge Distillation Knowledge distillation (Hinton et al., 2015) is closely related to our T2R conversion and uses a similar pipeline: a teacher model with large capacity is first trained and is used to generate silver training data for a new lightweight inference model. It has been successfully applied to machine translation (e.g., Kim and Rush, 2016; Gu et al., 2018) to make generation efficient. In particular, several prior works distill a transformer translation model to an RNN (Senellart et al., 2018; Kim et al., 2019). We share the same motivation toward fast generation with light memory, but our approach differs in two ways: the original training data are used for finetuning an RNN model, and its model parameters are initialized with the “teacher” transformer. Our method does not use the computationally expensive teacher model to generate new training data. While data generation is a one-time computational cost, it becomes expensive as the teacher model size and training data increase. Moreover, since the pretrained parameters can be directly used, conversion requires fewer GPU hours th"
2021.emnlp-main.830,2020.acl-main.270,1,0.780234,", it becomes expensive as the teacher model size and training data increase. Moreover, since the pretrained parameters can be directly used, conversion requires fewer GPU hours than training a brand new lightweight model from scratch (§3.3). 5.2 Efficient Transformers Prior work suggested many other strategies to improve efficiency in transformers, such as weight sharing and factorization (Dehghani et al., 2019; Lan et al., 2020), weight and layer pruning (Michel et al., 2019; Fan et al., 2020), quantization (Zafrir et al., 2019; Shen et al., 2020), and modifying the combination of sublayers (Press et al., 2020; Mandava et al., 2020). Some of these methods present orthogonal design choices and can be integrated into our T2R model to gain further efficiency. For a more comprehensive survey, see Tay et al. (2020b). Below we describe several prior works along two major strategies: compressing the attention context and sparsifying the attention patterns. Attention Context Compression This strand of methods compresses the context that is attended to, thereby reducing the time and memory overhead in the attention. RNN models that we converted pretrained transformers into compress the context into a recurr"
2021.emnlp-main.830,2021.acl-long.427,1,0.794571,"uli, 2019; Fan et al., 2020): 32 layers, 8 heads, 128 head dimensions, 1024 model dimensions, 4096 fully connected dimensions and dropout (Srivastava et al., 2014) and layer dropout rates of 0.2. We partition the training data into non-overlapping blocks of 512 contiguous tokens ignoring document boundaries and train the model to predict each token from left to right (Baevski and Auli, 2019). Validation and test perplexity are measured by predicting the last 256 words out of the input of 512 consecutive words to avoid evaluating tokens in the beginning with limited context (early token curse, Press et al., 2021). We generally follow the optimization method from Baevski and Auli (2019), but some hyperparameters, such as the learning rate for the T2R finetuning, are adjusted for better convergence than randomly initialized training. See Appendix A.1 for more details. 3.2.2 Machine Translation final model (Vaswani et al., 2017). In inference, we apply beam search with size 5 and length penalty 0.6. Consistent with previous practice, we evaluate with tokenized BLEU (Papineni et al., 2002). Further details are described in Appendix A.1. ppl. Model ELU + Random Init. RFA + Random Init. T2R + Random Init. E"
2021.emnlp-main.830,E17-2025,0,0.057825,"Missing"
2021.emnlp-main.830,2020.findings-emnlp.232,0,0.0258688,"ang et al., 2020; Tay et al., 2021) and adding a memory module that can access multiple tokens at once (Liu et al., 2018; Dai et al., 2019; Lee et al., 2019; Ainslie et al., 2020; Rae et al., 2020; Beltagy et al., 2020; Zaheer et al., 2020). Sparse Attention Patterns Another approach to reducing the time and memory overhead from the attention computation is to limit the tokens that are attended to by sparsifying the attention patterns. These patterns can be set in advance or learned during training (Tay et al., 2020b). For example, prior works introduced fixed patterns of blockwise attention (Qiu et al., 2020) and strided attention (Child et al., 2019; Beltagy et al., 2020; Zaheer et al., 2020). Other previous works presented methods to learn attention patterns from data (Sukhbaatar et al., 2019; Roy et al., 2020; Tay et al., 2020a). It should be noted that significant modifications 11 We do not consider random initialization baselines here are necessary to apply many of these methods to because random initialization makes it impossible to align attention heads and layers between models. autoregressive generation tasks such as language 10637 modeling and machine translation, and their empirical eva"
2021.emnlp-main.830,P16-1162,0,0.13063,"Missing"
2021.emnlp-main.830,P19-1032,0,0.0283712,"et al., 2020; Beltagy et al., 2020; Zaheer et al., 2020). Sparse Attention Patterns Another approach to reducing the time and memory overhead from the attention computation is to limit the tokens that are attended to by sparsifying the attention patterns. These patterns can be set in advance or learned during training (Tay et al., 2020b). For example, prior works introduced fixed patterns of blockwise attention (Qiu et al., 2020) and strided attention (Child et al., 2019; Beltagy et al., 2020; Zaheer et al., 2020). Other previous works presented methods to learn attention patterns from data (Sukhbaatar et al., 2019; Roy et al., 2020; Tay et al., 2020a). It should be noted that significant modifications 11 We do not consider random initialization baselines here are necessary to apply many of these methods to because random initialization makes it impossible to align attention heads and layers between models. autoregressive generation tasks such as language 10637 modeling and machine translation, and their empirical evaluation in these generation settings has yet to be conducted (Peng et al., 2021). This work presents extensive empirical evaluation in autoregressive generation settings. 6 Conclusion and F"
2021.emnlp-main.830,D19-1443,0,0.016994,"O(N hk). Comparing this with the pretrained transformer, we see that if the feature size is much smaller than input sequence lengths (k ≪ M, N ), the change in the attention stage from O(M N h) to O(hk(M + N )) in T2R brings a substantial speedup. Generation Memory Overhead T2R only needs to store the RNN state, and thus its space complexity is O(hk), constant in sequence length. This implies reduction in memory footprint when k ≪ M , compared to the transformer’s O(M h). 2.3 Autoregressive Linear Transformers In principle, any kernel function can be used as the similarity function in Eq. 2a (Tsai et al., 2019). Previous work proposed several untrainable feature map functions φ and developed autoregressive transformer variants with linear time and constant space complexity in sequence length (Katharopoulos et al., 2020; Peng et al., 2021; Choromanski et al., 2021). While those models follow similar computation steps to T2R, there are several differences in generation efficiency. Since the feature map in Katharopoulos et al. (2020) preserves input dimensions, the feature size is always the same as the head dimensions (k = d). This means that the speedup and memory savings from using a small feature s"
2021.emnlp-main.830,2020.acl-main.687,0,0.0180534,"it; otherwise it would disallow the linear complexity in causal attention. We conjecture that this is the reason why Performer becomes less stable in our experiments. We suspect that some techniques are necessary to improve numerical stability in language modeling and machine translation. We present extensive experiments on standard benchmarks for language modeling and machine translation. Our results show that T2R achieves 10633 is consistent with previous work that showed that causal attention can be more drastically simplified than cross attention in transformer machine translation models (You et al., 2020; Tay et al., 2021). 3.2.1 Language Modeling We use the WikiText-103 benchmark, which consists of 103M tokens sampled from English Wikipedia (Merity et al., 2017). We choose similar hyperparameters to prior work (Baevski and Auli, 2019; Fan et al., 2020): 32 layers, 8 heads, 128 head dimensions, 1024 model dimensions, 4096 fully connected dimensions and dropout (Srivastava et al., 2014) and layer dropout rates of 0.2. We partition the training data into non-overlapping blocks of 512 contiguous tokens ignoring document boundaries and train the model to predict each token from left to right (Bae"
2021.findings-acl.82,2020.emnlp-main.92,0,0.432146,"the RANDOMIZE model outperforms best-in-class graph transformers; Wang et al. 2020). Prior work, involving both GNNs and pretrained linearized models, has explored various ways of improving models’ sensitivity to the structure of the input graph. To better maintain fidelity to the graph, previous graph-to-text methods incorporate additional loss terms, specialized architectures, or generation-time ranking to influence the semantic accuracy of generation: ranking outputs by the correctness of the AMR parse (Mager et al., 2020; Harkous et al., 2020), jointly “back-parsing” graphs when decoding (Bai et al., 2020), or using distinct components to model different graph traversals (Ribeiro et al., 2019). These efforts suggest that explicitly accounting for graph structure can assist generation. Can we expand on this idea, and improve generation quality by inducing more robust internal graph representations? To answer this question, we propose secondary objectives designed to promote graph “awareness.” In addition to the above graph-to-text approaches, we also draw inspiration from denoising methods used in language model pretraining (Raffel et al., 2020; Lewis et al., 2020), as well as syntactic scaffold"
2021.findings-acl.82,W13-2322,0,0.0399391,"e and Rastogi, 2020; Harkous et al., 2020; Ribeiro et al., 2020). 945 1 Implementation available at github.com/ahoho/ transformers/tree/graph-promotion LDC2017T10 WebNLG N Dev. ppl. Avg. edges 36k 18k 21.1 9.2 11.4 3.0 Table 1: Dataset statistics. Perplexity estimated on the development set with GPT-2 (Radford et al., 2019) finetuned on the training data using default hyperparameters in the transformers library (gpt-2 model, Wolf et al., 2020). Graph-to-Text Generation Datasets We explore two datasets for generation from a graph structure to English text. Abstract Meaning Representation (AMR, Banarescu et al., 2013) is a formalism intended to represent the propositional meaning of utterances— “who is doing what to whom”—using graphs that have minimal dependence on the surface form. AMR graphs are directed and acyclic with a single “top” node (Goodman, 2020). They can be represented as either a graph, a tree, or sets of triples (van Noord and Bos, 2017). For our data, we use the AMR 2.0 release (LDC2017T10),2 both because it spans a varied set of domains and styles, and because of its extensive use in prior work. A simpler graph-to-text problem involves converting a set of RDF triples to natural text real"
2021.findings-acl.82,2020.acl-demos.35,0,0.455475,"ity estimated on the development set with GPT-2 (Radford et al., 2019) finetuned on the training data using default hyperparameters in the transformers library (gpt-2 model, Wolf et al., 2020). Graph-to-Text Generation Datasets We explore two datasets for generation from a graph structure to English text. Abstract Meaning Representation (AMR, Banarescu et al., 2013) is a formalism intended to represent the propositional meaning of utterances— “who is doing what to whom”—using graphs that have minimal dependence on the surface form. AMR graphs are directed and acyclic with a single “top” node (Goodman, 2020). They can be represented as either a graph, a tree, or sets of triples (van Noord and Bos, 2017). For our data, we use the AMR 2.0 release (LDC2017T10),2 both because it spans a varied set of domains and styles, and because of its extensive use in prior work. A simpler graph-to-text problem involves converting a set of RDF triples to natural text realizations of the information contained in the set, exemplified by the WebNLG dataset (Gardent et al., 2017). WebNLG pulls information from an existing knowledge base (DBPedia, Mendes et al., 2012) for a specific subset of 15 categories (e.g., “ast"
2021.findings-acl.82,2020.emnlp-main.703,0,0.0561837,"Missing"
2021.findings-acl.82,2020.acl-main.740,1,0.872291,"Missing"
2021.findings-acl.82,P13-2131,0,0.0199673,"adicals have crossed the long and porous border between the Kingdom and Iraq and joined up with Sunni Muslim insurgents there. Many young Saudi radicals have crossed the porous border from Iraq to the Kingdom and joined up with Sunni Islamic insurgents there. Many young Saudi radicals have crossed the porous long-term border with Iraq and joined up with Sunni Islamic insurgents there. Table 5: Selected predictions from the baseline and a model using the reordering-from-reconfigured scaffold (trained on the full data). Colored text denotes a semantically incorrect generation. rescoring metric (Cai and Knight, 2013). As seen in Fig. 4, there is a substantial negative relationship (Pearson’s ρ = −0.35∗ ) between these two variables, measured using outputs from the model trained with the reordering-from-reconfigured scaffold on the full data. To fully operationalize the above question, we estimate a linear regression on the M score of predicted sentences from the validation set. In this scenario, the linear regression can quantify how much variation in predicted sentences’ semantic fidelity (measured by the M-score) can be explained by model components and target sentence characteristics. If the coefficien"
2021.findings-acl.82,N18-2017,1,0.842909,"Missing"
2021.findings-acl.82,N19-1423,0,0.0223475,"Missing"
2021.findings-acl.82,N16-1087,1,0.841808,"ically transgressive (Bisk et al., 2020). Indeed, there is limited practical use for unconditional text generation: we expect language to relate to some identifiable, extrinsic meaning. When a system communicates information to an individual in natural language, it will typically rely on a structured representation of that information. Consequently, generating text that faithfully conveys structured data is an important goal in NLP, where inputs can take the form of tables (ToTTo, Parikh et al., 2020), RDF triples (e.g., WebNLG, Gardent et al., 2017), or Abstract Meaning Representations (AMR, Flanigan et al., 2016). NLP datasets in this domain consists of pairs of structured data (e.g., <henri_matisse, Finetuned Language Model To go the boy wants Figure 1: Diagram of our adversarial evaluation procedure for graph-to-text generation using pretrained language models (§3.2). (1) A graph can admit multiple possible linearizations. (2) Following standard practice, we train with a single linearization. (3) At evaluation time, we present the model with a meaning-preserving alternative. ∗ Work undertaken during an internship at AI2. hasOccupation, artist>) and a representation of that data in text (“Matisse is"
2021.findings-acl.82,W17-3518,0,0.370433,"fluent, but that closer inspection will often reveal to be semantically transgressive (Bisk et al., 2020). Indeed, there is limited practical use for unconditional text generation: we expect language to relate to some identifiable, extrinsic meaning. When a system communicates information to an individual in natural language, it will typically rely on a structured representation of that information. Consequently, generating text that faithfully conveys structured data is an important goal in NLP, where inputs can take the form of tables (ToTTo, Parikh et al., 2020), RDF triples (e.g., WebNLG, Gardent et al., 2017), or Abstract Meaning Representations (AMR, Flanigan et al., 2016). NLP datasets in this domain consists of pairs of structured data (e.g., <henri_matisse, Finetuned Language Model To go the boy wants Figure 1: Diagram of our adversarial evaluation procedure for graph-to-text generation using pretrained language models (§3.2). (1) A graph can admit multiple possible linearizations. (2) Following standard practice, we train with a single linearization. (3) At evaluation time, we present the model with a meaning-preserving alternative. ∗ Work undertaken during an internship at AI2. hasOccupation"
2021.findings-acl.82,2020.coling-main.218,0,0.259902,"Missing"
2021.findings-acl.82,D17-1215,0,0.0175626,"to superior generation over reasonable alternatives, e.g., DFS traversal paths (Mager et al., 2020). We will refer to the human-created linearizations in AMR corpora as CANONICAL, since annotators follow a standardized process. There is evidence that this format, in particular the relative ordering of edge types, leaks information about the associated sentence order (Konstas et al., 2017). We speculate that overparametrized models may overfit to such correlations rather than develop robust implicit graph encodings, since it has been repeatedly reported that large models use dataset shortcuts (Jia and Liang, 2017; Gururangan et al., 2018; Geva et al., 2019, among others). As an alternative linearization, Goodman (2020) defines the RECONFIGURE operation as creating a tree from an AMR graph, where order information from the canonical linearization is ignored, except for the top node (e.g., and in Figs. 2a and 2b). Although it is not a labeled element in the graph, the top node conveys structural information about the sentence—for instance, it is often the main verb. Reconfiguration can include reversals of edge labels (e.g., ARG0 to ARG0-of), therefore constituting a substantive change to the linearizat"
2021.findings-acl.82,2020.inlg-1.14,0,0.339698,"raphs corresponding to the sentence, “The film is a dream and, like a dream, is both fascinating and disturbing.” Note that the bolded relation in the graph, (resemble-01 :ARG1 and), is represented differently depending on the linearization. based encoders were introduced because they outperformed these sequence-to-sequence models. Recently, however, there has been a stark reversal: graph-encoder generation performance has been far surpassed by pretrained transformer language models (LMs) finetuned on pairs of linearized graphs and their corresponding surface realizations (Mager et al., 2020; Kale and Rastogi, 2020; Harkous et al., 2020; Ribeiro et al., 2020, henceforth termed pretrained linearized models). Moreover, both automated and human assessments indicate that text generated with LMs retains meaning at least as well as graph-encoding baselines (Mager et al., 2020). This is not the sole product of pretrained models’ general language knowledge: Mager et al. (2020), using a GPT-2-based (Radford et al., 2019) model, report that ablating structural graph information (e.g., edges) in the linearized representation notably degrades generation performance, particularly in AMR-to-text tasks. The remarkable"
2021.findings-acl.82,2020.emnlp-main.419,0,0.0251409,"in Table 2), but they ignore parentheses, only reodering node labels. Hence, their results cannot establish models’ graph-encoding ability, instead revealing that node 12 Human evaluation has been less thorough, although Mager et al. (2020) report improved human judgments on AMR-to-text generation. We note similar results in our own experiments. order is informative of word order, corroborating findings in Konstas et al. (2017). Both works, along with Mager et al. (2020), run ablations by removing parenthetical markers, finding that graph structure is necessary for strong generation. Finally, Kedzie and McKeown (2020), appearing contemporaneously to our work, seek to control the output generation by manipulating the input linearization order, using a randomization similar to ours as an “uncontrolled” baseline. Given their focus on task-oriented dialogue planning, which uses simpler meaning representations and sentences than the AMR dataset used here (i.e., shallower graphs and limited domains), we view their work as complementary to our own. 6 Conclusion In this work, we explore the graph-encoding ability of pretrained transformers through the lens of graph-to-text generation that relies on linearized grap"
2021.findings-acl.82,P17-1014,0,0.511984,"as graphs. Accordingly, advances in neural architectures designed to explicitly encode graphs, such as graph neural networks (GNNs, Kipf and Welling, 2017) and graph transformers, have been used in these graph-to-text settings (Zhu et al., 2019; Zhao et al., 2020; Wang et al., 2020, to name a few). But graphs can also be represented as text (see top portion of Fig. 1). Hence, as an alternative to constraining a model architecture with a graph structure, it is also possible to simply linearize a graph into a string and train a sequence-to-sequence model from scratch (Pourdamghani et al., 2016; Konstas et al., 2017; Vinyals et al., 2015). Graph944 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 944–956 August 1–6, 2021. ©2021 Association for Computational Linguistics (a / and (a / and (r / resemble-01 :op1 (d / dream-01 :op1 (d / dream-01 :ARG2 (d / dream-01 :ARG1 (f / film :ARG2-of (r / resemble-01) :op1-of (a / and :ARG0-of (d2 / disturb-01)) :ARG1 (f / film :op2 a2) :ARG2-of (r / resemble-01 :ARG0-of (f2 / fascinate-01) :ARG1 (f / film)) :ARG1 a2)) :ARG0-of d2)) :ARG1 (a2 / and :op2 (a2 / and :op2 (a2 / and :op1 (f2 / fascinate-01 :op1 (f2 / fascinate-01 :op2 (d2 / d"
2021.findings-acl.82,2020.acl-main.703,0,0.0295579,"k-parsing” graphs when decoding (Bai et al., 2020), or using distinct components to model different graph traversals (Ribeiro et al., 2019). These efforts suggest that explicitly accounting for graph structure can assist generation. Can we expand on this idea, and improve generation quality by inducing more robust internal graph representations? To answer this question, we propose secondary objectives designed to promote graph “awareness.” In addition to the above graph-to-text approaches, we also draw inspiration from denoising methods used in language model pretraining (Raffel et al., 2020; Lewis et al., 2020), as well as syntactic scaffolds that support semantic tasks with an auxiliary syntax-dependent loss (Swayamdipta et al., 2018). Intermediate auxiliary pretraining has been repeatedly shown to be successful in other contexts (Phang et al., 2018; Li et al., 2019; Gururangan et al., 2020). 4.1 Experimental Setup In particular, we propose unsupervised graphdenoising tasks that we train alongside AMR-totext generation, following the multi-task setup of Raffel et al. (2020). For each batch, we either optimize the likelihood in Section 2 or one of the objectives described below.7 7 Per-task batches"
2021.findings-acl.82,2020.coling-main.420,0,0.010489,"(Shazeer and Stern, 2018) with a learning rate of 0.0001, selected from the set {0.001, 0.0001, 3 × 10−5 , 1 × 10−5 , 1 × 10−6 } after tuning on 1000 training examples across five random seeds.4 We train until development set BLEU has not improved for 10 epochs. See Appendix A.1 for further details. Evaluation Measures As a primary metric, we evaluate generated text using BLEU (Papineni et al., 2002), calculated with SacreBLEU (Post, 2018). Despite its limitations in generation settings, BLEU still generally accords with rankings of models, either by human evaluations or by alternate metrics (Manning et al., 2020). We also evaluate our scaffolding models (§4) using BertScore (Zhang et al., 2020), which measures token similarity with contextual embeddings, permitting a more nuanced measure of semantic similarity. Lastly, we use the M portion of the MF-score (Opitz and Frank, 2020), which measures how well the source AMR graph can be reconstructed from the generated target sentence using an off-the-shelf parser. Unlike BLEU, which applies corpus-wide, this metric provides a best-guess at sentence-level accuracy. 3 RQ1: Robustness to Permutation of Graph Linearization In this section, we explore the exten"
2021.findings-acl.82,mendes-etal-2012-dbpedia,0,0.0240829,"Missing"
2021.findings-acl.82,P02-1040,0,0.110085,"ration on AMR (specifically, LDC2017T10) and WebNLG (Kale and Rastogi, 2020; Ribeiro et al., 2020). We modify the T5 implementation from the transformers library (Wolf et al., 2020).3 We 2 3 use the Adafactor optimizer (Shazeer and Stern, 2018) with a learning rate of 0.0001, selected from the set {0.001, 0.0001, 3 × 10−5 , 1 × 10−5 , 1 × 10−6 } after tuning on 1000 training examples across five random seeds.4 We train until development set BLEU has not improved for 10 epochs. See Appendix A.1 for further details. Evaluation Measures As a primary metric, we evaluate generated text using BLEU (Papineni et al., 2002), calculated with SacreBLEU (Post, 2018). Despite its limitations in generation settings, BLEU still generally accords with rankings of models, either by human evaluations or by alternate metrics (Manning et al., 2020). We also evaluate our scaffolding models (§4) using BertScore (Zhang et al., 2020), which measures token similarity with contextual embeddings, permitting a more nuanced measure of semantic similarity. Lastly, we use the M portion of the MF-score (Opitz and Frank, 2020), which measures how well the source AMR graph can be reconstructed from the generated target sentence using an"
2021.findings-acl.82,2020.emnlp-main.89,0,0.117034,"s succeed at generating text that is prima facie fluent, but that closer inspection will often reveal to be semantically transgressive (Bisk et al., 2020). Indeed, there is limited practical use for unconditional text generation: we expect language to relate to some identifiable, extrinsic meaning. When a system communicates information to an individual in natural language, it will typically rely on a structured representation of that information. Consequently, generating text that faithfully conveys structured data is an important goal in NLP, where inputs can take the form of tables (ToTTo, Parikh et al., 2020), RDF triples (e.g., WebNLG, Gardent et al., 2017), or Abstract Meaning Representations (AMR, Flanigan et al., 2016). NLP datasets in this domain consists of pairs of structured data (e.g., <henri_matisse, Finetuned Language Model To go the boy wants Figure 1: Diagram of our adversarial evaluation procedure for graph-to-text generation using pretrained language models (§3.2). (1) A graph can admit multiple possible linearizations. (2) Following standard practice, we train with a single linearization. (3) At evaluation time, we present the model with a meaning-preserving alternative. ∗ Work und"
2021.findings-acl.82,W18-6319,0,0.0194962,"(Kale and Rastogi, 2020; Ribeiro et al., 2020). We modify the T5 implementation from the transformers library (Wolf et al., 2020).3 We 2 3 use the Adafactor optimizer (Shazeer and Stern, 2018) with a learning rate of 0.0001, selected from the set {0.001, 0.0001, 3 × 10−5 , 1 × 10−5 , 1 × 10−6 } after tuning on 1000 training examples across five random seeds.4 We train until development set BLEU has not improved for 10 epochs. See Appendix A.1 for further details. Evaluation Measures As a primary metric, we evaluate generated text using BLEU (Papineni et al., 2002), calculated with SacreBLEU (Post, 2018). Despite its limitations in generation settings, BLEU still generally accords with rankings of models, either by human evaluations or by alternate metrics (Manning et al., 2020). We also evaluate our scaffolding models (§4) using BertScore (Zhang et al., 2020), which measures token similarity with contextual embeddings, permitting a more nuanced measure of semantic similarity. Lastly, we use the M portion of the MF-score (Opitz and Frank, 2020), which measures how well the source AMR graph can be reconstructed from the generated target sentence using an off-the-shelf parser. Unlike BLEU, whic"
2021.findings-acl.82,W16-6603,0,0.183678,"s of inputs can be encoded as graphs. Accordingly, advances in neural architectures designed to explicitly encode graphs, such as graph neural networks (GNNs, Kipf and Welling, 2017) and graph transformers, have been used in these graph-to-text settings (Zhu et al., 2019; Zhao et al., 2020; Wang et al., 2020, to name a few). But graphs can also be represented as text (see top portion of Fig. 1). Hence, as an alternative to constraining a model architecture with a graph structure, it is also possible to simply linearize a graph into a string and train a sequence-to-sequence model from scratch (Pourdamghani et al., 2016; Konstas et al., 2017; Vinyals et al., 2015). Graph944 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 944–956 August 1–6, 2021. ©2021 Association for Computational Linguistics (a / and (a / and (r / resemble-01 :op1 (d / dream-01 :op1 (d / dream-01 :ARG2 (d / dream-01 :ARG1 (f / film :ARG2-of (r / resemble-01) :op1-of (a / and :ARG0-of (d2 / disturb-01)) :ARG1 (f / film :op2 a2) :ARG2-of (r / resemble-01 :ARG0-of (f2 / fascinate-01) :ARG1 (f / film)) :ARG1 a2)) :ARG0-of d2)) :ARG1 (a2 / and :op2 (a2 / and :op2 (a2 / and :op1 (f2 / fascinate-01 :op1 (f2 / fas"
2021.findings-acl.82,D19-1314,0,0.247954,"ets this criterion, we call it linearization-invariant. catalog.ldc.upenn.edu/LDC2017T10 We use T5-Base for WebNLG and T5-Large for AMR, 946 finding that the larger model did not benefit the WebNLG task. 4 Less extensive experiments with the full dataset indicated the same optimal setting, although in general it is relatively robust to learning rate. 3.1 Experimental Setup mation to construct the surface sentence. We parse, reconfigure, and randomize graphs using the Penman library (Goodman, 2020),6 then replace variable names with their references and remove word sense information, following Ribeiro et al. (2019). To better understand models’ graph-encoding behavior, we experiment with adversarial linearization strategies in two graph-to-text settings.5 Permutations of AMR-Graph Linearizations Standard AMR corpora are linearized as spanning trees over the graphs in PENMAN notation (Matthiessen and Bateman 1991, see Fig. 2a). In the present work, we also linearize graphs using PENMAN , doing so for several reasons: (1) it is sufficiently flexible to accommodate significant changes to the linearization, discussed below; (2) it is more concise than sets of directed triples, both reducing training time an"
2021.findings-acl.82,D18-1412,1,0.914945,"re our investigation by first testing whether models’ encodings are invariant to the linearization strategy—the way in which a graph is traversed and encoded when producing the linearized representation (see Figure 1). We discover that generation suffers under random permutations of the linearization, and embrace a simple-but-effective training strategy to mitigate this problem: adversarial training (Goodfellow et al., 2015). Motivated by this finding, we encourage more faithful encodings of graph structure via denoising objectives in the more complex AMR setting. This multi-task scaffolding (Swayamdipta et al., 2018) reveals that straightforward masking of the graph input is sufficient to improve generation quality in low resource settings.1 Moreover, when treating this denoising performance as a proxy for the quality of models’ implicit graph encoding, we find that it correlates with the semantic fidelity of the resulting generation better than reasonable alternatives, suggesting possibilities for future evaluation metrics. We organize our investigation around two research questions: RQ1 To what extent are pretrained linearized models invariant to graph linearization strategy? (§3) RQ2 Does encouraging p"
2021.findings-acl.82,2020.tacl-1.2,0,0.0809908,"ing standard practice, we train with a single linearization. (3) At evaluation time, we present the model with a meaning-preserving alternative. ∗ Work undertaken during an internship at AI2. hasOccupation, artist>) and a representation of that data in text (“Matisse is an artist.”). Importantly, these types of inputs can be encoded as graphs. Accordingly, advances in neural architectures designed to explicitly encode graphs, such as graph neural networks (GNNs, Kipf and Welling, 2017) and graph transformers, have been used in these graph-to-text settings (Zhu et al., 2019; Zhao et al., 2020; Wang et al., 2020, to name a few). But graphs can also be represented as text (see top portion of Fig. 1). Hence, as an alternative to constraining a model architecture with a graph structure, it is also possible to simply linearize a graph into a string and train a sequence-to-sequence model from scratch (Pourdamghani et al., 2016; Konstas et al., 2017; Vinyals et al., 2015). Graph944 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 944–956 August 1–6, 2021. ©2021 Association for Computational Linguistics (a / and (a / and (r / resemble-01 :op1 (d / dream-01 :op1 (d / dream-01"
2021.findings-acl.82,2020.acl-main.224,0,0.0353218,"zations. (2) Following standard practice, we train with a single linearization. (3) At evaluation time, we present the model with a meaning-preserving alternative. ∗ Work undertaken during an internship at AI2. hasOccupation, artist>) and a representation of that data in text (“Matisse is an artist.”). Importantly, these types of inputs can be encoded as graphs. Accordingly, advances in neural architectures designed to explicitly encode graphs, such as graph neural networks (GNNs, Kipf and Welling, 2017) and graph transformers, have been used in these graph-to-text settings (Zhu et al., 2019; Zhao et al., 2020; Wang et al., 2020, to name a few). But graphs can also be represented as text (see top portion of Fig. 1). Hence, as an alternative to constraining a model architecture with a graph structure, it is also possible to simply linearize a graph into a string and train a sequence-to-sequence model from scratch (Pourdamghani et al., 2016; Konstas et al., 2017; Vinyals et al., 2015). Graph944 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 944–956 August 1–6, 2021. ©2021 Association for Computational Linguistics (a / and (a / and (r / resemble-01 :op1 (d / dream-01"
2021.findings-acl.82,D19-1548,0,0.0170565,"e possible linearizations. (2) Following standard practice, we train with a single linearization. (3) At evaluation time, we present the model with a meaning-preserving alternative. ∗ Work undertaken during an internship at AI2. hasOccupation, artist>) and a representation of that data in text (“Matisse is an artist.”). Importantly, these types of inputs can be encoded as graphs. Accordingly, advances in neural architectures designed to explicitly encode graphs, such as graph neural networks (GNNs, Kipf and Welling, 2017) and graph transformers, have been used in these graph-to-text settings (Zhu et al., 2019; Zhao et al., 2020; Wang et al., 2020, to name a few). But graphs can also be represented as text (see top portion of Fig. 1). Hence, as an alternative to constraining a model architecture with a graph structure, it is also possible to simply linearize a graph into a string and train a sequence-to-sequence model from scratch (Pourdamghani et al., 2016; Konstas et al., 2017; Vinyals et al., 2015). Graph944 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 944–956 August 1–6, 2021. ©2021 Association for Computational Linguistics (a / and (a / and (r / resemble-01"
2021.findings-emnlp.342,D13-1170,0,0.00334594,"ons drawn with a larger budget. Here the high variance of UBB likely plays a role the stability of its predictions; while it may be unbiased, the lower variance estimators are more reliable. 4.1 5 Experimental Setup We proceed by performing a sensitivity analysis: we run 100 trials of random hyperparameter search (far more than is typically necessary to establish that one model outperforms another in current practice) for a CNN (Kim, 2014) and a linear bag-ofembedding (LBoE) (Yogatama and Smith, 2015). These models are trained on the Stanford sentiment treebank 5-way text classification task (Socher et al., 2013). We include details about the dataset (and a link to download it) in Appendix B. For all three estimators, the CNN has higher expected performance than the LBoE, for all n ≤ B.3 We then simulate a more practical scenario 3 See Appendix C for details. Figure 3 shows expected validation curves for B = 100 for all three estimators; with Conclusion Drawing reproducible conclusions from our experimental results is of paramount importance to NLP researchers, practitioners, and users of language technologies. Expected validation performance curves are tools for comparing the results of hyperparamete"
2021.findings-emnlp.342,2020.acl-main.246,0,0.0869282,"B). This is estimating what the maxiMSE strikes a balance between bias and varimum of n trials would be, in expectation; this is ance, displaying a classic bias-variance tradethus a statistical estimation problem. The formulaoff. We use expected validation performance to compare between different models, and antion was introduced by Dodge et al. (2019), who alyze how frequently each estimator leads to proposed a first estimator (defined as VnB in Equadrawing incorrect conclusions about which of tion 2). This estimator was later shown to be biased two models performs best. We find that the by Tang et al. (2020), who introduced an unbiased two biased estimators lead to the fewest incorestimator (defined as UnB in Equation 3) for the rect conclusions, which hints at the importance same expected maximum. of minimizing variance and MSE. In Section 2 we use tools from combinatorics to 1 Introduction derive both previously-introduced estimators, relate Drawing robust conclusions when comparing dif- them to each other, and show that they make two opposing assumptions; we show that changing only ferent methods in natural language processing is one of these assumptions instead of both leads to a central to s"
2021.findings-emnlp.342,D15-1251,1,0.829312,"a practitioner would care about: the frequency with which one draws conclusions that would be consistent with conclusions drawn with a larger budget. Here the high variance of UBB likely plays a role the stability of its predictions; while it may be unbiased, the lower variance estimators are more reliable. 4.1 5 Experimental Setup We proceed by performing a sensitivity analysis: we run 100 trials of random hyperparameter search (far more than is typically necessary to establish that one model outperforms another in current practice) for a CNN (Kim, 2014) and a linear bag-ofembedding (LBoE) (Yogatama and Smith, 2015). These models are trained on the Stanford sentiment treebank 5-way text classification task (Socher et al., 2013). We include details about the dataset (and a link to download it) in Appendix B. For all three estimators, the CNN has higher expected performance than the LBoE, for all n ≤ B.3 We then simulate a more practical scenario 3 See Appendix C for details. Figure 3 shows expected validation curves for B = 100 for all three estimators; with Conclusion Drawing reproducible conclusions from our experimental results is of paramount importance to NLP researchers, practitioners, and users of"
2021.findings-emnlp.71,2020.acl-tutorials.1,0,0.153859,"trained on very large corpora (Peters et al., Finally, we compare probes across time with re2018; Devlin et al., 2019; Radford et al., 2018; search benchmark task performance across time Liu et al., 2019b; Brown et al., 2020). Many re- (§5). We find that most of these benchmark tasks searchers have sought to interpret what kinds of (e.g., SST-2, Socher et al., 2013, and SQuAD, knowledge are acquired during this “pretraining” Rajpurkar et al., 2016) require a relatively small phase (Clark et al., 2019; Hao et al., 2019; Koval- number of pretraining steps to achieve high perforeva et al., 2019; Belinkov et al., 2020). Extend- mance, which is similar to the fast learning patterns ing Chiang et al. (2020), we systematically con- shown by linguistic probes. Some other tasks that duct probing across the pretraining iterations, to are designed to test more complex knowledge (e.g., understand not just what is learned (as explored in ReCoRD, Zhang et al., 2018 and WSC, Levesque numerous past analyses of fixed, already-trained et al., 2012) benefit from longer pretraining time, ∗ Equal contribution. aligning well with our findings for the correspond820 Findings of the Association for Computational Linguistics: EM"
2021.findings-emnlp.71,2020.emnlp-main.553,0,0.0442426,"re2018; Devlin et al., 2019; Radford et al., 2018; search benchmark task performance across time Liu et al., 2019b; Brown et al., 2020). Many re- (§5). We find that most of these benchmark tasks searchers have sought to interpret what kinds of (e.g., SST-2, Socher et al., 2013, and SQuAD, knowledge are acquired during this “pretraining” Rajpurkar et al., 2016) require a relatively small phase (Clark et al., 2019; Hao et al., 2019; Koval- number of pretraining steps to achieve high perforeva et al., 2019; Belinkov et al., 2020). Extend- mance, which is similar to the fast learning patterns ing Chiang et al. (2020), we systematically con- shown by linguistic probes. Some other tasks that duct probing across the pretraining iterations, to are designed to test more complex knowledge (e.g., understand not just what is learned (as explored in ReCoRD, Zhang et al., 2018 and WSC, Levesque numerous past analyses of fixed, already-trained et al., 2012) benefit from longer pretraining time, ∗ Equal contribution. aligning well with our findings for the correspond820 Findings of the Association for Computational Linguistics: EMNLP 2021, pages 820–842 November 7–11, 2021. ©2021 Association for Computational Linguis"
2021.findings-emnlp.71,W19-4828,0,0.0190918,"iverse Current NLP approaches lean heavily on language domains is more important than the quantity alone. models trained on very large corpora (Peters et al., Finally, we compare probes across time with re2018; Devlin et al., 2019; Radford et al., 2018; search benchmark task performance across time Liu et al., 2019b; Brown et al., 2020). Many re- (§5). We find that most of these benchmark tasks searchers have sought to interpret what kinds of (e.g., SST-2, Socher et al., 2013, and SQuAD, knowledge are acquired during this “pretraining” Rajpurkar et al., 2016) require a relatively small phase (Clark et al., 2019; Hao et al., 2019; Koval- number of pretraining steps to achieve high perforeva et al., 2019; Belinkov et al., 2020). Extend- mance, which is similar to the fast learning patterns ing Chiang et al. (2020), we systematically con- shown by linguistic probes. Some other tasks that duct probing across the pretraining iterations, to are designed to test more complex knowledge (e.g., understand not just what is learned (as explored in ReCoRD, Zhang et al., 2018 and WSC, Levesque numerous past analyses of fixed, already-trained et al., 2012) benefit from longer pretraining time, ∗ Equal contribution"
2021.findings-emnlp.71,N19-1423,0,0.195512,"led learning that these models undergo and patterns of different types of knowledge generally guide us toward more efficient approaches that hold regardless of the data variation. However, difaccomplish necessary learning faster. ferent data choices do have an impact on the learning speed and the final performance. Our findings 1 Introduction suggest that the inclusion of data in more diverse Current NLP approaches lean heavily on language domains is more important than the quantity alone. models trained on very large corpora (Peters et al., Finally, we compare probes across time with re2018; Devlin et al., 2019; Radford et al., 2018; search benchmark task performance across time Liu et al., 2019b; Brown et al., 2020). Many re- (§5). We find that most of these benchmark tasks searchers have sought to interpret what kinds of (e.g., SST-2, Socher et al., 2013, and SQuAD, knowledge are acquired during this “pretraining” Rajpurkar et al., 2016) require a relatively small phase (Clark et al., 2019; Hao et al., 2019; Koval- number of pretraining steps to achieve high perforeva et al., 2019; Belinkov et al., 2020). Extend- mance, which is similar to the fast learning patterns ing Chiang et al. (2020), we sy"
2021.findings-emnlp.71,I05-5002,0,0.0104199,"Missing"
2021.findings-emnlp.71,D19-1445,0,0.0347624,"Missing"
2021.findings-emnlp.71,N19-1112,1,0.691418,"m future work on more efficient pretraining (e.g., Models of language trained on very large corfewer iterations are needed to acquire some kinds pora have been demonstrated useful for natof knowledge) and on understanding dependencies ural language processing. As fixed artifacts, among different kinds of knowledge. they have become the object of intense study, Specifically, we apply a probing across time with many researchers “probing” the extent to which they acquire and readily demonstrate framework to the widely used RoBERTa masked linguistic abstractions, factual and commonlanguage model (Liu et al., 2019b). We reproduce sense knowledge, and reasoning abilities. Rethe pretraining of RoBERTa and apply a suite of cent work applied several probes to intermeprobes at many checkpoint iterations across prediate training stages to observe the developtraining (§3). Our rich probe suite covers a dimental process of a large-scale model (Chiverse range of desirable knowledge types: linguisang et al., 2020). Following this effort, we tic properties (Liu et al., 2019a), factual knowledge systematically answer a question: for vari(Petroni et al., 2019), and commonsense (Zhou ous types of knowledge a languag"
2021.findings-emnlp.71,N16-1098,0,0.0628918,"Missing"
2021.findings-emnlp.71,2020.acl-main.240,0,0.013765,"transferability (LKT) of contextual representations. For all tasks in LKT, we train a linear classifier model to predict the linguistic annotation of each word in a sentence. Through the performance of the classifier, we measure how closely the encoded information in the word representations conforms to linguistic annotations from human experts. Following Liu et al. (2019a), we use learnable coefficients to weigh a sum of representations from all the transformer layers, and compute the input vector to the classifier. We measure the probe performance by accuracy or F1 on the test sets. BL I MP Salazar et al. (2020) introduce a behavioral linguistic probe suite on the benchmark of linguistic minimal pairs (BL I MP, Warstadt et al., 2020). This benchmark isolates specific phenomena in syntax, morphology, or semantics such as island effects and subject-verb agreement. As seen in Table 1, input sentence pairs differ only by a word or a short phrase, but contrast in grammatical 2.1 Probe Suite Construction acceptability. We test whether RoBERTa scores the grammatical sentence higher than the ungramBelinkov et al. (2020) categorize existing probes matical one. The score for a sentence is calcuinto two familie"
2021.findings-emnlp.71,N19-1329,0,0.0483699,"Missing"
2021.findings-emnlp.71,D13-1170,0,0.0127317,"do have an impact on the learning speed and the final performance. Our findings 1 Introduction suggest that the inclusion of data in more diverse Current NLP approaches lean heavily on language domains is more important than the quantity alone. models trained on very large corpora (Peters et al., Finally, we compare probes across time with re2018; Devlin et al., 2019; Radford et al., 2018; search benchmark task performance across time Liu et al., 2019b; Brown et al., 2020). Many re- (§5). We find that most of these benchmark tasks searchers have sought to interpret what kinds of (e.g., SST-2, Socher et al., 2013, and SQuAD, knowledge are acquired during this “pretraining” Rajpurkar et al., 2016) require a relatively small phase (Clark et al., 2019; Hao et al., 2019; Koval- number of pretraining steps to achieve high perforeva et al., 2019; Belinkov et al., 2020). Extend- mance, which is similar to the fast learning patterns ing Chiang et al. (2020), we systematically con- shown by linguistic probes. Some other tasks that duct probing across the pretraining iterations, to are designed to test more complex knowledge (e.g., understand not just what is learned (as explored in ReCoRD, Zhang et al., 2018 a"
2021.findings-emnlp.71,2020.tacl-1.25,0,0.0156644,"e linguistic annotation of each word in a sentence. Through the performance of the classifier, we measure how closely the encoded information in the word representations conforms to linguistic annotations from human experts. Following Liu et al. (2019a), we use learnable coefficients to weigh a sum of representations from all the transformer layers, and compute the input vector to the classifier. We measure the probe performance by accuracy or F1 on the test sets. BL I MP Salazar et al. (2020) introduce a behavioral linguistic probe suite on the benchmark of linguistic minimal pairs (BL I MP, Warstadt et al., 2020). This benchmark isolates specific phenomena in syntax, morphology, or semantics such as island effects and subject-verb agreement. As seen in Table 1, input sentence pairs differ only by a word or a short phrase, but contrast in grammatical 2.1 Probe Suite Construction acceptability. We test whether RoBERTa scores the grammatical sentence higher than the ungramBelinkov et al. (2020) categorize existing probes matical one. The score for a sentence is calcuinto two families. Structural probes train a lated by sequentially masking one word at a time lightweight classifier that predicts a label o"
2021.findings-emnlp.71,D14-1162,0,0.0850785,"er targeted knowledge can be easily extracted with few or zero additional parameters (i.e., ease of extraction, as suggested by Pimentel et al., 2020). We treat the probing scores as relative performance; and we address their concerns by comparing RoBERTa with the the following baselines: • Random Guess randomly selects one class label or token from the candidate pool. • Random Vector + Linear Classifier uses a random vector to represent each type, and trains a linear classifier on the top to predict the label with the token vector being frozen. • GloVe + Linear Classifier uses GloVe vectors (Pennington et al., 2014), and trains a linear classifier on top to predict the label. • Original RoBERTa probes the officially released checkpoint2 of RoBERTa base to see if our checkpoints are pretrained properly and can achieve reasonable performance. Moreover, our probing results on different checkpoints can illustrate the relative performance change during pretraining. 2.3 Pretraining Setups We choose base-size RoBERTa as a case study. In order to conduct probing over time, we replicate the RoBERTa pretraining procedure and periodically save checkpoints for later probing. To ensure that probe-task relevant text i"
2021.findings-emnlp.71,Q19-1040,0,0.044648,"Missing"
2021.findings-emnlp.71,N18-1202,0,0.176215,"Missing"
2021.findings-emnlp.71,N18-1101,0,0.0477928,"Missing"
2021.findings-emnlp.71,D19-1250,0,0.113859,"d linguistic abstractions, factual and commonlanguage model (Liu et al., 2019b). We reproduce sense knowledge, and reasoning abilities. Rethe pretraining of RoBERTa and apply a suite of cent work applied several probes to intermeprobes at many checkpoint iterations across prediate training stages to observe the developtraining (§3). Our rich probe suite covers a dimental process of a large-scale model (Chiverse range of desirable knowledge types: linguisang et al., 2020). Following this effort, we tic properties (Liu et al., 2019a), factual knowledge systematically answer a question: for vari(Petroni et al., 2019), and commonsense (Zhou ous types of knowledge a language model learns, when during (pre)training are they acet al., 2020) and basic reasoning capabilities (Talquired? Using RoBERTa as a case study, we mor et al., 2019). Our main finding is that linguistic find: linguistic knowledge is acquired fast, stainformation tends to be acquired fast, factual and bly, and robustly across domains. Facts and commonsense knowledge slower, and reasoning commonsense are slower and more domainabilities are largely unlearned. sensitive. Reasoning abilities are, in general, We next apply probing across time to"
2021.findings-emnlp.71,2020.acl-main.420,0,0.0436311,"into pretraining RoBERTa. 2.2 Baselines for Relative Performance Probes are not a perfect, absolute measure of encoded knowledge. In particular, Hewitt and Liang (2019) find that probing classifiers can memorize labeling decisions independently of the linguistic knowledge of the representations. Pimentel et al. Talmor et al. (2019) introduce a be- (2020) argue that a tighter estimate of the encoded 822 knowledge can be obtained by complex probing models. We ask whether targeted knowledge can be easily extracted with few or zero additional parameters (i.e., ease of extraction, as suggested by Pimentel et al., 2020). We treat the probing scores as relative performance; and we address their concerns by comparing RoBERTa with the the following baselines: • Random Guess randomly selects one class label or token from the candidate pool. • Random Vector + Linear Classifier uses a random vector to represent each type, and trains a linear classifier on the top to predict the label with the token vector being frozen. • GloVe + Linear Classifier uses GloVe vectors (Pennington et al., 2014), and trains a linear classifier on top to predict the label. • Original RoBERTa probes the officially released checkpoint2 of"
2021.findings-emnlp.71,2020.emnlp-main.58,0,0.0334738,"Missing"
2021.mrl-1.5,2020.acl-main.536,0,0.361634,"ach is derived from analogous monolingual models (Devlin et al., 2019; Liu et al., 2019; Peters et al., 2018). However, considering the diversity of the world’s languages and the great data imbalance among them, it is natural to question whether the current multilingual paradigm can be improved upon for low-resource languages. Indeed, past work has demonstrated that it can. For instance, Wu and Dredze (2020) find that multilingual models often lag behind non-contextualized baselines for the lowest-resource languages in their training data, drawing into question their utility in such settings. Conneau et al. (2020a) posit that this phenomenon is a result of limited model capacity, which proves to be a bottleneck for sufficient transfer to low-resource languages. In fact, with multilingual models only being pretrained on a limited set of languages, most of the world’s languages are unseen by the model. For such languages, the performance of such models is even worse (Chau et al., 2020), due in part to the diversity of scripts across the world’s languages (Muller et al., 2021; Pfeiffer et al., 2021b; Rust et al., 2021) as compared to the models’ Latin-centricity (Ács, 2019). Nonetheless, there have been"
2021.mrl-1.5,N19-1423,0,0.660816,"ges fall into the latter category, even some with a high number of speakers. This presents unique challenges compared to high-resource languages: effectively modeling low-resource languages involves both accurately tokenizing text in such languages and maximally leveraging the limited available data. One common approach to low-resource NLP is the multilingual paradigm, in which methods that have shown success in English are applied to the union of many languages’ data,1 enabling transfer between languages. For instance, multilingual contextual word representations (CWRs) from language models (Devlin et al., 2019; Huang et al., 2019; Lample and Conneau, 2019, inter alia) are conventionally “pretrained” on large multilingual 1 2 Within the multilingual paradigm, a distinction is sometimes made between massively multilingual methods, which consider tens or hundreds of languages; and polyglot methods, which use only a handful. In this paper, all mentions of “multilingual” refer to the former. We use specialization to denote preparing a model for use on a specific target language, to the exclusion of others. This is a subset of adaptation, which includes all techniques that adjust a model for use on targe"
2021.mrl-1.5,W18-2501,0,0.0281376,"00 and a linear warmup of 1 epoch. For LAPT, TVA, and BERT, we train for up to 20 epochs total, selecting the highest-performing epoch based on validation masked language modeling loss. FAST T models are trained with the skipgram model for five epochs, with the default hyperparameters of Bojanowski et al. (2017). Training of downstream parsers and taggers follows Chau et al. (2020) and Kondratyuk and Straka (2019), with an inverse square-root learning rate decay and linear warmup, and layer-wise gradual unfreezing and discriminative finetuning. Models are trained with AllenNLP, version 2.1.0 (Gardner et al., 2018), for up to 200 epochs with early stopping based on validation performance. We choose batch sizes to be the maximum that allows for successful training on one GPU. 54 Rep. TVA BE * (0) 68.84 ± 7.16 91.00 ± 0.30 94.57 ± 0.45 95.74 ± 0.44 95.28 ± 0.51 Rep. BE FAST T BERT M B ERT LAPT BG (0) 88.86 ± 0.37 94.48 ± 0.10 96.98 ± 0.08 97.15 ± 0.04 97.20 ± 0.06 GA (1) 86.87 ± 2.55 90.36 ± 0.20 91.91 ± 0.25 93.28 ± 0.19 93.33 ± 0.16 MT (2) 89.68 ± 2.15 92.61 ± 0.10 94.01 ± 0.17 95.76 ± 0.09 96.33 ± 0.09 UG (2) 89.45 ± 1.37 90.87 ± 0.13 78.07 ± 0.22 79.88 ± 0.27 91.49 ± 0.13 UR (0) 90.81 ± 0.31 89.88 ± 0"
2021.mrl-1.5,2020.acl-main.740,1,0.841385,"Missing"
2021.mrl-1.5,D19-1433,0,0.0200009,"reserving the remainder; and Muller et al. (2021), who transliterate target language data into Latin script to improve vocabulary coverage. We deliver new insights on the effectiveness and applicability of these methods. Adapting Language Models Several prior studies have proposed methods for adapting pretrained models to a downstream task. The simplest of these is to perform additional pretraining on unlabeled data in the target language (Chau et al., 2020; Muller et al., 2021; Pfeiffer et al., 2020), which in turn builds off similar approaches for domain adaptation (Gururangan et al., 2020; Han and Eisenstein, 2019). Recent work uses one or more of these additional pretraining stages to specifically train modular adapter layers for specific tasks or languages, with the goal of maintaining a language-agnostic model while improving performance on individual languages (Pfeiffer et al., 2020, 2021a; Vidoni et al., 2020). However, as Muller et al. (2021) note, the typological diversity of the world’s languages ultimately limits the viability of this approach. On the other hand, many adaptation techniques have focused on improving representation of the target language by modifying the model’s vocabulary or tok"
2021.mrl-1.5,D19-1252,0,0.019849,"ter category, even some with a high number of speakers. This presents unique challenges compared to high-resource languages: effectively modeling low-resource languages involves both accurately tokenizing text in such languages and maximally leveraging the limited available data. One common approach to low-resource NLP is the multilingual paradigm, in which methods that have shown success in English are applied to the union of many languages’ data,1 enabling transfer between languages. For instance, multilingual contextual word representations (CWRs) from language models (Devlin et al., 2019; Huang et al., 2019; Lample and Conneau, 2019, inter alia) are conventionally “pretrained” on large multilingual 1 2 Within the multilingual paradigm, a distinction is sometimes made between massively multilingual methods, which consider tens or hundreds of languages; and polyglot methods, which use only a handful. In this paper, all mentions of “multilingual” refer to the former. We use specialization to denote preparing a model for use on a specific target language, to the exclusion of others. This is a subset of adaptation, which includes all techniques that adjust a model for use on target languages, regardl"
2021.mrl-1.5,2020.acl-main.421,0,0.0344116,"apter layers for specific tasks or languages, with the goal of maintaining a language-agnostic model while improving performance on individual languages (Pfeiffer et al., 2020, 2021a; Vidoni et al., 2020). However, as Muller et al. (2021) note, the typological diversity of the world’s languages ultimately limits the viability of this approach. On the other hand, many adaptation techniques have focused on improving representation of the target language by modifying the model’s vocabulary or tokenization schemes (Chung et al., 2020; Clark et al., 2021; Wang et al., 2021). This is wellmotivated: Artetxe et al. (2020) emphasize representation in the vocabulary as a key factor for effective crosslingual transfer, while Rust et al. (2021) find that M B ERT’s tokenization scheme for many languages is subpar. Pfeiffer et al. (2021b) further 5 Conclusion We explore the interactions between vocabulary augmentation and script transliteration for specializing multilingual contextual word representations in low-resource settings. We confirm vocabulary augmentation’s effectiveness on multiple languages, scripts, and tasks; identify the mix-in stage as amenable to specialization; and observe a negative interaction be"
2021.mrl-1.5,Q17-1010,0,0.318986,"ove the transformer outputs. We train models on five different random seeds and report average scores and standard errors. 2.2.1 2.2.2 Baselines To measure the effectiveness of TVA, we benchmark it against unadapted M B ERT, as well as directly pretraining M B ERT on the unlabeled data without modifying the vocabulary (Chau et al., 2020; Muller et al., 2021; Pfeiffer et al., 2020). Following Chau et al. (2020), we refer to the latter approach as language-adaptive pretraining (LAPT). We also evaluate two monolingual baselines that are trained on our unlabeled data: fastText embeddings (FAST T; Bojanowski et al., 2017), which represent a static word vector approach; and a BERT model trained from scratch (BERT). For Languages and Datasets We select a set of nine typologically diverse lowresource languages for evaluation, including three of the original four used by Chau et al. (2020). These languages use three different scripts and are chosen based on the availability of labeled datasets and their exemplification of the three language types identified by Chau et al. (2020). Of the lan6 Based on https://meta.wikimedia.org/ wiki/List_of_Wikipedias. 53 Language Bulgarian (BG) Belarusian (BE) Meadow Mari (MHR) V"
2021.mrl-1.5,D19-1279,0,0.332288,"(2020), which we recast more generally in light of recent work (§2.1). We evaluate their claims on three different tasks, using a diverse set of languages in multiple scripts (§2.2), and find that the results hold to an even more pronounced degree in unseen low-resource languages with non-Latin scripts (§2.3). 2.1 Method Overview Following Chau et al. (2020), we consider how to apply the pretrained multilingual BERT model (M B ERT; Devlin et al., 2019) to a target lowresource language, for which both labeled and unlabeled data is scarce. This model has produced strong CWRs for many languages (Kondratyuk and Straka, 2019, inter alia) and has been the starting model for many studies on low-resource languages (Muller et al., 2021; Pfeiffer et al., 2020; Wang et al., 2020). M B ERT covers the languages with the 104 largest Wikipedias, and it uses this data to con4 Muller et al. (2021) further subdivide Type 2 into Easy, Medium, and Hard languages, based on the performance of M B ERT after being exposed to these languages. However, this categorization cannot be determined a priori for a given language. 5 The “unknown” wordpiece is inserted when the wordpiece algorithm is unable to segment a word-level token with"
2021.mrl-1.5,2020.findings-emnlp.118,1,0.0592323,"instance, Wu and Dredze (2020) find that multilingual models often lag behind non-contextualized baselines for the lowest-resource languages in their training data, drawing into question their utility in such settings. Conneau et al. (2020a) posit that this phenomenon is a result of limited model capacity, which proves to be a bottleneck for sufficient transfer to low-resource languages. In fact, with multilingual models only being pretrained on a limited set of languages, most of the world’s languages are unseen by the model. For such languages, the performance of such models is even worse (Chau et al., 2020), due in part to the diversity of scripts across the world’s languages (Muller et al., 2021; Pfeiffer et al., 2021b; Rust et al., 2021) as compared to the models’ Latin-centricity (Ács, 2019). Nonetheless, there have been multiple attempts to remedy this discrepancy by specializing2 a multilingual model to a given target low-resource language, from which we take inspiration. Among them, Chau et al. (2020) augment the model’s vocabulary to more effectively tokenize text, then pretrain on a small amount of data in the target language; they report significant performance improvements on a small s"
2021.mrl-1.5,D18-2012,0,0.057658,"Missing"
2021.mrl-1.5,2020.emnlp-main.367,0,0.0364699,"one or more of these additional pretraining stages to specifically train modular adapter layers for specific tasks or languages, with the goal of maintaining a language-agnostic model while improving performance on individual languages (Pfeiffer et al., 2020, 2021a; Vidoni et al., 2020). However, as Muller et al. (2021) note, the typological diversity of the world’s languages ultimately limits the viability of this approach. On the other hand, many adaptation techniques have focused on improving representation of the target language by modifying the model’s vocabulary or tokenization schemes (Chung et al., 2020; Clark et al., 2021; Wang et al., 2021). This is wellmotivated: Artetxe et al. (2020) emphasize representation in the vocabulary as a key factor for effective crosslingual transfer, while Rust et al. (2021) find that M B ERT’s tokenization scheme for many languages is subpar. Pfeiffer et al. (2021b) further 5 Conclusion We explore the interactions between vocabulary augmentation and script transliteration for specializing multilingual contextual word representations in low-resource settings. We confirm vocabulary augmentation’s effectiveness on multiple languages, scripts, and tasks; identify"
2021.mrl-1.5,2020.findings-emnlp.240,0,0.208178,"ach is derived from analogous monolingual models (Devlin et al., 2019; Liu et al., 2019; Peters et al., 2018). However, considering the diversity of the world’s languages and the great data imbalance among them, it is natural to question whether the current multilingual paradigm can be improved upon for low-resource languages. Indeed, past work has demonstrated that it can. For instance, Wu and Dredze (2020) find that multilingual models often lag behind non-contextualized baselines for the lowest-resource languages in their training data, drawing into question their utility in such settings. Conneau et al. (2020a) posit that this phenomenon is a result of limited model capacity, which proves to be a bottleneck for sufficient transfer to low-resource languages. In fact, with multilingual models only being pretrained on a limited set of languages, most of the world’s languages are unseen by the model. For such languages, the performance of such models is even worse (Chau et al., 2020), due in part to the diversity of scripts across the world’s languages (Muller et al., 2021; Pfeiffer et al., 2021b; Rust et al., 2021) as compared to the models’ Latin-centricity (Ács, 2019). Nonetheless, there have been"
2021.mrl-1.5,P17-1178,0,0.025741,"test sets. Labeled Datasets For dependency parsing and part-of-speech tagging, we use datasets and train/test splits from Universal Dependencies (Nivre et al., 2020), version 2.5 (Zeman et al., 2019). POS tagging uses language-specific partof-speech tags (XPOS) to evaluate understanding of language-specific syntactic phenomena. The Belarusian treebank lacks XPOS tags for certain examples, so we use universal part-of-speech tags instead. Dependency parsers are trained with gold word segmentation and no part-of-speech features. Experiments with named entity recognition use the WikiAnn dataset (Pan et al., 2017), following past work (Muller et al., 2021; Pfeiffer et al., 2020; Wu and Dredze, 2020). Specifically, we use the balanced train/test splits of (Rahimi et al., 2019). We note that UD datasets were unavailable for Meadow Mari, and partitioned WikiAnn datasets were missing for Wolof. Experiments We expand on the dependency parsing evaluations of Chau et al. (2020) by additionally considering named entity recognition and part-of-speech tagging. We follow Kondratyuk and Straka (2019) and compute the CWR for each token as a weighted sum of the activations at each M B ERT layer. For dependency parsi"
2021.mrl-1.5,D19-1077,0,0.019183,"es no consistent improvement to the monolingual baselines, since the noisy transliteration process removes information without improving crosslingual alignment. However, TVA and transliteration appear to interact negatively. Although TVA with transliteration 4 Related Work Our work follows a long line of studies investigating the performance of multilingual language models like M B ERT in various settings. The exact source of such models’ crosslingual ability is contested: early studies attributed M B ERT’s success to vocabulary overlap between languages (Cao et al., 2020; Pires et al., 2019; Wu and Dredze, 2019), 9 https://github.com/benjamin-mlr/ mbert-unseen-languages 57 Rep. FAST T BERT M B ERT LAPT TVA MHR (NER) 35.28 → 41.32 (+6.04) 54.17 → 48.45 (–5.72) 61.85 → 63.84 (+1.99) 59.17 → 63.68 (+4.51) 64.23 → 63.19 (–1.04) (NER) – 61.54 → 63.05 (+1.51) 50.76 → 56.80 (+6.04) 56.76 → 67.57 (+10.81) 68.93 → 67.10 (–1.83) UG UG (POS) 89.45 → 89.03 (–0.42) 90.87 → 90.76 (–0.09) 78.07 → 91.34 (+13.27) 79.88 → 92.59 (+12.71) 91.49 → 92.64 (+1.15) UG (UD) 54.52 → 54.45 (–0.07) 60.34 → 60.08 (–0.26) 47.70 → 65.85 (+18.15) 50.67 → 69.39 (+18.72) 67.55 → 68.58 (+1.03) Table 4: Comparison of model performance b"
2021.mrl-1.5,N18-1202,0,0.0747556,"Missing"
2021.mrl-1.5,2020.repl4nlp-1.16,0,0.144268,"versity of Washington ? Allen Institute for Artificial Intelligence {echau18,nasmith}@cs.washington.edu Abstract corpora before being “finetuned” directly on supervised tasks; this pretraining-finetuning approach is derived from analogous monolingual models (Devlin et al., 2019; Liu et al., 2019; Peters et al., 2018). However, considering the diversity of the world’s languages and the great data imbalance among them, it is natural to question whether the current multilingual paradigm can be improved upon for low-resource languages. Indeed, past work has demonstrated that it can. For instance, Wu and Dredze (2020) find that multilingual models often lag behind non-contextualized baselines for the lowest-resource languages in their training data, drawing into question their utility in such settings. Conneau et al. (2020a) posit that this phenomenon is a result of limited model capacity, which proves to be a bottleneck for sufficient transfer to low-resource languages. In fact, with multilingual models only being pretrained on a limited set of languages, most of the world’s languages are unseen by the model. For such languages, the performance of such models is even worse (Chau et al., 2020), due in part"
2021.mrl-1.5,2021.eacl-main.39,0,0.0807525,"Missing"
2021.mrl-1.5,2020.emnlp-main.617,0,0.0363988,"Missing"
2021.mrl-1.5,2021.emnlp-main.800,0,0.0596756,"Missing"
2021.mrl-1.5,P19-1493,0,0.0190936,"ansliteration provides no consistent improvement to the monolingual baselines, since the noisy transliteration process removes information without improving crosslingual alignment. However, TVA and transliteration appear to interact negatively. Although TVA with transliteration 4 Related Work Our work follows a long line of studies investigating the performance of multilingual language models like M B ERT in various settings. The exact source of such models’ crosslingual ability is contested: early studies attributed M B ERT’s success to vocabulary overlap between languages (Cao et al., 2020; Pires et al., 2019; Wu and Dredze, 2019), 9 https://github.com/benjamin-mlr/ mbert-unseen-languages 57 Rep. FAST T BERT M B ERT LAPT TVA MHR (NER) 35.28 → 41.32 (+6.04) 54.17 → 48.45 (–5.72) 61.85 → 63.84 (+1.99) 59.17 → 63.68 (+4.51) 64.23 → 63.19 (–1.04) (NER) – 61.54 → 63.05 (+1.51) 50.76 → 56.80 (+6.04) 56.76 → 67.57 (+10.81) 68.93 → 67.10 (–1.83) UG UG (POS) 89.45 → 89.03 (–0.42) 90.87 → 90.76 (–0.09) 78.07 → 91.34 (+13.27) 79.88 → 92.59 (+12.71) 91.49 → 92.64 (+1.15) UG (UD) 54.52 → 54.45 (–0.07) 60.34 → 60.08 (–0.26) 47.70 → 65.85 (+18.15) 50.67 → 69.39 (+18.72) 67.55 → 68.58 (+1.03) Table 4: Comparison"
2021.mrl-1.5,P19-1015,0,0.0238952,"020), version 2.5 (Zeman et al., 2019). POS tagging uses language-specific partof-speech tags (XPOS) to evaluate understanding of language-specific syntactic phenomena. The Belarusian treebank lacks XPOS tags for certain examples, so we use universal part-of-speech tags instead. Dependency parsers are trained with gold word segmentation and no part-of-speech features. Experiments with named entity recognition use the WikiAnn dataset (Pan et al., 2017), following past work (Muller et al., 2021; Pfeiffer et al., 2020; Wu and Dredze, 2020). Specifically, we use the balanced train/test splits of (Rahimi et al., 2019). We note that UD datasets were unavailable for Meadow Mari, and partitioned WikiAnn datasets were missing for Wolof. Experiments We expand on the dependency parsing evaluations of Chau et al. (2020) by additionally considering named entity recognition and part-of-speech tagging. We follow Kondratyuk and Straka (2019) and compute the CWR for each token as a weighted sum of the activations at each M B ERT layer. For dependency parsing, we follow the setup of Chau et al. (2020) and Muller et al. (2021) and use the CWRs as input to the graph-based dependency parser of Dozat and Manning (2017). Fo"
2021.mrl-1.5,2021.acl-long.243,0,0.0533199,"Missing"
2021.mrl-1.5,2021.naacl-main.40,0,0.0154416,"ing stages to specifically train modular adapter layers for specific tasks or languages, with the goal of maintaining a language-agnostic model while improving performance on individual languages (Pfeiffer et al., 2020, 2021a; Vidoni et al., 2020). However, as Muller et al. (2021) note, the typological diversity of the world’s languages ultimately limits the viability of this approach. On the other hand, many adaptation techniques have focused on improving representation of the target language by modifying the model’s vocabulary or tokenization schemes (Chung et al., 2020; Clark et al., 2021; Wang et al., 2021). This is wellmotivated: Artetxe et al. (2020) emphasize representation in the vocabulary as a key factor for effective crosslingual transfer, while Rust et al. (2021) find that M B ERT’s tokenization scheme for many languages is subpar. Pfeiffer et al. (2021b) further 5 Conclusion We explore the interactions between vocabulary augmentation and script transliteration for specializing multilingual contextual word representations in low-resource settings. We confirm vocabulary augmentation’s effectiveness on multiple languages, scripts, and tasks; identify the mix-in stage as amenable to special"
2021.naacl-main.279,2020.emnlp-main.525,0,0.52699,"y after some period of use, for example while authorWe propose C HOOSE YOUR OWN A DVENTURE ing an entire story or poem (e.g., Clark et al., 2018; Ghazvininejad et al., 2017). A model’s quality (C YOA), a protocol for pairwise evaluations of collaborative writing models, focusing on story generis measured using Likert scale scores, sometimes ation. Instead of scoring a single model, we comcombined with additional analysis, like the type pare two models. At fixed points during the writor quantity of writer edits (e.g., Roemmele and ing process, each generates a suggestion, and writGordon, 2015; Akoury et al., 2020). In contrast, a pairwise system evaluation— ers choose one to continue their story (see Fig. 1). The result is utterance-level feedback on which where evaluators are given two suggestions at the same time and asked to choose between them— model’s generated text writers prefer at that point in the story. Along with the writer’s revisions to the would allow researchers to compare generation generated suggestions and comparisons between models directly. Comparative evaluations have the generated and human-authored portions of the been shown to produce more reliable and consistent story, this evi"
2021.naacl-main.279,2020.nuse-1.6,1,0.618446,"analyses researchers could run with the data gathered from C YOA beyond those listed here; we include some examples. (Q1) Is my model better at generating story suggestions than a baseline model? C YOA reports how many of the model’s suggestions people chose to work with vs. the baseline’s suggestions. We further break this down by the suggestion round (1–5) to see if the writers’ preferences change over the course of the story. Another option would be to break down the writers’ preferences by writer attributes, e.g., to analyze the effect of the author on the stories or desired suggestions (August et al., 2020). (Q2) How useful are the models’ suggestions? We analyze the revisions writers make to the suggestions to see how much of the generated text they find useful for continuing their story. We use three metrics to see how much of the original text is preserved after a writer’s revisions. Levenshtein edit distance measures the number of character insertions, deletions, and substitutions the writers made, and Jaccard similarity measures the proportion of tokens that are shared between the original and the edited text. User Story Edit Ratings (USER; Akoury et al., 2020)2 measures similarity by recur"
2021.naacl-main.279,W07-0718,0,0.0240167,"e their story (see Fig. 1). The result is utterance-level feedback on which where evaluators are given two suggestions at the same time and asked to choose between them— model’s generated text writers prefer at that point in the story. Along with the writer’s revisions to the would allow researchers to compare generation generated suggestions and comparisons between models directly. Comparative evaluations have the generated and human-authored portions of the been shown to produce more reliable and consistent story, this evidence can help a researcher answer results than Likert-scale ratings (Callison-Burch et al., 2007; Kiritchenko and Mohammad, 2017), the following questions about their model: and they have been used to evaluate natural lan1. Is my model better at generating story suggesguage generation systems for translation and diations than a baseline model? logue (Otani et al., 2016; Sedoc et al., 2019). 2. How useful are my model’s suggestions? 3566 Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 3566–3575 June 6–11, 2021. ©2021 Association for Computational Linguistics 3. How does my model’s generat"
2021.naacl-main.279,2020.emnlp-main.349,0,0.0318014,"Missing"
2021.naacl-main.279,N19-4011,0,0.141168,"compare generation generated suggestions and comparisons between models directly. Comparative evaluations have the generated and human-authored portions of the been shown to produce more reliable and consistent story, this evidence can help a researcher answer results than Likert-scale ratings (Callison-Burch et al., 2007; Kiritchenko and Mohammad, 2017), the following questions about their model: and they have been used to evaluate natural lan1. Is my model better at generating story suggesguage generation systems for translation and diations than a baseline model? logue (Otani et al., 2016; Sedoc et al., 2019). 2. How useful are my model’s suggestions? 3566 Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 3566–3575 June 6–11, 2021. ©2021 Association for Computational Linguistics 3. How does my model’s generated text compare to human-authored text? In this paper, we show how C YOA can answer these questions and provide insights into story model behavior, both in cases when the expected differences in text quality are large (e.g., the text is generated with two different models; §3) and when they are"
2021.naacl-main.279,K19-1079,0,0.0858024,"earcher is interested in capturing broader notions of similarity, e.g., embedding-based measures like cosine similarity or BERTScore (Zhang et al., 2020). (Q3) How do the models’ generated texts compare to human-authored text? Pairwise comparison gives us the models’ relative quality; comparing them to human-authored text gives an idea of their absolute quality. To do this, we take the parts of the story the writer wrote alone (i.e., the turns without generated suggestions) and compare it to the generated text. We look at average sentence length (a common proxy for text complexity in stories; See et al., 2019; Roemmele et al., 2017) and distinct-n, a measure of repetition (Li et al., 2016). As in See et al. (2019), we also look the concreteness of the text’s nouns and verbs, us2 github.com/dojoteef/storium-frontend ing the concreteness ratings from Brysbaert et al. (2014).3 If the system is being used to evaluate a model that focuses on a specific aspect of stories, e.g., events or characters, this analysis could be extended to compare how these specific elements are introduced and referenced in the machinegenerated vs. human-authored text. 3 Experiment #1: FUSION vs. GPT 2 We first test C YOA wit"
2021.naacl-main.279,2020.emnlp-main.226,0,0.091075,"Missing"
2021.naacl-main.365,2020.findings-emnlp.66,0,0.0659748,"Missing"
2021.naacl-main.365,D18-1241,0,0.0207644,"dHop (Welbl et al., 2018). While these datasets enabled significant progress in machine comprehension, they include biases in questions that may not reflect real-world settings (Kwiatkowski et al., 2019). Information-Seeking QA in General Domain Recognizing this challenge, others have followed an information-seeking paradigm where the writer of questions is genuinely interested in finding the answer to the question, or at least does not have access to the answer. Examples of such datasets include WikiQA (Yang et al., 2015), NewsQA (Trischler et al., 2017), MsMarco (Campos et al., 2016), QuAC (Choi et al., 2018), Natural Questions (Kwiatkowski et al., 2019), TyDiQA (Clark et al., 2020), and IIRC (Ferguson et al., 2020). Unlike Q ASPER, Natural Questions and TyDiQA12 12 TyDiQA uses short snippets to prime annotators to write questions of interest, but the annotation process does not requestions are not grounded in any contexts, and the associated documents are linked to the questions after they are written. In contrast, Q ASPER’s questions are real follow-up questions about a paper that a reader of appropriate domain expertise would have after reading the title and the abstract. The priming lets the r"
2021.naacl-main.365,2020.tacl-1.30,0,0.0543757,"This setup results in questions requiring more complex document-level reasoning than prior datasets, because (i) abstracts provide rich prompts for questions that can be asked as follow-up and (ii) academic research papers naturally trigger quesMachines built to assist humans who engage with texts to seek information ought to be designed with an awareness of the information need. Abstractly, the human’s need should define the lens through which the system views the text in order to find desired information. Existing information-seeking machine reading datasets (e.g., Kwiatkowski et al., 2019; Clark et al., 2020) have led to significant progress in reading at scale (e.g., Asai et al., 2020; Guu et al., 2020; Liu et al., 2020). However, most 1 Loosely derived from Question Answering over Scienof those benchmarks focus on an “open domain” tific Research Papers. The dataset, baseline code, and other setting where the questions are not anchored in any information about the project can be found at https:// particular user context. The result is an emphasis allenai.org/project/qasper. 4599 Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Huma"
2021.naacl-main.365,D19-1606,1,0.874501,"Missing"
2021.naacl-main.365,N19-1423,0,0.0740128,"Missing"
2021.naacl-main.365,N19-1246,1,0.878072,"Missing"
2021.naacl-main.365,2020.emnlp-main.86,1,0.823881,"ey include biases in questions that may not reflect real-world settings (Kwiatkowski et al., 2019). Information-Seeking QA in General Domain Recognizing this challenge, others have followed an information-seeking paradigm where the writer of questions is genuinely interested in finding the answer to the question, or at least does not have access to the answer. Examples of such datasets include WikiQA (Yang et al., 2015), NewsQA (Trischler et al., 2017), MsMarco (Campos et al., 2016), QuAC (Choi et al., 2018), Natural Questions (Kwiatkowski et al., 2019), TyDiQA (Clark et al., 2020), and IIRC (Ferguson et al., 2020). Unlike Q ASPER, Natural Questions and TyDiQA12 12 TyDiQA uses short snippets to prime annotators to write questions of interest, but the annotation process does not requestions are not grounded in any contexts, and the associated documents are linked to the questions after they are written. In contrast, Q ASPER’s questions are real follow-up questions about a paper that a reader of appropriate domain expertise would have after reading the title and the abstract. The priming lets the readers ask detailed questions that are specific to the papers in context, those that require a deeper underst"
2021.naacl-main.365,P18-1031,0,0.0684602,"Missing"
2021.naacl-main.365,D19-1259,0,0.0256337,"In contrast, Q ASPER’s questions are real follow-up questions about a paper that a reader of appropriate domain expertise would have after reading the title and the abstract. The priming lets the readers ask detailed questions that are specific to the papers in context, those that require a deeper understanding of the contexts, like those shown in Figure 1 and Table 1. QuAC used similar data collection method but with focus on entities, which Q ASPER does not impose. Domain-Specific Information-seeking QA Some work has been done on information-seeking QA on academic research papers. PubmedQA (Jin et al., 2019) derives Yes/No/Maybe questions from PubMed paper titles answered from the conclusion sections of the corresponding abstracts. BioAsq benchmarks (Balikas et al., 2013; Nentidis et al., 2018; Krallinger et al., 2020) focus on open-domain QA over PubMed abstracts. Like Q ASPER, BioAsq answers can take different forms (e.g., yes/no, extracted span(s)). Q ASPER differs from BioAsq in that questions are grounded in a single paper of interest. Furthermore, Q ASPER uses the paper full text, not just the abstract. To the best of our knowledge, Q ASPER is the first information-seeking QA dataset in a c"
2021.naacl-main.365,P17-1147,0,0.0202731,"4: Model performance on the Q ASPER test set on answering questions given gold evidence. We do not show performance on Yes/No and Unanswerable types because they can be trivially predicted to a large extent from the absence of gold evidence. 6 Related Work Information-Verifying QA A large body of work on question answering follows the information-verifying paradigm where the writer of the question already knows its answer, and the questions are written solely for evaluating the knowledge or understanding capabilities of machines. Some examples include SQuAD (Rajpurkar et al., 2016), TriviaQA (Joshi et al., 2017), NarrativeQA (Koˇciský et al., 2018), WikiHop (Welbl et al., 2018), HotpotQA (Yang et al., 2018), CoQA (Reddy et al., 2019), DROP (Dua et al., 2019), Q UOREF (Dasigi et al., 2019). Most datasets for QA on academic research papers also fall within the information-verifying paradigm as they automatically construct QA examples using extracted entities and relations and structured knowledge resources, like DrugBank. Some examples include emrQA (Pampari et al., 2018), BioRead (Pappas et al., 2018), BioMRC (Pappas et al., 2020), MedHop (Welbl et al., 2018). While these datasets enabled significant"
2021.naacl-main.365,2020.findings-emnlp.171,0,0.0591944,"ned Transformer (Vaswani et al., 2017) models which currently produce state-of-the-art results on a majority of QA tasks.9 Recall that Q ASPER introduces two main modeling challenges – different answer types and long input documents. First, Q ASPER includes a variety of answer types, including extractive, abstractive, yes/no, and unanswerable questions, which means a typical span-selection BERT-based QA model (Devlin et al., 2019) is not sufficient to support all these answer types. We address this by converting all answer types into a single task: generating answer text (Raffel et al., 2020; Khashabi et al., 2020).10 This is a sequence-to-sequence formulation that requires an encoder-decoder Transformer model where the encoder reads the question and the document and the decoder generates the answer text. Second, research papers are much longer than the typical 512 or 1024 token limit of most BERTlike models, so we need a Transformer model that can process long inputs. We use the LongformerEncoder-Decoder (LED; Beltagy et al., 2020), an encoder-decoder Transformer model that can efficiently process input sequences thousands of tokens long. With LED’s support for input sequence length of 16K tokens, we c"
2021.naacl-main.365,Q18-1023,0,0.0604265,"Missing"
2021.naacl-main.365,Q19-1026,0,0.0850267,"required to arrive at it. This setup results in questions requiring more complex document-level reasoning than prior datasets, because (i) abstracts provide rich prompts for questions that can be asked as follow-up and (ii) academic research papers naturally trigger quesMachines built to assist humans who engage with texts to seek information ought to be designed with an awareness of the information need. Abstractly, the human’s need should define the lens through which the system views the text in order to find desired information. Existing information-seeking machine reading datasets (e.g., Kwiatkowski et al., 2019; Clark et al., 2020) have led to significant progress in reading at scale (e.g., Asai et al., 2020; Guu et al., 2020; Liu et al., 2020). However, most 1 Loosely derived from Question Answering over Scienof those benchmarks focus on an “open domain” tific Research Papers. The dataset, baseline code, and other setting where the questions are not anchored in any information about the project can be found at https:// particular user context. The result is an emphasis allenai.org/project/qasper. 4599 Proceedings of the 2021 Conference of the North American Chapter of the Association for Computatio"
2021.naacl-main.365,2020.acl-main.703,0,0.024624,"d dure estimates the human performance to be 60.9 on the type uses a different head to predict the corresponding Answer-F1 , and 71.6 Evidence-F1 . Note that given answer. This model performed much worse than the proposed the disagreements among the workers estimated seq2seq formulation. 4603 of Longformer. This allows each token to attend to only its local window and a pre-specified set of global locations of interest, thereby scaling self-attention computation linearly with the input size (as opposed to quadratically with full context self-attention). LED has a similar architecture to BART (Lewis et al., 2020) in terms of number of layers and hidden state sizes, with the distinction that it has a larger position embeddings matrix, allowing it to process inputs of up to 16K tokens long (up from 1K tokens in the original BART model). In practice, LED’s parameters are initialized from a pretrained BART model, and LED copies BART’s position embeddings 16 times to fill the entire 16K position embeddings matrix. For all experiments we use the LED-base sized model, which uses BART-base weights. Input and Output Encoding For the input, we follow the Longformer QA models (Beltagy et al., 2020) and encode th"
2021.naacl-main.365,2020.acl-main.604,0,0.0132563,"racts provide rich prompts for questions that can be asked as follow-up and (ii) academic research papers naturally trigger quesMachines built to assist humans who engage with texts to seek information ought to be designed with an awareness of the information need. Abstractly, the human’s need should define the lens through which the system views the text in order to find desired information. Existing information-seeking machine reading datasets (e.g., Kwiatkowski et al., 2019; Clark et al., 2020) have led to significant progress in reading at scale (e.g., Asai et al., 2020; Guu et al., 2020; Liu et al., 2020). However, most 1 Loosely derived from Question Answering over Scienof those benchmarks focus on an “open domain” tific Research Papers. The dataset, baseline code, and other setting where the questions are not anchored in any information about the project can be found at https:// particular user context. The result is an emphasis allenai.org/project/qasper. 4599 Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 4599–4610 June 6–11, 2021. ©2021 Association for Computational Linguistics tions by"
2021.naacl-main.365,2020.acl-main.447,1,0.716329,"nsolved problems. Additionally, we experiment with oracles that answer questions from gold evidence and find that better pretraining and domain-adaptation might be helpful. 2 Building the Q ASPER Dataset We now describe our process for constructing the dataset. We began with a set of open-access NLP papers, recruited NLP practitioners who are regular readers of research papers, and designed two different data collection interfaces: one for collecting follow-up questions given titles and abstracts, and another for obtaining evidence and answers to those questions. 2.1 Papers We filtered S2ORC (Lo et al., 2020),2 a collection of machine-readable full text for open-access pa2 We accessed both release versions 20190928 and 20200705v1. pers, to (i) those from arXiv with an associated LaTeX source file,3 and (ii) are in the computational linguistics domain.4 We limited our domain to computational linguistics to ensure high quality as we have access to realistic users through our research network; broader domain collection is left to future work and should be enabled by the proof-of-concept of our protocols given in this paper. We used the S2ORC parser (which normalizes multi-file LaTeX sources and resol"
2021.naacl-main.365,2020.emnlp-demos.17,1,0.853597,"Missing"
2021.naacl-main.365,D18-1258,0,0.0200319,"or evaluating the knowledge or understanding capabilities of machines. Some examples include SQuAD (Rajpurkar et al., 2016), TriviaQA (Joshi et al., 2017), NarrativeQA (Koˇciský et al., 2018), WikiHop (Welbl et al., 2018), HotpotQA (Yang et al., 2018), CoQA (Reddy et al., 2019), DROP (Dua et al., 2019), Q UOREF (Dasigi et al., 2019). Most datasets for QA on academic research papers also fall within the information-verifying paradigm as they automatically construct QA examples using extracted entities and relations and structured knowledge resources, like DrugBank. Some examples include emrQA (Pampari et al., 2018), BioRead (Pappas et al., 2018), BioMRC (Pappas et al., 2020), MedHop (Welbl et al., 2018). While these datasets enabled significant progress in machine comprehension, they include biases in questions that may not reflect real-world settings (Kwiatkowski et al., 2019). Information-Seeking QA in General Domain Recognizing this challenge, others have followed an information-seeking paradigm where the writer of questions is genuinely interested in finding the answer to the question, or at least does not have access to the answer. Examples of such datasets include WikiQA (Yang et al., 2015), NewsQ"
2021.naacl-main.365,L18-1439,0,0.0218284,"nderstanding capabilities of machines. Some examples include SQuAD (Rajpurkar et al., 2016), TriviaQA (Joshi et al., 2017), NarrativeQA (Koˇciský et al., 2018), WikiHop (Welbl et al., 2018), HotpotQA (Yang et al., 2018), CoQA (Reddy et al., 2019), DROP (Dua et al., 2019), Q UOREF (Dasigi et al., 2019). Most datasets for QA on academic research papers also fall within the information-verifying paradigm as they automatically construct QA examples using extracted entities and relations and structured knowledge resources, like DrugBank. Some examples include emrQA (Pampari et al., 2018), BioRead (Pappas et al., 2018), BioMRC (Pappas et al., 2020), MedHop (Welbl et al., 2018). While these datasets enabled significant progress in machine comprehension, they include biases in questions that may not reflect real-world settings (Kwiatkowski et al., 2019). Information-Seeking QA in General Domain Recognizing this challenge, others have followed an information-seeking paradigm where the writer of questions is genuinely interested in finding the answer to the question, or at least does not have access to the answer. Examples of such datasets include WikiQA (Yang et al., 2015), NewsQA (Trischler et al., 2017), MsM"
2021.naacl-main.365,2020.bionlp-1.15,0,0.0158903,"achines. Some examples include SQuAD (Rajpurkar et al., 2016), TriviaQA (Joshi et al., 2017), NarrativeQA (Koˇciský et al., 2018), WikiHop (Welbl et al., 2018), HotpotQA (Yang et al., 2018), CoQA (Reddy et al., 2019), DROP (Dua et al., 2019), Q UOREF (Dasigi et al., 2019). Most datasets for QA on academic research papers also fall within the information-verifying paradigm as they automatically construct QA examples using extracted entities and relations and structured knowledge resources, like DrugBank. Some examples include emrQA (Pampari et al., 2018), BioRead (Pappas et al., 2018), BioMRC (Pappas et al., 2020), MedHop (Welbl et al., 2018). While these datasets enabled significant progress in machine comprehension, they include biases in questions that may not reflect real-world settings (Kwiatkowski et al., 2019). Information-Seeking QA in General Domain Recognizing this challenge, others have followed an information-seeking paradigm where the writer of questions is genuinely interested in finding the answer to the question, or at least does not have access to the answer. Examples of such datasets include WikiQA (Yang et al., 2015), NewsQA (Trischler et al., 2017), MsMarco (Campos et al., 2016), Qu"
2021.naacl-main.365,D16-1264,0,0.0237654,"9.11 28.92 44.96 60.03 61.39 Table 4: Model performance on the Q ASPER test set on answering questions given gold evidence. We do not show performance on Yes/No and Unanswerable types because they can be trivially predicted to a large extent from the absence of gold evidence. 6 Related Work Information-Verifying QA A large body of work on question answering follows the information-verifying paradigm where the writer of the question already knows its answer, and the questions are written solely for evaluating the knowledge or understanding capabilities of machines. Some examples include SQuAD (Rajpurkar et al., 2016), TriviaQA (Joshi et al., 2017), NarrativeQA (Koˇciský et al., 2018), WikiHop (Welbl et al., 2018), HotpotQA (Yang et al., 2018), CoQA (Reddy et al., 2019), DROP (Dua et al., 2019), Q UOREF (Dasigi et al., 2019). Most datasets for QA on academic research papers also fall within the information-verifying paradigm as they automatically construct QA examples using extracted entities and relations and structured knowledge resources, like DrugBank. Some examples include emrQA (Pampari et al., 2018), BioRead (Pappas et al., 2018), BioMRC (Pappas et al., 2020), MedHop (Welbl et al., 2018). While thes"
2021.naacl-main.365,D19-1500,0,0.0258485,"Missing"
2021.naacl-main.365,Q19-1016,0,0.0206369,"o and Unanswerable types because they can be trivially predicted to a large extent from the absence of gold evidence. 6 Related Work Information-Verifying QA A large body of work on question answering follows the information-verifying paradigm where the writer of the question already knows its answer, and the questions are written solely for evaluating the knowledge or understanding capabilities of machines. Some examples include SQuAD (Rajpurkar et al., 2016), TriviaQA (Joshi et al., 2017), NarrativeQA (Koˇciský et al., 2018), WikiHop (Welbl et al., 2018), HotpotQA (Yang et al., 2018), CoQA (Reddy et al., 2019), DROP (Dua et al., 2019), Q UOREF (Dasigi et al., 2019). Most datasets for QA on academic research papers also fall within the information-verifying paradigm as they automatically construct QA examples using extracted entities and relations and structured knowledge resources, like DrugBank. Some examples include emrQA (Pampari et al., 2018), BioRead (Pappas et al., 2018), BioMRC (Pappas et al., 2020), MedHop (Welbl et al., 2018). While these datasets enabled significant progress in machine comprehension, they include biases in questions that may not reflect real-world settings (Kwiatkowski et"
2021.naacl-main.365,W17-2623,0,0.0147692,"ioRead (Pappas et al., 2018), BioMRC (Pappas et al., 2020), MedHop (Welbl et al., 2018). While these datasets enabled significant progress in machine comprehension, they include biases in questions that may not reflect real-world settings (Kwiatkowski et al., 2019). Information-Seeking QA in General Domain Recognizing this challenge, others have followed an information-seeking paradigm where the writer of questions is genuinely interested in finding the answer to the question, or at least does not have access to the answer. Examples of such datasets include WikiQA (Yang et al., 2015), NewsQA (Trischler et al., 2017), MsMarco (Campos et al., 2016), QuAC (Choi et al., 2018), Natural Questions (Kwiatkowski et al., 2019), TyDiQA (Clark et al., 2020), and IIRC (Ferguson et al., 2020). Unlike Q ASPER, Natural Questions and TyDiQA12 12 TyDiQA uses short snippets to prime annotators to write questions of interest, but the annotation process does not requestions are not grounded in any contexts, and the associated documents are linked to the questions after they are written. In contrast, Q ASPER’s questions are real follow-up questions about a paper that a reader of appropriate domain expertise would have after r"
2021.naacl-main.365,Q18-1021,0,0.0206324,"given gold evidence. We do not show performance on Yes/No and Unanswerable types because they can be trivially predicted to a large extent from the absence of gold evidence. 6 Related Work Information-Verifying QA A large body of work on question answering follows the information-verifying paradigm where the writer of the question already knows its answer, and the questions are written solely for evaluating the knowledge or understanding capabilities of machines. Some examples include SQuAD (Rajpurkar et al., 2016), TriviaQA (Joshi et al., 2017), NarrativeQA (Koˇciský et al., 2018), WikiHop (Welbl et al., 2018), HotpotQA (Yang et al., 2018), CoQA (Reddy et al., 2019), DROP (Dua et al., 2019), Q UOREF (Dasigi et al., 2019). Most datasets for QA on academic research papers also fall within the information-verifying paradigm as they automatically construct QA examples using extracted entities and relations and structured knowledge resources, like DrugBank. Some examples include emrQA (Pampari et al., 2018), BioRead (Pappas et al., 2018), BioMRC (Pappas et al., 2020), MedHop (Welbl et al., 2018). While these datasets enabled significant progress in machine comprehension, they include biases in questions"
2021.naacl-main.365,D15-1237,0,0.0322932,"Missing"
2021.naacl-main.365,D18-1259,0,0.0200587,"show performance on Yes/No and Unanswerable types because they can be trivially predicted to a large extent from the absence of gold evidence. 6 Related Work Information-Verifying QA A large body of work on question answering follows the information-verifying paradigm where the writer of the question already knows its answer, and the questions are written solely for evaluating the knowledge or understanding capabilities of machines. Some examples include SQuAD (Rajpurkar et al., 2016), TriviaQA (Joshi et al., 2017), NarrativeQA (Koˇciský et al., 2018), WikiHop (Welbl et al., 2018), HotpotQA (Yang et al., 2018), CoQA (Reddy et al., 2019), DROP (Dua et al., 2019), Q UOREF (Dasigi et al., 2019). Most datasets for QA on academic research papers also fall within the information-verifying paradigm as they automatically construct QA examples using extracted entities and relations and structured knowledge resources, like DrugBank. Some examples include emrQA (Pampari et al., 2018), BioRead (Pappas et al., 2018), BioMRC (Pappas et al., 2020), MedHop (Welbl et al., 2018). While these datasets enabled significant progress in machine comprehension, they include biases in questions that may not reflect real-wor"
C10-1092,D09-1075,0,0.586832,"ed lattices for a subset of source words as references for segmentation when translating into English, and then learned the segmentation of the source words to optimize the translation with respect to these references. He showed that the parameters of the model can be applied to similar languages when translating into English. However, manually creating these lattices is time-consuming and requires a bilingual person with some knowledge of the underlying statistical machine translation system. There have been some attempts to apply unsupervised methods for tokenization in machine translation (Chung and Gildea, 2009; Xu et al., 2008). The alignment model of Chung and Gildea (2009) forces every source word to align with a target word. Xu et al. (2008) modeled the source-to-null alignment as in the source word to target word model. Their models are special cases of our proposed model when the source model2 is a unigram model. Like Xu et al. (2008), we use Gibbs sampling for inference. Chung and Gildea (2009) applied efﬁcient dynamic programming-based variational inference algorithms. We beneﬁt from existing unsupervised monolingual segmentation. The source model uses the nested Pitman-Yor model as describe"
C10-1092,N10-1081,1,0.834946,"bilingual extension of what is described by Goldwater et al. (2006) for monolingual segmentation. Nonparametric models have received attention in machine translation recently. For example, DeNero et al. (2008) proposed a hierarchical Dirichlet process model to learn the weights of phrase pairs to address the degeneration in phrase extraction. Teh (2006) used a hierarchical PitmanYor process as a smoothing method for language models. Recent work on multilingual language learning successfully used nonparametric models for language induction tasks such as grammar induction (Snyder et al., 2009; Cohen et al., 2010), morphological segmentation (Goldwater et al., 2006; Snyder and Barzilay, 2008), and part-of-speech tagging (Goldwater and Grifﬁths, 2007; Snyder et al., 2 Note that “source model” here means a model of source text, not a source model in the noisy channel paradigm. 816 2008). 3 Models We start with the generative process for a source sentence and its alignment with a target sentence. Then we describe individual models employed by this generation scheme. 3.1 Generative Story A source sentence is a sequence of word tokens, and each word is either aligned or not aligned. We focus only on the seg"
C10-1092,D08-1033,0,0.391284,"rds to optimize the translation with respect to these references. He showed that the parameters of the model can be applied to similar languages when translating into English. However, manually creating these lattices is time-consuming and requires a bilingual person with some knowledge of the underlying statistical machine translation system. There have been some attempts to apply unsupervised methods for tokenization in machine translation (Chung and Gildea, 2009; Xu et al., 2008). The alignment model of Chung and Gildea (2009) forces every source word to align with a target word. Xu et al. (2008) modeled the source-to-null alignment as in the source word to target word model. Their models are special cases of our proposed model when the source model2 is a unigram model. Like Xu et al. (2008), we use Gibbs sampling for inference. Chung and Gildea (2009) applied efﬁcient dynamic programming-based variational inference algorithms. We beneﬁt from existing unsupervised monolingual segmentation. The source model uses the nested Pitman-Yor model as described by Mochihashi et al. (2009). When sampling each potential word boundary, our inference technique is a bilingual extension of what is de"
C10-1092,N09-1046,0,0.0279273,"problem of segmentation for machine translation has been studied extensively in recent literature. Most of the work used some linguistic knowledge about the source and the target languages (Nießen and Ney, 2004; Goldwater and McClosky, 2005). Sadat and Habash (2006) experimented with a wide range of tokenization schemes for Arabic-English translation. These experiments further show that even for a single language pair, different tokenizations are needed depending on the training corpus size. The experiments are very expensive to conduct and do not generalize to other language pairs. Recently, Dyer (2009) created manually crafted lattices for a subset of source words as references for segmentation when translating into English, and then learned the segmentation of the source words to optimize the translation with respect to these references. He showed that the parameters of the model can be applied to similar languages when translating into English. However, manually creating these lattices is time-consuming and requires a bilingual person with some knowledge of the underlying statistical machine translation system. There have been some attempts to apply unsupervised methods for tokenization i"
C10-1092,P07-1094,0,0.053087,"ived attention in machine translation recently. For example, DeNero et al. (2008) proposed a hierarchical Dirichlet process model to learn the weights of phrase pairs to address the degeneration in phrase extraction. Teh (2006) used a hierarchical PitmanYor process as a smoothing method for language models. Recent work on multilingual language learning successfully used nonparametric models for language induction tasks such as grammar induction (Snyder et al., 2009; Cohen et al., 2010), morphological segmentation (Goldwater et al., 2006; Snyder and Barzilay, 2008), and part-of-speech tagging (Goldwater and Grifﬁths, 2007; Snyder et al., 2 Note that “source model” here means a model of source text, not a source model in the noisy channel paradigm. 816 2008). 3 Models We start with the generative process for a source sentence and its alignment with a target sentence. Then we describe individual models employed by this generation scheme. 3.1 Generative Story A source sentence is a sequence of word tokens, and each word is either aligned or not aligned. We focus only on the segmentation problem and not reordering source words; therefore, the model will not generate the order of the target word tokens. A sentence"
C10-1092,H05-1085,0,0.0368615,"ce text. Our experiments show that the proposed segmentation method leads to improvements on Arabic-English and Chinese-English translation tasks. In the next section we will discuss related work. Section 3 will describe our model in detail. The inference will be covered in Section 4, and decoding in Section 5. Experiments and results will be presented in Section 6. 2 Related Work The problem of segmentation for machine translation has been studied extensively in recent literature. Most of the work used some linguistic knowledge about the source and the target languages (Nießen and Ney, 2004; Goldwater and McClosky, 2005). Sadat and Habash (2006) experimented with a wide range of tokenization schemes for Arabic-English translation. These experiments further show that even for a single language pair, different tokenizations are needed depending on the training corpus size. The experiments are very expensive to conduct and do not generalize to other language pairs. Recently, Dyer (2009) created manually crafted lattices for a subset of source words as references for segmentation when translating into English, and then learned the segmentation of the source words to optimize the translation with respect to these"
C10-1092,P06-1085,0,0.557435,"e source-to-null alignment as in the source word to target word model. Their models are special cases of our proposed model when the source model2 is a unigram model. Like Xu et al. (2008), we use Gibbs sampling for inference. Chung and Gildea (2009) applied efﬁcient dynamic programming-based variational inference algorithms. We beneﬁt from existing unsupervised monolingual segmentation. The source model uses the nested Pitman-Yor model as described by Mochihashi et al. (2009). When sampling each potential word boundary, our inference technique is a bilingual extension of what is described by Goldwater et al. (2006) for monolingual segmentation. Nonparametric models have received attention in machine translation recently. For example, DeNero et al. (2008) proposed a hierarchical Dirichlet process model to learn the weights of phrase pairs to address the degeneration in phrase extraction. Teh (2006) used a hierarchical PitmanYor process as a smoothing method for language models. Recent work on multilingual language learning successfully used nonparametric models for language induction tasks such as grammar induction (Snyder et al., 2009; Cohen et al., 2010), morphological segmentation (Goldwater et al., 2"
C10-1092,P05-1071,0,0.00573616,"ne them using the “grow-diag-ﬁnal-and” heuristic. The output of combining GIZA++ alignment for a sentence pair is a sequence of si -tj entries where i is an index of the source sentence and j is an index of the target sentence. As our model allows only one-to-one mappings between the words in the source and target sentences, we remove si -tj from the sequence if either the source word si or target word tj is already in a previous entry of the combined alignment sequence. The resulting alignment is our initial alignment for the inference. We also apply the MADA morphology segmentation toolkit (Habash and Rambow, 2005) to preprocess the Arabic corpus. We use the D3 scheme (each Arabic word is segmented into morphemes in sequence [CONJ+ [PART+ [Al+ BASE +PRON]]]), mark the morpheme boundaries, and then combine the morphemes again to have words in their original full word form. During inference, we only sample over these morpheme boundaries as potential word boundaries. In this way, we limit the search space, allowing only segmentations consistent with MADA-D3. The inference samples 150 iterations through the whole training set and uses the posterior probability distribution from the last iteration for decodi"
C10-1092,P07-2045,0,0.00835706,"nly segmentations consistent with MADA-D3. The inference samples 150 iterations through the whole training set and uses the posterior probability distribution from the last iteration for decoding. The decoding process is then applied to the entire training set as well as to the development and test sets to generate a consistent tokenization across all three data sets. We used the OpenFST toolkit (Allauzen et al., 2007) for ﬁnite-state machine implementation and operations. The output of the decoding is the preprocessed data for translation. We use the open source Moses phrase-based MT system (Koehn et al., 2007) to test the impact of the preprocessing technique on translation quality.5 6.3.1 Arabic-English Translation Results We consider the Arabic-English setting. We use two baselines: original full word form and MADA-D3 tokenization scheme for ArabicEnglish translation. Table 1 compares the translation results of our segmentation methods with these baselines. Our segmentation method shows improvement over the two baselines on both the development and test sets. According to Sadat and Habash (2006), the MADA-D3 scheme per5 The Moses translation alignment is the output of GIZA++, not from our MCMC in"
C10-1092,P09-1012,0,0.364677,"the best alignment, and thereby the best translation system. We propose an unsupervised tokenization method for machine translation by formulating a generative Bayesian model to “explain” the bilingual training data. Generation of a sentence pair is described as follows: ﬁrst a monolingual tokenization model generates the source sentence, then the alignment model generates the target sentence through the alignments with the source sentence. Breaking this generation process into two steps provides ﬂexibility to incorporate existing monolingual morphological segmentation models such as those of Mochihashi et al. (2009) or Creutz and Lagus (2007). Using nonparametric 815 Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 815–823, Beijing, August 2010 models and the Bayesian framework makes it possible to incorporate linguistic knowledge as prior distributions and obtain the posterior distribution through inference techniques such as MCMC or variational inference. As new test source sentences do not have translations which can help to infer the best segmentation, we decode the source string according to the posterior distribution from the inference step. In summ"
C10-1092,J04-2003,0,0.0188815,"the preprocessed source text. Our experiments show that the proposed segmentation method leads to improvements on Arabic-English and Chinese-English translation tasks. In the next section we will discuss related work. Section 3 will describe our model in detail. The inference will be covered in Section 4, and decoding in Section 5. Experiments and results will be presented in Section 6. 2 Related Work The problem of segmentation for machine translation has been studied extensively in recent literature. Most of the work used some linguistic knowledge about the source and the target languages (Nießen and Ney, 2004; Goldwater and McClosky, 2005). Sadat and Habash (2006) experimented with a wide range of tokenization schemes for Arabic-English translation. These experiments further show that even for a single language pair, different tokenizations are needed depending on the training corpus size. The experiments are very expensive to conduct and do not generalize to other language pairs. Recently, Dyer (2009) created manually crafted lattices for a subset of source words as references for segmentation when translating into English, and then learned the segmentation of the source words to optimize the tra"
C10-1092,J03-1002,0,0.00570158,"lish sentence pairs. The development set and the test set each have 489 Chinese sentences and each sentence has 7 English references. 6.3 Results We will report the translation results where the preprocessing of the source text are our unigram, bigram, and trigram source models and source-tonull model. The MCMC inference algorithm starts with an initial segmentation of the source text into full word forms. For Chinese, we use the original word segmentation as distributed by IWSLT. To get an initial alignment, we generate the IBM4 Viterbi alignments in both directions using the GIZA++ toolkit (Och and Ney, 2003) and combine them using the “grow-diag-ﬁnal-and” heuristic. The output of combining GIZA++ alignment for a sentence pair is a sequence of si -tj entries where i is an index of the source sentence and j is an index of the target sentence. As our model allows only one-to-one mappings between the words in the source and target sentences, we remove si -tj from the sequence if either the source word si or target word tj is already in a previous entry of the combined alignment sequence. The resulting alignment is our initial alignment for the inference. We also apply the MADA morphology segmentation"
C10-1092,P06-1001,0,0.116056,"hat the proposed segmentation method leads to improvements on Arabic-English and Chinese-English translation tasks. In the next section we will discuss related work. Section 3 will describe our model in detail. The inference will be covered in Section 4, and decoding in Section 5. Experiments and results will be presented in Section 6. 2 Related Work The problem of segmentation for machine translation has been studied extensively in recent literature. Most of the work used some linguistic knowledge about the source and the target languages (Nießen and Ney, 2004; Goldwater and McClosky, 2005). Sadat and Habash (2006) experimented with a wide range of tokenization schemes for Arabic-English translation. These experiments further show that even for a single language pair, different tokenizations are needed depending on the training corpus size. The experiments are very expensive to conduct and do not generalize to other language pairs. Recently, Dyer (2009) created manually crafted lattices for a subset of source words as references for segmentation when translating into English, and then learned the segmentation of the source words to optimize the translation with respect to these references. He showed tha"
C10-1092,P08-1084,0,0.251677,"monolingual segmentation. Nonparametric models have received attention in machine translation recently. For example, DeNero et al. (2008) proposed a hierarchical Dirichlet process model to learn the weights of phrase pairs to address the degeneration in phrase extraction. Teh (2006) used a hierarchical PitmanYor process as a smoothing method for language models. Recent work on multilingual language learning successfully used nonparametric models for language induction tasks such as grammar induction (Snyder et al., 2009; Cohen et al., 2010), morphological segmentation (Goldwater et al., 2006; Snyder and Barzilay, 2008), and part-of-speech tagging (Goldwater and Grifﬁths, 2007; Snyder et al., 2 Note that “source model” here means a model of source text, not a source model in the noisy channel paradigm. 816 2008). 3 Models We start with the generative process for a source sentence and its alignment with a target sentence. Then we describe individual models employed by this generation scheme. 3.1 Generative Story A source sentence is a sequence of word tokens, and each word is either aligned or not aligned. We focus only on the segmentation problem and not reordering source words; therefore, the model will not"
C10-1092,D08-1109,0,0.0441549,"Missing"
C10-1092,P09-1009,0,0.0175641,"erence technique is a bilingual extension of what is described by Goldwater et al. (2006) for monolingual segmentation. Nonparametric models have received attention in machine translation recently. For example, DeNero et al. (2008) proposed a hierarchical Dirichlet process model to learn the weights of phrase pairs to address the degeneration in phrase extraction. Teh (2006) used a hierarchical PitmanYor process as a smoothing method for language models. Recent work on multilingual language learning successfully used nonparametric models for language induction tasks such as grammar induction (Snyder et al., 2009; Cohen et al., 2010), morphological segmentation (Goldwater et al., 2006; Snyder and Barzilay, 2008), and part-of-speech tagging (Goldwater and Grifﬁths, 2007; Snyder et al., 2 Note that “source model” here means a model of source text, not a source model in the noisy channel paradigm. 816 2008). 3 Models We start with the generative process for a source sentence and its alignment with a target sentence. Then we describe individual models employed by this generation scheme. 3.1 Generative Story A source sentence is a sequence of word tokens, and each word is either aligned or not aligned. We"
C10-1092,P06-1124,0,0.0575072,"onal inference algorithms. We beneﬁt from existing unsupervised monolingual segmentation. The source model uses the nested Pitman-Yor model as described by Mochihashi et al. (2009). When sampling each potential word boundary, our inference technique is a bilingual extension of what is described by Goldwater et al. (2006) for monolingual segmentation. Nonparametric models have received attention in machine translation recently. For example, DeNero et al. (2008) proposed a hierarchical Dirichlet process model to learn the weights of phrase pairs to address the degeneration in phrase extraction. Teh (2006) used a hierarchical PitmanYor process as a smoothing method for language models. Recent work on multilingual language learning successfully used nonparametric models for language induction tasks such as grammar induction (Snyder et al., 2009; Cohen et al., 2010), morphological segmentation (Goldwater et al., 2006; Snyder and Barzilay, 2008), and part-of-speech tagging (Goldwater and Grifﬁths, 2007; Snyder et al., 2 Note that “source model” here means a model of source text, not a source model in the noisy channel paradigm. 816 2008). 3 Models We start with the generative process for a source"
C10-1092,C96-2141,1,0.468209,"y existing probabilistic monolingual segmentation to generate the source sentence. For example, the source model can be the nested Pitman-Yor process as described by Mochihashi et al. (2009), the minimum description length model presented by Creutz and Lagus (2007), or something else. Also the source model can incorporate linguistic knowledge from a rule-based or statistical morphological disambiguator. The model generates the alignment after the source sentence with word boundaries already generated. Therefore, the alignment model can be any existing word alignment model (Brown et al., 1993; Vogel et al., 1996). Even though the choices of source model or alignment model can lead to different inference methods, the model we propose here is highly extensible. Note that we assume that the alignment consists of at most one-to-one mappings between source and target words, with null alignments possible on both sides. Another advantage of a separate source model lies in the segmentation of an unseen test set. In section 5 we will show how to apply the source model distribution learned from training data to ﬁnd the best segmentation of an unseen test set. Notation and Parameters We will use bold font for a"
C10-1092,C08-1128,0,0.26841,"of source words as references for segmentation when translating into English, and then learned the segmentation of the source words to optimize the translation with respect to these references. He showed that the parameters of the model can be applied to similar languages when translating into English. However, manually creating these lattices is time-consuming and requires a bilingual person with some knowledge of the underlying statistical machine translation system. There have been some attempts to apply unsupervised methods for tokenization in machine translation (Chung and Gildea, 2009; Xu et al., 2008). The alignment model of Chung and Gildea (2009) forces every source word to align with a target word. Xu et al. (2008) modeled the source-to-null alignment as in the source word to target word model. Their models are special cases of our proposed model when the source model2 is a unigram model. Like Xu et al. (2008), we use Gibbs sampling for inference. Chung and Gildea (2009) applied efﬁcient dynamic programming-based variational inference algorithms. We beneﬁt from existing unsupervised monolingual segmentation. The source model uses the nested Pitman-Yor model as described by Mochihashi et"
C10-1092,knight-al-onaizan-1998-translation,0,0.0211093,"tences. Only the source model is used in preprocessing. ∗ The best segmentation  s of a string of characters c = c1 , . . . , c|c |according to the n-gram source model is: i=|s| ∗ s = argmax p (|s|) s from c i=1 p (si |si−n , . . . , si−1 ) We use a stochastic ﬁnite-state machine for decoding. This is possible by composition of the following two ﬁnite state machines: 820 • Acceptor Ac . The string of characters c is represented as an ﬁnite state acceptor machine where any path through the machine represents an unweighted segmentation of c. • Source model weighted ﬁnite state transducer Lc . Knight and Al-Onaizan (1998) show how to build an n-gram language model by a weighted ﬁnite state machine. The states of the transducer are (n − 1)gram history, the edges are words from the language. The arc si coming from state (si−n , . . . , si−1 ) to state (si−n+1 , . . . , si ) has weight p (si |si−n , . . . , si−1 ). The best segmentation s∗ is given as s∗ = BestPath(Ac ◦ Lc ). 6 Experiments This section presents experimental results on Arabic-English and Chinese-English translation tasks using the proposed segmentation technique. 6.1 Arabic-English As a training set we use the BTEC corpus distributed by the Intern"
C10-1092,J93-2003,0,\N,Missing
C10-1092,2005.iwslt-1.1,0,\N,Missing
C14-1084,P11-1054,0,0.0123665,"orty iterations. After estimating parameters, we decode using the Viterbi algorithm. 4 Experiments Our experiments compare three methods for aligning segments of privacy policies, at both the paragraph and the section level: • A greedy divising clustering algorithm, as implemented in CLUTO.12 The algorithm performs a sequence of bisections until the desired number of clusters is reached. In each step, a cluster is selected and partitioned so as to optimize the clustering criterion. CLUTO demonstrated robust performance in several related NLP tasks (Zhong and Ghosh, 2005; Lin and Bilmes, 2011; Chen et al., 2011). • The Viterbi state assignment from the EM-estimated HMM. We report averaged results over ten runs, with random initialization. • The Viterbi state assignment after VB inference, using the mean field parameters. We report averaged results over ten runs, with random initialization. Our evaluation metrics are precision, recall, and F -score on the identification of section or paragraph pairs annotated “yes.” 4.1 Results In Figure 1, we present performance of different algorithms using a range of hidden state values K ∈ {1, 2, . . . , 20}. The top row shows precision, recall and F -scores on se"
C14-1084,D08-1035,0,0.0113779,"amm-Leach-Bliley Act requires these institutions to address a specific set of issues. Sadeh et al. (2013) describe our larger research initiative to incorporate automation into privacy policy analysis. Methodologically, the HMM used above is very similar to extensive previous uses of HMMs for POS tagging (Merialdo, 1994), including with Bayesian inference (Goldwater and Griffiths, 2007; Johnson, 2007; Gao and Johnson, 2008). Bayesian topic models (Blei et al., 2003) are a related set of techniques, and future exploration might consider their use in automatically discovering document sections (Eisenstein and Barzilay, 2008), rather than fixing section or paragraph boundaries. 6 Conclusion This paper presents an exploration of alignment-by-paragraph and -section of website privacy policies. We contribute an improved annotated dataset for pairwise evaluation of automatic methods and an exploration of clustering and HMM-based alignment methods. Our results show that these algorithms achieve agreement on par with the lower half of crowdworkers, with about half of the difference from the median due to the bag of words representation and half due to the inherent greediness of the methods. Acknowledgments The authors g"
C14-1084,D08-1036,0,0.0676509,"VB We consider two estimation methods, neither novel. Both are greedy hillclimbing methods that locally optimize functions based on likelihood under the HMM. The first method is EM, adapted for the multiple emission case; the equations for the E-step (forwardbackward algorithm and subsequent posterior calculations) and the M-step are shown in Table 5. We also consider Bayesian inference, which seeks to marginalize out the parameter values, since we are really only interested in the assignment of sections to hidden states. Further, Bayesian inference has been found favorable on small datasets (Gao and Johnson, 2008). We assume symmetric Dirichlet priors on the transition and emission distributions, parameterized respectively by λ = 1 and λ0 = 0.1. We apply mean-field variational approximate inference as described by Beal (2003), which amounts to an EM-like procedure. The E-step is identical to EM, and the M-step involves a transformation of the expected counts, shown in Table 5. (We also explored Gibbs sampling; performance was less stable but generally similar; for clarity we do not report the results here.) 889 3.3 Implementation Details In modeling, the vocabulary excludes 429 stopwords,11 words whose"
C14-1084,P07-1094,0,0.0111223,"ial information, sharing with third parties, and deletion of data. This expectation is supported by recommendation by privacy experts (Gellman, 2014) and policymakers (Federal Trade Commission, 2012); in the financial services sector, the Gramm-Leach-Bliley Act requires these institutions to address a specific set of issues. Sadeh et al. (2013) describe our larger research initiative to incorporate automation into privacy policy analysis. Methodologically, the HMM used above is very similar to extensive previous uses of HMMs for POS tagging (Merialdo, 1994), including with Bayesian inference (Goldwater and Griffiths, 2007; Johnson, 2007; Gao and Johnson, 2008). Bayesian topic models (Blei et al., 2003) are a related set of techniques, and future exploration might consider their use in automatically discovering document sections (Eisenstein and Barzilay, 2008), rather than fixing section or paragraph boundaries. 6 Conclusion This paper presents an exploration of alignment-by-paragraph and -section of website privacy policies. We contribute an improved annotated dataset for pairwise evaluation of automatic methods and an exploration of clustering and HMM-based alignment methods. Our results show that these algor"
C14-1084,D07-1031,0,0.0163184,"hird parties, and deletion of data. This expectation is supported by recommendation by privacy experts (Gellman, 2014) and policymakers (Federal Trade Commission, 2012); in the financial services sector, the Gramm-Leach-Bliley Act requires these institutions to address a specific set of issues. Sadeh et al. (2013) describe our larger research initiative to incorporate automation into privacy policy analysis. Methodologically, the HMM used above is very similar to extensive previous uses of HMMs for POS tagging (Merialdo, 1994), including with Bayesian inference (Goldwater and Griffiths, 2007; Johnson, 2007; Gao and Johnson, 2008). Bayesian topic models (Blei et al., 2003) are a related set of techniques, and future exploration might consider their use in automatically discovering document sections (Eisenstein and Barzilay, 2008), rather than fixing section or paragraph boundaries. 6 Conclusion This paper presents an exploration of alignment-by-paragraph and -section of website privacy policies. We contribute an improved annotated dataset for pairwise evaluation of automatic methods and an exploration of clustering and HMM-based alignment methods. Our results show that these algorithms achieve a"
C14-1084,P11-1052,0,0.0328184,"ently happens within forty iterations. After estimating parameters, we decode using the Viterbi algorithm. 4 Experiments Our experiments compare three methods for aligning segments of privacy policies, at both the paragraph and the section level: • A greedy divising clustering algorithm, as implemented in CLUTO.12 The algorithm performs a sequence of bisections until the desired number of clusters is reached. In each step, a cluster is selected and partitioned so as to optimize the clustering criterion. CLUTO demonstrated robust performance in several related NLP tasks (Zhong and Ghosh, 2005; Lin and Bilmes, 2011; Chen et al., 2011). • The Viterbi state assignment from the EM-estimated HMM. We report averaged results over ten runs, with random initialization. • The Viterbi state assignment after VB inference, using the mean field parameters. We report averaged results over ten runs, with random initialization. Our evaluation metrics are precision, recall, and F -score on the identification of section or paragraph pairs annotated “yes.” 4.1 Results In Figure 1, we present performance of different algorithms using a range of hidden state values K ∈ {1, 2, . . . , 20}. The top row shows precision, recall"
C14-1084,J94-2001,0,0.00885974,"on of a user’s contact, location, health, and financial information, sharing with third parties, and deletion of data. This expectation is supported by recommendation by privacy experts (Gellman, 2014) and policymakers (Federal Trade Commission, 2012); in the financial services sector, the Gramm-Leach-Bliley Act requires these institutions to address a specific set of issues. Sadeh et al. (2013) describe our larger research initiative to incorporate automation into privacy policy analysis. Methodologically, the HMM used above is very similar to extensive previous uses of HMMs for POS tagging (Merialdo, 1994), including with Bayesian inference (Goldwater and Griffiths, 2007; Johnson, 2007; Gao and Johnson, 2008). Bayesian topic models (Blei et al., 2003) are a related set of techniques, and future exploration might consider their use in automatically discovering document sections (Eisenstein and Barzilay, 2008), rather than fixing section or paragraph boundaries. 6 Conclusion This paper presents an exploration of alignment-by-paragraph and -section of website privacy policies. We contribute an improved annotated dataset for pairwise evaluation of automatic methods and an exploration of clustering"
C14-1084,P14-2099,1,0.300019,"nce of the mobile Web and the Internet of Things, with early efforts in this area including the use of static analysis to identify sensitive data flows in mobile apps (Lin et al., 2012) and the development of mobile app privacy profiles (Liu et al., 2014). Increased automation for such efforts motivates our interest in privacy policies as a text genre for NLP, with the general goal of supporting both user-oriented tools that interpret policies and studies of the contents of policies by legal scholars. In this paper, we start with a corpus of 1,010 policies collected from widely-used websites (Ramanath et al., 2014),1 and seek to automatically align segments of policies. We believe this is a worthwhile first This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/ 1 http://www.usableprivacy.org/data 884 Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers, pages 884–894, Dublin, Ireland, August 23-29 2014. Amazon.com Privacy Notice ... What About Cookies? Cookies are unique identifiers that we tra"
D07-1003,J90-2002,0,0.0433493,"l. (2005) experimented with dependency tree kernels to compute similarity between parse trees. Cui et al. (2005) measured sentence similarity based on similarity measures between dependency paths among aligned words. They used heuristic functions similar to mutual information to assign scores to matched pairs of dependency links. Shen and Klakow (2006) extend the idea further through the use of log-linear models to learn a scoring function for relation pairs. Echihabi and Marcu (2003) presented a noisychannel approach in which they adapted the IBM model 4 from statistical machine translation (Brown et al., 1990; Brown et al., 1993) and applied it to QA. Similarly, Murdock and Croft (2005) adopted a simple translation model from IBM model 1 (Brown et al., 1990; Brown et al., 1993) and applied it to QA. Porting the translation model to QA is not straightforward; it involves parse-tree pruning heuristics (the first two deterministic steps in Echihabi and Marcu, 2003) and also replacing the lexical translation table with a monolingual “dictionary” which simply encodes the identity relation. This brings us to the question that drives this work: is there a statistical translation-like model that is natura"
D07-1003,J93-2003,0,0.0563337,"ed with dependency tree kernels to compute similarity between parse trees. Cui et al. (2005) measured sentence similarity based on similarity measures between dependency paths among aligned words. They used heuristic functions similar to mutual information to assign scores to matched pairs of dependency links. Shen and Klakow (2006) extend the idea further through the use of log-linear models to learn a scoring function for relation pairs. Echihabi and Marcu (2003) presented a noisychannel approach in which they adapted the IBM model 4 from statistical machine translation (Brown et al., 1990; Brown et al., 1993) and applied it to QA. Similarly, Murdock and Croft (2005) adopted a simple translation model from IBM model 1 (Brown et al., 1990; Brown et al., 1993) and applied it to QA. Porting the translation model to QA is not straightforward; it involves parse-tree pruning heuristics (the first two deterministic steps in Echihabi and Marcu, 2003) and also replacing the lexical translation table with a monolingual “dictionary” which simply encodes the identity relation. This brings us to the question that drives this work: is there a statistical translation-like model that is natural and accurate for qu"
D07-1003,P05-1067,0,0.0586,"pardy model, and our parameter estimation method are described in Section 4. Experimental results comparing our approach to two state-of-the-art baselines are presented in Section 5. We discuss portability to cross-lingual QA and other applied semantic processing tasks in Section 6. 2 Related Work To model the syntactic transformation process, researchers in these fields—especially in machine translation—have developed powerful grammatical formalisms and statistical models for representing and learning these tree-to-tree relations (Wu and Wong, 1998; Eisner, 2003; Gildea, 2003; Melamed, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Galley et al., 2006; Smith and Eisner, 2006, inter alia). We can also observe a trend in recent work in textual entailment that more emphasis is put on explicit learning of the syntactic graph mapping between the entailed and entailed-by sentences (MacCartney et al., 2006). However, relatively fewer attempts have been made in the QA community. As pointed out by Katz and Lin (2003), most early experiments in QA that tried to bring in syntactic or semantic features showed little or no improvement, and it was often the case that performance actually degraded (Litkowski, 1999"
D07-1003,P03-1003,0,0.0374351,"ast-growing research problem. Stateof-the-art QA systems are extremely complex. They usually take the form of a pipeline architecture, chaining together modules that perform tasks such as answer type analysis (identifying whether the correct answer will be a person, location, date, etc.), document retrieval, answer candidate extraction, and answer reranking. This architecture is so predominant that each task listed above has evolved into its own sub-field and is often studied and evaluated independently (Shima et al., 2006). At a high level, the QA task boils down to only two essential steps (Echihabi and Marcu, 2003). The first step, retrieval, narrows down the search space from a corpus of millions of documents to a focused set of maybe a few hundred using an IR engine, where efficiency and recall are the main focus. The second step, selection, assesses each candidate answer string proposed by the first step, and finds the one that is most likely to be an answer to the given question. The granularity of the target answer string varies depending on the type of the question. For example, answers to factoid questions (e.g., Who, When, Where) are usually single words or short phrases, while definitional ques"
D07-1003,P03-2041,0,0.0350525,"ection 3. Our version of QG, called the Jeopardy model, and our parameter estimation method are described in Section 4. Experimental results comparing our approach to two state-of-the-art baselines are presented in Section 5. We discuss portability to cross-lingual QA and other applied semantic processing tasks in Section 6. 2 Related Work To model the syntactic transformation process, researchers in these fields—especially in machine translation—have developed powerful grammatical formalisms and statistical models for representing and learning these tree-to-tree relations (Wu and Wong, 1998; Eisner, 2003; Gildea, 2003; Melamed, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Galley et al., 2006; Smith and Eisner, 2006, inter alia). We can also observe a trend in recent work in textual entailment that more emphasis is put on explicit learning of the syntactic graph mapping between the entailed and entailed-by sentences (MacCartney et al., 2006). However, relatively fewer attempts have been made in the QA community. As pointed out by Katz and Lin (2003), most early experiments in QA that tried to bring in syntactic or semantic features showed little or no improvement, and it was often the case"
D07-1003,P06-1121,0,0.0474339,"ethod are described in Section 4. Experimental results comparing our approach to two state-of-the-art baselines are presented in Section 5. We discuss portability to cross-lingual QA and other applied semantic processing tasks in Section 6. 2 Related Work To model the syntactic transformation process, researchers in these fields—especially in machine translation—have developed powerful grammatical formalisms and statistical models for representing and learning these tree-to-tree relations (Wu and Wong, 1998; Eisner, 2003; Gildea, 2003; Melamed, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Galley et al., 2006; Smith and Eisner, 2006, inter alia). We can also observe a trend in recent work in textual entailment that more emphasis is put on explicit learning of the syntactic graph mapping between the entailed and entailed-by sentences (MacCartney et al., 2006). However, relatively fewer attempts have been made in the QA community. As pointed out by Katz and Lin (2003), most early experiments in QA that tried to bring in syntactic or semantic features showed little or no improvement, and it was often the case that performance actually degraded (Litkowski, 1999; Attardi et al., 2001). More recent atte"
D07-1003,P03-1011,0,0.0554453,"version of QG, called the Jeopardy model, and our parameter estimation method are described in Section 4. Experimental results comparing our approach to two state-of-the-art baselines are presented in Section 5. We discuss portability to cross-lingual QA and other applied semantic processing tasks in Section 6. 2 Related Work To model the syntactic transformation process, researchers in these fields—especially in machine translation—have developed powerful grammatical formalisms and statistical models for representing and learning these tree-to-tree relations (Wu and Wong, 1998; Eisner, 2003; Gildea, 2003; Melamed, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Galley et al., 2006; Smith and Eisner, 2006, inter alia). We can also observe a trend in recent work in textual entailment that more emphasis is put on explicit learning of the syntactic graph mapping between the entailed and entailed-by sentences (MacCartney et al., 2006). However, relatively fewer attempts have been made in the QA community. As pointed out by Katz and Lin (2003), most early experiments in QA that tried to bring in syntactic or semantic features showed little or no improvement, and it was often the case that performa"
D07-1003,P06-1114,0,0.0164312,"gine a generative story for QA in which the question is generated from the answer sentence through a series of syntactic and semantic transformations. The same story has been told for machine translation (Yamada and Knight, 2001, inter alia), in which a target language sentence (the desired output) has undergone semantic transformation (word to word translation) and syntactic transformation (syntax divergence across languages) to generate the source language sentence (noisy-channel model). Similar stories can also be found in paraphrasing (Quirk et al., 2004; Wu, 2005) and textual entailment (Harabagiu and Hickl, 2006; Wu, 2005). Our story makes use of a weighted formalism known as quasi-synchronous grammar (hereafter, QG), originally developed by D. Smith and Eisner (2006) for machine translation. Unlike most synchronous formalisms, QG does not posit a strict isomorphism between the two trees, and it provides 23 an elegant description for the set of local configurations. In Section 2 we situate our contribution in the context of earlier work, and we give a brief discussion of quasi-synchronous grammars in Section 3. Our version of QG, called the Jeopardy model, and our parameter estimation method are desc"
D07-1003,N06-1006,0,0.0625658,"Missing"
D07-1003,J93-2004,0,0.0507162,"Missing"
D07-1003,P05-1012,0,0.0337334,"p(a |q) ∝ p(q | a) · p(a). Because A is known and is assumed to be generated by an external extraction system, we could use that extraction system to assign scores (and hence, probabilities p(a)) to the candidate answers. Other scores could also be used, such as reputability of the document the answer came from, grammaticality, etc. Here, aiming for simplicity, we do not aim to use such information. Hence we treat p(a) as uniform over A.3 The second adjustment adds a labeled, directed dependency tree to the question and the answer. The tree is produced by a state-of-the-art dependency parser (McDonald et al., 2005) trained on the Wall Street Journal Penn Treebank (Marcus et al., 1993). A dependency tree on a sequence w = hw1 , ..., wk i is a mapping of indices of words to indices of their syntactic parents and a label for the syntactic relation, τ : {1, ..., k} → {0, ..., k} × L. Each word wi has a single parent, denoted wτ (i).par . Cycles are not permitted. w0 is taken to be the invisible “wall” symbol at the left edge of the sentence; it has a single child (|{i : τ (i) = 0} |= 1). The label for wi is denoted τ (i).lab. The third adjustment involves a hidden variable X, the alignment between question"
D07-1003,2004.tmi-1.5,0,0.00748673,"called the Jeopardy model, and our parameter estimation method are described in Section 4. Experimental results comparing our approach to two state-of-the-art baselines are presented in Section 5. We discuss portability to cross-lingual QA and other applied semantic processing tasks in Section 6. 2 Related Work To model the syntactic transformation process, researchers in these fields—especially in machine translation—have developed powerful grammatical formalisms and statistical models for representing and learning these tree-to-tree relations (Wu and Wong, 1998; Eisner, 2003; Gildea, 2003; Melamed, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Galley et al., 2006; Smith and Eisner, 2006, inter alia). We can also observe a trend in recent work in textual entailment that more emphasis is put on explicit learning of the syntactic graph mapping between the entailed and entailed-by sentences (MacCartney et al., 2006). However, relatively fewer attempts have been made in the QA community. As pointed out by Katz and Lin (2003), most early experiments in QA that tried to bring in syntactic or semantic features showed little or no improvement, and it was often the case that performance actually de"
D07-1003,H05-1086,0,0.0498115,"between parse trees. Cui et al. (2005) measured sentence similarity based on similarity measures between dependency paths among aligned words. They used heuristic functions similar to mutual information to assign scores to matched pairs of dependency links. Shen and Klakow (2006) extend the idea further through the use of log-linear models to learn a scoring function for relation pairs. Echihabi and Marcu (2003) presented a noisychannel approach in which they adapted the IBM model 4 from statistical machine translation (Brown et al., 1990; Brown et al., 1993) and applied it to QA. Similarly, Murdock and Croft (2005) adopted a simple translation model from IBM model 1 (Brown et al., 1990; Brown et al., 1993) and applied it to QA. Porting the translation model to QA is not straightforward; it involves parse-tree pruning heuristics (the first two deterministic steps in Echihabi and Marcu, 2003) and also replacing the lexical translation table with a monolingual “dictionary” which simply encodes the identity relation. This brings us to the question that drives this work: is there a statistical translation-like model that is natural and accurate for question answering? We propose Smith and Eisner’s (2006) qua"
D07-1003,W04-3219,0,0.00920502,"Missing"
D07-1003,P05-1034,0,0.0605414,"rameter estimation method are described in Section 4. Experimental results comparing our approach to two state-of-the-art baselines are presented in Section 5. We discuss portability to cross-lingual QA and other applied semantic processing tasks in Section 6. 2 Related Work To model the syntactic transformation process, researchers in these fields—especially in machine translation—have developed powerful grammatical formalisms and statistical models for representing and learning these tree-to-tree relations (Wu and Wong, 1998; Eisner, 2003; Gildea, 2003; Melamed, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Galley et al., 2006; Smith and Eisner, 2006, inter alia). We can also observe a trend in recent work in textual entailment that more emphasis is put on explicit learning of the syntactic graph mapping between the entailed and entailed-by sentences (MacCartney et al., 2006). However, relatively fewer attempts have been made in the QA community. As pointed out by Katz and Lin (2003), most early experiments in QA that tried to bring in syntactic or semantic features showed little or no improvement, and it was often the case that performance actually degraded (Litkowski, 1999; Attardi et al., 20"
D07-1003,N03-2029,0,0.0120182,"e definitional questions and other more complex question types (e.g., How, Why) look for sentences or short passages. In this work, we fix the granularity of an answer to a single sentence. Earlier work on answer selection relies only on the surface-level text information. Two approaches are most common: surface pattern matching, and similarity measures on the question and answer, represented as bags of words. In the former, patterns for a certain answer type are either crafted manually (Soubbotin and Soubbotin, 2001) or acquired from training examples automatically (Ittycheriah et al., 2001; Ravichandran et al., 2003; Licuanan and Weischedel, 2003). In the latter, measures like cosine-similarity are applied to (usually) bag-of-words representations of the question and answer. Although many of these systems have achieved very good results in TREC-style evaluations, shallow methods using the bag-of-word representation clearly have their limitations. Examples of 22 Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational c Natural Language Learning, pp. 22–32, Prague, June 2007. 2007 Association for Computational Linguistics cases where the bag-of-words a"
D07-1003,P06-1112,0,0.0329313,"he usual similarity measures can then be used on the new feature representation. For example, Punyakanok et al. (2004) used approximate tree matching and tree-edit-distance to compute a similarity score between the question and answer parse trees. Similarly, Shen et al. (2005) experimented with dependency tree kernels to compute similarity between parse trees. Cui et al. (2005) measured sentence similarity based on similarity measures between dependency paths among aligned words. They used heuristic functions similar to mutual information to assign scores to matched pairs of dependency links. Shen and Klakow (2006) extend the idea further through the use of log-linear models to learn a scoring function for relation pairs. Echihabi and Marcu (2003) presented a noisychannel approach in which they adapted the IBM model 4 from statistical machine translation (Brown et al., 1990; Brown et al., 1993) and applied it to QA. Similarly, Murdock and Croft (2005) adopted a simple translation model from IBM model 1 (Brown et al., 1990; Brown et al., 1993) and applied it to QA. Porting the translation model to QA is not straightforward; it involves parse-tree pruning heuristics (the first two deterministic steps in E"
D07-1003,I05-1045,0,0.0152485,"tried to bring in syntactic or semantic features showed little or no improvement, and it was often the case that performance actually degraded (Litkowski, 1999; Attardi et al., 2001). More recent attempts have tried to augment the bag-ofwords representation—which, after all, is simply a real-valued feature vector—with syntactic features. The usual similarity measures can then be used on the new feature representation. For example, Punyakanok et al. (2004) used approximate tree matching and tree-edit-distance to compute a similarity score between the question and answer parse trees. Similarly, Shen et al. (2005) experimented with dependency tree kernels to compute similarity between parse trees. Cui et al. (2005) measured sentence similarity based on similarity measures between dependency paths among aligned words. They used heuristic functions similar to mutual information to assign scores to matched pairs of dependency links. Shen and Klakow (2006) extend the idea further through the use of log-linear models to learn a scoring function for relation pairs. Echihabi and Marcu (2003) presented a noisychannel approach in which they adapted the IBM model 4 from statistical machine translation (Brown et"
D07-1003,shima-etal-2006-modular,1,0.379433,"1 Introduction and Motivation Open-domain question answering (QA) is a widelystudied and fast-growing research problem. Stateof-the-art QA systems are extremely complex. They usually take the form of a pipeline architecture, chaining together modules that perform tasks such as answer type analysis (identifying whether the correct answer will be a person, location, date, etc.), document retrieval, answer candidate extraction, and answer reranking. This architecture is so predominant that each task listed above has evolved into its own sub-field and is often studied and evaluated independently (Shima et al., 2006). At a high level, the QA task boils down to only two essential steps (Echihabi and Marcu, 2003). The first step, retrieval, narrows down the search space from a corpus of millions of documents to a focused set of maybe a few hundred using an IR engine, where efficiency and recall are the main focus. The second step, selection, assesses each candidate answer string proposed by the first step, and finds the one that is most likely to be an answer to the given question. The granularity of the target answer string varies depending on the type of the question. For example, answers to factoid quest"
D07-1003,W06-3104,0,0.113674,"cience Carnegie Mellon University Pittsburgh, PA 15213 USA {mengqiu,nasmith,teruko}@cs.cmu.edu Abstract This paper presents a syntax-driven approach to question answering, specifically the answer-sentence selection problem for short-answer questions. Rather than using syntactic features to augment existing statistical classifiers (as in previous work), we build on the idea that questions and their (correct) answers relate to each other via loose but predictable syntactic transformations. We propose a probabilistic quasi-synchronous grammar, inspired by one proposed for machine translation (D. Smith and Eisner, 2006), and parameterized by mixtures of a robust nonlexical syntax/alignment model with a(n optional) lexical-semantics-driven log-linear model. Our model learns soft alignments as a hidden variable in discriminative training. Experimental results using the TREC dataset are shown to significantly outperform strong state-of-the-art baselines. 1 Introduction and Motivation Open-domain question answering (QA) is a widelystudied and fast-growing research problem. Stateof-the-art QA systems are extremely complex. They usually take the form of a pipeline architecture, chaining together modules that perfo"
D07-1003,P98-2230,0,0.0718167,"onous grammars in Section 3. Our version of QG, called the Jeopardy model, and our parameter estimation method are described in Section 4. Experimental results comparing our approach to two state-of-the-art baselines are presented in Section 5. We discuss portability to cross-lingual QA and other applied semantic processing tasks in Section 6. 2 Related Work To model the syntactic transformation process, researchers in these fields—especially in machine translation—have developed powerful grammatical formalisms and statistical models for representing and learning these tree-to-tree relations (Wu and Wong, 1998; Eisner, 2003; Gildea, 2003; Melamed, 2004; Ding and Palmer, 2005; Quirk et al., 2005; Galley et al., 2006; Smith and Eisner, 2006, inter alia). We can also observe a trend in recent work in textual entailment that more emphasis is put on explicit learning of the syntactic graph mapping between the entailed and entailed-by sentences (MacCartney et al., 2006). However, relatively fewer attempts have been made in the QA community. As pointed out by Katz and Lin (2003), most early experiments in QA that tried to bring in syntactic or semantic features showed little or no improvement, and it was"
D07-1003,W05-1205,0,0.0133641,"sing. Indeed, in this work, we imagine a generative story for QA in which the question is generated from the answer sentence through a series of syntactic and semantic transformations. The same story has been told for machine translation (Yamada and Knight, 2001, inter alia), in which a target language sentence (the desired output) has undergone semantic transformation (word to word translation) and syntactic transformation (syntax divergence across languages) to generate the source language sentence (noisy-channel model). Similar stories can also be found in paraphrasing (Quirk et al., 2004; Wu, 2005) and textual entailment (Harabagiu and Hickl, 2006; Wu, 2005). Our story makes use of a weighted formalism known as quasi-synchronous grammar (hereafter, QG), originally developed by D. Smith and Eisner (2006) for machine translation. Unlike most synchronous formalisms, QG does not posit a strict isomorphism between the two trees, and it provides 23 an elegant description for the set of local configurations. In Section 2 we situate our contribution in the context of earlier work, and we give a brief discussion of quasi-synchronous grammars in Section 3. Our version of QG, called the Jeopardy m"
D07-1003,P01-1067,0,0.0412892,"semantic variations occur in almost every question-answer pair, and typically they cannot be easily captured using shallow representations. It is also worth noting that such syntactic and semantic variations are not unique to QA; they can be found in many other closely related NLP tasks, motivating extensive community efforts in syntactic and semantic processing. Indeed, in this work, we imagine a generative story for QA in which the question is generated from the answer sentence through a series of syntactic and semantic transformations. The same story has been told for machine translation (Yamada and Knight, 2001, inter alia), in which a target language sentence (the desired output) has undergone semantic transformation (word to word translation) and syntactic transformation (syntax divergence across languages) to generate the source language sentence (noisy-channel model). Similar stories can also be found in paraphrasing (Quirk et al., 2004; Wu, 2005) and textual entailment (Harabagiu and Hickl, 2006; Wu, 2005). Our story makes use of a weighted formalism known as quasi-synchronous grammar (hereafter, QG), originally developed by D. Smith and Eisner (2006) for machine translation. Unlike most synchr"
D07-1003,C98-2225,0,\N,Missing
D07-1014,W03-1019,0,0.0213394,"irst, there were generative, stochastic models like HMMs, PCFGs, and Eisner’s (1996) models. Local discriminative classifiers were proposed by McCallum et al. (2000) for sequence modeling, by Ratnaparkhi et al. (1994) for constituent parsing, and by Hall et al. (2006) (among others) for 132 Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational c Natural Language Learning, pp. 132–140, Prague, June 2007. 2007 Association for Computational Linguistics dependencies. Large-margin whole-structure models were proposed for sequence labeling by Altun et al. (2003), for constituents by Taskar et al. (2004), and for dependency trees by McDonald et al. (2005a). In this paper, we propose a model most similar to the conditional random fields— interpretable as log-linear models—of Lafferty et al. (2001), which are now widely used for sequence labeling. Log-linear models have been used in parsing by Riezler et al. (2000) (for constraint-based grammars) and Johnson (2001) and Miyao and Tsujii (2002) (for CFGs). Like McDonald et al., we use an edge-factored model that permits nonprojective trees; like Lafferty et al., we argue for an alternative interpretation"
D07-1014,W06-2920,0,0.480413,"ncy parsing tasks and can be trained and applied without marginalization, “summing trees” permits some alternative techniques of interest. Using the summing algorithm, we present competitive experimental results on four nonprojective languages, for maximum conditional likelihood estimation, minimum Bayes-risk parsing, and hidden variable training. 1 Noah A. Smith Language Technologies Institute School of Computer Science Carnegie Mellon University Pittsburgh, PA 15213 USA nasmith@cs.cmu.edu Introduction Recently dependency parsing has received renewed interest, both in the parsing literature (Buchholz and Marsi, 2006) and in applications like translation (Quirk et al., 2005) and information extraction (Culotta and Sorensen, 2004). Dependency parsing can be used to provide a “bare bones” syntactic structure that approximates semantics, and it has the additional advantage of admitting fast parsing algorithms (Eisner, 1996; McDonald et al., 2005b) with a negligible grammar constant in many cases. Here we consider weighted dependency parsing models that can be used to define well-formed conditional distributions p(y |x), for dependency trees y and a sentence x. Conditional distributions over outputs (here, tre"
D07-1014,C02-1126,0,0.0145015,"though it may be possible to develop efficient approximations. 5 Hidden Variables A third application of probability distributions over trees is hidden-variable learning. The ExpectationMaximization (EM) algorithm (Baum and Petrie, 1966; Dempster et al., 1977; Baker, 1979), for example, is a way to maximum the likelihood of training data, marginalizing out hidden variables. 137 This has been applied widely in unsupervised parsing (Carroll and Charniak, 1992; Klein and Manning, 2002). More recently, EM has been used to learn hidden variables in parse trees; these can be head-child annotations (Chiang and Bikel, 2002), latent head features (Matsuzaki et al., 2005; Prescher, 2005; Dreyer and Eisner, 2006), or hierarchicallysplit nonterminal states (Petrov et al., 2006). To date, we know of no attempts to apply hidden variables to supervised dependency tree models. If the trees are constrained to be projective, EM is easily applied using the inside-outside variant of the parsing algorithm described by Eisner (1996) to compute the marginal probability. Moving to the nonprojective case, there are two difficulties: (a) we must marginalize over nonprojective trees and (b) we must define a generative model over ("
D07-1014,P04-1054,0,0.00587294,"ve techniques of interest. Using the summing algorithm, we present competitive experimental results on four nonprojective languages, for maximum conditional likelihood estimation, minimum Bayes-risk parsing, and hidden variable training. 1 Noah A. Smith Language Technologies Institute School of Computer Science Carnegie Mellon University Pittsburgh, PA 15213 USA nasmith@cs.cmu.edu Introduction Recently dependency parsing has received renewed interest, both in the parsing literature (Buchholz and Marsi, 2006) and in applications like translation (Quirk et al., 2005) and information extraction (Culotta and Sorensen, 2004). Dependency parsing can be used to provide a “bare bones” syntactic structure that approximates semantics, and it has the additional advantage of admitting fast parsing algorithms (Eisner, 1996; McDonald et al., 2005b) with a negligible grammar constant in many cases. Here we consider weighted dependency parsing models that can be used to define well-formed conditional distributions p(y |x), for dependency trees y and a sentence x. Conditional distributions over outputs (here, trees) given inputs (here, sentences) have certain advantages. They permit marginalization over trees to compute post"
D07-1014,W06-1638,0,0.0146662,"rd application of probability distributions over trees is hidden-variable learning. The ExpectationMaximization (EM) algorithm (Baum and Petrie, 1966; Dempster et al., 1977; Baker, 1979), for example, is a way to maximum the likelihood of training data, marginalizing out hidden variables. 137 This has been applied widely in unsupervised parsing (Carroll and Charniak, 1992; Klein and Manning, 2002). More recently, EM has been used to learn hidden variables in parse trees; these can be head-child annotations (Chiang and Bikel, 2002), latent head features (Matsuzaki et al., 2005; Prescher, 2005; Dreyer and Eisner, 2006), or hierarchicallysplit nonterminal states (Petrov et al., 2006). To date, we know of no attempts to apply hidden variables to supervised dependency tree models. If the trees are constrained to be projective, EM is easily applied using the inside-outside variant of the parsing algorithm described by Eisner (1996) to compute the marginal probability. Moving to the nonprojective case, there are two difficulties: (a) we must marginalize over nonprojective trees and (b) we must define a generative model over (X, Y). We have already shown in Section 3 how to solve (a); here we avoid (b) by maximiz"
D07-1014,C96-1058,0,0.712912,"nd hidden variable training. 1 Noah A. Smith Language Technologies Institute School of Computer Science Carnegie Mellon University Pittsburgh, PA 15213 USA nasmith@cs.cmu.edu Introduction Recently dependency parsing has received renewed interest, both in the parsing literature (Buchholz and Marsi, 2006) and in applications like translation (Quirk et al., 2005) and information extraction (Culotta and Sorensen, 2004). Dependency parsing can be used to provide a “bare bones” syntactic structure that approximates semantics, and it has the additional advantage of admitting fast parsing algorithms (Eisner, 1996; McDonald et al., 2005b) with a negligible grammar constant in many cases. Here we consider weighted dependency parsing models that can be used to define well-formed conditional distributions p(y |x), for dependency trees y and a sentence x. Conditional distributions over outputs (here, trees) given inputs (here, sentences) have certain advantages. They permit marginalization over trees to compute posteriors of interesting sub-events (e.g., the probability that two noun tokens bear a relation, regardless of which tree is correct). A probability model permits alternative decoding procedures (G"
D07-1014,P96-1024,0,0.549313,"6; McDonald et al., 2005b) with a negligible grammar constant in many cases. Here we consider weighted dependency parsing models that can be used to define well-formed conditional distributions p(y |x), for dependency trees y and a sentence x. Conditional distributions over outputs (here, trees) given inputs (here, sentences) have certain advantages. They permit marginalization over trees to compute posteriors of interesting sub-events (e.g., the probability that two noun tokens bear a relation, regardless of which tree is correct). A probability model permits alternative decoding procedures (Goodman, 1996). Well-motivated hidden variable training procedures (such as EM and conditional EM) are also readily available for probabilistic models. Finally, probability models can be chained together (as in a noisy channel model), mixed, or combined in a product-of-experts. Sequence models, context-free models, and dependency models have appeared in several guises; a cross-model comparison clarifies the contribution of this paper. First, there were generative, stochastic models like HMMs, PCFGs, and Eisner’s (1996) models. Local discriminative classifiers were proposed by McCallum et al. (2000) for sequ"
D07-1014,P01-1042,0,0.0489209,"Language Learning, pp. 132–140, Prague, June 2007. 2007 Association for Computational Linguistics dependencies. Large-margin whole-structure models were proposed for sequence labeling by Altun et al. (2003), for constituents by Taskar et al. (2004), and for dependency trees by McDonald et al. (2005a). In this paper, we propose a model most similar to the conditional random fields— interpretable as log-linear models—of Lafferty et al. (2001), which are now widely used for sequence labeling. Log-linear models have been used in parsing by Riezler et al. (2000) (for constraint-based grammars) and Johnson (2001) and Miyao and Tsujii (2002) (for CFGs). Like McDonald et al., we use an edge-factored model that permits nonprojective trees; like Lafferty et al., we argue for an alternative interpretation as a log-linear model over structures, conditioned on the observed sentence. In Section 2 we point out what would be required, computationally, for conditional training of nonprojective dependency models. The solution to the conditionalization problem is given in Section 3, using a widely-known but newly-applied Matrix Tree Theorem due to Tutte (1984), and experimental results are presented with a compari"
D07-1014,P02-1017,0,0.0212776,"al model is not edge-factored. All that is required are the marginals pθ~ (Y(i) = y(i) |x), which may be intractable to compute exactly, though it may be possible to develop efficient approximations. 5 Hidden Variables A third application of probability distributions over trees is hidden-variable learning. The ExpectationMaximization (EM) algorithm (Baum and Petrie, 1966; Dempster et al., 1977; Baker, 1979), for example, is a way to maximum the likelihood of training data, marginalizing out hidden variables. 137 This has been applied widely in unsupervised parsing (Carroll and Charniak, 1992; Klein and Manning, 2002). More recently, EM has been used to learn hidden variables in parse trees; these can be head-child annotations (Chiang and Bikel, 2002), latent head features (Matsuzaki et al., 2005; Prescher, 2005; Dreyer and Eisner, 2006), or hierarchicallysplit nonterminal states (Petrov et al., 2006). To date, we know of no attempts to apply hidden variables to supervised dependency tree models. If the trees are constrained to be projective, EM is easily applied using the inside-outside variant of the parsing algorithm described by Eisner (1996) to compute the marginal probability. Moving to the nonprojec"
D07-1014,H05-1064,0,0.0205116,"ective, EM is easily applied using the inside-outside variant of the parsing algorithm described by Eisner (1996) to compute the marginal probability. Moving to the nonprojective case, there are two difficulties: (a) we must marginalize over nonprojective trees and (b) we must define a generative model over (X, Y). We have already shown in Section 3 how to solve (a); here we avoid (b) by maximizing conditional likelihood, marginalizing out the hidden variable, denoted z: X X p˜(x, y) log pθ~ (y, z |x) (17) max θ~ x,y z This sort of conditional training with hidden variables was carried out by Koo and Collins (2005), for example, in reranking; it is related to the information bottleneck method (Tishby et al., 1999) and contrastive estimation (Smith and Eisner, 2005). 5.1 Latent Dependency Labels Noting that our model is edge-factored (Eq. 2), we define our hidden variables to be edge-factored as well. We can think of the hidden variables as clusters on dependency tokens, and redefine the score of an edge to be: sx,θ~ (i, j) = X ~ ~ eθ·f (x,xi ,xj ,z) (18) z∈Z where Z is a set of dependency clusters. Note that keeping the model edge-factored means that the cluster of each dependency in a tree is condition"
D07-1014,D07-1015,0,0.689582,", using a widely-known but newly-applied Matrix Tree Theorem due to Tutte (1984), and experimental results are presented with a comparison to the MIRA learning algorithm used by McDonald et al. (2005a). We go on to describe and experiment with two useful applications of conditional modeling: minimum Bayesrisk decoding (Section 4) and hidden-variable training by conditional maximum likelihood estimation (Section 5). Discussion in Section 6 considers the implications of our experimental results. Two indepedent papers, published concurrently with this one, report closely related results to ours. Koo et al. (2007) and McDonald and Satta (2007) both describe how the Matrix Tree Theorem can be applied to computing the sum of scores of edgefactored dependency trees and the edge marginals. Koo et al. compare conditional likelihood training (as here) to the averaged perceptron and a maximum margin model trained using exponentiatedgradient (Bartlett et al., 2004); the latter requires the same marginalization calculations as conditional log-linear estimation. McDonald and Satta discuss a variety of applications (including minimum Bayesrisk decoding) and give complexity results for nonedge-factored models. Int"
D07-1014,N04-1022,0,0.0125304,"of probability distributions over trees is the alternative decoding algorithm known as minimum Bayes-risk (mBr) decoding. The more commonly used maximum a posteriori decoding (also known as “Viterbi” decoding) that we applied in Section 3.3 sought to minimize the expected whole-tree loss: ˆ = argmax pθ~ (y |x) = argmin Ep~(Y|x) [−δ(y, Y)] y θ y y (14) Minimum Bayes-risk decoding generalizes this idea to an arbitrary loss function ` on the proposed tree: ˆ = argmin Ep~(Y|x) [`(y, Y)] y θ (15) y This technique was originally applied in speech recognition (Goel and Byrne, 2000) and translation (Kumar and Byrne, 2004); Goodman (1996) proposed a similar idea in probabilistic context-free parsing, seeking to maximize expected recall. For more applications in parsing, see Petrov and Klein (2007). The most common loss function used to evaluate dependency parsers is the number of attachment errors, so we seek to decode using: &quot; n # X ˆ = argmin Ep~ (Y|x) y −δ(y(i), Y(i)) θ y = argmax y i=1 n X pθ~ (Y(i) = y(i) |x) (16) i=1 To apply this decoding method, we make use of Eq. 13, which gives us the posterior probabilities of edges under the model, and the same ChiuLiu-Edmonds maximum directed spanning tree algorith"
D07-1014,P05-1010,0,0.0246773,"approximations. 5 Hidden Variables A third application of probability distributions over trees is hidden-variable learning. The ExpectationMaximization (EM) algorithm (Baum and Petrie, 1966; Dempster et al., 1977; Baker, 1979), for example, is a way to maximum the likelihood of training data, marginalizing out hidden variables. 137 This has been applied widely in unsupervised parsing (Carroll and Charniak, 1992; Klein and Manning, 2002). More recently, EM has been used to learn hidden variables in parse trees; these can be head-child annotations (Chiang and Bikel, 2002), latent head features (Matsuzaki et al., 2005; Prescher, 2005; Dreyer and Eisner, 2006), or hierarchicallysplit nonterminal states (Petrov et al., 2006). To date, we know of no attempts to apply hidden variables to supervised dependency tree models. If the trees are constrained to be projective, EM is easily applied using the inside-outside variant of the parsing algorithm described by Eisner (1996) to compute the marginal probability. Moving to the nonprojective case, there are two difficulties: (a) we must marginalize over nonprojective trees and (b) we must define a generative model over (X, Y). We have already shown in Section 3 how"
D07-1014,E06-1011,0,0.540636,"e that the features are edge-factored: f~(x, y) = n X f~(x, xi , xy(i) ) (2) i=1 In other words, the dependencies between words in the tree are all conditionally independent of each other, given the sequence x and the fact that the parse is a spanning tree. Despite the constraints they impose on features, edge-factored models have the advantage of tractable O(n3 ) inference algorithms or, with some trickery, O(n2 ) maximum a posteriori (“best parse tree”) inference algorithms in the nonprojective case. Exact nonprojective inference and estimation become intractable if we break edge factoring (McDonald and Pereira, 2006). We wish to estimate the parameters θ~ by maximizing the conditional likelihood (like a CRF) rather 1 To be precise, every word has in-degree 1, with the sole edge pointing from the word’s parent, xy(i) → xi . x0 has indegree 0. By definition, trees are acyclic. The edges need not be planar and may “cross” in the plane, since we do not have a projectivity constraint. In some formulations, exactly one node in x can attach to x0 ; here we allow multiple nodes to attach to x0 , since this occurs with some frequency in many existing datasets. Summation over trees where x0 has exactly one child is"
D07-1014,W07-2216,0,0.424405,"but newly-applied Matrix Tree Theorem due to Tutte (1984), and experimental results are presented with a comparison to the MIRA learning algorithm used by McDonald et al. (2005a). We go on to describe and experiment with two useful applications of conditional modeling: minimum Bayesrisk decoding (Section 4) and hidden-variable training by conditional maximum likelihood estimation (Section 5). Discussion in Section 6 considers the implications of our experimental results. Two indepedent papers, published concurrently with this one, report closely related results to ours. Koo et al. (2007) and McDonald and Satta (2007) both describe how the Matrix Tree Theorem can be applied to computing the sum of scores of edgefactored dependency trees and the edge marginals. Koo et al. compare conditional likelihood training (as here) to the averaged perceptron and a maximum margin model trained using exponentiatedgradient (Bartlett et al., 2004); the latter requires the same marginalization calculations as conditional log-linear estimation. McDonald and Satta discuss a variety of applications (including minimum Bayesrisk decoding) and give complexity results for nonedge-factored models. Interested readers are referred t"
D07-1014,P05-1012,0,0.74593,"005a) or local parsing decision scores (Hall et al., 2006). In the works cited, these scores are not intended to be interpreted as probabilistic quantities. A notable gap in research on statistical dependency parsing is a proper conditional probability distribution over nonprojective dependency trees for a given sentence. We exploit the Matrix Tree Theorem (Tutte, 1984) to derive an algorithm that efficiently sums the scores of all nonprojective trees in a sentence, permitting the definition of a conditional log-linear model over trees. While discriminative methods, such as those presented in McDonald et al. (2005b), obtain very high accuracy on standard dependency parsing tasks and can be trained and applied without marginalization, “summing trees” permits some alternative techniques of interest. Using the summing algorithm, we present competitive experimental results on four nonprojective languages, for maximum conditional likelihood estimation, minimum Bayes-risk parsing, and hidden variable training. 1 Noah A. Smith Language Technologies Institute School of Computer Science Carnegie Mellon University Pittsburgh, PA 15213 USA nasmith@cs.cmu.edu Introduction Recently dependency parsing has received r"
D07-1014,H05-1066,0,0.42133,"Missing"
D07-1014,P93-1024,0,0.0601716,"res. We propose several interpretations. First, it may simply be that many more clusters may be required. Note that the label-set sizes for the labeled versions of these datasets are larger than 32 (e.g., 50 for Danish). This has the unfortunate effect of blowing up the feature space beyond the memory capacity of our machines (hence our attempts at squeezing high-dimensional features through the clusters). Of course, improved clustering methods may also improve performance. In particular, a clusterlearning algorithm that permits clusters to split and/or merge, as in Petrov et al. (2006) or in Pereira et al. (1993), may be appropriate. Given the relative simplicity of clustering methods for context-free parsing to date (gains were found just by using Expectation-Maximization), we believe the fundamental reason clustering was not particularly helpful here is a structural one. In context-free parsing, the latent features are (in published work to date) on nonterminal states, which are the stuctural “bridge” between context-free rules. Adding features to those states is a way of pushing information—encoded indirectly, perhaps—farther around the tree, and therefore circumventing the strict independence assu"
D07-1014,N07-1051,0,0.0074192,"also known as “Viterbi” decoding) that we applied in Section 3.3 sought to minimize the expected whole-tree loss: ˆ = argmax pθ~ (y |x) = argmin Ep~(Y|x) [−δ(y, Y)] y θ y y (14) Minimum Bayes-risk decoding generalizes this idea to an arbitrary loss function ` on the proposed tree: ˆ = argmin Ep~(Y|x) [`(y, Y)] y θ (15) y This technique was originally applied in speech recognition (Goel and Byrne, 2000) and translation (Kumar and Byrne, 2004); Goodman (1996) proposed a similar idea in probabilistic context-free parsing, seeking to maximize expected recall. For more applications in parsing, see Petrov and Klein (2007). The most common loss function used to evaluate dependency parsers is the number of attachment errors, so we seek to decode using: &quot; n # X ˆ = argmin Ep~ (Y|x) y −δ(y(i), Y(i)) θ y = argmax y i=1 n X pθ~ (Y(i) = y(i) |x) (16) i=1 To apply this decoding method, we make use of Eq. 13, which gives us the posterior probabilities of edges under the model, and the same ChiuLiu-Edmonds maximum directed spanning tree algorithm used for maximum a posteriori decoding. Note that this decoding method can be applied regardless of how the model is trained. It merely requires assuming that the tree scores u"
D07-1014,P06-1055,0,0.116612,"able learning. The ExpectationMaximization (EM) algorithm (Baum and Petrie, 1966; Dempster et al., 1977; Baker, 1979), for example, is a way to maximum the likelihood of training data, marginalizing out hidden variables. 137 This has been applied widely in unsupervised parsing (Carroll and Charniak, 1992; Klein and Manning, 2002). More recently, EM has been used to learn hidden variables in parse trees; these can be head-child annotations (Chiang and Bikel, 2002), latent head features (Matsuzaki et al., 2005; Prescher, 2005; Dreyer and Eisner, 2006), or hierarchicallysplit nonterminal states (Petrov et al., 2006). To date, we know of no attempts to apply hidden variables to supervised dependency tree models. If the trees are constrained to be projective, EM is easily applied using the inside-outside variant of the parsing algorithm described by Eisner (1996) to compute the marginal probability. Moving to the nonprojective case, there are two difficulties: (a) we must marginalize over nonprojective trees and (b) we must define a generative model over (X, Y). We have already shown in Section 3 how to solve (a); here we avoid (b) by maximizing conditional likelihood, marginalizing out the hidden variable"
D07-1014,W05-1512,0,0.0168306,"Variables A third application of probability distributions over trees is hidden-variable learning. The ExpectationMaximization (EM) algorithm (Baum and Petrie, 1966; Dempster et al., 1977; Baker, 1979), for example, is a way to maximum the likelihood of training data, marginalizing out hidden variables. 137 This has been applied widely in unsupervised parsing (Carroll and Charniak, 1992; Klein and Manning, 2002). More recently, EM has been used to learn hidden variables in parse trees; these can be head-child annotations (Chiang and Bikel, 2002), latent head features (Matsuzaki et al., 2005; Prescher, 2005; Dreyer and Eisner, 2006), or hierarchicallysplit nonterminal states (Petrov et al., 2006). To date, we know of no attempts to apply hidden variables to supervised dependency tree models. If the trees are constrained to be projective, EM is easily applied using the inside-outside variant of the parsing algorithm described by Eisner (1996) to compute the marginal probability. Moving to the nonprojective case, there are two difficulties: (a) we must marginalize over nonprojective trees and (b) we must define a generative model over (X, Y). We have already shown in Section 3 how to solve (a); he"
D07-1014,P05-1034,0,0.0195596,"lization, “summing trees” permits some alternative techniques of interest. Using the summing algorithm, we present competitive experimental results on four nonprojective languages, for maximum conditional likelihood estimation, minimum Bayes-risk parsing, and hidden variable training. 1 Noah A. Smith Language Technologies Institute School of Computer Science Carnegie Mellon University Pittsburgh, PA 15213 USA nasmith@cs.cmu.edu Introduction Recently dependency parsing has received renewed interest, both in the parsing literature (Buchholz and Marsi, 2006) and in applications like translation (Quirk et al., 2005) and information extraction (Culotta and Sorensen, 2004). Dependency parsing can be used to provide a “bare bones” syntactic structure that approximates semantics, and it has the additional advantage of admitting fast parsing algorithms (Eisner, 1996; McDonald et al., 2005b) with a negligible grammar constant in many cases. Here we consider weighted dependency parsing models that can be used to define well-formed conditional distributions p(y |x), for dependency trees y and a sentence x. Conditional distributions over outputs (here, trees) given inputs (here, sentences) have certain advantages"
D07-1014,P00-1061,0,0.0318368,"n Natural Language Processing and Computational c Natural Language Learning, pp. 132–140, Prague, June 2007. 2007 Association for Computational Linguistics dependencies. Large-margin whole-structure models were proposed for sequence labeling by Altun et al. (2003), for constituents by Taskar et al. (2004), and for dependency trees by McDonald et al. (2005a). In this paper, we propose a model most similar to the conditional random fields— interpretable as log-linear models—of Lafferty et al. (2001), which are now widely used for sequence labeling. Log-linear models have been used in parsing by Riezler et al. (2000) (for constraint-based grammars) and Johnson (2001) and Miyao and Tsujii (2002) (for CFGs). Like McDonald et al., we use an edge-factored model that permits nonprojective trees; like Lafferty et al., we argue for an alternative interpretation as a log-linear model over structures, conditioned on the observed sentence. In Section 2 we point out what would be required, computationally, for conditional training of nonprojective dependency models. The solution to the conditionalization problem is given in Section 3, using a widely-known but newly-applied Matrix Tree Theorem due to Tutte (1984), an"
D07-1014,P05-1044,1,0.0822349,"oving to the nonprojective case, there are two difficulties: (a) we must marginalize over nonprojective trees and (b) we must define a generative model over (X, Y). We have already shown in Section 3 how to solve (a); here we avoid (b) by maximizing conditional likelihood, marginalizing out the hidden variable, denoted z: X X p˜(x, y) log pθ~ (y, z |x) (17) max θ~ x,y z This sort of conditional training with hidden variables was carried out by Koo and Collins (2005), for example, in reranking; it is related to the information bottleneck method (Tishby et al., 1999) and contrastive estimation (Smith and Eisner, 2005). 5.1 Latent Dependency Labels Noting that our model is edge-factored (Eq. 2), we define our hidden variables to be edge-factored as well. We can think of the hidden variables as clusters on dependency tokens, and redefine the score of an edge to be: sx,θ~ (i, j) = X ~ ~ eθ·f (x,xi ,xj ,z) (18) z∈Z where Z is a set of dependency clusters. Note that keeping the model edge-factored means that the cluster of each dependency in a tree is conditionally independent of all the others, given the words. This is computationally advantageous (we can factor out the marginalization of the hidden variable b"
D07-1014,W04-3201,0,0.0335905,"odels like HMMs, PCFGs, and Eisner’s (1996) models. Local discriminative classifiers were proposed by McCallum et al. (2000) for sequence modeling, by Ratnaparkhi et al. (1994) for constituent parsing, and by Hall et al. (2006) (among others) for 132 Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational c Natural Language Learning, pp. 132–140, Prague, June 2007. 2007 Association for Computational Linguistics dependencies. Large-margin whole-structure models were proposed for sequence labeling by Altun et al. (2003), for constituents by Taskar et al. (2004), and for dependency trees by McDonald et al. (2005a). In this paper, we propose a model most similar to the conditional random fields— interpretable as log-linear models—of Lafferty et al. (2001), which are now widely used for sequence labeling. Log-linear models have been used in parsing by Riezler et al. (2000) (for constraint-based grammars) and Johnson (2001) and Miyao and Tsujii (2002) (for CFGs). Like McDonald et al., we use an edge-factored model that permits nonprojective trees; like Lafferty et al., we argue for an alternative interpretation as a log-linear model over structures, con"
D07-1014,D07-1070,1,\N,Missing
D07-1022,P06-1084,0,0.244988,"lready (perfectly) morphologically disambiguated. This assumption is very common in multilingual parsing (see, for example, Cowan et al., 2005, and Buchholz et al., 2006). Until recently, the NLP literature on morphological processing was dominated by the largely non-probabilistic application of finite-state transducers (Kaplan and Kay, 1981; Koskenniemi, 1983; Beesley and Karttunen, 2003) and the largely unsupervised discovery of morphological patterns in text (Goldsmith, 2001; Wicentowski, 2002); Hebrew morphology receives special attention in Levinger et al. (1995), Daya et al. (2004), and Adler and Elhadad (2006). Lately a few supervised disambiguation methods have come about, including hidden Markov models (Hakkani-T¨ur et al., 2000; Hajiˇc et al., 2001), conditional random fields (Kudo et al., 2004; Smith et al., 2005b), and local support vector machines (Habash and Rambow, 2005). There are also morphological disambiguators designed specifically for Hebrew (Segal, 2000; Bar-Haim et al., 2005). 2.2 Why Joint Inference? In NLP, the separation of syntax and morphology is understandable when the latter is impoverished, as 209 in English. When both involve high levels of ambiguity, this separation become"
D07-1022,W05-0706,0,0.208385,"Missing"
D07-1022,H91-1060,0,0.172336,"izes each violation (by a morpheme) of a one-to-one correspondence,6 and each character edit required to transform one side of a correspondence into the other (without whitespace). Word boundaries are (here) known and included as index positions. In the case where m ~ˆ = m ~ ∗ (or equal up to whitespace) the method is identical to Parseval (and also to Tsarfaty, 2006). POS tag accuracy is evaluated the same way, for the same reasons; we report F1 -accuracy for tagging and parsing. 5.2 Experimental Comparison In our experiment we vary four settings: Evaluation Measures The “Parseval” measures (Black et al., 1991) are used to evaluate a parser’s phrase-structure trees against a gold standard. They compute precision and recall of constituents, each indexed by a label and two endpoints. As pointed out by Tsarfaty (2006), joint parsing of morphology and syntax renders this indexing inappropriate, since it assumes the yields of the trees are identical—that assumption is violated if there are any errors in the hypothesized m. ~ Tsarfaty (2006) instead indexed by non-whitespace character positions, to deal with segmentation mismatches. In general (and in this work) that is still insufficient, since L(~x) may"
D07-1022,W06-2920,0,0.0913054,"Missing"
D07-1022,A00-2018,0,0.280817,"Missing"
D07-1022,H05-1100,0,0.149941,"Missing"
D07-1022,W04-3246,0,0.0306848,"nput to the parser was already (perfectly) morphologically disambiguated. This assumption is very common in multilingual parsing (see, for example, Cowan et al., 2005, and Buchholz et al., 2006). Until recently, the NLP literature on morphological processing was dominated by the largely non-probabilistic application of finite-state transducers (Kaplan and Kay, 1981; Koskenniemi, 1983; Beesley and Karttunen, 2003) and the largely unsupervised discovery of morphological patterns in text (Goldsmith, 2001; Wicentowski, 2002); Hebrew morphology receives special attention in Levinger et al. (1995), Daya et al. (2004), and Adler and Elhadad (2006). Lately a few supervised disambiguation methods have come about, including hidden Markov models (Hakkani-T¨ur et al., 2000; Hajiˇc et al., 2001), conditional random fields (Kudo et al., 2004; Smith et al., 2005b), and local support vector machines (Habash and Rambow, 2005). There are also morphological disambiguators designed specifically for Hebrew (Segal, 2000; Bar-Haim et al., 2005). 2.2 Why Joint Inference? In NLP, the separation of syntax and morphology is understandable when the latter is impoverished, as 209 in English. When both involve high levels of amb"
D07-1022,J01-2001,0,0.00963688,"of 88,747 words (4,783 sentences) and parsed it using a probabilistic model. However, they assumed that the input to the parser was already (perfectly) morphologically disambiguated. This assumption is very common in multilingual parsing (see, for example, Cowan et al., 2005, and Buchholz et al., 2006). Until recently, the NLP literature on morphological processing was dominated by the largely non-probabilistic application of finite-state transducers (Kaplan and Kay, 1981; Koskenniemi, 1983; Beesley and Karttunen, 2003) and the largely unsupervised discovery of morphological patterns in text (Goldsmith, 2001; Wicentowski, 2002); Hebrew morphology receives special attention in Levinger et al. (1995), Daya et al. (2004), and Adler and Elhadad (2006). Lately a few supervised disambiguation methods have come about, including hidden Markov models (Hakkani-T¨ur et al., 2000; Hajiˇc et al., 2001), conditional random fields (Kudo et al., 2004; Smith et al., 2005b), and local support vector machines (Habash and Rambow, 2005). There are also morphological disambiguators designed specifically for Hebrew (Segal, 2000; Bar-Haim et al., 2005). 2.2 Why Joint Inference? In NLP, the separation of syntax and morph"
D07-1022,P96-1024,0,0.0674228,"ate the full distribution over morphological analyses for the sentence by a simpler, sentence-specific unigram model that assumes each word’s analysis is to be chosen independently of the others. Note that our model (pL ) does not make such an assumption, only the approximate model p0L does, and the approximation is per-sentence. The idea resembles a mean-field variational approximation for graphical models. Turning to implementation, we can solve for pL (m ~ i |~x) exactly using the forward-backward algorithm. We will call this method fvari,α (see Eq. 5). A closely related method, applied by Goodman (1996) is called minimum-risk decoding. Goodman called it “maximum expected recall” when applying it to parsing. In the HMM community it 4 In prior work involving factored syntax models— lexicalized (Klein and Manning, 2003b) and bilingual (Smith and Smith, 2004)—fpoe,1 was applied, and the asymptotic runtime went to O(n5 ) and O(n7 ). fpoe,α (~x) = argmax log pG (τ, m) ~ + α log pL (m ~ |~x) (4) hm,τ ~ i∈GEN(~ x) fvari,α (~x) = argmax log pG (τ, m) ~ +α Pn log pG (τ, m) ~ +α Pn hm,τ ~ i∈GEN(~ x) frisk,α (~x) = argmax hm,τ ~ i∈GEN(~ x) is sometimes called “posterior decoding.” Minimum risk decoding"
D07-1022,P05-1071,0,0.0273247,"ilistic application of finite-state transducers (Kaplan and Kay, 1981; Koskenniemi, 1983; Beesley and Karttunen, 2003) and the largely unsupervised discovery of morphological patterns in text (Goldsmith, 2001; Wicentowski, 2002); Hebrew morphology receives special attention in Levinger et al. (1995), Daya et al. (2004), and Adler and Elhadad (2006). Lately a few supervised disambiguation methods have come about, including hidden Markov models (Hakkani-T¨ur et al., 2000; Hajiˇc et al., 2001), conditional random fields (Kudo et al., 2004; Smith et al., 2005b), and local support vector machines (Habash and Rambow, 2005). There are also morphological disambiguators designed specifically for Hebrew (Segal, 2000; Bar-Haim et al., 2005). 2.2 Why Joint Inference? In NLP, the separation of syntax and morphology is understandable when the latter is impoverished, as 209 in English. When both involve high levels of ambiguity, this separation becomes harder to justify, as argued by Tsarfaty (2006). To our knowledge, that is the only study to move toward joint inference of syntax and morphology, presenting joint models and testing approximation of these models with two parsers: one a pipeline (segmentation → tagging →"
D07-1022,P01-1035,0,0.108436,"Missing"
D07-1022,C00-1042,0,0.128821,"Missing"
D07-1022,J98-4004,0,0.0164249,"Missing"
D07-1022,P03-1054,0,0.0360985,"rs inherent in pipelines. 3 Joint Inference of Morphology and Syntax GEN(~x) = {hm, ~ τi |m ~ ∈ L(~x), τ ∈ DG (m)} ~ 3.2 Product of Experts Our mapping f (~x) is based on a joint probability model p(τ, m ~ |~x) which combines two probability models pG (τ, m) ~ (a PCFG built on the grammar G) and pL (m ~ |~x) (a morphological disambiguation model built on the lexicon L). Factoring the joint model into sub-models simplifies training, since we can train each model separately, and inference (parsing), as we will see later in this section. Factored estimation has been quite popular in NLP of late (Klein and Manning, 2003b; Smith and Smith, 2004; Smith et al., 2005a, inter alia). The most obvious joint parser uses pG as a conditional model over trees given morphemes and maximizes the joint likelihood: flik (~x) We now formalize the problem and supply the necessary framework for performing joint morphological disambiguation and syntactic parsing. 3.1 classifier. We use DG (m) ~ ⊆ T to denote the set of valid trees under a grammar G (here, a PCFG with terminal alphabet M) for morpheme sequence m. ~ To be precise, f (~x) selects a mutually consistent morphological and syntactic analysis from = argmax pG (τ |m) ~"
D07-1022,W04-3230,0,0.0168627,"terature on morphological processing was dominated by the largely non-probabilistic application of finite-state transducers (Kaplan and Kay, 1981; Koskenniemi, 1983; Beesley and Karttunen, 2003) and the largely unsupervised discovery of morphological patterns in text (Goldsmith, 2001; Wicentowski, 2002); Hebrew morphology receives special attention in Levinger et al. (1995), Daya et al. (2004), and Adler and Elhadad (2006). Lately a few supervised disambiguation methods have come about, including hidden Markov models (Hakkani-T¨ur et al., 2000; Hajiˇc et al., 2001), conditional random fields (Kudo et al., 2004; Smith et al., 2005b), and local support vector machines (Habash and Rambow, 2005). There are also morphological disambiguators designed specifically for Hebrew (Segal, 2000; Bar-Haim et al., 2005). 2.2 Why Joint Inference? In NLP, the separation of syntax and morphology is understandable when the latter is impoverished, as 209 in English. When both involve high levels of ambiguity, this separation becomes harder to justify, as argued by Tsarfaty (2006). To our knowledge, that is the only study to move toward joint inference of syntax and morphology, presenting joint models and testing approx"
D07-1022,2004.tmi-1.1,0,0.0251319,"τ ~ i∈GEN(~ x) pL (m ~ 0 , ~x) pG (τ 0 , m) ~ argmax τ0 Notation and Morphological Sausages Let X be the language’s word vocabulary and M be its morpheme inventory. The set of valid analyses for a surface word is defined using a morphological lexicon L, which defines L(x) ⊆ M+ . L(~x) ⊆ (M+ )+ (sequence of sequences) is the set of wholesentence analyses for sentence ~x = hx1 , x2 , ..., xn i, produced by concatenating elements of L(xi ) in order. L(~x) can be represented as an acyclic lattice with a “sausage” shape familiar from speech recognition (Mangu et al., 1999) and machine translation (Lavie et al., 2004). Fig. 1h shows a sausage lattice for a sentence in Hebrew. We use m ~ to denote an element of L(~x) and m ~ i to denote an element of L(xi ); in general, m ~ = hm ~ 1, m ~ 2 , ..., m ~ n i. We are interested in a function f : X+ → (M+ )+ × T, where T is the set of syntactic trees for the language. f can be viewed as a structured 210 (1) hm,τ ~ i∈GEN(~ x) m ~0 This is not straightforward, because it involves summing up the trees for each m ~ to compute pG (m), ~ which calls for the O(|m| ~ 3 )-Inside algorithm to be called on each m. ~ Instead, we use the joint, pG (τ, m), ~ which, strictly sp"
D07-1022,J95-3004,0,0.104992,"they assumed that the input to the parser was already (perfectly) morphologically disambiguated. This assumption is very common in multilingual parsing (see, for example, Cowan et al., 2005, and Buchholz et al., 2006). Until recently, the NLP literature on morphological processing was dominated by the largely non-probabilistic application of finite-state transducers (Kaplan and Kay, 1981; Koskenniemi, 1983; Beesley and Karttunen, 2003) and the largely unsupervised discovery of morphological patterns in text (Goldsmith, 2001; Wicentowski, 2002); Hebrew morphology receives special attention in Levinger et al. (1995), Daya et al. (2004), and Adler and Elhadad (2006). Lately a few supervised disambiguation methods have come about, including hidden Markov models (Hakkani-T¨ur et al., 2000; Hajiˇc et al., 2001), conditional random fields (Kudo et al., 2004; Smith et al., 2005b), and local support vector machines (Habash and Rambow, 2005). There are also morphological disambiguators designed specifically for Hebrew (Segal, 2000; Bar-Haim et al., 2005). 2.2 Why Joint Inference? In NLP, the separation of syntax and morphology is understandable when the latter is impoverished, as 209 in English. When both involv"
D07-1022,P05-1010,0,0.00730791,"n the grammar constant (at best, quadratic in the number of nonterminals), parsing with pI is not computationally attractive.4 fpoe,α is not, then, a scalable solution when we wish to use a morphology model pL that can make interdependent decisions about different words in ~x in context. We propose two new, efficient dynamic programming solutions for joint parsing. In the first, we approximate the distribution ~ |~x) using a unigram-factored model of the pL (M form in Eq. 3: Q ~i = m (7) ~ |~x) = ni=1 pL (M ~ |~x) p0L (m {z i } | posterior, depends on all of ~x Similar methods were applied by Matsuzaki et al. (2005) and Petrov and Klein (2007) for parsing under a PCFG with nonterminals with latent annotations. Their approach was variational, approximating the true posterior over coarse parses using a sentence-specific PCFG on the coarse nonterminals, created directly out of the true fine-grained PCFG. In our case, we approximate the full distribution over morphological analyses for the sentence by a simpler, sentence-specific unigram model that assumes each word’s analysis is to be chosen independently of the others. Note that our model (pL ) does not make such an assumption, only the approximate model p"
D07-1022,N07-1051,0,0.0350744,"est, quadratic in the number of nonterminals), parsing with pI is not computationally attractive.4 fpoe,α is not, then, a scalable solution when we wish to use a morphology model pL that can make interdependent decisions about different words in ~x in context. We propose two new, efficient dynamic programming solutions for joint parsing. In the first, we approximate the distribution ~ |~x) using a unigram-factored model of the pL (M form in Eq. 3: Q ~i = m (7) ~ |~x) = ni=1 pL (M ~ |~x) p0L (m {z i } | posterior, depends on all of ~x Similar methods were applied by Matsuzaki et al. (2005) and Petrov and Klein (2007) for parsing under a PCFG with nonterminals with latent annotations. Their approach was variational, approximating the true posterior over coarse parses using a sentence-specific PCFG on the coarse nonterminals, created directly out of the true fine-grained PCFG. In our case, we approximate the full distribution over morphological analyses for the sentence by a simpler, sentence-specific unigram model that assumes each word’s analysis is to be chosen independently of the others. Note that our model (pL ) does not make such an assumption, only the approximate model p0L does, and the approximati"
D07-1022,roark-etal-2006-sparseval,0,0.0212971,"old standard. They compute precision and recall of constituents, each indexed by a label and two endpoints. As pointed out by Tsarfaty (2006), joint parsing of morphology and syntax renders this indexing inappropriate, since it assumes the yields of the trees are identical—that assumption is violated if there are any errors in the hypothesized m. ~ Tsarfaty (2006) instead indexed by non-whitespace character positions, to deal with segmentation mismatches. In general (and in this work) that is still insufficient, since L(~x) may include m ~ that are not simply segmentations of ~x (see §4.2.1). Roark et al. (2006) propose an evaluation metric for comparing a parse tree over a sentence generated by a speech recognizer to a gold-standard parse. As in our case, the hypothesized tree could have a different yield than the original gold-standard 214 • Decoding algorithm: fpoe,α , frisk,α , or fvari,α (§3.3). • Syntax model: Gvan or Gv=2 (§4.1). crf • Morphology model: puni L or pL (§4.2). In the latter case, we can use the scores over morpheme sequences only (summing out tags before lattice parsing; denoted m.-pcrf L ) or the full model over 7 morphemes and tags, denoted t.-pcrf L . • α, the relative strengt"
D07-1022,W04-3207,1,0.811606,"3 Joint Inference of Morphology and Syntax GEN(~x) = {hm, ~ τi |m ~ ∈ L(~x), τ ∈ DG (m)} ~ 3.2 Product of Experts Our mapping f (~x) is based on a joint probability model p(τ, m ~ |~x) which combines two probability models pG (τ, m) ~ (a PCFG built on the grammar G) and pL (m ~ |~x) (a morphological disambiguation model built on the lexicon L). Factoring the joint model into sub-models simplifies training, since we can train each model separately, and inference (parsing), as we will see later in this section. Factored estimation has been quite popular in NLP of late (Klein and Manning, 2003b; Smith and Smith, 2004; Smith et al., 2005a, inter alia). The most obvious joint parser uses pG as a conditional model over trees given morphemes and maximizes the joint likelihood: flik (~x) We now formalize the problem and supply the necessary framework for performing joint morphological disambiguation and syntactic parsing. 3.1 classifier. We use DG (m) ~ ⊆ T to denote the set of valid trees under a grammar G (here, a PCFG with terminal alphabet M) for morpheme sequence m. ~ To be precise, f (~x) selects a mutually consistent morphological and syntactic analysis from = argmax pG (τ |m) ~ · pL (m ~ |~x) = pL (m,"
D07-1022,P05-1003,0,0.100974,"ogical processing was dominated by the largely non-probabilistic application of finite-state transducers (Kaplan and Kay, 1981; Koskenniemi, 1983; Beesley and Karttunen, 2003) and the largely unsupervised discovery of morphological patterns in text (Goldsmith, 2001; Wicentowski, 2002); Hebrew morphology receives special attention in Levinger et al. (1995), Daya et al. (2004), and Adler and Elhadad (2006). Lately a few supervised disambiguation methods have come about, including hidden Markov models (Hakkani-T¨ur et al., 2000; Hajiˇc et al., 2001), conditional random fields (Kudo et al., 2004; Smith et al., 2005b), and local support vector machines (Habash and Rambow, 2005). There are also morphological disambiguators designed specifically for Hebrew (Segal, 2000; Bar-Haim et al., 2005). 2.2 Why Joint Inference? In NLP, the separation of syntax and morphology is understandable when the latter is impoverished, as 209 in English. When both involve high levels of ambiguity, this separation becomes harder to justify, as argued by Tsarfaty (2006). To our knowledge, that is the only study to move toward joint inference of syntax and morphology, presenting joint models and testing approximation of these mod"
D07-1022,H05-1060,1,0.902295,"ogical processing was dominated by the largely non-probabilistic application of finite-state transducers (Kaplan and Kay, 1981; Koskenniemi, 1983; Beesley and Karttunen, 2003) and the largely unsupervised discovery of morphological patterns in text (Goldsmith, 2001; Wicentowski, 2002); Hebrew morphology receives special attention in Levinger et al. (1995), Daya et al. (2004), and Adler and Elhadad (2006). Lately a few supervised disambiguation methods have come about, including hidden Markov models (Hakkani-T¨ur et al., 2000; Hajiˇc et al., 2001), conditional random fields (Kudo et al., 2004; Smith et al., 2005b), and local support vector machines (Habash and Rambow, 2005). There are also morphological disambiguators designed specifically for Hebrew (Segal, 2000; Bar-Haim et al., 2005). 2.2 Why Joint Inference? In NLP, the separation of syntax and morphology is understandable when the latter is impoverished, as 209 in English. When both involve high levels of ambiguity, this separation becomes harder to justify, as argued by Tsarfaty (2006). To our knowledge, that is the only study to move toward joint inference of syntax and morphology, presenting joint models and testing approximation of these mod"
D07-1022,P06-3009,0,0.484611,"uation methods have come about, including hidden Markov models (Hakkani-T¨ur et al., 2000; Hajiˇc et al., 2001), conditional random fields (Kudo et al., 2004; Smith et al., 2005b), and local support vector machines (Habash and Rambow, 2005). There are also morphological disambiguators designed specifically for Hebrew (Segal, 2000; Bar-Haim et al., 2005). 2.2 Why Joint Inference? In NLP, the separation of syntax and morphology is understandable when the latter is impoverished, as 209 in English. When both involve high levels of ambiguity, this separation becomes harder to justify, as argued by Tsarfaty (2006). To our knowledge, that is the only study to move toward joint inference of syntax and morphology, presenting joint models and testing approximation of these models with two parsers: one a pipeline (segmentation → tagging → parsing), the other involved joint inference of segmentation and tagging, with the result piped to the parser. The latter was slightly more accurate. Tsarfaty discussed but did not carry out joint inference. In a morphologically rich language, the different morphemes that make up a word can play a variety of different syntactic roles. A reasonable linguistic analysis might"
D07-1022,W05-0702,0,0.0679741,"Missing"
D07-1022,J03-4003,0,\N,Missing
D08-1017,P99-1070,0,0.0394765,"Missing"
D08-1017,W06-2920,0,0.089328,"l the arcs using a log-linear classifier with access to the full unlabeled parse (McDonald et al., 2005a; 162 McDonald et al., 2005b; McDonald and Pereira, 2006). In stacking experiments, the arc labels from the level 0 parser are also used as a feature.4 In the following subsections, we refer to our modification of the MSTParser as MST 1O (the arcfactored version) and MST 2O (the second-order arc-pair-factored version). All our experiments use the non-projective version of this parser. We refer to the MaltParser as Malt. We report experiments on twelve languages from the CoNLL-X shared task (Buchholz and Marsi, 2006).5 All experiments are evaluated using the labeled attachment score (LAS), using the default settings.6 Statistical significance is measured using Dan Bikel’s randomized parsing evaluation comparator with 10,000 iterations.7 The additional features used in the level 1 parser are enumerated in Table 1 and their various subsets are depicted in Table 2. The PredEdge features are exactly the six features used by Nivre and McDonald (2008) in their MSTMalt parser; therefore, feature set A is a replication of this parser except for modifications noted in footnote 4. In all our experiments, the number"
D08-1017,P04-1054,0,0.0321457,"Missing"
D08-1017,P05-1067,0,0.0603954,"Missing"
D08-1017,P99-1059,0,0.362595,"art dependency parsers. 1 • Feature-rich parsers must resort to search or greediness, (Ratnaparkhi et al., 1994; Sagae and Lavie, 2005; Hall et al., 2006), so that parsing solutions are inexact and learned models may be subject to certain kinds of bias (Lafferty et al., 2001). Introduction In this paper we address a representation-efficiency tradeoff in statistical natural language processing through the use of stacked learning (Wolpert, 1992). This tradeoff is exemplified in dependency parsing, illustrated in Fig. 1, on which we focus in this paper: • Exact algorithms for dependency parsing (Eisner and Satta, 1999; McDonald et al., 2005b) are tractable only when the model makes very strong, linguistically unsupportable independence A solution that leverages the complementary strengths of these two approaches—described in detail by McDonald and Nivre (2007)—was recently and successfully explored by Nivre and McDonald (2008). Our contribution begins by reinterpreting and generalizing their parser combination scheme as a stacking of parsers. We give a new theoretical motivation for stacking parsers, in terms of extending a parsing model’s feature space. Specifically, we view stacked learning as a way of a"
D08-1017,C96-1058,0,0.903966,"of these two approaches—described in detail by McDonald and Nivre (2007)—was recently and successfully explored by Nivre and McDonald (2008). Our contribution begins by reinterpreting and generalizing their parser combination scheme as a stacking of parsers. We give a new theoretical motivation for stacking parsers, in terms of extending a parsing model’s feature space. Specifically, we view stacked learning as a way of approximating non-local features in a linear model, rather than making empirically dubious independence (McDonald et al., 2005b) or structural assumptions (e.g., projectivity, Eisner, 1996), using search approximations (Sagae and Lavie, 2005; Hall et al., 2006; McDonald and Pereira, 2006), solving a (generally NP-hard) integer linear program (Riedel and Clarke, 2006), or adding latent variables (Titov and Henderson, 2007). Notably, we introduce the use of very rich non-local approximate features in one parser, through the output of another parser. Related approaches are the belief propagation algorithm of Smith and Eisner (2008), and the “trading of structure for features” explored by Liang et al. 157 Proceedings of the 2008 Conference on Empirical Methods in Natural Language Pr"
D08-1017,P06-2041,0,0.494122,"tures for another. We show that this is an example of stacked learning, in which a second predictor is trained to improve the performance of the first. Further, we argue that this technique is a novel way of approximating rich non-local features in the second parser, without sacrificing efficient, model-optimal prediction. Experiments on twelve languages show that stacking transition-based and graphbased parsers improves performance over existing state-of-the-art dependency parsers. 1 • Feature-rich parsers must resort to search or greediness, (Ratnaparkhi et al., 1994; Sagae and Lavie, 2005; Hall et al., 2006), so that parsing solutions are inexact and learned models may be subject to certain kinds of bias (Lafferty et al., 2001). Introduction In this paper we address a representation-efficiency tradeoff in statistical natural language processing through the use of stacked learning (Wolpert, 1992). This tradeoff is exemplified in dependency parsing, illustrated in Fig. 1, on which we focus in this paper: • Exact algorithms for dependency parsing (Eisner and Satta, 1999; McDonald et al., 2005b) are tractable only when the model makes very strong, linguistically unsupportable independence A solution"
D08-1017,D07-1013,0,0.0628025,"of bias (Lafferty et al., 2001). Introduction In this paper we address a representation-efficiency tradeoff in statistical natural language processing through the use of stacked learning (Wolpert, 1992). This tradeoff is exemplified in dependency parsing, illustrated in Fig. 1, on which we focus in this paper: • Exact algorithms for dependency parsing (Eisner and Satta, 1999; McDonald et al., 2005b) are tractable only when the model makes very strong, linguistically unsupportable independence A solution that leverages the complementary strengths of these two approaches—described in detail by McDonald and Nivre (2007)—was recently and successfully explored by Nivre and McDonald (2008). Our contribution begins by reinterpreting and generalizing their parser combination scheme as a stacking of parsers. We give a new theoretical motivation for stacking parsers, in terms of extending a parsing model’s feature space. Specifically, we view stacked learning as a way of approximating non-local features in a linear model, rather than making empirically dubious independence (McDonald et al., 2005b) or structural assumptions (e.g., projectivity, Eisner, 1996), using search approximations (Sagae and Lavie, 2005; Hall"
D08-1017,E06-1011,0,0.769242,"and successfully explored by Nivre and McDonald (2008). Our contribution begins by reinterpreting and generalizing their parser combination scheme as a stacking of parsers. We give a new theoretical motivation for stacking parsers, in terms of extending a parsing model’s feature space. Specifically, we view stacked learning as a way of approximating non-local features in a linear model, rather than making empirically dubious independence (McDonald et al., 2005b) or structural assumptions (e.g., projectivity, Eisner, 1996), using search approximations (Sagae and Lavie, 2005; Hall et al., 2006; McDonald and Pereira, 2006), solving a (generally NP-hard) integer linear program (Riedel and Clarke, 2006), or adding latent variables (Titov and Henderson, 2007). Notably, we introduce the use of very rich non-local approximate features in one parser, through the output of another parser. Related approaches are the belief propagation algorithm of Smith and Eisner (2008), and the “trading of structure for features” explored by Liang et al. 157 Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 157–166, c Honolulu, October 2008. 2008 Association for Computational Linguistics le"
D08-1017,W07-2216,0,0.226249,"nts to solving arg maxy∈Y(x) w> f (x, y), where w is a weight vector. With a projectivity constraint and arc factorization, the parsing problem can be solved in cubic time by dynamic programming (Eisner, 1996), and with a weaker “tree” constraint (permitting nonprojective parses) and arc factorization, a quadratic-time algorithm exists (Chu and Liu, 1965; Edmonds, 1967), as shown by McDonald et al. (2005b). In the projective case, the arc-factored assumption can be weakened in certain ways while maintaining polynomial parser runtime (Eisner and Satta, 1999), but not in the nonprojective case (McDonald and Satta, 2007), where finding the highest-scoring tree becomes NP-hard. McDonald and Pereira (2006) adopted an approximation based on O(n3 ) projective parsing followed by rearrangement to permit crossing arcs, achieving higher performance. In §3 we adopt a framework that maintains O(n2 ) runtime (still exploiting the Chu-Liu-Edmonds algorithm) while approximating non arc-factored features. 2.2 Stacked Learning Stacked generalization was first proposed by Wolpert (1992) and Breiman (1996) for regression. The idea is to include two “levels” of predictors. The first level, “level 0,” includes one or more pred"
D08-1017,P05-1012,0,0.565159,"c P. Xing∗ ∗ School of Computer Science, Carnegie Mellon University, Pittsburgh, PA 15213, USA † Instituto de Telecomunicac¸o˜ es, Instituto Superior T´ecnico, Lisboa, Portugal {afm,dipanjan,nasmith,epxing}@cs.cmu.edu Abstract assumptions, such as “arc factorization” for nonprojective dependency parsing (McDonald and Satta, 2007). We explore a stacked framework for learning to predict dependency structures for natural language sentences. A typical approach in graph-based dependency parsing has been to assume a factorized model, where local features are used but a global function is optimized (McDonald et al., 2005b). Recently Nivre and McDonald (2008) used the output of one dependency parser to provide features for another. We show that this is an example of stacked learning, in which a second predictor is trained to improve the performance of the first. Further, we argue that this technique is a novel way of approximating rich non-local features in the second parser, without sacrificing efficient, model-optimal prediction. Experiments on twelve languages show that stacking transition-based and graphbased parsers improves performance over existing state-of-the-art dependency parsers. 1 • Feature-rich p"
D08-1017,H05-1066,0,0.389842,"c P. Xing∗ ∗ School of Computer Science, Carnegie Mellon University, Pittsburgh, PA 15213, USA † Instituto de Telecomunicac¸o˜ es, Instituto Superior T´ecnico, Lisboa, Portugal {afm,dipanjan,nasmith,epxing}@cs.cmu.edu Abstract assumptions, such as “arc factorization” for nonprojective dependency parsing (McDonald and Satta, 2007). We explore a stacked framework for learning to predict dependency structures for natural language sentences. A typical approach in graph-based dependency parsing has been to assume a factorized model, where local features are used but a global function is optimized (McDonald et al., 2005b). Recently Nivre and McDonald (2008) used the output of one dependency parser to provide features for another. We show that this is an example of stacked learning, in which a second predictor is trained to improve the performance of the first. Further, we argue that this technique is a novel way of approximating rich non-local features in the second parser, without sacrificing efficient, model-optimal prediction. Experiments on twelve languages show that stacking transition-based and graphbased parsers improves performance over existing state-of-the-art dependency parsers. 1 • Feature-rich p"
D08-1017,W06-2932,0,0.0406989,"odifications noted in footnote 4. In all our experiments, the number of ˜ is L = 2. partitions used to create D 5.2 Experiment: MST 2O + MST 2O Our first experiment stacks the highly accurate MST 2O parser with itself. At level 0, the parser uses only the standard features (§5.1), and at level 1, these are augmented by various subsets of features of x along with the output of the level 0 parser, g(x) (Table 2). The results are shown in Table 3. While we see improvements over the single-parser baseline 4 We made other modifications to MSTParser, implementing many of the successes described by (McDonald et al., 2006). Our version of the code is publicly available at http: //www.ark.cs.cmu.edu/MSTParserStacked. The modifications included an approximation to lemmas for datasets without lemmas (three-character prefixes), and replacing morphology/word and morphology/lemma features with morphology/POS features. 5 The CoNLL-X shared task actually involves thirteen languages; our experiments do not include Czech (the largest dataset), due to time constraints. Therefore, the average results plotted in the last rows of Tables 3, 4, and 5 are not directly comparable with previously published averages over thirteen"
D08-1017,P08-1108,0,0.189783,"ience, Carnegie Mellon University, Pittsburgh, PA 15213, USA † Instituto de Telecomunicac¸o˜ es, Instituto Superior T´ecnico, Lisboa, Portugal {afm,dipanjan,nasmith,epxing}@cs.cmu.edu Abstract assumptions, such as “arc factorization” for nonprojective dependency parsing (McDonald and Satta, 2007). We explore a stacked framework for learning to predict dependency structures for natural language sentences. A typical approach in graph-based dependency parsing has been to assume a factorized model, where local features are used but a global function is optimized (McDonald et al., 2005b). Recently Nivre and McDonald (2008) used the output of one dependency parser to provide features for another. We show that this is an example of stacked learning, in which a second predictor is trained to improve the performance of the first. Further, we argue that this technique is a novel way of approximating rich non-local features in the second parser, without sacrificing efficient, model-optimal prediction. Experiments on twelve languages show that stacking transition-based and graphbased parsers improves performance over existing state-of-the-art dependency parsers. 1 • Feature-rich parsers must resort to search or greedi"
D08-1017,W04-2407,0,0.0834669,"Missing"
D08-1017,W06-2933,0,0.0492279,"Missing"
D08-1017,W06-1616,0,0.178317,"reinterpreting and generalizing their parser combination scheme as a stacking of parsers. We give a new theoretical motivation for stacking parsers, in terms of extending a parsing model’s feature space. Specifically, we view stacked learning as a way of approximating non-local features in a linear model, rather than making empirically dubious independence (McDonald et al., 2005b) or structural assumptions (e.g., projectivity, Eisner, 1996), using search approximations (Sagae and Lavie, 2005; Hall et al., 2006; McDonald and Pereira, 2006), solving a (generally NP-hard) integer linear program (Riedel and Clarke, 2006), or adding latent variables (Titov and Henderson, 2007). Notably, we introduce the use of very rich non-local approximate features in one parser, through the output of another parser. Related approaches are the belief propagation algorithm of Smith and Eisner (2008), and the “trading of structure for features” explored by Liang et al. 157 Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 157–166, c Honolulu, October 2008. 2008 Association for Computational Linguistics learning via the EM algorithm – none of which have 2.1 Dependency Parsing previous"
D08-1017,W05-1513,0,0.192799,"y parser to provide features for another. We show that this is an example of stacked learning, in which a second predictor is trained to improve the performance of the first. Further, we argue that this technique is a novel way of approximating rich non-local features in the second parser, without sacrificing efficient, model-optimal prediction. Experiments on twelve languages show that stacking transition-based and graphbased parsers improves performance over existing state-of-the-art dependency parsers. 1 • Feature-rich parsers must resort to search or greediness, (Ratnaparkhi et al., 1994; Sagae and Lavie, 2005; Hall et al., 2006), so that parsing solutions are inexact and learned models may be subject to certain kinds of bias (Lafferty et al., 2001). Introduction In this paper we address a representation-efficiency tradeoff in statistical natural language processing through the use of stacked learning (Wolpert, 1992). This tradeoff is exemplified in dependency parsing, illustrated in Fig. 1, on which we focus in this paper: • Exact algorithms for dependency parsing (Eisner and Satta, 1999; McDonald et al., 2005b) are tractable only when the model makes very strong, linguistically unsupportable inde"
D08-1017,D08-1016,0,0.103348,"non-local features in a linear model, rather than making empirically dubious independence (McDonald et al., 2005b) or structural assumptions (e.g., projectivity, Eisner, 1996), using search approximations (Sagae and Lavie, 2005; Hall et al., 2006; McDonald and Pereira, 2006), solving a (generally NP-hard) integer linear program (Riedel and Clarke, 2006), or adding latent variables (Titov and Henderson, 2007). Notably, we introduce the use of very rich non-local approximate features in one parser, through the output of another parser. Related approaches are the belief propagation algorithm of Smith and Eisner (2008), and the “trading of structure for features” explored by Liang et al. 157 Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 157–166, c Honolulu, October 2008. 2008 Association for Computational Linguistics learning via the EM algorithm – none of which have 2.1 Dependency Parsing previously been to have exact learning via the known EM algorithm – nonenon-projective of which have implementations. previously been known to have exact non-projective Dependency syntax is a lightweight syntactic repWe then switch to models that account for where implementa"
D08-1017,W07-2218,0,0.0295174,"on scheme as a stacking of parsers. We give a new theoretical motivation for stacking parsers, in terms of extending a parsing model’s feature space. Specifically, we view stacked learning as a way of approximating non-local features in a linear model, rather than making empirically dubious independence (McDonald et al., 2005b) or structural assumptions (e.g., projectivity, Eisner, 1996), using search approximations (Sagae and Lavie, 2005; Hall et al., 2006; McDonald and Pereira, 2006), solving a (generally NP-hard) integer linear program (Riedel and Clarke, 2006), or adding latent variables (Titov and Henderson, 2007). Notably, we introduce the use of very rich non-local approximate features in one parser, through the output of another parser. Related approaches are the belief propagation algorithm of Smith and Eisner (2008), and the “trading of structure for features” explored by Liang et al. 157 Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 157–166, c Honolulu, October 2008. 2008 Association for Computational Linguistics learning via the EM algorithm – none of which have 2.1 Dependency Parsing previously been to have exact learning via the known EM algorith"
D08-1017,D07-1003,1,0.392434,"Missing"
D08-1017,W03-3023,0,0.499777,"Missing"
D09-1023,J00-1004,0,0.0318613,"at each target word aligned to at most one source word, which we also do here.6 Which translations are possible depends heavily on the configurations that the QDG permits. Formally, for a parent-child pair htτt (j) , tj i in τt , we consider the relationship between a(τt (j)) and a(j), the source-side words to which tτt (j) and tj align. If, for example, we require that, for all j, a(τt (j)) = τs (a(j)) or a(j) = 0, and that the root of τt must align to the root of τs or to NULL, then strict isomorphism must hold between τs and τt , and we have implemented a synchronous CF dependency grammar (Alshawi et al., 2000; Ding and Palmer, 2005). Smith and Eisner (2006) grouped all possible configurations into eight classes and explored the effects of permitting different sets of classes in word alignment. (“a(τt (j)) = τs (a(j))” corresponds to their “parent-child” configuration; see Fig. 3 in Smith and Eisner (2006) for illustrations of the rest.) More generally, we can define features on tree pairs that factor into these local configurations, as shown in Eq. 7 (Tab. 2). Note that the QDG instantiates the model in Eq. 2. Of the features discussed in §2, f lex , f att , f val , and f dist can be easily incorp"
D09-1023,W05-0909,0,0.0233161,"decoding method in §4, except for each state j, 224 the sausage lattice only includes arcs from j − 1 to j that are labeled with the known target word tj . If a is the number of arcs in the lattice, which is O(mn), this algorithm runs in O(a3 ) time and requires O(a2 ) space. Because we use a hard upper bound on |Trans(s) |for all s ∈ Σ, this summation is much faster in practice than the one over words and alignments. ment set of 934 sentences, and a test set of 500 sentences. We evaluate translation output using case-insensitive BLEU (Papineni et al., 2001), as provided by NIST, and METEOR (Banerjee and Lavie, 2005), version 0.6, with Porter stemming and WordNet synonym matching. 5.3 Our base system uses features as discussed in §2. To obtain lexical translation features g trans (s, a, t), we use the Moses pipeline (Koehn et al., 2007). We perform word alignment using GIZA++ (Och and Ney, 2003), symmetrize the alignments using the “grow-diag-final-and” heuristic, and extract phrases up to length 3. We define f lex by the lexical probabilities p(t |s) and p(s |t) estimated from the symmetrized alignments. After discarding phrase pairs with only one target-side word (since we only allow a target word to al"
D09-1023,D08-1023,0,0.026862,"for clarity only the first five are shown. The output of the decoder consists of lattice arcs Decoding Given a sentence s and its parse τs , at decoding time we seek the target sentence t∗ , the target tree τt∗ , and the alignments a∗ that are most probable, as defined in Eq. 1.7 (In §5 we will consider kbest and all-translations variations on this prob6 I.e., from here on, a : {1, . . . , m} → {0, . . . , n} where 0 denotes alignment to NULL. 7 Arguably, we seek argmaxt p(t |s), marginalizing out everything else. Approximate solutions have been proposed for that problem in several settings (Blunsom and Osborne, 2008; Sun and Tsujii, 2009); we leave their combination with our approach to future work. 222 Source: $ konnten sie es übersetzen ? Reference: could you translate it ? Decoder output: $ konnten:could konnten:could es:it ?:? ?:? übersetzen: translate übersetzen: translate sie:you sie:you konnten:could konnten:couldn konnten:might es:it sie:you übersetzen: translated übersetzen: translated sie:let ?:? es:it es:it es:it sie:them konnten:could NULL:to ... ... übersetzen: translate ... ... ... Figure 1: Decoding as lattice parsing, with the highest-scoring translation denoted by black lattice arcs (oth"
D09-1023,P08-1024,0,0.183612,"features and permit hidden variables during training, we use a single globally-normalized conditional log-linear model. That is, p(t, τt , a |s, τs ) = exp{θ > g(s, τs , a, t, τt )} > 0 0 0 a0 ,t0 ,τ 0 exp{θ g(s, τs , a , t , τt )} P Lexical Translations (2) t 4 There are two conventional definitions of feature functions. One is to let the range of these functions be conditional probability estimates (Och and Ney, 2002). These estimates are usually heuristic and inconsistent (Koehn et al., 2003). An alternative is to instantiate features for different structural patterns (Liang et al., 2006; Blunsom et al., 2008). This offers more expressive power but may require much more training data to avoid overfitting. For this reason, and to keep training fast, we opt for the former convention, though our decoder can handle both, and the factorings we describe are agnostic about this choice. where the g are arbitrary feature functions and the θ are feature weights. If one or both parse trees or the word alignments are unavailable, they can be ignored or marginalized out as hidden variables. In a log-linear model over structured objects, the choice of feature functions g has a huge effect 3 We assume in this wor"
D09-1023,J93-2003,0,0.0122443,"ing the source sentence with the synchronous grammar, ensuring that every phrase and word has an analogue in τt (or a deliberate choice is made by the decoder to translate it to NULL). In such systems, we do not need to use features to implement source-side coverage, as it is assumed as a hard constraint always respected by the decoder. Our QDG decoder has no way to enforce coverage; it does not track any kind of state in τs apart from a single recently aligned word. This is a problem with other direct translation models, such as IBM model 1 used as a direct model rather than a channel model (Brown et al., 1993). This sacrifice is the result of our choice to use a conditional model (§2). The solution is to introduce a set of coverage features g cov (a). Here, these include: words: Of these, only f scov is local. 4.3 Non-Local Features The lattice QDG parsing decoder incorporates many of the features we have discussed, but not all of them. Phrase lexicon features f phr , language model features f N for N > 1, and most coverage features are non-local with respect to our QDG. Recently Chiang (2007) introduced “cube pruning” as an approximate decoding method that extends a DP decoder with the ability to"
D09-1023,D08-1024,0,0.0502878,"ntly maximum likelihood estimates of conditional probabilities in a probabilistic grammar, but other syntactic features are possible. For example, Quirk et al. (2005) use features involving phrases and sourceside dependency trees and Mi et al. (2008) use features from a forest of parses of the source sentence. There is also substantial work in the use of target-side syntax (Galley et al., 2006; Marcu et al., 2006; Shen et al., 2008). In addition, researchers have recently added syntactic features to phrase-based and hierarchical phrase-based models (Gimpel and Smith, 2008; Haque et al., 2009; Chiang et al., 2008). In this work, we focus on syntactic features of target-side dependency trees, τt , along with the words t. These include attachment features that relate a word to its syntactic parent, and valence features. They factor as in Eq. 5 (Tab. 2). Features that consider only target-side syntax and words without considering s can be seen as “syntactic language model” features (Shen et al., 2008). 3 Quasi-Synchronous Grammars A quasi-synchronous dependency grammar (QDG; Smith and Eisner, 2006) specifies a conditional model p(t, τt , a |s, τs ). Given a source sentence s and its parse τs , a QDG induc"
D09-1023,P05-1033,0,0.122116,"s are unavailable, they can be ignored or marginalized out as hidden variables. In a log-linear model over structured objects, the choice of feature functions g has a huge effect 3 We assume in this work that s is parsed. In principle, we might include source-side parsing as part of decoding. 220 (Koehn et al., 2003); they can overlap.5 Additionally, since phrase features can be any function of words and alignments, we permit features that consider phrase pairs in which a target word outside the target phrase aligns to a source word inside the source phrase, as well as phrase pairs with gaps (Chiang, 2005; Ittycheriah and Roukos, 2007). Lexical translation features factor as in Eq. 3 (Tab. 2). We score all phrase pairs in a sentence pair that pair a target phrase with the smallest source phrase that contains all of the alignments in S the target phrase; if k:i≤k≤j a(k) = ∅, no phrase feature fires for tji . 2.2 g trans (s, a, t) Pm P j=1 P i∈a(j) f lex (si , tj ) last(i,j) g lm (t) = g syn (t, τt ) = g reor (s, τs , a, t, τt ) = +f val (tj , j, τt−1 (j)) Pm P j=1 i∈a(j) f dist (i, j) g tree 2 (τs , a, τt ) = m X i,j:1≤i<j≤m 2.4 Reordering Reordering features take many forms in MT. In phrase-ba"
D09-1023,J07-2003,0,0.820051,"emoved, allowing simulation of “string-to-tree,” “tree-to-string,” “treeto-tree,” and “phrase-based” models, among many others. We follow the widespread use of log-linear modeling for direct translation modeling; the novelty is in the use of richer feature sets than have been previously used in a single model. Decoding as QG parsing (§3–4): We present a novel decoder based on lattice parsing with quasisynchronous grammar (QG; Smith and Eisner, 2006).2 Further, we exploit generic approximate inference techniques to incorporate arbitrary “nonlocal” features in the dynamic programming algorithm (Chiang, 2007; Gimpel and Smith, 2009). Parameter estimation (§5): We exploit similar approximate inference methods in regularized pseudolikelihood estimation (Besag, 1975) with hidden variables to discriminatively and efficiently train our model. Because we start with inference (the key subroutine in training), many other learning algorithms are possible. Experimental platform (§6): The flexibility of our model/decoder permits carefully controlled experiments. We compare lexical phrase and dependency syntax features, as well as a novel comWe present a machine translation framework that can incorporate arb"
D09-1023,P09-1053,1,0.81996,"ndependence assumptions, which are in turn exploited by efficient inference algorithms, including decoders (Koehn et al., 2003; Yamada and Knight, 2001). Hence a tension is visible in the many recent research efforts aiming to decode with “non-local” features (Chiang, 2007; Huang and Chiang, 2007). Lopez (2009) recently argued for a separation between features/formalisms (and the indepen2 To date, QG has been used for word alignment (Smith and Eisner, 2006), adaptation and projection in parsing (Smith and Eisner, 2009), and various monolingual recognition and scoring tasks (Wang et al., 2007; Das and Smith, 2009); this paper represents its first application to MT. 1 Informally, features are “parts” of a parallel sentence pair and/or their mutual derivation structure (trees, alignments, etc.). Features are often implied by a choice of formalism. 219 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 219–228, c Singapore, 6-7 August 2009. 2009 ACL and AFNLP Σ, T source and target language vocabularies, respectively Trans : Σ ∪ {NULL} → 2T function mapping each source word to target words to which it may translate s = hs0 , . . . , sn i ∈ Σn source language sent"
D09-1023,P05-1067,0,0.0328551,"igned to at most one source word, which we also do here.6 Which translations are possible depends heavily on the configurations that the QDG permits. Formally, for a parent-child pair htτt (j) , tj i in τt , we consider the relationship between a(τt (j)) and a(j), the source-side words to which tτt (j) and tj align. If, for example, we require that, for all j, a(τt (j)) = τs (a(j)) or a(j) = 0, and that the root of τt must align to the root of τs or to NULL, then strict isomorphism must hold between τs and τt , and we have implemented a synchronous CF dependency grammar (Alshawi et al., 2000; Ding and Palmer, 2005). Smith and Eisner (2006) grouped all possible configurations into eight classes and explored the effects of permitting different sets of classes in word alignment. (“a(τt (j)) = τs (a(j))” corresponds to their “parent-child” configuration; see Fig. 3 in Smith and Eisner (2006) for illustrations of the rest.) More generally, we can define features on tree pairs that factor into these local configurations, as shown in Eq. 7 (Tab. 2). Note that the QDG instantiates the model in Eq. 2. Of the features discussed in §2, f lex , f att , f val , and f dist can be easily incorporated into the QDG as d"
D09-1023,H05-1036,1,0.796739,"making the following approximation (Be10 Alignments could be supplied by automatic word alignment algorithms. We chose to leave them hidden so that we could make the best use of our parsed training data when configuration constraints are imposed, since it is not always possible to reconcile automatic word alignments with automatic parses. 11 When the function’s value is computed by “inside” DP, the corresponding “outside” algorithm can be used to obtain the gradient. Because outside algorithms can be automatically derived from inside ones, we discuss only inside algorithms in this paper; see Eisner et al. (2005). 5.2 Summing over τt and a For the summation over dependency trees and alignments given fixed t, required for p(τt | t, s, τs ), we perform “inside” lattice parsing with Gs,τs . The technique is the summing variant of the decoding method in §4, except for each state j, 224 the sausage lattice only includes arcs from j − 1 to j that are labeled with the known target word tj . If a is the number of arcs in the lattice, which is O(mn), this algorithm runs in O(a3 ) time and requires O(a2 ) space. Because we use a hard upper bound on |Trans(s) |for all s ∈ Σ, this summation is much faster in prac"
D09-1023,1997.iwpt-1.10,0,0.088984,"h. Let the states be numbered 0 to `; states from bρ`c to ` are final states (for some ρ ∈ (0, 1)). For every position between consecutive states j − 1 and j (0 < j ≤ `), and for every word si in s, and for every word t ∈ Trans(si ), we instantiate an arc annotated with t and i. The weight of such an arc is exp{θ > f }, where f is the sum of feature functions that fire when si translates as t in target position j (e.g., f lex (si , t) and f dist (i, j)). Given the lattice and Gs,τs , lattice parsing is a straightforward generalization of standard context-free dependency parsing DP algorithms (Eisner, 1997). This decoder accounts for f lex , f att , f val , f dist , and f qg as local features. Figure 1 gives an example, showing a German sentence and dependency tree from an automatic parser, an English reference, and a lattice representing possible translations. In each bundle, the arcs are listed in decreasing order according to weight and for clarity only the first five are shown. The output of the decoder consists of lattice arcs Decoding Given a sentence s and its parse τs , at decoding time we seek the target sentence t∗ , the target tree τt∗ , and the alignments a∗ that are most probable, a"
D09-1023,P06-1121,0,0.0602962,"two additional sets of features. Target Syntax There have been many features proposed that consider source- and target-language syntax during translation. Syntax-based MT systems often use features on grammar rules, frequently maximum likelihood estimates of conditional probabilities in a probabilistic grammar, but other syntactic features are possible. For example, Quirk et al. (2005) use features involving phrases and sourceside dependency trees and Mi et al. (2008) use features from a forest of parses of the source sentence. There is also substantial work in the use of target-side syntax (Galley et al., 2006; Marcu et al., 2006; Shen et al., 2008). In addition, researchers have recently added syntactic features to phrase-based and hierarchical phrase-based models (Gimpel and Smith, 2008; Haque et al., 2009; Chiang et al., 2008). In this work, we focus on syntactic features of target-side dependency trees, τt , along with the words t. These include attachment features that relate a word to its syntactic parent, and valence features. They factor as in Eq. 5 (Tab. 2). Features that consider only target-side syntax and words without considering s can be seen as “syntactic language model” features (Sh"
D09-1023,W08-0302,1,0.868317,"often use features on grammar rules, frequently maximum likelihood estimates of conditional probabilities in a probabilistic grammar, but other syntactic features are possible. For example, Quirk et al. (2005) use features involving phrases and sourceside dependency trees and Mi et al. (2008) use features from a forest of parses of the source sentence. There is also substantial work in the use of target-side syntax (Galley et al., 2006; Marcu et al., 2006; Shen et al., 2008). In addition, researchers have recently added syntactic features to phrase-based and hierarchical phrase-based models (Gimpel and Smith, 2008; Haque et al., 2009; Chiang et al., 2008). In this work, we focus on syntactic features of target-side dependency trees, τt , along with the words t. These include attachment features that relate a word to its syntactic parent, and valence features. They factor as in Eq. 5 (Tab. 2). Features that consider only target-side syntax and words without considering s can be seen as “syntactic language model” features (Shen et al., 2008). 3 Quasi-Synchronous Grammars A quasi-synchronous dependency grammar (QDG; Smith and Eisner, 2006) specifies a conditional model p(t, τt , a |s, τs ). Given a source"
D09-1023,E09-1037,1,0.929278,"ng simulation of “string-to-tree,” “tree-to-string,” “treeto-tree,” and “phrase-based” models, among many others. We follow the widespread use of log-linear modeling for direct translation modeling; the novelty is in the use of richer feature sets than have been previously used in a single model. Decoding as QG parsing (§3–4): We present a novel decoder based on lattice parsing with quasisynchronous grammar (QG; Smith and Eisner, 2006).2 Further, we exploit generic approximate inference techniques to incorporate arbitrary “nonlocal” features in the dynamic programming algorithm (Chiang, 2007; Gimpel and Smith, 2009). Parameter estimation (§5): We exploit similar approximate inference methods in regularized pseudolikelihood estimation (Besag, 1975) with hidden variables to discriminatively and efficiently train our model. Because we start with inference (the key subroutine in training), many other learning algorithms are possible. Experimental platform (§6): The flexibility of our model/decoder permits carefully controlled experiments. We compare lexical phrase and dependency syntax features, as well as a novel comWe present a machine translation framework that can incorporate arbitrary features of both i"
D09-1023,2009.eamt-1.32,0,0.0300839,"rammar rules, frequently maximum likelihood estimates of conditional probabilities in a probabilistic grammar, but other syntactic features are possible. For example, Quirk et al. (2005) use features involving phrases and sourceside dependency trees and Mi et al. (2008) use features from a forest of parses of the source sentence. There is also substantial work in the use of target-side syntax (Galley et al., 2006; Marcu et al., 2006; Shen et al., 2008). In addition, researchers have recently added syntactic features to phrase-based and hierarchical phrase-based models (Gimpel and Smith, 2008; Haque et al., 2009; Chiang et al., 2008). In this work, we focus on syntactic features of target-side dependency trees, τt , along with the words t. These include attachment features that relate a word to its syntactic parent, and valence features. They factor as in Eq. 5 (Tab. 2). Features that consider only target-side syntax and words without considering s can be seen as “syntactic language model” features (Shen et al., 2008). 3 Quasi-Synchronous Grammars A quasi-synchronous dependency grammar (QDG; Smith and Eisner, 2006) specifies a conditional model p(t, τt , a |s, τs ). Given a source sentence s and its"
D09-1023,P07-1019,0,0.0272789,"arious restrictions on nonisomorphism. 1 Introduction We have seen rapid recent progress in machine translation through the use of rich features and the development of improved decoding algorithms, often based on grammatical formalisms.1 If we view MT as a machine learning problem, features and formalisms imply structural independence assumptions, which are in turn exploited by efficient inference algorithms, including decoders (Koehn et al., 2003; Yamada and Knight, 2001). Hence a tension is visible in the many recent research efforts aiming to decode with “non-local” features (Chiang, 2007; Huang and Chiang, 2007). Lopez (2009) recently argued for a separation between features/formalisms (and the indepen2 To date, QG has been used for word alignment (Smith and Eisner, 2006), adaptation and projection in parsing (Smith and Eisner, 2009), and various monolingual recognition and scoring tasks (Wang et al., 2007; Das and Smith, 2009); this paper represents its first application to MT. 1 Informally, features are “parts” of a parallel sentence pair and/or their mutual derivation structure (trees, alignments, etc.). Features are often implied by a choice of formalism. 219 Proceedings of the 2009 Conference on"
D09-1023,N07-1008,0,0.0296098,"ble, they can be ignored or marginalized out as hidden variables. In a log-linear model over structured objects, the choice of feature functions g has a huge effect 3 We assume in this work that s is parsed. In principle, we might include source-side parsing as part of decoding. 220 (Koehn et al., 2003); they can overlap.5 Additionally, since phrase features can be any function of words and alignments, we permit features that consider phrase pairs in which a target word outside the target phrase aligns to a source word inside the source phrase, as well as phrase pairs with gaps (Chiang, 2005; Ittycheriah and Roukos, 2007). Lexical translation features factor as in Eq. 3 (Tab. 2). We score all phrase pairs in a sentence pair that pair a target phrase with the smallest source phrase that contains all of the alignments in S the target phrase; if k:i≤k≤j a(k) = ∅, no phrase feature fires for tji . 2.2 g trans (s, a, t) Pm P j=1 P i∈a(j) f lex (si , tj ) last(i,j) g lm (t) = g syn (t, τt ) = g reor (s, τs , a, t, τt ) = +f val (tj , j, τt−1 (j)) Pm P j=1 i∈a(j) f dist (i, j) g tree 2 (τs , a, τt ) = m X i,j:1≤i<j≤m 2.4 Reordering Reordering features take many forms in MT. In phrase-based systems, reordering is acco"
D09-1023,P04-1061,0,0.0334321,"carding phrase pairs with only one target-side word (since we only allow a target word to align to at most one source word), we define f phr by 8 features: {2, 3} target words × phrase conditional and “lexical smoothing” probabilities × two conditional directions. Bigram and trigam language model features, f 2 and f 3 , are estimated using the SRI toolkit (Stolcke, 2002) with modified Kneser-Ney smoothing (Chen and Goodman, 1998). For our target-language syntactic features g syn , we use features similar to lexicalized CFG events (Collins, 1999), specifically following the dependency model of Klein and Manning (2004). These include probabilities associated with individual attachments (f att ) and child-generation valence probabilities (f val ). These probabilities are estimated on the training corpus parsed using the Stanford factored parser (Klein and Manning, 2003). The same probabilities are also included using 50 hard word classes derived from the parallel corpus using the GIZA++ mkcls utility (Och and Ney, 2003). In total, there are 7 lexical and 7 word-class syntax features. For reordering, we use a single absolute distortion feature f dist (i, j) that returns |i−j |whenever a(j) = i and i, j > 0. ("
D09-1023,N03-1017,0,0.464412,"decoder to conduct controlled experiments on a German-to-English translation task, to compare lexical phrase, syntax, and combined models, and to measure effects of various restrictions on nonisomorphism. 1 Introduction We have seen rapid recent progress in machine translation through the use of rich features and the development of improved decoding algorithms, often based on grammatical formalisms.1 If we view MT as a machine learning problem, features and formalisms imply structural independence assumptions, which are in turn exploited by efficient inference algorithms, including decoders (Koehn et al., 2003; Yamada and Knight, 2001). Hence a tension is visible in the many recent research efforts aiming to decode with “non-local” features (Chiang, 2007; Huang and Chiang, 2007). Lopez (2009) recently argued for a separation between features/formalisms (and the indepen2 To date, QG has been used for word alignment (Smith and Eisner, 2006), adaptation and projection in parsing (Smith and Eisner, 2009), and various monolingual recognition and scoring tasks (Wang et al., 2007; Das and Smith, 2009); this paper represents its first application to MT. 1 Informally, features are “parts” of a parallel sent"
D09-1023,P07-2045,0,0.0519099,"e that contains all of the alignments in S the target phrase; if k:i≤k≤j a(k) = ∅, no phrase feature fires for tji . 2.2 g trans (s, a, t) Pm P j=1 P i∈a(j) f lex (si , tj ) last(i,j) g lm (t) = g syn (t, τt ) = g reor (s, τs , a, t, τt ) = +f val (tj , j, τt−1 (j)) Pm P j=1 i∈a(j) f dist (i, j) g tree 2 (τs , a, τt ) = m X i,j:1≤i<j≤m 2.4 Reordering Reordering features take many forms in MT. In phrase-based systems, reordering is accomplished both within phrase pairs (local reordering) as well as through distance-based distortion models (Koehn et al., 2003) and lexicalized reordering models (Koehn et al., 2007). In syntax-based systems, reordering is typically parameterized by grammar rules. For generality we permit these features to “see” all structures and denote them g reor (s, τs , a, t, τt ). Eq. 6 (Tab. 2) shows a factoring of reordering features based on absolute positions of aligned words. We turn next to the “backbone” model for our decoder; the formalism and the properties of its decoding algorithm will inspire two additional sets of features. Target Syntax There have been many features proposed that consider source- and target-language syntax during translation. Syntax-based MT systems of"
D09-1023,H05-1064,0,0.0228546,"0 0 k∈τt−1 (j) i =0 t ∈Trans(si0 ) >` a ´o f lex (si , t0 ) + f att ($, 0, t0 , k) + f qg (0, i, 0, k) (11) «ff  „ f lex (si0 , t0 ) + f att (t, j, t0 , k)+ (12) S(k, i0 , t0 ) × exp θ > −1 0 f val (t, j, τt (j)) + f qg (i, i , j, k) n ` ´o if τt−1 (j) = ∅ (13) S(j, i, t) = exp θ > f val (t, j, τt−1 (j)) Table 3: Eq. 9: Log-likelihood. Eq. 10: Pseudolikelihood. In both cases we maximize w.r.t. θ. Eqs. 11–13: Recursive DP equations for summing over t and a. alignments are treated as a hidden variable to be marginalized out.10 Optimization problems of this form are by now widely known in NLP (Koo and Collins, 2005), and have recently been used for machine translation as well (Blunsom et al., 2008). Such problems are typically solved using variations of gradient ascent; in our experiments, we will use an online method called stochastic gradient ascent (SGA). This requires us to calculate the function’s gradient (vector of first derivatives) with respect to θ.11 sag, 1975): p(t, τt |s, τs ) ≈ p(t |τt , s, τs ) × p(τt |t, s, τs ) Plugging this into Eq. 9, we arrive at Eq. 10 (Tab. 3). The two parenthesized terms in Eq. 10 each have their own numerators and denominators (not shown). The numerators are ident"
D09-1023,P06-1096,0,0.0946841,"Missing"
D09-1023,E09-1061,0,0.0181354,"nisomorphism. 1 Introduction We have seen rapid recent progress in machine translation through the use of rich features and the development of improved decoding algorithms, often based on grammatical formalisms.1 If we view MT as a machine learning problem, features and formalisms imply structural independence assumptions, which are in turn exploited by efficient inference algorithms, including decoders (Koehn et al., 2003; Yamada and Knight, 2001). Hence a tension is visible in the many recent research efforts aiming to decode with “non-local” features (Chiang, 2007; Huang and Chiang, 2007). Lopez (2009) recently argued for a separation between features/formalisms (and the indepen2 To date, QG has been used for word alignment (Smith and Eisner, 2006), adaptation and projection in parsing (Smith and Eisner, 2009), and various monolingual recognition and scoring tasks (Wang et al., 2007; Das and Smith, 2009); this paper represents its first application to MT. 1 Informally, features are “parts” of a parallel sentence pair and/or their mutual derivation structure (trees, alignments, etc.). Features are often implied by a choice of formalism. 219 Proceedings of the 2009 Conference on Empirical Met"
D09-1023,W06-1606,0,0.0184383,"of features. Target Syntax There have been many features proposed that consider source- and target-language syntax during translation. Syntax-based MT systems often use features on grammar rules, frequently maximum likelihood estimates of conditional probabilities in a probabilistic grammar, but other syntactic features are possible. For example, Quirk et al. (2005) use features involving phrases and sourceside dependency trees and Mi et al. (2008) use features from a forest of parses of the source sentence. There is also substantial work in the use of target-side syntax (Galley et al., 2006; Marcu et al., 2006; Shen et al., 2008). In addition, researchers have recently added syntactic features to phrase-based and hierarchical phrase-based models (Gimpel and Smith, 2008; Haque et al., 2009; Chiang et al., 2008). In this work, we focus on syntactic features of target-side dependency trees, τt , along with the words t. These include attachment features that relate a word to its syntactic parent, and valence features. They factor as in Eq. 5 (Tab. 2). Features that consider only target-side syntax and words without considering s can be seen as “syntactic language model” features (Shen et al., 2008). 3"
D09-1023,P08-1023,0,0.031142,"of aligned words. We turn next to the “backbone” model for our decoder; the formalism and the properties of its decoding algorithm will inspire two additional sets of features. Target Syntax There have been many features proposed that consider source- and target-language syntax during translation. Syntax-based MT systems often use features on grammar rules, frequently maximum likelihood estimates of conditional probabilities in a probabilistic grammar, but other syntactic features are possible. For example, Quirk et al. (2005) use features involving phrases and sourceside dependency trees and Mi et al. (2008) use features from a forest of parses of the source sentence. There is also substantial work in the use of target-side syntax (Galley et al., 2006; Marcu et al., 2006; Shen et al., 2008). In addition, researchers have recently added syntactic features to phrase-based and hierarchical phrase-based models (Gimpel and Smith, 2008; Haque et al., 2009; Chiang et al., 2008). In this work, we focus on syntactic features of target-side dependency trees, τt , along with the words t. These include attachment features that relate a word to its syntactic parent, and valence features. They factor as in Eq."
D09-1023,P02-1038,0,0.549002,"hrase features used in this work and those used elsewhere is that we do not assume that phrases segment into disjoint parts of the source and target sentences (1) In order to include overlapping features and permit hidden variables during training, we use a single globally-normalized conditional log-linear model. That is, p(t, τt , a |s, τs ) = exp{θ > g(s, τs , a, t, τt )} > 0 0 0 a0 ,t0 ,τ 0 exp{θ g(s, τs , a , t , τt )} P Lexical Translations (2) t 4 There are two conventional definitions of feature functions. One is to let the range of these functions be conditional probability estimates (Och and Ney, 2002). These estimates are usually heuristic and inconsistent (Koehn et al., 2003). An alternative is to instantiate features for different structural patterns (Liang et al., 2006; Blunsom et al., 2008). This offers more expressive power but may require much more training data to avoid overfitting. For this reason, and to keep training fast, we opt for the former convention, though our decoder can handle both, and the factorings we describe are agnostic about this choice. where the g are arbitrary feature functions and the θ are feature weights. If one or both parse trees or the word alignments are"
D09-1023,J03-1002,0,0.0201692,"e a hard upper bound on |Trans(s) |for all s ∈ Σ, this summation is much faster in practice than the one over words and alignments. ment set of 934 sentences, and a test set of 500 sentences. We evaluate translation output using case-insensitive BLEU (Papineni et al., 2001), as provided by NIST, and METEOR (Banerjee and Lavie, 2005), version 0.6, with Porter stemming and WordNet synonym matching. 5.3 Our base system uses features as discussed in §2. To obtain lexical translation features g trans (s, a, t), we use the Moses pipeline (Koehn et al., 2007). We perform word alignment using GIZA++ (Och and Ney, 2003), symmetrize the alignments using the “grow-diag-final-and” heuristic, and extract phrases up to length 3. We define f lex by the lexical probabilities p(t |s) and p(s |t) estimated from the symmetrized alignments. After discarding phrase pairs with only one target-side word (since we only allow a target word to align to at most one source word), we define f phr by 8 features: {2, 3} target words × phrase conditional and “lexical smoothing” probabilities × two conditional directions. Bigram and trigam language model features, f 2 and f 3 , are estimated using the SRI toolkit (Stolcke, 2002) wi"
D09-1023,P03-1021,0,0.0624178,", 2009). Cube summing is based on a slightly less greedy variation of cube pruning (Chiang, 2007) that maintains k-best lists of derivations for each DP chart item. Cube summing augments the k-best list with a residual term that sums over remaining structures not in the k-best list, albeit without their non-local features. Using the machinery of cube summing, it is straightforward to include the desired non-local features in the summations required for pseudolikelihood, as well as to compute their approximate gradients. Our approach permits an alternative to minimum error-rate training (MERT; Och, 2003); it is discriminative but handles latent structure and regularization in more principled ways. The pseudolikelihood calculations for a sentence pair, taken together, are faster than (k-best) decoding, making SGA’s inner loop faster than MERT’s inner loop. 6 Experiments Our decoding framework allows us to perform many experiments with the same feature representation and inference algorithms, including combining and comparing phrase-based and syntax-based features and examining how isomorphism constraints of synchronous formalisms affect translation output. 6.1 Data and Evaluation We use the Ge"
D09-1023,2001.mtsummit-papers.68,0,0.0214557,"with Gs,τs . The technique is the summing variant of the decoding method in §4, except for each state j, 224 the sausage lattice only includes arcs from j − 1 to j that are labeled with the known target word tj . If a is the number of arcs in the lattice, which is O(mn), this algorithm runs in O(a3 ) time and requires O(a2 ) space. Because we use a hard upper bound on |Trans(s) |for all s ∈ Σ, this summation is much faster in practice than the one over words and alignments. ment set of 934 sentences, and a test set of 500 sentences. We evaluate translation output using case-insensitive BLEU (Papineni et al., 2001), as provided by NIST, and METEOR (Banerjee and Lavie, 2005), version 0.6, with Porter stemming and WordNet synonym matching. 5.3 Our base system uses features as discussed in §2. To obtain lexical translation features g trans (s, a, t), we use the Moses pipeline (Koehn et al., 2007). We perform word alignment using GIZA++ (Och and Ney, 2003), symmetrize the alignments using the “grow-diag-final-and” heuristic, and extract phrases up to length 3. We define f lex by the lexical probabilities p(t |s) and p(s |t) estimated from the symmetrized alignments. After discarding phrase pairs with only o"
D09-1023,P92-1017,0,0.127686,"tures at a time. We must sum over target word sequences and word alignments (with fixed τt ), and separately over target trees and word alignments (with fixed t). Computing the numerator in Eq. 9 involves summing over all possible alignments; with QDG and a hard bound of 1 on |a(j) |for all j, a fast “inside” DP solution is known (Smith and Eisner, 2006; Wang et al., 2007). It runs in O(mn2 ) time and O(mn) space. 5.1 Summing over t and a The summation over target word sequences and alignments given fixed τt bears a resemblance to the inside algorithm, except that the tree structure is fixed (Pereira and Schabes, 1992). Let S(j, i, t) denote the sum of all translations rooted at position j in τt such that a(j) = i and tj = t. Tab. 3 gives the equations for this DP: Eq. 11 is the quantity of interest, Eq. 12 is the recursion, and Eq. 13 shows the base cases for leaves of τt . Letting q = max0≤i≤n |Trans(si )|, this algorithm runs in O(mn2 q 2 ) time and O(mnq) space. For efficiency we place a hard upper bound on q during training (details in §6). Computing the denominator in Eq. 9 requires summing over all word sequences and dependency trees for the target language sentence and all word alignments between th"
D09-1023,P05-1034,0,0.0656359,"). Eq. 6 (Tab. 2) shows a factoring of reordering features based on absolute positions of aligned words. We turn next to the “backbone” model for our decoder; the formalism and the properties of its decoding algorithm will inspire two additional sets of features. Target Syntax There have been many features proposed that consider source- and target-language syntax during translation. Syntax-based MT systems often use features on grammar rules, frequently maximum likelihood estimates of conditional probabilities in a probabilistic grammar, but other syntactic features are possible. For example, Quirk et al. (2005) use features involving phrases and sourceside dependency trees and Mi et al. (2008) use features from a forest of parses of the source sentence. There is also substantial work in the use of target-side syntax (Galley et al., 2006; Marcu et al., 2006; Shen et al., 2008). In addition, researchers have recently added syntactic features to phrase-based and hierarchical phrase-based models (Gimpel and Smith, 2008; Haque et al., 2009; Chiang et al., 2008). In this work, we focus on syntactic features of target-side dependency trees, τt , along with the words t. These include attachment features tha"
D09-1023,P08-1066,0,0.047245,"Syntax There have been many features proposed that consider source- and target-language syntax during translation. Syntax-based MT systems often use features on grammar rules, frequently maximum likelihood estimates of conditional probabilities in a probabilistic grammar, but other syntactic features are possible. For example, Quirk et al. (2005) use features involving phrases and sourceside dependency trees and Mi et al. (2008) use features from a forest of parses of the source sentence. There is also substantial work in the use of target-side syntax (Galley et al., 2006; Marcu et al., 2006; Shen et al., 2008). In addition, researchers have recently added syntactic features to phrase-based and hierarchical phrase-based models (Gimpel and Smith, 2008; Haque et al., 2009; Chiang et al., 2008). In this work, we focus on syntactic features of target-side dependency trees, τt , along with the words t. These include attachment features that relate a word to its syntactic parent, and valence features. They factor as in Eq. 5 (Tab. 2). Features that consider only target-side syntax and words without considering s can be seen as “syntactic language model” features (Shen et al., 2008). 3 Quasi-Synchronous Gr"
D09-1023,W06-3104,0,0.102126,"st popular MT features and can be used to encode any features on source and target sentences, dependency trees, and alignments. The trees are optional and can be easily removed, allowing simulation of “string-to-tree,” “tree-to-string,” “treeto-tree,” and “phrase-based” models, among many others. We follow the widespread use of log-linear modeling for direct translation modeling; the novelty is in the use of richer feature sets than have been previously used in a single model. Decoding as QG parsing (§3–4): We present a novel decoder based on lattice parsing with quasisynchronous grammar (QG; Smith and Eisner, 2006).2 Further, we exploit generic approximate inference techniques to incorporate arbitrary “nonlocal” features in the dynamic programming algorithm (Chiang, 2007; Gimpel and Smith, 2009). Parameter estimation (§5): We exploit similar approximate inference methods in regularized pseudolikelihood estimation (Besag, 1975) with hidden variables to discriminatively and efficiently train our model. Because we start with inference (the key subroutine in training), many other learning algorithms are possible. Experimental platform (§6): The flexibility of our model/decoder permits carefully controlled e"
D09-1023,D09-1086,0,0.0414309,"ormalisms.1 If we view MT as a machine learning problem, features and formalisms imply structural independence assumptions, which are in turn exploited by efficient inference algorithms, including decoders (Koehn et al., 2003; Yamada and Knight, 2001). Hence a tension is visible in the many recent research efforts aiming to decode with “non-local” features (Chiang, 2007; Huang and Chiang, 2007). Lopez (2009) recently argued for a separation between features/formalisms (and the indepen2 To date, QG has been used for word alignment (Smith and Eisner, 2006), adaptation and projection in parsing (Smith and Eisner, 2009), and various monolingual recognition and scoring tasks (Wang et al., 2007; Das and Smith, 2009); this paper represents its first application to MT. 1 Informally, features are “parts” of a parallel sentence pair and/or their mutual derivation structure (trees, alignments, etc.). Features are often implied by a choice of formalism. 219 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 219–228, c Singapore, 6-7 August 2009. 2009 ACL and AFNLP Σ, T source and target language vocabularies, respectively Trans : Σ ∪ {NULL} → 2T function mapping each source"
D09-1023,E09-1088,0,0.0224157,"five are shown. The output of the decoder consists of lattice arcs Decoding Given a sentence s and its parse τs , at decoding time we seek the target sentence t∗ , the target tree τt∗ , and the alignments a∗ that are most probable, as defined in Eq. 1.7 (In §5 we will consider kbest and all-translations variations on this prob6 I.e., from here on, a : {1, . . . , m} → {0, . . . , n} where 0 denotes alignment to NULL. 7 Arguably, we seek argmaxt p(t |s), marginalizing out everything else. Approximate solutions have been proposed for that problem in several settings (Blunsom and Osborne, 2008; Sun and Tsujii, 2009); we leave their combination with our approach to future work. 222 Source: $ konnten sie es übersetzen ? Reference: could you translate it ? Decoder output: $ konnten:could konnten:could es:it ?:? ?:? übersetzen: translate übersetzen: translate sie:you sie:you konnten:could konnten:couldn konnten:might es:it sie:you übersetzen: translated übersetzen: translated sie:let ?:? es:it es:it es:it sie:them konnten:could NULL:to ... ... übersetzen: translate ... ... ... Figure 1: Decoding as lattice parsing, with the highest-scoring translation denoted by black lattice arcs (others are grayed out) and"
D09-1023,D07-1003,1,0.951839,"imply structural independence assumptions, which are in turn exploited by efficient inference algorithms, including decoders (Koehn et al., 2003; Yamada and Knight, 2001). Hence a tension is visible in the many recent research efforts aiming to decode with “non-local” features (Chiang, 2007; Huang and Chiang, 2007). Lopez (2009) recently argued for a separation between features/formalisms (and the indepen2 To date, QG has been used for word alignment (Smith and Eisner, 2006), adaptation and projection in parsing (Smith and Eisner, 2009), and various monolingual recognition and scoring tasks (Wang et al., 2007; Das and Smith, 2009); this paper represents its first application to MT. 1 Informally, features are “parts” of a parallel sentence pair and/or their mutual derivation structure (trees, alignments, etc.). Features are often implied by a choice of formalism. 219 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 219–228, c Singapore, 6-7 August 2009. 2009 ACL and AFNLP Σ, T source and target language vocabularies, respectively Trans : Σ ∪ {NULL} → 2T function mapping each source word to target words to which it may translate s = hs0 , . . . , sn i ∈ Σ"
D09-1023,P01-1067,0,0.18514,"controlled experiments on a German-to-English translation task, to compare lexical phrase, syntax, and combined models, and to measure effects of various restrictions on nonisomorphism. 1 Introduction We have seen rapid recent progress in machine translation through the use of rich features and the development of improved decoding algorithms, often based on grammatical formalisms.1 If we view MT as a machine learning problem, features and formalisms imply structural independence assumptions, which are in turn exploited by efficient inference algorithms, including decoders (Koehn et al., 2003; Yamada and Knight, 2001). Hence a tension is visible in the many recent research efforts aiming to decode with “non-local” features (Chiang, 2007; Huang and Chiang, 2007). Lopez (2009) recently argued for a separation between features/formalisms (and the indepen2 To date, QG has been used for word alignment (Smith and Eisner, 2006), adaptation and projection in parsing (Smith and Eisner, 2009), and various monolingual recognition and scoring tasks (Wang et al., 2007; Das and Smith, 2009); this paper represents its first application to MT. 1 Informally, features are “parts” of a parallel sentence pair and/or their mut"
D09-1023,D08-1022,0,\N,Missing
D09-1023,J03-4003,0,\N,Missing
D09-1023,P02-1040,0,\N,Missing
D09-1023,D08-1076,0,\N,Missing
D10-1004,W06-2920,0,0.0814866,", using only features in F. Since the other features have not been used before, they have a zero weight, hence can be ignored. When β = ∞, the variational problem in Eq. 24 consists of a MAP computation and the soluˆ t ∈ Y(xt ). Only the tion corresponds to one output y ˆ t but not in yt , or vice-versa, parts that are active in y will have features that might receive a nonzero update. Those parts are reexamined for new features and the active set F is updated accordingly. 6 Experiments We trained non-projective dependency parsers for 14 languages, using datasets from the CoNLL-X shared task (Buchholz and Marsi, 2006) and two datasets for English: one from the CoNLL-2008 shared task (Surdeanu et al., 2008), which contains non-projective arcs, and another derived from the Penn Treebank applying the standard head rules of Yamada and Matsumoto (2003), in which all parse trees are projective.12 We implemented Alg. 1, 12 We used the provided train/test splits for all datasets. For English, we used the standard test partitions (section 23 of the Wall Street Journal). We did not exploit the fact that some datasets only contain projective trees and have unique roots. 42 which handles any loss function Lβ,γ .13 Whe"
D10-1004,P08-1109,0,0.0540744,"light on the optimization problems tackled in each method. We also propose a new aggressive online algorithm to learn the model parameters, which makes use of the underlying variational representation. The algorithm does not require a learning rate parameter and provides a single framework for a wide family of convex loss functions, including CRFs and structured SVMs. Experiments show state-of-the-art performance for 14 languages. 1 Introduction Feature-rich discriminative models that break locality/independence assumptions can boost a parser’s performance (McDonald et al., 2006; Huang, 2008; Finkel et al., 2008; Smith and Eisner, 2008; Martins et al., 2009; Koo and Collins, 2010). Often, inference with such models becomes computationally intractable, causing a demand for understanding and improving approximate parsing algorithms. In this paper, we show a formal connection between two recently-proposed approximate inference techniques for non-projective dependency parsing: loopy belief propagation (Smith and Eisner, 2008) and linear programming relaxation (Martins et al., 2009). While those two parsers are differently motivated, we show that both correspond to inference in a factor graph, and both op"
D10-1004,N10-1112,1,0.181926,"constant, and L(θ; x, y) is a nonnegative convex loss. Examples include the logistic loss used in CRFs (− log Prθ (y|x)) and the hinge loss of structured SVMs (maxy0 ∈Y(x) θ &gt; (φ(x, y0 )− • O(n2 ) XOR - WITH - OUTPUT factors to impose the φ(x, y)) + `(y0 , y) for some cost function `). These constraint that words don’t consume other words’ are both special cases of the family defined in Fig. 4, commodities; i.e., if h 6= k and k 6= 0, then there which also includes the structured perceptron’s loss is a path from h to k iff exactly one outgoing arc (β → ∞, γ = 0) and the softmax-margin loss of Gimpel and Smith (2010; β = γ = 1). in {hh, mi}m=1,...,n carries flow to k: Pn Alg. 1 is closely related to stochastic or online k , h, k ∈ {0, . . . , n}, k ∈ / {0, h}. phk = m=1 fhh,mi (19) gradient descent methods, but with the key advantage of not needing a learning rate hyperparameter. L(G0x ) is thus defined by the constraints in Eq. 12 We sketch the derivation of Alg. 1; full details can and 15–19. The approximate MAP problem, that be found in Martins et al. (2010a). On the tth round, replaces M(G0x ) by L(G0x ) in Eq. 10, thus becomes: one example hxt , yt i is considered. We seek to solve maxz,f ,p θ &gt; F(x"
D10-1004,P08-1067,0,0.058961,"aph, we shed light on the optimization problems tackled in each method. We also propose a new aggressive online algorithm to learn the model parameters, which makes use of the underlying variational representation. The algorithm does not require a learning rate parameter and provides a single framework for a wide family of convex loss functions, including CRFs and structured SVMs. Experiments show state-of-the-art performance for 14 languages. 1 Introduction Feature-rich discriminative models that break locality/independence assumptions can boost a parser’s performance (McDonald et al., 2006; Huang, 2008; Finkel et al., 2008; Smith and Eisner, 2008; Martins et al., 2009; Koo and Collins, 2010). Often, inference with such models becomes computationally intractable, causing a demand for understanding and improving approximate parsing algorithms. In this paper, we show a formal connection between two recently-proposed approximate inference techniques for non-projective dependency parsing: loopy belief propagation (Smith and Eisner, 2008) and linear programming relaxation (Martins et al., 2009). While those two parsers are differently motivated, we show that both correspond to inference in a fact"
D10-1004,P98-1106,0,0.0963536,"qs. 12 and 15–19 are satisfied. This is exactly the LP relaxation considered by Martins et al. (2009) in their multi-commodity flow model, for the configuration with siblings and grandparent features.7 They also considered a configuration with non-projectivity features—which fire if an arc is non-projective.8 That configuration can also be obtained here if variables {nhh,mi } are 7 To be precise, the constraints of Martins et al. (2009) are recovered after eliminating the path variables, via Eqs. 18–19. 8 An arc hh, mi is non-projective if there is some word in its span not descending from h (Kahane et al., 1998). 40 2 minθ,ξ λm 2 kθ − θ t k + ξ s.t. L(θ; xt , yt ) ≤ ξ, ξ ≥ 0, 9 (23) Given what was just exposed, it seems appealing to try max-product loopy BP on the factor graph of Fig. 1, or sumproduct loopy BP on the one in Fig. 3. Both attempts present serious challenges: the former requires computing messages sent by the tree factor, which requires O(n2 ) calls to the Chu-LiuEdmonds algorithm and hence O(n5 ) time. No obvious strategy seems to exist for simultaneous computation of all messages, unlike in the sum-product case. The latter is even more challenging, as standard sum-product loopy BP has"
D10-1004,P10-1001,0,0.382192,"propose a new aggressive online algorithm to learn the model parameters, which makes use of the underlying variational representation. The algorithm does not require a learning rate parameter and provides a single framework for a wide family of convex loss functions, including CRFs and structured SVMs. Experiments show state-of-the-art performance for 14 languages. 1 Introduction Feature-rich discriminative models that break locality/independence assumptions can boost a parser’s performance (McDonald et al., 2006; Huang, 2008; Finkel et al., 2008; Smith and Eisner, 2008; Martins et al., 2009; Koo and Collins, 2010). Often, inference with such models becomes computationally intractable, causing a demand for understanding and improving approximate parsing algorithms. In this paper, we show a formal connection between two recently-proposed approximate inference techniques for non-projective dependency parsing: loopy belief propagation (Smith and Eisner, 2008) and linear programming relaxation (Martins et al., 2009). While those two parsers are differently motivated, we show that both correspond to inference in a factor graph, and both optimize objective functions over local approximations of the marginal p"
D10-1004,D07-1015,0,0.0201707,"anning tree. There is a unary soft factor per arc, whose log-potential reflects the score of that arc. There are also O(n3 ) pairwise factors; their log-potentials reflect the scores of sibling and grandparent arcs. These factors create loops, thus calling for approximate inference. Without them, the model is arc-factored, and exact inference in it is well studied: finding the most probable parse tree takes O(n3 ) time with the ChuLiu-Edmonds algorithm (McDonald et al., 2005),2 and computing posterior marginals for all arcs takes O(n3 ) time via the matrix-tree theorem (Smith and Smith, 2007; Koo et al., 2007). Message-passing algorithms. In general factor graphs, both inference problems— obtaining the most probable output (the MAP) argmaxy∈Y(x) Prθ (y|x), and computing the marginals Prθ (Yi = yi |x)—can be addressed with the belief propagation (BP) algorithm (Pearl, 1988), which iteratively passes messages between variables and factors reflecting their local “beliefs.” There is a faster but more involved O(n2 ) algorithm due to Tarjan (1977). 2 v1 , . . . , vn ∈ SC where SC ⊆ {0, 1}n . otherwise, P Q vi • Message-induced distribution: ω , hmj→C ij=1,...,n • Partition function: ZC (ω) , hv1 ,...,vn"
D10-1004,D10-1125,0,0.168528,"ences (Smith and Eisner, 2008), as well as high order factors with countbased potentials (Tarlow et al., 2010), among others. Some of our combinatorial factors (OR, OR - WITH OUTPUT ) and the analogous entropy computations were never considered, to the best of our knowledge. Prop. 1 appears in Wainwright and Jordan (2008) for canonical overcomplete models; we adapt it here for models with shared features. We rely on the variational interpretation of loopy BP, due to Yedidia et al. (2001), to derive the objective being optimized by Smith and Eisner’s loopy BP parser. Independently of our work, Koo et al. (2010) recently proposed an efficient dual decomposition method to solve an LP problem similar (but not equal) to the one in Eq. 20,15 with excellent parsing performance. Their parser is also an instance of a turbo parser since it relies on a local approximation of a marginal polytope. While one can also use dual decomposition to address our MAP problem, the fact that our model does not decompose as nicely as the one in Koo et al. (2010) would likely result in slower convergence. 8 Conclusion We presented a unified view of two recent approximate dependency parsers, by stating their underlying factor"
D10-1004,D08-1017,1,0.731315,"Missing"
D10-1004,P09-1039,1,0.118924,"rtins∗† Noah A. Smith∗ Eric P. Xing∗ ∗ School of Computer Science Carnegie Mellon University Pittsburgh, PA 15213, USA {afm,nasmith,epxing}@cs.cmu.edu M´ario A. T. Figueiredo† † Instituto de Telecomunicac¸o˜ es Instituto Superior T´ecnico Lisboa, Portugal mtf@lx.it.pt Pedro M. Q. Aguiar‡ ‡ Instituto de Sistemas e Rob´otica Instituto Superior T´ecnico Lisboa, Portugal aguiar@isr.ist.utl.pt Abstract We present a unified view of two state-of-theart non-projective dependency parsers, both approximate: the loopy belief propagation parser of Smith and Eisner (2008) and the relaxed linear program of Martins et al. (2009). By representing the model assumptions with a factor graph, we shed light on the optimization problems tackled in each method. We also propose a new aggressive online algorithm to learn the model parameters, which makes use of the underlying variational representation. The algorithm does not require a learning rate parameter and provides a single framework for a wide family of convex loss functions, including CRFs and structured SVMs. Experiments show state-of-the-art performance for 14 languages. 1 Introduction Feature-rich discriminative models that break locality/independence assumptions c"
D10-1004,D10-1004,1,0.135472,"1: Factor graph corresponding to the dependency parsing model of Smith and Eisner (2008) with sibling and grandparent features. Circles denote variable nodes, and squares denote factor nodes. Note the loops created by the inclusion of pairwise factors (GRAND and SIB). In Table 1 we present closed-form expressions for the factor-to-variable message ratios mC→i , MC→i (1)/MC→i (0) in terms of their variable-tofactor counterparts mi→C , Mi→C (1)/Mi→C (0); these ratios are all that is necessary when the variables are binary. Detailed derivations are presented in an extended version of this paper (Martins et al., 2010b). 3 Variational Representations Let Px , {Prθ (.|x) |θ ∈ Rd } be the family of all distributions of the form in Eq. 2. We next present an alternative parametrization for the distributions in Px in terms of factor marginals. We will see that each distribution can be seen as a point in the socalled marginal polytope (Wainwright and Jordan, 2008); this will pave the way for the variational representations to be derived next. Parts and Output Indicators. A part is a pair hC, yC i, where C is a soft factor and yC a partial output assignment. We let R = {hC, yC i |C ∈ Q Csoft , yC ∈ i∈C Yi } be th"
D10-1004,H05-1066,0,0.804287,"gs to the dependency tree. There is a hard factor TREE connected to all variables, that constrains the overall arc configurations to form a spanning tree. There is a unary soft factor per arc, whose log-potential reflects the score of that arc. There are also O(n3 ) pairwise factors; their log-potentials reflect the scores of sibling and grandparent arcs. These factors create loops, thus calling for approximate inference. Without them, the model is arc-factored, and exact inference in it is well studied: finding the most probable parse tree takes O(n3 ) time with the ChuLiu-Edmonds algorithm (McDonald et al., 2005),2 and computing posterior marginals for all arcs takes O(n3 ) time via the matrix-tree theorem (Smith and Smith, 2007; Koo et al., 2007). Message-passing algorithms. In general factor graphs, both inference problems— obtaining the most probable output (the MAP) argmaxy∈Y(x) Prθ (y|x), and computing the marginals Prθ (Yi = yi |x)—can be addressed with the belief propagation (BP) algorithm (Pearl, 1988), which iteratively passes messages between variables and factors reflecting their local “beliefs.” There is a faster but more involved O(n2 ) algorithm due to Tarjan (1977). 2 v1 , . . . , vn ∈"
D10-1004,W06-2932,0,0.140018,"ptions with a factor graph, we shed light on the optimization problems tackled in each method. We also propose a new aggressive online algorithm to learn the model parameters, which makes use of the underlying variational representation. The algorithm does not require a learning rate parameter and provides a single framework for a wide family of convex loss functions, including CRFs and structured SVMs. Experiments show state-of-the-art performance for 14 languages. 1 Introduction Feature-rich discriminative models that break locality/independence assumptions can boost a parser’s performance (McDonald et al., 2006; Huang, 2008; Finkel et al., 2008; Smith and Eisner, 2008; Martins et al., 2009; Koo and Collins, 2010). Often, inference with such models becomes computationally intractable, causing a demand for understanding and improving approximate parsing algorithms. In this paper, we show a formal connection between two recently-proposed approximate inference techniques for non-projective dependency parsing: loopy belief propagation (Smith and Eisner, 2008) and linear programming relaxation (Martins et al., 2009). While those two parsers are differently motivated, we show that both correspond to infere"
D10-1004,N03-1028,0,0.0351156,"challenges. Often only “supported” features—those observed in the training data—are included, and even those are commonly eliminated when their frequencies fall below a threshold. Important information may be lost as a result of these expedient choices. S Formally, the supported feature set is Fsupp , m i=1 supp φ(xi , yi ), where supp u , {j |uj 6= 0} denotes the support of vector u. Fsupp is a subset of the complete feature set, comprised of those features output, S occur in some candidate Sm that 0 Fcomp , i=1 y0 ∈Y(xi ) supp φ(xi , yi ). Features i in Fcomp Fsupp are called unsupported. Sha and Pereira (2003) have shown that training a CRF-based shallow parser with the complete feature set may improve performance (over the supported one), at the cost of 4.6 times more features. Dependency parsing has a much higher ratio (around 20 for bilexical word-word features, as estimated in the Penn Treebank), due to the quadratic or faster growth of the number of parts, of which only a few are active in a legal output. We propose a simple strategy for handling Fcomp efficiently, which can be applied for those losses in Fig. 4 where β = ∞. (e.g., the structured SVM and perceptron). Our procedure is the follo"
D10-1004,D08-1016,0,0.063561,"rsing by Approximate Variational Inference Andr´e F. T. Martins∗† Noah A. Smith∗ Eric P. Xing∗ ∗ School of Computer Science Carnegie Mellon University Pittsburgh, PA 15213, USA {afm,nasmith,epxing}@cs.cmu.edu M´ario A. T. Figueiredo† † Instituto de Telecomunicac¸o˜ es Instituto Superior T´ecnico Lisboa, Portugal mtf@lx.it.pt Pedro M. Q. Aguiar‡ ‡ Instituto de Sistemas e Rob´otica Instituto Superior T´ecnico Lisboa, Portugal aguiar@isr.ist.utl.pt Abstract We present a unified view of two state-of-theart non-projective dependency parsers, both approximate: the loopy belief propagation parser of Smith and Eisner (2008) and the relaxed linear program of Martins et al. (2009). By representing the model assumptions with a factor graph, we shed light on the optimization problems tackled in each method. We also propose a new aggressive online algorithm to learn the model parameters, which makes use of the underlying variational representation. The algorithm does not require a learning rate parameter and provides a single framework for a wide family of convex loss functions, including CRFs and structured SVMs. Experiments show state-of-the-art performance for 14 languages. 1 Introduction Feature-rich discriminati"
D10-1004,D07-1014,1,0.820051,"Missing"
D10-1004,W08-2121,0,0.0715542,"Missing"
D10-1004,W03-3023,0,0.164051,"y the tion corresponds to one output y ˆ t but not in yt , or vice-versa, parts that are active in y will have features that might receive a nonzero update. Those parts are reexamined for new features and the active set F is updated accordingly. 6 Experiments We trained non-projective dependency parsers for 14 languages, using datasets from the CoNLL-X shared task (Buchholz and Marsi, 2006) and two datasets for English: one from the CoNLL-2008 shared task (Surdeanu et al., 2008), which contains non-projective arcs, and another derived from the Penn Treebank applying the standard head rules of Yamada and Matsumoto (2003), in which all parse trees are projective.12 We implemented Alg. 1, 12 We used the provided train/test splits for all datasets. For English, we used the standard test partitions (section 23 of the Wall Street Journal). We did not exploit the fact that some datasets only contain projective trees and have unique roots. 42 which handles any loss function Lβ,γ .13 When β &lt; ∞, Turbo Parser #1 and the loopy BP algorithm of Smith and Eisner (2008) is used; otherwise, Turbo Parser #2 is used and the LP relaxation is solved with CPLEX. In both cases, we employed the same pruning strategy as Martins et"
D10-1004,C98-1102,0,\N,Missing
D10-1124,N10-1038,1,0.204566,"tions. Using term frequency features xd for each author, we predict locations with wordgeography weights a ∈ R2W : lat T lon f (xd ; a) = (xT d a , xd a ) 1282 Weights are trained to minimize the sum of squared Euclidean distances, subject to L1 regularization: X lat lat 2 T lon (xT − ydlon )2 d a − yd ) + (xd a d + λlat ||alat ||1 + λlon ||alon ||1 The minimization problem decouples into two separate latitude and longitude models, which we fit using the glmnet elastic net regularized regression package (Friedman et al., 2010), which obtained good results on other text-based prediction tasks (Joshi et al., 2010). Regularization parameters were tuned on the development set. The L1 penalty outperformed L2 and mixtures of L1 and L2 . Note that for both word-level linear regression here, and the topic-level linear regression in SLDA, the choice of squared Euclidean distance dovetails with our use of spatial Gaussian likelihoods in the geographic topic models, since optimizing a is equivalent to maximum likelihood estimation under the assumption that locations are drawn from equivariant circular Gaussians centered around each f (xd ; a) linear prediction. We experimented with decorrelating the location di"
D11-1005,P10-1131,0,0.76042,"sed parameter estimation. Some previous multilingual research, such as Bayesian parameter tying across languages (Cohen and Smith, 2009) or models of parameter 1 Although the stated objective is often to build systems for resource-poor languages and domains, for evaluation purposes, annotated treebank test data figure prominently in this research (including in this paper). Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 50–61, c Edinburgh, Scotland, UK, July 27–31, 2011. 2011 Association for Computational Linguistics drift down phylogenetic trees (Berg-Kirkpatrick and Klein, 2010) is comparable, but the practical assumption of supervised helper languages is new to this work. Naseem et al. (2010) used universal syntactic categories and rules to improve grammar induction, but their model required expert handwritten rules as constraints. Herein, we specifically focus on two problems in linguistic structure prediction: unsupervised POS tagging and unsupervised dependency grammar induction. Our experiments demonstrate that the presented method outperforms strong state-of-the-art unsupervised baselines for both tasks. Our approach can be applied to other problems in which a"
D11-1005,N10-1083,0,0.0976671,"Missing"
D11-1005,W06-2920,0,0.0501669,"l during unsupervised learning, and are initialized using standard methods. 6 Experiments and Results In this section, we describe the experiments undertaken and the results achieved. We first note the characteristics of the datasets and the universal POS tags used in multilingual modeling. 6.1 Data For our experiments, we fixed a set of four helper languages with relatively large amounts of data, displaying nontrivial linguistic diversity: Czech (Slavic), English (West-Germanic), German (WestGermanic), and Italian (Romance). The datasets are the CoNLL-X shared task data for Czech and German (Buchholz and Marsi, 2006),5 the Penn Treebank for English (Marcus et al., 1993), and the CoNLL 2007 shared task data for Italian (Montemagni et al., 2003). This was the only set of helper languages we tested; improvements are likely possible. We leave an exploration of helper language choice (a subset selection problem) to future research, instead demonstrating that the concept has merit. We considered ten target languages: Bulgarian (Bg), Danish (Da), Dutch (Nl), Greek (El), Japanese (Jp), Portuguese (Pt), Slovene (Sl), Spanish (Es), Swedish (Sv), and Turkish (Tr). The data come from the CoNLL-X and CoNLL 2007 shared"
D11-1005,D08-1092,0,0.0368004,"such as POS tag sequences and parse trees (Klein and Manning, 2004; Johnson et al., 2007; Berg-Kirkpatrick et al., 2010; Cohen and Smith, 2010, inter alia). In its purest form, such research has improved our understanding of unsupervised learning practically and formally, and has led to a wide range of new algorithmic ideas. Another strain of research has sought to exploit resources and tools in some languages (especially English) to construct similar resources and tools for other languages, through heuristic “projection” (Yarowsky and Ngai, 2001; Xi and Hwa, 2005) or constraints in learning (Burkett and Klein, 2008; Smith and Eisner, 2009; Das and Petrov, 2011; McDonald et al., 2011) or inference (Smith and Smith, 2004). Joint unsupervised learning (Snyder and Barzilay, 2008; Naseem et al., 2009; Snyder et al., 50 2009) is yet another research direction that seeks to learn models for many languages at once, exploiting linguistic universals and language similarity. The driving force behind all of this work has been the hope of building NLP tools for languages that lack annotated resources.1 In this paper, we present an approach to using annotated data from one or more languages (helper languages) to lear"
D11-1005,D07-1022,1,0.760276,"escribed in §6.4. All tag induction uses a dictionary as specified in §6.3. The last row in this table indicates the best results using multilingual guidance taken from our methods in Table 3. “Avg” denotes macro-average across the ten languages. 2. Apply the fine-grained tagger to the words in the training data for the dependency parser. We consider two variants: the most probable assignment of tags to words (denoted “Pipeline”), and the posterior distribution over tags for each word, represented as a weighted “sausage” lattice (denoted “Joint”). This idea was explored for joint inference by Cohen and Smith (2007). 3. We apply the Mixture+EM unsupervised parser learning method from §6.4 to the automatically tagged sentences, or the lattices. 4. Given the two models, we infer POS tags on the test data using DG or Mixture+DG to get a lattice (Joint) or a sequence (Pipeline) and then parse using the model from the previous step.10 The resulting dependency trees are evaluated against the gold standard. Results are reported in Table 4. In almost all cases, joint decoding of tags and trees performs better than the pipeline. Even though our part-of-speech tagger with multilingual guidance outperforms the comp"
D11-1005,N09-1009,1,0.852683,"begin with supervised maximum likelihood estimates for models of the helper languages. In the second stage, we learn a model for the target language using unannotated data, maximizing likelihood over interpolations of the helper language models’ distributions. The tying is performed at the parameter level, through coarse, nearly-universal syntactic categories (POS tags). The resulting model is then used to initialize learning of the target language’s model using standard unsupervised parameter estimation. Some previous multilingual research, such as Bayesian parameter tying across languages (Cohen and Smith, 2009) or models of parameter 1 Although the stated objective is often to build systems for resource-poor languages and domains, for evaluation purposes, annotated treebank test data figure prominently in this research (including in this paper). Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 50–61, c Edinburgh, Scotland, UK, July 27–31, 2011. 2011 Association for Computational Linguistics drift down phylogenetic trees (Berg-Kirkpatrick and Klein, 2010) is comparable, but the practical assumption of supervised helper languages is new to this work. Naseem"
D11-1005,P11-1061,1,0.909688,"and Manning, 2004; Johnson et al., 2007; Berg-Kirkpatrick et al., 2010; Cohen and Smith, 2010, inter alia). In its purest form, such research has improved our understanding of unsupervised learning practically and formally, and has led to a wide range of new algorithmic ideas. Another strain of research has sought to exploit resources and tools in some languages (especially English) to construct similar resources and tools for other languages, through heuristic “projection” (Yarowsky and Ngai, 2001; Xi and Hwa, 2005) or constraints in learning (Burkett and Klein, 2008; Smith and Eisner, 2009; Das and Petrov, 2011; McDonald et al., 2011) or inference (Smith and Smith, 2004). Joint unsupervised learning (Snyder and Barzilay, 2008; Naseem et al., 2009; Snyder et al., 50 2009) is yet another research direction that seeks to learn models for many languages at once, exploiting linguistic universals and language similarity. The driving force behind all of this work has been the hope of building NLP tools for languages that lack annotated resources.1 In this paper, we present an approach to using annotated data from one or more languages (helper languages) to learn models for another language that lacks annot"
D11-1005,A94-1009,0,0.0200607,"most frequent parts of speech and exist in one form or another in all of the languages that we studied. For each language in our data, a mapping from the fine-grained treebank POS tags to these universal POS tags was constructed manually by Petrov et al. (2011). 6.3 θy→y0 = Part-of-Speech Tagging 6.3.1 L X (`) β`,y θy→y0 (11) `=1 Our first experimental task is POS tagging, and here we describe the specific details of the model, training and inference and the results attained. Model The model is a hidden Markov model (HMM), which has been popular for unsupervised tagging tasks (Merialdo, 1994; Elworthy, 1994; Smith and Eisner, 2005; Berg-Kirkpatrick et al., 2010).6 We use a bigram model and a locally normalized loglinear parameterization, like Berg-Kirkpatrick et al. (2010). These locally normalized log-linear models can look at various aspects of the observation x given a tag y, or the pair of tags in a transition, incorporating overlapping features. In basic monolin6 gual experiments, we used the same set of features as Berg-Kirkpatrick et al. (2010). For the transition log-linear model, Berg-Kirkpatrick et al. (2010) used only a single indicator feature of a tag pair, essentially equating to a"
D11-1005,P10-2036,0,0.0191573,"Missing"
D11-1005,N06-1041,0,0.0979683,"described in §5. We call this model “Uniform+DG.” The second version estimates the mixture coefficients to maximize likelihood, then expands the POS tags (§5), using the result to initialize training of the final model. We call this model “Mixture+DG.” No Tag Dictionary For each of the above configurations, we ran purely unsupervised training without a tag dictionary, and evaluated using one-to-one mapping accuracy constraining at most one HMM state to map to a unique treebank tag in the test data, using maximum bipartite matching. This is a variant of the greedy one-to-one mapping scheme of Haghighi and Klein (2006).8 With a Tag Dictionary We also ran a second version of each experimental configuration, where we used a tag dictionary to restrict the possible path sequences of the HMM during both learning and inference. This tag dictionary was constructed only from the training section of a given language’s treebank. It is widely known that such knowledge improves the quality of the model, though it is an open debate whether such knowledge is realistic to assume. For this experiment we removed punctuation from the training and test data, enabling direct use within the dependency grammar induction experime"
D11-1005,N09-1012,0,0.0439172,"dency grammar induction. For Turkish, Japanese, Slovene and Dutch, our unsupervised learner from words outperforms unsupervised parsing using gold-standard part-of-speech tags. We note that some recent work gives a treatment to unsupervised parsing (but not of dependencies) 10 The decoding method on test data (Joint or Pipeline) was matched to the training method, though they are orthogonal in principle. 59 directly from words (Seginer, 2007). Earlier work that induced part-of-speech tags and then performed unsupervised parsing in a pipeline includes Klein and Manning (2004) and Smith (2006). Headden et al. (2009) described the use of a lexicalized variant of the DMV model, with the use of gold part-ofspeech tags. 7 Conclusion We presented an approach to exploiting annotated data in helper languages to infer part-of-speech tagging and dependency parsing models in a different, target language, without parallel data. Our approach performs well in many cases. We also described a way to do joint decoding of part-of-speech tags and dependencies which performs better than a pipeline. Future work might consider exploiting a larger number of treebanks, and more powerful techniques for combining models than sim"
D11-1005,N07-1018,0,0.0117058,"d data from a set of one or more helper languages. Our approach is based on a model that locally mixes between supervised models from the helper languages. Parallel data is not used, allowing the technique to be applied even in domains where human-translated texts are unavailable. We obtain state-of-theart performance for two tasks of structure prediction: unsupervised part-of-speech tagging and unsupervised dependency parsing. 1 Introduction A major focus of recent NLP research has involved unsupervised learning of structure such as POS tag sequences and parse trees (Klein and Manning, 2004; Johnson et al., 2007; Berg-Kirkpatrick et al., 2010; Cohen and Smith, 2010, inter alia). In its purest form, such research has improved our understanding of unsupervised learning practically and formally, and has led to a wide range of new algorithmic ideas. Another strain of research has sought to exploit resources and tools in some languages (especially English) to construct similar resources and tools for other languages, through heuristic “projection” (Yarowsky and Ngai, 2001; Xi and Hwa, 2005) or constraints in learning (Burkett and Klein, 2008; Smith and Eisner, 2009; Das and Petrov, 2011; McDonald et al.,"
D11-1005,P04-1061,0,0.550547,"available, using annotated data from a set of one or more helper languages. Our approach is based on a model that locally mixes between supervised models from the helper languages. Parallel data is not used, allowing the technique to be applied even in domains where human-translated texts are unavailable. We obtain state-of-theart performance for two tasks of structure prediction: unsupervised part-of-speech tagging and unsupervised dependency parsing. 1 Introduction A major focus of recent NLP research has involved unsupervised learning of structure such as POS tag sequences and parse trees (Klein and Manning, 2004; Johnson et al., 2007; Berg-Kirkpatrick et al., 2010; Cohen and Smith, 2010, inter alia). In its purest form, such research has improved our understanding of unsupervised learning practically and formally, and has led to a wide range of new algorithmic ideas. Another strain of research has sought to exploit resources and tools in some languages (especially English) to construct similar resources and tools for other languages, through heuristic “projection” (Yarowsky and Ngai, 2001; Xi and Hwa, 2005) or constraints in learning (Burkett and Klein, 2008; Smith and Eisner, 2009; Das and Petrov, 2"
D11-1005,J93-2004,0,0.0374901,"tandard methods. 6 Experiments and Results In this section, we describe the experiments undertaken and the results achieved. We first note the characteristics of the datasets and the universal POS tags used in multilingual modeling. 6.1 Data For our experiments, we fixed a set of four helper languages with relatively large amounts of data, displaying nontrivial linguistic diversity: Czech (Slavic), English (West-Germanic), German (WestGermanic), and Italian (Romance). The datasets are the CoNLL-X shared task data for Czech and German (Buchholz and Marsi, 2006),5 the Penn Treebank for English (Marcus et al., 1993), and the CoNLL 2007 shared task data for Italian (Montemagni et al., 2003). This was the only set of helper languages we tested; improvements are likely possible. We leave an exploration of helper language choice (a subset selection problem) to future research, instead demonstrating that the concept has merit. We considered ten target languages: Bulgarian (Bg), Danish (Da), Dutch (Nl), Greek (El), Japanese (Jp), Portuguese (Pt), Slovene (Sl), Spanish (Es), Swedish (Sv), and Turkish (Tr). The data come from the CoNLL-X and CoNLL 2007 shared tasks (Buchholz and Marsi, 2006; Nivre et al., 2007)."
D11-1005,D11-1006,0,0.610872,"nson et al., 2007; Berg-Kirkpatrick et al., 2010; Cohen and Smith, 2010, inter alia). In its purest form, such research has improved our understanding of unsupervised learning practically and formally, and has led to a wide range of new algorithmic ideas. Another strain of research has sought to exploit resources and tools in some languages (especially English) to construct similar resources and tools for other languages, through heuristic “projection” (Yarowsky and Ngai, 2001; Xi and Hwa, 2005) or constraints in learning (Burkett and Klein, 2008; Smith and Eisner, 2009; Das and Petrov, 2011; McDonald et al., 2011) or inference (Smith and Smith, 2004). Joint unsupervised learning (Snyder and Barzilay, 2008; Naseem et al., 2009; Snyder et al., 50 2009) is yet another research direction that seeks to learn models for many languages at once, exploiting linguistic universals and language similarity. The driving force behind all of this work has been the hope of building NLP tools for languages that lack annotated resources.1 In this paper, we present an approach to using annotated data from one or more languages (helper languages) to learn models for another language that lacks annotated data (the target la"
D11-1005,J94-2001,0,0.0620279,"bset) cover the most frequent parts of speech and exist in one form or another in all of the languages that we studied. For each language in our data, a mapping from the fine-grained treebank POS tags to these universal POS tags was constructed manually by Petrov et al. (2011). 6.3 θy→y0 = Part-of-Speech Tagging 6.3.1 L X (`) β`,y θy→y0 (11) `=1 Our first experimental task is POS tagging, and here we describe the specific details of the model, training and inference and the results attained. Model The model is a hidden Markov model (HMM), which has been popular for unsupervised tagging tasks (Merialdo, 1994; Elworthy, 1994; Smith and Eisner, 2005; Berg-Kirkpatrick et al., 2010).6 We use a bigram model and a locally normalized loglinear parameterization, like Berg-Kirkpatrick et al. (2010). These locally normalized log-linear models can look at various aspects of the observation x given a tag y, or the pair of tags in a transition, incorporating overlapping features. In basic monolin6 gual experiments, we used the same set of features as Berg-Kirkpatrick et al. (2010). For the transition log-linear model, Berg-Kirkpatrick et al. (2010) used only a single indicator feature of a tag pair, essential"
D11-1005,D10-1120,0,0.401513,"2009) or models of parameter 1 Although the stated objective is often to build systems for resource-poor languages and domains, for evaluation purposes, annotated treebank test data figure prominently in this research (including in this paper). Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 50–61, c Edinburgh, Scotland, UK, July 27–31, 2011. 2011 Association for Computational Linguistics drift down phylogenetic trees (Berg-Kirkpatrick and Klein, 2010) is comparable, but the practical assumption of supervised helper languages is new to this work. Naseem et al. (2010) used universal syntactic categories and rules to improve grammar induction, but their model required expert handwritten rules as constraints. Herein, we specifically focus on two problems in linguistic structure prediction: unsupervised POS tagging and unsupervised dependency grammar induction. Our experiments demonstrate that the presented method outperforms strong state-of-the-art unsupervised baselines for both tasks. Our approach can be applied to other problems in which a subset of the model parameters can be linked across languages. We also experiment with unsupervised learning of depen"
D11-1005,P07-1049,0,0.0401133,"tilingual guidance outperforms the completely unsupervised baseline, there is not always an advantage of using this multilingually guided partof-speech tagger for dependency grammar induction. For Turkish, Japanese, Slovene and Dutch, our unsupervised learner from words outperforms unsupervised parsing using gold-standard part-of-speech tags. We note that some recent work gives a treatment to unsupervised parsing (but not of dependencies) 10 The decoding method on test data (Joint or Pipeline) was matched to the training method, though they are orthogonal in principle. 59 directly from words (Seginer, 2007). Earlier work that induced part-of-speech tags and then performed unsupervised parsing in a pipeline includes Klein and Manning (2004) and Smith (2006). Headden et al. (2009) described the use of a lexicalized variant of the DMV model, with the use of gold part-ofspeech tags. 7 Conclusion We presented an approach to exploiting annotated data in helper languages to infer part-of-speech tagging and dependency parsing models in a different, target language, without parallel data. Our approach performs well in many cases. We also described a way to do joint decoding of part-of-speech tags and dep"
D11-1005,P05-1044,1,0.488925,"rts of speech and exist in one form or another in all of the languages that we studied. For each language in our data, a mapping from the fine-grained treebank POS tags to these universal POS tags was constructed manually by Petrov et al. (2011). 6.3 θy→y0 = Part-of-Speech Tagging 6.3.1 L X (`) β`,y θy→y0 (11) `=1 Our first experimental task is POS tagging, and here we describe the specific details of the model, training and inference and the results attained. Model The model is a hidden Markov model (HMM), which has been popular for unsupervised tagging tasks (Merialdo, 1994; Elworthy, 1994; Smith and Eisner, 2005; Berg-Kirkpatrick et al., 2010).6 We use a bigram model and a locally normalized loglinear parameterization, like Berg-Kirkpatrick et al. (2010). These locally normalized log-linear models can look at various aspects of the observation x given a tag y, or the pair of tags in a transition, incorporating overlapping features. In basic monolin6 gual experiments, we used the same set of features as Berg-Kirkpatrick et al. (2010). For the transition log-linear model, Berg-Kirkpatrick et al. (2010) used only a single indicator feature of a tag pair, essentially equating to a traditional multinomial"
D11-1005,D09-1086,0,0.10063,"and parse trees (Klein and Manning, 2004; Johnson et al., 2007; Berg-Kirkpatrick et al., 2010; Cohen and Smith, 2010, inter alia). In its purest form, such research has improved our understanding of unsupervised learning practically and formally, and has led to a wide range of new algorithmic ideas. Another strain of research has sought to exploit resources and tools in some languages (especially English) to construct similar resources and tools for other languages, through heuristic “projection” (Yarowsky and Ngai, 2001; Xi and Hwa, 2005) or constraints in learning (Burkett and Klein, 2008; Smith and Eisner, 2009; Das and Petrov, 2011; McDonald et al., 2011) or inference (Smith and Smith, 2004). Joint unsupervised learning (Snyder and Barzilay, 2008; Naseem et al., 2009; Snyder et al., 50 2009) is yet another research direction that seeks to learn models for many languages at once, exploiting linguistic universals and language similarity. The driving force behind all of this work has been the hope of building NLP tools for languages that lack annotated resources.1 In this paper, we present an approach to using annotated data from one or more languages (helper languages) to learn models for another lan"
D11-1005,W04-3207,1,0.820928,"t al., 2010; Cohen and Smith, 2010, inter alia). In its purest form, such research has improved our understanding of unsupervised learning practically and formally, and has led to a wide range of new algorithmic ideas. Another strain of research has sought to exploit resources and tools in some languages (especially English) to construct similar resources and tools for other languages, through heuristic “projection” (Yarowsky and Ngai, 2001; Xi and Hwa, 2005) or constraints in learning (Burkett and Klein, 2008; Smith and Eisner, 2009; Das and Petrov, 2011; McDonald et al., 2011) or inference (Smith and Smith, 2004). Joint unsupervised learning (Snyder and Barzilay, 2008; Naseem et al., 2009; Snyder et al., 50 2009) is yet another research direction that seeks to learn models for many languages at once, exploiting linguistic universals and language similarity. The driving force behind all of this work has been the hope of building NLP tools for languages that lack annotated resources.1 In this paper, we present an approach to using annotated data from one or more languages (helper languages) to learn models for another language that lacks annotated data (the target language). Unlike the previous work men"
D11-1005,P08-1084,0,0.0323582,"ts purest form, such research has improved our understanding of unsupervised learning practically and formally, and has led to a wide range of new algorithmic ideas. Another strain of research has sought to exploit resources and tools in some languages (especially English) to construct similar resources and tools for other languages, through heuristic “projection” (Yarowsky and Ngai, 2001; Xi and Hwa, 2005) or constraints in learning (Burkett and Klein, 2008; Smith and Eisner, 2009; Das and Petrov, 2011; McDonald et al., 2011) or inference (Smith and Smith, 2004). Joint unsupervised learning (Snyder and Barzilay, 2008; Naseem et al., 2009; Snyder et al., 50 2009) is yet another research direction that seeks to learn models for many languages at once, exploiting linguistic universals and language similarity. The driving force behind all of this work has been the hope of building NLP tools for languages that lack annotated resources.1 In this paper, we present an approach to using annotated data from one or more languages (helper languages) to learn models for another language that lacks annotated data (the target language). Unlike the previous work mentioned above, our framework does not rely on parallel da"
D11-1005,P09-1009,0,0.126357,"Missing"
D11-1005,N10-1116,0,0.0267179,"fficients are learned automatically fares better than uniform weighting. With a tag dictionary, the multilingual variants outperform the baseline in seven out of ten cases, and the learned mixture outperforms or matches the uniform mixture in five of those seven (Table 2b). 6.4 Dependency Grammar Induction We next describe experiments for dependency grammar induction. As the basic grammatical model, we adopt the dependency model with valence (Klein and Manning, 2004), which forms the basis for stateof-the-art results for dependency grammar induction in various settings (Cohen and Smith, 2009; Spitkovsky et al., 2010; Gillenwater et al., 2010; Berg-Kirkpatrick and Klein, 2010). As shown in Table 3, DMV obtains much higher accuracy in the supervised setting than the unsupervised setting, suggesting that more can be achieved with this model family.9 For this reason, and because DMV is easily interpreted as a PCFG, it is our starting point and baseline. We consider four conditions. The independent variables are (1) whether we use uniform β (all set to 1 4 ) or estimate them using EM (as described in §4), and (2) whether we simply use the mixture model to decode the test data, or to initialize EM for the DMV."
D11-1005,H05-1107,0,0.0956265,"s involved unsupervised learning of structure such as POS tag sequences and parse trees (Klein and Manning, 2004; Johnson et al., 2007; Berg-Kirkpatrick et al., 2010; Cohen and Smith, 2010, inter alia). In its purest form, such research has improved our understanding of unsupervised learning practically and formally, and has led to a wide range of new algorithmic ideas. Another strain of research has sought to exploit resources and tools in some languages (especially English) to construct similar resources and tools for other languages, through heuristic “projection” (Yarowsky and Ngai, 2001; Xi and Hwa, 2005) or constraints in learning (Burkett and Klein, 2008; Smith and Eisner, 2009; Das and Petrov, 2011; McDonald et al., 2011) or inference (Smith and Smith, 2004). Joint unsupervised learning (Snyder and Barzilay, 2008; Naseem et al., 2009; Snyder et al., 50 2009) is yet another research direction that seeks to learn models for many languages at once, exploiting linguistic universals and language similarity. The driving force behind all of this work has been the hope of building NLP tools for languages that lack annotated resources.1 In this paper, we present an approach to using annotated data f"
D11-1005,N01-1026,0,0.0625168,"of recent NLP research has involved unsupervised learning of structure such as POS tag sequences and parse trees (Klein and Manning, 2004; Johnson et al., 2007; Berg-Kirkpatrick et al., 2010; Cohen and Smith, 2010, inter alia). In its purest form, such research has improved our understanding of unsupervised learning practically and formally, and has led to a wide range of new algorithmic ideas. Another strain of research has sought to exploit resources and tools in some languages (especially English) to construct similar resources and tools for other languages, through heuristic “projection” (Yarowsky and Ngai, 2001; Xi and Hwa, 2005) or constraints in learning (Burkett and Klein, 2008; Smith and Eisner, 2009; Das and Petrov, 2011; McDonald et al., 2011) or inference (Smith and Smith, 2004). Joint unsupervised learning (Snyder and Barzilay, 2008; Naseem et al., 2009; Snyder et al., 50 2009) is yet another research direction that seeks to learn models for many languages at once, exploiting linguistic universals and language similarity. The driving force behind all of this work has been the hope of building NLP tools for languages that lack annotated resources.1 In this paper, we present an approach to usi"
D11-1005,D07-1096,0,\N,Missing
D11-1022,P11-1048,0,0.0382728,"demands some form of approximate decoding, such as sampling, heuristic search, or variational inference. In this paper, we focus on parsers built from linear programming relaxations, the so-called “turbo parsers” (Martins et al., 2009a; Martins et al., 2010). Rush et al. (2010) applied dual decomposition as a way of combining models which alone permit efficient decoding, but whose combination is intractable. This results in a relaxation of the original problem that is elegantly solved with the subgradient algorithm. While this technique has proven quite effective in parsing (Koo et al., 2010; Auli and Lopez, 2011) as well as machine translation (Rush and Collins, 2011), we show here that its 238 success is strongly tied to the ability of finding a “good” decomposition, i.e., one involving few overlapping components (or slaves). With many components, the subgradient algorithm exhibits extremely slow convergence (cf. Fig. 2). Unfortunately, a lightweight decomposition is not always at hand, either because the problem does not factor in a natural way, or because one would like to incorporate features that cannot be easily absorbed in few tractable components. Examples include features generated by stateme"
D11-1022,P11-1049,0,0.0365357,"Missing"
D11-1022,W06-2920,0,0.0135036,"e are now endowed with a procedure for many other cases, such that AND - WITH - OUTPUT and formulas with universal quantifiers (e.g., R(x) := ∀y : Q(x, y)). Up to a log-factor, the runtimes will be linear in the number of predicates. and Bertsekas (1992) show that the ADMM algorithm may still converge when the z-updates are inexact. Hence the method may still work if the slaves are solved numerically up to some accuracy. We defer this to future investigation. 5 Experiments: Dependency Parsing We used 14 datasets with non-projective dependencies from the CoNLL-2006 and CoNLL-2008 shared tasks (Buchholz and Marsi, 2006; Surdeanu et al., 2008). We also used a projective English dataset derived from the Penn Treebank by applying the standard head rules of Yamada and Matsumoto (2003).11 We did not force the parser to output projective trees or unique roots for any of the datasets; everything is learned from the data. We trained by running 10 iterations of the cost-augmented MIRA algorithm (Crammer et al., 2006) with LP-relaxed decoding, as in Martins et al. (2009b). Following common practice (Charniak and Johnson, 2005; Carreras et al., 2008), we employed a coarse-to-fine procedure to prune away unlikely candi"
D11-1022,W08-2102,0,0.019225,"tive dependencies from the CoNLL-2006 and CoNLL-2008 shared tasks (Buchholz and Marsi, 2006; Surdeanu et al., 2008). We also used a projective English dataset derived from the Penn Treebank by applying the standard head rules of Yamada and Matsumoto (2003).11 We did not force the parser to output projective trees or unique roots for any of the datasets; everything is learned from the data. We trained by running 10 iterations of the cost-augmented MIRA algorithm (Crammer et al., 2006) with LP-relaxed decoding, as in Martins et al. (2009b). Following common practice (Charniak and Johnson, 2005; Carreras et al., 2008), we employed a coarse-to-fine procedure to prune away unlikely candidate arcs, as described by Koo and Collins (2010). To ensure valid parse trees at test time, we rounded fractional Larger Slaves. The only disadvantage of DDADMM in comparison with the subgradient algorithm is that there is not an obvious way of solving the subproblem in Eq. 14 exactly for large combinatorial factors, such as the TREE constraint in dependency parsing, or a sequence model. Hence, our method seems to be more suitable for decompositions which involve “simple slaves,” even if their 11 As usual, we train on sectio"
D11-1022,D07-1101,0,0.0925167,"see Fig. 1); in phrase-based parsing, it can be the set of possible spans; in sequence labeling, it can be the set of possible labels at each position. Our only assumption is that we can “read out” y from the basic parts it contains. For convenience, we represent y as a binary vector, y = hy(r)ir∈R , where y(r) = 1 if part r belongs to y, and 0 otherwise. Figure 1: Parts used by our parser. Arcs are the basic parts: any dependency tree can be “read out” from the arcs it contains. Consecutive siblings and grandparent parts introduce horizontal and vertical Markovization (McDonald et al., 2006; Carreras, 2007). We break the horizontal Markov assumption via all siblings parts and the vertical one through parts which indicate a directed path between two words. Inspired by transition-based parsers, we also adopt head bigram parts, which look at the heads attached to consecutive words. Finally, we follow Martins et al. (2009a) and have parts which indicate if an arc is non-projective (i.e., if it spans words that do not descend from its head). Each Ys is associated with its own set of parts Rs , in the same sense as above; we represent the elements of Ys as binary vectors zs = hzs (r)ir∈Rs . Examples a"
D11-1022,P05-1022,0,0.0602341,"14 datasets with non-projective dependencies from the CoNLL-2006 and CoNLL-2008 shared tasks (Buchholz and Marsi, 2006; Surdeanu et al., 2008). We also used a projective English dataset derived from the Penn Treebank by applying the standard head rules of Yamada and Matsumoto (2003).11 We did not force the parser to output projective trees or unique roots for any of the datasets; everything is learned from the data. We trained by running 10 iterations of the cost-augmented MIRA algorithm (Crammer et al., 2006) with LP-relaxed decoding, as in Martins et al. (2009b). Following common practice (Charniak and Johnson, 2005; Carreras et al., 2008), we employed a coarse-to-fine procedure to prune away unlikely candidate arcs, as described by Koo and Collins (2010). To ensure valid parse trees at test time, we rounded fractional Larger Slaves. The only disadvantage of DDADMM in comparison with the subgradient algorithm is that there is not an obvious way of solving the subproblem in Eq. 14 exactly for large combinatorial factors, such as the TREE constraint in dependency parsing, or a sequence model. Hence, our method seems to be more suitable for decompositions which involve “simple slaves,” even if their 11 As u"
D11-1022,P08-1109,0,0.0101225,"ce of rich features or logical constraints), the original subgradient algorithm is inefficient. We sidestep that difficulty by adopting an augmented Lagrangian method that accelerates model consensus by regularizing towards the averaged votes. We show how first-order logical constraints can be handled efficiently, even though the corresponding subproblems are no longer combinatorial, and report experiments in dependency parsing, with state-of-the-art results. 1 Introduction The last years have witnessed increasingly accurate models for syntax, semantics, and machine translation (Chiang, 2007; Finkel et al., 2008; Petrov and Klein, 2008; Smith and Eisner, 2008; Martins et al., 2009a; Johansson and Nugues, 2008; Koo et al., 2010). The predictive power of such models stems from their ability to break locality assumptions. The resulting combinatorial explosion typically demands some form of approximate decoding, such as sampling, heuristic search, or variational inference. In this paper, we focus on parsers built from linear programming relaxations, the so-called “turbo parsers” (Martins et al., 2009a; Martins et al., 2010). Rush et al. (2010) applied dual decomposition as a way of combining models which"
D11-1022,gimenez-marquez-2004-svmtool,0,0.0165788,"Missing"
D11-1022,P10-1110,0,0.0100494,"1 we recover known methods: • Resorting to the tree and consecutive sibling formulae gives one of the models in Koo et al. (2010), with the same linear relaxation (a proof of this fact is included in App. F); • Resorting to tree, all siblings, grandparent, and non-projective arcs, recovers a multi-commodity flow configuration proposed by Martins et al. (2009a); the relaxation is also the same.12 The experimental results are shown in Tab. 2. For comparison, we include the best published results for each dataset (at the best of our knowledge), among transition-based parsers (Nivre et al., 2006; Huang and Sagae, 2010), graph-based parsers (McDonald et al., 2006; Koo and Collins, 2010), hybrid methods (Nivre and McDonald, 2008; Martins et al., 2008), and turbo parsers (Martins et al., 2010; Koo et al., 2010). Our full model achieved the best reported scores for 7 datasets. The last two columns show a consistent improvement (with the exceptions of Chinese and Arabic) when using the full set of features over a second order model with grandparent and consecutive siblings, which is our reproduction of the model of Koo et al. (2010).13 12 Although Martins et al. (2009a) also incorporated consecutive siblings in"
D11-1022,D08-1008,0,0.0139122,"nt. We sidestep that difficulty by adopting an augmented Lagrangian method that accelerates model consensus by regularizing towards the averaged votes. We show how first-order logical constraints can be handled efficiently, even though the corresponding subproblems are no longer combinatorial, and report experiments in dependency parsing, with state-of-the-art results. 1 Introduction The last years have witnessed increasingly accurate models for syntax, semantics, and machine translation (Chiang, 2007; Finkel et al., 2008; Petrov and Klein, 2008; Smith and Eisner, 2008; Martins et al., 2009a; Johansson and Nugues, 2008; Koo et al., 2010). The predictive power of such models stems from their ability to break locality assumptions. The resulting combinatorial explosion typically demands some form of approximate decoding, such as sampling, heuristic search, or variational inference. In this paper, we focus on parsers built from linear programming relaxations, the so-called “turbo parsers” (Martins et al., 2009a; Martins et al., 2010). Rush et al. (2010) applied dual decomposition as a way of combining models which alone permit efficient decoding, but whose combination is intractable. This results in a relaxatio"
D11-1022,P10-1001,0,0.486395,"e also used a projective English dataset derived from the Penn Treebank by applying the standard head rules of Yamada and Matsumoto (2003).11 We did not force the parser to output projective trees or unique roots for any of the datasets; everything is learned from the data. We trained by running 10 iterations of the cost-augmented MIRA algorithm (Crammer et al., 2006) with LP-relaxed decoding, as in Martins et al. (2009b). Following common practice (Charniak and Johnson, 2005; Carreras et al., 2008), we employed a coarse-to-fine procedure to prune away unlikely candidate arcs, as described by Koo and Collins (2010). To ensure valid parse trees at test time, we rounded fractional Larger Slaves. The only disadvantage of DDADMM in comparison with the subgradient algorithm is that there is not an obvious way of solving the subproblem in Eq. 14 exactly for large combinatorial factors, such as the TREE constraint in dependency parsing, or a sequence model. Hence, our method seems to be more suitable for decompositions which involve “simple slaves,” even if their 11 As usual, we train on sections §02–21, use §22 as validation number is large. However, this does not rule out the data, and test on §23. We ran SV"
D11-1022,D10-1125,0,0.143941,"lty by adopting an augmented Lagrangian method that accelerates model consensus by regularizing towards the averaged votes. We show how first-order logical constraints can be handled efficiently, even though the corresponding subproblems are no longer combinatorial, and report experiments in dependency parsing, with state-of-the-art results. 1 Introduction The last years have witnessed increasingly accurate models for syntax, semantics, and machine translation (Chiang, 2007; Finkel et al., 2008; Petrov and Klein, 2008; Smith and Eisner, 2008; Martins et al., 2009a; Johansson and Nugues, 2008; Koo et al., 2010). The predictive power of such models stems from their ability to break locality assumptions. The resulting combinatorial explosion typically demands some form of approximate decoding, such as sampling, heuristic search, or variational inference. In this paper, we focus on parsers built from linear programming relaxations, the so-called “turbo parsers” (Martins et al., 2009a; Martins et al., 2010). Rush et al. (2010) applied dual decomposition as a way of combining models which alone permit efficient decoding, but whose combination is intractable. This results in a relaxation of the original p"
D11-1022,P11-1060,0,0.00812487,"ence properties of DD-ADMM, complementing the theoretical treatment in Martins et al. (2011). DD-ADMM compares favourably against the subgradient method in several aspects: it is faster to reach a consensus, it has better stopping conditions, and it works better in non-lightweight decompositions. While its slave subproblems are more involved, we derived closedform solutions for many cases of interest, such as first-order logic formulas and combinatorial factors. DD-ADMM may be useful in other frameworks involving logical constraints, such as the models for compositional semantics presented by Liang et al. (2011). Non-logical constraints may also yield efficient subproblems, e.g., the length constraints in summarization and compression (Clarke and Lapata, 2008; Martins and Smith, 2009; BergKirkpatrick et al., 2011). Finally, DD-ADMM can be adapted to tighten its relaxations towards exact decoding, as in Sontag et al. (2008) and Rush and Collins (2011). We defer this for future work. Acknowledgments We thank all reviewers for their comments, Eric Xing for helpful discussions, and Terry Koo and Sasha Rush for answering questions about their parser and for providing code. A. M. was supported by a FCT/ICT"
D11-1022,W09-1801,1,0.81558,": it is faster to reach a consensus, it has better stopping conditions, and it works better in non-lightweight decompositions. While its slave subproblems are more involved, we derived closedform solutions for many cases of interest, such as first-order logic formulas and combinatorial factors. DD-ADMM may be useful in other frameworks involving logical constraints, such as the models for compositional semantics presented by Liang et al. (2011). Non-logical constraints may also yield efficient subproblems, e.g., the length constraints in summarization and compression (Clarke and Lapata, 2008; Martins and Smith, 2009; BergKirkpatrick et al., 2011). Finally, DD-ADMM can be adapted to tighten its relaxations towards exact decoding, as in Sontag et al. (2008) and Rush and Collins (2011). We defer this for future work. Acknowledgments We thank all reviewers for their comments, Eric Xing for helpful discussions, and Terry Koo and Sasha Rush for answering questions about their parser and for providing code. A. M. was supported by a FCT/ICTI grant through the CMU-Portugal Program, and by Priberam. This work was partially supported by the FET programme (EU FP7), under the SIMBAD project (contract 213250). N. S. w"
D11-1022,D08-1017,1,0.847301,"h the same linear relaxation (a proof of this fact is included in App. F); • Resorting to tree, all siblings, grandparent, and non-projective arcs, recovers a multi-commodity flow configuration proposed by Martins et al. (2009a); the relaxation is also the same.12 The experimental results are shown in Tab. 2. For comparison, we include the best published results for each dataset (at the best of our knowledge), among transition-based parsers (Nivre et al., 2006; Huang and Sagae, 2010), graph-based parsers (McDonald et al., 2006; Koo and Collins, 2010), hybrid methods (Nivre and McDonald, 2008; Martins et al., 2008), and turbo parsers (Martins et al., 2010; Koo et al., 2010). Our full model achieved the best reported scores for 7 datasets. The last two columns show a consistent improvement (with the exceptions of Chinese and Arabic) when using the full set of features over a second order model with grandparent and consecutive siblings, which is our reproduction of the model of Koo et al. (2010).13 12 Although Martins et al. (2009a) also incorporated consecutive siblings in one of their configurations, our constraints are tighter than theirs. See App. F. 13 Note however that the actual results of Koo et a"
D11-1022,P09-1039,1,0.362218,"algorithm is inefficient. We sidestep that difficulty by adopting an augmented Lagrangian method that accelerates model consensus by regularizing towards the averaged votes. We show how first-order logical constraints can be handled efficiently, even though the corresponding subproblems are no longer combinatorial, and report experiments in dependency parsing, with state-of-the-art results. 1 Introduction The last years have witnessed increasingly accurate models for syntax, semantics, and machine translation (Chiang, 2007; Finkel et al., 2008; Petrov and Klein, 2008; Smith and Eisner, 2008; Martins et al., 2009a; Johansson and Nugues, 2008; Koo et al., 2010). The predictive power of such models stems from their ability to break locality assumptions. The resulting combinatorial explosion typically demands some form of approximate decoding, such as sampling, heuristic search, or variational inference. In this paper, we focus on parsers built from linear programming relaxations, the so-called “turbo parsers” (Martins et al., 2009a; Martins et al., 2010). Rush et al. (2010) applied dual decomposition as a way of combining models which alone permit efficient decoding, but whose combination is intractable"
D11-1022,D10-1004,1,0.107347,"ingly accurate models for syntax, semantics, and machine translation (Chiang, 2007; Finkel et al., 2008; Petrov and Klein, 2008; Smith and Eisner, 2008; Martins et al., 2009a; Johansson and Nugues, 2008; Koo et al., 2010). The predictive power of such models stems from their ability to break locality assumptions. The resulting combinatorial explosion typically demands some form of approximate decoding, such as sampling, heuristic search, or variational inference. In this paper, we focus on parsers built from linear programming relaxations, the so-called “turbo parsers” (Martins et al., 2009a; Martins et al., 2010). Rush et al. (2010) applied dual decomposition as a way of combining models which alone permit efficient decoding, but whose combination is intractable. This results in a relaxation of the original problem that is elegantly solved with the subgradient algorithm. While this technique has proven quite effective in parsing (Koo et al., 2010; Auli and Lopez, 2011) as well as machine translation (Rush and Collins, 2011), we show here that its 238 success is strongly tied to the ability of finding a “good” decomposition, i.e., one involving few overlapping components (or slaves). With many componen"
D11-1022,W06-2932,0,0.389702,"sible dependency arcs (see Fig. 1); in phrase-based parsing, it can be the set of possible spans; in sequence labeling, it can be the set of possible labels at each position. Our only assumption is that we can “read out” y from the basic parts it contains. For convenience, we represent y as a binary vector, y = hy(r)ir∈R , where y(r) = 1 if part r belongs to y, and 0 otherwise. Figure 1: Parts used by our parser. Arcs are the basic parts: any dependency tree can be “read out” from the arcs it contains. Consecutive siblings and grandparent parts introduce horizontal and vertical Markovization (McDonald et al., 2006; Carreras, 2007). We break the horizontal Markov assumption via all siblings parts and the vertical one through parts which indicate a directed path between two words. Inspired by transition-based parsers, we also adopt head bigram parts, which look at the heads attached to consecutive words. Finally, we follow Martins et al. (2009a) and have parts which indicate if an arc is non-projective (i.e., if it spans words that do not descend from its head). Each Ys is associated with its own set of parts Rs , in the same sense as above; we represent the elements of Ys as binary vectors zs = hzs (r)i"
D11-1022,P08-1108,0,0.0219951,"in Koo et al. (2010), with the same linear relaxation (a proof of this fact is included in App. F); • Resorting to tree, all siblings, grandparent, and non-projective arcs, recovers a multi-commodity flow configuration proposed by Martins et al. (2009a); the relaxation is also the same.12 The experimental results are shown in Tab. 2. For comparison, we include the best published results for each dataset (at the best of our knowledge), among transition-based parsers (Nivre et al., 2006; Huang and Sagae, 2010), graph-based parsers (McDonald et al., 2006; Koo and Collins, 2010), hybrid methods (Nivre and McDonald, 2008; Martins et al., 2008), and turbo parsers (Martins et al., 2010; Koo et al., 2010). Our full model achieved the best reported scores for 7 datasets. The last two columns show a consistent improvement (with the exceptions of Chinese and Arabic) when using the full set of features over a second order model with grandparent and consecutive siblings, which is our reproduction of the model of Koo et al. (2010).13 12 Although Martins et al. (2009a) also incorporated consecutive siblings in one of their configurations, our constraints are tighter than theirs. See App. F. 13 Note however that the act"
D11-1022,W06-2933,0,0.105724,"Missing"
D11-1022,D08-1091,0,0.0183398,"r logical constraints), the original subgradient algorithm is inefficient. We sidestep that difficulty by adopting an augmented Lagrangian method that accelerates model consensus by regularizing towards the averaged votes. We show how first-order logical constraints can be handled efficiently, even though the corresponding subproblems are no longer combinatorial, and report experiments in dependency parsing, with state-of-the-art results. 1 Introduction The last years have witnessed increasingly accurate models for syntax, semantics, and machine translation (Chiang, 2007; Finkel et al., 2008; Petrov and Klein, 2008; Smith and Eisner, 2008; Martins et al., 2009a; Johansson and Nugues, 2008; Koo et al., 2010). The predictive power of such models stems from their ability to break locality assumptions. The resulting combinatorial explosion typically demands some form of approximate decoding, such as sampling, heuristic search, or variational inference. In this paper, we focus on parsers built from linear programming relaxations, the so-called “turbo parsers” (Martins et al., 2009a; Martins et al., 2010). Rush et al. (2010) applied dual decomposition as a way of combining models which alone permit efficient"
D11-1022,W06-1616,0,0.369967,"Missing"
D11-1022,P11-1008,0,0.282099,"pling, heuristic search, or variational inference. In this paper, we focus on parsers built from linear programming relaxations, the so-called “turbo parsers” (Martins et al., 2009a; Martins et al., 2010). Rush et al. (2010) applied dual decomposition as a way of combining models which alone permit efficient decoding, but whose combination is intractable. This results in a relaxation of the original problem that is elegantly solved with the subgradient algorithm. While this technique has proven quite effective in parsing (Koo et al., 2010; Auli and Lopez, 2011) as well as machine translation (Rush and Collins, 2011), we show here that its 238 success is strongly tied to the ability of finding a “good” decomposition, i.e., one involving few overlapping components (or slaves). With many components, the subgradient algorithm exhibits extremely slow convergence (cf. Fig. 2). Unfortunately, a lightweight decomposition is not always at hand, either because the problem does not factor in a natural way, or because one would like to incorporate features that cannot be easily absorbed in few tractable components. Examples include features generated by statements in first-order logic, features that violate Markov a"
D11-1022,D10-1001,0,0.521096,"or syntax, semantics, and machine translation (Chiang, 2007; Finkel et al., 2008; Petrov and Klein, 2008; Smith and Eisner, 2008; Martins et al., 2009a; Johansson and Nugues, 2008; Koo et al., 2010). The predictive power of such models stems from their ability to break locality assumptions. The resulting combinatorial explosion typically demands some form of approximate decoding, such as sampling, heuristic search, or variational inference. In this paper, we focus on parsers built from linear programming relaxations, the so-called “turbo parsers” (Martins et al., 2009a; Martins et al., 2010). Rush et al. (2010) applied dual decomposition as a way of combining models which alone permit efficient decoding, but whose combination is intractable. This results in a relaxation of the original problem that is elegantly solved with the subgradient algorithm. While this technique has proven quite effective in parsing (Koo et al., 2010; Auli and Lopez, 2011) as well as machine translation (Rush and Collins, 2011), we show here that its 238 success is strongly tied to the ability of finding a “good” decomposition, i.e., one involving few overlapping components (or slaves). With many components, the subgradient"
D11-1022,D08-1016,0,0.539453,"the original subgradient algorithm is inefficient. We sidestep that difficulty by adopting an augmented Lagrangian method that accelerates model consensus by regularizing towards the averaged votes. We show how first-order logical constraints can be handled efficiently, even though the corresponding subproblems are no longer combinatorial, and report experiments in dependency parsing, with state-of-the-art results. 1 Introduction The last years have witnessed increasingly accurate models for syntax, semantics, and machine translation (Chiang, 2007; Finkel et al., 2008; Petrov and Klein, 2008; Smith and Eisner, 2008; Martins et al., 2009a; Johansson and Nugues, 2008; Koo et al., 2010). The predictive power of such models stems from their ability to break locality assumptions. The resulting combinatorial explosion typically demands some form of approximate decoding, such as sampling, heuristic search, or variational inference. In this paper, we focus on parsers built from linear programming relaxations, the so-called “turbo parsers” (Martins et al., 2009a; Martins et al., 2010). Rush et al. (2010) applied dual decomposition as a way of combining models which alone permit efficient decoding, but whose comb"
D11-1022,W08-2121,0,0.129424,"Missing"
D11-1022,N06-1054,0,0.0733185,"Computational Linguistics (1) Designing the model must obey certain practical considerations. If efficiency is the major concern, a simple model is usually chosen so that Eq. 1 can be solved efficiently, at the cost of limited expressive power. If we care more about accuracy, a model with richer features and more involved score functions may be designed. Decoding, however, will be more expensive, and approximations are often necessary. A typical source of intractability comes from the combinatorial explosion inherent in the composition of two or more tractable models (Bar-Hillel et al., 1964; Tromble and Eisner, 2006). Recently, Rush et al. (2010) have proposed a dual decomposition framework to address NLP problems in which the global score decomposes as f (y) = f1 (z1 )+f2 (z2 ), where z1 and z2 are two overlapping “views” of the output, so that Eq. 1 becomes: maximize w.r.t. s.t. f1 (z1 ) + f2 (z2 ) z1 ∈ Y1 , z2 ∈ Y2 z1 ∼ z2 . (2) Above, the notation z1 ∼ z2 means that z1 and z2 “agree on their overlaps,” and an isomorphism Y &apos; {hz1 , z2 i ∈ Y1 × Y2 |z1 ∼ z2 } is assumed. We next formalize these notions and proceed to compositions of an arbitrary number of models. Of special interest is the unexplored se"
D11-1022,W03-3023,0,0.0260678,"a log-factor, the runtimes will be linear in the number of predicates. and Bertsekas (1992) show that the ADMM algorithm may still converge when the z-updates are inexact. Hence the method may still work if the slaves are solved numerically up to some accuracy. We defer this to future investigation. 5 Experiments: Dependency Parsing We used 14 datasets with non-projective dependencies from the CoNLL-2006 and CoNLL-2008 shared tasks (Buchholz and Marsi, 2006; Surdeanu et al., 2008). We also used a projective English dataset derived from the Penn Treebank by applying the standard head rules of Yamada and Matsumoto (2003).11 We did not force the parser to output projective trees or unique roots for any of the datasets; everything is learned from the data. We trained by running 10 iterations of the cost-augmented MIRA algorithm (Crammer et al., 2006) with LP-relaxed decoding, as in Martins et al. (2009b). Following common practice (Charniak and Johnson, 2005; Carreras et al., 2008), we employed a coarse-to-fine procedure to prune away unlikely candidate arcs, as described by Koo and Collins (2010). To ensure valid parse trees at test time, we rounded fractional Larger Slaves. The only disadvantage of DDADMM in"
D11-1022,J07-2003,0,\N,Missing
D11-1044,W09-0437,0,0.0126704,"ish and UrduEnglish translation over a phrase-based baseline. We also investigate the use of unsupervised dependency parsers, reporting encouraging preliminary results. 1 Introduction Two approaches currently dominate statistical machine translation (MT) research. Phrase-based models (Koehn et al., 2003) excel at capturing local reordering phenomena and memorizing multi-word translations. Models that employ syntax or syntaxlike representations (Chiang, 2005; Galley et al., 2006; Zollmann and Venugopal, 2006; Huang et al., 2006) handle long-distance reordering better than phrase-based systems (Auli et al., 2009) but often require constraints on the formalism or rule extraction method in order to achieve computational tractability. As a result, certain instances of syntactic divergence are more naturally handled by phrase-based systems (DeNeefe et al., 2007). In this paper we present a new way of combining the advantages of phrase-based and syntax-based MT. We propose a model in which phrases are organized into a tree structure inspired by dependency To leverage standard phrase-based features alongside our novel features, we require a formalism that supports flexible feature combination and efficient"
D11-1044,C10-1007,0,0.0202267,"threshold that leaves fewer than 1000 edges in the resulting lattice. As complexity is quadratic in E, forcing E to be less than 1000 improves runtime substantially. After pruning, the lattices still contain more than 1016 paths on average and oracle BLEU scores are typically 12-15 points higher than the model-best paths. Pass 2: Parent Ranking Given a pruned lattice, we then remove some candidate dependency arcs from consideration. It is common in dependency parsing to use a coarse model to rank the top k parents for each word, and to only consider these during parsing (Martins et al., 2009; Bergsma and Cherry, 2010). Unlike string parsing, our phrase lattices impose several types of constraints on allowable arcs. For example, each node in the phrase lattice is annotated with a coverage vector—a bit vector indicating which words in the source sentence have been translated—which implies a topological ordering of the nodes. To handle constraints like these, we first use the Floyd-Warshall algorithm (Floyd, 1962) to find the best score between every pair of nodes in the lattice. This algorithm also tells us whether each edge is reachable from each other edge, allowing us to immediately prune dependency arcs"
D11-1044,D10-1117,0,0.0675722,"Missing"
D11-1044,J92-4003,0,0.0175609,"f extracted 3 For a monolingual task, Wu et al. (2009) used a shallow parser to convert lexical dependencies from a dependency parser into phrase dependencies. 477 phrase dependencies of the form hu, v, di, where u is the head phrase, v is the child phrase, and d ∈ {left, right} is the direction, we then estimate conditional probabilities p(v|u, d) using relative frequency estimation. Table 3 shows the most probable child phrases for an example parent phrase. To combat data sparseness, we perform the same procedure with each word replaced by its word cluster ID obtained from Brown clustering (Brown et al., 1992). We include a feature in the model for the sum of the scaled log-probabilities of each attachment: 0 m X i=1   max 0, C + log p(φi |φτφ (i) , d(i) (1) where d(i) = I[τφ (i) − i > 0] is the direction of the dependency arc. Although we use log-probabilities in this feature function, we first add a constant C to each to ensure they are all positive.4 The max expression protects unseen parent-child phrase dependencies from causing the score to be negative infinity. Our motivation is a desire for the features to be used to prefer one derivation over another but not to rule out a derivation compl"
D11-1044,D09-1021,0,0.0724346,"ncy syntax on phrase pairs. Shen et al. (2008) presented an extension to Hiero (Chiang, 2005) in which rules have target-side dependency syntax and therefore enable the use of a dependency language model. More recently, researchers have sought the benefits of dependency syntax while preserving the advantages of phrase-based models, such as efficiency and coverage. Galley and Manning (2009) loosened standard assumptions about dependency parsing so that the efficient left-to-right decoding procedure of phrase-based translation could be retained while a dependency language model is incorporated. Carreras and Collins (2009) presented a string-todependency system that permits non-projective dependency trees (thereby allowing a larger space of translations) and use a rule extraction procedure that includes rules for every phrase in the phrase table. We take an additional step in this direction by working with dependency grammars on the phrases themselves, thereby bringing together the structural components of phrase-based and dependency-based MT in a single model. While others have worked on combining rules from multiple syntax-based systems (Liu et al., 2009) or using posteriors from multiple models to score tran"
D11-1044,W08-0336,0,0.0420574,". Our training procedure requires two executions of MERT, and the second typically takes more iterations to converge (10 to 20 is typical) than the first due to the use of a larger feature set and increased possibility for search error due to the enlarged search space. 7 Experiments For experimental evaluation, we consider Chineseto-English (ZH-EN) and Urdu-to-English (UREN) translation and compare our system to Moses (Koehn et al., 2007). For ZH-EN, we used 303k sentence pairs from the FBIS corpus (LDC2003E14). We segmented the Chinese data using the Stanford Chinese segmenter in “CTB” mode (Chang et al., 2008), giving us 7.9M Chinese words and 9.4M English words. For UR-EN, we used parallel data from the NIST MT08 evaluation consisting of 1.2M Urdu words and 1.1M English words. We trained a baseline Moses system using default settings and features. Word alignment was performed using GIZA++ (Och and Ney, 2003) in both directions and the grow-diag-final-and heuristic was used to symmetrize the alignments. We used a max phrase length of 7 when extracting phrases. Trigram language models were estimated using the SRI language modeling toolkit (Stolcke, 2002) with modified Kneser-Ney smoothing (Chen and"
D11-1044,P05-1033,0,0.601758,"we describe a coarse-to-fine approach based on lattice dependency parsing of phrase lattices. We demonstrate performance improvements for Chinese-English and UrduEnglish translation over a phrase-based baseline. We also investigate the use of unsupervised dependency parsers, reporting encouraging preliminary results. 1 Introduction Two approaches currently dominate statistical machine translation (MT) research. Phrase-based models (Koehn et al., 2003) excel at capturing local reordering phenomena and memorizing multi-word translations. Models that employ syntax or syntaxlike representations (Chiang, 2005; Galley et al., 2006; Zollmann and Venugopal, 2006; Huang et al., 2006) handle long-distance reordering better than phrase-based systems (Auli et al., 2009) but often require constraints on the formalism or rule extraction method in order to achieve computational tractability. As a result, certain instances of syntactic divergence are more naturally handled by phrase-based systems (DeNeefe et al., 2007). In this paper we present a new way of combining the advantages of phrase-based and syntax-based MT. We propose a model in which phrases are organized into a tree structure inspired by depende"
D11-1044,P09-1053,1,0.853685,"ponding feature weights. Table 1 summarizes our notation. In modeling p(t, γ, φ, τφ , a |s, τs ), we make use of quasi-synchronous grammar (QG; Smith and Eisner, 2006). Given a source sentence and its parse, a QG induces a probabilistic monolingual grammar over sentences “inspired” by the source sentence and tree. We denote this grammar by Gs,τs ; its (weighted) language is the set of translations of s. Quasi-synchronous grammar makes no restrictions on the form of the target monolingual grammar, though dependency grammars have been used in most previous applications of QG (Wang et al., 2007; Das and Smith, 2009; Smith and Eisner, 2009), including previous work in MT (Smith and Eisner, 2006; Gimpel and Smith, 2009). We previously presented a word-based machine translation model based on a quasi-synchronous dependency grammar. However, it is well-known in the MT community that translation quality is improved when larger units are modeled. Therefore, we use a dependency grammar in which the leaves are phrases rather than words. We define a phrase dependency grammar as a model p(φ, τφ |t) over the joint space of segmentations of a sentence into phrases and dependency trees on the phrases.2 Phrase depend"
D11-1044,D07-1079,0,0.0152683,"on (MT) research. Phrase-based models (Koehn et al., 2003) excel at capturing local reordering phenomena and memorizing multi-word translations. Models that employ syntax or syntaxlike representations (Chiang, 2005; Galley et al., 2006; Zollmann and Venugopal, 2006; Huang et al., 2006) handle long-distance reordering better than phrase-based systems (Auli et al., 2009) but often require constraints on the formalism or rule extraction method in order to achieve computational tractability. As a result, certain instances of syntactic divergence are more naturally handled by phrase-based systems (DeNeefe et al., 2007). In this paper we present a new way of combining the advantages of phrase-based and syntax-based MT. We propose a model in which phrases are organized into a tree structure inspired by dependency To leverage standard phrase-based features alongside our novel features, we require a formalism that supports flexible feature combination and efficient decoding. Quasi-synchronous grammar (QG) provides this backbone (Smith and Eisner, 2006); we describe a coarse-to-fine approach for decoding within this framework, advancing substantially over earlier QG machine translation systems (Gimpel and Smith,"
D11-1044,N10-1141,0,0.020005,"a string-todependency system that permits non-projective dependency trees (thereby allowing a larger space of translations) and use a rule extraction procedure that includes rules for every phrase in the phrase table. We take an additional step in this direction by working with dependency grammars on the phrases themselves, thereby bringing together the structural components of phrase-based and dependency-based MT in a single model. While others have worked on combining rules from multiple syntax-based systems (Liu et al., 2009) or using posteriors from multiple models to score translations (DeNero et al., 2010), we are not aware of any other work that seeks to directly integrate phrase-based and syntax-based machine translation at the modeling level.1 1 Dymetman and Cancedda (2010) present a formal analy475 Model ht∗, γ ∗, φ∗, τφ∗ , a∗ i = argmax p(t, γ, φ, τφ , a |s, τs ) ht,γ,φ,τφ ,ai We use a linear model (Och and Ney, 2002): p(t, γ, φ, τφ , a |s, τs ) ∝ exp{θ > g(s, τs , t, γ, φ, τφ , a)} where g is a vector of arbitrary feature functions on the full set of structures and θ holds corresponding feature weights. Table 1 summarizes our notation. In modeling p(t, γ, φ, τφ , a |s, τs ), we make use o"
D11-1044,W10-3801,0,0.0227672,"includes rules for every phrase in the phrase table. We take an additional step in this direction by working with dependency grammars on the phrases themselves, thereby bringing together the structural components of phrase-based and dependency-based MT in a single model. While others have worked on combining rules from multiple syntax-based systems (Liu et al., 2009) or using posteriors from multiple models to score translations (DeNero et al., 2010), we are not aware of any other work that seeks to directly integrate phrase-based and syntax-based machine translation at the modeling level.1 1 Dymetman and Cancedda (2010) present a formal analy475 Model ht∗, γ ∗, φ∗, τφ∗ , a∗ i = argmax p(t, γ, φ, τφ , a |s, τs ) ht,γ,φ,τφ ,ai We use a linear model (Och and Ney, 2002): p(t, γ, φ, τφ , a |s, τs ) ∝ exp{θ > g(s, τs , t, γ, φ, τφ , a)} where g is a vector of arbitrary feature functions on the full set of structures and θ holds corresponding feature weights. Table 1 summarizes our notation. In modeling p(t, γ, φ, τφ , a |s, τs ), we make use of quasi-synchronous grammar (QG; Smith and Eisner, 2006). Given a source sentence and its parse, a QG induces a probabilistic monolingual grammar over sentences “inspired” by"
D11-1044,H05-1036,1,0.889731,"= 400. Input: tuning set D = hS, T i, initial weights λ0 for coarse model, initial weights ψ 0 for additional features in fine model Output: coarse model learned weights: λM , fine model learned weights: hλ∗ , ψ ∗ i λM ← MERT (S, T , λ0 , 100, M OSES); LMERT ← GenerateLattices (S, λM ); LFB ← FBPrune (LMERT , λM ); hλ∗ , ψ ∗ i ← MERT (LFB , T , hλM , ψ 0 i, 200, QGD EP PARSE); return λM , hλ∗ , ψ ∗ i; Algorithm 2: CoarseToFineTrain Pass 3: Lattice Dependency Parsing After completing the coarse passes, we parse using bottom-up dynamic programming based on the agenda algorithm (Nederhof, 2003; Eisner et al., 2005). We only consider arcs that survived the filtering in Pass 2. We weight agenda items by the sum of their scores and the Floyd-Warshall best path scores both from the start node of the lattice to the beginning of the item and the end of the item to any final node. This heuristic helps us to favor exploration of items that are highly likely under the phrase-based model. If the score of the partial structure can only get worse when combining it with other structures (e.g., in a PCFG), then the first time that we pop an item of type G OAL from the agenda, we are guaranteed to have the best parse."
D11-1044,C96-1058,0,0.109409,"., 2002; Macherey et al., 2008; Tromble et al., 2008). Each edge in a phrase lattice corresponds to a phrase pair and each path through the lattice corresponds to a tuple ht, γ, φ, ai for the input s. Decoding for a phrase lattice consists of finding the highest-scoring path, which is done using dynamic programming. To also maximize over τφ , we perform lattice dependency parsing, which allows us to search over the space of tuples ht, γ, φ, a, τφ i. Given the lattice and Gs,τs , lattice parsing is a straightforward generalization of the standard arc-factored dynamic programming algorithm from Eisner (1996). The lattice parsing algorithm requires O(E 2 V ) time and O(E 2 + V E) space, where E is the number of edges in the lattice and V is the number of nodes.7 Typical phrase lattices might easily contain tens of thousands of nodes and edges, making exact search prohibitively expensive for all but the smallest lattices. So, we use approximate search based on coarse-to-fine decoding. We now discuss each step of this procedure; an outline is shown as Alg. 1. Pass 1: Lattice Pruning After generating phrase lattices using a phrase-based MT system, we prune lattice edges using forward-backward pruning"
D11-1044,P09-1087,0,0.327394,"ne. Aside from QG, there have been many efforts to use dependency syntax in machine translation. Quirk et al. (2005) used a source-side dependency parser and projected automatic parses across word alignments in order to model dependency syntax on phrase pairs. Shen et al. (2008) presented an extension to Hiero (Chiang, 2005) in which rules have target-side dependency syntax and therefore enable the use of a dependency language model. More recently, researchers have sought the benefits of dependency syntax while preserving the advantages of phrase-based models, such as efficiency and coverage. Galley and Manning (2009) loosened standard assumptions about dependency parsing so that the efficient left-to-right decoding procedure of phrase-based translation could be retained while a dependency language model is incorporated. Carreras and Collins (2009) presented a string-todependency system that permits non-projective dependency trees (thereby allowing a larger space of translations) and use a rule extraction procedure that includes rules for every phrase in the phrase table. We take an additional step in this direction by working with dependency grammars on the phrases themselves, thereby bringing together th"
D11-1044,N10-1140,0,0.0403402,"s described in §4. In our experiments, we compare our QPDG system (lattice parsing on each lattice) to the Moses baseline (finding the best path through each lattice). The conventional wisdom holds that hierarchical phrase-based translation (Chiang, 2005) performs better than phrasebased translation for language pairs that require large amounts of reordering, such as ZH-EN and UR-EN. However, researchers have shown that this performance gap diminishes when using a larger distortion limit (Zollmann et al., 2008) and may disappear entirely when using a lexicalized reordering model (Lopez, 2008; Galley and Manning, 2010). So, we increase the Moses distortion limit from 6 (the default) to 10 and use Moses’ default lexicalized reordering model (Koehn et al., 2005). We parsed the Chinese text using the Stanford parser (Levy and Manning, 2003) and the English text using TurboParser (Martins et al., 2009). We note that computing our features requires parsing the target (English) side of the parallel text, but not the source side. We only need to parse the source side of the tuning and test sets, and the only features that look at the source-side parse are those from §4.3. To obtain Brown clusters for the target tr"
D11-1044,P06-1121,0,0.0482079,"coarse-to-fine approach based on lattice dependency parsing of phrase lattices. We demonstrate performance improvements for Chinese-English and UrduEnglish translation over a phrase-based baseline. We also investigate the use of unsupervised dependency parsers, reporting encouraging preliminary results. 1 Introduction Two approaches currently dominate statistical machine translation (MT) research. Phrase-based models (Koehn et al., 2003) excel at capturing local reordering phenomena and memorizing multi-word translations. Models that employ syntax or syntaxlike representations (Chiang, 2005; Galley et al., 2006; Zollmann and Venugopal, 2006; Huang et al., 2006) handle long-distance reordering better than phrase-based systems (Auli et al., 2009) but often require constraints on the formalism or rule extraction method in order to achieve computational tractability. As a result, certain instances of syntactic divergence are more naturally handled by phrase-based systems (DeNeefe et al., 2007). In this paper we present a new way of combining the advantages of phrase-based and syntax-based MT. We propose a model in which phrases are organized into a tree structure inspired by dependency To leverage stand"
D11-1044,D09-1023,1,0.861443,"are vertices, our trees have phrases as vertices. We describe a simple heuristic to extract phrase dependencies from an aligned parallel corpus parsed on the target side, and use them to compute target-side tree features. We define additional string-to-tree features and, if a source-side dependency parser is available, tree-to-tree features to capture properties of how phrase dependencies interact with reordering. We present a quasi-synchronous dependency grammar (Smith and Eisner, 2006) for machine translation in which the leaves of the tree are phrases rather than words as in previous work (Gimpel and Smith, 2009). This formulation allows us to combine structural components of phrase-based and syntax-based MT in a single model. We describe a method of extracting phrase dependencies from parallel text using a target-side dependency parser. For decoding, we describe a coarse-to-fine approach based on lattice dependency parsing of phrase lattices. We demonstrate performance improvements for Chinese-English and UrduEnglish translation over a phrase-based baseline. We also investigate the use of unsupervised dependency parsers, reporting encouraging preliminary results. 1 Introduction Two approaches current"
D11-1044,2006.amta-papers.8,0,0.164723,"y parsing of phrase lattices. We demonstrate performance improvements for Chinese-English and UrduEnglish translation over a phrase-based baseline. We also investigate the use of unsupervised dependency parsers, reporting encouraging preliminary results. 1 Introduction Two approaches currently dominate statistical machine translation (MT) research. Phrase-based models (Koehn et al., 2003) excel at capturing local reordering phenomena and memorizing multi-word translations. Models that employ syntax or syntaxlike representations (Chiang, 2005; Galley et al., 2006; Zollmann and Venugopal, 2006; Huang et al., 2006) handle long-distance reordering better than phrase-based systems (Auli et al., 2009) but often require constraints on the formalism or rule extraction method in order to achieve computational tractability. As a result, certain instances of syntactic divergence are more naturally handled by phrase-based systems (DeNeefe et al., 2007). In this paper we present a new way of combining the advantages of phrase-based and syntax-based MT. We propose a model in which phrases are organized into a tree structure inspired by dependency To leverage standard phrase-based features alongside our novel featu"
D11-1044,P04-1061,0,0.138389,"Missing"
D11-1044,N03-1017,0,0.356345,"d syntax-based MT in a single model. We describe a method of extracting phrase dependencies from parallel text using a target-side dependency parser. For decoding, we describe a coarse-to-fine approach based on lattice dependency parsing of phrase lattices. We demonstrate performance improvements for Chinese-English and UrduEnglish translation over a phrase-based baseline. We also investigate the use of unsupervised dependency parsers, reporting encouraging preliminary results. 1 Introduction Two approaches currently dominate statistical machine translation (MT) research. Phrase-based models (Koehn et al., 2003) excel at capturing local reordering phenomena and memorizing multi-word translations. Models that employ syntax or syntaxlike representations (Chiang, 2005; Galley et al., 2006; Zollmann and Venugopal, 2006; Huang et al., 2006) handle long-distance reordering better than phrase-based systems (Auli et al., 2009) but often require constraints on the formalism or rule extraction method in order to achieve computational tractability. As a result, certain instances of syntactic divergence are more naturally handled by phrase-based systems (DeNeefe et al., 2007). In this paper we present a new way"
D11-1044,2005.iwslt-1.8,0,0.0246777,"gh each lattice). The conventional wisdom holds that hierarchical phrase-based translation (Chiang, 2005) performs better than phrasebased translation for language pairs that require large amounts of reordering, such as ZH-EN and UR-EN. However, researchers have shown that this performance gap diminishes when using a larger distortion limit (Zollmann et al., 2008) and may disappear entirely when using a lexicalized reordering model (Lopez, 2008; Galley and Manning, 2010). So, we increase the Moses distortion limit from 6 (the default) to 10 and use Moses’ default lexicalized reordering model (Koehn et al., 2005). We parsed the Chinese text using the Stanford parser (Levy and Manning, 2003) and the English text using TurboParser (Martins et al., 2009). We note that computing our features requires parsing the target (English) side of the parallel text, but not the source side. We only need to parse the source side of the tuning and test sets, and the only features that look at the source-side parse are those from §4.3. To obtain Brown clusters for the target tree features in §4.1, we used code from Liang (2005).8 We induced 100 clusters from the English side of the parallel corpus concatenated with 10M"
D11-1044,P07-2045,0,0.0179078,"nition of QG. The alignment variable in QG links target tree nodes to source tree nodes. However, we never commit to a source phrase dependency tree, instead using a source lexical dependency tree output by a dependency parser, so our alignment variable a is a function from target tree nodes (phrases in φ) to source phrases in γ, which might not be source tree nodes. The features in our model may consider a large number of source phrase dependency trees as long as they are consistent with τs . 4 Features Our model contains all of the standard phrase-based features found in systems like Moses (Koehn et al., 2007), including four phrase table probability features, a phrase penalty feature, an n-gram language model, a distortion cost, six lexicalized reordering features, and a word penalty feature. We now describe in detail the additional features 476 $ ← said : $ ← said that $ ← is a $ ← will be $ ← it is $ ← this is $ ← we must the → united states the → development of the two → countries he → said : $ ← he said : $ ← we should $ ← has been - us → relations $ ← he said cross - strait → relations $ ← pointed out that , and → is the chinese → government $ ← is the $ ← said , one - china → principle sino"
D11-1044,P03-1056,0,0.0969451,"sed translation (Chiang, 2005) performs better than phrasebased translation for language pairs that require large amounts of reordering, such as ZH-EN and UR-EN. However, researchers have shown that this performance gap diminishes when using a larger distortion limit (Zollmann et al., 2008) and may disappear entirely when using a lexicalized reordering model (Lopez, 2008; Galley and Manning, 2010). So, we increase the Moses distortion limit from 6 (the default) to 10 and use Moses’ default lexicalized reordering model (Koehn et al., 2005). We parsed the Chinese text using the Stanford parser (Levy and Manning, 2003) and the English text using TurboParser (Martins et al., 2009). We note that computing our features requires parsing the target (English) side of the parallel text, but not the source side. We only need to parse the source side of the tuning and test sets, and the only features that look at the source-side parse are those from §4.3. To obtain Brown clusters for the target tree features in §4.1, we used code from Liang (2005).8 We induced 100 clusters from the English side of the parallel corpus concatenated with 10M words of randomly-selected Gigaword sentences. Only words that appeared at lea"
D11-1044,P09-1065,0,0.0164493,"a dependency language model is incorporated. Carreras and Collins (2009) presented a string-todependency system that permits non-projective dependency trees (thereby allowing a larger space of translations) and use a rule extraction procedure that includes rules for every phrase in the phrase table. We take an additional step in this direction by working with dependency grammars on the phrases themselves, thereby bringing together the structural components of phrase-based and dependency-based MT in a single model. While others have worked on combining rules from multiple syntax-based systems (Liu et al., 2009) or using posteriors from multiple models to score translations (DeNero et al., 2010), we are not aware of any other work that seeks to directly integrate phrase-based and syntax-based machine translation at the modeling level.1 1 Dymetman and Cancedda (2010) present a formal analy475 Model ht∗, γ ∗, φ∗, τφ∗ , a∗ i = argmax p(t, γ, φ, τφ , a |s, τs ) ht,γ,φ,τφ ,ai We use a linear model (Och and Ney, 2002): p(t, γ, φ, τφ , a |s, τs ) ∝ exp{θ > g(s, τs , t, γ, φ, τφ , a)} where g is a vector of arbitrary feature functions on the full set of structures and θ holds corresponding feature weights. T"
D11-1044,C08-1064,0,0.0133262,"QPDG features described in §4. In our experiments, we compare our QPDG system (lattice parsing on each lattice) to the Moses baseline (finding the best path through each lattice). The conventional wisdom holds that hierarchical phrase-based translation (Chiang, 2005) performs better than phrasebased translation for language pairs that require large amounts of reordering, such as ZH-EN and UR-EN. However, researchers have shown that this performance gap diminishes when using a larger distortion limit (Zollmann et al., 2008) and may disappear entirely when using a lexicalized reordering model (Lopez, 2008; Galley and Manning, 2010). So, we increase the Moses distortion limit from 6 (the default) to 10 and use Moses’ default lexicalized reordering model (Koehn et al., 2005). We parsed the Chinese text using the Stanford parser (Levy and Manning, 2003) and the English text using TurboParser (Martins et al., 2009). We note that computing our features requires parsing the target (English) side of the parallel text, but not the source side. We only need to parse the source side of the tuning and test sets, and the only features that look at the source-side parse are those from §4.3. To obtain Brown"
D11-1044,D08-1076,0,0.0253598,"ce s and its parse τs , i.e., finding the most probable derivation under the s/τs -specific grammar Gs,τs . We follow Gimpel and Smith (2009) in constructing a lattice to represent Gs,τs and using lattice parsing to search for the best derivation, but we construct the lattice differently and employ a coarse-tofine strategy (Petrov, 2009) to speed up decoding. It has become common in recent years for MT researchers to exploit efficient data structures for encoding concise representations of the pruned search space of the model, such as phrase lattices for phrase-based MT (Ueffing et al., 2002; Macherey et al., 2008; Tromble et al., 2008). Each edge in a phrase lattice corresponds to a phrase pair and each path through the lattice corresponds to a tuple ht, γ, φ, ai for the input s. Decoding for a phrase lattice consists of finding the highest-scoring path, which is done using dynamic programming. To also maximize over τφ , we perform lattice dependency parsing, which allows us to search over the space of tuples ht, γ, φ, a, τφ i. Given the lattice and Gs,τs , lattice parsing is a straightforward generalization of the standard arc-factored dynamic programming algorithm from Eisner (1996). The lattice par"
D11-1044,J93-2004,0,0.0369839,"Missing"
D11-1044,P09-1039,1,0.943789,"find the most liberal threshold that leaves fewer than 1000 edges in the resulting lattice. As complexity is quadratic in E, forcing E to be less than 1000 improves runtime substantially. After pruning, the lattices still contain more than 1016 paths on average and oracle BLEU scores are typically 12-15 points higher than the model-best paths. Pass 2: Parent Ranking Given a pruned lattice, we then remove some candidate dependency arcs from consideration. It is common in dependency parsing to use a coarse model to rank the top k parents for each word, and to only consider these during parsing (Martins et al., 2009; Bergsma and Cherry, 2010). Unlike string parsing, our phrase lattices impose several types of constraints on allowable arcs. For example, each node in the phrase lattice is annotated with a coverage vector—a bit vector indicating which words in the source sentence have been translated—which implies a topological ordering of the nodes. To handle constraints like these, we first use the Floyd-Warshall algorithm (Floyd, 1962) to find the best score between every pair of nodes in the lattice. This algorithm also tells us whether each edge is reachable from each other edge, allowing us to immedia"
D11-1044,J03-1006,0,0.0670421,", α = 100, and β = 400. Input: tuning set D = hS, T i, initial weights λ0 for coarse model, initial weights ψ 0 for additional features in fine model Output: coarse model learned weights: λM , fine model learned weights: hλ∗ , ψ ∗ i λM ← MERT (S, T , λ0 , 100, M OSES); LMERT ← GenerateLattices (S, λM ); LFB ← FBPrune (LMERT , λM ); hλ∗ , ψ ∗ i ← MERT (LFB , T , hλM , ψ 0 i, 200, QGD EP PARSE); return λM , hλ∗ , ψ ∗ i; Algorithm 2: CoarseToFineTrain Pass 3: Lattice Dependency Parsing After completing the coarse passes, we parse using bottom-up dynamic programming based on the agenda algorithm (Nederhof, 2003; Eisner et al., 2005). We only consider arcs that survived the filtering in Pass 2. We weight agenda items by the sum of their scores and the Floyd-Warshall best path scores both from the start node of the lattice to the beginning of the item and the end of the item to any final node. This heuristic helps us to favor exploration of items that are highly likely under the phrase-based model. If the score of the partial structure can only get worse when combining it with other structures (e.g., in a PCFG), then the first time that we pop an item of type G OAL from the agenda, we are guaranteed t"
D11-1044,P02-1038,0,0.0870009,", thereby bringing together the structural components of phrase-based and dependency-based MT in a single model. While others have worked on combining rules from multiple syntax-based systems (Liu et al., 2009) or using posteriors from multiple models to score translations (DeNero et al., 2010), we are not aware of any other work that seeks to directly integrate phrase-based and syntax-based machine translation at the modeling level.1 1 Dymetman and Cancedda (2010) present a formal analy475 Model ht∗, γ ∗, φ∗, τφ∗ , a∗ i = argmax p(t, γ, φ, τφ , a |s, τs ) ht,γ,φ,τφ ,ai We use a linear model (Och and Ney, 2002): p(t, γ, φ, τφ , a |s, τs ) ∝ exp{θ > g(s, τs , t, γ, φ, τφ , a)} where g is a vector of arbitrary feature functions on the full set of structures and θ holds corresponding feature weights. Table 1 summarizes our notation. In modeling p(t, γ, φ, τφ , a |s, τs ), we make use of quasi-synchronous grammar (QG; Smith and Eisner, 2006). Given a source sentence and its parse, a QG induces a probabilistic monolingual grammar over sentences “inspired” by the source sentence and tree. We denote this grammar by Gs,τs ; its (weighted) language is the set of translations of s. Quasi-synchronous grammar m"
D11-1044,J03-1002,0,0.00315822,"ation, we consider Chineseto-English (ZH-EN) and Urdu-to-English (UREN) translation and compare our system to Moses (Koehn et al., 2007). For ZH-EN, we used 303k sentence pairs from the FBIS corpus (LDC2003E14). We segmented the Chinese data using the Stanford Chinese segmenter in “CTB” mode (Chang et al., 2008), giving us 7.9M Chinese words and 9.4M English words. For UR-EN, we used parallel data from the NIST MT08 evaluation consisting of 1.2M Urdu words and 1.1M English words. We trained a baseline Moses system using default settings and features. Word alignment was performed using GIZA++ (Och and Ney, 2003) in both directions and the grow-diag-final-and heuristic was used to symmetrize the alignments. We used a max phrase length of 7 when extracting phrases. Trigram language models were estimated using the SRI language modeling toolkit (Stolcke, 2002) with modified Kneser-Ney smoothing (Chen and Goodman, 1998). To estimate language models for each language pair, we used the English side of the parallel corpus concatenated with 200M words of randomly-selected sentences from the Gigaword v4 corpus (excluding the NY Times and LA Times). We used this baseline Moses system to generate phrase lattices"
D11-1044,P03-1021,0,0.0163051,"making this property no longer hold; as a result, G OAL items may be popped out of order from the agenda. Therefore, we use an approximation, simply popping G G OAL items from the agenda and then stopping. The items are sorted by their scores and the best is returned by the decoder (or the k best in the case of MERT). In our experiments, we set G = 4000. The combined strategy yields average decoding times in the range of 30 seconds per sentence, which is comparable to other syntax-based MT systems. 6 Training For tuning the coarse and fine parameters, we use minimum error rate training (MERT; Och, 2003) in a procedure shown as Alg. 2. We first use MERT to train parameters for the coarse phrase-based model used to generate phrase lattices. Then, after generating the lattices, we prune them and run MERT a 480 second time to tune parameters of the fine model, which includes all phrase-based and QPDG parameters. The arguments to MERT are a vector of source sentences (or lattices), a vector of target sentences, the initial parameter values, the size of the k-best list, and finally the decoder. We initialize λ to the default Moses feature weights and for ψ we initialize the two target phrase depen"
D11-1044,2001.mtsummit-papers.68,0,0.0892975,"Missing"
D11-1044,P05-1034,0,0.0590897,"that Related Work We previously applied quasi-synchronous grammar to machine translation (Gimpel and Smith, 2009), but that system performed translation fundamentally at the word level. Here we generalize that model to function on phrases, enabling a tighter coupling between the phrase segmentation and syntactic structures. We also present a decoder efficient enough to scale to large data sets and present performance improvements in large-scale experiments over a stateof-the-art phrase-based baseline. Aside from QG, there have been many efforts to use dependency syntax in machine translation. Quirk et al. (2005) used a source-side dependency parser and projected automatic parses across word alignments in order to model dependency syntax on phrase pairs. Shen et al. (2008) presented an extension to Hiero (Chiang, 2005) in which rules have target-side dependency syntax and therefore enable the use of a dependency language model. More recently, researchers have sought the benefits of dependency syntax while preserving the advantages of phrase-based models, such as efficiency and coverage. Galley and Manning (2009) loosened standard assumptions about dependency parsing so that the efficient left-to-right"
D11-1044,P08-1066,0,0.287817,"ly at the word level. Here we generalize that model to function on phrases, enabling a tighter coupling between the phrase segmentation and syntactic structures. We also present a decoder efficient enough to scale to large data sets and present performance improvements in large-scale experiments over a stateof-the-art phrase-based baseline. Aside from QG, there have been many efforts to use dependency syntax in machine translation. Quirk et al. (2005) used a source-side dependency parser and projected automatic parses across word alignments in order to model dependency syntax on phrase pairs. Shen et al. (2008) presented an extension to Hiero (Chiang, 2005) in which rules have target-side dependency syntax and therefore enable the use of a dependency language model. More recently, researchers have sought the benefits of dependency syntax while preserving the advantages of phrase-based models, such as efficiency and coverage. Galley and Manning (2009) loosened standard assumptions about dependency parsing so that the efficient left-to-right decoding procedure of phrase-based translation could be retained while a dependency language model is incorporated. Carreras and Collins (2009) presented a string"
D11-1044,W06-3104,0,0.636635,"eristy Pittsburgh, PA 15213, USA {kgimpel,nasmith}@cs.cmu.edu Abstract syntax. Instead of standard dependency trees in which words are vertices, our trees have phrases as vertices. We describe a simple heuristic to extract phrase dependencies from an aligned parallel corpus parsed on the target side, and use them to compute target-side tree features. We define additional string-to-tree features and, if a source-side dependency parser is available, tree-to-tree features to capture properties of how phrase dependencies interact with reordering. We present a quasi-synchronous dependency grammar (Smith and Eisner, 2006) for machine translation in which the leaves of the tree are phrases rather than words as in previous work (Gimpel and Smith, 2009). This formulation allows us to combine structural components of phrase-based and syntax-based MT in a single model. We describe a method of extracting phrase dependencies from parallel text using a target-side dependency parser. For decoding, we describe a coarse-to-fine approach based on lattice dependency parsing of phrase lattices. We demonstrate performance improvements for Chinese-English and UrduEnglish translation over a phrase-based baseline. We also inves"
D11-1044,D09-1086,0,0.148772,"ts. Table 1 summarizes our notation. In modeling p(t, γ, φ, τφ , a |s, τs ), we make use of quasi-synchronous grammar (QG; Smith and Eisner, 2006). Given a source sentence and its parse, a QG induces a probabilistic monolingual grammar over sentences “inspired” by the source sentence and tree. We denote this grammar by Gs,τs ; its (weighted) language is the set of translations of s. Quasi-synchronous grammar makes no restrictions on the form of the target monolingual grammar, though dependency grammars have been used in most previous applications of QG (Wang et al., 2007; Das and Smith, 2009; Smith and Eisner, 2009), including previous work in MT (Smith and Eisner, 2006; Gimpel and Smith, 2009). We previously presented a word-based machine translation model based on a quasi-synchronous dependency grammar. However, it is well-known in the MT community that translation quality is improved when larger units are modeled. Therefore, we use a dependency grammar in which the leaves are phrases rather than words. We define a phrase dependency grammar as a model p(φ, τφ |t) over the joint space of segmentations of a sentence into phrases and dependency trees on the phrases.2 Phrase dependency grammars sis of the"
D11-1044,N03-1033,0,0.100675,"Missing"
D11-1044,D08-1065,0,0.252728,"i.e., finding the most probable derivation under the s/τs -specific grammar Gs,τs . We follow Gimpel and Smith (2009) in constructing a lattice to represent Gs,τs and using lattice parsing to search for the best derivation, but we construct the lattice differently and employ a coarse-tofine strategy (Petrov, 2009) to speed up decoding. It has become common in recent years for MT researchers to exploit efficient data structures for encoding concise representations of the pruned search space of the model, such as phrase lattices for phrase-based MT (Ueffing et al., 2002; Macherey et al., 2008; Tromble et al., 2008). Each edge in a phrase lattice corresponds to a phrase pair and each path through the lattice corresponds to a tuple ht, γ, φ, ai for the input s. Decoding for a phrase lattice consists of finding the highest-scoring path, which is done using dynamic programming. To also maximize over τφ , we perform lattice dependency parsing, which allows us to search over the space of tuples ht, γ, φ, a, τφ i. Given the lattice and Gs,τs , lattice parsing is a straightforward generalization of the standard arc-factored dynamic programming algorithm from Eisner (1996). The lattice parsing algorithm requires"
D11-1044,W02-1021,0,0.13366,"ntages of phrase-based and syntax-based MT. We propose a model in which phrases are organized into a tree structure inspired by dependency To leverage standard phrase-based features alongside our novel features, we require a formalism that supports flexible feature combination and efficient decoding. Quasi-synchronous grammar (QG) provides this backbone (Smith and Eisner, 2006); we describe a coarse-to-fine approach for decoding within this framework, advancing substantially over earlier QG machine translation systems (Gimpel and Smith, 2009). The decoder involves generating a phrase lattice (Ueffing et al., 2002) in a coarse pass using a phrase-based model, followed by lattice dependency parsing of the phrase lattice. This approach allows us to feasibly explore the combined search space of segmentations, phrase alignments, and target phrase dependency trees. Our experiments demonstrate an average improvement of +0.65 BLEU in Chinese-English translation across three test sets and an improvement of +0.75 BLEU in Urdu-English translation over a phrase-based baseline. We also describe experiments in which we replace supervised dependency parsers with unsupervised parsers, reporting promising results: usin"
D11-1044,D07-1003,1,0.897041,"and θ holds corresponding feature weights. Table 1 summarizes our notation. In modeling p(t, γ, φ, τφ , a |s, τs ), we make use of quasi-synchronous grammar (QG; Smith and Eisner, 2006). Given a source sentence and its parse, a QG induces a probabilistic monolingual grammar over sentences “inspired” by the source sentence and tree. We denote this grammar by Gs,τs ; its (weighted) language is the set of translations of s. Quasi-synchronous grammar makes no restrictions on the form of the target monolingual grammar, though dependency grammars have been used in most previous applications of QG (Wang et al., 2007; Das and Smith, 2009; Smith and Eisner, 2009), including previous work in MT (Smith and Eisner, 2006; Gimpel and Smith, 2009). We previously presented a word-based machine translation model based on a quasi-synchronous dependency grammar. However, it is well-known in the MT community that translation quality is improved when larger units are modeled. Therefore, we use a dependency grammar in which the leaves are phrases rather than words. We define a phrase dependency grammar as a model p(φ, τφ |t) over the joint space of segmentations of a sentence into phrases and dependency trees on the ph"
D11-1044,D09-1159,0,0.233739,"n} → {0, . . . , n} τφ : {1, . . . , m0 } → {0, . . . , m0 } a : {1, . . . , m0 } → {1, . . . , n0 } θ = hλ, ψi source language sentence target language sentence, translation of s segmentation of s into phrases segmentation of t into phrases dependency tree on source words s, where τs (i) is the index of the parent of word si (0 is the root, $) dependency tree on target phrases φ, where τφ (i) is the index of the parent of phrase φi one-to-one alignment from phrases in φ to phrases in γ parameters of the full model (λ = phrase-based, ψ = QPDG) Table 1: Key notation. have recently been used by Wu et al. (2009) for feature extraction for opinion mining. When used for translation modeling, they allow us to capture phenomena like local reordering and idiomatic translations within each phrase as well as long-distance relationships among the phrases in a sentence. We then define a quasi-synchronous phrase dependency grammar (QPDG) as a conditional model p(t, γ, φ, τφ , a |s, τs ) that induces a probabilistic monolingual phrase dependency grammar over sentences inspired by the source sentence and (lexical) dependency tree. The source and target sentences are segmented into phrases and the phrases are ali"
D11-1044,W06-3119,0,0.062679,"ach based on lattice dependency parsing of phrase lattices. We demonstrate performance improvements for Chinese-English and UrduEnglish translation over a phrase-based baseline. We also investigate the use of unsupervised dependency parsers, reporting encouraging preliminary results. 1 Introduction Two approaches currently dominate statistical machine translation (MT) research. Phrase-based models (Koehn et al., 2003) excel at capturing local reordering phenomena and memorizing multi-word translations. Models that employ syntax or syntaxlike representations (Chiang, 2005; Galley et al., 2006; Zollmann and Venugopal, 2006; Huang et al., 2006) handle long-distance reordering better than phrase-based systems (Auli et al., 2009) but often require constraints on the formalism or rule extraction method in order to achieve computational tractability. As a result, certain instances of syntactic divergence are more naturally handled by phrase-based systems (DeNeefe et al., 2007). In this paper we present a new way of combining the advantages of phrase-based and syntax-based MT. We propose a model in which phrases are organized into a tree structure inspired by dependency To leverage standard phrase-based features alon"
D11-1044,C08-1144,0,0.136981,"Missing"
D11-1044,P02-1040,0,\N,Missing
D11-1055,J96-1002,0,0.00963011,"in this collection, complete data for its papers postdate the end of the last training period, so we chose to exclude it from our dataset. 3 Model Our forecasting approach is based on generalized linear models for regression and classification. The models are trained with an `2 -penalty, often called a “ridge” model (Hoerl and Kennard, 1970).3 For the NBER data, where (log) number of downloads is nearly a continuous measure, we use linear regression. For the ACL data, where response is the binary cited-or-not variable we use logistic regression, often referred to as a “maximum entropy” model (Berger et al., 1996) or a log-linear model. We briefly review the class of models. Then, we describe a time series model appropriate for time series data. 3.1 3.2 Generalized Linear Models Consider a model that predicts a response y given a vector input x = hx1 , . . . , xd i ∈ Rd . Our models are linear functions of x and parameterized by the vector β. Given a corpus of M document features, X, and responses Y , we estimate: ˆ = argmin R(β) + L(β, X, Y ) β β where there is a feature vector β c for each class c ∈ C. Under this interpretation, parameter estimation is maximum a posteriori inference for β, and R(β) i"
D11-1055,D08-1038,0,0.179494,"●● ●● ● ●● ● ●● ● ● ● ● ● ● ● ●● ● ●● ● ● ●● ● ● ●● ● ●● ● ● ●● ● ●●●● ● ● ● ● ● ● 1990 2000 cur frequently in conference session titles. On the right are term frequencies (with smoothing, since year-to-year frequencies are bumpy). Most terms decline over time. On the left, by contrast, are the weights learned by our time series model. They tell a very different story: for example, parsing has shown a definite increase in interest, while interest in grammars (e.g., formalisms) has declined somewhat. These trends have face validity, giving credence to our analysis; they also broadly agree with Hall et al. (2008). Authors The regression method also allows analysis of author influence, since we fit a coefficient for each of the authors in the ACL dataset. Figure 6(a) addresses the following question: do prolific authors get cited more often, even after accounting for the content of their papers?13 The effect is present but relatively small according to our model: the total number of papers co-authored by an author has a weak correlation to the author’s citation prediction coefficient (τ = 0.16). Next, does the model provide more information than the simple citation probability of an author? Figure 6(b)"
D11-1055,N10-1038,1,0.872504,"ices of train/test separation, and we elucidate these issues. In addition, we introduce a new regularization technique that leverages the intuition that the relationship between observable features and response should evolve smoothly over time. This regularizer allows the learner to rely more strongly on more recent evidence, while taking into account a long history of training data. Our time series-inspired regularizer is computationally efficient in learning and is a significant advance over earlier text-driven forecasting models that ignore the time variable altogether (Kogan et al., 2009; Joshi et al., 2010). We evaluate our approaches in two novel experimental settings: predicting downloads of economics articles and predicting citation of papers at ACL conferences. Our approaches substantially outperProceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 594–604, c Edinburgh, Scotland, UK, July 27–31, 2011. 2011 Association for Computational Linguistics # docs. 2500 1500 # docs. 0 0 4 log(# downloads) 9 0 # citations 18 Figure 1: Left: the distribution of log download counts for papers in the NBER dataset one year after posting. Right: the distribution of wi"
D11-1055,N09-1031,1,0.710307,"the usual best practices of train/test separation, and we elucidate these issues. In addition, we introduce a new regularization technique that leverages the intuition that the relationship between observable features and response should evolve smoothly over time. This regularizer allows the learner to rely more strongly on more recent evidence, while taking into account a long history of training data. Our time series-inspired regularizer is computationally efficient in learning and is a significant advance over earlier text-driven forecasting models that ignore the time variable altogether (Kogan et al., 2009; Joshi et al., 2010). We evaluate our approaches in two novel experimental settings: predicting downloads of economics articles and predicting citation of papers at ACL conferences. Our approaches substantially outperProceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 594–604, c Edinburgh, Scotland, UK, July 27–31, 2011. 2011 Association for Computational Linguistics # docs. 2500 1500 # docs. 0 0 4 log(# downloads) 9 0 # citations 18 Figure 1: Left: the distribution of log download counts for papers in the NBER dataset one year after posting. Right: t"
D11-1055,C08-1087,0,0.0401132,"ike authors, topical categories, and publication venues. 1 Introduction Written communication is an essential component of the complex social phenomenon of science. As such, natural language processing is well-positioned to provide tools for understanding the scientific process, by analyzing the textual artifacts (papers, proceedings, etc.) that it produces. This paper is about modeling collections of scientific documents to understand how their textual content relates to how a scientific community responds to them. While past work has often focused on citation structure (Borner et al., 2003; Qazvinian and Radev, 2008), our emphasis is on the text content, following Ramage et al. (2010) and Gerrish and Blei (2010). Instead of task-independent exploratory data analysis (e.g., topic modeling) or multi-document sum594 Noah A. Smith School of Computer Science Carnegie Mellon University Pittsburgh, PA 15213, USA nasmith@cs.cmu.edu marization, we consider supervised models of the collective response of a scientific community to a published article. There are many measures of impact of a scientific paper; ours come from direct measurements of the number of downloads (from an established website where prominent eco"
D11-1055,W09-3607,0,0.114182,"Missing"
D11-1139,W06-2920,0,0.015104,"Missing"
D11-1139,W02-1001,0,0.649601,"king, entity recognition, and dependency parsing) show gains in performance, compactness, and model interpretability. 1 Introduction Models for structured outputs are in demand across natural language processing, with applications in information extraction, parsing, and machine translation. State-of-the-art models usually involve linear combinations of features and are trained discriminatively; examples are conditional random fields (Lafferty et al., 2001), structured support vector machines (Altun et al., 2003; Taskar et al., 2003; Tsochantaridis et al., 2004), and the structured perceptron (Collins, 2002a). In all these cases, the underlying optimization problems differ only in the choice of loss function; choosing among them has usually a small impact on predictive performance. In this paper, we are concerned with model selection: which features should be used to define the prediction score? The fact that models with few features (“sparse” models) are desirable for several 1500 reasons (compactness, interpretability, good generalization) has stimulated much research work which has produced a wide variety of methods (Della Pietra et al., 1997; Guyon and Elisseeff, 2003; McCallum, 2003). Our f"
D11-1139,P11-1137,1,0.784,"eatures individually), or to make the inclusion of some features depend on the inclusion of other features. We achieve the goal stated above by employing regularizers which promote structured sparsity. Such regularizers are able to encode prior knowledge and guide the selection of features by modeling the structure of the feature space. Lately, this type of regularizers has received a lot of attention in computer vision, signal processing, and computational biology (Zhao et al., 2009; Kim and Xing, 2010; Jenatton et al., 2009; Obozinski et al., 2010; Jenatton et al., 2010; Bach et al., 2011). Eisenstein et al. (2011) employed structured sparsity in computational sociolinguistics. However, none of these works have addressed structured prediction. Here, we combine these two levels of structure: structure in the output space, and structure in the feature space. The result is a framework that allows building structured predictors with high predictive power, while reducing manual feature engineering. We obtain models that are interpretable, accurate, and often much more compact than L2 -regularized ones. Compared with L1 -regularized models, ours are often more accurate and yield faster runtime. Proceedings of"
D11-1139,P07-1104,0,0.0101358,"sparsity. While this has been a topic of intense research in signal processing and 1501 λ 2 2 kθk2 = λ 2 PD 2 d=1 θd ; (6) • L1 -regularization (Kazama and Tsujii, 2003; Goodman, 2004): 1 ΩL τ (θ) , τ kθk1 = τ PD d=1 |θd |. (7) The latter is known as “Lasso,” as popularized by Tibshirani (1996) in the context of sparse regression. In the two cases above, λ and τ are nonnegative coefficients controlling the intensity of the reg2 ularization. ΩL λ usually leads to easier optimization 1 and robust performance; ΩL τ encourages sparser models, where only a few features receive nonzero weights; see Gao et al. (2007) for an empirical comparison. More recently, Petrov and Klein (2008b) applied L1 regularization for structure learning in phrase-based parsing; a comparison with L2 appears in Petrov and Klein (2008a). Elastic nets interpolate between L1 and L2 , having been proposed by Zou and Hastie (2005) and used by Lavergne et al. (2010) to regularize CRFs. Neither of the regularizers just described “looks” at the structure of the feature space, since they all treat each dimension independently—we call them unstructured regularizers, as opposed to the structured ones that we next describe. 3 Structured Sp"
D11-1139,N04-1039,0,0.490061,"Missing"
D11-1139,W03-1018,0,0.365199,"Missing"
D11-1139,P10-1052,0,0.0483335,"Missing"
D11-1139,D10-1004,1,0.232465,"e output is y; our goal is to learn θ with small expected cost on unseen data. To achieve this goal, linear models are usually trained by solving a problem of the form b = arg minθ Ω(θ) + 1 PN L(θ, xi , yi ), (2) θ i=1 N LCRF (θ, x, y) = − log Pθ (y|x), computational biology (Jenatton et al., 2009; Liu and Ye, 2010; Bach et al., 2011), it has not yet received much attention in the NLP community, where the choice of regularization for supervised learning has essentially been limited to the following: (5) Empirical comparison among these loss functions can be found in the literature (see, e.g., Martins et al., 2010, who also consider interpolations of the losses above). In practice, it has been observed that the choice of loss has far less impact than the model design and choice of features. Hence, in this paper, we focus our attention on the regularization term in Eq. 2. We specifically address ways in which this term can be used to help design the model by promoting structured sparsity. While this has been a topic of intense research in signal processing and 1501 λ 2 2 kθk2 = λ 2 PD 2 d=1 θd ; (6) • L1 -regularization (Kazama and Tsujii, 2003; Goodman, 2004): 1 ΩL τ (θ) , τ kθk1 = τ PD d=1 |θd |. (7)"
D11-1139,H05-1066,0,0.0347046,"on-overlapping group-Lasso, coarse-to-fine group-Lasso (C2F), and a filter-based method based on information gain for selecting feature templates in multilingual dependency parsing. The x-axis is the total number of features at different regularization levels, and the y-axis is the unlabeled attachment score. The plots illustrate how accurate the parsers are as a function of the model sparsity achieved, for each method. The standard Lasso (which does not select templates, but individual features) is also shown for comparison. We use arc-factored models, for which exact inference is tractable (McDonald et al., 2005). We defined M = 684 feature templates for each candidate arc by conjoining the words, shapes, lemmas, and POS of the head and the modifier, as well as the contextual POS, and the distance and direction of attachment. We followed the same two-stage approach as before, and compared with a baseline which selects feature templates by ranking them according to the information gain criterion. This baseline assigns a score to each template Tm which reflects an empirical estimate of the mutual information between Tm and the binary variable A that indicates the presence/absence of a dependency link: I"
D11-1139,D08-1091,0,0.0249143,"ignal processing and 1501 λ 2 2 kθk2 = λ 2 PD 2 d=1 θd ; (6) • L1 -regularization (Kazama and Tsujii, 2003; Goodman, 2004): 1 ΩL τ (θ) , τ kθk1 = τ PD d=1 |θd |. (7) The latter is known as “Lasso,” as popularized by Tibshirani (1996) in the context of sparse regression. In the two cases above, λ and τ are nonnegative coefficients controlling the intensity of the reg2 ularization. ΩL λ usually leads to easier optimization 1 and robust performance; ΩL τ encourages sparser models, where only a few features receive nonzero weights; see Gao et al. (2007) for an empirical comparison. More recently, Petrov and Klein (2008b) applied L1 regularization for structure learning in phrase-based parsing; a comparison with L2 appears in Petrov and Klein (2008a). Elastic nets interpolate between L1 and L2 , having been proposed by Zou and Hastie (2005) and used by Lavergne et al. (2010) to regularize CRFs. Neither of the regularizers just described “looks” at the structure of the feature space, since they all treat each dimension independently—we call them unstructured regularizers, as opposed to the structured ones that we next describe. 3 Structured Sparsity 1 We are interested in regularizers that share with ΩL τ the"
D11-1139,W00-0726,0,0.12932,"regularization and using the loss L which one wants to optimize. 7 To see why this is the case, note that both gradient and proximal updates come scaled by η0 ; and that the gradient of the loss is ∇LSP (θ, xt , yt ) = φ(xt , ybt ) − φ(xt , yt ), where ybt is the prediction under the current model, which is insensitive to the scaling of θ. This independence on η0 does not hold when the loss is LSVM or LCRF . 1506 5 Experiments We present experiments in three structured prediction tasks for several group choices. Text Chunking. We use the English dataset provided in the CoNLL 2000 shared task (Sang and Buchholz, 2000), which consists of 8,936 training and 2,012 testing sentences (sections 15–18 and 20 of the WSJ.) The input observations are the token words and their POS tags; we want to predict the sequences of IOB tags representing phrase chunks. We built 96 contextual feature templates as follows: • Up to 5-grams of POS tags, in windows of 5 tokens on the left and 5 tokens on the right; • Up to 3-grams of words, in windows of 3 tokens on the left and 3 tokens on the right; • Up to 2-grams of word shapes, in windows of 2 tokens on the left and 2 tokens on the right. Each shape replaces characters by their"
D11-1139,W03-0419,0,0.111532,"Missing"
D11-1139,W02-2024,0,0.0385244,"Missing"
D11-1139,P09-1054,0,0.0344113,"= kθ m k2 −dm d kθ m k2 θ m otherwise. (14) which can be seen as a generalization of Eq. 13: if the L2 -norm of the m-th group is less than dm , the entire group is discarded; otherwise it is scaled so that its L2 -norm decreases by an amount of dm . When groups overlap, the proximity operator lacks a closed form. When G is tree-structured, it can still be efficiently computed by a recursive procedure (Jenatton et al., 2010). When G is not treestructured, no specialized procedure is known, and a convex optimizer is necessary to solve Eq. 12. 4 Online Prox-Grad Algorithm Langford et al., 2009; Tsuruoka et al., 2009). The algorithm that we use in this paper (Alg. 1) extends the stochastic gradient methods for group-Lasso regularization; a similar algorithm was used by Martins et al. (2011) for multiple kernel learning. Alg. 1 addresses the learning problem in Eq. 2 by alternating between online (sub-)gradient steps with respect to the loss term, and proximal steps with respect to the regularizer. Proximal-gradient methods are very popular in sparse modeling, both in batch (Liu and Ye, 2010; Bach et al., 2011) and online (Duchi and Singer, 2009; Xiao, 2009) settings. The reason we have chosen the algorithm"
D11-1139,P02-1062,0,\N,Missing
D12-1124,D10-1124,1,0.87297,"Missing"
D12-1124,P11-1137,1,0.861191,"d. In particular, they showed that price affects how “authenticity” is realized in marketing language, a point we return to in §5. This is an example of how price can affect how an underlying variable is expressed in language. Among other explorations in this paper, we consider how price interacts with expression of sentiment in user reviews of restaurants. As mentioned above, our work is related to research in predicting real-world quantities using text data (Koppel and Shtrimberg, 2006; Ghose et al., 2007; Lerman et al., 2008; Kogan et al., 2009; Joshi et al., 2010; Eisenstein et al., 2010; Eisenstein et al., 2011; Yogatama et al., 2011). Like much of this prior work, we aim to learn how language is used in a specific context while building models that achieve competitive performance on a quantitative prediction task. Along these lines, there is recent interest in exploring the relationship between product sales and user-generated text, particularly online product reviews. For example, Ghose and Ipeirotis (2011) studied the sales impact of particular properties of review text, such as readability, the presence of spelling errors, and the balance between subjective and objective statements. Archak et al"
D12-1124,P07-1053,0,0.0338299,"on how different socioeconomic groups selfidentify using language and how they are linguistically targeted. In particular, they showed that price affects how “authenticity” is realized in marketing language, a point we return to in §5. This is an example of how price can affect how an underlying variable is expressed in language. Among other explorations in this paper, we consider how price interacts with expression of sentiment in user reviews of restaurants. As mentioned above, our work is related to research in predicting real-world quantities using text data (Koppel and Shtrimberg, 2006; Ghose et al., 2007; Lerman et al., 2008; Kogan et al., 2009; Joshi et al., 2010; Eisenstein et al., 2010; Eisenstein et al., 2011; Yogatama et al., 2011). Like much of this prior work, we aim to learn how language is used in a specific context while building models that achieve competitive performance on a quantitative prediction task. Along these lines, there is recent interest in exploring the relationship between product sales and user-generated text, particularly online product reviews. For example, Ghose and Ipeirotis (2011) studied the sales impact of particular properties of review text, such as readabil"
D12-1124,N10-1038,1,0.945812,"ustomer writes an unfavorable review of a restaurant, how is her word choice affected by the restaurant’s prices? In this paper, we explore questions like these that relate restaurant menus, prices, and customer sentiment. Our goal is to understand how language is used in the food domain, and we direct our investigation using external variables such as restaurant menu prices. We build on a thread of NLP research that seeks linguistic understanding by predicting real-world quantities from text data. Recent examples include prediction of stock volatility (Kogan et al., 2009) and movie revenues (Joshi et al., 2010). There, prediction tasks were used for quantitative evaluation and objective model comparison, while analysis of learned models gave insight about the social process behind the data. We echo this pattern here as we turn our attention to language use on restaurant menus and in user restaurant reviews. We use data from a large corpus of restaurant menus and reviews crawled from the web and formulate several prediction tasks. In addition to predicting menu prices, we also consider predicting sentiment along with price. The relationship between language and sentiment is an active area of investig"
D12-1124,N09-1031,1,0.950619,". charbroiled affect its price? When a customer writes an unfavorable review of a restaurant, how is her word choice affected by the restaurant’s prices? In this paper, we explore questions like these that relate restaurant menus, prices, and customer sentiment. Our goal is to understand how language is used in the food domain, and we direct our investigation using external variables such as restaurant menu prices. We build on a thread of NLP research that seeks linguistic understanding by predicting real-world quantities from text data. Recent examples include prediction of stock volatility (Kogan et al., 2009) and movie revenues (Joshi et al., 2010). There, prediction tasks were used for quantitative evaluation and objective model comparison, while analysis of learned models gave insight about the social process behind the data. We echo this pattern here as we turn our attention to language use on restaurant menus and in user restaurant reviews. We use data from a large corpus of restaurant menus and reviews crawled from the web and formulate several prediction tasks. In addition to predicting menu prices, we also consider predicting sentiment along with price. The relationship between language and"
D12-1124,C08-1060,0,0.027295,"cioeconomic groups selfidentify using language and how they are linguistically targeted. In particular, they showed that price affects how “authenticity” is realized in marketing language, a point we return to in §5. This is an example of how price can affect how an underlying variable is expressed in language. Among other explorations in this paper, we consider how price interacts with expression of sentiment in user reviews of restaurants. As mentioned above, our work is related to research in predicting real-world quantities using text data (Koppel and Shtrimberg, 2006; Ghose et al., 2007; Lerman et al., 2008; Kogan et al., 2009; Joshi et al., 2010; Eisenstein et al., 2010; Eisenstein et al., 2011; Yogatama et al., 2011). Like much of this prior work, we aim to learn how language is used in a specific context while building models that achieve competitive performance on a quantitative prediction task. Along these lines, there is recent interest in exploring the relationship between product sales and user-generated text, particularly online product reviews. For example, Ghose and Ipeirotis (2011) studied the sales impact of particular properties of review text, such as readability, the presence of"
D12-1124,D11-1055,1,0.907537,"Missing"
D13-1010,W11-1701,0,0.0308477,"specific political issues, putting them on a left-right spectrum, while Thomas et al. (2006) made use of floor debate speeches to predict votes. Likewise, taking advantage of the proliferation of text today, numerous techniques have been developed to identify topics and perspectives in the media (Gentzkow and Shapiro, 2005; Lin et al., 2008; Fortuna et al., 2009; Gentzkow and Shapiro, 2010); determine the political leanings of a document or author (Laver et al., 2003; Efron, 2004; Mullen and Malouf, 2006; Fader et al., 2007); or recognize stances in debates (So100 masundaran and Wiebe, 2009; Anand et al., 2011). Going beyong lexical indicators, Greene and Resnik (2009) investigated syntactic features to identify perspectives or implicit sentiment. 7 Conclusions We introduced CLIP, a domain-informed, Bayesian model of ideological proportions in political language. We showed how ideological cues could be discovered from a lightly labeled corpus of ideological writings, then incorporated into CLIP. The resulting inferences are largely consistent with a set of preregistered hypotheses about candidates in the 2008 and 2012 Presidential elections. Acknowledgments For thoughtful feedback on this research,"
D13-1010,J93-2003,0,0.0231239,"Missing"
D13-1010,D07-1069,0,0.122869,"s of cues and lags (filler distinguished only by its length in words). We apply a domain-informed Bayesian HMM to infer the proportions of ideologies each candidate uses in each campaign. The results are validated against a set of preregistered, domain expertauthored hypotheses. 1 Introduction The artful use of language is central to politics, and the language of politicians has attracted considerable interest among scholars of political communication and rhetoric (Charteris-Black, 2005; Hart, 2009; Deirmeier et al., 2012; Hart et al., 2013) and computational linguistics (Thomas et al., 2006; Fader et al., 2007; Gerrish and Blei, 2011, inter alia). In American politics, candidates for office give speeches and write books and manifestos expounding their ideas. Every political season, however, there are accusations of candidates “flipflopping” on issues, with opinion shows, late-night comedies, and talk radio hosts replaying clips of candidates contradicting earlier statements. Presidential candidate Mitt Romney’s own aide infamously proclaimed in 2012: “I think you hit a reset button for the fall campaign [i.e., the general election]. Everything changes. It’s almost like an Etcha-Sketch. You can kind"
D13-1010,N09-1057,0,0.131271,"ht spectrum, while Thomas et al. (2006) made use of floor debate speeches to predict votes. Likewise, taking advantage of the proliferation of text today, numerous techniques have been developed to identify topics and perspectives in the media (Gentzkow and Shapiro, 2005; Lin et al., 2008; Fortuna et al., 2009; Gentzkow and Shapiro, 2010); determine the political leanings of a document or author (Laver et al., 2003; Efron, 2004; Mullen and Malouf, 2006; Fader et al., 2007); or recognize stances in debates (So100 masundaran and Wiebe, 2009; Anand et al., 2011). Going beyong lexical indicators, Greene and Resnik (2009) investigated syntactic features to identify perspectives or implicit sentiment. 7 Conclusions We introduced CLIP, a domain-informed, Bayesian model of ideological proportions in political language. We showed how ideological cues could be discovered from a lightly labeled corpus of ideological writings, then incorporated into CLIP. The resulting inferences are largely consistent with a set of preregistered hypotheses about candidates in the 2008 and 2012 Presidential elections. Acknowledgments For thoughtful feedback on this research, the authors thank: several anonymous reviewers, Amber Boyds"
D13-1010,H05-1052,0,0.0277881,"istribution: p(sj |si , `; ζ, θ, ρ) = (1 − ρ)`+1 ptree (sj |si ; ζ, θ) + (1 − (1 − ρ)`+1 )ptree (sj |sB A C K G R O U N D ; ζ, θ) Note that, if ρ = 1, there is no Markovian dependency between states (i.e., there is always a restart), so CLIP reverts to a mixture model. This approach allows us to parameterize the full set of |I|2 transitions with O(|I|) parameters.7 Since the graph is a tree and the walks are not allowed to backtrack, the only ambiguity in the transition is due to the restart probability; this distinguishes CLIP from other algorithms based on random walks (Brin and Page, 1998; Mihalcea, 2005; Toutanova et al., 2004; Collins-Thompson and Callan, 2005). 3.3.2 Emission Parameterization Recall that, at time step t, CLIP emits a cue from the lexicon L and an integer-valued lag. For each state s, we let the probability of emitting cue w be denoted by ψs,w ; ψ s is a multinomial distribution over the entire lexicon L. This allows our approach to handle ambiguous cues that can associate with more than one ideology, and also to associate a cue with a different ideology than our cue discovery method proposed, if the signal from the data is sufficiently strong. We assume each lag to be gene"
D13-1010,P09-1026,0,0.254008,"Missing"
D13-1010,W06-1639,0,0.708788,"andidates as sequences of cues and lags (filler distinguished only by its length in words). We apply a domain-informed Bayesian HMM to infer the proportions of ideologies each candidate uses in each campaign. The results are validated against a set of preregistered, domain expertauthored hypotheses. 1 Introduction The artful use of language is central to politics, and the language of politicians has attracted considerable interest among scholars of political communication and rhetoric (Charteris-Black, 2005; Hart, 2009; Deirmeier et al., 2012; Hart et al., 2013) and computational linguistics (Thomas et al., 2006; Fader et al., 2007; Gerrish and Blei, 2011, inter alia). In American politics, candidates for office give speeches and write books and manifestos expounding their ideas. Every political season, however, there are accusations of candidates “flipflopping” on issues, with opinion shows, late-night comedies, and talk radio hosts replaying clips of candidates contradicting earlier statements. Presidential candidate Mitt Romney’s own aide infamously proclaimed in 2012: “I think you hit a reset button for the fall campaign [i.e., the general election]. Everything changes. It’s almost like an Etcha-"
D13-1174,2010.amta-papers.4,0,0.126235,"tions on source context to make “local” decisions about what inflections may be used before combining the phrases into a complete sentence translation. Combination pre-/post-processing solutions are also frequently proposed. In these, the target language is generally transformed from multimorphemic surface words into smaller units more amenable to direct translation, and then a postprocessing step is applied independent of the translation model. For example, Oflazer and El-Kahlout (2007) experiment with partial morpheme groupings to produce novel inflected forms when translating into Turkish; Al-Haj and Lavie (2010) compare different processing schemes for Arabic. A related but different approach is to enrich the source language items with grammatical features (e.g., a source sentence like John saw Mary is preprocessed into, e.g., John+subj saw+msubj+fobj Mary+obj) so as to make the source and target lexicons have similar morphological contrasts (Avramidis and Koehn, 2008; Yeniterzi and Oflazer, 2010; Chang et al., 2009). In general, this work suffers from the problem that it is extremely difficult to know a priori what the right preprocessing is for a given language pair, data size, and domain. Several"
D13-1174,W13-2205,1,0.788135,"monolingual text). The setup is identical except for the addition of sparse 9 http://www.statmt.org/wmt13/ translation-task.html 10 http://sw.globalvoicesonline.org 11 http://www.aakkl.helsinki.fi/cameel/ corpus/intro.htm 1684 EN → RU 14.7±0.1 15.7±0.1 EN → HE 15.8±0.3 16.8±0.4 EN→ SW 18.3±0.1 18.7±0.2 16.2±0.1 16.7±0.1 17.6±0.1 — 19.0±0.1 — rule shape indicator features and bigram cluster features. In these large scale conditions, the BLEU score improves from 18.8 to 19.6 with the addition of word clusters and reaches 20.0 with synthetic phrases. Details regarding this system are reported in Ammar et al. (2013). 7 Related Work Translation into morphologically rich languages is a widely studied problem and there is a tremendous amount of related work. Our technique of synthesizing translation options to improve generation of inflected forms is closely related to the factored translation approach proposed by Koehn and Hoang (2007); however, an important difference to that work is that we use a discriminative model that conditions on source context to make “local” decisions about what inflections may be used before combining the phrases into a complete sentence translation. Combination pre-/post-proces"
D13-1174,P08-1087,0,0.0835203,"slation, and then a postprocessing step is applied independent of the translation model. For example, Oflazer and El-Kahlout (2007) experiment with partial morpheme groupings to produce novel inflected forms when translating into Turkish; Al-Haj and Lavie (2010) compare different processing schemes for Arabic. A related but different approach is to enrich the source language items with grammatical features (e.g., a source sentence like John saw Mary is preprocessed into, e.g., John+subj saw+msubj+fobj Mary+obj) so as to make the source and target lexicons have similar morphological contrasts (Avramidis and Koehn, 2008; Yeniterzi and Oflazer, 2010; Chang et al., 2009). In general, this work suffers from the problem that it is extremely difficult to know a priori what the right preprocessing is for a given language pair, data size, and domain. Several post-processing approaches have relied on supervised classifiers to predict the optimal complete inflection for an incomplete or lemmatized translation. Minkov et al. (2007) present a method for predicting the inflection of Russian and Arabic sentences aligned to English sentences. They train a sequence model to predict target morphological features from the le"
D13-1174,N03-2002,0,0.0801637,"ontext to generate additional local translation options and by leaving the choice of the full sentence translation to the decoder, we sidestep the difficulty of computing features on target translations hypotheses. However, many morphological processes (most notably, agreement) are most best modeled using target language context. To capture target context effects, we depend on strong target language models. Therefore, an important extension of our work is to explore the interaction of our approach with more sophisticated language models that more directly model morphology, e.g., the models of Bilmes and Kirchhoff (2003), or, alternatively, ways to incorporate target language context in the inflection model. We also achieve language independence by exploiting unsupervised morphological segmentations in the absence of linguistically informed morphological analyses. Code for replicating the experiments is available from https://github.com/eschling/morphogen; further details are available in (Schlinger et al., 2013). Acknowledgments This work was supported by the U. S. Army Research Laboratory and the U. S. Army Research Office under contract/grant number W911NF-10-1-0533. We would like to thank Kim Spasaro for"
D13-1174,P08-1024,0,0.0115532,"li word wakiwapiga will produce the following features: ψprefix[−3][wa] (µ) = 1, ψprefix[−1][wa] (µ) = 1. 4 Statistics of the parallel corpora used to train the inflection model are summarized in Table 1. It is important to note here that our richly parameterized model is trained on the full parallel training corpus, not just on a handful of development sentences (which are typically used to tune MT system parameters). Despite this scale, training is simple: the inflection model is trained to discriminate among different inflectional paradigms, not over all possible target language sentences (Blunsom et al., 2008) or learning from all observable rules (Subotin, 2011). This makes the training problem relatively tractable: all experiments in this paper were trained on a single processor using a Cython implementation of the SGD optimizer. For our largest model, trained on 3.3M Russian words, n = 231K × m = 336 features were produced, and 10 SGD iterations were performed in less than 16 hours. 4.1 ψprefix[−2][ki] (µ) = 1, Inflection Model Parameter Estimation To set the parameters W and V of the inflection prediction model (Eq. 1), we use stochastic gradient descent to maximize the conditional log-likeliho"
D13-1174,J93-2003,0,0.036659,"Missing"
D13-1174,N06-1003,0,0.0465741,"uistic knowledge in the generation process. Several authors have proposed using conditional models to predict the probability of phrase translation in context (Gimpel and Smith, 2008; Chan et al., 2007; Carpuat and Wu, 2007; Jeong et al., 2010). Of particular note is the work of Subotin (2011), who use a conditional model to predict morphological features conditioned on rich linguistic features; however, this latter work also conditions on target 1685 context, which substantially complicates decoding. Finally, synthetic phrases have been used for different purposes than generating morphology. Callison-Burch et al. (2006) expanded the coverage of a phrase table by adding synthesized phrases by paraphrasing source language phrases, Chen et al. (2011) produced “fabricated” phrases by paraphrasing both source and target phrases, and Habash (2009) created new rules to handle out-ofvocabulary words. In related work, Tsvetkov et al. (2013) used synthetic phrases to improve generation of (in)definite articles when translating into English from Russian and Czech, two languages which do not lexically mark definiteness. 8 Conclusion We have presented an efficient technique that exploits morphologically analyzed corpora"
D13-1174,D07-1007,0,0.027646,"mplex languages. Stallard et al. (2012) show that an unsupervised approach to Arabic segmentation performs as well as a supervised segmenter for sourceside preprocessing (in terms of English translation quality). For translation into morphological rich languages, Clifton and Sarkar (2011) use an unsupervised morphological analyzer to produce morphological affixes in Finnish, injecting some linguistic knowledge in the generation process. Several authors have proposed using conditional models to predict the probability of phrase translation in context (Gimpel and Smith, 2008; Chan et al., 2007; Carpuat and Wu, 2007; Jeong et al., 2010). Of particular note is the work of Subotin (2011), who use a conditional model to predict morphological features conditioned on rich linguistic features; however, this latter work also conditions on target 1685 context, which substantially complicates decoding. Finally, synthetic phrases have been used for different purposes than generating morphology. Callison-Burch et al. (2006) expanded the coverage of a phrase table by adding synthesized phrases by paraphrasing source language phrases, Chen et al. (2011) produced “fabricated” phrases by paraphrasing both source and ta"
D13-1174,2012.eamt-1.60,0,0.0154987,"based language model to be especially 8 For Swahili and Hebrew, n = 6; for Russian, n = 7. helpful here and capture some basic agreement patterns that can be learned more easily on dense clusters than from plain word sequences. Table 4: Translation quality (measured by BLEU) averaged over 3 MIRA runs. Baseline +Class LM +Synthetic unsupervised supervised Detailed corpus statistics are given in Table 1: • The Russian data consist of the News Commentary parallel corpus and additional monolingual data crawled from news websites.9 • The Hebrew parallel corpus is composed of transcribed TED talks (Cettolo et al., 2012). Additional monolingual news data is also used. • The Swahili parallel corpus was obtained by crawling the Global Voices project website10 for parallel articles. Additional monolingual data was taken from the Helsinki Corpus of Swahili.11 We evaluate translation quality by translating and measuring the BLEU score of a 2000–3000 sentencelong evaluation corpus, averaging the results over 3 MIRA runs to control for optimizer instability (Clark et al., 2011). Table 4 reports the results. For all languages, using class language models improves over the baseline. When synthetic phrases are added, s"
D13-1174,P07-1005,0,0.0415702,"morphologically complex languages. Stallard et al. (2012) show that an unsupervised approach to Arabic segmentation performs as well as a supervised segmenter for sourceside preprocessing (in terms of English translation quality). For translation into morphological rich languages, Clifton and Sarkar (2011) use an unsupervised morphological analyzer to produce morphological affixes in Finnish, injecting some linguistic knowledge in the generation process. Several authors have proposed using conditional models to predict the probability of phrase translation in context (Gimpel and Smith, 2008; Chan et al., 2007; Carpuat and Wu, 2007; Jeong et al., 2010). Of particular note is the work of Subotin (2011), who use a conditional model to predict morphological features conditioned on rich linguistic features; however, this latter work also conditions on target 1685 context, which substantially complicates decoding. Finally, synthetic phrases have been used for different purposes than generating morphology. Callison-Burch et al. (2006) expanded the coverage of a phrase table by adding synthesized phrases by paraphrasing source language phrases, Chen et al. (2011) produced “fabricated” phrases by paraphras"
D13-1174,W09-0436,0,0.0550669,"endent of the translation model. For example, Oflazer and El-Kahlout (2007) experiment with partial morpheme groupings to produce novel inflected forms when translating into Turkish; Al-Haj and Lavie (2010) compare different processing schemes for Arabic. A related but different approach is to enrich the source language items with grammatical features (e.g., a source sentence like John saw Mary is preprocessed into, e.g., John+subj saw+msubj+fobj Mary+obj) so as to make the source and target lexicons have similar morphological contrasts (Avramidis and Koehn, 2008; Yeniterzi and Oflazer, 2010; Chang et al., 2009). In general, this work suffers from the problem that it is extremely difficult to know a priori what the right preprocessing is for a given language pair, data size, and domain. Several post-processing approaches have relied on supervised classifiers to predict the optimal complete inflection for an incomplete or lemmatized translation. Minkov et al. (2007) present a method for predicting the inflection of Russian and Arabic sentences aligned to English sentences. They train a sequence model to predict target morphological features from the lemmas and the syntactic structures of both aligned"
D13-1174,2011.iwslt-evaluation.19,0,0.0452575,"slation in context (Gimpel and Smith, 2008; Chan et al., 2007; Carpuat and Wu, 2007; Jeong et al., 2010). Of particular note is the work of Subotin (2011), who use a conditional model to predict morphological features conditioned on rich linguistic features; however, this latter work also conditions on target 1685 context, which substantially complicates decoding. Finally, synthetic phrases have been used for different purposes than generating morphology. Callison-Burch et al. (2006) expanded the coverage of a phrase table by adding synthesized phrases by paraphrasing source language phrases, Chen et al. (2011) produced “fabricated” phrases by paraphrasing both source and target phrases, and Habash (2009) created new rules to handle out-ofvocabulary words. In related work, Tsvetkov et al. (2013) used synthetic phrases to improve generation of (in)definite articles when translating into English from Russian and Czech, two languages which do not lexically mark definiteness. 8 Conclusion We have presented an efficient technique that exploits morphologically analyzed corpora to produce new inflections possibly unseen in the bilingual training data. Our method decomposes into two simple independent steps"
D13-1174,J07-2003,0,0.121987,"process of producing a translation for a word (or phrase) into two steps. First, a meaning-bearing stem is chosen and then an appropriate inflection is selected using a feature-rich discriminative model that conditions on the source context of the word being translated. Rather than attempting to directly produce fullsentence translations using such an elementary process, we use our model to generate translations of individual words and short phrases that augment— on a sentence-by-sentence basis—the inventory of translation rules obtained using standard translation rule extraction techniques (Chiang, 2007). We call these synthetic phrases. The major advantages of our approach are: (i) synthesized forms are targeted to a specific translation context; (ii) multiple, alternative phrases may be generated with the final choice among rules left to the global translation model; (iii) virtually no language-specific engineering is necessary; (iv) any phrase- or syntax-based decoder can be used without modification; and (v) we can generate forms that were not attested in the bilingual training data. The paper is structured as follows. We first present our “translate-and-inflect” model for predicting lexi"
D13-1174,P11-2031,1,0.516392,"lel corpus and additional monolingual data crawled from news websites.9 • The Hebrew parallel corpus is composed of transcribed TED talks (Cettolo et al., 2012). Additional monolingual news data is also used. • The Swahili parallel corpus was obtained by crawling the Global Voices project website10 for parallel articles. Additional monolingual data was taken from the Helsinki Corpus of Swahili.11 We evaluate translation quality by translating and measuring the BLEU score of a 2000–3000 sentencelong evaluation corpus, averaging the results over 3 MIRA runs to control for optimizer instability (Clark et al., 2011). Table 4 reports the results. For all languages, using class language models improves over the baseline. When synthetic phrases are added, significant additional improvements are obtained. For the English–Russian language pair, where both supervised and unsupervised analyses can be obtained, we notice that expert-crafted morphological analyzers are more efficient at improving translation quality. Globally, the amount of improvement observed varies depending on the language; this is most likely indicative of the quality of unsupervised morphological segmentations produced and the kinds of gram"
D13-1174,P11-1004,0,0.126155,"odel morphological features rather than directly inflected forms. However, that work may be criticized for providing no mechanism to translate surface forms directly, even when evidence for a direct translation is available in the parallel data. Unsupervised morphology has begun to play a role in translation between morphologically complex languages. Stallard et al. (2012) show that an unsupervised approach to Arabic segmentation performs as well as a supervised segmenter for sourceside preprocessing (in terms of English translation quality). For translation into morphological rich languages, Clifton and Sarkar (2011) use an unsupervised morphological analyzer to produce morphological affixes in Finnish, injecting some linguistic knowledge in the generation process. Several authors have proposed using conditional models to predict the probability of phrase translation in context (Gimpel and Smith, 2008; Chan et al., 2007; Carpuat and Wu, 2007; Jeong et al., 2010). Of particular note is the work of Subotin (2011), who use a conditional model to predict morphological features conditioned on rich linguistic features; however, this latter work also conditions on target 1685 context, which substantially complic"
D13-1174,P10-4002,1,0.764306,"Missing"
D13-1174,2012.eamt-1.6,0,0.224483,"Missing"
D13-1174,E12-1068,0,0.191071,"h sentences. They train a sequence model to predict target morphological features from the lemmas and the syntactic structures of both aligned sentences and demonstrate its ability to recover accurately inflections on reference translations. Toutanova et al. (2008) apply this method to generate inflections after translation in two different ways: by rescoring inflected n-best outputs or by translating lemmas and re-inflecting them a posteriori. El Kholy and Habash (2012) follow a similar method and compare different approaches for generating rich morphology in Arabic after a translation step. Fraser et al. (2012) observe improvements for translation into German with a similar method. As in that work, we model morphological features rather than directly inflected forms. However, that work may be criticized for providing no mechanism to translate surface forms directly, even when evidence for a direct translation is available in the parallel data. Unsupervised morphology has begun to play a role in translation between morphologically complex languages. Stallard et al. (2012) show that an unsupervised approach to Arabic segmentation performs as well as a supervised segmenter for sourceside preprocessing"
D13-1174,W08-0302,1,0.823134,"e in translation between morphologically complex languages. Stallard et al. (2012) show that an unsupervised approach to Arabic segmentation performs as well as a supervised segmenter for sourceside preprocessing (in terms of English translation quality). For translation into morphological rich languages, Clifton and Sarkar (2011) use an unsupervised morphological analyzer to produce morphological affixes in Finnish, injecting some linguistic knowledge in the generation process. Several authors have proposed using conditional models to predict the probability of phrase translation in context (Gimpel and Smith, 2008; Chan et al., 2007; Carpuat and Wu, 2007; Jeong et al., 2010). Of particular note is the work of Subotin (2011), who use a conditional model to predict morphological features conditioned on rich linguistic features; however, this latter work also conditions on target 1685 context, which substantially complicates decoding. Finally, synthetic phrases have been used for different purposes than generating morphology. Callison-Burch et al. (2006) expanded the coverage of a phrase table by adding synthesized phrases by paraphrasing source language phrases, Chen et al. (2011) produced “fabricated” p"
D13-1174,P05-1071,0,0.0166254,"rvised methods. 3.1 The state-of-the-art in morphological analysis uses unweighted morphological transduction rules (usu3 • Dependency parsing with TurboParser (Martins et al., 2010), a non-projective dependency 1679 Supervised Morphology The entire monolingual data available for the translation task of the 8th ACL Workshop on Statistical Machine Translation was used. ally in the form of an FST) to produce candidate analyses for each word in a sentence and then statistical models to disambiguate among the analyses in context (Hakkani-T¨ur et al., 2000; Hajiˇc et al., 2001; Smith et al., 2005; Habash and Rambow, 2005, inter alia). While this technique is capable of producing high quality linguistic analyses, it is expensive to develop, requiring hand-crafted rulebased analyzers and annotated corpora to train the disambiguation models. As a result, such analyzers are only available for a small number of languages, and, as a practical matter, each analyzer (which resulted from different development efforts) operates differently from the others. We therefore focus on using supervised analysis for a single target language, Russian. We use the analysis tool of Sharoff et al. (2008) which produces for each word"
D13-1174,P01-1035,0,0.0193571,"Missing"
D13-1174,C00-1042,0,0.268241,"Missing"
D13-1174,2010.amta-papers.33,0,0.211534,"ard et al. (2012) show that an unsupervised approach to Arabic segmentation performs as well as a supervised segmenter for sourceside preprocessing (in terms of English translation quality). For translation into morphological rich languages, Clifton and Sarkar (2011) use an unsupervised morphological analyzer to produce morphological affixes in Finnish, injecting some linguistic knowledge in the generation process. Several authors have proposed using conditional models to predict the probability of phrase translation in context (Gimpel and Smith, 2008; Chan et al., 2007; Carpuat and Wu, 2007; Jeong et al., 2010). Of particular note is the work of Subotin (2011), who use a conditional model to predict morphological features conditioned on rich linguistic features; however, this latter work also conditions on target 1685 context, which substantially complicates decoding. Finally, synthetic phrases have been used for different purposes than generating morphology. Callison-Burch et al. (2006) expanded the coverage of a phrase table by adding synthesized phrases by paraphrasing source language phrases, Chen et al. (2011) produced “fabricated” phrases by paraphrasing both source and target phrases, and Hab"
D13-1174,W08-0704,0,0.0306035,"sls independently from θs ; and a stem σ ∼ θσ . (c) Concatenate prefixes, the stem, and suffixes: w = p1 +· · ·+plp +σ+s1 +· · ·+sls . We use blocked Gibbs sampling to sample segmentations for each word in the training vocabulary. Because of our particular choice of priors, it possible to approximately decompose the posterior over the arcs of a compact finite-state machine. Sampling a segmentation or obtaining the most likely segmentation a posteriori then reduces to familiar FST operations. This model is reminiscent of work on learning morphology using adaptor grammars (Johnson et al., 2006; Johnson, 2008). The inferred morphological grammar is very sensitive to the Dirichlet hyperparameters (αp , αs , ασ ) and these are, in turn, sensitive to the number of types in the vocabulary. Using αp , αs  ασ  1 tended to recover useful segmentations, but we have not yet been able to find reliable generic priors for these values. Therefore, we selected them empirically to obtain a stem vocabulary size on the parallel data that is one-to-one with English.4 Future work 4 Our default starting point was to use αp = αs = 10 , ασ = 10−4 and then to adjust all parameters by factors of 10. −6 Table 1: Corpus s"
D13-1174,D07-1091,0,0.150963,"17.6±0.1 — 19.0±0.1 — rule shape indicator features and bigram cluster features. In these large scale conditions, the BLEU score improves from 18.8 to 19.6 with the addition of word clusters and reaches 20.0 with synthetic phrases. Details regarding this system are reported in Ammar et al. (2013). 7 Related Work Translation into morphologically rich languages is a widely studied problem and there is a tremendous amount of related work. Our technique of synthesizing translation options to improve generation of inflected forms is closely related to the factored translation approach proposed by Koehn and Hoang (2007); however, an important difference to that work is that we use a discriminative model that conditions on source context to make “local” decisions about what inflections may be used before combining the phrases into a complete sentence translation. Combination pre-/post-processing solutions are also frequently proposed. In these, the target language is generally transformed from multimorphemic surface words into smaller units more amenable to direct translation, and then a postprocessing step is applied independent of the translation model. For example, Oflazer and El-Kahlout (2007) experiment"
D13-1174,D07-1104,0,0.0572931,"Missing"
D13-1174,D10-1004,1,0.475972,"s of English text.3 We then extract binary features from e using this information, by considering the aligned source word ei , its preceding and following words, and its syntactic neighbors. These are detailed in Figure 2. 3 Morphological Grammars and Features We now describe how to obtain morphological analyses and convert them into feature vectors (ψ) for our target languages, Russian, Hebrew, and Swahili, using supervised and unsupervised methods. 3.1 The state-of-the-art in morphological analysis uses unweighted morphological transduction rules (usu3 • Dependency parsing with TurboParser (Martins et al., 2010), a non-projective dependency 1679 Supervised Morphology The entire monolingual data available for the translation task of the 8th ACL Workshop on Statistical Machine Translation was used. ally in the form of an FST) to produce candidate analyses for each word in a sentence and then statistical models to disambiguate among the analyses in context (Hakkani-T¨ur et al., 2000; Hajiˇc et al., 2001; Smith et al., 2005; Habash and Rambow, 2005, inter alia). While this technique is capable of producing high quality linguistic analyses, it is expensive to develop, requiring hand-crafted rulebased anal"
D13-1174,P07-1017,0,0.256142,", a source sentence like John saw Mary is preprocessed into, e.g., John+subj saw+msubj+fobj Mary+obj) so as to make the source and target lexicons have similar morphological contrasts (Avramidis and Koehn, 2008; Yeniterzi and Oflazer, 2010; Chang et al., 2009). In general, this work suffers from the problem that it is extremely difficult to know a priori what the right preprocessing is for a given language pair, data size, and domain. Several post-processing approaches have relied on supervised classifiers to predict the optimal complete inflection for an incomplete or lemmatized translation. Minkov et al. (2007) present a method for predicting the inflection of Russian and Arabic sentences aligned to English sentences. They train a sequence model to predict target morphological features from the lemmas and the syntactic structures of both aligned sentences and demonstrate its ability to recover accurately inflections on reference translations. Toutanova et al. (2008) apply this method to generate inflections after translation in two different ways: by rescoring inflected n-best outputs or by translating lemmas and re-inflecting them a posteriori. El Kholy and Habash (2012) follow a similar method and"
D13-1174,W07-0704,0,0.0236609,"proach proposed by Koehn and Hoang (2007); however, an important difference to that work is that we use a discriminative model that conditions on source context to make “local” decisions about what inflections may be used before combining the phrases into a complete sentence translation. Combination pre-/post-processing solutions are also frequently proposed. In these, the target language is generally transformed from multimorphemic surface words into smaller units more amenable to direct translation, and then a postprocessing step is applied independent of the translation model. For example, Oflazer and El-Kahlout (2007) experiment with partial morpheme groupings to produce novel inflected forms when translating into Turkish; Al-Haj and Lavie (2010) compare different processing schemes for Arabic. A related but different approach is to enrich the source language items with grammatical features (e.g., a source sentence like John saw Mary is preprocessed into, e.g., John+subj saw+msubj+fobj Mary+obj) so as to make the source and target lexicons have similar morphological contrasts (Avramidis and Koehn, 2008; Yeniterzi and Oflazer, 2010; Chang et al., 2009). In general, this work suffers from the problem that it"
D13-1174,D13-1174,1,0.106134,"Missing"
D13-1174,H05-1060,1,0.344355,"upervised and unsupervised methods. 3.1 The state-of-the-art in morphological analysis uses unweighted morphological transduction rules (usu3 • Dependency parsing with TurboParser (Martins et al., 2010), a non-projective dependency 1679 Supervised Morphology The entire monolingual data available for the translation task of the 8th ACL Workshop on Statistical Machine Translation was used. ally in the form of an FST) to produce candidate analyses for each word in a sentence and then statistical models to disambiguate among the analyses in context (Hakkani-T¨ur et al., 2000; Hajiˇc et al., 2001; Smith et al., 2005; Habash and Rambow, 2005, inter alia). While this technique is capable of producing high quality linguistic analyses, it is expensive to develop, requiring hand-crafted rulebased analyzers and annotated corpora to train the disambiguation models. As a result, such analyzers are only available for a small number of languages, and, as a practical matter, each analyzer (which resulted from different development efforts) operates differently from the others. We therefore focus on using supervised analysis for a single target language, Russian. We use the analysis tool of Sharoff et al. (2008) whi"
D13-1174,P12-2063,0,0.0334722,"h (2012) follow a similar method and compare different approaches for generating rich morphology in Arabic after a translation step. Fraser et al. (2012) observe improvements for translation into German with a similar method. As in that work, we model morphological features rather than directly inflected forms. However, that work may be criticized for providing no mechanism to translate surface forms directly, even when evidence for a direct translation is available in the parallel data. Unsupervised morphology has begun to play a role in translation between morphologically complex languages. Stallard et al. (2012) show that an unsupervised approach to Arabic segmentation performs as well as a supervised segmenter for sourceside preprocessing (in terms of English translation quality). For translation into morphological rich languages, Clifton and Sarkar (2011) use an unsupervised morphological analyzer to produce morphological affixes in Finnish, injecting some linguistic knowledge in the generation process. Several authors have proposed using conditional models to predict the probability of phrase translation in context (Gimpel and Smith, 2008; Chan et al., 2007; Carpuat and Wu, 2007; Jeong et al., 201"
D13-1174,P11-1024,0,0.261482,"ix[−3][wa] (µ) = 1, ψprefix[−1][wa] (µ) = 1. 4 Statistics of the parallel corpora used to train the inflection model are summarized in Table 1. It is important to note here that our richly parameterized model is trained on the full parallel training corpus, not just on a handful of development sentences (which are typically used to tune MT system parameters). Despite this scale, training is simple: the inflection model is trained to discriminate among different inflectional paradigms, not over all possible target language sentences (Blunsom et al., 2008) or learning from all observable rules (Subotin, 2011). This makes the training problem relatively tractable: all experiments in this paper were trained on a single processor using a Cython implementation of the SGD optimizer. For our largest model, trained on 3.3M Russian words, n = 231K × m = 336 features were produced, and 10 SGD iterations were performed in less than 16 hours. 4.1 ψprefix[−2][ki] (µ) = 1, Inflection Model Parameter Estimation To set the parameters W and V of the inflection prediction model (Eq. 1), we use stochastic gradient descent to maximize the conditional log-likelihood of a training set consisting of pairs of source (En"
D13-1174,P08-1059,0,0.65058,"at the right preprocessing is for a given language pair, data size, and domain. Several post-processing approaches have relied on supervised classifiers to predict the optimal complete inflection for an incomplete or lemmatized translation. Minkov et al. (2007) present a method for predicting the inflection of Russian and Arabic sentences aligned to English sentences. They train a sequence model to predict target morphological features from the lemmas and the syntactic structures of both aligned sentences and demonstrate its ability to recover accurately inflections on reference translations. Toutanova et al. (2008) apply this method to generate inflections after translation in two different ways: by rescoring inflected n-best outputs or by translating lemmas and re-inflecting them a posteriori. El Kholy and Habash (2012) follow a similar method and compare different approaches for generating rich morphology in Arabic after a translation step. Fraser et al. (2012) observe improvements for translation into German with a similar method. As in that work, we model morphological features rather than directly inflected forms. However, that work may be criticized for providing no mechanism to translate surface"
D13-1174,W13-2234,1,0.711961,"to predict morphological features conditioned on rich linguistic features; however, this latter work also conditions on target 1685 context, which substantially complicates decoding. Finally, synthetic phrases have been used for different purposes than generating morphology. Callison-Burch et al. (2006) expanded the coverage of a phrase table by adding synthesized phrases by paraphrasing source language phrases, Chen et al. (2011) produced “fabricated” phrases by paraphrasing both source and target phrases, and Habash (2009) created new rules to handle out-ofvocabulary words. In related work, Tsvetkov et al. (2013) used synthetic phrases to improve generation of (in)definite articles when translating into English from Russian and Czech, two languages which do not lexically mark definiteness. 8 Conclusion We have presented an efficient technique that exploits morphologically analyzed corpora to produce new inflections possibly unseen in the bilingual training data. Our method decomposes into two simple independent steps involving well-understood discriminative models. By relying on source-side context to generate additional local translation options and by leaving the choice of the full sentence translat"
D13-1174,P10-1047,0,0.059066,"cessing step is applied independent of the translation model. For example, Oflazer and El-Kahlout (2007) experiment with partial morpheme groupings to produce novel inflected forms when translating into Turkish; Al-Haj and Lavie (2010) compare different processing schemes for Arabic. A related but different approach is to enrich the source language items with grammatical features (e.g., a source sentence like John saw Mary is preprocessed into, e.g., John+subj saw+msubj+fobj Mary+obj) so as to make the source and target lexicons have similar morphological contrasts (Avramidis and Koehn, 2008; Yeniterzi and Oflazer, 2010; Chang et al., 2009). In general, this work suffers from the problem that it is extremely difficult to know a priori what the right preprocessing is for a given language pair, data size, and domain. Several post-processing approaches have relied on supervised classifiers to predict the optimal complete inflection for an incomplete or lemmatized translation. Minkov et al. (2007) present a method for predicting the inflection of Russian and Arabic sentences aligned to English sentences. They train a sequence model to predict target morphological features from the lemmas and the syntactic struct"
D13-1174,sharoff-etal-2008-designing,0,\N,Missing
D13-1191,D10-1111,0,0.11784,"Missing"
D13-1191,W11-1701,0,0.0183132,"Missing"
D13-1191,P12-2041,0,0.0657852,"Missing"
D13-1191,P12-2013,0,0.0241432,"Missing"
D13-1191,P05-1045,0,0.0024014,"late elements such as navigation and advertisments) using Boilerpipe (Kohlsch¨utter et al., 2010).5 We tokenized the text and filtered stopwords.6 We considered both unigrams and bigrams in our model, keeping all unigrams and removing bigram types that appeared less than 5 times in the corpus. Although our modeling approach ultimately treats texts as bags of terms (unigrams and bigrams), one important preprocessing step was taken to further improve the interpretability of the inferred representation: named entity mentions of persons. We identified these mentions of persons using Stanford NER (Finkel et al., 2005) and treated each person mention as a single token. In our qualitative analysis of the model (§4.2), we will show how this special treatment of person mentions enables the association of well-known individuals with debate topics. Though not part of our experimental evaluation in this paper, such associations are, we believe, an interesting direction for future applications of the model. 3 Model Our model defines a probability distribution over terms7 that are observed in the corpus. Each term occurs in a context defined by the tuple hd, q, s, ai (respectively, a debate, a question within the d"
D13-1191,C08-1031,0,0.014975,"Missing"
D13-1191,N09-1057,0,0.058725,"Missing"
D13-1191,W02-1011,0,0.0141142,"Missing"
D13-1191,D10-1007,0,0.01089,"Missing"
D13-1191,D13-1010,1,0.351102,"Missing"
D13-1191,P09-1026,0,0.102041,"Missing"
D13-1191,H05-2018,0,0.0077349,"o a position. b. ∀ questions q in d: i. Draw topic mixture proportions θ d,q ∼ Dirichlet(α). ii. ∀ arguments a under question q and term positions n in a: A. Draw topic label zd,q,s,a ∼ Multinomial(θ d,q ). B. Draw functional term type yd,q,s,a ∼ Multinomial(µ). C. Draw term wd,q,s,a ∼ Multinomial (φyd,q,s,a |id,1 , id,2 , zd,q,s,a ). Figure 2: Generative story for our model of Debatepedia. apply empirical Bayesian techniques to estimate the hyperparameters. Motivated by past efforts to exploit prior knowledge (Zhao et al., 2010; Lin and He, 2009), we use the OpinionFinder sentiment lexicon9 (Wilson et al., 2005) to construct η i and η o . Specifically, terms w in the lexicon were given pai = η o = 0.01, and other terms were rameters ηw w i o = 0.001, capturing our prior belief given ηw = ηw that opinion-expressing terms are likely to be used in expressing positions. 5,451 types were given a “boost” through this prior. Information retrieval has long exploited the observation that a term’s document frequency (i.e., the number of documents a term occurs in) is inversely related its usefulness in retrieval (Jones, 1972). We encode this in η b , the prior over the background term distribution, by setting"
D13-1191,D09-1159,0,0.0174039,"Missing"
D13-1191,D10-1006,1,0.196152,"debates d: a. Draw id,1 , id,2 ∼ Multinomial(ι), assigning each of the two sides to a position. b. ∀ questions q in d: i. Draw topic mixture proportions θ d,q ∼ Dirichlet(α). ii. ∀ arguments a under question q and term positions n in a: A. Draw topic label zd,q,s,a ∼ Multinomial(θ d,q ). B. Draw functional term type yd,q,s,a ∼ Multinomial(µ). C. Draw term wd,q,s,a ∼ Multinomial (φyd,q,s,a |id,1 , id,2 , zd,q,s,a ). Figure 2: Generative story for our model of Debatepedia. apply empirical Bayesian techniques to estimate the hyperparameters. Motivated by past efforts to exploit prior knowledge (Zhao et al., 2010; Lin and He, 2009), we use the OpinionFinder sentiment lexicon9 (Wilson et al., 2005) to construct η i and η o . Specifically, terms w in the lexicon were given pai = η o = 0.01, and other terms were rameters ηw w i o = 0.001, capturing our prior belief given ηw = ηw that opinion-expressing terms are likely to be used in expressing positions. 5,451 types were given a “boost” through this prior. Information retrieval has long exploited the observation that a term’s document frequency (i.e., the number of documents a term occurs in) is inversely related its usefulness in retrieval (Jones, 1972)"
D13-1191,P12-1042,0,\N,Missing
D14-1108,N13-1037,0,0.0101107,"t al., 1993), augmented using semi-supervision and in-domain data. Others, such as Soni et al. (2014), have used existing Penn Treebank–trained models on tweets. In this work, we argue that the Penn Treebank approach to annotation—while well-matched to edited genres like newswire—is poorly suited to more informal genres. Our starting point is that rapid, small-scale annotation efforts performed by imperfectly-trained annotators should provide enough evidence to train an effective parser. We see this starting point as a necessity, given observations about the rapidly changing nature of tweets (Eisenstein, 2013), the attested difficulties of domain adaptation for parsing (Dredze et al., 2007), and the expense of creating Penn Treebank–style annotations (Marcus et al., 1993). This paper presents T WEEBOPARSER, the first syntactic dependency parser designed explicitly for English tweets. We developed this parser following current best practices in empirical NLP: we annotate a corpus (T WEEBANK) and train the parameters of a statistical parsing algorithm. Our research contributions include: • a survey of key challenges posed by syntactic analysis of tweets (by humans or machines) and decisions motivated"
D14-1108,C96-1058,0,0.325768,"is used in parsing algorithms that seek to find: parse∗ (x) = argmax w⊺ g(x,y) (1) y∈Yx The score is parameterized by a vector w of weights, which are learned from data (most commonly using MIRA, McDonald et al., 2005a). The decomposition of the features into local “parts” is a critical choice affecting the computational difficulty of solving Eq. 1. The most aggressive decomposition leads to an “arc-factored” or “first-order” model, which permits exact, efficient solution of Eq. 1 using spanning tree algorithms (McDonald et al., 2005b) or, with a projectivity constraint, dynamic programming (Eisner, 1996). Second- and third-order models have also been introduced, typically relying on approximations, since less-local features increase the computational cost, sometimes to the point of NP-hardness (McDonald and Satta, 2007). TurboParser attacks the parsing problem using a compact integer linear programming (ILP) representation of Eq. 1 (Martins et al., 2009), then employing alternating directions dual decomposition (AD3 ; Martins et al., 2011). This enables inclusion of second-order features (e.g., on a word with its sibling or grandparent; Carreras, 2007) and third-order features (e.g., a word w"
D14-1108,J92-4003,0,0.124367,"x p or xg , and transitions that consider unselected tokens as children, are eliminated. In order to allow the scores to depend on unselected tokens between xc and xc′ , we added the binned counts of unselected tokens (mostly punctuation) joint with the word form and POS tag of x p and the POS tag of xc and xc′ as features scored in the sibling(p,c,c′ ) part. The changes discussed above comprise the totality of adaptations we made to the TurboParser algorithm; we refer to them as “parsing adaptations” in the experiments. 4.3 Additional Features Brown clusters. Owoputi et al. (2013) found that Brown et al. (1992) clusters served as excellent features in Twitter POS tagging. Others have found them useful in parsing (Koo et al., 2008) and other tasks (Turian et al., 2010). We therefore follow Koo et al. in incorporating Brown clusters as features, making use of the publicly available Twitter clusters from Owoputi et al.5 We use 4 and 6 bit cluster representations to create features wherever POS tags are used, and full bit strings to create features wherever words were used. Penn Treebank features. A potential danger of our choice to “start from scratch” in developing a dependency parser for Twitter is t"
D14-1108,N13-1070,0,0.00887336,"$a with the coordinator and labeling Annotation Conventions A wide range of dependency conventions are in use; in many cases these are conversion conventions specifying how dependency trees can be derived from phrase-structure trees. For English, the most popular are due to Yamada and Matsumoto (2003) and de Marneffe and Manning (2008), known as “Yamada-Matsumoto” (YM) and “Stanford” dependencies, respectively. The main differences between them are in whether the auxiliary is the parent of the main verb (or vice versa) and whether the preposition or its argument heads a prepositional phrase (Elming et al., 2013). A full discussion of our annotation conventions is out of scope. We largely followed the conventions suggested by Schneider et al. (2013), which in turn are close to those of YM. Auxiliary verbs are parents of main verbs, and prepositions are parents of their arguments. The key differences from YM are in coordination structures (discussed in §3.2; YM makes the first conjunct the head) and possessive structures, in which the possessor is the child of the clitic, which is the child of the semantic head, e.g., the > king > ’s > horses. 3.4 Intrinsic Quality Our approach to developing this initi"
D14-1108,W06-2920,0,0.0727057,"Missing"
D14-1108,N09-1037,0,0.0604952,"Missing"
D14-1108,P14-1070,0,0.0175782,"c function in a ten-fold cross-validation experiment. To take Multiword Expressions We consider multiword expressions (MWEs) of two kinds. The first, proper names, have been widely modeled for information extraction purposes, and even incorporated into parsing (Finkel and Manning, 2009). (An example found in Figure 1 is LA Times.) The second, lexical idioms, have been a “pain in the neck” for many years (Sag et al., 2002) and have recently received shallow treatment in NLP (Baldwin and Kim, 2010; Constant and Sigogne, 2011; Schneider et al., 2014). Constant et al. (2012), Green et al. (2012), Candito and Constant (2014), and Le Roux et al. (2014) considered MWEs in parsing. Figure 1 provides LA Times and All the Rage as examples. Penn Treebank–style syntactic analysis (and dependency representations derived from it) does not give first-class treatment to this phenomenon, though there is precedent for marking multiword lexical units and certain kinds of idiomatic relationships (Hajiˇc et al., 2012; Abeillé et al., 2003).1 We argue that internal analysis of MWEs is not critical for many downstream applications, and therefore annotators should not expend energy on developing and respecting conventions (or makin"
D14-1108,D07-1101,0,0.0132253,"jectivity constraint, dynamic programming (Eisner, 1996). Second- and third-order models have also been introduced, typically relying on approximations, since less-local features increase the computational cost, sometimes to the point of NP-hardness (McDonald and Satta, 2007). TurboParser attacks the parsing problem using a compact integer linear programming (ILP) representation of Eq. 1 (Martins et al., 2009), then employing alternating directions dual decomposition (AD3 ; Martins et al., 2011). This enables inclusion of second-order features (e.g., on a word with its sibling or grandparent; Carreras, 2007) and third-order features (e.g., a word with its parent, grandparent, and a sibling, or with its parent and two siblings; Koo and Collins, 2010). For a collection of (possibly overlapping) parts for input x, Sx (which includes the union of all parts of all trees in Yx ), we will use the following notation. Let g(x,y) = ∑ fs (x,y), (2) s∈Sx where fs only considers part s and is nonzero only if s is present in y. In the ILP framework, each s has a corresponding binary variable zs indicating whether part s is included in the output. A collection of constraints relating zs define the set of feasib"
D14-1108,W02-1001,0,0.20767,"Missing"
D14-1108,W11-0809,0,0.0137621,"Missing"
D14-1108,P12-1022,0,0.0128621,"the task of selecting tokens with a syntactic function in a ten-fold cross-validation experiment. To take Multiword Expressions We consider multiword expressions (MWEs) of two kinds. The first, proper names, have been widely modeled for information extraction purposes, and even incorporated into parsing (Finkel and Manning, 2009). (An example found in Figure 1 is LA Times.) The second, lexical idioms, have been a “pain in the neck” for many years (Sag et al., 2002) and have recently received shallow treatment in NLP (Baldwin and Kim, 2010; Constant and Sigogne, 2011; Schneider et al., 2014). Constant et al. (2012), Green et al. (2012), Candito and Constant (2014), and Le Roux et al. (2014) considered MWEs in parsing. Figure 1 provides LA Times and All the Rage as examples. Penn Treebank–style syntactic analysis (and dependency representations derived from it) does not give first-class treatment to this phenomenon, though there is precedent for marking multiword lexical units and certain kinds of idiomatic relationships (Hajiˇc et al., 2012; Abeillé et al., 2003).1 We argue that internal analysis of MWEs is not critical for many downstream applications, and therefore annotators should not expend energy"
D14-1108,W08-1301,0,0.0281126,"Missing"
D14-1108,P11-2008,1,0.814385,"Missing"
D14-1108,N13-1013,0,0.0174979,"ally modified the automatically converted trees by: (a) Performing token selection (§2.1) to remove the tokens which have no syntactic function. (b) Grouping MWEs (§2.2). Here, most of the MWEs are named entities such as Manchester United. (c) Attaching the roots of the utterance in tweets to the “wall” symbol (§2.3).8 6 Stacking is a machine learning method where the predictions of one model are used to create features for another. The second model may be from a different family. Stacking has been found successful for dependency parsing by Nivre and McDonald (2008) and Martins et al. (2008). Johansson (2013) describes further advances that use path features. 7 http://nlp.cs.lth.se/software/treebank_ converter; run with -rightBranching=false -coordStructure=prague -prepAsHead=true -posAsHead=true -subAsHead=true -imAsHead=true -whAsHead=false. 8 This was infrequent; their annotations split most multitweets unique tweets tokens selected tokens types utterances multi-root tweets MWEs TRAIN TEST- NEW TEST-F OSTER 717 569 9,310 7,015 3,566 1,473 398 387 201 201 2,839 2,158 1,461 429 123 78 < 250† < 250† 2,841 2,366 1,230 337 60 109 Table 1: Statistics of our datasets. (A tweet with k annotations in th"
D14-1108,J98-4004,0,0.0947122,"Colors highlight token selection (gray; §2.1), multiword expressions (blue; §2.2), multiple roots (red; §2.3), coordination (dotted arcs, green; §3.2), and noun phrase internal structure (orange; §2.4). The internal structure of multiword expressions (dashed arcs below the sentence) was predicted automatically by a parser, as described in §2.2. tractive property from the perspective of semantic processing. To allow training a fairly conventional statistical dependency parser from these annotations, we find it expedient to apply an automatic conversion to the MWE annotations, in the spirit of Johnson (1998). We apply an existing dependency parser, the first-order TurboParser (Martins et al., 2009) trained on the Penn Treebank, to parse each MWE independently, assigning structures like those for LA Times and All the Rage in Figure 1. Arcs involving the MWE in the annotation are then reconnected to the MWE-internal root, so that the resulting tree respects the original tokenization. The MWE-internal arcs are given a special label so that the transformation can be reversed and MWEs reconstructed from parser output. 2.3 Multiple Roots For news text such as that found in the Penn Treebank, sentence s"
D14-1108,P08-1068,0,0.0484886,"end on unselected tokens between xc and xc′ , we added the binned counts of unselected tokens (mostly punctuation) joint with the word form and POS tag of x p and the POS tag of xc and xc′ as features scored in the sibling(p,c,c′ ) part. The changes discussed above comprise the totality of adaptations we made to the TurboParser algorithm; we refer to them as “parsing adaptations” in the experiments. 4.3 Additional Features Brown clusters. Owoputi et al. (2013) found that Brown et al. (1992) clusters served as excellent features in Twitter POS tagging. Others have found them useful in parsing (Koo et al., 2008) and other tasks (Turian et al., 2010). We therefore follow Koo et al. in incorporating Brown clusters as features, making use of the publicly available Twitter clusters from Owoputi et al.5 We use 4 and 6 bit cluster representations to create features wherever POS tags are used, and full bit strings to create features wherever words were used. Penn Treebank features. A potential danger of our choice to “start from scratch” in developing a dependency parser for Twitter is that the resulting annotation conventions, data, and desired output are very different from dependency parses derived from"
D14-1108,D08-1017,1,0.807895,"erienced annotator manually modified the automatically converted trees by: (a) Performing token selection (§2.1) to remove the tokens which have no syntactic function. (b) Grouping MWEs (§2.2). Here, most of the MWEs are named entities such as Manchester United. (c) Attaching the roots of the utterance in tweets to the “wall” symbol (§2.3).8 6 Stacking is a machine learning method where the predictions of one model are used to create features for another. The second model may be from a different family. Stacking has been found successful for dependency parsing by Nivre and McDonald (2008) and Martins et al. (2008). Johansson (2013) describes further advances that use path features. 7 http://nlp.cs.lth.se/software/treebank_ converter; run with -rightBranching=false -coordStructure=prague -prepAsHead=true -posAsHead=true -subAsHead=true -imAsHead=true -whAsHead=false. 8 This was infrequent; their annotations split most multitweets unique tweets tokens selected tokens types utterances multi-root tweets MWEs TRAIN TEST- NEW TEST-F OSTER 717 569 9,310 7,015 3,566 1,473 398 387 201 201 2,839 2,158 1,461 429 123 78 < 250† < 250† 2,841 2,366 1,230 337 60 109 Table 1: Statistics of our datasets. (A tweet with k"
D14-1108,D11-1022,1,0.315387,", which permits exact, efficient solution of Eq. 1 using spanning tree algorithms (McDonald et al., 2005b) or, with a projectivity constraint, dynamic programming (Eisner, 1996). Second- and third-order models have also been introduced, typically relying on approximations, since less-local features increase the computational cost, sometimes to the point of NP-hardness (McDonald and Satta, 2007). TurboParser attacks the parsing problem using a compact integer linear programming (ILP) representation of Eq. 1 (Martins et al., 2009), then employing alternating directions dual decomposition (AD3 ; Martins et al., 2011). This enables inclusion of second-order features (e.g., on a word with its sibling or grandparent; Carreras, 2007) and third-order features (e.g., a word with its parent, grandparent, and a sibling, or with its parent and two siblings; Koo and Collins, 2010). For a collection of (possibly overlapping) parts for input x, Sx (which includes the union of all parts of all trees in Yx ), we will use the following notation. Let g(x,y) = ∑ fs (x,y), (2) s∈Sx where fs only considers part s and is nonzero only if s is present in y. In the ILP framework, each s has a corresponding binary variable zs in"
D14-1108,P09-1039,1,0.732309,"ultiple roots (red; §2.3), coordination (dotted arcs, green; §3.2), and noun phrase internal structure (orange; §2.4). The internal structure of multiword expressions (dashed arcs below the sentence) was predicted automatically by a parser, as described in §2.2. tractive property from the perspective of semantic processing. To allow training a fairly conventional statistical dependency parser from these annotations, we find it expedient to apply an automatic conversion to the MWE annotations, in the spirit of Johnson (1998). We apply an existing dependency parser, the first-order TurboParser (Martins et al., 2009) trained on the Penn Treebank, to parse each MWE independently, assigning structures like those for LA Times and All the Rage in Figure 1. Arcs involving the MWE in the annotation are then reconnected to the MWE-internal root, so that the resulting tree respects the original tokenization. The MWE-internal arcs are given a special label so that the transformation can be reversed and MWEs reconstructed from parser output. 2.3 Multiple Roots For news text such as that found in the Penn Treebank, sentence segmentation is generally considered a very easy task (Reynar and Ratnaparkhi, 1997). Tweets,"
D14-1108,D10-1004,1,0.488796,"Missing"
D14-1108,P05-1012,0,0.0428376,"errors included, for example, an incorrect dependency relation between an auxiliary verb and the main verb (like ima > [have to]). Minor errors included an incorrect attachment between two modifiers of the same head, as in the > only > [grocery store]—the correct annotation would have two attachments to a single head, i.e. the > [grocery store] < only (or equivalent). feature vector representation g is used in parsing algorithms that seek to find: parse∗ (x) = argmax w⊺ g(x,y) (1) y∈Yx The score is parameterized by a vector w of weights, which are learned from data (most commonly using MIRA, McDonald et al., 2005a). The decomposition of the features into local “parts” is a critical choice affecting the computational difficulty of solving Eq. 1. The most aggressive decomposition leads to an “arc-factored” or “first-order” model, which permits exact, efficient solution of Eq. 1 using spanning tree algorithms (McDonald et al., 2005b) or, with a projectivity constraint, dynamic programming (Eisner, 1996). Second- and third-order models have also been introduced, typically relying on approximations, since less-local features increase the computational cost, sometimes to the point of NP-hardness (McDonald a"
D14-1108,H05-1066,0,0.195682,"Missing"
D14-1108,P10-1001,0,0.00934067,"approximations, since less-local features increase the computational cost, sometimes to the point of NP-hardness (McDonald and Satta, 2007). TurboParser attacks the parsing problem using a compact integer linear programming (ILP) representation of Eq. 1 (Martins et al., 2009), then employing alternating directions dual decomposition (AD3 ; Martins et al., 2011). This enables inclusion of second-order features (e.g., on a word with its sibling or grandparent; Carreras, 2007) and third-order features (e.g., a word with its parent, grandparent, and a sibling, or with its parent and two siblings; Koo and Collins, 2010). For a collection of (possibly overlapping) parts for input x, Sx (which includes the union of all parts of all trees in Yx ), we will use the following notation. Let g(x,y) = ∑ fs (x,y), (2) s∈Sx where fs only considers part s and is nonzero only if s is present in y. In the ILP framework, each s has a corresponding binary variable zs indicating whether part s is included in the output. A collection of constraints relating zs define the set of feasible vectors z that correspond to valid outputs and enfore agreement between parts that overlap. Many different versions of these constraints have"
D14-1108,W07-2216,0,0.0321536,"al., 2005a). The decomposition of the features into local “parts” is a critical choice affecting the computational difficulty of solving Eq. 1. The most aggressive decomposition leads to an “arc-factored” or “first-order” model, which permits exact, efficient solution of Eq. 1 using spanning tree algorithms (McDonald et al., 2005b) or, with a projectivity constraint, dynamic programming (Eisner, 1996). Second- and third-order models have also been introduced, typically relying on approximations, since less-local features increase the computational cost, sometimes to the point of NP-hardness (McDonald and Satta, 2007). TurboParser attacks the parsing problem using a compact integer linear programming (ILP) representation of Eq. 1 (Martins et al., 2009), then employing alternating directions dual decomposition (AD3 ; Martins et al., 2011). This enables inclusion of second-order features (e.g., on a word with its sibling or grandparent; Carreras, 2007) and third-order features (e.g., a word with its parent, grandparent, and a sibling, or with its parent and two siblings; Koo and Collins, 2010). For a collection of (possibly overlapping) parts for input x, Sx (which includes the union of all parts of all tree"
D14-1108,D10-1125,0,0.0181274,"Missing"
D14-1108,P14-5021,1,0.819279,"Missing"
D14-1108,P08-1108,0,0.0093581,"losely as possible.7 2. An experienced annotator manually modified the automatically converted trees by: (a) Performing token selection (§2.1) to remove the tokens which have no syntactic function. (b) Grouping MWEs (§2.2). Here, most of the MWEs are named entities such as Manchester United. (c) Attaching the roots of the utterance in tweets to the “wall” symbol (§2.3).8 6 Stacking is a machine learning method where the predictions of one model are used to create features for another. The second model may be from a different family. Stacking has been found successful for dependency parsing by Nivre and McDonald (2008) and Martins et al. (2008). Johansson (2013) describes further advances that use path features. 7 http://nlp.cs.lth.se/software/treebank_ converter; run with -rightBranching=false -coordStructure=prague -prepAsHead=true -posAsHead=true -subAsHead=true -imAsHead=true -whAsHead=false. 8 This was infrequent; their annotations split most multitweets unique tweets tokens selected tokens types utterances multi-root tweets MWEs TRAIN TEST- NEW TEST-F OSTER 717 569 9,310 7,015 3,566 1,473 398 387 201 201 2,839 2,158 1,461 429 123 78 < 250† < 250† 2,841 2,366 1,230 337 60 109 Table 1: Statistics of our"
D14-1108,C14-1177,0,0.0252561,"Missing"
D14-1108,N13-1039,1,0.599524,"Missing"
D14-1108,P14-2128,0,0.0325991,"Missing"
D14-1108,J93-2004,0,0.0527718,"tools. Among the tools currently available for tweets are a POS tagger (Gimpel et al., 2011; Owoputi et al., 2013) and a named entity recognizer (Ritter et al., 2011)—but not a parser. Important steps have been taken. The English Web Treebank (Bies et al., 2012) represents an annotation effort on web text—which likely lies somewhere between newspaper text and social media messages in formality and care of editing—that was sufficient to support a shared task (Petrov and McDonald, 2012). Foster et al. (2011b) annotated a small test set of tweets to evaluate parsers trained on the Penn Treebank (Marcus et al., 1993), augmented using semi-supervision and in-domain data. Others, such as Soni et al. (2014), have used existing Penn Treebank–trained models on tweets. In this work, we argue that the Penn Treebank approach to annotation—while well-matched to edited genres like newswire—is poorly suited to more informal genres. Our starting point is that rapid, small-scale annotation efforts performed by imperfectly-trained annotators should provide enough evidence to train an effective parser. We see this starting point as a necessity, given observations about the rapidly changing nature of tweets (Eisenstein,"
D14-1108,P13-2109,1,0.553015,"Missing"
D14-1108,P92-1017,0,0.491182,"s whose roots are marked by the ** symbol. Schneider et al.’s GFL offers some additional features, only some of which we made use of in this project. One important feature allows an annotator to leave the parse underspecified in some ways. We allowed our annotators to make use of this feature; however, we excluded from our training and testing data any parse that was incomplete (i.e., any parse that contained multiple disconnected fragments with no explicit root, excluding unselected tokens). Learning to parse from incomplete annotations is a fascinating topic explored in the past (Hwa, 2001; Pereira and Schabes, 1992) and, in the case of tweets, left for future work. An important feature of GFL that we did use is special notation for coordination structures. For the coordination structure in Figure 1, for example, the notation is: $a :: {♥ want} :: the attachments specially for postprocessing, following Schneider et al. (2013). In our evaluation (§5), these are treated like other attachments. {&} where $a creates a new node in the parse tree as it is visualized for the annotator, and this new node attaches to the syntactic parent of the conjoined structure, avoiding the classic forced choice between coordi"
D14-1108,A97-1004,0,0.0375716,"der TurboParser (Martins et al., 2009) trained on the Penn Treebank, to parse each MWE independently, assigning structures like those for LA Times and All the Rage in Figure 1. Arcs involving the MWE in the annotation are then reconnected to the MWE-internal root, so that the resulting tree respects the original tokenization. The MWE-internal arcs are given a special label so that the transformation can be reversed and MWEs reconstructed from parser output. 2.3 Multiple Roots For news text such as that found in the Penn Treebank, sentence segmentation is generally considered a very easy task (Reynar and Ratnaparkhi, 1997). Tweets, however, often contain multiple sentences or fragments, which we call “utterances,” each with its own syntactic root disconnected from the others. The selected tokens in Figure 1 comprise four utterances. Our approach to annotation allows multiple utterances to emerge directly from the connectedness properties of the graph implied by an annotator’s decisions. Our parser allows multiple attachments to the “wall” symbol, so that multi-rooted analyses can be predicted. 2.4 Noun Phrase Internal Structure A potentially important drawback of deriving dependency structures from phrase-struc"
D14-1108,W06-1616,0,0.0166238,"ion of (possibly overlapping) parts for input x, Sx (which includes the union of all parts of all trees in Yx ), we will use the following notation. Let g(x,y) = ∑ fs (x,y), (2) s∈Sx where fs only considers part s and is nonzero only if s is present in y. In the ILP framework, each s has a corresponding binary variable zs indicating whether part s is included in the output. A collection of constraints relating zs define the set of feasible vectors z that correspond to valid outputs and enfore agreement between parts that overlap. Many different versions of these constraints have been studied (Riedel and Clarke, 2006; Smith and Eisner, 2008; Martins et al., 2009, 2010). A key attraction of TurboParser is that many overlapping parts can be handled, making use of separate combinatorial algorithms for efficiently handling subsets of constraints. For example, the constraints that force z to encode a valid tree can be exploited within the framework by making calls 1005 to classic arborescence algorithms (Chu and Liu, 1965; Edmonds, 1967). As a result, when describing modifications to TurboParser, we need only to explain additional constraints and features imposed on parts. 4.2 Adapted Parse Parts The first col"
D14-1108,D11-1141,0,0.491647,"f traditional publications such as news reports, social media text closely represents language as it is used by people in their everyday lives. These informal texts, which account for ever larger proportions of written content, are of considerable interest to researchers, with applications such as sentiment analysis (Greene and Resnik, 2009; Kouloumpis et al., 2011). However, their often nonstandard content makes them challenging for traditional NLP tools. Among the tools currently available for tweets are a POS tagger (Gimpel et al., 2011; Owoputi et al., 2013) and a named entity recognizer (Ritter et al., 2011)—but not a parser. Important steps have been taken. The English Web Treebank (Bies et al., 2012) represents an annotation effort on web text—which likely lies somewhere between newspaper text and social media messages in formality and care of editing—that was sufficient to support a shared task (Petrov and McDonald, 2012). Foster et al. (2011b) annotated a small test set of tweets to evaluate parsers trained on the Penn Treebank (Marcus et al., 1993), augmented using semi-supervision and in-domain data. Others, such as Soni et al. (2014), have used existing Penn Treebank–trained models on twee"
D14-1108,Q14-1016,1,0.746015,"tags) average accuracy in the task of selecting tokens with a syntactic function in a ten-fold cross-validation experiment. To take Multiword Expressions We consider multiword expressions (MWEs) of two kinds. The first, proper names, have been widely modeled for information extraction purposes, and even incorporated into parsing (Finkel and Manning, 2009). (An example found in Figure 1 is LA Times.) The second, lexical idioms, have been a “pain in the neck” for many years (Sag et al., 2002) and have recently received shallow treatment in NLP (Baldwin and Kim, 2010; Constant and Sigogne, 2011; Schneider et al., 2014). Constant et al. (2012), Green et al. (2012), Candito and Constant (2014), and Le Roux et al. (2014) considered MWEs in parsing. Figure 1 provides LA Times and All the Rage as examples. Penn Treebank–style syntactic analysis (and dependency representations derived from it) does not give first-class treatment to this phenomenon, though there is precedent for marking multiword lexical units and certain kinds of idiomatic relationships (Hajiˇc et al., 2012; Abeillé et al., 2003).1 We argue that internal analysis of MWEs is not critical for many downstream applications, and therefore annotators s"
D14-1108,W13-2307,1,0.848519,"Missing"
D14-1108,D08-1016,0,0.00740608,"ing) parts for input x, Sx (which includes the union of all parts of all trees in Yx ), we will use the following notation. Let g(x,y) = ∑ fs (x,y), (2) s∈Sx where fs only considers part s and is nonzero only if s is present in y. In the ILP framework, each s has a corresponding binary variable zs indicating whether part s is included in the output. A collection of constraints relating zs define the set of feasible vectors z that correspond to valid outputs and enfore agreement between parts that overlap. Many different versions of these constraints have been studied (Riedel and Clarke, 2006; Smith and Eisner, 2008; Martins et al., 2009, 2010). A key attraction of TurboParser is that many overlapping parts can be handled, making use of separate combinatorial algorithms for efficiently handling subsets of constraints. For example, the constraints that force z to encode a valid tree can be exploited within the framework by making calls 1005 to classic arborescence algorithms (Chu and Liu, 1965; Edmonds, 1967). As a result, when describing modifications to TurboParser, we need only to explain additional constraints and features imposed on parts. 4.2 Adapted Parse Parts The first collection of parts we adap"
D14-1108,P14-2068,0,0.0232384,"; Owoputi et al., 2013) and a named entity recognizer (Ritter et al., 2011)—but not a parser. Important steps have been taken. The English Web Treebank (Bies et al., 2012) represents an annotation effort on web text—which likely lies somewhere between newspaper text and social media messages in formality and care of editing—that was sufficient to support a shared task (Petrov and McDonald, 2012). Foster et al. (2011b) annotated a small test set of tweets to evaluate parsers trained on the Penn Treebank (Marcus et al., 1993), augmented using semi-supervision and in-domain data. Others, such as Soni et al. (2014), have used existing Penn Treebank–trained models on tweets. In this work, we argue that the Penn Treebank approach to annotation—while well-matched to edited genres like newswire—is poorly suited to more informal genres. Our starting point is that rapid, small-scale annotation efforts performed by imperfectly-trained annotators should provide enough evidence to train an effective parser. We see this starting point as a necessity, given observations about the rapidly changing nature of tweets (Eisenstein, 2013), the attested difficulties of domain adaptation for parsing (Dredze et al., 2007),"
D14-1108,P10-1040,0,0.0319044,"and xc′ , we added the binned counts of unselected tokens (mostly punctuation) joint with the word form and POS tag of x p and the POS tag of xc and xc′ as features scored in the sibling(p,c,c′ ) part. The changes discussed above comprise the totality of adaptations we made to the TurboParser algorithm; we refer to them as “parsing adaptations” in the experiments. 4.3 Additional Features Brown clusters. Owoputi et al. (2013) found that Brown et al. (1992) clusters served as excellent features in Twitter POS tagging. Others have found them useful in parsing (Koo et al., 2008) and other tasks (Turian et al., 2010). We therefore follow Koo et al. in incorporating Brown clusters as features, making use of the publicly available Twitter clusters from Owoputi et al.5 We use 4 and 6 bit cluster representations to create features wherever POS tags are used, and full bit strings to create features wherever words were used. Penn Treebank features. A potential danger of our choice to “start from scratch” in developing a dependency parser for Twitter is that the resulting annotation conventions, data, and desired output are very different from dependency parses derived from the Penn Treebank. Indeed, Foster et a"
D14-1108,P07-1031,0,0.0119923,"re 1 comprise four utterances. Our approach to annotation allows multiple utterances to emerge directly from the connectedness properties of the graph implied by an annotator’s decisions. Our parser allows multiple attachments to the “wall” symbol, so that multi-rooted analyses can be predicted. 2.4 Noun Phrase Internal Structure A potentially important drawback of deriving dependency structures from phrase-structure annotations, as is typically done using the Penn Treebank, is that flat annotations lead to loss of information. This is especially notable for noun phrases in the Penn Treebank (Vadas and Curran, 2007). Consider Teen Pop Star Heartthrob in Figure 1; Penn Treebank conventions would label this as a single NP with four NN children and no internal structure. Dependency conversion tools would likely attach the first three words in the NP to Heartthrob. Direct dependency annotation (rather than phrase-structure annotation followed by automatic conversion) allows a richer treatment of such structures, which is potentially important for semantic analysis (Vecchi et al., 2013). 3 A Twitter Dependency Corpus In this section, we describe the T WEEBANK corpus, highlighting data selection (§3.1), the an"
D14-1108,D13-1015,0,0.0116876,", is that flat annotations lead to loss of information. This is especially notable for noun phrases in the Penn Treebank (Vadas and Curran, 2007). Consider Teen Pop Star Heartthrob in Figure 1; Penn Treebank conventions would label this as a single NP with four NN children and no internal structure. Dependency conversion tools would likely attach the first three words in the NP to Heartthrob. Direct dependency annotation (rather than phrase-structure annotation followed by automatic conversion) allows a richer treatment of such structures, which is potentially important for semantic analysis (Vecchi et al., 2013). 3 A Twitter Dependency Corpus In this section, we describe the T WEEBANK corpus, highlighting data selection (§3.1), the annotation process (§3.2), important convention choices (§3.3), and measures of quality (§3.4). 3.1 Data Selection We added manual dependency parses to 929 tweets (12,318 tokens) drawn from the POS-tagged Twitter corpus of Owoputi et al. (2013), which are tokenized and contain manually annotated POS tags. Owoputi et al.’s data consists of two parts. The first, originally annotated by Gimpel et al. (2011), consists of tweets sampled from a particular day, October 27, 2010—t"
D14-1108,W03-3023,0,0.0804945,"s it is visualized for the annotator, and this new node attaches to the syntactic parent of the conjoined structure, avoiding the classic forced choice between coordinator and conjunct as parent. For learning to parse, we transform GFL’s coordination structures into specially-labeled dependency parses collapsing nodes like $a with the coordinator and labeling Annotation Conventions A wide range of dependency conventions are in use; in many cases these are conversion conventions specifying how dependency trees can be derived from phrase-structure trees. For English, the most popular are due to Yamada and Matsumoto (2003) and de Marneffe and Manning (2008), known as “Yamada-Matsumoto” (YM) and “Stanford” dependencies, respectively. The main differences between them are in whether the auxiliary is the parent of the main verb (or vice versa) and whether the preposition or its argument heads a prepositional phrase (Elming et al., 2013). A full discussion of our annotation conventions is out of scope. We largely followed the conventions suggested by Schneider et al. (2013), which in turn are close to those of YM. Auxiliary verbs are parents of main verbs, and prepositions are parents of their arguments. The key di"
D14-1108,N09-1057,0,\N,Missing
D14-1108,I11-1100,0,\N,Missing
D14-1108,D07-1112,0,\N,Missing
D14-1108,J13-1009,0,\N,Missing
D15-1008,J12-2003,0,0.0716419,"Missing"
D15-1008,D11-1142,0,0.0553544,"ns. Aside from their intrinsic value, estimates of users’ political ideologies have been useful for quantifying the orientation of news media sources (Park et al., 2011; Zhou et al., 2011). We consider in this work a different task: estimating the political import of propositions like O BAMA IS A S OCIALIST. In focusing on propositional statements, we draw on a parallel, but largely independent, strand of research in open information extraction. IE systems, from early slot-filling models with predetermined ontologies (Hobbs et al., 1993) to the largescale open-vocabulary systems in use today (Fader et al., 2011; Mitchell et al., 2015) have worked toward learning type-level propositional information from text, such as BARACK O BAMA IS P RES IDENT . To a large extent, the ability to learn these facts from text is dependent on having data sources that are either relatively factual in their presentation (e.g., news articles and Wikipedia) or are sufficiently diverse to average over conflicting opinions (e.g., broad, random samples of the web). Many of the propositional statements that individuals make online are, of course, not objective descriptions of reality at all, but rather reflect their own belie"
D15-1008,H93-1026,0,0.353123,"elf-declarations, political following behavior, or third-party categorizations. Aside from their intrinsic value, estimates of users’ political ideologies have been useful for quantifying the orientation of news media sources (Park et al., 2011; Zhou et al., 2011). We consider in this work a different task: estimating the political import of propositions like O BAMA IS A S OCIALIST. In focusing on propositional statements, we draw on a parallel, but largely independent, strand of research in open information extraction. IE systems, from early slot-filling models with predetermined ontologies (Hobbs et al., 1993) to the largescale open-vocabulary systems in use today (Fader et al., 2011; Mitchell et al., 2015) have worked toward learning type-level propositional information from text, such as BARACK O BAMA IS P RES IDENT . To a large extent, the ability to learn these facts from text is dependent on having data sources that are either relatively factual in their presentation (e.g., news articles and Wikipedia) or are sufficiently diverse to average over conflicting opinions (e.g., broad, random samples of the web). Many of the propositional statements that individuals make online are, of course, not o"
D15-1008,P14-1105,0,0.183124,"tin and Quinn, 2002) estimate the political ideologies of legislators through their observed voting behavior, possibly paired with the textual content of bills (Gerrish and Blei, 2012) and debate text (Nguyen et al., 2015); other unsupervised models estimate ideologies of politicians from their speeches alone (Sim et al., 2013). Twitter users have also been modeled in a similar framework, using their observed following behavior of political elites as evidence to be explained (Barber´a, 2015). Supervised models, likewise, have not only been used for assessing the political stance of sentences (Iyyer et al., 2014) but are also very popular for predicting the holistic ideologies of everyday users on Twitter (Rao et al., 2010; Pennacchiotti and Popescu, 2011; Al Zamal et al., 2012; Cohen and Ruths, 2013; 76 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 76–85, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational Linguistics. predicate propositions, and present models that capture not only the variation in what subjects (e.g., O BAMA, ABORTION, GUN CONTROL) that individual communities are more likely to discuss, but also the variation"
D15-1008,P14-5010,0,0.0032657,"n to be a tuple comprised of a subject and predicate, each consisting of one or more words, such as hglobal warming, is a hoaxi.1 We adopt an open vocabulary approach where each unique predicate defines a unary relation. 2.1 Articles 10,305 46,068 46,114 16,830 14993 3,396 4,948 142,654 2.2 Extracting Propositions The blog comments in table 1 provide raw data from which to mine propositional assertions. In order to extract structured hsubject, predicatei propositions from text, we first parse all comments using the collapsed dependencies (de Marneffe and Manning, 2008) of the Stanford parser (Manning et al., 2014), and identify all subjects as those that hold an nsubj or nsubjpass relation to their head. In order to balance the tradeoff between generality and specificity in the representation of assertions, we extract three representations of each predicate. 1. Exact strings, which capture verbatim the specific nuance of the assertion. This includes all subjects paired with their heads and all descendants of that head. Tense and number are preserved. Data In order to extract propositions that are likely to be political in nature and exhibit variability according to ideology, we collect data from a poli"
D15-1008,D13-1010,1,0.645501,"aset of propositions judged on a political spectrum. 1 Introduction Over the past few years, much work has focussed on inferring political preferences of people from their behavior, both in unsupervised and supervised settings. Classical ideal point models (Poole and Rosenthal, 1985; Martin and Quinn, 2002) estimate the political ideologies of legislators through their observed voting behavior, possibly paired with the textual content of bills (Gerrish and Blei, 2012) and debate text (Nguyen et al., 2015); other unsupervised models estimate ideologies of politicians from their speeches alone (Sim et al., 2013). Twitter users have also been modeled in a similar framework, using their observed following behavior of political elites as evidence to be explained (Barber´a, 2015). Supervised models, likewise, have not only been used for assessing the political stance of sentences (Iyyer et al., 2014) but are also very popular for predicting the holistic ideologies of everyday users on Twitter (Rao et al., 2010; Pennacchiotti and Popescu, 2011; Al Zamal et al., 2012; Cohen and Ruths, 2013; 76 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 76–85, c Lisbon, Por"
D15-1008,P14-1018,0,0.166743,"Missing"
D15-1008,P14-1095,0,0.0161138,"ively factual in their presentation (e.g., news articles and Wikipedia) or are sufficiently diverse to average over conflicting opinions (e.g., broad, random samples of the web). Many of the propositional statements that individuals make online are, of course, not objective descriptions of reality at all, but rather reflect their own beliefs, opinions and other private mental states (Wiebe et al., 2005). While much work has investigated methods for establishing the truth content of individual sentences — whether from the perspective of veridicality (de Marneffe et al., 2012), fact assessment (Nakashole and Mitchell, 2014), or subjectivity analysis (Wiebe et al., 2003; Wilson, 2008) — the structure that exists between users and their assertions gives us an opportunity to situate them both in the same political space: in this work we operate at the level of subjectText data has recently been used as evidence in estimating the political ideologies of individuals, including political elites and social media users. While inferences about people are often the intrinsic quantity of interest, we draw inspiration from open information extraction to identify a new task: inferring the political import of propositions lik"
D15-1008,P15-1139,0,0.0936351,"ake to learn latent positions of people and propositions at the same time, and we evaluate them on a novel dataset of propositions judged on a political spectrum. 1 Introduction Over the past few years, much work has focussed on inferring political preferences of people from their behavior, both in unsupervised and supervised settings. Classical ideal point models (Poole and Rosenthal, 1985; Martin and Quinn, 2002) estimate the political ideologies of legislators through their observed voting behavior, possibly paired with the textual content of bills (Gerrish and Blei, 2012) and debate text (Nguyen et al., 2015); other unsupervised models estimate ideologies of politicians from their speeches alone (Sim et al., 2013). Twitter users have also been modeled in a similar framework, using their observed following behavior of political elites as evidence to be explained (Barber´a, 2015). Supervised models, likewise, have not only been used for assessing the political stance of sentences (Iyyer et al., 2014) but are also very popular for predicting the holistic ideologies of everyday users on Twitter (Rao et al., 2010; Pennacchiotti and Popescu, 2011; Al Zamal et al., 2012; Cohen and Ruths, 2013; 76 Proceed"
D15-1041,Q13-1034,0,0.0297403,"Missing"
D15-1041,D14-1082,0,0.480661,"m incorporating the character-based encodings of words. 1 Introduction At the heart of natural language parsing is the challenge of representing the “state” of an algorithm— what parts of a parse have been built and what parts of the input string are not yet accounted for— as it incrementally constructs a parse. Traditional approaches rely on independence assumptions, decomposition of scoring functions, and/or greedy approximations to keep this space manageable. Continuous-state parsers have been proposed, in which the state is embedded as a vector (Titov and Henderson, 2007; Stenetorp, 2013; Chen and Manning, 2014; Dyer et al., 2015; Zhou et al., 2015; Weiss et al., 2015). Dyer et al. reported state-of-the-art performance on English and Chinese benchmarks using a transition-based parser whose continuous-state embeddings were constructed using LSTM recurrent neural networks (RNNs) whose parameters were estimated to maximize the probability of a gold-standard sequence of parse actions. 1 Software for replicating the experiments is available from https://github.com/clab/lstm-parser. 349 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 349–359, c Lisbon, Portuga"
D15-1041,P14-2111,0,0.0181423,"hment scores. For English, we report (Weiss et al., 2015) and for Chinese, we report (Dyer et al., 2015) which is Words + POS but with pretrained word embeddings. 5 Related Work Character-based representations have been explored in other NLP tasks; for instance, dos Santos and Zadrozny (2014) and dos Santos and Guimar˜aes (2015) learned character-level neural representations for POS tagging and named entity recognition, getting a large error reduction in both tasks. Our approach is similar to theirs. Others have used character-based models as features to improve existing models. For instance, Chrupała (2014) used character-based recurrent neural networks to normalize tweets. Botha and Blunsom (2014) show that stems, prefixes and suffixes can be used to learn useful word representations but relying on an external morphological analyzer. That is, they learn the morpheme-meaning relationship with an additive model, whereas we do not need a morphological analyzer. Similarly, Chen et al. (2015) proposed joint learning of character and word embeddings for Chinese, claiming that characters contain rich information. 6 Conclusion We have presented several interesting findings. First, we add new evidence t"
D15-1041,D07-1022,1,0.91229,"on practice to encode morphological information in treebank POS tags; for instance, the Penn Treebank includes English number and tense (e.g., NNS is plural noun and VBD is verb in past tense). Even if our character-based representations are capable of encoding the same kind of information, existing POS tags suffice for high accuracy. However, the POS tags in treebanks for morphologically rich languages do not seem to be enough. Swedish, English, and French use suffixes for the verb tenses and number,8 while Hebrew uses prepositional particles rather than grammatical case. Tsarfaty (2006) and Cohen and Smith (2007) argued that, for Hebrew, determining the correct morphological segmentation is dependent on syntactic context. Our approach sidesteps this step, capturing the same kind of information in the vectors, and learning it from syntactic context. Even for Chinese, which is not morphologically rich, Chars shows a benefit over Words, perhaps by capturing regularities in syllable structure within words. • Words: words only, as in §3.1 (but without POS tags) • Chars: character-based representations of words with bidirectional LSTMs, as in §3.2 (but without POS tags) • Words + POS: words and POS tags (§3"
D15-1041,W15-3904,0,0.0444823,"Missing"
D15-1041,P15-1033,1,0.075747,"ecially in agglutinative languages and the ones that present extensive case systems (§4). In languages that show little morphology, performance remains good, showing that the RNN composition strategy is capable of capturing both morphological regularities and arbitrariness in the sense of Saussure (1916). Finally, a particularly noteworthy result is that we find that character-based word embeddings in some cases obviate explicit POS information, which is usually found to be indispensable for accurate parsing. A secondary contribution of this work is to show that the continuous-state parser of Dyer et al. (2015) can learn to generate nonprojective trees. We do this by augmenting its transition operations We present extensions to a continuousstate dependency parsing method that makes it applicable to morphologically rich languages. Starting with a highperformance transition-based parser that uses long short-term memory (LSTM) recurrent neural networks to learn representations of the parser state, we replace lookup-based word representations with representations constructed from the orthographic representations of the words, also using LSTMs. This allows statistical sharing across word forms that are s"
D15-1041,P11-2124,0,0.0240466,"orted (“Best Published Result”). All of the systems we compare against use explicit morphological features and/or one of the following: pretrained word embeddings, unlabeled data and a combination of parsers; our models do not. B’13 is Ballesteros (2013); N+’06a is Nivre et al. (2006a); D+’15 is Dyer et al. (2015); B+’13 is Bj¨orkelund et al. (2013); B+’14 is Bj¨orkelund et al. (2014); K+’10 is Koo et al. (2010); W+’15 is Weiss et al. (2015). Methods for joint morphological disambiguation and parsing have been widely explored Tsarfaty (2006; Cohen and Smith (2007; Goldberg and Tsarfaty (2008; Goldberg and Elhadad (2011). More recently, Bohnet et al. (2013) presented an arc-standard transition-based parser that performs competitively for joint morphological tagging and dependency parsing for richly inflected languages, such as Czech, Finnish, German, Hungarian, and Russian. Our model seeks to achieve a similar benefit to parsing without explicitly reasoning about the internal structure of words. Zhang et al. (2013) presented efforts on Chinese parsing with characters showing that Chinese can be parsed at the character level, and that Chinese word segmentation is useful for predicting the correct POS tags (Zha"
D15-1041,W13-4907,1,0.945071,"on University, Pittsburgh, PA, USA ♣ Marianas Labs, Pittsburgh, PA, USA ♥ Computer Science & Engineering, University of Washington, Seattle, WA, USA miguel.ballesteros@upf.edu, chris@marianaslabs.com, nasmith@cs.washington.edu Abstract The primary contribution made in this work is to take the idea of continuous-state parsing a step further by making the word embeddings that are used to construct the parse state sensitive to the morphology of the words.1 Since it it is well known that a word’s form often provides strong evidence regarding its grammatical role in morphologically rich languages (Ballesteros, 2013, inter alia), this has promise to improve accuracy and statistical efficiency relative to traditional approaches that treat each word type as opaque and independently modeled. In the traditional parameterization, words with similar grammatical roles will only be embedded near each other if they are observed in similar contexts with sufficient frequency. Our approach reparameterizes word embeddings using the same RNN machinery used in the parser: a word’s vector is calculated based on the sequence of orthographic symbols representing it (§3). Although our model is provided no supervision in th"
D15-1041,Q13-1033,0,0.0347924,"sualization was produced using t-SNE; see http: //lvdmaaten.github.io/tsne/. word of the input sentence; however, at test time these results could be cached. On average, Words parses a sentence in 44 ms, whileChars needs 130 ms.9 Training time is affected by the same cons9 We are using a machine with 32 Intel Xeon CPU E52650 at 2.00GHz; the parser runs on a single core. 355 Comparison with State-of-the-Art Table 3 shows a comparison with state-of-theart parsers. We include greedy transition-based parsers that, like ours, do not apply a beam search (Zhang and Clark, 2008b) or a dynamic oracle (Goldberg and Nivre, 2013). For all the SPMRL languages we show the results of Ballesteros (2013), who reported results after carrying out a careful automatic morphological feature selection experiment. For Turkish, we show the results of Nivre et al. (2006a) which also carried out a careful manual morphological feature selection. Our parser outperforms these in most cases. Since those systems rely on morphological features, we believe that this comparison shows even more that the character-based representations are capturing morphological information, though without explicit morphological features. For English and Chi"
D15-1041,W13-4916,0,0.0362755,"Missing"
D15-1041,P08-1043,0,0.0336093,"Result”) and best results reported (“Best Published Result”). All of the systems we compare against use explicit morphological features and/or one of the following: pretrained word embeddings, unlabeled data and a combination of parsers; our models do not. B’13 is Ballesteros (2013); N+’06a is Nivre et al. (2006a); D+’15 is Dyer et al. (2015); B+’13 is Bj¨orkelund et al. (2013); B+’14 is Bj¨orkelund et al. (2014); K+’10 is Koo et al. (2010); W+’15 is Weiss et al. (2015). Methods for joint morphological disambiguation and parsing have been widely explored Tsarfaty (2006; Cohen and Smith (2007; Goldberg and Tsarfaty (2008; Goldberg and Elhadad (2011). More recently, Bohnet et al. (2013) presented an arc-standard transition-based parser that performs competitively for joint morphological tagging and dependency parsing for richly inflected languages, such as Czech, Finnish, German, Hungarian, and Russian. Our model seeks to achieve a similar benefit to parsing without explicitly reasoning about the internal structure of words. Zhang et al. (2013) presented efforts on Chinese parsing with characters showing that Chinese can be parsed at the character level, and that Chinese word segmentation is useful for predict"
D15-1041,W14-6111,0,0.046747,"traditional parameterization, words with similar grammatical roles will only be embedded near each other if they are observed in similar contexts with sufficient frequency. Our approach reparameterizes word embeddings using the same RNN machinery used in the parser: a word’s vector is calculated based on the sequence of orthographic symbols representing it (§3). Although our model is provided no supervision in the form of explicit morphological annotation, we find that it gives a large performance increase when parsing morphologically rich languages in the SPMRL datasets (Seddah et al., 2013; Seddah and Tsarfaty, 2014), especially in agglutinative languages and the ones that present extensive case systems (§4). In languages that show little morphology, performance remains good, showing that the RNN composition strategy is capable of capturing both morphological regularities and arbitrariness in the sense of Saussure (1916). Finally, a particularly noteworthy result is that we find that character-based word embeddings in some cases obviate explicit POS information, which is usually found to be indispensable for accurate parsing. A secondary contribution of this work is to show that the continuous-state parse"
D15-1041,P13-1088,0,0.0104461,"te (learned) embeddings of words and relations, script symbols indicate the corresponding words and relations. Dyer et al. (2015) used the SHIFT and REDUCE operations in their continuous-state parser; we add SWAP. the representations of the two subtrees popped from S (we denote these by u and v), resulting in a new vector gr (u, v) or gr (v, u), depending on the direction of attachment. The resulting vector embeds the tree fragment in the same space as the words and other tree fragments. This kind of composition was thoroughly explored in prior work (Socher et al., 2011; Socher et al., 2013b; Hermann and Blunsom, 2013; Socher et al., 2013a); for details, see Dyer et al. (2015). 2.3 nonprojective trees. Here, the inclusion of the SWAP operation requires breaking the linearity of the stack by removing tokens that are not at the top of the stack. This is easily handled with the stack LSTM. Figure 1 shows how the parser is capable of moving words from the stack (S) to the buffer (B), breaking the linear order of words. Since a node that is swapped may have already been assigned as the head of a dependent, the buffer (B) can now also contain tree fragments. Predicting Parser Decisions 3 The parser uses a probab"
D15-1041,P82-1020,0,0.877457,"Missing"
D15-1041,D10-1125,0,0.00949521,"t performance of our best results (according to UAS or LAS, whichever has the larger difference), compared to state-of-the-art greedy transition-based parsers (“Best Greedy Result”) and best results reported (“Best Published Result”). All of the systems we compare against use explicit morphological features and/or one of the following: pretrained word embeddings, unlabeled data and a combination of parsers; our models do not. B’13 is Ballesteros (2013); N+’06a is Nivre et al. (2006a); D+’15 is Dyer et al. (2015); B+’13 is Bj¨orkelund et al. (2013); B+’14 is Bj¨orkelund et al. (2014); K+’10 is Koo et al. (2010); W+’15 is Weiss et al. (2015). Methods for joint morphological disambiguation and parsing have been widely explored Tsarfaty (2006; Cohen and Smith (2007; Goldberg and Tsarfaty (2008; Goldberg and Elhadad (2011). More recently, Bohnet et al. (2013) presented an arc-standard transition-based parser that performs competitively for joint morphological tagging and dependency parsing for richly inflected languages, such as Czech, Finnish, German, Hungarian, and Russian. Our model seeks to achieve a similar benefit to parsing without explicitly reasoning about the internal structure of words. Zhang"
D15-1041,seeker-kuhn-2012-making,0,0.013972,"se vectors and a (learned) representation of their tag to produce the representation w. As in §3.1, a linear map (V) is applied and passed through a component-wise ReLU. n o → ← x = max 0, V[w; w; t] + b sults below. 4.1 Data In order to find out whether the character-based representations are capable of learning the morphology of words, we applied the parser to morphologically rich languages specifically the treebanks of the SPMRL shared task (Seddah et al., 2013; Seddah and Tsarfaty, 2014): Arabic (Maamouri et al., 2004), Basque (Aduriz et al., 2003), French (Abeill´e et al., 2003), German (Seeker and Kuhn, 2012), Hebrew (Sima’an et al., 2001), Hungarian (Vincze et al., 2010), Korean ´ (Choi, 2013), Polish (Swidzi´ nski and Woli´nski, 2010) and Swedish (Nivre et al., 2006b). For all the corpora of the SPMRL Shared Task we used predicted POS tags as provided by the shared task organizers.3 For these datasets, evaluation is calculated using eval07.pl, which includes punctuation. We also experimented with the Turkish dependency treebank4 (Oflazer et al., 2003) of the CoNLL-X Shared Task (Buchholz and Marsi, 2006). We used gold POS tags, as is common with the CoNLL-X data sets. To put our results in conte"
D15-1041,J93-2004,0,0.0644981,"98.72. Korean: 94.03. Polish: 98.12. Swedish: 97.27. 4 Since the Turkish dependency treebank does not have a development set, we extracted the last 150 sentences from the 4996 sentences of the training set as a development set. Experiments We applied our parsing model and several variations of it to several parsing tasks and report re352 setups as Chen and Manning (2014) and Dyer et al. (2015). For Chinese, we use the Penn Chinese Treebank 5.1 (CTB5) following Zhang and Clark (2008b),5 with gold POS tags. For English, we used the Stanford Dependency (SD) representation of the Penn Treebank6 (Marcus et al., 1993; Marneffe et al., 2006).7 . Results for Turkish, Chinese, and English are calculated using the CoNLL-X eval.pl script, which ignores punctuation symbols. 4.2 4.3 Training Procedure Parameters are initialized randomly—refer to Dyer et al. (2015) for specifics—and optimized using stochastic gradient descent (without minibatches) using derivatives of the negative log likelihood of the sequence of parsing actions computed using backpropagation. Training is stopped when the learned model’s UAS stops improving on the development set, and this model is used to parse the test set. No pretraining of a"
D15-1041,de-marneffe-etal-2006-generating,0,0.0548307,"Missing"
D15-1041,D13-1032,0,0.0302696,"Missing"
D15-1041,D13-1170,0,0.00309807,"s. Bold symbols indicate (learned) embeddings of words and relations, script symbols indicate the corresponding words and relations. Dyer et al. (2015) used the SHIFT and REDUCE operations in their continuous-state parser; we add SWAP. the representations of the two subtrees popped from S (we denote these by u and v), resulting in a new vector gr (u, v) or gr (v, u), depending on the direction of attachment. The resulting vector embeds the tree fragment in the same space as the words and other tree fragments. This kind of composition was thoroughly explored in prior work (Socher et al., 2011; Socher et al., 2013b; Hermann and Blunsom, 2013; Socher et al., 2013a); for details, see Dyer et al. (2015). 2.3 nonprojective trees. Here, the inclusion of the SWAP operation requires breaking the linearity of the stack by removing tokens that are not at the top of the stack. This is easily handled with the stack LSTM. Figure 1 shows how the parser is capable of moving words from the stack (S) to the buffer (B), breaking the linear order of words. Since a node that is swapped may have already been assigned as the head of a dependent, the buffer (B) can now also contain tree fragments. Predicting Parser Decision"
D15-1041,W06-2933,0,0.0487182,"Missing"
D15-1041,nivre-etal-2006-talbanken05,0,0.0429632,"LU. n o → ← x = max 0, V[w; w; t] + b sults below. 4.1 Data In order to find out whether the character-based representations are capable of learning the morphology of words, we applied the parser to morphologically rich languages specifically the treebanks of the SPMRL shared task (Seddah et al., 2013; Seddah and Tsarfaty, 2014): Arabic (Maamouri et al., 2004), Basque (Aduriz et al., 2003), French (Abeill´e et al., 2003), German (Seeker and Kuhn, 2012), Hebrew (Sima’an et al., 2001), Hungarian (Vincze et al., 2010), Korean ´ (Choi, 2013), Polish (Swidzi´ nski and Woli´nski, 2010) and Swedish (Nivre et al., 2006b). For all the corpora of the SPMRL Shared Task we used predicted POS tags as provided by the shared task organizers.3 For these datasets, evaluation is calculated using eval07.pl, which includes punctuation. We also experimented with the Turkish dependency treebank4 (Oflazer et al., 2003) of the CoNLL-X Shared Task (Buchholz and Marsi, 2006). We used gold POS tags, as is common with the CoNLL-X data sets. To put our results in context with the most recent neural network transition-based parsers, we run the parser in the same Chinese and English This process is shown schematically in Figure 3"
D15-1041,W07-2218,0,0.0081179,"ges show that the parsing model benefits from incorporating the character-based encodings of words. 1 Introduction At the heart of natural language parsing is the challenge of representing the “state” of an algorithm— what parts of a parse have been built and what parts of the input string are not yet accounted for— as it incrementally constructs a parse. Traditional approaches rely on independence assumptions, decomposition of scoring functions, and/or greedy approximations to keep this space manageable. Continuous-state parsers have been proposed, in which the state is embedded as a vector (Titov and Henderson, 2007; Stenetorp, 2013; Chen and Manning, 2014; Dyer et al., 2015; Zhou et al., 2015; Weiss et al., 2015). Dyer et al. reported state-of-the-art performance on English and Chinese benchmarks using a transition-based parser whose continuous-state embeddings were constructed using LSTM recurrent neural networks (RNNs) whose parameters were estimated to maximize the probability of a gold-standard sequence of parse actions. 1 Software for replicating the experiments is available from https://github.com/clab/lstm-parser. 349 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Pro"
D15-1041,W04-0308,0,0.155705,"ng the parser to produce nonprojective dependencies which are often found in morphologically rich languages. 2 An LSTM Dependency Parser We begin by reviewing the parsing approach of Dyer et al. (2015) on which our work is based. Like most transition-based parsers, Dyer et al.’s parser can be understood as the sequential manipulation of three data structures: a buffer B initialized with the sequence of words to be parsed, a stack S containing partially-built parses, and a list A of actions previously taken by the parser. In particular, the parser implements the arc-standard parsing algorithm (Nivre, 2004). At each time step t, a transition action is applied that alters these data structures by pushing or popping words from the stack and the buffer; the operations are listed in Figure 1. Along with the discrete transitions above, the parser calculates a vector representation of the states of B, S, and A; at time step t these are denoted by bt , st , and at , respectively. The total parser state at t is given by pt = max {0, W[st ; bt ; at ] + d} it = σ(Wix xt + Wih ht−1 + Wic ct−1 + bi ) ft = 1 − it ct = ft ct−1 + it tanh(Wcx xt + Wch ht−1 + bc ) ot = σ(Wox xt + Woh ht−1 + Woc ct + bo ) ht = ot"
D15-1041,N03-1033,0,0.216746,"Missing"
D15-1041,P09-1040,0,0.427849,"stm-parser. 349 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 349–359, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational Linguistics. Past work explains the computation within an LSTM through the metaphors of deciding how much of the current input to pass into memory (it ) or forget (ft ). We refer interested readers to the original papers and present only the recursive equations updating the memory cell ct and hidden state ht given xt , the previous hidden state ht−1 , and the memory cell ct−1 : with a SWAP operation (Nivre, 2009) (§2.4), enabling the parser to produce nonprojective dependencies which are often found in morphologically rich languages. 2 An LSTM Dependency Parser We begin by reviewing the parsing approach of Dyer et al. (2015) on which our work is based. Like most transition-based parsers, Dyer et al.’s parser can be understood as the sequential manipulation of three data structures: a buffer B initialized with the sequence of words to be parsed, a stack S containing partially-built parses, and a list A of actions previously taken by the parser. In particular, the parser implements the arc-standard pars"
D15-1041,P06-3009,0,0.199759,"rs + POS. It is common practice to encode morphological information in treebank POS tags; for instance, the Penn Treebank includes English number and tense (e.g., NNS is plural noun and VBD is verb in past tense). Even if our character-based representations are capable of encoding the same kind of information, existing POS tags suffice for high accuracy. However, the POS tags in treebanks for morphologically rich languages do not seem to be enough. Swedish, English, and French use suffixes for the verb tenses and number,8 while Hebrew uses prepositional particles rather than grammatical case. Tsarfaty (2006) and Cohen and Smith (2007) argued that, for Hebrew, determining the correct morphological segmentation is dependent on syntactic context. Our approach sidesteps this step, capturing the same kind of information in the vectors, and learning it from syntactic context. Even for Chinese, which is not morphologically rich, Chars shows a benefit over Words, perhaps by capturing regularities in syllable structure within words. • Words: words only, as in §3.1 (but without POS tags) • Chars: character-based representations of words with bidirectional LSTMs, as in §3.2 (but without POS tags) • Words +"
D15-1041,P15-1032,0,0.60146,"oduction At the heart of natural language parsing is the challenge of representing the “state” of an algorithm— what parts of a parse have been built and what parts of the input string are not yet accounted for— as it incrementally constructs a parse. Traditional approaches rely on independence assumptions, decomposition of scoring functions, and/or greedy approximations to keep this space manageable. Continuous-state parsers have been proposed, in which the state is embedded as a vector (Titov and Henderson, 2007; Stenetorp, 2013; Chen and Manning, 2014; Dyer et al., 2015; Zhou et al., 2015; Weiss et al., 2015). Dyer et al. reported state-of-the-art performance on English and Chinese benchmarks using a transition-based parser whose continuous-state embeddings were constructed using LSTM recurrent neural networks (RNNs) whose parameters were estimated to maximize the probability of a gold-standard sequence of parse actions. 1 Software for replicating the experiments is available from https://github.com/clab/lstm-parser. 349 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 349–359, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational"
D15-1041,P08-1101,0,0.0599117,"he SPMRL Shared Task (Bj¨orkelund et al., 2013). Arabic: 97.38. Basque: 97.02. French: 97.61. German: 98.10. Hebrew: 97.09. Hungarian: 98.72. Korean: 94.03. Polish: 98.12. Swedish: 97.27. 4 Since the Turkish dependency treebank does not have a development set, we extracted the last 150 sentences from the 4996 sentences of the training set as a development set. Experiments We applied our parsing model and several variations of it to several parsing tasks and report re352 setups as Chen and Manning (2014) and Dyer et al. (2015). For Chinese, we use the Penn Chinese Treebank 5.1 (CTB5) following Zhang and Clark (2008b),5 with gold POS tags. For English, we used the Stanford Dependency (SD) representation of the Penn Treebank6 (Marcus et al., 1993; Marneffe et al., 2006).7 . Results for Turkish, Chinese, and English are calculated using the CoNLL-X eval.pl script, which ignores punctuation symbols. 4.2 4.3 Training Procedure Parameters are initialized randomly—refer to Dyer et al. (2015) for specifics—and optimized using stochastic gradient descent (without minibatches) using derivatives of the negative log likelihood of the sequence of parsing actions computed using backpropagation. Training is stopped wh"
D15-1041,D08-1059,0,0.407672,"he SPMRL Shared Task (Bj¨orkelund et al., 2013). Arabic: 97.38. Basque: 97.02. French: 97.61. German: 98.10. Hebrew: 97.09. Hungarian: 98.72. Korean: 94.03. Polish: 98.12. Swedish: 97.27. 4 Since the Turkish dependency treebank does not have a development set, we extracted the last 150 sentences from the 4996 sentences of the training set as a development set. Experiments We applied our parsing model and several variations of it to several parsing tasks and report re352 setups as Chen and Manning (2014) and Dyer et al. (2015). For Chinese, we use the Penn Chinese Treebank 5.1 (CTB5) following Zhang and Clark (2008b),5 with gold POS tags. For English, we used the Stanford Dependency (SD) representation of the Penn Treebank6 (Marcus et al., 1993; Marneffe et al., 2006).7 . Results for Turkish, Chinese, and English are calculated using the CoNLL-X eval.pl script, which ignores punctuation symbols. 4.2 4.3 Training Procedure Parameters are initialized randomly—refer to Dyer et al. (2015) for specifics—and optimized using stochastic gradient descent (without minibatches) using derivatives of the negative log likelihood of the sequence of parsing actions computed using backpropagation. Training is stopped wh"
D15-1041,P13-1013,0,0.0124566,"2010); W+’15 is Weiss et al. (2015). Methods for joint morphological disambiguation and parsing have been widely explored Tsarfaty (2006; Cohen and Smith (2007; Goldberg and Tsarfaty (2008; Goldberg and Elhadad (2011). More recently, Bohnet et al. (2013) presented an arc-standard transition-based parser that performs competitively for joint morphological tagging and dependency parsing for richly inflected languages, such as Czech, Finnish, German, Hungarian, and Russian. Our model seeks to achieve a similar benefit to parsing without explicitly reasoning about the internal structure of words. Zhang et al. (2013) presented efforts on Chinese parsing with characters showing that Chinese can be parsed at the character level, and that Chinese word segmentation is useful for predicting the correct POS tags (Zhang and Clark, 2008a). To the best of our knowledge, previous work has not used character-based embeddings to improve dependency parsers, as done in this paper. tem for all languages. Note that the comparison is harsh to our system, which does not use unlabeled data or explicit morphological features nor any combination of different parsers. For Turkish, we report the results of Koo et al. (2010), wh"
D15-1041,P15-1117,0,0.0362065,"gs of words. 1 Introduction At the heart of natural language parsing is the challenge of representing the “state” of an algorithm— what parts of a parse have been built and what parts of the input string are not yet accounted for— as it incrementally constructs a parse. Traditional approaches rely on independence assumptions, decomposition of scoring functions, and/or greedy approximations to keep this space manageable. Continuous-state parsers have been proposed, in which the state is embedded as a vector (Titov and Henderson, 2007; Stenetorp, 2013; Chen and Manning, 2014; Dyer et al., 2015; Zhou et al., 2015; Weiss et al., 2015). Dyer et al. reported state-of-the-art performance on English and Chinese benchmarks using a transition-based parser whose continuous-state embeddings were constructed using LSTM recurrent neural networks (RNNs) whose parameters were estimated to maximize the probability of a gold-standard sequence of parse actions. 1 Software for replicating the experiments is available from https://github.com/clab/lstm-parser. 349 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 349–359, c Lisbon, Portugal, 17-21 September 2015. 2015 Associat"
D15-1041,D15-1176,1,\N,Missing
D15-1041,W13-4917,0,\N,Missing
D15-1041,Q14-1017,0,\N,Missing
D15-1041,vincze-etal-2010-hungarian,0,\N,Missing
D15-1175,D08-1038,0,0.0446157,"gh latent topic mixtures of documents they have co-authored (Rosen-Zvi et al., 2004) and their collaboration networks (Johri et al., 2011). 8 A closely related problem is that of authorship attribution. There has been extensive research on authorship attribution focusing mainly on learning “stylometric” features of authors; see Stamatatos (2009) for a detailed review. Like our paper, the latter two are based on topic models, which have been popular for modeling the content of scientific articles. For instance, Gerrish and Blei (2010) measured scholarly impact using dynamic topic models, while Hall et al. (2008) analyzed the output of topic models to study the “history of ideas.” Predicting responses to scientific articles was explored in two shared tasks at KDD Cup 2003 (Brank and Leskovec, 2003; McGovern et al., 2003) and by Yogatama et al. (2011), which served as a baseline for our experiments and whose timeseries prior we used in our model. Furthermore, there has been considerable research using topic models to predict (or recommend) citations (instead of aggregate counts), such as modeling link probabilities within the LDA framework (Cohn and Hofmann, 2000; Erosheva et al., 2004; Nallapati and C"
D15-1175,N15-1008,0,0.0293461,"two shared tasks at KDD Cup 2003 (Brank and Leskovec, 2003; McGovern et al., 2003) and by Yogatama et al. (2011), which served as a baseline for our experiments and whose timeseries prior we used in our model. Furthermore, there has been considerable research using topic models to predict (or recommend) citations (instead of aggregate counts), such as modeling link probabilities within the LDA framework (Cohn and Hofmann, 2000; Erosheva et al., 2004; Nallapati and Cohen, 2008; Kataria et al., 2010; Zhu et al., 2013) and augmenting topics with discriminative author features (Liu et al., 2009; Tanner and Charniak, 2015). We modeled both interests of authors and responses to their articles jointly, by assuming authors’ text production is an expected utilitymaximizing decision. This approach is similar to our earlier work (Sim et al., 2015), where authors are rational agents writing texts to maximize the chance of a favorable decision by a judicial court. In that study, we did not consider the unique preferences of each decision making agent, nor the extrinsic-intrinsic reward tradeoffs that these agents face when authoring a document. Our utility model can also be viewed as a form of natural language generato"
D15-1175,N03-1033,0,0.0339783,"Missing"
D15-1175,N13-1127,0,0.0263305,"e unique preferences of each decision making agent, nor the extrinsic-intrinsic reward tradeoffs that these agents face when authoring a document. Our utility model can also be viewed as a form of natural language generator, where we take into account the context of an author (i.e., his preferences, the tradeoff coefficient, and what is popular) to generate his document. This is related to natural language pragmatics, where text is influenced by context.9 Hovy (1990) approached the problem of generating text under pragmatic circumstances from a planning and goal-orientation perspective, while Vogel et al. (2013) used multiagent decision-theoretic models to show cooperative pragmatic behavior. Vogel et al.’s models suggest an interesting extension of ours for future work: modeling cooperation among co-authors and, perhaps, in the larger scientific discourse. 9 The β vectors can be seen as a na¨ıve representation of world knowledge that motivates an author to select content that reflects his behavioral preferences and intentions. 1517 7 Conclusions likelihood: We presented a model of scientific authorship in which authors trade off between seeking citation by others and staying true to their individual"
D15-1175,D11-1055,1,0.900204,"Missing"
D15-1228,P13-1020,0,0.165137,"P might be supplanted by alternatives designed specifically for embeddings. In this work, we consider summarization. Classical approaches to extractive summarization represent each sentence as a bag of terms (typically bigrams) and seek a subset of sentences from the input document(s) that either (a) trade off between high relevance and low redundancy (Carbonell and Goldstein, 1998; McDonald, 2007), or (b) maximize bigram coverage (Yih et al., 2007; Gillick et al., 2008). The sentence representation is fundamentally discrete, and a range of greedy (Carbonell and Goldstein, 1998), approximate (Almeida and Martins, 2013), and exact optimization algorithms (McDonald, 2007; Martins and Smith, 2009; Berg-Kirkpatrick et al., 2011) have been proposed. Recent studies have explored continuous sentence representations, including the paragraph vector (Le and Mikolov, 2014), a convolutional neural network architecture (Kalchbrenner et al., 2014), and a dictionary learning approach (Jenatton et al., 2011). If sentences are represented as low-dimensional embeddings in a distributed semantic space, then we begin to imagine a geometric relationship between a summary and a document. We propose that the volume of a summary ("
D15-1228,P11-1049,0,0.107664,"mmarization. Classical approaches to extractive summarization represent each sentence as a bag of terms (typically bigrams) and seek a subset of sentences from the input document(s) that either (a) trade off between high relevance and low redundancy (Carbonell and Goldstein, 1998; McDonald, 2007), or (b) maximize bigram coverage (Yih et al., 2007; Gillick et al., 2008). The sentence representation is fundamentally discrete, and a range of greedy (Carbonell and Goldstein, 1998), approximate (Almeida and Martins, 2013), and exact optimization algorithms (McDonald, 2007; Martins and Smith, 2009; Berg-Kirkpatrick et al., 2011) have been proposed. Recent studies have explored continuous sentence representations, including the paragraph vector (Le and Mikolov, 2014), a convolutional neural network architecture (Kalchbrenner et al., 2014), and a dictionary learning approach (Jenatton et al., 2011). If sentences are represented as low-dimensional embeddings in a distributed semantic space, then we begin to imagine a geometric relationship between a summary and a document. We propose that the volume of a summary (i.e., the semantic subspace spanned by the selected sentences) should ideally be large. We therefore formali"
D15-1228,D12-1065,0,0.0165877,"s We use SVD in this study for computing sentence embeddings. As mentioned previously, our summariza1964 tion approach can benefit from advances in neuralnetwork-based sentence representations (Jenatton et al., 2011; Le and Mikolov, 2014; Kalchbrenner et al., 2014). These models can also produce vector representations of sentences, so Algorithm 1 can be readily applied to the learned representations. Our work opens up a possibility to make summarization a future benchmark task for evaluating the quality of sentence representations. Our method is related to determinantal point processes (DPPs; Gillenwater et al., 2012; Kulesza and Taskar, 2012) in that they both seek to maximize the volume spanned by sentence vectors to produce a summary. In DPP-based approaches, quality and selectional diversity correspond to vector magnitude and angle respectively. In this work, the length of a sentence vector is not tailored to encode quality in terms of representativeness directly. In contrast, we rely on sentence embedding methods to produce a semantic space and assume that a good summary should have a large volume in the semantic space. We show that a simple singular value decomposition embedding of sentences—one tha"
D15-1228,P14-1062,0,0.031379,"igh relevance and low redundancy (Carbonell and Goldstein, 1998; McDonald, 2007), or (b) maximize bigram coverage (Yih et al., 2007; Gillick et al., 2008). The sentence representation is fundamentally discrete, and a range of greedy (Carbonell and Goldstein, 1998), approximate (Almeida and Martins, 2013), and exact optimization algorithms (McDonald, 2007; Martins and Smith, 2009; Berg-Kirkpatrick et al., 2011) have been proposed. Recent studies have explored continuous sentence representations, including the paragraph vector (Le and Mikolov, 2014), a convolutional neural network architecture (Kalchbrenner et al., 2014), and a dictionary learning approach (Jenatton et al., 2011). If sentences are represented as low-dimensional embeddings in a distributed semantic space, then we begin to imagine a geometric relationship between a summary and a document. We propose that the volume of a summary (i.e., the semantic subspace spanned by the selected sentences) should ideally be large. We therefore formalize a new objective function for summarization based on semantic volume (§2), and we provide a fast greedy algorithm that can be used to maximize it (§3). We show that our method outperforms competing extractive ba"
D15-1228,D12-1022,0,0.0258427,"e that is not especially tuned for this task—produces reasonably good results. We leave exploration of other sentence embedding methods to future work. Finally, an interesting future direction is finding an exact tractable solution to the volume maximization problem (or demonstrating that one does not exist). Future work Our method could be extended for compressive summarization, by simply including compressed sentences in the embedded space and running Algorithm 1 without any change. This resembles the summarization methods that jointly extracts and compresses (Berg-Kirkpatrick et al., 2011; Woodsend and Lapata, 2012; Almeida and Martins, 2013). Another alternative is a pipeline approach, where extractive summarization is followed or preceded by a sentence compression module, which can be built and tuned independent of our proposed extractive method (Knight and Marcu, 2000; Lin, 2003; Zajic et al., 2007; Wang et al., 2013; Li et al., 2013). We are also interested in exploring volume as a relevance function within MMR. MMR avoids redundancy by penalizing redundant sentences, whereas in our method semantic redundancy is inherently discouraged since the method chooses sentences to maximize volume. Depending"
D15-1228,D13-1047,1,0.453076,"d be extended for compressive summarization, by simply including compressed sentences in the embedded space and running Algorithm 1 without any change. This resembles the summarization methods that jointly extracts and compresses (Berg-Kirkpatrick et al., 2011; Woodsend and Lapata, 2012; Almeida and Martins, 2013). Another alternative is a pipeline approach, where extractive summarization is followed or preceded by a sentence compression module, which can be built and tuned independent of our proposed extractive method (Knight and Marcu, 2000; Lin, 2003; Zajic et al., 2007; Wang et al., 2013; Li et al., 2013). We are also interested in exploring volume as a relevance function within MMR. MMR avoids redundancy by penalizing redundant sentences, whereas in our method semantic redundancy is inherently discouraged since the method chooses sentences to maximize volume. Depending on the method used to embed sentences, this might not translate directly into avoiding n-gram redundancy. Plugging our scoring function to an MMR objective is a simple way to enforce diversity. Sanjeev Arora, Rong Ge, Ravi Kannan, and Ankur Moitra. 2012. Computing a nonnegative matrix factorization – provably. In Proc. of STOC."
D15-1228,N10-1134,0,0.0157165,"Goldstein, 1998) considers the following scoring function: score(D, y) = N X yi Rel(si ) − i=1 N X yi yj Sim(si , sj ) i,j=1 where Rel(si ) measures the relevancy of sentence i and Sim(si , sj ) measures the (e.g., cosine) similarity between sentence i and sentence j. The intuition is to choose sentences that are highly relevant to the document(s) and avoid redundancy. The above maximization problem has been shown to be NP-hard, solvable exactly using ILP (McDonald, 2007). A greedy algorithm that approximates the global solution by adding one sentence at a time to maximize the overall score (Lin and Bilmes, 2010) is often used in practice. 2.2 Coverage-Based Summarization Another popular scoring function aims to give higher scores for covering more diverse concepts in the summary. Gillick et al. (2008) use bigrams as a surrogate for concepts. Following convention, we extract bigrams from each sentence si ∈ D. Denote the number of unique bigrams extracted from all sentences by B. We introduce another binary vector z ∈ RB to indicate the presence or absence of a bigram in the summary, and a binary indicator matrix M ∈ RN ×B , where mi,j is 1 if and only if bigram j is present in sentence i and 0 otherwi"
D15-1228,W03-1101,0,0.0240553,"t one does not exist). Future work Our method could be extended for compressive summarization, by simply including compressed sentences in the embedded space and running Algorithm 1 without any change. This resembles the summarization methods that jointly extracts and compresses (Berg-Kirkpatrick et al., 2011; Woodsend and Lapata, 2012; Almeida and Martins, 2013). Another alternative is a pipeline approach, where extractive summarization is followed or preceded by a sentence compression module, which can be built and tuned independent of our proposed extractive method (Knight and Marcu, 2000; Lin, 2003; Zajic et al., 2007; Wang et al., 2013; Li et al., 2013). We are also interested in exploring volume as a relevance function within MMR. MMR avoids redundancy by penalizing redundant sentences, whereas in our method semantic redundancy is inherently discouraged since the method chooses sentences to maximize volume. Depending on the method used to embed sentences, this might not translate directly into avoiding n-gram redundancy. Plugging our scoring function to an MMR objective is a simple way to enforce diversity. Sanjeev Arora, Rong Ge, Ravi Kannan, and Ankur Moitra. 2012. Computing a nonne"
D15-1228,W04-1013,0,0.0182322,". “Volume” refers to our method, shown with two embedding sizes. 12.0 We evaluate our proposed method on the nonupdate portion of TAC-2008 and TAC-2009. The datasets contain 48 and 44 multi-document summarization problems, respectively. Each problem has 10 news articles as input; each is to be summarized in a maximum of L = 100 words. There are 4 human reference summaries for each problem, against which an automatically generated summary is compared. We compare our method with two baselines: Maximal Marginal Relevance (MMR, §2.1) and the coverage-based summarization method (CBS, §2.2). ROUGE (Lin, 2004) is used to evaluate the summarization results. For preprocessing, we tokenize, stem with the Porter (1980) stemmer, and split documents into sentences. We remove bigrams consisting of only stopwords and bigrams which appear in less than 3 sentences. As a result, we have 2,746 and 3,273 bigrams for the TAC-2008 and TAC-2009 datasets respectively. Unlabeled data can help generate better sentence representations. For each summarization problem in each dataset, we use other problems in the same dataset as unlabeled data. We concatenate every problem in each dataset and perform SVD on this matrix"
D15-1228,W09-1801,1,0.64519,"this work, we consider summarization. Classical approaches to extractive summarization represent each sentence as a bag of terms (typically bigrams) and seek a subset of sentences from the input document(s) that either (a) trade off between high relevance and low redundancy (Carbonell and Goldstein, 1998; McDonald, 2007), or (b) maximize bigram coverage (Yih et al., 2007; Gillick et al., 2008). The sentence representation is fundamentally discrete, and a range of greedy (Carbonell and Goldstein, 1998), approximate (Almeida and Martins, 2013), and exact optimization algorithms (McDonald, 2007; Martins and Smith, 2009; Berg-Kirkpatrick et al., 2011) have been proposed. Recent studies have explored continuous sentence representations, including the paragraph vector (Le and Mikolov, 2014), a convolutional neural network architecture (Kalchbrenner et al., 2014), and a dictionary learning approach (Jenatton et al., 2011). If sentences are represented as low-dimensional embeddings in a distributed semantic space, then we begin to imagine a geometric relationship between a summary and a document. We propose that the volume of a summary (i.e., the semantic subspace spanned by the selected sentences) should ideall"
D15-1228,P13-1136,0,0.0175161,"ork Our method could be extended for compressive summarization, by simply including compressed sentences in the embedded space and running Algorithm 1 without any change. This resembles the summarization methods that jointly extracts and compresses (Berg-Kirkpatrick et al., 2011; Woodsend and Lapata, 2012; Almeida and Martins, 2013). Another alternative is a pipeline approach, where extractive summarization is followed or preceded by a sentence compression module, which can be built and tuned independent of our proposed extractive method (Knight and Marcu, 2000; Lin, 2003; Zajic et al., 2007; Wang et al., 2013; Li et al., 2013). We are also interested in exploring volume as a relevance function within MMR. MMR avoids redundancy by penalizing redundant sentences, whereas in our method semantic redundancy is inherently discouraged since the method chooses sentences to maximize volume. Depending on the method used to embed sentences, this might not translate directly into avoiding n-gram redundancy. Plugging our scoring function to an MMR objective is a simple way to enforce diversity. Sanjeev Arora, Rong Ge, Ravi Kannan, and Ankur Moitra. 2012. Computing a nonnegative matrix factorization – provably."
D15-1251,C08-2004,0,0.0133464,"91.61 Table 3: Comparisons on the Amazon electronics and IMDB reviews datasets. SVM results are from Wang and Manning (2012), the RBM (restricted Bolzmann machine) result is from Dahl et al. (2012), NN and CNN results are from Johnson and Zhang (2015), and LR-{1, 2, 3, 4, 5}-grams and compressive feature learning results are from Paskov et al. (2013). Test size = 20, 000 for both datasets. Congressional vote (Thomas et al., 2006)—Table 4. A dataset of transcripts from the U.S. Congressional debates: http://www.cs.cornell.edu/~ainur/sle-data.html. Similar to previous work (Thomas et al., 2006; Bansal et al., 2008; Yessenalina et al., 2010), we consider the task to predict the vote (“yea” or “nay”) for the speaker of each speech segment (speaker-based speech-segment classification). Our method outperforms the best results of Yessenalina et al. (2010), which use a multi-level structured 5 As noted, semi-supervised and ensemble methods are excluded for a fair comparison. 6 This approach is based on minimum description length, using unlabeled data to select a set of higher-order n-grams to use as features. model based on a latent-variable SVM. We show comparisons to two weaker baselines as well. Method SV"
D15-1251,D11-1014,0,0.0474717,"p.stanford.edu/sentiment. We use the binary classification task where the goal is to predict whether a review is positive or negative (no neutral). Our logistic regression model outperforms the baseline SVM reported by Socher et al. (2013), who used only unigrams but did not specify the weighting scheme for their SVM baseline. While our result is still below the state-of-the-art based on the the recursive neural tensor networks (Socher et al., 2013) and the paragraph vector (Le and Mikolov, 2014), we show that logistic regression is comparable with recursive and matrix-vector neural networks (Socher et al., 2011; Socher et al., 2012). Experiments We fix L to logistic regression. We optimize text representation based on the types of n-grams used, the type of weighting scheme, and the removal of stopwords; we also optimize the regularizer and training convergence criterion, which interact with the representation. See Table 1 for a complete list. Note that even with this limited number of options, the number of possible combinations is huge,3 so exhaustive search is computationally expensive. In all our experiments for all datasets, we limit ourselves to 30 trials per dataset. The only preprocessing we"
D15-1251,D12-1110,0,0.0110841,"ent. We use the binary classification task where the goal is to predict whether a review is positive or negative (no neutral). Our logistic regression model outperforms the baseline SVM reported by Socher et al. (2013), who used only unigrams but did not specify the weighting scheme for their SVM baseline. While our result is still below the state-of-the-art based on the the recursive neural tensor networks (Socher et al., 2013) and the paragraph vector (Le and Mikolov, 2014), we show that logistic regression is comparable with recursive and matrix-vector neural networks (Socher et al., 2011; Socher et al., 2012). Experiments We fix L to logistic regression. We optimize text representation based on the types of n-grams used, the type of weighting scheme, and the removal of stopwords; we also optimize the regularizer and training convergence criterion, which interact with the representation. See Table 1 for a complete list. Note that even with this limited number of options, the number of possible combinations is huge,3 so exhaustive search is computationally expensive. In all our experiments for all datasets, we limit ourselves to 30 trials per dataset. The only preprocessing we applied was downcasing"
D15-1251,D13-1170,0,0.0244435,"ch of our datasets, we select supervised, nonensemble classification methods from previous literature as baselines. In each case, we emphasize comparisons with the best-published linear method 3 It is actually infinite since the reg. strength and conv. tolerance are continuous values, but we could discretize them. Method Na¨ıve Bayes SVM Vector average Recursive neural networks LR (this work) Matrix-vector RNN Recursive neural tensor networks Paragraph vector Acc. 81.8 79.4 80.1 82.4 82.4 82.9 85.4 87.8 Table 2: Comparisons on the Stanford sentiment treebank dataset. Scores are as reported by Socher et al. (2013) and Le and Mikolov (2014). Test size = 6, 920. Amazon electronics (McAuley and Leskovec, 2013)—Table 3. A binary sentiment analysis dataset of Amazon electronics product reviews: http://riejohnson.com/cnn data.html. The bestperforming methods on this dataset are based on convolutional neural networks (Johnson and Zhang, 2015).4 Our method is on par with the secondbest of these, outperforming all of the reported feed-forward neural networks and SVM variants Johnson and Zhang used as baselines. They varied 4 These are convolutional neural networks with a rectifier activation function, trained u"
D15-1251,N15-1011,0,0.0882421,"a¨ıve Bayes SVM Vector average Recursive neural networks LR (this work) Matrix-vector RNN Recursive neural tensor networks Paragraph vector Acc. 81.8 79.4 80.1 82.4 82.4 82.9 85.4 87.8 Table 2: Comparisons on the Stanford sentiment treebank dataset. Scores are as reported by Socher et al. (2013) and Le and Mikolov (2014). Test size = 6, 920. Amazon electronics (McAuley and Leskovec, 2013)—Table 3. A binary sentiment analysis dataset of Amazon electronics product reviews: http://riejohnson.com/cnn data.html. The bestperforming methods on this dataset are based on convolutional neural networks (Johnson and Zhang, 2015).4 Our method is on par with the secondbest of these, outperforming all of the reported feed-forward neural networks and SVM variants Johnson and Zhang used as baselines. They varied 4 These are convolutional neural networks with a rectifier activation function, trained under `2 regularization with stochastic gradient descent. The authors also consider an extension based on parallel CNN that we do not include here. 2102 the representations, and used log term frequency and normalization to unit vectors as the weighting scheme, after finding that this outperformed term frequency. Our method achi"
D15-1251,Q15-1016,0,0.0467936,"NLP systems that work with raw text and do not require manual tuning. 1 Introduction NLP researchers and practitioners spend a considerable amount of time comparing machine-learned models of text that differ in relatively uninteresting ways. For example, in categorizing texts, should the “bag of words” include bigrams, and is tf-idf weighting a good idea? In learning word embeddings, distributional similarity approaches have been shown to perform competitively with neural network models when the hyperparameters (e.g., context window, subsampling rate, smoothing constant) are carefully tuned (Levy et al., 2015). These choices matter experimentally, often leading to big differences in performance, with little consistency across tasks and datasets in which combination of choices works best. Unfortunately, these differences tell us little about language or the problems that machine learners are supposed to solve. We propose that these decisions can be automated in a similar way to hyperparameter selection (e.g., choosing the strength of a ridge or lasso regularizer). Given a particular text dataset and classification task, we show a technique for optimizing over the space of representational choices, a"
D15-1251,P11-1015,0,0.0396938,"rd neural networks and SVM variants Johnson and Zhang used as baselines. They varied 4 These are convolutional neural networks with a rectifier activation function, trained under `2 regularization with stochastic gradient descent. The authors also consider an extension based on parallel CNN that we do not include here. 2102 the representations, and used log term frequency and normalization to unit vectors as the weighting scheme, after finding that this outperformed term frequency. Our method achieved the best performance with binary weighting, which they did not consider. IMDB movie reviews (Maas et al., 2011)— Table 3. A binary sentiment analysis dataset of highly polar IMDB movie reviews: http://ai.stanford.edu/~amaas/data/sentiment. The results parallel those for Amazon electronics; our method comes close to convolutional neural networks (Johnson and Zhang, 2015), which are state-of-the-art.5 It outperforms SVMs and feed-forward neural networks, the restricted Boltzmann machine approach presented by Dahl et al. (2012), and compressive feature learning (Paskov et al., 2013).6 Method SVM-unigrams RBM SVM-{1, 2}-grams Compressive feature learning SVM-{1, 2, 3}-grams LR-{1, 2, 3, 4, 5}-grams NN-{1,"
D15-1251,W06-1639,0,0.0148483,"ams NN-{1, 2, 3}-grams LR (this work) Bag of words CNN Sequential CNN Accuracy Amazon IMDB 88.29 88.64 89.23 90.95 90.26 90.40 91.29 90.58 90.60 91.52 90.83 91.56 90.85 91.61 91.34 92.52 91.61 Table 3: Comparisons on the Amazon electronics and IMDB reviews datasets. SVM results are from Wang and Manning (2012), the RBM (restricted Bolzmann machine) result is from Dahl et al. (2012), NN and CNN results are from Johnson and Zhang (2015), and LR-{1, 2, 3, 4, 5}-grams and compressive feature learning results are from Paskov et al. (2013). Test size = 20, 000 for both datasets. Congressional vote (Thomas et al., 2006)—Table 4. A dataset of transcripts from the U.S. Congressional debates: http://www.cs.cornell.edu/~ainur/sle-data.html. Similar to previous work (Thomas et al., 2006; Bansal et al., 2008; Yessenalina et al., 2010), we consider the task to predict the vote (“yea” or “nay”) for the speaker of each speech segment (speaker-based speech-segment classification). Our method outperforms the best results of Yessenalina et al. (2010), which use a multi-level structured 5 As noted, semi-supervised and ensemble methods are excluded for a fair comparison. 6 This approach is based on minimum description len"
D15-1251,P12-2018,0,0.025162,"e-of-the-art.5 It outperforms SVMs and feed-forward neural networks, the restricted Boltzmann machine approach presented by Dahl et al. (2012), and compressive feature learning (Paskov et al., 2013).6 Method SVM-unigrams RBM SVM-{1, 2}-grams Compressive feature learning SVM-{1, 2, 3}-grams LR-{1, 2, 3, 4, 5}-grams NN-{1, 2, 3}-grams LR (this work) Bag of words CNN Sequential CNN Accuracy Amazon IMDB 88.29 88.64 89.23 90.95 90.26 90.40 91.29 90.58 90.60 91.52 90.83 91.56 90.85 91.61 91.34 92.52 91.61 Table 3: Comparisons on the Amazon electronics and IMDB reviews datasets. SVM results are from Wang and Manning (2012), the RBM (restricted Bolzmann machine) result is from Dahl et al. (2012), NN and CNN results are from Johnson and Zhang (2015), and LR-{1, 2, 3, 4, 5}-grams and compressive feature learning results are from Paskov et al. (2013). Test size = 20, 000 for both datasets. Congressional vote (Thomas et al., 2006)—Table 4. A dataset of transcripts from the U.S. Congressional debates: http://www.cs.cornell.edu/~ainur/sle-data.html. Similar to previous work (Thomas et al., 2006; Bansal et al., 2008; Yessenalina et al., 2010), we consider the task to predict the vote (“yea” or “nay”) for the speaker of"
D15-1251,D10-1102,0,0.226901,"isons on the Amazon electronics and IMDB reviews datasets. SVM results are from Wang and Manning (2012), the RBM (restricted Bolzmann machine) result is from Dahl et al. (2012), NN and CNN results are from Johnson and Zhang (2015), and LR-{1, 2, 3, 4, 5}-grams and compressive feature learning results are from Paskov et al. (2013). Test size = 20, 000 for both datasets. Congressional vote (Thomas et al., 2006)—Table 4. A dataset of transcripts from the U.S. Congressional debates: http://www.cs.cornell.edu/~ainur/sle-data.html. Similar to previous work (Thomas et al., 2006; Bansal et al., 2008; Yessenalina et al., 2010), we consider the task to predict the vote (“yea” or “nay”) for the speaker of each speech segment (speaker-based speech-segment classification). Our method outperforms the best results of Yessenalina et al. (2010), which use a multi-level structured 5 As noted, semi-supervised and ensemble methods are excluded for a fair comparison. 6 This approach is based on minimum description length, using unlabeled data to select a set of higher-order n-grams to use as features. model based on a latent-variable SVM. We show comparisons to two weaker baselines as well. Method SVM-link Min-cut SVM-SLE LR ("
D16-1028,D13-1059,0,0.0334481,"Missing"
D16-1028,N10-1083,0,0.0606133,"Missing"
D16-1028,J92-4003,0,0.620219,"Missing"
D16-1028,P14-1099,1,0.91772,"rvised learning algorithms for the structured problems found in NLP are computationally expensive, requiring multiple decoding passes through the unlabeled data, or expensive similarity graphs. More scalable learning algorithms are in demand. In this paper, we propose a moment-matching method for semi-supervised learning of sequence models. Spectral learning and moment-matching approaches have recently proved a viable alternative to expectation-maximization (EM) for unsupervised learning (Hsu et al., 2012; Balle and Mohri, 2012; Bailly et al., 2013), supervised learning with latent variables (Cohen and Collins, 2014; Quattoni et al., 2014; Stratos et al., 2013) and topic modeling (Arora et al., 2013; Nguyen et al., 2015). These methods have learnability guarantees, do not suffer from local optima, and are computationally less demanding. Unlike spectral methods, ours does not require an orthogonal decomposition of any matrix or tensor. Instead, it considers a more restricted form of supervision: words that have unambiguous annotations, so-called anchor words (Arora et al., 2013). Rather than identifying anchor words from unlabeled data (Stratos et al., 2016), we extract them from a small labeled dataset o"
D16-1028,N13-1015,1,0.928849,"w), we can estimate the emission probabilities O by direct application of Bayes rule: Therefore, given the set of anchor words A(h), the bw,h = p(H = h |X = w) × p(X = w) (15) O hth column of R can be estimated in a single pass p(H = h) over the unlabeled data, as follows: Eq. 7 z}|{ P γ bw,c × pbw ψ (z)1(x ∈ A(h)) c =P . (16) bc,h = x,z∈D P U R (12) bw0 ,c × pbw0 w0 γ 1(x ∈ A(h)) x,z∈DU 290 These parameters are guaranteed to lie in the probability simplex, avoiding the need of heuristics for dealing with “negative” and “unnormalized” probabilities required by prior work in spectral learning (Cohen et al., 2013). 3.5 Transition Distributions It remains to estimate the transition matrix T. For the problems tackled in this paper, the number of labels K is small, compared to the vocabulary size V . The transition matrix has only O(K 2 ) degrees of freedom, and we found it effective to estimate it using the labeled sequences in DL alone, without any refinement. This was done by smoothed maximum likelihood estimation on the labeled data, which boils down to counting occurrences of consecutive labels, applying add-one smoothing to avoid zero probabilities for unobserved transitions, and normalizing. For pr"
D16-1028,P13-1057,0,0.224936,"rameters are estimated by solving a small quadratic program for each feature. Experiments on part-of-speech (POS) tagging for Twitter and for a low-resource language (Malagasy) show that our method can learn from very few annotated sentences. 1 Introduction Statistical learning of NLP models is often limited by the scarcity of annotated data. Weakly supervised methods have been proposed as an alternative to laborious manual annotation, combining large amounts of unlabeled data with limited resources, such as tag dictionaries or small annotated datasets (Merialdo, 1994; Smith and Eisner, 2005; Garrette et al., 2013). Unfortunately, most semisupervised learning algorithms for the structured problems found in NLP are computationally expensive, requiring multiple decoding passes through the unlabeled data, or expensive similarity graphs. More scalable learning algorithms are in demand. In this paper, we propose a moment-matching method for semi-supervised learning of sequence models. Spectral learning and moment-matching approaches have recently proved a viable alternative to expectation-maximization (EM) for unsupervised learning (Hsu et al., 2012; Balle and Mohri, 2012; Bailly et al., 2013), supervised le"
D16-1028,P11-2008,1,0.79776,"Missing"
D16-1028,Q15-1016,0,0.0308812,"s the matrix Q ∈ RC×V , defined as: Qc,w := E[ψc (Z) |X = w]. (3) Expectations here are taken with respect to the probabilistic model in Eq. 1 that generates the data. The following quantities will also be necessary: Figure 1: HMM, context (green) conditionally independent of present (red) w` given state h` . and will be formally defined in §3.1. Such cooccurrence matrices are often collected in NLP, for various problems, ranging from dimensionality reduction of documents using latent semantic indexing (Deerwester et al., 1990; Landauer et al., 1998), distributional semantics (Sch¨utze, 1998; Levy et al., 2015) and word embedding generation (Dhillon et al., 2015; Osborne et al., 2016). We can build such a moment matrix entirely from the unlabeled data DU . The same unlabeled data is used to build an estimate of a context-label moment matrix R ∈ RC×K , as explained in §3.3. This is done by first identifying words that are unambiguously associated with each label h, called anchor words, with the aid of a few labeled data; this is outlined in §3.2. Finally, given empirical estimates of Q and R, we estimate the emission matrix O by solving a small optimization problem independently per word (§3.4). The"
D16-1028,P12-3005,0,0.0268494,"ter experiments, we also evaluated a stacked architecture in which we derived features from our model’s predictions to improve a state-ofthe-art POS tagger (MEMM).4 6.1 Twitter POS Tagging For the Twitter experiment, we used the Oct27 dataset of Gimpel et al. (2011), with the provided partitions (1,000 tweets for training and 328 for validation), and tested on the Daily547 dataset (547 tweets). Anchor words were selected from the training partition as described in §5. We used 2.7M unlabeled tweets (O’Connor et al., 2010) to train the semi-supervised methods, filtering the English tweets as in Lui and Baldwin (2012), tokenizing them as in Owoputi et al. (2013), and normalizing at-mentions, URLs, and emoticons. We used as word features φ(X) the word iself, as well as binary features for capitalization, titles, and digits (Berg-Kirkpatrick et al., 2010), the word shape, and the Unicode class of each character. Similarly to Owoputi et al. (2013), we also used suffixes and prefixes (up to length 3), and Twitter4 http://www.ark.cs.cmu.edu/TweetNLP/ 293 0.95 Tagging accuracy (0/1 loss) select the anchors on the validation set, using steps of 0.1 in the unit interval, and making sure that all tags have at least"
D16-1028,J94-2001,0,0.254846,"collect moment statistics. The model parameters are estimated by solving a small quadratic program for each feature. Experiments on part-of-speech (POS) tagging for Twitter and for a low-resource language (Malagasy) show that our method can learn from very few annotated sentences. 1 Introduction Statistical learning of NLP models is often limited by the scarcity of annotated data. Weakly supervised methods have been proposed as an alternative to laborious manual annotation, combining large amounts of unlabeled data with limited resources, such as tag dictionaries or small annotated datasets (Merialdo, 1994; Smith and Eisner, 2005; Garrette et al., 2013). Unfortunately, most semisupervised learning algorithms for the structured problems found in NLP are computationally expensive, requiring multiple decoding passes through the unlabeled data, or expensive similarity graphs. More scalable learning algorithms are in demand. In this paper, we propose a moment-matching method for semi-supervised learning of sequence models. Spectral learning and moment-matching approaches have recently proved a viable alternative to expectation-maximization (EM) for unsupervised learning (Hsu et al., 2012; Balle and"
D16-1028,N15-1076,0,0.0174388,"ultiple decoding passes through the unlabeled data, or expensive similarity graphs. More scalable learning algorithms are in demand. In this paper, we propose a moment-matching method for semi-supervised learning of sequence models. Spectral learning and moment-matching approaches have recently proved a viable alternative to expectation-maximization (EM) for unsupervised learning (Hsu et al., 2012; Balle and Mohri, 2012; Bailly et al., 2013), supervised learning with latent variables (Cohen and Collins, 2014; Quattoni et al., 2014; Stratos et al., 2013) and topic modeling (Arora et al., 2013; Nguyen et al., 2015). These methods have learnability guarantees, do not suffer from local optima, and are computationally less demanding. Unlike spectral methods, ours does not require an orthogonal decomposition of any matrix or tensor. Instead, it considers a more restricted form of supervision: words that have unambiguous annotations, so-called anchor words (Arora et al., 2013). Rather than identifying anchor words from unlabeled data (Stratos et al., 2016), we extract them from a small labeled dataset or from a dictionary. Given the anchor words, the estimation of the model parameters can be made efficient b"
D16-1028,Q16-1030,1,0.819216,"ations here are taken with respect to the probabilistic model in Eq. 1 that generates the data. The following quantities will also be necessary: Figure 1: HMM, context (green) conditionally independent of present (red) w` given state h` . and will be formally defined in §3.1. Such cooccurrence matrices are often collected in NLP, for various problems, ranging from dimensionality reduction of documents using latent semantic indexing (Deerwester et al., 1990; Landauer et al., 1998), distributional semantics (Sch¨utze, 1998; Levy et al., 2015) and word embedding generation (Dhillon et al., 2015; Osborne et al., 2016). We can build such a moment matrix entirely from the unlabeled data DU . The same unlabeled data is used to build an estimate of a context-label moment matrix R ∈ RC×K , as explained in §3.3. This is done by first identifying words that are unambiguously associated with each label h, called anchor words, with the aid of a few labeled data; this is outlined in §3.2. Finally, given empirical estimates of Q and R, we estimate the emission matrix O by solving a small optimization problem independently per word (§3.4). The transition matrix T is obtained directly from the labeled dataset DL by max"
D16-1028,N13-1039,1,0.813851,"Missing"
D16-1028,petrov-etal-2012-universal,0,0.0129586,"mpute a mapping from mean parameters µh 3 As shown by Xiaojin Zhu (1999) and Yasemin Altun to canonical parameters θ h , we use the well-known (2006), this regularization is equivalent, in the dual, to a “soft” Fenchel-Legendre duality between the entropy and constraint kEθh [φ(X) |H = h] − µh k2 ≤ , as opposed to a the log-partition function (Wainwright and Jordan, strict equality. 292 6 Experiments We evaluated our method on two tasks: POS tagging of Twitter text (in English), and POS tagging for a low-resource language (Malagasy). For all the experiments, we used the universal POS tagset (Petrov et al., 2012), which consists of K = 12 tags. We compared our method against supervised baselines (HMM and FHMM), which use the labeled data only, and two semi-supervised baselines that exploit the unlabeled data: self-training and EM. For the Twitter experiments, we also evaluated a stacked architecture in which we derived features from our model’s predictions to improve a state-ofthe-art POS tagger (MEMM).4 6.1 Twitter POS Tagging For the Twitter experiment, we used the Oct27 dataset of Gimpel et al. (2011), with the provided partitions (1,000 tweets for training and 328 for validation), and tested on th"
D16-1028,J98-1004,0,0.524216,"Missing"
D16-1028,P05-1044,1,0.844823,"statistics. The model parameters are estimated by solving a small quadratic program for each feature. Experiments on part-of-speech (POS) tagging for Twitter and for a low-resource language (Malagasy) show that our method can learn from very few annotated sentences. 1 Introduction Statistical learning of NLP models is often limited by the scarcity of annotated data. Weakly supervised methods have been proposed as an alternative to laborious manual annotation, combining large amounts of unlabeled data with limited resources, such as tag dictionaries or small annotated datasets (Merialdo, 1994; Smith and Eisner, 2005; Garrette et al., 2013). Unfortunately, most semisupervised learning algorithms for the structured problems found in NLP are computationally expensive, requiring multiple decoding passes through the unlabeled data, or expensive similarity graphs. More scalable learning algorithms are in demand. In this paper, we propose a moment-matching method for semi-supervised learning of sequence models. Spectral learning and moment-matching approaches have recently proved a viable alternative to expectation-maximization (EM) for unsupervised learning (Hsu et al., 2012; Balle and Mohri, 2012; Bailly et a"
D16-1028,W13-3507,1,0.860689,"oblems found in NLP are computationally expensive, requiring multiple decoding passes through the unlabeled data, or expensive similarity graphs. More scalable learning algorithms are in demand. In this paper, we propose a moment-matching method for semi-supervised learning of sequence models. Spectral learning and moment-matching approaches have recently proved a viable alternative to expectation-maximization (EM) for unsupervised learning (Hsu et al., 2012; Balle and Mohri, 2012; Bailly et al., 2013), supervised learning with latent variables (Cohen and Collins, 2014; Quattoni et al., 2014; Stratos et al., 2013) and topic modeling (Arora et al., 2013; Nguyen et al., 2015). These methods have learnability guarantees, do not suffer from local optima, and are computationally less demanding. Unlike spectral methods, ours does not require an orthogonal decomposition of any matrix or tensor. Instead, it considers a more restricted form of supervision: words that have unambiguous annotations, so-called anchor words (Arora et al., 2013). Rather than identifying anchor words from unlabeled data (Stratos et al., 2016), we extract them from a small labeled dataset or from a dictionary. Given the anchor words, t"
D16-1028,Q16-1018,0,0.0111305,"), supervised learning with latent variables (Cohen and Collins, 2014; Quattoni et al., 2014; Stratos et al., 2013) and topic modeling (Arora et al., 2013; Nguyen et al., 2015). These methods have learnability guarantees, do not suffer from local optima, and are computationally less demanding. Unlike spectral methods, ours does not require an orthogonal decomposition of any matrix or tensor. Instead, it considers a more restricted form of supervision: words that have unambiguous annotations, so-called anchor words (Arora et al., 2013). Rather than identifying anchor words from unlabeled data (Stratos et al., 2016), we extract them from a small labeled dataset or from a dictionary. Given the anchor words, the estimation of the model parameters can be made efficient by collecting moment statistics from unlabeled data, then solving a small quadratic program for each word. Our contributions are as follows: • We adapt anchor methods to semi-supervised learning of generative sequence models. • We show how our method can also handle loglinear feature-based emissions. • We apply this model to POS tagging. Our experiments on the Twitter dataset introduced by Gimpel et al. (2011) and on the dataset introduced by"
D16-1148,W11-0705,0,0.0553328,"hem (rare but specific words as opposed to more generic but potentially obvious terms). the relationships between entities (O’Connor et al., 2013; Iyyer et al., 2016), and predicting personality types from text (Flekova and Gurevych, 2015). Bamman also applied variants of the DPM to characters in novels (Bamman et al., 2014). Previous work on sentiment, stance, and opinion mining has focused on recognizing stance or political sentiment in online ideological debates (Somasundaran and Wiebe, 2010; Hasan and Ng, 2014; Sridhar et al., 2015), and other forms of social media (O’Connor et al., 2010; Agarwal et al., 2011), and recently through the lens of connotation frames (Rashkin et al., 2016). Opinion mining and sentiment analysis are the subject of ongoing research in NLP and have long served as test platforms for new methodologies (Socher et al., 2013; ˙Irsoy and Cardie, 2014; Tai et al., 2015) Framing is arguably one of the most important concepts in the social sciences, with roots in to sociology, psychology, and mass communication (Gitlin, 1980; Benford and Snow, 2000; D’Angelo and Kuypers, 2010); the scope and relevance of framing is widely debated (Rees et al., 2001), with many authors applying the"
D16-1148,P13-1035,1,0.9119,"Missing"
D16-1148,P14-1035,1,0.934081,"oximately 37,000 articles about immigration; we train the persona model on this larger dataset, only using the smaller set for evaluation on a secondary task. Note that the MFC annotations are not used by our model; rather, we hypothesize that the personas it discovers may serve as features to help predict framing—this serves as one of our evaluations (§7). 5 Identifying Entities The original focus of the DPM was on named characters in movies, which could be identified using named entity recognition and pronominal coreference (Bamman et al., 2013), or name matching for pre-defined characters (Bamman et al., 2014). Here, we are interested in applying our model to entities about which we assume no specific prior knowledge. In order to include a broader set of entities, we preprocess the corpus and apply a series of filters. First, we obtain lemmas, part-of-speech tags, dependencies, coreference resolution, and named entities from the Stanford CoreNLP pipeline (Manning et al., 2014), as well as supersense tags from the AMALGrAM tagger (Schneider and Smith, 2015). For each document, we consider all tokens with a 2 The MFC also contains more fine-grained annotations of spans of text which cue each of the f"
D16-1148,N15-1171,0,0.105119,"the Media Frames Corpus. We also introduce automated model selection as a fair and robust form of feature evaluation. 1 Introduction Social science tells us that communication almost inescapably involves framing—choosing “a few elements of perceived reality and assembling a narrative that highlights connections among them to promote a particular interpretation” (Entman, 2007). Memorable examples include loaded phrases (death tax, war on terror), but the literature attests a much wider range of linguistic means toward this end (Pan and Kosicki, 1993; Greene and Resnik, 2009; Choi et al., 2012; Baumer et al., 2015). Framing is associated with several phenomena to which NLP has been applied, including ideology (Lin et al., 2006; Hardisty et al., 2010; Iyyer et al., 2014), sentiment (Pang and Lee, 2008; Feldman, 2013), and stance (Walker et al., 2012; Hasan and Ng, 2013). Although such author attributes are interesting, framing scholarship is concerned with persistent patterns of representation of particular issues— without necessarily tying these to the states or intentions of authors—and the effects that such patterns Can framing be automatically recognized? If so, social-scientific studies of framing w"
D16-1148,J92-4003,0,0.166795,"usters.) formance. For this experiment, we use the tree-structured Parzen estimator for Bayesian optimization (Bergstra et al., 2015), with L1 -regularized logistic regression as the underlying classifier, and set R = 40. In addition to the entities and story clusters identified by these models, we allow these classifiers access to a large set of features, including unigrams, bigrams, parts of speech, named entities, dependency tuples, ordinal sentiment values (Manning et al., 2014), multi-word expressions (Justeson and Katz, 1995), supersense tags (Schneider and Smith, 2015), Brown clusters (Brown et al., 1992), frame semantic features (Das et al., 2010), and topics produced by standard LDA (Blei et al., 2003). The inclusion or exclusion of each feature is determined automatically on each iteration, along with feature transformations (removal of rare words, lowercasing, and binary or normalized counts). The baseline, denoted “B,” offers all features except personas and story clusters to Bayesopt; we consider adding DPM personas, our model’s personas, and our model’s personas and story clusters. Table 4 shows test-set accuracy for each setup, averaged across the three best models returned by Bayesopt"
D16-1148,P15-2072,1,0.855438,"at our model produces interpretable clusters that provide insight into our corpus of immigration news articles (§6). • We propose a new kind of evaluation based on Bayesian optimization. Given a supervised learning problem, we treat the inclusion of a candidate feature set (here, personas) as a hyperparameter to be optimized alongside other hyperparameters (§7). • In the case of U.S. news stories about immigration, we find that personas are, in many cases, helpful for automatically inferring the coarsegrained framing and tone employed in a piece of text, as defined in the Media Frames Corpus (Card et al., 2015) (§7). 2 Model Description The plate diagram for the new model is shown in Figure 1 (right), with the original DPM (Bamman et al., 2013) shown on the left. As evidence, the model considers tuples hw, r, e, ii, where w is a word token and r is the category of syntactic relation1 it bears to an entity with index e mentioned in document with index i. The model’s generative story explains this evidence 1 We adopt the terminology from Bamman et al. (2013) of “agent”, “patient”, and “attribute”, even though these categories of relations are defined in terms of syntactic dependences. 1411 ( D &apos; $ p """
D16-1148,P09-1068,0,0.0964034,"Missing"
D16-1148,W12-3809,0,0.0138966,"ing annotations in the Media Frames Corpus. We also introduce automated model selection as a fair and robust form of feature evaluation. 1 Introduction Social science tells us that communication almost inescapably involves framing—choosing “a few elements of perceived reality and assembling a narrative that highlights connections among them to promote a particular interpretation” (Entman, 2007). Memorable examples include loaded phrases (death tax, war on terror), but the literature attests a much wider range of linguistic means toward this end (Pan and Kosicki, 1993; Greene and Resnik, 2009; Choi et al., 2012; Baumer et al., 2015). Framing is associated with several phenomena to which NLP has been applied, including ideology (Lin et al., 2006; Hardisty et al., 2010; Iyyer et al., 2014), sentiment (Pang and Lee, 2008; Feldman, 2013), and stance (Walker et al., 2012; Hasan and Ng, 2013). Although such author attributes are interesting, framing scholarship is concerned with persistent patterns of representation of particular issues— without necessarily tying these to the states or intentions of authors—and the effects that such patterns Can framing be automatically recognized? If so, social-scientifi"
D16-1148,N10-1138,1,0.791241,"the tree-structured Parzen estimator for Bayesian optimization (Bergstra et al., 2015), with L1 -regularized logistic regression as the underlying classifier, and set R = 40. In addition to the entities and story clusters identified by these models, we allow these classifiers access to a large set of features, including unigrams, bigrams, parts of speech, named entities, dependency tuples, ordinal sentiment values (Manning et al., 2014), multi-word expressions (Justeson and Katz, 1995), supersense tags (Schneider and Smith, 2015), Brown clusters (Brown et al., 1992), frame semantic features (Das et al., 2010), and topics produced by standard LDA (Blei et al., 2003). The inclusion or exclusion of each feature is determined automatically on each iteration, along with feature transformations (removal of rare words, lowercasing, and binary or normalized counts). The baseline, denoted “B,” offers all features except personas and story clusters to Bayesopt; we consider adding DPM personas, our model’s personas, and our model’s personas and story clusters. Table 4 shows test-set accuracy for each setup, averaged across the three best models returned by Bayesopt. Using this more rigorous form of evaluatio"
D16-1148,D15-1208,0,0.021435,"ed personas, based on input from additional experts. Moreover, it also illustrates the challenge of trying to match the output of an unsupervised model to expected results. Not only is some merging and splitting of categories inevitable, there was a mismatch in this case in the types of entities to be described (people as opposed to more abstract entities), and the ways of describing them (rare but specific words as opposed to more generic but potentially obvious terms). the relationships between entities (O’Connor et al., 2013; Iyyer et al., 2016), and predicting personality types from text (Flekova and Gurevych, 2015). Bamman also applied variants of the DPM to characters in novels (Bamman et al., 2014). Previous work on sentiment, stance, and opinion mining has focused on recognizing stance or political sentiment in online ideological debates (Somasundaran and Wiebe, 2010; Hasan and Ng, 2014; Sridhar et al., 2015), and other forms of social media (O’Connor et al., 2010; Agarwal et al., 2011), and recently through the lens of connotation frames (Rashkin et al., 2016). Opinion mining and sentiment analysis are the subject of ongoing research in NLP and have long served as test platforms for new methodologie"
D16-1148,N09-1057,0,0.10073,"t the coarse-grained framing annotations in the Media Frames Corpus. We also introduce automated model selection as a fair and robust form of feature evaluation. 1 Introduction Social science tells us that communication almost inescapably involves framing—choosing “a few elements of perceived reality and assembling a narrative that highlights connections among them to promote a particular interpretation” (Entman, 2007). Memorable examples include loaded phrases (death tax, war on terror), but the literature attests a much wider range of linguistic means toward this end (Pan and Kosicki, 1993; Greene and Resnik, 2009; Choi et al., 2012; Baumer et al., 2015). Framing is associated with several phenomena to which NLP has been applied, including ideology (Lin et al., 2006; Hardisty et al., 2010; Iyyer et al., 2014), sentiment (Pang and Lee, 2008; Feldman, 2013), and stance (Walker et al., 2012; Hasan and Ng, 2013). Although such author attributes are interesting, framing scholarship is concerned with persistent patterns of representation of particular issues— without necessarily tying these to the states or intentions of authors—and the effects that such patterns Can framing be automatically recognized? If s"
D16-1148,D10-1028,0,0.0163925,"ial science tells us that communication almost inescapably involves framing—choosing “a few elements of perceived reality and assembling a narrative that highlights connections among them to promote a particular interpretation” (Entman, 2007). Memorable examples include loaded phrases (death tax, war on terror), but the literature attests a much wider range of linguistic means toward this end (Pan and Kosicki, 1993; Greene and Resnik, 2009; Choi et al., 2012; Baumer et al., 2015). Framing is associated with several phenomena to which NLP has been applied, including ideology (Lin et al., 2006; Hardisty et al., 2010; Iyyer et al., 2014), sentiment (Pang and Lee, 2008; Feldman, 2013), and stance (Walker et al., 2012; Hasan and Ng, 2013). Although such author attributes are interesting, framing scholarship is concerned with persistent patterns of representation of particular issues— without necessarily tying these to the states or intentions of authors—and the effects that such patterns Can framing be automatically recognized? If so, social-scientific studies of framing will be enabled by new measurements, and new applications might bring framing effects to the consciousness of everyday readers. Several re"
D16-1148,I13-1191,0,0.0161082,"d assembling a narrative that highlights connections among them to promote a particular interpretation” (Entman, 2007). Memorable examples include loaded phrases (death tax, war on terror), but the literature attests a much wider range of linguistic means toward this end (Pan and Kosicki, 1993; Greene and Resnik, 2009; Choi et al., 2012; Baumer et al., 2015). Framing is associated with several phenomena to which NLP has been applied, including ideology (Lin et al., 2006; Hardisty et al., 2010; Iyyer et al., 2014), sentiment (Pang and Lee, 2008; Feldman, 2013), and stance (Walker et al., 2012; Hasan and Ng, 2013). Although such author attributes are interesting, framing scholarship is concerned with persistent patterns of representation of particular issues— without necessarily tying these to the states or intentions of authors—and the effects that such patterns Can framing be automatically recognized? If so, social-scientific studies of framing will be enabled by new measurements, and new applications might bring framing effects to the consciousness of everyday readers. Several recent studies have begun to explore unsupervised framing analysis of political text using autoregressive and hierarchical t"
D16-1148,D14-1083,0,0.016664,"ntities to be described (people as opposed to more abstract entities), and the ways of describing them (rare but specific words as opposed to more generic but potentially obvious terms). the relationships between entities (O’Connor et al., 2013; Iyyer et al., 2016), and predicting personality types from text (Flekova and Gurevych, 2015). Bamman also applied variants of the DPM to characters in novels (Bamman et al., 2014). Previous work on sentiment, stance, and opinion mining has focused on recognizing stance or political sentiment in online ideological debates (Somasundaran and Wiebe, 2010; Hasan and Ng, 2014; Sridhar et al., 2015), and other forms of social media (O’Connor et al., 2010; Agarwal et al., 2011), and recently through the lens of connotation frames (Rashkin et al., 2016). Opinion mining and sentiment analysis are the subject of ongoing research in NLP and have long served as test platforms for new methodologies (Socher et al., 2013; ˙Irsoy and Cardie, 2014; Tai et al., 2015) Framing is arguably one of the most important concepts in the social sciences, with roots in to sociology, psychology, and mass communication (Gitlin, 1980; Benford and Snow, 2000; D’Angelo and Kuypers, 2010); the"
D16-1148,D14-1080,0,0.0765443,"Missing"
D16-1148,P14-1105,0,0.13959,"at communication almost inescapably involves framing—choosing “a few elements of perceived reality and assembling a narrative that highlights connections among them to promote a particular interpretation” (Entman, 2007). Memorable examples include loaded phrases (death tax, war on terror), but the literature attests a much wider range of linguistic means toward this end (Pan and Kosicki, 1993; Greene and Resnik, 2009; Choi et al., 2012; Baumer et al., 2015). Framing is associated with several phenomena to which NLP has been applied, including ideology (Lin et al., 2006; Hardisty et al., 2010; Iyyer et al., 2014), sentiment (Pang and Lee, 2008; Feldman, 2013), and stance (Walker et al., 2012; Hasan and Ng, 2013). Although such author attributes are interesting, framing scholarship is concerned with persistent patterns of representation of particular issues— without necessarily tying these to the states or intentions of authors—and the effects that such patterns Can framing be automatically recognized? If so, social-scientific studies of framing will be enabled by new measurements, and new applications might bring framing effects to the consciousness of everyday readers. Several recent studies have beg"
D16-1148,N16-1180,0,0.0540162,"Missing"
D16-1148,W06-2915,0,0.0655521,"1 Introduction Social science tells us that communication almost inescapably involves framing—choosing “a few elements of perceived reality and assembling a narrative that highlights connections among them to promote a particular interpretation” (Entman, 2007). Memorable examples include loaded phrases (death tax, war on terror), but the literature attests a much wider range of linguistic means toward this end (Pan and Kosicki, 1993; Greene and Resnik, 2009; Choi et al., 2012; Baumer et al., 2015). Framing is associated with several phenomena to which NLP has been applied, including ideology (Lin et al., 2006; Hardisty et al., 2010; Iyyer et al., 2014), sentiment (Pang and Lee, 2008; Feldman, 2013), and stance (Walker et al., 2012; Hasan and Ng, 2013). Although such author attributes are interesting, framing scholarship is concerned with persistent patterns of representation of particular issues— without necessarily tying these to the states or intentions of authors—and the effects that such patterns Can framing be automatically recognized? If so, social-scientific studies of framing will be enabled by new measurements, and new applications might bring framing effects to the consciousness of every"
D16-1148,P14-5010,0,0.0122698,"Entities The original focus of the DPM was on named characters in movies, which could be identified using named entity recognition and pronominal coreference (Bamman et al., 2013), or name matching for pre-defined characters (Bamman et al., 2014). Here, we are interested in applying our model to entities about which we assume no specific prior knowledge. In order to include a broader set of entities, we preprocess the corpus and apply a series of filters. First, we obtain lemmas, part-of-speech tags, dependencies, coreference resolution, and named entities from the Stanford CoreNLP pipeline (Manning et al., 2014), as well as supersense tags from the AMALGrAM tagger (Schneider and Smith, 2015). For each document, we consider all tokens with a 2 The MFC also contains more fine-grained annotations of spans of text which cue each of the framing dimensions, but we do not make use of those here. NN* or PRP part of speech as possible entities, partially clustered by coreference. We then merge all clusters (including singletons) within each document that share a non-pronomial mention word. Next, we exclude all clusters lacking at least one mention classified as a person, organization, location, group, object,"
D16-1148,P15-1139,0,0.0225577,"are interesting, framing scholarship is concerned with persistent patterns of representation of particular issues— without necessarily tying these to the states or intentions of authors—and the effects that such patterns Can framing be automatically recognized? If so, social-scientific studies of framing will be enabled by new measurements, and new applications might bring framing effects to the consciousness of everyday readers. Several recent studies have begun to explore unsupervised framing analysis of political text using autoregressive and hierarchical topic models (Nguyen et al., 2013; Nguyen et al., 2015; Tsur et al., 2015), but most of these conceptualize framing along a single dimension. Rather than trying to place individual articles on a continuum from liberal to conservative or positive to negative, we are interested in discovering broad-based patterns in the ways in which the media communicate about issues. Here, our focus is on the narratives found in news stories, specifically the participants in those stories. Insofar as journalists make use of archetypal narratives (e.g., the struggle of an individual against a more powerful adversary), we expect to see recurring representations of"
D16-1148,P13-1108,1,0.845324,"Missing"
D16-1148,P16-1030,0,0.0271402,"ous terms). the relationships between entities (O’Connor et al., 2013; Iyyer et al., 2016), and predicting personality types from text (Flekova and Gurevych, 2015). Bamman also applied variants of the DPM to characters in novels (Bamman et al., 2014). Previous work on sentiment, stance, and opinion mining has focused on recognizing stance or political sentiment in online ideological debates (Somasundaran and Wiebe, 2010; Hasan and Ng, 2014; Sridhar et al., 2015), and other forms of social media (O’Connor et al., 2010; Agarwal et al., 2011), and recently through the lens of connotation frames (Rashkin et al., 2016). Opinion mining and sentiment analysis are the subject of ongoing research in NLP and have long served as test platforms for new methodologies (Socher et al., 2013; ˙Irsoy and Cardie, 2014; Tai et al., 2015) Framing is arguably one of the most important concepts in the social sciences, with roots in to sociology, psychology, and mass communication (Gitlin, 1980; Benford and Snow, 2000; D’Angelo and Kuypers, 2010); the scope and relevance of framing is widely debated (Rees et al., 2001), with many authors applying the concept of framing to analyzing documents on particular issues (Baumgartner"
D16-1148,W09-1119,0,0.0595701,"l., 2001), with many authors applying the concept of framing to analyzing documents on particular issues (Baumgartner et al., 2008; Berinsky and Kinder, 2006). 9 The authors thank members of the ARK group and anonymous reviewers for helpful feedback on this work. This research was made possible by a Natural Sciences and Engineering Research Council of Canada Postgraduate Scholarship (to D.C.), a Bloomberg Data Science Research Grant (to J.H.G., A.E.B., and N.A.S.), and a University of Washington Innovation Award (to N.A.S.). Related Work Much NLP has focused on identifying entities or events (Ratinov and Roth, 2009; Ritter et al., 2012), analyzing schemes or narrative events in terms of characters (Chambers and Jurafsky, 2009), inferring 1418 10 Conclusion We have extended models for discovering latent personas to simultaneously cluster documents by their “casts” of personas. Our exploration of the model’s inferences and their incorporation into a challenging text analysis task—characterizing coarse-grained framing in news articles—demonstrate that personas are a useful abstraction when applying NLP to social-scientific inquiry. Finally, we introduced a Bayesian optimization approach to rigorously asses"
D16-1148,N15-1177,1,0.921476,"hich could be identified using named entity recognition and pronominal coreference (Bamman et al., 2013), or name matching for pre-defined characters (Bamman et al., 2014). Here, we are interested in applying our model to entities about which we assume no specific prior knowledge. In order to include a broader set of entities, we preprocess the corpus and apply a series of filters. First, we obtain lemmas, part-of-speech tags, dependencies, coreference resolution, and named entities from the Stanford CoreNLP pipeline (Manning et al., 2014), as well as supersense tags from the AMALGrAM tagger (Schneider and Smith, 2015). For each document, we consider all tokens with a 2 The MFC also contains more fine-grained annotations of spans of text which cue each of the framing dimensions, but we do not make use of those here. NN* or PRP part of speech as possible entities, partially clustered by coreference. We then merge all clusters (including singletons) within each document that share a non-pronomial mention word. Next, we exclude all clusters lacking at least one mention classified as a person, organization, location, group, object, artifact, process, or act (by CoreNLP or AMALGrAM). From these, we extract hw, r"
D16-1148,D13-1170,0,0.00345392,"amman also applied variants of the DPM to characters in novels (Bamman et al., 2014). Previous work on sentiment, stance, and opinion mining has focused on recognizing stance or political sentiment in online ideological debates (Somasundaran and Wiebe, 2010; Hasan and Ng, 2014; Sridhar et al., 2015), and other forms of social media (O’Connor et al., 2010; Agarwal et al., 2011), and recently through the lens of connotation frames (Rashkin et al., 2016). Opinion mining and sentiment analysis are the subject of ongoing research in NLP and have long served as test platforms for new methodologies (Socher et al., 2013; ˙Irsoy and Cardie, 2014; Tai et al., 2015) Framing is arguably one of the most important concepts in the social sciences, with roots in to sociology, psychology, and mass communication (Gitlin, 1980; Benford and Snow, 2000; D’Angelo and Kuypers, 2010); the scope and relevance of framing is widely debated (Rees et al., 2001), with many authors applying the concept of framing to analyzing documents on particular issues (Baumgartner et al., 2008; Berinsky and Kinder, 2006). 9 The authors thank members of the ARK group and anonymous reviewers for helpful feedback on this work. This research was"
D16-1148,W10-0214,0,0.0541487,"in this case in the types of entities to be described (people as opposed to more abstract entities), and the ways of describing them (rare but specific words as opposed to more generic but potentially obvious terms). the relationships between entities (O’Connor et al., 2013; Iyyer et al., 2016), and predicting personality types from text (Flekova and Gurevych, 2015). Bamman also applied variants of the DPM to characters in novels (Bamman et al., 2014). Previous work on sentiment, stance, and opinion mining has focused on recognizing stance or political sentiment in online ideological debates (Somasundaran and Wiebe, 2010; Hasan and Ng, 2014; Sridhar et al., 2015), and other forms of social media (O’Connor et al., 2010; Agarwal et al., 2011), and recently through the lens of connotation frames (Rashkin et al., 2016). Opinion mining and sentiment analysis are the subject of ongoing research in NLP and have long served as test platforms for new methodologies (Socher et al., 2013; ˙Irsoy and Cardie, 2014; Tai et al., 2015) Framing is arguably one of the most important concepts in the social sciences, with roots in to sociology, psychology, and mass communication (Gitlin, 1980; Benford and Snow, 2000; D’Angelo and"
D16-1148,P15-1012,0,0.0120922,"bed (people as opposed to more abstract entities), and the ways of describing them (rare but specific words as opposed to more generic but potentially obvious terms). the relationships between entities (O’Connor et al., 2013; Iyyer et al., 2016), and predicting personality types from text (Flekova and Gurevych, 2015). Bamman also applied variants of the DPM to characters in novels (Bamman et al., 2014). Previous work on sentiment, stance, and opinion mining has focused on recognizing stance or political sentiment in online ideological debates (Somasundaran and Wiebe, 2010; Hasan and Ng, 2014; Sridhar et al., 2015), and other forms of social media (O’Connor et al., 2010; Agarwal et al., 2011), and recently through the lens of connotation frames (Rashkin et al., 2016). Opinion mining and sentiment analysis are the subject of ongoing research in NLP and have long served as test platforms for new methodologies (Socher et al., 2013; ˙Irsoy and Cardie, 2014; Tai et al., 2015) Framing is arguably one of the most important concepts in the social sciences, with roots in to sociology, psychology, and mass communication (Gitlin, 1980; Benford and Snow, 2000; D’Angelo and Kuypers, 2010); the scope and relevance of"
D16-1148,P15-1150,0,0.0123804,"acters in novels (Bamman et al., 2014). Previous work on sentiment, stance, and opinion mining has focused on recognizing stance or political sentiment in online ideological debates (Somasundaran and Wiebe, 2010; Hasan and Ng, 2014; Sridhar et al., 2015), and other forms of social media (O’Connor et al., 2010; Agarwal et al., 2011), and recently through the lens of connotation frames (Rashkin et al., 2016). Opinion mining and sentiment analysis are the subject of ongoing research in NLP and have long served as test platforms for new methodologies (Socher et al., 2013; ˙Irsoy and Cardie, 2014; Tai et al., 2015) Framing is arguably one of the most important concepts in the social sciences, with roots in to sociology, psychology, and mass communication (Gitlin, 1980; Benford and Snow, 2000; D’Angelo and Kuypers, 2010); the scope and relevance of framing is widely debated (Rees et al., 2001), with many authors applying the concept of framing to analyzing documents on particular issues (Baumgartner et al., 2008; Berinsky and Kinder, 2006). 9 The authors thank members of the ARK group and anonymous reviewers for helpful feedback on this work. This research was made possible by a Natural Sciences and Engi"
D16-1148,P15-1157,0,0.161528,"ing scholarship is concerned with persistent patterns of representation of particular issues— without necessarily tying these to the states or intentions of authors—and the effects that such patterns Can framing be automatically recognized? If so, social-scientific studies of framing will be enabled by new measurements, and new applications might bring framing effects to the consciousness of everyday readers. Several recent studies have begun to explore unsupervised framing analysis of political text using autoregressive and hierarchical topic models (Nguyen et al., 2013; Nguyen et al., 2015; Tsur et al., 2015), but most of these conceptualize framing along a single dimension. Rather than trying to place individual articles on a continuum from liberal to conservative or positive to negative, we are interested in discovering broad-based patterns in the ways in which the media communicate about issues. Here, our focus is on the narratives found in news stories, specifically the participants in those stories. Insofar as journalists make use of archetypal narratives (e.g., the struggle of an individual against a more powerful adversary), we expect to see recurring representations of characters in these"
D16-1148,N12-1072,0,0.0128538,"perceived reality and assembling a narrative that highlights connections among them to promote a particular interpretation” (Entman, 2007). Memorable examples include loaded phrases (death tax, war on terror), but the literature attests a much wider range of linguistic means toward this end (Pan and Kosicki, 1993; Greene and Resnik, 2009; Choi et al., 2012; Baumer et al., 2015). Framing is associated with several phenomena to which NLP has been applied, including ideology (Lin et al., 2006; Hardisty et al., 2010; Iyyer et al., 2014), sentiment (Pang and Lee, 2008; Feldman, 2013), and stance (Walker et al., 2012; Hasan and Ng, 2013). Although such author attributes are interesting, framing scholarship is concerned with persistent patterns of representation of particular issues— without necessarily tying these to the states or intentions of authors—and the effects that such patterns Can framing be automatically recognized? If so, social-scientific studies of framing will be enabled by new measurements, and new applications might bring framing effects to the consciousness of everyday readers. Several recent studies have begun to explore unsupervised framing analysis of political text using autoregressi"
D16-1148,P12-2018,0,0.0158834,"m DPM, P2 = personas from our model, S = story clusters; MF = always predict most frequent class.) * indicates a statistically significant difference compared to the (W) baseline (p&lt;0.05). document. We did not use the topics (z) discovered by our model as features. 7.1 Experiment 1: Direct Comparison For the first experiment, we train independent multiclass logistic regression classifiers for predicting primary frame and tone. We consider adding persona and/or story cluster features to baseline classifiers based only on unigrams and bigrams with binarized counts, a simple but robust baseline (Wang and Manning, 2012).6 In all cases, we use L1 regularization and use 5-fold cross validation within each split’s training set to determine the strength of regularization. We then repeat this for each of the 10 folds, thereby producing one prediction (of primary frame and tone) for every annotated article. The results of this experiment are given in Table 3; for predicting the primary frame, classifiers that used persona and/or story cluster features achieve higher accuracy than the bag-of-words baseline (W); the classifier using personas from our model but not story clusters is significantly better than the base"
D16-1148,D15-1251,1,0.84919,"ally see them as a straw man. We propose a new and more rigorous method of comparison, in which a wide range of features are offered to an automatic model selection algorithm for each of the prediction tasks, with the features to be evaluated withheld from the baseline. Because no single combination of features and regularization strength is best for all situations, it is an empirical question which features are best for each task. We therefore make use of Bayesian optimization (Bayesopt) to make as many modeling decisions as possible (Pelikan, 2005; Snoek et al., 2012; Bergstra et al., 2015; Yogatama et al., 2015). In particular, let F be the set of features that might be used as input to any text classification algorithm. Let f be a new feature that is being proposed. Allow the inclusion or exclusion of each feature in the feature set to be a hyperparameter to be optimized, along with any additional decisions such as input transformations (e.g., lowercasing), and feature transformations (e.g., normalization). Using an automatic model selection algorithm such as Bayesian optimization, allow the performance on the validation set to guide choices about all of these hyperparameters on each iteration, and"
D16-1178,E14-1069,0,0.228088,"Missing"
D16-1178,I13-1042,0,0.0445811,"Missing"
D16-1178,P12-1078,0,0.0619688,"Missing"
D16-1180,P16-1231,0,0.391211,"ing standard distillation objectives to problems with structured outputs. The first-order distillation parser matches or surpasses the state of the art on English, Chinese, and German. 1 We give a probabilistic interpretation to the ensemble parser (with a minor modification), viewing it as an instance of minimum Bayes risk inference. We propose that disagreements among the ensemble’s members may be taken as a signal that an attachment decision is difficult or ambiguous. Introduction Neural network dependency parsers achieve state of the art performance (Dyer et al., 2015; Weiss et al., 2015; Andor et al., 2016), but training them involves gradient descent on non-convex objectives, which is unstable with respect to initial parameter values. For some tasks, an ensemble of neural networks from different random initializations has been found to improve performance over individual models (Sutskever et al., 2014; Vinyals et al., 2015, inter alia). In §3, we apply this idea to build a firstorder graph-based (FOG) ensemble parser (Sagae Ensemble parsing is not a practical solution, however, since an ensemble of N parsers requires N times as much computation, plus the runtime of finding consensus. We address"
D16-1180,D15-1041,1,0.89892,"Missing"
D16-1180,D16-1211,1,0.803959,"Missing"
D16-1180,D12-1133,0,0.212269,"ensemble pipeline. Accuracy. All scores are shown in Table 5. First, consider the neural FOG parser trained with Hamming cost (CH in the second-to-last row). This is a very strong benchmark, outperforming many higherorder graph-based and neural network models on all three datasets. Nonetheless, training the same model with distillation cost gives consistent improvements for all languages. For English, we see that this model comes close to the slower ensemble it was trained to simulate. For Chinese, it achieves the best published scores, for German the best published UAS scores, and just after Bohnet and Nivre (2012) for LAS. Effects of Pre-trained Word Embedding. As an ablation study, we ran experiments on English without pre-trained word embedding, both with the Hamming and distillation costs. The model trained with Hamming cost achieved 93.1 UAS and 90.9 LAS, compared to 93.6 UAS and 91.1 LAS for the model with distillation cost. This result further showcases the consistent improvements from using the distillation cost across different settings and languages. We conclude that “soft targets” derived from ensemble uncertainty offer useful guidance, through the distillation cost function and discriminativ"
D16-1180,D14-1082,0,0.22734,"ized with pretrained vectors and backpropagated during training. we apply a per-epoch learning rate decay of 0.05 to the Adam optimizer. While the Adam optimizer automatically adjusts the global learning rate according to past gradient magnitudes, we find that this additional per-epoch decay consistently improves performance across all settings and languages. 6 Experiments We ran experiments on the English PTB-SD version 3.3.0, Penn Chinese Treebank (Xue et al., 2002), and German CoNLL 2009 (Hajiˇc et al., 2009) tasks. Experimental settings. We used the standard splits for all languages. Like Chen and Manning (2014) and Dyer et al. (2015), we use predicted tags with the Stanford tagger (Toutanova et al., 2003) for English and gold tags for Chinese. For German we use the predicted tags provided by the CoNLL 2009 shared task organizers. All models were augmented with pretrained structured-skipgram (Ling et al., 2015) embeddings; for English we used the Gigaword corpus and 100 dimensions, for Chinese Gigaword and 80, and for German WMT 2010 monolingual data and 64. Hyperparameters. The hyperparameters for neural FOG are summarized in Table 4. For the Adam optimizer we use the default settings in the CNN neu"
D16-1180,P15-1033,1,0.267775,"sentropy computations required by applying standard distillation objectives to problems with structured outputs. The first-order distillation parser matches or surpasses the state of the art on English, Chinese, and German. 1 We give a probabilistic interpretation to the ensemble parser (with a minor modification), viewing it as an instance of minimum Bayes risk inference. We propose that disagreements among the ensemble’s members may be taken as a signal that an attachment decision is difficult or ambiguous. Introduction Neural network dependency parsers achieve state of the art performance (Dyer et al., 2015; Weiss et al., 2015; Andor et al., 2016), but training them involves gradient descent on non-convex objectives, which is unstable with respect to initial parameter values. For some tasks, an ensemble of neural networks from different random initializations has been found to improve performance over individual models (Sutskever et al., 2014; Vinyals et al., 2015, inter alia). In §3, we apply this idea to build a firstorder graph-based (FOG) ensemble parser (Sagae Ensemble parsing is not a practical solution, however, since an ensemble of N parsers requires N times as much computation, plus the"
D16-1180,C96-1058,0,0.199002,"ing function that considers only a single dependency arc at a time. (We suppress dependency labels; there are various ways to incorporate them, discussed later.) To define s, McDonald et al. (2005a) used hand-engineered features of the surrounding and in-between context of xh and xm ; more recently, Kiperwasser and Goldberg (2016) used a bidirectional LSTM followed by a single hidden layer with non-linearity. The exact solution to Eq. 1 can be found using a minimum (directed) spanning tree algorithm (McDonald et al., 2005b) or, under a projectivity constraint, a dynamic programming algorithm (Eisner, 1996), in O(n2 ) or O(n3 ) runtime, respectively. We refer to parsing with a minimum spanning tree algorithm as MST parsing. An alternative that runs in linear time is transition-based parsing, which recasts parsing as a sequence of actions that manipulate auxiliary data structures to incrementally build a parse tree (Nivre, 2003). Such parsers can return a solution in a faster O(n) asymptotic runtime. Unlike FOG parsers, transition-based parsers allow the use of scoring functions with history-based features, so that attachment decisions can interact more freely; the best performing parser at the t"
D16-1180,N10-1112,1,0.815815,"POS tag embeddings (Dyer et al., 2015), allowing the model to simultaneously leverage pretrained vectors and learn a taskspecific representation.7 Unlike Kiperwasser and Goldberg (2016), we did not observe any degradation by incorporating the pretrained vectors. Second, 6 Alternatives that do not use cost functions include probabilistic parsers, whether locally normalized like the stack LSTM parser used within our ensemble, or globally normalized, as in Andor et al. (2016); cost functions can be incorporated in such cases with minimum risk training (Smith and Eisner, 2006) or softmax margin (Gimpel and Smith, 2010). 7 To our understanding, Kiperwasser and Goldberg (2016) initialized with pretrained vectors and backpropagated during training. we apply a per-epoch learning rate decay of 0.05 to the Adam optimizer. While the Adam optimizer automatically adjusts the global learning rate according to past gradient magnitudes, we find that this additional per-epoch decay consistently improves performance across all settings and languages. 6 Experiments We ran experiments on the English PTB-SD version 3.3.0, Penn Chinese Treebank (Xue et al., 2002), and German CoNLL 2009 (Hajiˇc et al., 2009) tasks. Experiment"
D16-1180,D16-1139,0,0.0603475,"ensemble training to re-lexicalize a transfer model for a target language. We similarly use an ensemble of models as a supervision for a sin10 Our cost is zero when the correct arc is predicted, regardless of what the soft target thinks, something a compression model without gold supervision cannot do. gle model. By incorporating the ensemble uncertainty estimates in the cost function, our approach is cheaper, not requiring any marginalization during training. An additional difference is that we learn from the gold labels (“hard targets”) rather than only ensemble estimates on unlabeled data. Kim and Rush (2016) proposed a distillation model at the sequence level, with application in sequence-to-sequence neural machine translation. There are two primary differences with this work. First, we use a global model to distill the ensemble, instead of a sequential one. Second, Kim and Rush (2016) aim to distill a larger model into a smaller one, while we propose to distill an ensemble instead of a single model. 8 Conclusions We demonstrate that an ensemble of 20 greedy stack LSTMs (Dyer et al., 2015) can achieve state of the art accuracy on English dependency parsing. This approach corresponds to minimum Ba"
D16-1180,Q16-1023,0,0.116166,"w cost function, inspired by the notion of “soft targets” (Hinton et al., 2015). The essential idea is to derive the cost of each possible attachment from the ensemble’s division of votes, and use this cost in discriminative learning. The application of distilliation to structured prediction is, to our knowledge, new, as is the idea of empirically estimating cost functions. The distilled model performs almost as well as the ensemble consensus and much better than (i) a strong LSTM FOG parser trained using the conventional Hamming cost function, (ii) recently published strong LSTM FOG parsers (Kiperwasser and Goldberg, 2016; Wang and Chang, 2016), and (iii) many higher-order graph-based parsers (Koo and Collins, 2010; Martins et al., 2013; Le and Zuidema, 2014). It represents a new state of the art for graphbased dependency parsing for English, Chinese, and 1744 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1744–1753, c Austin, Texas, November 1-5, 2016. 2016 Association for Computational Linguistics German. The code to reproduce our results is publicly available.1 2 Notation and Definitions Let x = hx1 , . . . , xn i denote an n-length sentence. A dependency parse"
D16-1180,P10-1001,0,0.0124705,"o derive the cost of each possible attachment from the ensemble’s division of votes, and use this cost in discriminative learning. The application of distilliation to structured prediction is, to our knowledge, new, as is the idea of empirically estimating cost functions. The distilled model performs almost as well as the ensemble consensus and much better than (i) a strong LSTM FOG parser trained using the conventional Hamming cost function, (ii) recently published strong LSTM FOG parsers (Kiperwasser and Goldberg, 2016; Wang and Chang, 2016), and (iii) many higher-order graph-based parsers (Koo and Collins, 2010; Martins et al., 2013; Le and Zuidema, 2014). It represents a new state of the art for graphbased dependency parsing for English, Chinese, and 1744 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1744–1753, c Austin, Texas, November 1-5, 2016. 2016 Association for Computational Linguistics German. The code to reproduce our results is publicly available.1 2 Notation and Definitions Let x = hx1 , . . . , xn i denote an n-length sentence. A dependency parse for x, denoted y, is a set of tuples (h, m, `), where h is the index of a head, m the index of"
D16-1180,D14-1081,0,0.0194869,"he ensemble’s division of votes, and use this cost in discriminative learning. The application of distilliation to structured prediction is, to our knowledge, new, as is the idea of empirically estimating cost functions. The distilled model performs almost as well as the ensemble consensus and much better than (i) a strong LSTM FOG parser trained using the conventional Hamming cost function, (ii) recently published strong LSTM FOG parsers (Kiperwasser and Goldberg, 2016; Wang and Chang, 2016), and (iii) many higher-order graph-based parsers (Koo and Collins, 2010; Martins et al., 2013; Le and Zuidema, 2014). It represents a new state of the art for graphbased dependency parsing for English, Chinese, and 1744 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1744–1753, c Austin, Texas, November 1-5, 2016. 2016 Association for Computational Linguistics German. The code to reproduce our results is publicly available.1 2 Notation and Definitions Let x = hx1 , . . . , xn i denote an n-length sentence. A dependency parse for x, denoted y, is a set of tuples (h, m, `), where h is the index of a head, m the index of a modifier, and ` a dependency label (or rel"
D16-1180,N15-1142,1,0.446195,"oves performance across all settings and languages. 6 Experiments We ran experiments on the English PTB-SD version 3.3.0, Penn Chinese Treebank (Xue et al., 2002), and German CoNLL 2009 (Hajiˇc et al., 2009) tasks. Experimental settings. We used the standard splits for all languages. Like Chen and Manning (2014) and Dyer et al. (2015), we use predicted tags with the Stanford tagger (Toutanova et al., 2003) for English and gold tags for Chinese. For German we use the predicted tags provided by the CoNLL 2009 shared task organizers. All models were augmented with pretrained structured-skipgram (Ling et al., 2015) embeddings; for English we used the Gigaword corpus and 100 dimensions, for Chinese Gigaword and 80, and for German WMT 2010 monolingual data and 64. Hyperparameters. The hyperparameters for neural FOG are summarized in Table 4. For the Adam optimizer we use the default settings in the CNN neural network library.8 Since the ensemble is used to obtain the uncertainty on the training set, it is imperative that the stack LSTMs do not overfit the training set. To address this issue, we performed five-way jackknifing of the training data for each stack LSTM model to obtain the training data uncert"
D16-1180,C12-2077,0,0.0564198,"Missing"
D16-1180,D08-1017,1,0.760634,"cost—and use it in discriminative training of a single FOG parser. This allows us to distill what has been learned by the ensemble into a single model. 5 Distilling the Ensemble Despite its state of the art performance, our ensemble requires N parsing calls to decode each sentence. To reduce the computational cost, we introduce a method for “distilling” the ensemble’s knowledge into a single parser, making use of a novel cost function to communicate this knowledge from the ensemble to the distilled model. While models that combine the outputs of other parsing models have been proposed before (Martins et al., 2008; Nivre and McDonald, 2008; Zhang and Clark, 2008, inter alia), these works incorporated the scores or outputs of the baseline parsers as features and as such require running the first-stage models at test-time. Creating a cost function from a data analysis procedure is, to our knowledge, a new idea. The idea is attractive because cost functions are model-agnostic; they can be used with any parser amenable to discriminative training. Further, only the training procedure changes; parsing at test time does not require consulting the ensemble at all, avoiding the costly application of the N parse"
D16-1180,P13-2109,1,0.895869,"Missing"
D16-1180,P05-1012,0,0.876112,"(h, m, `), where h is the index of a head, m the index of a modifier, and ` a dependency label (or relation type). Most dependency parsers are constrained to return y that form a directed tree. A first-order graph-based (FOG; also known as “arc-factored”) dependency parser exactly solves y ˆ(x) = arg max y∈T (x) X s(h, m, x), (h,m)∈y | {z S(y,x) (1) } where T (x) is the set of directed trees over x, and s is a local scoring function that considers only a single dependency arc at a time. (We suppress dependency labels; there are various ways to incorporate them, discussed later.) To define s, McDonald et al. (2005a) used hand-engineered features of the surrounding and in-between context of xh and xm ; more recently, Kiperwasser and Goldberg (2016) used a bidirectional LSTM followed by a single hidden layer with non-linearity. The exact solution to Eq. 1 can be found using a minimum (directed) spanning tree algorithm (McDonald et al., 2005b) or, under a projectivity constraint, a dynamic programming algorithm (Eisner, 1996), in O(n2 ) or O(n3 ) runtime, respectively. We refer to parsing with a minimum spanning tree algorithm as MST parsing. An alternative that runs in linear time is transition-based par"
D16-1180,H05-1066,0,0.452646,"Missing"
D16-1180,P08-1108,0,0.0151135,"criminative training of a single FOG parser. This allows us to distill what has been learned by the ensemble into a single model. 5 Distilling the Ensemble Despite its state of the art performance, our ensemble requires N parsing calls to decode each sentence. To reduce the computational cost, we introduce a method for “distilling” the ensemble’s knowledge into a single parser, making use of a novel cost function to communicate this knowledge from the ensemble to the distilled model. While models that combine the outputs of other parsing models have been proposed before (Martins et al., 2008; Nivre and McDonald, 2008; Zhang and Clark, 2008, inter alia), these works incorporated the scores or outputs of the baseline parsers as features and as such require running the first-stage models at test-time. Creating a cost function from a data analysis procedure is, to our knowledge, a new idea. The idea is attractive because cost functions are model-agnostic; they can be used with any parser amenable to discriminative training. Further, only the training procedure changes; parsing at test time does not require consulting the ensemble at all, avoiding the costly application of the N parsers to new data, unlike mod"
D16-1180,W03-3017,0,0.0218202,"016) used a bidirectional LSTM followed by a single hidden layer with non-linearity. The exact solution to Eq. 1 can be found using a minimum (directed) spanning tree algorithm (McDonald et al., 2005b) or, under a projectivity constraint, a dynamic programming algorithm (Eisner, 1996), in O(n2 ) or O(n3 ) runtime, respectively. We refer to parsing with a minimum spanning tree algorithm as MST parsing. An alternative that runs in linear time is transition-based parsing, which recasts parsing as a sequence of actions that manipulate auxiliary data structures to incrementally build a parse tree (Nivre, 2003). Such parsers can return a solution in a faster O(n) asymptotic runtime. Unlike FOG parsers, transition-based parsers allow the use of scoring functions with history-based features, so that attachment decisions can interact more freely; the best performing parser at the time of this writing employ neural networks (Andor et al., 2016). 1 https://github.com/adhigunasurya/ distillation_parser.git 1745 Let hy (m) denote the parent of xm in y (using a special null symbol when m is the root of the tree), and hy0 (m) denotes the parent of xm in the predicted tree y 0 . Given two dependency parses of"
D16-1180,N10-1003,0,0.026416,"denoted with an underline. The † sign indicates the use of predicted tags for Chinese in the original publication, although we report accuracy using gold Chinese tags based on private correspondence with the authors. ered a FOG parser, though future work might investigate any parser amenable to training to minimize a cost-aware loss like the structured hinge. 7 Related Work Our work on ensembling dependency parsers is based on Sagae and Lavie (2006) and Surdeanu and Manning (2010); an additional contribution of this work is to show that the normalized ensemble votes correspond to MBR parsing. Petrov (2010) proposed a similar model combination with random initializations for phrase-structure parsing, using products of constituent marginals. The local optima in his base model’s training objective arise from latent variables instead of neural networks (in our case). Model distillation was proposed by Bucilˇa et al. (2006), who used a single neural network to simulate a large ensemble of classifiers. More recently, Ba and Caruana (2014) showed that a single shal1751 low neural network can closely replicate the performance of an ensemble of deep neural networks in phoneme recognition and object dete"
D16-1180,N06-2033,0,0.0848776,"ble, N = 10, MST ensemble, N = 15, MST ensemble, N = 20, MST Consensus and Minimum Bayes Risk Despite the recent success of neural network dependency parsers, most prior works exclusively report single-model performance. Ensembling neural network models trained from different random starting points is a standard technique in a variety of problems, such as machine translation (Sutskever et al., 2014) and constituency parsing (Vinyals et al., 2015). We aim to investigate the benefit of ensembling independently trained neural network dependency parsers by applying the parser ensembling method of Sagae and Lavie (2006) to a collection of N strong neural network base parsers. Here, each base parser is an instance of the greedy, transition-based parser of Dyer et al. (2015), known as the stack LSTM parser, trained from a different random initial estimate. Given a sentence x, the consensus FOG parser (Eq. 1) defines score s(h, m, x) as the number of base parsers that include the attachment (h, m), which we denote votes(h, m).2 An example of this scoring function with an ensemble of 20 models is shown in Figure 1 We assign to dependency (h, m) the label most frequently selected by the base parsers that attach m"
D16-1180,P06-2101,0,0.0302359,"nd compose them with learned embeddings and POS tag embeddings (Dyer et al., 2015), allowing the model to simultaneously leverage pretrained vectors and learn a taskspecific representation.7 Unlike Kiperwasser and Goldberg (2016), we did not observe any degradation by incorporating the pretrained vectors. Second, 6 Alternatives that do not use cost functions include probabilistic parsers, whether locally normalized like the stack LSTM parser used within our ensemble, or globally normalized, as in Andor et al. (2016); cost functions can be incorporated in such cases with minimum risk training (Smith and Eisner, 2006) or softmax margin (Gimpel and Smith, 2010). 7 To our understanding, Kiperwasser and Goldberg (2016) initialized with pretrained vectors and backpropagated during training. we apply a per-epoch learning rate decay of 0.05 to the Adam optimizer. While the Adam optimizer automatically adjusts the global learning rate according to past gradient magnitudes, we find that this additional per-epoch decay consistently improves performance across all settings and languages. 6 Experiments We ran experiments on the English PTB-SD version 3.3.0, Penn Chinese Treebank (Xue et al., 2002), and German CoNLL 2"
D16-1180,N10-1091,0,0.261328,"ndencies version 3.3.0 (De Marneffe and Manning, 2008) Penn Treebank task. As noted, the base parsers instantiate the greedy stack LSTM parser (Dyer et al., 2015).3 Table 1 shows that ensembles, even with small N , strongly outperform a single stack LSTM parser. Our ensembles of greedy, locally normalized parsers perform comparably to the best previously reported, due to Andor et al. (2016), which uses a beam (width 32) for training and decoding. 4 What is Ensemble Uncertainty? While previous works have already demonstrated the merit of ensembling in dependency parsing (Sagae and Lavie, 2006; Surdeanu and Manning, 2010), usually with diverse base parsers, we consider whether the posterior marginals estimated by pˆ((h, m) ∈ Y |x) = votes(h, m)/N can be interpreted. We conjecture that disagreement among base parsers about where to attach xm (i.e., uncertainty in the posterior) is a sign of difficulty or am3 We use the standard data split (02–21 for training, 22 for development, 23 for test), automatically predicted partof-speech tags, same pretrained word embedding as Dyer et al. (2015), and recommended hyperparameters; https:// github.com/clab/lstm-parser, each with a different random initialization; this dif"
D16-1180,N13-1126,0,0.0166262,"Missing"
D16-1180,N03-1033,0,0.0210996,"ate decay of 0.05 to the Adam optimizer. While the Adam optimizer automatically adjusts the global learning rate according to past gradient magnitudes, we find that this additional per-epoch decay consistently improves performance across all settings and languages. 6 Experiments We ran experiments on the English PTB-SD version 3.3.0, Penn Chinese Treebank (Xue et al., 2002), and German CoNLL 2009 (Hajiˇc et al., 2009) tasks. Experimental settings. We used the standard splits for all languages. Like Chen and Manning (2014) and Dyer et al. (2015), we use predicted tags with the Stanford tagger (Toutanova et al., 2003) for English and gold tags for Chinese. For German we use the predicted tags provided by the CoNLL 2009 shared task organizers. All models were augmented with pretrained structured-skipgram (Ling et al., 2015) embeddings; for English we used the Gigaword corpus and 100 dimensions, for Chinese Gigaword and 80, and for German WMT 2010 monolingual data and 64. Hyperparameters. The hyperparameters for neural FOG are summarized in Table 4. For the Adam optimizer we use the default settings in the CNN neural network library.8 Since the ensemble is used to obtain the uncertainty on the training set,"
D16-1180,P16-1218,0,0.223292,"notion of “soft targets” (Hinton et al., 2015). The essential idea is to derive the cost of each possible attachment from the ensemble’s division of votes, and use this cost in discriminative learning. The application of distilliation to structured prediction is, to our knowledge, new, as is the idea of empirically estimating cost functions. The distilled model performs almost as well as the ensemble consensus and much better than (i) a strong LSTM FOG parser trained using the conventional Hamming cost function, (ii) recently published strong LSTM FOG parsers (Kiperwasser and Goldberg, 2016; Wang and Chang, 2016), and (iii) many higher-order graph-based parsers (Koo and Collins, 2010; Martins et al., 2013; Le and Zuidema, 2014). It represents a new state of the art for graphbased dependency parsing for English, Chinese, and 1744 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1744–1753, c Austin, Texas, November 1-5, 2016. 2016 Association for Computational Linguistics German. The code to reproduce our results is publicly available.1 2 Notation and Definitions Let x = hx1 , . . . , xn i denote an n-length sentence. A dependency parse for x, denoted y, is a"
D16-1180,P15-1032,0,0.0231937,"ns required by applying standard distillation objectives to problems with structured outputs. The first-order distillation parser matches or surpasses the state of the art on English, Chinese, and German. 1 We give a probabilistic interpretation to the ensemble parser (with a minor modification), viewing it as an instance of minimum Bayes risk inference. We propose that disagreements among the ensemble’s members may be taken as a signal that an attachment decision is difficult or ambiguous. Introduction Neural network dependency parsers achieve state of the art performance (Dyer et al., 2015; Weiss et al., 2015; Andor et al., 2016), but training them involves gradient descent on non-convex objectives, which is unstable with respect to initial parameter values. For some tasks, an ensemble of neural networks from different random initializations has been found to improve performance over individual models (Sutskever et al., 2014; Vinyals et al., 2015, inter alia). In §3, we apply this idea to build a firstorder graph-based (FOG) ensemble parser (Sagae Ensemble parsing is not a practical solution, however, since an ensemble of N parsers requires N times as much computation, plus the runtime of finding"
D16-1180,C02-1145,0,0.117563,"isk training (Smith and Eisner, 2006) or softmax margin (Gimpel and Smith, 2010). 7 To our understanding, Kiperwasser and Goldberg (2016) initialized with pretrained vectors and backpropagated during training. we apply a per-epoch learning rate decay of 0.05 to the Adam optimizer. While the Adam optimizer automatically adjusts the global learning rate according to past gradient magnitudes, we find that this additional per-epoch decay consistently improves performance across all settings and languages. 6 Experiments We ran experiments on the English PTB-SD version 3.3.0, Penn Chinese Treebank (Xue et al., 2002), and German CoNLL 2009 (Hajiˇc et al., 2009) tasks. Experimental settings. We used the standard splits for all languages. Like Chen and Manning (2014) and Dyer et al. (2015), we use predicted tags with the Stanford tagger (Toutanova et al., 2003) for English and gold tags for Chinese. For German we use the predicted tags provided by the CoNLL 2009 shared task organizers. All models were augmented with pretrained structured-skipgram (Ling et al., 2015) embeddings; for English we used the Gigaword corpus and 100 dimensions, for Chinese Gigaword and 80, and for German WMT 2010 monolingual data a"
D16-1180,K15-1015,0,0.0197952,"Missing"
D16-1180,D08-1059,0,0.0578633,"single FOG parser. This allows us to distill what has been learned by the ensemble into a single model. 5 Distilling the Ensemble Despite its state of the art performance, our ensemble requires N parsing calls to decode each sentence. To reduce the computational cost, we introduce a method for “distilling” the ensemble’s knowledge into a single parser, making use of a novel cost function to communicate this knowledge from the ensemble to the distilled model. While models that combine the outputs of other parsing models have been proposed before (Martins et al., 2008; Nivre and McDonald, 2008; Zhang and Clark, 2008, inter alia), these works incorporated the scores or outputs of the baseline parsers as features and as such require running the first-stage models at test-time. Creating a cost function from a data analysis procedure is, to our knowledge, a new idea. The idea is attractive because cost functions are model-agnostic; they can be used with any parser amenable to discriminative training. Further, only the training procedure changes; parsing at test time does not require consulting the ensemble at all, avoiding the costly application of the N parsers to new data, unlike model combination techniqu"
D16-1180,P11-2033,0,0.0443312,"Missing"
D16-1180,P15-1112,0,0.0208299,"Missing"
D16-1202,Q15-1008,0,0.366058,"Missing"
D16-1202,W11-0611,0,0.0317273,"suggests that colors and words are associated in the brain. The brain uses different regions to perceive various modalities, but processing a color word activates the same brain region as the color it denotes (del Prado Mart´ın et al., 2006; Simmons et al., 2007). Closer to NLP, the relationship between visual stimuli and their linguistic descriptions by humans has been explored extensively through automatic text generation from images (Kiros et al., 2014; Karpathy and Fei-Fei, 2014; Xu et al., 2015). Color association with word semantics has also been investigated in several previous papers (Mohammad, 2011; Heer and Stone, 2012; Andreas and Klein, 2014; McMahan and Stone, 2015). 7 Conclusion In this paper, we introduced a computational model to predict a point in color space from the sequence of characters in the color’s name. Using a large set of color–name pairs obtained from a color design forum, we evaluate our model on a “color Turing test” and find that, given a name, the colors predicted by our model are preferred by annotators to color names created by humans. We also investigate the reverse mapping, from colors to names. We compare a conditional LSTM language model to a new latent-vari"
D16-1202,D16-1243,0,0.205224,"Missing"
D16-1202,W14-1607,0,\N,Missing
D16-1211,P16-1231,0,0.489171,".94 84.56 English UAS LAS 91.12 88.69 91.59 89.15 91.62 89.23 92.22 89.87 90.75 88.14 91.44 89.29 93.22 91.23 German UAS LAS 88.09 85.24 88.56 86.15 89.80 87.29 90.34 88.17 89.6 86.0 89.12 86.95 90.91 89.15 Japanese UAS LAS 93.10 92.28 – – 93.47 92.70 – – – – 93.71 92.85 93.65 92.84 Spanish UAS LAS 89.08 85.03 90.76 87.48 89.53 85.69 91.09 87.95 88.3 85.4 91.01 88.14 92.62 89.95 Table 2: Dependency parsing results. The dynamic oracle uses α = 0.75 (selected on English; see Table 1). PP refers to pseudoprojective parsing. Y’15 and A’16 are beam = 1 parsers from Yazdani and Henderson (2015) and Andor et al. (2016), respectively. A’16-beam is the parser with beam larger than 1 by Andor et al. (2016). Bold numbers indicate the best results among the greedy parsers. The error-exploring dynamic-oracle training always improves over static oracle training controlling for the transition system, but the arc-hybrid system slightly under-performs the arc-standard system when trained with static oracle. Flattening the sampling distribution (α = 0.75) is especially beneficial when training with pretrained word embeddings. In order to be able to compare with similar greedy parsers (Yazdani and Henderson, 2015; Ando"
D16-1211,W15-2210,0,0.0477838,"Missing"
D16-1211,P05-1022,0,0.0279624,"English UAS LAS 92.40 90.04 92.08 89.80 92.66 90.43 92.73 90.60 Chinese UAS LAS 85.48 83.94 85.66 84.03 86.07 84.46 86.13 84.53 93.04 92.78 93.15 93.56 86.85 86.94 87.05 87.65 90.87 90.67 91.05 91.42 85.36 85.46 85.63 86.21 Table 1: Dependency parsing: English (SD) and Chinese. The score achieved by the dynamic oracle for English is 93.56 UAS. This is remarkable given that the parser uses a completely greedy search procedure. Moreover, the Chinese score establishes the state-of-the-art, using the same settings as Chen and Manning (2014). 3 A similar objective was used by Riezler et al (2000), Charniak and Johnson (2005) and Goldberg (2013) in the context of log-linear probabilistic models. 4 The results on the development sets are similar and only used for optimization and validation. Method Arc-standard, static + PP + pre-training Arc-hybrid, dyn. + PP + pre-training Y’15 A’16 + pre-training A’16-beam Catalan UAS LAS 89.60 85.45 – – 90.45 86.38 – – – – 91.24 88.21 92.67 89.83 Chinese UAS LAS 79.68 75.08 82.45 78.55 80.74 76.52 83.54 79.66 – – 81.29 77.29 84.72 80.85 Czech UAS LAS 77.96 71.06 – – 85.68 79.38 – – 85.2 77.5 85.78 80.63 88.94 84.56 English UAS LAS 91.12 88.69 91.59 89.15 91.62 89.23 92.22 89.87"
D16-1211,D14-1082,0,0.316369,"urns out to be very beneficial for the configurations that make use of external embeddings. Indeed, these configurations achieve high accuracies and sharp class distributions early on in the training process. The parser is trained to maximize the likelihood of a correct action zg at each parsing state pt according to Equation 1. When using the dynamic oracle, a state pt may admit multiple correct actions zg = {zgi , . . . , zgk }. Our objective in such cases is the marginal likelihood of all correct actions,3 X p(zg |pt ) = p(zgi |pt ). (3) zgi ∈zg 3 Experiments Following the same settings of Chen and Manning (2014) and Dyer et al (2015) we report results4 in the English PTB and Chinese CTB-5. Table 1 shows the results of the parser in its different configurations. The table also shows the best result obtained with the static oracle (obtained by rerunning Dyer et al. parser) for the sake of comparison between static and dynamic training strategies. Method Arc-standard (Dyer et al.) Arc-hybrid (static) Arc-hybrid (dynamic) Arc-hybrid (dyn., α = 0.75) + pre-training: Arc-standard (Dyer et al.) Arc-hybrid (static) Arc-hybrid (dynamic) Arc-hybrid (dyn., α = 0.75) English UAS LAS 92.40 90.04 92.08 89.80 92.66"
D16-1211,P15-1033,1,0.660448,"Natural language parsing can be formulated as a series of decisions that read words in sequence and incrementally combine them to form syntactic structures; this formalization is known as transitionbased parsing, and is often coupled with a greedy search procedure (Yamada and Matsumoto, 2003; Nivre, 2003; Nivre, 2004; Nivre, 2008). The literature on transition-based parsing is vast, but all works share in common a classification component that takes into account features of the current parser state1 and predicts the next action to take conditioned on the state. The state is of unbounded size. Dyer et al. (2015) presented a parser in which the parser’s unbounded state is embedded in a fixeddimensional continuous space using recurrent neural networks. Coupled with a recursive tree composition function, the feature representation is able 1 The term “state” refers to the collection of previous decisions (sometimes called the history), resulting partial structures, which are typically stored in a stack data structure, and the words remaining to be processed. to capture information from the entirety of the state, without resorting to locality assumptions that were common in most other transition-based par"
D16-1211,C12-1059,1,0.947246,"ces, given words. At test time, the parser makes greedy decisions according to the learned model. Although this setup obtains very good performance, the training and testing conditions are mismatched in the following way: at training time the historical context of an action is always derived from the gold standard (i.e., perfectly correct past actions), but at test time, it will be a model prediction. In this work, we adapt the training criterion so as to explore parser states drawn not only from the training data, but also from the model as it is being learned. To do so, we use the method of Goldberg and Nivre (2012; 2013) to dynamically chose an optimal (relative to the final attachment accuracy) action given an imperfect history. By interpolating between algorithm states sampled from the model and those sampled from the training data, more robust predictions at test time can be made. We show that the technique can be used to improve the strong parser of Dyer et al. 2 Parsing Model and Parameter Learning Our departure point is the parsing model described by Dyer et al. (2015). We do not describe the model in detail, and refer the reader to the original work. At each stage t of the parsing process, the p"
D16-1211,Q13-1033,1,0.93704,"ing with Exploration Improves a Greedy Stack LSTM Parser Miguel Ballesteros♦ Yoav Goldberg♣ Chris Dyer♠ Noah A. Smith♥ ♦ NLP Group, Pompeu Fabra University, Barcelona, Spain ♣ Computer Science Department, Bar-Ilan University, Ramat Gan, Israel ♠ Google DeepMind, London, UK ♥ Computer Science & Engineering, University of Washington, Seattle, WA, USA miguel.ballesteros@upf.edu, yoav.goldberg@gmail.com, cdyer@google.com, nasmith@cs.washington.edu Abstract We adapt the greedy stack LSTM dependency parser of Dyer et al. (2015) to support a training-with-exploration procedure using dynamic oracles (Goldberg and Nivre, 2013) instead of assuming an error-free action history. This form of training, which accounts for model predictions at training time, improves parsing accuracies. We discuss some modifications needed in order to get training with exploration to work well for a probabilistic neural network dependency parser. 1 Introduction Natural language parsing can be formulated as a series of decisions that read words in sequence and incrementally combine them to form syntactic structures; this formalization is known as transitionbased parsing, and is often coupled with a greedy search procedure (Yamada and Mats"
D16-1211,Q14-1010,1,0.946318,"if the best tree that can be reached after taking the action is no worse (in terms of accuracy with respect to the gold tree) than the best tree that could be reached prior to taking that action. Goldberg and Nivre (2013) define the arcdecomposition property of transition systems, and show how to derive efficient dynamic oracles for transition systems that are arc-decomposable.2 Unfortunately, the arc-standard transition system does 2 Specifically: for every parser configuration p and group of not have this property. While it is possible to compute dynamic oracles for the arc-standard system (Goldberg et al., 2014), the computation relies on a dynamic programming algorithm which is polynomial in the length of the stack. As the dynamic oracle has to be queried for each parser state seen during training, the use of this dynamic oracle will make the training runtime several times longer. We chose instead to switch to the arc-hybrid transition system (Kuhlmann et al., 2011), which is very similar to the arc-standard system but is arc-decomposable and hence admits an efficient O(1) dynamic oracle, resulting in only negligible increase to training runtime. We implemented the dynamic oracle to the arc-hybrid s"
D16-1211,W13-5709,1,0.953811,"the training examples states that result from wrong parsing decisions, together with the optimal transitions to take in these states. To this end we reconsider which training examples to show, and what it means to behave optimally on these training examples. The framework of training with exploration using dynamic oracles suggested by Goldberg and Nivre (2012; 2013) provides answers to these questions. While the application of dynamic oracle training is relatively straightforward, some adaptations were needed to accommodate the probabilistic training objective. These adaptations mostly follow Goldberg (2013). Dynamic Oracles. A dynamic oracle is the component that, given a gold parse tree, provides the optimal set of possible actions to take for any valid parser state. In contrast to static oracles that derive a canonical state sequence for each gold parse tree and say nothing about states that deviate from this canonical path, the dynamic oracle is well defined for states that result from parsing mistakes, and they may produce more than a single gold action for a given state. Under the dynamic oracle framework, an action is said to be optimal for a state if the best tree that can be reached afte"
D16-1211,P15-2042,0,0.0776288,"Missing"
D16-1211,D14-1099,0,0.0889626,"Missing"
D16-1211,Q14-1011,0,0.0127702,"rained word embeddings for English, Chinese, German, and Spanish following the same training setup as Dyer et al. (2015); for English and Chinese we used the same pretrained word embeddings as in Table 1, for German we used the monolingual training data from the WMT 2015 dataset and for Spanish we used the Spanish Gigaword version 3. See Table 2. 4 Related Work Training greedy parsers on non-gold outcomes, facilitated by dynamic oracles, has been explored by several researchers in different ways (Goldberg and Nivre, 2012; Goldberg and Nivre, 2013; Goldberg et al., 2014; Honnibal et al., 2013; Honnibal and Johnson, 2014; G´omez-Rodr´ıguez et al., 2014; 5 We report the performance of these parsers in the most comparable setup, that is, with beam size 1 or greedy search. 2008 Bj¨orkelund and Nivre, 2015; Tokg¨oz and Eryi˘git, 2015; G´omez-Rodr´ıguez and Fern´andez-Gonz´alez, 2015; Vaswani and Sagae, 2016). More generally, training greedy search systems by paying attention to the expected classifier behavior during test time has been explored under the imitation learning and learning-to-search frameworks (Abbeel and Ng, 2004; Daum´e III and Marcu, 2005; Vlachos, 2012; He et al., 2012; Daum´e III et al., 2009; R"
D16-1211,W13-3518,1,0.840178,"clude results with pretrained word embeddings for English, Chinese, German, and Spanish following the same training setup as Dyer et al. (2015); for English and Chinese we used the same pretrained word embeddings as in Table 1, for German we used the monolingual training data from the WMT 2015 dataset and for Spanish we used the Spanish Gigaword version 3. See Table 2. 4 Related Work Training greedy parsers on non-gold outcomes, facilitated by dynamic oracles, has been explored by several researchers in different ways (Goldberg and Nivre, 2012; Goldberg and Nivre, 2013; Goldberg et al., 2014; Honnibal et al., 2013; Honnibal and Johnson, 2014; G´omez-Rodr´ıguez et al., 2014; 5 We report the performance of these parsers in the most comparable setup, that is, with beam size 1 or greedy search. 2008 Bj¨orkelund and Nivre, 2015; Tokg¨oz and Eryi˘git, 2015; G´omez-Rodr´ıguez and Fern´andez-Gonz´alez, 2015; Vaswani and Sagae, 2016). More generally, training greedy search systems by paying attention to the expected classifier behavior during test time has been explored under the imitation learning and learning-to-search frameworks (Abbeel and Ng, 2004; Daum´e III and Marcu, 2005; Vlachos, 2012; He et al., 2012"
D16-1211,P11-1068,0,0.126938,"Missing"
D16-1211,P05-1013,0,0.0596932,"rc-hybrid system slightly under-performs the arc-standard system when trained with static oracle. Flattening the sampling distribution (α = 0.75) is especially beneficial when training with pretrained word embeddings. In order to be able to compare with similar greedy parsers (Yazdani and Henderson, 2015; Andor et al., 2016)5 we report the performance of the parser on the multilingual treebanks of the CoNLL 2009 shared task (Hajiˇc et al., 2009). Since some of the treebanks contain nonprojective sentences and archybrid does not allow nonprojective trees, we use the pseudo-projective approach (Nivre and Nilsson, 2005). We used predicted part-of-speech tags provided by the CoNLL 2009 shared task organizers. We also include results with pretrained word embeddings for English, Chinese, German, and Spanish following the same training setup as Dyer et al. (2015); for English and Chinese we used the same pretrained word embeddings as in Table 1, for German we used the monolingual training data from the WMT 2015 dataset and for Spanish we used the Spanish Gigaword version 3. See Table 2. 4 Related Work Training greedy parsers on non-gold outcomes, facilitated by dynamic oracles, has been explored by several resea"
D16-1211,W03-3017,0,0.353357,"ssuming an error-free action history. This form of training, which accounts for model predictions at training time, improves parsing accuracies. We discuss some modifications needed in order to get training with exploration to work well for a probabilistic neural network dependency parser. 1 Introduction Natural language parsing can be formulated as a series of decisions that read words in sequence and incrementally combine them to form syntactic structures; this formalization is known as transitionbased parsing, and is often coupled with a greedy search procedure (Yamada and Matsumoto, 2003; Nivre, 2003; Nivre, 2004; Nivre, 2008). The literature on transition-based parsing is vast, but all works share in common a classification component that takes into account features of the current parser state1 and predicts the next action to take conditioned on the state. The state is of unbounded size. Dyer et al. (2015) presented a parser in which the parser’s unbounded state is embedded in a fixeddimensional continuous space using recurrent neural networks. Coupled with a recursive tree composition function, the feature representation is able 1 The term “state” refers to the collection of previous de"
D16-1211,W04-0308,0,0.52657,"ror-free action history. This form of training, which accounts for model predictions at training time, improves parsing accuracies. We discuss some modifications needed in order to get training with exploration to work well for a probabilistic neural network dependency parser. 1 Introduction Natural language parsing can be formulated as a series of decisions that read words in sequence and incrementally combine them to form syntactic structures; this formalization is known as transitionbased parsing, and is often coupled with a greedy search procedure (Yamada and Matsumoto, 2003; Nivre, 2003; Nivre, 2004; Nivre, 2008). The literature on transition-based parsing is vast, but all works share in common a classification component that takes into account features of the current parser state1 and predicts the next action to take conditioned on the state. The state is of unbounded size. Dyer et al. (2015) presented a parser in which the parser’s unbounded state is embedded in a fixeddimensional continuous space using recurrent neural networks. Coupled with a recursive tree composition function, the feature representation is able 1 The term “state” refers to the collection of previous decisions (some"
D16-1211,J08-4003,0,0.0983095,"on history. This form of training, which accounts for model predictions at training time, improves parsing accuracies. We discuss some modifications needed in order to get training with exploration to work well for a probabilistic neural network dependency parser. 1 Introduction Natural language parsing can be formulated as a series of decisions that read words in sequence and incrementally combine them to form syntactic structures; this formalization is known as transitionbased parsing, and is often coupled with a greedy search procedure (Yamada and Matsumoto, 2003; Nivre, 2003; Nivre, 2004; Nivre, 2008). The literature on transition-based parsing is vast, but all works share in common a classification component that takes into account features of the current parser state1 and predicts the next action to take conditioned on the state. The state is of unbounded size. Dyer et al. (2015) presented a parser in which the parser’s unbounded state is embedded in a fixeddimensional continuous space using recurrent neural networks. Coupled with a recursive tree composition function, the feature representation is able 1 The term “state” refers to the collection of previous decisions (sometimes called t"
D16-1211,P00-1061,0,0.278952,"brid (dyn., α = 0.75) English UAS LAS 92.40 90.04 92.08 89.80 92.66 90.43 92.73 90.60 Chinese UAS LAS 85.48 83.94 85.66 84.03 86.07 84.46 86.13 84.53 93.04 92.78 93.15 93.56 86.85 86.94 87.05 87.65 90.87 90.67 91.05 91.42 85.36 85.46 85.63 86.21 Table 1: Dependency parsing: English (SD) and Chinese. The score achieved by the dynamic oracle for English is 93.56 UAS. This is remarkable given that the parser uses a completely greedy search procedure. Moreover, the Chinese score establishes the state-of-the-art, using the same settings as Chen and Manning (2014). 3 A similar objective was used by Riezler et al (2000), Charniak and Johnson (2005) and Goldberg (2013) in the context of log-linear probabilistic models. 4 The results on the development sets are similar and only used for optimization and validation. Method Arc-standard, static + PP + pre-training Arc-hybrid, dyn. + PP + pre-training Y’15 A’16 + pre-training A’16-beam Catalan UAS LAS 89.60 85.45 – – 90.45 86.38 – – – – 91.24 88.21 92.67 89.83 Chinese UAS LAS 79.68 75.08 82.45 78.55 80.74 76.52 83.54 79.66 – – 81.29 77.29 84.72 80.85 Czech UAS LAS 77.96 71.06 – – 85.68 79.38 – – 85.2 77.5 85.78 80.63 88.94 84.56 English UAS LAS 91.12 88.69 91.59"
D16-1211,P15-3004,0,0.0262076,"Missing"
D16-1211,Q16-1014,0,0.0220862,"nish we used the Spanish Gigaword version 3. See Table 2. 4 Related Work Training greedy parsers on non-gold outcomes, facilitated by dynamic oracles, has been explored by several researchers in different ways (Goldberg and Nivre, 2012; Goldberg and Nivre, 2013; Goldberg et al., 2014; Honnibal et al., 2013; Honnibal and Johnson, 2014; G´omez-Rodr´ıguez et al., 2014; 5 We report the performance of these parsers in the most comparable setup, that is, with beam size 1 or greedy search. 2008 Bj¨orkelund and Nivre, 2015; Tokg¨oz and Eryi˘git, 2015; G´omez-Rodr´ıguez and Fern´andez-Gonz´alez, 2015; Vaswani and Sagae, 2016). More generally, training greedy search systems by paying attention to the expected classifier behavior during test time has been explored under the imitation learning and learning-to-search frameworks (Abbeel and Ng, 2004; Daum´e III and Marcu, 2005; Vlachos, 2012; He et al., 2012; Daum´e III et al., 2009; Ross et al., 2011; Chang et al., 2015). Directly modeling the probability of making a mistake has also been explored for parsing (Yazdani and Henderson, 2015). Generally, the use of RNNs to conditionally predict actions in sequence given a history is spurring increased interest in training"
D16-1211,W03-3023,0,0.18219,"nd Nivre, 2013) instead of assuming an error-free action history. This form of training, which accounts for model predictions at training time, improves parsing accuracies. We discuss some modifications needed in order to get training with exploration to work well for a probabilistic neural network dependency parser. 1 Introduction Natural language parsing can be formulated as a series of decisions that read words in sequence and incrementally combine them to form syntactic structures; this formalization is known as transitionbased parsing, and is often coupled with a greedy search procedure (Yamada and Matsumoto, 2003; Nivre, 2003; Nivre, 2004; Nivre, 2008). The literature on transition-based parsing is vast, but all works share in common a classification component that takes into account features of the current parser state1 and predicts the next action to take conditioned on the state. The state is of unbounded size. Dyer et al. (2015) presented a parser in which the parser’s unbounded state is embedded in a fixeddimensional continuous space using recurrent neural networks. Coupled with a recursive tree composition function, the feature representation is able 1 The term “state” refers to the collection o"
D16-1211,K15-1015,0,0.032838,"ize 1 or greedy search. 2008 Bj¨orkelund and Nivre, 2015; Tokg¨oz and Eryi˘git, 2015; G´omez-Rodr´ıguez and Fern´andez-Gonz´alez, 2015; Vaswani and Sagae, 2016). More generally, training greedy search systems by paying attention to the expected classifier behavior during test time has been explored under the imitation learning and learning-to-search frameworks (Abbeel and Ng, 2004; Daum´e III and Marcu, 2005; Vlachos, 2012; He et al., 2012; Daum´e III et al., 2009; Ross et al., 2011; Chang et al., 2015). Directly modeling the probability of making a mistake has also been explored for parsing (Yazdani and Henderson, 2015). Generally, the use of RNNs to conditionally predict actions in sequence given a history is spurring increased interest in training regimens that make the learned model more robust to test-time prediction errors. Solutions based on curriculum learning (Bengio et al., 2015), expected loss training (Shen et al., 2015), and reinforcement learning have been proposed (Ranzato et al., 2016). Finally, abandoning greedy search in favor of approximate global search offers an alternative solution to the problems with greedy search (Andor et al., 2016), and has been analyzed as well (Kulesza and Pereira"
D17-1195,N15-1083,0,0.0350679,"dding of the entity et , not updated with ht . The intuition is that eet ,t−1 will help contextual information ht−1 to select the residual length of entity et . Wlength is the weight matrix for length prediction, with `max = 25 rows. Finally, the probability of a word x as the next token is jointly modeled by ht−1 and the vector representation of the most recently mentioned entity ecurrent : 1832 p(Xt = x |ht−1 , ecurrent ) ∝ CFSM(ht−1 + We ecurrent ), (6) where We is a transformation matrix to adjust the dimensionality of ecurrent . CFSM is a class factorized softmax function (Goodman, 2001; Baltescu and Blunsom, 2015). It uses a two-step prediction with predefined word classes instead of direct prediction on the whole vocabulary, and reduces the time complexity to the log of vocabulary size. et al. (2016) for the “memory blocks” in their recurrent entity network models. The difference is that their model updates all memory blocks in each time step. Instead, our updating scheme in Equation 8 only applies to the selected entity et at time step t. 2.4 The model is trained to maximize the log of the joint probability of R, E, L, and X: Dynamic entity representations Before predicting the entity at step t, we n"
D17-1195,P14-1005,0,0.0430609,"Missing"
D17-1195,P08-1090,0,0.0197777,"c representations. In previous work, such information has been added as features (Luo et al., 2004; Bj¨orkelund and Kuhn, 2014) or by computing distributed entity representations (Wiseman et al., 2016; Clark and Manning, 2016b). Our approach complements these previous methods. Entity prediction. The entity prediction task discussed in §5.3 is based on work by Modi et al. (2017). The main difference is that we do not assume that all entities belong to a previously known set of entity types specified for each narrative scenario. This task is also closely related to the “narrative cloze” task of Chambers and Jurafsky (2008) and the “story cloze test” of Mostafazadeh et al. (2016). Those studies aim to understand relationships between events, while our task focuses on predicting upcoming entity mentions. 7 Conclusion We have presented a neural language model, E N TITY NLM, that defines a distribution over texts and the mentioned entities. It provides vector representations for the entities and updates them dynamically in context. The dynamic representations are further used to help generate specific entity mentions and the following text. This model outperforms strong baselines and prior work on three tasks: lang"
D17-1195,D16-1245,0,0.290395,"sing their continuous representations. The score above is normalized over values {1, . . . , 1 + maxt0 &lt;t et0 }. f (e) represents a vector of distance features associated with e and the mentions of the existing entities. Hence two information sources are used to predict the next entity: (i) contextual information ht−1 , and (ii) distance features f (e) from the current mention to the closest mention from each previously mentioned entity. f (e) = 0 if e is a new entity. This term can also be extended to include other surface-form features for coreference resolution (Martschat and Strube, 2015; Clark and Manning, 2016b). For the chosen entity et from Equation 4, the distribution over its mention length is drawn according to p(Lt = ` |ht−1 , eet ,t−1 ) Probability distributions &gt; ∝ exp(Wlength,` [ht−1 ; eet ,t−1 ]), The generative story above referenced several parametric distributions defined based on vector representations of histories and entities. These are defined as follows. For r ∈ {0, 1}, p(Rt = r |ht−1 ) ∝ exp(h&gt; t−1 Wr r), (3) where r is the parameterized embedding associated with r, which paves the way for exploring entity type representations in future work; Wr is a parameter matrix for the bili"
D17-1195,P16-1061,0,0.281726,"sing their continuous representations. The score above is normalized over values {1, . . . , 1 + maxt0 &lt;t et0 }. f (e) represents a vector of distance features associated with e and the mentions of the existing entities. Hence two information sources are used to predict the next entity: (i) contextual information ht−1 , and (ii) distance features f (e) from the current mention to the closest mention from each previously mentioned entity. f (e) = 0 if e is a new entity. This term can also be extended to include other surface-form features for coreference resolution (Martschat and Strube, 2015; Clark and Manning, 2016b). For the chosen entity et from Equation 4, the distribution over its mention length is drawn according to p(Lt = ` |ht−1 , eet ,t−1 ) Probability distributions &gt; ∝ exp(Wlength,` [ht−1 ; eet ,t−1 ]), The generative story above referenced several parametric distributions defined based on vector representations of histories and entities. These are defined as follows. For r ∈ {0, 1}, p(Rt = r |ht−1 ) ∝ exp(h&gt; t−1 Wr r), (3) where r is the parameterized embedding associated with r, which paves the way for exploring entity type representations in future work; Wr is a parameter matrix for the bili"
D17-1195,N16-1024,1,0.839349,"due to the long-range dependency in recurrent neural networks, the search space of R, E, L during inference grows exponentially. We thus use importance sampling to approximate the marginal distribution of X. Specifically, with the samples from a proposal distribution Q(R, E, L|X), the approximated marginal probability is defined as X P (X) = P (X, R, E, L) R,E,L = X Q(R, E, L |X) R,E,L ≈ 1 N X {r(i) ,e(i) ,`(i) }∼Q P (X, R, E, L) Q(R, E, L |X) P (r(i) , e(i) , `(i) , x) Q(r(i) , e(i) , `(i) |x) (11) A similar idea of using importance sampling for language modeling evaluation has been used by Dyer et al. (2016). For language modeling evaluation, we train our model on the training set from the CoNLL 2012 dataset with coreference annotation. On the test data, we treat coreference structure as latent variables and use importance sampling to approximate the marginal distribution of X. For each document, the model randomly draws N = 100 samples from the proposal distribution, discussed next. Proposal distribution. For implementation of Q, we use a discriminative variant of E NTI TY NLM by taking the current word xt for predicting the entity-related variables in the same time step. Specifically, in the ge"
D17-1195,N10-1061,0,0.130565,"hen ecurrent still represents the most recently mentioned entity.) 4. Advance the RNN, i.e., feed it the word vector xt to compute ht (Equation 2). 5. If rt = 1, update eet ,t using eet ,t−1 and ht , then set ecurrent = eet ,t . Details of the entity updating are given in §2.4. 6. For every entity eι ∈ Et  {et }, set eι,t = eι,t−1 (i.e., no changes to other entities’ representations). Note that at any given time step t, ecurrent will always contain the most recent vector representation of the most recently mentioned entity. A generative model with a similar hierarchical structure was used by Haghighi and Klein (2010) for coreference resolution. Our approach differs in two important ways. First, our model defines a joint distribution over all of the text, not just the entity mentions. Second, we use representation learning rather than Bayesian nonparametrics, allowing natural integration with the language model. 2.3 To give the possibility of predicting a new entity, we need an entity embedding beforehand with index (1 + maxt0 &lt;t et0 ), which is randomly sampled from Equation 7. Then, for every e ∈ {1, . . . , 1 + maxt0 &lt;t et0 }: p(Et = e |Rt = 1, ht−1 ) &gt; ∝ exp(h&gt; t−1 Wentity ee,t−1 + wdist f (e)), where"
D17-1195,P13-2121,0,0.0773352,"Missing"
D17-1195,P82-1020,0,0.780172,"Missing"
D17-1195,N16-1037,1,0.8173,"ows the prediction accuracies. E NTITY NLM (line 4) significantly outperforms both baselines (line 1 and 2) and prior work (line 3) (p  0.01, paired t-test). The comparison between line 4 and 5 shows our model is even close to the human prediction performance. 6 Related Work Rich-context language models. The originally proposed recurrent neural network language models only capture information within sentences. To extend the capacity of RNNLMs, various researchers have incorporated information beyond sentence boundaries. Previous work focuses on contextual information from previous sentences (Ji et al., 2016a) or discourse relations between adjacent sentences (Ji et al., 2016b), showing improvements to language modeling and related tasks like coherence evaluation and discourse relation prediction. In this work, E NTITY NLM adds explicit entity information to the language model, which is another way of adding a memory Entity-related models. Two recent approaches to modeling entities in text are closely related to our model. The first is the “reference-aware” language models proposed by Yang et al. (2016), where the referred entities are from either a predefined item list, an external database, or"
D17-1195,H05-1004,0,0.184004,"Missing"
D17-1195,P04-1018,0,0.0138059,"obabillistic graphical model with the distance-dependent Chinese Restaurant Process (Pitman, 1995) for entity assignment, while our model is built on a recurrent neural network architecture. The reranking method considered in our coreference resolution evaluation could also be extended with samples from additional coreference resolution systems, to produce more variety (Ng, 2005). The benefit of such a system comes, we believe, from the explicit tracking of each entity throughout the text, providing entityspecific representations. In previous work, such information has been added as features (Luo et al., 2004; Bj¨orkelund and Kuhn, 2014) or by computing distributed entity representations (Wiseman et al., 2016; Clark and Manning, 2016b). Our approach complements these previous methods. Entity prediction. The entity prediction task discussed in §5.3 is based on work by Modi et al. (2017). The main difference is that we do not assume that all entities belong to a previously known set of entity types specified for each narrative scenario. This task is also closely related to the “narrative cloze” task of Chambers and Jurafsky (2008) and the “story cloze test” of Mostafazadeh et al. (2016). Those studi"
D17-1195,Q15-1029,1,0.936972,"ix for predicting entities using their continuous representations. The score above is normalized over values {1, . . . , 1 + maxt0 &lt;t et0 }. f (e) represents a vector of distance features associated with e and the mentions of the existing entities. Hence two information sources are used to predict the next entity: (i) contextual information ht−1 , and (ii) distance features f (e) from the current mention to the closest mention from each previously mentioned entity. f (e) = 0 if e is a new entity. This term can also be extended to include other surface-form features for coreference resolution (Martschat and Strube, 2015; Clark and Manning, 2016b). For the chosen entity et from Equation 4, the distribution over its mention length is drawn according to p(Lt = ` |ht−1 , eet ,t−1 ) Probability distributions &gt; ∝ exp(Wlength,` [ht−1 ; eet ,t−1 ]), The generative story above referenced several parametric distributions defined based on vector representations of histories and entities. These are defined as follows. For r ∈ {0, 1}, p(Rt = r |ht−1 ) ∝ exp(h&gt; t−1 Wr r), (3) where r is the parameterized embedding associated with r, which paves the way for exploring entity type representations in future work; Wr is a para"
D17-1195,Q17-1003,0,0.135791,"7. 2017 Association for Computational Linguistics • Et ∈ Et is the index of the entity referred to, if Rt = 1. The set Et consists of {1, . . . , 1 + maxt0 &lt;t et0 }, i.e., the indices of all previously mentioned entities plus an additional value for a new entity. Thus Et starts as {1} and grows monotonically with t, allowing for an arbitrary number of entities to be mentioned. We denote the value of Et by et . If Rt = 0, then Et is fixed to a special value ø. nally, the model can perform entity cloze tasks. As presented in §5.3, it achieves state-of-the-art performance on the InScript corpus (Modi et al., 2017). 2 Model A language model defines a distribution over sequences of word tokens; let Xt denote the random variable for the tth word in the sequence, xt denote the value of Xt and xt the distributed representation (embedding) of this word. Our starting point for language modeling is a recurrent neural network (Mikolov et al., 2010), which defines p(Xt |history) = softmax (Wh ht−1 + b) (1) ht−1 = LSTM(ht−2 , xt−1 ) (2) where Wh and b are parameters of the model (along with word embeddings xt ), LSTM is the widely used recurrent function known as “long short-term memory” (Hochreiter and Schmidhub"
D17-1195,W12-4501,0,0.199498,"guage model, augmented with random variables for entity mentions that capture coreference, and with dynamic representations of entities. We estimate the model’s parameters from data that is annotated with entity mentions and coreference. Because our model is generative, it can be queried in different ways. Marginalizing everything except the words, it can play the role of a language model. In §5.1, we find that it outperforms both a strong n-gram language model and a strong recurrent neural network language model on the English test set of the CoNLL 2012 shared task on coreference evaluation (Pradhan et al., 2012). The model can also identify entity mentions and coreference relationships among them. In §5.2, we show that it can easily be used to add a performance boost to a strong coreference resolution system, by reranking a list of k-best candidate outputs. On the CoNLL 2012 shared task test set, the reranked outputs are significantly better than the original top choices from the same system. Fi1830 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1830–1839 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics • Et ∈ E"
D17-1195,N04-1023,0,0.0266439,"we also use the marginalization method defined in Equation 11 on the development data to select the best configuration. However, we plan to use the same experimental setup for all experiments, instead of customizing our model for each individual task. 5.2 Coreference reranking Task description. We show how E NTITY LM, which allows an efficient computation of the probability P (R, E, L, X), can be used as a coreference reranker to improve a competitive coreference resolution system due to Martschat and Strube (2015). This task is analogous to the reranking approach used in machine translation (Shen et al., 2004). The specific formulation is as follows: arg max {r(i) ,e(i) ,l(i) }∈K P (r(i) , e(i) , l(i) , x) (12) where K is the k-best list for a given document. In our experiments, k = 100. To the best of our knowledge, the problem of obtaining k-best outputs of a coreference resolution system has not been studied before. Approximate k-best decoding. We rerank the output of a system that predicts an antecedent for each mention by relying on pairwise scores for mention pairs. This is the dominant approach for coreference resolution (Martschat and Strube, 2015; Clark and Manning, 2016a). The predictions"
D17-1195,N16-1036,0,0.0771503,"Missing"
D17-1195,M95-1005,0,0.693437,"Missing"
D17-1195,N16-1114,0,0.0759838,"Missing"
D17-1195,N16-1098,0,0.0263734,"een added as features (Luo et al., 2004; Bj¨orkelund and Kuhn, 2014) or by computing distributed entity representations (Wiseman et al., 2016; Clark and Manning, 2016b). Our approach complements these previous methods. Entity prediction. The entity prediction task discussed in §5.3 is based on work by Modi et al. (2017). The main difference is that we do not assume that all entities belong to a previously known set of entity types specified for each narrative scenario. This task is also closely related to the “narrative cloze” task of Chambers and Jurafsky (2008) and the “story cloze test” of Mostafazadeh et al. (2016). Those studies aim to understand relationships between events, while our task focuses on predicting upcoming entity mentions. 7 Conclusion We have presented a neural language model, E N TITY NLM, that defines a distribution over texts and the mentioned entities. It provides vector representations for the entities and updates them dynamically in context. The dynamic representations are further used to help generate specific entity mentions and the following text. This model outperforms strong baselines and prior work on three tasks: language modeling, coreference resolution, and entity predict"
D17-1195,P05-1020,0,0.0495578,"se their model for coreference reranking and entity prediction. Coreference resolution. The hierarchical structure of our entity generation model is inspired by 1837 Haghighi and Klein (2010). They implemented this idea as a probabillistic graphical model with the distance-dependent Chinese Restaurant Process (Pitman, 1995) for entity assignment, while our model is built on a recurrent neural network architecture. The reranking method considered in our coreference resolution evaluation could also be extended with samples from additional coreference resolution systems, to produce more variety (Ng, 2005). The benefit of such a system comes, we believe, from the explicit tracking of each entity throughout the text, providing entityspecific representations. In previous work, such information has been added as features (Luo et al., 2004; Bj¨orkelund and Kuhn, 2014) or by computing distributed entity representations (Wiseman et al., 2016; Clark and Manning, 2016b). Our approach complements these previous methods. Entity prediction. The entity prediction task discussed in §5.3 is based on work by Modi et al. (2017). The main difference is that we do not assume that all entities belong to a previou"
D17-1195,D14-1162,0,0.0965475,"M (Kingma and Ba, 2014) with default learning rate λ = 0.001 as the candidate optimizers of our model. For all the parameters, we use the initialization tricks recommended by Glorot and Bengio (2010). To avoid overfitting, we also employ dropout (Srivastava et al., 2014) with the candidate rates as {0.2, 0.5}. In addition, there are two tunable hyperparameters of E NTITY NLM: the size of word embeddings and the dimension of LSTM hidden states. For both of them, we consider the values {32, 48, 64, 128, 256}. We also experiment with the option to either use the pretrained GloVe word embeddings (Pennington et al., 2014) or randomly initialized word embeddings (then updated during training). For all experiments, the best configuration of hyperparameters and optimizers is selected based on the objective value on the development data. 4 Evaluation Tasks and Datasets We evaluate our model in diverse use scenarios: (i) language modeling, (ii) coreference resolution, 1833 and (iii) entity prediction. The evaluation on language modeling shows how the internal entity representation, when marginalized out, can improve the perplexity of language models. The evaluation on coreference resolution experiment shows how our"
D17-1195,P14-2006,0,0.0698471,"Missing"
D18-1034,D16-1250,0,0.390551,"e 2018 Conference on Empirical Methods in Natural Language Processing, pages 369–379 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics art cross-lingual results in Spanish and Dutch, and competitive results in German, even without a dictionary, completely removing the need for resources such as Wikipedia and parallel corpora. Next, we transfer English using the same approach into Uyghur, a truly low-resource language. With significantly fewer cross-lingual resources, our approach can still perform competitively with previous best results. 2014; Artetxe et al., 2016; Smith et al., 2017), or even in an entirely unsupervised manner using adversarial training or identical character strings (Zhang et al., 2017; Artetxe et al., 2017; Lample et al., 2018). Many approaches in the past have leveraged the shared embedding space for cross-lingual applications (Guo et al., 2015; Ammar et al., 2016b; Zhang et al., 2016; Fang and Cohn, 2017), including NER (Bharadwaj et al., 2016; Ni et al., 2017). The minimal dependency on parallel resources makes the embedding-based method much more suitable for low-resource languages. However, since different languages have differ"
D18-1034,P17-1042,0,0.167017,"tional Linguistics art cross-lingual results in Spanish and Dutch, and competitive results in German, even without a dictionary, completely removing the need for resources such as Wikipedia and parallel corpora. Next, we transfer English using the same approach into Uyghur, a truly low-resource language. With significantly fewer cross-lingual resources, our approach can still perform competitively with previous best results. 2014; Artetxe et al., 2016; Smith et al., 2017), or even in an entirely unsupervised manner using adversarial training or identical character strings (Zhang et al., 2017; Artetxe et al., 2017; Lample et al., 2018). Many approaches in the past have leveraged the shared embedding space for cross-lingual applications (Guo et al., 2015; Ammar et al., 2016b; Zhang et al., 2016; Fang and Cohn, 2017), including NER (Bharadwaj et al., 2016; Ni et al., 2017). The minimal dependency on parallel resources makes the embedding-based method much more suitable for low-resource languages. However, since different languages have different linguistic properties, it is hard, if not impossible, to align the two embedding spaces perfectly (see Figure 1). Meanwhile, because surface forms are not availa"
D18-1034,D16-1153,1,0.872625,"g the same approach into Uyghur, a truly low-resource language. With significantly fewer cross-lingual resources, our approach can still perform competitively with previous best results. 2014; Artetxe et al., 2016; Smith et al., 2017), or even in an entirely unsupervised manner using adversarial training or identical character strings (Zhang et al., 2017; Artetxe et al., 2017; Lample et al., 2018). Many approaches in the past have leveraged the shared embedding space for cross-lingual applications (Guo et al., 2015; Ammar et al., 2016b; Zhang et al., 2016; Fang and Cohn, 2017), including NER (Bharadwaj et al., 2016; Ni et al., 2017). The minimal dependency on parallel resources makes the embedding-based method much more suitable for low-resource languages. However, since different languages have different linguistic properties, it is hard, if not impossible, to align the two embedding spaces perfectly (see Figure 1). Meanwhile, because surface forms are not available, character-level features cannot be used, resulting in reduced tagging accuracy (as demonstrated in our experiments). 2 We establish our problem setting (§2.1), then present our methods in detail (§2.2), and provide some additional motivati"
D18-1034,Q17-1010,0,0.462655,"is shared space further, we iteratively perform a self-learning refinement step k 2 times by: 4. Train an NER model using the translated words along with the named entity tags from the English corpus (§2.2.4). We consider each in detail. 2.2.1 Learning Monolingual Embeddings Given text in the source and target language, we first independently learn word embedding matrices X and Y in the source and target languages respectively. These embeddings can be learned on monolingual text in both languages with any of the myriad of word embedding methods (Mikolov et al., 2013b; Pennington et al., 2014; Bojanowski et al., 2017). 2.2.2 Learning Bilingual Embeddings Next, we learn a cross-lingual projection of X and Y into a shared space. Assume we are given a dictionary {xi , yi }D i=1 , where xi and yi denote the embeddings of a word pair. Let XD = [x1 , x2 , · · · , xD ]> and YD = [y1 , y2 , · · · , yD ]> denote two embedding matrices consisting of word pairs from the dictionary. Following previous work (Zhang et al., 2016; Artetxe et al., 2016; Smith et al., 2017), we optimize the following objective: min W d X 2 kW xi − yi k s.t. W W > 1. Using the aligned embeddings to generate a new dictionary that consists of"
D18-1034,Q16-1026,0,0.0512909,"methods achieve state-of-the-art or competitive NER performance on commonly tested languages under a cross-lingual setting, with much lower resource requirements than past approaches. We also evaluate the challenges of applying these methods to Uyghur, a lowresource language.1 1 Introduction Named entity recognition (NER), the task of detecting and classifying named entities from text into a few predefined categories such as people, locations or organizations, has seen the state-of-theart greatly advanced by the introduction of neural architectures (Collobert et al., 2011; Huang et al., 2015; Chiu and Nichols, 2016; Lample et al., 2016; Yang et al., 2016; Ma and Hovy, 2016; Peters et al., 2017; Liu et al., 2018; Peters et al., 2018). However, the success of these methods is highly dependent on a reasonably large amount of annotated training data, and thus it remains a challenge to apply these models to languages with limited amounts of labeled data. Cross-lingual NER attempts to address this challenge by transferring 1 The source code is available at https://github. com/thespectrewithin/cross-lingual_NER 369 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 36"
D18-1034,P12-1073,0,0.0773916,"guage with abundant entity labels to a low-resource target language with few or no labels. Specifically, in this paper we attempt to tackle the extreme scenario of unsupervised transfer, where no labeled data is available in the target language. Within this paradigm, there are two major challenges to tackle: how to effectively perform lexical mapping between the languages, and how to address word order differences. To cope with the first challenge of lexical mapping, a number of methods use parallel corpora to project annotations between languages through word alignment (Ehrmann et al., 2011; Kim et al., 2012; Wang and Manning, 2014; Ni et al., 2017). Since parallel corpora may not be always available, Mayhew et al. (2017) proposed a “cheap translation” approach that uses a bilingual dictionary to perform word-level translation. The above approaches provide a reasonable proxy for the actual labeled training data, largely because the words that participate in entities can be translated relatively reliably given extensive parallel dictionaries or corpora (e.g., with 1 million word pairs or sentences). Additionally, as a side benefit of having explicitly translated words, models can directly exploit"
D18-1034,N16-1030,0,0.891813,"-the-art or competitive NER performance on commonly tested languages under a cross-lingual setting, with much lower resource requirements than past approaches. We also evaluate the challenges of applying these methods to Uyghur, a lowresource language.1 1 Introduction Named entity recognition (NER), the task of detecting and classifying named entities from text into a few predefined categories such as people, locations or organizations, has seen the state-of-theart greatly advanced by the introduction of neural architectures (Collobert et al., 2011; Huang et al., 2015; Chiu and Nichols, 2016; Lample et al., 2016; Yang et al., 2016; Ma and Hovy, 2016; Peters et al., 2017; Liu et al., 2018; Peters et al., 2018). However, the success of these methods is highly dependent on a reasonably large amount of annotated training data, and thus it remains a challenge to apply these models to languages with limited amounts of labeled data. Cross-lingual NER attempts to address this challenge by transferring 1 The source code is available at https://github. com/thespectrewithin/cross-lingual_NER 369 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 369–379 c Brussels, Bel"
D18-1034,I17-2016,0,0.0616114,"words to Wikipedia entries and uses the entry category as features to train language independent NER models. Recently, Ni et al. (2017) propose to project word embeddings into a common space as language independent features. These approaches utilize such features by training a model on the source language and directly applying it to the target language. Another way of performing language independent transfer resorts to multi-task learning, where a model is trained jointly across different languages by sharing parameters to allow for knowledge transfer (Ammar et al., 2016a; Yang et al., 2017; Cotterell and Duh, 2017; Lin et al., 2018). However, such approaches usually require some amounts of training data in the target language for bootstrapping, which is different from our unsupervised approach that requires no labeled resources in the target language. under a cross-lingual setting, with lower resource requirements than past approaches. We also evaluate the challenges of applying these methods to an extremely low-resource language, Uyghur. Bilingual Word Embeddings There have been two general paradigms in obtaining bilingual word vectors besides using dictionaries: through parallel corpora and through j"
D18-1034,P18-1074,0,0.126914,"es and uses the entry category as features to train language independent NER models. Recently, Ni et al. (2017) propose to project word embeddings into a common space as language independent features. These approaches utilize such features by training a model on the source language and directly applying it to the target language. Another way of performing language independent transfer resorts to multi-task learning, where a model is trained jointly across different languages by sharing parameters to allow for knowledge transfer (Ammar et al., 2016a; Yang et al., 2017; Cotterell and Duh, 2017; Lin et al., 2018). However, such approaches usually require some amounts of training data in the target language for bootstrapping, which is different from our unsupervised approach that requires no labeled resources in the target language. under a cross-lingual setting, with lower resource requirements than past approaches. We also evaluate the challenges of applying these methods to an extremely low-resource language, Uyghur. Bilingual Word Embeddings There have been two general paradigms in obtaining bilingual word vectors besides using dictionaries: through parallel corpora and through joint training. Appr"
D18-1034,P11-1061,0,0.0854468,"some named entities are not covered by the dictionary or the translation is not reliable. We suspect that the unreliable translation of named entities is the ma5 Related Work Cross-Lingual Learning Cross-lingual learning approaches can be loosely classified into two categories: annotation projection and languageindependent transfer. Annotation projection methods create training data by using parallel corpora to project annotations from the source to the target language. Such approaches have been applied to many tasks under the cross-lingual setting, such as POS tagging (Yarowsky et al., 2001; Das and Petrov, 2011; T¨ackstr¨om et al., 2013; Fang and Cohn, 2016), mention detection (Zitouni and Florian, 2008) and parsing (Hwa et al., 2005; McDonald et al., 2011). Language independent transfer-based approaches build models using language independent and delexicalized features. For instance, 6 https://github.com/facebookresearch/ fastText/blob/master/docs/crawl-vectors. md 376 Zirikly and Hagiwara (2015) transfers word cluster and gazetteer features through the use of comparable copora. Tsai et al. (2016) links words to Wikipedia entries and uses the entry category as features to train language independent"
D18-1034,R11-1017,0,0.479375,"gh-resource source language with abundant entity labels to a low-resource target language with few or no labels. Specifically, in this paper we attempt to tackle the extreme scenario of unsupervised transfer, where no labeled data is available in the target language. Within this paradigm, there are two major challenges to tackle: how to effectively perform lexical mapping between the languages, and how to address word order differences. To cope with the first challenge of lexical mapping, a number of methods use parallel corpora to project annotations between languages through word alignment (Ehrmann et al., 2011; Kim et al., 2012; Wang and Manning, 2014; Ni et al., 2017). Since parallel corpora may not be always available, Mayhew et al. (2017) proposed a “cheap translation” approach that uses a bilingual dictionary to perform word-level translation. The above approaches provide a reasonable proxy for the actual labeled training data, largely because the words that participate in entities can be translated relatively reliably given extensive parallel dictionaries or corpora (e.g., with 1 million word pairs or sentences). Additionally, as a side benefit of having explicitly translated words, models can"
D18-1034,P16-1101,0,0.605954,"gual dictionary to perform word-level translation. The above approaches provide a reasonable proxy for the actual labeled training data, largely because the words that participate in entities can be translated relatively reliably given extensive parallel dictionaries or corpora (e.g., with 1 million word pairs or sentences). Additionally, as a side benefit of having explicitly translated words, models can directly exploit features extracted from the surface forms (e.g. through character-level neural feature extractors), which has proven essential for high accuracy in the monolingual scenario (Ma and Hovy, 2016). However, these methods are largely predicated on the availability of large-scale parallel resources, and thus, their applicability to lowresource languages is limited. In contrast, it is also possible to learn lexical mappings through bilingual word embeddings (BWE). These bilingual embeddings can be obtained by using a small dictionary to project two sets of embeddings into a consistent space (Mikolov et al., 2013a; Faruqui and Dyer, For languages with no annotated resources, unsupervised transfer of natural language processing models such as named-entity recognition (NER) from resource-ric"
D18-1034,K16-1018,0,0.0300548,"onary or the translation is not reliable. We suspect that the unreliable translation of named entities is the ma5 Related Work Cross-Lingual Learning Cross-lingual learning approaches can be loosely classified into two categories: annotation projection and languageindependent transfer. Annotation projection methods create training data by using parallel corpora to project annotations from the source to the target language. Such approaches have been applied to many tasks under the cross-lingual setting, such as POS tagging (Yarowsky et al., 2001; Das and Petrov, 2011; T¨ackstr¨om et al., 2013; Fang and Cohn, 2016), mention detection (Zitouni and Florian, 2008) and parsing (Hwa et al., 2005; McDonald et al., 2011). Language independent transfer-based approaches build models using language independent and delexicalized features. For instance, 6 https://github.com/facebookresearch/ fastText/blob/master/docs/crawl-vectors. md 376 Zirikly and Hagiwara (2015) transfers word cluster and gazetteer features through the use of comparable copora. Tsai et al. (2016) links words to Wikipedia entries and uses the entry category as features to train language independent NER models. Recently, Ni et al. (2017) propose"
D18-1034,D17-1269,0,0.420462,"Missing"
D18-1034,P17-2093,0,0.159085,"rpora. Next, we transfer English using the same approach into Uyghur, a truly low-resource language. With significantly fewer cross-lingual resources, our approach can still perform competitively with previous best results. 2014; Artetxe et al., 2016; Smith et al., 2017), or even in an entirely unsupervised manner using adversarial training or identical character strings (Zhang et al., 2017; Artetxe et al., 2017; Lample et al., 2018). Many approaches in the past have leveraged the shared embedding space for cross-lingual applications (Guo et al., 2015; Ammar et al., 2016b; Zhang et al., 2016; Fang and Cohn, 2017), including NER (Bharadwaj et al., 2016; Ni et al., 2017). The minimal dependency on parallel resources makes the embedding-based method much more suitable for low-resource languages. However, since different languages have different linguistic properties, it is hard, if not impossible, to align the two embedding spaces perfectly (see Figure 1). Meanwhile, because surface forms are not available, character-level features cannot be used, resulting in reduced tagging accuracy (as demonstrated in our experiments). 2 We establish our problem setting (§2.1), then present our methods in detail (§2.2"
D18-1034,D11-1006,0,0.0876572,"ies is the ma5 Related Work Cross-Lingual Learning Cross-lingual learning approaches can be loosely classified into two categories: annotation projection and languageindependent transfer. Annotation projection methods create training data by using parallel corpora to project annotations from the source to the target language. Such approaches have been applied to many tasks under the cross-lingual setting, such as POS tagging (Yarowsky et al., 2001; Das and Petrov, 2011; T¨ackstr¨om et al., 2013; Fang and Cohn, 2016), mention detection (Zitouni and Florian, 2008) and parsing (Hwa et al., 2005; McDonald et al., 2011). Language independent transfer-based approaches build models using language independent and delexicalized features. For instance, 6 https://github.com/facebookresearch/ fastText/blob/master/docs/crawl-vectors. md 376 Zirikly and Hagiwara (2015) transfers word cluster and gazetteer features through the use of comparable copora. Tsai et al. (2016) links words to Wikipedia entries and uses the entry category as features to train language independent NER models. Recently, Ni et al. (2017) propose to project word embeddings into a common space as language independent features. These approaches uti"
D18-1034,E14-1049,0,0.117455,"Missing"
D18-1034,P15-1119,0,0.192844,"ng the need for resources such as Wikipedia and parallel corpora. Next, we transfer English using the same approach into Uyghur, a truly low-resource language. With significantly fewer cross-lingual resources, our approach can still perform competitively with previous best results. 2014; Artetxe et al., 2016; Smith et al., 2017), or even in an entirely unsupervised manner using adversarial training or identical character strings (Zhang et al., 2017; Artetxe et al., 2017; Lample et al., 2018). Many approaches in the past have leveraged the shared embedding space for cross-lingual applications (Guo et al., 2015; Ammar et al., 2016b; Zhang et al., 2016; Fang and Cohn, 2017), including NER (Bharadwaj et al., 2016; Ni et al., 2017). The minimal dependency on parallel resources makes the embedding-based method much more suitable for low-resource languages. However, since different languages have different linguistic properties, it is hard, if not impossible, to align the two embedding spaces perfectly (see Figure 1). Meanwhile, because surface forms are not available, character-level features cannot be used, resulting in reduced tagging accuracy (as demonstrated in our experiments). 2 We establish our p"
D18-1034,P17-1135,0,0.671727,"Missing"
D18-1034,P14-1006,0,0.028839,"m our unsupervised approach that requires no labeled resources in the target language. under a cross-lingual setting, with lower resource requirements than past approaches. We also evaluate the challenges of applying these methods to an extremely low-resource language, Uyghur. Bilingual Word Embeddings There have been two general paradigms in obtaining bilingual word vectors besides using dictionaries: through parallel corpora and through joint training. Approaches based on parallel corpora usually learn bilingual word embeddings that can produce similar representations for aligned sentences (Hermann and Blunsom, 2014; Chandar et al., 2014). Jointlytrained models combine the common monolingual training objective with a cross-lingual training objective that often comes from parallel corpus (Zou et al., 2013; Gouws et al., 2015). Recently, unsupervised approaches also have been used to align two sets of word embeddings by learning a mapping through adversarial learning or selflearning (Zhang et al., 2017; Artetxe et al., 2017; Lample et al., 2018). References 6 Acknowledgments We thank Stephen Mayhew for sharing the data, and Zihang Dai for meaningful discussion. This research was sponsored by Defense Advanc"
D18-1034,D14-1162,0,0.0789903,"efine the alignment in this shared space further, we iteratively perform a self-learning refinement step k 2 times by: 4. Train an NER model using the translated words along with the named entity tags from the English corpus (§2.2.4). We consider each in detail. 2.2.1 Learning Monolingual Embeddings Given text in the source and target language, we first independently learn word embedding matrices X and Y in the source and target languages respectively. These embeddings can be learned on monolingual text in both languages with any of the myriad of word embedding methods (Mikolov et al., 2013b; Pennington et al., 2014; Bojanowski et al., 2017). 2.2.2 Learning Bilingual Embeddings Next, we learn a cross-lingual projection of X and Y into a shared space. Assume we are given a dictionary {xi , yi }D i=1 , where xi and yi denote the embeddings of a word pair. Let XD = [x1 , x2 , · · · , xD ]> and YD = [y1 , y2 , · · · , yD ]> denote two embedding matrices consisting of word pairs from the dictionary. Following previous work (Zhang et al., 2016; Artetxe et al., 2016; Smith et al., 2017), we optimize the following objective: min W d X 2 kW xi − yi k s.t. W W > 1. Using the aligned embeddings to generate a new di"
D18-1034,P17-1161,0,0.02521,"languages under a cross-lingual setting, with much lower resource requirements than past approaches. We also evaluate the challenges of applying these methods to Uyghur, a lowresource language.1 1 Introduction Named entity recognition (NER), the task of detecting and classifying named entities from text into a few predefined categories such as people, locations or organizations, has seen the state-of-theart greatly advanced by the introduction of neural architectures (Collobert et al., 2011; Huang et al., 2015; Chiu and Nichols, 2016; Lample et al., 2016; Yang et al., 2016; Ma and Hovy, 2016; Peters et al., 2017; Liu et al., 2018; Peters et al., 2018). However, the success of these methods is highly dependent on a reasonably large amount of annotated training data, and thus it remains a challenge to apply these models to languages with limited amounts of labeled data. Cross-lingual NER attempts to address this challenge by transferring 1 The source code is available at https://github. com/thespectrewithin/cross-lingual_NER 369 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 369–379 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for C"
D18-1034,N18-1202,0,0.0600342,", with much lower resource requirements than past approaches. We also evaluate the challenges of applying these methods to Uyghur, a lowresource language.1 1 Introduction Named entity recognition (NER), the task of detecting and classifying named entities from text into a few predefined categories such as people, locations or organizations, has seen the state-of-theart greatly advanced by the introduction of neural architectures (Collobert et al., 2011; Huang et al., 2015; Chiu and Nichols, 2016; Lample et al., 2016; Yang et al., 2016; Ma and Hovy, 2016; Peters et al., 2017; Liu et al., 2018; Peters et al., 2018). However, the success of these methods is highly dependent on a reasonably large amount of annotated training data, and thus it remains a challenge to apply these models to languages with limited amounts of labeled data. Cross-lingual NER attempts to address this challenge by transferring 1 The source code is available at https://github. com/thespectrewithin/cross-lingual_NER 369 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 369–379 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics art cross-lingu"
D18-1034,N16-1156,0,0.243386,"edia and parallel corpora. Next, we transfer English using the same approach into Uyghur, a truly low-resource language. With significantly fewer cross-lingual resources, our approach can still perform competitively with previous best results. 2014; Artetxe et al., 2016; Smith et al., 2017), or even in an entirely unsupervised manner using adversarial training or identical character strings (Zhang et al., 2017; Artetxe et al., 2017; Lample et al., 2018). Many approaches in the past have leveraged the shared embedding space for cross-lingual applications (Guo et al., 2015; Ammar et al., 2016b; Zhang et al., 2016; Fang and Cohn, 2017), including NER (Bharadwaj et al., 2016; Ni et al., 2017). The minimal dependency on parallel resources makes the embedding-based method much more suitable for low-resource languages. However, since different languages have different linguistic properties, it is hard, if not impossible, to align the two embedding spaces perfectly (see Figure 1). Meanwhile, because surface forms are not available, character-level features cannot be used, resulting in reduced tagging accuracy (as demonstrated in our experiments). 2 We establish our problem setting (§2.1), then present our m"
D18-1034,D17-1035,0,0.019569,"tput of attention layer is defined as: Experimental Settings We evaluate our proposed methods on the benchmark CoNLL 2002 and 2003 NER datasets (Tjong Kim Sang, 2002; Tjong Kim Sang and De Meulder, 2003), which contain 4 European languages, English, German, Dutch and Spanish. For all experiments, we use English as the source language and translate its training data into the target language. We train a model on the translated data, and test it on the target language. For each experiment, we run our models 5 times using different seeds and report the mean and standard deviation, as suggested by Reimers and Gurevych (2017). Word Embeddings For all languages, we use two different embedding methods, fastText (Bojanowski et al., 2017) and GloVe (Pennington et al., 2014), to perform word-embedding based translations and train the NER model, respectively. For fastText, we use the publicly available embeddings trained on Wikipedia for all languages. For GloVe, we use the publicly available embeddings pre-trained on Gigaword and Wikipedia for English. For Spanish, German and Dutch, we use Spanish Gigaword and Wikipedia, German WMT News Crawl data and Wikipedia, and Dutch H a = softmax(QK > ) (E − I)H = [ha1 , ha2 , .."
D18-1034,P15-2064,0,0.261305,"y using parallel corpora to project annotations from the source to the target language. Such approaches have been applied to many tasks under the cross-lingual setting, such as POS tagging (Yarowsky et al., 2001; Das and Petrov, 2011; T¨ackstr¨om et al., 2013; Fang and Cohn, 2016), mention detection (Zitouni and Florian, 2008) and parsing (Hwa et al., 2005; McDonald et al., 2011). Language independent transfer-based approaches build models using language independent and delexicalized features. For instance, 6 https://github.com/facebookresearch/ fastText/blob/master/docs/crawl-vectors. md 376 Zirikly and Hagiwara (2015) transfers word cluster and gazetteer features through the use of comparable copora. Tsai et al. (2016) links words to Wikipedia entries and uses the entry category as features to train language independent NER models. Recently, Ni et al. (2017) propose to project word embeddings into a common space as language independent features. These approaches utilize such features by training a model on the source language and directly applying it to the target language. Another way of performing language independent transfer resorts to multi-task learning, where a model is trained jointly across differ"
D18-1034,D08-1063,0,0.0600569,"We suspect that the unreliable translation of named entities is the ma5 Related Work Cross-Lingual Learning Cross-lingual learning approaches can be loosely classified into two categories: annotation projection and languageindependent transfer. Annotation projection methods create training data by using parallel corpora to project annotations from the source to the target language. Such approaches have been applied to many tasks under the cross-lingual setting, such as POS tagging (Yarowsky et al., 2001; Das and Petrov, 2011; T¨ackstr¨om et al., 2013; Fang and Cohn, 2016), mention detection (Zitouni and Florian, 2008) and parsing (Hwa et al., 2005; McDonald et al., 2011). Language independent transfer-based approaches build models using language independent and delexicalized features. For instance, 6 https://github.com/facebookresearch/ fastText/blob/master/docs/crawl-vectors. md 376 Zirikly and Hagiwara (2015) transfers word cluster and gazetteer features through the use of comparable copora. Tsai et al. (2016) links words to Wikipedia entries and uses the entry category as features to train language independent NER models. Recently, Ni et al. (2017) propose to project word embeddings into a common space"
D18-1034,D13-1141,0,0.0502521,"ges of applying these methods to an extremely low-resource language, Uyghur. Bilingual Word Embeddings There have been two general paradigms in obtaining bilingual word vectors besides using dictionaries: through parallel corpora and through joint training. Approaches based on parallel corpora usually learn bilingual word embeddings that can produce similar representations for aligned sentences (Hermann and Blunsom, 2014; Chandar et al., 2014). Jointlytrained models combine the common monolingual training objective with a cross-lingual training objective that often comes from parallel corpus (Zou et al., 2013; Gouws et al., 2015). Recently, unsupervised approaches also have been used to align two sets of word embeddings by learning a mapping through adversarial learning or selflearning (Zhang et al., 2017; Artetxe et al., 2017; Lample et al., 2018). References 6 Acknowledgments We thank Stephen Mayhew for sharing the data, and Zihang Dai for meaningful discussion. This research was sponsored by Defense Advanced Research Projects Agency Information Innovation Office (I2O) under the Low Resource Languages for Emergent Incidents (LORELEI) program, issued by DARPA/I2O under Contract No. HR0011-15-C011"
D18-1034,Q13-1001,0,0.337503,"Missing"
D18-1034,N12-1052,0,0.317254,"Missing"
D18-1034,W02-2024,0,0.222579,"rs similar to those seen at training time, which we posit introduces a level of flexibility with respect to the word order, and thus may allow for better generalization. Let H = [h1 , h2 , · · · , hn ]> be a sequence of word-level hidden representations. We apply a single layer MLP on H to obtain the queries Q and keys K = tanh(HW + b), where W ∈ Rd×d is a parameter matrix and b ∈ Rd is a bias term, with d being the hidden state size. The output of attention layer is defined as: Experimental Settings We evaluate our proposed methods on the benchmark CoNLL 2002 and 2003 NER datasets (Tjong Kim Sang, 2002; Tjong Kim Sang and De Meulder, 2003), which contain 4 European languages, English, German, Dutch and Spanish. For all experiments, we use English as the source language and translate its training data into the target language. We train a model on the translated data, and test it on the target language. For each experiment, we run our models 5 times using different seeds and report the mean and standard deviation, as suggested by Reimers and Gurevych (2017). Word Embeddings For all languages, we use two different embedding methods, fastText (Bojanowski et al., 2017) and GloVe (Pennington et a"
D18-1034,W03-0419,0,0.350689,"Missing"
D18-1034,K16-1022,0,0.679344,"umber with 0 when used as input to the character level Bi-LSTM. Network Training We use SGD with momentum to train the NER model for 30 epochs, and select the best model on the target language development set. We choose the initial learning rate 4.2.1 Comparison with Dictionary-Based Translation Table 1 also presents results of a comparison between our proposed BWE translation method and the “cheap translation” baseline of (Mayhew et al., 2017). The size of the dictionaries used by both 4 https://github.com/facebookresearch/ MUSE 374 Model ∗ T¨ackstr¨om et al. (2012) ∗ Nothman et al. (2013) ∗ Tsai et al. (2016) ∗ Ni et al. (2017) ∗† Mayhew et al. (2017) ∗ Mayhew et al. (2017) (only Eng. data) Our methods: BWET (id.c.) BWET (id.c.) + self-att. BWET (adv.) BWET (adv.) + self-att. BWET BWET + self-att. ∗ BWET on data from Mayhew et al. (2017) ∗ BWET + self-att. on data from Mayhew et al. (2017) ∗ Our supervised results Spanish 59.30 61.0 60.55 65.10 65.95 51.82 Dutch 58.40 64.00 61.60 65.40 66.50 53.94 German 40.40 55.80 48.10 58.50 59.11 50.96 Extra Resources parallel corpus Wikipedia Wikipedia Wikipedia, parallel corpus, 5K dict. Wikipedia, 1M dict. 1M dict. 71.14 ± 0.60 72.37 ± 0.65 70.54 ± 0.85 71."
D18-1034,Q14-1005,0,0.030546,"t entity labels to a low-resource target language with few or no labels. Specifically, in this paper we attempt to tackle the extreme scenario of unsupervised transfer, where no labeled data is available in the target language. Within this paradigm, there are two major challenges to tackle: how to effectively perform lexical mapping between the languages, and how to address word order differences. To cope with the first challenge of lexical mapping, a number of methods use parallel corpora to project annotations between languages through word alignment (Ehrmann et al., 2011; Kim et al., 2012; Wang and Manning, 2014; Ni et al., 2017). Since parallel corpora may not be always available, Mayhew et al. (2017) proposed a “cheap translation” approach that uses a bilingual dictionary to perform word-level translation. The above approaches provide a reasonable proxy for the actual labeled training data, largely because the words that participate in entities can be translated relatively reliably given extensive parallel dictionaries or corpora (e.g., with 1 million word pairs or sentences). Additionally, as a side benefit of having explicitly translated words, models can directly exploit features extracted from"
D18-1034,H01-1035,0,0.339379,"esource languages when some named entities are not covered by the dictionary or the translation is not reliable. We suspect that the unreliable translation of named entities is the ma5 Related Work Cross-Lingual Learning Cross-lingual learning approaches can be loosely classified into two categories: annotation projection and languageindependent transfer. Annotation projection methods create training data by using parallel corpora to project annotations from the source to the target language. Such approaches have been applied to many tasks under the cross-lingual setting, such as POS tagging (Yarowsky et al., 2001; Das and Petrov, 2011; T¨ackstr¨om et al., 2013; Fang and Cohn, 2016), mention detection (Zitouni and Florian, 2008) and parsing (Hwa et al., 2005; McDonald et al., 2011). Language independent transfer-based approaches build models using language independent and delexicalized features. For instance, 6 https://github.com/facebookresearch/ fastText/blob/master/docs/crawl-vectors. md 376 Zirikly and Hagiwara (2015) transfers word cluster and gazetteer features through the use of comparable copora. Tsai et al. (2016) links words to Wikipedia entries and uses the entry category as features to trai"
D18-1034,P17-1179,0,0.166689,"ociation for Computational Linguistics art cross-lingual results in Spanish and Dutch, and competitive results in German, even without a dictionary, completely removing the need for resources such as Wikipedia and parallel corpora. Next, we transfer English using the same approach into Uyghur, a truly low-resource language. With significantly fewer cross-lingual resources, our approach can still perform competitively with previous best results. 2014; Artetxe et al., 2016; Smith et al., 2017), or even in an entirely unsupervised manner using adversarial training or identical character strings (Zhang et al., 2017; Artetxe et al., 2017; Lample et al., 2018). Many approaches in the past have leveraged the shared embedding space for cross-lingual applications (Guo et al., 2015; Ammar et al., 2016b; Zhang et al., 2016; Fang and Cohn, 2017), including NER (Bharadwaj et al., 2016; Ni et al., 2017). The minimal dependency on parallel resources makes the embedding-based method much more suitable for low-resource languages. However, since different languages have different linguistic properties, it is hard, if not impossible, to align the two embedding spaces perfectly (see Figure 1). Meanwhile, because surfac"
D18-1152,N18-1205,0,0.0924699,"997; Cho et al., 2014), extensive efforts have been devoted to developing alternatives (Balduzzi and Ghifary, 2016; Miao et al., 2016; Zoph and Le, 2017; Lee et al., 2017; Lei et al., 2017a; Vaswani et al., 2017; Gehring et al., 2017, inter alia). Departing from the above approaches, this work derives RNN architectures drawing inspiration from WFSAs. Another line of work studied the connections between WFSAs and RNNs in terms of modeling capacity, both empirically (Kolen, 1993; Giles et al., 1992; Weiss et al., 2018, inter alia) and theoretically (Cleeremans et al., 1989; Visser et al., 2001; Chen et al., 2018, inter alia). 8 Conclusion We presented rational recurrences, a new construction to study the recurrent updates in RNNs, drawing inspiration from WFSAs. We showed that rational recurrences are in frequent use by several recently proposed recurrent neural architectures, providing new understanding of them. Based on such connections, we discussed approaches to deriving novel neural architectures from WFSAs. Our empirical results demonstrate the potential of doing so. We publicly release our implementation at https://github.com/ Noahs-ARK/rational-recurrences. Acknowledgments We thank Jason Eisn"
D18-1152,Q15-1031,0,0.0161797,"(F) is especially useful in small data setups. As in the language modeling experiments, RRNN(B)m+ underperforms all other models in most cases, and in particular RRNN(B). These results provide evidence that replacing the real semiring in rational models might be challenging. We leave further exploration to future work. 7 Related Work Weighted finite state automata. WFSAs were once popular among many sequential tasks (Mohri et al., 2002; Kumar and Byrne, 2003; Cortes et al., 2004; Pardo and Birmingham, 2005; Moore et al., 2006, inter alia), and are still successful in morphology (Dreyer, 2011; Cotterell et al., 2015; Rastogi et al., 2016, inter alia). Compared to neural networks, WFSAs are better understood theoreti10 http://www.cs.uic.edu/?liub/FBS/ sentiment-analysis.html cally and arguably more interpretable. They were recently revisited in combination with the former in, e.g., text generation (Ghazvininejad et al., 2016, 2017; Lin et al., 2017) and automatic music accompaniment (Forsyth, 2016). Recurrent neural networks. RNNs (Elman, 1990; Jordan, 1989) prove to be strong models for sequential data (Siegelmann and Sontag, 1995). Besides the perhaps most notable gated variants (Hochreiter and Schmidhu"
D18-1152,C10-2028,0,0.025046,"Missing"
D18-1152,N16-1024,1,0.394928,"d with final weights. Arrows represent transitions, labeled by the symbols ↵ they consume, and the weights as a function of ↵. Arcs not drawn are assumed to have weight ¯0. For brevity, 8↵ means 8↵ 2 ⌃, with ⌃ being the alphabet. Introduction Neural models, and in particular gated variants of recurrent neural networks (RNNs, e.g., Hochreiter and Schmidhuber, 1997; Cho et al., 2014), have become a core building block for stateof-the-art approaches in NLP (Goldberg, 2016). While these models empirically outperform classical NLP methods on many tasks (Zaremba et al., 2014; Bahdanau et al., 2015; Dyer et al., 2016; Peng et al., 2017, inter alia), they typically lack the intuition offered by classical models, making it hard to understand the roles played by each of their components. In this work we show that many neural models are more interpretable than previously thought, by drawing connections to weighted finite state automata (WFSAs). We study several recently proposed RNN architectures and show that one can use WFSAs to characterize their recurrent updates. We call such models rational recurrences (§3).1 Analyzing recurrences in terms of WFSAs provides a new view of existing models and facilitates"
D18-1152,P02-1001,0,0.0885187,"inearity: ut = Wu vt . By Proposition 7 and Corollary 9: Corollary 10. The recurrences of single-layer SRU, T-RNN, and SCRN architectures are rational. It is slightly more complicated to analyze the recurrences of the QRNN, T-LSTM, and T-GRU. Although their hidden states ct are updated in the same way as Equation 5c, the input representations and gates may depend on previous inputs. For example, in T-LSTM and T-GRU, the forget gate is a function of two consecutive inputs: ft = (Vf vt 1 + Wf vt + bf ) . (10) QRNNs are similar, but may depend on up to K tokens, due to the K-window convolutions. Eisner (2002) discuss finite state machines for second (or higher) order probabilistic sequence models. Following the same intuition, we sketch the construction of WFSAs corresponding to QRNNs with 2-window convolutions in Appendix A, and summarize the key results here: Proposition 11. The recurrences of single-layer T-GRU, T-LSTM, and QRNN are rational. In particular, a single-layer d-dimensional QRNN using K-window convolutions can be recovered by a set of d WFSAs, each with O(2 |⌃|K 1 ) states. The size of WFSAs needed to recover QRNN grows exponentially in the window size. Therefore, at least for QRNNs"
D18-1152,D16-1126,0,0.00432377,"ture work. 7 Related Work Weighted finite state automata. WFSAs were once popular among many sequential tasks (Mohri et al., 2002; Kumar and Byrne, 2003; Cortes et al., 2004; Pardo and Birmingham, 2005; Moore et al., 2006, inter alia), and are still successful in morphology (Dreyer, 2011; Cotterell et al., 2015; Rastogi et al., 2016, inter alia). Compared to neural networks, WFSAs are better understood theoreti10 http://www.cs.uic.edu/?liub/FBS/ sentiment-analysis.html cally and arguably more interpretable. They were recently revisited in combination with the former in, e.g., text generation (Ghazvininejad et al., 2016, 2017; Lin et al., 2017) and automatic music accompaniment (Forsyth, 2016). Recurrent neural networks. RNNs (Elman, 1990; Jordan, 1989) prove to be strong models for sequential data (Siegelmann and Sontag, 1995). Besides the perhaps most notable gated variants (Hochreiter and Schmidhuber, 1997; Cho et al., 2014), extensive efforts have been devoted to developing alternatives (Balduzzi and Ghifary, 2016; Miao et al., 2016; Zoph and Le, 2017; Lee et al., 2017; Lei et al., 2017a; Vaswani et al., 2017; Gehring et al., 2017, inter alia). Departing from the above approaches, this work derives RNN a"
D18-1152,P17-4008,0,0.00774605,"Missing"
D18-1152,N03-1019,0,0.041538,"ificant differences between RRNN (F) and RRNN (C), while both outperform others. This may suggest that the interpolation of unigram and bigram features by RRNN(F) is especially useful in small data setups. As in the language modeling experiments, RRNN(B)m+ underperforms all other models in most cases, and in particular RRNN(B). These results provide evidence that replacing the real semiring in rational models might be challenging. We leave further exploration to future work. 7 Related Work Weighted finite state automata. WFSAs were once popular among many sequential tasks (Mohri et al., 2002; Kumar and Byrne, 2003; Cortes et al., 2004; Pardo and Birmingham, 2005; Moore et al., 2006, inter alia), and are still successful in morphology (Dreyer, 2011; Cotterell et al., 2015; Rastogi et al., 2016, inter alia). Compared to neural networks, WFSAs are better understood theoreti10 http://www.cs.uic.edu/?liub/FBS/ sentiment-analysis.html cally and arguably more interpretable. They were recently revisited in combination with the former in, e.g., text generation (Ghazvininejad et al., 2016, 2017; Lin et al., 2017) and automatic music accompaniment (Forsyth, 2016). Recurrent neural networks. RNNs (Elman, 1990; Jor"
D18-1152,D15-1180,0,0.0163074,"om a string kernel perspective (Lei et al., 2017a). Here we review its nonlinear bigram version: (1) 8↵/⌘1 (↵) 8↵/µ2,2 (↵) (11) Due to the self-loop over state q1 , t can be seen as a weighted sum of the µ1 terms up to xt (Equaltion 13). The second product term in Equation 11 then provides multiplicative interactions between µ2 , and the weighted sum of µ1 s. In this sense, it captures nonconsecutive bigram features. ct 8↵/µ1,1 (↵) (14b) where the ut s are computed similarly to Equa(2) tion 5b, and ct is used as output for onward computation. Different strategies to computing t were explored (Lei et al., 2015, 2016). When t is a constant, or depends only on xt , e.g., t = (W vt +b ), the ith dimension of Equations 14 Beyond Elementwise Operations So far we have discussed rational recurrences for models using elementwise recurrent updates (e.g., Equation 5c). This section uses an existing model as an example, to study a rational recurrence that is not elementwise. We focus on the input switched affine network (ISAN; Foerster et al., 2017). Aiming for efficiency and interpretability, it does not use any explicit nonlinearity; its affine transformation parameters depend only on the input: ct = W x t"
D18-1152,N16-1153,0,0.0171468,"Missing"
D18-1152,P04-1035,0,0.00902813,"Missing"
D18-1152,P17-1186,1,0.82596,"s. Arrows represent transitions, labeled by the symbols ↵ they consume, and the weights as a function of ↵. Arcs not drawn are assumed to have weight ¯0. For brevity, 8↵ means 8↵ 2 ⌃, with ⌃ being the alphabet. Introduction Neural models, and in particular gated variants of recurrent neural networks (RNNs, e.g., Hochreiter and Schmidhuber, 1997; Cho et al., 2014), have become a core building block for stateof-the-art approaches in NLP (Goldberg, 2016). While these models empirically outperform classical NLP methods on many tasks (Zaremba et al., 2014; Bahdanau et al., 2015; Dyer et al., 2016; Peng et al., 2017, inter alia), they typically lack the intuition offered by classical models, making it hard to understand the roles played by each of their components. In this work we show that many neural models are more interpretable than previously thought, by drawing connections to weighted finite state automata (WFSAs). We study several recently proposed RNN architectures and show that one can use WFSAs to characterize their recurrent updates. We call such models rational recurrences (§3).1 Analyzing recurrences in terms of WFSAs provides a new view of existing models and facilitates the development of"
D18-1152,D18-1477,0,0.0319395,"SRU; Lei et al., 2017b) aim to speed up the recurrent computation. To do so, they drop the matrix multiplication dependence on the previous hidden state, resulting in similar recurrences to that in Example 6.5 Other works start from different motivations, but land on similar recurrences, e.g., strongly-typed RNNs (TRNN; Balduzzi and Ghifary, 2016) and its gated 4 We restrict that both operations take constant time and space, to exclude the use of arbitrarily complex semirings (§4.3). 5 The SRU architecture discussed through this work is based on Lei et al. (2017b). In a later updated version, Lei et al. (2018) introduce diagonal matrix multiplication interaction in the hidden state updates, inspired by (Li et al., 2018), which yields a recurrence not obviously rational. variants (T-LSTM and T-GRU), and structurally constrained RNNs (SCRN; Mikolov et al., 2014). The analysis in §3.1 directly applies to SRU, T-RNN, and SCRN. In fact, Example 6 presents a slightly more complicated version of them. In these models, input representations are computed without the bias term or any nonlinearity: ut = Wu vt . By Proposition 7 and Corollary 9: Corollary 10. The recurrences of single-layer SRU, T-RNN, and SCR"
D18-1152,P18-1173,1,0.895246,"Missing"
D18-1152,D14-1162,0,0.117975,"Missing"
D18-1152,J93-2004,0,0.0611805,"ng (§5.1). We also compare to an LSTM baseline. Aiming to control for comfounding factors, we do not use highway connections in any of the models.6 In the interest of space, the full architectures and hyperparameters are detailed in Appendices D and E. 6.2 RRNN (C) RRNN (F) RRNN (B) RRNN (B)m+ RRNN (C) RRNN (F) Table 3: Language modeling perplexity on PTB test set (lower is better). LSTM numbers are taken from Lei et al. (2017b). ` denotes the number of layers. Bold font indicates best performance. Language Modeling Dataset and implementation. We experiment with the Penn Treebank corpus (PTB; Marcus et al., 1993). We use the preprocessing and splits from Mikolov et al. (2010), resulting in a vocabulary size of 10K and 1M tokens. Following standard practice, we treat the training data as one long sequence, split into mini batches, and train using BPTT truncated to 35 time steps (Williams and Peng, 1990). The input embeddings and output softmax weights are tied (Press and Wolf, 2017). Results. Following Collins et al. (2017) and Melis et al. (2018), we compare models controlling for parameter budget. Table 3 summarizes language modeling perplexities on PTB test set. The middle block compares all models"
D18-1152,E17-2025,0,0.00583644,"s better). LSTM numbers are taken from Lei et al. (2017b). ` denotes the number of layers. Bold font indicates best performance. Language Modeling Dataset and implementation. We experiment with the Penn Treebank corpus (PTB; Marcus et al., 1993). We use the preprocessing and splits from Mikolov et al. (2010), resulting in a vocabulary size of 10K and 1M tokens. Following standard practice, we treat the training data as one long sequence, split into mini batches, and train using BPTT truncated to 35 time steps (Williams and Peng, 1990). The input embeddings and output softmax weights are tied (Press and Wolf, 2017). Results. Following Collins et al. (2017) and Melis et al. (2018), we compare models controlling for parameter budget. Table 3 summarizes language modeling perplexities on PTB test set. The middle block compares all models with two layers and 10M trainable parameters. RRNN (B) and RRNN (C) achieve roughly the same performance; interpolating both unigram and bigram features, RRNN(F) outperforms others by more than 2.9 test perplexity. For the three-layer and 24M setting (the bottom block), we observe similar trends, except that RRNN(C) slightly underperforms RRNN(B). Here RRNN(F) outperforms o"
D18-1152,N16-1076,0,0.0132168,"in small data setups. As in the language modeling experiments, RRNN(B)m+ underperforms all other models in most cases, and in particular RRNN(B). These results provide evidence that replacing the real semiring in rational models might be challenging. We leave further exploration to future work. 7 Related Work Weighted finite state automata. WFSAs were once popular among many sequential tasks (Mohri et al., 2002; Kumar and Byrne, 2003; Cortes et al., 2004; Pardo and Birmingham, 2005; Moore et al., 2006, inter alia), and are still successful in morphology (Dreyer, 2011; Cotterell et al., 2015; Rastogi et al., 2016, inter alia). Compared to neural networks, WFSAs are better understood theoreti10 http://www.cs.uic.edu/?liub/FBS/ sentiment-analysis.html cally and arguably more interpretable. They were recently revisited in combination with the former in, e.g., text generation (Ghazvininejad et al., 2016, 2017; Lin et al., 2017) and automatic music accompaniment (Forsyth, 2016). Recurrent neural networks. RNNs (Elman, 1990; Jordan, 1989) prove to be strong models for sequential data (Siegelmann and Sontag, 1995). Besides the perhaps most notable gated variants (Hochreiter and Schmidhuber, 1997; Cho et al.,"
D18-1152,P18-1028,1,0.48448,"lly lack the intuition offered by classical models, making it hard to understand the roles played by each of their components. In this work we show that many neural models are more interpretable than previously thought, by drawing connections to weighted finite state automata (WFSAs). We study several recently proposed RNN architectures and show that one can use WFSAs to characterize their recurrent updates. We call such models rational recurrences (§3).1 Analyzing recurrences in terms of WFSAs provides a new view of existing models and facilitates the development of new ones. In recent work, Schwartz et al. (2018) introduced SoPa, an RNN constructed from WFSAs, and thus rational by our definition. They also showed that a single-layer max-pooled CNN (LeCun, 1998) can be simulated by a set of simple WFSAs (one per output dimension), and accordingly are also rational. In this paper we broaden such efforts, and show that rational recurrences are in frequent use (Mikolov et al., 2014; Balduzzi and Ghifary, 2016; Lei et al., 2016, 2017a,b; Bradbury et al., 2017; Foerster et al., 2017). For instance, we will show in §4 that the WFSA diagrammed 1 Where the term regular is used with unweighted FSAs (e.g., regul"
D18-1152,D13-1170,0,0.0014574,"t compared models outperform the LSTM baselines, whose numbers are taken from Lei et al. (2017b).7 6.3 Text Classification Implementation. We use unidirectional 2-layer architectures for all compared models. To build the classifiers, we feed the final RNN hidden states into a 2-layer tanh-MLP. Further implementation details are described in Appendix E. Datasets. We experiment with four binary text classification datasets, described below. • Amazon (electronic product review corpus; McAuley and Leskovec, 2013).8 We focus on the positive and negative reviews. • SST (Stanford sentiment treebank; Socher et al., 2013).9 We focus on the binary classification task. SST provides labels for syntactic phrases; we experiment with a more realistic setup, and 7 Melis et al. (2018) point out that carefully tuning LSTMs can achieve much stronger performance, at the cost of exceptionally large amounts of computational resources for tuning. 8 http://riejohnson.com/cnn_data.html 9 nlp.stanford.edu/sentiment/index.html 1210 Model Amazon LSTM 91.2±0.3 85.1±0.6 93.3±0.6 82.4±1.5 RRNN (B) RRNN (C) RRNN (B)m+ RRNN (F) SST subj CR 92.4±0.1 85.8±0.3 93.9±0.4 84.1±1.0 92.8±0.2 84.8±0.4 93.8±0.6 84.5±0.9 89.2±3.1 84.9±0.4 92.6±"
D18-1152,P18-2117,0,0.0463808,"and Sontag, 1995). Besides the perhaps most notable gated variants (Hochreiter and Schmidhuber, 1997; Cho et al., 2014), extensive efforts have been devoted to developing alternatives (Balduzzi and Ghifary, 2016; Miao et al., 2016; Zoph and Le, 2017; Lee et al., 2017; Lei et al., 2017a; Vaswani et al., 2017; Gehring et al., 2017, inter alia). Departing from the above approaches, this work derives RNN architectures drawing inspiration from WFSAs. Another line of work studied the connections between WFSAs and RNNs in terms of modeling capacity, both empirically (Kolen, 1993; Giles et al., 1992; Weiss et al., 2018, inter alia) and theoretically (Cleeremans et al., 1989; Visser et al., 2001; Chen et al., 2018, inter alia). 8 Conclusion We presented rational recurrences, a new construction to study the recurrent updates in RNNs, drawing inspiration from WFSAs. We showed that rational recurrences are in frequent use by several recently proposed recurrent neural architectures, providing new understanding of them. Based on such connections, we discussed approaches to deriving novel neural architectures from WFSAs. Our empirical results demonstrate the potential of doing so. We publicly release our implement"
D18-1412,S07-1018,0,0.0955337,"meNet 1.5 for frame SRL and on the test set of OntoNotes for both PropBank SRL and coreference. For the syntactic scaffold in each case, we use syntactic annotations from OntoNotes 5.0 (Weischedel et al., 2013; Pradhan et al., 2013).4 Further details on experimental settings and datasets have been elaborated in the supplemental material. Frame SRL. Table 1 shows the performance of all the scaffold models on frame SRL with respect to prior work and a semi-CRF baseline (§5.1) without a syntactic scaffold. We follow the official evaluation from the SemEval shared task for frame-semantic parsing (Baker et al., 2007). Prior work for frame SRL has relied on predicted syntactic trees, in two different ways: by using syntax-based rules to prune out spans of text that are unlikely to contain any frame’s argument; and by using syntactic features in their statistical model (Das et al., 2014; T¨ackstr¨om et al., 2015; FitzGerald et al., 2015; Kshirsagar et al., 2015). The best published results on FrameNet 1.5 are due to Yang and Mitchell (2017). In their sequential model (seq), they treat argument identification as a sequence-labeling problem using a deep bidirectional LSTM with a CRF layer. In their relational"
D18-1412,P98-1013,0,0.599303,"ssification among (i) noun phrase (or prepositional phrase, for frame SRL); (ii) any other category; and (iii) null. In Figure 1, for the span “encouraging them”, the constituent identity scaffold label is 1, the nonterminal label is S|VP, the non-terminal and parent label is S|VP+par=PP, and the common nonterminals label is set to OTHER. 3 Semantic Role Labeling We contribute a new SRL model which contributes a strong baseline for experiments with syntactic scaffolds. The performance of this baseline itself is competitive with state-of-the-art methods (§7). FrameNet. In the FrameNet lexicon (Baker et al., 1998), a frame represents a type of event, situation, or relationship, and is associated with a set of semantic roles, called frame elements. A frame can be evoked by a word or phrase in a sentence, called a target. Each frame element of an evoked frame can then be realized in the sentence as a sentential span, called an argument (or it can be unrealized). Arguments for a given frame do not overlap. (2) where wc is a parameter vector associated with category c. We sum the log loss terms for all the spans in a sentence to give its loss: X L2 (x, z) = − log p(zi: j |xi: j ). (3) 4.2 5 In the OntoNote"
D18-1412,P17-1110,0,0.015502,"es stand-alone methods for estimating word embeddings (Mikolov et al., 2013; Pennington et al., 2014; Peters et al., 2018). After training word embeddings, the tasks implied by models like the skip-gram or ELMo’s language model become irrelevant to the downstream use of the embeddings. A noteworthy difference is that, rather than pre-training, a scaffold is integrated directly into the training of T 1 through a multitask objective. Multitask learning. Neural architectures have often yielded performance gains when trained for multiple tasks together (Collobert et al., 2011; Luong et al., 2015; Chen et al., 2017; Hashimoto et al., 2017). In particular, performance of semantic role labeling tasks improves when done jointly with other semantic tasks (FitzGerald et al., 2015; Peng et al., 2017, 2018). Contemporaneously with this work, Hershcovich et al. (2018) proposed a multitask learning setting for universal syntactic dependencies and UCCA semantics (Abend and Rappoport, 2013). Syntactic scaffolds focus on a primary semantic task, treating syntax as an auxillary, eventually forgettable prediction task. 4 We assume two sources of supervision: a corpus D1 with instances x annotated for the primary task"
D18-1412,D16-1245,0,0.0211932,"(or, by extension, a larger vocabulary), though that might be a factor for frame SRL. A syntactic scaffold can match the performance of a pipeline containing carefully extracted syntactic features for semantic prediction (Swayamdipta et al., 2017). This, along with other recent approaches (He et al., 2017, 2018b) show that syntax remains useful, even with strong neural models for SRL. Coreference. We report the results on four standard scores from the CoNLL evaluation: MUC, B3 and CEAFφ4 , and their average F1 in Table 3. Prior competitive coreference resolution systems (Wiseman et al., 2016; Clark and Manning, 2016b,a) all incorporate synctactic information in a pipeline, using features and rules for mention proposals from predicted syntax. Our baseline is the model from Lee et al. (2017), described in §6. Similar to the baseline model for frame SRL, and in contrast with prior work, this model does not use any syntax. We experiment with the best syntactic scaffold from the frame SRL task. We used NP, OTHER, and null as the labels for the common nonterminals scaffold here, since coreferring mentions are rarely prepositional phrases. The syntactic scaffold outperforms the baseline by 0.6 absolute F1 . Con"
D18-1412,P16-1061,0,0.0192322,"(or, by extension, a larger vocabulary), though that might be a factor for frame SRL. A syntactic scaffold can match the performance of a pipeline containing carefully extracted syntactic features for semantic prediction (Swayamdipta et al., 2017). This, along with other recent approaches (He et al., 2017, 2018b) show that syntax remains useful, even with strong neural models for SRL. Coreference. We report the results on four standard scores from the CoNLL evaluation: MUC, B3 and CEAFφ4 , and their average F1 in Table 3. Prior competitive coreference resolution systems (Wiseman et al., 2016; Clark and Manning, 2016b,a) all incorporate synctactic information in a pipeline, using features and rules for mention proposals from predicted syntax. Our baseline is the model from Lee et al. (2017), described in §6. Similar to the baseline model for frame SRL, and in contrast with prior work, this model does not use any syntax. We experiment with the best syntactic scaffold from the frame SRL task. We used NP, OTHER, and null as the labels for the common nonterminals scaffold here, since coreferring mentions are rarely prepositional phrases. The syntactic scaffold outperforms the baseline by 0.6 absolute F1 . Con"
D18-1412,P17-1044,1,0.303465,"e on all three tasks. 1 Introduction As algorithms for the semantic analysis of natural language sentences have developed, the role of syntax has been repeatedly revisited. Linguistic theories have argued for a very tight integration of syntactic and semantic processing (Steedman, 2000; Copestake and Flickinger, 2000), and many systems have used syntactic dependency or phrase-based parsers as preprocessing for semantic analysis (Gildea and Palmer, 2002; Punyakanok et al., 2008; Das et al., 2014). Meanwhile, some recent methods forgo explicit syntactic processing altogether (Zhou and Xu, 2015; He et al., 2017; Lee et al., 2017; Peng et al., 2017). Because annotated training datasets for semantics will always be limited, we expect that syntax—which offers an incomplete but potentially useful view of semantic structure—will continue to offer useful inductive bias, encouraging semantic models toward better generalization. We address the central question: is there a way for semantic analyzers to benefit from syntax without the computational cost of syntactic parsing? We propose a multitask learning approach to incorporating syntactic information into learned representations of neural semantics models"
D18-1412,P18-1192,0,0.120004,"rmance by 1 F1 , and our common nonterminal scaffold by 3.1 F1 .6 Prec. Rec. F1 Kshirsagar et al. (2015) Yang and Mitchell (2017) (Rel) Yang and Mitchell (2017) (Seq) †Yang and Mitchell (2017) (All) 66.0 71.8 63.4 70.2 60.4 57.7 66.4 60.2 63.1 64.0 64.9 65.5 Semi-CRF baseline 67.8 66.2 67.0 68.1 68.8 69.4 69.2 67.4 68.2 68.0 69.0 67.7 68.5 68.7 69.1 Model + constituent identity + nonterminal and parent + nonterminal + common nonterminals Table 1: Frame SRL results on the test set of FrameNet 1.5., using gold frames. Ensembles are denoted by †. Prec. Rec. F1 Zhou and Xu (2015) He et al. (2017) He et al. (2018a) Tan et al. (2018) 81.7 83.9 81.9 81.6 73.7 83.6 81.3 81.7 82.1 82.7 Semi-CRF baseline 84.8 81.2 83.0 85.1 82.6 83.8 Model + common nonterminals Table 2: PropBank sSRL results, using gold predicates, on CoNLL 2012 test. For fair comparison, we show only non-ensembled models. 6 This result is not reported in Table 1 since Peng et al. (2018) used a preprocessing which renders the test set slightly larger — the difference we report is calculated using their test set. 3778 B3 Prec. Rec. F1 CEAFφ4 Prec. Rec. F1 Avg. F1 Wiseman et al. (2016) 77.5 69.8 73.4 Clark and Manning (2016b) 79.9 69.3 74.2"
D18-1412,copestake-flickinger-2000-open,0,0.0377689,"nformation into semantic tasks. Syntactic scaffolds avoid expensive syntactic processing at runtime, only making use of a treebank during training, through a multitask objective. We improve over strong baselines on PropBank semantics, frame semantics, and coreference resolution, achieving competitive performance on all three tasks. 1 Introduction As algorithms for the semantic analysis of natural language sentences have developed, the role of syntax has been repeatedly revisited. Linguistic theories have argued for a very tight integration of syntactic and semantic processing (Steedman, 2000; Copestake and Flickinger, 2000), and many systems have used syntactic dependency or phrase-based parsers as preprocessing for semantic analysis (Gildea and Palmer, 2002; Punyakanok et al., 2008; Das et al., 2014). Meanwhile, some recent methods forgo explicit syntactic processing altogether (Zhou and Xu, 2015; He et al., 2017; Lee et al., 2017; Peng et al., 2017). Because annotated training datasets for semantics will always be limited, we expect that syntax—which offers an incomplete but potentially useful view of semantic structure—will continue to offer useful inductive bias, encouraging semantic models toward better gen"
D18-1412,S12-1029,1,0.796445,"sible labeled segmentations. The following zeroth-order semi-Markov dynamic program (Sarawagi et al., 2004) efficiently computes the new partition function: X αj = αi−1 exp{Ψ(s, x) + cost(s, s∗ )}, (7) s=hi, j,yi: j i j−i6D where Z = αn , under the base case α0 = 1. The prediction under the model can be calculated using a similar dynamic program with the following recurrence where γ0 = 1: γ j = max γi−1 exp Ψ(s, x). s=hi, j,yi: j i j−i6D (8) Our model formulation enforces that arguments do not overlap. We do not enforce any other SRL constraints, such as non-repetition of core frame elements (Das et al., 2012). 5.3 Input Span Representation This section describes the neural architecture used to obtain the span embedding, vi: j , corresponding to a span xi: j and the target in consideration, t = htstart , tend i. For the scaffold task, since the syntactic treebank does not contain annotations for semantic targets, we use the last verb in the sentence as a placeholder target, wherever target features are used. If there are no verbs, we use the first token in the sentence as a placeholder target. The parameters used to learn v are shared between the tasks. We construct an embedding for the span using"
D18-1412,W06-1673,0,0.0377993,"sting alternatives. Pipelines. In a typical pipeline, T 1 and T 2 are separately trained, with the output of T 2 used to define the inputs to T 1 (Wolpert, 1992). Using syntax as T 2 in a pipeline is perhaps the most 3773 common approach for semantic structure prediction (Toutanova et al., 2008; Yang and Mitchell, 2017; Wiseman et al., 2016).2 However, pipelines introduce the problem of cascading errors (T 2 ’s mistakes affect the performance, and perhaps the training, of T 1 ; He et al., 2013). To date, remedies to cascading errors are so computationally expensive as to be impractical (e.g., Finkel et al., 2006). A syntactic scaffold is quite different from a pipeline since the output of T 2 is never explicitly used. Latent variables. Another solution is to treat the output of T 2 as a (perhaps structured) latent variable. This approach obviates the need of supervision for T 2 and requires marginalization (or some approximation to it) in order to reason about the outputs of T 1 . Syntax as a latent variable for semantics was explored by Zettlemoyer and Collins (2005) and Naradowsky et al. (2012). Apart from avoiding marginalization, the syntactic scaffold offers a way to use auxiliary syntacticallyan"
D18-1412,J13-4006,0,0.0237581,"mation to it) in order to reason about the outputs of T 1 . Syntax as a latent variable for semantics was explored by Zettlemoyer and Collins (2005) and Naradowsky et al. (2012). Apart from avoiding marginalization, the syntactic scaffold offers a way to use auxiliary syntacticallyannotated data as direct supervision for T 2 , and it need not overlap the T 1 training data. Joint learning of syntax and semantics. The motivation behind joint learning of syntactic and semantic representations is that any one task is helpful in predicting the other (Llu´ıs and M`arquez, 2008; Llu´ıs et al., 2013; Henderson et al., 2013; Swayamdipta et al., 2016). This typically requires joint prediction of the outputs of T 1 and T 2 , which tends to be computationally expensive at both training and test time. Part of speech scaffolds. Similar to our work, there have been multitask models that use partof-speech tagging as T 2 , with transition-based dependency parsing (Zhang and Weiss, 2016) and CCG supertagging (Søgaard and Goldberg, 2016) as T 1 . Both of the above approaches assumed parallel input data and used both tasks as supervision. Notably, we simplify our T 2 , throwing away the structured aspects of syntactic pars"
D18-1412,P18-1035,0,0.0286481,"ant to the downstream use of the embeddings. A noteworthy difference is that, rather than pre-training, a scaffold is integrated directly into the training of T 1 through a multitask objective. Multitask learning. Neural architectures have often yielded performance gains when trained for multiple tasks together (Collobert et al., 2011; Luong et al., 2015; Chen et al., 2017; Hashimoto et al., 2017). In particular, performance of semantic role labeling tasks improves when done jointly with other semantic tasks (FitzGerald et al., 2015; Peng et al., 2017, 2018). Contemporaneously with this work, Hershcovich et al. (2018) proposed a multitask learning setting for universal syntactic dependencies and UCCA semantics (Abend and Rappoport, 2013). Syntactic scaffolds focus on a primary semantic task, treating syntax as an auxillary, eventually forgettable prediction task. 4 We assume two sources of supervision: a corpus D1 with instances x annotated for the primary task’s outputs y (semantic role labeling or coreference resolution), and a treebank D2 with sentences x, each with a phrase-structure tree z. 4.1 Loss Each task has an associated loss, and we seek to minimize the combination of task losses, X X L1 (x, y)"
D18-1412,P15-2036,1,0.898483,"SRL. Table 1 shows the performance of all the scaffold models on frame SRL with respect to prior work and a semi-CRF baseline (§5.1) without a syntactic scaffold. We follow the official evaluation from the SemEval shared task for frame-semantic parsing (Baker et al., 2007). Prior work for frame SRL has relied on predicted syntactic trees, in two different ways: by using syntax-based rules to prune out spans of text that are unlikely to contain any frame’s argument; and by using syntactic features in their statistical model (Das et al., 2014; T¨ackstr¨om et al., 2015; FitzGerald et al., 2015; Kshirsagar et al., 2015). The best published results on FrameNet 1.5 are due to Yang and Mitchell (2017). In their sequential model (seq), they treat argument identification as a sequence-labeling problem using a deep bidirectional LSTM with a CRF layer. In their relational model (Rel), they treat the same problem as a span classification problem. Finally, they introduce an ensemble to integerate both models, and use an integer linear program for inference satisfying SRL constraints. Though their model does not do any syntactic pruning, it does use syntactic features for argument identification and labeling.5 Notably"
D18-1412,D15-1112,0,0.0644176,"Missing"
D18-1412,D17-1018,1,0.481129,"sks. 1 Introduction As algorithms for the semantic analysis of natural language sentences have developed, the role of syntax has been repeatedly revisited. Linguistic theories have argued for a very tight integration of syntactic and semantic processing (Steedman, 2000; Copestake and Flickinger, 2000), and many systems have used syntactic dependency or phrase-based parsers as preprocessing for semantic analysis (Gildea and Palmer, 2002; Punyakanok et al., 2008; Das et al., 2014). Meanwhile, some recent methods forgo explicit syntactic processing altogether (Zhou and Xu, 2015; He et al., 2017; Lee et al., 2017; Peng et al., 2017). Because annotated training datasets for semantics will always be limited, we expect that syntax—which offers an incomplete but potentially useful view of semantic structure—will continue to offer useful inductive bias, encouraging semantic models toward better generalization. We address the central question: is there a way for semantic analyzers to benefit from syntax without the computational cost of syntactic parsing? We propose a multitask learning approach to incorporating syntactic information into learned representations of neural semantics models (§2). Our approach"
D18-1412,J02-3001,0,0.475993,"nimizes an auxiliary supervised loss function, derived from a syntactic treebank. The goal is to steer the distributed, contextualized representations of words and spans toward accurate semantic and syntactic labeling. We avoid the cost of training or executing a full syntactic parser, and at test time (i.e., runtime in applications) the semantic analyzer has no additional cost over a syntax-free baseline. Further, the method does not assume that the syntactic treebank overlaps the dataset for the primary task. Many semantic tasks involve labeling spans, including semantic role labeling (SRL; Gildea and Jurafsky, 2002) and coreference resolution (Ng, 2010) (tasks we consider in this paper), as well as named entity recognition and some reading comprehension and question answering tasks (Rajpurkar et al., 2016). These spans are usually syntactic constituents (cf. PropBank; Palmer et al., 2005), making phrase-based syntax a natural choice for a scaffold. See Figure 1 for an example sentence with syntactic and semantic annotations. Since the scaffold task is not an end in itself, we relax the syntactic parsing problem to a collection of independent span-level predictions, with no constraint that they form a val"
D18-1412,N18-2108,1,0.84095,"rate synctactic information in a pipeline, using features and rules for mention proposals from predicted syntax. Our baseline is the model from Lee et al. (2017), described in §6. Similar to the baseline model for frame SRL, and in contrast with prior work, this model does not use any syntax. We experiment with the best syntactic scaffold from the frame SRL task. We used NP, OTHER, and null as the labels for the common nonterminals scaffold here, since coreferring mentions are rarely prepositional phrases. The syntactic scaffold outperforms the baseline by 0.6 absolute F1 . Contemporaneously, Lee et al. (2018) proposed a model which takes in account higher order inference and more aggressive pruning, as well as initialization with ELMo embeddings, resulting in 73.0 average F1 . All the above are orthogonal to our approach, and could be incorporated to yield higher gains. 8 Discussion To investigate the performance of the syntactic scaffold, we focus on the frame SRL results, where we observed the greatest improvement with respect to a non-syntactic baseline. We consider a breakdown of the performance by the syntactic phrase types of the arguments, provided in FrameNet7 in Figure 2. Not surpris7 We"
D18-1412,P02-1031,0,0.06036,"g, through a multitask objective. We improve over strong baselines on PropBank semantics, frame semantics, and coreference resolution, achieving competitive performance on all three tasks. 1 Introduction As algorithms for the semantic analysis of natural language sentences have developed, the role of syntax has been repeatedly revisited. Linguistic theories have argued for a very tight integration of syntactic and semantic processing (Steedman, 2000; Copestake and Flickinger, 2000), and many systems have used syntactic dependency or phrase-based parsers as preprocessing for semantic analysis (Gildea and Palmer, 2002; Punyakanok et al., 2008; Das et al., 2014). Meanwhile, some recent methods forgo explicit syntactic processing altogether (Zhou and Xu, 2015; He et al., 2017; Lee et al., 2017; Peng et al., 2017). Because annotated training datasets for semantics will always be limited, we expect that syntax—which offers an incomplete but potentially useful view of semantic structure—will continue to offer useful inductive bias, encouraging semantic models toward better generalization. We address the central question: is there a way for semantic analyzers to benefit from syntax without the computational cost"
D18-1412,Q13-1018,0,0.0760573,"Missing"
D18-1412,N10-1112,1,0.71015,"arameters of the semi-CRF are learned to maximize a criterion related to the conditional loglikelihood of the gold-standard segments in the training corpus (§5.2). The learner evaluates and adjusts segment scores ψ(sk , x) for every span in the sentence, which in turn involves learning embedded representations for all spans (§5.3). 5.2 Softmax-Margin Objective Typically CRF and semi-CRF models are trained to maximize a conditional log-likelihood objective. In early experiments, we found that incorporating a structured cost was beneficial; we do so by using a softmax-margin training objective (Gimpel and Smith, 2010), a “cost-aware” variant X L1 = − (x,s∗ )∈D1 Z(x, s∗ ) = X log exp Ψ(s∗ , x) , Z(x, s∗ ) exp {Ψ(s, x) + cost(s, s∗ )}. (4) (5) s We design the cost function so that it factors by predicted span, in the same way Ψ does: X X cost(s, s∗ ) = cost(s, s∗ ) = I(s &lt; s∗ ). (6) s∈s s∈s The softmax-margin criterion, like log-likelihood, is globally normalized over all of the exponentially many possible labeled segmentations. The following zeroth-order semi-Markov dynamic program (Sarawagi et al., 2004) efficiently computes the new partition function: X αj = αi−1 exp{Ψ(s, x) + cost(s, s∗ )}, (7) s=hi, j,y"
D18-1412,D17-1206,0,0.0301315,"ods for estimating word embeddings (Mikolov et al., 2013; Pennington et al., 2014; Peters et al., 2018). After training word embeddings, the tasks implied by models like the skip-gram or ELMo’s language model become irrelevant to the downstream use of the embeddings. A noteworthy difference is that, rather than pre-training, a scaffold is integrated directly into the training of T 1 through a multitask objective. Multitask learning. Neural architectures have often yielded performance gains when trained for multiple tasks together (Collobert et al., 2011; Luong et al., 2015; Chen et al., 2017; Hashimoto et al., 2017). In particular, performance of semantic role labeling tasks improves when done jointly with other semantic tasks (FitzGerald et al., 2015; Peng et al., 2017, 2018). Contemporaneously with this work, Hershcovich et al. (2018) proposed a multitask learning setting for universal syntactic dependencies and UCCA semantics (Abend and Rappoport, 2013). Syntactic scaffolds focus on a primary semantic task, treating syntax as an auxillary, eventually forgettable prediction task. 4 We assume two sources of supervision: a corpus D1 with instances x annotated for the primary task’s outputs y (semantic ro"
D18-1412,D13-1152,0,0.0594194,"Missing"
D18-1412,P18-2058,1,0.905144,"rmance by 1 F1 , and our common nonterminal scaffold by 3.1 F1 .6 Prec. Rec. F1 Kshirsagar et al. (2015) Yang and Mitchell (2017) (Rel) Yang and Mitchell (2017) (Seq) †Yang and Mitchell (2017) (All) 66.0 71.8 63.4 70.2 60.4 57.7 66.4 60.2 63.1 64.0 64.9 65.5 Semi-CRF baseline 67.8 66.2 67.0 68.1 68.8 69.4 69.2 67.4 68.2 68.0 69.0 67.7 68.5 68.7 69.1 Model + constituent identity + nonterminal and parent + nonterminal + common nonterminals Table 1: Frame SRL results on the test set of FrameNet 1.5., using gold frames. Ensembles are denoted by †. Prec. Rec. F1 Zhou and Xu (2015) He et al. (2017) He et al. (2018a) Tan et al. (2018) 81.7 83.9 81.9 81.6 73.7 83.6 81.3 81.7 82.1 82.7 Semi-CRF baseline 84.8 81.2 83.0 85.1 82.6 83.8 Model + common nonterminals Table 2: PropBank sSRL results, using gold predicates, on CoNLL 2012 test. For fair comparison, we show only non-ensembled models. 6 This result is not reported in Table 1 since Peng et al. (2018) used a preprocessing which renders the test set slightly larger — the difference we report is calculated using their test set. 3778 B3 Prec. Rec. F1 CEAFφ4 Prec. Rec. F1 Avg. F1 Wiseman et al. (2016) 77.5 69.8 73.4 Clark and Manning (2016b) 79.9 69.3 74.2"
D18-1412,W08-2124,0,0.0676864,"Missing"
D18-1412,D12-1074,0,0.084146,"Missing"
D18-1412,P10-1142,0,0.0212275,"from a syntactic treebank. The goal is to steer the distributed, contextualized representations of words and spans toward accurate semantic and syntactic labeling. We avoid the cost of training or executing a full syntactic parser, and at test time (i.e., runtime in applications) the semantic analyzer has no additional cost over a syntax-free baseline. Further, the method does not assume that the syntactic treebank overlaps the dataset for the primary task. Many semantic tasks involve labeling spans, including semantic role labeling (SRL; Gildea and Jurafsky, 2002) and coreference resolution (Ng, 2010) (tasks we consider in this paper), as well as named entity recognition and some reading comprehension and question answering tasks (Rajpurkar et al., 2016). These spans are usually syntactic constituents (cf. PropBank; Palmer et al., 2005), making phrase-based syntax a natural choice for a scaffold. See Figure 1 for an example sentence with syntactic and semantic annotations. Since the scaffold task is not an end in itself, we relax the syntactic parsing problem to a collection of independent span-level predictions, with no constraint that they form a valid parse tree. This means we never nee"
D18-1412,J05-1004,0,0.492254,"r, and at test time (i.e., runtime in applications) the semantic analyzer has no additional cost over a syntax-free baseline. Further, the method does not assume that the syntactic treebank overlaps the dataset for the primary task. Many semantic tasks involve labeling spans, including semantic role labeling (SRL; Gildea and Jurafsky, 2002) and coreference resolution (Ng, 2010) (tasks we consider in this paper), as well as named entity recognition and some reading comprehension and question answering tasks (Rajpurkar et al., 2016). These spans are usually syntactic constituents (cf. PropBank; Palmer et al., 2005), making phrase-based syntax a natural choice for a scaffold. See Figure 1 for an example sentence with syntactic and semantic annotations. Since the scaffold task is not an end in itself, we relax the syntactic parsing problem to a collection of independent span-level predictions, with no constraint that they form a valid parse tree. This means we never need to run a syntactic parsing algorithm. Our experiments demonstrate that the syntactic scaffold offers a substantial boost to state-of-theart baselines for two SRL tasks (§5) and coreference resolution (§6). Our models use the strongest ava"
D18-1412,P17-1186,1,0.932782,"n As algorithms for the semantic analysis of natural language sentences have developed, the role of syntax has been repeatedly revisited. Linguistic theories have argued for a very tight integration of syntactic and semantic processing (Steedman, 2000; Copestake and Flickinger, 2000), and many systems have used syntactic dependency or phrase-based parsers as preprocessing for semantic analysis (Gildea and Palmer, 2002; Punyakanok et al., 2008; Das et al., 2014). Meanwhile, some recent methods forgo explicit syntactic processing altogether (Zhou and Xu, 2015; He et al., 2017; Lee et al., 2017; Peng et al., 2017). Because annotated training datasets for semantics will always be limited, we expect that syntax—which offers an incomplete but potentially useful view of semantic structure—will continue to offer useful inductive bias, encouraging semantic models toward better generalization. We address the central question: is there a way for semantic analyzers to benefit from syntax without the computational cost of syntactic parsing? We propose a multitask learning approach to incorporating syntactic information into learned representations of neural semantics models (§2). Our approach, the syntactic scaf"
D18-1412,Q15-1003,0,0.0552312,"Missing"
D18-1412,N18-1135,1,0.842326,"5 68.7 69.1 Model + constituent identity + nonterminal and parent + nonterminal + common nonterminals Table 1: Frame SRL results on the test set of FrameNet 1.5., using gold frames. Ensembles are denoted by †. Prec. Rec. F1 Zhou and Xu (2015) He et al. (2017) He et al. (2018a) Tan et al. (2018) 81.7 83.9 81.9 81.6 73.7 83.6 81.3 81.7 82.1 82.7 Semi-CRF baseline 84.8 81.2 83.0 85.1 82.6 83.8 Model + common nonterminals Table 2: PropBank sSRL results, using gold predicates, on CoNLL 2012 test. For fair comparison, we show only non-ensembled models. 6 This result is not reported in Table 1 since Peng et al. (2018) used a preprocessing which renders the test set slightly larger — the difference we report is calculated using their test set. 3778 B3 Prec. Rec. F1 CEAFφ4 Prec. Rec. F1 Avg. F1 Wiseman et al. (2016) 77.5 69.8 73.4 Clark and Manning (2016b) 79.9 69.3 74.2 Clark and Manning (2016a) 79.2 70.4 74.6 66.8 57.0 61.5 71.0 56.5 63.0 69.9 58.0 63.4 62.1 53.9 57.7 63.8 54.3 58.7 63.5 55.5 59.2 64.2 65.3 65.7 Lee et al. (2017) 78.4 73.4 75.8 68.6 61.8 65.0 62.7 59.0 60.8 67.2 + common nonterminals 78.4 74.3 76.3 68.7 62.9 65.7 62.9 60.2 61.5 67.8 Model MUC Prec. Rec. F1 Table 3: Coreference resolution r"
D18-1412,D14-1162,0,0.0809139,"still be composed to obtain span representations. Instead, our approach learns span-level representations from phrase-type supervision directly, for semantic tasks. Additionally, these methods explore architectural variations in RNN layers for including supervision, whereas we focus on incorporating supervision with minimal changes to the baseline architecture. To the best of our knowledge, such simplified syntactic scaffolds have not been tried before. Word embeddings. Our definition of a scaffold task almost includes stand-alone methods for estimating word embeddings (Mikolov et al., 2013; Pennington et al., 2014; Peters et al., 2018). After training word embeddings, the tasks implied by models like the skip-gram or ELMo’s language model become irrelevant to the downstream use of the embeddings. A noteworthy difference is that, rather than pre-training, a scaffold is integrated directly into the training of T 1 through a multitask objective. Multitask learning. Neural architectures have often yielded performance gains when trained for multiple tasks together (Collobert et al., 2011; Luong et al., 2015; Chen et al., 2017; Hashimoto et al., 2017). In particular, performance of semantic role labeling tas"
D18-1412,N18-1202,1,0.82975,"here is only one task about whose performance we are concerned, denoted T 1 (in this paper, T 1 is either SRL or coreference resolution). We use the term “scaffold” to refer to a second task, T 2 , that can be combined with T 1 during multitask learning. A scaffold task is only used during training; it holds no intrinsic interest beyond biasing the learning of T 1 , and after learning is completed, the scaffold is discarded. A syntactic scaffold is a task designed to steer the (shared) model toward awareness of syntactic 1 This excludes models initialized with deep, contextualized embeddings (Peters et al., 2018), an approach orthogonal to ours. structure. It could be defined through a syntactic parser that shares some parameters with T 1 ’s model. Since syntactic parsing is costly, we use simpler syntactic prediction problems (discussed below) that do not produce whole trees. As with multitask learning in general, we do not assume that the same data are annotated with outputs for T 1 and T 2 . In this work, T 2 is defined using phrase-structure syntactic annotations from OntoNotes 5.0 (Weischedel et al., 2013; Pradhan et al., 2013). We experiment with three settings: one where the corpus for T 2 does"
D18-1412,W12-4501,0,0.144135,"Missing"
D18-1412,J08-2005,0,0.160547,"jective. We improve over strong baselines on PropBank semantics, frame semantics, and coreference resolution, achieving competitive performance on all three tasks. 1 Introduction As algorithms for the semantic analysis of natural language sentences have developed, the role of syntax has been repeatedly revisited. Linguistic theories have argued for a very tight integration of syntactic and semantic processing (Steedman, 2000; Copestake and Flickinger, 2000), and many systems have used syntactic dependency or phrase-based parsers as preprocessing for semantic analysis (Gildea and Palmer, 2002; Punyakanok et al., 2008; Das et al., 2014). Meanwhile, some recent methods forgo explicit syntactic processing altogether (Zhou and Xu, 2015; He et al., 2017; Lee et al., 2017; Peng et al., 2017). Because annotated training datasets for semantics will always be limited, we expect that syntax—which offers an incomplete but potentially useful view of semantic structure—will continue to offer useful inductive bias, encouraging semantic models toward better generalization. We address the central question: is there a way for semantic analyzers to benefit from syntax without the computational cost of syntactic parsing? We"
D18-1412,D16-1264,0,0.0416351,"d syntactic labeling. We avoid the cost of training or executing a full syntactic parser, and at test time (i.e., runtime in applications) the semantic analyzer has no additional cost over a syntax-free baseline. Further, the method does not assume that the syntactic treebank overlaps the dataset for the primary task. Many semantic tasks involve labeling spans, including semantic role labeling (SRL; Gildea and Jurafsky, 2002) and coreference resolution (Ng, 2010) (tasks we consider in this paper), as well as named entity recognition and some reading comprehension and question answering tasks (Rajpurkar et al., 2016). These spans are usually syntactic constituents (cf. PropBank; Palmer et al., 2005), making phrase-based syntax a natural choice for a scaffold. See Figure 1 for an example sentence with syntactic and semantic annotations. Since the scaffold task is not an end in itself, we relax the syntactic parsing problem to a collection of independent span-level predictions, with no constraint that they form a valid parse tree. This means we never need to run a syntactic parsing algorithm. Our experiments demonstrate that the syntactic scaffold offers a substantial boost to state-of-theart baselines for"
D18-1412,J08-2002,0,0.054317,"Missing"
D18-1412,N16-1114,0,0.073824,"pared to approaches which require multiple output labels over the same data, we offer the major advantage of not requiring any assumptions about, or specification of, the relationship between T 1 and T 2 output. 3 Related Work We briefly contrast the syntactic scaffold with existing alternatives. Pipelines. In a typical pipeline, T 1 and T 2 are separately trained, with the output of T 2 used to define the inputs to T 1 (Wolpert, 1992). Using syntax as T 2 in a pipeline is perhaps the most 3773 common approach for semantic structure prediction (Toutanova et al., 2008; Yang and Mitchell, 2017; Wiseman et al., 2016).2 However, pipelines introduce the problem of cascading errors (T 2 ’s mistakes affect the performance, and perhaps the training, of T 1 ; He et al., 2013). To date, remedies to cascading errors are so computationally expensive as to be impractical (e.g., Finkel et al., 2006). A syntactic scaffold is quite different from a pipeline since the output of T 2 is never explicitly used. Latent variables. Another solution is to treat the output of T 2 as a (perhaps structured) latent variable. This approach obviates the need of supervision for T 2 and requires marginalization (or some approximation"
D18-1412,D17-1128,0,0.427202,"SRL and coreference). Compared to approaches which require multiple output labels over the same data, we offer the major advantage of not requiring any assumptions about, or specification of, the relationship between T 1 and T 2 output. 3 Related Work We briefly contrast the syntactic scaffold with existing alternatives. Pipelines. In a typical pipeline, T 1 and T 2 are separately trained, with the output of T 2 used to define the inputs to T 1 (Wolpert, 1992). Using syntax as T 2 in a pipeline is perhaps the most 3773 common approach for semantic structure prediction (Toutanova et al., 2008; Yang and Mitchell, 2017; Wiseman et al., 2016).2 However, pipelines introduce the problem of cascading errors (T 2 ’s mistakes affect the performance, and perhaps the training, of T 1 ; He et al., 2013). To date, remedies to cascading errors are so computationally expensive as to be impractical (e.g., Finkel et al., 2006). A syntactic scaffold is quite different from a pipeline since the output of T 2 is never explicitly used. Latent variables. Another solution is to treat the output of T 2 as a (perhaps structured) latent variable. This approach obviates the need of supervision for T 2 and requires marginalization"
D18-1412,P16-1147,0,0.0316632,"raining data. Joint learning of syntax and semantics. The motivation behind joint learning of syntactic and semantic representations is that any one task is helpful in predicting the other (Llu´ıs and M`arquez, 2008; Llu´ıs et al., 2013; Henderson et al., 2013; Swayamdipta et al., 2016). This typically requires joint prediction of the outputs of T 1 and T 2 , which tends to be computationally expensive at both training and test time. Part of speech scaffolds. Similar to our work, there have been multitask models that use partof-speech tagging as T 2 , with transition-based dependency parsing (Zhang and Weiss, 2016) and CCG supertagging (Søgaard and Goldberg, 2016) as T 1 . Both of the above approaches assumed parallel input data and used both tasks as supervision. Notably, we simplify our T 2 , throwing away the structured aspects of syntactic parsing, whereas part-of-speech tagging has very little structure to begin with. While their approach results in improved token-level representations learned via supervision from POS tags, these must still be composed to obtain span representations. Instead, our approach learns span-level representations from phrase-type supervision directly, for semantic tasks. A"
D18-1412,P15-1109,0,0.217767,"petitive performance on all three tasks. 1 Introduction As algorithms for the semantic analysis of natural language sentences have developed, the role of syntax has been repeatedly revisited. Linguistic theories have argued for a very tight integration of syntactic and semantic processing (Steedman, 2000; Copestake and Flickinger, 2000), and many systems have used syntactic dependency or phrase-based parsers as preprocessing for semantic analysis (Gildea and Palmer, 2002; Punyakanok et al., 2008; Das et al., 2014). Meanwhile, some recent methods forgo explicit syntactic processing altogether (Zhou and Xu, 2015; He et al., 2017; Lee et al., 2017; Peng et al., 2017). Because annotated training datasets for semantics will always be limited, we expect that syntax—which offers an incomplete but potentially useful view of semantic structure—will continue to offer useful inductive bias, encouraging semantic models toward better generalization. We address the central question: is there a way for semantic analyzers to benefit from syntax without the computational cost of syntactic parsing? We propose a multitask learning approach to incorporating syntactic information into learned representations of neural"
D18-1412,P16-2038,0,0.0317744,"mantics. The motivation behind joint learning of syntactic and semantic representations is that any one task is helpful in predicting the other (Llu´ıs and M`arquez, 2008; Llu´ıs et al., 2013; Henderson et al., 2013; Swayamdipta et al., 2016). This typically requires joint prediction of the outputs of T 1 and T 2 , which tends to be computationally expensive at both training and test time. Part of speech scaffolds. Similar to our work, there have been multitask models that use partof-speech tagging as T 2 , with transition-based dependency parsing (Zhang and Weiss, 2016) and CCG supertagging (Søgaard and Goldberg, 2016) as T 1 . Both of the above approaches assumed parallel input data and used both tasks as supervision. Notably, we simplify our T 2 , throwing away the structured aspects of syntactic parsing, whereas part-of-speech tagging has very little structure to begin with. While their approach results in improved token-level representations learned via supervision from POS tags, these must still be composed to obtain span representations. Instead, our approach learns span-level representations from phrase-type supervision directly, for semantic tasks. Additionally, these methods explore architectural v"
D18-1412,K16-1019,1,0.877792,"o reason about the outputs of T 1 . Syntax as a latent variable for semantics was explored by Zettlemoyer and Collins (2005) and Naradowsky et al. (2012). Apart from avoiding marginalization, the syntactic scaffold offers a way to use auxiliary syntacticallyannotated data as direct supervision for T 2 , and it need not overlap the T 1 training data. Joint learning of syntax and semantics. The motivation behind joint learning of syntactic and semantic representations is that any one task is helpful in predicting the other (Llu´ıs and M`arquez, 2008; Llu´ıs et al., 2013; Henderson et al., 2013; Swayamdipta et al., 2016). This typically requires joint prediction of the outputs of T 1 and T 2 , which tends to be computationally expensive at both training and test time. Part of speech scaffolds. Similar to our work, there have been multitask models that use partof-speech tagging as T 2 , with transition-based dependency parsing (Zhang and Weiss, 2016) and CCG supertagging (Søgaard and Goldberg, 2016) as T 1 . Both of the above approaches assumed parallel input data and used both tasks as supervision. Notably, we simplify our T 2 , throwing away the structured aspects of syntactic parsing, whereas part-of-speech"
D19-1005,D19-1522,0,0.0518018,"Missing"
D19-1005,D17-1284,1,0.863563,"Missing"
D19-1005,D18-1454,0,0.0247236,"nguage models Some previous work has focused on adding KBs to generative language models (LMs) (Ahn et al., 2017; Yang et al., 2017; Logan et al., 2019) or building entity-centric LMs (Ji et al., 2017). However, these methods introduce latent variables that require full annotation for training, or marginalization. In contrast, we adopt a method that allows training with large amounts of unannotated text. Task-specific KB architectures Other work has focused on integrating KBs into neural architectures for specific downstream tasks (Yang and Mitchell, 2017; Sun et al., 2018; Chen et al., 2018; Bauer et al., 2018; Mihaylov and Frank, 2018; Wang and Jiang, 2019; Yang et al., 2019). Our approach instead uses KBs to learn more generally transferable representations that can be used to improve a variety of downstream tasks. Related Work Pretrained word representations Initial work learning word vectors focused on static word embeddings using multi-task learning objectives (Collobert and Weston, 2008) or corpus level cooccurence statistics (Mikolov et al., 2013a; Pennington et al., 2014). Recently the field has shifted toward learning context-sensitive embeddings (Dai and Le, 2015; Peters et al., 2018; Dev"
D19-1005,W09-2415,0,0.0815158,"Missing"
D19-1005,D11-1072,0,0.332131,"Missing"
D19-1005,D17-1195,1,0.70636,"ng based on the smaller BERTBASE model, the experiments demonstrate improved masked language model perplexity and ability to recall facts over BERTLARGE . The extrinsic evaluations demonstrate improvements for challenging relationship extraction, entity typing and word sense disambiguation datasets, and often outperform other contemporaneous attempts to incorporate external knowledge into BERT. 2 Entity-aware language models Some previous work has focused on adding KBs to generative language models (LMs) (Ahn et al., 2017; Yang et al., 2017; Logan et al., 2019) or building entity-centric LMs (Ji et al., 2017). However, these methods introduce latent variables that require full annotation for training, or marginalization. In contrast, we adopt a method that allows training with large amounts of unannotated text. Task-specific KB architectures Other work has focused on integrating KBs into neural architectures for specific downstream tasks (Yang and Mitchell, 2017; Sun et al., 2018; Chen et al., 2018; Bauer et al., 2018; Mihaylov and Frank, 2018; Wang and Jiang, 2019; Yang et al., 2019). Our approach instead uses KBs to learn more generally transferable representations that can be used to improve a"
D19-1005,P18-1224,0,0.201073,"ns in the input text and use an entity linker to retrieve relevant entity embeddings from a KB to form knowledge enhanced entity-span representations. Then, the model recontextualizes the entity-span representations with word-toentity attention to allow long range interactions between contextual word representations and all entity spans in the context. The entire KAR is inserted between two layers in the middle of a pretrained model such as BERT. In contrast to previous approaches that integrate external knowledge into task-specific models with task supervision (e.g., Yang and Mitchell, 2017; Chen et al., 2018), our approach learns the entity linkers with self-supervision on unlabeled data. This results in general purpose knowledge enhanced representations that can be applied to a wide range of downstream tasks. Our approach has several other benefits. First, it leaves the top layers of the original model unchanged so we may retain the output loss layers and fine-tune on unlabeled corpora while training the KAR. This also allows us to simply swap out BERT for KnowBert in any downstream application. Second, by taking advantage of the existing high capacity layers in the original model, the KAR is lig"
D19-1005,K18-1050,0,0.171803,"tential mention from among the available candidates. It first runs mention-span self-attention to compute Se = TransformerBlock(S). (3) and max-margin, (2) LEL = max(0, γ − ψmg ) + X max(0, γ + ψmk ), The span self-attention is identical to the typical transformer layer, exception that the self-attention is between mention-span vectors instead of word piece vectors. This allows KnowBert to incorporate global information into each linking decision so that it can take advantage of entity-entity cooccurrence and resolve which of several overlapping candidate mentions should be linked.1 Following Kolitsas et al. (2018), Se is used to score each of the candidate entities while incorporating the candidate entity prior from the KB. Each candidate span m has an associated mention-span (5) emk 6=emg formulations (see Sec. 4.1 for details). Knowledge enhanced entity-span representations KnowBert next injects the KB entity information into the mention-span representations computed from BERT vectors (sem ) to form entityspan representations. For a given span m, we first disregard all candidate entities with score ψ below a fixed threshold, and softmax normalize the remaining scores:  exp(ψmk )   , ψmk ≥ δ  P ex"
D19-1005,D14-1110,0,0.0324097,"knowledge graph. These methods broadly fall into two categories: translational distance models (Bordes et al., 2013; Wang et al., 2014b; Lin et al., 2015; Xiao et al., 2016) which use a distance-based scoring function, and linear models (Nickel et al., 2011; Yang et al., 2014; Trouillon et al., 2016; Dettmers et al., 2018) which use a similarity-based scoring function. We experiment with TuckER (Balazevic et al., 2019) embeddings, a recent linear model which generalizes many of the aforecited models. Other methods combine entity metadata with the graph (Xie et al., 2016), use entity contexts (Chen et al., 2014; Ganea and Hofmann, 2017), or a combination of contexts and the KB (Wang et al., 2014a; Gupta 3.1 Pretrained BERT We describe KnowBert as an extension to (and candidate replacement for) BERT, although the method is general and can be applied to any deep pretrained model including left-to-right and rightto-left LMs such as ELMo and GPT. Formally, BERT accepts as input a sequence of N WordPiece tokens (Sennrich et al., 2016; Wu et al., 2016), (x1 , . . . , xN ), and computes L layers of D-dimensional contextual representations Hi ∈ RN ×D by successively applying non-linear functions Hi = Fi (Hi"
D19-1005,D17-1018,0,0.0321533,"back to the BERT dimension (7) resulting in H0i . vector sem (computed via Eq. 2), Mm candidate entities with embeddings emk (from the KB), and prior probabilities pmk . We compute Mm scores using the prior and dot product between the entityspan vectors and entity embeddings, jected to the entity dimension (E, typically 200 or 300, see Sec. 4.1) with a linear projection, proj Hi proj = Hi W1 proj + b1 . (1) Then, the KAR computes C mention-span representations sm ∈ RE , one for each candidate mention, by pooling over all word pieces in a mentionspan using the self-attentive span pooling from Lee et al. (2017). The mention-spans are stacked into a matrix S ∈ RC×E . ψmk = MLP(pmk , sem · emk ), with a two-layer MLP (100 hidden dimensions). If entity linking (EL) supervision is available, we can compute a loss with the gold entity emg . The exact form of the loss depends on the KB, and we use both log-likelihood,   X exp(ψmg ) LEL = − log P , (4) k exp(ψmk ) m Entity linker The entity linker is responsible for performing entity disambiguation for each potential mention from among the available candidates. It first runs mention-span self-attention to compute Se = TransformerBlock(S). (3) and max-mar"
D19-1005,P18-1009,0,0.0644275,"Missing"
D19-1005,Q15-1023,1,0.851813,"input a sequence of N WordPiece tokens (Sennrich et al., 2016; Wu et al., 2016), (x1 , . . . , xN ), and computes L layers of D-dimensional contextual representations Hi ∈ RN ×D by successively applying non-linear functions Hi = Fi (Hi−1 ). The non-linear function is a multi-headed self-attention layer followed by a position-wise multilayer perceptron (MLP) 44 In practice, these are often implemented using precomputed dictionaries (e.g., CrossWikis; Spitkovsky and Chang, 2012), KB specific rules (e.g., a WordNet lemmatizer), or other heuristics (e.g., string match; Mihaylov and Frank, 2018). Ling et al. (2015) showed that incorporating candidate priors into entity linkers can be a powerful signal, so we optionally allow for the candidate selector to return an associated prior probability for each entity candidate. In some cases, it is beneficial to over-generate potential candidates and add a special NULL entity to each candidate list, thereby allowing the linker to discriminate between actual links and false positive candidates. In this work, the entity candidate selectors are fixed but their output is passed to a learned context dependent entity linker to disambiguate the candidate mentions. Fina"
D19-1005,P19-1598,1,0.857562,"eam performance on relationship extraction, entity typing, and word sense disambiguation. KnowBert’s runtime is comparable to BERT’s and it scales to large KBs. 1 Introduction Large pretrained models such as ELMo (Peters et al., 2018), GPT (Radford et al., 2018), and BERT (Devlin et al., 2019) have significantly improved the state of the art for a wide range of NLP tasks. These models are trained on large amounts of raw text using self-supervised objectives. However, they do not contain any explicit grounding to real world entities and as a result have difficulty recovering factual knowledge (Logan et al., 2019). Knowledge bases (KBs) provide a rich source of high quality, human-curated knowledge that can be used to ground these models. In addition, they 43 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 43–54, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics et al., 2017). Our approach is agnostic to the details of the entity embedding method and as a result is able to use any of these methods. KB, subject to a small set of requirements (see Se"
D19-1005,K16-1006,0,0.0381149,"performance as they must balance specializing for the entity linking task and learning general purpose representations suitable for language modeling. Table 2 displays fine-grained WSD F1 using the evaluation framework from Navigli et al. (2017) and the ALL dataset (combing SemEval 2007, 2013, 2015 and Senseval 2 and 3). By linking to nodes in our WordNet graph and restricting to gold lemmas at test time we can recast the WSD task under our general entity linking framework. The ELMo and BERT baselines use a nearest neighbor approach trained on the SemCor dataset, similar to the evaluation in Melamud et al. (2016), which has previously been shown to be competitive with task-specific architectures (Raganato et al., 2017). As can be seen, KnowBert provides competitive performance, and KnowBert-W+W is able to 4.3 Downstream Tasks This section evaluates KnowBert on downstream tasks to validate that the addition of knowledge improves performance on tasks expected to benefit from it. Given the overall superior performance of KnowBert-W+W on the intrinsic evaluations, we focus on it exclusively for evaluation in this section. The main results are included in this section; see the supplementary material for fu"
D19-1005,N19-1423,0,0.721409,"nd-to-end in a multitask setting that combines a small amount of entity linking supervision with a large amount of raw text. After integrating WordNet and a subset of Wikipedia into BERT, the knowledge enhanced BERT (KnowBert) demonstrates improved perplexity, ability to recall facts as measured in a probing task and downstream performance on relationship extraction, entity typing, and word sense disambiguation. KnowBert’s runtime is comparable to BERT’s and it scales to large KBs. 1 Introduction Large pretrained models such as ELMo (Peters et al., 2018), GPT (Radford et al., 2018), and BERT (Devlin et al., 2019) have significantly improved the state of the art for a wide range of NLP tasks. These models are trained on large amounts of raw text using self-supervised objectives. However, they do not contain any explicit grounding to real world entities and as a result have difficulty recovering factual knowledge (Logan et al., 2019). Knowledge bases (KBs) provide a rich source of high quality, human-curated knowledge that can be used to ground these models. In addition, they 43 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Confere"
D19-1005,P18-1076,0,0.131211,"revious work has focused on adding KBs to generative language models (LMs) (Ahn et al., 2017; Yang et al., 2017; Logan et al., 2019) or building entity-centric LMs (Ji et al., 2017). However, these methods introduce latent variables that require full annotation for training, or marginalization. In contrast, we adopt a method that allows training with large amounts of unannotated text. Task-specific KB architectures Other work has focused on integrating KBs into neural architectures for specific downstream tasks (Yang and Mitchell, 2017; Sun et al., 2018; Chen et al., 2018; Bauer et al., 2018; Mihaylov and Frank, 2018; Wang and Jiang, 2019; Yang et al., 2019). Our approach instead uses KBs to learn more generally transferable representations that can be used to improve a variety of downstream tasks. Related Work Pretrained word representations Initial work learning word vectors focused on static word embeddings using multi-task learning objectives (Collobert and Weston, 2008) or corpus level cooccurence statistics (Mikolov et al., 2013a; Pennington et al., 2014). Recently the field has shifted toward learning context-sensitive embeddings (Dai and Le, 2015; Peters et al., 2018; Devlin et al., 2019). We buil"
D19-1005,D17-1169,0,0.0298708,"projected word piece representations and knowledge enhanced entityspan vectors. As introduced by Vaswani et al. (2017), the contextual embeddings Hi are used for the query, key, and value in multi-headed selfattention. The word-to-entity-span attention in proj KnowBert substitutes Hi for the query, and S0e for both the key and value: 0 proj Hi proj 0 3.4 Our training regime incrementally pretrains increasingly larger portions of KnowBert before fine-tuning all trainable parameters in a multitask setting with any available EL supervision. It is similar in spirit to the “chain-thaw” approach in Felbo et al. (2017), and is summarized in Alg. 1. We assume access to a pretrained BERT model and one or more KBs with their entity candidate selectors. To add the first KB, we begin by pretraining entity embeddings (if not already provided from another source), then freeze them in all subsequent training, including task-specific finetuning. If EL supervision is available, it is used to pretrain the KB specific EL parameters, while freezing the remainder of the network. Finally, the entire network is fine-tuned to convergence by minimizing 0 = MLP(MultiHeadAttn(Hi , S e , S e )). This allows each word piece to a"
D19-1005,D17-1277,0,0.337604,"ese methods broadly fall into two categories: translational distance models (Bordes et al., 2013; Wang et al., 2014b; Lin et al., 2015; Xiao et al., 2016) which use a distance-based scoring function, and linear models (Nickel et al., 2011; Yang et al., 2014; Trouillon et al., 2016; Dettmers et al., 2018) which use a similarity-based scoring function. We experiment with TuckER (Balazevic et al., 2019) embeddings, a recent linear model which generalizes many of the aforecited models. Other methods combine entity metadata with the graph (Xie et al., 2016), use entity contexts (Chen et al., 2014; Ganea and Hofmann, 2017), or a combination of contexts and the KB (Wang et al., 2014a; Gupta 3.1 Pretrained BERT We describe KnowBert as an extension to (and candidate replacement for) BERT, although the method is general and can be applied to any deep pretrained model including left-to-right and rightto-left LMs such as ELMo and GPT. Formally, BERT accepts as input a sequence of N WordPiece tokens (Sennrich et al., 2016; Wu et al., 2016), (x1 , . . . , xN ), and computes L layers of D-dimensional contextual representations Hi ∈ RN ×D by successively applying non-linear functions Hi = Fi (Hi−1 ). The non-linear funct"
D19-1005,H94-1046,0,0.0558172,"dings. We used TuckER (Balazevic et al., 2019) to compute 200dimensional vectors for each synset and lemma using the relationship graph. Then, we extracted the gloss for each synset and used an off-theshelf state-of-the-art sentence embedding method (Subramanian et al., 2018) to produce 2048dimensional vectors. These are concatenated to the TuckER embeddings. To reduce the dimensionality for use in KnowBert, the frozen 2248dimensional embeddings are projected to 200dimensions with a learned linear transformation. For supervision, we combined the SemCor word sense disambiguation (WSD) dataset (Miller et al., 1994) with all lemma example usages from WordNet6 and link directly to synsets. The loss function is Eq. 4. At train time, we did not provide gold lemmas or POS tags, so KnowBert must learn to implicitly model coarse grained POS tags to disambiguate each word. At test time when evaluating we restricted candidate entities to just those matching the gold lemma and POS tag, consistent with the standard WSD evaluation. 4.2 Intrinsic Evaluation Perplexity Table 1 compares masked LM perplexity for KnowBert with BERTBASE and BERTLARGE . To rule out minor differences due to our data preparation, the BERT m"
D19-1005,E17-1010,0,0.0327968,"ameters are actually used. In contrast, BERTLARGE uses the majority of its 336M parameters for each input. Integrated EL It is also possible to evaluate the performance of the integrated entity linkers inside KnowBert using diagnostic probes without any further fine-tuning. As these were trained in a multitask setting primarily with raw text, we do not a priori expect high performance as they must balance specializing for the entity linking task and learning general purpose representations suitable for language modeling. Table 2 displays fine-grained WSD F1 using the evaluation framework from Navigli et al. (2017) and the ALL dataset (combing SemEval 2007, 2013, 2015 and Senseval 2 and 3). By linking to nodes in our WordNet graph and restricting to gold lemmas at test time we can recast the WSD task under our general entity linking framework. The ELMo and BERT baselines use a nearest neighbor approach trained on the SemCor dataset, similar to the evaluation in Melamud et al. (2016), which has previously been shown to be competitive with task-specific architectures (Raganato et al., 2017). As can be seen, KnowBert provides competitive performance, and KnowBert-W+W is able to 4.3 Downstream Tasks This se"
D19-1005,D14-1162,0,0.0943202,"ing KBs into neural architectures for specific downstream tasks (Yang and Mitchell, 2017; Sun et al., 2018; Chen et al., 2018; Bauer et al., 2018; Mihaylov and Frank, 2018; Wang and Jiang, 2019; Yang et al., 2019). Our approach instead uses KBs to learn more generally transferable representations that can be used to improve a variety of downstream tasks. Related Work Pretrained word representations Initial work learning word vectors focused on static word embeddings using multi-task learning objectives (Collobert and Weston, 2008) or corpus level cooccurence statistics (Mikolov et al., 2013a; Pennington et al., 2014). Recently the field has shifted toward learning context-sensitive embeddings (Dai and Le, 2015; Peters et al., 2018; Devlin et al., 2019). We build upon these by incorporating structured knowledge into these models. 3 KnowBert KnowBert incorporates knowledge bases into BERT using the Knowledge Attention and Recontextualization component (KAR). We start by describing the BERT and KB components. We then move to introducing KAR. Finally, we describe the training procedure, including the multitask training regime for jointly training KnowBert and an entity linker. Entity embeddings Entity embeddi"
D19-1005,N18-1202,1,0.886558,"supervised language modeling objective are jointly trained end-to-end in a multitask setting that combines a small amount of entity linking supervision with a large amount of raw text. After integrating WordNet and a subset of Wikipedia into BERT, the knowledge enhanced BERT (KnowBert) demonstrates improved perplexity, ability to recall facts as measured in a probing task and downstream performance on relationship extraction, entity typing, and word sense disambiguation. KnowBert’s runtime is comparable to BERT’s and it scales to large KBs. 1 Introduction Large pretrained models such as ELMo (Peters et al., 2018), GPT (Radford et al., 2018), and BERT (Devlin et al., 2019) have significantly improved the state of the art for a wide range of NLP tasks. These models are trained on large amounts of raw text using self-supervised objectives. However, they do not contain any explicit grounding to real world entities and as a result have difficulty recovering factual knowledge (Logan et al., 2019). Knowledge bases (KBs) provide a rich source of high quality, human-curated knowledge that can be used to ground these models. In addition, they 43 Proceedings of the 2019 Conference on Empirical Methods in Natural"
D19-1005,P19-1219,0,0.0221389,"n adding KBs to generative language models (LMs) (Ahn et al., 2017; Yang et al., 2017; Logan et al., 2019) or building entity-centric LMs (Ji et al., 2017). However, these methods introduce latent variables that require full annotation for training, or marginalization. In contrast, we adopt a method that allows training with large amounts of unannotated text. Task-specific KB architectures Other work has focused on integrating KBs into neural architectures for specific downstream tasks (Yang and Mitchell, 2017; Sun et al., 2018; Chen et al., 2018; Bauer et al., 2018; Mihaylov and Frank, 2018; Wang and Jiang, 2019; Yang et al., 2019). Our approach instead uses KBs to learn more generally transferable representations that can be used to improve a variety of downstream tasks. Related Work Pretrained word representations Initial work learning word vectors focused on static word embeddings using multi-task learning objectives (Collobert and Weston, 2008) or corpus level cooccurence statistics (Mikolov et al., 2013a; Pennington et al., 2014). Recently the field has shifted toward learning context-sensitive embeddings (Dai and Le, 2015; Peters et al., 2018; Devlin et al., 2019). We build upon these by incorp"
D19-1005,N19-1128,0,0.0605837,"Missing"
D19-1005,P19-1132,0,0.0588253,"Missing"
D19-1005,P16-1123,0,0.0556875,"Missing"
D19-1005,D17-1120,0,0.0301197,"resentations suitable for language modeling. Table 2 displays fine-grained WSD F1 using the evaluation framework from Navigli et al. (2017) and the ALL dataset (combing SemEval 2007, 2013, 2015 and Senseval 2 and 3). By linking to nodes in our WordNet graph and restricting to gold lemmas at test time we can recast the WSD task under our general entity linking framework. The ELMo and BERT baselines use a nearest neighbor approach trained on the SemCor dataset, similar to the evaluation in Melamud et al. (2016), which has previously been shown to be competitive with task-specific architectures (Raganato et al., 2017). As can be seen, KnowBert provides competitive performance, and KnowBert-W+W is able to 4.3 Downstream Tasks This section evaluates KnowBert on downstream tasks to validate that the addition of knowledge improves performance on tasks expected to benefit from it. Given the overall superior performance of KnowBert-W+W on the intrinsic evaluations, we focus on it exclusively for evaluation in this section. The main results are included in this section; see the supplementary material for full details. The baselines we compare against are BERTBASE , BERTLARGE , the pre-BERT state of the art, and t"
D19-1005,D14-1167,0,0.0467978,"using the Knowledge Attention and Recontextualization component (KAR). We start by describing the BERT and KB components. We then move to introducing KAR. Finally, we describe the training procedure, including the multitask training regime for jointly training KnowBert and an entity linker. Entity embeddings Entity embedding methods produce continuous vector representations from external knowledge sources. Knowledge graphbased methods optimize the score of observed triples in a knowledge graph. These methods broadly fall into two categories: translational distance models (Bordes et al., 2013; Wang et al., 2014b; Lin et al., 2015; Xiao et al., 2016) which use a distance-based scoring function, and linear models (Nickel et al., 2011; Yang et al., 2014; Trouillon et al., 2016; Dettmers et al., 2018) which use a similarity-based scoring function. We experiment with TuckER (Balazevic et al., 2019) embeddings, a recent linear model which generalizes many of the aforecited models. Other methods combine entity metadata with the graph (Xie et al., 2016), use entity contexts (Chen et al., 2014; Ganea and Hofmann, 2017), or a combination of contexts and the KB (Wang et al., 2014a; Gupta 3.1 Pretrained BERT We"
D19-1005,P16-1162,0,0.0394323,"embeddings, a recent linear model which generalizes many of the aforecited models. Other methods combine entity metadata with the graph (Xie et al., 2016), use entity contexts (Chen et al., 2014; Ganea and Hofmann, 2017), or a combination of contexts and the KB (Wang et al., 2014a; Gupta 3.1 Pretrained BERT We describe KnowBert as an extension to (and candidate replacement for) BERT, although the method is general and can be applied to any deep pretrained model including left-to-right and rightto-left LMs such as ELMo and GPT. Formally, BERT accepts as input a sequence of N WordPiece tokens (Sennrich et al., 2016; Wu et al., 2016), (x1 , . . . , xN ), and computes L layers of D-dimensional contextual representations Hi ∈ RN ×D by successively applying non-linear functions Hi = Fi (Hi−1 ). The non-linear function is a multi-headed self-attention layer followed by a position-wise multilayer perceptron (MLP) 44 In practice, these are often implemented using precomputed dictionaries (e.g., CrossWikis; Spitkovsky and Chang, 2012), KB specific rules (e.g., a WordNet lemmatizer), or other heuristics (e.g., string match; Mihaylov and Frank, 2018). Ling et al. (2015) showed that incorporating candidate priors"
D19-1005,P19-1279,0,0.0283521,"it. Given the overall superior performance of KnowBert-W+W on the intrinsic evaluations, we focus on it exclusively for evaluation in this section. The main results are included in this section; see the supplementary material for full details. The baselines we compare against are BERTBASE , BERTLARGE , the pre-BERT state of the art, and two contemporaneous papers that add similar types of knowledge to BERT. ERNIE (Zhang et al., 2019) uses TAGME (Ferragina and Scaiella, 2010) to link entities to Wikidata, retrieves the associated entity embeddings, and fuses them into BERTBASE by fine-tuning. Soares et al. (2019) learns relationship representations by fine-tuning BERTLARGE with large scale “matching the blanks” (MTB) pretraining using entity linked text. 50 System ELMo† BERTBASE † BERTLARGE † BERTLARGE †† KnowBert-W+W Accuracy System 57.7 65.4 65.5 69.5 70.9 UFET BERTBASE ERNIE KnowBert-W+W P R F1 68.8 76.4 78.4 78.6 53.3 71.0 72.9 73.7 60.1 73.6 75.6 76.1 Table 7: Test set results for entity typing using the nine general types from (Choi et al., 2018). Table 6: Test set results for the WiC dataset (v1.0). † Pilehvar and Camacho-Collados (2019) †† Wang et al. (2019a) Entity typing We also evaluated Kn"
D19-1005,spitkovsky-chang-2012-cross,0,0.314947,"eral and can be applied to any deep pretrained model including left-to-right and rightto-left LMs such as ELMo and GPT. Formally, BERT accepts as input a sequence of N WordPiece tokens (Sennrich et al., 2016; Wu et al., 2016), (x1 , . . . , xN ), and computes L layers of D-dimensional contextual representations Hi ∈ RN ×D by successively applying non-linear functions Hi = Fi (Hi−1 ). The non-linear function is a multi-headed self-attention layer followed by a position-wise multilayer perceptron (MLP) 44 In practice, these are often implemented using precomputed dictionaries (e.g., CrossWikis; Spitkovsky and Chang, 2012), KB specific rules (e.g., a WordNet lemmatizer), or other heuristics (e.g., string match; Mihaylov and Frank, 2018). Ling et al. (2015) showed that incorporating candidate priors into entity linkers can be a powerful signal, so we optionally allow for the candidate selector to return an associated prior probability for each entity candidate. In some cases, it is beneficial to over-generate potential candidates and add a special NULL entity to each candidate list, thereby allowing the linker to discriminate between actual links and false positive candidates. In this work, the entity candidate"
D19-1005,P19-1226,0,0.0248349,"tive language models (LMs) (Ahn et al., 2017; Yang et al., 2017; Logan et al., 2019) or building entity-centric LMs (Ji et al., 2017). However, these methods introduce latent variables that require full annotation for training, or marginalization. In contrast, we adopt a method that allows training with large amounts of unannotated text. Task-specific KB architectures Other work has focused on integrating KBs into neural architectures for specific downstream tasks (Yang and Mitchell, 2017; Sun et al., 2018; Chen et al., 2018; Bauer et al., 2018; Mihaylov and Frank, 2018; Wang and Jiang, 2019; Yang et al., 2019). Our approach instead uses KBs to learn more generally transferable representations that can be used to improve a variety of downstream tasks. Related Work Pretrained word representations Initial work learning word vectors focused on static word embeddings using multi-task learning objectives (Collobert and Weston, 2008) or corpus level cooccurence statistics (Mikolov et al., 2013a; Pennington et al., 2014). Recently the field has shifted toward learning context-sensitive embeddings (Dai and Le, 2015; Peters et al., 2018; Devlin et al., 2019). We build upon these by incorporating structured k"
D19-1005,D18-1455,0,0.0713268,"Missing"
D19-1005,P17-1132,0,0.0797122,"plicitly model entity spans in the input text and use an entity linker to retrieve relevant entity embeddings from a KB to form knowledge enhanced entity-span representations. Then, the model recontextualizes the entity-span representations with word-toentity attention to allow long range interactions between contextual word representations and all entity spans in the context. The entire KAR is inserted between two layers in the middle of a pretrained model such as BERT. In contrast to previous approaches that integrate external knowledge into task-specific models with task supervision (e.g., Yang and Mitchell, 2017; Chen et al., 2018), our approach learns the entity linkers with self-supervision on unlabeled data. This results in general purpose knowledge enhanced representations that can be applied to a wide range of downstream tasks. Our approach has several other benefits. First, it leaves the top layers of the original model unchanged so we may retain the output loss layers and fine-tune on unlabeled corpora while training the KAR. This also allows us to simply swap out BERT for KnowBert in any downstream application. Second, by taking advantage of the existing high capacity layers in the original m"
D19-1005,D17-1197,0,0.0344531,"luate KnowBert with a mix of intrinsic and extrinsic tasks. Despite being based on the smaller BERTBASE model, the experiments demonstrate improved masked language model perplexity and ability to recall facts over BERTLARGE . The extrinsic evaluations demonstrate improvements for challenging relationship extraction, entity typing and word sense disambiguation datasets, and often outperform other contemporaneous attempts to incorporate external knowledge into BERT. 2 Entity-aware language models Some previous work has focused on adding KBs to generative language models (LMs) (Ahn et al., 2017; Yang et al., 2017; Logan et al., 2019) or building entity-centric LMs (Ji et al., 2017). However, these methods introduce latent variables that require full annotation for training, or marginalization. In contrast, we adopt a method that allows training with large amounts of unannotated text. Task-specific KB architectures Other work has focused on integrating KBs into neural architectures for specific downstream tasks (Yang and Mitchell, 2017; Sun et al., 2018; Chen et al., 2018; Bauer et al., 2018; Mihaylov and Frank, 2018; Wang and Jiang, 2019; Yang et al., 2019). Our approach instead uses KBs to learn more"
D19-1005,D18-1244,0,0.0614284,"Missing"
D19-1005,D17-1004,0,0.0721139,"Missing"
D19-1005,P19-1139,0,0.110116,"is able to 4.3 Downstream Tasks This section evaluates KnowBert on downstream tasks to validate that the addition of knowledge improves performance on tasks expected to benefit from it. Given the overall superior performance of KnowBert-W+W on the intrinsic evaluations, we focus on it exclusively for evaluation in this section. The main results are included in this section; see the supplementary material for full details. The baselines we compare against are BERTBASE , BERTLARGE , the pre-BERT state of the art, and two contemporaneous papers that add similar types of knowledge to BERT. ERNIE (Zhang et al., 2019) uses TAGME (Ferragina and Scaiella, 2010) to link entities to Wikidata, retrieves the associated entity embeddings, and fuses them into BERTBASE by fine-tuning. Soares et al. (2019) learns relationship representations by fine-tuning BERTLARGE with large scale “matching the blanks” (MTB) pretraining using entity linked text. 50 System ELMo† BERTBASE † BERTLARGE † BERTLARGE †† KnowBert-W+W Accuracy System 57.7 65.4 65.5 69.5 70.9 UFET BERTBASE ERNIE KnowBert-W+W P R F1 68.8 76.4 78.4 78.6 53.3 71.0 72.9 73.7 60.1 73.6 75.6 76.1 Table 7: Test set results for entity typing using the nine general"
D19-1110,P07-1056,0,0.0182093,"sso regularization, using increasingly large regularization strengths, resulting in increasingly compact models. As the goal of our experiments is to demonstrate the ability of our approach to reduce the number of parameters, we only consider rational baselines: the same rational RNNs trained without group lasso.6 We manually tune the number and sizes of the baselines WFSAs, and then compare the tradeoff curve between model size and accuracy. We describe our experiments below. For more details, see Appendix A. Data We experiment with the Amazon reviews binary sentiment classification dataset (Blitzer et al., 2007), composed of 22 product categories. We examine the standard dataset (original mix) comprised of a mixture of data from the different categories (Johnson and Zhang, 2015).7 We also examine three of the largest individual categories as separate datasets (kitchen, dvd, and books), following Johnson and Zhang (2015). The three category datasets do not overlap with each other (though they do with original mix), and are significantly different in size (see Appendix A), so we can see how our approach behaves with different amounts of training data. Implementation details To classify text, we concate"
D19-1110,N19-1423,0,0.0339076,"elease our code.1 1 (1) w1 (2) w1 (3) w1 (2) w2 (3) w2 (1) w3 (2) w3 (3) w3 Base structure (1) wˆ1 (2) (1) wˆ2 (2) (1) wˆ3 wˆ1 wˆ2 (0) (0) (0) (0) Learned structure Figure 1: Our approach learns a sparse structure (right hand side) of a base rational RNN (left hand side) where each hidden unit corresponds to a WFSA (in this example, three hidden units, represented by the three rows). Grayed-out, dashed states are removed from the model, while retained states are marked in bold green. Introduction State-of-the-art neural models for NLP are heavily parameterized, requiring hundreds of millions (Devlin et al., 2019) and even billions (Radford et al., 2019) of parameters. While overparameterized models can sometimes be easier to train (Livni et al., 2014), they may also introduce memory problems on small devices and lead to increased carbon emission (Strubell et al., 2019; Schwartz et al., 2019). In feature-based NLP, structured-sparse regularization, in particular the group lasso (Yuan and 1 https://github.com/dodgejesse/ sparsifying_regularizers_for_RRNNs (1) w2 Lin, 2006), has been proposed as a method to reduce model size while preserving performance (Martins et al., 2011). But, in neural NLP, some of"
D19-1110,N15-1011,0,0.0230283,"bility of our approach to reduce the number of parameters, we only consider rational baselines: the same rational RNNs trained without group lasso.6 We manually tune the number and sizes of the baselines WFSAs, and then compare the tradeoff curve between model size and accuracy. We describe our experiments below. For more details, see Appendix A. Data We experiment with the Amazon reviews binary sentiment classification dataset (Blitzer et al., 2007), composed of 22 product categories. We examine the standard dataset (original mix) comprised of a mixture of data from the different categories (Johnson and Zhang, 2015).7 We also examine three of the largest individual categories as separate datasets (kitchen, dvd, and books), following Johnson and Zhang (2015). The three category datasets do not overlap with each other (though they do with original mix), and are significantly different in size (see Appendix A), so we can see how our approach behaves with different amounts of training data. Implementation details To classify text, we concatenate the scores computed by each WFSA, then feed this d-dimensional vector of scores into a linear binary classifier. We use log loss. We experiment with both type-level"
D19-1110,P18-1028,1,0.90485,"rameters (other than, perhaps, division into layers), so the use of structured sparsity at first may appear incongruous. In this paper we show that group lasso can be successfully applied to neural NLP models. We focus on a family of neural models for which the hidden state exhibits a natural structure: rational RNNs (Peng et al., 2018). In a rational RNN, the value of each hidden dimension is the score of a weighted finite-state automaton (WFSA) on (a prefix of) the input vector sequence. This property offers a natural grouping of the transition function parameters for each WFSA. As shown by Schwartz et al. (2018) and Peng et al. (2018), a variety of state-of-the-art neural architectures are rational (Lei et al., 2017; Bradbury et al., 2017; Foerster et al., 2017, inter alia), so learning parameterefficient rational RNNs is of practical value. We 1179 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 1179–1184, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics also take advantage of the natural interpretation of rational RNNs as “soft” patterns (Schw"
D19-1110,P19-1355,0,0.0173307,"t hand side) where each hidden unit corresponds to a WFSA (in this example, three hidden units, represented by the three rows). Grayed-out, dashed states are removed from the model, while retained states are marked in bold green. Introduction State-of-the-art neural models for NLP are heavily parameterized, requiring hundreds of millions (Devlin et al., 2019) and even billions (Radford et al., 2019) of parameters. While overparameterized models can sometimes be easier to train (Livni et al., 2014), they may also introduce memory problems on small devices and lead to increased carbon emission (Strubell et al., 2019; Schwartz et al., 2019). In feature-based NLP, structured-sparse regularization, in particular the group lasso (Yuan and 1 https://github.com/dodgejesse/ sparsifying_regularizers_for_RRNNs (1) w2 Lin, 2006), has been proposed as a method to reduce model size while preserving performance (Martins et al., 2011). But, in neural NLP, some of the most widely used models—LSTMs (Hochreiter and Schmidhuber, 1997) and GRUs (Cho et al., 2014)—do not have an obvious, intuitive notion of “structure” in their parameters (other than, perhaps, division into layers), so the use of structured sparsity at firs"
D19-1110,D11-1139,1,0.775047,"quiring hundreds of millions (Devlin et al., 2019) and even billions (Radford et al., 2019) of parameters. While overparameterized models can sometimes be easier to train (Livni et al., 2014), they may also introduce memory problems on small devices and lead to increased carbon emission (Strubell et al., 2019; Schwartz et al., 2019). In feature-based NLP, structured-sparse regularization, in particular the group lasso (Yuan and 1 https://github.com/dodgejesse/ sparsifying_regularizers_for_RRNNs (1) w2 Lin, 2006), has been proposed as a method to reduce model size while preserving performance (Martins et al., 2011). But, in neural NLP, some of the most widely used models—LSTMs (Hochreiter and Schmidhuber, 1997) and GRUs (Cho et al., 2014)—do not have an obvious, intuitive notion of “structure” in their parameters (other than, perhaps, division into layers), so the use of structured sparsity at first may appear incongruous. In this paper we show that group lasso can be successfully applied to neural NLP models. We focus on a family of neural models for which the hidden state exhibits a natural structure: rational RNNs (Peng et al., 2018). In a rational RNN, the value of each hidden dimension is the score"
D19-1110,D18-1152,1,0.906466,"Carnegie Mellon University, Pittsburgh, PA, USA ♦ Allen Institute for Artificial Intelligence, Seattle, WA, USA ♠ Paul G. Allen School of Computer Science & Engineering, University of Washington, Seattle, WA, USA jessed@cs.cmu.edu roys@allenai.org {hapeng,nasmith}@cs.washington.edu Abstract Neural models for NLP typically use large numbers of parameters to reach state-of-theart performance, which can lead to excessive memory usage and increased runtime. We present a structure learning method for learning sparse, parameter-efficient NLP models. Our method applies group lasso to rational RNNs (Peng et al., 2018), a family of models that is closely connected to weighted finitestate automata (WFSAs). We take advantage of rational RNNs’ natural grouping of the weights, so the group lasso penalty directly removes WFSA states, substantially reducing the number of parameters in the model. Our experiments on a number of sentiment analysis datasets, using both GloVe and BERT embeddings, show that our approach learns neural structures which have fewer parameters without sacrificing performance relative to parameter-rich baselines. Our method also highlights the interpretable properties of rational RNNs. We sh"
D19-1110,D14-1162,0,0.0997069,"Missing"
D19-1159,D14-1162,0,0.0821077,"puting top-k rollouts. 3 P RETRAINED LM S AND S TOCHASTIC S AMPLING 1495 • fθx→e : x → e, where x = [x1 , · · · , xL ] is represented as its (contextualized) word embedding form e = [e1 , · · · , eL ], with ei as the representation for word xi ; • fθe→z : e → z t : For each embedded instruction e, we ground its representations as ci,t for state st via neural attention. To handle language variability, one may aggregate features of multiple instructions Ct = {ci,t }M i=1 1 PM into a single joint feature z t = M c .4 i,t i=1 Previous methods in VLN learn e either from pretrained word embeddings (Pennington et al., 2014) which do not take into account word context, or from scratch. As a result, their representations do not capture contextual information within each instruction. More importantly, they tend to overfit the training instructions associated with seen environments, limiting their utility in unseen environments. To remedy these issues, we propose to represent e with contextualized word embeddings produced using large-scale pretrained language models, such as BERT and GPT. Instruction Encoder. The agent’s memory vector ht−1 captures the perception and action history and is used to attend to the instr"
D19-1159,N19-1268,0,0.10248,". gio et al., 2015). Two widely used training strategies are student-forcing and teacher-forcing (described in detail in Section 2.2). It is well-known that the sequence length determines which training strategy is more effective. In the VLN literature, student-forcing has been widely used, as early work (Anderson et al., 2018) used long trajectories (up to 20 steps) with a simple discrete action space. Most recent work, however, has relied on a panoramic action space (Fried et al., 2018) in which most trajectories are only up to seven steps long. In such cases, teacher-forcing is preferable (Tan et al., 2019). Neither strategy is perfect: teacher-forcing has exposure bias, while studentforcing’s random actions can cause an agent to deviate far from the correct path, rendering the original instruction invalid.2 To tackle these challenges, we have developed two techniques to enable the agent to navigate more efficiently. For the first challenge, we leverage the recent large-scale pretrained language models, BERT (Devlin et al., 2019) and GPT (Radford et al., 2018), to improve the agent’s robustness in unseen environments. We show that large-scale language-only pretraining improves generalization in"
D19-1159,N19-1423,0,\N,Missing
D19-1159,N19-1197,1,\N,Missing
D19-1224,Q17-1033,0,0.0166996,"tion. They recommend detailing experimental results found throughout the research process in a time-stamped document, as is done in other experimental science fields. Our work formalizes these issues and provides an ac2192 tionable set of recommendations to address them. Reproducibility issues relating to standard data splits (Schwartz et al., 2011; Gorman and Bedrick, 2019; Recht et al., 2019a,b) have surfaced in a number of areas. Shuffling standard training, validation, and test set splits led to a drop in performance, and in a number of cases the inability to reproduce rankings of models. Dror et al. (2017) studied reproducibility in the context of consistency among multiple comparisons. Limited community standards exist for documenting datasets and models. To address this, Gebru et al. (2018) recommend pairing new datasets with a “datasheet” which includes information such as how the data was collected, how it was cleaned, and the motivation behind building the dataset. Similarly, Mitchell et al. (2019) advocate for including a “model card” with trained models which document training data, model assumptions, and intended use, among other things. Our recommendations in §5 are meant to document r"
D19-1224,W18-2501,0,0.0308221,"tivity analysis in Zhang and Wallace (2015). We run 50 trials of random hyperparameter search for each classifier. Our results (Fig. 1) confirm previous findings (Zhang and Wallace, 2015): under a budget of fewer than 10 hyperparameter 11 https://github.com/allenai/ show-your-work 0.90 0.88 0.86 0.84 0.82 0.80 0.78 GloVe + ELMo (FT) GloVe + ELMo (FR) GloVe 0.76 For each experiment, we document the hyperparameter search space, hardware, average runtime, number of samples, and links to model implementations. We use public implementations for all models in our experiments, primarily in AllenNLP (Gardner et al., 2018). We use Tune (Liaw et al., 2018) to run parallel evaluations of uniformly sampled hyperparameter values. 4.2 SST (binary) 0.92 Expected validation accuracy match their reported performance. We find these budget estimates vary drastically. Consistently, we see that the best model is a function of the budget. We publicly release the search space and training configurations used for each case study. 11 Note that we do not report test performance in our experiments, as our purpose is not to establish a benchmark level for a model, but to demonstrate the utility of expected validation performance"
D19-1224,P19-1267,0,0.0285409,"ectly attributing empirical gains to modeling choices when they came from other sources such as hyperparameter tuning. Sculley et al. (2018) list examples of similar evaluation issues, and suggest encouraging stronger standards for empirical evaluation. They recommend detailing experimental results found throughout the research process in a time-stamped document, as is done in other experimental science fields. Our work formalizes these issues and provides an ac2192 tionable set of recommendations to address them. Reproducibility issues relating to standard data splits (Schwartz et al., 2011; Gorman and Bedrick, 2019; Recht et al., 2019a,b) have surfaced in a number of areas. Shuffling standard training, validation, and test set splits led to a drop in performance, and in a number of cases the inability to reproduce rankings of models. Dror et al. (2017) studied reproducibility in the context of consistency among multiple comparisons. Limited community standards exist for documenting datasets and models. To address this, Gebru et al. (2018) recommend pairing new datasets with a “datasheet” which includes information such as how the data was collected, how it was cleaned, and the motivation behind building"
D19-1224,P17-1152,0,0.031975,"the hyperparameters they did list in addition to standard choices (such as the learning rate). In neither case do they report the method used to tune the hyperparameters, and we suspect they tuned them manually. Our experiments here are meant give an idea of the budget that would be required to reproduce their results or to apply their models to other datasets under random hyperparameter value selection. SciTail When introducing the SciTail textual entailment dataset, Khot et al. (2018) compared four models: an n-gram baseline, which measures word-overlap as an indicator of entailment, ESIM (Chen et al., 2017), a sequence-based entailment model, DAM (Parikh et al., 2016), a bagof-words entailment model, and their proposed model, DGEM (Khot et al., 2018), a graph-based structured entailment model. Their conclusion was that DGEM outperforms the other models. 12 Peters et al. (2019) use a BCN with frozen embeddings and a BiLSTM BCN for fine-tuning. We conducted experiments with both a BCN and a BiLSTM with frozen and finetuned embeddings, and found our conclusions to be consistent. We report the full hyperparameter search space, which matched Peters et al. (2019) as closely as their reporting allowed,"
D19-1224,N19-1423,0,0.0419714,"ng to a model on a leaderboard is difficult if they only report test scores. For example, on the GLUE benchmark (Wang et al., 2018), the differences in test set performance between the top performing models can be on the order of a tenth of a percent, while the difference between test and validation performance might be one percent or larger. Verifying that a new implementation matches established performance requires submitting to the leaderboard, wasting test evaluations. Thus, we recommend leaderboards report validation performance for models evaluated on test sets. As an example, consider Devlin et al. (2019), which introduced BERT and reported state-of-theart results on the GLUE benchmark. The authors provide some details about the experimental setup, but do not report a specific budget. Subsequent work which extended BERT (Phang et al., 2018) included distributions of validation results, and we highlight this as a positive example of how to report experimental results. To achieve comparable test performance to Devlin et al. (2019), the authors report the best of twenty or one hundred random initializations. Their validation performance reporting not only illuminates the budget required to fine-t"
D19-1224,D16-1244,0,0.0858124,"Missing"
D19-1224,D14-1162,0,0.0820497,"s K times (where K is large), and crete random variables, 2188 4.1 Experimental Details Validating Previous Findings We start by applying our technique on a text classification task in order to confirm a well-established observation (Yogatama and Smith, 2015): logistic regression has reasonable performance with minimal hyperparameter tuning, but a well-tuned convolutional neural network (CNN) can perform better. We experiment with the fine-grained Stanford Sentiment Treebank text classification dataset (Socher et al., 2013). For the CNN classifier, we embed the text with 50-dim GloVe vectors (Pennington et al., 2014), feed the vectors to a ConvNet encoder, and feed the output representation into a softmax classification layer. We use the scikit-learn implementation of logistic regression with bag-of-word counts and a linear classification layer. The hyperparameter spaces HCNN and HLR are detailed in Appendix B. For logistic regression we used bounds suggested by Yogatama and Smith (2015), which include term weighting, ngrams, stopwords, and learning rate. For the CNN we follow the hyperparameter sensitivity analysis in Zhang and Wallace (2015). We run 50 trials of random hyperparameter search for each cla"
D19-1224,W19-4302,1,0.871645,"plot shows that for each approach (GloVe, ELMo frozen, and ELMo fine-tuned), there exists a budget for which it is preferable. search trials, logistic regression achieves a higher expected validation accuracy than the CNN. As the budget increases, the CNN gradually improves to a higher overall expected validation accuracy. For all budgets, logistic regression has lower variance, so may be a more suitable approach for fast prototyping. 4.3 Contextual Representations We next explore how computational budget affects the performance of contextual embedding models (Peters et al., 2018). Recently, Peters et al. (2019) compared two methods for using contextual representations for downstream tasks: feature extraction, where features are fixed after pretraining and passed into a task-specific model, or fine-tuning, where they are updated during task training. Peters et al. (2019) found that feature extraction is preferable to fine-tuning ELMo embeddings. Here we set to explore whether this conclusion depends on the experimental budget. Closely following their experimental setup, in Fig. 2 we show the expected performance of the biattentive classification network (BCN; McCann et al., 2017) with three embedding"
D19-1224,N18-1202,0,0.150615,"e mance (V ∗ ) matches a desired performance level. the mean and variance of the best validation perWe present three examples that demonstrate these formance. The bootstrap (Efron and Tibshirani, use cases. First, we reproduce previous findings 1994) is a general method which can be used to that compared different models for text classifiestimate statistics that do not have a closed form. cation. Second, we explore the time vs. perforThe bootstrap process is as follows: draw N i.i.d. mance tradeoff of models that use contextual word samples (in our case, N model evaluations). From embeddings (Peters et al., 2018). Third, from these N points, sample n points (with replacetwo previously published papers, we examine the ment), and compute the statistic of interest (e.g., budget required for our expected performance to the max). Do this K times (where K is large), and crete random variables, 2188 4.1 Experimental Details Validating Previous Findings We start by applying our technique on a text classification task in order to confirm a well-established observation (Yogatama and Smith, 2015): logistic regression has reasonable performance with minimal hyperparameter tuning, but a well-tuned convolutional ne"
D19-1224,P11-1067,1,0.744768,"rning, including incorrectly attributing empirical gains to modeling choices when they came from other sources such as hyperparameter tuning. Sculley et al. (2018) list examples of similar evaluation issues, and suggest encouraging stronger standards for empirical evaluation. They recommend detailing experimental results found throughout the research process in a time-stamped document, as is done in other experimental science fields. Our work formalizes these issues and provides an ac2192 tionable set of recommendations to address them. Reproducibility issues relating to standard data splits (Schwartz et al., 2011; Gorman and Bedrick, 2019; Recht et al., 2019a,b) have surfaced in a number of areas. Shuffling standard training, validation, and test set splits led to a drop in performance, and in a number of cases the inability to reproduce rankings of models. Dror et al. (2017) studied reproducibility in the context of consistency among multiple comparisons. Limited community standards exist for documenting datasets and models. To address this, Gebru et al. (2018) recommend pairing new datasets with a “datasheet” which includes information such as how the data was collected, how it was cleaned, and the"
D19-1224,D13-1170,0,0.00488383,"atistic of interest (e.g., budget required for our expected performance to the max). Do this K times (where K is large), and crete random variables, 2188 4.1 Experimental Details Validating Previous Findings We start by applying our technique on a text classification task in order to confirm a well-established observation (Yogatama and Smith, 2015): logistic regression has reasonable performance with minimal hyperparameter tuning, but a well-tuned convolutional neural network (CNN) can perform better. We experiment with the fine-grained Stanford Sentiment Treebank text classification dataset (Socher et al., 2013). For the CNN classifier, we embed the text with 50-dim GloVe vectors (Pennington et al., 2014), feed the vectors to a ConvNet encoder, and feed the output representation into a softmax classification layer. We use the scikit-learn implementation of logistic regression with bag-of-word counts and a linear classification layer. The hyperparameter spaces HCNN and HLR are detailed in Appendix B. For logistic regression we used bounds suggested by Yogatama and Smith (2015), which include term weighting, ngrams, stopwords, and learning rate. For the CNN we follow the hyperparameter sensitivity anal"
D19-1224,P19-1355,0,0.0224817,"t performing model is a function of the computational budget. In §4.4 we show how our approach can be used to estimate the budget that went into obtaining previous results; in one example, we see a too-small budget for baselines, while in another we estimate a budget of about 18 GPU days was used (but not reported). Previous work on reporting validation performance used the bootstrap to approximate the mean and variance of the best performing model (Lucic et al., 2018); in §3.2 we show that our ap2 Recent work has also called attention to the environmental cost of intensive model exploration (Strubell et al., 2019). 3 We use the term performance as a general evaluation measure, e.g., accuracy, F1 , etc. 4 We leave forecasting performance with larger budgets n > N to future work. proach computes these values with strictly less error than the bootstrap. We conclude by presenting a set of recommendations for researchers that will improve scientific reporting over current practice. We emphasize this work is about reporting, not about running additional experiments (which undoubtedly can improve evidence in comparisons among models). Our reporting recommendations aim at reproducibility and improved understan"
D19-1224,W18-5446,0,0.0244779,"ling for the amount of training data, which is an established norm in NLP research. helped to mitigate the so-called “replication crisis” happening in fields such as psychology and medicine (Ioannidis, 2005; Gelman and Loken, 2014). Unfortunately, leaderboards can create additional reproducibility issues (Rogers, 2019). First, leaderboards obscure the budget that was used to tune hyperparameters, and thus the amount of work required to apply a model to a new dataset. Second, comparing to a model on a leaderboard is difficult if they only report test scores. For example, on the GLUE benchmark (Wang et al., 2018), the differences in test set performance between the top performing models can be on the order of a tenth of a percent, while the difference between test and validation performance might be one percent or larger. Verifying that a new implementation matches established performance requires submitting to the leaderboard, wasting test evaluations. Thus, we recommend leaderboards report validation performance for models evaluated on test sets. As an example, consider Devlin et al. (2019), which introduced BERT and reported state-of-theart results on the GLUE benchmark. The authors provide some de"
D19-1224,D15-1251,1,0.874752,"aw N i.i.d. mance tradeoff of models that use contextual word samples (in our case, N model evaluations). From embeddings (Peters et al., 2018). Third, from these N points, sample n points (with replacetwo previously published papers, we examine the ment), and compute the statistic of interest (e.g., budget required for our expected performance to the max). Do this K times (where K is large), and crete random variables, 2188 4.1 Experimental Details Validating Previous Findings We start by applying our technique on a text classification task in order to confirm a well-established observation (Yogatama and Smith, 2015): logistic regression has reasonable performance with minimal hyperparameter tuning, but a well-tuned convolutional neural network (CNN) can perform better. We experiment with the fine-grained Stanford Sentiment Treebank text classification dataset (Socher et al., 2013). For the CNN classifier, we embed the text with 50-dim GloVe vectors (Pennington et al., 2014), feed the vectors to a ConvNet encoder, and feed the output representation into a softmax classification layer. We use the scikit-learn implementation of logistic regression with bag-of-word counts and a linear classification layer. T"
D19-1224,D16-1264,0,0.0282532,"deviation, within the observed range of values. It takes about 18 days (55 hyperparameter trials) for the expected performance to match the reported results. Our results (Fig. 3) show that the different models require different budgets to reach their reported performance in expectation, ranging from 2 (ngram) to 20 (DGEM). Moreover, providing a large budget for each approach improves performance substantially over reported numbers. Finally, under different computation budgets, the top performing model changes (though the neural models are similar). SQuAD Next, we turn our attention to SQuAD (Rajpurkar et al., 2016) and report performance of the commonly-used BiDAF model (Seo et al., 2017). The set of hyperparameters we tune covers those mentioned in addition to standard choices (details in Appendix D). We see in Fig. 4 that we require a budget of 18 GPU days in order for the expected maximum validation performance to match the value reported in the original paper. This suggests that some combination of prior intuition and extensive hyperparameter tuning were used by the original authors, though neither were reported. 5 Recommendations Experimental results checklist The findings discussed in this paper a"
D19-1224,D17-1035,0,0.0292507,"dotted lines). https://github.com/allenai/allentune 2185 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 2185–2194, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics tained purely through more intensive hyperparameter tuning (Melis et al., 2018; Lipton and Steinhardt, 2018).2 Moreover, recent investigations into “state-ofthe-art” claims have found competing methods to only be comparable, without clear superiority, even against baselines (Reimers and Gurevych, 2017; Lucic et al., 2018; Li and Talwalkar, 2019); this has exposed the need for reporting more than a single point estimate of performance. Echoing calls for more rigorous scientific practice in machine learning (Lipton and Steinhardt, 2018; Sculley et al., 2018), we draw attention to the weaknesses in current reporting practices and propose solutions which would allow for fairer comparisons and improved reproducibility. Our primary technical contribution is the introduction of a tool for reporting validation results in an easily interpretable way: expected validation performance of the best mode"
D19-1376,N18-1202,0,0.0596875,"Missing"
D19-1376,J91-3004,0,0.442397,"w that LMs can benefit from syntacticallyinspired encoding of the context. We introduce PaLM (parser and language model; Fig. 1), a novel hybrid model combining an RNN language model with a constituency parser. The LM in PaLM attends over spans of tokens, implicitly learning which syntactic constituents are likely. A span-based parser is then derived from the attention information (Stern et al., 2017). PaLM has several benefits. First, it is an intuitive and lightweight way of incorporating structural information (§2.1), requiring no marginal inference, which can be computationally expensive (Jelinek and Lafferty, 1991; Chelba and Jelinek, 1998; Roark, 2001; Dyer et al., 2016; Buys and Blunsom, 2018; Kim et al., 2019, inter alia). Second, the attention can be syntactically informed, in the sense that the attention component can optionally be supervised using syntactic annotations, either through pretraining or by joint training with the LM (§2.2). Last, PaLM can derive an unsupervised constituency parser (§2.2), whose parameters are estimated purely using the language modeling objective. To demonstrate the empirical benefits of PaLM, we experiment with language modeling (§3). PaLM outperforms the AWD-LSTM m"
D19-1376,N19-1114,0,0.0226965,"age model; Fig. 1), a novel hybrid model combining an RNN language model with a constituency parser. The LM in PaLM attends over spans of tokens, implicitly learning which syntactic constituents are likely. A span-based parser is then derived from the attention information (Stern et al., 2017). PaLM has several benefits. First, it is an intuitive and lightweight way of incorporating structural information (§2.1), requiring no marginal inference, which can be computationally expensive (Jelinek and Lafferty, 1991; Chelba and Jelinek, 1998; Roark, 2001; Dyer et al., 2016; Buys and Blunsom, 2018; Kim et al., 2019, inter alia). Second, the attention can be syntactically informed, in the sense that the attention component can optionally be supervised using syntactic annotations, either through pretraining or by joint training with the LM (§2.2). Last, PaLM can derive an unsupervised constituency parser (§2.2), whose parameters are estimated purely using the language modeling objective. To demonstrate the empirical benefits of PaLM, we experiment with language modeling (§3). PaLM outperforms the AWD-LSTM model (Merity et al., 2018) on both the Penn Treebank 3644 Proceedings of the 2019 Conference on Empi"
D19-1376,D17-1018,0,0.0285876,"t step t, PaLM attends over the spans ending at t − 1, up to a maximum length m, i.e., t−1 {[i, t − 1]}i=t−m .3 Essentially, this can be seen as splitting the prefix span [t − m, t − 1] into two, and attending over the one on the right. Such a span attention mechanism is inspired by the top-down greedy span parser of Stern et al. (2017), which recursively divides phrases. In §2.2, we will use a similar algorithm to derive a constituency parser from the span attention weights. Bidirectional span representation with rational RNNs. Meaningful span representations are crucial in span-based tasks (Lee et al., 2017; Peng et al., 2018c; Swayamdipta et al., 2018, inter 1 We experiment with a strong LSTM implementation for language modeling (Merity et al., 2018), see §3. 2 Standard token-based self-attention naturally relates to dependency structures through head selection (Strubell et al., 2018). In a left-to-right factored language model, dependencies are less natural if we want to allow a child to precede its parent. 3 m is set to 20. This reduces the number of considered spans from O(n2 ) to O(mn). Besides practical concerns, it makes less sense if a phrase goes beyond one single sentence (the average"
D19-1376,J93-2004,0,0.0649163,"2.2). Last, PaLM can derive an unsupervised constituency parser (§2.2), whose parameters are estimated purely using the language modeling objective. To demonstrate the empirical benefits of PaLM, we experiment with language modeling (§3). PaLM outperforms the AWD-LSTM model (Merity et al., 2018) on both the Penn Treebank 3644 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 3644–3651, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics (PTB; Marcus et al., 1993) and WikiText-2 (Merity et al., 2017) datasets by small but consistent margins in the unsupervised setup. When the parser is trained jointly with the language model, we see additional perplexity reductions in both cases. Our implementation is available at https: //github.com/Noahs-ARK/PaLM. 2 PaLM—Parser and Language Model We describe PaLM in detail. At its core is an attention component, gathering the representations of preceding spans at each time step. Similar to self-attention, PaLM can be implemented on top of RNN encoders (Parikh et al., 2016), or as it is (Vaswani et al., 2017). Here we"
D19-1376,P17-1076,0,0.333111,"akanok et al., 2008; Das et al., 2012, inter alia) and recent state-of-the-art neural models (Dyer et al., 2016; Swayamdipta et al., 2018; Peng et al., 2018b; Strubell et al., 2018, inter alia). In this paper we show that LMs can benefit from syntacticallyinspired encoding of the context. We introduce PaLM (parser and language model; Fig. 1), a novel hybrid model combining an RNN language model with a constituency parser. The LM in PaLM attends over spans of tokens, implicitly learning which syntactic constituents are likely. A span-based parser is then derived from the attention information (Stern et al., 2017). PaLM has several benefits. First, it is an intuitive and lightweight way of incorporating structural information (§2.1), requiring no marginal inference, which can be computationally expensive (Jelinek and Lafferty, 1991; Chelba and Jelinek, 1998; Roark, 2001; Dyer et al., 2016; Buys and Blunsom, 2018; Kim et al., 2019, inter alia). Second, the attention can be syntactically informed, in the sense that the attention component can optionally be supervised using syntactic annotations, either through pretraining or by joint training with the LM (§2.2). Last, PaLM can derive an unsupervised cons"
D19-1376,D18-1548,0,0.147359,"Peters et al., 2018; Radford et al., 2019), which benefit many NLP tasks such as text classification (Howard and Ruder, 2018) and dataset creation (Zellers et al., 2018). Language models are typically trained on large amounts of raw text, and therefore do not explicitly encode any notion of structural information. Structures in the form of syntactic trees have been shown to benefit both classical NLP models (Gildea and Palmer, 2002; Punyakanok et al., 2008; Das et al., 2012, inter alia) and recent state-of-the-art neural models (Dyer et al., 2016; Swayamdipta et al., 2018; Peng et al., 2018b; Strubell et al., 2018, inter alia). In this paper we show that LMs can benefit from syntacticallyinspired encoding of the context. We introduce PaLM (parser and language model; Fig. 1), a novel hybrid model combining an RNN language model with a constituency parser. The LM in PaLM attends over spans of tokens, implicitly learning which syntactic constituents are likely. A span-based parser is then derived from the attention information (Stern et al., 2017). PaLM has several benefits. First, it is an intuitive and lightweight way of incorporating structural information (§2.1), requiring no marginal inference, which"
D19-1376,D18-1412,1,0.910151,"cluding, most notably, contextual embeddings (Peters et al., 2018; Radford et al., 2019), which benefit many NLP tasks such as text classification (Howard and Ruder, 2018) and dataset creation (Zellers et al., 2018). Language models are typically trained on large amounts of raw text, and therefore do not explicitly encode any notion of structural information. Structures in the form of syntactic trees have been shown to benefit both classical NLP models (Gildea and Palmer, 2002; Punyakanok et al., 2008; Das et al., 2012, inter alia) and recent state-of-the-art neural models (Dyer et al., 2016; Swayamdipta et al., 2018; Peng et al., 2018b; Strubell et al., 2018, inter alia). In this paper we show that LMs can benefit from syntacticallyinspired encoding of the context. We introduce PaLM (parser and language model; Fig. 1), a novel hybrid model combining an RNN language model with a constituency parser. The LM in PaLM attends over spans of tokens, implicitly learning which syntactic constituents are likely. A span-based parser is then derived from the attention information (Stern et al., 2017). PaLM has several benefits. First, it is an intuitive and lightweight way of incorporating structural information (§2"
D19-1376,D18-1489,0,0.017797,"yields no improvement over the baseline. Unsupervised constituency parsing. We evaluate the parser component of PaLM-U on WSJ40. It uses the same data as in language modeling, but filters out sentences longer than 40 tokens after 10 Preliminary experiments show that including the span attention after the last layer yields similar empirical results, but is more sensitive to hyperparameters. 11 We use the WSJ portion of PTB for parsing annotations. 12 We set scores to m, m − 1, . . . , 1, before the softmax. 13 Several recent works report better language modeling perplexity (Yang et al., 2019; Takase et al., 2018; Dai et al., 2019, inter alia). Their contribution is orthogonal to ours and not head-to-head comparable to the models in the table. # Params. Dev. Test AWD-LSTM 24M 60.0 57.3 PRPN ON-LSTM 25M 58.3 62.0 56.2 PaLM-U PaLM-RB PaLM-S 24M 24M 24M 58.6 60.1 57.9 56.4 57.5 55.5 Table 1: PTB language modeling perplexity (lower is better). Bold fonts indicate best performance.13 Model # Params. Dev. Test AWD-LSTM 33M 68.6 65.8 PaLM-U PaLM-S 36M 36M 68.4 65.5 65.4 63.2 Table 2: WikiText-2 language modeling perplexity (lower is better). Bold fonts indicate best performance. punctuation removal. The mode"
D19-1376,D16-1244,0,0.109695,"Missing"
D19-1376,D18-1152,1,0.938629,"textual embeddings (Peters et al., 2018; Radford et al., 2019), which benefit many NLP tasks such as text classification (Howard and Ruder, 2018) and dataset creation (Zellers et al., 2018). Language models are typically trained on large amounts of raw text, and therefore do not explicitly encode any notion of structural information. Structures in the form of syntactic trees have been shown to benefit both classical NLP models (Gildea and Palmer, 2002; Punyakanok et al., 2008; Das et al., 2012, inter alia) and recent state-of-the-art neural models (Dyer et al., 2016; Swayamdipta et al., 2018; Peng et al., 2018b; Strubell et al., 2018, inter alia). In this paper we show that LMs can benefit from syntacticallyinspired encoding of the context. We introduce PaLM (parser and language model; Fig. 1), a novel hybrid model combining an RNN language model with a constituency parser. The LM in PaLM attends over spans of tokens, implicitly learning which syntactic constituents are likely. A span-based parser is then derived from the attention information (Stern et al., 2017). PaLM has several benefits. First, it is an intuitive and lightweight way of incorporating structural information (§2.1), requiring no m"
D19-1376,D18-1009,1,0.821835,"e taking the top scoring attended span (red solid line) and the prefix leading to it. It then recursively splits the two sub-spans using the same procedure (line 3). Finally, spans of length two are trivially split into terminal nodes (line 4). Introduction Recent language models have shown very strong data-fitting performance (Jozefowicz et al., 2016; Merity et al., 2018). They offer useful products including, most notably, contextual embeddings (Peters et al., 2018; Radford et al., 2019), which benefit many NLP tasks such as text classification (Howard and Ruder, 2018) and dataset creation (Zellers et al., 2018). Language models are typically trained on large amounts of raw text, and therefore do not explicitly encode any notion of structural information. Structures in the form of syntactic trees have been shown to benefit both classical NLP models (Gildea and Palmer, 2002; Punyakanok et al., 2008; Das et al., 2012, inter alia) and recent state-of-the-art neural models (Dyer et al., 2016; Swayamdipta et al., 2018; Peng et al., 2018b; Strubell et al., 2018, inter alia). In this paper we show that LMs can benefit from syntacticallyinspired encoding of the context. We introduce PaLM (parser and language"
D19-1425,W19-5201,0,0.0146563,"arners to master. These challenges are evident from the most indicative words of our model. Observe also that the LO - TOP -50 model is somewhere in the middle: it includes some proper nouns (including geographical terms such as eu or us) but also several function words. A more detailed analysis of these observations is left for future work. Recently, there has been a debate on whether attention can be used to explain model decisions (Serrano and Smith, 2019; Jain and Wallace, 2019; Wiegreffe and Pinter, 2019), we thus present additional analysis of our proposed method based on saliency maps (Ding et al., 2019). Saliency maps have been shown to better capture word alignment than attention probabilities in neural machine translation. This method is based on computing the gradient of the probability of the predicted label with respect to each word in the input text and normalizing the gradient to obtain probabilities. We use saliency maps to generate lexicons similar to the ones generated using attention. As shown in table 8, the top indicative words for baseline and LO - TOP -50 follow a similar pattern as the ones obtained with attention scores. In line with results in Table 7, salient words for ALT"
D19-1425,D18-1002,0,0.413214,"to discover stylistic features present in the input that are indicative of the author’s L1. However, a model trained to predict L1 is likely to predict that a person is, say, a native Greek speaker, if the texts authored by that person mention Greece, because the training data exhibits such topical correlations (§2). This problem is the focus of our work, and we address it in two steps. First, we introduce a novel method for representing latent confounds. Recent relevant work in the area of domain adaptation (Ganin et al., 2016) and deconfounding for text classification (Pryzant et al., 2018; Elazar and Goldberg, 2018) assumes that the set of confounds is known a priori, and their values are given as part of the training data. This is an unrealistic setting that limits the applicability of such models in real world scenarios. In contrast, we introduce a new method, based on log-odds ratio with Dirichlet prior (Monroe et al., 2008), for identifying and representing latent confounds as probability distributions (§3). Second, we propose a novel alternating learning procedure with multiple adversarial discriminators, inspired by adversarial learning (Goodfellow et al., 2014), that demotes latent confounds and r"
D19-1425,D18-1395,1,0.84017,"itialized at random and learned) and passed these embeddings to a bidirectional LSTM encoder (one layer for each direction) with attention (h(x); Pryzant et al., 2018). Each LSTM layer had a hidden dimension of 128. We used two layered feed forward networks with a tanh activation function in the middle layer (of size 256), followed by a softmax in the final layer, as c(.) and adv(.). 5.3 Baselines We consider several baselines that are intended to capture the stylistic features of the texts, explicitly avoiding content. 4157 Linear classifier with content-independent features (LR) Replicating Goldin et al. (2018), we trained a logistic regression classifier with three types of features: function words, POS trigrams, and sentence length, all of which are reflective of the style of writing. We deliberately avoided using content features (e.g., word frequencies). Classification with no adversary on masked texts (LO - TOP -K) We mask the top-K words (based on log-odds scores) in both the train and the test sets (as in §2); we train the classification model again without training adv(.). After masking the top words, we expect patterns of writing style (and, therefore, L1) to become more apparent. Adversari"
D19-1425,N19-1357,0,0.0385091,"The distribution of determiners is also a challenge for nonnatives, and 4159 the correct usage of the in particular is quite hard for learners to master. These challenges are evident from the most indicative words of our model. Observe also that the LO - TOP -50 model is somewhere in the middle: it includes some proper nouns (including geographical terms such as eu or us) but also several function words. A more detailed analysis of these observations is left for future work. Recently, there has been a debate on whether attention can be used to explain model decisions (Serrano and Smith, 2019; Jain and Wallace, 2019; Wiegreffe and Pinter, 2019), we thus present additional analysis of our proposed method based on saliency maps (Ding et al., 2019). Saliency maps have been shown to better capture word alignment than attention probabilities in neural machine translation. This method is based on computing the gradient of the probability of the predicted label with respect to each word in the input text and normalizing the gradient to obtain probabilities. We use saliency maps to generate lexicons similar to the ones generated using attention. As shown in table 8, the top indicative words for baseline and LO -"
D19-1425,S18-2005,0,0.0313045,"on about the confound. We show that this model generalizes better and learns features that are indicative of the writing style rather than the content.1 1 Introduction Text classification systems based on neural networks are biased towards learning frequent spurious correlations in the training data that may be confounds in the actual classification task (Leino et al., 2019). A major challenge in building such systems is to discover features that are not just correlated with the signals in the training data, but are true indicators of these signals, and therefore generalize well. For example, Kiritchenko and Mohammad (2018) found that sentiment analysis systems implicitly overfit to demographic confounds, systematically amplifying the intensity ratings of posts 1 The code is available at: https://github.com/ Sachin19/adversarial-classify written by women. Zhao et al. (2017) showed that visual semantic role labeling models implicitly capture actions stereotypically associated with men or women (e.g., women are cooking and men are fixing a faucet), and in cases of higher model uncertainty assign stereotypical labels to actions and objects, thereby amplifying social biases found in the training data. We focus on th"
D19-1425,N18-1146,0,0.149534,"e aim of this task is to discover stylistic features present in the input that are indicative of the author’s L1. However, a model trained to predict L1 is likely to predict that a person is, say, a native Greek speaker, if the texts authored by that person mention Greece, because the training data exhibits such topical correlations (§2). This problem is the focus of our work, and we address it in two steps. First, we introduce a novel method for representing latent confounds. Recent relevant work in the area of domain adaptation (Ganin et al., 2016) and deconfounding for text classification (Pryzant et al., 2018; Elazar and Goldberg, 2018) assumes that the set of confounds is known a priori, and their values are given as part of the training data. This is an unrealistic setting that limits the applicability of such models in real world scenarios. In contrast, we introduce a new method, based on log-odds ratio with Dirichlet prior (Monroe et al., 2008), for identifying and representing latent confounds as probability distributions (§3). Second, we propose a novel alternating learning procedure with multiple adversarial discriminators, inspired by adversarial learning (Goodfellow et al., 2014), that de"
D19-1425,Q18-1024,1,0.909604,"ral Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 4153–4163, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics Note that these two proposals are taskindependent and can be extended to a vast array of text classification tasks where confounding factors are not known a priori. For concreteness, however, we evaluate our approach on the task of L1ID (§5). We experiment with two different datasets: a small corpus of student written essays (Malmasi et al., 2017) and a large and noisy dataset of Reddit posts (Rabinovich et al., 2018). We show that classifiers trained on these datasets without any intervention learn spurious topical correlations that are not indicative of style, and that our proposed deconfounded classifiers alleviate this problem (§6). We present an analysis of the features discovered after demoting these confounds in §7. The main contributions of this work are: 1. We introduce a novel method for representing and identifying variables which are confounds in text classification tasks. 2. We propose a classification model and an algorithm aimed at learning textual representations that are invariant to the c"
D19-1425,P18-2005,0,0.061756,"Missing"
D19-1425,W17-5007,0,0.1775,"4153 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 4153–4163, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics Note that these two proposals are taskindependent and can be extended to a vast array of text classification tasks where confounding factors are not known a priori. For concreteness, however, we evaluate our approach on the task of L1ID (§5). We experiment with two different datasets: a small corpus of student written essays (Malmasi et al., 2017) and a large and noisy dataset of Reddit posts (Rabinovich et al., 2018). We show that classifiers trained on these datasets without any intervention learn spurious topical correlations that are not indicative of style, and that our proposed deconfounded classifiers alleviate this problem (§6). We present an analysis of the features discovered after demoting these confounds in §7. The main contributions of this work are: 1. We introduce a novel method for representing and identifying variables which are confounds in text classification tasks. 2. We propose a classification model and an algorit"
D19-1425,K17-1018,0,0.170131,"Missing"
D19-1425,P19-1282,1,0.840889,"y L2, English included). The distribution of determiners is also a challenge for nonnatives, and 4159 the correct usage of the in particular is quite hard for learners to master. These challenges are evident from the most indicative words of our model. Observe also that the LO - TOP -50 model is somewhere in the middle: it includes some proper nouns (including geographical terms such as eu or us) but also several function words. A more detailed analysis of these observations is left for future work. Recently, there has been a debate on whether attention can be used to explain model decisions (Serrano and Smith, 2019; Jain and Wallace, 2019; Wiegreffe and Pinter, 2019), we thus present additional analysis of our proposed method based on saliency maps (Ding et al., 2019). Saliency maps have been shown to better capture word alignment than attention probabilities in neural machine translation. This method is based on computing the gradient of the probability of the predicted label with respect to each word in the input text and normalizing the gradient to obtain probabilities. We use saliency maps to generate lexicons similar to the ones generated using attention. As shown in table 8, the top indicative wor"
D19-1425,P14-1017,0,0.0780898,"Missing"
D19-1425,W13-1706,0,0.0450352,"Missing"
D19-1425,W07-0602,0,0.0433251,"Missing"
D19-1425,L18-1445,1,0.878121,"Missing"
D19-1425,D19-1002,0,0.0360982,"rminers is also a challenge for nonnatives, and 4159 the correct usage of the in particular is quite hard for learners to master. These challenges are evident from the most indicative words of our model. Observe also that the LO - TOP -50 model is somewhere in the middle: it includes some proper nouns (including geographical terms such as eu or us) but also several function words. A more detailed analysis of these observations is left for future work. Recently, there has been a debate on whether attention can be used to explain model decisions (Serrano and Smith, 2019; Jain and Wallace, 2019; Wiegreffe and Pinter, 2019), we thus present additional analysis of our proposed method based on saliency maps (Ding et al., 2019). Saliency maps have been shown to better capture word alignment than attention probabilities in neural machine translation. This method is based on computing the gradient of the probability of the predicted label with respect to each word in the input text and normalizing the gradient to obtain probabilities. We use saliency maps to generate lexicons similar to the ones generated using attention. As shown in table 8, the top indicative words for baseline and LO - TOP -50 follow a similar pat"
D19-1425,U09-1008,0,0.0905086,"Missing"
D19-1425,D11-1148,0,0.0606105,"Missing"
D19-1425,D17-1323,0,0.0536227,"orrelations in the training data that may be confounds in the actual classification task (Leino et al., 2019). A major challenge in building such systems is to discover features that are not just correlated with the signals in the training data, but are true indicators of these signals, and therefore generalize well. For example, Kiritchenko and Mohammad (2018) found that sentiment analysis systems implicitly overfit to demographic confounds, systematically amplifying the intensity ratings of posts 1 The code is available at: https://github.com/ Sachin19/adversarial-classify written by women. Zhao et al. (2017) showed that visual semantic role labeling models implicitly capture actions stereotypically associated with men or women (e.g., women are cooking and men are fixing a faucet), and in cases of higher model uncertainty assign stereotypical labels to actions and objects, thereby amplifying social biases found in the training data. We focus on the task of native language identification (L1ID), which aims at automatically identifying the native language (L1) of an individual based on their language production in a second language (L2, English in this work). The aim of this task is to discover styl"
D19-1606,W18-2501,1,0.779011,"Net (Yu et al., 2018), currently the best-performing published model on SQuAD 1.1 without data augmentation or pretraining; (2) QANet + BERT, which enhances the QANet model by concatenating frozen BERT representations to the original input embeddings; (3) BERT QA (Devlin et al., 2019), the adversarial baseline used in data construction, and (4) XLNet QA (Yang et al., 2019), another large pretrained language model based on the Transformer architecture (Vaswani et al., 2017) that outperforms BERT QA on reading comprehension 5927 benchmarks SQuAD and RACE (Lai et al., 2017). We use the AllenNLP (Gardner et al., 2018) implementation of QANet modified to use the marginal objective proposed by Clark and Gardner (2018) and pytorch-transformers6 implementation of base BERT QA7 and base XLNet QA. BERT is pretrained on English Wikipedia and BookCorpus (Zhu et al., 2015) (3.87B wordpieces, 13GB of plain text) and XLNet additionally on Giga5 (Napoles et al., 2012), ClueWeb 2012-B (extended from Smucker et al., 2009), and Common Crawl8 (32.89B wordpieces, 78GB of plain text). 4.2 Heuristic Baselines In light of recent work exposing predictive artifacts in crowdsourced NLP datasets (Gururangan et al., 2018; Kaushik"
D19-1606,L16-1021,0,0.122979,"Missing"
D19-1606,N15-1117,0,0.0285915,"sets like those of Pradhan et al. (2007), Ghaddar and Langlais (2016), Chen et al. (2018) and Poesio et al. (2018), which aim to obtain complete coref5928 erence clusters, our questions require understanding coreference between only a few spans. While this means that the notion of coreference captured by our dataset is less comprehensive, it is also less conservative and allows questions about coreference relations that are not marked in OntoNotes annotations. Since the notion is not as strict, it does not require linguistic expertise from annotators, making it more amenable to crowdsourcing. Guha et al. (2015) present the limitations of annotating coreference in newswire texts alone, and like us, built a non-newswire coreference resolution dataset focusing on Quiz Bowl questions. There is some other recent work (Poesio et al., 2019; Aralikatte and Søgaard, 2019) in crowdsourcing coreference judgments that relies on a relaxed notion of coreference as well. Reading comprehension datasets There are many reading comprehension datasets (Richardson et al., 2013; Rajpurkar et al., 2016; Kwiatkowski et al., 2019; Dua et al., 2019, inter alia). Most of these datasets principally require understanding local"
D19-1606,N18-2017,1,0.840015,"e AllenNLP (Gardner et al., 2018) implementation of QANet modified to use the marginal objective proposed by Clark and Gardner (2018) and pytorch-transformers6 implementation of base BERT QA7 and base XLNet QA. BERT is pretrained on English Wikipedia and BookCorpus (Zhu et al., 2015) (3.87B wordpieces, 13GB of plain text) and XLNet additionally on Giga5 (Napoles et al., 2012), ClueWeb 2012-B (extended from Smucker et al., 2009), and Common Crawl8 (32.89B wordpieces, 78GB of plain text). 4.2 Heuristic Baselines In light of recent work exposing predictive artifacts in crowdsourced NLP datasets (Gururangan et al., 2018; Kaushik and Lipton, 2018, inter alia), we estimate the effect of predictive artifacts by training BERT QA and XLNet QA to predict a single start and end index given only the passage as input (passage-only). 4.3 Results Table 3 presents the performance of all baseline models on Q UOREF. The best performing model is XLNet QA, which reaches an F1 score of 70.5 in the test set. However, it is still more than 20 F1 points below human performance.9 BERT QA trained on Q UOREF under-performs XLNet QA, but still gets a decent F1 score of 66.4. Note that BERT QA trained on SQuAD would have achieved an"
D19-1606,P17-1147,0,0.0275088,"tching highlights are anchored. Next to the questions are the relevant coreferent mentions from the paragraph. They are bolded for the first question, italicized for the second, and underlined for the third in the paragraph. Introduction Paragraphs and other longer texts typically make multiple references to the same entities. Tracking these references and resolving coreference is essential for full machine comprehension of these texts. Significant progress has recently been made in reading comprehension research, due to large crowdsourced datasets (Rajpurkar et al., 2016; Bajaj et al., 2016; Joshi et al., 2017; Kwiatkowski et al., 2019, inter alia). However, these datasets focus largely on understanding local predicateargument structure, with very few questions requiring long-distance entity tracking. Obtaining such questions is hard for two reasons: (1) teaching crowdworkers about coreference is challenging, with even experts disagreeing on its nuances (Pradhan et al., 2007; Versley, 2008; Recasens et al., 2011; Poesio et al., 2018), and (2) even if we can get crowdworkers to target coreference phenomena in their questions, these questions may contain giveaways that let models arrive at the correc"
D19-1606,D18-1546,0,0.0301144,"., 2018) implementation of QANet modified to use the marginal objective proposed by Clark and Gardner (2018) and pytorch-transformers6 implementation of base BERT QA7 and base XLNet QA. BERT is pretrained on English Wikipedia and BookCorpus (Zhu et al., 2015) (3.87B wordpieces, 13GB of plain text) and XLNet additionally on Giga5 (Napoles et al., 2012), ClueWeb 2012-B (extended from Smucker et al., 2009), and Common Crawl8 (32.89B wordpieces, 78GB of plain text). 4.2 Heuristic Baselines In light of recent work exposing predictive artifacts in crowdsourced NLP datasets (Gururangan et al., 2018; Kaushik and Lipton, 2018, inter alia), we estimate the effect of predictive artifacts by training BERT QA and XLNet QA to predict a single start and end index given only the passage as input (passage-only). 4.3 Results Table 3 presents the performance of all baseline models on Q UOREF. The best performing model is XLNet QA, which reaches an F1 score of 70.5 in the test set. However, it is still more than 20 F1 points below human performance.9 BERT QA trained on Q UOREF under-performs XLNet QA, but still gets a decent F1 score of 66.4. Note that BERT QA trained on SQuAD would have achieved an F1 score of 0, since our"
D19-1606,Q19-1026,0,0.216951,"e anchored. Next to the questions are the relevant coreferent mentions from the paragraph. They are bolded for the first question, italicized for the second, and underlined for the third in the paragraph. Introduction Paragraphs and other longer texts typically make multiple references to the same entities. Tracking these references and resolving coreference is essential for full machine comprehension of these texts. Significant progress has recently been made in reading comprehension research, due to large crowdsourced datasets (Rajpurkar et al., 2016; Bajaj et al., 2016; Joshi et al., 2017; Kwiatkowski et al., 2019, inter alia). However, these datasets focus largely on understanding local predicateargument structure, with very few questions requiring long-distance entity tracking. Obtaining such questions is hard for two reasons: (1) teaching crowdworkers about coreference is challenging, with even experts disagreeing on its nuances (Pradhan et al., 2007; Versley, 2008; Recasens et al., 2011; Poesio et al., 2018), and (2) even if we can get crowdworkers to target coreference phenomena in their questions, these questions may contain giveaways that let models arrive at the correct answer without performin"
D19-1606,N19-1225,1,0.83452,"best performing model is XLNet QA, which reaches an F1 score of 70.5 in the test set. However, it is still more than 20 F1 points below human performance.9 BERT QA trained on Q UOREF under-performs XLNet QA, but still gets a decent F1 score of 66.4. Note that BERT QA trained on SQuAD would have achieved an F1 score of 0, since our dataset was constructed with that model as the adversary. The extent to which BERT QA does well on Q UOREF might indicate its capacity for coreferential reasoning that was not exploited when it was trained on SQuAD (for a detailed discussion of this phenomenon, see Liu et al., 2019). Our analysis of model errors in §4.4 shows that some of the improved performance may also be due to artifacts in Q UOREF. We notice smaller improvements from XLNet QA over BERT QA (4.12 in F1 test score, 2.6 6 https://github.com/huggingface/ pytorch-transformers 7 The large BERT model does not fit in the available GPU memory. 8 https://commoncrawl.org/ 9 Human performance was estimated from the authors’ answers of 400 questions from the test set, scored with the same metric used for systems. in EM test score) on Q UOREF compared to other reading comprehension benchmarks: SQuAD and RACE (see"
D19-1606,W12-3018,0,0.0601011,"Missing"
D19-1606,N19-1176,0,0.0255683,"en only a few spans. While this means that the notion of coreference captured by our dataset is less comprehensive, it is also less conservative and allows questions about coreference relations that are not marked in OntoNotes annotations. Since the notion is not as strict, it does not require linguistic expertise from annotators, making it more amenable to crowdsourcing. Guha et al. (2015) present the limitations of annotating coreference in newswire texts alone, and like us, built a non-newswire coreference resolution dataset focusing on Quiz Bowl questions. There is some other recent work (Poesio et al., 2019; Aralikatte and Søgaard, 2019) in crowdsourcing coreference judgments that relies on a relaxed notion of coreference as well. Reading comprehension datasets There are many reading comprehension datasets (Richardson et al., 2013; Rajpurkar et al., 2016; Kwiatkowski et al., 2019; Dua et al., 2019, inter alia). Most of these datasets principally require understanding local predicate-argument structure in a paragraph of text. Q UOREF also requires understanding local predicate-argument structure, but makes the reading task harder by explicitly querying anaphoric references, requiring a system to"
D19-1606,D16-1264,0,0.547128,"in paragraphs is where the questions with matching highlights are anchored. Next to the questions are the relevant coreferent mentions from the paragraph. They are bolded for the first question, italicized for the second, and underlined for the third in the paragraph. Introduction Paragraphs and other longer texts typically make multiple references to the same entities. Tracking these references and resolving coreference is essential for full machine comprehension of these texts. Significant progress has recently been made in reading comprehension research, due to large crowdsourced datasets (Rajpurkar et al., 2016; Bajaj et al., 2016; Joshi et al., 2017; Kwiatkowski et al., 2019, inter alia). However, these datasets focus largely on understanding local predicateargument structure, with very few questions requiring long-distance entity tracking. Obtaining such questions is hard for two reasons: (1) teaching crowdworkers about coreference is challenging, with even experts disagreeing on its nuances (Pradhan et al., 2007; Versley, 2008; Recasens et al., 2011; Poesio et al., 2018), and (2) even if we can get crowdworkers to target coreference phenomena in their questions, these questions may contain giveaw"
D19-1606,D13-1020,0,0.0932073,"es annotations. Since the notion is not as strict, it does not require linguistic expertise from annotators, making it more amenable to crowdsourcing. Guha et al. (2015) present the limitations of annotating coreference in newswire texts alone, and like us, built a non-newswire coreference resolution dataset focusing on Quiz Bowl questions. There is some other recent work (Poesio et al., 2019; Aralikatte and Søgaard, 2019) in crowdsourcing coreference judgments that relies on a relaxed notion of coreference as well. Reading comprehension datasets There are many reading comprehension datasets (Richardson et al., 2013; Rajpurkar et al., 2016; Kwiatkowski et al., 2019; Dua et al., 2019, inter alia). Most of these datasets principally require understanding local predicate-argument structure in a paragraph of text. Q UOREF also requires understanding local predicate-argument structure, but makes the reading task harder by explicitly querying anaphoric references, requiring a system to track entities throughout the discourse. 6 Conclusion We present Q UOREF, a focused reading comprehension benchmark that evaluates the ability of models to resolve coreference. We crowdsourced questions over paragraphs from Wiki"
D19-1606,D17-1082,0,\N,Missing
D19-1606,W18-0702,0,\N,Missing
D19-1606,N19-1246,1,\N,Missing
D19-1606,P18-1078,1,\N,Missing
D19-1606,N19-1423,0,\N,Missing
E09-1037,D08-1017,1,0.840978,". There have been many algorithms proposed for approximately solving instances of these decoding and summing problems with non-local features. Some stem from work on graphical models, including loopy belief propagation (Sutton and McCallum, 2004; Smith and Eisner, 2008), Gibbs sampling (Finkel et al., 2005), sequential Monte Carlo methods such as particle filtering (Levy et al., 2008), and variational inference (Jordan et al., 1999; MacKay, 1997; Kurihara and Sato, 2006). Also relevant are stacked learning (Cohen and Carvalho, 2005), interpretable as approximation of non-local feature values (Martins et al., 2008), and M-estimation (Smith et al., 2007), which allows training without inference. Several other approaches used frequently in NLP are approximate methods for decoding only. These include beam search (Lowerre, 1976), cube pruning, which we discuss in §3, integer linear programming (Roth and Yih, 2004), in which arbitrary features can act as constraints on y, and approximate solutions like 3.1 Approximations in Cube Pruning Cube pruning is an approximate solution to the decoding problem (Eq. 4) in two ways. Approximation 1: k < ∞. Cube pruning uses a finite k for the k-best lists stored in each"
E09-1037,E06-1011,0,0.0427657,", though our version is more likely to be used in practice for both the Viterbi proof and k-best proof semirings. 3 The theorem indexing scheme might be based on a topological ordering given by the proof structure, but is not important for our purposes. 320 proof, normally at the first opportunity. Any feature function can be expressed this way. For local features, we can go farther; we define a function top(`) that returns the proof string corresponding to the antecedents and consequent of the last inference step in `. Local features have the property: hloc m (x, y) = Pt i=1 fm (x, top(`i )) McDonald and Pereira (2006), in which an exact solution to a related decoding problem is found and then modified to fit the problem of interest. 3 Approximate Decoding Cube pruning (Chiang, 2007; Huang and Chiang, 2007) is an approximate technique for decoding (Eq. 4); it is used widely in machine translation. Given proof locality, it is essentially an efficient implementation of the k-best proof semiring. Cube pruning goes farther in that it permits nonlocal features to weigh in on the proof probabilities, at the expense of making the k-best operation approximate. We describe the two approximations cube pruning makes,"
E09-1037,P92-1017,0,0.191961,"Missing"
E09-1037,W04-2401,0,0.0348634,"2005), sequential Monte Carlo methods such as particle filtering (Levy et al., 2008), and variational inference (Jordan et al., 1999; MacKay, 1997; Kurihara and Sato, 2006). Also relevant are stacked learning (Cohen and Carvalho, 2005), interpretable as approximation of non-local feature values (Martins et al., 2008), and M-estimation (Smith et al., 2007), which allows training without inference. Several other approaches used frequently in NLP are approximate methods for decoding only. These include beam search (Lowerre, 1976), cube pruning, which we discuss in §3, integer linear programming (Roth and Yih, 2004), in which arbitrary features can act as constraints on y, and approximate solutions like 3.1 Approximations in Cube Pruning Cube pruning is an approximate solution to the decoding problem (Eq. 4) in two ways. Approximation 1: k < ∞. Cube pruning uses a finite k for the k-best lists stored in each value. If k = ∞, the algorithm performs exact decoding with non-local features (at obviously formidable expense in combinatorial problems). Approximation 2: lazy computation. Cube pruning exploits the fact that k < ∞ to use lazy computation. When combining the k-best proof lists of d theorems’ values"
E09-1037,D08-1016,0,0.0423917,"a fully connected graphical model). Sometimes, it is possible to achieve proof locality by adding more indices to the DP variables (for example, consider modifying the bigram HMM Viterbi algorithm for trigram HMMs). This increases the number of variables and hence computational cost. In general, it leads to exponential-time inference in the worst case. There have been many algorithms proposed for approximately solving instances of these decoding and summing problems with non-local features. Some stem from work on graphical models, including loopy belief propagation (Sutton and McCallum, 2004; Smith and Eisner, 2008), Gibbs sampling (Finkel et al., 2005), sequential Monte Carlo methods such as particle filtering (Levy et al., 2008), and variational inference (Jordan et al., 1999; MacKay, 1997; Kurihara and Sato, 2006). Also relevant are stacked learning (Cohen and Carvalho, 2005), interpretable as approximation of non-local feature values (Martins et al., 2008), and M-estimation (Smith et al., 2007), which allows training without inference. Several other approaches used frequently in NLP are approximate methods for decoding only. These include beam search (Lowerre, 1976), cube pruning, which we discuss in"
E09-1037,P07-1095,1,0.848712,"d for approximately solving instances of these decoding and summing problems with non-local features. Some stem from work on graphical models, including loopy belief propagation (Sutton and McCallum, 2004; Smith and Eisner, 2008), Gibbs sampling (Finkel et al., 2005), sequential Monte Carlo methods such as particle filtering (Levy et al., 2008), and variational inference (Jordan et al., 1999; MacKay, 1997; Kurihara and Sato, 2006). Also relevant are stacked learning (Cohen and Carvalho, 2005), interpretable as approximation of non-local feature values (Martins et al., 2008), and M-estimation (Smith et al., 2007), which allows training without inference. Several other approaches used frequently in NLP are approximate methods for decoding only. These include beam search (Lowerre, 1976), cube pruning, which we discuss in §3, integer linear programming (Roth and Yih, 2004), in which arbitrary features can act as constraints on y, and approximate solutions like 3.1 Approximations in Cube Pruning Cube pruning is an approximate solution to the decoding problem (Eq. 4) in two ways. Approximation 1: k < ∞. Cube pruning uses a finite k for the k-best lists stored in each value. If k = ∞, the algorithm performs"
E09-1037,D07-1003,1,0.894163,"Missing"
E09-1037,D08-1023,0,0.0653066,"ph data structure known variously as an arithmetic circuit or computation graph.10 Arithmetic circuits have recently drawn interest in the graphical model community as a For a proof list u ¯ , we uk to denote the sum P use k¯ of all proof scores, i:hui ,Ui i∈¯u ui . The aggregation operator over operands 8 {ui }N i=1 , all such that uis = 0, is defined by: LN (14) (12) S  N + Res u ¯ i=1 i , S  E N max-k u ¯ , g , 0 0 i=1 i DP N i=1 ui0 6 Algebraic structures are typically defined with binary operators only, so we were unable to find a suitable term for this structure in the literature. 7 Blunsom and Osborne (2008) described a related approach to approximate summing using the chart computed during cube pruning, but did not keep track of the residual terms as we do here. 8 We assume that operands ui to ⊕cs will never be such that uis = 1 (non-local feature functions). This is reasonable in the widely used log-linear model setting we have adopted, where weights λm are factors in a proof’s product score. 9 The bottom-up agenda algorithm in Eisner et al. (2005) might possibly be generalized so that associativity, distributivity, and binary operators are not required (John Blatz, p.c.). 10 This data structur"
E09-1037,P08-1024,0,0.113899,"Missing"
E09-1037,J07-2003,0,0.395095,"ing over discrete structures with non-local features, which we relate to cube pruning (§4). We discuss implementation (§5) and show that cube summing becomes exact and expressible as a semiring when restricted to local features; this semiring generalizes many commonly-used semirings in dynamic programming (§6). We introduce cube summing, a technique that permits dynamic programming algorithms for summing over structures (like the forward and inside algorithms) to be extended with non-local features that violate the classical structural independence assumptions. It is inspired by cube pruning (Chiang, 2007; Huang and Chiang, 2007) in its computation of non-local features dynamically using scored k-best lists, but also maintains additional residual quantities used in calculating approximate marginals. When restricted to local features, cube summing reduces to a novel semiring (k-best+residual) that generalizes many of the semirings of Goodman (1999). When non-local features are included, cube summing does not reduce to any semiring, but is compatible with generic techniques for solving dynamic programming equations. 1 Introduction Probabilistic NLP researchers frequently make independence assump"
E09-1037,H05-1036,1,0.612539,"99) augmented such logic programs with semiring weights, giving an algebraic explanation for the intuitive connections among classes of algorithms with the same logical structure. For example, in Goodman’s framework, the forward algorithm and the Viterbi algorithm are comprised of the same logic program with different semirings. Goodman defined other semirings, including ones we will use here. This formal framework was the basis for the Dyna programming language, which permits a declarative specification of the logic program and compiles it into an efficient, agendabased, bottom-up procedure (Eisner et al., 2005). For our purposes, a DP consists of a set of recursive equations over a set of indexed variables. For example, the probabilistic CKY algorithm (run on sentence w1 w2 ...wn ) is written as CX,i−1,i = pX→wi CX,i,k = Semirings define these values and define two operators over them, called “aggregation” (max in Eq. 1) and “combination” (× in Eq. 1). Goodman and Eisner et al. assumed that the values of the variables are in a semiring, and that the equations are defined solely in terms of the two semiring operations. We will often refer to the “probability” of a proof, by which we mean a nonnegativ"
E09-1037,P02-1001,0,0.134031,"from each node to the nodes it depends on; ⊕ vertices depend on ⊗ vertices, which depend on ⊕ and axiom vertices. Arithmetic circuits are amenable to automatic differentiation in the reverse mode (Griewank and Corliss, 1991), commonly used in backpropagation algorithms. Importantly, this permits us to calculate the exact gradient of the approximate summation with respect to axiom values, following Eisner et al. (2005). This is desirable when carrying out the optimization problems involved in parameter estimation. Another differentiation technique, implemented within the semiring, is given by Eisner (2002). Cube pruning is based on the k-best algorithms of Huang and Chiang (2005), which save time over generic semiring implementations through lazy computation in both the aggregation and combination operations. Their techniques are not as clearly applicable here, because our goal is to sum over all proofs instead of only finding a small subset of them. If computing non-local features is a computational bottleneck, they can be computed only for the O(k) proofs considered when choosing the best k as in cube pruning. Then, the computational requirements for approximate summing are nearly equivalent"
E09-1037,P05-1045,0,0.0193977,"imes, it is possible to achieve proof locality by adding more indices to the DP variables (for example, consider modifying the bigram HMM Viterbi algorithm for trigram HMMs). This increases the number of variables and hence computational cost. In general, it leads to exponential-time inference in the worst case. There have been many algorithms proposed for approximately solving instances of these decoding and summing problems with non-local features. Some stem from work on graphical models, including loopy belief propagation (Sutton and McCallum, 2004; Smith and Eisner, 2008), Gibbs sampling (Finkel et al., 2005), sequential Monte Carlo methods such as particle filtering (Levy et al., 2008), and variational inference (Jordan et al., 1999; MacKay, 1997; Kurihara and Sato, 2006). Also relevant are stacked learning (Cohen and Carvalho, 2005), interpretable as approximation of non-local feature values (Martins et al., 2008), and M-estimation (Smith et al., 2007), which allows training without inference. Several other approaches used frequently in NLP are approximate methods for decoding only. These include beam search (Lowerre, 1976), cube pruning, which we discuss in §3, integer linear programming (Roth"
E09-1037,J99-4004,0,0.170178,"ique that permits dynamic programming algorithms for summing over structures (like the forward and inside algorithms) to be extended with non-local features that violate the classical structural independence assumptions. It is inspired by cube pruning (Chiang, 2007; Huang and Chiang, 2007) in its computation of non-local features dynamically using scored k-best lists, but also maintains additional residual quantities used in calculating approximate marginals. When restricted to local features, cube summing reduces to a novel semiring (k-best+residual) that generalizes many of the semirings of Goodman (1999). When non-local features are included, cube summing does not reduce to any semiring, but is compatible with generic techniques for solving dynamic programming equations. 1 Introduction Probabilistic NLP researchers frequently make independence assumptions to keep inference algorithms tractable. Doing so limits the features that are available to our models, requiring features to be structurally local. Yet many problems in NLP—machine translation, parsing, named-entity recognition, and others—have benefited from the addition of non-local features that break classical independence assumptions. D"
E09-1037,W05-1506,0,0.0554184,"⊗ vertices, which depend on ⊕ and axiom vertices. Arithmetic circuits are amenable to automatic differentiation in the reverse mode (Griewank and Corliss, 1991), commonly used in backpropagation algorithms. Importantly, this permits us to calculate the exact gradient of the approximate summation with respect to axiom values, following Eisner et al. (2005). This is desirable when carrying out the optimization problems involved in parameter estimation. Another differentiation technique, implemented within the semiring, is given by Eisner (2002). Cube pruning is based on the k-best algorithms of Huang and Chiang (2005), which save time over generic semiring implementations through lazy computation in both the aggregation and combination operations. Their techniques are not as clearly applicable here, because our goal is to sum over all proofs instead of only finding a small subset of them. If computing non-local features is a computational bottleneck, they can be computed only for the O(k) proofs considered when choosing the best k as in cube pruning. Then, the computational requirements for approximate summing are nearly equivalent to cube pruning, but the approximation is less accurate. 6 i k-best proof ("
E09-1037,P07-1019,0,0.212081,"ete structures with non-local features, which we relate to cube pruning (§4). We discuss implementation (§5) and show that cube summing becomes exact and expressible as a semiring when restricted to local features; this semiring generalizes many commonly-used semirings in dynamic programming (§6). We introduce cube summing, a technique that permits dynamic programming algorithms for summing over structures (like the forward and inside algorithms) to be extended with non-local features that violate the classical structural independence assumptions. It is inspired by cube pruning (Chiang, 2007; Huang and Chiang, 2007) in its computation of non-local features dynamically using scored k-best lists, but also maintains additional residual quantities used in calculating approximate marginals. When restricted to local features, cube summing reduces to a novel semiring (k-best+residual) that generalizes many of the semirings of Goodman (1999). When non-local features are included, cube summing does not reduce to any semiring, but is compatible with generic techniques for solving dynamic programming equations. 1 Introduction Probabilistic NLP researchers frequently make independence assumptions to keep inference a"
E09-1037,P08-1067,0,0.0703983,"ns within the semiring value. Recall that values in the k-best proof semiring fall in Ak = (R≥0 ×L)≤k . For cube decoding, we use a different set Acd defined as ..., huk g 0 (Uk ), Uk ii Here, max-k is simply used to re-sort the k-best proof list following function evaluation. The semiring properties fail to hold when introducing non-local features in this way. In particular, ⊗cd is not associative when 1 < k < ∞. For example, consider the probabilistic CKY algorithm as above, but using the cube decoding semiring with the non-local feature functions collectively known as “NGramTree” features (Huang, 2008) that score the string of terminals and nonterminals along the path from word j to word j + 1 when two constituents CY,i,j and CZ,j,k are combined. The semiring value associated with such a feature is u = hhi, NGramTree π (), 1i (for a specific path π), and we rewrite Eq. 1 as follows (where ranges for summation are omitted for space): L CX,i,k = cd pX→Y Z ⊗cd CY,i,j ⊗cd CZ,j,k ⊗cd u Acd = (R≥0 × L)≤k ×G × {0, 1} {z } | Ak where the binary variable indicates whether the value contains a k-best list (0, which we call an “ordinary” value) or a non-local feature function in G (1, which we call a"
E09-1037,W01-1812,0,0.07274,"al, the approach is exact. With non-local features, the k-best list may not contain the k-best proofs, and the residual score, while including all possible proofs, may not include all of the non-local features in all of those proofs’ probabilities. Acs = R≥0 × (R≥0 × L)≤k × G × {0, 1} A value u ∈ Acs is defined as u = hu0 , hhu1 , U1 i, hu2 , U2 i, ..., huk , Uk ii, gu , us i {z } | u ¯ i=1 ui = 5 Implementation We have so far viewed dynamic programming algorithms in terms of their declarative specifications as semiring-weighted logic programs. Solvers have been proposed by Goodman (1999), by Klein and Manning (2001) using a hypergraph representation, and by Eisner et al. (2005). Because Goodman’s and Eisner et al.’s algorithms assume semirings, adapting them for cube summing is non-trivial.9 To generalize Goodman’s algorithm, we suggest using the directed-graph data structure known variously as an arithmetic circuit or computation graph.10 Arithmetic circuits have recently drawn interest in the graphical model community as a For a proof list u ¯ , we uk to denote the sum P use k¯ of all proof scores, i:hui ,Ui i∈¯u ui . The aggregation operator over operands 8 {ui }N i=1 , all such that uis = 0, is defin"
E09-1037,H05-1064,0,0.153915,"Missing"
E12-1017,W10-2417,0,0.263206,"development and test data; the remainder is used in training. 4 Models Our starting point for statistical NER is a featurebased linear model over sequences, trained using the structured perceptron (Collins, 2002).8 In addition to lexical and morphological9 feaTraining words NEs ACE+ANER 212,839 15,796 Wikipedia (unlabeled, 397 docs) 1,110,546 — Development ACE 7,776 638 Wikipedia (4 domains, 8 docs) 21,203 2,073 Test ACE 7,789 621 Wikipedia (4 domains, 20 docs) 52,650 3,781 Table 4: Number of words (entity mentions) in data sets. tures known to work well for Arabic NER (Benajiba et al., 2008; Abdul-Hamid and Darwish, 2010), we incorporate some additional features enabled by Wikipedia. We do not employ a gazetteer, as the construction of a broad-domain gazetteer is a significant undertaking orthogonal to the challenges of a new text domain like Wikipedia.10 A descriptive list of our features is available in the supplementary document. We use a first-order structured perceptron; none of our features consider more than a pair of consecutive BIO labels at a time. The model enforces the constraint that NE sequences must begin with B (so the bigram hO, Ii is disallowed). Training this model on ACE and ANER data achie"
E12-1017,attia-etal-2010-automatically,0,0.305231,"Missing"
E12-1017,W03-2201,0,0.00816761,"but not entirely, consistent with each other in their creation of custom categories. Further, almost all of our article-specific categories correspond to classes in the extended NE taxonomy of (Sekine et al., 2002), which speaks to the reasonableness of both sets of categories—and by extension, our open-ended annotation process. Our annotation of named entities outside of the traditional POL classes creates a useful resource for entity detection and recognition in new domains. Even the ability to detect non-canonical types of NEs should help applications such as QA and MT (Toral et al., 2005; Babych and Hartley, 2003). Possible avenues for future work include annotating and projecting non-canonical 5 When it came to tagging NEs, one of the two annotators was assigned to each article. Custom categories only suggested by the other annotator were ignored. 164 NEs from English articles to their Arabic counterparts (Hassan et al., 2007), automatically clustering non-canonical types of entities into articlespecific or cross-article classes (cf. Frietag, 2004), or using non-canonical classes to improve the (author-specified) article categories in Wikipedia. Hereafter, we merge all article-specific categories with"
E12-1017,W09-3302,0,0.174637,"hard Stallman Amman were reserved Ummaya Mosque Muhammad al-Razi FIFA World Cup X Window System for training annotators, Claudio Filippone (PER) àñJ.ÊJ ¯ ñK XñÊ¿; Linux (SOFTWARE) ºJJ Ë; Spanish Gulf War for esti KðQK.; nuclear and League (CHAMPIONSHIPS) úGAJ.B@ ø PðYË@; proton (PARTICLE) àñ mating inter-annotator   agreement.  ; Real Zaragoza (ORG) é¢¯Qå ÈAK P radiation (GENERIC - MISC) ø ðñJË@ ¨AªB@ History Science Sports Technology appropriate entity classes will vary widely by domain; occurrence rates for entity classes are quite different in news text vs. Wikipedia, for instance (Balasuriya et al., 2009). This is abundantly clear in technical and scientific discourse, where much of the terminology is domain-specific, but it holds elsewhere. Non-POL entities in the history domain, for instance, include important events (wars, famines) and cultural movements (romanticism). Ignoring such domain-critical entities likely limits the usefulness of the NE analysis. Recognizing this limitation, some work on NER has sought to codify more robust inventories of general-purpose entity types (Sekine et al., 2002; Weischedel and Brunstein, 2005; Grouin et al., 2011) or to enumerate domain-specific types (Se"
E12-1017,D08-1030,0,0.302426,"Missing"
E12-1017,W03-1022,0,0.0325535,"elsewhere. Non-POL entities in the history domain, for instance, include important events (wars, famines) and cultural movements (romanticism). Ignoring such domain-critical entities likely limits the usefulness of the NE analysis. Recognizing this limitation, some work on NER has sought to codify more robust inventories of general-purpose entity types (Sekine et al., 2002; Weischedel and Brunstein, 2005; Grouin et al., 2011) or to enumerate domain-specific types (Settles, 2004; Yao et al., 2003). Coarse, general-purpose categories have also been used for semantic tagging of nouns and verbs (Ciaramita and Johnson, 2003). Yet as the number of classes or domains grows, rigorously documenting and organizing the classes—even for a single language—requires intensive effort. Ideally, an NER system would refine the traditional classes (Hovy et al., 2011) or identify new entity classes when they arise in new domains, adapting to new data. For this reason, we believe it is valuable to consider NER systems that identify (but do not necessarily label) entity mentions, and also to consider annotation schemes that allow annotators more freedom in defining entity classes. Our aim in creating an annotated dataset is to pro"
E12-1017,W03-0407,0,0.0451401,"recall errors (yi 6= O ∧ yˆi = O), and entity class/position errors (other cases where yi 6= yˆi ). As will be shown below, a key problem in cross-domain NER is poor recall, so we will penalize recall errors more severely:  M  0 if yi = yi0 X β if yi 6= O ∧ yi0 = O (3) c(y, y 0 ) =  i=1 1 otherwise Algorithm 1: Self-training. there is no available labeled training data. Yet the available unlabeled data is vast, so we turn to semisupervised learning. Here we adapt self-training, a simple technique that leverages a supervised learner (like the perceptron) to perform semisupervised learning (Clark et al., 2003; Mihalcea, 2004; McClosky et al., 2006). In our version, a model is trained on the labeled data, then used to label the unlabeled target data. We iterate between training on the hypothetically-labeled target data plus the original labeled set, and relabeling the target data; see Algorithm 1. Before self-training, we remove sentences hypothesized not to contain any named entity mentions, which we found avoids further encouragement of the model toward low recall. 5 for a penalty parameter β &gt; 1. We call our learner the “recall-oriented” perceptron (ROP). We note that Minkov et al. (2006) simila"
E12-1017,W02-1001,0,0.00590103,"test data. A larger set of Arabic Wikipedia articles, selected on the basis of quality heuristics, serves as unlabeled data for semisupervised learning. Our out-of-domain labeled NE data is drawn from the ANER (Benajiba et al., 2007) and ACE-2005 (Walker et al., 2006) newswire corpora. Entity types in this data are POL categories (PER, ORG, LOC) and MIS. Portions of the ACE corpus were held out as development and test data; the remainder is used in training. 4 Models Our starting point for statistical NER is a featurebased linear model over sequences, trained using the structured perceptron (Collins, 2002).8 In addition to lexical and morphological9 feaTraining words NEs ACE+ANER 212,839 15,796 Wikipedia (unlabeled, 397 docs) 1,110,546 — Development ACE 7,776 638 Wikipedia (4 domains, 8 docs) 21,203 2,073 Test ACE 7,789 621 Wikipedia (4 domains, 20 docs) 52,650 3,781 Table 4: Number of words (entity mentions) in data sets. tures known to work well for Arabic NER (Benajiba et al., 2008; Abdul-Hamid and Darwish, 2010), we incorporate some additional features enabled by Wikipedia. We do not employ a gazetteer, as the construction of a broad-domain gazetteer is a significant undertaking orthogonal"
E12-1017,D07-1074,0,0.00392108,"ance. There is evidence that models trained on Wikipedia data generalize and perform well on corpora with narrower domains. Nothman et al. (2009) and Balasuriya et al. (2009) show that NER models trained on both automatically and manually annotated Wikipedia corpora perform reasonably well on news corpora. The reverse scenario does not hold for models trained on news text, a result we also observe in Arabic NER. Other work has gone beyond the entity detection problem: Florian et al. (2004) additionally predict within-document entity coreference for Arabic, Chinese, and English ACE text, while Cucerzan (2007) aims to resolve every mention detected in English Wikipedia pages to a canonical article devoted to the entity in question. The domain and topic diversity of NEs has been studied in the framework of domain adaptation research. A group of these methods use selftraining and select the most informative features and training instances to adapt a source domain learner to the new target domain. Wu et al. (2009) bootstrap the NER leaner with a subset of unlabeled instances that bridge the source and target domains. Jiang and Zhai (2006) and Daum´e III (2007) make use of some labeled target-domain da"
E12-1017,P07-1033,0,0.00539308,"Missing"
E12-1017,farber-etal-2008-improving,0,0.399022,"the choice of this value (figure 1 shows how we tuned it on development data), and thus we anticipate that, in general, such tuning will be essential to leveraging the benefits of arrogance. 7 Related Work Our approach draws on insights from work in the areas of NER, domain adaptation, NLP with Wikipedia, and semisupervised learning. As all are broad areas of research, we highlight only the most relevant contributions here. Research in Arabic NER has been focused on compiling and optimizing the gazetteers and fea169 ture sets for standard sequential modeling algorithms (Benajiba et al., 2008; Farber et al., 2008; Shaalan and Raza, 2008; Abdul-Hamid and Darwish, 2010). We make use of features identified in this prior work to construct a strong baseline system. We are unaware of any Arabic NER work that has addressed diverse text domains like Wikipedia. Both the English and Arabic versions of Wikipedia have been used, however, as resources in service of traditional NER (Kazama and Torisawa, 2007; Benajiba et al., 2008). Attia et al. (2010) heuristically induce a mapping between Arabic Wikipedia and Arabic WordNet to construct Arabic NE gazetteers. Balasuriya et al. (2009) highlight the substantial dive"
E12-1017,D10-1033,0,0.0133877,"mains. Jiang and Zhai (2006) and Daum´e III (2007) make use of some labeled target-domain data to tune or augment the features of the source model towards the target domain. Here, in contrast, we use labeled target-domain data only for tuning and evaluation. Another important distinction is that domain variation in this prior work is restricted to topically-related corpora (e.g. newswire vs. broadcast news), whereas in our work, major topical differences distinguish the training and test corpora—and consequently, their salient NE classes. In these respects our NER setting is closer to that of Florian et al. (2010), who recognize English entities in noisy text, (Surdeanu et al., 2011), which concerns information extraction in a topically distinct target domain, and (Dalton et al., 2011), which addresses English NER in noisy and topically divergent text. Self-training (Clark et al., 2003; Mihalcea, 2004; McClosky et al., 2006) is widely used in NLP and has inspired related techniques that learn from automatically labeled data (Liang et al., 2008; Petrov et al., 2010). Our self-training procedure differs from some others in that we use all of the automatically labeled examples, rather than filtering them"
E12-1017,W04-3234,0,0.0317638,"Missing"
E12-1017,N10-1112,1,0.0770113,"2) Input: labeled data hhx(n) , y (n) iiN n=1 ; unlabeled ¯ (j) iJj=1 ; supervised learner L; data hx number of iterations T 0 Output: w w ← L(hhx(n) , y (n) iiN n=1 ) for t = 1 to T 0 do for j = 1 to J do ¯ (j) , y) yˆ(j) ← arg maxy w&gt; g(x ¯ (j) , yˆ(j) iiJj=1 ) w ← L(hhx(n) , y (n) iiN n=1 ∪ hhx y0 which amounts to performing stochastic subgradient ascent on an objective function with the Eq. 1 loss (Ratliff et al., 2006). In this framework, cost functions can be formulated to distinguish between different types of errors made during training. For a tag sequence y = hy1 , y2 , . . . , yM i, Gimpel and Smith (2010b) define word-local cost functions that differently penalize precision errors (i.e., yi = O ∧ yˆi 6= O for the ith word), recall errors (yi 6= O ∧ yˆi = O), and entity class/position errors (other cases where yi 6= yˆi ). As will be shown below, a key problem in cross-domain NER is poor recall, so we will penalize recall errors more severely:  M  0 if yi = yi0 X β if yi 6= O ∧ yi0 = O (3) c(y, y 0 ) =  i=1 1 otherwise Algorithm 1: Self-training. there is no available labeled training data. Yet the available unlabeled data is vast, so we turn to semisupervised learning. Here we adapt self-t"
E12-1017,W11-0411,0,0.0136423,"ews text vs. Wikipedia, for instance (Balasuriya et al., 2009). This is abundantly clear in technical and scientific discourse, where much of the terminology is domain-specific, but it holds elsewhere. Non-POL entities in the history domain, for instance, include important events (wars, famines) and cultural movements (romanticism). Ignoring such domain-critical entities likely limits the usefulness of the NE analysis. Recognizing this limitation, some work on NER has sought to codify more robust inventories of general-purpose entity types (Sekine et al., 2002; Weischedel and Brunstein, 2005; Grouin et al., 2011) or to enumerate domain-specific types (Settles, 2004; Yao et al., 2003). Coarse, general-purpose categories have also been used for semantic tagging of nouns and verbs (Ciaramita and Johnson, 2003). Yet as the number of classes or domains grows, rigorously documenting and organizing the classes—even for a single language—requires intensive effort. Ideally, an NER system would refine the traditional classes (Hovy et al., 2011) or identify new entity classes when they arise in new domains, adapting to new data. For this reason, we believe it is valuable to consider NER systems that identify (bu"
E12-1017,P05-1071,0,0.00777494,"ature weights (model parameters) w, the structured hinge loss is `hinge (x, y, w) =   &gt; 0 0 max w g(x, y ) + c(y, y ) − w&gt; g(x, y) 0 y 6 Additional details appear in the supplement. We downloaded a snapshot of Arabic Wikipedia (http://ar.wikipedia.org) on 8/29/2009 and preprocessed the articles to extract main body text and metadata using the mwlib package for Python (PediaPress, 2010). 8 A more leisurely discussion of the structured perceptron and its connection to empirical risk minimization can be found in the supplementary document. 9 We obtain morphological analyses from the MADA tool (Habash and Rambow, 2005; Roth et al., 2008). 7 Recall-Oriented Perceptron (1) The maximization problem inside the parentheses is known as cost-augmented decoding. If c fac10 A gazetteer ought to yield further improvements in line with previous findings in NER (Ratinov and Roth, 2009). 11 Though optimizing NER systems for F1 has been called into question (Manning, 2006), no alternative metric has achieved widespread acceptance in the community. 165 tors similarly to the feature function g(x, y), then we can increase penalties for y that have more local mistakes. This raises the learner’s awareness about how it will b"
E12-1017,N06-2015,0,0.019601,"a well-established discriminative NER model and feature set. Experiments show consistent gains on the challenging problem of identifying named entities in Arabic Wikipedia text. 2 Arabic Wikipedia NE Annotation Most of the effort in NER has been focused around a small set of domains and general-purpose entity classes relevant to those domains—especially the categories PER ( SON ), ORG ( ANIZATION ), and LOC ( ATION ) (POL), which are highly prominent in news text. Arabic is no exception: the publicly available NER corpora—ACE (Walker et al., 2006), ANER (Benajiba et al., 2008), and OntoNotes (Hovy et al., 2006)—all are in the news domain.2 However, 2 OntoNotes contains news-related text. ACE includes some text from blogs. In addition to the POL classes, both corpora include additional NE classes such as facility, event, product, vehicle, etc. These entities are infrequent and may not be comprehensive enough to cover the larger set of pos162 Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 162–173, c Avignon, France, April 23 - 27 2012. 2012 Association for Computational Linguistics Table 1: Translated titles of Arabic Wikipedia arti´"
E12-1017,P11-1147,0,0.0170901,"ng this limitation, some work on NER has sought to codify more robust inventories of general-purpose entity types (Sekine et al., 2002; Weischedel and Brunstein, 2005; Grouin et al., 2011) or to enumerate domain-specific types (Settles, 2004; Yao et al., 2003). Coarse, general-purpose categories have also been used for semantic tagging of nouns and verbs (Ciaramita and Johnson, 2003). Yet as the number of classes or domains grows, rigorously documenting and organizing the classes—even for a single language—requires intensive effort. Ideally, an NER system would refine the traditional classes (Hovy et al., 2011) or identify new entity classes when they arise in new domains, adapting to new data. For this reason, we believe it is valuable to consider NER systems that identify (but do not necessarily label) entity mentions, and also to consider annotation schemes that allow annotators more freedom in defining entity classes. Our aim in creating an annotated dataset is to provide a testbed for evaluation of new NER models. We will use these data as development and sible NEs (Sekine et al., 2002). Nezda et al. (2006) annotated and evaluated an Arabic NE corpus with an extended set of 18 classes (includin"
E12-1017,N06-1010,0,0.0244745,"t entity coreference for Arabic, Chinese, and English ACE text, while Cucerzan (2007) aims to resolve every mention detected in English Wikipedia pages to a canonical article devoted to the entity in question. The domain and topic diversity of NEs has been studied in the framework of domain adaptation research. A group of these methods use selftraining and select the most informative features and training instances to adapt a source domain learner to the new target domain. Wu et al. (2009) bootstrap the NER leaner with a subset of unlabeled instances that bridge the source and target domains. Jiang and Zhai (2006) and Daum´e III (2007) make use of some labeled target-domain data to tune or augment the features of the source model towards the target domain. Here, in contrast, we use labeled target-domain data only for tuning and evaluation. Another important distinction is that domain variation in this prior work is restricted to topically-related corpora (e.g. newswire vs. broadcast news), whereas in our work, major topical differences distinguish the training and test corpora—and consequently, their salient NE classes. In these respects our NER setting is closer to that of Florian et al. (2010), who r"
E12-1017,D07-1073,0,0.00752362,"t only the most relevant contributions here. Research in Arabic NER has been focused on compiling and optimizing the gazetteers and fea169 ture sets for standard sequential modeling algorithms (Benajiba et al., 2008; Farber et al., 2008; Shaalan and Raza, 2008; Abdul-Hamid and Darwish, 2010). We make use of features identified in this prior work to construct a strong baseline system. We are unaware of any Arabic NER work that has addressed diverse text domains like Wikipedia. Both the English and Arabic versions of Wikipedia have been used, however, as resources in service of traditional NER (Kazama and Torisawa, 2007; Benajiba et al., 2008). Attia et al. (2010) heuristically induce a mapping between Arabic Wikipedia and Arabic WordNet to construct Arabic NE gazetteers. Balasuriya et al. (2009) highlight the substantial divergence between entities appearing in English Wikipedia versus traditional corpora, and the effects of this divergence on NER performance. There is evidence that models trained on Wikipedia data generalize and perform well on corpora with narrower domains. Nothman et al. (2009) and Balasuriya et al. (2009) show that NER models trained on both automatically and manually annotated Wikipedi"
E12-1017,P11-2016,0,0.0185145,"English NER in noisy and topically divergent text. Self-training (Clark et al., 2003; Mihalcea, 2004; McClosky et al., 2006) is widely used in NLP and has inspired related techniques that learn from automatically labeled data (Liang et al., 2008; Petrov et al., 2010). Our self-training procedure differs from some others in that we use all of the automatically labeled examples, rather than filtering them based on a confidence score. Cost functions have been used in nonstructured classification settings to penalize certain types of errors more than others (Chan and Stolfo, 1998; Domingos, 1999; Kiddon and Brun, 2011). The goal of optimizing our structured NER model for recall is quite similar to the scenario explored by Minkov et al. (2006), as noted above. 8 Conclusion We explored the problem of learning an NER model suited to domains for which no labeled training data are available. A loss function to encourage recall over precision during supervised discriminative learning substantially improves recall and overall entity detection performance, especially when combined with a semisupervised learning regimen incorporating the same bias. We have also developed a small corpus of Arabic Wikipedia articles v"
E12-1017,W04-3250,0,0.00512011,"sed model as the initial labeler for recall-oriented self-training. • ROP/ROP (the “double ROP” condition): recalloriented supervised model as the initial labeler for recall-oriented self-training. Note that the two ROPs can use different cost parameters. For evaluating our models we consider the named entity detection task, i.e., recognizing which spans of words constitute entities. This is measured by per-entity precision, recall, and F1 .13 To measure statistical significance of differences between models we use Gimpel and Smith’s (2010) implementation of the paired bootstrap resampler of (Koehn, 2004), taking 10,000 samples for each comparison. 5.1 Baseline Our baseline is the perceptron, trained on the POL entity boundaries in the ACE+ANER corpus (reg/none).14 Development data was used to select the number of iterations (10). We performed 3-fold cross-validation on the ACE data and found wide variance in the in-domain entity detection performance of this model: fold 1 fold 2 fold 3 average P 70.43 87.48 65.09 74.33 R 63.08 81.13 51.13 65.11 F1 66.55 84.18 57.27 69.33 (Fold 1 corresponds to the ACE test set described in table 4.) We also trained the model to perform POL detection and class"
E12-1017,N06-1020,0,0.0445779,"and entity class/position errors (other cases where yi 6= yˆi ). As will be shown below, a key problem in cross-domain NER is poor recall, so we will penalize recall errors more severely:  M  0 if yi = yi0 X β if yi 6= O ∧ yi0 = O (3) c(y, y 0 ) =  i=1 1 otherwise Algorithm 1: Self-training. there is no available labeled training data. Yet the available unlabeled data is vast, so we turn to semisupervised learning. Here we adapt self-training, a simple technique that leverages a supervised learner (like the perceptron) to perform semisupervised learning (Clark et al., 2003; Mihalcea, 2004; McClosky et al., 2006). In our version, a model is trained on the labeled data, then used to label the unlabeled target data. We iterate between training on the hypothetically-labeled target data plus the original labeled set, and relabeling the target data; see Algorithm 1. Before self-training, we remove sentences hypothesized not to contain any named entity mentions, which we found avoids further encouragement of the model toward low recall. 5 for a penalty parameter β &gt; 1. We call our learner the “recall-oriented” perceptron (ROP). We note that Minkov et al. (2006) similarly explored the recall vs. precision tr"
E12-1017,W04-2405,0,0.0438164,"= O ∧ yˆi = O), and entity class/position errors (other cases where yi 6= yˆi ). As will be shown below, a key problem in cross-domain NER is poor recall, so we will penalize recall errors more severely:  M  0 if yi = yi0 X β if yi 6= O ∧ yi0 = O (3) c(y, y 0 ) =  i=1 1 otherwise Algorithm 1: Self-training. there is no available labeled training data. Yet the available unlabeled data is vast, so we turn to semisupervised learning. Here we adapt self-training, a simple technique that leverages a supervised learner (like the perceptron) to perform semisupervised learning (Clark et al., 2003; Mihalcea, 2004; McClosky et al., 2006). In our version, a model is trained on the labeled data, then used to label the unlabeled target data. We iterate between training on the hypothetically-labeled target data plus the original labeled set, and relabeling the target data; see Algorithm 1. Before self-training, we remove sentences hypothesized not to contain any named entity mentions, which we found avoids further encouragement of the model toward low recall. 5 for a penalty parameter β &gt; 1. We call our learner the “recall-oriented” perceptron (ROP). We note that Minkov et al. (2006) similarly explored the"
E12-1017,N06-2024,0,0.386018,"earning (Clark et al., 2003; Mihalcea, 2004; McClosky et al., 2006). In our version, a model is trained on the labeled data, then used to label the unlabeled target data. We iterate between training on the hypothetically-labeled target data plus the original labeled set, and relabeling the target data; see Algorithm 1. Before self-training, we remove sentences hypothesized not to contain any named entity mentions, which we found avoids further encouragement of the model toward low recall. 5 for a penalty parameter β &gt; 1. We call our learner the “recall-oriented” perceptron (ROP). We note that Minkov et al. (2006) similarly explored the recall vs. precision tradeoff in NER. Their technique was to directly tune the weight of a single feature—the feature marking O (nonentity tokens); a lower weight for this feature will incur a greater penalty for predicting O. Below we demonstrate that our method, which is less coarse, is more successful in our setting.12 In our experiments we will show that injecting “arrogance” into the learner via the recall-oriented loss function substantially improves recall, especially for non-POL entities (§5.3). 4.2 Self-Training and Semisupervised Learning As we will show exper"
E12-1017,nezda-etal-2006-world,0,0.220858,"e—requires intensive effort. Ideally, an NER system would refine the traditional classes (Hovy et al., 2011) or identify new entity classes when they arise in new domains, adapting to new data. For this reason, we believe it is valuable to consider NER systems that identify (but do not necessarily label) entity mentions, and also to consider annotation schemes that allow annotators more freedom in defining entity classes. Our aim in creating an annotated dataset is to provide a testbed for evaluation of new NER models. We will use these data as development and sible NEs (Sekine et al., 2002). Nezda et al. (2006) annotated and evaluated an Arabic NE corpus with an extended set of 18 classes (including temporal and numeric entities); this corpus has not been released publicly. testing examples, but not as training data. In §4 we will discuss our semisupervised approach to learning, which leverages ACE and ANER data as an annotated training corpus. 2.1 Annotation Strategy We conducted a small annotation project on Arabic Wikipedia articles. Two college-educated native Arabic speakers annotated about 3,000 sentences from 31 articles. We identified four topical areas of interest—history, technology, scien"
E12-1017,E09-1070,0,0.00793652,"glish and Arabic versions of Wikipedia have been used, however, as resources in service of traditional NER (Kazama and Torisawa, 2007; Benajiba et al., 2008). Attia et al. (2010) heuristically induce a mapping between Arabic Wikipedia and Arabic WordNet to construct Arabic NE gazetteers. Balasuriya et al. (2009) highlight the substantial divergence between entities appearing in English Wikipedia versus traditional corpora, and the effects of this divergence on NER performance. There is evidence that models trained on Wikipedia data generalize and perform well on corpora with narrower domains. Nothman et al. (2009) and Balasuriya et al. (2009) show that NER models trained on both automatically and manually annotated Wikipedia corpora perform reasonably well on news corpora. The reverse scenario does not hold for models trained on news text, a result we also observe in Arabic NER. Other work has gone beyond the entity detection problem: Florian et al. (2004) additionally predict within-document entity coreference for Arabic, Chinese, and English ACE text, while Cucerzan (2007) aims to resolve every mention detected in English Wikipedia pages to a canonical article devoted to the entity in question. The d"
E12-1017,D10-1069,0,0.00615818,"stinguish the training and test corpora—and consequently, their salient NE classes. In these respects our NER setting is closer to that of Florian et al. (2010), who recognize English entities in noisy text, (Surdeanu et al., 2011), which concerns information extraction in a topically distinct target domain, and (Dalton et al., 2011), which addresses English NER in noisy and topically divergent text. Self-training (Clark et al., 2003; Mihalcea, 2004; McClosky et al., 2006) is widely used in NLP and has inspired related techniques that learn from automatically labeled data (Liang et al., 2008; Petrov et al., 2010). Our self-training procedure differs from some others in that we use all of the automatically labeled examples, rather than filtering them based on a confidence score. Cost functions have been used in nonstructured classification settings to penalize certain types of errors more than others (Chan and Stolfo, 1998; Domingos, 1999; Kiddon and Brun, 2011). The goal of optimizing our structured NER model for recall is quite similar to the scenario explored by Minkov et al. (2006), as noted above. 8 Conclusion We explored the problem of learning an NER model suited to domains for which no labeled"
E12-1017,W09-1119,0,0.00861532,"9/2009 and preprocessed the articles to extract main body text and metadata using the mwlib package for Python (PediaPress, 2010). 8 A more leisurely discussion of the structured perceptron and its connection to empirical risk minimization can be found in the supplementary document. 9 We obtain morphological analyses from the MADA tool (Habash and Rambow, 2005; Roth et al., 2008). 7 Recall-Oriented Perceptron (1) The maximization problem inside the parentheses is known as cost-augmented decoding. If c fac10 A gazetteer ought to yield further improvements in line with previous findings in NER (Ratinov and Roth, 2009). 11 Though optimizing NER systems for F1 has been called into question (Manning, 2006), no alternative metric has achieved widespread acceptance in the community. 165 tors similarly to the feature function g(x, y), then we can increase penalties for y that have more local mistakes. This raises the learner’s awareness about how it will be evaluated. Incorporating cost-augmented decoding into the perceptron leads to this decoding step:   yˆ ← arg max w&gt; g(x, y 0 ) + c(y, y 0 ) , (2) Input: labeled data hhx(n) , y (n) iiN n=1 ; unlabeled ¯ (j) iJj=1 ; supervised learner L; data hx number of it"
E12-1017,P08-2030,0,0.0266617,"meters) w, the structured hinge loss is `hinge (x, y, w) =   &gt; 0 0 max w g(x, y ) + c(y, y ) − w&gt; g(x, y) 0 y 6 Additional details appear in the supplement. We downloaded a snapshot of Arabic Wikipedia (http://ar.wikipedia.org) on 8/29/2009 and preprocessed the articles to extract main body text and metadata using the mwlib package for Python (PediaPress, 2010). 8 A more leisurely discussion of the structured perceptron and its connection to empirical risk minimization can be found in the supplementary document. 9 We obtain morphological analyses from the MADA tool (Habash and Rambow, 2005; Roth et al., 2008). 7 Recall-Oriented Perceptron (1) The maximization problem inside the parentheses is known as cost-augmented decoding. If c fac10 A gazetteer ought to yield further improvements in line with previous findings in NER (Ratinov and Roth, 2009). 11 Though optimizing NER systems for F1 has been called into question (Manning, 2006), no alternative metric has achieved widespread acceptance in the community. 165 tors similarly to the feature function g(x, y), then we can increase penalties for y that have more local mistakes. This raises the learner’s awareness about how it will be evaluated. Incorpo"
E12-1017,sekine-etal-2002-extended,0,0.045577,"nce rates for entity classes are quite different in news text vs. Wikipedia, for instance (Balasuriya et al., 2009). This is abundantly clear in technical and scientific discourse, where much of the terminology is domain-specific, but it holds elsewhere. Non-POL entities in the history domain, for instance, include important events (wars, famines) and cultural movements (romanticism). Ignoring such domain-critical entities likely limits the usefulness of the NE analysis. Recognizing this limitation, some work on NER has sought to codify more robust inventories of general-purpose entity types (Sekine et al., 2002; Weischedel and Brunstein, 2005; Grouin et al., 2011) or to enumerate domain-specific types (Settles, 2004; Yao et al., 2003). Coarse, general-purpose categories have also been used for semantic tagging of nouns and verbs (Ciaramita and Johnson, 2003). Yet as the number of classes or domains grows, rigorously documenting and organizing the classes—even for a single language—requires intensive effort. Ideally, an NER system would refine the traditional classes (Hovy et al., 2011) or identify new entity classes when they arise in new domains, adapting to new data. For this reason, we believe it"
E12-1017,W04-1221,0,0.0307168,"9). This is abundantly clear in technical and scientific discourse, where much of the terminology is domain-specific, but it holds elsewhere. Non-POL entities in the history domain, for instance, include important events (wars, famines) and cultural movements (romanticism). Ignoring such domain-critical entities likely limits the usefulness of the NE analysis. Recognizing this limitation, some work on NER has sought to codify more robust inventories of general-purpose entity types (Sekine et al., 2002; Weischedel and Brunstein, 2005; Grouin et al., 2011) or to enumerate domain-specific types (Settles, 2004; Yao et al., 2003). Coarse, general-purpose categories have also been used for semantic tagging of nouns and verbs (Ciaramita and Johnson, 2003). Yet as the number of classes or domains grows, rigorously documenting and organizing the classes—even for a single language—requires intensive effort. Ideally, an NER system would refine the traditional classes (Hovy et al., 2011) or identify new entity classes when they arise in new domains, adapting to new data. For this reason, we believe it is valuable to consider NER systems that identify (but do not necessarily label) entity mentions, and also"
E12-1017,W11-0902,0,0.0289153,"Missing"
E12-1017,D09-1158,0,0.0127085,"ic NER. Other work has gone beyond the entity detection problem: Florian et al. (2004) additionally predict within-document entity coreference for Arabic, Chinese, and English ACE text, while Cucerzan (2007) aims to resolve every mention detected in English Wikipedia pages to a canonical article devoted to the entity in question. The domain and topic diversity of NEs has been studied in the framework of domain adaptation research. A group of these methods use selftraining and select the most informative features and training instances to adapt a source domain learner to the new target domain. Wu et al. (2009) bootstrap the NER leaner with a subset of unlabeled instances that bridge the source and target domains. Jiang and Zhai (2006) and Daum´e III (2007) make use of some labeled target-domain data to tune or augment the features of the source model towards the target domain. Here, in contrast, we use labeled target-domain data only for tuning and evaluation. Another important distinction is that domain variation in this prior work is restricted to topically-related corpora (e.g. newswire vs. broadcast news), whereas in our work, major topical differences distinguish the training and test corpora—"
E12-1017,W03-1708,0,0.0109477,"ndantly clear in technical and scientific discourse, where much of the terminology is domain-specific, but it holds elsewhere. Non-POL entities in the history domain, for instance, include important events (wars, famines) and cultural movements (romanticism). Ignoring such domain-critical entities likely limits the usefulness of the NE analysis. Recognizing this limitation, some work on NER has sought to codify more robust inventories of general-purpose entity types (Sekine et al., 2002; Weischedel and Brunstein, 2005; Grouin et al., 2011) or to enumerate domain-specific types (Settles, 2004; Yao et al., 2003). Coarse, general-purpose categories have also been used for semantic tagging of nouns and verbs (Ciaramita and Johnson, 2003). Yet as the number of classes or domains grows, rigorously documenting and organizing the classes—even for a single language—requires intensive effort. Ideally, an NER system would refine the traditional classes (Hovy et al., 2011) or identify new entity classes when they arise in new domains, adapting to new data. For this reason, we believe it is valuable to consider NER systems that identify (but do not necessarily label) entity mentions, and also to consider annota"
E12-1017,N04-1001,0,\N,Missing
E17-1117,P16-1231,0,0.223878,"Missing"
E17-1117,C02-1126,0,0.0882932,"Missing"
E17-1117,D16-1257,0,0.541811,"n all past actions. The joint probability estimate p(x, y) can be used for both phrase-structure parsing (finding arg maxy p(y |x)) and language modeling (finding p(x) by marginalizing over the set of possible parses for x). Both inference problems can be solved using an importance sampling procedure.4 We report all RNNG performance based on the corrigendum to Dyer et al. (2016). 3 Composition is Key Given the same data, under both the discriminative and generative settings RNNGs were found to parse with significantly higher accuracy than (respectively) the models of Vinyals et al. (2015) and Choe and Charniak (2016) that represent y as a “linearized” sequence of symbols and parentheses without explicitly capturing the tree structure, or even constraining the y to be a well-formed tree (see Table 1). Vinyals et al. (2015) directly predict the sequence of nonterminals, “shifts” (which consume a terminal symbol), and parentheses from left to right, conditional on the input terminal sequence x, while Choe and Charniak (2016) used a sequential LSTM language model on the same linearized trees to create a generative variant of the Vinyals et al. (2015) model. The generative model is used to re-rank parse candid"
E17-1117,P97-1003,0,0.860051,"guistics: Volume 1, Long Papers, pages 1249–1258, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics 2 Recurrent Neural Network Grammars An RNNG defines a joint probability distribution over string terminals and phrase-structure nonterminals.3 Formally, the RNNG is defined by a triple hN, Σ, Θi, where N denotes the set of nonterminal symbols (NP, VP, etc.), Σ the set of all terminal symbols (we assume that N ∩ Σ = ∅), and Θ the set of all model parameters. Unlike previous works that rely on hand-crafted rules to compose more fine-grained phrase representations (Collins, 1997; Klein and Manning, 2003), the RNNG implicitly parameterizes the information passed through compositions of phrases (in Θ and the neural network architecture), hence weakening the strong independence assumptions in classical probabilistic context-free grammars. The RNNG is based on an abstract state machine like those used in transition-based parsing, with its algorithmic state consisting of a stack of partially completed constituents, a buffer of already-generated terminal symbols, and a list of past actions. To generate a sentence x and its phrase-structure tree y, the RNNG samples a sequen"
E17-1117,de-marneffe-etal-2006-generating,0,0.0559403,"Missing"
E17-1117,P81-1022,0,0.79275,"Missing"
E17-1117,N16-1024,1,0.631277,"ial for achieving the best performance. Through the attention mechanism, we find that headedness plays a central role in phrasal representation (with the model’s latent attention largely agreeing with predictions made by hand-crafted head rules, albeit with some important differences). By training grammars without nonterminal labels, we find that phrasal representations depend minimally on nonterminals, providing support for the endocentricity hypothesis. 1 Introduction In this paper, we focus on a recently proposed class of probability distributions, recurrent neural network grammars (RNNGs; Dyer et al., 2016), designed to model syntactic derivations of sentences. We focus on RNNGs as generative probabilistic models over trees, as summarized in §2. Fitting a probabilistic model to data has often been understood as a way to test or confirm some aspect of a theory. We talk about a model’s assumptions and sometimes explore its parameters or posteriors over its latent variables in order to gain understanding of what it “discovers” from the data. In some sense, such models can be thought of as mini-scientists. Neural networks, including RNNGs, are capable of representing larger classes of hypotheses tha"
E17-1117,Q16-1023,0,0.0670931,"Missing"
E17-1117,P02-1017,0,0.525775,"(that is, the representation of a phrase is built from within depending on its components but independent of explicit category labels), then the nonterminal types should be easily inferred given the endocentrically-composed representation, and that ablating the nonterminal information would not make much difference in performance. Specifically, we train a GA-RNNG on unlabeled trees (only bracketings without nonterminal types), denoted U-GA-RNNG. This idea has been explored in research on methods for learning syntax with less complete annotation (Pereira and Schabes, 1992). A key finding from Klein and Manning (2002) was that, 1255 1 2 3 4 5 6 7 8 9 10 Noun phrases Canadian (0.09) Auto (0.31) Workers (0.2) union (0.22) president (0.18) no (0.29) major (0.05) Eurobond (0.32) or (0.01) foreign (0.01) bond (0.1) offerings (0.22) Saatchi (0.12) client (0.14) Philips (0.21) Lighting (0.24) Co. (0.29) nonperforming (0.18) commercial (0.23) real (0.25) estate (0.1) assets (0.25) the (0.1) Jamaica (0.1) Tourist (0.03) Board (0.17) ad (0.20) account (0.40) the (0.0) final (0.18) hour (0.81) their (0.0) first (0.23) test (0.77) Apple (0.62) , (0.02) Compaq (0.1) and (0.01) IBM (0.25) both (0.02) stocks (0.03) and ("
E17-1117,P03-1054,0,0.278815,"e 1, Long Papers, pages 1249–1258, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics 2 Recurrent Neural Network Grammars An RNNG defines a joint probability distribution over string terminals and phrase-structure nonterminals.3 Formally, the RNNG is defined by a triple hN, Σ, Θi, where N denotes the set of nonterminal symbols (NP, VP, etc.), Σ the set of all terminal symbols (we assume that N ∩ Σ = ∅), and Θ the set of all model parameters. Unlike previous works that rely on hand-crafted rules to compose more fine-grained phrase representations (Collins, 1997; Klein and Manning, 2003), the RNNG implicitly parameterizes the information passed through compositions of phrases (in Θ and the neural network architecture), hence weakening the strong independence assumptions in classical probabilistic context-free grammars. The RNNG is based on an abstract state machine like those used in transition-based parsing, with its algorithmic state consisting of a stack of partially completed constituents, a buffer of already-generated terminal symbols, and a list of past actions. To generate a sentence x and its phrase-structure tree y, the RNNG samples a sequence of actions to construct"
E17-1117,D15-1278,0,0.0303018,"f understanding neural network models in NLP has been previously studied for sequential RNNs (Karpathy et al., 2015; Li et al., 2016). Shi et al. (2016) showed that sequence-tosequence neural translation models capture a certain degree of syntactic knowledge of the source language, such as voice (active or passive) and tense information, as a by-product of the translation objective. Our experiment on the importance of composition function was motivated by Vinyals et al. (2015) and Wiseman and Rush (2016), who achieved competitive parsing accuracy without explicit composition. In another work, Li et al. (2015) investigated the importance of recursive tree structures (as opposed to linear recurrent models) in four different tasks, including sentiment and semantic relation classification. Their findings suggest that recursive tree structures are beneficial for tasks that require identifying long-range relations, such as semantic relationship classification, with no conclusive advantage for sentiment classification and discourse parsing. Through the stackonly ablation we demonstrate that the RNNG composition function is crucial to obtaining state-ofthe-art parsing performance. Extensive prior work on"
E17-1117,N16-1082,0,0.0221556,"put the verb as the head of the verb phrase. Another interesting finding is that the model pays attention to polarity information, where negations are almost always assigned nontrivial attention weights.7 Furthermore, we find that the model attends to the conjunction terminal in conjunctions of verb phrases (e.g., “VP → VP and VP”, 10), reinforcing the similar finding for conjunction of noun phrases. PPs. In almost all cases, the model attends to the preposition terminal instead of the noun phrases or complete clauses under it, regardless of the type of preposition. Even when the preposi7 Cf. Li et al. (2016), where sequential LSTMs discover polarity information in sentiment analysis, although perhaps more surprising as polarity information is less intuitively central to syntax and language modeling. 1254 tional phrase is only used to make a connection between two noun phrases (e.g., “PP → NP after NP”, 10), the prepositional connector is still considered the most salient element. This is less consistent with the Collins and Stanford head rules, where prepositions are assigned a lower priority when composing PPs, although more consistent with the Johansson head rule (Johansson and Nugues, 2007). 3"
E17-1117,P92-1017,0,0.673069,"ecessary. If the endocentric hypothesis is true (that is, the representation of a phrase is built from within depending on its components but independent of explicit category labels), then the nonterminal types should be easily inferred given the endocentrically-composed representation, and that ablating the nonterminal information would not make much difference in performance. Specifically, we train a GA-RNNG on unlabeled trees (only bracketings without nonterminal types), denoted U-GA-RNNG. This idea has been explored in research on methods for learning syntax with less complete annotation (Pereira and Schabes, 1992). A key finding from Klein and Manning (2002) was that, 1255 1 2 3 4 5 6 7 8 9 10 Noun phrases Canadian (0.09) Auto (0.31) Workers (0.2) union (0.22) president (0.18) no (0.29) major (0.05) Eurobond (0.32) or (0.01) foreign (0.01) bond (0.1) offerings (0.22) Saatchi (0.12) client (0.14) Philips (0.21) Lighting (0.24) Co. (0.29) nonperforming (0.18) commercial (0.23) real (0.25) estate (0.1) assets (0.25) the (0.1) Jamaica (0.1) Tourist (0.03) Board (0.17) ad (0.20) account (0.40) the (0.0) final (0.18) hour (0.81) their (0.0) first (0.23) test (0.77) Apple (0.62) , (0.02) Compaq (0.1) and (0.0"
E17-1117,P06-1055,0,0.0272857,"information, even when trained on a fairly small amount of syntactic data. SBAR ADVP WHADVP S PP ADVP above NP SBAR because S PP by NP PP to NP SBAR for S PP at NP PP to NP PP after NP PP at NP PP by NP PP to NP PP under NP PP on NP SBAR as S PP ADVP above NP SBAR than S SBAR SBAR and SBAR PP than NP PP about NP PP about NP SBAR WHADVP SBAR that S SBAR that S SBAR that S SBAR S SBAR WHNP S SBAR S SBAR S PP from S SBAR WHNP S SBAR S SBAR S SBAR WHNP S PP of S PP of NP PP of NP PP of NP PP of NP Figure 5: Sample of PP and SBAR phrase representations. 7 and Bikel, 2002; Klein and Manning, 2002; Petrov et al., 2006). In a similar work, Sangati and Zuidema (2009) proposed entropy minimization and greedy familiarity maximization techniques to obtain lexical heads from labeled phrase-structure trees in an unsupervised manner. In contrast, we used neural attention to obtain the “head rules” in the GARNNG; the whole model is trained end-to-end to maximize the log probability of the correct action given the history. Unlike prior work, GARNNG allows the attention weight to be divided among phrase constituents, essentially propagating (weighted) headedness information from multiple components. 8 Related Work The"
E17-1117,E09-1080,0,0.0214366,"ly small amount of syntactic data. SBAR ADVP WHADVP S PP ADVP above NP SBAR because S PP by NP PP to NP SBAR for S PP at NP PP to NP PP after NP PP at NP PP by NP PP to NP PP under NP PP on NP SBAR as S PP ADVP above NP SBAR than S SBAR SBAR and SBAR PP than NP PP about NP PP about NP SBAR WHADVP SBAR that S SBAR that S SBAR that S SBAR S SBAR WHNP S SBAR S SBAR S PP from S SBAR WHNP S SBAR S SBAR S SBAR WHNP S PP of S PP of NP PP of NP PP of NP PP of NP Figure 5: Sample of PP and SBAR phrase representations. 7 and Bikel, 2002; Klein and Manning, 2002; Petrov et al., 2006). In a similar work, Sangati and Zuidema (2009) proposed entropy minimization and greedy familiarity maximization techniques to obtain lexical heads from labeled phrase-structure trees in an unsupervised manner. In contrast, we used neural attention to obtain the “head rules” in the GARNNG; the whole model is trained end-to-end to maximize the log probability of the correct action given the history. Unlike prior work, GARNNG allows the attention weight to be divided among phrase constituents, essentially propagating (weighted) headedness information from multiple components. 8 Related Work The problem of understanding neural network models"
E17-1117,D16-1159,0,0.0208633,"ads from labeled phrase-structure trees in an unsupervised manner. In contrast, we used neural attention to obtain the “head rules” in the GARNNG; the whole model is trained end-to-end to maximize the log probability of the correct action given the history. Unlike prior work, GARNNG allows the attention weight to be divided among phrase constituents, essentially propagating (weighted) headedness information from multiple components. 8 Related Work The problem of understanding neural network models in NLP has been previously studied for sequential RNNs (Karpathy et al., 2015; Li et al., 2016). Shi et al. (2016) showed that sequence-tosequence neural translation models capture a certain degree of syntactic knowledge of the source language, such as voice (active or passive) and tense information, as a by-product of the translation objective. Our experiment on the importance of composition function was motivated by Vinyals et al. (2015) and Wiseman and Rush (2016), who achieved competitive parsing accuracy without explicit composition. In another work, Li et al. (2015) investigated the importance of recursive tree structures (as opposed to linear recurrent models) in four different tasks, including sen"
E17-1117,D16-1137,0,0.0306962,", essentially propagating (weighted) headedness information from multiple components. 8 Related Work The problem of understanding neural network models in NLP has been previously studied for sequential RNNs (Karpathy et al., 2015; Li et al., 2016). Shi et al. (2016) showed that sequence-tosequence neural translation models capture a certain degree of syntactic knowledge of the source language, such as voice (active or passive) and tense information, as a by-product of the translation objective. Our experiment on the importance of composition function was motivated by Vinyals et al. (2015) and Wiseman and Rush (2016), who achieved competitive parsing accuracy without explicit composition. In another work, Li et al. (2015) investigated the importance of recursive tree structures (as opposed to linear recurrent models) in four different tasks, including sentiment and semantic relation classification. Their findings suggest that recursive tree structures are beneficial for tasks that require identifying long-range relations, such as semantic relationship classification, with no conclusive advantage for sentiment classification and discourse parsing. Through the stackonly ablation we demonstrate that the RNNG"
E17-1117,W07-2416,0,0.0109927,"preposi7 Cf. Li et al. (2016), where sequential LSTMs discover polarity information in sentiment analysis, although perhaps more surprising as polarity information is less intuitively central to syntax and language modeling. 1254 tional phrase is only used to make a connection between two noun phrases (e.g., “PP → NP after NP”, 10), the prepositional connector is still considered the most salient element. This is less consistent with the Collins and Stanford head rules, where prepositions are assigned a lower priority when composing PPs, although more consistent with the Johansson head rule (Johansson and Nugues, 2007). 3 2.5 2 1.5 1 ADJP VP NP PP QP SBAR Figure 3: Average perplexity of the learned attention vectors on the test set (blue), as opposed to the average perplexity of the uniform distribution (red), computed for each major phrase type. 5.2 Comparison to Existing Head Rules To better measure the overlap between the attention vectors and existing head rules, we converted the trees in PTB §23 into a dependency representation using the attention weights. In this case, the attention weight functions as a “dynamic” head rule, where all other constituents within the same composed phrase are considered t"
E17-1117,J98-4004,0,0.453425,"and semantic relation classification. Their findings suggest that recursive tree structures are beneficial for tasks that require identifying long-range relations, such as semantic relationship classification, with no conclusive advantage for sentiment classification and discourse parsing. Through the stackonly ablation we demonstrate that the RNNG composition function is crucial to obtaining state-ofthe-art parsing performance. Extensive prior work on phrase-structure parsing typically employs the probabilistic context-free grammar formalism, with lexicalized (Collins, 1997) and nonterminal (Johnson, 1998; Klein and Manning, 2003) augmentations. The conjecture that fine-grained nonterminal rules and labels can be discovered given weaker bracketing structures was based on several studies (Chiang Conclusion We probe what recurrent neural network grammars learn about syntax, through ablation scenarios and a novel variant with a gated attention mechanism on the composition function. The composition function, a key differentiator between the RNNG and other neural models of syntax, is crucial for good performance. Using the attention vectors we discover that the model is learning something similar t"
H05-1036,J98-2004,0,0.0122284,"-cycles in FSMs (Eisner, 2002). 13 One can declare the conditions under which items of a particular type (constit or goal) should be treated as having converged. Then asking for the value of goal will run the agenda algorithm not until the agenda is empty, but only until chart[goal] has converged by this criterion. 4.5 Prioritization The order in which items are chosen at line 4 does not affect the soundness of the agenda algorithm, but can greatly affect its speed. We implement the agenda as a priority queue whose priority function may be specified by the user.15 Charniak et al. (1998) and Caraballo and Charniak (1998) showed that, when seeking the best parse (using min= or max=), best-first parsing can be extremely effective. Klein and Manning (2003a) went on to describe admissible heuristics and an A* framework for parsing. For A* in our general framework, the priority of item a should be an estimate of the value of the best proof of goal that uses a. (This non-standard formulation is carefully chosen.16 ) If so, goal is guaranteed to converge the very first time it is selected from the priority-queue agenda. Prioritizing “good” items first can also be useful in other circumstances. The inside-outside tra"
H05-1036,W98-1115,0,0.0186678,"cycles (Stolcke, 1995), or -cycles in FSMs (Eisner, 2002). 13 One can declare the conditions under which items of a particular type (constit or goal) should be treated as having converged. Then asking for the value of goal will run the agenda algorithm not until the agenda is empty, but only until chart[goal] has converged by this criterion. 4.5 Prioritization The order in which items are chosen at line 4 does not affect the soundness of the agenda algorithm, but can greatly affect its speed. We implement the agenda as a priority queue whose priority function may be specified by the user.15 Charniak et al. (1998) and Caraballo and Charniak (1998) showed that, when seeking the best parse (using min= or max=), best-first parsing can be extremely effective. Klein and Manning (2003a) went on to describe admissible heuristics and an A* framework for parsing. For A* in our general framework, the priority of item a should be an estimate of the value of the best proof of goal that uses a. (This non-standard formulation is carefully chosen.16 ) If so, goal is guaranteed to converge the very first time it is selected from the priority-queue agenda. Prioritizing “good” items first can also be useful in other cir"
H05-1036,P81-1022,0,0.662932,"Missing"
H05-1036,W05-1504,1,0.258721,"the introduction of declarations that control which items use the agenda or are memoized in the chart. This can be used to support lazy or “on-the-fly” computation (Mohri et al., 1998) and asymptotic space-saving tricks (Binder et al., 1997). 7 7.1 Usefulness of the Implementation Applications The current Dyna compiler has proved indispensable in our own recent projects, in the sense that we would not have attempted many of them without it. In some cases, we were experimenting with genuinely new algorithms not supported by any existing tool, as in our work on dependency-lengthlimited parsing (Eisner and Smith, 2005b) and loosely syntax-based machine translation (Eisner and D. Smith, 2005). (Dyna would have been equally helpful in the first author’s earlier work on new algorithms for lexicalized and CCG parsing, syntactic MT, transformational syntax, trainable parameterized FSMs, and finite-state phonology.) In other cases (Smith and Eisner, 2004; Smith and Smith, 2004; Smith et al., 2005), Dyna let us quickly replicate, tweak, and combine useful techniques from the literature. These techniques included unweighted FS morphology, conditional random fields (Lafferty et al., 2001), synchronous parsers (Wu,"
H05-1036,P02-1001,1,0.76106,"ther NP-hard constraint satisfaction problems by using cyclic rules with negation over finitely many boolean-valued items (Niemel¨a, 1998). Here the agenda algorithm can end up flipping values forever between false and true; a more general solver would have to be called in order to find a stable model of a SAT problem’s equations. 14 Still assuming the number of items is finite, one could in principle materialize the system of equations and call a dedicated numerical solver. In some special cases only a linear solver is needed: e.g., for unary rule cycles (Stolcke, 1995), or -cycles in FSMs (Eisner, 2002). 13 One can declare the conditions under which items of a particular type (constit or goal) should be treated as having converged. Then asking for the value of goal will run the agenda algorithm not until the agenda is empty, but only until chart[goal] has converged by this criterion. 4.5 Prioritization The order in which items are chosen at line 4 does not affect the soundness of the agenda algorithm, but can greatly affect its speed. We implement the agenda as a priority queue whose priority function may be specified by the user.15 Charniak et al. (1998) and Caraballo and Charniak (1998) sh"
H05-1036,J99-4004,0,0.748874,"ductive algorithms. Our implemented system encapsulates these implementation techniques behind a clean interface—a small high-level specification language, Dyna, which compiles into C++ classes. This system should help the HLT community to experiment more easily with new models and algorithms. 1.1 Dynamic programming as deduction The “parsing as deduction” framework (Pereira and Warren, 1983) is now over 20 years old. It provides an elegant notation for specifying a variety of parsing algorithms (Shieber et al., 1995), including algorithms for probabilistic or other semiring-weighted parsing (Goodman, 1999). In the parsing community, new algorithms are often stated simply as a set of deductive inference rules (Sikkel, 1997; Eisner and Satta, 1999). It is also straightforward to specify other NLP algorithms this way. Syntactic MT models, language models, and stack decoders can be easily described using deductive rules. So can operations on finitestate and infinite-state machines. ∗ We thank Joshua Goodman, David McAllester, and Paul Ruhlen for useful early discussions; pioneer users Markus Dreyer, David Smith, and Roy Tromble for their feedback and input; John Blatz for discussion of program tran"
H05-1036,P02-1017,0,0.0116196,"orithms for lexicalized and CCG parsing, syntactic MT, transformational syntax, trainable parameterized FSMs, and finite-state phonology.) In other cases (Smith and Eisner, 2004; Smith and Smith, 2004; Smith et al., 2005), Dyna let us quickly replicate, tweak, and combine useful techniques from the literature. These techniques included unweighted FS morphology, conditional random fields (Lafferty et al., 2001), synchronous parsers (Wu, 1997; Melamed, 2003), lexicalized parsers (Eisner and Satta, 1999),22 partially supervised training a` la (Pereira and Schabes, 1992),23 and grammar induction (Klein and Manning, 2002). These replications were easy to write and extend, and to train via §5.2. 7.2 Experiments We compared the current Dyna compiler to handbuilt systems on a variety of parsing tasks. These problems were chosen not for their novelty or interesting structure, but for the availability of existing well-tuned implementations. Best parse. We compared a Dyna CFG parser to the Java parser of Klein and Manning (2003b),24 22 Markus Dreyer’s reimplementation of the complex Collins (1999) parser uses under 30 lines of Dyna. 23 For example, lines 2–3 of Fig. 1 can be extended with whenever permitted(X,I,K)."
H05-1036,N03-1016,0,0.0597224,"ed as having converged. Then asking for the value of goal will run the agenda algorithm not until the agenda is empty, but only until chart[goal] has converged by this criterion. 4.5 Prioritization The order in which items are chosen at line 4 does not affect the soundness of the agenda algorithm, but can greatly affect its speed. We implement the agenda as a priority queue whose priority function may be specified by the user.15 Charniak et al. (1998) and Caraballo and Charniak (1998) showed that, when seeking the best parse (using min= or max=), best-first parsing can be extremely effective. Klein and Manning (2003a) went on to describe admissible heuristics and an A* framework for parsing. For A* in our general framework, the priority of item a should be an estimate of the value of the best proof of goal that uses a. (This non-standard formulation is carefully chosen.16 ) If so, goal is guaranteed to converge the very first time it is selected from the priority-queue agenda. Prioritizing “good” items first can also be useful in other circumstances. The inside-outside training algorithm requires one to find all parses, but finding the high-probability parses first allows one to ignore the rest by “early"
H05-1036,P03-1054,0,0.136066,"ed as having converged. Then asking for the value of goal will run the agenda algorithm not until the agenda is empty, but only until chart[goal] has converged by this criterion. 4.5 Prioritization The order in which items are chosen at line 4 does not affect the soundness of the agenda algorithm, but can greatly affect its speed. We implement the agenda as a priority queue whose priority function may be specified by the user.15 Charniak et al. (1998) and Caraballo and Charniak (1998) showed that, when seeking the best parse (using min= or max=), best-first parsing can be extremely effective. Klein and Manning (2003a) went on to describe admissible heuristics and an A* framework for parsing. For A* in our general framework, the priority of item a should be an estimate of the value of the best proof of goal that uses a. (This non-standard formulation is carefully chosen.16 ) If so, goal is guaranteed to converge the very first time it is selected from the priority-queue agenda. Prioritizing “good” items first can also be useful in other circumstances. The inside-outside training algorithm requires one to find all parses, but finding the high-probability parses first allows one to ignore the rest by “early"
H05-1036,N03-1021,0,0.0103129,"loosely syntax-based machine translation (Eisner and D. Smith, 2005). (Dyna would have been equally helpful in the first author’s earlier work on new algorithms for lexicalized and CCG parsing, syntactic MT, transformational syntax, trainable parameterized FSMs, and finite-state phonology.) In other cases (Smith and Eisner, 2004; Smith and Smith, 2004; Smith et al., 2005), Dyna let us quickly replicate, tweak, and combine useful techniques from the literature. These techniques included unweighted FS morphology, conditional random fields (Lafferty et al., 2001), synchronous parsers (Wu, 1997; Melamed, 2003), lexicalized parsers (Eisner and Satta, 1999),22 partially supervised training a` la (Pereira and Schabes, 1992),23 and grammar induction (Klein and Manning, 2002). These replications were easy to write and extend, and to train via §5.2. 7.2 Experiments We compared the current Dyna compiler to handbuilt systems on a variety of parsing tasks. These problems were chosen not for their novelty or interesting structure, but for the availability of existing well-tuned implementations. Best parse. We compared a Dyna CFG parser to the Java parser of Klein and Manning (2003b),24 22 Markus Dreyer’s rei"
H05-1036,P96-1033,0,0.170489,"mputing goal is equivalent to an aggregation over many separate parse trees. That is not the case for heterogeneous programs. 283 pressions. This allows specification of a wider class of algorithms from NLP and elsewhere (e.g., minimum expected loss decoding, smoothing formulas, neural networks, game tree analysis, and constraint programming). Although §4 and §5 have space to present only techniques for the semiring case, these can be generalized. Our approach may be most closely related to deductive databases, which even in their heyday were apparently ignored by the CL community (except for Minnen, 1996). Deductive database systems permit inference rules that can derive new database facts from old ones.5 They are essentially declarative logic programming languages (with restrictions or extensions) that are—or could be—implemented using efficient database techniques. Some implemented deductive databases such as CORAL (Ramakrishnan et al., 1994) and LOLA (Zukowski and Freitag, 1997) support aggregation (as in Dyna’s +=, log+=, max=, . . . ), although only “stratified” forms of it that exclude unary CFG rule cycles.6 Ross and Sagiv (1992) (and in a more restricted way, Kifer and Subrahmanian, 19"
H05-1036,J03-1006,0,0.576175,"e with parameter optimization code. Second, we fully generalize the agenda-based strategy of Shieber et al. (1995) to the weighted case—in particular supporting a prioritized agenda. That allows probabilities to guide the search for the best parse(s), a crucial technique in state-of-theart context-free parsers.3 We also give a “reverse” agenda algorithm to compute gradients or outside probabilities for parameter estimation. Third, regarding weights, the Dyna language is designed to express systems of arbitrary, heterogeneous equations over item values. In previous work such as (Goodman, 1999; Nederhof, 2003), one only specifies the inference rules as unweighted Horn clauses, and then weights are added automatically in a standard way: all values have the same type W, and all rules transform to equations of the form c ⊕= a1 ⊗ a2 ⊗ · · · ⊗ ak , where ⊕ and ⊗ give W the structure of a semiring.4 In Dyna one writes these equations explicitly in place of Horn clauses (Fig. 1). Accordingly, heterogeneous Dyna programs, to be supported soon by our compiler, will allow items of different types to have values of different types, computed by different aggregation operations over arbitrary right-hand-side ex"
H05-1036,P92-1017,0,0.101118,"helpful in the first author’s earlier work on new algorithms for lexicalized and CCG parsing, syntactic MT, transformational syntax, trainable parameterized FSMs, and finite-state phonology.) In other cases (Smith and Eisner, 2004; Smith and Smith, 2004; Smith et al., 2005), Dyna let us quickly replicate, tweak, and combine useful techniques from the literature. These techniques included unweighted FS morphology, conditional random fields (Lafferty et al., 2001), synchronous parsers (Wu, 1997; Melamed, 2003), lexicalized parsers (Eisner and Satta, 1999),22 partially supervised training a` la (Pereira and Schabes, 1992),23 and grammar induction (Klein and Manning, 2002). These replications were easy to write and extend, and to train via §5.2. 7.2 Experiments We compared the current Dyna compiler to handbuilt systems on a variety of parsing tasks. These problems were chosen not for their novelty or interesting structure, but for the availability of existing well-tuned implementations. Best parse. We compared a Dyna CFG parser to the Java parser of Klein and Manning (2003b),24 22 Markus Dreyer’s reimplementation of the complex Collins (1999) parser uses under 30 lines of Dyna. 23 For example, lines 2–3 of Fig."
H05-1036,P83-1021,0,0.727088,"e that is efficient enough for real NLP research, though still several times slower than hand-crafted code. 1 Introduction In this paper, we generalize some modern probabilistic parsing techniques to a broader class of weighted deductive algorithms. Our implemented system encapsulates these implementation techniques behind a clean interface—a small high-level specification language, Dyna, which compiles into C++ classes. This system should help the HLT community to experiment more easily with new models and algorithms. 1.1 Dynamic programming as deduction The “parsing as deduction” framework (Pereira and Warren, 1983) is now over 20 years old. It provides an elegant notation for specifying a variety of parsing algorithms (Shieber et al., 1995), including algorithms for probabilistic or other semiring-weighted parsing (Goodman, 1999). In the parsing community, new algorithms are often stated simply as a set of deductive inference rules (Sikkel, 1997; Eisner and Satta, 1999). It is also straightforward to specify other NLP algorithms this way. Syntactic MT models, language models, and stack decoders can be easily described using deductive rules. So can operations on finitestate and infinite-state machines. ∗"
H05-1036,P04-1062,1,0.883504,"es in the objective function as the axiom weights change, (3) can prune by skipping useless updates a that scarcely affected goal (e.g., 5.3 Parameter estimation To support parameter training using these gradients, our implementation of Dyna includes a training module, DynaMITE. DynaMITE supports the EM algorithm (and many variants), supervised and unsupervised training of log-linear (“maximum entropy”) models using quasi-Newton methods, and smoothing-parameter tuning on development data. As an object-oriented C++ library, it also facilitates rapid implementation of new estimation techniques (Smith and Eisner, 2004; Smith and Eisner, 2005). 6 Program Transformations Another interest of Dyna is that its high-level specifications can be manipulated by mechanical sourceto-source program transformations. This makes it possible to derive new algorithms from old ones. §5.1 already sketched the gradient transformation for finding ∇goal. We note a few other examples. Bounding transformations generate a new program that computes upper or lower bounds on goal, via generic bounding techniques (Prieditis, 1993; Culberson and Schaeffer, 1998). The A* heuristics explored by Klein and Manning (2003a) can be seen as re"
H05-1036,P05-1044,1,0.54338,"tion as the axiom weights change, (3) can prune by skipping useless updates a that scarcely affected goal (e.g., 5.3 Parameter estimation To support parameter training using these gradients, our implementation of Dyna includes a training module, DynaMITE. DynaMITE supports the EM algorithm (and many variants), supervised and unsupervised training of log-linear (“maximum entropy”) models using quasi-Newton methods, and smoothing-parameter tuning on development data. As an object-oriented C++ library, it also facilitates rapid implementation of new estimation techniques (Smith and Eisner, 2004; Smith and Eisner, 2005). 6 Program Transformations Another interest of Dyna is that its high-level specifications can be manipulated by mechanical sourceto-source program transformations. This makes it possible to derive new algorithms from old ones. §5.1 already sketched the gradient transformation for finding ∇goal. We note a few other examples. Bounding transformations generate a new program that computes upper or lower bounds on goal, via generic bounding techniques (Prieditis, 1993; Culberson and Schaeffer, 1998). The A* heuristics explored by Klein and Manning (2003a) can be seen as resulting from bounding tra"
H05-1036,W04-3207,1,0.526925,"nt projects, in the sense that we would not have attempted many of them without it. In some cases, we were experimenting with genuinely new algorithms not supported by any existing tool, as in our work on dependency-lengthlimited parsing (Eisner and Smith, 2005b) and loosely syntax-based machine translation (Eisner and D. Smith, 2005). (Dyna would have been equally helpful in the first author’s earlier work on new algorithms for lexicalized and CCG parsing, syntactic MT, transformational syntax, trainable parameterized FSMs, and finite-state phonology.) In other cases (Smith and Eisner, 2004; Smith and Smith, 2004; Smith et al., 2005), Dyna let us quickly replicate, tweak, and combine useful techniques from the literature. These techniques included unweighted FS morphology, conditional random fields (Lafferty et al., 2001), synchronous parsers (Wu, 1997; Melamed, 2003), lexicalized parsers (Eisner and Satta, 1999),22 partially supervised training a` la (Pereira and Schabes, 1992),23 and grammar induction (Klein and Manning, 2002). These replications were easy to write and extend, and to train via §5.2. 7.2 Experiments We compared the current Dyna compiler to handbuilt systems on a variety of parsing ta"
H05-1036,H05-1060,1,0.608751,"se that we would not have attempted many of them without it. In some cases, we were experimenting with genuinely new algorithms not supported by any existing tool, as in our work on dependency-lengthlimited parsing (Eisner and Smith, 2005b) and loosely syntax-based machine translation (Eisner and D. Smith, 2005). (Dyna would have been equally helpful in the first author’s earlier work on new algorithms for lexicalized and CCG parsing, syntactic MT, transformational syntax, trainable parameterized FSMs, and finite-state phonology.) In other cases (Smith and Eisner, 2004; Smith and Smith, 2004; Smith et al., 2005), Dyna let us quickly replicate, tweak, and combine useful techniques from the literature. These techniques included unweighted FS morphology, conditional random fields (Lafferty et al., 2001), synchronous parsers (Wu, 1997; Melamed, 2003), lexicalized parsers (Eisner and Satta, 1999),22 partially supervised training a` la (Pereira and Schabes, 1992),23 and grammar induction (Klein and Manning, 2002). These replications were easy to write and extend, and to train via §5.2. 7.2 Experiments We compared the current Dyna compiler to handbuilt systems on a variety of parsing tasks. These problems w"
H05-1036,J95-2002,0,0.0781184,"(say) hW, ⊕, ⊗i = hR≥0 , min, +i, where values are negated log probabilities. Positive-weight cycles will not affect the min. (Negative-weight cycles, however, would correctly cause the computation to diverge; these do not arise with probabilities.) If one is using hW, ⊕, ⊗i = hR≥0 , +, ∗i to compute the total weight of all proofs or parses, as in the inside algorithm, then Dyna must solve a system of nonlinear equations. The agenda algorithm does this by iterative approximation (propagating updates around any cycles in the proof graph until numerical convergence), essentially as suggested by Stolcke (1995) for the case of Earley’s algorithm.14 Again, the computation may diverge. 12 This holds for all Datalog programs, for instance. This argument does not hold if Dyna is used to express programs outside the semiring. In particular, one can write instances of SAT and other NP-hard constraint satisfaction problems by using cyclic rules with negation over finitely many boolean-valued items (Niemel¨a, 1998). Here the agenda algorithm can end up flipping values forever between false and true; a more general solver would have to be called in order to find a stable model of a SAT problem’s equations. 1"
H05-1036,J97-3002,0,0.0265683,"2005b) and loosely syntax-based machine translation (Eisner and D. Smith, 2005). (Dyna would have been equally helpful in the first author’s earlier work on new algorithms for lexicalized and CCG parsing, syntactic MT, transformational syntax, trainable parameterized FSMs, and finite-state phonology.) In other cases (Smith and Eisner, 2004; Smith and Smith, 2004; Smith et al., 2005), Dyna let us quickly replicate, tweak, and combine useful techniques from the literature. These techniques included unweighted FS morphology, conditional random fields (Lafferty et al., 2001), synchronous parsers (Wu, 1997; Melamed, 2003), lexicalized parsers (Eisner and Satta, 1999),22 partially supervised training a` la (Pereira and Schabes, 1992),23 and grammar induction (Klein and Manning, 2002). These replications were easy to write and extend, and to train via §5.2. 7.2 Experiments We compared the current Dyna compiler to handbuilt systems on a variety of parsing tasks. These problems were chosen not for their novelty or interesting structure, but for the availability of existing well-tuned implementations. Best parse. We compared a Dyna CFG parser to the Java parser of Klein and Manning (2003b),24 22 Mar"
H05-1036,W06-3104,1,\N,Missing
H05-1036,J03-4003,0,\N,Missing
H05-1036,P99-1059,1,\N,Missing
H05-1060,W98-1110,0,0.0718073,"Missing"
H05-1060,W02-0506,0,0.0624393,"Missing"
H05-1060,W04-3246,0,0.0645424,"Missing"
H05-1060,N04-4038,0,0.0539525,"Missing"
H05-1060,P05-1071,0,0.26518,"Missing"
H05-1060,P01-1035,0,0.648454,"Missing"
H05-1060,C00-1042,0,0.24983,"Missing"
H05-1060,J00-1006,0,0.0185782,"Missing"
H05-1060,W04-3230,0,0.0214129,", θ~ = log P ps y0 |θ~ 0 y ∈d(x) The sum in the denominator is computed using a dynamic programming algorithm (akin to the forward algorithm); it involves computing the sum of all paths through the “sausage” lattice of possible analyses for x. By doing this, we allow knowledge of the support of the channel model to enter into our estimation of the source model. It is important to note that the estimation of the model (the objective function used in training, Eq. 3) is distinct from the source-channel structure of the model (Eq. 1). The lattice-conditional estimation approach was first used by Kudo et al. (2004) for Japanese segmentation and hierarchical POS-tagging and by Smith and Smith (2004) for Korean morphological disambiguation. The resulting model is an instance of a conditional random field (CRF; Lafferty et al., 2001). When training a CRF for POS tagging, IOB chunking (Sha and Pereira, 2003), or word segmentation (Peng et al., 2004), one typically structures the conditional probabilities (in the objective function) using domain knowledge: in POS tagging, the set of allowed tags for a word is used; in IOB chunking, the bigram “O I” is disallowed; and in segmentation, a lexicon is used to enu"
H05-1060,J95-3004,0,0.151142,"r sentences in three languages. In this paper, the dictionary defines the support set of the channel model. That is, pc (x |y) &gt; 0 if and only if y ∈ d(x). This is a clean way to incorporate domain knowledge into the probabilistic model; this kind of constraint has been applied in previous work at decoding time (Hakkani-T¨ur et al., 2000; Hajiˇc et al., 2001). In such a model, each word is independent of its neighbors (because the dictionary ignores context). Estimation. A unigram channel model defines 2 Probabilistic modeling of what we call the morphological channel was first carried out by Levinger et al. (1995), who used unlabeled data to estimate p(~ y |x) for Hebrew, with the support defined by a dictionary. 476 def pc (x |y) = |x| Y p(xi |yi ) (2) i=1 The simplest estimate of this model is to make p(·, ·) uniform over (x, ~y ) such that ~y ∈ d(x). Doing so and marginalizing to get p(x |~y ) makes the channel model encode categorial information only, leaving all learning to the source model.3 Another way to estimate this model is, of course, from data. This is troublesome, because—modulo optionality—x is expected to be known given ~y , resulting in a huge model with mostly 1-valued probabilities."
H05-1060,C04-1081,0,0.0254105,"ation of the source model. It is important to note that the estimation of the model (the objective function used in training, Eq. 3) is distinct from the source-channel structure of the model (Eq. 1). The lattice-conditional estimation approach was first used by Kudo et al. (2004) for Japanese segmentation and hierarchical POS-tagging and by Smith and Smith (2004) for Korean morphological disambiguation. The resulting model is an instance of a conditional random field (CRF; Lafferty et al., 2001). When training a CRF for POS tagging, IOB chunking (Sha and Pereira, 2003), or word segmentation (Peng et al., 2004), one typically structures the conditional probabilities (in the objective function) using domain knowledge: in POS tagging, the set of allowed tags for a word is used; in IOB chunking, the bigram “O I” is disallowed; and in segmentation, a lexicon is used to enumerate the possible word boundaries.4 4 This refinement is in the same vein as the move from maximum likelihood estimation to conditional estimation. MLE would make the sum in the denominator of Eq. 4 Y+ , which for log-linear models is often intractable to compute (and for sequence models may not converge). Conditional estimation limi"
H05-1060,N03-1028,0,0.00899842,"t of the channel model to enter into our estimation of the source model. It is important to note that the estimation of the model (the objective function used in training, Eq. 3) is distinct from the source-channel structure of the model (Eq. 1). The lattice-conditional estimation approach was first used by Kudo et al. (2004) for Japanese segmentation and hierarchical POS-tagging and by Smith and Smith (2004) for Korean morphological disambiguation. The resulting model is an instance of a conditional random field (CRF; Lafferty et al., 2001). When training a CRF for POS tagging, IOB chunking (Sha and Pereira, 2003), or word segmentation (Peng et al., 2004), one typically structures the conditional probabilities (in the objective function) using domain knowledge: in POS tagging, the set of allowed tags for a word is used; in IOB chunking, the bigram “O I” is disallowed; and in segmentation, a lexicon is used to enumerate the possible word boundaries.4 4 This refinement is in the same vein as the move from maximum likelihood estimation to conditional estimation. MLE would make the sum in the denominator of Eq. 4 Y+ , which for log-linear models is often intractable to compute (and for sequence models may"
H05-1060,W04-3207,1,0.901276,"ynamic programming algorithm (akin to the forward algorithm); it involves computing the sum of all paths through the “sausage” lattice of possible analyses for x. By doing this, we allow knowledge of the support of the channel model to enter into our estimation of the source model. It is important to note that the estimation of the model (the objective function used in training, Eq. 3) is distinct from the source-channel structure of the model (Eq. 1). The lattice-conditional estimation approach was first used by Kudo et al. (2004) for Japanese segmentation and hierarchical POS-tagging and by Smith and Smith (2004) for Korean morphological disambiguation. The resulting model is an instance of a conditional random field (CRF; Lafferty et al., 2001). When training a CRF for POS tagging, IOB chunking (Sha and Pereira, 2003), or word segmentation (Peng et al., 2004), one typically structures the conditional probabilities (in the objective function) using domain knowledge: in POS tagging, the set of allowed tags for a word is used; in IOB chunking, the bigram “O I” is disallowed; and in segmentation, a lexicon is used to enumerate the possible word boundaries.4 4 This refinement is in the same vein as the mo"
H05-1060,P05-1003,0,0.0053139,"etraining. Prior work (factored training). Separately training different models that predict the same variables (e.g., x and y) then combining them for consensus-based inference (either through a mixture or a product of probabilities) is an old idea (Genest and Zidek, 1986). Recent work in learning weights for the component “expert” models has turned to cooperative techniques (Hinton, 1999). Decoding that finds y (given x) to maximize some weighted average of log-probabilities is known as a logarithmic opinion pool (LOP). LOPs were applied to CRFs (for named entity recognition and tagging) by Smith et al. (2005), with an eye toward regularization. Their experts (each a CRF) contained overlapping feature sets, and the combined model achieved much the same effect as training a single model with smoothing. Note that our models, unlike theirs, partition the feature space; there is only one CRF, but some parameters are ignored when estimating other parameters. We have not estimated log-domain mixing coefficients—we weight all models’ contributions equally. Sutton and McCallum (2005) have applied factored estimation to CRFs, motivated (like us) by speed; they also describe how factored estimation 9 Lemma-t"
J03-3002,J90-2002,0,0.654334,"the Internet Archive for mining parallel text from the Web on a large scale. Finally, the value of these techniques is demonstrated in the construction of a significant parallel corpus for a low-density language pair. 1. Introduction Parallel corpora—bodies of text in parallel translation, also known as bitexts—have taken on an important role in machine translation and multilingual natural language processing. They represent resources for automatic lexical acquisition (e.g., Gale and Church 1991; Melamed 1997), they provide indispensable training data for statistical translation models (e.g., Brown et al. 1990; Melamed 2000; Och and Ney 2002), and they can provide the connection between vocabularies in cross-language information retrieval (e.g., Davis and Dunning 1995; Landauer and Littman 1990; see also Oard 1997). More recently, researchers at Johns Hopkins University and the University of Maryland have been exploring new ways to exploit parallel corpora in order to develop monolingual resources and tools, using a process of annotation, projection, and training: Given a parallel corpus in English and a less resource-rich language, we project English annotations across the parallel corpus to the s"
J03-3002,J93-1001,0,0.0541204,"Johns Hopkins University, Baltimore, MD 21218. E-mail: nasmith@cs.jhu.edu c 2003 Association for Computational Linguistics  Computational Linguistics Volume 29, Number 3 Melamed 1997; Menezes and Richardson 2001), and the like. Even for the top handful of majority languages, the available parallel corpora tend to be unbalanced, representing primarily governmental or newswire-style texts. In addition, like other language resources, parallel corpora are often encumbered by fees or licensing restrictions. For all these reasons, it is difficult to follow the “more data are better data” advice of Church and Mercer (1993), abandoning balance in favor of volume, with respect to parallel text. Then there is the World Wide Web. People tend to see the Web as a reflection of their own way of viewing the world—as a huge semantic network, or an enormous historical archive, or a grand social experiment. We are no different: As computational linguists working on multilingual issues, we view the Web as a great big body of text waiting to be mined, a huge fabric of linguistic data often interwoven with parallel threads. This article describes our techniques for mining the Web in order to extract the parallel text it cont"
J03-3002,W02-0506,0,0.0174661,"Missing"
J03-3002,P02-1033,1,0.23898,"1990; see also Oard 1997). More recently, researchers at Johns Hopkins University and the University of Maryland have been exploring new ways to exploit parallel corpora in order to develop monolingual resources and tools, using a process of annotation, projection, and training: Given a parallel corpus in English and a less resource-rich language, we project English annotations across the parallel corpus to the second language, using word-level alignments as the bridge, and then use robust statistical techniques in learning from the resulting noisy annotations (Cabezas, Dorr, and Resnik 2001; Diab and Resnik 2002; Hwa et al. 2002; Lopez et al. 2002; Yarowsky, Ngai, and Wicentowski 2001; Yarowsky and Ngai 2001; Riloff, Schafer, and Yarowsky 2002). For these reasons, parallel corpora can be thought of as a critical resource. Unfortunately, they are not readily available in the necessary quantities. Until very recently, for example, statistical work in machine translation focused heavily on French-English translation because the Canadian parliamentary proceedings (Hansards) in English and French were the only large bitext available. Things have improved somewhat, but it is still fair to say that for all"
J03-3002,H91-1026,0,0.0603442,"performance, a new contentbased measure of translational equivalence, and adaptation of the system to take advantage of the Internet Archive for mining parallel text from the Web on a large scale. Finally, the value of these techniques is demonstrated in the construction of a significant parallel corpus for a low-density language pair. 1. Introduction Parallel corpora—bodies of text in parallel translation, also known as bitexts—have taken on an important role in machine translation and multilingual natural language processing. They represent resources for automatic lexical acquisition (e.g., Gale and Church 1991; Melamed 1997), they provide indispensable training data for statistical translation models (e.g., Brown et al. 1990; Melamed 2000; Och and Ney 2002), and they can provide the connection between vocabularies in cross-language information retrieval (e.g., Davis and Dunning 1995; Landauer and Littman 1990; see also Oard 1997). More recently, researchers at Johns Hopkins University and the University of Maryland have been exploring new ways to exploit parallel corpora in order to develop monolingual resources and tools, using a process of annotation, projection, and training: Given a parallel co"
J03-3002,P02-1050,1,0.32216,"Missing"
J03-3002,1999.mtsummit-1.79,0,0.806817,"rther discussion in Section 4.3), and a modest amount of monolingual data for training n-gram-based language identifiers (typically 50,000 to 100,000 characters of text per language). Word-level translations are worth exploiting when they are available. In Section 3 we describe a bitext-matching process using a content-based similarity score grounded in information theory, and in Section 5 we show how structural and content-based criteria can be combined in order to obtain performance superior to that obtained using either method alone. 5 Many details of this technique are left unspecified in Ma and Liberman (1999), such as the threshold for the similarity score, the distance threshold, and matching of non-one-word to one-word entries in the dictionary. 359 Computational Linguistics X: Maria does NULL Y: Maria n’ n’t Volume 29, Number 3 like NULL aime fruit NULL pas de fruits Figure 4 An example of two texts with links shown. There are seven link tokens, five of which are lexical (non-NULL) in X (the English side), six in Y (French). 3. Content-Based Matching The approach discussed thus far relies heavily on document structure. However, as Ma and Liberman (1999) point out, not all translators create tra"
J03-3002,W97-0311,0,0.0125252,"tentbased measure of translational equivalence, and adaptation of the system to take advantage of the Internet Archive for mining parallel text from the Web on a large scale. Finally, the value of these techniques is demonstrated in the construction of a significant parallel corpus for a low-density language pair. 1. Introduction Parallel corpora—bodies of text in parallel translation, also known as bitexts—have taken on an important role in machine translation and multilingual natural language processing. They represent resources for automatic lexical acquisition (e.g., Gale and Church 1991; Melamed 1997), they provide indispensable training data for statistical translation models (e.g., Brown et al. 1990; Melamed 2000; Och and Ney 2002), and they can provide the connection between vocabularies in cross-language information retrieval (e.g., Davis and Dunning 1995; Landauer and Littman 1990; see also Oard 1997). More recently, researchers at Johns Hopkins University and the University of Maryland have been exploring new ways to exploit parallel corpora in order to develop monolingual resources and tools, using a process of annotation, projection, and training: Given a parallel corpus in English"
J03-3002,J00-2004,0,0.516677,"e for mining parallel text from the Web on a large scale. Finally, the value of these techniques is demonstrated in the construction of a significant parallel corpus for a low-density language pair. 1. Introduction Parallel corpora—bodies of text in parallel translation, also known as bitexts—have taken on an important role in machine translation and multilingual natural language processing. They represent resources for automatic lexical acquisition (e.g., Gale and Church 1991; Melamed 1997), they provide indispensable training data for statistical translation models (e.g., Brown et al. 1990; Melamed 2000; Och and Ney 2002), and they can provide the connection between vocabularies in cross-language information retrieval (e.g., Davis and Dunning 1995; Landauer and Littman 1990; see also Oard 1997). More recently, researchers at Johns Hopkins University and the University of Maryland have been exploring new ways to exploit parallel corpora in order to develop monolingual resources and tools, using a process of annotation, projection, and training: Given a parallel corpus in English and a less resource-rich language, we project English annotations across the parallel corpus to the second language"
J03-3002,W01-1406,0,0.00995262,"Missing"
J03-3002,P02-1038,0,0.0881138,"arallel text from the Web on a large scale. Finally, the value of these techniques is demonstrated in the construction of a significant parallel corpus for a low-density language pair. 1. Introduction Parallel corpora—bodies of text in parallel translation, also known as bitexts—have taken on an important role in machine translation and multilingual natural language processing. They represent resources for automatic lexical acquisition (e.g., Gale and Church 1991; Melamed 1997), they provide indispensable training data for statistical translation models (e.g., Brown et al. 1990; Melamed 2000; Och and Ney 2002), and they can provide the connection between vocabularies in cross-language information retrieval (e.g., Davis and Dunning 1995; Landauer and Littman 1990; see also Oard 1997). More recently, researchers at Johns Hopkins University and the University of Maryland have been exploring new ways to exploit parallel corpora in order to develop monolingual resources and tools, using a process of annotation, projection, and training: Given a parallel corpus in English and a less resource-rich language, we project English annotations across the parallel corpus to the second language, using word-level"
J03-3002,resnik-1998-parallel,1,0.538868,"the Web as a reflection of their own way of viewing the world—as a huge semantic network, or an enormous historical archive, or a grand social experiment. We are no different: As computational linguists working on multilingual issues, we view the Web as a great big body of text waiting to be mined, a huge fabric of linguistic data often interwoven with parallel threads. This article describes our techniques for mining the Web in order to extract the parallel text it contains. It presents, in revised and considerably extended form, our early work on mining the Web for bilingual text (STRAND) (Resnik 1998, 1999), incorporating new work on content-based detection of translations (Smith 2001, 2002), and efficient exploitation of the Internet Archive. In Section 2 we lay out the STRAND architecture, which is based on the insight that translated Web pages tend quite strongly to exhibit parallel structure, permitting them to be identified even without looking at content; we also show how we have improved STRAND’s performance by training a supervised classifier using structural parameters rather than relying on manually tuned thresholds. In Section 3 we present an approach to detecting translations"
J03-3002,P99-1068,1,0.418713,"Missing"
J03-3002,A97-1050,1,0.37856,"Missing"
J03-3002,H01-1033,1,0.330058,"Missing"
J03-3002,C02-1070,0,0.0162036,"Missing"
J03-3002,W02-1013,1,0.593288,"sim to tsim = number of two-word links in best matching number of links in best matching (4) The key reason to compute tsim under the equiprobability assumption is that we need not compute the MWBM, but may find just the maximum cardinality bipartite √ matching (MCBM), since all potential links have the same weight. An O(e v) (or  O(|X |· |Y |· |X |+ |Y|) for this purpose) algorithm exists for MCBM (Ahuja, Magnati, and Orlin 1993). For example, if the matching shown in Figure 4 is the MCBM (for some translation lexicon), then tsim(X, Y) = 47 under the simplifying assumption. In earlier work (Smith 2002), we sought to show how multiple linguistic resources could be exploited in combination to recognize translation, and how the equiprobability assumption allowed straightforward combination of resources (i.e., set union of translation lexicon entries). In Section 3.2.1 we provide a clean solution to the problem of using unweighted translation lexicons along with probabilistic ones that improves performance over the earlier result. This would appear to make the equiprobability assumption unnecessary (apart from concerns about computational expense). However, we found that, if p(x, y) is set to t"
J03-3002,W99-0626,0,0.0331893,"Missing"
J03-3002,P95-1026,0,0.20594,"gm for the construction of parallel corpora. Beginning with a seed set of translation information (either parallel corpora or a bilingual dictionary), high-precision initial classifiers might be constructed using content and/or structural features (whichever are available). We might then iteratively select additional page pairs in which the current classifier has high confidence of translational equivalence, gradually increasing the pool of parallel data and at the same time expanding the bilingual lexicon. This approach to minimally supervised classifier construction has been widely studied (Yarowsky 1995), especially in cases in which the features of interest are orthogonal in some sense (e.g., Blum and Mitchell 1998; Abney 2002). With respect to the generation of candidate pairs, we have described a progression from index-based searches on AltaVista to exhaustive matching of URLs on the Internet Archive. The combination of these approaches may be profitable, particularly for languages that are represented only very sparsely on the Web. For such languages, index-based searches on words from a language of interest might be used to identify sites potentially containing parallel text. Within such"
J03-3002,N01-1026,0,0.171684,"Missing"
J03-3002,H01-1035,0,0.0255538,"Missing"
J03-3002,2001.mtsummit-ebmt.4,0,\N,Missing
J03-3002,P02-1046,0,\N,Missing
J07-4003,P99-1070,0,0.465814,"Missing"
J07-4003,P98-1035,0,0.0112557,"ion), or both. (Note that the choice of estimation procedure (b) is in principle orthogonal to the choice of model, and conditional estimation should not be conflated with log-linear modeling.) For a given estimation criterion, weighted CFGs, and Mealy MRFs, in particular, cannot be expected to behave any differently than PCFGs and HMMs, respectively, unless they are augmented with more features. 6. Related Work Abney, McAllester, and Pereira (1999) addressed the relationship between PCFGs and probabilistic models based on push-down automaton operations (e.g., the structured language model of Chelba and Jelinek, 1998). They proved that, although the conversion may not be simple (indeed, a blow-up in the automaton’s size may be incurred), given G, PPCFG and the set of distributions expressible by shift-reduce probabilistic push-down automata are weakly equivalent. Importantly, the standard conversion of a CFG into a shift-reduce PDA, when applied in the stochastic case, does not always preserve the probability distribution over trees. Our Theorem 2 bears a resemblance to that result. Further work on the relationship between weighted CFGs and weighted PDAs is described in Nederhof and Satta (2004). MacKay (1"
J07-4003,J99-1004,0,0.0973983,"expressive power of weighted context-free grammars (WCFGs), where each rule is associated with a positive weight, to that of the corresponding PCFGs, that is, with the same rules but where the weights of the rules expanding a nonterminal must sum to one. One might expect that because normalization removes one or more degrees of freedom, unnormalized models should be more expressive than normalized, probabilistic models. Perhaps counterintuitively, previous work has shown that the classes of probability distributions defined by WCFGs and PCFGs are the same (Abney, McAllester, and Pereira 1999; Chi 1999). However, this result does not completely settle the question about the expressive power of WCFGs and PCFGs. As we show herein, a WCFG can define a conditional distribution from strings to trees even if it does not define a probability distribution over trees. Because these conditional distributions are what are used in classification tasks and related tasks such as parsing, we need to know the relationship between the classes of conditional distributions defined by WCFGs and PCFGs. In fact we extend the results of Chi and of Abney et al., and show that WCFGs and PCFGs both define the same cl"
J07-4003,P01-1042,1,0.859052,"te that even if we allow the 8 The HMM shown in Example 3 can be factored into a Moore HMM without any change to the distribution. 488 Smith and Johnson Weighted and Probabilistic CFGs MEMM to “look back” and condition on earlier symbols (or states), it cannot represent the distribution in Example 3.  Generally speaking, this limitation of MEMMs has nothing to do with the estimation procedure (we have committed to no estimation procedure in particular) but rather with the conditional structure of the model. That some model structures work better than others at real NLP tasks was discussed by Johnson (2001) and Klein and Manning (2002). Our result—that the class of distributions allowed by MEMMs is a strict subset of those allowed by Mealy HMMs—makes this unsurprising. 5. Practical Implications Our result is that weighted generalizations of classical probabilistic grammars (PCFGs and HMMs) are no more powerful than the probabilistic models. This means that, insofar as log-linear models for NLP tasks like tagging and parsing are more successful than their probabilistic cousins, it is due to either (a) additional features added to the model, (b) improved estimation procedures (e.g., maximum condit"
J07-4003,P99-1069,1,0.367445,"ic Sciences, Brown University, Providence, RI 02912, USA. E-mail: Mark Johnson@brown.edu. Submission received: 30 November 2005; revised submission received: 11 January 2007; accepted for publication: 30 March 2007. © 2007 Association for Computational Linguistics Computational Linguistics Volume 33, Number 4 treebank, maximum likelihood estimation can be applied to learn the probability values in the model. More recently, new machine learning methods have been developed or extended to handle models of grammatical structure. Notably, conditional estimation (Ratnaparkhi, Roukos, and Ward 1994; Johnson et al. 1999; Lafferty, McCallum, and Pereira 2001), maximum margin estimation (Taskar et al. 2004), and unsupervised contrastive estimation (Smith and Eisner 2005) have been applied to structured models. Weighted grammars learned in this way differ in two important ways from traditional, generative models. First, the weights can be any positive value; they need not sum to one. Second, features can “overlap,” and it can be difficult to design a generative model that uses such features. The benefits of new features and discriminative training methods are widely documented and recognized. This article focus"
J07-4003,W02-1002,0,0.00817733,"allow the 8 The HMM shown in Example 3 can be factored into a Moore HMM without any change to the distribution. 488 Smith and Johnson Weighted and Probabilistic CFGs MEMM to “look back” and condition on earlier symbols (or states), it cannot represent the distribution in Example 3.  Generally speaking, this limitation of MEMMs has nothing to do with the estimation procedure (we have committed to no estimation procedure in particular) but rather with the conditional structure of the model. That some model structures work better than others at real NLP tasks was discussed by Johnson (2001) and Klein and Manning (2002). Our result—that the class of distributions allowed by MEMMs is a strict subset of those allowed by Mealy HMMs—makes this unsurprising. 5. Practical Implications Our result is that weighted generalizations of classical probabilistic grammars (PCFGs and HMMs) are no more powerful than the probabilistic models. This means that, insofar as log-linear models for NLP tasks like tagging and parsing are more successful than their probabilistic cousins, it is due to either (a) additional features added to the model, (b) improved estimation procedures (e.g., maximum conditional likelihood estimation o"
J07-4003,P04-1069,0,0.0116814,"e model of Chelba and Jelinek, 1998). They proved that, although the conversion may not be simple (indeed, a blow-up in the automaton’s size may be incurred), given G, PPCFG and the set of distributions expressible by shift-reduce probabilistic push-down automata are weakly equivalent. Importantly, the standard conversion of a CFG into a shift-reduce PDA, when applied in the stochastic case, does not always preserve the probability distribution over trees. Our Theorem 2 bears a resemblance to that result. Further work on the relationship between weighted CFGs and weighted PDAs is described in Nederhof and Satta (2004). MacKay (1996) proved that linear Boltzmann chains (a class of weighted models that is essentially the same as Moore MRFs) express the same set of distributions as Moore HMMs, under the condition that the Boltzmann chain has a single specific end state. MacKay avoided the divergence problem by defining the Boltzmann chain always to condition on the length of the sequence; he tacitly requires all of his models to be in GZn &lt;∞ . We have suggested a more applicable notion of model equivalence (equivalence of the conditional distribution) and our Theorem 1 generalizes to context-free models. 7. C"
J07-4003,P05-1044,1,0.807515,"eceived: 11 January 2007; accepted for publication: 30 March 2007. © 2007 Association for Computational Linguistics Computational Linguistics Volume 33, Number 4 treebank, maximum likelihood estimation can be applied to learn the probability values in the model. More recently, new machine learning methods have been developed or extended to handle models of grammatical structure. Notably, conditional estimation (Ratnaparkhi, Roukos, and Ward 1994; Johnson et al. 1999; Lafferty, McCallum, and Pereira 2001), maximum margin estimation (Taskar et al. 2004), and unsupervised contrastive estimation (Smith and Eisner 2005) have been applied to structured models. Weighted grammars learned in this way differ in two important ways from traditional, generative models. First, the weights can be any positive value; they need not sum to one. Second, features can “overlap,” and it can be difficult to design a generative model that uses such features. The benefits of new features and discriminative training methods are widely documented and recognized. This article focuses specifically on the first of these differences. It compares the expressive power of weighted context-free grammars (WCFGs), where each rule is associ"
J07-4003,J95-2002,0,0.094771,"X→α = |α|  Zαi (Θ ) i=1 ZX (Θ ) where αi is the ith element of α and |α |is the length of α. Chi proved that GΘ is a PCFG and that PΘ (τ ) = sΘ (τ )/Z(Θ ) for all trees τ ∈ Ω(G). Chi did not describe how to compute the nonterminal-specific partition functions ZX (Θ ). The ZX (Θ ) are related by equations of the form  ZX (Θ ) = θX→α α:X→α∈R |α|  Zαi (Θ ) i=1 which constitute a set of nonlinear polynomial equations in ZX (Θ ). Although a numerical solver might be employed to find the ZX (Θ ), we have found that in practice iterative propagation of weights following the method described by Stolcke (1995, Section 4.7.1) converges quickly when Z(Θ ) is finite. 3. Classifiers and Conditional Distributions A common application of weighted grammars is parsing. One way to select a parse tree for a sentence x is to choose the maximum weighted parse that is consistent with the observation x: τ∗ (x) = argmax sΘ ( τ ) (3) τ∈Ω(G):y(τ )=x where y(τ ) is the yield of τ. Other decision criteria exist, including minimum-loss decoding and re-ranked n-best decoding. All of these classifiers use some kind of dynamic programming algorithm to optimize over trees, and they also exploit the conditional distributi"
J07-4003,P80-1024,0,0.537079,"Missing"
J07-4003,W04-3201,0,\N,Missing
J07-4003,C98-1035,0,\N,Missing
J08-3007,D07-1090,0,0.0178766,"els, and shallow parsing/ chunking. The Forward, Viterbi, Viterbi n-best, Forward–Backward algorithms, and “Forward–Backward Decoding” (also known as posterior or minimum Bayes risk decoding) are covered with examples. This chapter is not as leisurely as the treatments ¨ of HMMs by Manning and Schutze (1999) or Charniak (1993), and it omits basic background on probabilistic modeling. For example, why must we ensure that an n-gram model’s total probability sums exactly to one? The answer relies on an understanding of perplexity and its use in evaluation, now in decline (cf. “stupid backoff” in Brants et al. 2007). The chapter does not reconnect with the algebraic view presented in Chapter 1; for example, the connection between HMMs and WFSAs is never expressed. Chapter 7 introduces context-free grammars and their parsers, broken down into “deterministic” and “nondeterministic” approaches.2 Probabilistic CFGs and treebanks are introduced informally alongside the latter, which may confuse some readers. Ambiguity is only presented as a natural phenomenon, not a problem of crude, overgenerating grammars. The probabilistic CKY and Earley algorithms are presented. The Inside–Outside algorithm is presented i"
J08-3007,P97-1003,0,0.104928,"nerating grammars. The probabilistic CKY and Earley algorithms are presented. The Inside–Outside algorithm is presented in the context of Goodman’s (1996) maximum expected recall parsing (another instance of minimum Bayes risk). As in the case of the dynamic programming algorithms for HMMs in Chapter 6, the exposition is probably too brisk to be an introduction to the topic. Chapter 8 contains a thoughtful discussion of many best practices in statistical parsing: treebank “decoration” techniques such as parent annotation and lexicalization, and the probability models underlying the parsers of Collins (1997) and Charniak (1997). Dependency parsing, unsupervised grammar induction, and ﬁnite-state approximations to PCFGs are allotted short sections. Chapter 9 covers context-sensitive models of syntax. Uniﬁcation-based parsing is presented at a high level, without formal details of uniﬁcation or the differences between theories such as LFG and HPSG. The “lexicalized” models (TAG and CCG) are treated more thoroughly; pseudocode for a TAG dynamic programming parser is provided. There is brief treatment of Data-Oriented Parsing, reranking (a section that 2 These terms, though in wide use, are misnomers"
J08-3007,P02-1001,0,0.0321238,"th, and Tromble 2005). The chapter focuses on the approaches of Goldsmith (2001), Schone and Jurafsky (2001), and Yarowsky and Wicentowski (2001). Although each approach is interesting on its own, little effort is made to unify work in this area, and none to bring the reader back full circle to ﬁnite-state models or the problem of inducing from data regular grammars (Stolcke and Omohundro 1993) or their weights 1 The authors rightly point out that Koskenniemi deserves much credit for building an implementation that aimed to have broad coverage, not merely a proof-of-concept. 454 Book Reviews (Eisner 2002). Another missed opportunity here is the recent introduction of Bayesian learning for word segmentation (Goldwater, Grifﬁths, and Johnson 2006). Part I, in summary, aims to reduce many accounts of morphological phenomena to ﬁnite-state transducer composition, drawing on a wealth of illustrative examples. Twenty-two languages are listed in the language index at the end of the book, and, tellingly, all of them are discussed exclusively in Part I. These chapters are good diplomacy toward theoretical linguistics, showing how computational arguments can have theoretical implications. 3. Part II (Ch"
J08-3007,C94-2163,0,0.060823,"ded morphology rules are argued to be a historical accident; if only computers had been more powerful in the 1980s, compilation of those rules into FSTs might have been automated, and in fact Kaplan and Kay had already developed the algorithms.1 In the spirit of the previous chapter, Sproat and Roark also note that morphological accounts that use one, two, or more “cascaded” levels are all computationally equivalent rational relations under the ﬁnite-state approach, and that Optimality Theory can (under certain assumptions about constraints) be implemented with ﬁnite-state operations as well (Ellison 1994). Chapter 5, “Machine learning of morphology,” focuses on unsupervised morphology induction methods. There is about a page of discussion on statistical language modeling approaches for disambiguation in agglutinative languages; no mention is made of the more recent use of discriminative machine learning in morphological disambiguation (Kudo, Yamamoto, and Matsumoto 2004; Habash and Rambow 2005; Smith, Smith, and Tromble 2005). The chapter focuses on the approaches of Goldsmith (2001), Schone and Jurafsky (2001), and Yarowsky and Wicentowski (2001). Although each approach is interesting on its"
J08-3007,P06-1085,0,0.0661274,"Missing"
J08-3007,W04-3230,0,0.0907015,"Missing"
J08-3007,H05-1060,1,0.892198,"Missing"
J12-3003,P05-1022,0,0.0220149,"he relative gains in moving to a more restricted parametric family, could offer practical advantages to language engineers. We note that grammar learning has been studied in formal settings as a problem of grammatical inference—learning the structure of a grammar or an automaton (Angluin 1987; Clark and Thollard 2004; de la Higuera 2005; Clark, Eyraud, and Habrard 2008, among others). Our setting in this article is different. We assume that we have a ﬁxed grammar, and our goal is to estimate its parameters. This approach has shown great empirical success, both in the supervised (Collins 2003; Charniak and Johnson 2005) and the unsupervised (Carroll and Charniak 1992; Pereira and Schabes 1992; Klein and Manning 2004; Cohen and Smith 2010a) settings. There has also been some discussion of sample complexity bounds for statistical parsing models, in a distribution-free setting (Collins 2004). The distribution-free setting, however, is not ideal for analysis of natural language, as it has to account for pathological cases of distributions that generate data. We develop a framework for deriving sample complexity bounds using the maximum likelihood principle for probabilistic grammars in a distribution-dependent s"
J12-3003,J99-1004,0,0.675922,"µ(k,i−1),2 = 1 − µ(k,i−1),2 . G , µ is a weighted context-free grammar such that the µ(k,i),1 corresponds to the ith event in the k multinomial of the original grammar. Let z be a derivation in G and z = ΥG →G (z). Then, from Utility Lemma 1 and the construction of g , we have that: p(z |θ, G) = Nk K   ψ (z) θk,ik,i k =1 i =1 Nk ψk,i (z) K    = θk,i k =1 i =1 l =1 Nk ψk,i (z) K    = k =1 i =1 l =1 Nk K   = k =1 i =1     i− 1  j =1 i− 1   µ(k,j),2  µ(k,i),1  ψk,i (z)  µψk,i (z) µ(k,j),2 (k,i),1 j=1 Nk 2 K    = ψ (z ) k,j µ(k,j),i k =1 j =1 i =1 = p(z |µ, G ) From Chi (1999), we know that the weighted grammar G , µ can be converted to a probabilistic context-free grammar G , θ , through a construction of θ based on µ, such that p(z |µ, G ) = p(z |θ , G ).  The proof for Theorem 1 gives a construction the parameters θ of G such that G, θ is equivalent to G , θ . The construction of θ can also be reversed: Given θ for G , we can construct θ for G so that again we have equivalence between G, θ and G , θ . In this section, we focused on presenting parametrized, empirically justiﬁed distributional assumptions about language data that will make the a"
J12-3003,P10-1152,1,0.932275,"note that grammar learning has been studied in formal settings as a problem of grammatical inference—learning the structure of a grammar or an automaton (Angluin 1987; Clark and Thollard 2004; de la Higuera 2005; Clark, Eyraud, and Habrard 2008, among others). Our setting in this article is different. We assume that we have a ﬁxed grammar, and our goal is to estimate its parameters. This approach has shown great empirical success, both in the supervised (Collins 2003; Charniak and Johnson 2005) and the unsupervised (Carroll and Charniak 1992; Pereira and Schabes 1992; Klein and Manning 2004; Cohen and Smith 2010a) settings. There has also been some discussion of sample complexity bounds for statistical parsing models, in a distribution-free setting (Collins 2004). The distribution-free setting, however, is not ideal for analysis of natural language, as it has to account for pathological cases of distributions that generate data. We develop a framework for deriving sample complexity bounds using the maximum likelihood principle for probabilistic grammars in a distribution-dependent setting. Distribution dependency is introduced here by making empirically justiﬁed assumptions about the distributions th"
J12-3003,J03-4003,0,0.0604102,"nnotate, and the relative gains in moving to a more restricted parametric family, could offer practical advantages to language engineers. We note that grammar learning has been studied in formal settings as a problem of grammatical inference—learning the structure of a grammar or an automaton (Angluin 1987; Clark and Thollard 2004; de la Higuera 2005; Clark, Eyraud, and Habrard 2008, among others). Our setting in this article is different. We assume that we have a ﬁxed grammar, and our goal is to estimate its parameters. This approach has shown great empirical success, both in the supervised (Collins 2003; Charniak and Johnson 2005) and the unsupervised (Carroll and Charniak 1992; Pereira and Schabes 1992; Klein and Manning 2004; Cohen and Smith 2010a) settings. There has also been some discussion of sample complexity bounds for statistical parsing models, in a distribution-free setting (Collins 2004). The distribution-free setting, however, is not ideal for analysis of natural language, as it has to account for pathological cases of distributions that generate data. We develop a framework for deriving sample complexity bounds using the maximum likelihood principle for probabilistic grammars i"
J12-3003,N06-1043,0,0.099011,"s optimization problem is   θk,i = min  1 − γ, max     γ,  n  j=1  &quot; ψˆ j,k,i   n  2  j=1 i  =1    ψˆ j,k,i   (22) where ψˆ j,k,i is the number of times that ψk,i ﬁres in Example j. (We include a full derivation of this result in Appendix B.) The interpretation of Equation (22) is simple: We count the number of times a rule appears in the samples and then normalize this value by the total number of times rules associated with the same multinomial appear in the samples. This frequency count is the maximum likelihood solution with respect to the full hypothesis class H (Corazza and Satta 2006; see Appendix B). Because we constrain ourselves to obtain a value away from 0 or 1 by a margin of γ, we need to truncate this solution, as done in Equation (22). This truncation to a margin γ can be thought of as a smoothing factor that enables us to compute sample complexity bounds. We explore this connection to smoothing with a Dirichlet prior in a Maximum a posteriori (MAP) Bayesian setting in Section 7.2. 6.2 Unsupervised Case Similarly to the supervised case, minimizing the empirical log-loss in the unsupervised setting requires minimizing (with respect to θ) the following:     p˜n"
J12-3003,N10-1118,0,0.0265415,"of G as deﬁned earlier. Then, there exists θ for G such that for any z ∈ D(G) we have p(z |θ, G) = p(ΥG →G (z) |θ , G ). 6 We note that this notion of binarization is different from previous types of binarization appearing in computational linguistics for grammars. Typically in previous work about binarized grammars such as CFGs, the grammars are constrained to have at most two nonterminals in the right side in Chomsky normal form. Another form of binarization for linear context-free rewriting systems is restriction of the ´ fan-out of the rules to two (Gomez-Rodr´ ıguez and Satta 2009; Gildea 2010). We, however, limit the number of rules for each nonterminal (or more generally, the number of elements in each multinomial). 491 Computational Linguistics Volume 38, Number 3 Proof For the grammar G, index the set {1, ..., K} with nonterminals ranging from A1 to AK . Deﬁne G as before. We need to deﬁne θ . Index the multinomials in G by (k, i), each having two events. Let µ(k,i),1 = θk,i , µ(k,i),2 = 1 − θk,i for i = 1 and set µk,i,1 = θk,i /µ(k,i−1),2 , and µ(k,i−1),2 = 1 − µ(k,i−1),2 . G , µ is a weighted context-free grammar such that the µ(k,i),1 corresponds to the ith event in the k"
J12-3003,P09-1111,0,0.0605962,"Missing"
J12-3003,P04-1061,0,0.0656274,"o language engineers. We note that grammar learning has been studied in formal settings as a problem of grammatical inference—learning the structure of a grammar or an automaton (Angluin 1987; Clark and Thollard 2004; de la Higuera 2005; Clark, Eyraud, and Habrard 2008, among others). Our setting in this article is different. We assume that we have a ﬁxed grammar, and our goal is to estimate its parameters. This approach has shown great empirical success, both in the supervised (Collins 2003; Charniak and Johnson 2005) and the unsupervised (Carroll and Charniak 1992; Pereira and Schabes 1992; Klein and Manning 2004; Cohen and Smith 2010a) settings. There has also been some discussion of sample complexity bounds for statistical parsing models, in a distribution-free setting (Collins 2004). The distribution-free setting, however, is not ideal for analysis of natural language, as it has to account for pathological cases of distributions that generate data. We develop a framework for deriving sample complexity bounds using the maximum likelihood principle for probabilistic grammars in a distribution-dependent setting. Distribution dependency is introduced here by making empirically justiﬁed assumptions abou"
J12-3003,P89-1017,0,0.445798,"ticle, we limit the degree of the grammar by making the assumption that all Nk ≤ 2. This assumption may seem, at ﬁrst glance, somewhat restrictive, but we show next that for PCFGs (and as a consequence, other formalisms), this assumption does not limit the total generative capacity that we can have across all context-free grammars. We ﬁrst show that any context-free grammar with arbitrary degree can be mapped to a corresponding grammar with all Nk ≤ 2 that generates derivations equivalent to derivations in the original grammar. Such a grammar is also called a “covering grammar” (Nijholt 1980; Leermakers 1989). Let G be a CFG. Let A be the kth nonterminal. Consider the rules A → αi for i ≤ Nk where A appears on the left side. For each rule 490 Cohen and Smith Empirical Risk Minimization for Probabilistic Grammars Figure 2 Example of a context-free grammar and its equivalent binarized form. A → αi , i < Nk , we create a new nonterminal in G such that Ai has two rewrite rules: Ai → αi and Ai → Ai+1 . In addition, we create rules A → A1 and ANk → αNk . Figure 2 demonstrates an example of this transformation on a small context-free grammar. It is easy to verify that the resulting grammar G has an equ"
J12-3003,P92-1017,0,0.382509,"fer practical advantages to language engineers. We note that grammar learning has been studied in formal settings as a problem of grammatical inference—learning the structure of a grammar or an automaton (Angluin 1987; Clark and Thollard 2004; de la Higuera 2005; Clark, Eyraud, and Habrard 2008, among others). Our setting in this article is different. We assume that we have a ﬁxed grammar, and our goal is to estimate its parameters. This approach has shown great empirical success, both in the supervised (Collins 2003; Charniak and Johnson 2005) and the unsupervised (Carroll and Charniak 1992; Pereira and Schabes 1992; Klein and Manning 2004; Cohen and Smith 2010a) settings. There has also been some discussion of sample complexity bounds for statistical parsing models, in a distribution-free setting (Collins 2004). The distribution-free setting, however, is not ideal for analysis of natural language, as it has to account for pathological cases of distributions that generate data. We develop a framework for deriving sample complexity bounds using the maximum likelihood principle for probabilistic grammars in a distribution-dependent setting. Distribution dependency is introduced here by making empirically j"
J14-1002,S07-1018,0,0.666381,"Missing"
J14-1002,boas-2002-bilingual,0,0.0274075,"experiments on recently released FrameNet 1.5 data. In other work based on FrameNet, Matsubayashi, Okazaki, and Tsujii (2009) investigated various uses of FrameNet’s taxonomic relations for learning generalizations over roles; they trained a log-linear model on the SemEval 2007 data to evaluate features for the subtask of argument identification. Another line of work has sought to extend the coverage of FrameNet by exploiting VerbNet and WordNet (Shi and Mihalcea 2005; Giuglea and Moschitti 2006; Pennacchiotti et al. 2008) and by projecting entries and annotations within and across languages (Boas 2002; Fung and Chen 2004; Pado ¨ and Lapata 2005; Furstenau and Lapata 2009b). Others have explored the application of frame-semantic structures to tasks such as information extraction (Moschitti, Morarescu, and Harabagiu 2003; Surdeanu et al. 2003), textual entailment (Burchardt and Frank 2006; Burchardt et al. 2009), question answering (Narayanan and Harabagiu 2004; Shen and Lapata 2007), and paraphrase recognition (Pado´ and Erk 2005). 2.3 Semi-Supervised Methods Although there has been a significant amount of work in supervised shallow semantic parsing using both PropBank- and FrameNet-style r"
J14-1002,W04-2412,0,0.0162126,"Missing"
J14-1002,W05-0620,0,0.105832,"Missing"
J14-1002,D11-1003,0,0.0540671,"kled with off-the-shelf solvers, Rush et al. (2010) proposed subgradient-based dual decomposition (also called Lagrangian relaxation) as a way of exploiting the structure of the problem and existing combinatorial algorithms. The method allows the combination of models that are individually tractable, but not jointly tractable, by solving a relaxation of the original problem. Since then, dual decomposition has been used to build more accurate models for dependency parsing (Koo et al. 2010), combinatory categorical grammar supertagging and parsing (Auli and Lopez 2011), and machine translation (Chang and Collins 2011; DeNero and Macherey 2011; Rush and Collins 2011). Recently, Martins et al. (2011b) showed that the success of subgradient-based dual decomposition strongly relies on breaking down the original problem into a “good” 4 In prior work, there are exceptions where identification and classification of arguments have been treated in one step; for more details, please refer to the systems participating in the CoNLL-2004 shared task on semantic role labeling (Carreras and M`arquez 2004). 16 Das et al. Frame-Semantic Parsing decomposition, that is, one with few overlapping components. This leaves out m"
J14-1002,S10-1059,1,0.89659,"y filled; in the SemEval 2007 development data, the average number of roles an evoked frame defines is 6.7, but the average number of overt arguments is only 1.7.29 In 29 In the annotated data, each core role is filled with one of three types of null instantiations indicating how the role is conveyed implicitly. For instance, the imperative construction implicitly designates a role as filled by the addressee, and the corresponding filler is thus CNI (constructional null instantiation). In this work we do not distinguish different types of null instantiation. The interested reader may refer to Chen et al. (2010), who handle the different types of null instantions during argument identification. 31 Computational Linguistics Volume 40, Number 1 training, if a labeled argument is not a subtree of the dependency parse, we add its span to S.30 Let Ai denote the mapping of roles in Rfi to spans in S. Our model makes a prediction for each Ai (rk ) (for all roles rk ∈ Rfi ) using: Ai (rk ) ← argmax pψ (s |rk , fi , ti , x) s∈S (7) We use a conditional log-linear model over spans for each role of each evoked frame: exp ψ h(s, rk , fi , ti , x) pψ (Ai (rk ) = s |fi , ti , x) =  exp ψ h(s , rk , fi , ti , x"
J14-1002,S12-1029,1,0.386125,"Missing"
J14-1002,P11-1061,1,0.647271,"Missing"
J14-1002,N10-1138,1,0.945675,"sition algorithm (Section 7) that collectively predicts all the arguments of a frame together, thereby incorporating linguistic constraints in a principled fashion. Our open-source parser, named SEMAFOR (Semantic Analyzer of Frame Representations)1 achieves the best published results to date on the SemEval 2007 frame-semantic structure extraction task (Baker, Ellsworth, and Erk 2007). Herein, we also present results on newly released data with FrameNet 1.5, the latest edition of the lexicon. Some of the material presented in this article has appeared in previously published conference papers: Das et al. (2010) presented the basic model, Das and Smith (2011) described semi-supervised lexicon expansion, Das and Smith (2012) demonstrated a sparse variant of lexicon expansion, and Das, Martins, and Smith (2012) presented the dual decomposition algorithm for constrained joint argument identification. We present here a synthesis of those results and several additional details: 1. The set of features used in the two statistical models for frame identification and argument identification. 2. Details of a greedy beam search algorithm for argument identification that avoids illegal argument overlap. 3. Error"
J14-1002,P11-1144,1,0.926493,"y predicts all the arguments of a frame together, thereby incorporating linguistic constraints in a principled fashion. Our open-source parser, named SEMAFOR (Semantic Analyzer of Frame Representations)1 achieves the best published results to date on the SemEval 2007 frame-semantic structure extraction task (Baker, Ellsworth, and Erk 2007). Herein, we also present results on newly released data with FrameNet 1.5, the latest edition of the lexicon. Some of the material presented in this article has appeared in previously published conference papers: Das et al. (2010) presented the basic model, Das and Smith (2011) described semi-supervised lexicon expansion, Das and Smith (2012) demonstrated a sparse variant of lexicon expansion, and Das, Martins, and Smith (2012) presented the dual decomposition algorithm for constrained joint argument identification. We present here a synthesis of those results and several additional details: 1. The set of features used in the two statistical models for frame identification and argument identification. 2. Details of a greedy beam search algorithm for argument identification that avoids illegal argument overlap. 3. Error analysis pertaining to the dual decomposition a"
J14-1002,N12-1086,1,0.870084,"ating linguistic constraints in a principled fashion. Our open-source parser, named SEMAFOR (Semantic Analyzer of Frame Representations)1 achieves the best published results to date on the SemEval 2007 frame-semantic structure extraction task (Baker, Ellsworth, and Erk 2007). Herein, we also present results on newly released data with FrameNet 1.5, the latest edition of the lexicon. Some of the material presented in this article has appeared in previously published conference papers: Das et al. (2010) presented the basic model, Das and Smith (2011) described semi-supervised lexicon expansion, Das and Smith (2012) demonstrated a sparse variant of lexicon expansion, and Das, Martins, and Smith (2012) presented the dual decomposition algorithm for constrained joint argument identification. We present here a synthesis of those results and several additional details: 1. The set of features used in the two statistical models for frame identification and argument identification. 2. Details of a greedy beam search algorithm for argument identification that avoids illegal argument overlap. 3. Error analysis pertaining to the dual decomposition argument identification algorithm, in contrast with the beam search"
J14-1002,P11-1043,0,0.0102251,"solvers, Rush et al. (2010) proposed subgradient-based dual decomposition (also called Lagrangian relaxation) as a way of exploiting the structure of the problem and existing combinatorial algorithms. The method allows the combination of models that are individually tractable, but not jointly tractable, by solving a relaxation of the original problem. Since then, dual decomposition has been used to build more accurate models for dependency parsing (Koo et al. 2010), combinatory categorical grammar supertagging and parsing (Auli and Lopez 2011), and machine translation (Chang and Collins 2011; DeNero and Macherey 2011; Rush and Collins 2011). Recently, Martins et al. (2011b) showed that the success of subgradient-based dual decomposition strongly relies on breaking down the original problem into a “good” 4 In prior work, there are exceptions where identification and classification of arguments have been treated in one step; for more details, please refer to the systems participating in the CoNLL-2004 shared task on semantic role labeling (Carreras and M`arquez 2004). 16 Das et al. Frame-Semantic Parsing decomposition, that is, one with few overlapping components. This leaves out many declarative constraine"
J14-1002,D09-1003,0,0.0126412,"een verbs using a graph alignment method; this method represents sentences and their syntactic analysis as graphs and graph alignment is used to project annotations from seed examples to unlabeled sentences. This alignment problem is again modeled as a linear program. ¨ Furstenau and Lapata (2012) present an detailed expansion of the aforementioned papers. Although this line of work presents a novel direction in the area of SRL, the published approach does not yet deal with non-verbal predicates and does not evaluate the presented methods on the full text annotations of the FrameNet releases. Deschacht and Moens (2009) present a technique of incorporating additional information from unlabeled data by using a latent words language model. Latent variables are used to model the underlying representation of words, and parameters of this model 15 Computational Linguistics Volume 40, Number 1 are estimated using standard unsupervised methods. Next, the latent information is used as features for an SRL model. Improvements over supervised SRL techniques are observed with the augmentation of these extra features. The authors also compare ¨ their method with the aforementioned two methods of Furstenau and Lapata (200"
J14-1002,W03-1007,0,0.112387,"Missing"
J14-1002,C04-1134,0,0.0437403,"on recently released FrameNet 1.5 data. In other work based on FrameNet, Matsubayashi, Okazaki, and Tsujii (2009) investigated various uses of FrameNet’s taxonomic relations for learning generalizations over roles; they trained a log-linear model on the SemEval 2007 data to evaluate features for the subtask of argument identification. Another line of work has sought to extend the coverage of FrameNet by exploiting VerbNet and WordNet (Shi and Mihalcea 2005; Giuglea and Moschitti 2006; Pennacchiotti et al. 2008) and by projecting entries and annotations within and across languages (Boas 2002; Fung and Chen 2004; Pado ¨ and Lapata 2005; Furstenau and Lapata 2009b). Others have explored the application of frame-semantic structures to tasks such as information extraction (Moschitti, Morarescu, and Harabagiu 2003; Surdeanu et al. 2003), textual entailment (Burchardt and Frank 2006; Burchardt et al. 2009), question answering (Narayanan and Harabagiu 2004; Shen and Lapata 2007), and paraphrase recognition (Pado´ and Erk 2005). 2.3 Semi-Supervised Methods Although there has been a significant amount of work in supervised shallow semantic parsing using both PropBank- and FrameNet-style representations, a fe"
J14-1002,P10-1160,0,0.00634321,"ables z are binary. Here, apart from the ILP formulation, we will consider the following relaxation of Equation (11), which replaces the binary constraint z ∈ {0, 1}d by a unit interval constraint z ∈ [0, 1]d , yielding a linear program: maximize  c(r, s) × zr,s r∈Rf s∈S with respect to such that z ∈ [0, 1]d Az ≤ b. (17) 42 We noticed that, in the annotated data, in some cases, the “requires” constraint is violated by the FrameNet annotators. This happens mostly when one of the required roles is absent in the sentence containing the target, but is rather instantiated in an earlier sentence (Gerber and Chai 2010). We apply the hard constraint in Equation (16), though extending our algorithm to seek arguments outside the sentence is straightforward. For preliminary work extending SEMAFOR this way, see Chen et al. (2010). 41 Computational Linguistics Volume 40, Number 1 There are several LP and ILP solvers available, and a great deal of effort has been spent by the optimization community to devise efficient generic solvers. An example is CPLEX, a state-of-the-art solver for mixed integer programming that we use as a baseline to solve the ILP in Equation (11) as well as its LP relaxation in Equation (17)"
J14-1002,J02-3001,0,0.989881,"012; accepted for publication: 22 December 2012. doi:10.1162/COLI a 00163 © 2014 Association for Computational Linguistics Computational Linguistics Volume 40, Number 1 1. Introduction FrameNet (Fillmore, Johnson, and Petruck 2003) is a linguistic resource storing considerable information about lexical and predicate-argument semantics in English. Grounded in the theory of frame semantics (Fillmore 1982), it suggests—but does not formally define—a semantic representation that blends representations familiar from word-sense disambiguation (Ide and V´eronis 1998) and semantic role labeling (SRL; Gildea and Jurafsky 2002). Given the limited size of available resources, accurately producing richly structured frame-semantic structures with high coverage will require data-driven techniques beyond simple supervised classification, such as latent variable modeling, semi-supervised learning, and joint inference. In this article, we present a computational and statistical model for frame-semantic parsing, the problem of extracting from text semantic predicate-argument structures such as those shown in Figure 1. We aim to predict a frame-semantic representation with two statistical models rather than a collection of l"
J14-1002,S07-1003,0,0.00995843,"Missing"
J14-1002,W09-1201,0,0.0598567,"Missing"
J14-1002,J98-1001,0,0.0264943,"Missing"
J14-1002,S07-1048,0,0.102556,"s handling many more labels, and resulting in richer frame-semantic parses. Recent work in frame-semantic parsing—in which sentences may contain multiple frames which need to be recognized along with their arguments—was undertaken as the SemEval 2007 task 19 of frame-semantic structure extraction (Baker, Ellsworth, and Erk 2007). This task leveraged FrameNet 1.3, and also released a small corpus 3 Available at http://framenet.icsi.berkeley.edu as of 19 January 2013. 14 Das et al. Frame-Semantic Parsing containing a little more than 2,000 sentences with full text annotations. The LTH system of Johansson and Nugues (2007), which we use as our baseline (Section 3.4), had the best performance in the SemEval 2007 task in terms of full frame-semantic parsing. Johansson and Nugues broke down the task as identifying targets that could evoke frames in a sentence, identifying the correct semantic frame for a target, and finally determining the arguments that fill the semantic roles of a frame. They used a series of SVMs to classify the frames for a given target, associating unseen lexical items to frames and identifying and classifying token spans as various semantic roles. Both the full text annotation corpus as well"
J14-1002,D08-1008,0,0.0243643,"Missing"
J14-1002,kingsbury-palmer-2002-treebank,0,0.713501,"ly discuss work done on PropBank-style semantic role labeling, following which we will concentrate on the more relevant problem of frame-semantic structure extraction. Next, we review previous work that has used semi-supervised learning for shallow semantic parsing. Finally, we discuss prior work on joint structure prediction relevant to frame-semantic parsing. 2.1 Semantic Role Labeling Since Gildea and Jurafsky (2002) pioneered statistical semantic role labeling, there has been a great deal of computational work using predicate-argument structures for semantics. The development of PropBank (Kingsbury and Palmer 2002), followed by CoNLL shared tasks on semantic role labeling (Carreras and M`arquez 2004, 2005) boosted research in this area. Figure 2(a) shows an annotation from PropBank. PropBank annotations are closely tied to syntax, because the data set consists of the 1 See http://www.ark.cs.cmu.edu/SEMAFOR. 11 Computational Linguistics Volume 40, Number 1 (a) (b) Figure 2 (a) A phrase-structure tree taken from the Penn Treebank and annotated with PropBank predicate-argument structures. The verbs created and pushed serve as predicates in this sentence. Dotted arrows connect each predicate to its semantic"
J14-1002,D10-1125,0,0.0072494,", as well as PropBank SRL (Punyakanok et al. 2004). Whereas early work in this area focused on declarative formulations tackled with off-the-shelf solvers, Rush et al. (2010) proposed subgradient-based dual decomposition (also called Lagrangian relaxation) as a way of exploiting the structure of the problem and existing combinatorial algorithms. The method allows the combination of models that are individually tractable, but not jointly tractable, by solving a relaxation of the original problem. Since then, dual decomposition has been used to build more accurate models for dependency parsing (Koo et al. 2010), combinatory categorical grammar supertagging and parsing (Auli and Lopez 2011), and machine translation (Chang and Collins 2011; DeNero and Macherey 2011; Rush and Collins 2011). Recently, Martins et al. (2011b) showed that the success of subgradient-based dual decomposition strongly relies on breaking down the original problem into a “good” 4 In prior work, there are exceptions where identification and classification of arguments have been treated in one step; for more details, please refer to the systems participating in the CoNLL-2004 shared task on semantic role labeling (Carreras and M`"
J14-1002,N10-1137,0,0.00679874,"d Lapata (2009a, 2009b) and show relative improvements. Experiments are performed on the CoNLL 2008 shared task data set (Surdeanu et al. 2008), which follows the PropBank conventions and only labels verbal and nominal predicates—in contrast to our work, which includes most lexicosyntactic categories. A similar approach is presented by Weston, Ratle, and Collobert (2008), who use neural embeddings of words, which are eventually used for SRL; improvements over state-of-the-art PropBank-style SRL systems are observed. Recently, there has been related work in unsupervised semantic role labeling (Lang and Lapata 2010, 2011; Titov and Klementiev 2012) that attempts to induce semantic roles automatically from unannotated data. This line of work may be useful in discovering new semantic frames and roles, but here we stick to the concrete representation provided in FrameNet, without seeking to expand its inventory of semantic types. We present a new semi-supervised technique to expand the set of lexical items with the potential semantic frames that they could evoke; we use a graph-based semi-supervised learning framework to achieve this goal (Section 5.5). 2.4 Joint Inference and Shallow Semantic Parsing Most"
J14-1002,D11-1122,0,0.0108005,"Missing"
J14-1002,P93-1016,0,0.0449607,"e constraints on frame identification. 5.5.1 Graph Construction. We construct a graph with lexical units as vertices. Thus, each vertex corresponds to a lemmatized word or phrase appended with a coarse POS tag. We use two resources for graph construction. First, we take all the words and phrases present in a dependency-based thesaurus constructed using syntactic cooccurrence statistics (Lin 1998), and aggregate words and phrases that share the same lemma and coarse POS tag. To construct this resource, Lin used a corpus containing 64 million words that was parsed with a fast dependency parser (Lin 1993, 1994), and syntactic contexts were used to find similar lexical items for a given word or phrase. Lin separately treated nouns, verbs, and adjectives/adverbs, so these form the three parts of the thesaurus. This resource gave us a list of possible LUs, much larger in size than the LUs present in FrameNet data. The second component of graph construction comes from FrameNet itself. We scanned the exemplar sentences in FrameNet 1.5 and the training section of the full text annotations and gathered a distribution over frames for each LU appearing in FrameNet data. For a pair of LUs, we measured"
J14-1002,C94-1079,0,0.14144,"Missing"
J14-1002,P98-2127,0,0.0298059,"ed learning of POS taggers by using bilingual graph-based projections (Das and Petrov 2011). We describe our approach to graph construction, propagation for lexicon expansion, and the use of the result to impose constraints on frame identification. 5.5.1 Graph Construction. We construct a graph with lexical units as vertices. Thus, each vertex corresponds to a lemmatized word or phrase appended with a coarse POS tag. We use two resources for graph construction. First, we take all the words and phrases present in a dependency-based thesaurus constructed using syntactic cooccurrence statistics (Lin 1998), and aggregate words and phrases that share the same lemma and coarse POS tag. To construct this resource, Lin used a corpus containing 64 million words that was parsed with a fast dependency parser (Lin 1993, 1994), and syntactic contexts were used to find similar lexical items for a given word or phrase. Lin separately treated nouns, verbs, and adjectives/adverbs, so these form the three parts of the thesaurus. This resource gave us a list of possible LUs, much larger in size than the LUs present in FrameNet data. The second component of graph construction comes from FrameNet itself. We sca"
J14-1002,S07-1005,0,0.0401753,"Missing"
J14-1002,J93-2004,0,0.0478041,"Missing"
J14-1002,J08-2001,0,0.0552021,"Missing"
J14-1002,D11-1022,1,0.788077,"Missing"
J14-1002,P09-1039,1,0.427643,"Missing"
J14-1002,D10-1004,1,0.864365,"r knowledge, the separate line of work investigating frame-semantic parsing has not previously dealt with joint inference. A common trait in prior work, both in PropBank and FrameNet conventions, has been the use of a two-stage model that identifies arguments first, then labels them, often using dynamic programming or integer linear programs (ILPs); we treat both problems together here.4 Recent work in natural language processing (NLP) problems has focused on ILP formulations for complex structure prediction tasks like dependency parsing (Riedel and Clarke 2006; Martins, Smith, and Xing 2009; Martins et al. 2010), sequence tagging (Roth and Yih 2004), as well as PropBank SRL (Punyakanok et al. 2004). Whereas early work in this area focused on declarative formulations tackled with off-the-shelf solvers, Rush et al. (2010) proposed subgradient-based dual decomposition (also called Lagrangian relaxation) as a way of exploiting the structure of the problem and existing combinatorial algorithms. The method allows the combination of models that are individually tractable, but not jointly tractable, by solving a relaxation of the original problem. Since then, dual decomposition has been used to build more ac"
J14-1002,P09-1003,0,0.0644821,"Missing"
J14-1002,P05-1012,0,0.0484643,"Missing"
J14-1002,W04-2705,0,0.72468,".v, ... Inheritance relation Causative_of relation Excludes relation Figure 3 Partial illustration of frames, roles, and lexical units related to the C AUSE TO MAKE NOISE frame, from the FrameNet lexicon. Core roles are filled bars. Non-core roles (such as Place and Time) are unfilled bars. No particular significance is ascribed to the ordering of a frame’s roles in its lexicon entry (the selection and ordering of roles above is for illustrative convenience). C AUSE TO MAKE NOISE defines a total of 14 roles, many of them not shown here. Whereas PropBank contains verbal predicates and NomBank (Meyers et al. 2004) contains nominal predicates, FrameNet counts these as well as allowing adjectives, adverbs, and prepositions among its lexical units. Finally, FrameNet frames organize predicates according to semantic principles, both by allowing related terms to evoke a common frame (e.g., push.V, raise.V, and growth.N for C AUSE CHANGE POSITION ON A SCALE) and by defining frames and their roles within a hierarchy (see Figure 3). PropBank does not explicitly encode relationships among predicates. Most early work on frame-semantic parsing has made use of the exemplar sentences in the FrameNet corpus (see Sect"
J14-1002,C04-1100,0,0.0140332,"n. Another line of work has sought to extend the coverage of FrameNet by exploiting VerbNet and WordNet (Shi and Mihalcea 2005; Giuglea and Moschitti 2006; Pennacchiotti et al. 2008) and by projecting entries and annotations within and across languages (Boas 2002; Fung and Chen 2004; Pado ¨ and Lapata 2005; Furstenau and Lapata 2009b). Others have explored the application of frame-semantic structures to tasks such as information extraction (Moschitti, Morarescu, and Harabagiu 2003; Surdeanu et al. 2003), textual entailment (Burchardt and Frank 2006; Burchardt et al. 2009), question answering (Narayanan and Harabagiu 2004; Shen and Lapata 2007), and paraphrase recognition (Pado´ and Erk 2005). 2.3 Semi-Supervised Methods Although there has been a significant amount of work in supervised shallow semantic parsing using both PropBank- and FrameNet-style representations, a few improve¨ ments over vanilla supervised methods using unlabeled data are notable. Furstenau and Lapata (2009b) present a method of projecting predicate-argument structures from some seed examples to unlabeled sentences, and use a linear program formulation to find the best alignment explaining the projection. Next, the projected information a"
J14-1002,H05-1108,0,0.0829981,"Missing"
J14-1002,D08-1048,0,0.100472,"Missing"
J14-1002,N04-1030,0,0.00857398,"-DIR, and ARGM-TMP are shown in the figure. PropBank defines core roles ARG0 through ARG5, which receive different interpretations for different predicates. Additional modifier roles ARGM-* include ARGM-TMP (temporal) and ARGM-DIR (directional), as shown in Figure 2(a). The PropBank representation therefore has a small number of roles, and the training data set comprises some 40,000 sentences, thus making the semantic role labeling task an attractive one from the perspective of machine learning. There are many instances of influential work on semantic role labeling using PropBank conventions. Pradhan et al. (2004) present a system that uses support vector machines (SVMs) to identify the arguments in a syntax tree that can serve as semantic roles, followed by classification of the identified arguments to role names via a collection of binary SVMs. Punyakanok et al. (2004) describe a semantic role labeler that uses integer linear programming for inference and uses several global constraints to find the best 12 Das et al. Frame-Semantic Parsing suited predicate-argument structures. Joint modeling for semantic role labeling with discriminative log-linear models is presented by Toutanova, Haghighi, and Mann"
J14-1002,J08-2005,0,0.157863,"Missing"
J14-1002,C04-1197,0,0.401501,"igure 2(a). The PropBank representation therefore has a small number of roles, and the training data set comprises some 40,000 sentences, thus making the semantic role labeling task an attractive one from the perspective of machine learning. There are many instances of influential work on semantic role labeling using PropBank conventions. Pradhan et al. (2004) present a system that uses support vector machines (SVMs) to identify the arguments in a syntax tree that can serve as semantic roles, followed by classification of the identified arguments to role names via a collection of binary SVMs. Punyakanok et al. (2004) describe a semantic role labeler that uses integer linear programming for inference and uses several global constraints to find the best 12 Das et al. Frame-Semantic Parsing suited predicate-argument structures. Joint modeling for semantic role labeling with discriminative log-linear models is presented by Toutanova, Haghighi, and Manning (2005), where global features looking at all arguments of a particular verb together are incorporated into a dynamic programming and reranking framework. The Computational Linguistics special issue on semantic role labeling (M`arquez et al. 2008) includes ot"
J14-1002,W96-0213,0,0.464731,"Missing"
J14-1002,W06-1616,0,0.0176524,"or semantic role labeling (M`arquez et al. 2008). To our knowledge, the separate line of work investigating frame-semantic parsing has not previously dealt with joint inference. A common trait in prior work, both in PropBank and FrameNet conventions, has been the use of a two-stage model that identifies arguments first, then labels them, often using dynamic programming or integer linear programs (ILPs); we treat both problems together here.4 Recent work in natural language processing (NLP) problems has focused on ILP formulations for complex structure prediction tasks like dependency parsing (Riedel and Clarke 2006; Martins, Smith, and Xing 2009; Martins et al. 2010), sequence tagging (Roth and Yih 2004), as well as PropBank SRL (Punyakanok et al. 2004). Whereas early work in this area focused on declarative formulations tackled with off-the-shelf solvers, Rush et al. (2010) proposed subgradient-based dual decomposition (also called Lagrangian relaxation) as a way of exploiting the structure of the problem and existing combinatorial algorithms. The method allows the combination of models that are individually tractable, but not jointly tractable, by solving a relaxation of the original problem. Since th"
J14-1002,W04-2401,0,0.0218668,"nvestigating frame-semantic parsing has not previously dealt with joint inference. A common trait in prior work, both in PropBank and FrameNet conventions, has been the use of a two-stage model that identifies arguments first, then labels them, often using dynamic programming or integer linear programs (ILPs); we treat both problems together here.4 Recent work in natural language processing (NLP) problems has focused on ILP formulations for complex structure prediction tasks like dependency parsing (Riedel and Clarke 2006; Martins, Smith, and Xing 2009; Martins et al. 2010), sequence tagging (Roth and Yih 2004), as well as PropBank SRL (Punyakanok et al. 2004). Whereas early work in this area focused on declarative formulations tackled with off-the-shelf solvers, Rush et al. (2010) proposed subgradient-based dual decomposition (also called Lagrangian relaxation) as a way of exploiting the structure of the problem and existing combinatorial algorithms. The method allows the combination of models that are individually tractable, but not jointly tractable, by solving a relaxation of the original problem. Since then, dual decomposition has been used to build more accurate models for dependency parsing ("
J14-1002,P11-1008,0,0.0341732,") proposed subgradient-based dual decomposition (also called Lagrangian relaxation) as a way of exploiting the structure of the problem and existing combinatorial algorithms. The method allows the combination of models that are individually tractable, but not jointly tractable, by solving a relaxation of the original problem. Since then, dual decomposition has been used to build more accurate models for dependency parsing (Koo et al. 2010), combinatory categorical grammar supertagging and parsing (Auli and Lopez 2011), and machine translation (Chang and Collins 2011; DeNero and Macherey 2011; Rush and Collins 2011). Recently, Martins et al. (2011b) showed that the success of subgradient-based dual decomposition strongly relies on breaking down the original problem into a “good” 4 In prior work, there are exceptions where identification and classification of arguments have been treated in one step; for more details, please refer to the systems participating in the CoNLL-2004 shared task on semantic role labeling (Carreras and M`arquez 2004). 16 Das et al. Frame-Semantic Parsing decomposition, that is, one with few overlapping components. This leaves out many declarative constrained problems, for which su"
J14-1002,D10-1001,0,0.0101386,"Missing"
J14-1002,N03-1028,0,0.0145026,"labels (Subramanya and Bilmes 2008, 2009; Talukdar and Crammer 2009). For us, graph vertices correspond to natural language types (not tokens) and undirected edges between them are weighted using a similarity metric. Recently, this set-up has been used to learn soft labels on natural language types (say, word n-grams or in our case, syntactically disambiguated 27 The use of unsupported features (i.e., those that can fire for an analysis in the partition function but not observed to fire in the training data) has been observed to give performance improvements in NLP problems; see, for example, Sha and Pereira (2003) and Martins et al. (2010). 27 Computational Linguistics Volume 40, Number 1 predicates) from seed data, resulting in large but noisy lexicons, which are used to constrain structured prediction models. Applications have ranged from domain adaptation of sequence models (Subramanya, Petrov, and Pereira 2010) to unsupervised learning of POS taggers by using bilingual graph-based projections (Das and Petrov 2011). We describe our approach to graph construction, propagation for lexicon expansion, and the use of the result to impose constraints on frame identification. 5.5.1 Graph Construction. We c"
J14-1002,D07-1002,0,0.273124,"ught to extend the coverage of FrameNet by exploiting VerbNet and WordNet (Shi and Mihalcea 2005; Giuglea and Moschitti 2006; Pennacchiotti et al. 2008) and by projecting entries and annotations within and across languages (Boas 2002; Fung and Chen 2004; Pado ¨ and Lapata 2005; Furstenau and Lapata 2009b). Others have explored the application of frame-semantic structures to tasks such as information extraction (Moschitti, Morarescu, and Harabagiu 2003; Surdeanu et al. 2003), textual entailment (Burchardt and Frank 2006; Burchardt et al. 2009), question answering (Narayanan and Harabagiu 2004; Shen and Lapata 2007), and paraphrase recognition (Pado´ and Erk 2005). 2.3 Semi-Supervised Methods Although there has been a significant amount of work in supervised shallow semantic parsing using both PropBank- and FrameNet-style representations, a few improve¨ ments over vanilla supervised methods using unlabeled data are notable. Furstenau and Lapata (2009b) present a method of projecting predicate-argument structures from some seed examples to unlabeled sentences, and use a linear program formulation to find the best alignment explaining the projection. Next, the projected information as well as the seeds are"
J14-1002,W04-2008,0,0.0362235,"r roles within a hierarchy (see Figure 3). PropBank does not explicitly encode relationships among predicates. Most early work on frame-semantic parsing has made use of the exemplar sentences in the FrameNet corpus (see Section 3.1), each of which is annotated for a single frame and its arguments. Gildea and Jurafsky (2002) presented a discriminative model for arguments given the frame; Thompson, Levy, and Manning (2003) used a generative model for both the frame and its arguments. Fleischman, Kwon, and Hovy (2003) first used maximum entropy models to find and label arguments given the frame. Shi and Mihalcea (2004) developed a rule-based system to predict frames and their arguments in text, and Erk and Pado´ (2006) introduced the Shalmaneser tool, which uses naive Bayes classifiers to do the same. Other FrameNet SRL systems (Giuglea and Moschitti 2006, for instance) have used SVMs. Most of this work was done on an older, smaller version of FrameNet, containing around 300 frames and fewer than 500 unique semantic roles. Unlike this body of work, we experimented with the larger SemEval 2007 shared task data set, and also the newer FrameNet 1.5,3 which lists 877 frames and 1,068 role types—thus handling ma"
J14-1002,D08-1016,0,0.00944644,"Missing"
J14-1002,D08-1114,0,0.00506907,"en shown to perform better than several other semi-supervised algorithms ¨ on benchmark data sets (Chapelle, Scholkopf, and Zien 2006, chapter 21). The method constructs a graph where a small portion of vertices correspond to labeled instances, and the rest are unlabeled. Pairs of vertices are connected by weighted edges denoting the similarity between the pair. Traditionally, Markov random walks (Szummer and Jaakkola 2001; Baluja et al. 2008) or optimization of a loss function based on smoothness properties of the graph (e.g., Corduneanu and Jaakkola 2003; Zhu, Ghahramani, and Lafferty 2003; Subramanya and Bilmes 2008) are performed to propagate labels from the labeled vertices to the unlabeled ones. In our work, we are interested in multi-class generalizations of graph-propagation algorithms suitable for NLP applications, where each graph vertex can assume one or more out of many possible labels (Subramanya and Bilmes 2008, 2009; Talukdar and Crammer 2009). For us, graph vertices correspond to natural language types (not tokens) and undirected edges between them are weighted using a similarity metric. Recently, this set-up has been used to learn soft labels on natural language types (say, word n-grams or i"
J14-1002,D10-1017,0,0.0238482,"Missing"
J14-1002,P03-1002,0,0.0695193,"rained a log-linear model on the SemEval 2007 data to evaluate features for the subtask of argument identification. Another line of work has sought to extend the coverage of FrameNet by exploiting VerbNet and WordNet (Shi and Mihalcea 2005; Giuglea and Moschitti 2006; Pennacchiotti et al. 2008) and by projecting entries and annotations within and across languages (Boas 2002; Fung and Chen 2004; Pado ¨ and Lapata 2005; Furstenau and Lapata 2009b). Others have explored the application of frame-semantic structures to tasks such as information extraction (Moschitti, Morarescu, and Harabagiu 2003; Surdeanu et al. 2003), textual entailment (Burchardt and Frank 2006; Burchardt et al. 2009), question answering (Narayanan and Harabagiu 2004; Shen and Lapata 2007), and paraphrase recognition (Pado´ and Erk 2005). 2.3 Semi-Supervised Methods Although there has been a significant amount of work in supervised shallow semantic parsing using both PropBank- and FrameNet-style representations, a few improve¨ ments over vanilla supervised methods using unlabeled data are notable. Furstenau and Lapata (2009b) present a method of projecting predicate-argument structures from some seed examples to unlabeled sentences, and"
J14-1002,E12-1003,0,0.0172185,"show relative improvements. Experiments are performed on the CoNLL 2008 shared task data set (Surdeanu et al. 2008), which follows the PropBank conventions and only labels verbal and nominal predicates—in contrast to our work, which includes most lexicosyntactic categories. A similar approach is presented by Weston, Ratle, and Collobert (2008), who use neural embeddings of words, which are eventually used for SRL; improvements over state-of-the-art PropBank-style SRL systems are observed. Recently, there has been related work in unsupervised semantic role labeling (Lang and Lapata 2010, 2011; Titov and Klementiev 2012) that attempts to induce semantic roles automatically from unannotated data. This line of work may be useful in discovering new semantic frames and roles, but here we stick to the concrete representation provided in FrameNet, without seeking to expand its inventory of semantic types. We present a new semi-supervised technique to expand the set of lexical items with the potential semantic frames that they could evoke; we use a graph-based semi-supervised learning framework to achieve this goal (Section 5.5). 2.4 Joint Inference and Shallow Semantic Parsing Most high-performance SRL systems that"
J14-1002,P05-1073,0,0.0166888,"Missing"
J14-1002,P10-1040,0,0.0142959,"Missing"
J14-1002,W04-3212,0,0.126024,"Missing"
J14-1002,N07-1069,0,0.0626288,"Missing"
J14-1002,erk-pado-2006-shalmaneser,0,\N,Missing
J14-1002,W08-2121,0,\N,Missing
J14-1002,E09-1026,0,\N,Missing
J14-1002,D09-1002,0,\N,Missing
J14-1002,P11-1048,0,\N,Missing
J14-1002,J12-1005,0,\N,Missing
J14-1002,C98-2122,0,\N,Missing
J14-1002,P10-2069,0,\N,Missing
J14-2005,2008.amta-srw.1,0,0.0249847,"ical machine translation (Yamada and Knight 2001). Syntax-based translation models are diverse, using different grammatical formalisms and features. Some use a parse tree for the source sentence (“tree-to-string”), others produce a parse when generating the target sentence (“string-to-tree”), and others combine both (“tree-to-tree”). We focus on the final category in this article. Tree-to-tree translation has proved to be a difficult modeling problem, as initial attempts at it underperformed systems that used no syntax at all (Cowan, Kuˇcerov´a, ¨ and Liu 2009). Subsequent reand Collins 2006; Ambati and Lavie 2008; Liu, Lu, search showed that substantial performance gains can be achieved if hard constraints— specifically, isomorphism between a source sentence’s parse and the parse of its ¨ and Liu 2009; Chiang 2010; Zhang, Zhai, and Zong translation—are relaxed (Liu, Lu, 2011; Hanneman and Lavie 2011). This suggests that constraints must be handled with care. Yet the classic approach to tree-to-tree translation imposes hard constraints through the use of synchronous grammars developed for programming language compilation (Aho and Ullman 1969). A synchronous grammar derives two strings simultaneously: o"
J14-2005,W09-0434,0,0.05099,"Missing"
J14-2005,D10-1117,0,0.0243122,"ees are available (e.g., the Penn Treebank; Marcus, Santorini, & Marcinkiewitz 1993), allowing the use of highly accurate statistical parsers (Levy and Manning 2003; Rafferty and Manning 2008; Martins, Smith, and Xing 2009). We also want to apply our model to languages that do not have tree350 Gimpel and Smith Phrase Dependency MT with Quasi-Synchronous Tree-to-Tree Features banks (e.g., Urdu and Malagasy), and for this we turn to unsupervised parsing. The NLP community has developed a range of statistical algorithms for building unsupervised parsers (Klein and Manning 2002, 2004; Smith 2006; Blunsom and Cohn 2010; Naseem et al. 2010; Spitkovsky, Alshawi, and Jurafsky 2010; Cohen 2011). They require only raw, unannotated text in the language of interest, making them ideal for use in translation. Unsupervised shallow syntactic analysis has been used successfully for translation modeling by Zollmann and Vogel (2011), who showed that unsupervised part-of-speech tags could be used to label the hierarchical translation rules of Chiang (2005) to match the performance of a system that uses supervised full syntactic parses. We take additional steps in this direction, leveraging state-of-the-art unsupervised mo"
J14-2005,C10-1011,0,0.0351016,"we obtained from the Stanford Chinese segmenter.15 For Urdu and Malagasy, we turn to unsupervised parsing. To measure the impact of using unsupervised parsers, we also performed experiments in which we replaced supervised parsers for Chinese and English with unsupervised counterparts. We now describe how we trained unsupervised parsers for these four languages. 15 More dependency parsers have been made available by the research community since we began this research and would be natural choices for further experimentation, such as ParZu (Sennrich et al. 2009) for German, the parser model from Bohnet (2010) adapted for German by Seeker and Kuhn (2012), and DuDuPlus (Chen et al. 2012) for Chinese. 382 Gimpel and Smith Phrase Dependency MT with Quasi-Synchronous Tree-to-Tree Features The most common approach to unsupervised parsing is to train models on sentences from treebanks (without using the annotated trees, of course) along with their gold standard POS tags. This practice must be changed if we wish to use unsupervised parsing for machine translation, because we do not have gold standard POS tags for our data. Fortunately, Smith (2006) and Spitkovsky et al. (2011) have shown that using automa"
J14-2005,J92-4003,0,0.360252,"es between long phrases, we expect to face problems of data sparseness. Long phrases do not occur very often, so pairs of long phrases will occur less often still. One way to address this is to also extract rules that use part-of-speech (POS) tags in place of words. However, since words can have multiple POS tags, we would then need to infer POS tags for the words in order to determine which rule is applicable. So we instead use hard word clusters, which provide a deterministic mapping from words to cluster identifiers. Furthermore, certain types of hard word clusters, such as Brown clusters (Brown et al. 1992), have been shown to correspond well to POS tag categories (Christodoulopoulos, Goldwater, and Steedman 2010). We chose Brown clusters for this reason. Brown clustering uses a bigram hidden Markov model (HMM) in which states are hard cluster labels and observations are words. The emission distributions are constrained such that each word has a nonzero emission probability from at most one cluster label. Clusters can be obtained efficiently through a greedy algorithm that approximately maximizes the HMM’s log-likelihood by alternately proposing new clusters and merging existing ones. This proce"
J14-2005,W06-2920,0,0.0397563,"for many language pairs requires addressing a wide range of translation divergence phenomena. Several researchers have studied divergence between languages in corpora and found it to be considerable, even for closely related languages (Dorr 1994; Fox 2002; Wellington, Waxmonsky, and Melamed 2006; Søgaard and Kuhn 2009). To address this, many have incorporated linguistic syntax into translation model design. The statistical natural language processing (NLP) community has developed automatic parsers that can produce syntactic analyses for sentences in several languages (Klein and Manning 2003; Buchholz and Marsi 2006; Nivre et al. ∗ Toyota Technological Institute at Chicago, Chicago, IL 60637. E-mail: kgimpel@ttic.edu. ∗∗ School of Computer Science, Carnegie Mellon University, Pittsburgh, PA 15213. E-mail: nasmith@cs.cmu.edu. Submission received: 10 November 2012; revised submission received: 12 May 2013; accepted for publication: 23 June 2013. doi:10.1162/COLI a 00175 © 2014 Association for Computational Linguistics Computational Linguistics Volume 40, Number 2 2007). The availability of these parsers, and gains in their accuracy, triggered research interest in syntax-based statistical machine translatio"
J14-2005,D09-1021,0,0.0397908,"Missing"
J14-2005,W08-0336,0,0.038351,"Missing"
J14-2005,P05-1022,0,0.0104731,"ath from xj to xk , minDirPathLen returns ∞. Adding these two features gives us a total of 88 QPD features. Along with the 14 phrase-based features there are a total of 102 features in our model. 6. Decoding For our model, decoding consists of solving Equation (1)—that is, finding the highestscoring tuple hy , π, φ, τφ , b i for an input sentence x and its parse τx . This is a challenging search problem, because it is at least as hard as the search problem for phrase-based models, which is intractable (Koehn, Och, and Marcu 2003). Because of this we use a coarse-to-fine strategy for decoding (Charniak and Johnson 2005; Petrov 2009). Coarseto-fine inference is a general term for procedures that make two (or more) passes over the search space, pruning the space with each pass. Typically, feature complexity is increased in each pass, as richer features can often be computed more easily in the smaller search space. One simple coarse-to-fine procedure for our model would start by generating a k-best list of derivations using a phrase-based decoder. This “coarse model” would account for all of the phrase-based features. Then we could parse each derivation to incorporate the QPD features and rerank the k-best lis"
J14-2005,P05-1033,0,0.463927,"pervised parsing. The NLP community has developed a range of statistical algorithms for building unsupervised parsers (Klein and Manning 2002, 2004; Smith 2006; Blunsom and Cohn 2010; Naseem et al. 2010; Spitkovsky, Alshawi, and Jurafsky 2010; Cohen 2011). They require only raw, unannotated text in the language of interest, making them ideal for use in translation. Unsupervised shallow syntactic analysis has been used successfully for translation modeling by Zollmann and Vogel (2011), who showed that unsupervised part-of-speech tags could be used to label the hierarchical translation rules of Chiang (2005) to match the performance of a system that uses supervised full syntactic parses. We take additional steps in this direction, leveraging state-of-the-art unsupervised models for full syntactic analysis (Klein and Manning 2004; Berg-Kirkpatrick et al. 2010; Gimpel and Smith 2012a) to obtain improvements in translation quality. We find that replacing a supervised parser for Chinese with an unsupervised one has no effect on performance, and using an unsupervised English parser only hurts slightly. We use unsupervised parsing to apply our full model to Urdu→English and English→Malagasy translation"
J14-2005,J07-2003,0,0.285541,"maximum of 15 iterations. We used k-best lists of size 150 and a fixed, untuned value of λ = 0.1 for all experiments. 6.5 Comparison to Earlier Work The decoder described above represents some advances over those presented in earlier papers. Our original decoder was designed for a lexical dependency model; we used lattice dependency parsing on lattices in which each edge contained a single sourcetarget word pair (Gimpel and Smith 2009b). Inference was approximated using cube decoding (Gimpel and Smith 2009a), an algorithm that incorporates non-local features in a way similar to cube pruning (Chiang 2007). After developing our QPD model, we moved to phrase lattices but still approximated inference using an agenda algorithm (Nederhof 2003; Eisner, Goldlust, and Smith 2005) with pre-pruning of dependency edges in a coarse pass (Gimpel and Smith 2011). 14 We used OOQP (Gertz and Wright 2003) to solve the quadratic program in the inner loop, which uses HSL, a collection of Fortran codes for large-scale scientific computation (www.hsl.rl.ac.uk). 380 Gimpel and Smith Phrase Dependency MT with Quasi-Synchronous Tree-to-Tree Features All decoders used lattice dependency parsing, but our current decode"
J14-2005,P10-1146,0,0.0239339,"others produce a parse when generating the target sentence (“string-to-tree”), and others combine both (“tree-to-tree”). We focus on the final category in this article. Tree-to-tree translation has proved to be a difficult modeling problem, as initial attempts at it underperformed systems that used no syntax at all (Cowan, Kuˇcerov´a, ¨ and Liu 2009). Subsequent reand Collins 2006; Ambati and Lavie 2008; Liu, Lu, search showed that substantial performance gains can be achieved if hard constraints— specifically, isomorphism between a source sentence’s parse and the parse of its ¨ and Liu 2009; Chiang 2010; Zhang, Zhai, and Zong translation—are relaxed (Liu, Lu, 2011; Hanneman and Lavie 2011). This suggests that constraints must be handled with care. Yet the classic approach to tree-to-tree translation imposes hard constraints through the use of synchronous grammars developed for programming language compilation (Aho and Ullman 1969). A synchronous grammar derives two strings simultaneously: one in the source language and one in the target language. A single derivation is used for both strings, which limits the divergence phenomena that can be captured. As a result, researchers have developed s"
J14-2005,D10-1056,0,0.0277978,"Missing"
J14-2005,D11-1005,1,0.903197,"Missing"
J14-2005,W06-1628,0,0.0564866,"Missing"
J14-2005,2009.eamt-1.10,0,0.0238875,"words must be present between both the source and target phrase pairs. We note that this rule says nothing about what fills the gap. In particular, the gap-filling material does not have to be translationally equivalent, and indeed in the given sentence pair it is not. As opposed to rules in hierarchical phrase-based models (Chiang 2005), which typically specify translationally equivalent substructures, this rule simply models the reordering and long-distance movement of the infinitive. Much prior work has found phrase pairs with gaps to be useful for machine translation (Simard et al. 2005; Crego and Yvon 2009; Galley and Manning 2010), and we extract tuples as in Equation (7) so that we can model such structures, even though we do not directly model gap-filling like hierarchical models and other models based on synchronous context-free grammar (Zollmann and Venugopal 2006, inter alia). 10 This non-overlapping constraint is what differentiates these tuples from the target-tree rule tuples from the previous section, which are extracted even when the source phrases overlap. 365 Computational Linguistics Volume 40, Number 2 (a) ich meine deshalb , dass es eine frage der geeigneten methodik ist . i thi"
J14-2005,P09-1053,1,0.8273,"l and Smith Phrase Dependency MT with Quasi-Synchronous Tree-to-Tree Features Quasi-synchronous grammar, like synchronous grammar, can in principle be instantiated for a wide range of formalisms. Dependency syntax (which we discuss in Section 2.3) has been used in most previous applications of QG, including word alignment (Smith and Eisner 2006) and machine translation (Gimpel and Smith 2009b). Aside from translation, QG has been used for a variety of applications involving relationships among sentences, including question answering (Wang, Smith, and Mitamura 2007), paraphrase identification (Das and Smith 2009), parser projection and adaptation (Smith and Eisner 2009), title generation (Woodsend, Feng, and Lapata 2010), sentence simplification (Woodsend and Lapata 2011), information retrieval (Park, Croft, and Smith 2011), and supervised parsing from multiple treebanks with different annotation conventions (Li, Liu, and Che 2012). 2.3 Dependency Syntax and Machine Translation Many syntactic theories have been applied to translation modeling, but we focus in this article on dependency syntax (Tesni`ere 1959). Dependency syntax is a lightweight formalism that builds trees consisting of a set of direct"
J14-2005,D11-1018,0,0.0585073,"rzilay 2009). Techniques are also available for grammar induction using treebanks in different languages that are not built on parallel data (Cohen, Das, and Smith 2011). Researchers have recently begun to target learning of parsers specifically for applications like machine translation. Hall et al. (2011) developed a framework to train supervised parsers for use in particular applications by optimizing arbitrary evaluation metrics; Katz-Brown et al. (2011) used this framework to train a parser for reordering 391 Computational Linguistics Volume 40, Number 2 in machine translation. Relatedly, DeNero and Uszkoreit (2011) tailored unsupervised learning of syntactic structure in parallel text to target reordering phenomena. In addition, we may not need full monolingual syntactic parses to obtain the benefits of syntax-based translation modeling. Indeed, the widely used hierarchical phrase-based model of Chiang (2005) induces a synchronous grammar from parallel text without any linguistic annotations. Zollmann and Vogel (2011) and Zollmann (2011) showed that using a supervised POS tagger to label these synchronous rules can improve performance up to the level of a model that uses a supervised full syntactic pars"
J14-2005,P05-1067,0,0.485025,"-tree translation imposes hard constraints through the use of synchronous grammars developed for programming language compilation (Aho and Ullman 1969). A synchronous grammar derives two strings simultaneously: one in the source language and one in the target language. A single derivation is used for both strings, which limits the divergence phenomena that can be captured. As a result, researchers have developed synchronous grammars with larger rules that, rule-internally, capture more phenomena, typically at increased computational expense (Shieber and Schabes 1990; Eisner 2003; Gildea 2003; Ding and Palmer 2005). We take a different approach. We take inspiration from a family of formalisms called quasi-synchronous grammar (QG; Smith and Eisner 2006). Unlike synchronous grammar, QG assumes the entire input sentence and some syntactic parse of it are provided and fixed. QG then defines a monolingual grammar whose language is a set of translations inspired by the input sentence and tree. The productions in this monolingual grammar generate a piece of the translation’s tree and align it to a piece of the fixed input tree. Therefore, arbitrary non-isomorphic structures are possible between the two trees."
J14-2005,J94-4004,0,0.0604105,"vantages of phrase-based and syntax-based translation. We report statistically significant improvements over a phrasebased baseline on five of seven test sets across four language pairs. We also present encouraging preliminary results on the use of unsupervised dependency parsing for syntax-based machine translation. 1. Introduction Building translation systems for many language pairs requires addressing a wide range of translation divergence phenomena. Several researchers have studied divergence between languages in corpora and found it to be considerable, even for closely related languages (Dorr 1994; Fox 2002; Wellington, Waxmonsky, and Melamed 2006; Søgaard and Kuhn 2009). To address this, many have incorporated linguistic syntax into translation model design. The statistical natural language processing (NLP) community has developed automatic parsers that can produce syntactic analyses for sentences in several languages (Klein and Manning 2003; Buchholz and Marsi 2006; Nivre et al. ∗ Toyota Technological Institute at Chicago, Chicago, IL 60637. E-mail: kgimpel@ttic.edu. ∗∗ School of Computer Science, Carnegie Mellon University, Pittsburgh, PA 15213. E-mail: nasmith@cs.cmu.edu. Submissio"
J14-2005,P10-4002,0,0.0156596,"mpel (2012). Our Chinese and English unsupervised parsers are roughly 30 percentage points worse than supervised parsers in dependency attachment accuracy on standard treebank test sets. We also compared the supervised and unsupervised parsers to a uniform-at-random parser. Well-known algorithms exist for sampling derivations under a context-free grammar for a sentence (Johnson, Griffiths, and Goldwater 2007). These algorithms can be used to sample projective dependency trees by representing a projective dependency grammar using a context-free grammar (Smith 2006; Johnson 2007). We used cdec (Dyer et al. 2010) to sample projective dependency trees uniformly at random for each sentence.16 We only compared the random parser for source-side parsing. Swapping parsers for the target language requires parsing the target side of the parallel corpus, rerunning rule extraction and feature computation with the new parses, and finally re-tuning to learn new feature weights. By contrast, changing the source-side parser only requires re-parsing the source side of the tuning and test sets and re-tuning. 7.2 Results We now present our main results, shown in Tables 6–9. We see that enlarging the search space resul"
J14-2005,C96-1058,0,0.0301186,"hs explored during the beam search. 6.2 Lattice Dependency Parsing Each path in a phrase lattice corresponds to a tuple hy , π, φ, b i for the input x . To also maximize over τφ , we perform lattice dependency parsing, which allows us to search over the space of tuples hy , π, φ, b , τφ i. Lattice parsing jointly maximizes over paths through a lattice and parse structures on those paths. Because we use an arc-factored phrase dependency model (Equation (3)), the lattice dependency parsing algorithm we use is a straightforward generalization of the arcfactored dynamic programming algorithm from Eisner (1996). The algorithm is shown in Figure 10. It is shown as a set of recursive equations in which shapes are used in place of function names and shape indices are used in place of function arguments. The equations ground out in functions edgeScore and arcScore that score individual lattice edges and phrase dependency arcs, respectively.13 A semiring-generic format is used; for decoding, the semiring “plus” operator (⊕) would be defined as max and the semiring “times” operator (⊗) would be defined as +. The entry point when executing the algorithm is to build G OAL, which in turn requires building th"
J14-2005,P03-2041,0,0.314358,"lassic approach to tree-to-tree translation imposes hard constraints through the use of synchronous grammars developed for programming language compilation (Aho and Ullman 1969). A synchronous grammar derives two strings simultaneously: one in the source language and one in the target language. A single derivation is used for both strings, which limits the divergence phenomena that can be captured. As a result, researchers have developed synchronous grammars with larger rules that, rule-internally, capture more phenomena, typically at increased computational expense (Shieber and Schabes 1990; Eisner 2003; Gildea 2003; Ding and Palmer 2005). We take a different approach. We take inspiration from a family of formalisms called quasi-synchronous grammar (QG; Smith and Eisner 2006). Unlike synchronous grammar, QG assumes the entire input sentence and some syntactic parse of it are provided and fixed. QG then defines a monolingual grammar whose language is a set of translations inspired by the input sentence and tree. The productions in this monolingual grammar generate a piece of the translation’s tree and align it to a piece of the fixed input tree. Therefore, arbitrary non-isomorphic structures"
J14-2005,H05-1036,1,0.812842,"Missing"
J14-2005,W05-1504,1,0.879634,"yi’ yj’ yl’ yk’ xi xl xj xk yi’ yj’ yl’ yk’ Figure 7 String-to-tree configurations; each is associated with a feature that counts its occurrences in a derivation. 371 Computational Linguistics Volume 40, Number 2 5.2.2 Dependency Length Features. Related to the string-to-tree configurations are features that score source- and target-side lengths (i.e., number of words crossed) of target-side phrase dependencies. These lengths can also be useful for hard constraints to speed up inference; we return to this in Section 6. These features and constraints are similar to those used in vine grammar (Eisner and Smith 2005). We first include a feature that counts the number of source-side words between the 0 j0 aligned source phrases in each attachment in τφ . Letting πc0 = x i0 and πd0 = x lk0 :       fsrc = I dir(c0 , d0 ) = left k0 − (j0 + 1) + I dir(c0 , d0 ) = right i0 − (l0 + 1) vine (34) Although this feature requires the segmentation of the source sentence in order to determine the number of source words crossed, the actual identities of those words are not needed, so the feature does not depend on x. We would expect this feature’s weight to be negative for most language pairs, encouraging closenes"
J14-2005,W02-1039,0,0.56847,"phrase-based and syntax-based translation. We report statistically significant improvements over a phrasebased baseline on five of seven test sets across four language pairs. We also present encouraging preliminary results on the use of unsupervised dependency parsing for syntax-based machine translation. 1. Introduction Building translation systems for many language pairs requires addressing a wide range of translation divergence phenomena. Several researchers have studied divergence between languages in corpora and found it to be considerable, even for closely related languages (Dorr 1994; Fox 2002; Wellington, Waxmonsky, and Melamed 2006; Søgaard and Kuhn 2009). To address this, many have incorporated linguistic syntax into translation model design. The statistical natural language processing (NLP) community has developed automatic parsers that can produce syntactic analyses for sentences in several languages (Klein and Manning 2003; Buchholz and Marsi 2006; Nivre et al. ∗ Toyota Technological Institute at Chicago, Chicago, IL 60637. E-mail: kgimpel@ttic.edu. ∗∗ School of Computer Science, Carnegie Mellon University, Pittsburgh, PA 15213. E-mail: nasmith@cs.cmu.edu. Submission received"
J14-2005,P09-1087,0,0.313606,"ependency trees with word alignment. Arrows are drawn from children to parents. A child word is a modifier of its parent. Each word has exactly one parent and $ is a special “wall” symbol that serves as the parent of all root words in the tree (i.e., those with no other parent). 353 Computational Linguistics Volume 40, Number 2 But most who have used dependency syntax have done so either on the source side in tree-to-string systems (Lin 2004; Xiong, Liu, and Lin 2007; Xie, Mi, and Liu 2011) or the target side in string-to-tree systems (Shen, Xu, and Weischedel 2008; Carreras and Collins 2009; Galley and Manning 2009; Hunter and Resnik 2010; Su et al. 2010; Tu et al. 2010). Others have added features derived from source dependency parses to phrasebased or hierarchical phrase-based translation models (Gimpel and Smith 2008; Gao, Koehn, and Birch 2011). 2.4 Motivating Examples Although Fox (2002) found that dependencies are more often preserved across handaligned bitext than constituents, there are still several concerns when using dependency syntax for tree-to-tree translation. First, we only have hand-aligned sentence pairs for small data sets and few language pairs, so in practice we must deal with the n"
J14-2005,P09-1042,0,0.0551068,"Missing"
J14-2005,D11-1079,0,0.0422617,"Missing"
J14-2005,P03-1011,0,0.219752,"ch to tree-to-tree translation imposes hard constraints through the use of synchronous grammars developed for programming language compilation (Aho and Ullman 1969). A synchronous grammar derives two strings simultaneously: one in the source language and one in the target language. A single derivation is used for both strings, which limits the divergence phenomena that can be captured. As a result, researchers have developed synchronous grammars with larger rules that, rule-internally, capture more phenomena, typically at increased computational expense (Shieber and Schabes 1990; Eisner 2003; Gildea 2003; Ding and Palmer 2005). We take a different approach. We take inspiration from a family of formalisms called quasi-synchronous grammar (QG; Smith and Eisner 2006). Unlike synchronous grammar, QG assumes the entire input sentence and some syntactic parse of it are provided and fixed. QG then defines a monolingual grammar whose language is a set of translations inspired by the input sentence and tree. The productions in this monolingual grammar generate a piece of the translation’s tree and align it to a piece of the fixed input tree. Therefore, arbitrary non-isomorphic structures are possible"
J14-2005,W08-0302,1,0.829259,"nt of all root words in the tree (i.e., those with no other parent). 353 Computational Linguistics Volume 40, Number 2 But most who have used dependency syntax have done so either on the source side in tree-to-string systems (Lin 2004; Xiong, Liu, and Lin 2007; Xie, Mi, and Liu 2011) or the target side in string-to-tree systems (Shen, Xu, and Weischedel 2008; Carreras and Collins 2009; Galley and Manning 2009; Hunter and Resnik 2010; Su et al. 2010; Tu et al. 2010). Others have added features derived from source dependency parses to phrasebased or hierarchical phrase-based translation models (Gimpel and Smith 2008; Gao, Koehn, and Birch 2011). 2.4 Motivating Examples Although Fox (2002) found that dependencies are more often preserved across handaligned bitext than constituents, there are still several concerns when using dependency syntax for tree-to-tree translation. First, we only have hand-aligned sentence pairs for small data sets and few language pairs, so in practice we must deal with the noise in automatic word aligners and parsers. Second, not all dependencies are preserved in hand-aligned data, so we would need to be able to handle non-isomorphic structure even if we did have perfect tools. T"
J14-2005,E09-1037,1,0.941369,"sed an alternative to synchronous grammar— quasi-synchronous grammar (QG)—that exploits this fact for increased flexibility in translation modeling. A QG assumes the source sentence and a parse are given and scores possible translations of the source sentence along with their parses. That is, a quasi-synchronous grammar is a monolingual grammar that derives strings in the target language. The strings’ derivations are scored using feature functions on an alignment from nodes in the target tree to nodes in the source tree. The quasi-synchronous dependency grammars of Smith and Eisner (2006) and Gimpel and Smith (2009b) can generate the translations in Figure 1, as can phrase-based models like Moses (Koehn et al. 2007) and the phrase dependency model we present in Section 3. wir wollen keinen . we do not want one . wir durchleben keine wiederholung des jahres 1938 . we are not living a replay of 1938 . Figure 1 Examples of word alignment patterns in German–English that require the increased expressive power of synchronous tree adjoining grammar. 352 Gimpel and Smith Phrase Dependency MT with Quasi-Synchronous Tree-to-Tree Features Quasi-synchronous grammar, like synchronous grammar, can in principle be ins"
J14-2005,D09-1023,1,0.927912,"Missing"
J14-2005,D11-1044,1,0.873755,"Missing"
J14-2005,N12-1069,1,0.943037,"only raw, unannotated text in the language of interest, making them ideal for use in translation. Unsupervised shallow syntactic analysis has been used successfully for translation modeling by Zollmann and Vogel (2011), who showed that unsupervised part-of-speech tags could be used to label the hierarchical translation rules of Chiang (2005) to match the performance of a system that uses supervised full syntactic parses. We take additional steps in this direction, leveraging state-of-the-art unsupervised models for full syntactic analysis (Klein and Manning 2004; Berg-Kirkpatrick et al. 2010; Gimpel and Smith 2012a) to obtain improvements in translation quality. We find that replacing a supervised parser for Chinese with an unsupervised one has no effect on performance, and using an unsupervised English parser only hurts slightly. We use unsupervised parsing to apply our full model to Urdu→English and English→Malagasy translation, reporting statistically significant improvements over our baselines. These initial results offer promise for researchers to apply syntactic translation models to the thousands of languages for which we do not have manually annotated corpora, and naturally suggest future resea"
J14-2005,N12-1023,1,0.875806,"Missing"
J14-2005,D11-1138,0,0.0133818,"al. 2005) or modeling the noisy projection process (Ganchev, Gillenwater, and Taskar 2009; Smith and Eisner 2009). If we do not have parsers for either language, grammar induction models have been developed to exploit parallel text without using any annotations on either side (Kuhn 2004; Snyder, Naseem, and Barzilay 2009). Techniques are also available for grammar induction using treebanks in different languages that are not built on parallel data (Cohen, Das, and Smith 2011). Researchers have recently begun to target learning of parsers specifically for applications like machine translation. Hall et al. (2011) developed a framework to train supervised parsers for use in particular applications by optimizing arbitrary evaluation metrics; Katz-Brown et al. (2011) used this framework to train a parser for reordering 391 Computational Linguistics Volume 40, Number 2 in machine translation. Relatedly, DeNero and Uszkoreit (2011) tailored unsupervised learning of syntactic structure in parallel text to target reordering phenomena. In addition, we may not need full monolingual syntactic parses to obtain the benefits of syntax-based translation modeling. Indeed, the widely used hierarchical phrase-based mo"
J14-2005,W11-1011,0,0.012807,"”), and others combine both (“tree-to-tree”). We focus on the final category in this article. Tree-to-tree translation has proved to be a difficult modeling problem, as initial attempts at it underperformed systems that used no syntax at all (Cowan, Kuˇcerov´a, ¨ and Liu 2009). Subsequent reand Collins 2006; Ambati and Lavie 2008; Liu, Lu, search showed that substantial performance gains can be achieved if hard constraints— specifically, isomorphism between a source sentence’s parse and the parse of its ¨ and Liu 2009; Chiang 2010; Zhang, Zhai, and Zong translation—are relaxed (Liu, Lu, 2011; Hanneman and Lavie 2011). This suggests that constraints must be handled with care. Yet the classic approach to tree-to-tree translation imposes hard constraints through the use of synchronous grammars developed for programming language compilation (Aho and Ullman 1969). A synchronous grammar derives two strings simultaneously: one in the source language and one in the target language. A single derivation is used for both strings, which limits the divergence phenomena that can be captured. As a result, researchers have developed synchronous grammars with larger rules that, rule-internally, capture more phenomena, typ"
J14-2005,W11-2123,0,0.0151211,"riments. Language Models. Language models were trained using the target side of the parallel corpus in each case augmented with 24,760,743 lines (601,052,087 tokens) of randomly selected sentences from the Gigaword v4 corpus (excluding the New York Times and Los Angeles Times). The minimum count cutoff for unigrams, bigrams, and trigrams was one and the cutoff for fourgrams and fivegrams was three. Language models were estimated using the SRI Language Modeling toolkit (Stolcke 2002) with modified Kneser-Ney smoothing (Chen and Goodman 1998). Language model inference was performed using KenLM (Heafield 2011) within Moses. For EN→MG, we estimated a 5-gram language model using only the target side of the parallel corpus, which contained 89,107 lines with 2,031,814 tokens. We did not use any additional Malagasy data for estimating the EN→MG language models in order to explore a scenario in which target-language text is limited or expensive to obtain. Word Clustering. Brown clusters (Brown et al. 1992) were generated using code provided by Liang (2005). For each language pair, 100 word clusters were generated for the target language. The implementation allows the use of a token count cutoff, which ca"
J14-2005,P08-1067,0,0.240556,"ass, as richer features can often be computed more easily in the smaller search space. One simple coarse-to-fine procedure for our model would start by generating a k-best list of derivations using a phrase-based decoder. This “coarse model” would account for all of the phrase-based features. Then we could parse each derivation to incorporate the QPD features and rerank the k-best list with the modified scores; this is the “fine model.” The advantage of this approach is its simplicity, but other research has shown that k-best lists for structured prediction tend to have very little diversity (Huang 2008), and we expect even less diversity in cases like machine translation where latent variables are almost always present. Instead, we generate a phrase lattice (Ueffing, Och, and Ney 2002) in a coarse pass and perform lattice dependency parsing as the fine pass. The remainder of this section is laid out as follows. We begin by reviewing phrase lattices in Section 6.1. In Section 6.2 we present our basic lattice dependency parsing algorithm. We give three ways to speed it up in Section 6.3; one enables a more judicious search without affecting the search space, and the other two prune the search"
J14-2005,W05-1506,0,0.0127642,"hrase-based model used to generate phrase lattices. Then, after generating the lattices, we prune them (Section 6.3.2) and use a second round of tuning to learn parameters of the fine model, which includes all phrase-based and QPD feature weights. We initialized the phrase-based feature weights using the default Moses weights. For the QPD features, we initialized the phrase dependency probability feature weights to 0.002 and the weights for all other features to 0. For tuning, we need the k-best outputs, for which efficient dynamic programming algorithms are available. We use Algorithm 3 from Huang and Chiang (2005), which lazily finds the k best derivations efficiently. In preliminary testing, we found that the k-best lists tended to be dominated by repeated translations with different derivations, so we used the technique presented by Huang, Knight, and Joshi (2006), which finds a unique k-best list, returning the highest-scoring derivation for each of k unique translations. This modification requires the maintenance of additional data structures to store all of the previously found string yields for each item built during parsing. This incurs additional overhead but allows us to obtain a far more dive"
J14-2005,2006.amta-papers.8,0,0.218632,"Missing"
J14-2005,P07-1022,0,0.0187182,"procedure is described in Gimpel (2012). Our Chinese and English unsupervised parsers are roughly 30 percentage points worse than supervised parsers in dependency attachment accuracy on standard treebank test sets. We also compared the supervised and unsupervised parsers to a uniform-at-random parser. Well-known algorithms exist for sampling derivations under a context-free grammar for a sentence (Johnson, Griffiths, and Goldwater 2007). These algorithms can be used to sample projective dependency trees by representing a projective dependency grammar using a context-free grammar (Smith 2006; Johnson 2007). We used cdec (Dyer et al. 2010) to sample projective dependency trees uniformly at random for each sentence.16 We only compared the random parser for source-side parsing. Swapping parsers for the target language requires parsing the target side of the parallel corpus, rerunning rule extraction and feature computation with the new parses, and finally re-tuning to learn new feature weights. By contrast, changing the source-side parser only requires re-parsing the source side of the tuning and test sets and re-tuning. 7.2 Results We now present our main results, shown in Tables 6–9. We see that"
J14-2005,N07-1018,0,0.0550626,"Missing"
J14-2005,D11-1017,0,0.0138224,"her language, grammar induction models have been developed to exploit parallel text without using any annotations on either side (Kuhn 2004; Snyder, Naseem, and Barzilay 2009). Techniques are also available for grammar induction using treebanks in different languages that are not built on parallel data (Cohen, Das, and Smith 2011). Researchers have recently begun to target learning of parsers specifically for applications like machine translation. Hall et al. (2011) developed a framework to train supervised parsers for use in particular applications by optimizing arbitrary evaluation metrics; Katz-Brown et al. (2011) used this framework to train a parser for reordering 391 Computational Linguistics Volume 40, Number 2 in machine translation. Relatedly, DeNero and Uszkoreit (2011) tailored unsupervised learning of syntactic structure in parallel text to target reordering phenomena. In addition, we may not need full monolingual syntactic parses to obtain the benefits of syntax-based translation modeling. Indeed, the widely used hierarchical phrase-based model of Chiang (2005) induces a synchronous grammar from parallel text without any linguistic annotations. Zollmann and Vogel (2011) and Zollmann (2011) sh"
J14-2005,P02-1017,0,0.126625,"ish), treebanks of hand-annotated parse trees are available (e.g., the Penn Treebank; Marcus, Santorini, & Marcinkiewitz 1993), allowing the use of highly accurate statistical parsers (Levy and Manning 2003; Rafferty and Manning 2008; Martins, Smith, and Xing 2009). We also want to apply our model to languages that do not have tree350 Gimpel and Smith Phrase Dependency MT with Quasi-Synchronous Tree-to-Tree Features banks (e.g., Urdu and Malagasy), and for this we turn to unsupervised parsing. The NLP community has developed a range of statistical algorithms for building unsupervised parsers (Klein and Manning 2002, 2004; Smith 2006; Blunsom and Cohn 2010; Naseem et al. 2010; Spitkovsky, Alshawi, and Jurafsky 2010; Cohen 2011). They require only raw, unannotated text in the language of interest, making them ideal for use in translation. Unsupervised shallow syntactic analysis has been used successfully for translation modeling by Zollmann and Vogel (2011), who showed that unsupervised part-of-speech tags could be used to label the hierarchical translation rules of Chiang (2005) to match the performance of a system that uses supervised full syntactic parses. We take additional steps in this direction, le"
J14-2005,P04-1061,0,0.312071,"Alshawi, and Jurafsky 2010; Cohen 2011). They require only raw, unannotated text in the language of interest, making them ideal for use in translation. Unsupervised shallow syntactic analysis has been used successfully for translation modeling by Zollmann and Vogel (2011), who showed that unsupervised part-of-speech tags could be used to label the hierarchical translation rules of Chiang (2005) to match the performance of a system that uses supervised full syntactic parses. We take additional steps in this direction, leveraging state-of-the-art unsupervised models for full syntactic analysis (Klein and Manning 2004; Berg-Kirkpatrick et al. 2010; Gimpel and Smith 2012a) to obtain improvements in translation quality. We find that replacing a supervised parser for Chinese with an unsupervised one has no effect on performance, and using an unsupervised English parser only hurts slightly. We use unsupervised parsing to apply our full model to Urdu→English and English→Malagasy translation, reporting statistically significant improvements over our baselines. These initial results offer promise for researchers to apply syntactic translation models to the thousands of languages for which we do not have manually"
J14-2005,W04-3250,0,0.0637841,"give examples. We conclude in Section 7.4 with a runtime analysis of our decoder and show the impact of decoding constraints on speed and translation quality. 7.1 Experimental Setup In this section we describe details common to the experiments reported in this section. Details about decoding and learning were described in Section 6. Full details about language pairs, data sets, and baseline systems are given in Appendix A and Appendix B. We repeat important details here. We use case-insensitive IBM BLEU (Papineni et al. 2002) for evaluation. To measure significance, we use a paired bootstrap (Koehn 2004) with 100,000 samples (p ≤ 0.05). 7.1.1 Language Pairs. We consider German→English (DE→EN), Chinese→English (ZH→EN), Urdu→English (UR→EN), and English→Malagasy (EN→MG) translation. These four languages exhibit a range of syntactic divergence from English. They also vary in the availability of resources like parallel data, monolingual target-language data, and treebanks. It is standard practice to evaluate unsupervised parsers on languages that do actually have treebanks, which are used for evaluation. We consider this case as well, comparing supervised parsers for English and Chinese to our un"
J14-2005,P07-2045,0,0.124468,"ased flexibility in translation modeling. A QG assumes the source sentence and a parse are given and scores possible translations of the source sentence along with their parses. That is, a quasi-synchronous grammar is a monolingual grammar that derives strings in the target language. The strings’ derivations are scored using feature functions on an alignment from nodes in the target tree to nodes in the source tree. The quasi-synchronous dependency grammars of Smith and Eisner (2006) and Gimpel and Smith (2009b) can generate the translations in Figure 1, as can phrase-based models like Moses (Koehn et al. 2007) and the phrase dependency model we present in Section 3. wir wollen keinen . we do not want one . wir durchleben keine wiederholung des jahres 1938 . we are not living a replay of 1938 . Figure 1 Examples of word alignment patterns in German–English that require the increased expressive power of synchronous tree adjoining grammar. 352 Gimpel and Smith Phrase Dependency MT with Quasi-Synchronous Tree-to-Tree Features Quasi-synchronous grammar, like synchronous grammar, can in principle be instantiated for a wide range of formalisms. Dependency syntax (which we discuss in Section 2.3) has been"
J14-2005,N03-1017,0,0.0829903,"Missing"
J14-2005,P04-1060,0,0.0282907,"f parallel text is available and we have a parser for one of the languages: The parallel text can be word-aligned and the annotations can be projected across the word alignments (Yarowsky and Ngai 2001; Yarowsky, Ngai, and Wicentoswki 2001). The projected parses can be improved by applying manually written rules (Hwa et al. 2005) or modeling the noisy projection process (Ganchev, Gillenwater, and Taskar 2009; Smith and Eisner 2009). If we do not have parsers for either language, grammar induction models have been developed to exploit parallel text without using any annotations on either side (Kuhn 2004; Snyder, Naseem, and Barzilay 2009). Techniques are also available for grammar induction using treebanks in different languages that are not built on parallel data (Cohen, Das, and Smith 2011). Researchers have recently begun to target learning of parsers specifically for applications like machine translation. Hall et al. (2011) developed a framework to train supervised parsers for use in particular applications by optimizing arbitrary evaluation metrics; Katz-Brown et al. (2011) used this framework to train a parser for reordering 391 Computational Linguistics Volume 40, Number 2 in machine"
J14-2005,P03-1056,0,0.285639,"opinion mining and previously for machine translation by Hunter and Resnik (2010). Because we combine phrase dependencies with features from quasi-synchronous grammar, we refer to our model as a quasi-synchronous phrase dependency (QPD) translation model. Our tree-to-tree approach requires parsers for both the source and target languages. For two of the language pairs we consider (Chinese→English and German→English), treebanks of hand-annotated parse trees are available (e.g., the Penn Treebank; Marcus, Santorini, & Marcinkiewitz 1993), allowing the use of highly accurate statistical parsers (Levy and Manning 2003; Rafferty and Manning 2008; Martins, Smith, and Xing 2009). We also want to apply our model to languages that do not have tree350 Gimpel and Smith Phrase Dependency MT with Quasi-Synchronous Tree-to-Tree Features banks (e.g., Urdu and Malagasy), and for this we turn to unsupervised parsing. The NLP community has developed a range of statistical algorithms for building unsupervised parsers (Klein and Manning 2002, 2004; Smith 2006; Blunsom and Cohn 2010; Naseem et al. 2010; Spitkovsky, Alshawi, and Jurafsky 2010; Cohen 2011). They require only raw, unannotated text in the language of interest,"
J14-2005,W08-0402,0,0.0129461,"d thus far. So, every node in the lattice is annotated with the coverage vector of all paths that end there. This is shown for three of the nodes in the figure. The lattice is constructed such that all features in the model are locally computable on individual lattice edges. To make n-gram language model features local, all paths leading to a given node must end in the same n − 1 words.12 In the example, there are two nodes with equivalent coverage vectors that are separated because they end in 12 In practice, this state replication can be reduced by exploiting sparsity in the language model (Li and Khudanpur 2008). 375 Computational Linguistics Volume 40, Number 2 different words (you vs. could). Decoders like Moses can output phrase lattices like these; the lattice simply encodes the paths explored during the beam search. 6.2 Lattice Dependency Parsing Each path in a phrase lattice corresponds to a tuple hy , π, φ, b i for the input x . To also maximize over τφ , we perform lattice dependency parsing, which allows us to search over the space of tuples hy , π, φ, b , τφ i. Lattice parsing jointly maximizes over paths through a lattice and parse structures on those paths. Because we use an arc-factored"
J14-2005,P12-1071,0,0.0370512,"Missing"
J14-2005,C04-1072,0,0.0341904,"translation y when describing the reranker and omit the additional input and output variables τx , π, φ, τφ , and b , but they are always present and used for computing features. We R assume a tuning set with N source sentences: {x i }N i=1 . Let Yi be the set of reference (1) (k) translations for source sentence x i . Let Yi = {y i . . . y i } denote the set of k candidate translations (outputs of our lattice dependency parsing decoder) for x i . Let y ∗i denote the highest-quality translation in the set, that is, y ∗i = argminy ∈Yi `(YRi , y ), where `(YRi , y ) is the negated BLEU+1 score (Lin and Och 2004) of y evaluated against references YRi . 379 Computational Linguistics Volume 40, Number 2 We use the following cost function for sentence i and candidate translation y : L(YRi , y ) = `(YRi , y ) − `(YRi , y ∗i ) (38) that is, the negated BLEU+1 score of translation y i relative to that of the best translation (yy∗i ) in the set. Yadollahpour, Batra, and Shakhnarovich (2013) formulate the reranking learning problem as an L2 -regularized slack-rescaled structured support vector machine (SSVM; Tsochantaridis et al. 2005). The feature weights θ for the fine model are learned by solving the follo"
J14-2005,C04-1090,0,0.265499,"parses across word alignments in order to model dependency syntax on phrase pairs. $ konnten sie es übersetzen ? $ could you translate it ? Figure 2 Examples of dependency trees with word alignment. Arrows are drawn from children to parents. A child word is a modifier of its parent. Each word has exactly one parent and $ is a special “wall” symbol that serves as the parent of all root words in the tree (i.e., those with no other parent). 353 Computational Linguistics Volume 40, Number 2 But most who have used dependency syntax have done so either on the source side in tree-to-string systems (Lin 2004; Xiong, Liu, and Lin 2007; Xie, Mi, and Liu 2011) or the target side in string-to-tree systems (Shen, Xu, and Weischedel 2008; Carreras and Collins 2009; Galley and Manning 2009; Hunter and Resnik 2010; Su et al. 2010; Tu et al. 2010). Others have added features derived from source dependency parses to phrasebased or hierarchical phrase-based translation models (Gimpel and Smith 2008; Gao, Koehn, and Birch 2011). 2.4 Motivating Examples Although Fox (2002) found that dependencies are more often preserved across handaligned bitext than constituents, there are still several concerns when using"
J14-2005,P07-1089,0,0.021249,"decrease of 0.5 BLEU. When pairing unsupervised English parsing with supervised Chinese parsing, we see an average drop of just 0.2 BLEU compared to the fully supervised case. When both parsers are unsupervised, BLEU scores drop further but are still above the best Moses baseline on average. One idea that we have not explored is to parse our parallel corpus using each parser (unsupervised and supervised), then extract rules consistent with any of the parses. This might give us some of the benefits of forest-based rule extraction, which has frequently been shown to improve translation quality (Liu et al. 2007; Mi, Huang, and Liu 2008; Mi and Huang 2008). Similarly, because we train systems for several language pairs, we could pool the rules extracted from all parallel corpora for computing targetsyntactic features. For example, adding the English phrase dependency rules from the DE→EN corpus could improve performance of our ZH→EN and UR→EN systems. Moving beyond translation, we could use the pool of extracted rules from all systems (and using all parsers) to build monolingual phrase dependency parsers for use in other applications (Wu et al. 2009). 7.2.2 Feature Ablation. We performed feature abla"
J14-2005,J93-2004,0,0.0466416,"Missing"
J14-2005,P09-1039,1,0.885898,"Missing"
J14-2005,D10-1004,1,0.883885,"ation. Our model organizes phrases into a tree structure inspired by dependency syntax (Tesni`ere 1959). Instead of standard dependency trees in which words are vertices, our trees have phrases as vertices. The result captures phenomena like local reordering and idiomatic translations within phrases, as well as long-distance relationships among the phrases in a sentence. We use the term phrase dependency tree when referring to this type of dependency tree; phrase dependencies have also been used by Wu et al. (2009) for opinion mining and previously for machine translation by Hunter and Resnik (2010). Because we combine phrase dependencies with features from quasi-synchronous grammar, we refer to our model as a quasi-synchronous phrase dependency (QPD) translation model. Our tree-to-tree approach requires parsers for both the source and target languages. For two of the language pairs we consider (Chinese→English and German→English), treebanks of hand-annotated parse trees are available (e.g., the Penn Treebank; Marcus, Santorini, & Marcinkiewitz 1993), allowing the use of highly accurate statistical parsers (Levy and Manning 2003; Rafferty and Manning 2008; Martins, Smith, and Xing 2009)."
J14-2005,N03-1021,0,0.0297769,"is summarized in Table 1. 351 Computational Linguistics Volume 40, Number 2 Table 1 Notation used in this article. i, j, k, l x, y xi j xi [i] |x | integers vectors entry i in vector x sequence from entry i to entry j (inclusive) in vector x the set containing the first i positive integers length of vector x 2.2 Synchronous and Quasi-Synchronous Grammars To model syntactic transformations, researchers have developed powerful grammatical formalisms, many of which are variations of synchronous grammars. The most widely used is synchronous context-free grammar (Wu 1997; Gildea 2003; Chiang 2005; Melamed 2003), an extension of context-free grammar to a bilingual setting where two strings are generated simultaneously with a single derivation. Synchronous context-free grammars are computationally attractive but researchers have shown that they cannot handle certain phenomena in manually aligned parallel data (Wellington, Waxmonsky, and Melamed 2006; Søgaard and Kuhn 2009). Figure 1 shows two such examples of word alignment patterns in German–English data. These patterns were called “crossserial discontinuous translation units” (CDTUs) by Søgaard and Kuhn (2009). CDTUs cannot even be handled by the mo"
J14-2005,D08-1022,0,0.0257376,"vised English parsing with supervised Chinese parsing, we see an average drop of just 0.2 BLEU compared to the fully supervised case. When both parsers are unsupervised, BLEU scores drop further but are still above the best Moses baseline on average. One idea that we have not explored is to parse our parallel corpus using each parser (unsupervised and supervised), then extract rules consistent with any of the parses. This might give us some of the benefits of forest-based rule extraction, which has frequently been shown to improve translation quality (Liu et al. 2007; Mi, Huang, and Liu 2008; Mi and Huang 2008). Similarly, because we train systems for several language pairs, we could pool the rules extracted from all parallel corpora for computing targetsyntactic features. For example, adding the English phrase dependency rules from the DE→EN corpus could improve performance of our ZH→EN and UR→EN systems. Moving beyond translation, we could use the pool of extracted rules from all systems (and using all parsers) to build monolingual phrase dependency parsers for use in other applications (Wu et al. 2009). 7.2.2 Feature Ablation. We performed feature ablation experiments for UR→EN translation, shown"
J14-2005,P08-1023,0,0.142212,"Missing"
J14-2005,D10-1120,0,0.0229544,", the Penn Treebank; Marcus, Santorini, & Marcinkiewitz 1993), allowing the use of highly accurate statistical parsers (Levy and Manning 2003; Rafferty and Manning 2008; Martins, Smith, and Xing 2009). We also want to apply our model to languages that do not have tree350 Gimpel and Smith Phrase Dependency MT with Quasi-Synchronous Tree-to-Tree Features banks (e.g., Urdu and Malagasy), and for this we turn to unsupervised parsing. The NLP community has developed a range of statistical algorithms for building unsupervised parsers (Klein and Manning 2002, 2004; Smith 2006; Blunsom and Cohn 2010; Naseem et al. 2010; Spitkovsky, Alshawi, and Jurafsky 2010; Cohen 2011). They require only raw, unannotated text in the language of interest, making them ideal for use in translation. Unsupervised shallow syntactic analysis has been used successfully for translation modeling by Zollmann and Vogel (2011), who showed that unsupervised part-of-speech tags could be used to label the hierarchical translation rules of Chiang (2005) to match the performance of a system that uses supervised full syntactic parses. We take additional steps in this direction, leveraging state-of-the-art unsupervised models for full syntac"
J14-2005,J03-1006,0,0.0952783,"perator (⊕) would be defined as max and the semiring “times” operator (⊗) would be defined as +. The entry point when executing the algorithm is to build G OAL, which in turn requires building the other structures. We use a simple top–down implementation with memoization. Our style of specifying dynamic programming algorithms is similar to weighted deduction, but additionally specifies indices and ranges of iteration, which are useful for a top–down implementation. Top–down dynamic programming avoids the overhead of maintaining a priority queue that is required by bottom–up agenda algorithms (Nederhof 2003; Eisner, Goldlust, and Smith 2005). The disadvantage of top–down dynamic programming is that wasted work can be done; structures can be built that are never used in any full parse. This problem appears when parsing with context-free grammars, and so the CKY algorithm works bottom– up, starting with the smallest constituents and incrementally building larger ones. This is because context-free grammars may contain rules with only non-terminals. Top– down execution may consider the application of such rules in sequence, producing long derivations of non-terminals that never “ground out” in any s"
J14-2005,P03-1021,0,0.00908061,"we used the technique presented by Huang, Knight, and Joshi (2006), which finds a unique k-best list, returning the highest-scoring derivation for each of k unique translations. This modification requires the maintenance of additional data structures to store all of the previously found string yields for each item built during parsing. This incurs additional overhead but allows us to obtain a far more diverse k-best list given a fixed time and memory budget. For the first round of tuning, we use R AMPION (Gimpel and Smith 2012b), which performs competitively with minimum error rate training (Och 2003) but is more stable. For training the fine model, however, we found that R AMPION did not lead to substantial improvements over the output of the coarse phrase-based model alone. We found better performance by using a fine learner designed for the k-best reranking setting, in particular the structured support vector machine reranker described by Yadollahpour, Batra, and Shakhnarovich (2013). Though we are doing lattice reranking rather than k-best reranking, the learning problem for our fine model is similar to that for k-best reranking in that the decoder is exact (i.e., there is no pruning t"
J14-2005,J03-1002,0,0.00722327,"Missing"
J14-2005,P02-1040,0,0.0911232,"iments in Section 7.2.2. We present the results of a manual evaluation in Section 7.3 and give examples. We conclude in Section 7.4 with a runtime analysis of our decoder and show the impact of decoding constraints on speed and translation quality. 7.1 Experimental Setup In this section we describe details common to the experiments reported in this section. Details about decoding and learning were described in Section 6. Full details about language pairs, data sets, and baseline systems are given in Appendix A and Appendix B. We repeat important details here. We use case-insensitive IBM BLEU (Papineni et al. 2002) for evaluation. To measure significance, we use a paired bootstrap (Koehn 2004) with 100,000 samples (p ≤ 0.05). 7.1.1 Language Pairs. We consider German→English (DE→EN), Chinese→English (ZH→EN), Urdu→English (UR→EN), and English→Malagasy (EN→MG) translation. These four languages exhibit a range of syntactic divergence from English. They also vary in the availability of resources like parallel data, monolingual target-language data, and treebanks. It is standard practice to evaluate unsupervised parsers on languages that do actually have treebanks, which are used for evaluation. We consider t"
J14-2005,P05-1034,0,0.17418,"Missing"
J14-2005,W08-1006,0,0.182932,"iously for machine translation by Hunter and Resnik (2010). Because we combine phrase dependencies with features from quasi-synchronous grammar, we refer to our model as a quasi-synchronous phrase dependency (QPD) translation model. Our tree-to-tree approach requires parsers for both the source and target languages. For two of the language pairs we consider (Chinese→English and German→English), treebanks of hand-annotated parse trees are available (e.g., the Penn Treebank; Marcus, Santorini, & Marcinkiewitz 1993), allowing the use of highly accurate statistical parsers (Levy and Manning 2003; Rafferty and Manning 2008; Martins, Smith, and Xing 2009). We also want to apply our model to languages that do not have tree350 Gimpel and Smith Phrase Dependency MT with Quasi-Synchronous Tree-to-Tree Features banks (e.g., Urdu and Malagasy), and for this we turn to unsupervised parsing. The NLP community has developed a range of statistical algorithms for building unsupervised parsers (Klein and Manning 2002, 2004; Smith 2006; Blunsom and Cohn 2010; Naseem et al. 2010; Spitkovsky, Alshawi, and Jurafsky 2010; Cohen 2011). They require only raw, unannotated text in the language of interest, making them ideal for use"
J14-2005,N06-1032,0,0.0655531,"Missing"
J14-2005,seeker-kuhn-2012-making,0,0.0172191,"e segmenter.15 For Urdu and Malagasy, we turn to unsupervised parsing. To measure the impact of using unsupervised parsers, we also performed experiments in which we replaced supervised parsers for Chinese and English with unsupervised counterparts. We now describe how we trained unsupervised parsers for these four languages. 15 More dependency parsers have been made available by the research community since we began this research and would be natural choices for further experimentation, such as ParZu (Sennrich et al. 2009) for German, the parser model from Bohnet (2010) adapted for German by Seeker and Kuhn (2012), and DuDuPlus (Chen et al. 2012) for Chinese. 382 Gimpel and Smith Phrase Dependency MT with Quasi-Synchronous Tree-to-Tree Features The most common approach to unsupervised parsing is to train models on sentences from treebanks (without using the annotated trees, of course) along with their gold standard POS tags. This practice must be changed if we wish to use unsupervised parsing for machine translation, because we do not have gold standard POS tags for our data. Fortunately, Smith (2006) and Spitkovsky et al. (2011) have shown that using automatic POS tags for dependency grammar induction"
J14-2005,P08-1066,0,0.140416,"Missing"
J14-2005,C90-3045,0,0.695122,"ndled with care. Yet the classic approach to tree-to-tree translation imposes hard constraints through the use of synchronous grammars developed for programming language compilation (Aho and Ullman 1969). A synchronous grammar derives two strings simultaneously: one in the source language and one in the target language. A single derivation is used for both strings, which limits the divergence phenomena that can be captured. As a result, researchers have developed synchronous grammars with larger rules that, rule-internally, capture more phenomena, typically at increased computational expense (Shieber and Schabes 1990; Eisner 2003; Gildea 2003; Ding and Palmer 2005). We take a different approach. We take inspiration from a family of formalisms called quasi-synchronous grammar (QG; Smith and Eisner 2006). Unlike synchronous grammar, QG assumes the entire input sentence and some syntactic parse of it are provided and fixed. QG then defines a monolingual grammar whose language is a set of translations inspired by the input sentence and tree. The productions in this monolingual grammar generate a piece of the translation’s tree and align it to a piece of the fixed input tree. Therefore, arbitrary non-isomorphi"
J14-2005,H05-1095,0,0.0345308,"indicate that other words must be present between both the source and target phrase pairs. We note that this rule says nothing about what fills the gap. In particular, the gap-filling material does not have to be translationally equivalent, and indeed in the given sentence pair it is not. As opposed to rules in hierarchical phrase-based models (Chiang 2005), which typically specify translationally equivalent substructures, this rule simply models the reordering and long-distance movement of the infinitive. Much prior work has found phrase pairs with gaps to be useful for machine translation (Simard et al. 2005; Crego and Yvon 2009; Galley and Manning 2010), and we extract tuples as in Equation (7) so that we can model such structures, even though we do not directly model gap-filling like hierarchical models and other models based on synchronous context-free grammar (Zollmann and Venugopal 2006, inter alia). 10 This non-overlapping constraint is what differentiates these tuples from the target-tree rule tuples from the previous section, which are extracted even when the source phrases overlap. 365 Computational Linguistics Volume 40, Number 2 (a) ich meine deshalb , dass es eine frage der geeigneten"
J14-2005,A97-1014,0,0.0579414,"ee-to-tree divergence in German–English data.1 We consider the German– English parallel corpus used in our experiments (and described in Appendix A). We parsed the English side using TurboParser (Martins et al. 2010), a state-of-the-art dependency parser. TurboParser was trained on the Penn Treebank (Marcus, Santorini, and Marcinkiewicz 1993) converted to dependencies using the Yamada-Matsumoto head rules (Yamada and Matsumoto 2003). We parsed the German side using the factored model in the Stanford parser (Rafferty and Manning 2008), which is trained from the NEGRA phrase-structure treebank (Skut et al. 1997). The Stanford parser’s source code defines a set of head rules for converting the phrase-structure parse output to dependencies.2 The first example is shown in Figure 3. The bold words illustrate a “sibling” relationship, meaning that the source words aligned to the parent and child in the English sentence have the same parent on the German side. Many sibling configurations appear when the English dependency is DET→N within a PP. By convention, the NEGRA treebank uses flat structures for PPs like “P DET N” rather than using a separate NP for DET N. When the parser converts this to a dependenc"
J14-2005,W06-3104,0,0.351833,"Ullman 1969). A synchronous grammar derives two strings simultaneously: one in the source language and one in the target language. A single derivation is used for both strings, which limits the divergence phenomena that can be captured. As a result, researchers have developed synchronous grammars with larger rules that, rule-internally, capture more phenomena, typically at increased computational expense (Shieber and Schabes 1990; Eisner 2003; Gildea 2003; Ding and Palmer 2005). We take a different approach. We take inspiration from a family of formalisms called quasi-synchronous grammar (QG; Smith and Eisner 2006). Unlike synchronous grammar, QG assumes the entire input sentence and some syntactic parse of it are provided and fixed. QG then defines a monolingual grammar whose language is a set of translations inspired by the input sentence and tree. The productions in this monolingual grammar generate a piece of the translation’s tree and align it to a piece of the fixed input tree. Therefore, arbitrary non-isomorphic structures are possible between the two trees. A weighted QG uses feature functions to softly penalize or encourage particular types of syntactic divergence. In this article, we present a"
J14-2005,D09-1086,0,0.133319,"Tree-to-Tree Features Quasi-synchronous grammar, like synchronous grammar, can in principle be instantiated for a wide range of formalisms. Dependency syntax (which we discuss in Section 2.3) has been used in most previous applications of QG, including word alignment (Smith and Eisner 2006) and machine translation (Gimpel and Smith 2009b). Aside from translation, QG has been used for a variety of applications involving relationships among sentences, including question answering (Wang, Smith, and Mitamura 2007), paraphrase identification (Das and Smith 2009), parser projection and adaptation (Smith and Eisner 2009), title generation (Woodsend, Feng, and Lapata 2010), sentence simplification (Woodsend and Lapata 2011), information retrieval (Park, Croft, and Smith 2011), and supervised parsing from multiple treebanks with different annotation conventions (Li, Liu, and Che 2012). 2.3 Dependency Syntax and Machine Translation Many syntactic theories have been applied to translation modeling, but we focus in this article on dependency syntax (Tesni`ere 1959). Dependency syntax is a lightweight formalism that builds trees consisting of a set of directed arcs from words to their syntactic heads (also called “"
J14-2005,P09-1009,0,0.195419,"Missing"
J14-2005,W09-2303,0,0.0847503,"rt statistically significant improvements over a phrasebased baseline on five of seven test sets across four language pairs. We also present encouraging preliminary results on the use of unsupervised dependency parsing for syntax-based machine translation. 1. Introduction Building translation systems for many language pairs requires addressing a wide range of translation divergence phenomena. Several researchers have studied divergence between languages in corpora and found it to be considerable, even for closely related languages (Dorr 1994; Fox 2002; Wellington, Waxmonsky, and Melamed 2006; Søgaard and Kuhn 2009). To address this, many have incorporated linguistic syntax into translation model design. The statistical natural language processing (NLP) community has developed automatic parsers that can produce syntactic analyses for sentences in several languages (Klein and Manning 2003; Buchholz and Marsi 2006; Nivre et al. ∗ Toyota Technological Institute at Chicago, Chicago, IL 60637. E-mail: kgimpel@ttic.edu. ∗∗ School of Computer Science, Carnegie Mellon University, Pittsburgh, PA 15213. E-mail: nasmith@cs.cmu.edu. Submission received: 10 November 2012; revised submission received: 12 May 2013; acc"
J14-2005,D11-1118,0,0.054708,"2009) for German, the parser model from Bohnet (2010) adapted for German by Seeker and Kuhn (2012), and DuDuPlus (Chen et al. 2012) for Chinese. 382 Gimpel and Smith Phrase Dependency MT with Quasi-Synchronous Tree-to-Tree Features The most common approach to unsupervised parsing is to train models on sentences from treebanks (without using the annotated trees, of course) along with their gold standard POS tags. This practice must be changed if we wish to use unsupervised parsing for machine translation, because we do not have gold standard POS tags for our data. Fortunately, Smith (2006) and Spitkovsky et al. (2011) have shown that using automatic POS tags for dependency grammar induction can work as well as or better than gold standard POS tags. For syntax-based translation, Zollmann and Vogel (2011) showed that unsupervised tags could work as well as those from a supervised POS tagger. For Urdu and Malagasy, we use fully unsupervised POS tagging, using the approach from Berg-Kirkpatrick et al. (2010) with 40 tags. We use the “direct gradient” version optimized by L-BFGS (Liu and Nocedal 1989). For Chinese and English, we use the gold standard POS tags from their respective treebanks for training the pa"
J14-2005,N10-1116,0,0.0218812,"Missing"
J14-2005,N03-1033,0,0.0830797,"s for dependency grammar induction can work as well as or better than gold standard POS tags. For syntax-based translation, Zollmann and Vogel (2011) showed that unsupervised tags could work as well as those from a supervised POS tagger. For Urdu and Malagasy, we use fully unsupervised POS tagging, using the approach from Berg-Kirkpatrick et al. (2010) with 40 tags. We use the “direct gradient” version optimized by L-BFGS (Liu and Nocedal 1989). For Chinese and English, we use the gold standard POS tags from their respective treebanks for training the parser, then use the Stanford POS tagger (Toutanova et al. 2003) to tag the parallel data, tuning, and test sets. As our dependency parsing model, we use the dependency model with valence (Klein and Manning 2004) initialized with a convex initializer (Gimpel and Smith 2012a). The training procedure is described in Gimpel (2012). Our Chinese and English unsupervised parsers are roughly 30 percentage points worse than supervised parsers in dependency attachment accuracy on standard treebank test sets. We also compared the supervised and unsupervised parsers to a uniform-at-random parser. Well-known algorithms exist for sampling derivations under a context-fr"
J14-2005,D08-1065,0,0.0176142,"check reachability of the item endpoints and only proceed if one can reach the other. We modified the algorithm to output maximum lengths because we use the maximum lengths to compute the target-side vine grammar features and constraints, as mentioned in Section 5.2.2. In particular we use a feature ftgt that is a target-side vine analog to fsrc but using the Floyd-Warshall maximum path lengths in place of the actual vine lengths. 6.3.2 Lattice Pruning. To reduce phrase lattice sizes, we prune lattice edges using forward–backward pruning (Sixtus and Ortmanns 1999), which has also been used by Tromble et al. (2008). This pruning method computes the max-marginal for each lattice edge, which is the score of the best full path that uses that edge, then prunes edges whose max-marginal is below a certain fraction of the best path score in the lattice. Maxmarginals have been used for other coarse-to-fine learning frameworks (Weiss, Sapp, and Taskar 2010) and offer the advantage that the best path in the lattice is preserved during pruning. We only use the score contribution from the phrase-based features when computing these max-marginals. For each lattice, we use a grid search to find the most liberal thresh"
J14-2005,C10-1123,0,0.0171536,"en to parents. A child word is a modifier of its parent. Each word has exactly one parent and $ is a special “wall” symbol that serves as the parent of all root words in the tree (i.e., those with no other parent). 353 Computational Linguistics Volume 40, Number 2 But most who have used dependency syntax have done so either on the source side in tree-to-string systems (Lin 2004; Xiong, Liu, and Lin 2007; Xie, Mi, and Liu 2011) or the target side in string-to-tree systems (Shen, Xu, and Weischedel 2008; Carreras and Collins 2009; Galley and Manning 2009; Hunter and Resnik 2010; Su et al. 2010; Tu et al. 2010). Others have added features derived from source dependency parses to phrasebased or hierarchical phrase-based translation models (Gimpel and Smith 2008; Gao, Koehn, and Birch 2011). 2.4 Motivating Examples Although Fox (2002) found that dependencies are more often preserved across handaligned bitext than constituents, there are still several concerns when using dependency syntax for tree-to-tree translation. First, we only have hand-aligned sentence pairs for small data sets and few language pairs, so in practice we must deal with the noise in automatic word aligners and parsers. Second, not"
J14-2005,W02-1021,0,0.0258444,"Missing"
J14-2005,D07-1003,1,0.871521,"Missing"
J14-2005,P06-1123,0,0.0621487,"Missing"
J14-2005,D10-1050,0,0.0532535,"Missing"
J14-2005,D11-1038,0,0.0243739,"Missing"
J14-2005,J97-3002,0,0.262601,"ive integers as [k]. This notation is summarized in Table 1. 351 Computational Linguistics Volume 40, Number 2 Table 1 Notation used in this article. i, j, k, l x, y xi j xi [i] |x | integers vectors entry i in vector x sequence from entry i to entry j (inclusive) in vector x the set containing the first i positive integers length of vector x 2.2 Synchronous and Quasi-Synchronous Grammars To model syntactic transformations, researchers have developed powerful grammatical formalisms, many of which are variations of synchronous grammars. The most widely used is synchronous context-free grammar (Wu 1997; Gildea 2003; Chiang 2005; Melamed 2003), an extension of context-free grammar to a bilingual setting where two strings are generated simultaneously with a single derivation. Synchronous context-free grammars are computationally attractive but researchers have shown that they cannot handle certain phenomena in manually aligned parallel data (Wellington, Waxmonsky, and Melamed 2006; Søgaard and Kuhn 2009). Figure 1 shows two such examples of word alignment patterns in German–English data. These patterns were called “crossserial discontinuous translation units” (CDTUs) by Søgaard and Kuhn (2009"
J14-2005,D09-1159,0,0.053068,"Missing"
J14-2005,D11-1020,0,0.133979,"Missing"
J14-2005,W07-0706,0,0.487515,"Missing"
J14-2005,W03-3023,0,0.0447235,"servation across languages by using dependencies on phrases—flat multi-word units—rather than words. To motivate these choices, we now give two frequently occurring examples of dependency tree-to-tree divergence in German–English data.1 We consider the German– English parallel corpus used in our experiments (and described in Appendix A). We parsed the English side using TurboParser (Martins et al. 2010), a state-of-the-art dependency parser. TurboParser was trained on the Penn Treebank (Marcus, Santorini, and Marcinkiewicz 1993) converted to dependencies using the Yamada-Matsumoto head rules (Yamada and Matsumoto 2003). We parsed the German side using the factored model in the Stanford parser (Rafferty and Manning 2008), which is trained from the NEGRA phrase-structure treebank (Skut et al. 1997). The Stanford parser’s source code defines a set of head rules for converting the phrase-structure parse output to dependencies.2 The first example is shown in Figure 3. The bold words illustrate a “sibling” relationship, meaning that the source words aligned to the parent and child in the English sentence have the same parent on the German side. Many sibling configurations appear when the English dependency is DET"
J14-2005,P01-1067,0,0.747174,"ivre et al. ∗ Toyota Technological Institute at Chicago, Chicago, IL 60637. E-mail: kgimpel@ttic.edu. ∗∗ School of Computer Science, Carnegie Mellon University, Pittsburgh, PA 15213. E-mail: nasmith@cs.cmu.edu. Submission received: 10 November 2012; revised submission received: 12 May 2013; accepted for publication: 23 June 2013. doi:10.1162/COLI a 00175 © 2014 Association for Computational Linguistics Computational Linguistics Volume 40, Number 2 2007). The availability of these parsers, and gains in their accuracy, triggered research interest in syntax-based statistical machine translation (Yamada and Knight 2001). Syntax-based translation models are diverse, using different grammatical formalisms and features. Some use a parse tree for the source sentence (“tree-to-string”), others produce a parse when generating the target sentence (“string-to-tree”), and others combine both (“tree-to-tree”). We focus on the final category in this article. Tree-to-tree translation has proved to be a difficult modeling problem, as initial attempts at it underperformed systems that used no syntax at all (Cowan, Kuˇcerov´a, ¨ and Liu 2009). Subsequent reand Collins 2006; Ambati and Lavie 2008; Liu, Lu, search showed tha"
J14-2005,N01-1026,0,0.110426,"Missing"
J14-2005,H01-1035,0,0.020934,"Missing"
J14-2005,D11-1019,0,0.0392025,"Missing"
J14-2005,W06-3119,0,0.0418071,"it is not. As opposed to rules in hierarchical phrase-based models (Chiang 2005), which typically specify translationally equivalent substructures, this rule simply models the reordering and long-distance movement of the infinitive. Much prior work has found phrase pairs with gaps to be useful for machine translation (Simard et al. 2005; Crego and Yvon 2009; Galley and Manning 2010), and we extract tuples as in Equation (7) so that we can model such structures, even though we do not directly model gap-filling like hierarchical models and other models based on synchronous context-free grammar (Zollmann and Venugopal 2006, inter alia). 10 This non-overlapping constraint is what differentiates these tuples from the target-tree rule tuples from the previous section, which are extracted even when the source phrases overlap. 365 Computational Linguistics Volume 40, Number 2 (a) ich meine deshalb , dass es eine frage der geeigneten methodik ist . i think that it is consequently a question of the appropriate methodologies . dass es ... ist that it is (b) abschließend möchte ich herrn langen herzlich für seinen bericht danken ,... finally , mr president , i would like to thank mr langen warmly for his report ,... für"
J14-2005,C08-1144,0,0.175247,"e 4 Example of a sentence pair containing a frequently-observed “grandparent-grandchild” relationship in German–English data: the English parent and child words in the until←recently dependency are aligned to German words in a grandparent-grandchild relationship. 355 Computational Linguistics Volume 40, Number 2 On the other hand, models that use rules employing syntax (Yamada and Knight 2001) or syntax-like representations (Chiang 2005) handle long-distance reordering better than phrase-based systems (Birch, Blunsom, and Osborne 2009), and therefore perform better for certain language pairs (Zollmann et al. 2008). In order to better handle syntactic divergence and obtain the benefits of these two types of models, we use rules that combine phrases and syntax. In particular, our rules use dependencies between phrases rather than words; we call them phrase dependencies. When adding in source syntax, we eschew the constraints of synchronous grammar in favor of the feature-based approach of quasi-synchronous grammar. So we call our model a quasi-synchronous phrase dependency (QPD) translation model. In Section 3.1, we define phrase dependency trees and in Section 3.2 we present our model. We discuss rule e"
J14-2005,P11-1001,0,0.293962,"and Smith Phrase Dependency MT with Quasi-Synchronous Tree-to-Tree Features banks (e.g., Urdu and Malagasy), and for this we turn to unsupervised parsing. The NLP community has developed a range of statistical algorithms for building unsupervised parsers (Klein and Manning 2002, 2004; Smith 2006; Blunsom and Cohn 2010; Naseem et al. 2010; Spitkovsky, Alshawi, and Jurafsky 2010; Cohen 2011). They require only raw, unannotated text in the language of interest, making them ideal for use in translation. Unsupervised shallow syntactic analysis has been used successfully for translation modeling by Zollmann and Vogel (2011), who showed that unsupervised part-of-speech tags could be used to label the hierarchical translation rules of Chiang (2005) to match the performance of a system that uses supervised full syntactic parses. We take additional steps in this direction, leveraging state-of-the-art unsupervised models for full syntactic analysis (Klein and Manning 2004; Berg-Kirkpatrick et al. 2010; Gimpel and Smith 2012a) to obtain improvements in translation quality. We find that replacing a supervised parser for Chinese with an unsupervised one has no effect on performance, and using an unsupervised English par"
J14-2005,N10-1140,0,\N,Missing
J14-2005,N10-1083,0,\N,Missing
J14-2005,C10-2136,0,\N,Missing
J14-2005,N03-1031,0,\N,Missing
J14-2005,P09-1063,0,\N,Missing
J14-2005,D07-1096,0,\N,Missing
J14-2005,W90-0102,0,\N,Missing
J17-2002,Q16-1031,1,0.843163,"information in a joint model. Similar improvements may be achieved in an outof-domain scenario. Even though the parser is greedy, it provides very consistent results comparable with the best parsers of the state-of-the-art. We even obtained further improvement as demonstrated with the experiments with the dynamic oracles, that provide a push over the baseline while still maintaining very fast (and greedy) parsing speed. 339 Computational Linguistics Volume 43, Number 2 Finally, it is worth noting that a modified version of the same parser has been used to create a polyglot dependency parser (Ammar et al. 2016) with the universal dependency treebanks (Nivre et al. 2015), and the stack LSTMs have also been applied to solve other core natural language processing tasks, such as named entity recognition (Lample et al. 2016), phrase-structure parsing and language modeling (Dyer et al. 2016), and joint syntactic and semantic parsing (Swayamdipta et al. 2016). Acknowledgments We would like to thank Lingpeng Kong and Jacob Eisenstein for comments on an earlier version of this article and Danqi Chen for assistance with the parsing data sets. We would also like to thank Bernd Bohnet and Joakim Nivre for their"
J17-2002,P16-1231,0,0.190862,"w view” of the parser state when extracting features used to predict actions. Our work is complementary to previous approaches that develop alternative transition operations to simplify the modeling problem and enable better attachment decisions (Nivre 2007, 2008, 2009; Bohnet and Nivre 2012; Choi and McCallum 2013) and to feature engineering (Zhang and Nivre 2011; Ballesteros and Bohnet 2014; Chen, Zhang, and Zhang 2014; Ballesteros and Nivre 2016). Our model is related to recent work that uses neural networks in dependency parsing (Chen and Manning 2014; Weiss et al. 2015; Zhou et al. 2015; Andor et al. 2016). 2 The term “state” refers to the collection of previous decisions (sometimes called the history), resulting partial structures, which are stored in a stack data structure, and the words remaining to be processed. 3 These state equivalence assumptions are similar to the Markov assumptions made in modeling stochastic processes, according to which a potentially unbounded state history can be represented only in terms of a finite number of the most recent states. 312 Ballesteros et al. Greedy Transition-Based Dependency Parsing with Stack LSTMs That work can broadly be understood as replacing a"
J17-2002,P81-1022,0,0.756303,"Missing"
J17-2002,D12-1133,0,0.163485,"Missing"
J17-2002,Q13-1034,0,0.205338,"Missing"
J17-2002,W06-2920,0,0.0983953,"ddah et al. 2013; Seddah, Kubler, and Tsarfaty 2014): Arabic (Maamouri et al. 2004), Basque (Aduriz et al. 2003), French (Abeill´e, Cl´ement, and Toussenel 2003), German (Seeker and Kuhn 2012), Hebrew (Sima’an et al. 2001), Hungarian (Vincze ´ ´ ´ et al. 2010), Korean (Choi 2013), Polish (Swidzi nski and Wolinski 2010), and Swedish (Nivre, Nilsson, and Hall 2006). For all the corpora of the SPMRL Shared Task, we used predicted POS tags as provided by the shared task organizers.15 We also ran the experiment with the Turkish dependency treebank16 (Oflazer et al. 2003) of the CoNLLX Shared Task (Buchholz and Marsi 2006) and we use gold POS tags when used as it is common with the CoNLL-X data sets. In addition to these, we include the English and Chinese data sets described in Section 6.2.1. 6.2.3 Data for the Dynamic Oracle. Because the arc-hybrid transition-based parsing algorithm is limited to fully projective trees, we use the same data as in Section 6.2.1, which makes it comparable with the basic model that uses standard word representations and a static oracle arc-standard algorithm. 6.2.4 CoNLL-2009 Data. We also report results with all the CoNLL 2009 data sets (Hajiˇc et al. 2009) to make a complete c"
J17-2002,P05-1022,0,0.0472943,"al embeddings. Indeed, these configurations achieve high accuracies and shared class distributions early on in the training process. The parser is trained to optimize the log-likelihood of a correct action zg at each parsing state pt according to Equation (1). When using the dynamic oracle, a state pt may admit multiple correct actions zg = {zgi , . . . , zgk }. Our objective in such cases is that the set of correct actions receives high probability. We optimize for the log of p(zg |pt ), defined as: p(zg |pt ) = X zgi ∈zg p(zgi |pt ) (3) A similar approach was taken by Riezler et al. (2000), Charniak and Johnson (2005), and Goldberg (2013) in the context of log-linear probabilistic models. 6. Experiments After describing the implementation details of our optimization procedure (Section 6.1) and the data used in our experiments (Section 6.2), we turn to four sets of experiments: 1. First, we assess the quality of our greedy, global-state stack LSTM parsers on a wide range of data sets, showing that it is highly competitive with the state of the art (Section 6.3). 2. We compare the performance of the two different word representations (Section 4) across different languages, finding that they are quite benefic"
J17-2002,D14-1082,0,0.112043,"ork in transition-based parsing that considers only a “narrow view” of the parser state when extracting features used to predict actions. Our work is complementary to previous approaches that develop alternative transition operations to simplify the modeling problem and enable better attachment decisions (Nivre 2007, 2008, 2009; Bohnet and Nivre 2012; Choi and McCallum 2013) and to feature engineering (Zhang and Nivre 2011; Ballesteros and Bohnet 2014; Chen, Zhang, and Zhang 2014; Ballesteros and Nivre 2016). Our model is related to recent work that uses neural networks in dependency parsing (Chen and Manning 2014; Weiss et al. 2015; Zhou et al. 2015; Andor et al. 2016). 2 The term “state” refers to the collection of previous decisions (sometimes called the history), resulting partial structures, which are stored in a stack data structure, and the words remaining to be processed. 3 These state equivalence assumptions are similar to the Markov assumptions made in modeling stochastic processes, according to which a potentially unbounded state history can be represented only in terms of a finite number of the most recent states. 312 Ballesteros et al. Greedy Transition-Based Dependency Parsing with Stack"
J17-2002,C14-1078,0,0.146438,"Missing"
J17-2002,P13-1104,0,0.246028,"Missing"
J17-2002,P14-2111,0,0.126115,"Missing"
J17-2002,D07-1022,1,0.889561,"Missing"
J17-2002,W15-3904,0,0.0920647,"Missing"
J17-2002,P15-1033,1,0.882509,"therefore also investigate a training procedure that also aims to teach the parser to make good predictions in sub-optimal states, facilitated by the use of dynamic oracles (Goldberg and Nivre 2012). The experiments in this article demonstrate that by coupling stack LSTM-based global state representations with dynamic oracle training, parsing with greedy decoding can achieve state-of-the-art parsing accuracies, rivaling the accuracies of previous parsers that rely on exhaustive search procedures. 313 Computational Linguistics Volume 43, Number 2 This article is an extension of publications by Dyer et al. (2015) and Ballesteros et al. (2015). It contains a more detailed background about LSTMs and parsing, a discussion about the effect of random initialization, more extensive experiments with standard data sets, experiments including explicit morphological information that yields much higher results than the ones reported before, and a new training algorithm following dynamic oracles that yield state-of-the-art performance while maintaining a fast parsing speed with a greedy model. An open-source implementation of our parser, in all its different instantiations, is available from https://github.com/cl"
J17-2002,W13-5709,1,0.94387,"the training examples states that result from wrong parsing decisions, together with the optimal transitions to take in these states. To this end we reconsider which training examples to show, and what it means to behave optimally on these training examples. The dynamic oracles framework for training with exploration, suggested by Goldberg and Nivre (2012, 2013), provides answers to these questions. Although the application of dynamic oracle training is relatively straightforward, some adaptations were needed to accommodate the probabilistic training objective. These adaptations mostly follow Goldberg (2013). 5.2.1 Dynamic Oracles. A dynamic oracle is the component that, given a gold parse tree, provides the optimal set of possible actions to take under a given parser state. In contrast to static oracles that derive a canonical sequence for each gold parse tree and say nothing about parsing states that do not stem from this canonical path, the dynamic oracle is well-defined for states that result from parsing mistakes, and may produce more than a single gold action for a given state. Under the dynamic oracle framework, a parsing action is said to be optimal in a given state if the best tree that"
J17-2002,P11-2124,1,0.874063,"nt neural networks to normalize tweets. Similarly, Botha and Blunsom (2014) show that stems, prefixes, and suffixes can be used to learn useful word representations. Our approach is learning similar representations by using the bidirectional LSTMs. Chen et al. (2015) propose joint learning of character and word embeddings for Chinese, claiming that characters contain rich internal information. Constructing word representations from characters using convolutional neural networks has also been proposed (Kim et al. 2016). Tsarfaty (2006), Cohen and Smith (2007), Goldberg and Tsarfaty (2008), and Goldberg and Elhadad (2011) presented methods for joint segmentation and phrase-structure parsing, by segmenting the words in useful morphologically oriented units. Bohnet et al. (2013) presented an arc-standard transition-based parser that performs competitively for joint morphological tagging and dependency parsing for richly inflected languages, such as Czech, Finnish, German, Hungarian, and Russian. Training greedy parsers on non-gold outcomes, facilitated by dynamic oracles, has been explored by several researchers in different ways (Goldberg and Nivre 2012, 2013; Goldberg, Sartorio, and Satta 2014; Honnibal, Goldb"
J17-2002,C12-1059,1,0.936735,"Missing"
J17-2002,Q13-1033,1,0.949811,"eaking the linearity of the stack by removing tokens that are not at the top of the stack; however, this is easily handled with the stack LSTM. Figure 4 shows how the parser is capable of moving words from the stack (S) to the buffer (B), breaking the linear order of words. Because a node that is swapped may have already been assigned as the head of a dependent, the buffer (B) can now also contain tree fragments. 3.3.3 Arc-Hybrid. For the dynamic oracle training scenario, described in Section 5.2, we switch to the arc-hybrid transition system, which is amenable to an efficient dynamic oracle (Goldberg and Nivre 2013). The arc-hybrid system is summarized in Figure 5. The SHIFT and REDUCE - RIGHT transitions are the same as in arc-standard. However, the REDUCE - LEFT transition pops the top of the stack and attaches it as a child of the first item in the buffer. Although it is possible to extend the arc-hybrid system to support nonprojectivity by adding a SWAP transition, this extension would invalidate an important guarantee enabling efficient dynamic oracles.10 We therefore restrict the dynamic-oracle experiments to the fully projective English and Chinese treebanks. In order to parse nonprojective trees,"
J17-2002,Q14-1010,1,0.935088,"Missing"
J17-2002,P08-1043,1,0.846251,"014) tried character-based recurrent neural networks to normalize tweets. Similarly, Botha and Blunsom (2014) show that stems, prefixes, and suffixes can be used to learn useful word representations. Our approach is learning similar representations by using the bidirectional LSTMs. Chen et al. (2015) propose joint learning of character and word embeddings for Chinese, claiming that characters contain rich internal information. Constructing word representations from characters using convolutional neural networks has also been proposed (Kim et al. 2016). Tsarfaty (2006), Cohen and Smith (2007), Goldberg and Tsarfaty (2008), and Goldberg and Elhadad (2011) presented methods for joint segmentation and phrase-structure parsing, by segmenting the words in useful morphologically oriented units. Bohnet et al. (2013) presented an arc-standard transition-based parser that performs competitively for joint morphological tagging and dependency parsing for richly inflected languages, such as Czech, Finnish, German, Hungarian, and Russian. Training greedy parsers on non-gold outcomes, facilitated by dynamic oracles, has been explored by several researchers in different ways (Goldberg and Nivre 2012, 2013; Goldberg, Sartorio"
J17-2002,P13-2111,1,0.891593,"work uses continuous-valued relaxations of these. 3. Dependency Parser We now turn to the problem of learning representations of dependency parser states. We preserve the standard data structures of a transition-based dependency parser, namely, a buffer of words to be processed (B) and a stack (S) of partially constructed syntactic elements. Each stack element is augmented with a continuous-space vector embedding representing a word and, in the case of S, any of its syntactic dependents. Additionally, we introduce a third stack (A) to represent the history of transition actions taken by the 5 Goldberg et al. (2013) propose a similar stack construction to prevent stack operations from invalidating existing references to the stack in a beam-search parser that must (efficiently) maintain a priority queue of stacks. 316 } ) T L( IF DSH RE od am … pt | {z TO P {z S Greedy Transition-Based Dependency Parsing with Stack LSTMs B P TO | Ballesteros et al. } amod ; was made TO |{z } an decision overhasty P root ; REDUCE-LEFT(amod) A SHIFT … Figure 2 Parser state computation encountered while parsing the sentence an overhasty decision was made. Here S designates the stack of partially constructed dependency subtre"
J17-2002,D14-1099,0,0.164827,"Missing"
J17-2002,W09-1201,0,0.137744,"Missing"
J17-2002,N03-1014,0,0.736917,"ounded state history can be represented only in terms of a finite number of the most recent states. 312 Ballesteros et al. Greedy Transition-Based Dependency Parsing with Stack LSTMs That work can broadly be understood as replacing a conventional linear classifier with a neural network classifier but still only considering a “narrow view” of the parser state, whereas our work uses recursively defined networks to incorporate sensitivity to the complete parser state. Using recursive/recurrent neural networks to compute representations of unboundedly large histories has been proposed previously (Henderson 2003; Titov and Henderson 2007b; Stenetorp 2013; Yazdani and Henderson 2015). Our innovation is to make the architecture of the neural network identical with the structure of stack data structures used to store the parser state. To do so, the technical challenge we must solve is being able to access, in constant time, a fixed-size representation of a stack as it evolves during the course of parsing. The technical innovation that lets us do this is a variation of recurrent neural networks with long short-term memory units (LSTMs), which we call stack LSTMs (Section 2). These can be understood as LS"
J17-2002,P04-1013,0,0.567345,"Missing"
J17-2002,P13-1088,0,0.0439757,"and qz is a bias term for action z. The set A(S, B) represents the valid transition actions that may be taken given the current contents of the stack and buffer.9 Because pt = f (st , bt , at ) encodes information about all previous decisions made by the parser, the chain rule may be invoked to write the probability of any valid sequence of parse transitions z conditional on the input as: p(z |w) = |z| Y t=1 p(zt |pt ) (2) 3.2 Composition Functions Recursive neural network models enable complex phrases to be represented compositionally in terms of their parts and the relations that link them (Hermann and Blunsom 2013; Socher et al. 2011, 2013a, 2013b). We follow previous work in embedding dependency tree fragments that are present in the stack S in the same vector space as the token embeddings discussed in Section 4. A particular challenge here is that a syntactic head may, in general, have an arbitrary number of dependents. To simplify the parameterization of our composition function, we combine head-modifier pairs one at a time, building up more complicated structures 8 In preliminary experiments, we tried several nonlinearities and found ReLU to work slightly better than the others. 9 In general, A(S,"
J17-2002,W13-3518,1,0.937487,"Missing"
J17-2002,Q14-1011,0,0.100404,"Missing"
J17-2002,P11-1068,0,0.272942,"Missing"
J17-2002,N16-1030,1,0.677399,"Missing"
J17-2002,D14-1081,0,0.0383284,"crafted and sensitive to only certain properties of the state, with the exception of Titov and Henderson (2007b), whereas we are conditioning on the global state. Like us, Stenetorp (2013) used recursively composed representations of the tree fragments (a head and its dependents). Titov and Henderson (2007b) used a generative latent variable based on incremental sigmoid belief networks to likewise condition on the global state. Neural networks have also been used to learn representations for use in phrase-structure parsing (Henderson 2003, 2004; Titov and Henderson 2007a; Socher et al. 2013; Le and Zuidema 2014). The work by Watanabe et al. (2015) is also similar to the work presented in this article but for phrase-structure parsing. Our model’s position relative to existing neural network parsers can be understood by analogy to neural language models. The context representation is either constructed subject to an n-gram window (Bengio et al. 2003; Mnih and Hinton 2007), or it may be constructed incrementally using recurrent neural networks to obtain potentially unbounded dependencies (Mikolov et al. 2010). Likewise, during parsing, the particular algorithm state (represented by the stack, buffer, an"
J17-2002,N15-1142,1,0.846911,"and words that are OOV in both. To ensure we have estimates of the OOVs in the parsing training data, we stochastically replace (with probability 0.5) each singleton word type in the parsing training data with the UNK token in each training iteration. Pretrained Word Embeddings. There are several options for creating word embeddings, meaning numerous pretraining options for w ˜ LM are available. However, for syntax modeling problems, embedding approaches that discard order perform less well (Bansal, Gimpel, and Livescu 2014); therefore, we used a variant of the skip n-gram model introduced by Ling et al. (2015a), named “structured skip n-gram,” where a different set of parameters is used to predict each context word depending on its position relative to the target word. 4.2 Modeling Characters Instead of Words Following Ling et al. (2015b), we compute character-based continuous-space vector embeddings of words using bidirectional LSTMs (Graves and Schmidhuber 2005). When the parser initiates the learning process and populates the buffer with all the words from the sentence, it reads the words character by character from left to right and computes a continuous-space vector embedding the character se"
J17-2002,D15-1176,1,0.903245,"Missing"
J17-2002,de-marneffe-etal-2006-generating,0,0.292516,"Missing"
J17-2002,D13-1032,0,0.0288931,"Experiments with Static Oracle and Standard Word Representations We report results on five experimental configurations per language, as well as the Chen and Manning (2014) baseline. These are: the full stack LSTM parsing model (S-LSTM), the stack LSTM parsing model without POS tags (−POS), the stack LSTM parsing model without pretrained language model embeddings (−pretraining), the 13 Training: 02-21. Development: 22. Test: 23. 14 Training: 001–815, 1001–1136. Development: 886–931, 1148–1151. Test: 816–885, 1137–1147. ¨ 15 The POS tags were calculated with MarMot tagger (Mueller, Schmid, and Schutze 2013) by the best ¨ performing system of the SPMRL Shared Task (Bjorkelund et al. 2013). Arabic: 97.38. Basque: 97.02. French: 97.61. German: 98.10. Hebrew: 97.09. Hungarian: 98.72. Korean: 94.03. Polish: 98.12. Swedish: 97.27. 16 Because the Turkish dependency treebank does not have a development set, we extracted the last 150 sentences from the 4,996 sentences of the training set as a development set. 328 Ballesteros et al. Greedy Transition-Based Dependency Parsing with Stack LSTMs Table 1 Unlabeled attachment scores and labeled attachment scores on the development sets (top) and the final test"
J17-2002,W03-3017,0,0.216793,"ril 2016; accepted for publication: 14 June 2016. doi:10.1162/COLI a 00285 © 2017 Association for Computational Linguistics Published under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0) license Computational Linguistics Volume 43, Number 2 1. Introduction Natural language parsing can be formulated as a series of decisions that read words in sequence and incrementally combine them to form syntactic structures; this formulation is known as transition-based parsing, and is often coupled with a greedy inference procedure (Yamada and Matsumoto 2003; Nivre 2003, 2004, 2008). Greedy transitionbased parsing is attractive because the number of operations required to build any projective parse tree is linear in the length of the sentence, making greedy versions of transition-based parsing computationally efficient relative to graph- and grammarbased alternative formalisms, which usually require solving superlinear search problems. The challenge in transition-based parsing is modeling which action should be taken in each of the states encountered as the parsing algorithm progresses.2 Because the parser state involves the complete sentence—which is unboun"
J17-2002,W04-0308,0,0.395191,"embeddings of the head, dependent, and relation and applying a linear operator and a component-wise nonlinearity as follows: c = tanh (U[h; m; r] + e) c, e, h, m ∈ Rdin r ∈ Rdrel U ∈ Rdin ×(2din +drel ) For the relation vector, we use an embedding of the parser action that was applied to construct the relation (i.e., the syntactic relation paired with the direction of attachment); U and e are additional parameters learned alongside those of the stack LSTMs. 3.3 Transition Systems Our parser implements various transition systems. The original version (Dyer et al. 2015) implements arc-standard (Nivre 2004) in its most basic set-up. In Ballesteros et al. (2015), we augmented the arc-standard system with the SWAP transition of Nivre (2009), allowing for nonprojective parsing. For the dynamic oracle training strategy, ´ we moved to an arc-hybrid system (Kuhlmann, Gomez-Rodr´ ıguez, and Satta 2011) for which an efficient dynamic oracle is available. It is worth noting that the stack LSTM parameterization is mostly orthogonal to the transition system being used (providing that the system can be expressed as operating on stacks), making it easy to substitute transition systems. 3.3.1 Arc-Standard. Th"
J17-2002,J08-4003,0,0.37078,"4. A particular challenge here is that a syntactic head may, in general, have an arbitrary number of dependents. To simplify the parameterization of our composition function, we combine head-modifier pairs one at a time, building up more complicated structures 8 In preliminary experiments, we tried several nonlinearities and found ReLU to work slightly better than the others. 9 In general, A(S, B) is the complete set of parser actions discussed in Section 3.3, but in some cases not all actions are available. For example, when S is empty and words remain in B, a SHIFT transition is obligatory (Nivre 2008). 318 Ballesteros et al. det Greedy Transition-Based Dependency Parsing with Stack LSTMs amod an overhasty mod an decision cc2 rel det head mod overhasty c1 head rel amod decision Figure 3 The representation of a dependency subtree (top) is computed by recursively applying composition functions to hhead, modifier, relationi triples. In the case of multiple dependents of a single head, the recursive branching order is imposed by the order of the parser’s reduce transition (bottom). in the order they are “reduced” in the parser, as illustrated in Figure 3. Each node in this expanded syntactic tr"
J17-2002,P09-1040,0,0.801651,"(U[h; m; r] + e) c, e, h, m ∈ Rdin r ∈ Rdrel U ∈ Rdin ×(2din +drel ) For the relation vector, we use an embedding of the parser action that was applied to construct the relation (i.e., the syntactic relation paired with the direction of attachment); U and e are additional parameters learned alongside those of the stack LSTMs. 3.3 Transition Systems Our parser implements various transition systems. The original version (Dyer et al. 2015) implements arc-standard (Nivre 2004) in its most basic set-up. In Ballesteros et al. (2015), we augmented the arc-standard system with the SWAP transition of Nivre (2009), allowing for nonprojective parsing. For the dynamic oracle training strategy, ´ we moved to an arc-hybrid system (Kuhlmann, Gomez-Rodr´ ıguez, and Satta 2011) for which an efficient dynamic oracle is available. It is worth noting that the stack LSTM parameterization is mostly orthogonal to the transition system being used (providing that the system can be expressed as operating on stacks), making it easy to substitute transition systems. 3.3.1 Arc-Standard. The arc-standard transition inventory (Nivre 2004) is given in Figure 4. The SHIFT transition moves a word from the buffer to the stack,"
J17-2002,W06-2933,0,0.104845,"Missing"
J17-2002,P05-1013,0,0.662475,"B B B (v, v), B Dependency — r u→v r u←v — Figure 4 Parser transitions of the arc-standard system (with swap, Section 3.3.2) indicating the action applied to the stack and buffer and the resulting stack and buffer states. Bold symbols indicate (learned) embeddings of words and relations, script symbols indicate the corresponding words and relations. (gr (x, y), x) refers to the composition function presented in 3.2. the stack. The arc-standard system allows building all and only projective trees. In order to parse nonprojective trees, this can be combined with the pseudo-projective approach (Nivre and Nilsson 2005) or follow what is presented in Section 3.3.2. 3.3.2 Arc-Standard with Swap. In order to deal with nonprojectivity, the arc-standard system can be augmented with a SWAP transition (Nivre 2009). The SWAP transition removes the second-to-top item from the stack and pushes it back to the buffer, allowing for the creation of nonprojective trees. We only use this transition when the training data set contains nonprojective trees. The inclusion of the SWAP transition requires breaking the linearity of the stack by removing tokens that are not at the top of the stack; however, this is easily handled"
J17-2002,nivre-etal-2006-talbanken05,0,0.111357,"82.15 79.04 92.57 90.21 Words + POS UAS LAS 86.85 85.36 93.04 90.87 Words + Chars UAS LAS 81.90 78.81 92.56 90.38 Words + Chars + POS UAS LAS 86.92 85.49 92.75 90.62 Test: Language Chinese English 6.4.3 Comparison with State-of-the-Art Parsers. Table 5 shows a comparison with state-ofthe-art parsers. We include greedy transition-based parsers that, like ours, do not apply beam search. For all the SPMRL languages we show the results of Ballesteros (2013), who reported results after carrying out a careful automatic morphological feature selection experiment. For Turkish, we show the results of Nivre et al. (2006), who also carried out a careful manual morphological feature selection. Our parser outperforms these in most cases. For English and Chinese, we report our results of the best parser on the develpment set in Section 6.3—which is Words + POS but with pretrained word embeddings. We also show the best reported results on these data sets. For the SPMRL data ¨ sets, the best performing system of the shared task is either Bjoreklund et al. (2013) or ¨ Bjorkelund et al. (2014), which are better than our system. Note that the comparison is harsh to our system, which does not use unlabeled data nor any"
J17-2002,J17-2002,1,0.0512826,"Missing"
J17-2002,P00-1061,0,0.0437166,"that make use of external embeddings. Indeed, these configurations achieve high accuracies and shared class distributions early on in the training process. The parser is trained to optimize the log-likelihood of a correct action zg at each parsing state pt according to Equation (1). When using the dynamic oracle, a state pt may admit multiple correct actions zg = {zgi , . . . , zgk }. Our objective in such cases is that the set of correct actions receives high probability. We optimize for the log of p(zg |pt ), defined as: p(zg |pt ) = X zgi ∈zg p(zgi |pt ) (3) A similar approach was taken by Riezler et al. (2000), Charniak and Johnson (2005), and Goldberg (2013) in the context of log-linear probabilistic models. 6. Experiments After describing the implementation details of our optimization procedure (Section 6.1) and the data used in our experiments (Section 6.2), we turn to four sets of experiments: 1. First, we assess the quality of our greedy, global-state stack LSTM parsers on a wide range of data sets, showing that it is highly competitive with the state of the art (Section 6.3). 2. We compare the performance of the two different word representations (Section 4) across different languages, findin"
J17-2002,W14-6111,0,0.0979038,"Missing"
J17-2002,W13-4917,1,0.910835,"Missing"
J17-2002,seeker-kuhn-2012-making,0,0.0639458,"uage model word embeddings were generated from the AFE portion of the English Gigaword corpus (version 5), and from the complete Chinese Gigaword corpus (version 2), as segmented by the Stanford Chinese Segmenter (Tseng et al. 2005). 6.2.2 Data to Test the Character-Based Representations and Static Oracle for Training. For the character-based representations we applied our model to the treebanks of the SPMRL ¨ Shared Task (Seddah et al. 2013; Seddah, Kubler, and Tsarfaty 2014): Arabic (Maamouri et al. 2004), Basque (Aduriz et al. 2003), French (Abeill´e, Cl´ement, and Toussenel 2003), German (Seeker and Kuhn 2012), Hebrew (Sima’an et al. 2001), Hungarian (Vincze ´ ´ ´ et al. 2010), Korean (Choi 2013), Polish (Swidzi nski and Wolinski 2010), and Swedish (Nivre, Nilsson, and Hall 2006). For all the corpora of the SPMRL Shared Task, we used predicted POS tags as provided by the shared task organizers.15 We also ran the experiment with the Turkish dependency treebank16 (Oflazer et al. 2003) of the CoNLLX Shared Task (Buchholz and Marsi 2006) and we use gold POS tags when used as it is common with the CoNLL-X data sets. In addition to these, we include the English and Chinese data sets described in Section"
J17-2002,P13-1045,0,0.0704272,"3, Number 2 manually crafted and sensitive to only certain properties of the state, with the exception of Titov and Henderson (2007b), whereas we are conditioning on the global state. Like us, Stenetorp (2013) used recursively composed representations of the tree fragments (a head and its dependents). Titov and Henderson (2007b) used a generative latent variable based on incremental sigmoid belief networks to likewise condition on the global state. Neural networks have also been used to learn representations for use in phrase-structure parsing (Henderson 2003, 2004; Titov and Henderson 2007a; Socher et al. 2013; Le and Zuidema 2014). The work by Watanabe et al. (2015) is also similar to the work presented in this article but for phrase-structure parsing. Our model’s position relative to existing neural network parsers can be understood by analogy to neural language models. The context representation is either constructed subject to an n-gram window (Bengio et al. 2003; Mnih and Hinton 2007), or it may be constructed incrementally using recurrent neural networks to obtain potentially unbounded dependencies (Mikolov et al. 2010). Likewise, during parsing, the particular algorithm state (represented by"
J17-2002,D13-1170,0,0.00710017,"Missing"
J17-2002,K16-1019,1,0.850904,"sh over the baseline while still maintaining very fast (and greedy) parsing speed. 339 Computational Linguistics Volume 43, Number 2 Finally, it is worth noting that a modified version of the same parser has been used to create a polyglot dependency parser (Ammar et al. 2016) with the universal dependency treebanks (Nivre et al. 2015), and the stack LSTMs have also been applied to solve other core natural language processing tasks, such as named entity recognition (Lample et al. 2016), phrase-structure parsing and language modeling (Dyer et al. 2016), and joint syntactic and semantic parsing (Swayamdipta et al. 2016). Acknowledgments We would like to thank Lingpeng Kong and Jacob Eisenstein for comments on an earlier version of this article and Danqi Chen for assistance with the parsing data sets. We would also like to thank Bernd Bohnet and Joakim Nivre for their help with the parsing algorithms. This work was sponsored in part by the U.S. Army Research Laboratory and the U.S. Army Research Office under contract/grant number W911NF-10-1-0533, and in part by NSF CAREER grant IIS-1054319. Miguel Ballesteros is supported by the European Commission under the contract numbers FP7-ICT-610411 (project MULTISENS"
J17-2002,P15-1150,0,0.158426,"Missing"
J17-2002,P07-1080,0,0.609122,"tory can be represented only in terms of a finite number of the most recent states. 312 Ballesteros et al. Greedy Transition-Based Dependency Parsing with Stack LSTMs That work can broadly be understood as replacing a conventional linear classifier with a neural network classifier but still only considering a “narrow view” of the parser state, whereas our work uses recursively defined networks to incorporate sensitivity to the complete parser state. Using recursive/recurrent neural networks to compute representations of unboundedly large histories has been proposed previously (Henderson 2003; Titov and Henderson 2007b; Stenetorp 2013; Yazdani and Henderson 2015). Our innovation is to make the architecture of the neural network identical with the structure of stack data structures used to store the parser state. To do so, the technical challenge we must solve is being able to access, in constant time, a fixed-size representation of a stack as it evolves during the course of parsing. The technical innovation that lets us do this is a variation of recurrent neural networks with long short-term memory units (LSTMs), which we call stack LSTMs (Section 2). These can be understood as LSTMs augmented with a stack"
J17-2002,W07-2218,0,0.744034,"tory can be represented only in terms of a finite number of the most recent states. 312 Ballesteros et al. Greedy Transition-Based Dependency Parsing with Stack LSTMs That work can broadly be understood as replacing a conventional linear classifier with a neural network classifier but still only considering a “narrow view” of the parser state, whereas our work uses recursively defined networks to incorporate sensitivity to the complete parser state. Using recursive/recurrent neural networks to compute representations of unboundedly large histories has been proposed previously (Henderson 2003; Titov and Henderson 2007b; Stenetorp 2013; Yazdani and Henderson 2015). Our innovation is to make the architecture of the neural network identical with the structure of stack data structures used to store the parser state. To do so, the technical challenge we must solve is being able to access, in constant time, a fixed-size representation of a stack as it evolves during the course of parsing. The technical innovation that lets us do this is a variation of recurrent neural networks with long short-term memory units (LSTMs), which we call stack LSTMs (Section 2). These can be understood as LSTMs augmented with a stack"
J17-2002,P15-3004,0,0.0618368,"so investigate a training procedure that also aims to teach the parser to make good predictions in sub-optimal states, facilitated by the use of dynamic oracles (Goldberg and Nivre 2012). The experiments in this article demonstrate that by coupling stack LSTM-based global state representations with dynamic oracle training, parsing with greedy decoding can achieve state-of-the-art parsing accuracies, rivaling the accuracies of previous parsers that rely on exhaustive search procedures. 313 Computational Linguistics Volume 43, Number 2 This article is an extension of publications by Dyer et al. (2015) and Ballesteros et al. (2015). It contains a more detailed background about LSTMs and parsing, a discussion about the effect of random initialization, more extensive experiments with standard data sets, experiments including explicit morphological information that yields much higher results than the ones reported before, and a new training algorithm following dynamic oracles that yield state-of-the-art performance while maintaining a fast parsing speed with a greedy model. An open-source implementation of our parser, in all its different instantiations, is available from https://github.com/cl"
J17-2002,N03-1033,0,0.365748,"Missing"
J17-2002,P06-3009,0,0.158184,"xisting models. For instance, Chrupala (2014) tried character-based recurrent neural networks to normalize tweets. Similarly, Botha and Blunsom (2014) show that stems, prefixes, and suffixes can be used to learn useful word representations. Our approach is learning similar representations by using the bidirectional LSTMs. Chen et al. (2015) propose joint learning of character and word embeddings for Chinese, claiming that characters contain rich internal information. Constructing word representations from characters using convolutional neural networks has also been proposed (Kim et al. 2016). Tsarfaty (2006), Cohen and Smith (2007), Goldberg and Tsarfaty (2008), and Goldberg and Elhadad (2011) presented methods for joint segmentation and phrase-structure parsing, by segmenting the words in useful morphologically oriented units. Bohnet et al. (2013) presented an arc-standard transition-based parser that performs competitively for joint morphological tagging and dependency parsing for richly inflected languages, such as Czech, Finnish, German, Hungarian, and Russian. Training greedy parsers on non-gold outcomes, facilitated by dynamic oracles, has been explored by several researchers in different w"
J17-2002,I05-3027,0,0.0858346,"Missing"
J17-2002,Q16-1014,0,0.139104,"Missing"
J17-2002,P15-1113,0,0.105221,"Missing"
J17-2002,P15-1032,0,0.317311,"parsing that considers only a “narrow view” of the parser state when extracting features used to predict actions. Our work is complementary to previous approaches that develop alternative transition operations to simplify the modeling problem and enable better attachment decisions (Nivre 2007, 2008, 2009; Bohnet and Nivre 2012; Choi and McCallum 2013) and to feature engineering (Zhang and Nivre 2011; Ballesteros and Bohnet 2014; Chen, Zhang, and Zhang 2014; Ballesteros and Nivre 2016). Our model is related to recent work that uses neural networks in dependency parsing (Chen and Manning 2014; Weiss et al. 2015; Zhou et al. 2015; Andor et al. 2016). 2 The term “state” refers to the collection of previous decisions (sometimes called the history), resulting partial structures, which are stored in a stack data structure, and the words remaining to be processed. 3 These state equivalence assumptions are similar to the Markov assumptions made in modeling stochastic processes, according to which a potentially unbounded state history can be represented only in terms of a finite number of the most recent states. 312 Ballesteros et al. Greedy Transition-Based Dependency Parsing with Stack LSTMs That work can"
J17-2002,W03-3023,0,0.341093,"ised version received: 6 April 2016; accepted for publication: 14 June 2016. doi:10.1162/COLI a 00285 © 2017 Association for Computational Linguistics Published under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0) license Computational Linguistics Volume 43, Number 2 1. Introduction Natural language parsing can be formulated as a series of decisions that read words in sequence and incrementally combine them to form syntactic structures; this formulation is known as transition-based parsing, and is often coupled with a greedy inference procedure (Yamada and Matsumoto 2003; Nivre 2003, 2004, 2008). Greedy transitionbased parsing is attractive because the number of operations required to build any projective parse tree is linear in the length of the sentence, making greedy versions of transition-based parsing computationally efficient relative to graph- and grammarbased alternative formalisms, which usually require solving superlinear search problems. The challenge in transition-based parsing is modeling which action should be taken in each of the states encountered as the parsing algorithm progresses.2 Because the parser state involves the complete sentence—whi"
J17-2002,K15-1015,0,0.465378,"finite number of the most recent states. 312 Ballesteros et al. Greedy Transition-Based Dependency Parsing with Stack LSTMs That work can broadly be understood as replacing a conventional linear classifier with a neural network classifier but still only considering a “narrow view” of the parser state, whereas our work uses recursively defined networks to incorporate sensitivity to the complete parser state. Using recursive/recurrent neural networks to compute representations of unboundedly large histories has been proposed previously (Henderson 2003; Titov and Henderson 2007b; Stenetorp 2013; Yazdani and Henderson 2015). Our innovation is to make the architecture of the neural network identical with the structure of stack data structures used to store the parser state. To do so, the technical challenge we must solve is being able to access, in constant time, a fixed-size representation of a stack as it evolves during the course of parsing. The technical innovation that lets us do this is a variation of recurrent neural networks with long short-term memory units (LSTMs), which we call stack LSTMs (Section 2). These can be understood as LSTMs augmented with a stack pointer that is manipulated by push and pop o"
J17-2002,D15-1251,1,0.901456,"Missing"
J17-2002,P16-1147,0,0.0322281,"rd embeddings are useful for other languages we also include results with pretrained word embeddings for English, Chinese, German, and Spanish following the same training set-up as in Section 4; for English and Chinese we used the same pretrained word embeddings as in previous experiments, for German we pretrained embeddings using the monolingual training data from the WMT 2015 data set22 , and for Spanish we used the Spanish Gigaword version 3. The results for the parser with character-based representations on these data sets (last line of the table) were published by Andor et al. (2016). In Zhang and Weiss (2016), it is also possible to find results of the same version of the parser on the Universal Dependency treebanks (Nivre et al. 2015). 21 We report the performance of these parsers in the most comparable set-up, that is, with beam = 1 or greedy. 22 http://www.statmt.org/wmt15/translation-task.html. 336 Ballesteros et al. Greedy Transition-Based Dependency Parsing with Stack LSTMs Table 7 Results of the parser in its different versions including comparison with other systems. St. refers to static oracle with the arc-standard parser and Dyn. refers to dynamic oracle with the arc-hybrid parser with α"
J17-2002,D08-1059,0,0.177172,"Missing"
J17-2002,P11-2033,0,0.0417267,"tions on this sequence, and r the complete contents of the stack of partially constructed syntactic structures. This global sensitivity of the state representation contrasts with most previous work in transition-based parsing that considers only a “narrow view” of the parser state when extracting features used to predict actions. Our work is complementary to previous approaches that develop alternative transition operations to simplify the modeling problem and enable better attachment decisions (Nivre 2007, 2008, 2009; Bohnet and Nivre 2012; Choi and McCallum 2013) and to feature engineering (Zhang and Nivre 2011; Ballesteros and Bohnet 2014; Chen, Zhang, and Zhang 2014; Ballesteros and Nivre 2016). Our model is related to recent work that uses neural networks in dependency parsing (Chen and Manning 2014; Weiss et al. 2015; Zhou et al. 2015; Andor et al. 2016). 2 The term “state” refers to the collection of previous decisions (sometimes called the history), resulting partial structures, which are stored in a stack data structure, and the words remaining to be processed. 3 These state equivalence assumptions are similar to the Markov assumptions made in modeling stochastic processes, according to which"
J17-2002,P15-1117,0,0.148064,"ders only a “narrow view” of the parser state when extracting features used to predict actions. Our work is complementary to previous approaches that develop alternative transition operations to simplify the modeling problem and enable better attachment decisions (Nivre 2007, 2008, 2009; Bohnet and Nivre 2012; Choi and McCallum 2013) and to feature engineering (Zhang and Nivre 2011; Ballesteros and Bohnet 2014; Chen, Zhang, and Zhang 2014; Ballesteros and Nivre 2016). Our model is related to recent work that uses neural networks in dependency parsing (Chen and Manning 2014; Weiss et al. 2015; Zhou et al. 2015; Andor et al. 2016). 2 The term “state” refers to the collection of previous decisions (sometimes called the history), resulting partial structures, which are stored in a stack data structure, and the words remaining to be processed. 3 These state equivalence assumptions are similar to the Markov assumptions made in modeling stochastic processes, according to which a potentially unbounded state history can be represented only in terms of a finite number of the most recent states. 312 Ballesteros et al. Greedy Transition-Based Dependency Parsing with Stack LSTMs That work can broadly be unders"
J17-2002,N07-1050,0,\N,Missing
J17-2002,D15-1041,1,\N,Missing
J17-2002,W13-4916,0,\N,Missing
J17-2002,J13-1002,1,\N,Missing
J17-2002,P14-6005,0,\N,Missing
J17-2002,W14-6110,0,\N,Missing
J17-2002,Q14-1017,0,\N,Missing
J17-2002,W13-4907,1,\N,Missing
J17-2002,P14-2131,0,\N,Missing
J17-2002,C14-1076,1,\N,Missing
J17-2002,W15-2210,0,\N,Missing
J17-2002,P15-2042,0,\N,Missing
K15-1003,J07-3004,0,0.0232518,") 0 A(y |y) = min 1, p(y |θ LCTX , θ RCTX ) For completeness, we note that the probability of a tree y given only the context parameters is:5 p(y |θ LCTX , θ RCTX ) = Y θ LCTX (yi−1,i |yij ) · θ RCTX (yj,j+1 |yij ) p(wi |yi,i+1 = t) = λt (T) · θtTERM (wi ) P + t→u λt (U) · θtUN (hui) 0≤i&lt;j≤n 5 Note that there may actually be multiple yij due to unary rules that “loop back” to the same position (i, j); all of these much be included in the product. · p(wi:j−1 |yij = u) 27 best-performing model of our previous work (Garrette et al., 2015), which SCM extends. We evaluated on the English CCGBank (Hockenmaier and Steedman, 2007), which is a transformation of the Penn Treebank (Marcus et al., 1993); the CTBCCG (Tse and Curran, 2010) transformation of the Penn Chinese Treebank (Xue et al., 2005); and the CCG-TUT corpus (Bos et al., 2009), built from the TUT corpus of Italian text (Bosco et al., 2000). Each corpus was divided into four distinct data sets: a set from which we extract the tag dictionaries, a set of raw (unannotated) sentences, a development set, and a test set. We use the same splits as Garrette et al. (2014). Since these treebanks use special representations for conjunctions, we chose to rewrite the tree"
K15-1003,C08-1008,1,0.86999,"equence ADJ NOUN frequently occurs between the tags DET and VERB. This DET—VERB context also frequently applies to the single-word sequence NOUN and to ADJ ADJ NOUN . From this, we might deduce that DET — VERB is a likely context for a noun phrase. CCM is able to learn which POS contexts are likely, and does so via a probabilistic generative model, providing a statistical, data-driven take on substitutability. However, since there is nothing intrinsic about the POS pair DET—VERB that indicates a priori that it is a likely constituent context, this fact must be inferred entirely from the data. Baldridge (2008) observed that unlike opaque, atomic POS labels, the rich structures of Combinatory Categorial Grammar (CCG) (Steedman, 2000; Steedman and Baldridge, 2011) categories reflect universal grammatical properties. CCG is a lexicalized grammar formalism in which every constituent in a sentence is associated with a structured category that specifies its syntactic relationship to other constituents. For example, a category might encode that “this constituent can combine with a noun phrase to the right (an object) and then a noun phrase to the left (a subject) to produce a sentence” instead of simply V"
K15-1003,N07-1018,0,0.284036,"design of models and learning procedures that result in better parsing tools. Given our desire to train NLP models in low-supervision scenarios, the possibility of constructing inductive biases out of universal properties of language is enticing: if we can do this well, then it only needs to be done once, and can be applied to any language or domain without adaptation. In this paper, we seek to learn from only raw data and an incomplete dictionary mapping some words to sets of potential supertags. In order to estimate the parameters of our model, we develop a blocked sampler based on that of Johnson et al. (2007) to sample parse trees for sentences in the raw training corpus according to their posterior probabilities. However, due to the very large sets of potential supertags used in a parse, computing inside charts is intractable, so we design a Metropolis-Hastings step that allows us to sample efficiently from the correct posterior. Our experiments show that the incorporation of supertag context parameters into the model improves learning, and that placing combinability-preferring priors on those parameters yields further gains in many scenarios. pler categories by encoding a notion of category simp"
K15-1003,P02-1017,0,0.256356,"it the properties of CCG. Our experiments show that our model outperforms a baseline in which this context information is not captured. 1 Introduction Learning parsers from incomplete or indirect supervision is an important component of moving NLP research toward new domains and languages. But with less information, it becomes necessary to devise ways of making better use of the information that is available. In general, this means constructing inductive biases that take advantage of unannotated data to train probabilistic models. One important example is the constituentcontext model (CCM) of Klein and Manning (2002), which was specifically designed to capture the linguistic observation made by Radford (1988) that there are regularities to the contexts in which constituents appear. This phenomenon, known as substitutability, says that phrases of the same type appear in similar contexts. For example, 22 Proceedings of the 19th Conference on Computational Language Learning, pages 22–31, c Beijing, China, July 30-31, 2015. 2015 Association for Computational Linguistics able data in the context of studying child language acquisition (e.g., Villavicencio, 2002; Goldwater, 2007), we are interested in applying t"
K15-1003,Q13-1007,0,0.0395254,"Missing"
K15-1003,D14-1107,0,0.0241254,"he interaction between universal grammar and observ23 n s s
p np np/n The pp n man (s
p)/pp pp/np walks to np work n/n n s
p The lazy dog sleeps Figure 2: Higher-level category n subsumes the categories of its constituents. Thus, n should have a strong prior on combinability with its adjacent supertags np/n and s
p. Figure 1: CCG parse for “The man walks to work.” Parameters: θ ROOT ∼ Dir(αROOT , θ ROOT-0 ) θtBIN ∼ Dir(αBIN , θ BIN -0 ) θtUN ∼ Dir(αUN , θ UN -0 ) θtTERM ∼ Dir(αTERM , θtTERM -0 ) λt ∼ Dir(αλ , λ0 ) θtLCTX ∼ Dir(αLCTX , θtLCTX -0 ) θtRCTX ∼ Dir(αRCTX , θtRCTX -0 ) We follow Lewis and Steedman (2014) in allowing a small set of generic, linguistically-plausible unary and binary grammar rules. We further add rules for combining with punctuation to the left and right and allow for the merge rule X → X X of Clark and Curran (2007). 3 np/n Generative Model ∀t ∈ T ∀t ∈ T ∀t ∈ T ∀t ∈ T ∀t ∈ T ∀t ∈ T Sentence: In this section, we present our novel supertagcontext model (SCM) that augments a standard PCFG with parameters governing the supertags to the left and right of each constituent. do s ∼ Cat(θ ROOT ) y |s ∼ SCM(s) until the tree y is valid The CCG formalism is said to be naturally associativ"
K15-1003,I11-1049,0,0.0375395,"Missing"
K15-1003,J93-2004,0,0.0511752,"robability of a tree y given only the context parameters is:5 p(y |θ LCTX , θ RCTX ) = Y θ LCTX (yi−1,i |yij ) · θ RCTX (yj,j+1 |yij ) p(wi |yi,i+1 = t) = λt (T) · θtTERM (wi ) P + t→u λt (U) · θtUN (hui) 0≤i&lt;j≤n 5 Note that there may actually be multiple yij due to unary rules that “loop back” to the same position (i, j); all of these much be included in the product. · p(wi:j−1 |yij = u) 27 best-performing model of our previous work (Garrette et al., 2015), which SCM extends. We evaluated on the English CCGBank (Hockenmaier and Steedman, 2007), which is a transformation of the Penn Treebank (Marcus et al., 1993); the CTBCCG (Tse and Curran, 2010) transformation of the Penn Chinese Treebank (Xue et al., 2005); and the CCG-TUT corpus (Bos et al., 2009), built from the TUT corpus of Italian text (Bosco et al., 2000). Each corpus was divided into four distinct data sets: a set from which we extract the tag dictionaries, a set of raw (unannotated) sentences, a development set, and a test set. We use the same splits as Garrette et al. (2014). Since these treebanks use special representations for conjunctions, we chose to rewrite the trees to use conjunction categories of the form (XX)/X rather than introd"
K15-1003,D10-1120,0,0.0495496,"Missing"
K15-1003,bosco-etal-2000-building,0,0.0377197,"≤i&lt;j≤n 5 Note that there may actually be multiple yij due to unary rules that “loop back” to the same position (i, j); all of these much be included in the product. · p(wi:j−1 |yij = u) 27 best-performing model of our previous work (Garrette et al., 2015), which SCM extends. We evaluated on the English CCGBank (Hockenmaier and Steedman, 2007), which is a transformation of the Penn Treebank (Marcus et al., 1993); the CTBCCG (Tse and Curran, 2010) transformation of the Penn Chinese Treebank (Xue et al., 2005); and the CCG-TUT corpus (Bos et al., 2009), built from the TUT corpus of Italian text (Bosco et al., 2000). Each corpus was divided into four distinct data sets: a set from which we extract the tag dictionaries, a set of raw (unannotated) sentences, a development set, and a test set. We use the same splits as Garrette et al. (2014). Since these treebanks use special representations for conjunctions, we chose to rewrite the trees to use conjunction categories of the form (XX)/X rather than introducing special conjunction rules. In order to increase the amount of raw data available to the sampler, we supplemented the English data with raw, unannotated newswire sentences from the NYT Gigaword 5 corp"
K15-1003,J99-1004,0,0.0388977,"adverb that modifies a verb); and (4) operators may occur at different rates, as given by pfwd . We can use PCAT to define priors on our production parameters that bias our model toward rules 1 Note that this version has also updated the probability definitions for modifiers to be sums, incorporating the fact that any A/A is also a A/B (likewise for AA). This ensures that our grammar defines a valid probability distribution. 2 The probability distribution over categories is guaranteed to be proper so long as pterm > 12 since the probability of the depth of a tree will decrease geometrically (Chi, 1999). Non-terminal production prior means For the root, binary, and unary parameters, we want to choose prior means that encode our bias 25 that result in a priori more likely categories:3 atoms have features associated, then the atoms are allowed to unify if the features match, or if at least one of them does not have a feature. In defining κ, it is also important to ignore possible arguments on the wrong side of the combination since they can be consumed without affecting the connection between the two. To achieve this for κ(t, u), it is assumed that it is possible to consume all preceding argum"
K15-1003,J07-4004,0,0.0423889,"n should have a strong prior on combinability with its adjacent supertags np/n and s
p. Figure 1: CCG parse for “The man walks to work.” Parameters: θ ROOT ∼ Dir(αROOT , θ ROOT-0 ) θtBIN ∼ Dir(αBIN , θ BIN -0 ) θtUN ∼ Dir(αUN , θ UN -0 ) θtTERM ∼ Dir(αTERM , θtTERM -0 ) λt ∼ Dir(αλ , λ0 ) θtLCTX ∼ Dir(αLCTX , θtLCTX -0 ) θtRCTX ∼ Dir(αRCTX , θtRCTX -0 ) We follow Lewis and Steedman (2014) in allowing a small set of generic, linguistically-plausible unary and binary grammar rules. We further add rules for combining with punctuation to the left and right and allow for the merge rule X → X X of Clark and Curran (2007). 3 np/n Generative Model ∀t ∈ T ∀t ∈ T ∀t ∈ T ∀t ∈ T ∀t ∈ T ∀t ∈ T Sentence: In this section, we present our novel supertagcontext model (SCM) that augments a standard PCFG with parameters governing the supertags to the left and right of each constituent. do s ∼ Cat(θ ROOT ) y |s ∼ SCM(s) until the tree y is valid The CCG formalism is said to be naturally associative since a constituent label is often able to combine on either the left or the right. As a motivating example, consider the sentence “The lazy dog sleeps”, as shown in Figure 2. The word lazy, with category n/n, can either combine"
K15-1003,D12-1075,1,0.861669,"categories:  σ · 1/|T | if κ(t, r) σ > 1 right P (r |t) = 1/|T | otherwise  σ · PCAT (r) if κ(t, r) σ > 1 right PCAT (r |t) = PCAT (r) otherwise θ ROOT-0 (t) = PCAT (t) θ BIN -0 (hu, vi) = PCAT (u) · PCAT (v) θ UN -0 (hui) = PCAT (u) For simplicity, we assume the production-type mixture prior to be uniform: λ0 = h 13 , 31 , 31 i. 3.2 Terminal production prior means We employ the same procedure as our previous work for setting the terminal production prior distributions θtTERM -0 (w) by estimating word-givencategory relationships from the weak supervision: the tag dictionary and raw corpus (Garrette and Baldridge, 2012; Garrette et al., 2015).4 This procedure attempts to automatically estimate the frequency of each word/tag combination by dividing the number of raw-corpus occurrences of each word in the dictionary evenly across all of its associated tags. These counts are then combined with estimates of the “openness” of each tag in order to assess its likelihood of appearing with new words. 3.3 Context parameter prior means In order to encourage our model to choose trees in which the constituent labels “fit” into their supertag contexts, we want to bias our context parameters toward context categories that"
K15-1003,C10-1122,0,0.0163412,"the context parameters is:5 p(y |θ LCTX , θ RCTX ) = Y θ LCTX (yi−1,i |yij ) · θ RCTX (yj,j+1 |yij ) p(wi |yi,i+1 = t) = λt (T) · θtTERM (wi ) P + t→u λt (U) · θtUN (hui) 0≤i&lt;j≤n 5 Note that there may actually be multiple yij due to unary rules that “loop back” to the same position (i, j); all of these much be included in the product. · p(wi:j−1 |yij = u) 27 best-performing model of our previous work (Garrette et al., 2015), which SCM extends. We evaluated on the English CCGBank (Hockenmaier and Steedman, 2007), which is a transformation of the Penn Treebank (Marcus et al., 1993); the CTBCCG (Tse and Curran, 2010) transformation of the Penn Chinese Treebank (Xue et al., 2005); and the CCG-TUT corpus (Bos et al., 2009), built from the TUT corpus of Italian text (Bosco et al., 2000). Each corpus was divided into four distinct data sets: a set from which we extract the tag dictionaries, a set of raw (unannotated) sentences, a development set, and a test set. We use the same splits as Garrette et al. (2014). Since these treebanks use special representations for conjunctions, we chose to rewrite the trees to use conjunction categories of the form (XX)/X rather than introducing special conjunction rules. In"
K15-1003,W14-1615,1,0.846747,"allows us to sample efficiently from the correct posterior. Our experiments show that the incorporation of supertag context parameters into the model improves learning, and that placing combinability-preferring priors on those parameters yields further gains in many scenarios. pler categories by encoding a notion of category simplicity into a prior (Garrette et al., 2015). Baldridge further notes that due to the natural associativity of CCG, adjacent categories tend to be combinable. We previously showed that incorporating this intuition into a Bayesian prior can help train a CCG supertagger (Garrette et al., 2014). In this paper, we present a novel parsing model that is designed specifically for the capacity to capture both of these universal, intrinsic properties of CCG. We do so by extending our previous, PCFG-based parsing model to include parameters that govern the relationship between constituent categories and the preterminal categories (also known as supertags) to the left and right. The advantage of modeling context within a CCG framework is that while CCM must learn which contexts are likely purely from the data, the CCG categories give us obvious a priori information about whether a context i"
K15-1003,D07-1071,0,0.0387248,"n we would have to take a score of zero for that sentence: every dependency would be “wrong”. Thus, it is important that we make a best effort to find a parse. To accomplish this, we implemented a parsing backoff strategy. The parser first tries to find a valid parse that has either sdcl or np at its root. If that fails, then it searches for a parse with any root. If no parse is found yet, then the parser attempts to strategically allow tokens to subsume a neighbor by making it a dependent (first with a restricted root set, then without). This is similar to the “deletion” strategy employed by Zettlemoyer and Collins (2007), but we do it directly in the grammar. We add unary rules of the form hDi→u 29 the constituents, allowing us to take advantage of the natural a priori knowledge about which contexts fit with which constituent labels. Other researchers have shown positive results for grammar induction by introducing relatively small amounts of linguistic knowledge. Naseem et al. (2010) induced dependency parsers by handconstructing a small set of linguistically-universal dependency rules and using them as soft constraints during learning. These rules were useful for disambiguating between various structures in"
K16-1019,J13-1002,1,0.83594,"data structures: a syntactic stack St , a semantic stack Mt —each containing partially built structures—and a buffer of input words Bt . Our algorithm also places partial syntactic and semantic parse structures onto the front of the buffer, so it is also implemented as a stack. Each arc in the output corresponds to a transition (or “action”) chosen based on the current state; every transition modifies the state by updating St , Mt , and Bt to St+1 , Mt+1 , and Bt+1 , respectively. While each state may license several valid actions, each action 2 This works better for the arc-eager algorithm (Ballesteros and Nivre, 2013), in contrast to Henderson et al. (2013), who initialized with root at the buffer front. 3 Note that in the original arc-eager algorithm (Nivre, 2008), S HIFT and R IGHT-A RC actions move the item on the buffer front to the stack, whereas we only copy it (to allow the semantic operations to have access to it). 1 https://github.com/clab/ joint-lstm-parser 188 and Congress has other fied and the algorithm returns to syntactic transitions. This implies that, for each word, its leftside syntactic dependencies are resolved before its left-side semantic dependencies. An example run of the algorithm"
K16-1019,D15-1041,1,0.865442,"le 3: Comparison on the CoNLL 2009 English test set. The first block presents results of other models evaluated for both syntax and semantics on the CoNLL 2009 task. The second block presents our models. The third block presents the best published models, each using its own syntactic preprocessing. corporate morphological features where available; this could potentially improve performance, especially in highly inflective languages like Czech. An alternative might be to infer word-internal representations using character-based word embeddings, which was found beneficial for syntactic parsing (Ballesteros et al., 2015). Language Catalan Chinese Czech English German Japanese Spanish Average #1 C+’09 81.84 76.38 83.27 87.00 82.44 85.65 81.90 82.64 #2 Z+ ’09a 83.01 76.23 80.87 87.69 81.22 85.28 83.31 82.52 #3 G+ ’09 82.66 76.15 83.21 86.03 79.59 84.91 82.43 82.14 7 Conclusion We presented an incremental, greedy parser for joint syntactic and semantic dependency parsing. Our model surpasses the performance of previous joint models on the CoNLL 2008 and 2009 English tasks, without using expert-crafted, expensive features of the full syntactic parse. Joint 82.40 79.27 79.53 87.45 81.05 80.91 83.11 81.96 Acknowled"
K16-1019,W09-1206,0,0.209308,"Missing"
K16-1019,C10-3009,0,0.177351,"Missing"
K16-1019,D13-1152,0,0.0460906,"Missing"
K16-1019,W08-2122,0,0.59599,"al setWe present a transition-based parser that jointly produces syntactic and semantic dependencies. It learns a representation of the entire algorithm state, using stack long short-term memories. Our greedy inference algorithm has linear time, including feature extraction. On the CoNLL 2008–9 English shared tasks, we obtain the best published parsing performance among models that jointly learn syntax and semantics. 1 Introduction We introduce a new joint syntactic and semantic dependency parser. Our parser draws from the algorithmic insights of the incremental structure building approach of Henderson et al. (2008), with two key differences. First, it learns representations for the parser’s entire algorithmic state, not just the top items on the stack or the most recent parser states; in fact, it uses no expert-crafted features at all. Second, it uses entirely greedy inference rather than beam search. We find that it outperforms all previous joint parsing models, including Henderson et al. (2008) and variants (Gesmundo et al., 2009; Titov et al., 2009; Henderson et al., 2013) on the CoNLL 2008 and 2009 (English) shared tasks. Our parser’s multilingual results are comparable to the top systems at CoNLL 2"
K16-1019,W05-0620,0,0.265775,"Missing"
K16-1019,W08-2134,0,0.144763,"updated in every training epoch with a decay rate of 0.1 (Dyer et al., 2015). Training is stopped when the development performance does not improve for approximately 6–7 hours of elapsed time. Experiments were run on a single thread on a CPU, with memory requirements of up to 512 MB. 5 Results and Discussion CoNLL 2008 (Table 2) Our joint model significantly outperforms the joint model of Henderson et al. (2008), from which our set of tran193 Model joint models: Llu´ıs and M`arquez (2008) Henderson et al. (2008) Johansson (2009) Titov et al. (2009) CoNLL 2008 best: #3: Zhao and Kit (2008) #2: Che et al. (2008) #2: Ciaramita et al. (2008) #1: J&N (2008) Joint (this work) Sem. Macro F1 F1 85.8 87.6 86.6 87.5 70.3 73.1 77.1 76.1 78.1 80.5 81.8 81.8 87.7 86.7 87.4 89.3 89.1 76.7 78.5 78.0 81.6 80.5 82.2 82.7 82.7 85.5 84.9 LAS The overall performance of Joint is on par with the other winning participants at the CoNLL 2009 shared task (Zhao et al., 2009; Che et al., 2009; Gesmundo et al., 2009), falling behind only Zhao et al. (2009), who carefully designed languagespecific features and used a series of pipelines for the joint task, resulting in an accurate but computationally expensive system. State-of"
K16-1019,J13-4006,0,0.565554,"and semantic dependency parser. Our parser draws from the algorithmic insights of the incremental structure building approach of Henderson et al. (2008), with two key differences. First, it learns representations for the parser’s entire algorithmic state, not just the top items on the stack or the most recent parser states; in fact, it uses no expert-crafted features at all. Second, it uses entirely greedy inference rather than beam search. We find that it outperforms all previous joint parsing models, including Henderson et al. (2008) and variants (Gesmundo et al., 2009; Titov et al., 2009; Henderson et al., 2013) on the CoNLL 2008 and 2009 (English) shared tasks. Our parser’s multilingual results are comparable to the top systems at CoNLL 2009. Joint models like ours have frequently been proposed as a way to avoid cascading errors in NLP pipelines; varying degrees of success have been attained for a range of joint syntactic-semantic analysis tasks (Sutton and McCallum, 2005; Henderson et al., 2008; Toutanova et al., 2008; Johansson, 2009; Llu´ıs et al., 2013, inter alia). One reason pipelines often dominate is that they make available the complete syntactic parse tree, 187 Proceedings of the 20th SIGN"
K16-1019,W09-1207,0,0.0159494,"performs the joint model of Henderson et al. (2008), from which our set of tran193 Model joint models: Llu´ıs and M`arquez (2008) Henderson et al. (2008) Johansson (2009) Titov et al. (2009) CoNLL 2008 best: #3: Zhao and Kit (2008) #2: Che et al. (2008) #2: Ciaramita et al. (2008) #1: J&N (2008) Joint (this work) Sem. Macro F1 F1 85.8 87.6 86.6 87.5 70.3 73.1 77.1 76.1 78.1 80.5 81.8 81.8 87.7 86.7 87.4 89.3 89.1 76.7 78.5 78.0 81.6 80.5 82.2 82.7 82.7 85.5 84.9 LAS The overall performance of Joint is on par with the other winning participants at the CoNLL 2009 shared task (Zhao et al., 2009; Che et al., 2009; Gesmundo et al., 2009), falling behind only Zhao et al. (2009), who carefully designed languagespecific features and used a series of pipelines for the joint task, resulting in an accurate but computationally expensive system. State-of-the-art SRL systems (shown in the last block of Table 3) which use advances orthogonal to the contributions in this paper, perform better than our models. Many of these systems use expert-crafted features derived from full syntactic parses in a pipeline of classifiers followed by a global reranker (Bj¨orkelund et al., 2009; Bj¨orkelund et al., 2010; Roth and W"
K16-1019,P82-1020,0,0.887895,"Missing"
K16-1019,W08-2138,0,0.0752159,"Missing"
K16-1019,W08-2123,0,0.016286,"L 2009 English test set on a single core. This is almost 2.5 times faster than the pipeline model of Lei et al. (2015) (439.9±42 seconds) on the same machine.8 Table 2: Joint parsers: comparison on the CoNLL 2008 test (WSJ+Brown) set. sitions is derived, showing the benefit of learning a representation for the entire algorithmic state. Several other joint learning models have been proposed (Llu´ıs and M`arquez, 2008; Johansson, 2009; Titov et al., 2009) for the same task; our joint model surpasses the performance of all these models. The best reported systems on the CoNLL 2008 task are due to Johansson and Nugues (2008), Che et al. (2008), Ciaramita et al. (2008) and Zhao and Kit (2008), all of which pipeline syntax and semantics; our system’s semantic and overall performance is comparable to these. We fall behind only Johansson and Nugues (2008), whose success was attributed to carefully designed global SRL features integrated into a pipeline of classifiers, making them asymptotically slower. CoNLL 2009 English (Table 3) All of our models (Syntax-only, Semantics-only, Hybrid and Joint) improve over Gesmundo et al. (2009) and Henderson et al. (2013), demonstrating the benefit of our entire-parser-state repre"
K16-1019,D09-1059,0,0.230367,"Barcelona, Spain ♠ Google DeepMind, London, UK ♥ Computer Science & Engineering, University of Washington, Seattle, WA 98195, USA swabha@cs.cmu.edu, miguel.ballesteros@upf.edu, cdyer@cs.cmu.edu, nasmith@cs.washington.edu Abstract and arbitrarily-scoped syntactic features—such as the “path” between predicate and argument, proposed by Gildea and Jurafsky (2002)—for semantic analysis. Such features are a mainstay of highperformance semantic role labeling (SRL) systems (Roth and Woodsend, 2014; Lei et al., 2015; FitzGerald et al., 2015; Foland and Martin, 2015), but they are expensive to extract (Johansson, 2009; He et al., 2013). This study shows how recent advances in representation learning can bypass those expensive features, discovering cheap alternatives available during a greedy parsing procedure. The specific advance we employ is the stack LSTM (Dyer et al., 2015), a neural network that continuously summarizes the contents of the stack data structures in which a transition-based parser’s state is conventionally encoded. Stack LSTMs were shown to obviate many features used in syntactic dependency parsing; here we find them to do the same for joint syntactic-semantic dependency parsing. We beli"
K16-1019,P14-1112,0,0.0304826,"of the U.S. Army Research Office or the U.S. Government. Miguel Ballesteros was supported by the European Commission under the contract numbers FP7-ICT-610411 (project MULTISENSOR) and H2020-RIA-645012 (project KRISTINA). Table 4: Comparison of macro F1 scores on the multilingual CoNLL 2009 test set. 6 a pipelined model. However, their training necessitates CCG annotation, ours does not. Moreover, their evaluation metric rewards semantic dependencies regardless of where they attach within the argument span given by a PropBank constituent, making direct comparison to our evaluation infeasible. Krishnamurthy and Mitchell (2014) propose a joint CCG parsing and relation extraction model which improves over pipelines, but their task is different from ours. Li et al. (2010) also perform joint syntactic and semantic dependency parsing for Chinese, but do not report results on the CoNLL 2009 dataset. There has also been an increased interest in models which use neural networks for SRL. Collobert et al. (2011) proposed models which perform many NLP tasks without hand-crafted features. Though they did not achieve the best results on the constituent-based SRL task (Carreras and M`arquez, 2005), their approach inspired Zhou a"
K16-1019,P15-1033,1,0.571352,"features—such as the “path” between predicate and argument, proposed by Gildea and Jurafsky (2002)—for semantic analysis. Such features are a mainstay of highperformance semantic role labeling (SRL) systems (Roth and Woodsend, 2014; Lei et al., 2015; FitzGerald et al., 2015; Foland and Martin, 2015), but they are expensive to extract (Johansson, 2009; He et al., 2013). This study shows how recent advances in representation learning can bypass those expensive features, discovering cheap alternatives available during a greedy parsing procedure. The specific advance we employ is the stack LSTM (Dyer et al., 2015), a neural network that continuously summarizes the contents of the stack data structures in which a transition-based parser’s state is conventionally encoded. Stack LSTMs were shown to obviate many features used in syntactic dependency parsing; here we find them to do the same for joint syntactic-semantic dependency parsing. We believe this is an especially important finding for greedy models that cast parsing as a sequence of decisions made based on algorithmic state, where linguistic theory and researcher intuitions offer less guidance in feature design. Our system’s performance does not ma"
K16-1019,D15-1112,0,0.418532,"Missing"
K16-1019,D15-1169,0,0.122887,"Missing"
K16-1019,S15-1033,0,0.183514,"Natural Language Processing Group, Universitat Pompeu Fabra, Barcelona, Spain ♠ Google DeepMind, London, UK ♥ Computer Science & Engineering, University of Washington, Seattle, WA 98195, USA swabha@cs.cmu.edu, miguel.ballesteros@upf.edu, cdyer@cs.cmu.edu, nasmith@cs.washington.edu Abstract and arbitrarily-scoped syntactic features—such as the “path” between predicate and argument, proposed by Gildea and Jurafsky (2002)—for semantic analysis. Such features are a mainstay of highperformance semantic role labeling (SRL) systems (Roth and Woodsend, 2014; Lei et al., 2015; FitzGerald et al., 2015; Foland and Martin, 2015), but they are expensive to extract (Johansson, 2009; He et al., 2013). This study shows how recent advances in representation learning can bypass those expensive features, discovering cheap alternatives available during a greedy parsing procedure. The specific advance we employ is the stack LSTM (Dyer et al., 2015), a neural network that continuously summarizes the contents of the stack data structures in which a transition-based parser’s state is conventionally encoded. Stack LSTMs were shown to obviate many features used in syntactic dependency parsing; here we find them to do the same for"
K16-1019,P10-1113,0,0.0151724,"oject MULTISENSOR) and H2020-RIA-645012 (project KRISTINA). Table 4: Comparison of macro F1 scores on the multilingual CoNLL 2009 test set. 6 a pipelined model. However, their training necessitates CCG annotation, ours does not. Moreover, their evaluation metric rewards semantic dependencies regardless of where they attach within the argument span given by a PropBank constituent, making direct comparison to our evaluation infeasible. Krishnamurthy and Mitchell (2014) propose a joint CCG parsing and relation extraction model which improves over pipelines, but their task is different from ours. Li et al. (2010) also perform joint syntactic and semantic dependency parsing for Chinese, but do not report results on the CoNLL 2009 dataset. There has also been an increased interest in models which use neural networks for SRL. Collobert et al. (2011) proposed models which perform many NLP tasks without hand-crafted features. Though they did not achieve the best results on the constituent-based SRL task (Carreras and M`arquez, 2005), their approach inspired Zhou and Xu (2015), who achieved state-of-the-art results using deep bidirectional LSTMs. Our approach for dependency-based SRL is not directly compara"
K16-1019,W09-1205,0,0.35133,"oduction We introduce a new joint syntactic and semantic dependency parser. Our parser draws from the algorithmic insights of the incremental structure building approach of Henderson et al. (2008), with two key differences. First, it learns representations for the parser’s entire algorithmic state, not just the top items on the stack or the most recent parser states; in fact, it uses no expert-crafted features at all. Second, it uses entirely greedy inference rather than beam search. We find that it outperforms all previous joint parsing models, including Henderson et al. (2008) and variants (Gesmundo et al., 2009; Titov et al., 2009; Henderson et al., 2013) on the CoNLL 2008 and 2009 (English) shared tasks. Our parser’s multilingual results are comparable to the top systems at CoNLL 2009. Joint models like ours have frequently been proposed as a way to avoid cascading errors in NLP pipelines; varying degrees of success have been attained for a range of joint syntactic-semantic analysis tasks (Sutton and McCallum, 2005; Henderson et al., 2008; Toutanova et al., 2008; Johansson, 2009; Llu´ıs et al., 2013, inter alia). One reason pipelines often dominate is that they make available the complete syntactic"
K16-1019,N15-1142,1,0.750894,"in the English CoNLL 2009 training data. where v and u are vectors corresponding to atomic words or composed parse fragments; l and r are learned vector representations for syntactic and semantic labels respectively. Syntactic and semantic parameters are separated (Zs , es and Zm , em , respectively). Finally, for predicates, we use another recursive function to compose the word representation, v with a learned representation for the dismabiguated sense of the predicate, p: gd (v, p) = tanh(Zd [v; p] + ed ) Pretrained Embeddings Following Dyer et al. (2015), “structured skipgram” embeddings (Ling et al., 2015) were used, trained on the English (AFP section), German, Spanish and Chinese Gigaword corpora, with a window of size 5; training was stopped after 5 epochs. For out-of-vocabulary words, a randomly initialized vector of the same dimension was used. Figure 5: Example of a joint parse tree fragment with vector representations shown at each node. The vectors are obtained by recursive composition of representations of head, dependent, and label vectors. Syntactic dependencies and labels are in green, semantic in blue. gs (v, u, l) = tanh(Zs [v; u; l] + es ) (5) Training Training the classifier req"
K16-1019,J02-3001,0,0.506702,"Joint Syntactic-Semantic Parsing with Stack LSTMs Swabha Swayamdipta♣ Miguel Ballesteros♦ Chris Dyer♠ Noah A. Smith♥ ♣ School of Computer Science, Carnegie Mellon University, Pittsburgh, PA 15213, USA ♦ Natural Language Processing Group, Universitat Pompeu Fabra, Barcelona, Spain ♠ Google DeepMind, London, UK ♥ Computer Science & Engineering, University of Washington, Seattle, WA 98195, USA swabha@cs.cmu.edu, miguel.ballesteros@upf.edu, cdyer@cs.cmu.edu, nasmith@cs.washington.edu Abstract and arbitrarily-scoped syntactic features—such as the “path” between predicate and argument, proposed by Gildea and Jurafsky (2002)—for semantic analysis. Such features are a mainstay of highperformance semantic role labeling (SRL) systems (Roth and Woodsend, 2014; Lei et al., 2015; FitzGerald et al., 2015; Foland and Martin, 2015), but they are expensive to extract (Johansson, 2009; He et al., 2013). This study shows how recent advances in representation learning can bypass those expensive features, discovering cheap alternatives available during a greedy parsing procedure. The specific advance we employ is the stack LSTM (Dyer et al., 2015), a neural network that continuously summarizes the contents of the stack data st"
K16-1019,W08-2124,0,0.0851174,"Missing"
K16-1019,P15-1032,0,0.050578,"Missing"
K16-1019,Q13-1018,0,0.0957373,"Missing"
K16-1019,J93-2004,0,0.0604819,"Missing"
K16-1019,W08-2127,0,0.227225,"nt descent was used and updated in every training epoch with a decay rate of 0.1 (Dyer et al., 2015). Training is stopped when the development performance does not improve for approximately 6–7 hours of elapsed time. Experiments were run on a single thread on a CPU, with memory requirements of up to 512 MB. 5 Results and Discussion CoNLL 2008 (Table 2) Our joint model significantly outperforms the joint model of Henderson et al. (2008), from which our set of tran193 Model joint models: Llu´ıs and M`arquez (2008) Henderson et al. (2008) Johansson (2009) Titov et al. (2009) CoNLL 2008 best: #3: Zhao and Kit (2008) #2: Che et al. (2008) #2: Ciaramita et al. (2008) #1: J&N (2008) Joint (this work) Sem. Macro F1 F1 85.8 87.6 86.6 87.5 70.3 73.1 77.1 76.1 78.1 80.5 81.8 81.8 87.7 86.7 87.4 89.3 89.1 76.7 78.5 78.0 81.6 80.5 82.2 82.7 82.7 85.5 84.9 LAS The overall performance of Joint is on par with the other winning participants at the CoNLL 2009 shared task (Zhao et al., 2009; Che et al., 2009; Gesmundo et al., 2009), falling behind only Zhao et al. (2009), who carefully designed languagespecific features and used a series of pipelines for the joint task, resulting in an accurate but computationally expe"
K16-1019,W04-2705,0,0.160463,"Missing"
K16-1019,W09-1209,0,0.713032,"ts of the stack data structures in which a transition-based parser’s state is conventionally encoded. Stack LSTMs were shown to obviate many features used in syntactic dependency parsing; here we find them to do the same for joint syntactic-semantic dependency parsing. We believe this is an especially important finding for greedy models that cast parsing as a sequence of decisions made based on algorithmic state, where linguistic theory and researcher intuitions offer less guidance in feature design. Our system’s performance does not match that of the top expert-crafted feature-based systems (Zhao et al., 2009; Bj¨orkelund et al., 2010; Roth and Woodsend, 2014; Lei et al., 2015), systems which perform optimal decoding (T¨ackstr¨om et al., 2015), or of systems that exploit additional, differently-annotated datasets (FitzGerald et al., 2015). Many advances in those systems are orthogonal to our model, and we expect future work to achieve further gains by integrating them. Because our system is very fast— with an end-to-end runtime of 177.6±18 seconds to parse the CoNLL 2009 English test data on a single core—we believe it will be useful in practical setWe present a transition-based parser that jointl"
K16-1019,P15-1109,0,0.238586,"Missing"
K16-1019,J08-4003,0,0.451374,"are expected to reopen soon expect.01 reopen.01 C-A1 A1 AM-TMP A1 Figure 1: Example of a joint parse. Syntactic dependencies are shown by arcs above the sentence and semantic dependencies below; predicates are marked in boldface. C- denotes continuation of argument A1. Correspondences between dependencies might be close (between expected and to) or not (between reopen and all). 2.2 There are separate sets of syntactic and semantic transitions; the former manipulate S and B, the latter M and B. All are formally defined in Table 1. The syntactic transitions are from the “arceager” algorithm of Nivre (2008). They include: • S-S HIFT, which copies3 an item from the front of B and pushes it on S. • S-R EDUCE pops an item from S. • S-R IGHT(`) creates a syntactic dependency. Let u be the element at the top of S and v be the element at the front of B. The new dependency has u as head, v as dependent, and label `. u is popped off S, and the resulting structure, rooted at u, is pushed on S. Finally, v is copied to the top of S. • S-L EFT(`) creates a syntactic dependency with label ` in the reverse direction as S-R IGHT. The top of S, u, is popped. The front of B, v, is replaced by the new structure,"
K16-1019,P09-1040,0,0.412162,"pty. Actions that pop from a stack (S-R EDUCE and M-R EDUCE) are forbidden when that stack is empty. We disallow actions corresponding to the same dependency, or the same predicate to be repeated in the sequence. Repetitive M-S WAP transitions are disallowed to avoid infinite swapping. Finally, as noted above, we restrict the parser to syntactic actions until it needs to shift an item from B to S, after which it can only execute semantic actions until it executes an M-S HIFT. Asymptotic runtime complexity of this greedy algorithm is linear in the length of the input, following the analysis by Nivre (2009).5 Because SRL graphs allow a node to be a semantic argument of two parents—like all in the example in Figure 1—M-L EFT and M-R IGHT do not remove the dependent from the semantic stack and buffer respectively, unlike their syntactic equivalents, S-L EFT and S-R IGHT. We use two other semantic transitions from Henderson et al. (2013) which have no syntactic analogues: • M-S WAP swaps the top two items on M , to allow for crossing semantic arcs. • M-P RED(p) marks the item at the front of B as a semantic predicate with the sense p, and replaces it with the disambiguated predicate. The CoNLL 2009"
K16-1019,J05-1004,0,0.264767,"Missing"
K16-1019,D14-1045,0,0.175162,"er Science, Carnegie Mellon University, Pittsburgh, PA 15213, USA ♦ Natural Language Processing Group, Universitat Pompeu Fabra, Barcelona, Spain ♠ Google DeepMind, London, UK ♥ Computer Science & Engineering, University of Washington, Seattle, WA 98195, USA swabha@cs.cmu.edu, miguel.ballesteros@upf.edu, cdyer@cs.cmu.edu, nasmith@cs.washington.edu Abstract and arbitrarily-scoped syntactic features—such as the “path” between predicate and argument, proposed by Gildea and Jurafsky (2002)—for semantic analysis. Such features are a mainstay of highperformance semantic role labeling (SRL) systems (Roth and Woodsend, 2014; Lei et al., 2015; FitzGerald et al., 2015; Foland and Martin, 2015), but they are expensive to extract (Johansson, 2009; He et al., 2013). This study shows how recent advances in representation learning can bypass those expensive features, discovering cheap alternatives available during a greedy parsing procedure. The specific advance we employ is the stack LSTM (Dyer et al., 2015), a neural network that continuously summarizes the contents of the stack data structures in which a transition-based parser’s state is conventionally encoded. Stack LSTMs were shown to obviate many features used i"
K16-1019,W08-2121,0,0.31456,"Missing"
K16-1019,W05-0636,0,0.0398908,"ures at all. Second, it uses entirely greedy inference rather than beam search. We find that it outperforms all previous joint parsing models, including Henderson et al. (2008) and variants (Gesmundo et al., 2009; Titov et al., 2009; Henderson et al., 2013) on the CoNLL 2008 and 2009 (English) shared tasks. Our parser’s multilingual results are comparable to the top systems at CoNLL 2009. Joint models like ours have frequently been proposed as a way to avoid cascading errors in NLP pipelines; varying degrees of success have been attained for a range of joint syntactic-semantic analysis tasks (Sutton and McCallum, 2005; Henderson et al., 2008; Toutanova et al., 2008; Johansson, 2009; Llu´ıs et al., 2013, inter alia). One reason pipelines often dominate is that they make available the complete syntactic parse tree, 187 Proceedings of the 20th SIGNLL Conference on Computational Natural Language Learning (CoNLL), pages 187–197, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics has a deterministic effect on the state of the algorithm. Initially, S0 and M0 are empty, and B0 contains the input sentence with the first word at the front of B and a special root symbol at the end.2"
K16-1019,Q15-1003,0,0.171472,"Missing"
K16-1019,J08-2002,0,0.328138,"Missing"
K17-1004,D14-1155,1,0.38453,"ons reaches state of the art performance on the story cloze challenge. Our results demonstrate that different task framings can dramatically affect the way people write.1 1 Ending She feels flattered and asks John on a date. The girl found this charming, and gave him a second chance. John was happy about being rejected. Table 1: Examples of stories from the story cloze task. The table shows a story prefix with three contrastive endings: The original ending, a coherent ending and a incoherent one. since different tasks likely engage different cognitive processes (Campbell and Pennebaker, 2003; Banerjee et al., 2014).2 We show that similar writing tasks with different constraints on the author can lead to measurable differences in her writing style. As a case study, we present experiments based on the recently introduced ROC story cloze task (Mostafazadeh et al., 2016a). In this task, authors were asked to write five-sentence self-contained stories, henceforth original stories. Then, each original story was given to a different author, who was shown only the first four sentences as a story context, and asked to write two contrasting story endings: a right (coherent) ending, and a wrong (incoherent) ending"
K17-1004,P82-1020,0,0.851485,"Missing"
K17-1004,N12-1033,0,0.00560131,"yle has been an active topic of research for decades. The models used to characterize style are often linear classifiers with style features such as character and word n-grams (Stamatatos, 2009; Koppel et al., 2009). Previous work has shown that different authors can be grouped by their writing style, according to factors such as age (Pennebaker and Stone, 2003; Argamon et al., 2003; Schler et al., 2006; Rosenthal and McKeown, 2011; Nguyen et al., 2011), gender (Argamon et al., 2003; Schler et al., 2006; Bamman et al., 2014), and native language (Koppel et al., 2005; Tsur and Rappoport, 2007; Bergsma et al., 2012). At the extreme case, each individual author adopts a unique writing style (Mosteller 11 Similar problems have been shown in visual question answering datasets, where simple models that rely mostly on the question text perform competitively with state of the art models by exploiting language biases (Zhou et al., 2015; Jabri et al., 2016). 22 on the effects that a writing prompt has on an author’s mental state, and also her concrete response. They also provide valuable lessons for designing new NLP datasets. 10 style and physical health. Psychological Science 14(1):60–65. Danqi Chen, Jason Bol"
K17-1004,D15-1075,0,0.02051,"ples compared to 3,742 in the story cloze task), this indicates that simple instructions may help alleviate the effects of writing style found in this paper. Another way to avoid such effects is to have people rate naturally occurring sentences by parameters such as coherence (or, conversely, the level of surprise), rather than asking them to generate new text. 8 Machine reading. The story cloze task, which is the focus of this paper, is part of a wide set of machine reading/comprehension challenges published in the last few years. These include datasets like bAbI (Weston et al., 2016), SNLI (Bowman et al., 2015), CNN/DailyMail (Hermann et al., 2015), LAMBADA (Paperno et al., 2016) and SQuAD (Rajpurkar et al., 2016). While these works have presented resources for researchers, it is often the case that these datasets suffer from methodological problems caused by applying noisy automatic tools to generate them (Chen et al., 2016).11 In this paper, we have pointed to another methodological challenge in designing machine reading tasks: different writing tasks used to generated the data affect writing style, confounding classification problems. 9 Conclusion Different writing tasks assigned to an author res"
K17-1004,P17-2097,0,0.437985,"Missing"
K17-1004,P14-2072,0,0.0173871,"Missing"
K17-1004,N16-1098,0,0.194848,"Washington, Seattle, WA, USA 2 Allen Institute for Artificial Intelligence, Seattle, WA, USA {roysch,msap,ikonstas,lzilles,yejin,nasmith}@cs.washington.edu Story Prefix John liked a girl at his work. He tried to get her attention by acting silly. She told him to grow up. John confesses he was trying to make her like him more. Abstract A writer’s style depends not just on personal traits but also on her intent and mental state. In this paper, we show how variants of the same writing task can lead to measurable differences in writing style. We present a case study based on the story cloze task (Mostafazadeh et al., 2016a), where annotators were assigned similar writing tasks with different constraints: (1) writing an entire story, (2) adding a story ending for a given story context, and (3) adding an incoherent ending to a story. We show that a simple linear classifier informed by stylistic features is able to successfully distinguish among the three cases, without even looking at the story context. In addition, combining our stylistic features with language model predictions reaches state of the art performance on the story cloze challenge. Our results demonstrate that different task framings can dramatical"
K17-1004,W17-0906,0,0.0269252,"Missing"
K17-1004,W16-2505,0,0.160323,"Washington, Seattle, WA, USA 2 Allen Institute for Artificial Intelligence, Seattle, WA, USA {roysch,msap,ikonstas,lzilles,yejin,nasmith}@cs.washington.edu Story Prefix John liked a girl at his work. He tried to get her attention by acting silly. She told him to grow up. John confesses he was trying to make her like him more. Abstract A writer’s style depends not just on personal traits but also on her intent and mental state. In this paper, we show how variants of the same writing task can lead to measurable differences in writing style. We present a case study based on the story cloze task (Mostafazadeh et al., 2016a), where annotators were assigned similar writing tasks with different constraints: (1) writing an entire story, (2) adding a story ending for a given story context, and (3) adding an incoherent ending to a story. We show that a simple linear classifier informed by stylistic features is able to successfully distinguish among the three cases, without even looking at the story context. In addition, combining our stylistic features with language model predictions reaches state of the art performance on the story cloze challenge. Our results demonstrate that different task framings can dramatical"
K17-1004,D16-1264,0,0.0242504,"15), LAMBADA Background: The Story Cloze Task To understand how different writing tasks affect writing style, we focus on the story cloze task (Mostafazadeh et al., 2016a). While this task was developed to facilitate representation and learning of commonsense story understanding, its design included a few key choices which make it ideal for our study. We describe the task below. ROC stories. The ROC story corpus consists of 49,255 five-sentence stories, collected on Ama3 Recently, additional 53K stories were released, which results in roughly 100K stories. 16 (Paperno et al., 2016) and SQuAD (Rajpurkar et al., 2016), for which results improved dramatically over similar or much shorter periods of time. This suggests that this task is challenging and that high performance is hard to achieve. In addition, Mostafazadeh et al. (2016a) made substantial efforts to ensure the quality of this dataset. First, each pair of endings was written by the same author, which ensured that style differences between authors could not be used to solve the task. Furthermore, Mostafazadeh et al. implemented nine baselines for the task, using surface level features as well as narrative-informed ones, and showed that each of them"
K17-1004,P11-1077,0,0.0110115,"eve state of the art results on the story cloze task. The findings presented in this paper have cognitive implications, as they motivate further research Related Work Writing style. Writing style has been an active topic of research for decades. The models used to characterize style are often linear classifiers with style features such as character and word n-grams (Stamatatos, 2009; Koppel et al., 2009). Previous work has shown that different authors can be grouped by their writing style, according to factors such as age (Pennebaker and Stone, 2003; Argamon et al., 2003; Schler et al., 2006; Rosenthal and McKeown, 2011; Nguyen et al., 2011), gender (Argamon et al., 2003; Schler et al., 2006; Bamman et al., 2014), and native language (Koppel et al., 2005; Tsur and Rappoport, 2007; Bergsma et al., 2012). At the extreme case, each individual author adopts a unique writing style (Mosteller 11 Similar problems have been shown in visual question answering datasets, where simple models that rely mostly on the question text perform competitively with state of the art models by exploiting language biases (Zhou et al., 2015; Jabri et al., 2016). 22 on the effects that a writing prompt has on an author’s mental state,"
K17-1004,W11-1515,1,0.178023,"Missing"
K17-1004,P11-1032,1,0.0248668,"Writing tasks can even have a long-term effect, as writing emotional texts was observed to benefit both physical and mental health (LepDesign of NLP tasks. Our study also provides important insights for the future design of NLP tasks. The story cloze task was very carefully designed. Many factors, such as topic diversity and 21 and Wallace, 1963; Pennebaker and King, 1999; Schwartz et al., 2013b). The line of work that most resembles our work is the detection of deceptive text. Several researchers have used stylometric features to predict deception (Newman et al., 2003; Hancock et al., 2007; Ott et al., 2011; Feng et al., 2012). Some works even showed that gender affects a person’s writing style when lying (P´erez-Rosas and Mihalcea, 2014a,b). In this work, we have shown that an even more subtle writing task—writing coherent and incoherent story endings—imposes different styles on the author. temporal and causal relation diversity, were controlled for (Mostafazadeh et al., 2016a). The authors also made sure each pair of endings was written by the same author, partly in order to avoid author-specific style effects. Nonetheless, despite these efforts, several significant style differences can be fo"
K17-1004,P16-1144,0,0.0591898,"Missing"
K17-1004,W17-0907,1,0.671362,"Missing"
K17-1004,D13-1193,1,0.264354,"periment 2 Table 5: The top 5 most heavily weighted features for predicting right vs. wrong endings (5a) and original vs. new (right) endings (5b). length is the sentence length feature (see Section 4). ore and Smyth, 2002; Frattaroli, 2006). Campbell and Pennebaker (2003) also showed that the health benefits of writing emotional text are accompanied by changes in writing style, mostly in the use of pronouns. Another line of work has shown that writing style is affected by mental state. First, an author’s personality traits (e.g., depression, neuroticism, narcissism) affect her writing style (Schwartz et al., 2013a; Ireland and Mehl, 2014). Second, temporary changes, such as a romantic relationship (Ireland et al., 2011; Bowen et al., 2016), work collaboration (Tausczik, 2009; Gonzales et al., 2009), or negotiation (Ireland and Henderson, 2014) may also affect writing style. Finally, writing style can also change from one sentence to another, for instance between positive and negative text (Davidov et al., 2010) or when writing sarcastic text (Tsur et al., 2010). This large body of work indicates a tight connection between writing tasks, mental states, and variation in writing style. This connection hi"
K17-1004,W07-0602,0,0.0529888,"Writing style. Writing style has been an active topic of research for decades. The models used to characterize style are often linear classifiers with style features such as character and word n-grams (Stamatatos, 2009; Koppel et al., 2009). Previous work has shown that different authors can be grouped by their writing style, according to factors such as age (Pennebaker and Stone, 2003; Argamon et al., 2003; Schler et al., 2006; Rosenthal and McKeown, 2011; Nguyen et al., 2011), gender (Argamon et al., 2003; Schler et al., 2006; Bamman et al., 2014), and native language (Koppel et al., 2005; Tsur and Rappoport, 2007; Bergsma et al., 2012). At the extreme case, each individual author adopts a unique writing style (Mosteller 11 Similar problems have been shown in visual question answering datasets, where simple models that rely mostly on the question text perform competitively with state of the art models by exploiting language biases (Zhou et al., 2015; Jabri et al., 2016). 22 on the effects that a writing prompt has on an author’s mental state, and also her concrete response. They also provide valuable lessons for designing new NLP datasets. 10 style and physical health. Psychological Science 14(1):60–65"
K17-1004,P13-1093,0,0.0486373,"Missing"
K17-1004,N12-1097,1,0.0996929,"Missing"
K17-1004,P12-2034,1,\N,Missing
K17-1004,P16-1223,0,\N,Missing
K19-1029,N19-1253,0,0.0302079,"Chinese and English/Arabic), but focus on high-resource tasks. Here we explore a wider range of languages, and analyze the particular efficacy of a crosslingual approach to dependency parsing in a low-resource setting. We use a strong graph-based dependency parser with BiLSTM and biaffine attention (Dozat and Manning, 2017), which is also used in related work (Schuster et al., 2019; Mulcaire et al., 2019). Crucially, our parser only takes as input word representations. Universal parts of speech have been shown useful for low-resource dependency parsing (Duong et al., 2015; Ammar et al., 2016; Ahmad et al., 2019), but many realistic lowresource scenarios lack reliable part-of-speech taggers; here, we do not use parts of speech as input, and thus avoid the error-prone part-of-speech tagging pipeline. For the fastText baseline, word embeddings are not updated during training, to preserve crosslingual alignment (Ammar et al., 2016). 3 Experiments We first conduct a set of experiments to assess the efficacy of multilingual CWRs for low-resource dependency parsing. 3.1 Zero-Target Dependency Parsing Following prior work on low-resource dependency parsing and crosslingual transfer (Zhang and Barzilay, 2015;"
K19-1029,N19-1391,0,0.0793212,"igence, Seattle, WA, USA {pmulc,jkasai,nasmith}@cs.washington.edu Abstract acteristics (e.g., morphological, lexical, syntactic) which can be exploited by training a single polyglot model with data from multiple languages (Ammar, 2016). Recent work has extended contextual word representations (CWRs) multilingually either by training a polyglot language model (LM) on a mixture of data from multiple languages (joint training approach; Mulcaire et al., 2019; Lample and Conneau, 2019) or by aligning multiple monolingual language models crosslingually (retrofitting approach; Schuster et al., 2019; Aldarmaki and Diab, 2019). These multilingual representations have been shown to facilitate crosslingual transfer on several tasks, including Universal Dependencies parsing and natural language inference. In this work, we assess these two types of methods by using them for low-resource dependency parsing, and discover that the joint training approach substantially outperforms the retrofitting approach. We further apply multilingual CWRs produced by the joint training approach to diverse languages, and show that it is still effective in transfer between distant languages, though we find that phylogenetically related so"
K19-1029,Q16-1031,1,0.883447,"O SOV/SVO SOV/SVO SVO SVO SVO SOV SOV/SVO SVO SVO SOV SOV SOV Size – 5241 6983 12269 12543 3997 910 12217 1400 1656 31 3685 Table 1: List of the languages used in our UD v2.2 experiments. Each shaded/unshaded section corresponds to a pair of “related” languages. WALS 81A denotes Feature 81A in WALS, Order of Subject, Object, and Verb (Dryer and Haspelmath, 2013). “Size” represents the downsampled size in # of sentences used for source treebanks. The four languages in bold face are truly low resource languages (< 2000 trees). Dependency Parsers We train polyglot parsers for multiple languages (Ammar et al., 2016) on top of multilingual CWR s. All parser parameters are shared between the source and target languages. Ammar et al. (2016) suggest that sharing parameters between languages can alleviate the low-resource problem in syntactic parsing, but their experiments are limited to (relatively similar) European languages. Mulcaire et al. (2019) also include experiments with dependency parsing using polyglot contextual representations between two language pairs (English/Chinese and English/Arabic), but focus on high-resource tasks. Here we explore a wider range of languages, and analyze the particular ef"
K19-1029,Q17-1010,0,0.688701,"n both languages. The hypothesis behind this approach is that, although each language is unique, different languages manifest similar char⇤ Equal contribution. Random order. 304 Proceedings of the 23rd Conference on Computational Natural Language Learning, pages 304–315 c Hong Kong, China, November 3-4, 2019. 2019 Association for Computational Linguistics dependent of a particular corpus. We show that decontextualized vectors from the joint training approach yield representations that score higher on a word translation task than the retrofitting approach or word type vectors such as fastText (Bojanowski et al., 2017). This finding provides evidence that polyglot language models encode crosslingual similarity, specifically crosslingual lexical correspondence, that a linear alignment between monolingual language models does not. 2 two LSTM layers) to compute the contextual repP (j) resentation ei,c for the word: ei,c = 2j=0 j hi,c (Peters et al., 2018).3 In the first step, we compute (j) (j) an “anchor” hi for each word by averaging hi,c over all occurrences in an LM corpus. We then apply a standard dictionary-based technique4 to create multilingual word embeddings (Mikolov et al., 2013; Conneau et al., 201"
K19-1029,D18-1029,0,0.0497753,"Missing"
K19-1029,K18-2005,0,0.304244,"we examine the non-contextual part of the learned language models (which we call a “decontextual probe”) to demonstrate that polyglot language models better encode crosslingual lexical correspondence compared to aligned monolingual language models. This analysis provides further evidence that polyglot training is an effective approach to crosslingual transfer. 1 Introduction Dependency parsing has achieved new states of the art using distributed word representations in neural networks, trained with large amounts of annotated data (Dozat and Manning, 2017; Dozat et al., 2017; Ma et al., 2018; Che et al., 2018). However, many languages are low-resource, with small or no treebanks, which presents a severe challenge in developing accurate parsing systems in those languages. One way to address this problem is with a crosslingual solution that makes use of a language with a large treebank and raw text in both languages. The hypothesis behind this approach is that, although each language is unique, different languages manifest similar char⇤ Equal contribution. Random order. 304 Proceedings of the 23rd Conference on Computational Natural Language Learning, pages 304–315 c Hong Kong, China, November 3-4, 2"
K19-1029,P15-1119,0,0.0444867,"but many realistic lowresource scenarios lack reliable part-of-speech taggers; here, we do not use parts of speech as input, and thus avoid the error-prone part-of-speech tagging pipeline. For the fastText baseline, word embeddings are not updated during training, to preserve crosslingual alignment (Ammar et al., 2016). 3 Experiments We first conduct a set of experiments to assess the efficacy of multilingual CWRs for low-resource dependency parsing. 3.1 Zero-Target Dependency Parsing Following prior work on low-resource dependency parsing and crosslingual transfer (Zhang and Barzilay, 2015; Guo et al., 2015; Ammar et al., 2016; Schuster et al., 2019), we conduct multi-source experiments on six languages (German, Spanish, French, Italian, Portuguese, and Swedish) from Google universal dependency treebank version 2.0 (McDonald et al., 2013).6 We train language models on the six languages and English to produce multilingual CWRs. For each tested language, we train a polyglot parser with the multilingual CWRs on the five other languages and English, and apply the parser to the test data for the target language. Importantly, the parsing annotation scheme is shared among the seven languages. Our resul"
K19-1029,P17-1044,0,0.0456946,"nce of decontextualized vectors We perform a brief experiment to find what information is successfully retained by the decontextualized vectors, by using them as inputs to three tasks (in a monolingual English setting, for simplicity). For Universal Dependencies (UD) parsing, semantic role labeling (SRL), and named entity recognition (NER), we used the standard train/development/test splits from UD English EWT (Zeman et al., 2018) and Ontonotes (Pradhan et al., 2013). Following Mulcaire et al. (2019), we use strong existing neural models for each task: Dozat and Manning (2017) for UD parsing, He et al. (2017) for SRL, and Peters et al. (2017) for NER. Table 5 compares the decontextualized vectors with the original CWRs (ELMo) and the conventional word type vectors, GloVe (Pennington et al., 2014) and fastText (Bojanowski et al., 2017). In all three tasks, the decontextualized vectors substantially improve over fastText and GloVe vectors, and perform nearly on par with contextual 5.3 Results We present word translation results from our decontextual probe in Table 6. We see that the first 311 LSTM layer generally achieves the best crosslingual alignment both in ELMos and Rosita. This finding mirrors"
K19-1029,D11-1005,1,0.797174,"ce than the one produced by aligning monolingual language models or word type vectors. Our results provide a strong basis for multilingual representation learning and for further study of crosslingual transfer in a low-resource setting beyond dependency parsing. Further Related Work In addition to the work mentioned above, much previous work has proposed techniques to transfer knowledge from a high-resource to a lowresource language for dependency parsing. Many of these methods use an essentially (either lexicalized or delexicalized) joint polyglot training setup (e.g., McDonald et al., 2011; Cohen et al., 2011; Duong et al., 2015; Guo et al., 2016; Vilares et al., 2016; Falenska and C ¸ etino˘glu, 2017 as well as many of the CoNLL 2017/2018 shared task participants: Lim and Poibeau (2017); Vania et al. (2017); de Lhoneux et al. (2017); Che et al. (2018); Wan et al. (2018); Smith et al. (2018); Lim et al. (2018)). Some use typological information to facilitate crosslingual transfer (e.g., Naseem et al., 2012; T¨ackstr¨om et al., 2013; Zhang and Barzilay, 2015; Wang and Eisner, 2016; Rasooli and Collins, 2017; Ammar et al., 2016). Others use bitext (Zeman et al., 2018), manually-specified rules (Nase"
K19-1029,N19-1419,0,0.0691092,"Missing"
K19-1029,N19-1423,0,0.247489,"rget lanP (j) guages are computed by 2j=0 j W⇤(j) hi,c and P2 (j) j=0 j hi,c respectively. We use publicly available dictionaries from Conneau et al. (2018)5 and align all languages to the English LM space, again following Schuster et al. (2019). Multilingual CWRs Prior methods to produce multilingual contextual word representations (CWRs) can be categorized into two major classes, which we call joint training and retrofitting.1 The joint training approach trains a single polgylot language model (LM) on a mixture of texts in multiple languages (Mulcaire et al., 2019; Lample and Conneau, 2019; Devlin et al., 2019),2 while the retrofitting approach trains separate LMs on each language and aligns the learned representations later (Schuster et al., 2019; Aldarmaki and Diab, 2019). We compare example approaches from these two classes using the same LM training data, and discover that the joint training approach generally yields better performance in low-resource dependency parsing, even without crosslingual supervision. Joint Training Approach Another approach to multilingual CWRs is to train a single LM on multiple languages (Tsvetkov et al., 2016; Ragni et al., ¨ 2016; Ostling and Tiedemann, 2017). We tr"
K19-1029,D18-1543,0,0.0372648,"Missing"
K19-1029,K17-3002,0,0.0235323,"naries or parallel text. Furthermore, we examine the non-contextual part of the learned language models (which we call a “decontextual probe”) to demonstrate that polyglot language models better encode crosslingual lexical correspondence compared to aligned monolingual language models. This analysis provides further evidence that polyglot training is an effective approach to crosslingual transfer. 1 Introduction Dependency parsing has achieved new states of the art using distributed word representations in neural networks, trained with large amounts of annotated data (Dozat and Manning, 2017; Dozat et al., 2017; Ma et al., 2018; Che et al., 2018). However, many languages are low-resource, with small or no treebanks, which presents a severe challenge in developing accurate parsing systems in those languages. One way to address this problem is with a crosslingual solution that makes use of a language with a large treebank and raw text in both languages. The hypothesis behind this approach is that, although each language is unique, different languages manifest similar char⇤ Equal contribution. Random order. 304 Proceedings of the 23rd Conference on Computational Natural Language Learning, pages 304–315"
K19-1029,K17-3022,0,0.0606166,"Missing"
K19-1029,K18-2014,0,0.0172426,"mentioned above, much previous work has proposed techniques to transfer knowledge from a high-resource to a lowresource language for dependency parsing. Many of these methods use an essentially (either lexicalized or delexicalized) joint polyglot training setup (e.g., McDonald et al., 2011; Cohen et al., 2011; Duong et al., 2015; Guo et al., 2016; Vilares et al., 2016; Falenska and C ¸ etino˘glu, 2017 as well as many of the CoNLL 2017/2018 shared task participants: Lim and Poibeau (2017); Vania et al. (2017); de Lhoneux et al. (2017); Che et al. (2018); Wan et al. (2018); Smith et al. (2018); Lim et al. (2018)). Some use typological information to facilitate crosslingual transfer (e.g., Naseem et al., 2012; T¨ackstr¨om et al., 2013; Zhang and Barzilay, 2015; Wang and Eisner, 2016; Rasooli and Collins, 2017; Ammar et al., 2016). Others use bitext (Zeman et al., 2018), manually-specified rules (Naseem et al., 2012), or surface statistics from gold universal part of speech (Wang and Eisner, 2018a,b) to map the source to target. The methods examined in this work to produce multilingual CWRs do not rely on such external information about the languages, and instead use relatively abundant LM data to lear"
K19-1029,P15-2139,0,0.149901,"ons between two language pairs (English/Chinese and English/Arabic), but focus on high-resource tasks. Here we explore a wider range of languages, and analyze the particular efficacy of a crosslingual approach to dependency parsing in a low-resource setting. We use a strong graph-based dependency parser with BiLSTM and biaffine attention (Dozat and Manning, 2017), which is also used in related work (Schuster et al., 2019; Mulcaire et al., 2019). Crucially, our parser only takes as input word representations. Universal parts of speech have been shown useful for low-resource dependency parsing (Duong et al., 2015; Ammar et al., 2016; Ahmad et al., 2019), but many realistic lowresource scenarios lack reliable part-of-speech taggers; here, we do not use parts of speech as input, and thus avoid the error-prone part-of-speech tagging pipeline. For the fastText baseline, word embeddings are not updated during training, to preserve crosslingual alignment (Ammar et al., 2016). 3 Experiments We first conduct a set of experiments to assess the efficacy of multilingual CWRs for low-resource dependency parsing. 3.1 Zero-Target Dependency Parsing Following prior work on low-resource dependency parsing and crossli"
K19-1029,K17-3006,0,0.0173518,"study of crosslingual transfer in a low-resource setting beyond dependency parsing. Further Related Work In addition to the work mentioned above, much previous work has proposed techniques to transfer knowledge from a high-resource to a lowresource language for dependency parsing. Many of these methods use an essentially (either lexicalized or delexicalized) joint polyglot training setup (e.g., McDonald et al., 2011; Cohen et al., 2011; Duong et al., 2015; Guo et al., 2016; Vilares et al., 2016; Falenska and C ¸ etino˘glu, 2017 as well as many of the CoNLL 2017/2018 shared task participants: Lim and Poibeau (2017); Vania et al. (2017); de Lhoneux et al. (2017); Che et al. (2018); Wan et al. (2018); Smith et al. (2018); Lim et al. (2018)). Some use typological information to facilitate crosslingual transfer (e.g., Naseem et al., 2012; T¨ackstr¨om et al., 2013; Zhang and Barzilay, 2015; Wang and Eisner, 2016; Rasooli and Collins, 2017; Ammar et al., 2016). Others use bitext (Zeman et al., 2018), manually-specified rules (Naseem et al., 2012), or surface statistics from gold universal part of speech (Wang and Eisner, 2018a,b) to map the source to target. The methods examined in this work to produce multil"
K19-1029,W17-6303,0,0.0555885,"Missing"
K19-1029,N15-1184,1,0.81768,",c . We use a trainable weighted average of the three layers (character-CNN and 3 Schuster et al. (2019) only used the first LSTM layer, but we found a performance benefit from using all layers in preliminary results. 4 Conneau et al. (2018) developed an unsupervised alignment technique that does not require a dictionary. We found that their unsupervised alignment yielded substantially degraded performance in downstream parsing in line with the findings of Schuster et al. (2019). 5 https://github.com/facebookresearch/ MUSE#ground-truth-bilingual-dictionaries 1 This term was originally used by Faruqui et al. (2015) to describe updates to word vectors, after estimating them from corpora, using semantic lexicons. We generalize it to capture the notion of a separate update to fit something other than the original data, applied after conventional training. 2 Multilingual BERT is documented in https: //github.com/google-research/bert/blob/ master/multilingual.md. 305 Lang English Arabic Hebrew Croatian Russian Dutch German Spanish Italian Chinese Japanese Hungarian Finnish Vietnamese Uyghur Kazakh Turkish Refinement after Joint Training It is possible to combine the two approaches above; the alignment proced"
K19-1029,N19-1112,1,0.827052,"ors, GloVe (Pennington et al., 2014) and fastText (Bojanowski et al., 2017). In all three tasks, the decontextualized vectors substantially improve over fastText and GloVe vectors, and perform nearly on par with contextual 5.3 Results We present word translation results from our decontextual probe in Table 6. We see that the first 311 LSTM layer generally achieves the best crosslingual alignment both in ELMos and Rosita. This finding mirrors recent studies on layerwise transferability; representations from the first LSTM layer in a language model are most transferable across a range of tasks (Liu et al., 2019). Our decontextual probe demonstrates that the first LSTM layer learns the most generalizable representations not only across tasks but also across languages. In all six languages, Rosita (joint LM training approach) outperforms ELMos (retrofitting approach) and the fastText vectors. This shows that for the polyglot (jointly trained) LMs, there is a preexisting similarity between languages’ vector spaces beyond what a linear transform provides. The resulting language-agnostic representations lead to polyglot training’s success in lowresource dependency parsing. 6 (2018) showed that the (contex"
K19-1029,W18-2501,0,0.0218244,"y-based technique4 to create multilingual word embeddings (Mikolov et al., 2013; Conneau et al., 2018). In particular, suppose that we have a word-translation dictionary from source language s to target language (j) (j) t. Let Hs , Ht be matrices whose columns are the anchors in the jth layer for the source and corresponding target words in the dictionary. For each layer j, find the linear transformation W⇤(j) such that Models We examine crosslingual solutions to lowresource dependency parsing, which make crucial use of multilingual CWRs. All models are implemented in AllenNLP, version 0.7.2 (Gardner et al., 2018) and the hyperparameters and training details are given in the appendix. 2.1 W⇤(j) = argmin ||WH(j) s W (j) Ht ||F The linear transformations are then used to map the LM hidden states for the source language to the target LM space. Specifically, contextual representations for the source and target lanP (j) guages are computed by 2j=0 j W⇤(j) hi,c and P2 (j) j=0 j hi,c respectively. We use publicly available dictionaries from Conneau et al. (2018)5 and align all languages to the English LM space, again following Schuster et al. (2019). Multilingual CWRs Prior methods to produce multilingual con"
K19-1029,P18-1130,0,0.018379,"ext. Furthermore, we examine the non-contextual part of the learned language models (which we call a “decontextual probe”) to demonstrate that polyglot language models better encode crosslingual lexical correspondence compared to aligned monolingual language models. This analysis provides further evidence that polyglot training is an effective approach to crosslingual transfer. 1 Introduction Dependency parsing has achieved new states of the art using distributed word representations in neural networks, trained with large amounts of annotated data (Dozat and Manning, 2017; Dozat et al., 2017; Ma et al., 2018; Che et al., 2018). However, many languages are low-resource, with small or no treebanks, which presents a severe challenge in developing accurate parsing systems in those languages. One way to address this problem is with a crosslingual solution that makes use of a language with a large treebank and raw text in both languages. The hypothesis behind this approach is that, although each language is unique, different languages manifest similar char⇤ Equal contribution. Random order. 304 Proceedings of the 23rd Conference on Computational Natural Language Learning, pages 304–315 c Hong Kong, Chi"
K19-1029,P13-2017,0,0.0818547,"Missing"
K19-1029,D11-1006,0,0.0473247,"al lexical correspondence than the one produced by aligning monolingual language models or word type vectors. Our results provide a strong basis for multilingual representation learning and for further study of crosslingual transfer in a low-resource setting beyond dependency parsing. Further Related Work In addition to the work mentioned above, much previous work has proposed techniques to transfer knowledge from a high-resource to a lowresource language for dependency parsing. Many of these methods use an essentially (either lexicalized or delexicalized) joint polyglot training setup (e.g., McDonald et al., 2011; Cohen et al., 2011; Duong et al., 2015; Guo et al., 2016; Vilares et al., 2016; Falenska and C ¸ etino˘glu, 2017 as well as many of the CoNLL 2017/2018 shared task participants: Lim and Poibeau (2017); Vania et al. (2017); de Lhoneux et al. (2017); Che et al. (2018); Wan et al. (2018); Smith et al. (2018); Lim et al. (2018)). Some use typological information to facilitate crosslingual transfer (e.g., Naseem et al., 2012; T¨ackstr¨om et al., 2013; Zhang and Barzilay, 2015; Wang and Eisner, 2016; Rasooli and Collins, 2017; Ammar et al., 2016). Others use bitext (Zeman et al., 2018), manually-s"
K19-1029,N19-1392,1,0.920368,"asai~⇤ Noah A. Smith~} ~ Paul G. Allen School of Computer Science & Engineering, University of Washington, Seattle, WA, USA } Allen Institute for Artificial Intelligence, Seattle, WA, USA {pmulc,jkasai,nasmith}@cs.washington.edu Abstract acteristics (e.g., morphological, lexical, syntactic) which can be exploited by training a single polyglot model with data from multiple languages (Ammar, 2016). Recent work has extended contextual word representations (CWRs) multilingually either by training a polyglot language model (LM) on a mixture of data from multiple languages (joint training approach; Mulcaire et al., 2019; Lample and Conneau, 2019) or by aligning multiple monolingual language models crosslingually (retrofitting approach; Schuster et al., 2019; Aldarmaki and Diab, 2019). These multilingual representations have been shown to facilitate crosslingual transfer on several tasks, including Universal Dependencies parsing and natural language inference. In this work, we assess these two types of methods by using them for low-resource dependency parsing, and discover that the joint training approach substantially outperforms the retrofitting approach. We further apply multilingual CWRs produced by the j"
K19-1029,P12-1066,0,0.0411112,"source to a lowresource language for dependency parsing. Many of these methods use an essentially (either lexicalized or delexicalized) joint polyglot training setup (e.g., McDonald et al., 2011; Cohen et al., 2011; Duong et al., 2015; Guo et al., 2016; Vilares et al., 2016; Falenska and C ¸ etino˘glu, 2017 as well as many of the CoNLL 2017/2018 shared task participants: Lim and Poibeau (2017); Vania et al. (2017); de Lhoneux et al. (2017); Che et al. (2018); Wan et al. (2018); Smith et al. (2018); Lim et al. (2018)). Some use typological information to facilitate crosslingual transfer (e.g., Naseem et al., 2012; T¨ackstr¨om et al., 2013; Zhang and Barzilay, 2015; Wang and Eisner, 2016; Rasooli and Collins, 2017; Ammar et al., 2016). Others use bitext (Zeman et al., 2018), manually-specified rules (Naseem et al., 2012), or surface statistics from gold universal part of speech (Wang and Eisner, 2018a,b) to map the source to target. The methods examined in this work to produce multilingual CWRs do not rely on such external information about the languages, and instead use relatively abundant LM data to learn crosslinguality that abstracts away from typological divergence. Recent work has developed sever"
K19-1029,E17-2102,0,0.0186593,"Conneau, 2019; Devlin et al., 2019),2 while the retrofitting approach trains separate LMs on each language and aligns the learned representations later (Schuster et al., 2019; Aldarmaki and Diab, 2019). We compare example approaches from these two classes using the same LM training data, and discover that the joint training approach generally yields better performance in low-resource dependency parsing, even without crosslingual supervision. Joint Training Approach Another approach to multilingual CWRs is to train a single LM on multiple languages (Tsvetkov et al., 2016; Ragni et al., ¨ 2016; Ostling and Tiedemann, 2017). We train a single bidirectional LM with charater CNNs and two-layer LSTMs on multiple languages (Rosita, Mulcaire et al., 2019). We then use the polyglot LM to provide contextual representations. Similarly to the retrofitting approach, we represent word i in context c as a trainable weighted average of the hidden states in the trained polyglot LM: P2 (j) j=0 j hi,c . In contrast to retrofitting, crosslinguality is learned implicitly by sharing all network parameters during LM training; no crosslingual dictionaries are used. Retrofitting Approach Following Schuster et al. (2019), we first tra"
K19-1029,D14-1162,0,0.082118,"(in a monolingual English setting, for simplicity). For Universal Dependencies (UD) parsing, semantic role labeling (SRL), and named entity recognition (NER), we used the standard train/development/test splits from UD English EWT (Zeman et al., 2018) and Ontonotes (Pradhan et al., 2013). Following Mulcaire et al. (2019), we use strong existing neural models for each task: Dozat and Manning (2017) for UD parsing, He et al. (2017) for SRL, and Peters et al. (2017) for NER. Table 5 compares the decontextualized vectors with the original CWRs (ELMo) and the conventional word type vectors, GloVe (Pennington et al., 2014) and fastText (Bojanowski et al., 2017). In all three tasks, the decontextualized vectors substantially improve over fastText and GloVe vectors, and perform nearly on par with contextual 5.3 Results We present word translation results from our decontextual probe in Table 6. We see that the first 311 LSTM layer generally achieves the best crosslingual alignment both in ELMos and Rosita. This finding mirrors recent studies on layerwise transferability; representations from the first LSTM layer in a language model are most transferable across a range of tasks (Liu et al., 2019). Our decontextual"
K19-1029,P17-1161,0,0.0305724,"s We perform a brief experiment to find what information is successfully retained by the decontextualized vectors, by using them as inputs to three tasks (in a monolingual English setting, for simplicity). For Universal Dependencies (UD) parsing, semantic role labeling (SRL), and named entity recognition (NER), we used the standard train/development/test splits from UD English EWT (Zeman et al., 2018) and Ontonotes (Pradhan et al., 2013). Following Mulcaire et al. (2019), we use strong existing neural models for each task: Dozat and Manning (2017) for UD parsing, He et al. (2017) for SRL, and Peters et al. (2017) for NER. Table 5 compares the decontextualized vectors with the original CWRs (ELMo) and the conventional word type vectors, GloVe (Pennington et al., 2014) and fastText (Bojanowski et al., 2017). In all three tasks, the decontextualized vectors substantially improve over fastText and GloVe vectors, and perform nearly on par with contextual 5.3 Results We present word translation results from our decontextual probe in Table 6. We see that the first 311 LSTM layer generally achieves the best crosslingual alignment both in ELMos and Rosita. This finding mirrors recent studies on layerwise trans"
K19-1029,P16-2069,0,0.0358767,"Missing"
K19-1029,N18-1202,0,0.279736,"l Linguistics dependent of a particular corpus. We show that decontextualized vectors from the joint training approach yield representations that score higher on a word translation task than the retrofitting approach or word type vectors such as fastText (Bojanowski et al., 2017). This finding provides evidence that polyglot language models encode crosslingual similarity, specifically crosslingual lexical correspondence, that a linear alignment between monolingual language models does not. 2 two LSTM layers) to compute the contextual repP (j) resentation ei,c for the word: ei,c = 2j=0 j hi,c (Peters et al., 2018).3 In the first step, we compute (j) (j) an “anchor” hi for each word by averaging hi,c over all occurrences in an LM corpus. We then apply a standard dictionary-based technique4 to create multilingual word embeddings (Mikolov et al., 2013; Conneau et al., 2018). In particular, suppose that we have a word-translation dictionary from source language s to target language (j) (j) t. Let Hs , Ht be matrices whose columns are the anchors in the jth layer for the source and corresponding target words in the dictionary. For each layer j, find the linear transformation W⇤(j) such that Models We examin"
K19-1029,P19-1493,0,0.0251651,"experiments: multilingual BERT is trained on much larger amounts of Wikipedia data compared to other LMs used in this work, and the WordPiece vocabulary (Wu et al., 2016) used in the cased multilingual BERT model has been shown to have a distribution skewed toward Latin ´ alphabets (Acs, 2019). These results are thus not directly comparable to those in Figure 1; nevertheless, it is interesting to see that the results obtained with ELMo-like LMs are comparable to and in some cases better than results using a BERT model trained on over a hundred languages. Our results broadly fit with those of Pires et al. (2019), who found that multilingual BERT was useful for zero-shot crosslingual syntactic transfer. In particular, we find nearly no performance benefit from cross-script transfer using BERT in a language pair (English-Japanese) for which they reported 5.1 Decontextualization Recall from Section 2 that we produce CWRs from bidirectional LMs with character CNNs and twolayer LSTMs. We propose a method to remove the dependence on context c for the two LSTM layers (the CNN layer is already context-independent by design). During LM training, the hidden states of each layer ht are computed by the standard"
K19-1029,Q16-1035,0,0.0480394,"thods use an essentially (either lexicalized or delexicalized) joint polyglot training setup (e.g., McDonald et al., 2011; Cohen et al., 2011; Duong et al., 2015; Guo et al., 2016; Vilares et al., 2016; Falenska and C ¸ etino˘glu, 2017 as well as many of the CoNLL 2017/2018 shared task participants: Lim and Poibeau (2017); Vania et al. (2017); de Lhoneux et al. (2017); Che et al. (2018); Wan et al. (2018); Smith et al. (2018); Lim et al. (2018)). Some use typological information to facilitate crosslingual transfer (e.g., Naseem et al., 2012; T¨ackstr¨om et al., 2013; Zhang and Barzilay, 2015; Wang and Eisner, 2016; Rasooli and Collins, 2017; Ammar et al., 2016). Others use bitext (Zeman et al., 2018), manually-specified rules (Naseem et al., 2012), or surface statistics from gold universal part of speech (Wang and Eisner, 2018a,b) to map the source to target. The methods examined in this work to produce multilingual CWRs do not rely on such external information about the languages, and instead use relatively abundant LM data to learn crosslinguality that abstracts away from typological divergence. Recent work has developed several probing methods for (monolingual) contextual representations (Liu et al."
K19-1029,Q18-1046,0,0.071877,"¸ etino˘glu, 2017 as well as many of the CoNLL 2017/2018 shared task participants: Lim and Poibeau (2017); Vania et al. (2017); de Lhoneux et al. (2017); Che et al. (2018); Wan et al. (2018); Smith et al. (2018); Lim et al. (2018)). Some use typological information to facilitate crosslingual transfer (e.g., Naseem et al., 2012; T¨ackstr¨om et al., 2013; Zhang and Barzilay, 2015; Wang and Eisner, 2016; Rasooli and Collins, 2017; Ammar et al., 2016). Others use bitext (Zeman et al., 2018), manually-specified rules (Naseem et al., 2012), or surface statistics from gold universal part of speech (Wang and Eisner, 2018a,b) to map the source to target. The methods examined in this work to produce multilingual CWRs do not rely on such external information about the languages, and instead use relatively abundant LM data to learn crosslinguality that abstracts away from typological divergence. Recent work has developed several probing methods for (monolingual) contextual representations (Liu et al., 2019; Hewitt and Manning, 2019; Tenney et al., 2019). Wada and Iwata Acknowledgments The authors thank Nikolaos Pappas and Tal Schuster as well as the anonymous reviewers for their helpful feedback. This research wa"
K19-1029,Q17-1020,0,0.239826,"y (either lexicalized or delexicalized) joint polyglot training setup (e.g., McDonald et al., 2011; Cohen et al., 2011; Duong et al., 2015; Guo et al., 2016; Vilares et al., 2016; Falenska and C ¸ etino˘glu, 2017 as well as many of the CoNLL 2017/2018 shared task participants: Lim and Poibeau (2017); Vania et al. (2017); de Lhoneux et al. (2017); Che et al. (2018); Wan et al. (2018); Smith et al. (2018); Lim et al. (2018)). Some use typological information to facilitate crosslingual transfer (e.g., Naseem et al., 2012; T¨ackstr¨om et al., 2013; Zhang and Barzilay, 2015; Wang and Eisner, 2016; Rasooli and Collins, 2017; Ammar et al., 2016). Others use bitext (Zeman et al., 2018), manually-specified rules (Naseem et al., 2012), or surface statistics from gold universal part of speech (Wang and Eisner, 2018a,b) to map the source to target. The methods examined in this work to produce multilingual CWRs do not rely on such external information about the languages, and instead use relatively abundant LM data to learn crosslinguality that abstracts away from typological divergence. Recent work has developed several probing methods for (monolingual) contextual representations (Liu et al., 2019; Hewitt and Manning,"
K19-1029,D18-1163,0,0.0613685,"¸ etino˘glu, 2017 as well as many of the CoNLL 2017/2018 shared task participants: Lim and Poibeau (2017); Vania et al. (2017); de Lhoneux et al. (2017); Che et al. (2018); Wan et al. (2018); Smith et al. (2018); Lim et al. (2018)). Some use typological information to facilitate crosslingual transfer (e.g., Naseem et al., 2012; T¨ackstr¨om et al., 2013; Zhang and Barzilay, 2015; Wang and Eisner, 2016; Rasooli and Collins, 2017; Ammar et al., 2016). Others use bitext (Zeman et al., 2018), manually-specified rules (Naseem et al., 2012), or surface statistics from gold universal part of speech (Wang and Eisner, 2018a,b) to map the source to target. The methods examined in this work to produce multilingual CWRs do not rely on such external information about the languages, and instead use relatively abundant LM data to learn crosslinguality that abstracts away from typological divergence. Recent work has developed several probing methods for (monolingual) contextual representations (Liu et al., 2019; Hewitt and Manning, 2019; Tenney et al., 2019). Wada and Iwata Acknowledgments The authors thank Nikolaos Pappas and Tal Schuster as well as the anonymous reviewers for their helpful feedback. This research wa"
K19-1029,K18-2019,0,0.0551901,"Missing"
K19-1029,N19-1162,0,0.0606411,"e for Artificial Intelligence, Seattle, WA, USA {pmulc,jkasai,nasmith}@cs.washington.edu Abstract acteristics (e.g., morphological, lexical, syntactic) which can be exploited by training a single polyglot model with data from multiple languages (Ammar, 2016). Recent work has extended contextual word representations (CWRs) multilingually either by training a polyglot language model (LM) on a mixture of data from multiple languages (joint training approach; Mulcaire et al., 2019; Lample and Conneau, 2019) or by aligning multiple monolingual language models crosslingually (retrofitting approach; Schuster et al., 2019; Aldarmaki and Diab, 2019). These multilingual representations have been shown to facilitate crosslingual transfer on several tasks, including Universal Dependencies parsing and natural language inference. In this work, we assess these two types of methods by using them for low-resource dependency parsing, and discover that the joint training approach substantially outperforms the retrofitting approach. We further apply multilingual CWRs produced by the joint training approach to diverse languages, and show that it is still effective in transfer between distant languages, though we find that"
K19-1029,K18-2011,0,0.0454126,"Missing"
K19-1029,K18-2001,0,0.0360948,"Missing"
K19-1029,N13-1126,0,0.14257,"Missing"
K19-1029,D15-1213,0,0.319122,"2016; Ahmad et al., 2019), but many realistic lowresource scenarios lack reliable part-of-speech taggers; here, we do not use parts of speech as input, and thus avoid the error-prone part-of-speech tagging pipeline. For the fastText baseline, word embeddings are not updated during training, to preserve crosslingual alignment (Ammar et al., 2016). 3 Experiments We first conduct a set of experiments to assess the efficacy of multilingual CWRs for low-resource dependency parsing. 3.1 Zero-Target Dependency Parsing Following prior work on low-resource dependency parsing and crosslingual transfer (Zhang and Barzilay, 2015; Guo et al., 2015; Ammar et al., 2016; Schuster et al., 2019), we conduct multi-source experiments on six languages (German, Spanish, French, Italian, Portuguese, and Swedish) from Google universal dependency treebank version 2.0 (McDonald et al., 2013).6 We train language models on the six languages and English to produce multilingual CWRs. For each tested language, we train a polyglot parser with the multilingual CWRs on the five other languages and English, and apply the parser to the test data for the target language. Importantly, the parsing annotation scheme is shared among the seven la"
K19-1029,P19-1452,0,0.0261651,"Missing"
K19-1029,N16-1161,0,0.0315393,"languages (Mulcaire et al., 2019; Lample and Conneau, 2019; Devlin et al., 2019),2 while the retrofitting approach trains separate LMs on each language and aligns the learned representations later (Schuster et al., 2019; Aldarmaki and Diab, 2019). We compare example approaches from these two classes using the same LM training data, and discover that the joint training approach generally yields better performance in low-resource dependency parsing, even without crosslingual supervision. Joint Training Approach Another approach to multilingual CWRs is to train a single LM on multiple languages (Tsvetkov et al., 2016; Ragni et al., ¨ 2016; Ostling and Tiedemann, 2017). We train a single bidirectional LM with charater CNNs and two-layer LSTMs on multiple languages (Rosita, Mulcaire et al., 2019). We then use the polyglot LM to provide contextual representations. Similarly to the retrofitting approach, we represent word i in context c as a trainable weighted average of the hidden states in the trained polyglot LM: P2 (j) j=0 j hi,c . In contrast to retrofitting, crosslinguality is learned implicitly by sharing all network parameters during LM training; no crosslingual dictionaries are used. Retrofitting App"
K19-1029,W13-3516,0,\N,Missing
K19-1029,K17-3010,0,\N,Missing
K19-1029,K18-2009,0,\N,Missing
N09-1009,D08-1092,0,0.027779,"nese is also the highest in the bilingual setting, with T IE A and T IE V&N. 5 In future work we plan to lexicalize the model, including a Bayesian grammar prior that accounts for the syntactic patterns of words. Nonparametric models (Teh, 2006) may be appropriate. We also believe that Bayesian discovery of cross-linguistic patterns is an exciting topic worthy of further exploration. 6 Bilingual Experiments Leveraging information from one language for the task of disambiguating another language has received considerable attention (Dagan, 1991; Smith and Smith, 2004; Snyder and Barzilay, 2008; Burkett and Klein, 2008). Usually such a setting requires a parallel corpus or other annotated data that ties between those two languages.5 Our bilingual experiments use the English and Chinese treebanks, which are not parallel corpora, to train parsers for both languages jointly. Shar5 Haghighi et al. (2008) presented a technique to learn bilingual lexicons from two non-parallel monolingual corpora. 81 Future Work Conclusion We described a Bayesian model that allows soft parameter tying among any weights in a probabilistic grammar. We used this model to improve unsupervised parsing accuracy on two different language"
N09-1009,P05-1022,0,0.00520607,"vation events in the probabilistic grammar, providing a new way to encode prior knowledge about an unknown grammar. We describe a variational EM algorithm for learning a probabilistic grammar based on this family of priors. We then experiment with unsupervised dependency grammar induction and show significant improvements using our model for both monolingual learning and bilingual learning with a non-parallel, multilingual corpus. 1 Introduction Probabilistic grammars have become an important tool in natural language processing. They are most commonly used for parsing and linguistic analysis (Charniak and Johnson, 2005; Collins, 2003), but are now commonly seen in applications like machine translation (Wu, 1997) and question answering (Wang et al., 2007). An attractive property of probabilistic grammars is that they permit the use of well-understood parameter estimation methods for learning—both from labeled and unlabeled data. Here we tackle the unsupervised grammar learning problem, specifically for unlexicalized context-free dependency grammars, using an empirical Bayesian approach with a novel family of priors. There has been an increased interest recently in employing Bayesian modeling for probabilisti"
N09-1009,J03-4003,0,0.0616512,"listic grammar, providing a new way to encode prior knowledge about an unknown grammar. We describe a variational EM algorithm for learning a probabilistic grammar based on this family of priors. We then experiment with unsupervised dependency grammar induction and show significant improvements using our model for both monolingual learning and bilingual learning with a non-parallel, multilingual corpus. 1 Introduction Probabilistic grammars have become an important tool in natural language processing. They are most commonly used for parsing and linguistic analysis (Charniak and Johnson, 2005; Collins, 2003), but are now commonly seen in applications like machine translation (Wu, 1997) and question answering (Wang et al., 2007). An attractive property of probabilistic grammars is that they permit the use of well-understood parameter estimation methods for learning—both from labeled and unlabeled data. Here we tackle the unsupervised grammar learning problem, specifically for unlexicalized context-free dependency grammars, using an empirical Bayesian approach with a novel family of priors. There has been an increased interest recently in employing Bayesian modeling for probabilistic grammars in di"
N09-1009,P91-1017,0,0.132253,"hieving highest performance with T IE V&N. Performance with Chinese is also the highest in the bilingual setting, with T IE A and T IE V&N. 5 In future work we plan to lexicalize the model, including a Bayesian grammar prior that accounts for the syntactic patterns of words. Nonparametric models (Teh, 2006) may be appropriate. We also believe that Bayesian discovery of cross-linguistic patterns is an exciting topic worthy of further exploration. 6 Bilingual Experiments Leveraging information from one language for the task of disambiguating another language has received considerable attention (Dagan, 1991; Smith and Smith, 2004; Snyder and Barzilay, 2008; Burkett and Klein, 2008). Usually such a setting requires a parallel corpus or other annotated data that ties between those two languages.5 Our bilingual experiments use the English and Chinese treebanks, which are not parallel corpora, to train parsers for both languages jointly. Shar5 Haghighi et al. (2008) presented a technique to learn bilingual lexicons from two non-parallel monolingual corpora. 81 Future Work Conclusion We described a Bayesian model that allows soft parameter tying among any weights in a probabilistic grammar. We used t"
N09-1009,W02-1009,0,0.0216379,"over the probability simplex. Cohen et al. used this prior to softly tie grammar weights through the covariance parameters of the LN. The prior encodes information about which grammar rules’ weights are likely to covary, a more intuitive and expressive representation of knowledge than offered by Dirichlet distributions.1 The contribution of this paper is two-fold. First, from the modeling perspective, we present a generalization of the LN prior of Cohen et al. (2008), showing how to extend the use of the LN prior to 1 Although the task, underlying model, and weights being tied were different, Eisner (2002) also showed evidence for the efficacy of parameter tying in grammar learning. Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 74–82, c Boulder, Colorado, June 2009. 2009 Association for Computational Linguistics tie between any grammar weights in a probabilistic grammar (instead of only allowing weights within the same multinomial distribution to covary). Second, from the experimental perspective, we show how such flexibility in parameter tying can help in unsupervised grammar learning in the well-known monolingual setting and in a new b"
N09-1009,P07-1035,0,0.0663304,"ood parameter estimation methods for learning—both from labeled and unlabeled data. Here we tackle the unsupervised grammar learning problem, specifically for unlexicalized context-free dependency grammars, using an empirical Bayesian approach with a novel family of priors. There has been an increased interest recently in employing Bayesian modeling for probabilistic grammars in different settings, ranging from putting priors over grammar probabilities (Johnson et al., 74 2007) to putting non-parametric priors over derivations (Johnson et al., 2006) to learning the set of states in a grammar (Finkel et al., 2007; Liang et al., 2007). Bayesian methods offer an elegant framework for combining prior knowledge with data. The main challenge in Bayesian grammar learning is efficiently approximating probabilistic inference, which is generally intractable. Most commonly variational (Johnson, 2007; Kurihara and Sato, 2006) or sampling techniques are applied (Johnson et al., 2006). Because probabilistic grammars are built out of multinomial distributions, the Dirichlet family (or, more precisely, a collection of Dirichlets) is a natural candidate for probabilistic grammars because of its conjugacy to the multi"
N09-1009,P96-1024,0,0.0217755,"notated data), and report final results on §23. For Chinese, we train on §1–270, use §301–1151 for development and report testing results on §271–300.3 To evaluate performance, we report the fraction of words whose predicted parent matches the gold standard corpus. This performance measure is also known as attachment accuracy. We considered two parsing methods after extracting a point estimate for the grammar: the most probable “Viterbi” parse (argmaxy p(y |x, θ)) and the minimum Bayes risk (MBR) parse (argminy Ep(y0 |x,θ) [`(y; x, y0 )]) with dependency attachment error as the loss function (Goodman, 1996). Performance with MBR parsing is consistently higher than its Viterbi counterpart, so we report only performance with MBR parsing. 4.1 Nouns, Verbs, and Adjectives In this paper, we use a few simple heuristics to decide which partition structure S to use. Our heuris3 Unsupervised training for these datasets can be costly, and requires iteratively running a cubic-time inside-outside dynamic programming algorithm, so we follow Klein and Manning (2004) in restricting the training set to sentences of ten or fewer words in length. Short sentences are also less structurally ambiguous and may theref"
N09-1009,P08-1088,0,0.0239793,"ayesian discovery of cross-linguistic patterns is an exciting topic worthy of further exploration. 6 Bilingual Experiments Leveraging information from one language for the task of disambiguating another language has received considerable attention (Dagan, 1991; Smith and Smith, 2004; Snyder and Barzilay, 2008; Burkett and Klein, 2008). Usually such a setting requires a parallel corpus or other annotated data that ties between those two languages.5 Our bilingual experiments use the English and Chinese treebanks, which are not parallel corpora, to train parsers for both languages jointly. Shar5 Haghighi et al. (2008) presented a technique to learn bilingual lexicons from two non-parallel monolingual corpora. 81 Future Work Conclusion We described a Bayesian model that allows soft parameter tying among any weights in a probabilistic grammar. We used this model to improve unsupervised parsing accuracy on two different languages, English and Chinese, achieving state-of-the-art results. We also showed how our model can be effectively used to simultaneously learn grammars in two languages from non-parallel multilingual data. Acknowledgments This research was supported by NSF IIS-0836431. The authors thank the"
N09-1009,N09-1012,0,0.527987,"ntire tree is given by p(x, y |θ) = P (y(0) |$, θ). The θ are the multinomial distributions θs (· |·, ·, ·) and θc (· |·, ·). To P (y(i) |xi , θ) = Q D∈{left,right} θs (stop × Q j∈yD (i) θs (¬stop |xi , D, [yD (i) = ∅]) (2) |xi , D, firsty (j)) × θc (xj |xi , D) × P (y(j) |xj , θ) Figure 1: The “dependency model with valence” recursive equation. firsty (j) is a predicate defined to be true iff xj is the closest child (on either side) to its parent xi . The probability of the tree p(x, y |θ) = P (y(0) |$, θ). follow the general setting of Eq. 1, we index these distributions as θ 1 , ..., θ K . Headden et al. (2009) extended DMV so that the distributions θc condition on the valence as well, with smoothing, and showed significant improvements for short sentences. Our experiments found that these improvements do not hold on longer sentences. Here we experiment only with DMV, but note that our techniques are also applicable to richer probabilistic grammars like that of Headden et al. 2.2 Learning DMV Klein and Manning (2004) learned the DMV probabilities θ from a corpus of part-of-speech-tagged sentences using the EM algorithm. EM manipulates θ to locally optimize the likelihood of the observed portion of t"
N09-1009,N07-1018,0,0.152845,"Missing"
N09-1009,D07-1031,0,0.10414,"n an increased interest recently in employing Bayesian modeling for probabilistic grammars in different settings, ranging from putting priors over grammar probabilities (Johnson et al., 74 2007) to putting non-parametric priors over derivations (Johnson et al., 2006) to learning the set of states in a grammar (Finkel et al., 2007; Liang et al., 2007). Bayesian methods offer an elegant framework for combining prior knowledge with data. The main challenge in Bayesian grammar learning is efficiently approximating probabilistic inference, which is generally intractable. Most commonly variational (Johnson, 2007; Kurihara and Sato, 2006) or sampling techniques are applied (Johnson et al., 2006). Because probabilistic grammars are built out of multinomial distributions, the Dirichlet family (or, more precisely, a collection of Dirichlets) is a natural candidate for probabilistic grammars because of its conjugacy to the multinomial family. Conjugacy implies a clean form for the posterior distribution over grammar probabilities (given the data and the prior), bestowing computational tractability. Following work by Blei and Lafferty (2006) for topic models, Cohen et al. (2008) proposed an alternative to"
N09-1009,P04-1061,0,0.718056,"in the well-known monolingual setting and in a new bilingual setting where grammars for two languages are learned at once (without parallel corpora). Our method is based on a distribution which we call the shared logistic normal distribution, which is a distribution over a collection of multinomials from different probability simplexes. We provide a variational EM algorithm for inference. The rest of this paper is organized as follows. In §2, we give a brief explanation of probabilistic grammars and introduce some notation for the specific type of dependency grammar used in this paper, due to Klein and Manning (2004). In §3, we present our model and a variational inference algorithm for it. In §4, we report on experiments for both monolingual settings and a bilingual setting and discuss them. We discuss future work (§5) and conclude in §6. 2 Probabilistic Grammars and Dependency Grammar Induction A probabilistic grammar defines a probability distribution over grammatical derivations generated through a step-by-step process. HMMs, for example, can be understood as a random walk through a probabilistic finite-state network, with an output symbol sampled at each state. Each “step” of the walk and each symbol"
N09-1009,D07-1072,0,0.149029,"ion methods for learning—both from labeled and unlabeled data. Here we tackle the unsupervised grammar learning problem, specifically for unlexicalized context-free dependency grammars, using an empirical Bayesian approach with a novel family of priors. There has been an increased interest recently in employing Bayesian modeling for probabilistic grammars in different settings, ranging from putting priors over grammar probabilities (Johnson et al., 74 2007) to putting non-parametric priors over derivations (Johnson et al., 2006) to learning the set of states in a grammar (Finkel et al., 2007; Liang et al., 2007). Bayesian methods offer an elegant framework for combining prior knowledge with data. The main challenge in Bayesian grammar learning is efficiently approximating probabilistic inference, which is generally intractable. Most commonly variational (Johnson, 2007; Kurihara and Sato, 2006) or sampling techniques are applied (Johnson et al., 2006). Because probabilistic grammars are built out of multinomial distributions, the Dirichlet family (or, more precisely, a collection of Dirichlets) is a natural candidate for probabilistic grammars because of its conjugacy to the multinomial family. Conjug"
N09-1009,J93-2004,0,0.0378494,"Missing"
N09-1009,W04-3207,1,0.631421,"st performance with T IE V&N. Performance with Chinese is also the highest in the bilingual setting, with T IE A and T IE V&N. 5 In future work we plan to lexicalize the model, including a Bayesian grammar prior that accounts for the syntactic patterns of words. Nonparametric models (Teh, 2006) may be appropriate. We also believe that Bayesian discovery of cross-linguistic patterns is an exciting topic worthy of further exploration. 6 Bilingual Experiments Leveraging information from one language for the task of disambiguating another language has received considerable attention (Dagan, 1991; Smith and Smith, 2004; Snyder and Barzilay, 2008; Burkett and Klein, 2008). Usually such a setting requires a parallel corpus or other annotated data that ties between those two languages.5 Our bilingual experiments use the English and Chinese treebanks, which are not parallel corpora, to train parsers for both languages jointly. Shar5 Haghighi et al. (2008) presented a technique to learn bilingual lexicons from two non-parallel monolingual corpora. 81 Future Work Conclusion We described a Bayesian model that allows soft parameter tying among any weights in a probabilistic grammar. We used this model to improve un"
N09-1009,P08-1084,0,0.0387932,"E V&N. Performance with Chinese is also the highest in the bilingual setting, with T IE A and T IE V&N. 5 In future work we plan to lexicalize the model, including a Bayesian grammar prior that accounts for the syntactic patterns of words. Nonparametric models (Teh, 2006) may be appropriate. We also believe that Bayesian discovery of cross-linguistic patterns is an exciting topic worthy of further exploration. 6 Bilingual Experiments Leveraging information from one language for the task of disambiguating another language has received considerable attention (Dagan, 1991; Smith and Smith, 2004; Snyder and Barzilay, 2008; Burkett and Klein, 2008). Usually such a setting requires a parallel corpus or other annotated data that ties between those two languages.5 Our bilingual experiments use the English and Chinese treebanks, which are not parallel corpora, to train parsers for both languages jointly. Shar5 Haghighi et al. (2008) presented a technique to learn bilingual lexicons from two non-parallel monolingual corpora. 81 Future Work Conclusion We described a Bayesian model that allows soft parameter tying among any weights in a probabilistic grammar. We used this model to improve unsupervised parsing accuracy"
N09-1009,P06-1124,0,0.0398998,"T IE V, T IE N, T IE V&N, and T IE A. After running the inference algorithm which learns the two models jointly, we use unseen data to test each learned model separately. Table 1 includes the results for these experiments. The performance on English improved significantly in the bilingual setting, achieving highest performance with T IE V&N. Performance with Chinese is also the highest in the bilingual setting, with T IE A and T IE V&N. 5 In future work we plan to lexicalize the model, including a Bayesian grammar prior that accounts for the syntactic patterns of words. Nonparametric models (Teh, 2006) may be appropriate. We also believe that Bayesian discovery of cross-linguistic patterns is an exciting topic worthy of further exploration. 6 Bilingual Experiments Leveraging information from one language for the task of disambiguating another language has received considerable attention (Dagan, 1991; Smith and Smith, 2004; Snyder and Barzilay, 2008; Burkett and Klein, 2008). Usually such a setting requires a parallel corpus or other annotated data that ties between those two languages.5 Our bilingual experiments use the English and Chinese treebanks, which are not parallel corpora, to train"
N09-1009,D07-1003,1,0.272145,"lgorithm for learning a probabilistic grammar based on this family of priors. We then experiment with unsupervised dependency grammar induction and show significant improvements using our model for both monolingual learning and bilingual learning with a non-parallel, multilingual corpus. 1 Introduction Probabilistic grammars have become an important tool in natural language processing. They are most commonly used for parsing and linguistic analysis (Charniak and Johnson, 2005; Collins, 2003), but are now commonly seen in applications like machine translation (Wu, 1997) and question answering (Wang et al., 2007). An attractive property of probabilistic grammars is that they permit the use of well-understood parameter estimation methods for learning—both from labeled and unlabeled data. Here we tackle the unsupervised grammar learning problem, specifically for unlexicalized context-free dependency grammars, using an empirical Bayesian approach with a novel family of priors. There has been an increased interest recently in employing Bayesian modeling for probabilistic grammars in different settings, ranging from putting priors over grammar probabilities (Johnson et al., 74 2007) to putting non-parametr"
N09-1009,J97-3002,0,0.0962191,"ar. We describe a variational EM algorithm for learning a probabilistic grammar based on this family of priors. We then experiment with unsupervised dependency grammar induction and show significant improvements using our model for both monolingual learning and bilingual learning with a non-parallel, multilingual corpus. 1 Introduction Probabilistic grammars have become an important tool in natural language processing. They are most commonly used for parsing and linguistic analysis (Charniak and Johnson, 2005; Collins, 2003), but are now commonly seen in applications like machine translation (Wu, 1997) and question answering (Wang et al., 2007). An attractive property of probabilistic grammars is that they permit the use of well-understood parameter estimation methods for learning—both from labeled and unlabeled data. Here we tackle the unsupervised grammar learning problem, specifically for unlexicalized context-free dependency grammars, using an empirical Bayesian approach with a novel family of priors. There has been an increased interest recently in employing Bayesian modeling for probabilistic grammars in different settings, ranging from putting priors over grammar probabilities (Johns"
N09-1009,D08-1109,0,\N,Missing
N09-1027,P05-1033,0,0.717992,"le labeled derivation, reducing the fragmentation problem. Solving this problem exactly is still an NP-hard consensus problem, but we provide approximations that build on well-known PSCFG decoding methods. Our model falls somewhere between PSCFGs that extract nonterminal symbols from parse trees and treat them as part of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 236–244, c Boulder, Colorado, June 2009. 2009 Association for Computational Linguistics the derivation (Zollmann and Venugopal, 2006) and unlabeled hierarchical structures (Chiang, 2005); we treat nonterminal labels as random variables chosen at each node, with each (unlabeled) rule expressing “preferences” for particular nonterminal labels, learned from data. The paper is organized as follows. In Section 2, we summarize the use of PSCFG grammars for translation. We describe our model (Section 3). Section 4 explains the preference-related calculations, and Section 5 addresses decoding. Experimental results using preference grammars in a loglinear translation model are presented for two standard Chinese-to-English tasks in Section 6. We review related work (Section 7) and conc"
N09-1027,P07-1019,0,0.0212625,"h0 = VPi |r)u(VP) = (0.4 × 0.8) + (0.3 × 0.1) = 0.35 v˜(SBAR) = p(hh = SBAR, h0 = VPi |r)u(VP) = (0.2 × 0.1) = 0.02 v = hv(S) = 0.35/(˜ v (S) + v˜(SBAR)), v(SBAR) = 0.02/˜ v (S) + v˜(SBAR)i = hv(S) = 0.35/0.37, v(SBAR) = 0.02/0.37i φ2 = u(VB) + u(VP) = 0.8 + 0.1 = 0.9 Figure 1: Calculating v and φ2 for the running example. search space further. To prevent this partitioning, we follow the approach of Venugopal et al. (2007). We keep track of u for the best performing derivation from the set of derivations that share [X, i, j, q(α)] in a first-pass decoding. In a second top-down pass similar to Huang and Chiang (2007), we can recalculate psyn (d) for alternative derivations in the hypergraph; potentially correcting search errors made in the first pass. We face another significant practical challenge during decoding. In real data conditions, the size of the preference vector for a single rule can be very high, especially for rules that include multiple nonterminal symbols that are located on the left and right boundaries of γ. For example, the Chineseto-English rule X → h X1 „ X2 # X1 ’s X2 i has over 24K elements in hargs(r) when learned for the medium-sized NIST task used below. In order to limit the expl"
N09-1027,J99-4005,0,0.0433364,"ules. A PSCFG derivation is a synchronous parse tree. Defining the translation function as finding the best derivation has the unfortunate side effect of forcing differently-derived versions of the same target sentence to compete with each other. In other words, the true score of each translation is “fragmented” across many derivations, so that each translation’s most probable derivation is the only one that matters. The more Bayesian approach of finding the most probable translation (integrating out the derivations) instantiates an NP-hard inference problem even for simple word-based models (Knight, 1999); for grammar-based translation it is known as the consensus problem (Casacuberta and de la Higuera, 2000; Sima’an, 2002). With weights interpreted as probabilities, the maximum-weighted derivation is the maximum a posteriori (MAP) derivation: We propose a novel probabilistic synchoronous context-free grammar formalism for statistical machine translation, in which syntactic nonterminal labels are represented as “soft” preferences rather than as “hard” matching constraints. This formalism allows us to efficiently score unlabeled synchronous derivations without forgoing traditional syntactic con"
N09-1027,N04-1022,0,0.0238305,"ating the selection of the most likely unlabeled derivation during search, rather than as a post-processing operation; the methods described above might improve this approximation, at some computational expense. Related Work There have been significant efforts in the both the monolingual parsing and machine translation literature to address the impact of the MAP approximation and the choice of labels in their respective models; we survey the work most closely related to our approach. May and Knight (2006) extract nbest lists containing unique translations rather than unique derivations, while Kumar and Byrne (2004) use the Minimum Bayes Risk decision rule to select the lowest risk (highest BLEU score) translation rather than derivation from an n-best list. Tromble et al. (2008) extend this work to lattice structures. All of these approaches only marginalize over alternative candidate derivations generated by a MAPdriven decoding process. More recently, work by Blunsom et al. (2007) propose a purely discriminative model whose decoding step approximates the selection of the most likely translation via beam search. Matsusaki et al. (2005) and Petrov et al. (2006) propose automatically learning annotations"
N09-1027,P05-1010,0,0.0447155,"ith the arguments of this higherup rule. We design psyn (d) to reflect compatibility between two rules (one expanding a right-hand side nonterminal in the other), based on label preference distributions. 3.2 Formal definition Probabilistic synchronous context-free preference grammars are defined as PSCFGs with the following additional elements: • H: a set of implicit labels, not to be confused 239 with the explicit label set N . • π: H → N , a function that associates each implicit label with a single explicit label. We can therefore think of H symbols as refinements of the nonterminals in N (Matsusaki et al., 2005). • For each rule r, we define a probability distribution over vectors ~h of implicit label bindings for its nonterminals, denoted ppref (~h |r). ~h includes bindings for the left-hand side nonterminal (h0 ) as well as each right-hand side nonterminal (h1 , ..., h|~h |). Each hi ∈ H. When N , H are defined to include just a single generic symbol as in (Chiang, 2005), we produce the unlabeled grammar discussed above. In this work, we define • N = {S, X} • H = {NP, DT, NN · · · } = NSAMT where N corresponds to the generic labels of Chiang (2005) and H corresponds to the syntactically motivated S"
N09-1027,N06-1045,0,0.026388,"tree rather than simply stripping annotations from the MAP annotated tree. In our work, we focused on approximating the selection of the most likely unlabeled derivation during search, rather than as a post-processing operation; the methods described above might improve this approximation, at some computational expense. Related Work There have been significant efforts in the both the monolingual parsing and machine translation literature to address the impact of the MAP approximation and the choice of labels in their respective models; we survey the work most closely related to our approach. May and Knight (2006) extract nbest lists containing unique translations rather than unique derivations, while Kumar and Byrne (2004) use the Minimum Bayes Risk decision rule to select the lowest risk (highest BLEU score) translation rather than derivation from an n-best list. Tromble et al. (2008) extend this work to lattice structures. All of these approaches only marginalize over alternative candidate derivations generated by a MAPdriven decoding process. More recently, work by Blunsom et al. (2007) propose a purely discriminative model whose decoding step approximates the selection of the most likely translati"
N09-1027,P03-1021,0,0.0280794,"(redundantly) requires every nonterminal token to be expanded by a rule with that nonterminal on its left-hand side. freq(r; d) denotes the frequency of the rule r in the derivation d. Note that λm+1 can be effectively ignored when psyn is defined as in Equation 3. Z(~λ) is a normalization constant that does not need to be computed during search under the argmax search criterion in Equation 1. Feature weights ~λ are trained discriminatively in concert with the language model weight to maximize the BLEU (Papineni et al., 2002) automatic evaluation metric via Minimum Error Rate Training (MERT) (Och, 2003). We use the open-source PSCFG rule extraction framework and decoder from Zollmann et al. (2008) as the framework for our experiments. The asymptotic runtime of this decoder is:  h iK  3 2(n−1) O |f ||N ||TT | (4) where K is the maximum number of nonterminal symbols per rule, |f |the source sentence length, and n is the order of the n-gram LM that is used to compute pLM . This constant factor in Equation 4 arises from the dynamic programming item structure used to perform search under this model. Using notation from Chiang (2007), the corresponding item structure is: [X, i, j, q(α)] : w (5)"
N09-1027,P02-1040,0,0.105687,"uence, a collection of m rule feature functions hi : R → R≥0 , and a “syntax” feature that (redundantly) requires every nonterminal token to be expanded by a rule with that nonterminal on its left-hand side. freq(r; d) denotes the frequency of the rule r in the derivation d. Note that λm+1 can be effectively ignored when psyn is defined as in Equation 3. Z(~λ) is a normalization constant that does not need to be computed during search under the argmax search criterion in Equation 1. Feature weights ~λ are trained discriminatively in concert with the language model weight to maximize the BLEU (Papineni et al., 2002) automatic evaluation metric via Minimum Error Rate Training (MERT) (Och, 2003). We use the open-source PSCFG rule extraction framework and decoder from Zollmann et al. (2008) as the framework for our experiments. The asymptotic runtime of this decoder is:  h iK  3 2(n−1) O |f ||N ||TT | (4) where K is the maximum number of nonterminal symbols per rule, |f |the source sentence length, and n is the order of the n-gram LM that is used to compute pLM . This constant factor in Equation 4 arises from the dynamic programming item structure used to perform search under this model. Using notation fr"
N09-1027,P06-1055,0,0.0156806,"ons rather than unique derivations, while Kumar and Byrne (2004) use the Minimum Bayes Risk decision rule to select the lowest risk (highest BLEU score) translation rather than derivation from an n-best list. Tromble et al. (2008) extend this work to lattice structures. All of these approaches only marginalize over alternative candidate derivations generated by a MAPdriven decoding process. More recently, work by Blunsom et al. (2007) propose a purely discriminative model whose decoding step approximates the selection of the most likely translation via beam search. Matsusaki et al. (2005) and Petrov et al. (2006) propose automatically learning annotations that add information to categories to improve monolingual parsing quality. Since the parsing task requires selecting the most non-annotated tree, the an243 Conclusions and Future Work We have proposed a novel grammar formalism that replaces hard syntactic constraints with “soft” preferences. These preferences are used to compute a machine translation feature (psyn (d)) that scores unlabeled derivations, taking into account traditional syntactic constraints. Representing syntactic constraints as a feature allows MERT to train the corresponding weight"
N09-1027,D08-1065,0,0.0118051,"pproximation, at some computational expense. Related Work There have been significant efforts in the both the monolingual parsing and machine translation literature to address the impact of the MAP approximation and the choice of labels in their respective models; we survey the work most closely related to our approach. May and Knight (2006) extract nbest lists containing unique translations rather than unique derivations, while Kumar and Byrne (2004) use the Minimum Bayes Risk decision rule to select the lowest risk (highest BLEU score) translation rather than derivation from an n-best list. Tromble et al. (2008) extend this work to lattice structures. All of these approaches only marginalize over alternative candidate derivations generated by a MAPdriven decoding process. More recently, work by Blunsom et al. (2007) propose a purely discriminative model whose decoding step approximates the selection of the most likely translation via beam search. Matsusaki et al. (2005) and Petrov et al. (2006) propose automatically learning annotations that add information to categories to improve monolingual parsing quality. Since the parsing task requires selecting the most non-annotated tree, the an243 Conclusion"
N09-1027,N07-1063,1,0.86028,"over the longer sequences in hargs(r) and include appropriate values from the additional “child” items’ preference vectors in the product. v˜(S) = ppref (hh = S, h0 = VBi |r)u(VB) + ppref (hh = S, h0 = VPi |r)u(VP) = (0.4 × 0.8) + (0.3 × 0.1) = 0.35 v˜(SBAR) = p(hh = SBAR, h0 = VPi |r)u(VP) = (0.2 × 0.1) = 0.02 v = hv(S) = 0.35/(˜ v (S) + v˜(SBAR)), v(SBAR) = 0.02/˜ v (S) + v˜(SBAR)i = hv(S) = 0.35/0.37, v(SBAR) = 0.02/0.37i φ2 = u(VB) + u(VP) = 0.8 + 0.1 = 0.9 Figure 1: Calculating v and φ2 for the running example. search space further. To prevent this partitioning, we follow the approach of Venugopal et al. (2007). We keep track of u for the best performing derivation from the set of derivations that share [X, i, j, q(α)] in a first-pass decoding. In a second top-down pass similar to Huang and Chiang (2007), we can recalculate psyn (d) for alternative derivations in the hypergraph; potentially correcting search errors made in the first pass. We face another significant practical challenge during decoding. In real data conditions, the size of the preference vector for a single rule can be very high, especially for rules that include multiple nonterminal symbols that are located on the left and right bou"
N09-1027,W06-3119,1,0.416619,"obable equivalence class of unlabeled derivations, rather than a single labeled derivation, reducing the fragmentation problem. Solving this problem exactly is still an NP-hard consensus problem, but we provide approximations that build on well-known PSCFG decoding methods. Our model falls somewhere between PSCFGs that extract nonterminal symbols from parse trees and treat them as part of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 236–244, c Boulder, Colorado, June 2009. 2009 Association for Computational Linguistics the derivation (Zollmann and Venugopal, 2006) and unlabeled hierarchical structures (Chiang, 2005); we treat nonterminal labels as random variables chosen at each node, with each (unlabeled) rule expressing “preferences” for particular nonterminal labels, learned from data. The paper is organized as follows. In Section 2, we summarize the use of PSCFG grammars for translation. We describe our model (Section 3). Section 4 explains the preference-related calculations, and Section 5 addresses decoding. Experimental results using preference grammars in a loglinear translation model are presented for two standard Chinese-to-English tasks in S"
N09-1027,C08-1144,1,0.729917,"nterminal on its left-hand side. freq(r; d) denotes the frequency of the rule r in the derivation d. Note that λm+1 can be effectively ignored when psyn is defined as in Equation 3. Z(~λ) is a normalization constant that does not need to be computed during search under the argmax search criterion in Equation 1. Feature weights ~λ are trained discriminatively in concert with the language model weight to maximize the BLEU (Papineni et al., 2002) automatic evaluation metric via Minimum Error Rate Training (MERT) (Och, 2003). We use the open-source PSCFG rule extraction framework and decoder from Zollmann et al. (2008) as the framework for our experiments. The asymptotic runtime of this decoder is:  h iK  3 2(n−1) O |f ||N ||TT | (4) where K is the maximum number of nonterminal symbols per rule, |f |the source sentence length, and n is the order of the n-gram LM that is used to compute pLM . This constant factor in Equation 4 arises from the dynamic programming item structure used to perform search under this model. Using notation from Chiang (2007), the corresponding item structure is: [X, i, j, q(α)] : w (5) (4) (3) (2) (1) where X is the nonterminal label of a derivation, i, j define a span in the sour"
N09-1027,P06-1121,0,\N,Missing
N09-1027,2006.iwslt-evaluation.1,0,\N,Missing
N09-1027,J07-2003,0,\N,Missing
N09-1027,P08-1024,0,\N,Missing
N09-1031,P07-1038,0,0.00982395,"with delisting. 7 Related Work In NLP, regression is not widely used, since most natural language-related data are discrete. Regression methods were pioneered by Yang and Chute (1992) and Yang and Chute (1993) for information retrieval purposes, but the predicted continuous variable was not an end in itself in that work. Blei and McAuliffe (2007) used latent “topic” variables to predict movie reviews and popularity from text. Lavrenko et al. (2000b) and Lavrenko et al. (2000a) modeled influences between text and time series financial data (stock prices) using language models. Farther afield, Albrecht and Hwa (2007) used SVR to train machine translation evaluation metrics to match human evaluation scores and compared techniques using correlation. Regression has also been used to order sentences in extractive summarization (Biadsy et al., 2008). While much of the information relevant for investors is communicated through text (rather than numbers), only recently is this link explored. Some papers relate news articles to earning forecasts, stock returns, volatility, and volume (Koppel and Shtrimberg, 2004; Tetlock, 2007; Tetlock et al., 2008; Gaa, 2007; Engelberg, 2007). Das and Chen (2001) and Antweiler a"
N09-1031,P08-1092,0,0.0229541,"l purposes, but the predicted continuous variable was not an end in itself in that work. Blei and McAuliffe (2007) used latent “topic” variables to predict movie reviews and popularity from text. Lavrenko et al. (2000b) and Lavrenko et al. (2000a) modeled influences between text and time series financial data (stock prices) using language models. Farther afield, Albrecht and Hwa (2007) used SVR to train machine translation evaluation metrics to match human evaluation scores and compared techniques using correlation. Regression has also been used to order sentences in extractive summarization (Biadsy et al., 2008). While much of the information relevant for investors is communicated through text (rather than numbers), only recently is this link explored. Some papers relate news articles to earning forecasts, stock returns, volatility, and volume (Koppel and Shtrimberg, 2004; Tetlock, 2007; Tetlock et al., 2008; Gaa, 2007; Engelberg, 2007). Das and Chen (2001) and Antweiler and Frank (2004) ask whether messages posted on message boards can help explain stock performance, while Li (2005) measures the association between frequency of words associated with risk and subsequent stock returns. Weiss-Hanley an"
N09-1031,C94-2174,0,0.0162875,"Hoberg (2008) study initial public offering disclosures using word statistics. Many researchers have focused the related problem of predicting sentiment 279 and opinion in text (Pang et al., 2002; Wiebe and Riloff, 2005), sometimes connected to extrinsic values like prediction markets (Lerman et al., 2008). In contrast to text regression, text classification comprises a widely studied set of problems involving the prediction of categorial variables related to text. Applications have included the categorization of documents by topic (Joachims, 1998), language (Cavnar and Trenkle, 1994), genre (Karlgren and Cutting, 1994), author (Bosch and Smith, 1998), sentiment (Pang et al., 2002), and desirability (Sahami et al., 1998). Text categorization has served as a test application for nearly every machine learning technique for discrete classification. 8 Conclusion We have introduced and motivated a new kind of task for NLP: text regression, in which text is used to make predictions about measurable phenomena in the real world. We applied the technique to predicting financial volatility from companies’ 10-K reports, and found text regression model predictions to correlate with true volatility nearly as well as hist"
N09-1031,C08-1060,0,0.0289394,"et al., 2008; Gaa, 2007; Engelberg, 2007). Das and Chen (2001) and Antweiler and Frank (2004) ask whether messages posted on message boards can help explain stock performance, while Li (2005) measures the association between frequency of words associated with risk and subsequent stock returns. Weiss-Hanley and Hoberg (2008) study initial public offering disclosures using word statistics. Many researchers have focused the related problem of predicting sentiment 279 and opinion in text (Pang et al., 2002; Wiebe and Riloff, 2005), sometimes connected to extrinsic values like prediction markets (Lerman et al., 2008). In contrast to text regression, text classification comprises a widely studied set of problems involving the prediction of categorial variables related to text. Applications have included the categorization of documents by topic (Joachims, 1998), language (Cavnar and Trenkle, 1994), genre (Karlgren and Cutting, 1994), author (Bosch and Smith, 1998), sentiment (Pang et al., 2002), and desirability (Sahami et al., 1998). Text categorization has served as a test application for nearly every machine learning technique for discrete classification. 8 Conclusion We have introduced and motivated a n"
N09-1031,W02-1011,0,0.0208567,"earning forecasts, stock returns, volatility, and volume (Koppel and Shtrimberg, 2004; Tetlock, 2007; Tetlock et al., 2008; Gaa, 2007; Engelberg, 2007). Das and Chen (2001) and Antweiler and Frank (2004) ask whether messages posted on message boards can help explain stock performance, while Li (2005) measures the association between frequency of words associated with risk and subsequent stock returns. Weiss-Hanley and Hoberg (2008) study initial public offering disclosures using word statistics. Many researchers have focused the related problem of predicting sentiment 279 and opinion in text (Pang et al., 2002; Wiebe and Riloff, 2005), sometimes connected to extrinsic values like prediction markets (Lerman et al., 2008). In contrast to text regression, text classification comprises a widely studied set of problems involving the prediction of categorial variables related to text. Applications have included the categorization of documents by topic (Joachims, 1998), language (Cavnar and Trenkle, 1994), genre (Karlgren and Cutting, 1994), author (Bosch and Smith, 1998), sentiment (Pang et al., 2002), and desirability (Sahami et al., 1998). Text categorization has served as a test application for nearly"
N09-1031,C92-2069,0,0.17057,"Missing"
N09-1054,P08-1031,0,0.0234631,"patterns of “topics” within a text collection. LDA was introduced by Blei et al. (2003) and has been especially popular because it can be understood as a generative model and because it discovers understandable topics in many scenarios (Steyvers and Griffiths, 2007). Its declarative specification makes it easy to extend for new kinds of text collections. The technique has been applied to Web document collections, notably for community discovery in social networks (Zhang et al., 2007), opinion mining in user reviews (Titov and McDonald, 2008), and sentiment discovery in free-text annotations (Branavan et al., 2008). Dredze et al. (2008) applied LDA to a collection of email for summary keyword extraction. The authors evaluated the model with proxy tasks such as recipient prediction. More closely related to the data considered in this work, Lin et al. (2008) applied a variation of LDA to ideological discourse. A notable trend in the recent research is to augment the models to describe non-textual evidence alongside the document collection. Several such studies are especially relevant to our work. Blei and Jordan (2003) were one of the earliest results in this trend. The concept was developed into more gen"
N09-1054,P08-1036,0,0.0273657,"ed text analysis tool. The basic aim of those models is to discover recurring patterns of “topics” within a text collection. LDA was introduced by Blei et al. (2003) and has been especially popular because it can be understood as a generative model and because it discovers understandable topics in many scenarios (Steyvers and Griffiths, 2007). Its declarative specification makes it easy to extend for new kinds of text collections. The technique has been applied to Web document collections, notably for community discovery in social networks (Zhang et al., 2007), opinion mining in user reviews (Titov and McDonald, 2008), and sentiment discovery in free-text annotations (Branavan et al., 2008). Dredze et al. (2008) applied LDA to a collection of email for summary keyword extraction. The authors evaluated the model with proxy tasks such as recipient prediction. More closely related to the data considered in this work, Lin et al. (2008) applied a variation of LDA to ideological discourse. A notable trend in the recent research is to augment the models to describe non-textual evidence alongside the document collection. Several such studies are especially relevant to our work. Blei and Jordan (2003) were one of t"
N09-1054,P08-1000,0,\N,Missing
N10-1038,P07-1053,0,0.00905126,"ge all of this work has used polarity of the review or the number of stars given to it by a critic, rather than the review text directly (Terry et al., 2005). Our task is related to sentiment analysis (Pang et al., 2002) on movie reviews. The key difference is that our goal is to predict a future real-valued quantity, restricting us from using any post-release text data such as user reviews. Further, the most important clues about revenue may have little to do with whether the reviewer liked the movie, but rather what the reviewer found worth mentioning. This paper is more in the tradition of Ghose et al. (2007) and Kogan et al. (2009), who used text regression to directly quantify review “value” and make predictions about future financial variables, respectively. Our aim in using the full text is to identify particular words and phrases that predict the movie-going tendencies of the public. We can also perform syntactic and semantic analysis on the text to identify richer constructions that are good predictors. Furthermore, since we consider multiple reviews for each movie, we can compare these features across reviews to observe how they differ both in frequency and predictive performance across dif"
N10-1038,N09-1031,1,0.594354,"used polarity of the review or the number of stars given to it by a critic, rather than the review text directly (Terry et al., 2005). Our task is related to sentiment analysis (Pang et al., 2002) on movie reviews. The key difference is that our goal is to predict a future real-valued quantity, restricting us from using any post-release text data such as user reviews. Further, the most important clues about revenue may have little to do with whether the reviewer liked the movie, but rather what the reviewer found worth mentioning. This paper is more in the tradition of Ghose et al. (2007) and Kogan et al. (2009), who used text regression to directly quantify review “value” and make predictions about future financial variables, respectively. Our aim in using the full text is to identify particular words and phrases that predict the movie-going tendencies of the public. We can also perform syntactic and semantic analysis on the text to identify richer constructions that are good predictors. Furthermore, since we consider multiple reviews for each movie, we can compare these features across reviews to observe how they differ both in frequency and predictive performance across different media outlets and"
N10-1038,W02-1011,0,0.0198514,"did not frame the task as a revenue prediction problem.) Zhang and Skiena (2009) used a news aggregation system to identify entities and obtain domain-specific sentiment for each entity in several domains. They used the aggregate sentiment scores and mention counts of each movie in news articles as predictors. While there has been substantial prior work on using critics’ reviews, to our knowledge all of this work has used polarity of the review or the number of stars given to it by a critic, rather than the review text directly (Terry et al., 2005). Our task is related to sentiment analysis (Pang et al., 2002) on movie reviews. The key difference is that our goal is to predict a future real-valued quantity, restricting us from using any post-release text data such as user reviews. Further, the most important clues about revenue may have little to do with whether the reviewer liked the movie, but rather what the reviewer found worth mentioning. This paper is more in the tradition of Ghose et al. (2007) and Kogan et al. (2009), who used text regression to directly quantify review “value” and make predictions about future financial variables, respectively. Our aim in using the full text is to identify"
N10-1038,W00-1308,0,0.035303,"o our pool of features in the following order: whether the 4.2 Text Features We extract three types of text features (described below). We only included feature instances that occurred in at least five different movies’ reviews. We stem and downcase individual word components in all our features. I. n-grams. We considered unigrams, bigrams, and trigrams. A 25-word stoplist was used; bigrams and trigrams were only filtered if all words were stopwords. II. Part-of-speech n-grams. As with words, we added unigrams, bigrams, and trigrams. Tags were obtained from the Stanford part-of-speech tagger (Toutanova and Manning, 2000). III. Dependency relations. We used the Stanford parser (Klein and Manning, 2003) to parse the critic reviews and extract syntactic dependencies. The dependency relation features consist of just the relation part of a dependency triple hrelation, head word, modifier wordi. We consider three ways to combine the collection of reviews for a given movie. The first (“−”) simply concatenates all of a movie’s reviews into a single document before extracting features. The second (“+”) conjoins each feature with the source site (e.g., New York Times) from whose review it was extracted. A third version"
N10-1081,N09-1009,1,0.0984104,"ng step (described in detail in §3.3) that defines, for each A ∈ M, a list of strings sA = hsA,1 , . . . , sA,NA i. Then, for q(zA,i |φA ) we use the grammaton G(A, sA,i ) and for q(zi |φ) we use the grammaton G(A, xi ) where xi is the ith observed sentence. We parametrize the grammaton with weights φA (or φ) for each rule in the grammaton. This makes the variational distributions over the trees for strings s (and trees for x) globally normalized weighted grammars. Choosing such distributions is motivated by their ability to make the variational bound tight (similar to Cohen et al., 2008, and Cohen and Smith, 2009). In practice we do not have to use rewrite rules for all strings t ⊆ s in the grammaton. It suffices to add rewrite rules only for the strings t = sA,i that have some grammaton attached q(v, θ, z) = (2) to them, G(A, s ). A,i ! NA n Y Y Y The variational distribution above yields a variq(θ A ) q(vA,i ) × q(zA,i ) × q(zi ) ational inference algorithm for approximating the i=1 i=1 A∈M posterior by estimating γ A,i , τ A , φA and φ itIt is natural to define the variational distributions eratively, given a fixed set of hyperparameters over θ and v to be Dirichlet distributions with pa- a, b and α"
N10-1081,N09-1019,0,0.00801963,"onds; (ii) each iteration took approximately 204 seconds, with convergence after 40 iterations, leading to 8,160 seconds of pure varia6 We used the code and data available at http://www. cog.brown.edu/˜mj/Software.htm. The machine used for this comparison is a 64-bit machine with 2.6GHz CPU, 4MB of cache memory and 8GB of RAM. 4.2 Dependency Grammar Induction We conclude our experiments with preliminary results for unsupervised syntax learning. This is a new application of adaptor grammars, which have so far been used in segmentation (Johnson and Goldwater, 2009) and named entity recognition (Elsner et al., 2009). The grammar we use is the dependency model with valence (DMV Klein and Manning, 2004) represented as a probabilistic context-free grammar, GDMV (Smith, 2006). We note that GDMV is recursive; this is not a problem (§3.1). We used part-of-speech sequences from the Wall Street Journal Penn Treebank (Marcus et al., 1993), stripped of words and punctuation. We follow standard parsing conventions and train on sections 2– 21 and test on section 23 (while using sentences of length 10 or less). Because of the unsupervised nature of the problem, we report results on the training set, in addition to th"
N10-1081,P07-1094,0,0.00950445,"ethods. To derive this method, we develop a stick-breaking representation of adaptor grammars, a representation that enables us to define adaptor grammars with recursion. We report experimental results on a word segmentation task, showing that variational inference performs comparably to MCMC. Further, we show a significant speed-up when parallelizing the algorithm. Finally, we report promising results for a new application for adaptor grammars, dependency grammar induction. 1 Introduction Recent research in unsupervised learning for NLP focuses on Bayesian methods for probabilistic grammars (Goldwater and Griffiths, 2007; Toutanova and Johnson, 2007; Johnson et al., 2007). Such methods have been made more flexible with nonparametric Bayesian (NP Bayes) methods, such as Dirichlet process mixture models (Antoniak, 1974; Pitman, 2002). One line of research uses NP Bayes methods on whole tree structures, in the form of adaptor grammars (Johnson et al., 2006; Johnson, 2008b; Johnson, 2008a; Johnson and Goldwater, 2009), in order to identify recurrent subtree patterns. Adaptor grammars provide a flexible distribution over parse trees that has more structure than a traditional context-free grammar. Adaptor grammars"
N10-1081,P96-1024,0,0.027574,"gs in the sorted list for the grammatons of A.5 3.4 Decoding The variational inference algorithm gives a distributions over parameters and hidden structures (through the grammatons). We experiment with two commonly used decoding methods: Viterbi decoding 5 The requirement to select NA in advance is strict. We experimented with dynamic expansions of the stick, in the spirit of Kurihara et al. (2006) and Wang and Blei (2009), but we did not achieve better performance and it had an adverse effect on runtime. For completeness, we give these results in §4. 569 and minimum Bayes risk decoding (MBR; Goodman, 1996). To parse a string with Viterbi (or MBR) decoding, we find the tree with highest score for the grammaton which is attached to that string. For all rules which rewrite to strings in the resulting tree, we again perform Viterbi (or MBR) decoding recursively using other grammatons. 4 Experiments We describe experiments with variational inference for adaptor grammars for word segmentation and dependency grammar induction. 4.1 Word Segmentation We follow the experimental setting of Johnson and Goldwater (2009), who present state-of-the-art results for inference with adaptor grammars using Gibbs sa"
N10-1081,N09-1012,0,0.0127891,"Missing"
N10-1081,N09-1036,0,0.566168,"lts for a new application for adaptor grammars, dependency grammar induction. 1 Introduction Recent research in unsupervised learning for NLP focuses on Bayesian methods for probabilistic grammars (Goldwater and Griffiths, 2007; Toutanova and Johnson, 2007; Johnson et al., 2007). Such methods have been made more flexible with nonparametric Bayesian (NP Bayes) methods, such as Dirichlet process mixture models (Antoniak, 1974; Pitman, 2002). One line of research uses NP Bayes methods on whole tree structures, in the form of adaptor grammars (Johnson et al., 2006; Johnson, 2008b; Johnson, 2008a; Johnson and Goldwater, 2009), in order to identify recurrent subtree patterns. Adaptor grammars provide a flexible distribution over parse trees that has more structure than a traditional context-free grammar. Adaptor grammars are used via posterior inference, the computational problem of determining the posterior distribution of parse trees given a set of observed sentences. Current posterior inference algorithms for adaptor grammars are based on MCMC sampling methods (Robert and Casella, 2005). MCMC methods are theoretically guaranteed to converge to the true posterior, but come at great expense: they are notoriously s"
N10-1081,N07-1018,0,0.0543151,"epresentation of adaptor grammars, a representation that enables us to define adaptor grammars with recursion. We report experimental results on a word segmentation task, showing that variational inference performs comparably to MCMC. Further, we show a significant speed-up when parallelizing the algorithm. Finally, we report promising results for a new application for adaptor grammars, dependency grammar induction. 1 Introduction Recent research in unsupervised learning for NLP focuses on Bayesian methods for probabilistic grammars (Goldwater and Griffiths, 2007; Toutanova and Johnson, 2007; Johnson et al., 2007). Such methods have been made more flexible with nonparametric Bayesian (NP Bayes) methods, such as Dirichlet process mixture models (Antoniak, 1974; Pitman, 2002). One line of research uses NP Bayes methods on whole tree structures, in the form of adaptor grammars (Johnson et al., 2006; Johnson, 2008b; Johnson, 2008a; Johnson and Goldwater, 2009), in order to identify recurrent subtree patterns. Adaptor grammars provide a flexible distribution over parse trees that has more structure than a traditional context-free grammar. Adaptor grammars are used via posterior inference, the computational"
N10-1081,W08-0704,0,0.0142245,"inally, we report promising results for a new application for adaptor grammars, dependency grammar induction. 1 Introduction Recent research in unsupervised learning for NLP focuses on Bayesian methods for probabilistic grammars (Goldwater and Griffiths, 2007; Toutanova and Johnson, 2007; Johnson et al., 2007). Such methods have been made more flexible with nonparametric Bayesian (NP Bayes) methods, such as Dirichlet process mixture models (Antoniak, 1974; Pitman, 2002). One line of research uses NP Bayes methods on whole tree structures, in the form of adaptor grammars (Johnson et al., 2006; Johnson, 2008b; Johnson, 2008a; Johnson and Goldwater, 2009), in order to identify recurrent subtree patterns. Adaptor grammars provide a flexible distribution over parse trees that has more structure than a traditional context-free grammar. Adaptor grammars are used via posterior inference, the computational problem of determining the posterior distribution of parse trees given a set of observed sentences. Current posterior inference algorithms for adaptor grammars are based on MCMC sampling methods (Robert and Casella, 2005). MCMC methods are theoretically guaranteed to converge to the true posterior, bu"
N10-1081,P08-1046,0,0.127259,"inally, we report promising results for a new application for adaptor grammars, dependency grammar induction. 1 Introduction Recent research in unsupervised learning for NLP focuses on Bayesian methods for probabilistic grammars (Goldwater and Griffiths, 2007; Toutanova and Johnson, 2007; Johnson et al., 2007). Such methods have been made more flexible with nonparametric Bayesian (NP Bayes) methods, such as Dirichlet process mixture models (Antoniak, 1974; Pitman, 2002). One line of research uses NP Bayes methods on whole tree structures, in the form of adaptor grammars (Johnson et al., 2006; Johnson, 2008b; Johnson, 2008a; Johnson and Goldwater, 2009), in order to identify recurrent subtree patterns. Adaptor grammars provide a flexible distribution over parse trees that has more structure than a traditional context-free grammar. Adaptor grammars are used via posterior inference, the computational problem of determining the posterior distribution of parse trees given a set of observed sentences. Current posterior inference algorithms for adaptor grammars are based on MCMC sampling methods (Robert and Casella, 2005). MCMC methods are theoretically guaranteed to converge to the true posterior, bu"
N10-1081,P04-1061,0,0.00705163,"iterations, leading to 8,160 seconds of pure varia6 We used the code and data available at http://www. cog.brown.edu/˜mj/Software.htm. The machine used for this comparison is a 64-bit machine with 2.6GHz CPU, 4MB of cache memory and 8GB of RAM. 4.2 Dependency Grammar Induction We conclude our experiments with preliminary results for unsupervised syntax learning. This is a new application of adaptor grammars, which have so far been used in segmentation (Johnson and Goldwater, 2009) and named entity recognition (Elsner et al., 2009). The grammar we use is the dependency model with valence (DMV Klein and Manning, 2004) represented as a probabilistic context-free grammar, GDMV (Smith, 2006). We note that GDMV is recursive; this is not a problem (§3.1). We used part-of-speech sequences from the Wall Street Journal Penn Treebank (Marcus et al., 1993), stripped of words and punctuation. We follow standard parsing conventions and train on sections 2– 21 and test on section 23 (while using sentences of length 10 or less). Because of the unsupervised nature of the problem, we report results on the training set, in addition to the test set. The nonterminals that we adapted correspond to nonterminals that define nou"
N10-1081,D07-1072,0,0.042418,"inference algorithms for adaptor grammars are based on MCMC sampling methods (Robert and Casella, 2005). MCMC methods are theoretically guaranteed to converge to the true posterior, but come at great expense: they are notoriously slow to converge, especially with complex hidden structures such as syntactic trees. Johnson (2008b) comments on this, and suggests the use of variational inference as a possible remedy. Variational inference provides a deterministic alternative to sampling. It was introduced for Dirichlet process mixtures by Blei and Jordan (2005) and applied to infinite grammars by Liang et al. (2007). With NP Bayes models, variational methods are based on the stick-breaking representation (Sethuraman, 1994). Devising a stick-breaking representation is a central challenge to using variational inference in this setting. The rest of this paper is organized as follows. In §2 we describe a stick-breaking representation of adaptor grammars, which enables variational inference (§3) and a well-defined incorporation of recursion into adaptor grammars. In §4 we give an empirical comparison of the algorithm to MCMC inference and describe a novel application of adaptor grammars to unsupervised depend"
N10-1081,J93-2004,0,0.0334897,"of RAM. 4.2 Dependency Grammar Induction We conclude our experiments with preliminary results for unsupervised syntax learning. This is a new application of adaptor grammars, which have so far been used in segmentation (Johnson and Goldwater, 2009) and named entity recognition (Elsner et al., 2009). The grammar we use is the dependency model with valence (DMV Klein and Manning, 2004) represented as a probabilistic context-free grammar, GDMV (Smith, 2006). We note that GDMV is recursive; this is not a problem (§3.1). We used part-of-speech sequences from the Wall Street Journal Penn Treebank (Marcus et al., 1993), stripped of words and punctuation. We follow standard parsing conventions and train on sections 2– 21 and test on section 23 (while using sentences of length 10 or less). Because of the unsupervised nature of the problem, we report results on the training set, in addition to the test set. The nonterminals that we adapted correspond to nonterminals that define noun constituents. We then use the preprocessing step defined in §3.3 with a uniform grammar and take the top 3,000 strings for each nonterminal of a noun constituent. The results are in Table 4.2. We report attachment accuracy, the fra"
N10-1081,P09-1012,0,0.0327989,"he strings generated by the adaptor grammars that yields an accurate variational estimation. We begin with a weighted context-free grammar Gheur that has the same rules as in G, only the weight for all of its rules is 1. We then compute the quantity: ! n 1 X c(A, s) = EGheur [fi (z; A, s)] − ρ log |s| n i=1 (3) where fi (z; A, s) is a function computing the count of constituents headed by A with yield s in the tree z for the sentence xi . This quantity can be computed by using the IO algorithm on Gheur . The term ρ log |s |is subtracted to avoid preference for shorter constituents, similar to Mochihashi et al. (2009). While computing c(A, s) using the IO algorithm, we sort the set of all substrings of s according to their expected counts (aggregated over all strings s). Then, we use the top NA strings in the sorted list for the grammatons of A.5 3.4 Decoding The variational inference algorithm gives a distributions over parameters and hidden structures (through the grammatons). We experiment with two commonly used decoding methods: Viterbi decoding 5 The requirement to select NA in advance is strict. We experimented with dynamic expansions of the stick, in the spirit of Kurihara et al. (2006) and Wang and"
N10-1086,J90-2002,0,0.0678144,"utomatically generates questions from texts to predict which user-generated questions the text might answer. In such work on question answering, question generation models are typically not evaluated for their intrinsic quality, but rather with respect to their utility as an intermediate step in the question answering process. QG is very different from many natural language generation problems because the input is natural language rather than a formal representation (cf. Reiter and Dale, 1997). It is also different from some other tasks related to generation: unlike machine translation (e.g., Brown et al., 1990), the input and output for QG are in the same language, and their length ratio is often far from one to one; and unlike sentence compression (e.g., Knight and Marcu, 2000), QG may involve substantial changes to words and their ordering, beyond simple removal of words. Some previous research has directly approached the topic of generating questions for educational 610 purposes (Mitkov and Ha, 2003; Kunichika et al., 2004; Gates, 2008; Rus and Graessar, 2009; Rus and Lester, 2009), but to our knowledge, none has involved statistical models for choosing among output candidates. Mitkov et al. (200"
N10-1086,I05-5002,0,0.0593005,"Missing"
N10-1086,W03-0501,0,0.0145769,"ce) by altering lexical items, syntactic structure, and semantics. Many existing NLP transformations could potentially be exploited in this step, including sentence compression, paraphrase generation, or lexical semantics for word substitution. In our implementation, a set of transformations derive a simpler form of the source sentence by removing phrase types such as leading conjunctions, sentence-level modifying phrases, and appositives. Tregex expressions identify the constituents to move, alter, or delete. Similar transformations have been utilized in previous work on headline generation (Dorr and Zajic, 2003) and summarization (Toutanova et al., 2007). To enable questions about syntactically embedded content, our implementation also extracts a set of declarative sentences from any finite clauses, rela3 See Heilman and Smith (2009) for details on the rule-based component. 611 tive clauses, appositives, and participial phrases that appear in the source sentence. For example, it transforms the sentence Selling snowballed because of waves of automatic stop-loss orders, which are triggered by computer when prices fall to certain levels into Automatic stop-loss orders are triggered by computer when pric"
N10-1086,P03-1003,0,0.00715328,"Work The generation of questions by humans has long motivated theoretical work in linguistics (e.g., Ross, 1967), particularly work that portrays questions as transformations of canonical declarative sentences (Chomsky, 1973). Questions have also been a major topic of study in computational linguistics, but primarily with the goal of answering questions (Dang et al., 2008). While much of the question answering research has focused on retrieval or extraction (e.g., Ravichandran and Hovy, 2001; Hovy et al., 2001), models of the transformation from answers to questions have also been developed (Echihabi and Marcu, 2003) with the goal of finding correct answers given a question (e.g., in a source-channel framework). Also, Harabagiu et al. (2005) present a system that automatically generates questions from texts to predict which user-generated questions the text might answer. In such work on question answering, question generation models are typically not evaluated for their intrinsic quality, but rather with respect to their utility as an intermediate step in the question answering process. QG is very different from many natural language generation problems because the input is natural language rather than a"
N10-1086,1993.eamt-1.1,0,0.351999,"Missing"
N10-1086,P05-1026,0,0.0334265,"ork that portrays questions as transformations of canonical declarative sentences (Chomsky, 1973). Questions have also been a major topic of study in computational linguistics, but primarily with the goal of answering questions (Dang et al., 2008). While much of the question answering research has focused on retrieval or extraction (e.g., Ravichandran and Hovy, 2001; Hovy et al., 2001), models of the transformation from answers to questions have also been developed (Echihabi and Marcu, 2003) with the goal of finding correct answers given a question (e.g., in a source-channel framework). Also, Harabagiu et al. (2005) present a system that automatically generates questions from texts to predict which user-generated questions the text might answer. In such work on question answering, question generation models are typically not evaluated for their intrinsic quality, but rather with respect to their utility as an intermediate step in the question answering process. QG is very different from many natural language generation problems because the input is natural language rather than a formal representation (cf. Reiter and Dale, 1997). It is also different from some other tasks related to generation: unlike mac"
N10-1086,ide-suderman-2004-american,0,0.0452348,"ds (9 + 0) The set includes boolean features for the presence of each possible WH word in the question. Negation (1 + 0) This is a boolean feature for the presence of not, never, or no in the question. N -Gram Language Model Features (6 + 0) The set includes real valued features for the log likelihoods and length-normalized log likelihoods of the question, the source sentence, and the answer phrase. Separate likelihood features are included for unigram and trigram language models. These language models were estimated from the written por614 tion of the American National Corpus Second Release (Ide and Suderman, 2004), which consists of approximately 20 million tokens, using Kneser and Ney (1995) smoothing. Grammatical Features (23 + 95) The set includes integer features for the numbers of proper nouns, pronouns, adjectives, adverbs, conjunctions, numbers, noun phrases, prepositional phrases, and subordinate clauses in the phrase structure parse trees for the question and answer phrase. It also includes one integer feature for the number of modifying phrases at the start of the question (e.g., as in At the end of the Civil War, who led the Union Army?); three boolean features for whether the main verb is i"
N10-1086,P98-1116,0,0.029055,"he verb phrase known as . . . at the end of the question. The characteristics of such phenomena are (arguably) difficult to learn from corpora, but they have been studied extensively in linguistics (Ross, 1967; Chomsky, 1973). We take a rule-based approach in order to leverage this linguistic knowledge. However, since many phenomena pertaining to question generation are not so easily encoded with rules, we include statistical ranking as an integral component. Thus, we employ an overgenerate-andrank approach, which has been applied successfully in areas such as generation (Walker et al., 2001; Langkilde and Knight, 1998) and syntactic parsing (Collins, 2000). Since large datasets of the appropriate domain, style, and form of questions are not available to train our ranking model, we learn to rank from a relatively small, tailored dataset of humanlabeled output from our rule-based system. The remainder of the paper is organized as fol2 The motivating example does not exhibit lexical semantic variations such as synonymy. In this work, we do not model complex paraphrasing, but believe that paraphrase generation techniques could be incorporated into our approach. 609 Human Language Technologies: The 2010 Annual C"
N10-1086,levy-andrew-2006-tregex,0,0.0135768,"he term “question phrase” refers to the phrase containing the WH word that replaces an answer phrase (e.g., Where in Where is Kenya located?). To represent the syntactic structure of sentences, we use simplified Penn Treebank-style phrase structure trees, including POS and category labels, as produced by the Stanford Parser (Klein and Manning, 2003). Noun phrase heads are selected using Collins’ rules (Collins, 1999). To implement the rules for transforming source sentences into questions, we use Tregex, a tree query language, and Tsurgeon, a tree manipulation language built on top of Tregex (Levy and Andrew, 2006). The Tregex language includes various relational operators based on the primitive relations of immediate dominance (denoted “<”) and immediate precedence (denoted “.”). Tsurgeon adds the ability to modify trees by relabeling, deleting, moving, and inserting nodes. 4 Rule-based Overgeneration Many useful questions can be viewed as lexical, syntactic, or semantic transformations of the declarative sentences in a text. We describe how to model this process in two steps, as proposed in §1.3 4.1 Sentence Simplification In the first step for transforming sentences into questions, each of the senten"
N10-1086,J93-2004,0,0.0596483,"training set included 1,328 questions about 12 articles, and the test set included 120 questions about 2 articles from this corpus. The second corpus was a random sample from the articles in the Simple English Wikipedia of similar length. This corpus provides similar text but at a reading level corresponding to elementary education or intermediate second language learning.9 The training set included 1,195 questions about 16 articles, and the test set included 118 questions about 2 articles from this corpus. The third corpus was Section 23 of the Wall Street Journal data in the Penn Treebank (Marcus et al., 1993).10 The training set included 284 questions about 8 articles, and the test set included 190 questions about 2 articles from this corpus. 6 Ranking We use a discriminative ranker to rank questions, similar to the approach described by Collins (2000) for ranking syntactic parses. Questions are ranked by the predictions of a logistic regression model of question acceptability. Given the question q and source text t, the model defines a binomial distribution p(R |q, t), with binary random variable R ranging over a (“acceptable”) and u (“unacceptable”). We estimate the parameters by optimizing the"
N10-1086,W03-0203,0,0.927453,"cause the input is natural language rather than a formal representation (cf. Reiter and Dale, 1997). It is also different from some other tasks related to generation: unlike machine translation (e.g., Brown et al., 1990), the input and output for QG are in the same language, and their length ratio is often far from one to one; and unlike sentence compression (e.g., Knight and Marcu, 2000), QG may involve substantial changes to words and their ordering, beyond simple removal of words. Some previous research has directly approached the topic of generating questions for educational 610 purposes (Mitkov and Ha, 2003; Kunichika et al., 2004; Gates, 2008; Rus and Graessar, 2009; Rus and Lester, 2009), but to our knowledge, none has involved statistical models for choosing among output candidates. Mitkov et al. (2006) demonstrated that automatic generation and manual correction of questions can be more time-efficient than manual authoring alone. Much of the prior QG research has evaluated systems in specific domains (e.g., introductory linguistics, English as a Second Language), and thus we do not attempt empirical comparisons. Existing QG systems model their transformations from source text to questions wi"
N10-1086,N01-1003,0,0.206259,"emantic argument of the verb phrase known as . . . at the end of the question. The characteristics of such phenomena are (arguably) difficult to learn from corpora, but they have been studied extensively in linguistics (Ross, 1967; Chomsky, 1973). We take a rule-based approach in order to leverage this linguistic knowledge. However, since many phenomena pertaining to question generation are not so easily encoded with rules, we include statistical ranking as an integral component. Thus, we employ an overgenerate-andrank approach, which has been applied successfully in areas such as generation (Walker et al., 2001; Langkilde and Knight, 1998) and syntactic parsing (Collins, 2000). Since large datasets of the appropriate domain, style, and form of questions are not available to train our ranking model, we learn to rank from a relatively small, tailored dataset of humanlabeled output from our rule-based system. The remainder of the paper is organized as fol2 The motivating example does not exhibit lexical semantic variations such as synonymy. In this work, we do not model complex paraphrasing, but believe that paraphrase generation techniques could be incorporated into our approach. 609 Human Language Te"
N10-1086,J03-4003,0,\N,Missing
N10-1086,C98-1112,0,\N,Missing
N10-1086,P02-1006,0,\N,Missing
N10-1112,W02-1001,0,0.173616,"θ &gt; f (x(i) , y)} &gt; (i) 0 y 0 ∈Y(x(i) ) exp{θ f (x , y )} cost(y (i) , y) P X −θ &gt; f (x(i) , y (i) ) + log i=1 (4) (5) exp{θ &gt; f (x(i) , y) + cost(y (i) , y)} (6) y∈Y(x(i) ) Figure 1: Objective functions for training linear models. Regularization terms (e.g., C annotated with four entity types: person, location, organization, and miscellaneous. Our experiments focus on comparing training objectives for structured sequential models for this task. For all objectives, we use the same standard set of feature templates, following Kazama and Torisawa (2007) with additional token shape like those in Collins (2002b) and simple gazetteer features. A feature was included if it occurred at least once in training data (total 1,312,255 features). The task is evaluated using the F1 score, which is the harmonic mean of precision and recall (computed at the level of entire entities). Since this metric is computed from corpus-level precision and recall, it is not easily decomposable into features used in standard chain CRFs. For simplicity, we only consider Hamming cost in this paper; experiments with other cost functions more targeted to NER are presented in Gimpel and Smith (2010). 4.1 (3) y∈Y(x(i) ) i=1 y∈Y("
N10-1112,H05-1087,0,0.0135,"Smith and Eisner (2006) minimized risk using k-best lists to define the distribution over output structures. Li and Eisner (2009) introduced a novel semiring for minimizing risk using dynamic programming; Xiong et al. (2009) minimized risk in a CRF. 2.1.4 Other Criteria Many other criteria have been proposed to attempt to tailor training conditions to match taskspecific evaluation metrics. These include the average per-label marginal likelihood for sequence labeling (Kakade et al., 2002), minimum error-rate training for machine translation (Och, 2003), F1 for logistic regression classifiers (Jansche, 2005), and a wide range of possible metrics for sequence labeling and segmentation tasks (Suzuki et al., 2006). 3 Softmax-Margin The softmax-margin objective is shown as Eq. 6 and is a generalization of that used by Povey et al. (2008) and similar to that used by Sha and Saul (2006). The simple intuition is the same as the intuition in max-margin learning: high-cost outputs for x(i) should be penalized more heavily. Another view says that we replace the probabilistic score inside the exp function of CLL with the “cost-augmented” score from max-margin. A third view says that we replace the “hard”P m"
N10-1112,D07-1033,0,0.0107519,"(i) ) + max   θ &gt; f (x(i) , y) + cost(y (i) , y) y∈Y(x(i) ) X exp{θ &gt; f (x(i) , y)} &gt; (i) 0 y 0 ∈Y(x(i) ) exp{θ f (x , y )} cost(y (i) , y) P X −θ &gt; f (x(i) , y (i) ) + log i=1 (4) (5) exp{θ &gt; f (x(i) , y) + cost(y (i) , y)} (6) y∈Y(x(i) ) Figure 1: Objective functions for training linear models. Regularization terms (e.g., C annotated with four entity types: person, location, organization, and miscellaneous. Our experiments focus on comparing training objectives for structured sequential models for this task. For all objectives, we use the same standard set of feature templates, following Kazama and Torisawa (2007) with additional token shape like those in Collins (2002b) and simple gazetteer features. A feature was included if it occurred at least once in training data (total 1,312,255 features). The task is evaluated using the F1 score, which is the harmonic mean of precision and recall (computed at the level of entire entities). Since this metric is computed from corpus-level precision and recall, it is not easily decomposable into features used in standard chain CRFs. For simplicity, we only consider Hamming cost in this paper; experiments with other cost functions more targeted to NER are presented"
N10-1112,W04-3250,0,0.0137351,"Missing"
N10-1112,D09-1005,0,0.0354659,"nce of the North American Chapter of the ACL, pages 733–736, c Los Angeles, California, June 2010. 2010 Association for Computational Linguistics on training data: Pn P i=1 (i) (i) y∈Y(x(i) ) pθ (y|x )cost(y , y) (2) With a log-linear model, learning then requires solving the problem shown in Eq. 5. Unlike the previous two criteria, risk is typically non-convex. Risk minimization first appeared in the speech recognition community (Kaiser et al., 2000; Povey and Woodland, 2002). In NLP, Smith and Eisner (2006) minimized risk using k-best lists to define the distribution over output structures. Li and Eisner (2009) introduced a novel semiring for minimizing risk using dynamic programming; Xiong et al. (2009) minimized risk in a CRF. 2.1.4 Other Criteria Many other criteria have been proposed to attempt to tailor training conditions to match taskspecific evaluation metrics. These include the average per-label marginal likelihood for sequence labeling (Kakade et al., 2002), minimum error-rate training for machine translation (Och, 2003), F1 for logistic regression classifiers (Jansche, 2005), and a wide range of possible metrics for sequence labeling and segmentation tasks (Suzuki et al., 2006). 3 Softmax"
N10-1112,P03-1021,0,0.07487,"ser et al., 2000; Povey and Woodland, 2002). In NLP, Smith and Eisner (2006) minimized risk using k-best lists to define the distribution over output structures. Li and Eisner (2009) introduced a novel semiring for minimizing risk using dynamic programming; Xiong et al. (2009) minimized risk in a CRF. 2.1.4 Other Criteria Many other criteria have been proposed to attempt to tailor training conditions to match taskspecific evaluation metrics. These include the average per-label marginal likelihood for sequence labeling (Kakade et al., 2002), minimum error-rate training for machine translation (Och, 2003), F1 for logistic regression classifiers (Jansche, 2005), and a wide range of possible metrics for sequence labeling and segmentation tasks (Suzuki et al., 2006). 3 Softmax-Margin The softmax-margin objective is shown as Eq. 6 and is a generalization of that used by Povey et al. (2008) and similar to that used by Sha and Saul (2006). The simple intuition is the same as the intuition in max-margin learning: high-cost outputs for x(i) should be penalized more heavily. Another view says that we replace the probabilistic score inside the exp function of CLL with the “cost-augmented” score from max"
N10-1112,P06-2101,0,0.0647648,"th respect to the conditional distribution pθ (y|x); 733 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 733–736, c Los Angeles, California, June 2010. 2010 Association for Computational Linguistics on training data: Pn P i=1 (i) (i) y∈Y(x(i) ) pθ (y|x )cost(y , y) (2) With a log-linear model, learning then requires solving the problem shown in Eq. 5. Unlike the previous two criteria, risk is typically non-convex. Risk minimization first appeared in the speech recognition community (Kaiser et al., 2000; Povey and Woodland, 2002). In NLP, Smith and Eisner (2006) minimized risk using k-best lists to define the distribution over output structures. Li and Eisner (2009) introduced a novel semiring for minimizing risk using dynamic programming; Xiong et al. (2009) minimized risk in a CRF. 2.1.4 Other Criteria Many other criteria have been proposed to attempt to tailor training conditions to match taskspecific evaluation metrics. These include the average per-label marginal likelihood for sequence labeling (Kakade et al., 2002), minimum error-rate training for machine translation (Och, 2003), F1 for logistic regression classifiers (Jansche, 2005), and a wi"
N10-1112,P06-1028,0,0.0223626,"tructures. Li and Eisner (2009) introduced a novel semiring for minimizing risk using dynamic programming; Xiong et al. (2009) minimized risk in a CRF. 2.1.4 Other Criteria Many other criteria have been proposed to attempt to tailor training conditions to match taskspecific evaluation metrics. These include the average per-label marginal likelihood for sequence labeling (Kakade et al., 2002), minimum error-rate training for machine translation (Och, 2003), F1 for logistic regression classifiers (Jansche, 2005), and a wide range of possible metrics for sequence labeling and segmentation tasks (Suzuki et al., 2006). 3 Softmax-Margin The softmax-margin objective is shown as Eq. 6 and is a generalization of that used by Povey et al. (2008) and similar to that used by Sha and Saul (2006). The simple intuition is the same as the intuition in max-margin learning: high-cost outputs for x(i) should be penalized more heavily. Another view says that we replace the probabilistic score inside the exp function of CLL with the “cost-augmented” score from max-margin. A third view says that we replace the “hard”P maximum of max-margin with the “softmax” (log exp) from CLL; hence we use the name “softmax-margin.” Like"
N10-1112,W03-0419,0,0.177423,"Missing"
N10-1112,J96-1002,0,\N,Missing
N10-1112,D08-1076,0,\N,Missing
N10-1112,P02-1062,0,\N,Missing
N10-1138,S07-1018,0,0.696004,"though our models often involve strong independence assumptions, the probabilistic framework we adopt is highly amenable to future extension through new features, relaxed independence assumptions, and semisupervised learning. Some novel aspects of our current approach include a latent-variable model that permits disambiguation of words not in the FrameNet lexicon, a unified model for finding and labeling arguments, and a precision-boosting constraint that forbids arguments of the same predicate to overlap. Our parser achieves the best published results to date on the SemEval’07 FrameNet task (Baker et al., 2007). 2 Resources and Task We consider frame-semantic parsing resources. 2.1 FrameNet Lexicon The FrameNet lexicon is a taxonomy of manually identified general-purpose frames for English.1 Listed in the lexicon with each frame are several lemmas (with part of speech) that can denote the frame or some aspect of it—these are called lexical units (LUs). In a sentence, word or phrase tokens that evoke a frame are known as targets. The set of LUs listed for a frame in FrameNet may not be exhaustive; we may see a target in new data that does not correspond to an LU for the frame it evokes. Each frame de"
N10-1138,boas-2002-bilingual,0,0.0220307,"7). The LTH system of Johansson and Nugues (2007), our baseline (§2.4), performed the best in the SemEval’07 task. Matsubayashi et al. (2009) trained a loglinear model on the SemEval’07 data to evaluate argument identification features exploiting various 955 types of taxonomic relations to generalize over roles. A line of work has sought to extend the coverage of FrameNet by exploiting VerbNet, WordNet, and Wikipedia (Shi and Mihalcea, 2005; Giuglea and Moschitti, 2006; Pennacchiotti et al., 2008; Tonelli and Giuliano, 2009), and projecting entries and annotations within and across languages (Boas, 2002; Fung and Chen, 2004; Pad´o and Lapata, 2005; F¨urstenau and Lapata, 2009). Others have applied frame-semantic structures to question answering, paraphrase/entailment recognition, and information extraction (Narayanan and Harabagiu, 2004; Shen and Lapata, 2007; Pad´o and Erk, 2005; Burchardt, 2006; Moschitti et al., 2003; Surdeanu et al., 2003). 7 Conclusion We have provided a supervised model for rich frame-semantic parsing, based on a combination of knowledge from FrameNet, two probabilistic models trained on SemEval’07 data, and expedient heuristics. Our system achieves improvements over t"
N10-1138,erk-pado-2006-shalmaneser,0,0.128834,"Missing"
N10-1138,W03-1007,0,0.0683486,"a set of shallow predicate-argument annotations for Wall Street Journal articles from the Penn Treebank (Marcus et al., 1993); a recent issue of CL (M`arquez et al., 2008) was devoted to the subject.) Most work on frame-semantic role labeling has made use of the exemplar sentences in the FrameNet corpus (see §2.1), each of which is annotated for a single frame and its arguments. On the probabilistic modeling front, Gildea and Jurafsky (2002) presented a discriminative model for arguments given the frame; Thompson et al. (2003) used a generative model for both the frame and its arguments; and Fleischman et al. (2003) first used maximum entropy models to find and label arguments given the frame. Shi and Mihalcea (2004) developed a rule-based system to predict frames and their arguments in text, and Erk and Pad´o (2006) introduced the Shalmaneser tool, which employs Na¨ıve Bayes classifiers to do the same. Other FrameNet SRL systems (Giuglea and Moschitti, 2006, for instance) have used SVMs. Most of this work was done on an older, smaller version of FrameNet. Recent work on frame-semantic parsing—in which sentences may contain multiple frames to be recognized along with their arguments—has used the SemEval’"
N10-1138,C04-1134,0,0.0348662,"system of Johansson and Nugues (2007), our baseline (§2.4), performed the best in the SemEval’07 task. Matsubayashi et al. (2009) trained a loglinear model on the SemEval’07 data to evaluate argument identification features exploiting various 955 types of taxonomic relations to generalize over roles. A line of work has sought to extend the coverage of FrameNet by exploiting VerbNet, WordNet, and Wikipedia (Shi and Mihalcea, 2005; Giuglea and Moschitti, 2006; Pennacchiotti et al., 2008; Tonelli and Giuliano, 2009), and projecting entries and annotations within and across languages (Boas, 2002; Fung and Chen, 2004; Pad´o and Lapata, 2005; F¨urstenau and Lapata, 2009). Others have applied frame-semantic structures to question answering, paraphrase/entailment recognition, and information extraction (Narayanan and Harabagiu, 2004; Shen and Lapata, 2007; Pad´o and Erk, 2005; Burchardt, 2006; Moschitti et al., 2003; Surdeanu et al., 2003). 7 Conclusion We have provided a supervised model for rich frame-semantic parsing, based on a combination of knowledge from FrameNet, two probabilistic models trained on SemEval’07 data, and expedient heuristics. Our system achieves improvements over the state of the art a"
N10-1138,E09-1026,0,0.0300148,"Missing"
N10-1138,J02-3001,0,0.820262,"riefly, we highlight some relevant work, particularly research that has made use of FrameNet. (Note that much related research has focused on PropBank (Kingsbury and Palmer, 2002), a set of shallow predicate-argument annotations for Wall Street Journal articles from the Penn Treebank (Marcus et al., 1993); a recent issue of CL (M`arquez et al., 2008) was devoted to the subject.) Most work on frame-semantic role labeling has made use of the exemplar sentences in the FrameNet corpus (see §2.1), each of which is annotated for a single frame and its arguments. On the probabilistic modeling front, Gildea and Jurafsky (2002) presented a discriminative model for arguments given the frame; Thompson et al. (2003) used a generative model for both the frame and its arguments; and Fleischman et al. (2003) first used maximum entropy models to find and label arguments given the frame. Shi and Mihalcea (2004) developed a rule-based system to predict frames and their arguments in text, and Erk and Pad´o (2006) introduced the Shalmaneser tool, which employs Na¨ıve Bayes classifiers to do the same. Other FrameNet SRL systems (Giuglea and Moschitti, 2006, for instance) have used SVMs. Most of this work was done on an older, s"
N10-1138,S07-1048,0,0.547847,"easure for frames and arguments; it also provides a score that gives partial credit for hypothesizing a frame related to the correct one. We present precision, recall, and F1 -measure microaveraged across the test documents, report labels-only matching scores (spans must match exactly), and do not use named entity labels. More details can be found in Baker et al. (2007). For our experiments, statistical significance is measured using a reimplementation of Dan Bikel’s randomized parsing evaluation comparator.3 2.4 Baseline A strong baseline for frame-semantic parsing is the system presented by Johansson and Nugues (2007, hereafter J&N’07), the best system in the SemEval’07 shared task. For frame identification, they used an SVM classifier to disambiguate frames for known frame-evoking words. They used WordNet synsets to extend the vocabulary of frameevoking words to cover unknown words, and then 3 http://www.cis.upenn.edu/˜dbikel/ software.html#comparator 950 TARGET I DENTIFICATION Our technique (§3) Baseline: J&N’07 P 89.92 87.87 R 70.79 67.11 F1 79.21 76.10 Table 3. Target identification results for our system and the baseline. Scores in bold denote significant improvements over the baseline (p &lt; 0.05). us"
N10-1138,D08-1008,0,0.010832,"dels trained on the given data split, a problem we have not sought to address here. 2 http://framenet.icsi.berkeley.edu/ semeval/FSSE.html Preprocessing. We preprocess sentences in our dataset with a standard set of annotations: POS tags from MXPOST (Ratnaparkhi, 1996) and dependency parses from the MST parser (McDonald et al., 2005) since manual syntactic parses are not available for most of the FrameNet-annotated documents. We used WordNet (Fellbaum, 1998) for lemmatization. We also labeled each verb in the data as having AC TIVE or PASSIVE voice, using code from the SRL system described by Johansson and Nugues (2008). 2.3 Task and Evaluation Automatic annotations of frame-semantic structure can be broken into three parts: (1) targets, the words or phrases that evoke frames; (2) the frame type, defined in the lexicon, evoked by each target; and (3) the arguments, or spans of words that serve to fill roles defined by each evoked frame. These correspond to the three subtasks in our parser, each described and evaluated in turn: target identification (§3), frame identification (§4, not unlike wordsense disambiguation), and argument identification (§5, not unlike semantic role labeling). The standard evaluation"
N10-1138,kingsbury-palmer-2002-treebank,0,0.137074,".57 68.46 49.68 42.82 46.00 58.08 38.76 46.49 51.59 35.44 42.01 partial frame matching P R F1 57.85 49.86 53.56 62.76 41.89 50.24 56.01 38.48 45.62 Table 5. Argument identification results. ∗ indicates that gold-standard labels were used for a given pipeline stage. For full parsing, bolded scores indicate significant improvements relative to the baseline (p &lt; 0.05). putational work has investigated predicate-argument structures for semantics. Briefly, we highlight some relevant work, particularly research that has made use of FrameNet. (Note that much related research has focused on PropBank (Kingsbury and Palmer, 2002), a set of shallow predicate-argument annotations for Wall Street Journal articles from the Penn Treebank (Marcus et al., 1993); a recent issue of CL (M`arquez et al., 2008) was devoted to the subject.) Most work on frame-semantic role labeling has made use of the exemplar sentences in the FrameNet corpus (see §2.1), each of which is annotated for a single frame and its arguments. On the probabilistic modeling front, Gildea and Jurafsky (2002) presented a discriminative model for arguments given the frame; Thompson et al. (2003) used a generative model for both the frame and its arguments; and"
N10-1138,J93-2004,0,0.0372018,"1 38.48 45.62 Table 5. Argument identification results. ∗ indicates that gold-standard labels were used for a given pipeline stage. For full parsing, bolded scores indicate significant improvements relative to the baseline (p &lt; 0.05). putational work has investigated predicate-argument structures for semantics. Briefly, we highlight some relevant work, particularly research that has made use of FrameNet. (Note that much related research has focused on PropBank (Kingsbury and Palmer, 2002), a set of shallow predicate-argument annotations for Wall Street Journal articles from the Penn Treebank (Marcus et al., 1993); a recent issue of CL (M`arquez et al., 2008) was devoted to the subject.) Most work on frame-semantic role labeling has made use of the exemplar sentences in the FrameNet corpus (see §2.1), each of which is annotated for a single frame and its arguments. On the probabilistic modeling front, Gildea and Jurafsky (2002) presented a discriminative model for arguments given the frame; Thompson et al. (2003) used a generative model for both the frame and its arguments; and Fleischman et al. (2003) first used maximum entropy models to find and label arguments given the frame. Shi and Mihalcea (2004"
N10-1138,J08-2001,0,0.0694048,"Missing"
N10-1138,P09-1003,0,0.164686,"ict frames and their arguments in text, and Erk and Pad´o (2006) introduced the Shalmaneser tool, which employs Na¨ıve Bayes classifiers to do the same. Other FrameNet SRL systems (Giuglea and Moschitti, 2006, for instance) have used SVMs. Most of this work was done on an older, smaller version of FrameNet. Recent work on frame-semantic parsing—in which sentences may contain multiple frames to be recognized along with their arguments—has used the SemEval’07 data (Baker et al., 2007). The LTH system of Johansson and Nugues (2007), our baseline (§2.4), performed the best in the SemEval’07 task. Matsubayashi et al. (2009) trained a loglinear model on the SemEval’07 data to evaluate argument identification features exploiting various 955 types of taxonomic relations to generalize over roles. A line of work has sought to extend the coverage of FrameNet by exploiting VerbNet, WordNet, and Wikipedia (Shi and Mihalcea, 2005; Giuglea and Moschitti, 2006; Pennacchiotti et al., 2008; Tonelli and Giuliano, 2009), and projecting entries and annotations within and across languages (Boas, 2002; Fung and Chen, 2004; Pad´o and Lapata, 2005; F¨urstenau and Lapata, 2009). Others have applied frame-semantic structures to quest"
N10-1138,P05-1012,0,0.00937302,"s. Notice that the test set contains more annotations per word, both in terms of frames and arguments. Moreover, there are many more out-of-lexicon frame, role, and LU types in the test set than in the training set. This inconsistency in the data results in poor recall scores for all models trained on the given data split, a problem we have not sought to address here. 2 http://framenet.icsi.berkeley.edu/ semeval/FSSE.html Preprocessing. We preprocess sentences in our dataset with a standard set of annotations: POS tags from MXPOST (Ratnaparkhi, 1996) and dependency parses from the MST parser (McDonald et al., 2005) since manual syntactic parses are not available for most of the FrameNet-annotated documents. We used WordNet (Fellbaum, 1998) for lemmatization. We also labeled each verb in the data as having AC TIVE or PASSIVE voice, using code from the SRL system described by Johansson and Nugues (2008). 2.3 Task and Evaluation Automatic annotations of frame-semantic structure can be broken into three parts: (1) targets, the words or phrases that evoke frames; (2) the frame type, defined in the lexicon, evoked by each target; and (3) the arguments, or spans of words that serve to fill roles defined by eac"
N10-1138,C04-1100,0,0.0279741,"ication features exploiting various 955 types of taxonomic relations to generalize over roles. A line of work has sought to extend the coverage of FrameNet by exploiting VerbNet, WordNet, and Wikipedia (Shi and Mihalcea, 2005; Giuglea and Moschitti, 2006; Pennacchiotti et al., 2008; Tonelli and Giuliano, 2009), and projecting entries and annotations within and across languages (Boas, 2002; Fung and Chen, 2004; Pad´o and Lapata, 2005; F¨urstenau and Lapata, 2009). Others have applied frame-semantic structures to question answering, paraphrase/entailment recognition, and information extraction (Narayanan and Harabagiu, 2004; Shen and Lapata, 2007; Pad´o and Erk, 2005; Burchardt, 2006; Moschitti et al., 2003; Surdeanu et al., 2003). 7 Conclusion We have provided a supervised model for rich frame-semantic parsing, based on a combination of knowledge from FrameNet, two probabilistic models trained on SemEval’07 data, and expedient heuristics. Our system achieves improvements over the state of the art at each stage of processing and collectively, and is amenable to future extension. Our parser is available for download at http://www.ark.cs.cmu.edu/SEMAFOR. Acknowledgments We thank Collin Baker, Katrin Erk, Richard J"
N10-1138,H05-1108,0,0.0518989,"Missing"
N10-1138,D08-1048,0,0.0129921,"Missing"
N10-1138,W96-0213,0,0.500149,"ta to create a development set for tuning model hyperparameters. Notice that the test set contains more annotations per word, both in terms of frames and arguments. Moreover, there are many more out-of-lexicon frame, role, and LU types in the test set than in the training set. This inconsistency in the data results in poor recall scores for all models trained on the given data split, a problem we have not sought to address here. 2 http://framenet.icsi.berkeley.edu/ semeval/FSSE.html Preprocessing. We preprocess sentences in our dataset with a standard set of annotations: POS tags from MXPOST (Ratnaparkhi, 1996) and dependency parses from the MST parser (McDonald et al., 2005) since manual syntactic parses are not available for most of the FrameNet-annotated documents. We used WordNet (Fellbaum, 1998) for lemmatization. We also labeled each verb in the data as having AC TIVE or PASSIVE voice, using code from the SRL system described by Johansson and Nugues (2008). 2.3 Task and Evaluation Automatic annotations of frame-semantic structure can be broken into three parts: (1) targets, the words or phrases that evoke frames; (2) the frame type, defined in the lexicon, evoked by each target; and (3) the ar"
N10-1138,D07-1002,0,0.0551122,"ious 955 types of taxonomic relations to generalize over roles. A line of work has sought to extend the coverage of FrameNet by exploiting VerbNet, WordNet, and Wikipedia (Shi and Mihalcea, 2005; Giuglea and Moschitti, 2006; Pennacchiotti et al., 2008; Tonelli and Giuliano, 2009), and projecting entries and annotations within and across languages (Boas, 2002; Fung and Chen, 2004; Pad´o and Lapata, 2005; F¨urstenau and Lapata, 2009). Others have applied frame-semantic structures to question answering, paraphrase/entailment recognition, and information extraction (Narayanan and Harabagiu, 2004; Shen and Lapata, 2007; Pad´o and Erk, 2005; Burchardt, 2006; Moschitti et al., 2003; Surdeanu et al., 2003). 7 Conclusion We have provided a supervised model for rich frame-semantic parsing, based on a combination of knowledge from FrameNet, two probabilistic models trained on SemEval’07 data, and expedient heuristics. Our system achieves improvements over the state of the art at each stage of processing and collectively, and is amenable to future extension. Our parser is available for download at http://www.ark.cs.cmu.edu/SEMAFOR. Acknowledgments We thank Collin Baker, Katrin Erk, Richard Johansson, and Nils Reit"
N10-1138,W04-2008,0,0.148854,"(Marcus et al., 1993); a recent issue of CL (M`arquez et al., 2008) was devoted to the subject.) Most work on frame-semantic role labeling has made use of the exemplar sentences in the FrameNet corpus (see §2.1), each of which is annotated for a single frame and its arguments. On the probabilistic modeling front, Gildea and Jurafsky (2002) presented a discriminative model for arguments given the frame; Thompson et al. (2003) used a generative model for both the frame and its arguments; and Fleischman et al. (2003) first used maximum entropy models to find and label arguments given the frame. Shi and Mihalcea (2004) developed a rule-based system to predict frames and their arguments in text, and Erk and Pad´o (2006) introduced the Shalmaneser tool, which employs Na¨ıve Bayes classifiers to do the same. Other FrameNet SRL systems (Giuglea and Moschitti, 2006, for instance) have used SVMs. Most of this work was done on an older, smaller version of FrameNet. Recent work on frame-semantic parsing—in which sentences may contain multiple frames to be recognized along with their arguments—has used the SemEval’07 data (Baker et al., 2007). The LTH system of Johansson and Nugues (2007), our baseline (§2.4), perfo"
N10-1138,P03-1002,0,0.025474,"ught to extend the coverage of FrameNet by exploiting VerbNet, WordNet, and Wikipedia (Shi and Mihalcea, 2005; Giuglea and Moschitti, 2006; Pennacchiotti et al., 2008; Tonelli and Giuliano, 2009), and projecting entries and annotations within and across languages (Boas, 2002; Fung and Chen, 2004; Pad´o and Lapata, 2005; F¨urstenau and Lapata, 2009). Others have applied frame-semantic structures to question answering, paraphrase/entailment recognition, and information extraction (Narayanan and Harabagiu, 2004; Shen and Lapata, 2007; Pad´o and Erk, 2005; Burchardt, 2006; Moschitti et al., 2003; Surdeanu et al., 2003). 7 Conclusion We have provided a supervised model for rich frame-semantic parsing, based on a combination of knowledge from FrameNet, two probabilistic models trained on SemEval’07 data, and expedient heuristics. Our system achieves improvements over the state of the art at each stage of processing and collectively, and is amenable to future extension. Our parser is available for download at http://www.ark.cs.cmu.edu/SEMAFOR. Acknowledgments We thank Collin Baker, Katrin Erk, Richard Johansson, and Nils Reiter for software, data, evaluation scripts, and methodological details. We thank the re"
N10-1138,D09-1029,0,0.0207559,"rames to be recognized along with their arguments—has used the SemEval’07 data (Baker et al., 2007). The LTH system of Johansson and Nugues (2007), our baseline (§2.4), performed the best in the SemEval’07 task. Matsubayashi et al. (2009) trained a loglinear model on the SemEval’07 data to evaluate argument identification features exploiting various 955 types of taxonomic relations to generalize over roles. A line of work has sought to extend the coverage of FrameNet by exploiting VerbNet, WordNet, and Wikipedia (Shi and Mihalcea, 2005; Giuglea and Moschitti, 2006; Pennacchiotti et al., 2008; Tonelli and Giuliano, 2009), and projecting entries and annotations within and across languages (Boas, 2002; Fung and Chen, 2004; Pad´o and Lapata, 2005; F¨urstenau and Lapata, 2009). Others have applied frame-semantic structures to question answering, paraphrase/entailment recognition, and information extraction (Narayanan and Harabagiu, 2004; Shen and Lapata, 2007; Pad´o and Erk, 2005; Burchardt, 2006; Moschitti et al., 2003; Surdeanu et al., 2003). 7 Conclusion We have provided a supervised model for rich frame-semantic parsing, based on a combination of knowledge from FrameNet, two probabilistic models trained on Se"
N10-1145,P04-1054,0,0.0798489,"is important: two parent nodes with the same children should not be considered exact matches if children are on different sides (e.g., defeated the insurgents and the insurgents defeated). 7 From experiments with the paraphrase training set (§5.2), performance does not appear sensitive to the decay parameters. Settings of 0.1, 0.2, 0.3, and 0.4 led to 10-fold cross-validation The main difference between our kernel and the CTK is that we sum over all pairs of subtrees (Equation 3). In contrast, the CTK only considers only one pair of subtrees. When the CTK is applied to relation extraction by Culotta and Sorensen (2004), each subtree is the smallest common subtree that includes the entities between which a relation may exist (e.g., the subtree for Texasbased energy company Exxon Mobil when extracting ORGANIZATION-LOCATION relations). 3.3 Constraints on the Search Space For computational efficiency, we impose the following three constraints to simplify the search space. Note that the first two simply prune away obviously unhelpful search states. 1. For INSERT-CHILD, INSERT-PARENT, and RELABEL-NODE edits, the lemma and POS of the node to insert must occur in the target tree. Also, the pair consisting of the le"
N10-1145,P09-1053,1,0.951347,"tion results, with precision and recall measures for true (positive) paraphrases. Wan et al. (2006) report precision and recall values with only two significant digits. System Punyakanok et al., 2004 +WN Cui et al., 2005 +WN Wang et al., 2007 +WN Tree Edit Model MAP 0.3814 0.4189 0.4350 0.4271 0.4828 0.6029 0.6091 MRR 0.4462 0.4939 0.5569 0.5259 0.5571 0.6852 0.6917 5.3 Table 5: Results for the task of answer selection for question answering. +WN denotes use of WordNet features. 5.2 utilizes entity labels from BBN Identifinder (Bikel et al., 1999) and lexical semantics knowledge from WordNet. Das and Smith (2009) also use a product of experts (PoE) (Hinton, 1999) to combine the QG model with lexical overlap features. Table 4 shows the test set results for all of the systems. While the tree edit model did not outperform the other systems, it produced competitive results. Moreover, the tree edit model does not make use of BLEU scores (Wan et al., 2006), entity labeling components, lexical semantics knowledge sources such as WordNet (beyond lemmatization), or system combination techniques (Das and Smith, 2009). Paraphrase Identification A tree edit model was trained and tested for paraphrase identificati"
N10-1145,C04-1051,0,0.792782,"ferent types of edits should affect the model’s decisions (e.g., about whether two sentences are paraphrases). The structure of this paper is as follows. §2 introduces our model and describes the edit operations that were implemented for our experiments. §3 details the search-based procedure for extracting edit sequences for pairs of sentences. §4 describes the classifier for sentence pairs based on features of their corresponding edit sequences. §5 describes and presents the results of experiments involving recognizing textual entailment (Giampiccolo et al., 2007), paraphrase identification (Dolan et al., 2004), and an answer selection task for question answering (Wang et al., 2007). §6 addresses related work, and §7 provides concluding remarks. 2 Extended Tree Edit Sequences This section defines a tree edit sequence and describes the operations used in our experiments. We begin with some conventions. We use dependency trees as the structure upon which the tree edits will operate. The child nodes for a given parent are represented in a head-outward fashion such that the left and right children are separate lists, with the left- and right-most elements as the last members of their respective lists, a"
N10-1145,C96-1058,0,0.0195592,"ion answering (Wang et al., 2007). §6 addresses related work, and §7 provides concluding remarks. 2 Extended Tree Edit Sequences This section defines a tree edit sequence and describes the operations used in our experiments. We begin with some conventions. We use dependency trees as the structure upon which the tree edits will operate. The child nodes for a given parent are represented in a head-outward fashion such that the left and right children are separate lists, with the left- and right-most elements as the last members of their respective lists, as in most generative dependency models (Eisner, 1996). Each node consists of a lemmatized word token as its main label (hereafter, lemma), a part of speech tag (POS), and a syntactic relation label for the edge to its parent. We assume the root node has a special dummy edge label ROOT. Let Tc be a “current tree” that is being transformed and let Tt be a “target tree” into which Tc will ultimately be transformed. Let T (i) be a node 1012 with an index i into the tree T , where the indices are arbitrary (e.g., they could be word positions). 2.1 Definition We define a tree edit sequence to be a series of edit operations that transform a source tree"
N10-1145,W07-1401,0,0.0254718,"ho wrote the ‘Tale of Genji’?) and a candidate answer sentence retrieved by the information retrieval component of a question answering system. For a positive instance, the text will correctly answer the question—though perhaps indirectly. It may also contain various extraneous information (e.g., Kano script made possible the development of a secular Japanese literature, beginning with such Late Heian classics as Lady Murasaki’s “Tales of Genji.”). For a given set of questions, the task here is to rank candidate answers (Wang et al., 2007). The experimental setup is the same as in Wang et al. (2007). We trained the tree edit model on the manually judged positive and negative QA pairs from previous QA tracks at the Text REtrieval Conference (TREC-8 through TREC-12). The goal of the task is to rank answer candidates rather than classify them; therefore, after training a logistic regression classifier, we rank the answer candidates for a given question by their posterior probabilities of correctness according to the model. We tested our model with QA pairs from TREC13. We report Mean Average Precision (MAP) and Mean Reciprocal Rank (MRR), which are information retrieval measures for ranked"
N10-1145,P03-1011,0,0.0469534,"ing cost, rather than a heuristic function plus the cost so far (e.g., number of edits), as in other types of search. Here, the initial search state is the source tree, the current state is Tc , and the goal state is Tt . The function for generating the successors for a given state returns returns trees for all possible specifications of operations on Tc (§2.2), subject to the minimal constraints to be described in §3.3. The enumeration order of the edits in the search procedure (i.e., the order in which states are explored) follows the order of their presentation in Table 1. In preliminary 3 Gildea (2003) proposes a dynamic programming algorithm for a related tree alignment problem, but it is still exponential in the maximum number of children for a node. 1013 experiments, varying this order had no effect on the extracted transformations. 3.2 Tree Kernel Heuristic In our greedy search approach, the evaluation function’s value for a state depends only on the heuristic function’s estimate of how different the current tree at that state is from the target tree. Using this function, at each step, the search routine chooses the next state (i.e., edit) so as to minimize the difference between the cu"
N10-1145,W07-1423,0,0.053614,"true entailments, and overall accuracy (i.e., percentage correct). We compare to four systems that use syntactic dependencies and lexical semantic information.13 De Marneffe et al. (2006) described an RTE system that finds word alignments and then classifies sentence pairs based on those alignments. MacCartney and Manning (2008) used an inference procedure based on Natural Logic, leading to a relatively high-precision, low-recall system. MacCartney and Manning (2008) also tested a hybrid of the natural logic system and the complementary system of de Marneffe et al. (2006) to improve coverage. Harmeling (2007) took an approach similar to ours involving classification based on transformation sequences, but with less general operations and a more complex, heuristic procedure for finding sequences. Table 3 presents RTE results, showing that the tree edit model performs competitively. While it does not outperform state-of-the-art RTE systems, the tree edit model is simpler and less tailored to this task than many other RTE systems based on similar linguistic information. 13 The top-performing RTE systems often involve significant manual engineering for the RTE task. Also, many employ techniques that ma"
N10-1145,C08-1066,0,0.0361143,"Missing"
N10-1145,H05-1066,0,0.0150735,"Missing"
N10-1145,P02-1040,0,0.0885054,"s for each pair (one in each direction) and summed the feature values. The model was evaluated with the standard test set. We report accuracy, positive class precision (i.e., percentage of predicted positive paraphrases that had positive gold-standard labels), and positive class recall (i.e., percentage of positive gold-standard labels that were predicted to be positive paraphrases). We compare to two of the best performance approaches to paraphrase. One approach, by Wan et al. (2006), uses an SVM classifier with features based on syntactic dependencies, TED, unigram overlap, and BLEU scores (Papineni et al., 2002). The other system, by Das and Smith (2009), is based on a quasi-synchronous grammar (QG; Smith and Eisner, 2006), a probabilistic model that allows loose alignments between trees but prefers tree isomorphism. In addition to syntactic dependencies, the QG model 1017 Answer Selection for Question Answering A tree edit model was trained for answer selection in question answering (QA). In this task, an instance consists of a short factual question (e.g., Who wrote the ‘Tale of Genji’?) and a candidate answer sentence retrieved by the information retrieval component of a question answering system."
N10-1145,W06-3104,0,0.0155258,"test set. We report accuracy, positive class precision (i.e., percentage of predicted positive paraphrases that had positive gold-standard labels), and positive class recall (i.e., percentage of positive gold-standard labels that were predicted to be positive paraphrases). We compare to two of the best performance approaches to paraphrase. One approach, by Wan et al. (2006), uses an SVM classifier with features based on syntactic dependencies, TED, unigram overlap, and BLEU scores (Papineni et al., 2002). The other system, by Das and Smith (2009), is based on a quasi-synchronous grammar (QG; Smith and Eisner, 2006), a probabilistic model that allows loose alignments between trees but prefers tree isomorphism. In addition to syntactic dependencies, the QG model 1017 Answer Selection for Question Answering A tree edit model was trained for answer selection in question answering (QA). In this task, an instance consists of a short factual question (e.g., Who wrote the ‘Tale of Genji’?) and a candidate answer sentence retrieved by the information retrieval component of a question answering system. For a positive instance, the text will correctly answer the question—though perhaps indirectly. It may also cont"
N10-1145,U06-1019,0,0.0607155,"guish between true and false paraphrases. Since there is no predefined direction for paraphrase pairs, we extracted two sequences for each pair (one in each direction) and summed the feature values. The model was evaluated with the standard test set. We report accuracy, positive class precision (i.e., percentage of predicted positive paraphrases that had positive gold-standard labels), and positive class recall (i.e., percentage of positive gold-standard labels that were predicted to be positive paraphrases). We compare to two of the best performance approaches to paraphrase. One approach, by Wan et al. (2006), uses an SVM classifier with features based on syntactic dependencies, TED, unigram overlap, and BLEU scores (Papineni et al., 2002). The other system, by Das and Smith (2009), is based on a quasi-synchronous grammar (QG; Smith and Eisner, 2006), a probabilistic model that allows loose alignments between trees but prefers tree isomorphism. In addition to syntactic dependencies, the QG model 1017 Answer Selection for Question Answering A tree edit model was trained for answer selection in question answering (QA). In this task, an instance consists of a short factual question (e.g., Who wrote t"
N10-1145,D07-1003,1,0.663063,"ther two sentences are paraphrases). The structure of this paper is as follows. §2 introduces our model and describes the edit operations that were implemented for our experiments. §3 details the search-based procedure for extracting edit sequences for pairs of sentences. §4 describes the classifier for sentence pairs based on features of their corresponding edit sequences. §5 describes and presents the results of experiments involving recognizing textual entailment (Giampiccolo et al., 2007), paraphrase identification (Dolan et al., 2004), and an answer selection task for question answering (Wang et al., 2007). §6 addresses related work, and §7 provides concluding remarks. 2 Extended Tree Edit Sequences This section defines a tree edit sequence and describes the operations used in our experiments. We begin with some conventions. We use dependency trees as the structure upon which the tree edits will operate. The child nodes for a given parent are represented in a head-outward fashion such that the left and right children are separate lists, with the left- and right-most elements as the last members of their respective lists, as in most generative dependency models (Eisner, 1996). Each node consists"
N10-1145,P06-1051,0,0.00687641,"d syntactic alignments (e.g., Gildea, 2003; Smith and Eisner, 2006; inter alia), and such methods have been applied successfully to semantic tasks (de Marneffe et al., 2006; Wang et al., 2007; Das and Smith, 2009). While we not describe connections to such approaches in detail due to space limitations, we note that theoretical connections are possible between transformations and alignments (Chawathe and Garcia-Molina, 1997). Tree kernels have been applied to a variety of natural language tasks (Collins and Duffy, 2001; Zelenko et al., 2003; Culotta and Sorensen, 2004). Of particular interest, Zanzotto and Moschitti (2006) describe a kernel for RTE that takes tree pairs, rather than single trees, as input. To our knowledge, our use of a tree kernel as a search heuristic is novel. 7 Conclusion We described tree edit models that generalize TED by allowing operations that better account for complex reordering phenomena and by learning from data how different edits should affect the models decisions about output variables of interest (e.g., the correctness of answers). They offer an intuitive and effective method for modeling sentence pairs. They led to competitive performance for three tasks: paraphrase identifica"
N12-1023,2007.mtsummit-papers.3,0,0.314589,"Missing"
N12-1023,D08-1023,0,0.0193604,"Missing"
N12-1023,P08-1024,0,0.105614,"Missing"
N12-1023,W08-0304,0,0.27423,"the best outputs from the decoder without any regularization (i.e., R(θ) = 0).4 The loss is non-convex and not differentiable for cost functions like BLEU, so Och (2003) developed a coordinate ascent procedure with a specialized line search. MERT avoids the need to compute feature vectors for the references (§1(i)) and allows corpuslevel metrics like BLEU to be easily incorporated. However, the complexity of the loss and the difficulty of the search lead to instabilities during learning. Remedies have been suggested, typically involving additional search directions and experiment replicates (Cer et al., 2008; Moore and Quirk, 2008; Foster and Kuhn, 2009; Clark et al., 2011). But despite these improvements, MERT is ineffectual for training weights for large numbers of features; in addition to anecdotal evidence from the MT community, Hopkins and May (2011) illustrated with synthetic data experiments that MERT struggles increasingly to find the optimal solution as the number of parameters grows. Example 2: Probabilistic Models. By exponentiating and normalizing score(x, y, h; θ), we obtain a conditional log-linear model, which is useful for training criteria with probabilistic interpretations: pθ ("
N12-1023,W08-0336,0,0.0631996,"Missing"
N12-1023,N12-1047,0,0.320612,"recognition and speech recognition communities (Duda and Hart, 1973; Juang et al., 1997); its first application to MT was by Och (2003). The loss function takes  ˜ ˜ the following  ( form: losscost X, Y , θ = )  N (i) ˜   cost Y , argmax score(x , y, h; θ) hy,hi∈T(x(i) ) i=1 (3) 2 We will abuse notation and allow cost to operate on both sets of sentences as well as individual sentences. For notational convenience we also let cost accept hidden variables but assume that the hidden variables do not affect the value; i.e., cost(hy, hi, hy 0 , h0 i) = cost(y, hy 0 , h0 i) = cost(y, y 0 ). 3 Cherry and Foster (2012) have concurrently performed a similar analysis. MERT directly minimizes the corpus-level cost function of the best outputs from the decoder without any regularization (i.e., R(θ) = 0).4 The loss is non-convex and not differentiable for cost functions like BLEU, so Och (2003) developed a coordinate ascent procedure with a specialized line search. MERT avoids the need to compute feature vectors for the references (§1(i)) and allows corpuslevel metrics like BLEU to be easily incorporated. However, the complexity of the loss and the difficulty of the search lead to instabilities during learning."
N12-1023,D08-1024,0,0.840805,"pplied to machine translation. Instead, most have implicit connections to particular forms of ramp loss. We propose to minimize ramp loss directly and present a training algorithm that is easy to implement and that performs comparably to others. Most notably, our structured ramp loss minimization algorithm, R AMPION, is less sensitive to initialization and random seeds than standard approaches. 1 Introduction Every statistical MT system relies on a training algorithm to fit the parameters of a scoring function to examples from parallel text. Well-known examples include MERT (Och, 2003), MIRA (Chiang et al., 2008), and PRO (Hopkins and May, 2011). While such procedures can be analyzed as machine learning algorithms—e.g., in the general framework of empirical risk minimization (Vapnik, 1998)—their procedural specifications have made this difficult. From a practical perspective, such algorithms are often complex, difficult to replicate, and sensitive to initialization, random seeds, and other hyperparameters. In this paper, we consider training algorithms that are first specified declaratively, as loss functions to Why isn’t the application of ML to MT more straightforward? We note two key reasons: (i) M"
N12-1023,N09-1025,0,0.713561,"Missing"
N12-1023,P05-1033,0,0.0719136,". Risk minimization corresponds to choosing argminθ∈Θ Ep(X,Y ) [loss (X, Y , θ)] (1) where p(X, Y ) is the (unknown) true joint distribution over corpora. We note that the loss function depends on the entire corpus, while the decoder operates independently on one sentence at a time. This is done to fit the standard assumptions in MT systems: the evaluation metric (e.g., BLEU) depends on 1 For phrase-based MT, a segmentation of the source and target sentences into phrases and an alignment between them (Koehn et al., 2003). For hierarchical phrase-based MT, a derivation under a synchronous CFG (Chiang, 2005). 222 the entire corpus and does not decompose linearly, while the model score does. Since in practice we do not know p(X, Y ), but we do have access to an ac˜ Y˜ i, where X ˜ = {x(i) }N and tual corpus pair hX, i=1 (i) N Y˜ = {y }i=1 , we instead consider regularized empirical risk minimization: ˜ Y˜ , θ) + R(θ) argminθ∈Θ loss(X, (2) where R(θ) is the regularization function used to mitigate overfitting. The regularization function is frequently a squared norm of the parameter vector, such as the `1 or `2 norm, but many other choices are possible. In this paper, we use `2 . Models are evaluat"
N12-1023,J07-2003,0,0.253188,"Missing"
N12-1023,P11-2031,1,0.0916464,".e., R(θ) = 0).4 The loss is non-convex and not differentiable for cost functions like BLEU, so Och (2003) developed a coordinate ascent procedure with a specialized line search. MERT avoids the need to compute feature vectors for the references (§1(i)) and allows corpuslevel metrics like BLEU to be easily incorporated. However, the complexity of the loss and the difficulty of the search lead to instabilities during learning. Remedies have been suggested, typically involving additional search directions and experiment replicates (Cer et al., 2008; Moore and Quirk, 2008; Foster and Kuhn, 2009; Clark et al., 2011). But despite these improvements, MERT is ineffectual for training weights for large numbers of features; in addition to anecdotal evidence from the MT community, Hopkins and May (2011) illustrated with synthetic data experiments that MERT struggles increasingly to find the optimal solution as the number of parameters grows. Example 2: Probabilistic Models. By exponentiating and normalizing score(x, y, h; θ), we obtain a conditional log-linear model, which is useful for training criteria with probabilistic interpretations: pθ (y, h|x) = 1 Z(x,θ) exp{score(x, y, h; θ)} (4) ˜ Y˜ , θ) = The log l"
N12-1023,W02-1001,0,0.135437,"titute Carnegie Mellon University Pittsburgh, PA 15213, USA {kgimpel,nasmith}@cs.cmu.edu Abstract be minimized. We relate well-known training algorithms for MT to particular loss functions. We show that a family of structured ramp loss functions (Do et al., 2008) is useful for this analysis. For example, McAllester and Keshet (2011) recently suggested that, while Chiang et al. (2008, 2009) described their algorithm as “MIRA” (Crammer et al., 2006), in fact it targets a kind of ramp loss. We note here other examples: Liang et al. (2006) described their algorithm as a variant of the perceptron (Collins, 2002), which has a unique loss function, but the loss actually optimized is closer to a particular ramp loss (that differs from the one targeted by Chiang et al.). Och and Ney (2002) sought to optimize log loss (likelihood in a probabilistic model; Lafferty et al., 2001) but actually optimized a version of the soft ramp loss. This paper seeks to close the gap between training algorithms used in statistical machine translation and machine learning, specifically the framework of empirical risk minimization. We review well-known algorithms, arguing that they do not optimize the loss functions they are"
N12-1023,W09-0439,0,0.207542,"t any regularization (i.e., R(θ) = 0).4 The loss is non-convex and not differentiable for cost functions like BLEU, so Och (2003) developed a coordinate ascent procedure with a specialized line search. MERT avoids the need to compute feature vectors for the references (§1(i)) and allows corpuslevel metrics like BLEU to be easily incorporated. However, the complexity of the loss and the difficulty of the search lead to instabilities during learning. Remedies have been suggested, typically involving additional search directions and experiment replicates (Cer et al., 2008; Moore and Quirk, 2008; Foster and Kuhn, 2009; Clark et al., 2011). But despite these improvements, MERT is ineffectual for training weights for large numbers of features; in addition to anecdotal evidence from the MT community, Hopkins and May (2011) illustrated with synthetic data experiments that MERT struggles increasingly to find the optimal solution as the number of parameters grows. Example 2: Probabilistic Models. By exponentiating and normalizing score(x, y, h; θ), we obtain a conditional log-linear model, which is useful for training criteria with probabilistic interpretations: pθ (y, h|x) = 1 Z(x,θ) exp{score(x, y, h; θ)} (4)"
N12-1023,N10-1112,1,0.11129,"Missing"
N12-1023,D11-1125,0,0.298817,"Instead, most have implicit connections to particular forms of ramp loss. We propose to minimize ramp loss directly and present a training algorithm that is easy to implement and that performs comparably to others. Most notably, our structured ramp loss minimization algorithm, R AMPION, is less sensitive to initialization and random seeds than standard approaches. 1 Introduction Every statistical MT system relies on a training algorithm to fit the parameters of a scoring function to examples from parallel text. Well-known examples include MERT (Och, 2003), MIRA (Chiang et al., 2008), and PRO (Hopkins and May, 2011). While such procedures can be analyzed as machine learning algorithms—e.g., in the general framework of empirical risk minimization (Vapnik, 1998)—their procedural specifications have made this difficult. From a practical perspective, such algorithms are often complex, difficult to replicate, and sensitive to initialization, random seeds, and other hyperparameters. In this paper, we consider training algorithms that are first specified declaratively, as loss functions to Why isn’t the application of ML to MT more straightforward? We note two key reasons: (i) ML generally assumes that the corr"
N12-1023,N03-1017,0,0.0188096,"anslations, and the model parameters to a real value indicating the quality of the parameters. Risk minimization corresponds to choosing argminθ∈Θ Ep(X,Y ) [loss (X, Y , θ)] (1) where p(X, Y ) is the (unknown) true joint distribution over corpora. We note that the loss function depends on the entire corpus, while the decoder operates independently on one sentence at a time. This is done to fit the standard assumptions in MT systems: the evaluation metric (e.g., BLEU) depends on 1 For phrase-based MT, a segmentation of the source and target sentences into phrases and an alignment between them (Koehn et al., 2003). For hierarchical phrase-based MT, a derivation under a synchronous CFG (Chiang, 2005). 222 the entire corpus and does not decompose linearly, while the model score does. Since in practice we do not know p(X, Y ), but we do have access to an ac˜ Y˜ i, where X ˜ = {x(i) }N and tual corpus pair hX, i=1 (i) N Y˜ = {y }i=1 , we instead consider regularized empirical risk minimization: ˜ Y˜ , θ) + R(θ) argminθ∈Θ loss(X, (2) where R(θ) is the regularization function used to mitigate overfitting. The regularization function is frequently a squared norm of the parameter vector, such as the `1 or `2 n"
N12-1023,P07-2045,0,0.0115244,"Missing"
N12-1023,P03-1051,0,0.0369964,"Missing"
N12-1023,D09-1005,0,0.024833,"Missing"
N12-1023,P06-1096,0,0.794861,"Missing"
N12-1023,C04-1072,0,0.360718,"Missing"
N12-1023,D08-1076,0,0.170118,"Missing"
N12-1023,C08-1074,0,0.0293292,"from the decoder without any regularization (i.e., R(θ) = 0).4 The loss is non-convex and not differentiable for cost functions like BLEU, so Och (2003) developed a coordinate ascent procedure with a specialized line search. MERT avoids the need to compute feature vectors for the references (§1(i)) and allows corpuslevel metrics like BLEU to be easily incorporated. However, the complexity of the loss and the difficulty of the search lead to instabilities during learning. Remedies have been suggested, typically involving additional search directions and experiment replicates (Cer et al., 2008; Moore and Quirk, 2008; Foster and Kuhn, 2009; Clark et al., 2011). But despite these improvements, MERT is ineffectual for training weights for large numbers of features; in addition to anecdotal evidence from the MT community, Hopkins and May (2011) illustrated with synthetic data experiments that MERT struggles increasingly to find the optimal solution as the number of parameters grows. Example 2: Probabilistic Models. By exponentiating and normalizing score(x, y, h; θ), we obtain a conditional log-linear model, which is useful for training criteria with probabilistic interpretations: pθ (y, h|x) = 1 Z(x,θ) exp{"
N12-1023,P02-1038,0,0.212031,"loss functions. We show that a family of structured ramp loss functions (Do et al., 2008) is useful for this analysis. For example, McAllester and Keshet (2011) recently suggested that, while Chiang et al. (2008, 2009) described their algorithm as “MIRA” (Crammer et al., 2006), in fact it targets a kind of ramp loss. We note here other examples: Liang et al. (2006) described their algorithm as a variant of the perceptron (Collins, 2002), which has a unique loss function, but the loss actually optimized is closer to a particular ramp loss (that differs from the one targeted by Chiang et al.). Och and Ney (2002) sought to optimize log loss (likelihood in a probabilistic model; Lafferty et al., 2001) but actually optimized a version of the soft ramp loss. This paper seeks to close the gap between training algorithms used in statistical machine translation and machine learning, specifically the framework of empirical risk minimization. We review well-known algorithms, arguing that they do not optimize the loss functions they are assumed to optimize when applied to machine translation. Instead, most have implicit connections to particular forms of ramp loss. We propose to minimize ramp loss directly and"
N12-1023,J03-1002,0,0.00451832,"Missing"
N12-1023,P03-1021,0,0.711819,"to optimize when applied to machine translation. Instead, most have implicit connections to particular forms of ramp loss. We propose to minimize ramp loss directly and present a training algorithm that is easy to implement and that performs comparably to others. Most notably, our structured ramp loss minimization algorithm, R AMPION, is less sensitive to initialization and random seeds than standard approaches. 1 Introduction Every statistical MT system relies on a training algorithm to fit the parameters of a scoring function to examples from parallel text. Well-known examples include MERT (Och, 2003), MIRA (Chiang et al., 2008), and PRO (Hopkins and May, 2011). While such procedures can be analyzed as machine learning algorithms—e.g., in the general framework of empirical risk minimization (Vapnik, 1998)—their procedural specifications have made this difficult. From a practical perspective, such algorithms are often complex, difficult to replicate, and sensitive to initialization, random seeds, and other hyperparameters. In this paper, we consider training algorithms that are first specified declaratively, as loss functions to Why isn’t the application of ML to MT more straightforward? We"
N12-1023,2001.mtsummit-papers.68,0,0.0457285,", θ) + R(θ) argminθ∈Θ loss(X, (2) where R(θ) is the regularization function used to mitigate overfitting. The regularization function is frequently a squared norm of the parameter vector, such as the `1 or `2 norm, but many other choices are possible. In this paper, we use `2 . Models are evaluated using a task-specific notion of error, here encoded as a cost function, cost : YN × YN → R≥0 , such that the worse a translation is, the higher its cost. The cost function will typically make use of an automatic evaluation metric for machine translation; e.g., cost might be 1 minus the BLEU score (Papineni et al., 2001).2 We note that our analysis in this paper is applicable for understanding the loss function being optimized given a fixed set of k-best lists.3 However, most training procedures periodically invoke the decoder to generate new k-best lists, which are then typically merged with those from previous training iterations. It is an open question how this practice affects the loss function being optimized by the procedure as a whole. Example 1: MERT. The most commonly-used training algorithm for machine translation is minimum error rate training, which seeks to directly minimize the cost of the predi"
N12-1023,P06-2101,0,0.0414099,"Missing"
N12-1023,D07-1080,0,0.796037,"Missing"
N12-1023,D07-1055,0,0.211629,"Missing"
N12-1023,P02-1040,0,\N,Missing
N12-1069,N10-1083,0,0.110527,"Missing"
N12-1069,D10-1117,0,0.132022,"U NIF U NIF U NIF R AND∗ K&M C CV 1 C CV 2 K&M R AND† K&M K&M K&M K&M0 Train ≤ 10 Test ≤ 10 ≤ ∞ 38.4 31.7 31.4 25.6 43.1 28.6 21.3 17.6 41.0 31.8 44.1 32.9 45.3 30.9 54.3 43.0 61.3 41.4 68.8 63.0 67.7 55.7 64.3 53.3 Train ≤ 20 Test ≤ 10 ≤ ∞ 38.4 31.7 31.0 23.7 43.9 27.1 21.3 16.4 51.9 37.8 53.9 36.7 64.3 53.1 - 59.1‡ Table 1: English attachment accuracies on Section 23, for short sentences (≤10 words) and all (≤∞). We include selected results on this same test set: Shared LN = Cohen and Smith (2009), L-EVG = Headden III et al. (2009), Feature DMV = Berg-Kirkpatrick et al. (2010), LexTSGDMV = Blunsom and Cohn (2010), Posterior Reg. = Gillenwater et al. (2010), Punc/UTags = Spitkovsky et al. (2011a). K&M0 is from Spitkovsky et al. (2011b). ∗ Accuracies are averages over 50 random initializers; σ = 10.9 for test sentences ≤ 10 and 8.3 for all. †Used many random initializers with unsupervised run selection. ‡Used staged training with sentences ≤ 45 words. proportional to their distances, then normalizes to obtain probability distributions. K&M is often described as corresponding to an initial E step for an unspecified model that favors short attachments. Procedure We run EM for our concave models for 100 it"
N12-1069,D10-1118,0,0.0216064,"at we can also include constraints in the sum over possible parents and still preserve concavity. Naseem et al. (2010) found that adding parentchild constraints to a grammar induction system can improve performance dramatically. We employ one simple rule: roots are likely to be verbs.2 We modify C CV 1 to restrict the summation over parents to exclude e00 if the child word is not a verb.3 We only employ this restriction during EM learning for sentences containing at least one verb. For sentences without verbs, we allow all words to be the root. We denote this model by C CV 2. In related work, Brody (2010) also developed grammar induction models based on the IBM word alignment models. However, while our goal is to develop concave models, Brody employed Bayesian nonparametrics in his version of Model 1, which makes the model non-concave. 4 Experiments We ran experiments to determine how well our concave grammar induction models C CV 1 and C CV 2 can perform on their own and when used as initializers for the DMV (Klein and Manning, 2004). The DMV is a generative model of POS tag sequences and projective dependency trees over them. It is the foundation of most state-of-the-art unsupervised grammar"
N12-1069,J93-2003,0,0.158755,"alignment (Brown et al., 1993) and analyze its properties, discussing why other models for unsupervised learning are so seldom concave. We then present concave models for dependency grammar induction and validate them experimentally. We find our concave models to be effective initializers for the dependency model of Klein and Manning (2004) and show that we can encode linguistic knowledge in them for improved performance. 1 Introduction In NLP, unsupervised learning typically implies optimization of a “bumpy” objective function riddled with local maxima. However, one exception is IBM Model 1 (Brown et al., 1993) for word alignment, which is the only model commonly used for unsupervised learning in NLP that has a concave loglikelihood function.1 For other models, such as those used in unsupervised part-of-speech tagging and grammar induction, and indeed for more sophisticated word alignment models, the log-likelihood function maximized by EM is non-concave. As a result, researchers are obligated to consider initialization in addition to model design (Klein and Manning, 2004; Goldberg et al., 2008). For example, consider the dependency grammar induction results shown in Table 1 when training the 1 It i"
N12-1069,W06-2920,0,0.0851498,"4 37.5 / 30.9 43.7 / 35.5 el hu 37/32 23/18 50/41 23/20 51/45 32/28 50/45 60/46 avg. log-likelihood -15.05 -14.84 -14.93 -14.45 Table 2: Test set attachment accuracies for 18 languages; first number in each cell is accuracy for sentences ≤ 10 words and second is for all sentences. For training, sentences ≤ 10 words from each treebank were used. In order, languages are Basque, Bulgarian, Catalan, Chinese, Czech, Danish, Dutch, English, German, Greek, Hungarian, Italian, Japanese, Portuguese, Slovenian, Spanish, Swedish, and Turkish. Data We use data prepared for the CoNLL 2006/07 shared tasks (Buchholz and Marsi, 2006; Nivre et al., 2007).4 We follow standard practice in removing punctuation and using short sentences (≤ 10 or ≤ 20 words) for training. For all experiments, we train on separate data from that used for testing and use gold POS tags for both training and testing. We report accuracy on (i) test set sentences ≤ 10 words and (ii) all sentences from the test set. Results Results for English are shown in Tab. 1. We train on §2–21 and test on §23 in the Penn Treebank. The constraint on sentence roots helps a great deal, as C CV 2 by itself is competitive with the DMV when testing on short sentences."
N12-1069,N09-1009,1,0.848241,". 2 579 Model ATT R IGHT C CV 1 C CV 2 DMV Shared LN L-EVG Feature DMV LexTSG-DMV Posterior Reg. Punc/UTags Init. N/A U NIF U NIF U NIF R AND∗ K&M C CV 1 C CV 2 K&M R AND† K&M K&M K&M K&M0 Train ≤ 10 Test ≤ 10 ≤ ∞ 38.4 31.7 31.4 25.6 43.1 28.6 21.3 17.6 41.0 31.8 44.1 32.9 45.3 30.9 54.3 43.0 61.3 41.4 68.8 63.0 67.7 55.7 64.3 53.3 Train ≤ 20 Test ≤ 10 ≤ ∞ 38.4 31.7 31.0 23.7 43.9 27.1 21.3 16.4 51.9 37.8 53.9 36.7 64.3 53.1 - 59.1‡ Table 1: English attachment accuracies on Section 23, for short sentences (≤10 words) and all (≤∞). We include selected results on this same test set: Shared LN = Cohen and Smith (2009), L-EVG = Headden III et al. (2009), Feature DMV = Berg-Kirkpatrick et al. (2010), LexTSGDMV = Blunsom and Cohn (2010), Posterior Reg. = Gillenwater et al. (2010), Punc/UTags = Spitkovsky et al. (2011a). K&M0 is from Spitkovsky et al. (2011b). ∗ Accuracies are averages over 50 random initializers; σ = 10.9 for test sentences ≤ 10 and 8.3 for all. †Used many random initializers with unsupervised run selection. ‡Used staged training with sentences ≤ 45 words. proportional to their distances, then normalizes to obtain probability distributions. K&M is often described as corresponding to an initia"
N12-1069,P08-1085,0,0.0690685,"Missing"
N12-1069,N09-1012,0,0.205543,"Missing"
N12-1069,P04-1061,0,0.514135,". Smith Language Technologies Institute Carnegie Mellon University Pittsburgh, PA 15213, USA {kgimpel,nasmith}@cs.cmu.edu Abstract We investigate models for unsupervised learning with concave log-likelihood functions. We begin with the most well-known example, IBM Model 1 for word alignment (Brown et al., 1993) and analyze its properties, discussing why other models for unsupervised learning are so seldom concave. We then present concave models for dependency grammar induction and validate them experimentally. We find our concave models to be effective initializers for the dependency model of Klein and Manning (2004) and show that we can encode linguistic knowledge in them for improved performance. 1 Introduction In NLP, unsupervised learning typically implies optimization of a “bumpy” objective function riddled with local maxima. However, one exception is IBM Model 1 (Brown et al., 1993) for word alignment, which is the only model commonly used for unsupervised learning in NLP that has a concave loglikelihood function.1 For other models, such as those used in unsupervised part-of-speech tagging and grammar induction, and indeed for more sophisticated word alignment models, the log-likelihood function max"
N12-1069,W11-3901,0,0.255875,"Missing"
N12-1069,D10-1120,0,0.332351,"the conditions observed. This conditioning information may innecessary to develop concave models for two tasks. clude the direction of the edge, its distance, and any properties about the words in the sentence. We 3.1 Part-of-Speech Tagging found that conditioning on direction improved perConsider a standard first-order hidden Markov formance: we rewrite the c distributions as c(ej | model for POS tagging. Letting y denote the tag e0i , sign(j − i)) and denote this model by C CV 1. 578 We note that we can also include constraints in the sum over possible parents and still preserve concavity. Naseem et al. (2010) found that adding parentchild constraints to a grammar induction system can improve performance dramatically. We employ one simple rule: roots are likely to be verbs.2 We modify C CV 1 to restrict the summation over parents to exclude e00 if the child word is not a verb.3 We only employ this restriction during EM learning for sentences containing at least one verb. For sentences without verbs, we allow all words to be the root. We denote this model by C CV 2. In related work, Brody (2010) also developed grammar induction models based on the IBM word alignment models. However, while our goal i"
N12-1069,petrov-etal-2012-universal,0,0.0450502,"d in our concave models. The DMV also has multinomial distributions for deciding whether to stop or continue generating children in each direction considering whether any children have already been generated in that direction. The majority of researchers use the original initializer from Klein and Manning (2004), denoted here K&M. K&M is a deterministic harmonic initializer that sets parent-child token affinities inversely ˇ This is similar to the rule used by Mareˇcek and Zabokrtsk´ y (2011) with empirical success. 3 As verbs, we take all tags that map to V in the universal tag mappings from Petrov et al. (2012). Thus, to apply this constraint to a new language, one would have to produce a similar tag mapping or identify verb tags through manual inspection. 2 579 Model ATT R IGHT C CV 1 C CV 2 DMV Shared LN L-EVG Feature DMV LexTSG-DMV Posterior Reg. Punc/UTags Init. N/A U NIF U NIF U NIF R AND∗ K&M C CV 1 C CV 2 K&M R AND† K&M K&M K&M K&M0 Train ≤ 10 Test ≤ 10 ≤ ∞ 38.4 31.7 31.4 25.6 43.1 28.6 21.3 17.6 41.0 31.8 44.1 32.9 45.3 30.9 54.3 43.0 61.3 41.4 68.8 63.0 67.7 55.7 64.3 53.3 Train ≤ 20 Test ≤ 10 ≤ ∞ 38.4 31.7 31.0 23.7 43.9 27.1 21.3 16.4 51.9 37.8 53.9 36.7 64.3 53.1 - 59.1‡ Table 1: English"
N12-1069,N10-1116,0,0.434287,"Missing"
N12-1069,D11-1118,0,0.116615,"Test ≤ 10 ≤ ∞ 38.4 31.7 31.4 25.6 43.1 28.6 21.3 17.6 41.0 31.8 44.1 32.9 45.3 30.9 54.3 43.0 61.3 41.4 68.8 63.0 67.7 55.7 64.3 53.3 Train ≤ 20 Test ≤ 10 ≤ ∞ 38.4 31.7 31.0 23.7 43.9 27.1 21.3 16.4 51.9 37.8 53.9 36.7 64.3 53.1 - 59.1‡ Table 1: English attachment accuracies on Section 23, for short sentences (≤10 words) and all (≤∞). We include selected results on this same test set: Shared LN = Cohen and Smith (2009), L-EVG = Headden III et al. (2009), Feature DMV = Berg-Kirkpatrick et al. (2010), LexTSGDMV = Blunsom and Cohn (2010), Posterior Reg. = Gillenwater et al. (2010), Punc/UTags = Spitkovsky et al. (2011a). K&M0 is from Spitkovsky et al. (2011b). ∗ Accuracies are averages over 50 random initializers; σ = 10.9 for test sentences ≤ 10 and 8.3 for all. †Used many random initializers with unsupervised run selection. ‡Used staged training with sentences ≤ 45 words. proportional to their distances, then normalizes to obtain probability distributions. K&M is often described as corresponding to an initial E step for an unspecified model that favors short attachments. Procedure We run EM for our concave models for 100 iterations. We evaluate the learned models directly as parsers on the test data and"
N12-1069,W11-0303,0,0.097129,"Test ≤ 10 ≤ ∞ 38.4 31.7 31.4 25.6 43.1 28.6 21.3 17.6 41.0 31.8 44.1 32.9 45.3 30.9 54.3 43.0 61.3 41.4 68.8 63.0 67.7 55.7 64.3 53.3 Train ≤ 20 Test ≤ 10 ≤ ∞ 38.4 31.7 31.0 23.7 43.9 27.1 21.3 16.4 51.9 37.8 53.9 36.7 64.3 53.1 - 59.1‡ Table 1: English attachment accuracies on Section 23, for short sentences (≤10 words) and all (≤∞). We include selected results on this same test set: Shared LN = Cohen and Smith (2009), L-EVG = Headden III et al. (2009), Feature DMV = Berg-Kirkpatrick et al. (2010), LexTSGDMV = Blunsom and Cohn (2010), Posterior Reg. = Gillenwater et al. (2010), Punc/UTags = Spitkovsky et al. (2011a). K&M0 is from Spitkovsky et al. (2011b). ∗ Accuracies are averages over 50 random initializers; σ = 10.9 for test sentences ≤ 10 and 8.3 for all. †Used many random initializers with unsupervised run selection. ‡Used staged training with sentences ≤ 45 words. proportional to their distances, then normalizes to obtain probability distributions. K&M is often described as corresponding to an initial E step for an unspecified model that favors short attachments. Procedure We run EM for our concave models for 100 iterations. We evaluate the learned models directly as parsers on the test data and"
N12-1069,P11-2081,0,0.0361349,"Missing"
N12-1069,D07-1096,0,\N,Missing
N12-1086,P11-1061,1,0.342589,"2009). For us, graph vertices correspond to natural language types (not tokens) and undirected edges between them are weighted using a similarity metric. Recently, this setup has been used to learn soft labels on natural language types (say, word n-grams or syntactically disambiguated predicates) from seed data, resulting in large but noisy lexicons, which are used to constrain structured prediction models. Applications have ranged from domain adaptation of part-of-speech (POS) taggers (Subramanya et al., 2010), unsupervised learning of POS taggers by using bilingual graph-based projections (Das and Petrov, 2011), and shallow semantic parsing for unknown predicates (Das and Smith, 2011). However, none of the above captured the empirical fact that only a few categories typically associate with a given type (vertex). Take the case of POS tagging: Subramanya et al. (2010) construct a graph over trigram types as vertices, with 45 possible tags for the middle word of a trigram as the 677 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 677–687, c Montr´eal, Canada, June 3-8, 2012. 2012 Association for Computational Linguistic"
N12-1086,P11-1144,1,0.41045,"ens) and undirected edges between them are weighted using a similarity metric. Recently, this setup has been used to learn soft labels on natural language types (say, word n-grams or syntactically disambiguated predicates) from seed data, resulting in large but noisy lexicons, which are used to constrain structured prediction models. Applications have ranged from domain adaptation of part-of-speech (POS) taggers (Subramanya et al., 2010), unsupervised learning of POS taggers by using bilingual graph-based projections (Das and Petrov, 2011), and shallow semantic parsing for unknown predicates (Das and Smith, 2011). However, none of the above captured the empirical fact that only a few categories typically associate with a given type (vertex). Take the case of POS tagging: Subramanya et al. (2010) construct a graph over trigram types as vertices, with 45 possible tags for the middle word of a trigram as the 677 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 677–687, c Montr´eal, Canada, June 3-8, 2012. 2012 Association for Computational Linguistics label set for each vertex. It is empirically observed that contextualized"
N12-1086,P09-1056,0,0.00726954,"roblems such as the induction of labels on natural language types, which typically associate with only a few labels. Compared to standard graph-based learning methods, for two lexicon expansion problems, our approach produces significantly smaller lexicons and obtains better predictive performance. 1 Introduction Semi-supervised learning (SSL) is attractive for the learning of complex phenomena, for example, linguistic structure, where data annotation is expensive. Natural language processing applications have benefited from various SSL techniques, such as distributional word representations (Huang and Yates, 2009; Turian et al., 2010; Dhillon et al., 2011), self-training (McClosky et al., 2006), and entropy regularization (Jiao et al., 2006; Smith and Eisner, 2007). In this paper, we focus on semi-supervised learning that uses a graph constructed from labeled and unlabeled data. This framework, graph-based SSL—see Bengio et al. (2006) and Zhu (2008) for introductory material on this topic—has been widely used and has been shown to perform better than several other semi-supervised algorithms on benchmark datasets (Chapelle et al., 2006, ch. 21). The method constructs a graph where a small portion of ve"
N12-1086,P06-1027,0,0.02999,"ard graph-based learning methods, for two lexicon expansion problems, our approach produces significantly smaller lexicons and obtains better predictive performance. 1 Introduction Semi-supervised learning (SSL) is attractive for the learning of complex phenomena, for example, linguistic structure, where data annotation is expensive. Natural language processing applications have benefited from various SSL techniques, such as distributional word representations (Huang and Yates, 2009; Turian et al., 2010; Dhillon et al., 2011), self-training (McClosky et al., 2006), and entropy regularization (Jiao et al., 2006; Smith and Eisner, 2007). In this paper, we focus on semi-supervised learning that uses a graph constructed from labeled and unlabeled data. This framework, graph-based SSL—see Bengio et al. (2006) and Zhu (2008) for introductory material on this topic—has been widely used and has been shown to perform better than several other semi-supervised algorithms on benchmark datasets (Chapelle et al., 2006, ch. 21). The method constructs a graph where a small portion of vertices correspond to labeled instances, and the rest are unlabeled. Pairs of vertices are connected by weighted edges denoting the"
N12-1086,J93-2004,0,0.0485563,"zed measures at each graph vertex. We achieve this by penalizing the graph propagation objective with the `1 norm or the mixed `1,2 norm (Kowalski and Torr´esani, 2009) of the measures at each vertex, aiming for global and vertex-level sparsity, respectively. Importantly, the proposed graph objective functions are convex, so we avoid degenerate solutions and local minima. We present experiments on two natural language lexicon expansion problems in a semi-supervised setting: (i) inducing distributions of POS tags over n-gram types in the Wall Street Journal section of the Penn Treebank corpus (Marcus et al., 1993) and (ii) inducing distributions of semantic frames (Fillmore, 1982) over predicates unseen in anno1 Moreover, we also assume the edge weights in a given graph are unconstrained, consistent with prior work on graphbased SSL (Das and Petrov, 2011; Das and Smith, 2011; Subramanya and Bilmes, 2008; Subramanya and Bilmes, 2009; Subramanya et al., 2010; Zhu and Ghahramani, 2002). 678 tated data. Our methods produce sparse measures at graph vertices resulting in compact lexicons, and also result in better performance with respect to label propagation using Gaussian penalties (Zhu and Ghahramani, 200"
N12-1086,N06-1020,0,0.00484305,"associate with only a few labels. Compared to standard graph-based learning methods, for two lexicon expansion problems, our approach produces significantly smaller lexicons and obtains better predictive performance. 1 Introduction Semi-supervised learning (SSL) is attractive for the learning of complex phenomena, for example, linguistic structure, where data annotation is expensive. Natural language processing applications have benefited from various SSL techniques, such as distributional word representations (Huang and Yates, 2009; Turian et al., 2010; Dhillon et al., 2011), self-training (McClosky et al., 2006), and entropy regularization (Jiao et al., 2006; Smith and Eisner, 2007). In this paper, we focus on semi-supervised learning that uses a graph constructed from labeled and unlabeled data. This framework, graph-based SSL—see Bengio et al. (2006) and Zhu (2008) for introductory material on this topic—has been widely used and has been shown to perform better than several other semi-supervised algorithms on benchmark datasets (Chapelle et al., 2006, ch. 21). The method constructs a graph where a small portion of vertices correspond to labeled instances, and the rest are unlabeled. Pairs of vertic"
N12-1086,D07-1070,0,0.0248675,"rning methods, for two lexicon expansion problems, our approach produces significantly smaller lexicons and obtains better predictive performance. 1 Introduction Semi-supervised learning (SSL) is attractive for the learning of complex phenomena, for example, linguistic structure, where data annotation is expensive. Natural language processing applications have benefited from various SSL techniques, such as distributional word representations (Huang and Yates, 2009; Turian et al., 2010; Dhillon et al., 2011), self-training (McClosky et al., 2006), and entropy regularization (Jiao et al., 2006; Smith and Eisner, 2007). In this paper, we focus on semi-supervised learning that uses a graph constructed from labeled and unlabeled data. This framework, graph-based SSL—see Bengio et al. (2006) and Zhu (2008) for introductory material on this topic—has been widely used and has been shown to perform better than several other semi-supervised algorithms on benchmark datasets (Chapelle et al., 2006, ch. 21). The method constructs a graph where a small portion of vertices correspond to labeled instances, and the rest are unlabeled. Pairs of vertices are connected by weighted edges denoting the similarity between the p"
N12-1086,D08-1114,0,0.102509,"his topic—has been widely used and has been shown to perform better than several other semi-supervised algorithms on benchmark datasets (Chapelle et al., 2006, ch. 21). The method constructs a graph where a small portion of vertices correspond to labeled instances, and the rest are unlabeled. Pairs of vertices are connected by weighted edges denoting the similarity between the pair. Traditionally, Markov random walks (Szummer and Jaakkola, 2001; Baluja et al., 2008) or optimization of a loss function based on smoothness properties of the graph (Corduneanu and Jaakkola, 2003; Zhu et al., 2003; Subramanya and Bilmes, 2008, inter alia) are performed to propagate labels from the labeled vertices to the unlabeled ones. In this work, we are interested in multi-class generalizations of graph-propagation algorithms suitable for NLP applications, where each graph vertex can assume one or more out of many possible labels (Talukdar and Crammer, 2009; Subramanya and Bilmes, 2008, 2009). For us, graph vertices correspond to natural language types (not tokens) and undirected edges between them are weighted using a similarity metric. Recently, this setup has been used to learn soft labels on natural language types (say, wo"
N12-1086,D10-1017,0,0.401698,"an assume one or more out of many possible labels (Talukdar and Crammer, 2009; Subramanya and Bilmes, 2008, 2009). For us, graph vertices correspond to natural language types (not tokens) and undirected edges between them are weighted using a similarity metric. Recently, this setup has been used to learn soft labels on natural language types (say, word n-grams or syntactically disambiguated predicates) from seed data, resulting in large but noisy lexicons, which are used to constrain structured prediction models. Applications have ranged from domain adaptation of part-of-speech (POS) taggers (Subramanya et al., 2010), unsupervised learning of POS taggers by using bilingual graph-based projections (Das and Petrov, 2011), and shallow semantic parsing for unknown predicates (Das and Smith, 2011). However, none of the above captured the empirical fact that only a few categories typically associate with a given type (vertex). Take the case of POS tagging: Subramanya et al. (2010) construct a graph over trigram types as vertices, with 45 possible tags for the middle word of a trigram as the 677 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technol"
N12-1086,P10-1040,0,0.0187725,"uction of labels on natural language types, which typically associate with only a few labels. Compared to standard graph-based learning methods, for two lexicon expansion problems, our approach produces significantly smaller lexicons and obtains better predictive performance. 1 Introduction Semi-supervised learning (SSL) is attractive for the learning of complex phenomena, for example, linguistic structure, where data annotation is expensive. Natural language processing applications have benefited from various SSL techniques, such as distributional word representations (Huang and Yates, 2009; Turian et al., 2010; Dhillon et al., 2011), self-training (McClosky et al., 2006), and entropy regularization (Jiao et al., 2006; Smith and Eisner, 2007). In this paper, we focus on semi-supervised learning that uses a graph constructed from labeled and unlabeled data. This framework, graph-based SSL—see Bengio et al. (2006) and Zhu (2008) for introductory material on this topic—has been widely used and has been shown to perform better than several other semi-supervised algorithms on benchmark datasets (Chapelle et al., 2006, ch. 21). The method constructs a graph where a small portion of vertices correspond to"
N12-1086,S07-1018,0,\N,Missing
N12-1097,N09-1031,1,0.851071,"r experimental results using text underscore the importance of considering the substance of policy proposals (here, bills) when attempting to explain their progress. An important research direction in political science, one in which NLP must play a role, is how different types of issues are managed in legislatures. Our results also suggest that political considerations may induce lawmakers to sponsor certain types of bills with no real expectation of seeing them enacted into law. Considerable recent work has modeled text alongside data about social behavior. This includes predictive settings (Kogan et al., 2009; Lerman et al., 2008), various kinds of sentiment and opinion analysis (Thomas et al., 2006; Monroe et al., 2008; O’Connor et al., 2010; Das et al., 2009), and exploratory models (Steyvers and Griffiths, 2007). In political science specifically, the “text as data” movement (Grimmer and Stewart, 2012; O’Connor et al., 2011) has leveraged tools from NLP in quantitative research. For example, Grimmer (2010) and Quinn et al. (2006) used topic models to study, respectively, Supreme Court proceedings and Senate speeches. Closest to this work, Gerrish and Blei (2011) combined topic models with spati"
N12-1097,C08-1060,0,0.0149336,"ts using text underscore the importance of considering the substance of policy proposals (here, bills) when attempting to explain their progress. An important research direction in political science, one in which NLP must play a role, is how different types of issues are managed in legislatures. Our results also suggest that political considerations may induce lawmakers to sponsor certain types of bills with no real expectation of seeing them enacted into law. Considerable recent work has modeled text alongside data about social behavior. This includes predictive settings (Kogan et al., 2009; Lerman et al., 2008), various kinds of sentiment and opinion analysis (Thomas et al., 2006; Monroe et al., 2008; O’Connor et al., 2010; Das et al., 2009), and exploratory models (Steyvers and Griffiths, 2007). In political science specifically, the “text as data” movement (Grimmer and Stewart, 2012; O’Connor et al., 2011) has leveraged tools from NLP in quantitative research. For example, Grimmer (2010) and Quinn et al. (2006) used topic models to study, respectively, Supreme Court proceedings and Senate speeches. Closest to this work, Gerrish and Blei (2011) combined topic models with spatial roll call models to"
N12-1097,P04-1035,0,0.00519156,"Missing"
N12-1097,W06-1639,0,0.139631,"policy proposals (here, bills) when attempting to explain their progress. An important research direction in political science, one in which NLP must play a role, is how different types of issues are managed in legislatures. Our results also suggest that political considerations may induce lawmakers to sponsor certain types of bills with no real expectation of seeing them enacted into law. Considerable recent work has modeled text alongside data about social behavior. This includes predictive settings (Kogan et al., 2009; Lerman et al., 2008), various kinds of sentiment and opinion analysis (Thomas et al., 2006; Monroe et al., 2008; O’Connor et al., 2010; Das et al., 2009), and exploratory models (Steyvers and Griffiths, 2007). In political science specifically, the “text as data” movement (Grimmer and Stewart, 2012; O’Connor et al., 2011) has leveraged tools from NLP in quantitative research. For example, Grimmer (2010) and Quinn et al. (2006) used topic models to study, respectively, Supreme Court proceedings and Senate speeches. Closest to this work, Gerrish and Blei (2011) combined topic models with spatial roll call models to predict votes in the legislature from text 801 Conclusions We present"
N12-4002,N12-4002,1,0.0513054,"Missing"
N13-1039,P11-1087,0,0.0199181,"of single-word contractions for “going to” and “trying to,” some of which have more complicated semantics.6 Finally, the HMM learns about orthographic variants, even though it treats all words as opaque symbols; cluster F consists almost entirely of variants of “so,” their frequencies monotonically decreasing in the number of vowel repetitions—a phenomenon called “expressive lengthening” or “affective lengthening” (Brody and Diakopoulos, 2011; Schnoebelen, 2012). This suggests a future direction to jointly model class sequence and orthographic information (Clark, 2003; Smith and Eisner, 2005; Blunsom and Cohn, 2011). We have built an HTML viewer to browse these and numerous other interesting examples.7 3.3 Emoticons and Emoji We use the term emoticon to mean a face or icon constructed with traditional alphabetic or punctua6 One coauthor, a native speaker of the Texan English dialect, notes “finna” (short for “fixing to”, cluster E1) may be an immediate future auxiliary, indicating an immediate future tense that is present in many languages (though not in standard English). To illustrate: “She finna go” approximately means “She will go,” but sooner, in the sense of “She is about to go.” 7 http://www.ark.c"
N13-1039,D11-1052,0,0.403712,": u = “you”) and prepositions (C: fir = “for”). There is also evidence of grammatical categories specific to conversational genres of English; clusters E1–E2 demonstrate variations of single-word contractions for “going to” and “trying to,” some of which have more complicated semantics.6 Finally, the HMM learns about orthographic variants, even though it treats all words as opaque symbols; cluster F consists almost entirely of variants of “so,” their frequencies monotonically decreasing in the number of vowel repetitions—a phenomenon called “expressive lengthening” or “affective lengthening” (Brody and Diakopoulos, 2011; Schnoebelen, 2012). This suggests a future direction to jointly model class sequence and orthographic information (Clark, 2003; Smith and Eisner, 2005; Blunsom and Cohn, 2011). We have built an HTML viewer to browse these and numerous other interesting examples.7 3.3 Emoticons and Emoji We use the term emoticon to mean a face or icon constructed with traditional alphabetic or punctua6 One coauthor, a native speaker of the Texan English dialect, notes “finna” (short for “fixing to”, cluster E1) may be an immediate future auxiliary, indicating an immediate future tense that is present in many"
N13-1039,J92-4003,0,0.437604,"Missing"
N13-1039,E03-1009,0,0.019282,"lusters E1–E2 demonstrate variations of single-word contractions for “going to” and “trying to,” some of which have more complicated semantics.6 Finally, the HMM learns about orthographic variants, even though it treats all words as opaque symbols; cluster F consists almost entirely of variants of “so,” their frequencies monotonically decreasing in the number of vowel repetitions—a phenomenon called “expressive lengthening” or “affective lengthening” (Brody and Diakopoulos, 2011; Schnoebelen, 2012). This suggests a future direction to jointly model class sequence and orthographic information (Clark, 2003; Smith and Eisner, 2005; Blunsom and Cohn, 2011). We have built an HTML viewer to browse these and numerous other interesting examples.7 3.3 Emoticons and Emoji We use the term emoticon to mean a face or icon constructed with traditional alphabetic or punctua6 One coauthor, a native speaker of the Texan English dialect, notes “finna” (short for “fixing to”, cluster E1) may be an immediate future auxiliary, indicating an immediate future tense that is present in many languages (though not in standard English). To illustrate: “She finna go” approximately means “She will go,” but sooner, in the"
N13-1039,P11-1137,1,0.142556,"g. clusters E1 and E2 in Fig. 2); and the word imma is the 962nd most common word in our unlabeled corpus, more frequent than cat or near. We do not attempt to do Twitter “normalization” into traditional written English (Han and Baldwin, 2011), which we view as a lossy translation task. In fact, many of Twitter’s unique linguistic phenomena are due not only to its informal nature, but also a set of authors that heavily skews towards younger ages and minorities, with heavy usage of dialects that are different than the standard American English most often seen in NLP datasets (Eisenstein, 2013; Eisenstein et al., 2011). For example, we suspect that imma may implicate tense and aspect markers from African-American Vernacular English.14 Trying to impose PTB-style tokenization on Twitter is linguistically inappropriate: should the lexico-syntactic behavior of casual conversational chatter by young minorities be straightjacketed into the stylistic conventions of the 1980s Wall Street Journal? Instead, we would like to directly analyze the syntax of online conversational text on its own terms. Thus, we choose to leave these word forms untokenized and use compound tags, viewing compositional multiword analysis as"
N13-1039,N13-1037,0,0.383435,"rs corresponding to some of these words. This paper describes release 0.3 of the “CMU Twitter Part-of-Speech Tagger” and annotated data. Introduction Online conversational text, typified by microblogs, chat, and text messages,1 is a challenge for natural language processing. Unlike the highly edited genres that conventional NLP tools have been developed for, conversational text contains many nonstandard lexical items and syntactic patterns. These are the result of unintentional errors, dialectal variation, conversational ellipsis, topic diversity, and creative use of language and orthography (Eisenstein, 2013). An example is shown in Fig. 1. As a result of this widespread variation, standard modeling assumptions that depend on lexical, syntactic, and orthographic regularity are inappropriate. There 1 smh ! ∧ http://www.ark.cs.cmu.edu/TweetNLP 1 ikr Also referred to as computer-mediated communication. is preliminary work on social media part-of-speech (POS) tagging (Gimpel et al., 2011), named entity recognition (Ritter et al., 2011; Liu et al., 2011), and parsing (Foster et al., 2011), but accuracy rates are still significantly lower than traditional well-edited genres like newswire. Even web text"
N13-1039,D11-1142,0,0.0448156,"g minorities be straightjacketed into the stylistic conventions of the 1980s Wall Street Journal? Instead, we would like to directly analyze the syntax of online conversational text on its own terms. Thus, we choose to leave these word forms untokenized and use compound tags, viewing compositional multiword analysis as challenging future work.15 We believe that our strategy is sufficient for many applications, such as chunking or named entity recognition; many applications such as sentiment analysis (Turney, 2002; Pang and Lee, 2008, §4.2.3), open information extraction (Carlson et al., 2010; Fader et al., 2011), and information retrieval (Allan and Raghavan, 2002) use POS 14 See “Tense and aspect” examples in http: //en.wikipedia.org/wiki/African_American_ Vernacular_English 15 For example, wtf has compositional behavior in “Wtf just happened??”, but only debatably so in “Huh wtf”. 90 Tagset Dates App. A Oct 27-28, 2010 App. A Jan 2011–Jun 2012 PTB-like Oct–Nov 2006 PTB-like unknown 85 ● Table 1: Annotated datasets: number of messages, tokens, tagset, and date range. More information in §5, §6.3, and §6.2. 6 Experiments We are primarily concerned with performance on our annotated datasets described"
N13-1039,P11-2008,1,0.727026,"Missing"
N13-1039,P11-1038,0,0.422239,"to analyze doncha as do/VBP ncha/PRP, but notes it would be difficult. We think this is impossible to handle in the rulebased framework used by English tokenizers, given the huge (and possibly growing) number of large compounds like imma, gonna, w/that, etc. These are not rare: the word clustering algorithm discovers hundreds of such words as statistically coherent classes (e.g. clusters E1 and E2 in Fig. 2); and the word imma is the 962nd most common word in our unlabeled corpus, more frequent than cat or near. We do not attempt to do Twitter “normalization” into traditional written English (Han and Baldwin, 2011), which we view as a lossy translation task. In fact, many of Twitter’s unique linguistic phenomena are due not only to its informal nature, but also a set of authors that heavily skews towards younger ages and minorities, with heavy usage of dialects that are different than the standard American English most often seen in NLP datasets (Eisenstein, 2013; Eisenstein et al., 2011). For example, we suspect that imma may implicate tense and aspect markers from African-American Vernacular English.14 Trying to impose PTB-style tokenization on Twitter is linguistically inappropriate: should the lexic"
N13-1039,P08-1068,0,0.420847,"t for social media analysis applications—which often require the processing of millions to billions of messages—so we make greedy decoding the default in the released software. P|x| Unsupervised Word Clusters Our POS tagger can make use of any number of possibly overlapping features. While we have only a small amount of hand-labeled data for training, we also have access to billions of tokens of unlabeled conversational text from the web. Previous work has shown that unlabeled text can be used to induce unsupervised word clusters which can improve the performance of many supervised NLP tasks (Koo et al., 2008; Turian et al., 2010; Täckström et al., 2012, inter alia). We use a similar approach here to improve tagging performance for online conversational text. We also make our induced clusters publicly available in the hope that they will be useful for other NLP tasks in this genre. 3 Runtimes observed on an Intel Core i5 2.4 GHz laptop. A1 A2 A3 A4 A5 B C D E1 E2 F G1 G2 G3 G4 Binary path Top words (by frequency) 111010100010 111010100011 111010100100 111010100101 11101011011100 lmao lmfao lmaoo lmaooo hahahahaha lool ctfu rofl loool lmfaoo lmfaooo lmaoooo lmbo lololol haha hahaha hehe hahahaha ha"
N13-1039,P11-1037,0,0.0938106,"e are the result of unintentional errors, dialectal variation, conversational ellipsis, topic diversity, and creative use of language and orthography (Eisenstein, 2013). An example is shown in Fig. 1. As a result of this widespread variation, standard modeling assumptions that depend on lexical, syntactic, and orthographic regularity are inappropriate. There 1 smh ! ∧ http://www.ark.cs.cmu.edu/TweetNLP 1 ikr Also referred to as computer-mediated communication. is preliminary work on social media part-of-speech (POS) tagging (Gimpel et al., 2011), named entity recognition (Ritter et al., 2011; Liu et al., 2011), and parsing (Foster et al., 2011), but accuracy rates are still significantly lower than traditional well-edited genres like newswire. Even web text parsing, which is a comparatively easier genre than social media, lags behind newspaper text (Petrov and McDonald, 2012), as does speech transcript parsing (McClosky et al., 2010). To tackle the challenge of novel words and constructions, we create a new Twitter part-of-speech tagger—building on previous work by Gimpel et al. (2011)—that includes new large-scale distributional features. This leads to state-of-the-art results in POS tagging for b"
N13-1039,P12-3005,0,0.246426,"Missing"
N13-1039,J93-2004,0,0.061082,"Missing"
N13-1039,N10-1004,0,0.161793,"regularity are inappropriate. There 1 smh ! ∧ http://www.ark.cs.cmu.edu/TweetNLP 1 ikr Also referred to as computer-mediated communication. is preliminary work on social media part-of-speech (POS) tagging (Gimpel et al., 2011), named entity recognition (Ritter et al., 2011; Liu et al., 2011), and parsing (Foster et al., 2011), but accuracy rates are still significantly lower than traditional well-edited genres like newswire. Even web text parsing, which is a comparatively easier genre than social media, lags behind newspaper text (Petrov and McDonald, 2012), as does speech transcript parsing (McClosky et al., 2010). To tackle the challenge of novel words and constructions, we create a new Twitter part-of-speech tagger—building on previous work by Gimpel et al. (2011)—that includes new large-scale distributional features. This leads to state-of-the-art results in POS tagging for both Twitter and Internet Relay Chat (IRC) text. We also annotated a new dataset of tweets with POS tags, improved the annotations in the previous dataset from Gimpel et al., and developed annotation guidelines for manual POS tagging of tweets. We release all of these resources to the research community: • an open-source part-of-"
N13-1039,W96-0213,0,0.684593,"community: • an open-source part-of-speech tagger for online conversational text (§2); • unsupervised Twitter word clusters (§3); • an improved emoticon detector for conversational text (§4); 380 Proceedings of NAACL-HLT 2013, pages 380–390, c Atlanta, Georgia, 9–14 June 2013. 2013 Association for Computational Linguistics • POS annotation guidelines (§5.1); and • a new dataset of 547 manually POS-annotated tweets (§5). 2 MEMM Tagger Our tagging model is a first-order maximum entropy Markov model (MEMM), a discriminative sequence model for which training and decoding are extremely efficient (Ratnaparkhi, 1996; McCallum et al., 2000).2 The probability of a tag yt is conditioned on the input sequence x and the tag to its left yt−1 , and is parameterized by a multiclass logistic regression: p(yt = k |y t−1 , x, t; β) ∝   P (obs) (trans) exp βyt−1 ,k + j βj,k fj (x, t) We use transition features for every pair of labels, and extract base observation features from token t and neighboring tokens, and conjoin them against all K = 25 possible outputs in our coarse tagset (Appendix A). Our feature sets will be discussed below in detail. Decoding. For experiments reported in this paper, we use the O(|x|K"
N13-1039,D11-1141,0,0.44769,"tactic patterns. These are the result of unintentional errors, dialectal variation, conversational ellipsis, topic diversity, and creative use of language and orthography (Eisenstein, 2013). An example is shown in Fig. 1. As a result of this widespread variation, standard modeling assumptions that depend on lexical, syntactic, and orthographic regularity are inappropriate. There 1 smh ! ∧ http://www.ark.cs.cmu.edu/TweetNLP 1 ikr Also referred to as computer-mediated communication. is preliminary work on social media part-of-speech (POS) tagging (Gimpel et al., 2011), named entity recognition (Ritter et al., 2011; Liu et al., 2011), and parsing (Foster et al., 2011), but accuracy rates are still significantly lower than traditional well-edited genres like newswire. Even web text parsing, which is a comparatively easier genre than social media, lags behind newspaper text (Petrov and McDonald, 2012), as does speech transcript parsing (McClosky et al., 2010). To tackle the challenge of novel words and constructions, we create a new Twitter part-of-speech tagger—building on previous work by Gimpel et al. (2011)—that includes new large-scale distributional features. This leads to state-of-the-art results i"
N13-1039,P05-1044,1,0.328937,"demonstrate variations of single-word contractions for “going to” and “trying to,” some of which have more complicated semantics.6 Finally, the HMM learns about orthographic variants, even though it treats all words as opaque symbols; cluster F consists almost entirely of variants of “so,” their frequencies monotonically decreasing in the number of vowel repetitions—a phenomenon called “expressive lengthening” or “affective lengthening” (Brody and Diakopoulos, 2011; Schnoebelen, 2012). This suggests a future direction to jointly model class sequence and orthographic information (Clark, 2003; Smith and Eisner, 2005; Blunsom and Cohn, 2011). We have built an HTML viewer to browse these and numerous other interesting examples.7 3.3 Emoticons and Emoji We use the term emoticon to mean a face or icon constructed with traditional alphabetic or punctua6 One coauthor, a native speaker of the Texan English dialect, notes “finna” (short for “fixing to”, cluster E1) may be an immediate future auxiliary, indicating an immediate future tense that is present in many languages (though not in standard English). To illustrate: “She finna go” approximately means “She will go,” but sooner, in the sense of “She is about t"
N13-1039,N12-1052,0,0.100512,"s—which often require the processing of millions to billions of messages—so we make greedy decoding the default in the released software. P|x| Unsupervised Word Clusters Our POS tagger can make use of any number of possibly overlapping features. While we have only a small amount of hand-labeled data for training, we also have access to billions of tokens of unlabeled conversational text from the web. Previous work has shown that unlabeled text can be used to induce unsupervised word clusters which can improve the performance of many supervised NLP tasks (Koo et al., 2008; Turian et al., 2010; Täckström et al., 2012, inter alia). We use a similar approach here to improve tagging performance for online conversational text. We also make our induced clusters publicly available in the hope that they will be useful for other NLP tasks in this genre. 3 Runtimes observed on an Intel Core i5 2.4 GHz laptop. A1 A2 A3 A4 A5 B C D E1 E2 F G1 G2 G3 G4 Binary path Top words (by frequency) 111010100010 111010100011 111010100100 111010100101 11101011011100 lmao lmfao lmaoo lmaooo hahahahaha lool ctfu rofl loool lmfaoo lmfaooo lmaoooo lmbo lololol haha hahaha hehe hahahaha hahah aha hehehe ahaha hah hahahah kk hahaa aha"
N13-1039,P10-1040,0,0.242904,"analysis applications—which often require the processing of millions to billions of messages—so we make greedy decoding the default in the released software. P|x| Unsupervised Word Clusters Our POS tagger can make use of any number of possibly overlapping features. While we have only a small amount of hand-labeled data for training, we also have access to billions of tokens of unlabeled conversational text from the web. Previous work has shown that unlabeled text can be used to induce unsupervised word clusters which can improve the performance of many supervised NLP tasks (Koo et al., 2008; Turian et al., 2010; Täckström et al., 2012, inter alia). We use a similar approach here to improve tagging performance for online conversational text. We also make our induced clusters publicly available in the hope that they will be useful for other NLP tasks in this genre. 3 Runtimes observed on an Intel Core i5 2.4 GHz laptop. A1 A2 A3 A4 A5 B C D E1 E2 F G1 G2 G3 G4 Binary path Top words (by frequency) 111010100010 111010100011 111010100100 111010100101 11101011011100 lmao lmfao lmaoo lmaooo hahahahaha lool ctfu rofl loool lmfaoo lmfaooo lmaoooo lmbo lololol haha hahaha hehe hahahaha hahah aha hehehe ahaha"
N13-1039,P02-1053,0,0.00928357,"y inappropriate: should the lexico-syntactic behavior of casual conversational chatter by young minorities be straightjacketed into the stylistic conventions of the 1980s Wall Street Journal? Instead, we would like to directly analyze the syntax of online conversational text on its own terms. Thus, we choose to leave these word forms untokenized and use compound tags, viewing compositional multiword analysis as challenging future work.15 We believe that our strategy is sufficient for many applications, such as chunking or named entity recognition; many applications such as sentiment analysis (Turney, 2002; Pang and Lee, 2008, §4.2.3), open information extraction (Carlson et al., 2010; Fader et al., 2011), and information retrieval (Allan and Raghavan, 2002) use POS 14 See “Tense and aspect” examples in http: //en.wikipedia.org/wiki/African_American_ Vernacular_English 15 For example, wtf has compositional behavior in “Wtf just happened??”, but only debatably so in “Huh wtf”. 90 Tagset Dates App. A Oct 27-28, 2010 App. A Jan 2011–Jun 2012 PTB-like Oct–Nov 2006 PTB-like unknown 85 ● Table 1: Annotated datasets: number of messages, tokens, tagset, and date range. More information in §5, §6.3, and"
N13-1039,petrov-etal-2012-universal,0,\N,Missing
N13-1073,J93-2003,0,0.254903,"l are also important. These play a crucial role in many scenarios such as parallel data mining and rapid large scale experimentation, and as subcomponents of other models or training and inference algorithms. For these reasons, IBM Models 1 and 2, which support exact inference in time Θ(|f |· |e|), continue to be widely used. This paper argues that both of these models are suboptimal, even in the space of models that permit such computationally cheap inference. Model 1 assumes all alignment structures are uniformly 2 Model Our model is a variation of the lexical translation models proposed by Brown et al. (1993). Lexical translation works as follows. Given a source sentence f with length n, first generate the length of the target sentence, m. Next, generate an alignment, a = ha1 , a2 , . . . , am i, that indicates which source word (or null token) each target word will be a translation of. Last, generate the m output words, where each ei depends only on fai . The model of alignment configurations we propose is a log-linear reparameterization of Model 2. 1 Model 2 has independent parameters for every alignment position, conditioned on the source length, target length, and current target index. 644 Pro"
N13-1073,N13-1140,1,0.531297,"ored this issue for training Model 3, we found that EM tended to find poor values for p0 , producing alignments that were overly sparse. By fixing the value at p0 = 0.08, we obtained minimal AER. Second, like Riley and Gildea (2012), we found that small values of α improved the alignment error rate, although the impact was not particularly strong over large ranges of 5 http://www.statmt.org/wmt12 While this computational effort is a small relative to the total cost in EM training, in algorithms where λ changes more rapidly, for example in Bayesian posterior inference with Monte Carlo methods (Chahuneau et al., 2013), this savings can have substantial value. 6 647 Table 1: CPU time (hours) required to train alignment models in one direction. Language Pair Chinese-English French-English Arabic-English Tokens 17.6M 117M 368M Model 4 2.7 17.2 63.2 Log-linear 0.2 1.7 6.0 Table 2: Alignment quality (AER) on the WMT 2012 French-English and FBIS Chinese-English. Rows with EM use expectation maximization to estimate the θf , and ∼Dir use variational Bayes. Model Model 1 Model 1 Model 2 Log-linear Log-linear Model 4 Estimator EM ∼Dir EM EM ∼Dir EM FR - EN ZH - EN 29.0 26.6 21.4 18.5 16.6 10.4 56.2 53.6 53.3 46.5 4"
N13-1073,J07-2003,0,0.00975985,"bove (§3.3). The algorithms are 7 As an anonymous reviewer pointed out, it is a near certainty that tuning of these hyperparameters for each alignment task would improve results; however, optimizing hyperparameters of alignment models is quite expensive. Our intention is to show that it is possible to obtain reasonable (if not optimal) results without careful tuning. compared in terms of (1) time required for training; (2) alignment error rate (AER, lower is better);8 and (3) translation quality (BLEU, higher is better) of hierarchical phrase-based translation system that used the alignments (Chiang, 2007). Table 1 shows the CPU time in hours required for training (one direction, English is generated). Our model is at least 10× faster to train than Model 4. Table 3 reports the differences in BLEU on a held-out test set. Our model’s alignments lead to consistently better scores than Model 4’s do.9 5 Conclusion We have presented a fast and effective reparameterization of IBM Model 2 that is a compelling replacement for for the standard Model 4. Although the alignment quality results measured in terms of AER are mixed, the alignments were shown to work exceptionally well in downstream translation"
N13-1073,N06-2013,0,0.00674238,"Missing"
N13-1073,J03-1002,0,0.124565,"mpute the posterior probability over alignments using the above probabilities, p(ai |ei , f, m, n) = Under our model, the marginal likelihood of a sentence pair hf, ei can be computed exactly in time (1) Finally, since all words in e (and their alignments) are conditionally independent,3 p(e |f) = Marginals p(ei , ai |f, m, n) . p(ei |f, m, n) = m Y p(ei i=1 m X n Y i=1 j=0 |f, m, n) δ(ai |i, m, n) × θ(ei |fai ). 2 Vogel et al. (1996) hint at a similar reparameterization of Model 2; however, its likelihood and its gradient are not efficient to evaluate, making it impractical to train and use. Och and Ney (2003) likewise remark on the overparameterization issue, removing a single variable of the original conditioning context, which only slightly improves matters. 645 3 We note here that Brown et al. (1993) derive their variant of this expression by starting with the joint probability of an alignment and translation, marginalizing, and then reorganizing common terms. While identical in implication, we find the direct probabilistic argument far more intuitive. 3.2 Efficient Partition Function Evaluation Evaluating and maximizing the data likelihood under log-linear models can be computationally expensi"
N13-1073,P12-2060,0,0.0202891,"probabilities decrease by a factor of r = exp −λ n per step. To compute the value of the partition, we only need to evaluate the unnormalized probabilities at j↑ and j↓ and then use the following identity, which gives the sum of the first ` terms of a geometric series (Courant and Robbins, 1996): s` (g1 , r) = ` X g1 rk−1 = g1 k=1 1 − r` . 1−r as counts and normalizing (Brown et al., 1993). In the experiments reported in this paper, we make the further assumption that θf ∼ Dirichlet(µ) where µi = 0.01 and approximate the posterior distribution over the θf ’s using a mean-field approximation (Riley and Gildea, 2012).4 During the M-step, the λ parameter must also be updated to make the E-step posterior distribution over alignment points maximally probable under δ(· |i, m, n). This maximizing value cannot be computed analytically, but a gradient-based optimization can be used, where the first derivative (here, for a single target word) is: ∇λ L = Ep(ai |ei ,f,m,n) [h(i, ai , m, n)]   − Eδ(j 0 |i,m,n) h(i, j 0 , m, n) The first term in this expression (the expected value of h under the E-step posterior) is fixed for the duration of each M-step, but the second term’s value (the derivative of the log-partit"
N13-1073,C96-2141,0,0.377167,"the probability that the ith word of e is ei can be computed as: p(ei , ai |f, m, n) = δ(ai |i, m, n) × θ(ei |fai ) n X p(ei |f, m, n) = p(ei , ai = j |f, m, n). j=0 We can also compute the posterior probability over alignments using the above probabilities, p(ai |ei , f, m, n) = Under our model, the marginal likelihood of a sentence pair hf, ei can be computed exactly in time (1) Finally, since all words in e (and their alignments) are conditionally independent,3 p(e |f) = Marginals p(ei , ai |f, m, n) . p(ei |f, m, n) = m Y p(ei i=1 m X n Y i=1 j=0 |f, m, n) δ(ai |i, m, n) × θ(ei |fai ). 2 Vogel et al. (1996) hint at a similar reparameterization of Model 2; however, its likelihood and its gradient are not efficient to evaluate, making it impractical to train and use. Och and Ney (2003) likewise remark on the overparameterization issue, removing a single variable of the original conditioning context, which only slightly improves matters. 645 3 We note here that Brown et al. (1993) derive their variant of this expression by starting with the joint probability of an alignment and translation, marginalizing, and then reorganizing common terms. While identical in implication, we find the direct probabi"
N13-1076,N10-1083,0,0.0254529,"ected. Formally: Let senses(w) be the ordered list of AWN senses of lemma w. Let senses(w, s) ⊆ senses(w) be those senses that map to a given supersense s. We choose arg max s (|senses(w, s)|/ mini:senses(w)i ∈senses(w,s) i). 3.2 Unsupervised Sequence Models Unsupervised sequence labeling is our second approach (Merialdo, 1994). Although it was largely developed for part-of-speech tagging, the hope is to use in-domain Arabic data (the unannotated Wikipedia corpus discussed in §2) to infer clusters that correlate well with supersense groupings. We applied the generative, feature-based model of Berg-Kirkpatrick et al. (2010), replicating a featureset used previously for NER (Mohit et al., 2012)— including context tokens, character n-grams, and POS—and adding the vocalized stem and several stem shape features: 1) ContainsDigit?; 2) digits replaced by #; 3) digit sequences replaced by # (for stems mixing digits with other characters); 4) YearLike?—true for 4-digit numerals starting with 19 or 20; 5) LatinWord?, per the morphological analysis; 6) the shape feature of Ciaramita and Altun (2006) (Latin words only). We used 50 iterations of learning (tuned on dev data). For evaluation, a many-to-one mapping from unsupe"
N13-1076,D08-1092,0,0.0201754,"s starting with 19 or 20; 5) LatinWord?, per the morphological analysis; 6) the shape feature of Ciaramita and Altun (2006) (Latin words only). We used 50 iterations of learning (tuned on dev data). For evaluation, a many-to-one mapping from unsupervised clusters to supersense tags is greedily induced to maximize their correspondence on evaluation data. 3.3 MT-in-the-Middle A standard approach to using supervised linguistic resources in a second language is cross-lingual projection (Yarowsky and Ngai, 2001; Yarowsky et al., 2001; Smith and Smith, 2004; Hwa et al., 2005; Mihalcea et al., 2007; Burkett and Klein, 2008; Burkett et al., 2010; Das and Petrov, 2011; Kim et al., 2012, 663 who use parallel sentences extracted from Wikipedia for NER). The simplest such approach starts with an aligned parallel corpus, applies supersense tagging to the English side, and projects the labels through the word alignments. A supervised monolingual tagger is then trained on the projected labels. Preliminary experiments, however, showed that this underperformed even the simple heuristic baseline above (likely due to domain mismatch), so it was abandoned in favor of a technique that we call MT-inthe-middle projection. This"
N13-1076,W10-2906,0,0.0157732,"5) LatinWord?, per the morphological analysis; 6) the shape feature of Ciaramita and Altun (2006) (Latin words only). We used 50 iterations of learning (tuned on dev data). For evaluation, a many-to-one mapping from unsupervised clusters to supersense tags is greedily induced to maximize their correspondence on evaluation data. 3.3 MT-in-the-Middle A standard approach to using supervised linguistic resources in a second language is cross-lingual projection (Yarowsky and Ngai, 2001; Yarowsky et al., 2001; Smith and Smith, 2004; Hwa et al., 2005; Mihalcea et al., 2007; Burkett and Klein, 2008; Burkett et al., 2010; Das and Petrov, 2011; Kim et al., 2012, 663 who use parallel sentences extracted from Wikipedia for NER). The simplest such approach starts with an aligned parallel corpus, applies supersense tagging to the English side, and projects the labels through the word alignments. A supervised monolingual tagger is then trained on the projected labels. Preliminary experiments, however, showed that this underperformed even the simple heuristic baseline above (likely due to domain mismatch), so it was abandoned in favor of a technique that we call MT-inthe-middle projection. This approach does not dep"
N13-1076,J07-2003,0,0.0858748,"Missing"
N13-1076,W06-1670,0,0.725634,"Missing"
N13-1076,W03-1022,0,0.189249,"applications communication ‘The window manager controls the configuration and layout of application windows.’ Figure 1: A sentence from the “X Window System” article with supersense taggings from two annotators and post hoc English glosses and translation. Introduction A taxonomic view of lexical semantics groups word senses/usages into categories of varying granularities. WordNet supersense tags denote coarse semantic classes, including person and artifact (for nouns) and motion and weather (for verbs); these categories can be taken as the top level of a taxonomy. Nominal supersense tagging (Ciaramita and Johnson, 2003) is the task of identifying lexical chunks in the sentence for common as well as proper nouns, and labeling each with one of the 25 nominal supersense categories. Figure 1 illustrates two such labelings of an Arabic sentence. Like the narrower problem of named entity recognition, supersense tagging of text holds attraction as a way of inferring representations that move toward language independence. Here we consider the problem of nominal supersense tagging for Arabic, a language with ca. 300 million speakers and moderate linguistic resources, including a WordNet (Elkateb et al., 2006), annota"
N13-1076,P11-1061,0,0.0261324,"e morphological analysis; 6) the shape feature of Ciaramita and Altun (2006) (Latin words only). We used 50 iterations of learning (tuned on dev data). For evaluation, a many-to-one mapping from unsupervised clusters to supersense tags is greedily induced to maximize their correspondence on evaluation data. 3.3 MT-in-the-Middle A standard approach to using supervised linguistic resources in a second language is cross-lingual projection (Yarowsky and Ngai, 2001; Yarowsky et al., 2001; Smith and Smith, 2004; Hwa et al., 2005; Mihalcea et al., 2007; Burkett and Klein, 2008; Burkett et al., 2010; Das and Petrov, 2011; Kim et al., 2012, 663 who use parallel sentences extracted from Wikipedia for NER). The simplest such approach starts with an aligned parallel corpus, applies supersense tagging to the English side, and projects the labels through the word alignments. A supervised monolingual tagger is then trained on the projected labels. Preliminary experiments, however, showed that this underperformed even the simple heuristic baseline above (likely due to domain mismatch), so it was abandoned in favor of a technique that we call MT-inthe-middle projection. This approach does not depend on having parallel"
N13-1076,P10-4002,1,0.808987,"Missing"
N13-1076,elkateb-etal-2006-building,0,0.0311891,"(Ciaramita and Johnson, 2003) is the task of identifying lexical chunks in the sentence for common as well as proper nouns, and labeling each with one of the 25 nominal supersense categories. Figure 1 illustrates two such labelings of an Arabic sentence. Like the narrower problem of named entity recognition, supersense tagging of text holds attraction as a way of inferring representations that move toward language independence. Here we consider the problem of nominal supersense tagging for Arabic, a language with ca. 300 million speakers and moderate linguistic resources, including a WordNet (Elkateb et al., 2006), annotated datasets (Maamouri et al., 2004; Hovy et al., 2006), monolingual corpora, and large amounts of Arabic-English parallel data. The supervised learning approach that is used in state-of-the-art English supersense taggers (Ciaramita and Altun, 2006) is problematic for Arabic, since there are supersense annotations for only a small amount of Arabic text (65,000 words by Schneider et al., 2012, versus the 360,000 words that are annotated for English). Here, we reserve that corpus for development and evaluation, not training. We explore several approaches in this paper, the most effective"
N13-1076,2005.eamt-1.15,0,0.0305449,"bout topical domain performance we were able to find that holds across annotators and systems, in contrast with the stark topical effects observed by Mohit et al. (2012) for NER. 665 noun coverage gains track overall improvements. If QCRI produces better translations, why is cdec more useful for supersense tagging? As noted in §3.3, cdec gives word-level alignments (even when the decoder uses large phrasal units for translation), allowing for more precise projections.11 We suspect this is especially important in light of findings that larger phrase sizes are indicative of better translations (Gamon et al., 2005), so these are exactly the cases where we expect the translations to be valuable. 5 Conclusion To our knowledge, this is the first study of automatic Arabic supersense tagging. We have shown empirically that an MT-in-the-middle technique is most effective of several approaches that do not require labeled training data. Analysis sheds light on several challenges that would need to be overcome for better Arabic lexical semantic tagging. Acknowledgments We thank Wajdi Zaghouani for the translation of the Arabic Wikipedia MT set, Francisco Guzman and Preslav Nakov for the output of QCRI’s MT syste"
N13-1076,P05-1071,0,0.0961947,"Missing"
N13-1076,N06-2015,0,0.0206598,"hunks in the sentence for common as well as proper nouns, and labeling each with one of the 25 nominal supersense categories. Figure 1 illustrates two such labelings of an Arabic sentence. Like the narrower problem of named entity recognition, supersense tagging of text holds attraction as a way of inferring representations that move toward language independence. Here we consider the problem of nominal supersense tagging for Arabic, a language with ca. 300 million speakers and moderate linguistic resources, including a WordNet (Elkateb et al., 2006), annotated datasets (Maamouri et al., 2004; Hovy et al., 2006), monolingual corpora, and large amounts of Arabic-English parallel data. The supervised learning approach that is used in state-of-the-art English supersense taggers (Ciaramita and Altun, 2006) is problematic for Arabic, since there are supersense annotations for only a small amount of Arabic text (65,000 words by Schneider et al., 2012, versus the 360,000 words that are annotated for English). Here, we reserve that corpus for development and evaluation, not training. We explore several approaches in this paper, the most effective of which is to (1) translate the Arabic sentence into English,"
N13-1076,P12-1073,0,0.0128156,"is; 6) the shape feature of Ciaramita and Altun (2006) (Latin words only). We used 50 iterations of learning (tuned on dev data). For evaluation, a many-to-one mapping from unsupervised clusters to supersense tags is greedily induced to maximize their correspondence on evaluation data. 3.3 MT-in-the-Middle A standard approach to using supervised linguistic resources in a second language is cross-lingual projection (Yarowsky and Ngai, 2001; Yarowsky et al., 2001; Smith and Smith, 2004; Hwa et al., 2005; Mihalcea et al., 2007; Burkett and Klein, 2008; Burkett et al., 2010; Das and Petrov, 2011; Kim et al., 2012, 663 who use parallel sentences extracted from Wikipedia for NER). The simplest such approach starts with an aligned parallel corpus, applies supersense tagging to the English side, and projects the labels through the word alignments. A supervised monolingual tagger is then trained on the projected labels. Preliminary experiments, however, showed that this underperformed even the simple heuristic baseline above (likely due to domain mismatch), so it was abandoned in favor of a technique that we call MT-inthe-middle projection. This approach does not depend on having parallel data in the train"
N13-1076,N03-1017,0,0.00971947,"Missing"
N13-1076,P07-2045,1,0.0132485,"Missing"
N13-1076,J94-2001,0,0.08432,"nstructs the Arabic-to-English mapping {1→person11 , 4→location43 , {5, 6}→artifact76 }, resulting in the tagging shown in the bottom row. weighted to favor earlier senses (presumed by lexicographers to be more frequent) and then the supersense with the greatest aggregate weight is selected. Formally: Let senses(w) be the ordered list of AWN senses of lemma w. Let senses(w, s) ⊆ senses(w) be those senses that map to a given supersense s. We choose arg max s (|senses(w, s)|/ mini:senses(w)i ∈senses(w,s) i). 3.2 Unsupervised Sequence Models Unsupervised sequence labeling is our second approach (Merialdo, 1994). Although it was largely developed for part-of-speech tagging, the hope is to use in-domain Arabic data (the unannotated Wikipedia corpus discussed in §2) to infer clusters that correlate well with supersense groupings. We applied the generative, feature-based model of Berg-Kirkpatrick et al. (2010), replicating a featureset used previously for NER (Mohit et al., 2012)— including context tokens, character n-grams, and POS—and adding the vocalized stem and several stem shape features: 1) ContainsDigit?; 2) digits replaced by #; 3) digit sequences replaced by # (for stems mixing digits with oth"
N13-1076,P07-1123,0,0.019554,"Missing"
N13-1076,H93-1061,0,0.0657483,"Missing"
N13-1076,E12-1017,1,0.866478,"Missing"
N13-1076,P02-1040,0,0.0859619,"Missing"
N13-1076,N12-1090,0,0.0532349,"ords that are annotated for English). Here, we reserve that corpus for development and evaluation, not training. We explore several approaches in this paper, the most effective of which is to (1) translate the Arabic sentence into English, returning the alignment structure between the source and target, (2) apply English supersense tagging to the target sentence, and (3) heuristically project the tags back to the Arabic sentence across these alignments. This “MT-in-themiddle” approach has also been successfully used for mention detection (Zitouni and Florian, 2008) and coreference resolution (Rahman and Ng, 2012). We first discuss the task and relevant resources (§2), then the approaches we explored (§3), and finally present experimental results and analysis in §4. 2 Task and Resources A gold standard corpus of sentences annotated with nominal supersenses (as in figure 1) facilitates automatic evaluation of supersense taggers. For development and evaluation we use 661 Proceedings of NAACL-HLT 2013, pages 661–667, c Atlanta, Georgia, 9–14 June 2013. 2013 Association for Computational Linguistics the AQMAR Arabic Wikipedia Supersense Corpus1 (Schneider et al., 2012), which augmented a named entity corpu"
N13-1076,P08-2030,0,0.0741844,"Missing"
N13-1076,P12-2050,1,0.801661,"toward language independence. Here we consider the problem of nominal supersense tagging for Arabic, a language with ca. 300 million speakers and moderate linguistic resources, including a WordNet (Elkateb et al., 2006), annotated datasets (Maamouri et al., 2004; Hovy et al., 2006), monolingual corpora, and large amounts of Arabic-English parallel data. The supervised learning approach that is used in state-of-the-art English supersense taggers (Ciaramita and Altun, 2006) is problematic for Arabic, since there are supersense annotations for only a small amount of Arabic text (65,000 words by Schneider et al., 2012, versus the 360,000 words that are annotated for English). Here, we reserve that corpus for development and evaluation, not training. We explore several approaches in this paper, the most effective of which is to (1) translate the Arabic sentence into English, returning the alignment structure between the source and target, (2) apply English supersense tagging to the target sentence, and (3) heuristically project the tags back to the Arabic sentence across these alignments. This “MT-in-themiddle” approach has also been successfully used for mention detection (Zitouni and Florian, 2008) and co"
N13-1076,W04-3207,1,0.770277,"ts with other characters); 4) YearLike?—true for 4-digit numerals starting with 19 or 20; 5) LatinWord?, per the morphological analysis; 6) the shape feature of Ciaramita and Altun (2006) (Latin words only). We used 50 iterations of learning (tuned on dev data). For evaluation, a many-to-one mapping from unsupervised clusters to supersense tags is greedily induced to maximize their correspondence on evaluation data. 3.3 MT-in-the-Middle A standard approach to using supervised linguistic resources in a second language is cross-lingual projection (Yarowsky and Ngai, 2001; Yarowsky et al., 2001; Smith and Smith, 2004; Hwa et al., 2005; Mihalcea et al., 2007; Burkett and Klein, 2008; Burkett et al., 2010; Das and Petrov, 2011; Kim et al., 2012, 663 who use parallel sentences extracted from Wikipedia for NER). The simplest such approach starts with an aligned parallel corpus, applies supersense tagging to the English side, and projects the labels through the word alignments. A supervised monolingual tagger is then trained on the projected labels. Preliminary experiments, however, showed that this underperformed even the simple heuristic baseline above (likely due to domain mismatch), so it was abandoned in"
N13-1076,2006.amta-papers.25,0,0.0975624,"Missing"
N13-1076,N01-1026,0,0.0399772,"t sequences replaced by # (for stems mixing digits with other characters); 4) YearLike?—true for 4-digit numerals starting with 19 or 20; 5) LatinWord?, per the morphological analysis; 6) the shape feature of Ciaramita and Altun (2006) (Latin words only). We used 50 iterations of learning (tuned on dev data). For evaluation, a many-to-one mapping from unsupervised clusters to supersense tags is greedily induced to maximize their correspondence on evaluation data. 3.3 MT-in-the-Middle A standard approach to using supervised linguistic resources in a second language is cross-lingual projection (Yarowsky and Ngai, 2001; Yarowsky et al., 2001; Smith and Smith, 2004; Hwa et al., 2005; Mihalcea et al., 2007; Burkett and Klein, 2008; Burkett et al., 2010; Das and Petrov, 2011; Kim et al., 2012, 663 who use parallel sentences extracted from Wikipedia for NER). The simplest such approach starts with an aligned parallel corpus, applies supersense tagging to the English side, and projects the labels through the word alignments. A supervised monolingual tagger is then trained on the projected labels. Preliminary experiments, however, showed that this underperformed even the simple heuristic baseline above (likely du"
N13-1076,H01-1035,0,0.0177244,"(for stems mixing digits with other characters); 4) YearLike?—true for 4-digit numerals starting with 19 or 20; 5) LatinWord?, per the morphological analysis; 6) the shape feature of Ciaramita and Altun (2006) (Latin words only). We used 50 iterations of learning (tuned on dev data). For evaluation, a many-to-one mapping from unsupervised clusters to supersense tags is greedily induced to maximize their correspondence on evaluation data. 3.3 MT-in-the-Middle A standard approach to using supervised linguistic resources in a second language is cross-lingual projection (Yarowsky and Ngai, 2001; Yarowsky et al., 2001; Smith and Smith, 2004; Hwa et al., 2005; Mihalcea et al., 2007; Burkett and Klein, 2008; Burkett et al., 2010; Das and Petrov, 2011; Kim et al., 2012, 663 who use parallel sentences extracted from Wikipedia for NER). The simplest such approach starts with an aligned parallel corpus, applies supersense tagging to the English side, and projects the labels through the word alignments. A supervised monolingual tagger is then trained on the projected labels. Preliminary experiments, however, showed that this underperformed even the simple heuristic baseline above (likely due to domain mismatch),"
N13-1076,D08-1063,0,0.11663,"0 words by Schneider et al., 2012, versus the 360,000 words that are annotated for English). Here, we reserve that corpus for development and evaluation, not training. We explore several approaches in this paper, the most effective of which is to (1) translate the Arabic sentence into English, returning the alignment structure between the source and target, (2) apply English supersense tagging to the target sentence, and (3) heuristically project the tags back to the Arabic sentence across these alignments. This “MT-in-themiddle” approach has also been successfully used for mention detection (Zitouni and Florian, 2008) and coreference resolution (Rahman and Ng, 2012). We first discuss the task and relevant resources (§2), then the approaches we explored (§3), and finally present experimental results and analysis in §4. 2 Task and Resources A gold standard corpus of sentences annotated with nominal supersenses (as in figure 1) facilitates automatic evaluation of supersense taggers. For development and evaluation we use 661 Proceedings of NAACL-HLT 2013, pages 661–667, c Atlanta, Georgia, 9–14 June 2013. 2013 Association for Computational Linguistics the AQMAR Arabic Wikipedia Supersense Corpus1 (Schneider et"
N13-1076,W05-0909,0,\N,Missing
N13-1140,2010.amta-papers.4,0,0.0192714,"Missing"
N13-1140,N03-2002,0,0.0424451,"ggested a third way between rule-based approaches and fully unsupervised learning that combines the best of both worlds. Morphological analysis or segmentation is crucial to the performance of several applications: machine translation (Goldwater and McClosky, 2005; AlHaj and Lavie, 2010; Oflazer and El-Kahlout, 2007; Minkov et al., 2007; Habash and Sadat, 2006, inter alia), automatic speech recognition (Creutz et al., 2007), and syntactic parsing (Tsarfaty et al., 2010). Several methods have been proposed to integrate morphology into n-gram language models, including factored language models (Bilmes and Kirchhoff, 2003), discriminative language modeling (Arısoy et al., 2008), and more heuristic approaches (Monz, 2011). Despite the fundamentally open nature of the lexicon (Heaps, 1978), there has been distressingly little attention to the general problem of open vocabulary language modeling problem (most applications make a closed-vocabulary assumption). The classic exploration of open vocabulary language modeling is Brown et al. (1992), which proposed the strategy of interpolating between word- and character-based models. Character-based language models are reviewed by Carpenter (2005). So-called hybrid mode"
N13-1140,bojar-prokopova-2006-czech,0,0.0273209,"hoices for the base word distribution: • As a baseline, we use a uniform base distribution over the target vocabulary: G0w = U(V ). • We define a stem distribution Gs [f ] for each source word f , a shared pattern distribution Gp , and set G0w [f ] = MP(Gs [f ], Gp ). In this case, we obtain the model depicted in Fig. 2. The stem and the pattern models are also given PY priors with uniform base distribution (G0s = U(S)). Finally, we put uninformative priors on the alignment distribution parameters: p0 ∼ Beta(α, β) is collapsed and λ ∼ Gamma(k, θ) is inferred using Metropolis-Hastings. ↵, from Bojar and Prokopová (2006). The morphological analyzer is provided by Xerox. Results Results are shown in Table 5. Our lightly parameterized model performs much better than IBM Model 4 in these small-data conditions. With an identical model, we find PY priors outperform traditional multinomial distributions. Adding morphology further reduced the alignment error rate, for both languages. Model Model 4 EM PY-U(V ) PY-U(S) p0 d w , ✓w d s , ✓s e Gw Gs G0s Gp G0p Table 5: Word alignment experiments on English-Turkish (en-tr) and English-Czech (en-cs) data. d p , ✓p Figure 2: Our alignment model, represented as a graphical"
N13-1140,J92-1002,0,0.752922,"Missing"
N13-1140,J93-2003,0,0.0700196,"tical model, we find PY priors outperform traditional multinomial distributions. Adding morphology further reduced the alignment error rate, for both languages. Model Model 4 EM PY-U(V ) PY-U(S) p0 d w , ✓w d s , ✓s e Gw Gs G0s Gp G0p Table 5: Word alignment experiments on English-Turkish (en-tr) and English-Czech (en-cs) data. d p , ✓p Figure 2: Our alignment model, represented as a graphical model. Experiments We evaluate the alignment error rate of our models for two language pairs with rich morphology on the target side. We compare to alignments inferred using IBM Model 4 trained with EM (Brown et al., 1993),10 a version of our baseline model (described above) without PY priors (learned using EM), and the PY-based baseline. We consider two language pairs. English-Turkish We use a 2.8M word cleaned version of the South-East European Times corpus (Tyers and Alperen, 2010) and gold-standard alignments from Çakmak et al. (2012). Our morphological analyzer is identical to the one used in the previous sections. As an example of how our model generalizes better, consider the sentence pair in Fig. 3, taken from the evaluation data. The two words composing the Turkish sentence are not found elsewhere in t"
N13-1140,W05-1107,0,0.0290275,"age models (Bilmes and Kirchhoff, 2003), discriminative language modeling (Arısoy et al., 2008), and more heuristic approaches (Monz, 2011). Despite the fundamentally open nature of the lexicon (Heaps, 1978), there has been distressingly little attention to the general problem of open vocabulary language modeling problem (most applications make a closed-vocabulary assumption). The classic exploration of open vocabulary language modeling is Brown et al. (1992), which proposed the strategy of interpolating between word- and character-based models. Character-based language models are reviewed by Carpenter (2005). So-called hybrid models that model both words and sublexical units have become popular in speech recognition (Shaik et al., 2012; Parada et al., 2011; Bazzi, 2002). Openvocabulary language language modeling has also recently been explored in the context of assistive technologies (Roark, 2009). Finally, Pitman-Yor processes (PYPs) have become widespread in natural language processing since they are natural power-law generators. It has been shown that the widely used modified KneserNey estimator (Chen and Goodman, 1998) for ngram language models is an approximation of the posterior predictive"
N13-1140,2012.eamt-1.60,0,0.0172065,"what we try to do by designing a lexicon-free analyzer for Russian. A guesser was developed in three hours; it is prone to over-generation and produces ambiguous analyses for most words but covers a large number of morphological phenomena (gender, case, tense, etc.). For example, the word иврите5 can be correctly analyzed as иврит+Noun+Masc+Prep+Sg but also as the incorrect forms: иврить+Verb+Pres+2P+Pl, иврита+Noun+Fem+Dat+Sg, ивритя+Noun+Fem+Prep+Sg, and more. 3.2 Disambiguation Experiments We train the unigram model on a 1.7M-word corpus of TED talks transcriptions translated into Russian (Cettolo et al., 2012) and evaluate our analyzer against a test set consisting of 1,500 goldstandard analyses obtained from the morphology disambiguation task of the DIALOG 2010 conference (Lyaševskaya et al., 2010).6 Each analysis is composed of a lemma (иврит), a part of speech (Noun), and a sequence of additional functional morphemes (Masc,Prep,Sg). We consider only open-class categories: nouns, ad5 6 иврите = Hebrew (masculine noun, prepositional case) http://ru-eval.ru 1209 jectives, adverbs and verbs, and evaluate the output of our model with three metrics: the lemma accuracy, the part-of-speech accuracy, and"
N13-1140,N07-1048,0,0.0147887,"l., 1995) have been shown to be effective, and Bayesian model-based versions have been proposed as well (Goldwater et al., 2011; Snyder and Barzilay, 2008; Snover and Brent, 2001). In §3, we suggested a third way between rule-based approaches and fully unsupervised learning that combines the best of both worlds. Morphological analysis or segmentation is crucial to the performance of several applications: machine translation (Goldwater and McClosky, 2005; AlHaj and Lavie, 2010; Oflazer and El-Kahlout, 2007; Minkov et al., 2007; Habash and Sadat, 2006, inter alia), automatic speech recognition (Creutz et al., 2007), and syntactic parsing (Tsarfaty et al., 2010). Several methods have been proposed to integrate morphology into n-gram language models, including factored language models (Bilmes and Kirchhoff, 2003), discriminative language modeling (Arısoy et al., 2008), and more heuristic approaches (Monz, 2011). Despite the fundamentally open nature of the lexicon (Heaps, 1978), there has been distressingly little attention to the general problem of open vocabulary language modeling problem (most applications make a closed-vocabulary assumption). The classic exploration of open vocabulary language modelin"
N13-1140,N13-1073,1,0.771743,"ogy. In fact, alignment models are a good candidate for using richer word distributions: they assume a target word distribution conditioned on each source word. When the target language is morphologically rich, classic independence assumptions produce very weak models unless some kind of preprocessing is applied to one side of the corpus. An alternative is to use our unigram model as a word translation distribution for each source word in the corpus. Our alignment model is based on a simple variant of IBM Model 2 where the alignment distribution is only controlled by two parameters, λ and p0 (Dyer et al., 2013). p0 is the probability of the null alignment. For a source sentence f of length n, a target sentence e of length m and a latent alignment a, we define the following alignment link probabilities (j 6= 0):   i j p(ai = j |n, m) ∝ (1 − p0 ) exp −λ − m n λ controls the flatness of this distribution: larger values make the probabilities more peaked around the diagonal of the alignment matrix. Each target word is then generated given a source word and a latent alignment link from the word translation distribution p(ei |fai , Gw ). Note that this is effectively a unigram distribution over target w"
N13-1140,J01-2001,0,0.0647422,"develop the Russian guesser. Since some inflected forms have several possible analyses, there has been a great deal of work on selecting the intended one in context (Hakkani-Tür et al., 2000; Hajiˇc et al., 2001; Habash and Rambow, 2005; Smith et al., 2005; Habash et al., 2009). Our disambiguation model is closely related to generative models used for this purpose (Hakkani-Tür et al., 2000). Rule-based analysis is not the only approach to modeling morphology, and many unsupervised models have been proposed.12 Heuristic segmentation approaches based on the minimum description length principle (Goldsmith, 2001; Creutz and Lagus, 2007; de Marcken, 1996; Brent et al., 1995) have been shown to be effective, and Bayesian model-based versions have been proposed as well (Goldwater et al., 2011; Snyder and Barzilay, 2008; Snover and Brent, 2001). In §3, we suggested a third way between rule-based approaches and fully unsupervised learning that combines the best of both worlds. Morphological analysis or segmentation is crucial to the performance of several applications: machine translation (Goldwater and McClosky, 2005; AlHaj and Lavie, 2010; Oflazer and El-Kahlout, 2007; Minkov et al., 2007; Habash and Sa"
N13-1140,H05-1085,0,0.0802413,"been proposed.12 Heuristic segmentation approaches based on the minimum description length principle (Goldsmith, 2001; Creutz and Lagus, 2007; de Marcken, 1996; Brent et al., 1995) have been shown to be effective, and Bayesian model-based versions have been proposed as well (Goldwater et al., 2011; Snyder and Barzilay, 2008; Snover and Brent, 2001). In §3, we suggested a third way between rule-based approaches and fully unsupervised learning that combines the best of both worlds. Morphological analysis or segmentation is crucial to the performance of several applications: machine translation (Goldwater and McClosky, 2005; AlHaj and Lavie, 2010; Oflazer and El-Kahlout, 2007; Minkov et al., 2007; Habash and Sadat, 2006, inter alia), automatic speech recognition (Creutz et al., 2007), and syntactic parsing (Tsarfaty et al., 2010). Several methods have been proposed to integrate morphology into n-gram language models, including factored language models (Bilmes and Kirchhoff, 2003), discriminative language modeling (Arısoy et al., 2008), and more heuristic approaches (Monz, 2011). Despite the fundamentally open nature of the lexicon (Heaps, 1978), there has been distressingly little attention to the general proble"
N13-1140,P05-1071,0,0.0219994,"irilece˘ginden, bitmesiyle, ... 1212 Kay, 1994). It is now widely accepted that finitestate transducers have sufficient power to express nearly all morphological phenomena, and the XFST toolkit (Beesley and Karttunen, 2003) has contributed to the practical adoption of this modeling approach. Recently, open-source tools have been released: in this paper, we used Foma (Hulden, 2009) to develop the Russian guesser. Since some inflected forms have several possible analyses, there has been a great deal of work on selecting the intended one in context (Hakkani-Tür et al., 2000; Hajiˇc et al., 2001; Habash and Rambow, 2005; Smith et al., 2005; Habash et al., 2009). Our disambiguation model is closely related to generative models used for this purpose (Hakkani-Tür et al., 2000). Rule-based analysis is not the only approach to modeling morphology, and many unsupervised models have been proposed.12 Heuristic segmentation approaches based on the minimum description length principle (Goldsmith, 2001; Creutz and Lagus, 2007; de Marcken, 1996; Brent et al., 1995) have been shown to be effective, and Bayesian model-based versions have been proposed as well (Goldwater et al., 2011; Snyder and Barzilay, 2008; Snover and"
N13-1140,N06-2013,0,0.165461,"human languages, many NLP systems treat fully inflected forms as the atomic units of language. By assuming independence of lexical stems’ various surface forms, this avoidance approach exacerbates the problem of data sparseness. If it is employed at all, morphological analysis of text tends to be treated as a preprocessing step to other NLP modules. While this latter disambiguation approach helps address data sparsity concerns, it has substantial drawbacks: it requires supervised learning from expert-annotated corpora, and determining the optimal morphological granularity is labor-intensive (Habash and Sadat, 2006). Neither approach fully exploits the finite-state transducer (FST) technology that has been so successful for modeling the mapping between surface forms and their morphological analyses (Karttunen and Beesley, 2005), and the mature collections of high quality transducers that already exist for many languages (e.g., Turkish, Russian, Arabic). Much linguistic knowledge is encoded in such FSTs. In this paper, we develop morphology-aware nonparametric Bayesian language models that bring together hand-written FSTs with statistical modeling and require no token-level annotation. The sparsity issue"
N13-1140,C00-1042,0,0.0608514,"4. 11 ödevinin, ödevini, ödevleri; bitmez, bitirilece˘ginden, bitmesiyle, ... 1212 Kay, 1994). It is now widely accepted that finitestate transducers have sufficient power to express nearly all morphological phenomena, and the XFST toolkit (Beesley and Karttunen, 2003) has contributed to the practical adoption of this modeling approach. Recently, open-source tools have been released: in this paper, we used Foma (Hulden, 2009) to develop the Russian guesser. Since some inflected forms have several possible analyses, there has been a great deal of work on selecting the intended one in context (Hakkani-Tür et al., 2000; Hajiˇc et al., 2001; Habash and Rambow, 2005; Smith et al., 2005; Habash et al., 2009). Our disambiguation model is closely related to generative models used for this purpose (Hakkani-Tür et al., 2000). Rule-based analysis is not the only approach to modeling morphology, and many unsupervised models have been proposed.12 Heuristic segmentation approaches based on the minimum description length principle (Goldsmith, 2001; Creutz and Lagus, 2007; de Marcken, 1996; Brent et al., 1995) have been shown to be effective, and Bayesian model-based versions have been proposed as well (Goldwater et al."
N13-1140,E09-2008,0,0.0256513,"derable attention in NLP since the early work on twolevel morphology (Koskenniemi, 1984; Kaplan and 10 We use the default GIZA++ stage training scheme: Model 1 + HMM + Model 3 + Model 4. 11 ödevinin, ödevini, ödevleri; bitmez, bitirilece˘ginden, bitmesiyle, ... 1212 Kay, 1994). It is now widely accepted that finitestate transducers have sufficient power to express nearly all morphological phenomena, and the XFST toolkit (Beesley and Karttunen, 2003) has contributed to the practical adoption of this modeling approach. Recently, open-source tools have been released: in this paper, we used Foma (Hulden, 2009) to develop the Russian guesser. Since some inflected forms have several possible analyses, there has been a great deal of work on selecting the intended one in context (Hakkani-Tür et al., 2000; Hajiˇc et al., 2001; Habash and Rambow, 2005; Smith et al., 2005; Habash et al., 2009). Our disambiguation model is closely related to generative models used for this purpose (Hakkani-Tür et al., 2000). Rule-based analysis is not the only approach to modeling morphology, and many unsupervised models have been proposed.12 Heuristic segmentation approaches based on the minimum description length princip"
N13-1140,J94-3001,0,0.687315,"Missing"
N13-1140,P84-1038,0,0.450265,"-base model to find the correct alignment (marked in black), while all the other models have no evidence for it and choose an arbitrary alignment (gray points). I was not able to finish my homework f a AER en-tr en-cs 52.1 34.5 43.8 28.9 39.2 25.7 33.8 24.8 ödevimi bitiremedim Figure 3: A complex Turkish-English word alignment (alignment points in gray: EM/PY-U(V ); black: PYU(S)). 6 Related Work English-Czech We use the 1.3M word News Commentary corpus and gold-standard alignments Computational morphology has received considerable attention in NLP since the early work on twolevel morphology (Koskenniemi, 1984; Kaplan and 10 We use the default GIZA++ stage training scheme: Model 1 + HMM + Model 3 + Model 4. 11 ödevinin, ödevini, ödevleri; bitmez, bitirilece˘ginden, bitmesiyle, ... 1212 Kay, 1994). It is now widely accepted that finitestate transducers have sufficient power to express nearly all morphological phenomena, and the XFST toolkit (Beesley and Karttunen, 2003) has contributed to the practical adoption of this modeling approach. Recently, open-source tools have been released: in this paper, we used Foma (Hulden, 2009) to develop the Russian guesser. Since some inflected forms have several p"
N13-1140,P07-1017,0,0.0650095,"length principle (Goldsmith, 2001; Creutz and Lagus, 2007; de Marcken, 1996; Brent et al., 1995) have been shown to be effective, and Bayesian model-based versions have been proposed as well (Goldwater et al., 2011; Snyder and Barzilay, 2008; Snover and Brent, 2001). In §3, we suggested a third way between rule-based approaches and fully unsupervised learning that combines the best of both worlds. Morphological analysis or segmentation is crucial to the performance of several applications: machine translation (Goldwater and McClosky, 2005; AlHaj and Lavie, 2010; Oflazer and El-Kahlout, 2007; Minkov et al., 2007; Habash and Sadat, 2006, inter alia), automatic speech recognition (Creutz et al., 2007), and syntactic parsing (Tsarfaty et al., 2010). Several methods have been proposed to integrate morphology into n-gram language models, including factored language models (Bilmes and Kirchhoff, 2003), discriminative language modeling (Arısoy et al., 2008), and more heuristic approaches (Monz, 2011). Despite the fundamentally open nature of the lexicon (Heaps, 1978), there has been distressingly little attention to the general problem of open vocabulary language modeling problem (most applications make a c"
N13-1140,D11-1080,0,0.0250277,"lds. Morphological analysis or segmentation is crucial to the performance of several applications: machine translation (Goldwater and McClosky, 2005; AlHaj and Lavie, 2010; Oflazer and El-Kahlout, 2007; Minkov et al., 2007; Habash and Sadat, 2006, inter alia), automatic speech recognition (Creutz et al., 2007), and syntactic parsing (Tsarfaty et al., 2010). Several methods have been proposed to integrate morphology into n-gram language models, including factored language models (Bilmes and Kirchhoff, 2003), discriminative language modeling (Arısoy et al., 2008), and more heuristic approaches (Monz, 2011). Despite the fundamentally open nature of the lexicon (Heaps, 1978), there has been distressingly little attention to the general problem of open vocabulary language modeling problem (most applications make a closed-vocabulary assumption). The classic exploration of open vocabulary language modeling is Brown et al. (1992), which proposed the strategy of interpolating between word- and character-based models. Character-based language models are reviewed by Carpenter (2005). So-called hybrid models that model both words and sublexical units have become popular in speech recognition (Shaik et al"
N13-1140,W07-0704,0,0.0188157,"sed on the minimum description length principle (Goldsmith, 2001; Creutz and Lagus, 2007; de Marcken, 1996; Brent et al., 1995) have been shown to be effective, and Bayesian model-based versions have been proposed as well (Goldwater et al., 2011; Snyder and Barzilay, 2008; Snover and Brent, 2001). In §3, we suggested a third way between rule-based approaches and fully unsupervised learning that combines the best of both worlds. Morphological analysis or segmentation is crucial to the performance of several applications: machine translation (Goldwater and McClosky, 2005; AlHaj and Lavie, 2010; Oflazer and El-Kahlout, 2007; Minkov et al., 2007; Habash and Sadat, 2006, inter alia), automatic speech recognition (Creutz et al., 2007), and syntactic parsing (Tsarfaty et al., 2010). Several methods have been proposed to integrate morphology into n-gram language models, including factored language models (Bilmes and Kirchhoff, 2003), discriminative language modeling (Arısoy et al., 2008), and more heuristic approaches (Monz, 2011). Despite the fundamentally open nature of the lexicon (Heaps, 1978), there has been distressingly little attention to the general problem of open vocabulary language modeling problem (most"
N13-1140,P11-1072,0,0.0298152,"the fundamentally open nature of the lexicon (Heaps, 1978), there has been distressingly little attention to the general problem of open vocabulary language modeling problem (most applications make a closed-vocabulary assumption). The classic exploration of open vocabulary language modeling is Brown et al. (1992), which proposed the strategy of interpolating between word- and character-based models. Character-based language models are reviewed by Carpenter (2005). So-called hybrid models that model both words and sublexical units have become popular in speech recognition (Shaik et al., 2012; Parada et al., 2011; Bazzi, 2002). Openvocabulary language language modeling has also recently been explored in the context of assistive technologies (Roark, 2009). Finally, Pitman-Yor processes (PYPs) have become widespread in natural language processing since they are natural power-law generators. It has been shown that the widely used modified KneserNey estimator (Chen and Goodman, 1998) for ngram language models is an approximation of the posterior predictive distribution of a language model with hierarchical PYP priors (Goldwater et al., 2011; Teh, 2006). 7 Conclusion We described a generative model which m"
N13-1140,H05-1060,1,0.806906,"e, ... 1212 Kay, 1994). It is now widely accepted that finitestate transducers have sufficient power to express nearly all morphological phenomena, and the XFST toolkit (Beesley and Karttunen, 2003) has contributed to the practical adoption of this modeling approach. Recently, open-source tools have been released: in this paper, we used Foma (Hulden, 2009) to develop the Russian guesser. Since some inflected forms have several possible analyses, there has been a great deal of work on selecting the intended one in context (Hakkani-Tür et al., 2000; Hajiˇc et al., 2001; Habash and Rambow, 2005; Smith et al., 2005; Habash et al., 2009). Our disambiguation model is closely related to generative models used for this purpose (Hakkani-Tür et al., 2000). Rule-based analysis is not the only approach to modeling morphology, and many unsupervised models have been proposed.12 Heuristic segmentation approaches based on the minimum description length principle (Goldsmith, 2001; Creutz and Lagus, 2007; de Marcken, 1996; Brent et al., 1995) have been shown to be effective, and Bayesian model-based versions have been proposed as well (Goldwater et al., 2011; Snyder and Barzilay, 2008; Snover and Brent, 2001). In §3,"
N13-1140,P01-1063,0,0.0360378,"ambow, 2005; Smith et al., 2005; Habash et al., 2009). Our disambiguation model is closely related to generative models used for this purpose (Hakkani-Tür et al., 2000). Rule-based analysis is not the only approach to modeling morphology, and many unsupervised models have been proposed.12 Heuristic segmentation approaches based on the minimum description length principle (Goldsmith, 2001; Creutz and Lagus, 2007; de Marcken, 1996; Brent et al., 1995) have been shown to be effective, and Bayesian model-based versions have been proposed as well (Goldwater et al., 2011; Snyder and Barzilay, 2008; Snover and Brent, 2001). In §3, we suggested a third way between rule-based approaches and fully unsupervised learning that combines the best of both worlds. Morphological analysis or segmentation is crucial to the performance of several applications: machine translation (Goldwater and McClosky, 2005; AlHaj and Lavie, 2010; Oflazer and El-Kahlout, 2007; Minkov et al., 2007; Habash and Sadat, 2006, inter alia), automatic speech recognition (Creutz et al., 2007), and syntactic parsing (Tsarfaty et al., 2010). Several methods have been proposed to integrate morphology into n-gram language models, including factored lan"
N13-1140,P08-1084,0,0.0289725,"et al., 2001; Habash and Rambow, 2005; Smith et al., 2005; Habash et al., 2009). Our disambiguation model is closely related to generative models used for this purpose (Hakkani-Tür et al., 2000). Rule-based analysis is not the only approach to modeling morphology, and many unsupervised models have been proposed.12 Heuristic segmentation approaches based on the minimum description length principle (Goldsmith, 2001; Creutz and Lagus, 2007; de Marcken, 1996; Brent et al., 1995) have been shown to be effective, and Bayesian model-based versions have been proposed as well (Goldwater et al., 2011; Snyder and Barzilay, 2008; Snover and Brent, 2001). In §3, we suggested a third way between rule-based approaches and fully unsupervised learning that combines the best of both worlds. Morphological analysis or segmentation is crucial to the performance of several applications: machine translation (Goldwater and McClosky, 2005; AlHaj and Lavie, 2010; Oflazer and El-Kahlout, 2007; Minkov et al., 2007; Habash and Sadat, 2006, inter alia), automatic speech recognition (Creutz et al., 2007), and syntactic parsing (Tsarfaty et al., 2010). Several methods have been proposed to integrate morphology into n-gram language model"
N13-1140,P06-1124,0,0.44035,"ependent is clearly unsatisfying. We therefore assume that both the stem and the pattern distributions are generated from PY processes, and that MP(Gs , Gp , G ENERATE) is itself the base distribution of a PYP. Pitman-Yor Processes Our work relies extensively on Pitman-Yor processes, which provide a flexible framework for expressing backoff and interpolation relationships and extending standard models with richer word distributions (Pitman and Yor, 1997). They have been shown to match the performance of state-of-the-art language models and to give estimates that follow appropriate power laws (Teh, 2006). A draw from a Pitman-Yor process (PYP), denoted G ∼ PY(d, θ, G0 ), is a discrete distribution over a (possibly infinite) set of events, which we denote abstractly E. The process is parameterized by a discount parameter 0 ≤ d &lt; 1, a strength parameter θ > −d, and a base distribution G0 over the event space E. In this work, our focus is on the base distribution G0 . We place vague priors on the hyperparameters d ∼ U([0, 1]) and (θ + d) ∼ Gamma(1, 1). Inference in PYPs is discussed below. 1207 Gs ∼ PY(ds , θs , G0s ) Gp ∼ PY(dp , θp , G0p ) Gw ∼ PY(d, θ, MP(Gs , Gp , G ENERATE)) A draw Gw from"
N13-1140,W10-1401,0,0.0239317,"d Bayesian model-based versions have been proposed as well (Goldwater et al., 2011; Snyder and Barzilay, 2008; Snover and Brent, 2001). In §3, we suggested a third way between rule-based approaches and fully unsupervised learning that combines the best of both worlds. Morphological analysis or segmentation is crucial to the performance of several applications: machine translation (Goldwater and McClosky, 2005; AlHaj and Lavie, 2010; Oflazer and El-Kahlout, 2007; Minkov et al., 2007; Habash and Sadat, 2006, inter alia), automatic speech recognition (Creutz et al., 2007), and syntactic parsing (Tsarfaty et al., 2010). Several methods have been proposed to integrate morphology into n-gram language models, including factored language models (Bilmes and Kirchhoff, 2003), discriminative language modeling (Arısoy et al., 2008), and more heuristic approaches (Monz, 2011). Despite the fundamentally open nature of the lexicon (Heaps, 1978), there has been distressingly little attention to the general problem of open vocabulary language modeling problem (most applications make a closed-vocabulary assumption). The classic exploration of open vocabulary language modeling is Brown et al. (1992), which proposed the st"
N13-1140,cakmak-etal-2012-word,0,0.0268039,"Missing"
N13-1140,W83-0114,0,\N,Missing
N13-1140,P01-1035,0,\N,Missing
N15-1080,W08-2102,0,0.41481,"araParser 9 http://nlp.cs.nyu.edu/evalb 7 7 794 Model PTB §23 F1 Sent./s. Charniak (2000) Stanford PCFG (2003) Petrov (2007) Zhu (2013) Carreras (2008) 89.5 85.5 90.1 90.3 91.1 – 5.3 8.6 39.0 – CJ Reranking (2005) Stanford RNN (2013) 91.5 90.0 4.3 2.8 PAD PAD (Pruned) 90.4 90.3 34.3 58.6 Model CTB F1 Charniak (2000) Bikel (2004) Petrov (2007) Zhu (2013) 80.8 80.6 83.3 83.2 PAD 82.4 Table 3: Accuracy and speed on PTB §23 and CTB 5.1 test split. Comparisons are to state-of-the-art non-reranking supervised phrase-structure parsers (Charniak, 2000; Klein and Manning, 2003; Petrov and Klein, 2007; Carreras et al., 2008; Zhu et al., 2013; Bikel, 2004), and semi-supervised and reranking parsers (Charniak and Johnson, 2005; Socher et al., 2013). F1 Sent./s. Oracle M ALT PARSER RS-K1 RS-K4 RS-K16 YARA -K1 YARA -K16 YARA -K32 YARA -K64 TP-BASIC TP-S TANDARD TP-F ULL 89.7 90.1 92.5 93.1 89.7 92.9 93.1 93.1 92.8 93.3 93.5 85.5 86.6 90.1 90.6 85.3 89.8 90.4 90.5 88.9 90.9 90.8 240.7 233.9 151.3 58.6 1265.8 157.5 48.3 47.3 132.8 27.2 13.2 87.8 87.6 91.5 92.5 86.7 91.7 92.0 92.2 90.8 92.6 92.9 dency constraints, the English results show that the parser is comparable in accuracy to many widelyused systems, and is sign"
N15-1080,P05-1022,0,0.605688,"sers are generally much faster than c-parsers, we consider an alternate pipeline (Section 3): d-parse first, then transform the dependency representation into a phrase-structure tree constrained to be consistent with the dependency parse. This idea was explored by Xia and Palmer (2001) and Xia et al. (2009) using hand-written rules. Instead, we present a data-driven algorithm using the structured prediction framework (Section 4). The approach can be understood as a specially-trained coarse-to-fine decoding algorithm where a d-parser provides “coarse” structure and the second stage refines it (Charniak and Johnson, 2005; Petrov and Klein, 2007). Our lexicalized phrase-structure parser, PAD, is asymptotically faster than parsing with a lexicalized context-free grammar: O(n2 ) plus d-parsing, vs. O(n5 ) worst case runtime in sentence length n, with the same grammar constant. Experiments show that our approach achieves linear observable runtime, and accuracy similar to state-of-the-art phrase-structure parsers without reranking or semisupervised training (Section 7). 2 Background We begin with the conventional development by first introducing c-parsing and then defining d-parses through a mechanical conversion"
N15-1080,A00-2018,0,0.543393,"Missing"
N15-1080,J05-1003,0,0.0608399,"Missing"
N15-1080,P99-1065,0,0.346961,"Missing"
N15-1080,J03-4003,0,0.892132,"otated with a terminal or nonterminal symbol and a derived head index. The blue and red vertices have the words automaker2 and sold3 as heads respectively. The vertex VP(3) implies that automaker2 is a left-dependent of sold3 , and that 2 ∈ L(3) in the d-parse. Dependency Parsing Dependency parses provide an alternative, and in some sense simpler, representation of sentence structure. These d-parses can be derived through mechanical transformation from context-free trees. There are several popular transformations in wide use; each provides a different representation of a sentence’s structure (Collins, 2003; De Marneffe and Manning, 2008; Yamada and Matsumoto, 2003; Johansson and Nugues, 2007). We consider the class of transformations that are defined through local head rules. For a binary CFG, define a collection of head rules as a mapping from each CFG rule to a head preference for its left or right child. We use the notation A → β1∗ β2 and A → β1 β2∗ to indicate a left- or right-headed rule, respectively. The head rules can be used to map a c-parse to a dependency tree (d-parse). In a d-parse, each word in the sentence is assigned as a dependent to a head word, h ∈ {0, . . . , n}, where 0 is"
N15-1080,W08-1301,0,0.115419,"Missing"
N15-1080,P05-1067,0,0.117591,"Missing"
N15-1080,P99-1059,0,0.23871,"to produce two vertices covering hi, ki and hk + 1, ji, and that the new head is index h has dependent index m. We say this production “completes” word m since it can no longer be the head of a larger span. Running the algorithm consists of bottom-up dynamic programming over these productions. However, applying this version of the CKY algorithm requires O(n5 |G|) time (linear in the number of productions), which is not practical to run without heavy pruning. Most lexicalized parsers therefore make further assumptions on the scoring function which can lead to asymptotically faster algorithms (Eisner and Satta, 1999). Instead, we consider the same objective, but constrain the c-parses to be consistent with a given dparse, d. By “consistent,” we mean that the cparse will be converted by the head rules to this exact d-parse.4 Define the set of consistent c-parses as Y(x, d) and the constrained search problem as arg maxy∈Y(x,d) s(y; x, d). Figure 3 (right) shows the algorithm for this new problem. The algorithm has several nice properties. All rules now must select words h and m that are consistent with the dependency parse (i.e., there is an arc (h, m)) so these variables are no longer free. Furthermore, si"
N15-1080,P15-1147,0,0.501488,"Missing"
N15-1080,W08-1007,0,0.511542,"Missing"
N15-1080,W07-2444,0,0.694585,"Missing"
N15-1080,P14-1022,0,0.183115,"− y 0 |where y is an indicator for production rules firing over pairs of adjacent spans (i.e., i, j, k). Xia et al. (2009) PAD (§19) PAD (§2–21) 88.1 95.9 97.5 PTB §22 Rec. F1 90.7 95.9 97.8 89.4 95.9 97.7 The objective is optimized using AdaGrad (Duchi et al., 2011). The gradient calculation requires computing a loss-augmented max-scoring c-parse for each training example which is done using the algorithm of Figure 3 (right). modifier word and part-of-speech, and head word and part-of-speech. The second set of features is modeled after the span features described in the X-bar-style parser of Hall et al. (2014). These include conjunctions of the rule with: first and last word of current span, preceding and following word of current span, adjacent words at split of current span, and binned length of the span. The full feature set is shown in Figure 4. After training, there are a total of around 2 million nonzero features. For efficiency, we use lossy feature hashing. We found this had no impact on parsing accuracy but made the parsing significantly faster. min Prec. Table 2: Comparison with the rule-based system of Xia et al. (2009). Results are shown using gold-standard tags and dependencies. Xia et"
N15-1080,W07-2416,0,0.0606049,"blue and red vertices have the words automaker2 and sold3 as heads respectively. The vertex VP(3) implies that automaker2 is a left-dependent of sold3 , and that 2 ∈ L(3) in the d-parse. Dependency Parsing Dependency parses provide an alternative, and in some sense simpler, representation of sentence structure. These d-parses can be derived through mechanical transformation from context-free trees. There are several popular transformations in wide use; each provides a different representation of a sentence’s structure (Collins, 2003; De Marneffe and Manning, 2008; Yamada and Matsumoto, 2003; Johansson and Nugues, 2007). We consider the class of transformations that are defined through local head rules. For a binary CFG, define a collection of head rules as a mapping from each CFG rule to a head preference for its left or right child. We use the notation A → β1∗ β2 and A → β1 β2∗ to indicate a left- or right-headed rule, respectively. The head rules can be used to map a c-parse to a dependency tree (d-parse). In a d-parse, each word in the sentence is assigned as a dependent to a head word, h ∈ {0, . . . , n}, where 0 is a special symbol indicating the pseudo-root of the sentence. For each h we define L(h) ⊂"
N15-1080,P03-1054,0,0.0928356,"performance (i.e., how well a perfect scoring function could do with a pruned Y(x, d)). Table 1 shows a comparison of these pruning methods on development data. The constrained parsing algorithm is much faster than standard lexicalized parsing, and Model L EX CKY∗ D EP CKY P RUNE 1 P RUNE 2 P RUNE 1+2 Complexity 5 n |G| P P h |L(h)||R(h)||G| h |L(h)||R(h)||GT | – – Sent./s. Ora. F1 0.25 71.2 336.0 96.6 425.1 100.0 92.6 92.5 92.5 92.5 We also explored binarization using horizontal and vertical markovization to include additional context of the tree, as found useful in unlexicalized approaches (Klein and Manning, 2003). Preliminary experiments showed that this increased the size of the grammar, and the runtime of the algorithm, without leading to improvements in accuracy. Phrase-structure trees also include unary rules of the form A → β1∗ . To handle unary rules we modify the parsing algorithms in Figure 3 to include a unary completion rule, Table 1: Comparison of three parsing setups: L EX CKY∗ is the complete lexicalized c-parser on Y(x), but limited to only sentences less than 20 words for tractability, D EP CKY is the constrained c-parser on Y(x, d), P RUNE 1, P RUNE 2, and P RUNE 1+2 are combinations o"
N15-1080,J93-2004,0,0.0507126,"have also been several papers that use ideas from dependency parsing to simplify and speed up phrase-structure prediction. Zhu et al. (2013) build a high-accuracy phrase-structure parser using a transition-based system. Hall et al. (2014) use a stripped down parser based on a simple X-bar grammar and a small set of lexicalized features. 6 Methods We ran a series of experiments to assess the accuracy, efficiency, and applicability of our parser, PAD, to several tasks. These experiments use the following setup. For English experiments we use the standard Penn Treebank (PTB) experimental setup (Marcus et al., 1993). Training is done on §2–21, development on §22, and testing on §23. We use the development set to tune the regularization parameter, λ = 1e−8, and the pruning threshold, γ = 0.95. For Chinese experiments, we use version 5.1 of the Penn Chinese Treebank 5.1 (CTB) (Xue et al., 2005). We followed previous work and used articles 001–270 and 440–1151 for training, 301–325 for development, and 271–300 for test. We also use the development set to tune the regularization parame6 https://github.com/syllog1sm/redshift http://stp.lingfil.uu.se/˜nivre/ research/chn_headrules.txt 8 https://github.com/yaho"
N15-1080,P13-2109,1,0.863476,"parser. For these experiments we treat our system and the Zhang-Nivre parser as an independently trained, but complete end-to-end c-parser. Runtime for these experiments includes both the time for dparsing and conversion. Despite the fixed depenUAS Table 4: The effect of d-parsing accuracy (PTB §22) on PAD and an oracle converter. Runtime includes d-parsing and cparsing. Inputs include MaltParser (Nivre et al., 2006), the RedShift and the Yara implementations of the parser of Zhang and Nivre (2011) with various beam size, and three versions of TurboParser trained with projective constraints (Martins et al., 2013). unlabeled accuracy score (UAS). We implemented the grammar binarization, head rules, and pruning tables in Python, and the parser, features, and training in C++. Experiments are performed on a Lenovo ThinkCentre desktop computer with 32GB of memory and Core i7-3770 3.4GHz 8M cache CPU. 7 Model Effect of Dependencies Table 4 shows experiments comparing the effect of different input dparses. For these experiments we used the same version of PAD with 11 different d-parsers of varying quality and speed. We measure for each parser: its UAS, speed, and labeled F1 when used with PAD and with an ora"
N15-1080,nivre-etal-2006-maltparser,0,0.0456621,"f dependency parsing accuracy, and the effect of the amount of annotated phrase-structure data. Parsing Accuracy Table 3 compares the accuracy and speed of the phrase-structure trees produced by the parser. For these experiments we treat our system and the Zhang-Nivre parser as an independently trained, but complete end-to-end c-parser. Runtime for these experiments includes both the time for dparsing and conversion. Despite the fixed depenUAS Table 4: The effect of d-parsing accuracy (PTB §22) on PAD and an oracle converter. Runtime includes d-parsing and cparsing. Inputs include MaltParser (Nivre et al., 2006), the RedShift and the Yara implementations of the parser of Zhang and Nivre (2011) with various beam size, and three versions of TurboParser trained with projective constraints (Martins et al., 2013). unlabeled accuracy score (UAS). We implemented the grammar binarization, head rules, and pruning tables in Python, and the parser, features, and training in C++. Experiments are performed on a Lenovo ThinkCentre desktop computer with 32GB of memory and Core i7-3770 3.4GHz 8M cache CPU. 7 Model Effect of Dependencies Table 4 shows experiments comparing the effect of different input dparses. For t"
N15-1080,N07-1051,0,0.655647,"er than c-parsers, we consider an alternate pipeline (Section 3): d-parse first, then transform the dependency representation into a phrase-structure tree constrained to be consistent with the dependency parse. This idea was explored by Xia and Palmer (2001) and Xia et al. (2009) using hand-written rules. Instead, we present a data-driven algorithm using the structured prediction framework (Section 4). The approach can be understood as a specially-trained coarse-to-fine decoding algorithm where a d-parser provides “coarse” structure and the second stage refines it (Charniak and Johnson, 2005; Petrov and Klein, 2007). Our lexicalized phrase-structure parser, PAD, is asymptotically faster than parsing with a lexicalized context-free grammar: O(n2 ) plus d-parsing, vs. O(n5 ) worst case runtime in sentence length n, with the same grammar constant. Experiments show that our approach achieves linear observable runtime, and accuracy similar to state-of-the-art phrase-structure parsers without reranking or semisupervised training (Section 7). 2 Background We begin with the conventional development by first introducing c-parsing and then defining d-parses through a mechanical conversion using head rules. In the"
N15-1080,D10-1001,1,0.934096,"Missing"
N15-1080,W13-2307,1,0.904871,"Missing"
N15-1080,P13-1045,0,0.0514711,"hu (2013) Carreras (2008) 89.5 85.5 90.1 90.3 91.1 – 5.3 8.6 39.0 – CJ Reranking (2005) Stanford RNN (2013) 91.5 90.0 4.3 2.8 PAD PAD (Pruned) 90.4 90.3 34.3 58.6 Model CTB F1 Charniak (2000) Bikel (2004) Petrov (2007) Zhu (2013) 80.8 80.6 83.3 83.2 PAD 82.4 Table 3: Accuracy and speed on PTB §23 and CTB 5.1 test split. Comparisons are to state-of-the-art non-reranking supervised phrase-structure parsers (Charniak, 2000; Klein and Manning, 2003; Petrov and Klein, 2007; Carreras et al., 2008; Zhu et al., 2013; Bikel, 2004), and semi-supervised and reranking parsers (Charniak and Johnson, 2005; Socher et al., 2013). F1 Sent./s. Oracle M ALT PARSER RS-K1 RS-K4 RS-K16 YARA -K1 YARA -K16 YARA -K32 YARA -K64 TP-BASIC TP-S TANDARD TP-F ULL 89.7 90.1 92.5 93.1 89.7 92.9 93.1 93.1 92.8 93.3 93.5 85.5 86.6 90.1 90.6 85.3 89.8 90.4 90.5 88.9 90.9 90.8 240.7 233.9 151.3 58.6 1265.8 157.5 48.3 47.3 132.8 27.2 13.2 87.8 87.6 91.5 92.5 86.7 91.7 92.0 92.2 90.8 92.6 92.9 dency constraints, the English results show that the parser is comparable in accuracy to many widelyused systems, and is significantly faster. The parser most competitive in both speed and accuracy is that of Zhu et al. (2013), a fast shift-reduce ph"
N15-1080,H01-1014,0,0.275729,"ite extensive work on directto-dependency parsing algorithms (which we call dparsing), the most accurate dependency parsers for English still involve phrase-structure parsing (which we call c-parsing) followed by rule-based extraction of dependencies (Kong and Smith, 2014). What if dependency annotations had come first? Because d-parsers are generally much faster than c-parsers, we consider an alternate pipeline (Section 3): d-parse first, then transform the dependency representation into a phrase-structure tree constrained to be consistent with the dependency parse. This idea was explored by Xia and Palmer (2001) and Xia et al. (2009) using hand-written rules. Instead, we present a data-driven algorithm using the structured prediction framework (Section 4). The approach can be understood as a specially-trained coarse-to-fine decoding algorithm where a d-parser provides “coarse” structure and the second stage refines it (Charniak and Johnson, 2005; Petrov and Klein, 2007). Our lexicalized phrase-structure parser, PAD, is asymptotically faster than parsing with a lexicalized context-free grammar: O(n2 ) plus d-parsing, vs. O(n5 ) worst case runtime in sentence length n, with the same grammar constant. E"
N15-1080,W03-3023,0,0.112004,"nd a derived head index. The blue and red vertices have the words automaker2 and sold3 as heads respectively. The vertex VP(3) implies that automaker2 is a left-dependent of sold3 , and that 2 ∈ L(3) in the d-parse. Dependency Parsing Dependency parses provide an alternative, and in some sense simpler, representation of sentence structure. These d-parses can be derived through mechanical transformation from context-free trees. There are several popular transformations in wide use; each provides a different representation of a sentence’s structure (Collins, 2003; De Marneffe and Manning, 2008; Yamada and Matsumoto, 2003; Johansson and Nugues, 2007). We consider the class of transformations that are defined through local head rules. For a binary CFG, define a collection of head rules as a mapping from each CFG rule to a head preference for its left or right child. We use the notation A → β1∗ β2 and A → β1 β2∗ to indicate a left- or right-headed rule, respectively. The head rules can be used to map a c-parse to a dependency tree (d-parse). In a d-parse, each word in the sentence is assigned as a dependent to a head word, h ∈ {0, . . . , n}, where 0 is a special symbol indicating the pseudo-root of the sentence"
N15-1080,P11-2033,0,0.0591488,"structure data. Parsing Accuracy Table 3 compares the accuracy and speed of the phrase-structure trees produced by the parser. For these experiments we treat our system and the Zhang-Nivre parser as an independently trained, but complete end-to-end c-parser. Runtime for these experiments includes both the time for dparsing and conversion. Despite the fixed depenUAS Table 4: The effect of d-parsing accuracy (PTB §22) on PAD and an oracle converter. Runtime includes d-parsing and cparsing. Inputs include MaltParser (Nivre et al., 2006), the RedShift and the Yara implementations of the parser of Zhang and Nivre (2011) with various beam size, and three versions of TurboParser trained with projective constraints (Martins et al., 2013). unlabeled accuracy score (UAS). We implemented the grammar binarization, head rules, and pruning tables in Python, and the parser, features, and training in C++. Experiments are performed on a Lenovo ThinkCentre desktop computer with 32GB of memory and Core i7-3770 3.4GHz 8M cache CPU. 7 Model Effect of Dependencies Table 4 shows experiments comparing the effect of different input dparses. For these experiments we used the same version of PAD with 11 different d-parsers of var"
N15-1080,P13-1043,0,0.405068,"oth for pruning and within a richer lexicalized parser. Similarly, Rush et al. (2010) use dual decomposition to combine a powerful dependency parser with a lexicalized phrase-structure model. This work differs in that we treat the dependency parse as a hard constraint, hence largely reduce the runtime of a fully lexicalized phrase structure parsing model while maintaining the ability, at least in principle, to generate highly accurate phrasestructure parses. Finally there have also been several papers that use ideas from dependency parsing to simplify and speed up phrase-structure prediction. Zhu et al. (2013) build a high-accuracy phrase-structure parser using a transition-based system. Hall et al. (2014) use a stripped down parser based on a simple X-bar grammar and a small set of lexicalized features. 6 Methods We ran a series of experiments to assess the accuracy, efficiency, and applicability of our parser, PAD, to several tasks. These experiments use the following setup. For English experiments we use the standard Penn Treebank (PTB) experimental setup (Marcus et al., 1993). Training is done on §2–21, development on §22, and testing on §23. We use the development set to tune the regularizatio"
N15-1114,W13-2322,0,0.0643628,"et al., 2013), but also for non-textual media (e.g., videos and image collections; Kim et al., 2014; Kuznetsova et al., 2014; Zhao and Xing, 2014), where extractive and compressive summarization techniques simply do not suffice. We believe that the challenge of abstractive summarization deserves renewed attention and propose that recent developments in semantic analysis have an important role to play. We conduct the first study exploring the feasibility of an abstractive summarization system based on transformations of semantic representations such as the Abstract Meaning Representation (AMR; Banarescu et al., 2013). Example sentences and their AMR graphs are shown in Fig. 1. AMR has much in common with earlier formalisms (Kasper, 1989; Dorr et al., 1998); today an annotated corpus comprised of over 20,000 AMR-analyzed English sentences (Knight et al., 2014) and an automatic AMR parser (JAMR; Flanigan et al., 2014) are available. In our framework, summarization consists of three steps illustrated in Fig. 1: (1) parsing the input sentences to individual AMR graphs, (2) combining and transforming those graphs into a single summary AMR graph, and (3) generating text from the summary graph. This paper focuse"
N15-1114,P11-1049,0,0.0249704,"el that is able to better discriminate among the enlarged output space, graph expansion still has promise to be helpful. 7 Related and Future Work According to Dang and Owczarzak (2008), the majority of competitive summarization systems are extractive, selecting representative sentences from input documents and concatenating them to form a summary. This is often combined with sentence compression, allowing more sentences to be included within a budget. ILPs and approximations have been used to encode compression and extraction (McDonald, 2007; Martins and Smith, 2009; Gillick and Favre, 2009; Berg-Kirkpatrick et al., 2011; Almeida and Martins, 2013; Li et al., 2014). Other decoding approaches have included a greedy method exploiting submodularity (Lin and Bilmes, 2010), document reconstruction (He et al., 2012), and graph cuts (Qian and Liu, 2013), among others. Previous work on abstractive summarization has explored user studies that compare extractive with NLG-based abstractive summarization (Carenini and Cheung, 2008). Ganesan et al. (2010) propose to construct summary sentences by repeatedly searching the highest scored graph paths. (Gerani et al., 2014) generate abstractive summaries by modifying discours"
N15-1114,P13-2131,0,0.168393,"generation module (step 3; §5). 3 Dataset To build and evaluate our framework, we require a dataset that includes inputs and summaries, each with gold-standard AMR annotations.4 This allows us to use a statistical model for step 2 (graph summarization) and to separate its errors from those in step 1 (AMR parsing), which is important in determining whether this approach is worth further investment. Fortunately, the “proxy report” section of the AMR Bank (Knight et al., 2014) suits our needs. A 2 http://www.isi.edu/˜ulf/amr/help/ amr-guidelines.pdf 3 AMR parse quality is evaluated using smatch (Cai and Knight, 2013), which measures the accuracy of concept and relation predictions. JAMR was trained on the in-domain training portion of LDC2014T12 for our experiments. 4 Traditional multi-document summarization datasets, such as the ones used in DUC and TAC competitions, do not have gold-standard AMR annotations. Ave. # Sents. Source Graph Summ. Doc. Nodes Edges Expand 298 1.5 17.5 127 188 2,670 35 1.4 19.2 143 220 3,203 33 1.4 20.5 162 255 4,002 # Docs. Train Dev. Test Table 1: Statistics of our dataset. “Expand” shows the number of edges after performing graph expansion. The numbers are averaged across all"
N15-1114,W08-1106,0,0.0320492,"more sentences to be included within a budget. ILPs and approximations have been used to encode compression and extraction (McDonald, 2007; Martins and Smith, 2009; Gillick and Favre, 2009; Berg-Kirkpatrick et al., 2011; Almeida and Martins, 2013; Li et al., 2014). Other decoding approaches have included a greedy method exploiting submodularity (Lin and Bilmes, 2010), document reconstruction (He et al., 2012), and graph cuts (Qian and Liu, 2013), among others. Previous work on abstractive summarization has explored user studies that compare extractive with NLG-based abstractive summarization (Carenini and Cheung, 2008). Ganesan et al. (2010) propose to construct summary sentences by repeatedly searching the highest scored graph paths. (Gerani et al., 2014) generate abstractive summaries by modifying discourse parse trees. Our work is similar in spirit to Cheung and Penn (2014), which splices and recombines dependency parse trees to produce abstractive summaries. In contrast, our work operates on semantic graphs, taking advantage of the recently developed AMR Bank. Also related to our work are graph-based summarization methods (Vanderwende et al., 2004; Erkan and Radev, 2004; Mihalcea and Tarau, 2004). Vande"
N15-1114,D14-1085,0,0.0432116,"ther decoding approaches have included a greedy method exploiting submodularity (Lin and Bilmes, 2010), document reconstruction (He et al., 2012), and graph cuts (Qian and Liu, 2013), among others. Previous work on abstractive summarization has explored user studies that compare extractive with NLG-based abstractive summarization (Carenini and Cheung, 2008). Ganesan et al. (2010) propose to construct summary sentences by repeatedly searching the highest scored graph paths. (Gerani et al., 2014) generate abstractive summaries by modifying discourse parse trees. Our work is similar in spirit to Cheung and Penn (2014), which splices and recombines dependency parse trees to produce abstractive summaries. In contrast, our work operates on semantic graphs, taking advantage of the recently developed AMR Bank. Also related to our work are graph-based summarization methods (Vanderwende et al., 2004; Erkan and Radev, 2004; Mihalcea and Tarau, 2004). Vanderwende et al. (2004) transform input to logical forms, score nodes using PageRank, and grow the graph from high-value nodes using heuristics. In Erkan and Radev (2004) and Mihalcea and Tarau (2004), the graph connects surface terms that co-occur. In both cases, t"
N15-1114,W02-1001,0,0.065115,"nstraint can be used to fix the size of the summary graph (measured by the number of edges) to L: XX ei,j = L (8) i j The performance of summarization systems depends strongly on their compression rate, so systems are only directly comparable when their compression rates are similar (Napoles et al., 2011). L is supplied to the system to control summary graph size. 5 http://www.gurobi.com 4.2.2 6 Parameter Estimation Given a collection of input and output pairs (here, source graphs and summary graphs), a natural starting place for learning the coefficients θ and ψ is the structured perceptron (Collins, 2002), which is easy to implement and often performs well. Alternatively, incorporating factored cost functions through a structured hinge loss leads to a structured support vector machine (SVM; Taskar et al., 2004) which can be learned with a very similar stochastic optimization algorithm. In our scenario, however, the gold-standard summary graph may not actually be a subset of the source graph. In machine translation, ramp loss has been found to work well in situations where the gold-standard output may not even be in the hypothesis space of the model (Gimpel and Smith, 2012). The structured perc"
N15-1114,dorr-etal-1998-thematic,0,0.039752,"where extractive and compressive summarization techniques simply do not suffice. We believe that the challenge of abstractive summarization deserves renewed attention and propose that recent developments in semantic analysis have an important role to play. We conduct the first study exploring the feasibility of an abstractive summarization system based on transformations of semantic representations such as the Abstract Meaning Representation (AMR; Banarescu et al., 2013). Example sentences and their AMR graphs are shown in Fig. 1. AMR has much in common with earlier formalisms (Kasper, 1989; Dorr et al., 1998); today an annotated corpus comprised of over 20,000 AMR-analyzed English sentences (Knight et al., 2014) and an automatic AMR parser (JAMR; Flanigan et al., 2014) are available. In our framework, summarization consists of three steps illustrated in Fig. 1: (1) parsing the input sentences to individual AMR graphs, (2) combining and transforming those graphs into a single summary AMR graph, and (3) generating text from the summary graph. This paper focuses on step 2, treating it as a structured prediction problem. We assume text documents as input1 and use JAMR for step 1. We use a simple metho"
N15-1114,P14-1134,1,0.755856,"ttention and propose that recent developments in semantic analysis have an important role to play. We conduct the first study exploring the feasibility of an abstractive summarization system based on transformations of semantic representations such as the Abstract Meaning Representation (AMR; Banarescu et al., 2013). Example sentences and their AMR graphs are shown in Fig. 1. AMR has much in common with earlier formalisms (Kasper, 1989; Dorr et al., 1998); today an annotated corpus comprised of over 20,000 AMR-analyzed English sentences (Knight et al., 2014) and an automatic AMR parser (JAMR; Flanigan et al., 2014) are available. In our framework, summarization consists of three steps illustrated in Fig. 1: (1) parsing the input sentences to individual AMR graphs, (2) combining and transforming those graphs into a single summary AMR graph, and (3) generating text from the summary graph. This paper focuses on step 2, treating it as a structured prediction problem. We assume text documents as input1 and use JAMR for step 1. We use a simple method to read a bag of words off the summary graph, allowing evaluation with ROUGE-1, and leave full text generation from AMR (step 3) to future work. The graph summar"
N15-1114,C10-1039,0,0.125403,"d within a budget. ILPs and approximations have been used to encode compression and extraction (McDonald, 2007; Martins and Smith, 2009; Gillick and Favre, 2009; Berg-Kirkpatrick et al., 2011; Almeida and Martins, 2013; Li et al., 2014). Other decoding approaches have included a greedy method exploiting submodularity (Lin and Bilmes, 2010), document reconstruction (He et al., 2012), and graph cuts (Qian and Liu, 2013), among others. Previous work on abstractive summarization has explored user studies that compare extractive with NLG-based abstractive summarization (Carenini and Cheung, 2008). Ganesan et al. (2010) propose to construct summary sentences by repeatedly searching the highest scored graph paths. (Gerani et al., 2014) generate abstractive summaries by modifying discourse parse trees. Our work is similar in spirit to Cheung and Penn (2014), which splices and recombines dependency parse trees to produce abstractive summaries. In contrast, our work operates on semantic graphs, taking advantage of the recently developed AMR Bank. Also related to our work are graph-based summarization methods (Vanderwende et al., 2004; Erkan and Radev, 2004; Mihalcea and Tarau, 2004). Vanderwende et al. (2004) tr"
N15-1114,D14-1168,0,0.0974604,"s and Smith, 2009; Gillick and Favre, 2009; Berg-Kirkpatrick et al., 2011; Almeida and Martins, 2013; Li et al., 2014). Other decoding approaches have included a greedy method exploiting submodularity (Lin and Bilmes, 2010), document reconstruction (He et al., 2012), and graph cuts (Qian and Liu, 2013), among others. Previous work on abstractive summarization has explored user studies that compare extractive with NLG-based abstractive summarization (Carenini and Cheung, 2008). Ganesan et al. (2010) propose to construct summary sentences by repeatedly searching the highest scored graph paths. (Gerani et al., 2014) generate abstractive summaries by modifying discourse parse trees. Our work is similar in spirit to Cheung and Penn (2014), which splices and recombines dependency parse trees to produce abstractive summaries. In contrast, our work operates on semantic graphs, taking advantage of the recently developed AMR Bank. Also related to our work are graph-based summarization methods (Vanderwende et al., 2004; Erkan and Radev, 2004; Mihalcea and Tarau, 2004). Vanderwende et al. (2004) transform input to logical forms, score nodes using PageRank, and grow the graph from high-value nodes using heuristics"
N15-1114,W09-1802,0,0.0247998,"a more sophisticated model that is able to better discriminate among the enlarged output space, graph expansion still has promise to be helpful. 7 Related and Future Work According to Dang and Owczarzak (2008), the majority of competitive summarization systems are extractive, selecting representative sentences from input documents and concatenating them to form a summary. This is often combined with sentence compression, allowing more sentences to be included within a budget. ILPs and approximations have been used to encode compression and extraction (McDonald, 2007; Martins and Smith, 2009; Gillick and Favre, 2009; Berg-Kirkpatrick et al., 2011; Almeida and Martins, 2013; Li et al., 2014). Other decoding approaches have included a greedy method exploiting submodularity (Lin and Bilmes, 2010), document reconstruction (He et al., 2012), and graph cuts (Qian and Liu, 2013), among others. Previous work on abstractive summarization has explored user studies that compare extractive with NLG-based abstractive summarization (Carenini and Cheung, 2008). Ganesan et al. (2010) propose to construct summary sentences by repeatedly searching the highest scored graph paths. (Gerani et al., 2014) generate abstractive"
N15-1114,N12-1023,1,0.581372,"is the structured perceptron (Collins, 2002), which is easy to implement and often performs well. Alternatively, incorporating factored cost functions through a structured hinge loss leads to a structured support vector machine (SVM; Taskar et al., 2004) which can be learned with a very similar stochastic optimization algorithm. In our scenario, however, the gold-standard summary graph may not actually be a subset of the source graph. In machine translation, ramp loss has been found to work well in situations where the gold-standard output may not even be in the hypothesis space of the model (Gimpel and Smith, 2012). The structured perceptron, hinge, and ramp losses are compared in Table 4. We explore learning by minimizing each of the perceptron, hinge, and ramp losses, each optimized using Adagrad (Duchi et al., 2011), a stochastic optimization procedure. Let β be one model parameter (coefficient from θ or ψ). Let g (t) be the subgradient of the loss on the instance considered on the tth iteration with respect to β. Given an initial step size η, the update for β on iteration t is: β (t+1) ← β (t) − qP t η τ =1 5 (g (τ ) )2 g (t) (9) Generation Generation from AMR-like representations has received some"
N15-1114,N06-2015,0,0.105943,"Missing"
N15-1114,H89-1022,0,0.119216,"d Xing, 2014), where extractive and compressive summarization techniques simply do not suffice. We believe that the challenge of abstractive summarization deserves renewed attention and propose that recent developments in semantic analysis have an important role to play. We conduct the first study exploring the feasibility of an abstractive summarization system based on transformations of semantic representations such as the Abstract Meaning Representation (AMR; Banarescu et al., 2013). Example sentences and their AMR graphs are shown in Fig. 1. AMR has much in common with earlier formalisms (Kasper, 1989; Dorr et al., 1998); today an annotated corpus comprised of over 20,000 AMR-analyzed English sentences (Knight et al., 2014) and an automatic AMR parser (JAMR; Flanigan et al., 2014) are available. In our framework, summarization consists of three steps illustrated in Fig. 1: (1) parsing the input sentences to individual AMR graphs, (2) combining and transforming those graphs into a single summary AMR graph, and (3) generating text from the summary graph. This paper focuses on step 2, treating it as a structured prediction problem. We assume text documents as input1 and use JAMR for step 1. W"
N15-1114,Q14-1028,0,0.0105344,"ents on goldstandard AMR annotations and system parses show promising results. Code is available at: https://github.com/summarization 1 Introduction Abstractive summarization is an elusive technological capability in which textual summaries of content are generated de novo. Demand is on the rise for high-quality summaries not just for lengthy texts (e.g., books; Bamman and Smith, 2013) and texts known to be prohibitively difficult for people to understand (e.g., website privacy policies; Sadeh et al., 2013), but also for non-textual media (e.g., videos and image collections; Kim et al., 2014; Kuznetsova et al., 2014; Zhao and Xing, 2014), where extractive and compressive summarization techniques simply do not suffice. We believe that the challenge of abstractive summarization deserves renewed attention and propose that recent developments in semantic analysis have an important role to play. We conduct the first study exploring the feasibility of an abstractive summarization system based on transformations of semantic representations such as the Abstract Meaning Representation (AMR; Banarescu et al., 2013). Example sentences and their AMR graphs are shown in Fig. 1. AMR has much in common with earlier for"
N15-1114,P98-1116,0,0.0155022,"ceptron, hinge, and ramp losses are compared in Table 4. We explore learning by minimizing each of the perceptron, hinge, and ramp losses, each optimized using Adagrad (Duchi et al., 2011), a stochastic optimization procedure. Let β be one model parameter (coefficient from θ or ψ). Let g (t) be the subgradient of the loss on the instance considered on the tth iteration with respect to β. Given an initial step size η, the update for β on iteration t is: β (t+1) ← β (t) − qP t η τ =1 5 (g (τ ) )2 g (t) (9) Generation Generation from AMR-like representations has received some attention, e.g., by Langkilde and Knight (1998) who described a statistical method. Though we know of work in progress driven by the goal of machine translation using AMR, there is currently no system available. We therefore use a heuristic approach to generate a bag of words. Given a predicted subgraph, a system summary is created by finding the most frequently aligned word span for each concept node. (Recall that the JAMR parser provides these alignments; §2). The words in the resulting spans are generated in no particular order. While this is not a natural language summary, it is suitable for unigram-based summarization evaluation metho"
N15-1114,D14-1076,1,0.828156,"output space, graph expansion still has promise to be helpful. 7 Related and Future Work According to Dang and Owczarzak (2008), the majority of competitive summarization systems are extractive, selecting representative sentences from input documents and concatenating them to form a summary. This is often combined with sentence compression, allowing more sentences to be included within a budget. ILPs and approximations have been used to encode compression and extraction (McDonald, 2007; Martins and Smith, 2009; Gillick and Favre, 2009; Berg-Kirkpatrick et al., 2011; Almeida and Martins, 2013; Li et al., 2014). Other decoding approaches have included a greedy method exploiting submodularity (Lin and Bilmes, 2010), document reconstruction (He et al., 2012), and graph cuts (Qian and Liu, 2013), among others. Previous work on abstractive summarization has explored user studies that compare extractive with NLG-based abstractive summarization (Carenini and Cheung, 2008). Ganesan et al. (2010) propose to construct summary sentences by repeatedly searching the highest scored graph paths. (Gerani et al., 2014) generate abstractive summaries by modifying discourse parse trees. Our work is similar in spirit"
N15-1114,N10-1134,0,0.0333914,"o Dang and Owczarzak (2008), the majority of competitive summarization systems are extractive, selecting representative sentences from input documents and concatenating them to form a summary. This is often combined with sentence compression, allowing more sentences to be included within a budget. ILPs and approximations have been used to encode compression and extraction (McDonald, 2007; Martins and Smith, 2009; Gillick and Favre, 2009; Berg-Kirkpatrick et al., 2011; Almeida and Martins, 2013; Li et al., 2014). Other decoding approaches have included a greedy method exploiting submodularity (Lin and Bilmes, 2010), document reconstruction (He et al., 2012), and graph cuts (Qian and Liu, 2013), among others. Previous work on abstractive summarization has explored user studies that compare extractive with NLG-based abstractive summarization (Carenini and Cheung, 2008). Ganesan et al. (2010) propose to construct summary sentences by repeatedly searching the highest scored graph paths. (Gerani et al., 2014) generate abstractive summaries by modifying discourse parse trees. Our work is similar in spirit to Cheung and Penn (2014), which splices and recombines dependency parse trees to produce abstractive sum"
N15-1114,W04-1013,0,0.0178592,"Missing"
N15-1114,W09-1801,1,0.713494,"th more training data, or a more sophisticated model that is able to better discriminate among the enlarged output space, graph expansion still has promise to be helpful. 7 Related and Future Work According to Dang and Owczarzak (2008), the majority of competitive summarization systems are extractive, selecting representative sentences from input documents and concatenating them to form a summary. This is often combined with sentence compression, allowing more sentences to be included within a budget. ILPs and approximations have been used to encode compression and extraction (McDonald, 2007; Martins and Smith, 2009; Gillick and Favre, 2009; Berg-Kirkpatrick et al., 2011; Almeida and Martins, 2013; Li et al., 2014). Other decoding approaches have included a greedy method exploiting submodularity (Lin and Bilmes, 2010), document reconstruction (He et al., 2012), and graph cuts (Qian and Liu, 2013), among others. Previous work on abstractive summarization has explored user studies that compare extractive with NLG-based abstractive summarization (Carenini and Cheung, 2008). Ganesan et al. (2010) propose to construct summary sentences by repeatedly searching the highest scored graph paths. (Gerani et al., 20"
N15-1114,P09-1039,1,0.53547,"Missing"
N15-1114,W04-3252,0,0.00680701,"tion (Carenini and Cheung, 2008). Ganesan et al. (2010) propose to construct summary sentences by repeatedly searching the highest scored graph paths. (Gerani et al., 2014) generate abstractive summaries by modifying discourse parse trees. Our work is similar in spirit to Cheung and Penn (2014), which splices and recombines dependency parse trees to produce abstractive summaries. In contrast, our work operates on semantic graphs, taking advantage of the recently developed AMR Bank. Also related to our work are graph-based summarization methods (Vanderwende et al., 2004; Erkan and Radev, 2004; Mihalcea and Tarau, 2004). Vanderwende et al. (2004) transform input to logical forms, score nodes using PageRank, and grow the graph from high-value nodes using heuristics. In Erkan and Radev (2004) and Mihalcea and Tarau (2004), the graph connects surface terms that co-occur. In both cases, the graphs are constructed based on surface text; it is not a representation of propositional semantics like AMR. However, future 1084 work might explore similar graph-based calculations to contribute features for subgraph selection in our framework. Our constructed source graph can easily reach ten times or more of the size of a"
N15-1114,W11-1611,0,0.0238146,"Missing"
N15-1114,D13-1156,0,0.00841813,"extractive, selecting representative sentences from input documents and concatenating them to form a summary. This is often combined with sentence compression, allowing more sentences to be included within a budget. ILPs and approximations have been used to encode compression and extraction (McDonald, 2007; Martins and Smith, 2009; Gillick and Favre, 2009; Berg-Kirkpatrick et al., 2011; Almeida and Martins, 2013; Li et al., 2014). Other decoding approaches have included a greedy method exploiting submodularity (Lin and Bilmes, 2010), document reconstruction (He et al., 2012), and graph cuts (Qian and Liu, 2013), among others. Previous work on abstractive summarization has explored user studies that compare extractive with NLG-based abstractive summarization (Carenini and Cheung, 2008). Ganesan et al. (2010) propose to construct summary sentences by repeatedly searching the highest scored graph paths. (Gerani et al., 2014) generate abstractive summaries by modifying discourse parse trees. Our work is similar in spirit to Cheung and Penn (2014), which splices and recombines dependency parse trees to produce abstractive summaries. In contrast, our work operates on semantic graphs, taking advantage of t"
N15-1114,W13-3508,0,0.026987,"In this preliminary study we force the summary graph to be tree-structured, requiring that there is at most one incoming edge for each node: i ∀j ≤ N, (5) ∀i, j ≤ N. (6) 1081 X ei,j ≤ 1, ∀j ≤ N. (7) j Interestingly, the formulation so far equates to an ILP for solving the prize-collecting Steiner tree problem (PCST; Segev, 1987), which is known to be NP-complete (Karp, 1972). Our ILP formulation is modified from that of Ljubi´c et al. (2006). Flow-based constraints for tree structures have also previously been used in NLP for dependency parsing (Martins et al., 2009) and sentence compression (Thadani and McKeown, 2013). In our experiments, we use an exact ILP solver,5 though many approximate methods are available. Finally, an optional constraint can be used to fix the size of the summary graph (measured by the number of edges) to L: XX ei,j = L (8) i j The performance of summarization systems depends strongly on their compression rate, so systems are only directly comparable when their compression rates are similar (Napoles et al., 2011). L is supplied to the system to control summary graph size. 5 http://www.gurobi.com 4.2.2 6 Parameter Estimation Given a collection of input and output pairs (here, source"
N15-1114,C98-1112,0,\N,Missing
N15-1114,P13-1020,0,\N,Missing
N15-1114,W01-0100,0,\N,Missing
N15-1177,attardi-etal-2010-resource,0,0.388457,"Missing"
N15-1177,J92-4003,0,0.141282,"in our data; for experiments we only consider tags occurring in train, yielding ∣Y∣ = 146. We also run a condition where the supersense refinements are collapsed, i.e. Y consists of the 8 MWE tags. This allows us to measure the impact of the supersenses on MWE identification performance. 4.5 Features We constrast three feature sets for full supersense tagging: (a) Schneider et al.’s (2014a) basic MWE features, which include lemmas, POS tags, word shapes, and whether the token potentially matches entries in any of several multiword lexicons; (b) the basic MWE features plus the Brown clusters (Brown et al., 1992) used by Schneider et al. (2014a); and (c) the basic MWE features and Brown clusters, plus several new features shown in figure 4. Chiefly, these new features consult the supersenses of WordNet synsets associated with words in the sentence: the first WordNet supersense feature is inspired by Ciaramita and Altun (2006) and subsequent work on supersense tagging, while the has-supersense feature is novel. There is also a feature aimed at distinguishing auxiliary verbs from main verbs, and new capitalization features take into account the capitalization of the first word in the sentence and the ma"
N15-1177,W06-1670,0,0.720384,"synsets are associated with lexical entries, the supersense categories are unlexicalized. The N : PERSON category, for instance, contains synsets for both principal and student. A different sense of principal falls under N : POSSESSION. As far as we are aware, the supersenses were originally intended only as a method of organizing the WordNet structure. But Ciaramita and Johnson (2003) pioneered the coarse word sense disambiguation task of supersense tagging, noting that the supersense categories provided a natural broadening of the traditional named entity categories to encompass all nouns. Ciaramita and Altun (2006) later expanded the task to include all verbs, and applied a supervised sequence modeling framework adapted from NER. Evaluation was against manually sensetagged data that had been automatically converted to the coarser supersenses. Similar taggers have since been built for Italian (Picca et al., 2008) and Chinese (Qiu et al., 2011), both of which have their own WordNets mapped to English WordNet. Although many of the annotated expressions in existing supersense datasets contain multiple words, the relationship between MWEs and supersenses has not received much attention. (Piao et al. (2003, 2"
N15-1177,W03-1022,0,0.128646,"e of the names overlap between the noun and verb inventories, but they are to be considered separate categories; hereafter, we will distinguish the noun and verb categories with prefixes, e.g. N : COGNITION vs. V: COGNITION. Though WordNet synsets are associated with lexical entries, the supersense categories are unlexicalized. The N : PERSON category, for instance, contains synsets for both principal and student. A different sense of principal falls under N : POSSESSION. As far as we are aware, the supersenses were originally intended only as a method of organizing the WordNet structure. But Ciaramita and Johnson (2003) pioneered the coarse word sense disambiguation task of supersense tagging, noting that the supersense categories provided a natural broadening of the traditional named entity categories to encompass all nouns. Ciaramita and Altun (2006) later expanded the task to include all verbs, and applied a supervised sequence modeling framework adapted from NER. Evaluation was against manually sensetagged data that had been automatically converted to the coarser supersenses. Similar taggers have since been built for Italian (Picca et al., 2008) and Chinese (Qiu et al., 2011), both of which have their ow"
N15-1177,W02-1001,0,0.03984,": SOCIAL N : PERSON I B I ¯ I O B o Figure 3: Tagging for part of the lexical semantic analysis depicted in figure 1. Note that for nominal and verbal MWEs, the supersense label is only attached to the first tag of the expression. Though our focus in this paper is on English, automatic supersense tagging has also been explored in Italian (Picca et al., 2008, 2009; Attardi et al., 2010, 2013; Rossi et al., 2013), Chinese (Qiu et al., 2011), and Arabic (Schneider et al., 2013). 4.2 Model Like Ciaramita and Altun (2006) and Schneider et al. (2014a), we train a first-order structured perceptron (Collins, 2002) with averaging. This is a standard discriminative modeling setup, involving: a linear scoring function over features of input–output pairs; a Viterbi search to choose the highest-scoring valid output tag sequence given the input; and an online learning algorithm that makes M passes through the training data, searching for the best tagging given the current model and updating the parameters (linear feature weights) where the best tagging doesn’t match the gold tagging. With a first-order Markov assumption and tagset Y, the Viterbi search for a sentence x requires O(∣Y∣2 ⋅ ∣x∣) runtime. The dat"
N15-1177,W13-3511,0,0.0641405,"neralize beyond lexical items lead to better supersense labeling. The best model has access to supersense information in the WordNet lexicon; it is 4 F1 points better at choosing the correct class label than its nearest competitor, which relies on word clusters to abstract away from individual lexical items. Nouns, verbs, and auxiliaries all see improvements. We also inspect the learned parameters. The highest-weighted parameters suggest that the best model relies heavily on the supersense lookup features, whereas the second-best model—lacking those—in large part relies on Brown clusters (cf. Grave et al., 2013). The auxiliary verb vs. main verb feature in the best model is highly weighted as well, helping to distinguish between `a and V: STATIVE. Polysemy. We have motivated the task of supersense tagging in part as a coarse form of word sense disambiguation. Therefore, it is worth investigating how well the learned model manages to choose the correct supersense for nouns and verbs that are ambiguous in the data. A handful of lemmas in test have at least two different supersenses predicted several times; an examination of four such lemmas in table 3 shows that for three of them the tagging accuracy e"
N15-1177,S14-1001,0,0.419509,"semantic classes. Here we build on prior work with an inventory of semantic classes (for nouns and verbs) known as supersenses. The 41 supersenses resemble the types used for named entities (PERSON, LOCATION, etc.), but are more general, with semantic categories relevant to common nouns and verbs as well. As a result, their application to Because most supersense tagging studies have worked with data originally annotated for finegrained WordNet senses, then automatically mapped to supersenses, the resulting systems have been tied to the lexical coverage of WordNet. Schneider et al. (2012) and Johannsen et al. (2014) overcame this limitation in part by annotating supersenses directly in text; thus, nouns and verbs not in WordNet were not neglected. However, the issue of which units ought to receive supersenses has not been addressed satisfactorily. We argue that the semantically holistic nature of multiword expressions (MWEs) including idioms, light verb constructions, verb-particle constructions, and many compounds (Baldwin and Kim, 2010) means that they should be considered as units for manual and automatic supersense tagging. Below, we motivate the need for an integrated representation for broad-covera"
N15-1177,W03-1807,0,0.0578435,"ita and Altun (2006) later expanded the task to include all verbs, and applied a supervised sequence modeling framework adapted from NER. Evaluation was against manually sensetagged data that had been automatically converted to the coarser supersenses. Similar taggers have since been built for Italian (Picca et al., 2008) and Chinese (Qiu et al., 2011), both of which have their own WordNets mapped to English WordNet. Although many of the annotated expressions in existing supersense datasets contain multiple words, the relationship between MWEs and supersenses has not received much attention. (Piao et al. (2003, 2005) did investigate MWEs in the context of a lexical tagger with a finer-grained taxonomy of semantic classes.) Consider these examples from online reviews: (1) IT IS NOT A HIGH END STEAK HOUSE (2) The white pages allowed me to get in touch with parents of my high school friends so that I could track people down one by one HIGH END functions as a unit to mean ‘sophisticated, expensive’. (It is not in WordNet, though NLTK (Bird et al., 2009) returns a synset’s lexicographer file. A subtle difference is that a special file called noun.Tops contains each noun supersense’s root synset (e.g., g"
N15-1177,W09-3531,0,0.17588,"Missing"
N15-1177,picca-etal-2008-supersense,0,0.894363,"a method of organizing the WordNet structure. But Ciaramita and Johnson (2003) pioneered the coarse word sense disambiguation task of supersense tagging, noting that the supersense categories provided a natural broadening of the traditional named entity categories to encompass all nouns. Ciaramita and Altun (2006) later expanded the task to include all verbs, and applied a supervised sequence modeling framework adapted from NER. Evaluation was against manually sensetagged data that had been automatically converted to the coarser supersenses. Similar taggers have since been built for Italian (Picca et al., 2008) and Chinese (Qiu et al., 2011), both of which have their own WordNets mapped to English WordNet. Although many of the annotated expressions in existing supersense datasets contain multiple words, the relationship between MWEs and supersenses has not received much attention. (Piao et al. (2003, 2005) did investigate MWEs in the context of a lexical tagger with a finer-grained taxonomy of semantic classes.) Consider these examples from online reviews: (1) IT IS NOT A HIGH END STEAK HOUSE (2) The white pages allowed me to get in touch with parents of my high school friends so that I could track"
N15-1177,W95-0107,0,0.199567,"Missing"
N15-1177,Q14-1016,1,0.119996,"ANGE 274 fix day EMOTION 249 love experience PERCEPTION 143 see review CONSUMPTION 93 have price BODY 82 get. . . done quality CREATION 64 cook amount CONTACT 46 put dog COMPETITION 11 win hair WEATHER 0 — pain all 15 VSSTs 7806 flower portion N/A (see §3.2) oil `a 1191 have discomfort ` 821 anyone process `j 54 fried reason ∗ result COMMUNIC . is short for square COMMUNICATION tree stuff Table 1: Summary of noun and verb supersense categories. Each entry shows the label along with the count and most frequent lexical item in the STREUSLE corpus. enrich the MWE annotations of the CMWE corpus1 (Schneider et al., 2014b), are publicly released under the name STREUSLE.2 This includes new guidelines for verb supersense annotation. Our open-source tagger, implemented in Python, is available from that page as well. 2 Background: Supersense Tags WordNet’s supersense categories are the top-level hypernyms in the taxonomy (sometimes known as semantic fields) which are designed to be broad enough to encompass all nouns and verbs (Miller, 1990; Fellbaum, 1990).3 1 2 http://www.ark.cs.cmu.edu/LexSem/ Supersense-Tagged Repository of English with a Unified Semantics for Lexical Expressions 3 WordNet synset entries were"
N15-1177,N13-1076,1,0.694134,": PERSON O O B N : GROUP I B N : COMMUNICATION I O V: COGNITION O friends so that I could track people down one by one ¯ ¯ N : PERSON O O O O V : SOCIAL N : PERSON I B I ¯ I O B o Figure 3: Tagging for part of the lexical semantic analysis depicted in figure 1. Note that for nominal and verbal MWEs, the supersense label is only attached to the first tag of the expression. Though our focus in this paper is on English, automatic supersense tagging has also been explored in Italian (Picca et al., 2008, 2009; Attardi et al., 2010, 2013; Rossi et al., 2013), Chinese (Qiu et al., 2011), and Arabic (Schneider et al., 2013). 4.2 Model Like Ciaramita and Altun (2006) and Schneider et al. (2014a), we train a first-order structured perceptron (Collins, 2002) with averaging. This is a standard discriminative modeling setup, involving: a linear scoring function over features of input–output pairs; a Viterbi search to choose the highest-scoring valid output tag sequence given the input; and an online learning algorithm that makes M passes through the training data, searching for the best tagging given the current model and updating the parameters (linear feature weights) where the best tagging doesn’t match the gold t"
N15-1177,P12-2050,1,0.898777,"nings is with coarse-grained semantic classes. Here we build on prior work with an inventory of semantic classes (for nouns and verbs) known as supersenses. The 41 supersenses resemble the types used for named entities (PERSON, LOCATION, etc.), but are more general, with semantic categories relevant to common nouns and verbs as well. As a result, their application to Because most supersense tagging studies have worked with data originally annotated for finegrained WordNet senses, then automatically mapped to supersenses, the resulting systems have been tied to the lexical coverage of WordNet. Schneider et al. (2012) and Johannsen et al. (2014) overcame this limitation in part by annotating supersenses directly in text; thus, nouns and verbs not in WordNet were not neglected. However, the issue of which units ought to receive supersenses has not been addressed satisfactorily. We argue that the semantically holistic nature of multiword expressions (MWEs) including idioms, light verb constructions, verb-particle constructions, and many compounds (Baldwin and Kim, 2010) means that they should be considered as units for manual and automatic supersense tagging. Below, we motivate the need for an integrated rep"
N15-1177,schneider-etal-2014-comprehensive,1,0.123901,"ANGE 274 fix day EMOTION 249 love experience PERCEPTION 143 see review CONSUMPTION 93 have price BODY 82 get. . . done quality CREATION 64 cook amount CONTACT 46 put dog COMPETITION 11 win hair WEATHER 0 — pain all 15 VSSTs 7806 flower portion N/A (see §3.2) oil `a 1191 have discomfort ` 821 anyone process `j 54 fried reason ∗ result COMMUNIC . is short for square COMMUNICATION tree stuff Table 1: Summary of noun and verb supersense categories. Each entry shows the label along with the count and most frequent lexical item in the STREUSLE corpus. enrich the MWE annotations of the CMWE corpus1 (Schneider et al., 2014b), are publicly released under the name STREUSLE.2 This includes new guidelines for verb supersense annotation. Our open-source tagger, implemented in Python, is available from that page as well. 2 Background: Supersense Tags WordNet’s supersense categories are the top-level hypernyms in the taxonomy (sometimes known as semantic fields) which are designed to be broad enough to encompass all nouns and verbs (Miller, 1990; Fellbaum, 1990).3 1 2 http://www.ark.cs.cmu.edu/LexSem/ Supersense-Tagged Repository of English with a Unified Semantics for Lexical Expressions 3 WordNet synset entries were"
N15-1177,tsvetkov-etal-2014-augmenting-english,1,0.721284,"014b) multiword expression annotations. Schneider et al. (2012) offered a methodology for noun supersense annotation in Arabic Wikipedia, and predicted that it would port well to other languages and domains. Our experience with English web reviews has borne this out. We generally adhered to the same supersense annotation process (for nouns); the most important difference was that the data had already been annotated for MWEs, and supersense labels apply to any strong5 MWEs as a whole. 4 Future supersense annotation schemes for additional parts of speech could be assimilated into our framework. Tsvetkov et al. (2014) take a step in this direction for adjectives. 5 The CMWE corpus distinguishes strong and weak MWEs— essentially, the former are strongly entrenched and likely noncompositional, whereas weak MWEs are merely statistically collocated. See Schneider et al. (2014b) for details. Because they are deemed semantically compositional, weak MWEs do not receive a supersense as a whole. 1539 The same annotators had already done the MWE annotation; whenever they encountered an apparent mistake from an earlier stage (usually an oversight), they were encouraged to correct it. Our annotation interface supports"
N15-1184,N09-1003,0,0.562483,"Missing"
N15-1184,N09-1014,0,0.00466202,"nalysis (Chang et al., 2013). The approach we propose is conceptually similar to previous work that uses graph structures to propagate information among semantic concepts (Zhu, 2005; Culp and Michailidis, 2008). Graph-based belief propagation has also been used to induce POS tags (Subramanya et al., 2010; Das and Petrov, 2011) and semantic frame associations (Das and Smith, 2011). In those efforts, labels for unknown words were inferred using a method similar to ours. Broadly, graph-based semi-supervised learning (Zhu, 2005; Talukdar and Pereira, 2010) has been applied to machine translation (Alexandrescu and Kirchhoff, 2009), unsupervised semantic role induction (Lang and Lapata, 2011), semantic document modeling (Schuhmacher and Ponzetto, 2014), language generation (Krahmer et al., 2003) and sentiment analysis (Goldberg and Zhu, 2006). 9 Conclusion We have proposed a simple and effective method named retrofitting to improve word vectors using word relation knowledge found in semantic lexicons. Retrofitting is used as a post-processing step to improve vector quality and is more modular than other approaches that use semantic information while training. It can be applied to vectors obtained from any word vector tr"
N15-1184,P98-1013,0,0.739801,"l., 1990) and using internal representations from neural network models of word sequences (Collobert and Weston, 2008). Because of their value as lexical semantic representations, there has been much research on improving the quality of vectors. Semantic lexicons, which provide type-level information about the semantics of words, typically by identifying synonymy, hypernymy, hyponymy, and paraphrase relations should be a valuable resource for improving the quality of word vectors that are trained solely on unlabeled corpora. Examples of such resources include WordNet (Miller, 1995), FrameNet (Baker et al., 1998) and the Paraphrase Database (Ganitkevitch et al., 2013). Recent work has shown that by either changing the objective of the word vector training algorithm in neural language models (Yu and Dredze, 2014; Xu et al., 2014; Bian et al., 2014; Fried and Duh, 2014) or by relation-specific augmentation of the cooccurence matrix in spectral word vector models to incorporate semantic knowledge (Yih et al., 2012; Chang et al., 2013), the quality of word vectors can be improved. However, these methods are limited to particular methods for constructing vectors. The contribution of this paper is a graph-b"
N15-1184,P12-1015,0,0.139533,"cts of the representations along with an extrinsic sentiment analysis task. Word Similarity. We evaluate our word representations on a variety of different benchmarks that have been widely used to measure word similarity. The first one is the WS-353 dataset (Finkelstein et al., 2001) containing 353 pairs of English words that have been assigned similarity ratings by humans. The second benchmark is the RG-65 (Rubenstein and Goodenough, 1965) dataset that contain 65 pairs of nouns. Since the commonly used word similarity datasets contain a small number of word pairs we also use the MEN dataset (Bruni et al., 2012) of 3,000 word pairs sampled from words that occur at least 700 times in a large web corpus. We calculate cosine similarity between the vectors of two words forming a test item, and report Spearman’s rank correlation coefficient (Myers and Well, 1995) between the rankings produced by our model against the human rankings. Syntactic Relations (SYN-REL). Mikolov et al. (2013b) present a syntactic relation dataset composed of analogous word pairs. It contains pairs of tuples of word relations that follow a common syntactic relation. For example, given walking and walked, the words are differently"
N15-1184,D13-1167,0,0.0169875,"luable resource for improving the quality of word vectors that are trained solely on unlabeled corpora. Examples of such resources include WordNet (Miller, 1995), FrameNet (Baker et al., 1998) and the Paraphrase Database (Ganitkevitch et al., 2013). Recent work has shown that by either changing the objective of the word vector training algorithm in neural language models (Yu and Dredze, 2014; Xu et al., 2014; Bian et al., 2014; Fried and Duh, 2014) or by relation-specific augmentation of the cooccurence matrix in spectral word vector models to incorporate semantic knowledge (Yih et al., 2012; Chang et al., 2013), the quality of word vectors can be improved. However, these methods are limited to particular methods for constructing vectors. The contribution of this paper is a graph-based learning technique for using lexical relational resources to obtain higher quality semantic vectors, which we call “retrofitting.” In contrast to previous work, retrofitting is applied as a post-processing step by running belief propagation on a graph constructed from lexicon-derived relational information to update word vectors (§2). This allows retrofitting to be used on pre-trained word vectors obtained using any ve"
N15-1184,P11-1061,0,0.0334295,"se to the observed value qˆi and close to its neighbors qj , ∀j such that (i, j) ∈ E, the objective to be minimized becomes:   n X X αi kqi − qˆi k2 + Ψ(Q) = βij kqi − qj k2  i=1 (i,j)∈E where α and β values control the relative strengths of associations (more details in §6.1). In this case, we first train the word vectors independent of the information in the semantic lexicons and then retrofit them. Ψ is convex in Q and its solution can be found by solving a system of linear equations. To do so, we use an efficient iterative updating method (Bengio et al., 2006; Subramanya et al., 2010; Das and Petrov, 2011; Das and Smith, 2011). The vectors in Q are initialized to be equal ˆ We take the first derivative of Ψ to the vectors in Q. with respect to one qi vector, and by equating it to zero arrive at the following online update: P j:(i,j)∈E βij qj + αi qˆi qi = P (1) j:(i,j)∈E βij + αi In practice, running this procedure for 10 iterations converges to changes in Euclidean distance of adjacent vertices of less than 10−2 . The retrofitting approach described above is modular; it can be applied to word vector representations obtained from any model as the updates in Eq. 1 are agnostic to the original v"
N15-1184,P11-1144,1,0.318338,"ue qˆi and close to its neighbors qj , ∀j such that (i, j) ∈ E, the objective to be minimized becomes:   n X X αi kqi − qˆi k2 + Ψ(Q) = βij kqi − qj k2  i=1 (i,j)∈E where α and β values control the relative strengths of associations (more details in §6.1). In this case, we first train the word vectors independent of the information in the semantic lexicons and then retrofit them. Ψ is convex in Q and its solution can be found by solving a system of linear equations. To do so, we use an efficient iterative updating method (Bengio et al., 2006; Subramanya et al., 2010; Das and Petrov, 2011; Das and Smith, 2011). The vectors in Q are initialized to be equal ˆ We take the first derivative of Ψ to the vectors in Q. with respect to one qi vector, and by equating it to zero arrive at the following online update: P j:(i,j)∈E βij qj + αi qˆi qi = P (1) j:(i,j)∈E βij + αi In practice, running this procedure for 10 iterations converges to changes in Euclidean distance of adjacent vertices of less than 10−2 . The retrofitting approach described above is modular; it can be applied to word vector representations obtained from any model as the updates in Eq. 1 are agnostic to the original vector training model o"
N15-1184,E14-1049,1,0.393831,"e use. In this model, each word’s Huffman code is used as an input to a log-linear classifier with a continuous projection layer and words within a given context window are predicted. The available vectors are trained on 100 billion words of Google news dataset and are of length 300.2 Global Context Vectors (GC). These vectors are learned using a recursive neural network that incorporates both local and global (document-level) context features (Huang et al., 2012). These vectors were trained on the first 1 billion words of English Wikipedia and are of length 50.3 Multilingual Vectors (Multi). Faruqui and Dyer (2014) learned vectors by first performing SVD on text in different languages, then applying canonical correlation analysis (CCA) on pairs of vectors for words that align in parallel corpora. The monolingual vectors were trained on WMT-2011 news corpus for English, French, German and Spanish. We use the Enligsh word vectors projected in the common English–German space. The monolingual English WMT corpus had 360 million words and the trained vectors are of length 512.4 4 Semantic Lexicons We use three different semantic lexicons to evaluate their utility in improving the word vectors. We include both"
N15-1184,N13-1092,0,0.314995,"Missing"
N15-1184,W06-3808,0,0.0111527,"elief propagation has also been used to induce POS tags (Subramanya et al., 2010; Das and Petrov, 2011) and semantic frame associations (Das and Smith, 2011). In those efforts, labels for unknown words were inferred using a method similar to ours. Broadly, graph-based semi-supervised learning (Zhu, 2005; Talukdar and Pereira, 2010) has been applied to machine translation (Alexandrescu and Kirchhoff, 2009), unsupervised semantic role induction (Lang and Lapata, 2011), semantic document modeling (Schuhmacher and Ponzetto, 2014), language generation (Krahmer et al., 2003) and sentiment analysis (Goldberg and Zhu, 2006). 9 Conclusion We have proposed a simple and effective method named retrofitting to improve word vectors using word relation knowledge found in semantic lexicons. Retrofitting is used as a post-processing step to improve vector quality and is more modular than other approaches that use semantic information while training. It can be applied to vectors obtained from any word vector training method. Our experiments explored the method’s performance across tasks, semantic lexicons, and languages and showed that it outperforms existing alternatives. The retrofitting tool is available at: https:// g"
N15-1184,D14-1012,0,0.00727559,"cal semantic evaluation tasks in several languages, we obtain substantial improvements starting with a variety of word vector models. Our refinement method outperforms prior techniques for incorporating semantic lexicons into word vector training algorithms. 1 Introduction Data-driven learning of word vectors that capture lexico-semantic information is a technique of central importance in NLP. These word vectors can in turn be used for identifying semantically related word pairs (Turney, 2006; Agirre et al., 2009) or as features in downstream text processing applications (Turian et al., 2010; Guo et al., 2014). A variety of approaches for constructing vector space embeddings of vocabularies are in use, notably including taking low rank approximations of cooccurrence statistics (Deerwester et al., 1990) and using internal representations from neural network models of word sequences (Collobert and Weston, 2008). Because of their value as lexical semantic representations, there has been much research on improving the quality of vectors. Semantic lexicons, which provide type-level information about the semantics of words, typically by identifying synonymy, hypernymy, hyponymy, and paraphrase relations"
N15-1184,I05-1067,0,0.0116987,"Spanish. We used the Universal WordNet (de Melo and Weikum, 2009), an automatically constructed multilingual lexical knowledge base based on WordNet.12 It contains words connected via different lexical relations to other words both within and across languages. We construct separate graphs for different languages (i.e., only linking words to other words in the same language) and apply retrofitting to each. Since not many word similarity evaluation benchmarks are available for languages other than English, we tested our baseline and improved vectors on one benchmark per language. We used RG-65 (Gurevych, 2005), RG-65 (Joubarne and Inkpen, 2011) and MC-30 (Hassan and Mihalcea, 2009) for German, French and Spanish, respectively.13 We trained SG vectors for each language of length 300 on a corpus of 1 billion tokens, each extracted from Wikipedia, and evaluate them on word similarity on the benchmarks before and after retrofitting. Table 5 shows that we obtain high improvements which strongly indicates that our method generalizes across these languages. 7 Further Analysis Retrofitting vs. vector length. With more dimensions, word vectors might be able to capture higher orders of semantic information a"
N15-1184,D09-1124,0,0.0382359,"09), an automatically constructed multilingual lexical knowledge base based on WordNet.12 It contains words connected via different lexical relations to other words both within and across languages. We construct separate graphs for different languages (i.e., only linking words to other words in the same language) and apply retrofitting to each. Since not many word similarity evaluation benchmarks are available for languages other than English, we tested our baseline and improved vectors on one benchmark per language. We used RG-65 (Gurevych, 2005), RG-65 (Joubarne and Inkpen, 2011) and MC-30 (Hassan and Mihalcea, 2009) for German, French and Spanish, respectively.13 We trained SG vectors for each language of length 300 on a corpus of 1 billion tokens, each extracted from Wikipedia, and evaluate them on word similarity on the benchmarks before and after retrofitting. Table 5 shows that we obtain high improvements which strongly indicates that our method generalizes across these languages. 7 Further Analysis Retrofitting vs. vector length. With more dimensions, word vectors might be able to capture higher orders of semantic information and retrofitting might be less helpful. We train SG vec12 https://github.c"
N15-1184,P12-1092,0,0.106061,"btained from different lexicons. and are of length 300.1 Skip-Gram Vectors (SG). The word2vec tool (Mikolov et al., 2013a) is fast and currently in wide use. In this model, each word’s Huffman code is used as an input to a log-linear classifier with a continuous projection layer and words within a given context window are predicted. The available vectors are trained on 100 billion words of Google news dataset and are of length 300.2 Global Context Vectors (GC). These vectors are learned using a recursive neural network that incorporates both local and global (document-level) context features (Huang et al., 2012). These vectors were trained on the first 1 billion words of English Wikipedia and are of length 50.3 Multilingual Vectors (Multi). Faruqui and Dyer (2014) learned vectors by first performing SVD on text in different languages, then applying canonical correlation analysis (CCA) on pairs of vectors for words that align in parallel corpora. The monolingual vectors were trained on WMT-2011 news corpus for English, French, German and Spanish. We use the Enligsh word vectors projected in the common English–German space. The monolingual English WMT corpus had 360 million words and the trained vector"
N15-1184,J03-1003,0,0.0119063,"Missing"
N15-1184,D11-1122,0,0.00432874,"ilar to previous work that uses graph structures to propagate information among semantic concepts (Zhu, 2005; Culp and Michailidis, 2008). Graph-based belief propagation has also been used to induce POS tags (Subramanya et al., 2010; Das and Petrov, 2011) and semantic frame associations (Das and Smith, 2011). In those efforts, labels for unknown words were inferred using a method similar to ours. Broadly, graph-based semi-supervised learning (Zhu, 2005; Talukdar and Pereira, 2010) has been applied to machine translation (Alexandrescu and Kirchhoff, 2009), unsupervised semantic role induction (Lang and Lapata, 2011), semantic document modeling (Schuhmacher and Ponzetto, 2014), language generation (Krahmer et al., 2003) and sentiment analysis (Goldberg and Zhu, 2006). 9 Conclusion We have proposed a simple and effective method named retrofitting to improve word vectors using word relation knowledge found in semantic lexicons. Retrofitting is used as a post-processing step to improve vector quality and is more modular than other approaches that use semantic information while training. It can be applied to vectors obtained from any word vector training method. Our experiments explored the method’s performan"
N15-1184,W14-1618,0,0.0675244,"rankings. Syntactic Relations (SYN-REL). Mikolov et al. (2013b) present a syntactic relation dataset composed of analogous word pairs. It contains pairs of tuples of word relations that follow a common syntactic relation. For example, given walking and walked, the words are differently inflected forms of the same verb. There are nine different kinds of relations and overall there are 10,675 syntactic pairs of word tuples. The task is to find a word d that best fits the following relationship: “a is to b as c is to d,” given a, b, and c. We use the vector offset method (Mikolov et al., 2013a; Levy and Goldberg, 2014), computing q = qa − qb + qc and returning the vector from Q which has the highest cosine similarity to q. Synonym Selection (TOEFL). The TOEFL synonym selection task is to select the semantically closest word to a target from a list of four candidates (Landauer and Dumais, 1997). The dataset contains 80 such questions. An example is “rug → {sofa, ottoman, carpet, hallway}”, with carpet being the most synonym-like candidate to the target. Sentiment Analysis (SA). Socher et al. (2013) created a treebank containing sentences annotated with fine-grained sentiment labels on phrases and sentences f"
N15-1184,N13-1090,0,0.687964,"tors. Global vectors for word representations (Pennington et al., 2014) are trained on aggregated global word-word co-occurrence statistics from a corpus, and the resulting representations show interesting linear substructures of the word vector space. These vectors were trained on 6 billion words from Wikipedia and English Gigaword 1608 Lexicon PPDB WordNetsyn WordNetall FrameNet Words 102,902 148,730 148,730 10,822 Edges 374,555 304,856 934,705 417,456 Table 1: Approximate size of the graphs obtained from different lexicons. and are of length 300.1 Skip-Gram Vectors (SG). The word2vec tool (Mikolov et al., 2013a) is fast and currently in wide use. In this model, each word’s Huffman code is used as an input to a log-linear classifier with a continuous projection layer and words within a given context window are predicted. The available vectors are trained on 100 billion words of Google news dataset and are of length 300.2 Global Context Vectors (GC). These vectors are learned using a recursive neural network that incorporates both local and global (document-level) context features (Huang et al., 2012). These vectors were trained on the first 1 billion words of English Wikipedia and are of length 50.3"
N15-1184,D14-1162,0,0.120168,"Missing"
N15-1184,D13-1170,0,0.00650579,"nship: “a is to b as c is to d,” given a, b, and c. We use the vector offset method (Mikolov et al., 2013a; Levy and Goldberg, 2014), computing q = qa − qb + qc and returning the vector from Q which has the highest cosine similarity to q. Synonym Selection (TOEFL). The TOEFL synonym selection task is to select the semantically closest word to a target from a list of four candidates (Landauer and Dumais, 1997). The dataset contains 80 such questions. An example is “rug → {sofa, ottoman, carpet, hallway}”, with carpet being the most synonym-like candidate to the target. Sentiment Analysis (SA). Socher et al. (2013) created a treebank containing sentences annotated with fine-grained sentiment labels on phrases and sentences from movie review excerpts. The coarsegrained treebank of positive and negative classes has been split into training, development, and test datasets containing 6,920, 872, and 1,821 sentences, respectively. We train an `2 -regularized logistic regression classifier on the average of the word vectors of a given sentence to predict the coarse-grained sentiment tag at the sentence level, and report the testset accuracy of the classifier. 6 Experiments We first show experiments measuring"
N15-1184,D10-1017,0,0.0133974,"red word vector to be close to the observed value qˆi and close to its neighbors qj , ∀j such that (i, j) ∈ E, the objective to be minimized becomes:   n X X αi kqi − qˆi k2 + Ψ(Q) = βij kqi − qj k2  i=1 (i,j)∈E where α and β values control the relative strengths of associations (more details in §6.1). In this case, we first train the word vectors independent of the information in the semantic lexicons and then retrofit them. Ψ is convex in Q and its solution can be found by solving a system of linear equations. To do so, we use an efficient iterative updating method (Bengio et al., 2006; Subramanya et al., 2010; Das and Petrov, 2011; Das and Smith, 2011). The vectors in Q are initialized to be equal ˆ We take the first derivative of Ψ to the vectors in Q. with respect to one qi vector, and by equating it to zero arrive at the following online update: P j:(i,j)∈E βij qj + αi qˆi qi = P (1) j:(i,j)∈E βij + αi In practice, running this procedure for 10 iterations converges to changes in Euclidean distance of adjacent vertices of less than 10−2 . The retrofitting approach described above is modular; it can be applied to word vector representations obtained from any model as the updates in Eq. 1 are agno"
N15-1184,P10-1149,0,0.00454798,"y induction (Yih et al., 2012) and multi-relational latent semantic analysis (Chang et al., 2013). The approach we propose is conceptually similar to previous work that uses graph structures to propagate information among semantic concepts (Zhu, 2005; Culp and Michailidis, 2008). Graph-based belief propagation has also been used to induce POS tags (Subramanya et al., 2010; Das and Petrov, 2011) and semantic frame associations (Das and Smith, 2011). In those efforts, labels for unknown words were inferred using a method similar to ours. Broadly, graph-based semi-supervised learning (Zhu, 2005; Talukdar and Pereira, 2010) has been applied to machine translation (Alexandrescu and Kirchhoff, 2009), unsupervised semantic role induction (Lang and Lapata, 2011), semantic document modeling (Schuhmacher and Ponzetto, 2014), language generation (Krahmer et al., 2003) and sentiment analysis (Goldberg and Zhu, 2006). 9 Conclusion We have proposed a simple and effective method named retrofitting to improve word vectors using word relation knowledge found in semantic lexicons. Retrofitting is used as a post-processing step to improve vector quality and is more modular than other approaches that use semantic information wh"
N15-1184,P10-1040,0,0.0887589,"tery of standard lexical semantic evaluation tasks in several languages, we obtain substantial improvements starting with a variety of word vector models. Our refinement method outperforms prior techniques for incorporating semantic lexicons into word vector training algorithms. 1 Introduction Data-driven learning of word vectors that capture lexico-semantic information is a technique of central importance in NLP. These word vectors can in turn be used for identifying semantically related word pairs (Turney, 2006; Agirre et al., 2009) or as features in downstream text processing applications (Turian et al., 2010; Guo et al., 2014). A variety of approaches for constructing vector space embeddings of vocabularies are in use, notably including taking low rank approximations of cooccurrence statistics (Deerwester et al., 1990) and using internal representations from neural network models of word sequences (Collobert and Weston, 2008). Because of their value as lexical semantic representations, there has been much research on improving the quality of vectors. Semantic lexicons, which provide type-level information about the semantics of words, typically by identifying synonymy, hypernymy, hyponymy, and pa"
N15-1184,J06-3003,0,0.0129744,", and it makes no assumptions about how the input vectors were constructed. Evaluated on a battery of standard lexical semantic evaluation tasks in several languages, we obtain substantial improvements starting with a variety of word vector models. Our refinement method outperforms prior techniques for incorporating semantic lexicons into word vector training algorithms. 1 Introduction Data-driven learning of word vectors that capture lexico-semantic information is a technique of central importance in NLP. These word vectors can in turn be used for identifying semantically related word pairs (Turney, 2006; Agirre et al., 2009) or as features in downstream text processing applications (Turian et al., 2010; Guo et al., 2014). A variety of approaches for constructing vector space embeddings of vocabularies are in use, notably including taking low rank approximations of cooccurrence statistics (Deerwester et al., 1990) and using internal representations from neural network models of word sequences (Collobert and Weston, 2008). Because of their value as lexical semantic representations, there has been much research on improving the quality of vectors. Semantic lexicons, which provide type-level inf"
N15-1184,D12-1111,0,0.0389385,"ons should be a valuable resource for improving the quality of word vectors that are trained solely on unlabeled corpora. Examples of such resources include WordNet (Miller, 1995), FrameNet (Baker et al., 1998) and the Paraphrase Database (Ganitkevitch et al., 2013). Recent work has shown that by either changing the objective of the word vector training algorithm in neural language models (Yu and Dredze, 2014; Xu et al., 2014; Bian et al., 2014; Fried and Duh, 2014) or by relation-specific augmentation of the cooccurence matrix in spectral word vector models to incorporate semantic knowledge (Yih et al., 2012; Chang et al., 2013), the quality of word vectors can be improved. However, these methods are limited to particular methods for constructing vectors. The contribution of this paper is a graph-based learning technique for using lexical relational resources to obtain higher quality semantic vectors, which we call “retrofitting.” In contrast to previous work, retrofitting is applied as a post-processing step by running belief propagation on a graph constructed from lexicon-derived relational information to update word vectors (§2). This allows retrofitting to be used on pre-trained word vectors"
N15-1184,P14-2089,0,0.636299,"search on improving the quality of vectors. Semantic lexicons, which provide type-level information about the semantics of words, typically by identifying synonymy, hypernymy, hyponymy, and paraphrase relations should be a valuable resource for improving the quality of word vectors that are trained solely on unlabeled corpora. Examples of such resources include WordNet (Miller, 1995), FrameNet (Baker et al., 1998) and the Paraphrase Database (Ganitkevitch et al., 2013). Recent work has shown that by either changing the objective of the word vector training algorithm in neural language models (Yu and Dredze, 2014; Xu et al., 2014; Bian et al., 2014; Fried and Duh, 2014) or by relation-specific augmentation of the cooccurence matrix in spectral word vector models to incorporate semantic knowledge (Yih et al., 2012; Chang et al., 2013), the quality of word vectors can be improved. However, these methods are limited to particular methods for constructing vectors. The contribution of this paper is a graph-based learning technique for using lexical relational resources to obtain higher quality semantic vectors, which we call “retrofitting.” In contrast to previous work, retrofitting is applied as a post-pr"
N15-1184,J90-1003,0,\N,Missing
N15-1184,J07-2002,0,\N,Missing
N15-1184,C98-1013,0,\N,Missing
N15-1184,D13-1141,0,\N,Missing
N16-1024,P99-1070,0,0.0566848,"imilar constraints have been proposed to deal with the analogous problem in bottom-up shift-reduce parsers (Sagae and Lavie, 2005). 201 Constraints on generator transitions. The generation algorithm also requires slightly modified constraints. These are: • The REDUCE operation can only be applied if the top of the stack is not an open nonterminal symbol and n ≥ 1. To designate the set of valid generator transitions, we write AG (T, S, n). This transition set generates trees using nearly the same structure building actions and stack configurations as the “top-down PDA” construction proposed by Abney et al. (1999), albeit without the restriction that the trees be in Chomsky normal form. 3.3 Transition Sequences from Trees Any parse tree can be converted to a sequence of transitions via a depth-first, left-to-right traversal of a parse tree. Since there is a unique depth-first, leftro-right traversal of a tree, there is exactly one transition sequence of each tree. For a tree y and a sequence of symbols x, we write a(x, y) to indicate the corresponding sequence of generation transitions, and b(x, y) to indicate the parser transitions. 3.4 Runtime Analysis A detailed analysis of the algorithmic propertie"
N16-1024,N15-1083,0,0.0397053,"Missing"
N16-1024,E03-1005,0,0.0656796,"Missing"
N16-1024,J92-4003,0,0.647152,"the size of AG (S, T, n), word generation is broken into two parts. First, the decision to generate is made (by predicting GEN as an action), and then choosing the word, conditional on the current parser state. To further reduce the computational complexity of modeling the generation of a word, we use a class-factored softmax (Baltescu pand Blunsom, 2015; Goodman, 2001). By using |Σ| classes for a vocabulary p of size |Σ|, this prediction step runs in time O( |Σ|) rather than the O(|Σ|) of the full-vocabulary softmax. To obtain clusters, we use the greedy agglomerative clustering algorithm of Brown et al. (1992). 4.3 Discriminative Parsing Model 5 Inference via Importance Sampling Our generative model p(x, y) defines a joint distribution on trees (y) and sequences of words (x). To evaluate this as a language model, it is necessary to compute the marginal probability p(x) = P 0 y 0 ∈Y(x) p(x, y ). And, to evaluate the model as a parser, we need to be able to find the MAP parse tree, i.e., the tree y ∈ Y(x) that maximizes p(x, y). However, because of the unbounded dependencies across the sequence of parsing actions in our model, exactly solving either of these inference problems is intractable. To obta"
N16-1024,W15-2108,0,0.16968,"c process reminiscent of probabilistic context-free grammar generation, but decisions are parameterized using RNNs that condition on the entire syntactic derivation history, greatly relaxing context-free independence assumptions. The foundation of this work is a top-down variant of transition-based parsing (§3). We give two variants of the algorithm, one for parsing (given an observed sentence, transform it into a tree), and one for generation. While several transition-based neural models of syntactic generation exist (Henderson, 2003, 2004; Emami and Jelinek, 2005; Titov and Henderson, 2007; Buys and Blunsom, 2015b), these have relied on structure building operations based on parsing actions in shift-reduce and leftcorner parsers which operate in a largely bottomup fashion. While this construction is appealing because inference is relatively straightforward, it limits the use of top-down grammar information, which is helpful for generation (Roark, 2001).1 RNNGs maintain the algorithmic convenience of transitionbased parsing but incorporate top-down (i.e., rootto-terminal) syntactic information (§4). The top-down transition set that RNNGs are based on lends itself to discriminative modeling as well, whe"
N16-1024,P15-2142,0,0.211515,"c process reminiscent of probabilistic context-free grammar generation, but decisions are parameterized using RNNs that condition on the entire syntactic derivation history, greatly relaxing context-free independence assumptions. The foundation of this work is a top-down variant of transition-based parsing (§3). We give two variants of the algorithm, one for parsing (given an observed sentence, transform it into a tree), and one for generation. While several transition-based neural models of syntactic generation exist (Henderson, 2003, 2004; Emami and Jelinek, 2005; Titov and Henderson, 2007; Buys and Blunsom, 2015b), these have relied on structure building operations based on parsing actions in shift-reduce and leftcorner parsers which operate in a largely bottomup fashion. While this construction is appealing because inference is relatively straightforward, it limits the use of top-down grammar information, which is helpful for generation (Roark, 2001).1 RNNGs maintain the algorithmic convenience of transitionbased parsing but incorporate top-down (i.e., rootto-terminal) syntactic information (§4). The top-down transition set that RNNGs are based on lends itself to discriminative modeling as well, whe"
N16-1024,A00-2018,0,0.925699,"le 4: Language model perplexity results. Parsing results on PTB §23 (D=discriminative, G=generative, S=semisupervised). Model Henderson (2004) Socher et al. (2013a) Zhu et al. (2013) Vinyals et al. (2015) – WSJ only Petrov and Klein (2007) Bod (2003) Shindo et al. (2012) – single Shindo et al. (2012) – ensemble Zhu et al. (2013) McClosky et al. (2006) Vinyals et al. (2015) – single Vinyals et al. (2015) – ensemble Discriminative, q(y |x) Generative, pˆ(y |x) type D D D D G G G G S S S S D G Table 3: Parsing results on CTB 5.1. Model Zhu et al. (2013) Wang et al. (2015) Huang and Harper (2009) Charniak (2000) Bikel (2004) Petrov and Klein (2007) Zhu et al. (2013) Wang and Xue (2014) Wang et al. (2015) Discriminative, q(y |x) Generative, pˆ(y |x) type D D D G G G S S S D G F1 89.4 90.4 90.4 90.5 90.1 90.7 91.1 92.4 91.3 92.1 92.5 92.8 89.8 92.4 F1 82.6 83.2 84.2 80.8 80.6 83.3 85.6 86.3 86.6 80.7 82.7 Discussion It is clear from our experiments that the proposed generative model is quite effective both as a parser and as a language model. This is the result of (i) relaxing conventional independence assumptions (e.g., context-freeness) and (ii) inferring continuous representations of symbols alongsi"
N16-1024,D10-1066,0,0.578391,"dling. However, leaving them out reduces the number of transitions by O(n) and also reduces the number of action types, both of which reduce the runtime. Furthermore, standard parsing evaluation scores do not depend on preterminal prediction accuracy. operation and later COMPLETEs them after consuming terminal symbols one at a time using SCAN (Earley, 1970). It is likewise closely related to the “linearized” parse trees proposed by Vinyals et al. (2015) and to the top-down, left-to-right decompositions of trees used in previous generative parsing and language modeling work (Roark, 2001, 2004; Charniak, 2010). A further connection is to LL(∗ ) parsing which uses an unbounded lookahead (compactly represented by a DFA) to distinguish between parse alternatives in a top-down parser (Parr and Fisher, 2011); however, our parser uses an RNN encoding of the lookahead rather than a DFA. unprocessed words, rather there is an output buffer (T ), and (ii) instead of a SHIFT operation there are GEN (x) operations which generate terminal symbol x ∈ Σ and add it to the top of the stack and the output buffer. At each timestep an action is stochastically selected according to a conditional distribution that depen"
N16-1024,P15-1033,1,0.788281,"contents (Cho et al., 2014). Since the output buffer and history of actions are only appended to and only contain symbols from a finite alphabet, it is straightforward to apply a standard RNN encoding architecture. The stack (S) is more complicated for two reasons. First, the elements of the stack are more complicated objects than symbols from a discrete alphabet: open nonterminals, terminals, and full trees, are all present on the stack. Second, it is manipulated using both push and pop operations. To efficiently obtain representations of S under push and pop operations, we use stack LSTMs (Dyer et al., 2015). 4.1 Syntactic Composition Function When a REDUCE operation is executed, the parser pops a sequence of completed subtrees and/or tokens (together with their vector embeddings) from the stack and makes them children of the most recent open nonterminal on the stack, “completing” the constituent. To compute an embedding of this new subtree, we use a composition function based on bidirectional LSTMs, which is illustrated in Fig. 6. The first vector read by the LSTM in both the forward and reverse directions is an embedding of the label on the constituent being constructed (in the figure, NP). Thi"
N16-1024,P81-1022,0,0.662754,"lated to the operations used in Earley’s algorithm which likewise introduces nonterminals symbols with its PREDICT 2 Preterminal symbols are, from the parsing algorithm’s point of view, just another kind of nonterminal symbol that requires no special handling. However, leaving them out reduces the number of transitions by O(n) and also reduces the number of action types, both of which reduce the runtime. Furthermore, standard parsing evaluation scores do not depend on preterminal prediction accuracy. operation and later COMPLETEs them after consuming terminal symbols one at a time using SCAN (Earley, 1970). It is likewise closely related to the “linearized” parse trees proposed by Vinyals et al. (2015) and to the top-down, left-to-right decompositions of trees used in previous generative parsing and language modeling work (Roark, 2001, 2004; Charniak, 2010). A further connection is to LL(∗ ) parsing which uses an unbounded lookahead (compactly represented by a DFA) to distinguish between parse alternatives in a top-down parser (Parr and Fisher, 2011); however, our parser uses an RNN encoding of the lookahead rather than a DFA. unprocessed words, rather there is an output buffer (T ), and (ii) i"
N16-1024,P06-1121,0,0.0200588,"ther than the importance sampling method based on a discriminative parser, §5) to produce a left-to-right marginalization algorithm that runs in expected linear time. Thus, they could be used in applications that require language models. A second possibility is to replace the sequential generation architectures found in many neural network transduction problems that produce sentences conditioned on some input. Previous work in machine translation has showed that conditional syntactic models can function quite well without the computationally expensive marginalization process at decoding time (Galley et al., 2006; Gimpel and Smith, 2014). A third consideration regarding how RNNGs, human sentence processing takes place in a left-toright, incremental order. While an RNNG is not a processing model (it is a grammar), the fact that it is 207 left-to-right opens up several possibilities for developing new sentence processing models based on an explicit grammars, similar to the processing model of Charniak (2010). Finally, although we considered only the supervised learning scenario, RNNGs are joint models that could be trained without trees, for example, using expectation maximization. 10 Conclusion We intr"
N16-1024,J14-2005,1,0.778993,"nce sampling method based on a discriminative parser, §5) to produce a left-to-right marginalization algorithm that runs in expected linear time. Thus, they could be used in applications that require language models. A second possibility is to replace the sequential generation architectures found in many neural network transduction problems that produce sentences conditioned on some input. Previous work in machine translation has showed that conditional syntactic models can function quite well without the computationally expensive marginalization process at decoding time (Galley et al., 2006; Gimpel and Smith, 2014). A third consideration regarding how RNNGs, human sentence processing takes place in a left-toright, incremental order. While an RNNG is not a processing model (it is a grammar), the fact that it is 207 left-to-right opens up several possibilities for developing new sentence processing models based on an explicit grammars, similar to the processing model of Charniak (2010). Finally, although we considered only the supervised learning scenario, RNNGs are joint models that could be trained without trees, for example, using expectation maximization. 10 Conclusion We introduced recurrent neural n"
N16-1024,N03-1014,0,0.528102,"lationships among words and phrases. RNNGs operate via a recursive syntactic process reminiscent of probabilistic context-free grammar generation, but decisions are parameterized using RNNs that condition on the entire syntactic derivation history, greatly relaxing context-free independence assumptions. The foundation of this work is a top-down variant of transition-based parsing (§3). We give two variants of the algorithm, one for parsing (given an observed sentence, transform it into a tree), and one for generation. While several transition-based neural models of syntactic generation exist (Henderson, 2003, 2004; Emami and Jelinek, 2005; Titov and Henderson, 2007; Buys and Blunsom, 2015b), these have relied on structure building operations based on parsing actions in shift-reduce and leftcorner parsers which operate in a largely bottomup fashion. While this construction is appealing because inference is relatively straightforward, it limits the use of top-down grammar information, which is helpful for generation (Roark, 2001).1 RNNGs maintain the algorithmic convenience of transitionbased parsing but incorporate top-down (i.e., rootto-terminal) syntactic information (§4). The top-down transitio"
N16-1024,P04-1013,0,0.146212,"latively straightforward, it limits the use of top-down grammar information, which is helpful for generation (Roark, 2001).1 RNNGs maintain the algorithmic convenience of transitionbased parsing but incorporate top-down (i.e., rootto-terminal) syntactic information (§4). The top-down transition set that RNNGs are based on lends itself to discriminative modeling as well, where sequences of transitions are modeled conditional on the full input sentence along with the incrementally constructed syntactic structures. Similar to previously published discriminative bottomup transition-based parsers (Henderson, 2004; Sagae and Lavie, 2005; Zhang and Clark, 2011, inter alia), greedy prediction with our model yields a lineartime deterministic parser (provided an upper bound on the number of actions taken between processing subsequent terminal symbols is imposed); however, our algorithm generates arbitrary tree structures directly, without the binarization required by shift-reduce parsers. The discriminative model also lets us use ancestor sampling to obtain samples of parse trees for sentences, and this is used to solve 1 The left-corner parsers used by Henderson (2003, 2004) incorporate limited top-down i"
N16-1024,D09-1087,0,0.056023,"Missing"
N16-1024,J91-3004,0,0.723725,"ta. 8 symbol), inverted, and exponentiated to yield the perplexity. Results are summarized in Table 4. 7 Model IKN 5-gram LSTM LM RNNG Related Work Our language model combines work from two modeling traditions: (i) recurrent neural network language models and (ii) syntactic language modeling. Recurrent neural network language models use RNNs to compute representations of an unbounded history of words in a left-to-right language model (Zaremba et al., 2015; Mikolov et al., 2010; Elman, 1990). Syntactic language models jointly generate a syntactic structure and a sequence of words (Baker, 1979; Jelinek and Lafferty, 1991). There is an extensive literature here, but one strand of work has emphasized a bottom-up generation of the tree, using variants of shift-reduce parser actions to define the probability space (Chelba and Jelinek, 2000; Emami and Jelinek, 2005). The neural-network–based model of Henderson (2004) is particularly similar to ours in using an unbounded history in a neural network architecture to parameterize generative parsing based on a left-corner model. Dependency-only language models have also been explored (Titov and Henderson, 2007; Buys and Blunsom, 2015a,b). Modeling generation top-down as"
N16-1024,P01-1042,0,0.112989,"generative model. We present a simple importance sampling algorithm which uses samples from the discriminative parser to solve inference problems in the generative model (§5). Experiments show that RNNGs are effective for both language modeling and parsing (§6). Our generative model obtains (i) the best-known parsing results using a single supervised generative model and (ii) better perplexities in language modeling than state-of-the-art sequential LSTM language models. Surprisingly—although in line with previous parsing results showing the effectiveness of generative models (Henderson, 2004; Johnson, 2001)— parsing with the generative model obtains significantly better results than parsing with the discriminative model. 2 RNN Grammars Formally, an RNNG is a triple (N, Σ, Θ) consisting of a finite set of nonterminal symbols (N ), a finite set of terminal symbols (Σ) such that N ∩ Σ = ∅, and a collection of neural network parameters Θ. It does not explicitly define rules since these are implicitly characterized by Θ. The algorithm that the grammar uses to generate trees and strings in the language is characterized in terms of a transition-based algorithm, which is outlined in the next section. In"
N16-1024,Q16-1032,0,0.0218427,"top-down as a rooted branching process that recursively rewrites nonterminals has been explored by Charniak (2000) and Roark (2001). Of particular note is the work of Charniak (2010), which uses random forests and hand-engineered features over the entire syntactic derivation history to make decisions over the next action to take. The neural networks we use to model sentences are structured according to the syntax of the sentence being generated. Syntactically structured neural architectures have been explored in a number of applications, including discriminative parsing (Socher et al., 2013a; Kiperwasser and Goldberg, 2016), sentiment analysis (Tai et al., 2015; Socher et al., 2013b), and sentence representation (Socher et al., 2011; Bowman et al., 2006). However, these models have been, without exception, discriminative; this is the first work to use syntactically structured neural models to generate language. Earlier work has demonstrated that sequential RNNs have the capacity to recognize contextfree (and beyond) languages (Sun et al., 1998; Siegelmann and Sontag, 1995). In contrast, our work may be understood as a way of incorporating a context-free inductive bias into the model structure. 9 Outlook RNNGs ca"
N16-1024,N06-1020,0,0.0266107,"Missing"
N16-1024,D13-1032,0,0.02466,"Missing"
N16-1024,N07-1051,0,0.0217429,"Missing"
N16-1024,J01-2004,0,0.40637,"for parsing (given an observed sentence, transform it into a tree), and one for generation. While several transition-based neural models of syntactic generation exist (Henderson, 2003, 2004; Emami and Jelinek, 2005; Titov and Henderson, 2007; Buys and Blunsom, 2015b), these have relied on structure building operations based on parsing actions in shift-reduce and leftcorner parsers which operate in a largely bottomup fashion. While this construction is appealing because inference is relatively straightforward, it limits the use of top-down grammar information, which is helpful for generation (Roark, 2001).1 RNNGs maintain the algorithmic convenience of transitionbased parsing but incorporate top-down (i.e., rootto-terminal) syntactic information (§4). The top-down transition set that RNNGs are based on lends itself to discriminative modeling as well, where sequences of transitions are modeled conditional on the full input sentence along with the incrementally constructed syntactic structures. Similar to previously published discriminative bottomup transition-based parsers (Henderson, 2004; Sagae and Lavie, 2005; Zhang and Clark, 2011, inter alia), greedy prediction with our model yields a line"
N16-1024,W05-1513,0,0.163106,"forward, it limits the use of top-down grammar information, which is helpful for generation (Roark, 2001).1 RNNGs maintain the algorithmic convenience of transitionbased parsing but incorporate top-down (i.e., rootto-terminal) syntactic information (§4). The top-down transition set that RNNGs are based on lends itself to discriminative modeling as well, where sequences of transitions are modeled conditional on the full input sentence along with the incrementally constructed syntactic structures. Similar to previously published discriminative bottomup transition-based parsers (Henderson, 2004; Sagae and Lavie, 2005; Zhang and Clark, 2011, inter alia), greedy prediction with our model yields a lineartime deterministic parser (provided an upper bound on the number of actions taken between processing subsequent terminal symbols is imposed); however, our algorithm generates arbitrary tree structures directly, without the binarization required by shift-reduce parsers. The discriminative model also lets us use ancestor sampling to obtain samples of parse trees for sentences, and this is used to solve 1 The left-corner parsers used by Henderson (2003, 2004) incorporate limited top-down information, but a compl"
N16-1024,P12-1046,0,0.149805,"Missing"
N16-1024,D13-1170,0,0.0582441,"what sort of head it should be looking for as it processes the child node embeddings. The final state of the forward and reverse LSTMs are concatenated, passed through an affine transformation and a tanh nonlinearity to become the subtree embedding.4 Because each of the child node embeddings (u, v, w in Fig. 6) is computed similarly (if it corresponds to an 4 We found the many previously proposed syntactic composition functions inadequate for our purposes. First, we must contend with an unbounded number of children, and many previously proposed functions are limited to binary branching nodes (Socher et al., 2013b; Dyer et al., 2015). Second, those that could deal with n-ary nodes made poor use of nonterminal information (Tai et al., 2015), which is crucial for our task. RE GEDUC NTN E ( NT NP) (VP ) … St z }| (S p(at ) { NP z ut (VP a&lt;t cat Tt }| hungry { The The hungry cat Figure 5: Neural architecture for defining a distribution over at given representations of the stack (St ), output buffer (Tt ) and history of actions (a&lt;t ). Details of the composition architecture of the NP, the action history LSTM, and the other elements of the stack are not shown. This architecture corresponds to the generator"
N16-1024,P15-1150,0,0.365082,"TMs are concatenated, passed through an affine transformation and a tanh nonlinearity to become the subtree embedding.4 Because each of the child node embeddings (u, v, w in Fig. 6) is computed similarly (if it corresponds to an 4 We found the many previously proposed syntactic composition functions inadequate for our purposes. First, we must contend with an unbounded number of children, and many previously proposed functions are limited to binary branching nodes (Socher et al., 2013b; Dyer et al., 2015). Second, those that could deal with n-ary nodes made poor use of nonterminal information (Tai et al., 2015), which is crucial for our task. RE GEDUC NTN E ( NT NP) (VP ) … St z }| (S p(at ) { NP z ut (VP a&lt;t cat Tt }| hungry { The The hungry cat Figure 5: Neural architecture for defining a distribution over at given representations of the stack (St ), output buffer (Tt ) and history of actions (a&lt;t ). Details of the composition architecture of the NP, the action history LSTM, and the other elements of the stack are not shown. This architecture corresponds to the generator state at line 7 of Figure 4. 4.4 x x A discriminative parsing model can be obtained by replacing the embedding of Tt at each tim"
N16-1024,W07-2218,0,0.0387922,"te via a recursive syntactic process reminiscent of probabilistic context-free grammar generation, but decisions are parameterized using RNNs that condition on the entire syntactic derivation history, greatly relaxing context-free independence assumptions. The foundation of this work is a top-down variant of transition-based parsing (§3). We give two variants of the algorithm, one for parsing (given an observed sentence, transform it into a tree), and one for generation. While several transition-based neural models of syntactic generation exist (Henderson, 2003, 2004; Emami and Jelinek, 2005; Titov and Henderson, 2007; Buys and Blunsom, 2015b), these have relied on structure building operations based on parsing actions in shift-reduce and leftcorner parsers which operate in a largely bottomup fashion. While this construction is appealing because inference is relatively straightforward, it limits the use of top-down grammar information, which is helpful for generation (Roark, 2001).1 RNNGs maintain the algorithmic convenience of transitionbased parsing but incorporate top-down (i.e., rootto-terminal) syntactic information (§4). The top-down transition set that RNNGs are based on lends itself to discriminati"
N16-1024,N03-1033,0,0.190016,"en, 2011). Our importance sampling algorithm uses a conditional proposal distribution q(y |x) with the following properties: (i) p(x, y) > 0 =⇒ q(y | x) > 0; (ii) samples y ∼ q(y |x) can be obtained efficiently; and (iii) the conditional probabilities q(y |x) of these samples are known. While many such distributions are available, the discrim5 Training The parameters in the model are learned to maximize the likelihood of a corpus of trees. 204 For the discriminative parser, the POS tags are processed similarly as in (Dyer et al., 2015); they are predicted for English with the Stanford Tagger (Toutanova et al., 2003) and Chinese with Marmot (Mueller et al., 2013). inatively trained variant of our parser (§4.4) fulfills these requirements: sequences of actions can be sampled using a simple ancestral sampling approach, and, since parse trees and action sequences exist in a one-to-one relationship, the product of the action probabilities is the conditional probability of the parse tree under q. We therefore use our discriminative parser as our proposal distribution. Importance sampling uses importance weights, which we define as w(x, y) = p(x, y)/q(y |x), to compute this estimate. Under this definition, we c"
N16-1024,P15-1110,0,0.219171,"Missing"
N16-1024,P14-1069,0,0.0783975,"Missing"
N16-1024,D15-1199,0,0.0152728,"Missing"
N16-1024,J11-1005,0,0.213134,"use of top-down grammar information, which is helpful for generation (Roark, 2001).1 RNNGs maintain the algorithmic convenience of transitionbased parsing but incorporate top-down (i.e., rootto-terminal) syntactic information (§4). The top-down transition set that RNNGs are based on lends itself to discriminative modeling as well, where sequences of transitions are modeled conditional on the full input sentence along with the incrementally constructed syntactic structures. Similar to previously published discriminative bottomup transition-based parsers (Henderson, 2004; Sagae and Lavie, 2005; Zhang and Clark, 2011, inter alia), greedy prediction with our model yields a lineartime deterministic parser (provided an upper bound on the number of actions taken between processing subsequent terminal symbols is imposed); however, our algorithm generates arbitrary tree structures directly, without the binarization required by shift-reduce parsers. The discriminative model also lets us use ancestor sampling to obtain samples of parse trees for sentences, and this is used to solve 1 The left-corner parsers used by Henderson (2003, 2004) incorporate limited top-down information, but a complete path from the root"
N16-1024,P13-1043,0,0.278571,"her assumptions. Assuming our fixed constraint on maximum depth, it is linear. 3.5 Comparison to Other Models Our generation algorithm algorithm differs from previous stack-based parsing/generation algorithms in two ways. First, it constructs rooted tree structures top down (rather than bottom up), and second, the transition operators are capable of directly generating arbitrary tree structures rather than, e.g., assuming binarized trees, as is the case in much prior work that has used transition-based algorithms to produce phrase-structure trees (Sagae and Lavie, 2005; Zhang and Clark, 2011; Zhu et al., 2013). 4 Generative Model RNNGs use the generator transition set just presented to define a joint distribution on syntax trees (y) and words (x). This distribution is defined as a sequence model over generator transitions that is parameterized using a continuous space embedding of the algorithm state at each time step (ut ); i.e., |a(x,y)| p(x, y) = Y p(at |a&lt;t ) t=1 |a(x,y)| = Y t=1 exp r> at ut + bat , > a0 ∈AG (Tt ,St ,nt ) exp ra0 ut + ba0 P and where action-specific embeddings ra and bias vector b are parameters in Θ. The representation of the algorithm state at time t, ut , is computed by com"
N16-1087,W13-2322,0,0.598809,"new method is trained statistically from AMRannotated English and consists of two major steps: (i) generating an appropriate spanning tree for the AMR, and (ii) applying tree-tostring transducers to generate English. The method relies on discriminative learning and an argument realization model to overcome data sparsity. Initial tests on held-out data show good promise despite the complexity of the task. The system is available open-source as part of JAMR at: http://github.com/jflanigan/jamr 1 Introduction We consider natural language generation from the Abstract Meaning Representation (AMR; Banarescu et al., 2013). AMR encodes the meaning of a sentence as a rooted, directed, acyclic graph, where concepts are nodes, and edges are relationships among the concepts. Because AMR models propositional meaning1 while abstracting away from surface syntactic realizations, and is designed with human annotation in mind, it suggests a separation of (i) engineering the application-specific propositions that need to be 1 In essence, AMR handles semantic roles, entity types, within-sentence coreference, discourse connectives, modality, negation, and some other phenomena. communicated about from (ii) general-purpose re"
N16-1087,W11-2832,0,0.013904,"y be because the synthetic rule model already contains much of the information in the basic and abstract rules. Removing the handwritten rules has a slightly larger effect, demonstrating the value of handwritten rules in this statistical system. 10 Related Work There is a large body of work for statistical and nonstatistical NLG from a variety of input representations. Statistical NLG systems have been built for input representations such as HPSG (Nakanishi et al., 2005), LFG (Cahill and Van Genabith, 2006; Hogan et al., 2007), and CCG (White et al., 2007), as well as surface and deep syntax (Belz et al., 2011). The deep syntax representations in Bohnet et al. (2010) and Belz et al. (2011) share similarities with AMR: the representations are graphs with re-entrancies, and have an concept inventory from PropBank (Palmer et al., 2005). The Nitrogen and Halogen systems (Langkilde and Knight, 1998; Langkilde, 2000) used an input representation that was a precursor to the modern version of AMR, which was also called AMR, although it was not the same representation as Banarescu et al. (2013). Techniques from statistical machine translation have been applied to the problem of NLG (Wong 738 Conclusion We ha"
N16-1087,C10-1012,0,0.0119508,"much of the information in the basic and abstract rules. Removing the handwritten rules has a slightly larger effect, demonstrating the value of handwritten rules in this statistical system. 10 Related Work There is a large body of work for statistical and nonstatistical NLG from a variety of input representations. Statistical NLG systems have been built for input representations such as HPSG (Nakanishi et al., 2005), LFG (Cahill and Van Genabith, 2006; Hogan et al., 2007), and CCG (White et al., 2007), as well as surface and deep syntax (Belz et al., 2011). The deep syntax representations in Bohnet et al. (2010) and Belz et al. (2011) share similarities with AMR: the representations are graphs with re-entrancies, and have an concept inventory from PropBank (Palmer et al., 2005). The Nitrogen and Halogen systems (Langkilde and Knight, 1998; Langkilde, 2000) used an input representation that was a precursor to the modern version of AMR, which was also called AMR, although it was not the same representation as Banarescu et al. (2013). Techniques from statistical machine translation have been applied to the problem of NLG (Wong 738 Conclusion We have presented a two-stage method for natural language gene"
N16-1087,P06-1130,0,0.0471353,"Missing"
N16-1087,W02-1001,0,0.145668,"ac and c. For i > c+1, li contains all words between ai−1 and ai , and for i = c + 1, li contains all words between c and ai . The tables for lex , left lex , and right lex are populated using the segmented basic rules. For each basic rule extracted from the training corpus and segmented according to the previous paragraph, f → c is added to lex , and Aki → hli , ri i is added to left lex for i ≤ c and right lex for i > c. The permutation ki is known during extraction in Eq. 8. The parameters ψ are trained using AdaGrad (Duchi et al., 2011) with the perceptron loss function (Rosenblatt, 1957; Collins, 2002) for 10 iterations over the basic rules. The features g are listed in Table 2. 7 Tokens 210,000 29,000 30,000 5,000 Table 3: Train/dev./test/MT09 split. Table 2: Synthetic rule model features. POS is the most common part-of-speech tag sequence for c, “dist” is the string “dist”, and side is “L” if i < c, “R” otherwise. + denotes string concatenation. l1 X1 r1 . . . c . . . lm Xm rm Sentences 10,000 1,400 1,400 204 Abstract Rules Like the synthetic rules, the abstract rules RA (G) generalize the basic rules. However, abstract rules 737 are much simpler generalizations which use partof-speech (P"
N16-1087,P10-4002,1,0.897274,"lly or verbally, and more—transforming an AMR graph into an English sentence is a nontrivial problem. To our knowledge, our system is the first for generating English from AMR. The approach is a statistical natural language generation (NLG) system, trained discriminatively using sentences in the AMR bank (Banarescu et al., 2013). It first transforms the graph into a tree, then decodes into a string using a weighted tree-to-string transducer and a language model (Graehl and Knight, 2004). The decoder bears a strong similarity to state-of-the-art machine translation systems (Koehn et al., 2007; Dyer et al., 2010), but with a rule extraction approach tailored to the NLG problem. 2 Overview Generation of English from AMR graphs is accomplished as follows: the input graph is converted to a tree, which is input into the weighted intersection of a tree-to-string transducer (§4) with a language model. The output English sentence is the (approximately) highest-scoring sentence according to a feature-rich discriminatively trained linear model. After discussing notation (§3), we describe our approach in §4. The transducer’s rules are extracted from the limited AMR corpus and learned general731 Proceedings of N"
N16-1087,P14-1134,1,0.394856,"ed by defining b(·) and e(·) recursively, bottom up: b(i) = min(aj , e(i) = max(a0j , Inducing Basic Rules min b(j)) max e(j)) j∈children(i) j∈children(i) The basic rules, denoted RB , are extracted from the training AMR data using an algorithm similar to extracting tree transucers from tree-string aligned parallel corpora (Galley et al., 2004). Informally, the rules are extracted from a sentence w = hw1 , . . . , wn i with AMR graph G as follows: Also define functions ˜b and e˜, from fragment indices to integers, as: 1. The AMR graph and the sentence are aligned; we use the JAMR aligner from Flanigan et al. (2014), which aligns non-overlapping subgraphs of the graph to spans of words. The subgraphs that JAMR aligns are called fragments. In JAMR’s aligner, all fragments are trees. For fragment i, let Ci = children(root(i)) − nodes(i), which is the children of the fragment’s root concept that are not included in the fragment. Let fi be the TI representation for fragment i.5 If Ci is empty, then the rule extracted for fragment i is: 2. G is replaced by its spanning tree by deleting relations that use a variable in the AMR annotation. 3. In the spanning tree, for each node i, we keep track of the word indi"
N16-1087,N04-1035,0,0.0173377,"e child nodes of a node. We consider a node aligned if it belongs to an aligned fragment. Let the span of an aligned node i be denoted by endpoints ai and a0i ; for unaligned nodes, ai = ∞ and a0i = −∞ (depicted with superscripts in Fig. 2). The node alignments are propagated by defining b(·) and e(·) recursively, bottom up: b(i) = min(aj , e(i) = max(a0j , Inducing Basic Rules min b(j)) max e(j)) j∈children(i) j∈children(i) The basic rules, denoted RB , are extracted from the training AMR data using an algorithm similar to extracting tree transucers from tree-string aligned parallel corpora (Galley et al., 2004). Informally, the rules are extracted from a sentence w = hw1 , . . . , wn i with AMR graph G as follows: Also define functions ˜b and e˜, from fragment indices to integers, as: 1. The AMR graph and the sentence are aligned; we use the JAMR aligner from Flanigan et al. (2014), which aligns non-overlapping subgraphs of the graph to spans of words. The subgraphs that JAMR aligns are called fragments. In JAMR’s aligner, all fragments are trees. For fragment i, let Ci = children(root(i)) − nodes(i), which is the children of the fragment’s root concept that are not included in the fragment. Let fi"
N16-1087,J99-4004,0,0.0965541,"rule extraction from an AMRannotated sentence. 736 where the max is over c ∈ 0 . . . m, k1 , . . . , km is any permutation of 1, . . . , m, and Ri ∈ left lex (Ai ) for i < c and Ri ∈ right lex (Ai ) for i > c. ∗ is used to denote the concept position.  is the empty string. The best solution to Eq. 10 is found exactly by brute force search over concept position c ∈ [0, m + 1] and the permutation k1 , . . . , km . With fixed concept position and permutation, each Ri for the arg max is found independently. To obtain the exact K-best solutions, we use dynamic programming with a K-best semiring (Goodman, 1999) to keep track of the K best sequences for each concept position and permutation, and take the best K sequences over all values of c and k· . The synthetic rule model’s parameters are estimated using basic rules extracted from the training data. Basic rules are put into the form of Eq. 9 by Feature name POS + Ai + “dist” POS + Ai + side POS + Ai + side + “dist” POS + Ai + Ri + side c + Ai + “dist” c + Ai + side c + Ai + side + “dist” c + POS + Ai + side + “dist” Value |c − i| 1.0 |c − i| 1.0 |c − i| 1.0 |c − i| |c − i| Split Train Dev. Test MT09 segmenting the RHS into the form (11) by choosin"
N16-1087,N04-1014,0,0.181127,"nd leaves underspecified many important details—including tense, number, definiteness, whether a concept should be referred to nominally or verbally, and more—transforming an AMR graph into an English sentence is a nontrivial problem. To our knowledge, our system is the first for generating English from AMR. The approach is a statistical natural language generation (NLG) system, trained discriminatively using sentences in the AMR bank (Banarescu et al., 2013). It first transforms the graph into a tree, then decodes into a string using a weighted tree-to-string transducer and a language model (Graehl and Knight, 2004). The decoder bears a strong similarity to state-of-the-art machine translation systems (Koehn et al., 2007; Dyer et al., 2010), but with a rule extraction approach tailored to the NLG problem. 2 Overview Generation of English from AMR graphs is accomplished as follows: the input graph is converted to a tree, which is input into the weighted intersection of a tree-to-string transducer (§4) with a language model. The output English sentence is the (approximately) highest-scoring sentence according to a feature-rich discriminatively trained linear model. After discussing notation (§3), we descri"
N16-1087,P13-2121,0,0.0197701,"ules is returned. 8 Handwritten Rules We have handwritten rules for dates, conjunctions, multiple sentences, and the concept have-org-role91. We also create pass-through rules for concepts by removing sense tags and quotes (for string literals). 9 Experiments We evaluate on the AMR Annotation Release version 1.0 (LDC2014T12) dataset. We follow the recommended train/dev./test splits, except that we remove MT09 data (204 sentences) from the training data and use it as another test set. Statistics for this dataset and splits are given in Table 3. We use a 5gram language model trained with KenLM (Heafield et al., 2013) on Gigaword (LDC2011T07), and use 100-best synthetic rules. We evaluate with the Bleu scoring metric (Papineni et al., 2002) (Table 4). We report single refRules Full Full − basic Full − synthetic Full − abstract Full − handwritten Test 22.1 22.1 9.1 22.0 21.9 MT09 21.2 20.9 7.8 21.2 20.5 Table 4: Uncased Bleu scores with various types of rules removed from the full system. and Mooney, 2006), and many grammar-based approaches can be formulated as weighted tree-tostring transducers. Jones et al. (2012) developed technology for generation and translation with synchronous hyperedge replacement ("
N16-1087,D07-1028,0,0.0567956,"Missing"
N16-1087,2006.amta-papers.8,0,0.0387566,"ual formatting of the TI representation in Fig. 1 is: (X want-01 (ARG0 (X boy)) (ARG1 (X ride-01 (ARG0 (X bicycle (mod (X red))))))) To ease notation, we use the function sort[] to lexicographically sort edge labels in a TI representation. Using this function, an equivalent way of representing the TI representation in Eq. 1, if the Li are unsorted, is: (X C sort[(L1 T1 ) . . . (Lm Tm )]) The TI representation is converted into a word sequence using a tree-to-string transducer. The tree transducer formalism we use is one-state extended linear, non-deleting tree-to-string (1-xRLNs) transducers (Huang et al., 2006; Graehl and Knight, 2004).3 Definition 1. (From Huang et al., 2006.) A 1xRLNs transducer is a tuple (N, Σ, W, R) where N 2 If there are duplicate child edge labels, then the conversion process is ambiguous and any of the conversions can be used. The ordering ambiguity will be handled later in the treetransducer rules. 3 Multiple states would be useful for modeling dependencies in the output, but we do not use them here. 732 boy ride-01 ARG0 X bicycle mod X red The boy wants to ride the red bicycle . Figure 1: The generation pipeline. An AMR graph (top), with a deleted re-entrancy (dashed), is"
N16-1087,C12-1083,0,0.0452424,"ataset and splits are given in Table 3. We use a 5gram language model trained with KenLM (Heafield et al., 2013) on Gigaword (LDC2011T07), and use 100-best synthetic rules. We evaluate with the Bleu scoring metric (Papineni et al., 2002) (Table 4). We report single refRules Full Full − basic Full − synthetic Full − abstract Full − handwritten Test 22.1 22.1 9.1 22.0 21.9 MT09 21.2 20.9 7.8 21.2 20.5 Table 4: Uncased Bleu scores with various types of rules removed from the full system. and Mooney, 2006), and many grammar-based approaches can be formulated as weighted tree-tostring transducers. Jones et al. (2012) developed technology for generation and translation with synchronous hyperedge replacement (SHRG) grammars applied to the GeoQuery corpus (Wong and Mooney, 2006), which in principle could be applied to AMR generation. 11 erence Bleu for the LCD2014T12 test set, and fourreference Bleu for the MT09 set. We report ablation experiments for different sources of rules. When ablating handwritten rules, we do not ablate passthrough rules. The full system achieves 22.1 Bleu on the test set, and 21.2 on MT09. Removing the synthetic rules drops the results to 9.1 Bleu on test and 7.8 on MT09. Removing t"
N16-1087,P07-2045,1,0.010277,"e referred to nominally or verbally, and more—transforming an AMR graph into an English sentence is a nontrivial problem. To our knowledge, our system is the first for generating English from AMR. The approach is a statistical natural language generation (NLG) system, trained discriminatively using sentences in the AMR bank (Banarescu et al., 2013). It first transforms the graph into a tree, then decodes into a string using a weighted tree-to-string transducer and a language model (Graehl and Knight, 2004). The decoder bears a strong similarity to state-of-the-art machine translation systems (Koehn et al., 2007; Dyer et al., 2010), but with a rule extraction approach tailored to the NLG problem. 2 Overview Generation of English from AMR graphs is accomplished as follows: the input graph is converted to a tree, which is input into the weighted intersection of a tree-to-string transducer (§4) with a language model. The output English sentence is the (approximately) highest-scoring sentence according to a feature-rich discriminatively trained linear model. After discussing notation (§3), we describe our approach in §4. The transducer’s rules are extracted from the limited AMR corpus and learned general"
N16-1087,P98-1116,0,0.174995,"ody of work for statistical and nonstatistical NLG from a variety of input representations. Statistical NLG systems have been built for input representations such as HPSG (Nakanishi et al., 2005), LFG (Cahill and Van Genabith, 2006; Hogan et al., 2007), and CCG (White et al., 2007), as well as surface and deep syntax (Belz et al., 2011). The deep syntax representations in Bohnet et al. (2010) and Belz et al. (2011) share similarities with AMR: the representations are graphs with re-entrancies, and have an concept inventory from PropBank (Palmer et al., 2005). The Nitrogen and Halogen systems (Langkilde and Knight, 1998; Langkilde, 2000) used an input representation that was a precursor to the modern version of AMR, which was also called AMR, although it was not the same representation as Banarescu et al. (2013). Techniques from statistical machine translation have been applied to the problem of NLG (Wong 738 Conclusion We have presented a two-stage method for natural language generation from AMR, setting a baseline for future work. We have also demonstrated the importance of modeling argument realization for good performance. Our feature-based, tree-transducer approach can be easily extended with rules and"
N16-1087,A00-2023,0,0.165467,"and nonstatistical NLG from a variety of input representations. Statistical NLG systems have been built for input representations such as HPSG (Nakanishi et al., 2005), LFG (Cahill and Van Genabith, 2006; Hogan et al., 2007), and CCG (White et al., 2007), as well as surface and deep syntax (Belz et al., 2011). The deep syntax representations in Bohnet et al. (2010) and Belz et al. (2011) share similarities with AMR: the representations are graphs with re-entrancies, and have an concept inventory from PropBank (Palmer et al., 2005). The Nitrogen and Halogen systems (Langkilde and Knight, 1998; Langkilde, 2000) used an input representation that was a precursor to the modern version of AMR, which was also called AMR, although it was not the same representation as Banarescu et al. (2013). Techniques from statistical machine translation have been applied to the problem of NLG (Wong 738 Conclusion We have presented a two-stage method for natural language generation from AMR, setting a baseline for future work. We have also demonstrated the importance of modeling argument realization for good performance. Our feature-based, tree-transducer approach can be easily extended with rules and features from othe"
N16-1087,W05-1510,0,0.0374539,"nthetic rules drops the results to 9.1 Bleu on test and 7.8 on MT09. Removing the basic and abstract rules has little impact on the results. This may be because the synthetic rule model already contains much of the information in the basic and abstract rules. Removing the handwritten rules has a slightly larger effect, demonstrating the value of handwritten rules in this statistical system. 10 Related Work There is a large body of work for statistical and nonstatistical NLG from a variety of input representations. Statistical NLG systems have been built for input representations such as HPSG (Nakanishi et al., 2005), LFG (Cahill and Van Genabith, 2006; Hogan et al., 2007), and CCG (White et al., 2007), as well as surface and deep syntax (Belz et al., 2011). The deep syntax representations in Bohnet et al. (2010) and Belz et al. (2011) share similarities with AMR: the representations are graphs with re-entrancies, and have an concept inventory from PropBank (Palmer et al., 2005). The Nitrogen and Halogen systems (Langkilde and Knight, 1998; Langkilde, 2000) used an input representation that was a precursor to the modern version of AMR, which was also called AMR, although it was not the same representation"
N16-1087,P03-1021,0,0.0208502,"transduction of G0 : ! e=E 4 arg max score(d; θ) d∈D(G0 ,T ) (6) If fi is just a single concept with no children, then m = 0 and fi = (X C). Eq. 6 is solved approximately using the cdec decoder for machine translation (Dyer et al., 2010). The score of the transduction is a linear function (with coefficients θ) of a vector of features including the output sequence’s language model logprobability and features associated with the rules in the derivation (denoted f ; Table 1): X score(d; θ) = θLM log(pLM (E(d))) + θ > f (r) r∈d The feature weights are trained on a development dataset using MERT (Och, 2003). In the next four sections, we describe the rules extracted and generalized from the training corpus. 5 1, . . . , F . Let nodes : {1, . . . , F } → 2{1,...,N } and root : {1, . . . , F } → {1, . . . , N } be functions that return the nodes in a fragment and the root of a fragment, respectively, and let children : {1, . . . , N } → 2{1,...,N } return the child nodes of a node. We consider a node aligned if it belongs to an aligned fragment. Let the span of an aligned node i be denoted by endpoints ai and a0i ; for unaligned nodes, ai = ∞ and a0i = −∞ (depicted with superscripts in Fig. 2). Th"
N16-1087,J05-1004,0,0.00451229,"s statistical system. 10 Related Work There is a large body of work for statistical and nonstatistical NLG from a variety of input representations. Statistical NLG systems have been built for input representations such as HPSG (Nakanishi et al., 2005), LFG (Cahill and Van Genabith, 2006; Hogan et al., 2007), and CCG (White et al., 2007), as well as surface and deep syntax (Belz et al., 2011). The deep syntax representations in Bohnet et al. (2010) and Belz et al. (2011) share similarities with AMR: the representations are graphs with re-entrancies, and have an concept inventory from PropBank (Palmer et al., 2005). The Nitrogen and Halogen systems (Langkilde and Knight, 1998; Langkilde, 2000) used an input representation that was a precursor to the modern version of AMR, which was also called AMR, although it was not the same representation as Banarescu et al. (2013). Techniques from statistical machine translation have been applied to the problem of NLG (Wong 738 Conclusion We have presented a two-stage method for natural language generation from AMR, setting a baseline for future work. We have also demonstrated the importance of modeling argument realization for good performance. Our feature-based, t"
N16-1087,P02-1040,0,0.126115,"have-org-role91. We also create pass-through rules for concepts by removing sense tags and quotes (for string literals). 9 Experiments We evaluate on the AMR Annotation Release version 1.0 (LDC2014T12) dataset. We follow the recommended train/dev./test splits, except that we remove MT09 data (204 sentences) from the training data and use it as another test set. Statistics for this dataset and splits are given in Table 3. We use a 5gram language model trained with KenLM (Heafield et al., 2013) on Gigaword (LDC2011T07), and use 100-best synthetic rules. We evaluate with the Bleu scoring metric (Papineni et al., 2002) (Table 4). We report single refRules Full Full − basic Full − synthetic Full − abstract Full − handwritten Test 22.1 22.1 9.1 22.0 21.9 MT09 21.2 20.9 7.8 21.2 20.5 Table 4: Uncased Bleu scores with various types of rules removed from the full system. and Mooney, 2006), and many grammar-based approaches can be formulated as weighted tree-tostring transducers. Jones et al. (2012) developed technology for generation and translation with synchronous hyperedge replacement (SHRG) grammars applied to the GeoQuery corpus (Wong and Mooney, 2006), which in principle could be applied to AMR generation."
N16-1087,2007.mtsummit-ucnlg.4,0,0.0605298,"abstract rules has little impact on the results. This may be because the synthetic rule model already contains much of the information in the basic and abstract rules. Removing the handwritten rules has a slightly larger effect, demonstrating the value of handwritten rules in this statistical system. 10 Related Work There is a large body of work for statistical and nonstatistical NLG from a variety of input representations. Statistical NLG systems have been built for input representations such as HPSG (Nakanishi et al., 2005), LFG (Cahill and Van Genabith, 2006; Hogan et al., 2007), and CCG (White et al., 2007), as well as surface and deep syntax (Belz et al., 2011). The deep syntax representations in Bohnet et al. (2010) and Belz et al. (2011) share similarities with AMR: the representations are graphs with re-entrancies, and have an concept inventory from PropBank (Palmer et al., 2005). The Nitrogen and Halogen systems (Langkilde and Knight, 1998; Langkilde, 2000) used an input representation that was a precursor to the modern version of AMR, which was also called AMR, although it was not the same representation as Banarescu et al. (2013). Techniques from statistical machine translation have been"
N16-1087,N06-1056,0,0.0309,"thetic rules. We evaluate with the Bleu scoring metric (Papineni et al., 2002) (Table 4). We report single refRules Full Full − basic Full − synthetic Full − abstract Full − handwritten Test 22.1 22.1 9.1 22.0 21.9 MT09 21.2 20.9 7.8 21.2 20.5 Table 4: Uncased Bleu scores with various types of rules removed from the full system. and Mooney, 2006), and many grammar-based approaches can be formulated as weighted tree-tostring transducers. Jones et al. (2012) developed technology for generation and translation with synchronous hyperedge replacement (SHRG) grammars applied to the GeoQuery corpus (Wong and Mooney, 2006), which in principle could be applied to AMR generation. 11 erence Bleu for the LCD2014T12 test set, and fourreference Bleu for the MT09 set. We report ablation experiments for different sources of rules. When ablating handwritten rules, we do not ablate passthrough rules. The full system achieves 22.1 Bleu on the test set, and 21.2 on MT09. Removing the synthetic rules drops the results to 9.1 Bleu on test and 7.8 on MT09. Removing the basic and abstract rules has little impact on the results. This may be because the synthetic rule model already contains much of the information in the basic a"
N16-1087,C98-1112,0,\N,Missing
N18-1088,D15-1041,1,0.919347,"overed by UD guidelines.1 Our annotation includes tokenization, part-of-speech (POS) tags, and (labeled) Universal Dependencies. We characterize the disagreements among our annotators and find that consistent annotation is still challenging to deliver even with the extended guidelines. Based on these annotations, we nonetheless designed a pipeline to parse raw tweets into Universal Dependencies. Our pipeline includes: a bidirectional LSTM (bi-LSTM) tokenizer, a word cluster–enhanced POS tagger (following Owoputi et al., 2013), and a stack LSTM parser with character-based word representations (Ballesteros et al., 2015), which we refer to as our “baseline” parser. To overcome the noise in our annotated We study the problem of analyzing tweets with Universal Dependencies (UD; Nivre et al., 2016). We extend the UD guidelines to cover special constructions in tweets that affect tokenization, part-ofspeech tagging, and labeled dependencies. Using the extended guidelines, we create a new tweet treebank for English (T WEEBANK V 2) that is four times larger than the (unlabeled) T WEEBANK V 1 introduced by Kong et al. (2014). We characterize the disagreements between our annotators and show that it is challenging to"
N18-1088,D16-1211,1,0.907114,"Missing"
N18-1088,J92-4003,0,0.519758,"Missing"
N18-1088,de-marneffe-etal-2014-universal,0,0.0901755,"Missing"
N18-1088,R13-1026,0,0.167981,"Missing"
N18-1088,D16-1180,1,0.877777,"e updated version of Twokenizer from Owoputi et al. (2013). 13 http://nlp.stanford.edu/data/glove.twitter. 27B.zip 971 System Kong et al. (2014) Dozat et al. (2017) Ballesteros et al. (2015) Ensemble (20) Distillation (α = 1.0) Distillation (α = 0.9) Distillation w/ exploration UAS 81.4 81.8 80.2 83.4 81.8 82.0 82.1 LAS 76.9 77.7 75.7 79.4 77.6 77.8 77.9 Kt/s 0.3 1.7 2.3 0.2 2.3 2.3 2.3 tems above (Table 6). Distillation. The shortcoming of the 20-parser ensemble is, of course, that it requires twenty times the runtime of a single greedy parser, making it the slowest system in our comparison. Kuncoro et al. (2016) proposed the distillation of 20 greedy transition-based parser into a single graphbased parser; they transformed the votes of the ensemble into a structured loss function. However, as Kuncoro et al. pointed out, it is not straightforward to use a structured loss in a transition-based parsing algorithm. Because fast runtime is so important for NLP on social media, we introduce a new way to distill our greedy ensemble into a single transition-based parser (the first such attempt, to our knowledge). Our approach applies techniques from Hinton et al. (2015) and Kim and Rush (2016) to parsing. Not"
N18-1088,K17-3002,0,0.103467,"ination of UD_English-EWT and T WEE BANK V 2 training sets. Gold-standard tokenization and automatic POS tags are used. Automatic POS tags are assigned with 5-fold jackknifing. Hyperparameters are tuned on the T WEEBANK V 2 development set. Unlabeled attachment score and labeled attachment score (including punctuation) are reported. All the experiments were run on a Xeon E5-2670 2.6 GHz machine. Reimers and Gurevych (2017) and others have 12 We use the updated version of Twokenizer from Owoputi et al. (2013). 13 http://nlp.stanford.edu/data/glove.twitter. 27B.zip 971 System Kong et al. (2014) Dozat et al. (2017) Ballesteros et al. (2015) Ensemble (20) Distillation (α = 1.0) Distillation (α = 0.9) Distillation w/ exploration UAS 81.4 81.8 80.2 83.4 81.8 82.0 82.1 LAS 76.9 77.7 75.7 79.4 77.6 77.8 77.9 Kt/s 0.3 1.7 2.3 0.2 2.3 2.3 2.3 tems above (Table 6). Distillation. The shortcoming of the 20-parser ensemble is, of course, that it requires twenty times the runtime of a single greedy parser, making it the slowest system in our comparison. Kuncoro et al. (2016) proposed the distillation of 20 greedy transition-based parser into a single graphbased parser; they transformed the votes of the ensemble int"
N18-1088,K17-3001,0,0.199799,"a tweet. We therefore treat the whole expression as non-syntactic, including assigning the other (X) part of speech to both RT and @coldplay, attaching the at-mention to RT with the discourse label and the colon to RT with the punct(uation) label, and attaching RT to the predicate of the following sentence. course while they used parataxis. In referential URLs, we use list (following the precedent of UD_English-EWT) while they used dep. Our choice of discourse for sentiment emoticons is inspired by the observation that emoticons are annotated as discourse by UD_English-EWT; Sanguinetti et al. (2017) used the same relation for the emoticons. Retweet constructions and truncated words were not explicitly touched by Sanguinetti et al. (2017). Judging from the released treebank8 , the RT marker, at-mention, and colon in the retweet construction are all attached to the predicate of the following sentence with dep, vocative:mention and punct. We expect that the official UD guidelines will eventually adopt standards for these constructions so the treebanks can be harmonized. Constructions handled by UD. A number of constructions that are especially common in tweets are handled by UD conventions:"
N18-1088,N13-1037,0,0.0477398,"Missing"
N18-1088,P16-1101,0,0.076965,"Missing"
N18-1088,P14-5010,0,0.00286153,"s the tokenization results, 11 Manual annotation was done with Arborator (Gerdes, 2013), a web platform for drawing dependency trees. 970 System Stanford CoreNLP Twokenizer UDPipe v1.2 our bi-LSTM tokenizer System Stanford CoreNLP Owoputi et al., 2013 (greedy) Owoputi et al., 2013 (CRF) Ma and Hovy, 2016 F1 97.3 94.6 97.4 98.3 Table 3: Tokenizer comparison on the T WEEBANK V 2 test set. Table 4: POS tagger comparison on gold-standard tokens in the T WEEBANK V 2 test set. Tokenization System Stanford CoreNLP our bi-LSTM tokenizer (§3.1) compared to other available tokenizers. Stanford CoreNLP (Manning et al., 2014) and Twokenizer (O’Connor et al., 2010)12 are rule-based systems and were not adapted to the UD tokenization scheme. The UDPipe v1.2 (Straka and Straková, 2017) model was re-trained on the same data as our system. Compared with UDPipe, we use an LSTM instead of a GRU in our model and we also use a larger size for hidden units (64 vs. 20), which has stronger representational power. Our bi-LSTM tokenizer achieves the best accuracy among all these tokenizers. These results speak to the value of statistical modeling in tokenization for informal texts. 3.2 Accuracy 90.6 93.7 94.6 92.5 F1 92.3 93.3"
N18-1088,J93-2004,0,0.062664,"to distill an ensemble of 20 transition-based parsers into a single one. Our parser achieves an improvement of 2.2 in LAS over the un-ensembled baseline and outperforms parsers that are state-ofthe-art on other treebanks in both accuracy and speed. 1 Noah A. Smith University of Washington Introduction NLP for social media messages is challenging, requiring domain adaptation and annotated datasets (e.g., treebanks) for training and evaluation. Pioneering work by Foster et al. (2011) annotated 7,630 tokens’ worth of tweets according to the phrase-structure conventions of the Penn Treebank (PTB; Marcus et al., 1993), enabling conversion to Stanford Dependencies. Kong et al. (2014) further studied the challenges in annotating tweets and 1 We developed our treebank independently of a similar effort for Italian tweets (Sanguinetti et al., 2017). See §2.5 for a comparison. 965 Proceedings of NAACL-HLT 2018, pages 965–975 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics Their aim was the rapid development of a dependency parser for tweets, and to that end they contributed a new annotated corpus, T WEEBANK, consisting of 12,149 tokens. Their annotations added unlabeled"
N18-1088,W13-3711,0,0.0175481,"rimental results. Our preliminary results showed that our model trained on the combination of UD_English-EWT and T WEEBANK V 2 outperformed the one trained only on the UD_EnglishEWT or T WEEBANK V 2, consistent with previous work on dialect treebank parsing (Wang et al., 2017). So we trained our tokenizer on the training portion of T WEEBANK V 2 combined with the UD_English-EWT training set and tested on the T WEEBANK V 2 test set. We report F1 scores, combining precision and recall for token identification. Table 3 shows the tokenization results, 11 Manual annotation was done with Arborator (Gerdes, 2013), a web platform for drawing dependency trees. 970 System Stanford CoreNLP Twokenizer UDPipe v1.2 our bi-LSTM tokenizer System Stanford CoreNLP Owoputi et al., 2013 (greedy) Owoputi et al., 2013 (CRF) Ma and Hovy, 2016 F1 97.3 94.6 97.4 98.3 Table 3: Tokenizer comparison on the T WEEBANK V 2 test set. Table 4: POS tagger comparison on gold-standard tokens in the T WEEBANK V 2 test set. Tokenization System Stanford CoreNLP our bi-LSTM tokenizer (§3.1) compared to other available tokenizers. Stanford CoreNLP (Manning et al., 2014) and Twokenizer (O’Connor et al., 2010)12 are rule-based systems a"
N18-1088,P11-2008,1,0.897627,"Missing"
N18-1088,C12-1059,0,0.0394777,"Missing"
N18-1088,N13-1039,1,0.950015,"Missing"
N18-1088,Q13-1033,0,0.0408296,"Missing"
N18-1088,D14-1162,0,0.0814147,"Missing"
N18-1088,D17-1256,0,0.437481,"Missing"
N18-1088,petrov-etal-2012-universal,0,0.0767282,"Missing"
N18-1088,P15-1119,1,0.752001,"ggested by Schneider et al. (2013), fairly close to Yamada and Matsumoto (2003) dependencies (without labels). Both annotation efforts were highly influenced by the PTB, whose guidelines have good grammatical coverage on newswire. However, when it comes to informal, unedited, user-generated text, the guidelines may leave many annotation decisions unspecified. Universal Dependencies (Nivre et al., 2016, UD) were introduced to enable consistent annotation across different languages. To allow such consistency, UD was designed to be adaptable to different genres (Wang et al., 2017) and languages (Guo et al., 2015; Ammar et al., 2016). We propose that analyzing the syntax of tweets can benefit from such adaptability. In this paper, we introduce a new English tweet treebank of 55,607 tokens that follows the UD guidelines, but also contends with social media-specific challenges that were not covered by UD guidelines.1 Our annotation includes tokenization, part-of-speech (POS) tags, and (labeled) Universal Dependencies. We characterize the disagreements among our annotators and find that consistent annotation is still challenging to deliver even with the extended guidelines. Based on these annotations, we"
N18-1088,D17-1035,0,0.0171475,"hich should help mitigate the challenge of spelling variation. We encourage the reader to refer their paper for more details about the model. In our initial experiments, we train our parser on the combination of UD_English-EWT and T WEE BANK V 2 training sets. Gold-standard tokenization and automatic POS tags are used. Automatic POS tags are assigned with 5-fold jackknifing. Hyperparameters are tuned on the T WEEBANK V 2 development set. Unlabeled attachment score and labeled attachment score (including punctuation) are reported. All the experiments were run on a Xeon E5-2670 2.6 GHz machine. Reimers and Gurevych (2017) and others have 12 We use the updated version of Twokenizer from Owoputi et al. (2013). 13 http://nlp.stanford.edu/data/glove.twitter. 27B.zip 971 System Kong et al. (2014) Dozat et al. (2017) Ballesteros et al. (2015) Ensemble (20) Distillation (α = 1.0) Distillation (α = 0.9) Distillation w/ exploration UAS 81.4 81.8 80.2 83.4 81.8 82.0 82.1 LAS 76.9 77.7 75.7 79.4 77.6 77.8 77.9 Kt/s 0.3 1.7 2.3 0.2 2.3 2.3 2.3 tems above (Table 6). Distillation. The shortcoming of the 20-parser ensemble is, of course, that it requires twenty times the runtime of a single greedy parser, making it the slowe"
N18-1088,D11-1141,0,0.495674,"Missing"
N18-1088,W17-6526,0,0.480518,"oth accuracy and speed. 1 Noah A. Smith University of Washington Introduction NLP for social media messages is challenging, requiring domain adaptation and annotated datasets (e.g., treebanks) for training and evaluation. Pioneering work by Foster et al. (2011) annotated 7,630 tokens’ worth of tweets according to the phrase-structure conventions of the Penn Treebank (PTB; Marcus et al., 1993), enabling conversion to Stanford Dependencies. Kong et al. (2014) further studied the challenges in annotating tweets and 1 We developed our treebank independently of a similar effort for Italian tweets (Sanguinetti et al., 2017). See §2.5 for a comparison. 965 Proceedings of NAACL-HLT 2018, pages 965–975 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics Their aim was the rapid development of a dependency parser for tweets, and to that end they contributed a new annotated corpus, T WEEBANK, consisting of 12,149 tokens. Their annotations added unlabeled dependencies to a portion of the data annotated with POS tags by Gimpel et al. (2011) and Owoputi et al. (2013) after rule-based tokenization (O’Connor et al., 2010). Kong et al. also contributed a system for parsing; we defer th"
N18-1088,D16-1139,0,0.0215692,"r comparison. Kuncoro et al. (2016) proposed the distillation of 20 greedy transition-based parser into a single graphbased parser; they transformed the votes of the ensemble into a structured loss function. However, as Kuncoro et al. pointed out, it is not straightforward to use a structured loss in a transition-based parsing algorithm. Because fast runtime is so important for NLP on social media, we introduce a new way to distill our greedy ensemble into a single transition-based parser (the first such attempt, to our knowledge). Our approach applies techniques from Hinton et al. (2015) and Kim and Rush (2016) to parsing. Note that training a transition-based parser typically involves the transformation of the training data into a sequence of “oracle” state-action pairs. Let q(a |s) denote the distilled model’s probability of an action a given parser state s; let p(a |s) be the probability under the ensemble (i.e., the average of the 20 separately-trained ensemble members). To train the distilled model, we minimize the interpolation between their distillation loss and the conventional log loss: XX argminq α −p(a |si ) · log q(a |si ) Table 6: Dependency parser comparison on T WEE BANK V 2 test set,"
N18-1088,Q16-1023,0,0.108829,"Missing"
N18-1088,W13-2307,1,0.929727,"Missing"
N18-1088,D14-1108,1,0.404607,"uti et al., 2013), and a stack LSTM parser with character-based word representations (Ballesteros et al., 2015), which we refer to as our “baseline” parser. To overcome the noise in our annotated We study the problem of analyzing tweets with Universal Dependencies (UD; Nivre et al., 2016). We extend the UD guidelines to cover special constructions in tweets that affect tokenization, part-ofspeech tagging, and labeled dependencies. Using the extended guidelines, we create a new tweet treebank for English (T WEEBANK V 2) that is four times larger than the (unlabeled) T WEEBANK V 1 introduced by Kong et al. (2014). We characterize the disagreements between our annotators and show that it is challenging to deliver consistent annotation due to ambiguity in understanding and explaining tweets. Nonetheless, using the new treebank, we build a pipeline system to parse raw tweets into UD. To overcome annotation noise without sacrificing computational efficiency, we propose a new method to distill an ensemble of 20 transition-based parsers into a single one. Our parser achieves an improvement of 2.2 in LAS over the un-ensembled baseline and outperforms parsers that are state-ofthe-art on other treebanks in bot"
N18-1088,K17-3009,0,0.0304172,"oreNLP Twokenizer UDPipe v1.2 our bi-LSTM tokenizer System Stanford CoreNLP Owoputi et al., 2013 (greedy) Owoputi et al., 2013 (CRF) Ma and Hovy, 2016 F1 97.3 94.6 97.4 98.3 Table 3: Tokenizer comparison on the T WEEBANK V 2 test set. Table 4: POS tagger comparison on gold-standard tokens in the T WEEBANK V 2 test set. Tokenization System Stanford CoreNLP our bi-LSTM tokenizer (§3.1) compared to other available tokenizers. Stanford CoreNLP (Manning et al., 2014) and Twokenizer (O’Connor et al., 2010)12 are rule-based systems and were not adapted to the UD tokenization scheme. The UDPipe v1.2 (Straka and Straková, 2017) model was re-trained on the same data as our system. Compared with UDPipe, we use an LSTM instead of a GRU in our model and we also use a larger size for hidden units (64 vs. 20), which has stronger representational power. Our bi-LSTM tokenizer achieves the best accuracy among all these tokenizers. These results speak to the value of statistical modeling in tokenization for informal texts. 3.2 Accuracy 90.6 93.7 94.6 92.5 F1 92.3 93.3 Table 5: Owoputi et al. (2013) POS tagging performance with automatic tokenization on the T WEEBANK V 2 test set. Experimental results. We tested the POS tagger"
N18-1088,P17-1159,0,0.100495,"d largely following conventions suggested by Schneider et al. (2013), fairly close to Yamada and Matsumoto (2003) dependencies (without labels). Both annotation efforts were highly influenced by the PTB, whose guidelines have good grammatical coverage on newswire. However, when it comes to informal, unedited, user-generated text, the guidelines may leave many annotation decisions unspecified. Universal Dependencies (Nivre et al., 2016, UD) were introduced to enable consistent annotation across different languages. To allow such consistency, UD was designed to be adaptable to different genres (Wang et al., 2017) and languages (Guo et al., 2015; Ammar et al., 2016). We propose that analyzing the syntax of tweets can benefit from such adaptability. In this paper, we introduce a new English tweet treebank of 55,607 tokens that follows the UD guidelines, but also contends with social media-specific challenges that were not covered by UD guidelines.1 Our annotation includes tokenization, part-of-speech (POS) tags, and (labeled) Universal Dependencies. We characterize the disagreements among our annotators and find that consistent annotation is still challenging to deliver even with the extended guidelines"
N18-1088,W03-3023,0,0.271111,"Missing"
N18-1088,L16-1262,0,\N,Missing
N18-1135,E14-1023,0,0.0385948,"tional Locative Relation Place Ground Quantity Individuals Figure 1: An example sentence from the FrameNet 1.5 corpus, shown with an author-annotated DM semantic dependency graph (above) and framesemantic annotation (below). Two more gold frames (and their arguments) have been omitted for space. Introduction Semantic parsing aims to automatically predict formal representations of meaning underlying natural language, and has been useful in question answering (Shen and Lapata, 2007), text-to-scene generation (Coyne et al., 2012), dialog systems (Chen et al., 2013) and social-network extraction (Agarwal et al., 2014), among others. Various formal meaning representations have been developed corresponding to different semantic theories (Fillmore, 1982; Palmer et al., 2005; Flickinger et al., 2012; Banarescu et al., 2013). The distributed nature of these efforts results in a set of annotated resources that are similar in spirit, but not strictly compatible. A major axis of structural divergence in semantic formalisms is whether based on spans (Baker et al., 1998; Palmer et al., 2005) or dependencies (Surdeanu et al., 2008; Oepen et al., 2014; Banarescu et al., 2013; Copestake et al., 2005, inter alia). Depen"
N18-1135,P98-1013,0,0.941652,"swering (Shen and Lapata, 2007), text-to-scene generation (Coyne et al., 2012), dialog systems (Chen et al., 2013) and social-network extraction (Agarwal et al., 2014), among others. Various formal meaning representations have been developed corresponding to different semantic theories (Fillmore, 1982; Palmer et al., 2005; Flickinger et al., 2012; Banarescu et al., 2013). The distributed nature of these efforts results in a set of annotated resources that are similar in spirit, but not strictly compatible. A major axis of structural divergence in semantic formalisms is whether based on spans (Baker et al., 1998; Palmer et al., 2005) or dependencies (Surdeanu et al., 2008; Oepen et al., 2014; Banarescu et al., 2013; Copestake et al., 2005, inter alia). Depending on application requirements, either might be most useful in a given situation. Learning from a union of these resources seems promising, since more data almost always translates into better performance. This is indeed the case for two prior techniques—parameter sharing (FitzGerald et al., 2015; Kshirsagar et al., 2015), and joint decoding across multiple formalisms using cross-task factors that score combinations of substructures from each (P"
N18-1135,W13-2322,0,0.39358,"annotation (below). Two more gold frames (and their arguments) have been omitted for space. Introduction Semantic parsing aims to automatically predict formal representations of meaning underlying natural language, and has been useful in question answering (Shen and Lapata, 2007), text-to-scene generation (Coyne et al., 2012), dialog systems (Chen et al., 2013) and social-network extraction (Agarwal et al., 2014), among others. Various formal meaning representations have been developed corresponding to different semantic theories (Fillmore, 1982; Palmer et al., 2005; Flickinger et al., 2012; Banarescu et al., 2013). The distributed nature of these efforts results in a set of annotated resources that are similar in spirit, but not strictly compatible. A major axis of structural divergence in semantic formalisms is whether based on spans (Baker et al., 1998; Palmer et al., 2005) or dependencies (Surdeanu et al., 2008; Oepen et al., 2014; Banarescu et al., 2013; Copestake et al., 2005, inter alia). Depending on application requirements, either might be most useful in a given situation. Learning from a union of these resources seems promising, since more data almost always translates into better performance"
N18-1135,J03-4003,0,0.049976,"arget in frame semantics, the destination corresponds to the argument, and the label corresponds to the role. The same set of labels are available for all arcs, in contrast to the frame-specific roles in FrameNet. 2.3 Spans vs. Dependencies Early semantic role labeling was span-based (Gildea and Jurafsky, 2002; Toutanova et al., 2008, inter alia), with spans corresponding to syntactic constituents. But, as in syntactic parsing, there are sometimes theoretical or practical reasons to prefer dependency graphs. To this end, Surdeanu et al. (2008) devised heuristics based on syntactic head rules (Collins, 2003) to transform PropBank (Palmer et al., 2005) annotations into dependencies. Hence, for PropBank at least, there is a very direct connection (through syntax) between spans and dependencies. For many other semantic representations, such a direct relationship might not be present. Some semantic representations are designed as graphs from the start (Hajiˇc et al., 2012; Banarescu et al., 2013), and have no gold alignment to spans. Conversely, some span-based formalisms are not annotated with syntax (Baker et al., 1998; He et al., 2015),3 and so head rules would require using (noisy and potentially"
N18-1135,C12-1042,0,0.0894431,"few books fell in the reading room . a few.art Theme Figure fall.v in.prep Motion Directional Locative Relation Place Ground Quantity Individuals Figure 1: An example sentence from the FrameNet 1.5 corpus, shown with an author-annotated DM semantic dependency graph (above) and framesemantic annotation (below). Two more gold frames (and their arguments) have been omitted for space. Introduction Semantic parsing aims to automatically predict formal representations of meaning underlying natural language, and has been useful in question answering (Shen and Lapata, 2007), text-to-scene generation (Coyne et al., 2012), dialog systems (Chen et al., 2013) and social-network extraction (Agarwal et al., 2014), among others. Various formal meaning representations have been developed corresponding to different semantic theories (Fillmore, 1982; Palmer et al., 2005; Flickinger et al., 2012; Banarescu et al., 2013). The distributed nature of these efforts results in a set of annotated resources that are similar in spirit, but not strictly compatible. A major axis of structural divergence in semantic formalisms is whether based on spans (Baker et al., 1998; Palmer et al., 2005) or dependencies (Surdeanu et al., 200"
N18-1135,N10-1138,1,0.883204,"se the same original texts as earlier efforts, the utility of this approach is limited. We propose an extension to Peng et al.’s formulation which addresses this limitation by considering disjoint resources, each containing only a single kind of annotation. Moreover, we consider structurally divergent formalisms, one dealing with semantic spans and the other with semantic 1492 Proceedings of NAACL-HLT 2018, pages 1492–1502 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics dependencies. We experiment on frame-semantic parsing (Gildea and Jurafsky, 2002; Das et al., 2010), a span-based semantic role labeling (SRL) task (§2.1), and on a dependency-based minimum recursion semantic parsing (DELPH-IN MRS, or DM; Flickinger et al., 2012) task (§2.2). See Figure 1 for an example sentence with gold FrameNet annotations, and author-annotated DM representations. Our joint inference formulation handles missing annotations by treating the structures that are not present in a given training example as latent variables (§3).1 Specifically, semantic dependencies are treated as a collection of latent variables when training on FrameNet examples. Using this latent variable fo"
N18-1135,D15-1112,0,0.0806338,"Missing"
N18-1135,P14-1134,1,0.878233,"in general graphical models, there are more efficient off-the-shelf implementations for approximate max-decoding, hence, we adopt a max-margin formulation. 5.2 Inference We formulate the maximizations in Equation 13 as 0–1 integer linear programs and use AD3 to solve them (Martins et al., 2011). We only enforce a non-overlapping constraint when decoding FrameNet structures, so that the argument identification subproblem can be efficiently solved by a dynamic program (Kong et al., 2016; Swayamdipta et al., 2017). When decoding semantic dependency graphs, we enforce the determinism constraint (Flanigan et al., 2014), where certain labels may appear on at most one arc outgoing from the same token. Inference speedup by promoting sparsity. As discussed in §3, even after pruning, the number of within-task parts is linear in the length of the input sentence, so the number of cross-task parts is quadratic. This leads to potentially very slow inference. We address this problem by imposing an `1 penalty on the cross-task part scores: X  sc (yi , zj ) , (14) L y∗ + λ (yi ,zj )∈C where λ is a hyperparameter, set to 0.01 as a practical tradeoff between efficiency and development set performance. Whenever the score"
N18-1135,J02-3001,0,0.7595,"rts cannot be expected to use the same original texts as earlier efforts, the utility of this approach is limited. We propose an extension to Peng et al.’s formulation which addresses this limitation by considering disjoint resources, each containing only a single kind of annotation. Moreover, we consider structurally divergent formalisms, one dealing with semantic spans and the other with semantic 1492 Proceedings of NAACL-HLT 2018, pages 1492–1502 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics dependencies. We experiment on frame-semantic parsing (Gildea and Jurafsky, 2002; Das et al., 2010), a span-based semantic role labeling (SRL) task (§2.1), and on a dependency-based minimum recursion semantic parsing (DELPH-IN MRS, or DM; Flickinger et al., 2012) task (§2.2). See Figure 1 for an example sentence with gold FrameNet annotations, and author-annotated DM representations. Our joint inference formulation handles missing annotations by treating the structures that are not present in a given training example as latent variables (§3).1 Specifically, semantic dependencies are treated as a collection of latent variables when training on FrameNet examples. Using this"
N18-1135,E17-1045,0,0.29891,"Missing"
N18-1135,P17-1044,0,0.0718086,"Missing"
N18-1135,D15-1076,0,0.013807,"et al. (2008) devised heuristics based on syntactic head rules (Collins, 2003) to transform PropBank (Palmer et al., 2005) annotations into dependencies. Hence, for PropBank at least, there is a very direct connection (through syntax) between spans and dependencies. For many other semantic representations, such a direct relationship might not be present. Some semantic representations are designed as graphs from the start (Hajiˇc et al., 2012; Banarescu et al., 2013), and have no gold alignment to spans. Conversely, some span-based formalisms are not annotated with syntax (Baker et al., 1998; He et al., 2015),3 and so head rules would require using (noisy and potentially expensive) predicted syntax. Inspired by the head rules of Surdeanu et al. (2008), we design cross-task parts, without relying 3 In FrameNet, phrase types of arguments and their grammatical function in relation to their target have been annotated. But in order to apply head rules, the internal structure of arguments (or at least their semantic heads) would also require syntactic annotations. on gold or predicted syntax (which may be either unavailable or error-prone) or on heuristics. 3 Model Given an input sentence x, and target"
N18-1135,P14-1136,0,0.701865,"c. Rec. F1 Roth T¨ackstr¨om FitzGerald FitzGerald (10×) open-SESAME open-SESAME (5×) Yang and Mitchell (R EL) †∗ Yang and Mitchell (A LL ) 72.2 75.4 74.8 75.0 71.0 71.2 77.1 78.8 68.0 65.8 65.5 67.3 67.8 70.5 68.7 74.5 70.0 70.3 69.9 70.9 69.4 70.9 72.7 76.6 † This work (F ULL) work (F ULL, 2×) 80.4 80.4 73.5 74.7 76.8 77.4 † This work (BASIC) work (N O CTP) 79.2 76.9 71.7 74.8 75.3 75.8 † This † This Table 2: FN 1.5 full structure extraction test performance. † denotes the models jointly predicting frames and arguments, and other systems implement two-stage pipelines and use the algorithm by Hermann et al. (2014) to predict frames. K× denotes a product-of-experts ensemble of K models. ∗ Ensembles a sequential tagging CRF and a relational model. Bold font indicates best performance among all systems. et al. (2017), denoted as NeurboParser (BASIC). To ensure fair comparison with our F ULL model, we made several modifications to their implementation (§6.3). We observed performance improvements from our reimplementation, which can be seen in Table 5. Pruning strategies. For frame SRL, we discard argument spans longer than 20 tokens (Swayamdipta et al., 2017). We further pretrain an unlabeled model and pru"
N18-1135,P15-1162,0,0.0999312,"Missing"
N18-1135,S07-1048,0,0.103703,"ossible frames it could evoke, F` . Every frame f ∈ F` is also associated with a set of roles, Rf under this ontology. For example, in Figure 1, the LU “fall.v” evokes the frame M OTION DIRECTIONAL. The roles T HEME and P LACE (which are specific to M O TION DIRECTIONAL), are filled by the spans “Only a few books” and “in the reading room” respectively. L OCATIVE RELATION has other roles (P ROFILED REGION, ACCESSIBILITY, D EIXIS, etc.) which are not realized in this sentence. In this work, we assume gold targets and LUs are given, and parse each target independently, following the literature (Johansson and Nugues, 2007; FitzGerald et al., 2015; Yang and Mitchell, 2017; Swayamdipta et al., 2017, inter alia). Moreover, following Yang and Mitchell (2017), we perform frame and argument identification jointly. Most prior work has enforced the constraint that a role may be filled by at most one argument span, but following Swayamdipta et al. (2017) we do not impose this constraint, requiring only that arguments for the same target do not overlap. 1 Following past work on support vector machines with latent variables (Yu and Joachims, 2009), we use the term “latent variable,” even though the model is not probabili"
N18-1135,P15-2036,1,0.949142,"lar in spirit, but not strictly compatible. A major axis of structural divergence in semantic formalisms is whether based on spans (Baker et al., 1998; Palmer et al., 2005) or dependencies (Surdeanu et al., 2008; Oepen et al., 2014; Banarescu et al., 2013; Copestake et al., 2005, inter alia). Depending on application requirements, either might be most useful in a given situation. Learning from a union of these resources seems promising, since more data almost always translates into better performance. This is indeed the case for two prior techniques—parameter sharing (FitzGerald et al., 2015; Kshirsagar et al., 2015), and joint decoding across multiple formalisms using cross-task factors that score combinations of substructures from each (Peng et al., 2017). Parameter sharing can be used in a wide range of multitask scenarios, when there is no data overlap or even any similarity between the tasks (Collobert and Weston, 2008; Søgaard and Goldberg, 2016). But techniques involving joint decoding have so far only been shown to work for parallel annotations of dependency-based formalisms, which are structurally very similar to each other (Llu´ıs et al., 2013; Peng et al., 2017). Of particular interest is the a"
N18-1135,D17-1018,0,0.0297325,"is mapped to an embedding vector. Two LSTMs (Hochreiter and Schmidhuber, 1997) are run in opposite directions over the input vector sequence. We use the concatenation of the two hidden representations at each position i as a contextualized word embedding for each token: 6 Most targets are single-words (§2.1). For multi-token targets, we consider only the first token, which is usually content-bearing. (6) z∈Z → − ← − hi = h i ; h i . (7) 7 Semantic dependency parses over a sentence are not constrained to be identical for different frame-semantic targets. 1495 Span representations. Following Lee et al. (2017), span representations are computed based on boundary word representations and discrete length and distance features. Concretely, given a target t and its associated argument a = (i, j, r) with boundary indices i and j, we compute three features φt (a) based on the length of a, and the distances from i and j to the start of t. We concatenate the token representations at a’s boundary with the discrete features φt (a). We then use a two-layer tanh-MLP to compute the span representation:  gspan (i, j) = MLPspan [hi ; hj ; φt (a)] . (8) where wua is a vector of learned weights. The scores for oth"
N18-1135,P14-1130,0,0.020276,"f learned weights. The scores for other types of parts are computed similarly, but with separate MLPs and weights. 4.4 Cross-Task Part Scoring As shown in Figure 2, each cross-task part c consists of two first-order parts: a frame argument part a, and an unlabeled dependency part, u. The score for a cross-task part incorporates both: sc (c) = W ⊗ U ⊗ V, gpred (f ) ⊗ garg (a) (12) ⊗ wua ⊗ gua (u) , The target representation is similarly computed using a separate MLPtgt , with a length feature but no distance features. where V is a low-rank order-2 tensor of parameters. Following previous work (Lei et al., 2014; Peng et al., 2017), we construct the parameter tensors W, U, and V so as to upper-bound their ranks. 4.2 5 gtgt (t) Frame and Argument Scoring As defined in §3, the representation for a predicate part incorporates representations of a target span, the associated LU and the frame evoked by the LU. The score for a predicate part is given by a multilinear mapping: gpred (f ) = gfr (f ) ⊗ gtgt (t) ⊗ glu (`) sf (p) = W, gpred (f ) , (9a) (9b) where W is a low-rank order-3 tensor of learned parameters, and gfr (f ) and glu (`) are learned lookup embeddings for the frame and LU. A candidate argumen"
N18-1135,Q13-1018,0,0.158093,"Missing"
N18-1135,S14-2082,0,0.267962,"single target here; handling of multiple targets is discussed in §6. 5 With pruning (described in §6) we reduce this to a number of parts linear in n. Also, |F` |is usually small (averaging 1.9), as is |Rf |(averaging 9.5). 1494 Evidence to support this argument includes … include.v Inclusion Total Figure 2: An example of cross-task parts from the FrameNet 1.5 development set. We enumerate all unlabeled semantic dependencies from the first word of the target (includes) to any token inside the span. The red bolded arc indicates the prediction of our model. Semantic dependency score. Following Martins and Almeida (2014), we consider three types of parts in a semantic dependency graph: semantic heads, unlabeled semantic arcs, and labeled semantic arcs. Analogous to Equation 3, the score for a dependency graph z is the sum of local scores: X Sd (z) = sd (zj ), (4) zj ∈z The computation of sd is described in §4.3. Cross task score. In addition to task-specific parts, we introduce a set C of cross-task parts. Each cross-task part relates an argument part from y to an unlabeled dependency arc from z. Based on the head-rules described in §2.3, we consider unlabeled arcs from the target to any token inside the span"
N18-1135,D11-1022,1,0.915468,"Missing"
N18-1135,D12-1074,0,0.13404,"Missing"
N18-1135,S15-2153,0,0.388147,"rop in performance. 6 Experiments Datasets. Our model is evaluated on two different releases of FrameNet: FN 1.5 and FN 1.7,9 using splits from Swayamdipta et al. (2017). Following Swayamdipta et al. (2017) and Yang and Mitchell (2017), each target annotation is treated as a separate training instance. We also include as training data the exemplar sentences, each annotated for a single target, as they have been reported to improve performance (Kshirsagar et al., 2015; Yang and Mitchell, 2017). For semantic dependencies, we use the English DM dataset from the SemEval 2015 Task 18 closed track (Oepen et al., 2015).10 DM contains instances from the WSJ corpus for training and both in-domain (id) and out-of-domain (ood) test sets, the latter from the Brown corpus.11 Table 1 summarizes the sizes of the datasets. Baselines. We compare FN performance of our joint learning model (F ULL) to two baselines: BASIC: A single-task frame SRL model, trained using a structured hinge objective. N O CTP: A joint model without cross-task parts. It demonstrates the effect of sharing parameters in word embeddings and LSTMs (like in F ULL). It does not use latent semantic dependency structures, and aims to minimize the sum"
N18-1135,L16-1630,0,0.15794,"Missing"
N18-1135,S14-2008,0,0.241184,"ialog systems (Chen et al., 2013) and social-network extraction (Agarwal et al., 2014), among others. Various formal meaning representations have been developed corresponding to different semantic theories (Fillmore, 1982; Palmer et al., 2005; Flickinger et al., 2012; Banarescu et al., 2013). The distributed nature of these efforts results in a set of annotated resources that are similar in spirit, but not strictly compatible. A major axis of structural divergence in semantic formalisms is whether based on spans (Baker et al., 1998; Palmer et al., 2005) or dependencies (Surdeanu et al., 2008; Oepen et al., 2014; Banarescu et al., 2013; Copestake et al., 2005, inter alia). Depending on application requirements, either might be most useful in a given situation. Learning from a union of these resources seems promising, since more data almost always translates into better performance. This is indeed the case for two prior techniques—parameter sharing (FitzGerald et al., 2015; Kshirsagar et al., 2015), and joint decoding across multiple formalisms using cross-task factors that score combinations of substructures from each (Peng et al., 2017). Parameter sharing can be used in a wide range of multitask sce"
N18-1135,D07-1002,0,0.163601,"1 arg2 BV arg1 mwe arg1 arg1 compound arg1 Only a few books fell in the reading room . a few.art Theme Figure fall.v in.prep Motion Directional Locative Relation Place Ground Quantity Individuals Figure 1: An example sentence from the FrameNet 1.5 corpus, shown with an author-annotated DM semantic dependency graph (above) and framesemantic annotation (below). Two more gold frames (and their arguments) have been omitted for space. Introduction Semantic parsing aims to automatically predict formal representations of meaning underlying natural language, and has been useful in question answering (Shen and Lapata, 2007), text-to-scene generation (Coyne et al., 2012), dialog systems (Chen et al., 2013) and social-network extraction (Agarwal et al., 2014), among others. Various formal meaning representations have been developed corresponding to different semantic theories (Fillmore, 1982; Palmer et al., 2005; Flickinger et al., 2012; Banarescu et al., 2013). The distributed nature of these efforts results in a set of annotated resources that are similar in spirit, but not strictly compatible. A major axis of structural divergence in semantic formalisms is whether based on spans (Baker et al., 1998; Palmer et a"
N18-1135,P16-2038,0,0.0503119,"might be most useful in a given situation. Learning from a union of these resources seems promising, since more data almost always translates into better performance. This is indeed the case for two prior techniques—parameter sharing (FitzGerald et al., 2015; Kshirsagar et al., 2015), and joint decoding across multiple formalisms using cross-task factors that score combinations of substructures from each (Peng et al., 2017). Parameter sharing can be used in a wide range of multitask scenarios, when there is no data overlap or even any similarity between the tasks (Collobert and Weston, 2008; Søgaard and Goldberg, 2016). But techniques involving joint decoding have so far only been shown to work for parallel annotations of dependency-based formalisms, which are structurally very similar to each other (Llu´ıs et al., 2013; Peng et al., 2017). Of particular interest is the approach of Peng et al., where three kinds of semantic graphs are jointly learned on the same input, using parallel annotations. However, as new annotation efforts cannot be expected to use the same original texts as earlier efforts, the utility of this approach is limited. We propose an extension to Peng et al.’s formulation which addresses"
N18-1135,W08-2121,0,0.219269,"Missing"
N18-1135,Q15-1003,0,0.0739241,"Missing"
N18-1135,J08-2002,0,0.123701,"Missing"
N18-1135,D17-1128,0,0.734008,"n Exemplars Dev. Test FN 1.5 FN 1.7 17,143 19,875 153,952 192,460 2,333 2,308 4,457 6,722 DM id DM ood 33,961 - - 1,692 - 1,410 1,849 Table 1: Number of instances in datasets. ing sparse graphical models only because they result in faster inference, not because we have any a priori belief about sparsity. This results in roughly a 14× speedup in our experiments, without any significant drop in performance. 6 Experiments Datasets. Our model is evaluated on two different releases of FrameNet: FN 1.5 and FN 1.7,9 using splits from Swayamdipta et al. (2017). Following Swayamdipta et al. (2017) and Yang and Mitchell (2017), each target annotation is treated as a separate training instance. We also include as training data the exemplar sentences, each annotated for a single target, as they have been reported to improve performance (Kshirsagar et al., 2015; Yang and Mitchell, 2017). For semantic dependencies, we use the English DM dataset from the SemEval 2015 Task 18 closed track (Oepen et al., 2015).10 DM contains instances from the WSJ corpus for training and both in-domain (id) and out-of-domain (ood) test sets, the latter from the Brown corpus.11 Table 1 summarizes the sizes of the datasets. Baselines. We co"
N18-1135,J05-1004,0,0.781956,"tic dependency graph (above) and framesemantic annotation (below). Two more gold frames (and their arguments) have been omitted for space. Introduction Semantic parsing aims to automatically predict formal representations of meaning underlying natural language, and has been useful in question answering (Shen and Lapata, 2007), text-to-scene generation (Coyne et al., 2012), dialog systems (Chen et al., 2013) and social-network extraction (Agarwal et al., 2014), among others. Various formal meaning representations have been developed corresponding to different semantic theories (Fillmore, 1982; Palmer et al., 2005; Flickinger et al., 2012; Banarescu et al., 2013). The distributed nature of these efforts results in a set of annotated resources that are similar in spirit, but not strictly compatible. A major axis of structural divergence in semantic formalisms is whether based on spans (Baker et al., 1998; Palmer et al., 2005) or dependencies (Surdeanu et al., 2008; Oepen et al., 2014; Banarescu et al., 2013; Copestake et al., 2005, inter alia). Depending on application requirements, either might be most useful in a given situation. Learning from a union of these resources seems promising, since more dat"
N18-1135,P17-1186,1,0.872128,"8; Palmer et al., 2005) or dependencies (Surdeanu et al., 2008; Oepen et al., 2014; Banarescu et al., 2013; Copestake et al., 2005, inter alia). Depending on application requirements, either might be most useful in a given situation. Learning from a union of these resources seems promising, since more data almost always translates into better performance. This is indeed the case for two prior techniques—parameter sharing (FitzGerald et al., 2015; Kshirsagar et al., 2015), and joint decoding across multiple formalisms using cross-task factors that score combinations of substructures from each (Peng et al., 2017). Parameter sharing can be used in a wide range of multitask scenarios, when there is no data overlap or even any similarity between the tasks (Collobert and Weston, 2008; Søgaard and Goldberg, 2016). But techniques involving joint decoding have so far only been shown to work for parallel annotations of dependency-based formalisms, which are structurally very similar to each other (Llu´ıs et al., 2013; Peng et al., 2017). Of particular interest is the approach of Peng et al., where three kinds of semantic graphs are jointly learned on the same input, using parallel annotations. However, as new"
N18-1135,D14-1162,0,0.0809067,"Missing"
N18-1135,C98-1013,0,\N,Missing
N18-1148,J08-4004,0,0.224721,"ght reasonably have access to all data of interest from the beginning of the project, such as when working with a historical corpus. Although a full proof is beyond the scope of this paper, in this case, the best approach is almost certainly to simply sample a random set of documents, label them using the annotation function, and report the relative prevalence of each label (Hopkins and King, 2010). Although this simple random sampling (SRS) approach ignores the text, it is an unbiased estimator with variance that can easily calculated, at least Mean AE involve large numbers of disagreements (Artstein and Poesio, 2008). Although it is conventional to treat disagreements as errors on the behalf of some subset of annotators, this paper provides an alternative way of understanding these. By treating annotation as a stochastic process, conditional on text, we can explain not only the disagreements between annotators, but also the lack of self-consistency that is also sometimes observed. Although the assumption that p(y |x) does not change is clearly a simplification, it seems reasonable when working with trained annotators. Certainly this assumption seems much better justified than the conventional assumption t"
N18-1148,W04-3202,0,0.082617,"o the annotations from the reliable annotators. This seems particularly appropriate when dealing with uncooperative annotators, as might be encountered, for example, in crowdsourcing (Snow et al., 2008; Zhang et al., 2016). However, with a team of trained annotators, we believe that honest disagreements could contain valuable information better not ignored. Finally, this work also relates to the problem of active learning, where the goal is to interactively choose instances to be labeled, in a way that maximizes accuracy while minimizing the total cost of annotation (Beygelzimer et al., 2009; Baldridge and Osborne, 2004; Rai et al., 2010; Settles, 2012). This is an interesting area that might be productively combined with the ideas in this paper. In general, however, the use of active learning involves additional logistical complications and does not always work better than random sampling in practice (Attenberg and Provost, 2011). 6 Conclusions When estimating proportions in a target corpus, it is important to take seriously the data generating process. We have argued that in the case of data annotated by humans in terms of categories designed to help answer social-scientific research questions, labels shou"
N18-1148,P15-2072,1,0.853197,"we use a dataset of several thousand news articles that have been annotated in terms of a set of broad-coverage framing dimensions (such as economics, morality, etc.). We treat annotations as indicating the presence or absence of each dimension, and consider each one as a separate sub-task. As with all datasets, we create a source and target corpus by dividing the datasets by year. Particularly for this dataset, it seems reasonable to posit that the annotation function was relatively constant between source and target, as the annotators worked without explicit knowledge of the article’s date (Card et al., 2015). Amazon reviews: As a secondary example of extrinsic labels, we make use of a subset of Amazon reviews for five different product categories, each of which has tens of thousands of reviews. For this dataset, we ignore the star rating associated with the review, and instead focus on predicting the proportion of people that would rate the review as helpful. Here we create separate subtasks for each product category by considering each pair of adjacent years as a source and target corpus, respectively (McAuley et al., 2015). Yelp reviews: As a primary example of a large dataset with intrinsic la"
N18-1148,P16-1151,0,0.145985,"to the researcher. We won’t always know the true distributional properties of our datasets, but distinguishing between intrinsic and extrinsic labels provides a guide. The critical point is that these two different labeling scenarios have different implications for robustness to distributional shift. In the case of extrinsic labels, especially when working with trained annotators, it is reasonable to assume that the behavior of the annotation function is determined purely by the text, such that p(y |x) is unchanged between source and target, and any change in label proportions is explained 4 Fong and Grimmer (2016) also consider this process in attempting to identify the causal effects of texts. by a change in the underlying distribution of text, p(x). With intrinsic labels, by contrast, it may be the case that p(x |y) is the same for the source and the target, assuming there are no additional factors influencing the generation of text. In that case, a shift in the distribution of features would be fully explained by a difference in the underlying label proportions. The idea that there are different data generating processes is obviously not new.5 What is novel here, however, is asking how these differe"
N18-1148,D15-1182,0,0.233301,"Missing"
N18-1148,W10-0104,0,0.0793938,"Missing"
N18-1148,N13-1132,0,0.0501323,"They perform a large number of experiments, but unfortunately, nearly all of their experiments involve only a very small difference in label proportions between the source and target (with the vast majority &lt; 0.01), which limits the generalizability of their findings. Additional methods for calibration could also be considered, such as the isotonic regression approach of Zadrozny and Elkan (2002), but in practice we would expect the results to be very similar to Platt scaling. Another line of work has approached the problem of aggregating labels from multiple annotators (Raykar et al., 2009; Hovy et al., 2013; Yan et al., 2013). That is, if we believe that some annotators are more reliable than others, it might make sense to try to determine this in an unsupervised manner, and give more weight to the annotations from the reliable annotators. This seems particularly appropriate when dealing with uncooperative annotators, as might be encountered, for example, in crowdsourcing (Snow et al., 2008; Zhang et al., 2016). However, with a team of trained annotators, we believe that honest disagreements could contain valuable information better not ignored. Finally, this work also relates to the problem of"
N18-1148,D08-1027,0,0.163515,"Missing"
N18-1148,D17-1323,0,0.0453486,"ers first decide on the sentiment they wish to convey and then write a blog post conditional on that sentiment. 1638 ability, p(y), which is assumed to have occurred (Hopkins and King, 2010). In the case of covariate shift, the difference in p(x) will result in a model that is not optimal (in terms of classification performance) for the target domain. In both cases, there is also the problem of classifier bias or miscalibration. Particularly in the case of unbalanced labels, a standard classifier is likely to be biased, overestimating the probability of the more common labels, and vice versa (Zhao et al., 2017). Here we present a simple but novel method for extrinsic labels, followed by a number of baseline approaches against which we will compare. (See supplementary material for additional details.) 3.1 Proposed method: calibrated probabilistic classify and count (PCCcal ) One simple solution, which we propose here, is to attempt to train a well-calibrated classifier. To be clear, calibration refers to the long-run accuracy of predicted probabilities. That is, a probabilistic classifier, hθ (x), is well calibrated at the level µ if, among all instances for which the classifier predicts class k with"
N18-1204,N15-1083,0,0.0185956,", pt [k], ecurrent [k]). (5) The max pooling technique originates from the design of convolutional neural networks and has been found useful elsewhere in NLP (Kalchbrenner et al., 2014). Other alternatives, including average pooling, min pooling, and element-wise multiplication on all three vectors, were considered in informal preliminary experiments on development data and found less effective than max pooling. This combined context vector ct is used to generate word wt by calculating the probability of each word type in the vocabulary. We use a classfactored softmax function (Goodman, 2001; Baltescu and Blunsom, 2015). This choice greatly reduces the runtime of word prediction. In practice, we often find it gives better performance than standard softmax. 2.4 Learning The training objective is to maximize the logprobability of X: X `(θ) = log P (X; θ) = log P (Xt ; θ) (6) t θ denotes all of the model’s parameters. Xt represents all decisions at timestep t about the word (whether it is part of a entity mention, and if so, the entity the mention refers to, the length of the mention, and the word itself). These decisions are made by calculating probabilities for each available option using the current state of"
N18-1204,J08-1001,0,0.0462623,"ortant source of information. We use a simple, parameter-free method for combining preceding context with entity context within an end-to-end– 2250 Proceedings of NAACL-HLT 2018, pages 2250–2260 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics trainable neural language generator. We evaluate our model’s performance through two automatic evaluation tasks. The first is a new mention generation task inspired by earlier work in referring expression generation (Dale and Reiter, 1995). The second is a sentence selection task inspired by coherence tests from Barzilay and Lapata (2008). Our model outperforms strong baselines on both tasks. We further conduct a human evaluation in which our model’s generated sentences are compared to a strong baseline model. This evaluation elucidates strengths and weaknesses of our model and offers guidance for future work on narrative text generation. 2 Model Description We propose an entity-based generation model (E N G EN)1 that combines three different sources of contextual information for text generation: 1. The content that has already been generated within the current sentence 2. The content that was generated in the previous sentenc"
N18-1204,J92-4003,0,0.497539,"Missing"
N18-1204,D16-1245,0,0.0545598,"Missing"
N18-1204,P16-1061,0,0.0231737,"nd entity cluster only. Across all the evaluation measures, E N G EN gives the highest MAP numbers. Recall that S2SA does not have a component for entity prediction, therefore we only compare it with E N G EN in the mention only case. The difference between line 4 and line 2 on the mention only column shows the benefit of adding entity representations for text generation. The difference between lines 3 and 4 shows that local context also gives a small boost. Although the distance between the current slot and previous entity mention has been shown as a useful feature in coreference resolution (Clark and Manning, 2016b), line 1 shows distance alone is not an effective heuristic for mention generation. 6 Experiment: Pairwise Sentence Selection The sentence selection task is inspired by tests of coherence used to assess text generation components automatically, without human evaluation (Barzilay and Lapata, 2008). It serves as a sanity check, as it was conducted prior to full generation and human evaluations (§7). Since the models under consideration are generative, they can be used to assign scores to candidate sentences, given a context. In our version of this task, we provide a model with n − 1 = 49 sente"
N18-1204,P09-4003,0,0.0190166,"of the mention generation task is related to cloze tests like the Children’s Book Test (Hill et al., 2016), the “Who-didWhat” Test (Onishi et al., 2016), and the CNN and Daily Mail test described by Hermann et al. (2015). However, unlike these tests, we predict all entity mentions in the text and from a dynamically expanding candidate list, typically much larger than those in other cloze tests. Story generation Work in story generation has incorporated structure and context through event representations (Martin et al., 2017) or semantic representations, like story graphs (Rishes et al., 2013; Elson and McKeown, 2009). In this work, we provide evidence for the value of entity representations as an additional form of structure, following work by Walker et al. (2011), Cavazza and Charles (2005), and Cavazza et al. (2002). 9 Conclusion Inspired by Centering Theory and the importance of characters in stories, we propose a neural model for text generation that incorporates context via entities. We found that combining entity representations with representations of the previous sentence and the hidden state (from a neural language model) improves performance on three tasks: mention generation, sentence selection"
N18-1204,W15-4627,0,0.0567439,"Missing"
N18-1204,J95-2003,0,0.799659,"Missing"
N18-1204,W11-2123,0,0.0355728,"t that the evaluator saw, not all 50 sentences from the original segment as in earlier experiments. For each context, we randomly sampled a sentence to continue the document, using each of two models: E N G EN and S2SA. These two models allowed us to see if adding the entity information noticeably improved the quality of the generation to evaluators. Initial experiments showed that fluency remains a problem for neural text generation. To reduce the effect of fluency on Turkers’ judgments, we generated 100 samples for each context/model pair and then reranked them with a 5-gram language model (Heafield, 2011) that was trained on the same training data. The two top ranked sentences (one for E N G EN and one for S2SA) were presented in random order and without reference to the models that generated them. For each of the 50 contexts, we had 11 Turkers pick a candidate sentence to continue the story passage. Turkers were paid $0.10 for each evaluation they completed. In total, 93 Turkers completed the task. The number of passages Turkers completed ranged from 1 to all 50 story segments (with an average of 6.1). While the quantitative portion of this task would be easy to scale, the qualitative portion"
N18-1204,D17-1195,1,0.80855,"and ht−1,j for coherence in text generation. In §2.3, we will combine this with ht,i for predicting the next word; we refer to that model as S2SA, and it serves as an entity-unaware baseline in our experiments. 2.2 Context from Entities In S2SA, the context of a sentence is (at best) represented by compressing information about the words that have appeared in the previous sentence. Past research has suggested several approaches to capturing other contextual information. For example, Lau et al. (2017) and Ghosh et al. (2016) have sought to capture longer contexts by modeling topics. Recently, Ji et al. (2017) introduced a language model, E NTITY NLM, that adds explicit tracking of entities, which have their own representations that are updated as the document progresses.2 That model was introduced for analysis tasks, such as language modeling and coreference resolution, where the texts (and their coreference information) are given, and the model is used to score the texts to help resolve coreference relationships.3 E NTITY NLM’s strong performance on language modeling suggests the potential of distributed entity representations as another source of contextual information for text generation. Inspi"
N18-1204,P14-1062,0,0.0268828,"city, we choose a combination function without any extra parameters, and leave the detailed investigation of paramaterized composition functions as future work. We use a max-pooling function to form a context vector ct with the same dimensionality as ht−1 (and, of course, pt and ecurrent ). Specifically, at time step t, each element of the combined context vector ct is calculated as follows. For k ∈ {1, ..., |ct |}, ct [k] = max(ht−1 [k], pt [k], ecurrent [k]). (5) The max pooling technique originates from the design of convolutional neural networks and has been found useful elsewhere in NLP (Kalchbrenner et al., 2014). Other alternatives, including average pooling, min pooling, and element-wise multiplication on all three vectors, were considered in informal preliminary experiments on development data and found less effective than max pooling. This combined context vector ct is used to generate word wt by calculating the probability of each word type in the vocabulary. We use a classfactored softmax function (Goodman, 2001; Baltescu and Blunsom, 2015). This choice greatly reduces the runtime of word prediction. In practice, we often find it gives better performance than standard softmax. 2.4 Learning The t"
N18-1204,D16-1032,0,0.0284881,"al., 1995). Recently, the E NTI TY NLM proposed by Ji et al. (2017) shows that adding entity related information can improve the performance of language modeling, which potentially provides a method for entity related text generation. We build on E NTITY NLM, combining entity context with previous-sentence context, and demonstrate the importance of the latter in a coherence test (§6). The max pooling combination we propose is simple but effective. Another line of related work on recipe generation included special treatment of entities as candidates in generating sentences, but not as context (Kiddon et al., 2016). Bosselut et al. (2018) also generated recipes, using neural process networks to track and update entity representations with the goal of modeling actions and their causal effects on entities. However, the entity representations are frozen during generation, rather than dynamically updated. Mention generation Our novel mention generation task is inspired by both referring expression generation (Dale and Reiter, 1995) and entity prediction (Modi et al., 2017). The major difference is that, unlike referring expression generation, our 2257 task includes all the mentions used for entities, includ"
N18-1204,J12-1006,0,0.0458028,"Missing"
N18-1204,Q17-1003,0,0.0826604,"t training time, it may still be helpful. For all experiments, the same preprocessed dataset and trained models were used. The best models were selected based on development set log likelihood (Equation 6). 5 Experiment: Mention Generation The goal of our first experiment is to investigate each model’s capacity to mention an entity in context. For example, in Figure 1, Emily and her are both possible mentions of E MILY’s character, but the two cannot be used interchangeably. Inspired by early work on referring expression generation (Dale and Reiter, 1995) and recent work on entity prediction (Modi et al., 2017), we propose a new task we call mention generation. Given a text and a slot to be filled with an entity mention, a model must choose among all preceding entity mentions and the correct mention. So if the model was choosing the next entity mention to be generated in Figure 1, it would select between all the previous entity mentions (Emily, the dragon, Seth, and her) and the correct mention (she). In our model, each candidate mention is augmented with the index of its entity. Therefore, performing well on this task requires choosing both the entity and the words used to refer to it; this notion"
N18-1204,D16-1241,0,0.023793,"red by both referring expression generation (Dale and Reiter, 1995) and entity prediction (Modi et al., 2017). The major difference is that, unlike referring expression generation, our 2257 task includes all the mentions used for entities, including pronouns; we believe it is a more realistic test of a model’s handling of entities. Krahmer and Van Deemter (2012) give a comprehensive survey on early work of referring expression generation. The mention only version of the mention generation task is related to cloze tests like the Children’s Book Test (Hill et al., 2016), the “Who-didWhat” Test (Onishi et al., 2016), and the CNN and Daily Mail test described by Hermann et al. (2015). However, unlike these tests, we predict all entity mentions in the text and from a dynamically expanding candidate list, typically much larger than those in other cloze tests. Story generation Work in story generation has incorporated structure and context through event representations (Martin et al., 2017) or semantic representations, like story graphs (Rishes et al., 2013; Elson and McKeown, 2009). In this work, we provide evidence for the value of entity representations as an additional form of structure, following work b"
N18-1204,W12-4501,0,0.0889466,"Missing"
N18-1204,N15-1020,1,0.929001,"mentions. Mentions marked with the same number refer to the same entity. The goal is to continue the story in a coherent way. The actual story reads, “Seth yelled at her to get back but she ignored him.” Introduction We consider the problem of automatically generating narrative text, a challenging problem at the junction of computational creativity and language technologies (Gerv´as, 2009). We are motivated in particular by potential applications in personalized education and assistive tools for human authors, though we believe narrative might also play a role in social conversational agents (Sordoni et al., 2015). In this work, the term “narrative text” refers primarily to fiction but might also include news and other kinds of stories. A notable difference between longstanding work in natural language generation and recent “neural” models is in the treatment of entities and the words used to refer to them. Particularly in the generation of narrative text, character-centered generation has been shown important in character dialogue generation (Walker et al., 2011; Cavazza and Charles, 2005) and story planning (Cavazza et al., 2002). Neural models, on the other hand, treat mentions as just more words, r"
N18-2017,D15-1075,1,0.69331,". This suggests that, despite recently reported progress, natural language inference remains an open problem. Introduction Natural language inference (NLI; also known as recognizing textual entailment, or RTE) is a widely-studied task in natural language processing, to which many complex semantic tasks, such as question answering and text summarization, can be reduced (Dagan et al., 2006). Given a pair of sentences, a premise p and a hypothesis h, the goal is to determine whether or not p semantically entails h. The problem of acquiring large amounts of labeled inference data was addressed by Bowman et al. (2015), who devised a method for crowdsourcing high-agreement entailment annotations en masse, creating the SNLI and later the genrediverse MultiNLI (Williams et al., 2018) datasets. In this process, crowd workers are presented with F h is definitely true given p h might be true given p h is definitely not true given p 2 Annotation Artifacts are Common We conjecture that the framing of the annotation task has a significant effect on the language generation choices that crowd workers make when authoring hypotheses, producing certain patterns in the data. We call these patterns annotation artifacts. T"
N18-2017,P17-2097,0,0.0400512,"ICK dataset (Marelli et al., 2014). Chen et al. (2016) revealed problems with the CNN/DailyMail dataset (Hermann et al., 2015) which resulted from apply4 The MultiNLI models were trained on MultiNLI data alone (as opposed to a blend of MultiNLI and SNLI data). 5 github.com/allenai/allennlp 6 github.com/nyu-mll/multiNLI 7 goo.gl/kCeZXm 110 ing automatic tools for annotation. Levy and Dagan (2016) showed that a relation inference benchmark (Zeichner et al., 2012) is severely biased towards distributional methods, since it was created using DIRT (Lin and Pantel, 2001). Schwartz et al. (2017) and Cai et al. (2017) showed that certain biases are prevalent in the ROC stories cloze task (Mostafazadeh et al., 2016), which allow models trained on the endings alone, and not the story prefix, to yield state-of-the-art results. Rudinger et al. (2017) revealed that elicited hypotheses in SNLI contain evidence of various gender, racial, religious, and aged-based stereotypes. In parallel to this work, Poliak et al. (2018) uncovered similar annotation biases across multiple NLI datasets. Indeed, annotation artifacts are not unique to the NLI datasets, and the danger of such biases should be carefully considered wh"
N18-2017,P16-1223,0,0.0889145,"Missing"
N18-2017,N18-2123,0,0.0357975,"Missing"
N18-2017,D17-1070,0,0.0753495,"Missing"
N18-2017,N16-1098,0,0.0292386,"yMail dataset (Hermann et al., 2015) which resulted from apply4 The MultiNLI models were trained on MultiNLI data alone (as opposed to a blend of MultiNLI and SNLI data). 5 github.com/allenai/allennlp 6 github.com/nyu-mll/multiNLI 7 goo.gl/kCeZXm 110 ing automatic tools for annotation. Levy and Dagan (2016) showed that a relation inference benchmark (Zeichner et al., 2012) is severely biased towards distributional methods, since it was created using DIRT (Lin and Pantel, 2001). Schwartz et al. (2017) and Cai et al. (2017) showed that certain biases are prevalent in the ROC stories cloze task (Mostafazadeh et al., 2016), which allow models trained on the endings alone, and not the story prefix, to yield state-of-the-art results. Rudinger et al. (2017) revealed that elicited hypotheses in SNLI contain evidence of various gender, racial, religious, and aged-based stereotypes. In parallel to this work, Poliak et al. (2018) uncovered similar annotation biases across multiple NLI datasets. Indeed, annotation artifacts are not unique to the NLI datasets, and the danger of such biases should be carefully considered when annotating new datasets. to use them for evaluating NLI models (in addition to the original benc"
N18-2017,D16-1244,0,0.193867,"Missing"
N18-2017,S18-2023,0,0.153649,"Missing"
N18-2017,D16-1264,0,0.144407,"Missing"
N18-2017,W17-1609,0,0.0464368,"Missing"
N18-2017,D17-1215,0,0.24022,"Missing"
N18-2017,K17-1004,1,0.376046,"f discourse markers such as because. Once again, we observe that the example from the SNLI annotation guidelines does just that, by adding the purpose clause to catch a stick (Table 3). Contradiction. Negation words such as nobody, no, never and nothing are strong indicators of contradiction.3 Other (non-negative) words appear to be part of heuristics for contradicting whatever information is displayed in the premise; sleeping contradicts any activity, and naked (further down the list) contradicts any description of clothing. 3 Similar findings were observed in the ROC story cloze annotation (Schwartz et al., 2017). 109 Model DAM ESIM DIIN Full SNLI Hard Easy MultiNLI Matched Full Hard Easy MultiNLI Mismatched Full Hard Easy 84.7 85.8 86.5 69.4 71.3 72.7 72.0 74.1 77.0 72.1 73.1 76.5 92.4 92.6 93.4 55.8 59.3 64.1 85.3 86.2 87.6 56.2 58.9 64.4 85.7 85.2 86.8 Table 5: Performance of high-performing NLI models on the full, Hard, and Easy NLI test sets. 4 Re-evaluating NLI Models not be as straightforward to eliminate annotation artifacts once the dataset has been collected. First, after removing the Easy examples, Hard examples might not necessarily be artifact-free. For instance, removing all contradictin"
N18-2017,E17-2068,0,0.0173405,"9 35.2 52.3 Table 2: Performance of a premise-oblivious text classifier on NLI. The MultiNLI benchmark contains two test sets: matched (in-domain examples) and mismatched (out-of-domain examples). A majority baseline is presented for reference. 3.1 To see whether the use of certain words is indicative of the inference class, we compute the pointwise mutual information (PMI) between each word and class in the training set: To determine the degree to which such artifacts exist, we train a model to predict the label of a given hypothesis without seeing the premise. Specifically, we use fastText (Joulin et al., 2017), an off-the-shelf text classifier that models text as a bag of words and bigrams, to predict the entailment label of the hypothesis.1 This classifier is completely oblivious to the premise. Table 2 shows that a significant portion of each test set can be correctly classified without looking at the premise, well beyond the most-frequentclass baseline.2 Our finding demonstrates that it is possible to perform well on these datasets without modeling natural language inference. 3 Lexical Choice PMI(word, class) = log p(word, class) p(word, ·)p(·, class) We apply add-100 smoothing to the raw statis"
N18-2017,N18-1101,1,0.645347,"wn as recognizing textual entailment, or RTE) is a widely-studied task in natural language processing, to which many complex semantic tasks, such as question answering and text summarization, can be reduced (Dagan et al., 2006). Given a pair of sentences, a premise p and a hypothesis h, the goal is to determine whether or not p semantically entails h. The problem of acquiring large amounts of labeled inference data was addressed by Bowman et al. (2015), who devised a method for crowdsourcing high-agreement entailment annotations en masse, creating the SNLI and later the genrediverse MultiNLI (Williams et al., 2018) datasets. In this process, crowd workers are presented with F h is definitely true given p h might be true given p h is definitely not true given p 2 Annotation Artifacts are Common We conjecture that the framing of the annotation task has a significant effect on the language generation choices that crowd workers make when authoring hypotheses, producing certain patterns in the data. We call these patterns annotation artifacts. These authors contributed equally to this work. 107 Proceedings of NAACL-HLT 2018, pages 107–112 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Compu"
N18-2017,S14-2055,0,0.0249089,"that such models rely heavily on annotation artifacts in the hypothesis to make their predictions. A natural question to ask is whether it is possible to select a set of NLI training and test samples which do not contain easy-to-exploit artifacts. One solution might be to filter Easy examples from the training set, retaining only Hard examples. However, initial experiments suggest that it might 5 Discussion We reflect on our results and relate them to other work that also analyzes annotation artifacts in NLP datasets, drawing three main conclusions. Many datasets contain annotation artifacts. Lai and Hockenmaier (2014) demonstrated that lexical features such as the presence of negation, word overlap, and hypernym relations are highly predictive of entailment classes in the SICK dataset (Marelli et al., 2014). Chen et al. (2016) revealed problems with the CNN/DailyMail dataset (Hermann et al., 2015) which resulted from apply4 The MultiNLI models were trained on MultiNLI data alone (as opposed to a blend of MultiNLI and SNLI data). 5 github.com/allenai/allennlp 6 github.com/nyu-mll/multiNLI 7 goo.gl/kCeZXm 110 ing automatic tools for annotation. Levy and Dagan (2016) showed that a relation inference benchmark"
N18-2017,P12-2031,0,0.0251786,"emonstrated that lexical features such as the presence of negation, word overlap, and hypernym relations are highly predictive of entailment classes in the SICK dataset (Marelli et al., 2014). Chen et al. (2016) revealed problems with the CNN/DailyMail dataset (Hermann et al., 2015) which resulted from apply4 The MultiNLI models were trained on MultiNLI data alone (as opposed to a blend of MultiNLI and SNLI data). 5 github.com/allenai/allennlp 6 github.com/nyu-mll/multiNLI 7 goo.gl/kCeZXm 110 ing automatic tools for annotation. Levy and Dagan (2016) showed that a relation inference benchmark (Zeichner et al., 2012) is severely biased towards distributional methods, since it was created using DIRT (Lin and Pantel, 2001). Schwartz et al. (2017) and Cai et al. (2017) showed that certain biases are prevalent in the ROC stories cloze task (Mostafazadeh et al., 2016), which allow models trained on the endings alone, and not the story prefix, to yield state-of-the-art results. Rudinger et al. (2017) revealed that elicited hypotheses in SNLI contain evidence of various gender, racial, religious, and aged-based stereotypes. In parallel to this work, Poliak et al. (2018) uncovered similar annotation biases across"
N18-2017,P16-2041,1,0.826417,"atasets contain annotation artifacts. Lai and Hockenmaier (2014) demonstrated that lexical features such as the presence of negation, word overlap, and hypernym relations are highly predictive of entailment classes in the SICK dataset (Marelli et al., 2014). Chen et al. (2016) revealed problems with the CNN/DailyMail dataset (Hermann et al., 2015) which resulted from apply4 The MultiNLI models were trained on MultiNLI data alone (as opposed to a blend of MultiNLI and SNLI data). 5 github.com/allenai/allennlp 6 github.com/nyu-mll/multiNLI 7 goo.gl/kCeZXm 110 ing automatic tools for annotation. Levy and Dagan (2016) showed that a relation inference benchmark (Zeichner et al., 2012) is severely biased towards distributional methods, since it was created using DIRT (Lin and Pantel, 2001). Schwartz et al. (2017) and Cai et al. (2017) showed that certain biases are prevalent in the ROC stories cloze task (Mostafazadeh et al., 2016), which allow models trained on the endings alone, and not the story prefix, to yield state-of-the-art results. Rudinger et al. (2017) revealed that elicited hypotheses in SNLI contain evidence of various gender, racial, religious, and aged-based stereotypes. In parallel to this wo"
N18-2017,N15-1098,1,0.787039,"lopment of additional challenging benchmarks that expose the true performance levels of state-of-the-art NLI models. Acknowledgments This research was supported in part by the DARPA CwC program through ARO (W911NF15-1-0543) and a hardware gift from NVIDIA Corporation. SB acknowledges gift support from Google and Tencent Holdings and support from Samsung Research. References Aishwarya Agrawal, Dhruv Batra, and Devi Parikh. 2016. Analyzing the behavior of visual question answering models. In Proc. of EMNLP. https: //aclweb.org/anthology/D16-1203. Supervised models leverage annotation artifacts. Levy et al. (2015) demonstrated that supervised lexical inference models rely heavily on artifacts in the datasets, particularly the tendency of some words to serve as prototypical hypernyms. Agrawal et al. (2016); Jabri et al. (2016); Goyal et al. (2017) all showed that state-of-the-art visual question answering (Antol et al., 2015) systems leverage annotation biases in the dataset. Cirik et al. (2018) find that complex models for referring expression recognition achieve high performance without any text input. In parallel to this work, Dasgupta et al. (2018) found that the InferSent model (Conneau et al., 201"
N18-2017,D16-1203,0,\N,Missing
N18-2017,P18-1224,0,\N,Missing
N18-5020,P17-3020,0,0.0264314,"ational AI that has challenged researchers since 1990. Recent work has addressed tasks where passing the Turing test is not a concern. Goaloriented conversational systems facilitate natural user interaction with devices via text and spoken language. These AI assistants typically focus on short interactions, as in commercial products such as Amazon Alexa, Microsoft Cortana, Google Assistant, and Apple Siri. General conversational systems, called chatbots, have constrained social interaction capabilities but have difficulty generating conversations with long-term coherence (Serban et al., 2017; Sato et al., 2017; Shao et al., 2017; Tian et al., 2017; Ghazvininejad et al., 2018). The Alexa Prize sets forth a new challenge: creating a system that can hold a coherent and engaging conversation on current events and popular topics such as sports, politics, entertainment, fashion and technology (Ram et al., 2017). Our system, Sound1 Figure 1: A sample dialog. Suspected speech recognition errors in the user utterances are underlined. ing Board,2 demonstrates that it is feasible to build an agent that can engage in long-term conversation when backed by rich content and knowledge of the user obtained through"
N18-5020,D17-1235,0,0.0251686,"challenged researchers since 1990. Recent work has addressed tasks where passing the Turing test is not a concern. Goaloriented conversational systems facilitate natural user interaction with devices via text and spoken language. These AI assistants typically focus on short interactions, as in commercial products such as Amazon Alexa, Microsoft Cortana, Google Assistant, and Apple Siri. General conversational systems, called chatbots, have constrained social interaction capabilities but have difficulty generating conversations with long-term coherence (Serban et al., 2017; Sato et al., 2017; Shao et al., 2017; Tian et al., 2017; Ghazvininejad et al., 2018). The Alexa Prize sets forth a new challenge: creating a system that can hold a coherent and engaging conversation on current events and popular topics such as sports, politics, entertainment, fashion and technology (Ram et al., 2017). Our system, Sound1 Figure 1: A sample dialog. Suspected speech recognition errors in the user utterances are underlined. ing Board,2 demonstrates that it is feasible to build an agent that can engage in long-term conversation when backed by rich content and knowledge of the user obtained through interaction. Soundi"
N18-5020,P17-2036,0,0.0298266,"hers since 1990. Recent work has addressed tasks where passing the Turing test is not a concern. Goaloriented conversational systems facilitate natural user interaction with devices via text and spoken language. These AI assistants typically focus on short interactions, as in commercial products such as Amazon Alexa, Microsoft Cortana, Google Assistant, and Apple Siri. General conversational systems, called chatbots, have constrained social interaction capabilities but have difficulty generating conversations with long-term coherence (Serban et al., 2017; Sato et al., 2017; Shao et al., 2017; Tian et al., 2017; Ghazvininejad et al., 2018). The Alexa Prize sets forth a new challenge: creating a system that can hold a coherent and engaging conversation on current events and popular topics such as sports, politics, entertainment, fashion and technology (Ram et al., 2017). Our system, Sound1 Figure 1: A sample dialog. Suspected speech recognition errors in the user utterances are underlined. ing Board,2 demonstrates that it is feasible to build an agent that can engage in long-term conversation when backed by rich content and knowledge of the user obtained through interaction. Sounding Board won the in"
N19-1112,E17-2039,0,0.0259669,"Missing"
N19-1112,J99-2004,0,0.205453,"dependently for each token (Belinkov et al., 2017a,b; Blevins et al., 2018, inter alia). We synthesize these disparate studies and build upon them by proposing additional probing tasks. The part-of-speech tagging (POS) task assesses whether CWRs capture basic syntax. We experiment with two standard datasets: the Penn Treebank (PTB; Marcus et al., 1993) and the Universal Dependencies English Web Treebank (UDEWT; Silveira et al., 2014). The CCG supertagging (CCG) task assesses the vectors’ fine-grained information about the syntactic roles of words in context. It is considered “almost parsing” (Bangalore and Joshi, 1999), since a sequence of supertags maps a sentence to a small set of possible parses. We use CCGbank (Hockenmaier and Steedman, 2007), a conversion of the PTB into CCG derivations. The syntactic constituency ancestor tagging tasks are designed to probe the vectors’ knowledge of hierarchical syntax. For a given word, the probing model is trained to predict the constituent la2 http://nelsonliu.me/papers/ contextual-repr-analysis 1074 where a prediction is made only for tokens corresponding to events (rather than every token in a sequence). Performance is measured using Pearson correlation (r); we r"
N19-1112,P17-1080,1,0.922231,"). CWRs are extraordinarily effective—using them in place of traditional static word vectors within the latest models leads to large gains across a variety of NLP tasks. The broad success of CWRs indicates that they encode useful, transferable features of language. However, their linguistic knowledge and transferability are not yet well understood. Recent work has explored the linguistic knowledge captured by language models and neural machine translation systems, but these studies often focus on a single phenomenon, e.g., knowledge of hierarchical syntax (Blevins et al., 2018) or morphology (Belinkov et al., 2017a). We extend prior work by studying CWRs with a diverse set of sixteen probing tasks designed to assess a wide array of phenomena, such as coreference, knowledge of semantic relations, and entity information, among 1073 Proceedings of NAACL-HLT 2019, pages 1073–1094 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics others. The result is a broader view of the linguistic knowledge encoded within CWRs. With respect to transferability, pretraining contextualizers on the language modeling task has had the most empirical success, but we can also conside"
N19-1112,I17-1001,1,0.899042,"Missing"
N19-1112,C16-1333,0,0.0322078,"ion task. Prepositions are marked by boldface, immediately followed by their labeled function. If applicable, ; precedes the preposition’s labeled role. Figure reproduced from Schneider et al. (2018). bel of its parent (Parent), grandparent (GParent), or great-grandparent (GGParent) in the phrasestructure tree (from the PTB). In the semantic tagging task (ST), tokens are assigned labels that reflect their semantic role in context. These semantic tags assess lexical semantics, and they abstract over redundant POS distinctions and disambiguate useful cases within POS tags. We use the dataset of Bjerva et al. (2016); the tagset has since been developed as part of the Parallel Meaning Bank (Abzianidze et al., 2017). Preposition supersense disambiguation is the task of classifying a preposition’s lexical semantic contribution (the function; PS-fxn) and the semantic role or relation it mediates (the role; PSrole). This task is a specialized kind of word sense disambiguation, and examines one facet of lexical semantic knowledge. In contrast to the tagging tasks above, the model is trained and evaluated on single-token prepositions (rather than making a decision for every token in a sequence). We use the STRE"
N19-1112,P18-2003,0,0.156618,"nguage modeling (Peters et al., 2018a). CWRs are extraordinarily effective—using them in place of traditional static word vectors within the latest models leads to large gains across a variety of NLP tasks. The broad success of CWRs indicates that they encode useful, transferable features of language. However, their linguistic knowledge and transferability are not yet well understood. Recent work has explored the linguistic knowledge captured by language models and neural machine translation systems, but these studies often focus on a single phenomenon, e.g., knowledge of hierarchical syntax (Blevins et al., 2018) or morphology (Belinkov et al., 2017a). We extend prior work by studying CWRs with a diverse set of sixteen probing tasks designed to assess a wide array of phenomena, such as coreference, knowledge of semantic relations, and entity information, among 1073 Proceedings of NAACL-HLT 2019, pages 1073–1094 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics others. The result is a broader view of the linguistic knowledge encoded within CWRs. With respect to transferability, pretraining contextualizers on the language modeling task has had the most empir"
N19-1112,P18-1246,0,0.0610796,"Missing"
N19-1112,P18-1008,0,0.0289435,"middle layers show varying performance. Across all models, the representations that are better-suited for language modeling are also those that exhibit worse probing task performance (Figure 3), indicating that contextualizer layers trade off between encoding general and task-specific features. These results also reveal a difference in the layerwise behavior of LSTMs and transformers; moving up the LSTM layers yields more taskspecific representations, but the same does not hold for transformers. Better understanding the differences between transformers and LSTMs is an active area of research (Chen et al., 2018; Tang et al., 2018), and we leave further exploration of these observations to future work. These observations motivate the gradual unfreezing method of Howard and Ruder (2018), where the model layers are progressively unfrozen (starting from the final layer) during the finetuning process. Given our observation that higherlevel LSTM layers are less general (and more pretraining task-specific), they likely have to be finetuned a bit more in order to make them appropriately task specific. Meanwhile, the base layer of the LSTM already learns highly transferable features, and may not benefit from"
N19-1112,W18-2501,1,0.861441,"Missing"
N19-1112,D17-1206,0,0.0593364,"Missing"
N19-1112,N16-1026,0,0.0599268,"Missing"
N19-1112,Q16-1037,0,0.0737692,"a variety of other methods to study the learned representations in neural models, such as directly examining the activations of individual neurons (Karpathy et al., 2015; Li et al., 2015; Shi et al., 2016a, inter alia), ablating components of the model and dataset (Kuncoro et al., 2017; Gaddy et al., 2018; Khandelwal et al., 2018), or interpreting attention mechanisms (Bahdanau et al., 2015); see Belinkov and Glass (2019) for a recent survey. One particularly relevant line of work involves the construction of synthetic tasks that a model can only solve if it captures a particular phenomenon (Linzen et al., 2016; Jumelet and Hupkes, 2018; Wilcox et al., 2018; Futrell and Levy, 2019, inter alia). Zhang and Bowman (2018) compare the syntactic knowledge of language models and neural machine translation systems. We widen the range of pretraining tasks and target probing model tasks to gain a more complete picture. We also focus on a stronger contextualizer architecture, ELMo (original), that has produced state-of-the-art results. Several studies have sought to intrinsically evaluate noncontextual word representations with word similarity tasks, such as analogies (Mikolov et al., 2013). These methods diff"
N19-1112,J93-2004,0,0.0674375,"ork in probing the contents of representations.2 See Appendix A for details about task setup. 2.1 Token Labeling The majority of past work in probing the internal representations of neural models has examined various token labeling tasks, where a decision is made independently for each token (Belinkov et al., 2017a,b; Blevins et al., 2018, inter alia). We synthesize these disparate studies and build upon them by proposing additional probing tasks. The part-of-speech tagging (POS) task assesses whether CWRs capture basic syntax. We experiment with two standard datasets: the Penn Treebank (PTB; Marcus et al., 1993) and the Universal Dependencies English Web Treebank (UDEWT; Silveira et al., 2014). The CCG supertagging (CCG) task assesses the vectors’ fine-grained information about the syntactic roles of words in context. It is considered “almost parsing” (Bangalore and Joshi, 1999), since a sequence of supertags maps a sentence to a small set of possible parses. We use CCGbank (Hockenmaier and Steedman, 2007), a conversion of the PTB into CCG derivations. The syntactic constituency ancestor tagging tasks are designed to probe the vectors’ knowledge of hierarchical syntax. For a given word, the probing m"
N19-1112,S15-2153,0,0.101468,"Missing"
N19-1112,D14-1162,0,0.0883484,"and what makes contextual word representations transferable, we compare language model pretraining with eleven supervised pretraining tasks. For any given task, pretraining on a closely related task yields better performance than language model pretraining (which is better on average) when the pretraining dataset is fixed. However, language model pretraining on more data gives the best results. 1 Figure 1: An illustration of the probing model setup used to study the linguistic knowledge within contextual word representations. Introduction Pretrained word representations (Mikolov et al., 2013; Pennington et al., 2014) are a key component of state-of-the-art neural NLP models. Traditionally, these word vectors are static—a single * Work done while at the Allen Institute for Artificial Intelligence. vector is assigned to each word. Recent work has explored contextual word representations (henceforth: CWRs), which assign each word a vector that is a function of the entire input sequence; this enables them to model the use of words in context. CWR s are typically the outputs of a neural network (which we call a contextualizer) trained on tasks with large datasets, such as machine translation (McCann et al., 20"
N19-1112,W19-4302,1,0.768829,"k, since such information is learnable by a task-specific contextualizer. This analysis also reveals insights about contextualizer fine-tuning, which seeks to specialize the CWRs for an end task (Howard and Ruder, 2018; Radford et al., 2018; Devlin et al., 2018). Our results confirm that task-trained contextualization is important when the end task requires specific information that may not be captured by the pretraining task (§4). However, such end-task– specific contextualization can come from either fine-tuning CWRs or using fixed output features as inputs to a task-trained contextualizer; Peters et al. (2019) begins to explore when each approach should be applied. 5 Analyzing Layerwise Transferability We quantify the transferability of CWRs by how well they can do on the range of probing tasks— representations that are more transferable will perform better than alternatives across tasks. When analyzing the representations produced by each layer of pretrained contextualizers, we observe marked patterns in layerwise transferability (Figure 3). The first layer of contextualization in recurrent models (original and 4-layer ELMo) is consistently the most transferable, even outperforming a scalar mix of"
N19-1112,N18-1202,1,0.897768,"te-of-the-art neural NLP models. Traditionally, these word vectors are static—a single * Work done while at the Allen Institute for Artificial Intelligence. vector is assigned to each word. Recent work has explored contextual word representations (henceforth: CWRs), which assign each word a vector that is a function of the entire input sequence; this enables them to model the use of words in context. CWR s are typically the outputs of a neural network (which we call a contextualizer) trained on tasks with large datasets, such as machine translation (McCann et al., 2017) and language modeling (Peters et al., 2018a). CWRs are extraordinarily effective—using them in place of traditional static word vectors within the latest models leads to large gains across a variety of NLP tasks. The broad success of CWRs indicates that they encode useful, transferable features of language. However, their linguistic knowledge and transferability are not yet well understood. Recent work has explored the linguistic knowledge captured by language models and neural machine translation systems, but these studies often focus on a single phenomenon, e.g., knowledge of hierarchical syntax (Blevins et al., 2018) or morphology"
N19-1112,D18-1179,1,0.89357,"te-of-the-art neural NLP models. Traditionally, these word vectors are static—a single * Work done while at the Allen Institute for Artificial Intelligence. vector is assigned to each word. Recent work has explored contextual word representations (henceforth: CWRs), which assign each word a vector that is a function of the entire input sequence; this enables them to model the use of words in context. CWR s are typically the outputs of a neural network (which we call a contextualizer) trained on tasks with large datasets, such as machine translation (McCann et al., 2017) and language modeling (Peters et al., 2018a). CWRs are extraordinarily effective—using them in place of traditional static word vectors within the latest models leads to large gains across a variety of NLP tasks. The broad success of CWRs indicates that they encode useful, transferable features of language. However, their linguistic knowledge and transferability are not yet well understood. Recent work has explored the linguistic knowledge captured by language models and neural machine translation systems, but these studies often focus on a single phenomenon, e.g., knowledge of hierarchical syntax (Blevins et al., 2018) or morphology"
N19-1112,W12-4501,0,0.0474045,"arsing, which score pairs of CWRs to make head attachment and arc labeling decisions (Dozat and Manning, 2016, 2018). To generate negative examples for the dependency arc prediction tasks, we take each positive example (whead , wmod ) and generate a new negative example (wrand , wmod ). wrand is a random token in the sentence that is not the head of wmod . Thus, the datasets used in these tasks are balanced. We also consider a coreference arc prediction task, where the model is trained to predict whether two entities corefer from their CWRs. We use the dataset from the CoNLL 2012 shared task (Pradhan et al., 2012). To generate negative examples, we follow a similar procedure as the dependency arc prediction tasks: given a positive example (wa , wb ), where wb occurs after wa and the two tokens share a coreference cluster, we create a negative example (wrandom entity , wb ), where wrandom entity is a token that occurs before wb and belongs to a different coreference cluster. 3 Models Probing Model We use a linear model as our probing model; limiting its capacity enables us to focus on what information can be easily extracted from CWRs. See Appendix B for probing model training hyperparameters and other"
N19-1112,N19-1162,0,0.032501,"yzing Layerwise Transferability We quantify the transferability of CWRs by how well they can do on the range of probing tasks— representations that are more transferable will perform better than alternatives across tasks. When analyzing the representations produced by each layer of pretrained contextualizers, we observe marked patterns in layerwise transferability (Figure 3). The first layer of contextualization in recurrent models (original and 4-layer ELMo) is consistently the most transferable, even outperforming a scalar mix of layers on most tasks (see Appendix D for scalar mix results). Schuster et al. (2019) see the same trend in English dependency parsing. By contrast, transformer-based contextualizers have no single most-transferable layer; the best performing layer for each task varies, and is usually near the middle. Accordingly, a scalar mix of transformer layers outperforms the best individual layer on most tasks (see Appendix D). Pretraining encourages the model to encode pretraining-task–specific information; they learn transferable features incidentally. We hypothesize that this is an inherent trade-off—since these models used fixed-sized vector representations, taskspecificity comes at"
N19-1112,D16-1248,0,0.459106,"ter understanding the linguistic knowledge and transferability of CWRs is necessary for their principled enhancement through new encoder architectures and pretraining tasks that build upon their strengths or alleviate their weaknesses (Linzen, 2018). This paper asks and answers: 1. What features of language do these vectors capture, and what do they miss? (§4) 2. How and why does transferability vary across representation layers in contextualizers? (§5) 3. How does the choice of pretraining task affect the vectors’ learned linguistic knowledge and transferability? (§6) We use probing models1 (Shi et al., 2016b; Adi et al., 2017; Hupkes et al., 2018; Belinkov and Glass, 2019) to analyze the linguistic information within CWRs. Concretely, we generate features for words from pretrained contextualizers and train a model to make predictions from those features alone (Figure 1). If a simple model can be trained to predict linguistic information about a word (e.g., its part-of-speech tag) or a pair of words (e.g., their semantic relation) from the CWR (s) alone, we can reasonably conclude that the CWR (s) encode this information. Our analysis reveals interesting insights such as: 1. Linear models trained"
N19-1112,D16-1159,0,0.598727,"ter understanding the linguistic knowledge and transferability of CWRs is necessary for their principled enhancement through new encoder architectures and pretraining tasks that build upon their strengths or alleviate their weaknesses (Linzen, 2018). This paper asks and answers: 1. What features of language do these vectors capture, and what do they miss? (§4) 2. How and why does transferability vary across representation layers in contextualizers? (§5) 3. How does the choice of pretraining task affect the vectors’ learned linguistic knowledge and transferability? (§6) We use probing models1 (Shi et al., 2016b; Adi et al., 2017; Hupkes et al., 2018; Belinkov and Glass, 2019) to analyze the linguistic information within CWRs. Concretely, we generate features for words from pretrained contextualizers and train a model to make predictions from those features alone (Figure 1). If a simple model can be trained to predict linguistic information about a word (e.g., its part-of-speech tag) or a pair of words (e.g., their semantic relation) from the CWR (s) alone, we can reasonably conclude that the CWR (s) encode this information. Our analysis reveals interesting insights such as: 1. Linear models trained"
N19-1112,P11-1019,0,0.0388573,"ang and Buchholz, 2000). Named entity recognition (NER) examines whether CWRs encode information about entity types. We use the CoNLL 2003 shared task dataset (Tjong Kim Sang and De Meulder, 2003). Grammatical error detection (GED) is the task of identifying tokens which need to be edited in order to produce a grammatically correct sentence. Given that CWRs are extracted from models trained on large amounts of grammatical text, this task assesses whether embeddings encode features that indicate anomalies in their input (in this case, ungrammaticality). We use the First Certificate in English (Yannakoudakis et al., 2011) dataset, converted into sequence-labeling format by Rei and Yannakoudakis (2016). The conjunct identification (Conj) task challenges the model to identify the tokens that comprise the conjuncts in a coordination construction. Doing so requires highly specific syntactic knowledge. The data comes from the coordinationannotated PTB of Ficler and Goldberg (2016). 2.3 Pairwise Relations We also design probing tasks that examine whether relationships between words are encoded in CWRs. In these tasks, given a word pair w1 , w2 , we input [w1 , w2 , w1 w2 ] into the probing model; it is trained to pr"
N19-1112,N18-1089,0,0.0423259,"Missing"
N19-1112,W18-5448,0,0.0345799,"ning the activations of individual neurons (Karpathy et al., 2015; Li et al., 2015; Shi et al., 2016a, inter alia), ablating components of the model and dataset (Kuncoro et al., 2017; Gaddy et al., 2018; Khandelwal et al., 2018), or interpreting attention mechanisms (Bahdanau et al., 2015); see Belinkov and Glass (2019) for a recent survey. One particularly relevant line of work involves the construction of synthetic tasks that a model can only solve if it captures a particular phenomenon (Linzen et al., 2016; Jumelet and Hupkes, 2018; Wilcox et al., 2018; Futrell and Levy, 2019, inter alia). Zhang and Bowman (2018) compare the syntactic knowledge of language models and neural machine translation systems. We widen the range of pretraining tasks and target probing model tasks to gain a more complete picture. We also focus on a stronger contextualizer architecture, ELMo (original), that has produced state-of-the-art results. Several studies have sought to intrinsically evaluate noncontextual word representations with word similarity tasks, such as analogies (Mikolov et al., 2013). These methods differ from our approach in that they require no extra parameters and directly assess the vectors, while our prob"
N19-1112,silveira-etal-2014-gold,0,0.135261,"Missing"
N19-1112,D18-1458,0,0.0621572,"Missing"
N19-1112,W00-0726,0,0.0824119,"Decompositional Semantics It Happened v2 dataset (Rudinger et al., 2018), and the model is trained to predict a (non)factuality value in the range [−3, 3]. Unlike the tagging tasks above, this task is treated as a regression problem, 2.2 Segmentation Several of our probing tasks involve segmentation using BIO or IO tags. Here the model is trained to predict labels from only a single word’s CWR. Syntactic chunking (Chunk) tests whether CWR s contain notions of spans and boundaries; the task is to segment text into shallow constituent chunks. We use the CoNLL 2000 shared task dataset (Tjong Kim Sang and Buchholz, 2000). Named entity recognition (NER) examines whether CWRs encode information about entity types. We use the CoNLL 2003 shared task dataset (Tjong Kim Sang and De Meulder, 2003). Grammatical error detection (GED) is the task of identifying tokens which need to be edited in order to produce a grammatically correct sentence. Given that CWRs are extracted from models trained on large amounts of grammatical text, this task assesses whether embeddings encode features that indicate anomalies in their input (in this case, ungrammaticality). We use the First Certificate in English (Yannakoudakis et al., 2"
N19-1112,D15-1243,0,0.0265978,"slation systems. We widen the range of pretraining tasks and target probing model tasks to gain a more complete picture. We also focus on a stronger contextualizer architecture, ELMo (original), that has produced state-of-the-art results. Several studies have sought to intrinsically evaluate noncontextual word representations with word similarity tasks, such as analogies (Mikolov et al., 2013). These methods differ from our approach in that they require no extra parameters and directly assess the vectors, while our probing models must be trained. In this regard, our method is similar to QVEC (Tsvetkov et al., 2015). 8 Conclusion We study the linguistic knowledge and transferability of contextualized word representations with a suite of sixteen diverse probing tasks. The features generated by pretrained contextualizers are sufficient for high performance on a broad set of tasks. For tasks that require specific information not captured by the contextual word representation, we show that learning task-specific contextual features helps to encode the requisite knowledge. In addition, our analysis of patterns in the transferability of contextualizer layers shows that the lowest layer of LSTMs encodes the mos"
N19-1112,W18-5423,0,0.0214566,"d representations in neural models, such as directly examining the activations of individual neurons (Karpathy et al., 2015; Li et al., 2015; Shi et al., 2016a, inter alia), ablating components of the model and dataset (Kuncoro et al., 2017; Gaddy et al., 2018; Khandelwal et al., 2018), or interpreting attention mechanisms (Bahdanau et al., 2015); see Belinkov and Glass (2019) for a recent survey. One particularly relevant line of work involves the construction of synthetic tasks that a model can only solve if it captures a particular phenomenon (Linzen et al., 2016; Jumelet and Hupkes, 2018; Wilcox et al., 2018; Futrell and Levy, 2019, inter alia). Zhang and Bowman (2018) compare the syntactic knowledge of language models and neural machine translation systems. We widen the range of pretraining tasks and target probing model tasks to gain a more complete picture. We also focus on a stronger contextualizer architecture, ELMo (original), that has produced state-of-the-art results. Several studies have sought to intrinsically evaluate noncontextual word representations with word similarity tasks, such as analogies (Mikolov et al., 2013). These methods differ from our approach in that they require no ex"
N19-1112,D17-1197,0,0.019245,"WR do not capture much transfer3 See Appendix C for references to the previous state of the art (without pretraining). 4 For brevity, in this section we omit probing tasks that cannot be compared to prior work. See Appendix D for pretrained contextualizer performance for all layers and all tasks. able information about entities and coreference phenomena in their input (e.g., the NER results in Table 1 and the coreference arc prediction results in Appendix D). To alleviate this weakness, future work could augment pretrained contextualizers with explicit entity representations (Ji et al., 2017; Yang et al., 2017; Bosselut et al., 2017). Probing Failures While probing models are at or near state-of-the-art performance across a number of tasks, they also do not perform as well on several others, including NER, grammatical error detection, and conjunct identification. This may occur because (1) the CWR simply does not encode the pertinent information or any predictive correlates, or (2) the probing model does not have the capacity necessary to extract the information or predictive correlates from the vector. In the former case, learning task-specific contextual features might be necessary for encoding t"
N19-1112,S14-2008,0,\N,Missing
N19-1112,J12-2002,0,\N,Missing
N19-1112,W03-0419,0,\N,Missing
N19-1112,J12-2003,0,\N,Missing
N19-1112,J07-3004,0,\N,Missing
N19-1112,P16-1079,0,\N,Missing
N19-1112,P16-1112,0,\N,Missing
N19-1112,E17-1117,1,\N,Missing
N19-1112,P18-2077,0,\N,Missing
N19-1112,Q19-1004,1,\N,Missing
N19-1112,N19-1423,0,\N,Missing
N19-1112,N16-1082,0,\N,Missing
N19-1225,D18-1316,0,0.0245864,"tandard dataset (e.g., SQuAD) and “Challenge” refers to the challenge dataset (e.g., Adversarial SQuAD). Outcomes are discussed in Section 2. Introduction NLP research progresses through the construction of dataset-benchmarks and the development of systems whose performance on them can be fairly compared. A recent pattern involves challenges to benchmarks:1 manipulations to input data that result in severe degradation of system performance, but not human performance. These challenges have been used as evidence that current systems are brittle (Belinkov and Bisk, 2018; Mudrakarta et al., 2018; Zhao et al., 2018; Glockner et al., 2018; Ebrahimi et al., 2018; Ribeiro et al., 2018, 1 inter alia). For instance, Naik et al. (2018) generated natural language inference challenge data by applying simple textual transformations to existing examples from MultiNLI (Williams et al., 2018) and SNLI (Bowman et al., 2015). Similarly, Jia and Liang (2017) built an adversarial evaluation dataset for reading comprehension based on SQuAD (Rajpurkar et al., 2016). What should we conclude when a system fails on a challenge dataset? In some cases, a challenge might exploit blind spots in the design of the original datase"
N19-1225,P18-2006,0,0.0150295,"ge” refers to the challenge dataset (e.g., Adversarial SQuAD). Outcomes are discussed in Section 2. Introduction NLP research progresses through the construction of dataset-benchmarks and the development of systems whose performance on them can be fairly compared. A recent pattern involves challenges to benchmarks:1 manipulations to input data that result in severe degradation of system performance, but not human performance. These challenges have been used as evidence that current systems are brittle (Belinkov and Bisk, 2018; Mudrakarta et al., 2018; Zhao et al., 2018; Glockner et al., 2018; Ebrahimi et al., 2018; Ribeiro et al., 2018, 1 inter alia). For instance, Naik et al. (2018) generated natural language inference challenge data by applying simple textual transformations to existing examples from MultiNLI (Williams et al., 2018) and SNLI (Bowman et al., 2015). Similarly, Jia and Liang (2017) built an adversarial evaluation dataset for reading comprehension based on SQuAD (Rajpurkar et al., 2016). What should we conclude when a system fails on a challenge dataset? In some cases, a challenge might exploit blind spots in the design of the original dataset (dataset weakness). In others, the challenge"
N19-1225,W18-2501,1,0.884509,"Missing"
N19-1225,P18-2103,0,0.183604,"g., SQuAD) and “Challenge” refers to the challenge dataset (e.g., Adversarial SQuAD). Outcomes are discussed in Section 2. Introduction NLP research progresses through the construction of dataset-benchmarks and the development of systems whose performance on them can be fairly compared. A recent pattern involves challenges to benchmarks:1 manipulations to input data that result in severe degradation of system performance, but not human performance. These challenges have been used as evidence that current systems are brittle (Belinkov and Bisk, 2018; Mudrakarta et al., 2018; Zhao et al., 2018; Glockner et al., 2018; Ebrahimi et al., 2018; Ribeiro et al., 2018, 1 inter alia). For instance, Naik et al. (2018) generated natural language inference challenge data by applying simple textual transformations to existing examples from MultiNLI (Williams et al., 2018) and SNLI (Bowman et al., 2015). Similarly, Jia and Liang (2017) built an adversarial evaluation dataset for reading comprehension based on SQuAD (Rajpurkar et al., 2016). What should we conclude when a system fails on a challenge dataset? In some cases, a challenge might exploit blind spots in the design of the original dataset (dataset weakness). I"
N19-1225,N18-2017,1,0.895702,"Missing"
N19-1225,D17-1215,0,0.436908,"hat particular weaknesses they reveal. For example, a challenge dataset may be difficult because it targets phenomena that current models cannot capture, or because it simply exploits blind spots in a model’s specific training set. We introduce inoculation by fine-tuning, a new analysis method for studying challenge datasets by exposing models (the metaphorical patient) to a small amount of data from the challenge dataset (a metaphorical pathogen) and assessing how well they can adapt. We apply our method to analyze the NLI “stress tests” (Naik et al., 2018) and the Adversarial SQuAD dataset (Jia and Liang, 2017). We show that after slight exposure, some of these datasets are no longer challenging, while others remain difficult. Our results indicate that failures on challenge datasets may lead to very different conclusions about models, training datasets, and the challenge datasets themselves. 1 Figure 1: An illustration of the standard challenge evaluation procedure (e.g., Jia and Liang, 2017) and our proposed analysis method. “Original” refers to the a standard dataset (e.g., SQuAD) and “Challenge” refers to the challenge dataset (e.g., Adversarial SQuAD). Outcomes are discussed in Section 2. Introd"
N19-1225,C18-1198,0,0.180841,"Missing"
N19-1225,D15-1075,0,\N,Missing
N19-1225,D16-1244,0,\N,Missing
N19-1225,P17-1152,0,\N,Missing
N19-1225,P18-1079,0,\N,Missing
N19-1225,N18-1101,0,\N,Missing
N19-1392,Q16-1031,1,0.711637,"to produce multilingual CWR by training a single “polyglot” language model on text in multiple languages. As our work is a multilingual extension of ELMo (Peters et al., 2018), we call it Rosita (after a bilingual character from Sesame Street). Our hypothesis is that, although each language is unique, different languages manifest similar characteristics (e.g., morphological, lexical, syntactic) which can be exploited by training a single model with data from multiple languages (Ammar, 2016). Previous work has shown this to be true to some degree in the context of syntactic dependency parsing (Ammar et al., 2016), semantic role labeling (Mulcaire et al., 2018), named entity recognition (Xie et al., 2018), and language modeling for phonetic sequences (Tsvetkov et al., 2016) and for speech recognition (Ragni et al., 2016). Recently, de Lhoneux et al. (2018) showed that parameter sharing between languages can improve performance in dependency parsing, but the effect is variable, depending on the language pair and the parameter sharing strategy. Other recent work also reported that concatenating data from different languages can hurt performance in dependency parsing (Che et al., 2018). These mixed result"
N19-1392,Q17-1010,0,0.108191,"search/ bert/blob/master/multilingual.md). Our initial exploration of multilingual BERT models raised sufficient questions about preprocessing that we defer exploration to future work. 3913 4 The idea of this word-level initialization is to bias the model toward crosslingual sharing; because words with similar meanings have similar representations, the features that the model learns are expected to be at least partially language-agnostic. The word type embeddings used for these models, as well as elsewhere in the paper, are trained on our language model training set using the fastText method (Bojanowski et al., 2017), and target language vectors are aligned with the English ones using supervised MUSE5 (Conneau et al., 2018). See appendix for more LM training details. 3 Experiments All of our task models (UD, SRL, and NER) are implemented in AllenNLP, version 0.7.2 (Gardner et al., 2018).6 We generally follow the default hyperparameters and training schemes provided in the AllenNLP library regardless of language. See appendix for the complete list of our hyperparameters. For each task, we experiment with five types of word representations: in addition to the three language model types (M ONO C HAR, ROSI TA"
N19-1392,P16-1157,0,0.0546064,"Missing"
N19-1392,P14-5010,0,0.0030883,"a for SRL and NER consist of Simplified Chinese, we train separate language models for the two variants. For English we use text from the Billion Word Benchmark (Chelba et al., 2013), for Traditional Chinese, wiki and web data provided for the CoNLL 2017 Shared Task (Ginter et al., 2017), for Simplified Chinese, newswire text from Xinhua,2 2 catalog.ldc.upenn.edu/LDC95T13 and for Arabic, newswire text from AFP.3 We use approximately 60 million tokens of news and web text for each language. We tokenized the language model training data for English and Simplified Chinese using Stanford CoreNLP (Manning et al., 2014). The Traditional Chinese corpus was already pre-segmented by UDPipe (Ginter et al., 2017; Straka et al., 2016). We found that the Arabic vocabulary from AFP matched both the UD and Ontonotes data reasonably well without additional tokenization. We also processed all corpora to normalize punctuation and remove non-text. 2.2 Models and Training We base our language models on the ELMo method (Peters et al., 2018), which encodes each word with a character CNN, then processes the word in context with a word-level LSTM.4 Following Che et al. (2018), who used 20 million words per language to train m"
N19-1392,D18-1034,1,0.870572,"languages. As our work is a multilingual extension of ELMo (Peters et al., 2018), we call it Rosita (after a bilingual character from Sesame Street). Our hypothesis is that, although each language is unique, different languages manifest similar characteristics (e.g., morphological, lexical, syntactic) which can be exploited by training a single model with data from multiple languages (Ammar, 2016). Previous work has shown this to be true to some degree in the context of syntactic dependency parsing (Ammar et al., 2016), semantic role labeling (Mulcaire et al., 2018), named entity recognition (Xie et al., 2018), and language modeling for phonetic sequences (Tsvetkov et al., 2016) and for speech recognition (Ragni et al., 2016). Recently, de Lhoneux et al. (2018) showed that parameter sharing between languages can improve performance in dependency parsing, but the effect is variable, depending on the language pair and the parameter sharing strategy. Other recent work also reported that concatenating data from different languages can hurt performance in dependency parsing (Che et al., 2018). These mixed results suggest that while crosslingual transfer in neural network models is a promising direction,"
N19-1392,P18-2106,1,0.815752,"gle “polyglot” language model on text in multiple languages. As our work is a multilingual extension of ELMo (Peters et al., 2018), we call it Rosita (after a bilingual character from Sesame Street). Our hypothesis is that, although each language is unique, different languages manifest similar characteristics (e.g., morphological, lexical, syntactic) which can be exploited by training a single model with data from multiple languages (Ammar, 2016). Previous work has shown this to be true to some degree in the context of syntactic dependency parsing (Ammar et al., 2016), semantic role labeling (Mulcaire et al., 2018), named entity recognition (Xie et al., 2018), and language modeling for phonetic sequences (Tsvetkov et al., 2016) and for speech recognition (Ragni et al., 2016). Recently, de Lhoneux et al. (2018) showed that parameter sharing between languages can improve performance in dependency parsing, but the effect is variable, depending on the language pair and the parameter sharing strategy. Other recent work also reported that concatenating data from different languages can hurt performance in dependency parsing (Che et al., 2018). These mixed results suggest that while crosslingual transfer in ne"
N19-1392,P17-1161,0,0.0397435,"Missing"
N19-1392,K18-2001,0,0.0450858,"Missing"
N19-1392,N18-1202,0,0.802453,"vidence for the benefits of polyglot learning, in which representations are shared across multiple languages. 1 Introduction State-of-the-art methods for crosslingual transfer make use of multilingual word embeddings, and much research has explored methods that align vector spaces for words in different languages (Faruqui and Dyer, 2014; Upadhyay et al., 2016; Ruder et al., 2017). On the other hand, contextual word representations (CWR) extracted from language models (LMs) have advanced the state of the art beyond what was achieved with word type representations on many monolingual NLP tasks (Peters et al., 2018). Thus, the question arises: can contextual word representations benefit from multilinguality? We introduce a method to produce multilingual CWR by training a single “polyglot” language model on text in multiple languages. As our work is a multilingual extension of ELMo (Peters et al., 2018), we call it Rosita (after a bilingual character from Sesame Street). Our hypothesis is that, although each language is unique, different languages manifest similar characteristics (e.g., morphological, lexical, syntactic) which can be exploited by training a single model with data from multiple languages ("
N19-1392,K18-2016,0,0.0609668,"L 2018 shared task on multilingual dependency parsing (Zeman et al., 2018). In particular, we use the GUM treebank for English,7 GSD for Chinese, and PADT for Arabic. For training and validation, we use the provided gold POS tags and word segmentation. For each configuration, we run experiments five times with random initializations and report the mean and standard deviation. For testing, we use the CoNLL 2018 evaluation script and consider two scenarios: (1) gold POS tags and word segmentations and (2) predicted POS tags and word segmentations from the system outputs of Che et al. (2018) and Qi et al. (2018).8 The former scenario enables us to purely assess parsing performance; see column 3 in Table 1 for these results on Chinese and Arabic. The latter allows for a direct comparison to the best previously reported parsers (Chinese, Che et al., 2018; Arabic, Qi et al., 2018). See Table 2 for these results. As seen in Table 1, the Universal Dependencies results generally show a significant improvement from the use of CWR. The best results for both languages come from the ROSITAC HAR LM and polyglot task models, showing that polyglot training helps, but that the word-embedding initialization of the"
N19-1392,W13-3516,0,\N,Missing
N19-1392,N16-1161,0,\N,Missing
N19-1392,L16-1680,0,\N,Missing
N19-1392,P17-1044,0,\N,Missing
N19-1392,W18-2501,0,\N,Missing
N19-1392,D18-1543,0,\N,Missing
N19-1392,E14-1049,0,\N,Missing
P04-1062,J90-2002,0,0.314832,"on. Applying DA to parsing and tagging models is shown to be straightforward; significant improvements over EM are shown on a part-of-speech tagging task. We describe a variant, skewed DA, which can incorporate a good initializer when it is available, and show significant improvements over EM on a grammar induction task. 1 Introduction Unlabeled data remains a tantalizing potential resource for NLP researchers. Some tasks can thrive on a nearly pure diet of unlabeled data (Yarowsky, 1995; Collins and Singer, 1999; Cucerzan and Yarowsky, 2003). But for other tasks, such as machine translation (Brown et al., 1990), the chief merit of unlabeled data is simply that nothing else is available; unsupervised parameter estimation is notorious for achieving mediocre results. The standard starting point is the ExpectationMaximization (EM) algorithm (Dempster et al., 1977). EM iteratively adjusts a model’s parameters from an initial guess until it converges to a local maximum. Unfortunately, likelihood functions in practice are riddled with suboptimal local maxima (e.g., Charniak, 1993, ch. 7). Moreover, maximizing likelihood is not equivalent to maximizing task-defined accuracy (e.g., Merialdo, 1994). Here we f"
P04-1062,W99-0613,0,0.036962,"intains a local maximum as it gradually morphs the function into the desired non-concave likelihood function. Applying DA to parsing and tagging models is shown to be straightforward; significant improvements over EM are shown on a part-of-speech tagging task. We describe a variant, skewed DA, which can incorporate a good initializer when it is available, and show significant improvements over EM on a grammar induction task. 1 Introduction Unlabeled data remains a tantalizing potential resource for NLP researchers. Some tasks can thrive on a nearly pure diet of unlabeled data (Yarowsky, 1995; Collins and Singer, 1999; Cucerzan and Yarowsky, 2003). But for other tasks, such as machine translation (Brown et al., 1990), the chief merit of unlabeled data is simply that nothing else is available; unsupervised parameter estimation is notorious for achieving mediocre results. The standard starting point is the ExpectationMaximization (EM) algorithm (Dempster et al., 1977). EM iteratively adjusts a model’s parameters from an initial guess until it converges to a local maximum. Unfortunately, likelihood functions in practice are riddled with suboptimal local maxima (e.g., Charniak, 1993, ch. 7). Moreover, maximizi"
P04-1062,N03-1006,0,0.0240312,"it gradually morphs the function into the desired non-concave likelihood function. Applying DA to parsing and tagging models is shown to be straightforward; significant improvements over EM are shown on a part-of-speech tagging task. We describe a variant, skewed DA, which can incorporate a good initializer when it is available, and show significant improvements over EM on a grammar induction task. 1 Introduction Unlabeled data remains a tantalizing potential resource for NLP researchers. Some tasks can thrive on a nearly pure diet of unlabeled data (Yarowsky, 1995; Collins and Singer, 1999; Cucerzan and Yarowsky, 2003). But for other tasks, such as machine translation (Brown et al., 1990), the chief merit of unlabeled data is simply that nothing else is available; unsupervised parameter estimation is notorious for achieving mediocre results. The standard starting point is the ExpectationMaximization (EM) algorithm (Dempster et al., 1977). EM iteratively adjusts a model’s parameters from an initial guess until it converges to a local maximum. Unfortunately, likelihood functions in practice are riddled with suboptimal local maxima (e.g., Charniak, 1993, ch. 7). Moreover, maximizing likelihood is not equivalen"
P04-1062,A94-1009,0,0.143805,"Missing"
P04-1062,P02-1017,0,0.674977,"ately damaged performance only as much as EM did or did slightly better than EM (but still hurt). This is unsurprising: Merialdo’s result demonstrated that ML and maximizing accuracy are generally not the same; the EM algorithm consistently degraded the accuracy of his supervised models. SDA is simply another search algorithm with the same criterion as EM. SDA did do what it was expected to do—it used the initializer, repairing DA damage. 6 Grammar induction We turn next to the problem of statistical grammar induction: inducing parse trees over unlabeled text. An excellent recent result is by Klein and Manning (2002). The constituent-context model (CCM) they present is a generative, deficient channel model of POS tag strings given binary tree bracketings. We first review the model and describe a small modification that reduces the deficiency, then compare both models under EM and DA. 6.1 Constituent-context model Let (x, y) be a (tag sequence, binary tree) pair. xji denotes the subsequence of x from the ith to the jth word. Let yi,j be 1 if the yield from i to j is a constituent in the tree y and 0 if it is not. The CCM gives to a pair (x, y) the following h  probability:  Pr(x, y) = Pr(y) · Y 1≤i≤j≤|x|"
P04-1062,J94-2001,0,0.938591,"ation (Brown et al., 1990), the chief merit of unlabeled data is simply that nothing else is available; unsupervised parameter estimation is notorious for achieving mediocre results. The standard starting point is the ExpectationMaximization (EM) algorithm (Dempster et al., 1977). EM iteratively adjusts a model’s parameters from an initial guess until it converges to a local maximum. Unfortunately, likelihood functions in practice are riddled with suboptimal local maxima (e.g., Charniak, 1993, ch. 7). Moreover, maximizing likelihood is not equivalent to maximizing task-defined accuracy (e.g., Merialdo, 1994). Here we focus on the search error problem. Assume that one has a model for which improving likelihood really will improve accuracy (e.g., at predicting hidden part-of-speech (POS) tags or parse trees). Hence, we seek methods that tend to locate mountaintops rather than hilltops of the likelihood function. Alternatively, we might want methods that find hilltops with other desirable properties.1 1 Wang et al. (2003) suggest that one should seek a highIn §2 we review deterministic annealing (DA) and show how it generalizes the EM algorithm. §3 shows how DA can be used for parameter estimation f"
P04-1062,P93-1024,0,0.305449,"Missing"
P04-1062,P95-1026,0,0.0184867,"function and maintains a local maximum as it gradually morphs the function into the desired non-concave likelihood function. Applying DA to parsing and tagging models is shown to be straightforward; significant improvements over EM are shown on a part-of-speech tagging task. We describe a variant, skewed DA, which can incorporate a good initializer when it is available, and show significant improvements over EM on a grammar induction task. 1 Introduction Unlabeled data remains a tantalizing potential resource for NLP researchers. Some tasks can thrive on a nearly pure diet of unlabeled data (Yarowsky, 1995; Collins and Singer, 1999; Cucerzan and Yarowsky, 2003). But for other tasks, such as machine translation (Brown et al., 1990), the chief merit of unlabeled data is simply that nothing else is available; unsupervised parameter estimation is notorious for achieving mediocre results. The standard starting point is the ExpectationMaximization (EM) algorithm (Dempster et al., 1977). EM iteratively adjusts a model’s parameters from an initial guess until it converges to a local maximum. Unfortunately, likelihood functions in practice are riddled with suboptimal local maxima (e.g., Charniak, 1993,"
P04-3032,J98-2004,0,0.0347295,"riant algorithms. Although Dyna supports stack and queue (LIFO and FIFO) disciplines, its default is to use a priority queue prioritized by the size of the update. When parsing with real values, this quickly accumulates a good approximation of the inside probabilities, which permits heuristic early stopping before the agenda is empty. With viterbi values, it amounts to uniform-cost search for the best parse, and an item’s value is guaranteed not to change once it is nonzero. Dyna will soon allow user-defined priority functions (themselves dynamic programs), which can greatly speed up parsing (Caraballo and Charniak, 1998; Klein and Manning, 2003). 2.4 Parameter Training Dyna provides facilities for training parameters. For example, from Fig. 1, it automatically derives the insideoutside (EM) algorithm for training PCFGs. How is this possible? Once the program of Fig. 1 has run, goal’s value is the probability of the input sentence under the grammar. This is a continuous function of the axiom values, which correspond to PCFG parameters (e.g., the weight of rewrite(np,Mary)). The function could be written out explicitly as a sum of products of sums of products of . . . of axiom values, with the details dependin"
P04-3032,P02-1001,1,0.876075,"Missing"
P04-3032,J99-4004,0,0.0268025,"rting at position 3. As usual, probabilistic, agenda-based lattice parsing comes for free, as does training. tempted similar syntheses (though without covering variant search and storage strategies, which Dyna handles). Shieber et al. (1995) (already noting that “many of the ideas we present are not new”) showed that several unweighted parsing algorithms can be specified in terms of inference rules, and used Prolog to implement an agendabased interpreter for such rules. McAllester (1999) made a similar case for static analysis algorithms, with a more rigorous discussion of indexing the chart. Goodman (1999) generalized this line of work to weighted parsing, using rules of the form c += a1 *a2 * · · · *ak (with side conditions allowed); he permitted values to fall in any semiring, and generalized the inside-outside algorithm. Our approach extends this to a wider variety of processing orders, and in particular shows how to use a prioritized agenda in the general case, using novel algorithms. We also extend to a wider class of formulas (e.g., neural networks). The closest implemented work we have found is PRISM (Zhou and Sato, 2003), a kind of probabilistic Prolog that claims to be efficient (thank"
P04-3032,N03-1016,0,0.0206753,"a supports stack and queue (LIFO and FIFO) disciplines, its default is to use a priority queue prioritized by the size of the update. When parsing with real values, this quickly accumulates a good approximation of the inside probabilities, which permits heuristic early stopping before the agenda is empty. With viterbi values, it amounts to uniform-cost search for the best parse, and an item’s value is guaranteed not to change once it is nonzero. Dyna will soon allow user-defined priority functions (themselves dynamic programs), which can greatly speed up parsing (Caraballo and Charniak, 1998; Klein and Manning, 2003). 2.4 Parameter Training Dyna provides facilities for training parameters. For example, from Fig. 1, it automatically derives the insideoutside (EM) algorithm for training PCFGs. How is this possible? Once the program of Fig. 1 has run, goal’s value is the probability of the input sentence under the grammar. This is a continuous function of the axiom values, which correspond to PCFG parameters (e.g., the weight of rewrite(np,Mary)). The function could be written out explicitly as a sum of products of sums of products of . . . of axiom values, with the details depending on the sentence and gram"
P04-3032,P92-1017,0,0.0594185,"the Toolkit for Advanced Optimization (Benson et al., 2000) together with a softmax 6 DynaMITE = Dyna Module for Iterative Training and Estimation. technique to enforce sum-to-one constraints. It supports maximum-entropy training and the EM algorithm.7 DynaMITE provides an object-oriented API that allows independent variation of such diverse elements of training as the model parameterization, optimization algorithm, smoothing techniques, priors, and datasets. How about supervised or partly supervised training? The role of supervision is to permit some constituents to be built but not others (Pereira and Schabes, 1992). Lines 2–3 of Fig. 1 can simply be extended with an additional antecedent permitted(X,I,K), which must be either asserted or derived for constit(X,I,K) to be derived. In “soft” supervision, the permitted axioms may have values between 0 and 1.8 3 C++ Interface and Implementation A Dyna program compiles to a set of portable C++ classes that manage the items and perform inference. These classes can be used in a larger C++ application.9 This strategy keeps Dyna both small and convenient. A C++ chart object supports the computation of item values and gradients. It keeps track of built items, thei"
P04-3032,P00-1061,0,0.0198448,"efine the weights of certain terms. In the current implementation, every rule must have the restricted form c += a1 *a2 * · · · *ak (where each ai is an item or side condition and (X, +, *) is a semiring of values). The design for Dyna’s next version lifts this restriction to allow arbitrary, type-heterogeneous expressions on the right-hand side of an inference rule.11 7 It will eventually offer additional methods, such as deterministic annealing, simulated annealing, and iterative scaling. 8 Such item values are not probabilities. We are generally interested in log-linear models for parsing (Riezler et al., 2000) and other tasks. 9 We are also now developing a default application: a visual debugger that allows a user to assert axioms and explore the proof forest created during inference. 10 Interned values are hashed so that equal values are represented by equal pointers. It is very fast to compare and hash such representations. 11 That will make Dyna useful for a wider variety of non-NLP algo4 Some Further Applications Dyna is useful for any problem where partial hypotheses are assembled, or where consistency has to be maintained. It is already being used for parsing, syntax-based machine translation"
P04-3032,P90-1001,0,0.0745958,"to it. To enable fast lookup of the other items that participate in these inference rules, it generates code to maintain appropriate indices on the chart. Objects such as constit(vp,1,3) are called terms and may be recursively nested to any depth. (Items are just terms with values.) Dyna has a full first-order type system for terms, including primitive and disjunctive types, and permitting compile-time type inference. These types are compiled into C++ classes that support constructors and accessors, garbage-collection, subterm sharing (which may lead to asymptotic speedups, as in CCG parsing (Vijay-Shanker and Weir, 1990)), and interning.10 Dyna can import new primitive term types and value types from C++, as well as C++ functions to combine values and to user-define the weights of certain terms. In the current implementation, every rule must have the restricted form c += a1 *a2 * · · · *ak (where each ai is an item or side condition and (X, +, *) is a semiring of values). The design for Dyna’s next version lifts this restriction to allow arbitrary, type-heterogeneous expressions on the right-hand side of an inference rule.11 7 It will eventually offer additional methods, such as deterministic annealing, simul"
P04-3032,J97-3002,0,0.0388419,"ing algorithms for CFG and other formalisms can be simply written in terms of inference rules. Fig. 2 renders one such example in Dyna, namely Earley’s algorithm. Two features are worth noting: the use of recursively nested subterms such as lists, and the SIDE function, which evaluates to 1 or 0 according to whether its argument has a defined value yet. These side conditions are used here to prevent hypothesizing a constituent until there is a possible left context that calls for it. Several recent syntax-directed statistical machine translation models are easy to build in Dyna. The simplest (Wu, 1997) uses constit(np,3,5,np,4,8) to denote a NP spanning positions 3–5 in the English string that is aligned with an NP spanning positions 4–8 in the Chinese string. When training or decoding, the hypotheses of better-trained monolingual parsers can provide either hard or soft partial supervision (section 2.4). Dyna can manipulate finite-state transducers. For instance, the weighted arcs of the composed FST M1 ◦ M2 can be deduced from the arcs of M1 and M2 . Training M1 ◦ M2 back-propagates to train the original weights in M1 and M2 , as in (Eisner, 2002). 5 Speed and Code Size One of our future p"
P05-1044,J97-4005,0,0.00483211,"on function Z(θ) by adding up the u-scores of all paths through the WFSA. For a k-state WFSA, this equates to solving a linear system of k equations in k variables (Tarjan, 1981). But if the WFSA contains cycles this infinite sum may diverge. Alternatives to exact com2 These are exemplified by CRFs (Lafferty et al., 2001), which can be viewed alternately as undirected dynamic graphical models with a chain topology, as log-linear models over entire sequences with local features, or as WFSAs. Because “CRF” implies CL estimation, we use the term “WFSA.” putation, like random sampling (see, e.g., Abney, 1997), will not help to avoid this difficulty; in addition, convergence rates are in general unknown and bounds difficult to prove. We would prefer to sum over finitely many paths in Bi . 3.1 Parameter estimation (supervised) For log-linear models, both CL and JL estimation (Tab. 1) are available. In terms of Eq. 5, both set Ai = {(xi , yi∗ )}. The difference is in B: for JL, Bi = X × Y, so summing over Bi is equiva~ Belent to computing the partition function Z(θ). cause that sum is typically difficult, CL is preferred; Bi = {xi } × Y for xi , which is often tractable. For sequence models like WFSA"
P05-1044,W03-0407,0,0.0384081,"ov random field is over labeling configurations for all examples, not, as in our case, complex structured labels for a particular example. Hence their B (Eq. 5), though very large, was finite and could be sampled. 361 65 60 55 50 45 40 Tagging dictionary contains words with count ≥ 3: 85 50 8 lin g   el sp + m gr a   4 × 10 5 tri l D EL O RT RANS 1 T RANS 1 L ENGTH EM 1 smoothing parameter od e 0.1 m 0 m 80 85 75 80 75 70 70 65 65 60 60 55 55 50 50 45 45 40 gr a Foremost for future work is the “minimally supervised” paradigm in which a small amount of labeled data is available (see, e.g., Clark et al. (2003)). Unlike well-known “bootstrapping” approaches (Yarowsky, 1995), EM and CE have the possible advantage of maintaining posteriors over hidden labels (or structure) throughout learning; bootstrapping either chooses, for each example, a single label, or remains completely agnostic. One can envision a mixed objective function that tries to fit the labeled examples while discriminating unlabeled examples from their neighborhoods.8 Regardless of how much (if any) data are labeled, the question of good smoothing techniques requires more attention. Here we used a single zero-mean, constant-variance G"
P05-1044,P02-1001,1,0.390507,"(9) CE may also be viewed as an importance sampling approximation to EM, where the sample space X is replaced by N(xi ). We will demonstrate experimentally that CE is not just an approximation to EM; it makes sense from a modeling perspective. In §4, we will describe neighborhoods of sequences that can be represented as acyclic lattices built directly from an observed sequence. The sum over Bi is then the total u-score in our model of all paths in the neighborhood lattice. To compute this, intersect the WFSA and the lattice, obtaining a new acyclic WFSA, and sum the u-scores of all its paths (Eisner, 2002) using a simple dynamic programming algorithm akin to the forward algorithm. The sum over Ai may be computed similarly. CE with lattice neighborhoods is not confined to the WFSAs of this paper; when estimating weighted CFGs, the key algorithm is the inside algorithm for lattice parsing (Smith and Eisner, 2005). 3.3 Numerical optimization To maximize the neighborhood likelihood (Eq. 7), we apply a standard numerical optimization method (L-BFGS) that iteratively climbs the function using knowledge of its value and gradient (Liu and Nocedal, 1989). The partial derivative of LN with respect to the"
P05-1044,P99-1069,0,0.00849745,"Missing"
P05-1044,P01-1042,0,0.0302063,"any paths in Bi . 3.1 Parameter estimation (supervised) For log-linear models, both CL and JL estimation (Tab. 1) are available. In terms of Eq. 5, both set Ai = {(xi , yi∗ )}. The difference is in B: for JL, Bi = X × Y, so summing over Bi is equiva~ Belent to computing the partition function Z(θ). cause that sum is typically difficult, CL is preferred; Bi = {xi } × Y for xi , which is often tractable. For sequence models like WFSAs it is computed using a dynamic programming algorithm (the forward algorithm for WFSAs). Klein and Manning (2002) argue for CL on grounds of accuracy, but see also Johnson (2001). See Tab. 2; other contrast sets Bi are also possible. When Bi contains only xi paired with the current best competitor (ˆ y ) to yi∗ , we have a technique that resembles maximum margin training (Crammer and Singer, 2001). Note that yˆ will then change across training iterations, making Bi dynamic. 3.2 Parameter estimation (unsupervised) The difference between supervised and unsupervised learning is that in the latter case, Ai is forced to sum over label sequences y because they weren’t observed. In the unsupervised case, CE maximizes  X   Y LN θ~ = log u xi , y |θ~ y∈Y X i   u x, y |θ~"
P05-1044,W02-1002,0,0.00463799,"known and bounds difficult to prove. We would prefer to sum over finitely many paths in Bi . 3.1 Parameter estimation (supervised) For log-linear models, both CL and JL estimation (Tab. 1) are available. In terms of Eq. 5, both set Ai = {(xi , yi∗ )}. The difference is in B: for JL, Bi = X × Y, so summing over Bi is equiva~ Belent to computing the partition function Z(θ). cause that sum is typically difficult, CL is preferred; Bi = {xi } × Y for xi , which is often tractable. For sequence models like WFSAs it is computed using a dynamic programming algorithm (the forward algorithm for WFSAs). Klein and Manning (2002) argue for CL on grounds of accuracy, but see also Johnson (2001). See Tab. 2; other contrast sets Bi are also possible. When Bi contains only xi paired with the current best competitor (ˆ y ) to yi∗ , we have a technique that resembles maximum margin training (Crammer and Singer, 2001). Note that yˆ will then change across training iterations, making Bi dynamic. 3.2 Parameter estimation (unsupervised) The difference between supervised and unsupervised learning is that in the latter case, Ai is forced to sum over label sequences y because they weren’t observed. In the unsupervised case, CE ma"
P05-1044,J94-2001,0,0.98117,"nary and unlabeled text—contrastive estimation outperforms EM (with the same feature set), is more robust to degradations of the dictionary, and can largely recover by modeling additional features. 1 Introduction Finding linguistic structure in raw text is not easy. The classical forward-backward and inside-outside algorithms try to guide probabilistic models to discover structure in text, but they tend to get stuck in local maxima (Charniak, 1993). Even when they avoid local maxima (e.g., through clever initialization) they typically deviate from human ideas of what the “right” structure is (Merialdo, 1994). One strategy is to incorporate domain knowledge into the model’s structure. Instead of blind HMMs or PCFGs, one could use models whose features ∗ This work was supported by a Fannie and John Hertz Foundation fellowship to the first author and NSF ITR grant IIS0313193 to the second author. The views expressed are not necessarily endorsed by the sponsors. The authors also thank three anonymous ACL reviewers for helpful comments, colleagues at JHU CLSP (especially David Smith and Roy Tromble) and Miles Osborne for insightful feedback, and Eric Goldlust and Markus Dreyer for Dyna language suppor"
P05-1044,P00-1061,0,0.0203746,"Missing"
P05-1044,N03-1028,0,0.102588,"Missing"
P05-1044,P04-1062,1,0.650356,"neighborhood lattice (via composition with the sentence, followed by determinization and minimization) is shown to its right. expectations in Eq. 10 are computed by the forwardbackward algorithm generalized to lattices. We emphasize that the function LN is not globally concave; our search will lead only to a local optimum.3 Therefore, as with all unsupervised statistical learning, the bias in the initialization of θ~ will affect the quality of the estimate and the performance of the method. In future we might wish to apply techniques for avoiding local optima, such as deterministic annealing (Smith and Eisner, 2004). 4 Lattice Neighborhoods We next consider some non-classical neighborhood functions for sequences. When X = Σ+ for some symbol alphabet Σ, certain kinds of neighborhoods have natural, compact representations. Given an input string x = hx1 , x2 , ..., xm i, we write xji for the substring hxi , xi+1 , ..., xj i and xm 1 for the whole string. Consider first the neighborhood consisting of all sequences generated by deleting a single symbol from the m-length sequence xm 1 : D EL 1W ORD(xm 1 ) = n o m x`−1 xm `+1 |1 ≤ ` ≤ m ∪ {x1 } 1 This set consists of m + 1 strings and can be compactly represent"
P05-1044,P95-1026,0,0.215723,"t, as in our case, complex structured labels for a particular example. Hence their B (Eq. 5), though very large, was finite and could be sampled. 361 65 60 55 50 45 40 Tagging dictionary contains words with count ≥ 3: 85 50 8 lin g   el sp + m gr a   4 × 10 5 tri l D EL O RT RANS 1 T RANS 1 L ENGTH EM 1 smoothing parameter od e 0.1 m 0 m 80 85 75 80 75 70 70 65 65 60 60 55 55 50 50 45 45 40 gr a Foremost for future work is the “minimally supervised” paradigm in which a small amount of labeled data is available (see, e.g., Clark et al. (2003)). Unlike well-known “bootstrapping” approaches (Yarowsky, 1995), EM and CE have the possible advantage of maintaining posteriors over hidden labels (or structure) throughout learning; bootstrapping either chooses, for each example, a single label, or remains completely agnostic. One can envision a mixed objective function that tries to fit the labeled examples while discriminating unlabeled examples from their neighborhoods.8 Regardless of how much (if any) data are labeled, the question of good smoothing techniques requires more attention. Here we used a single zero-mean, constant-variance Gaussian prior for all parameters. Better performance might be ac"
P05-1044,W03-0430,0,\N,Missing
P05-1044,W03-1019,0,\N,Missing
P06-1072,afonso-etal-2002-floresta,0,0.0287968,"Missing"
P06-1072,P96-1023,0,0.024445,"k, 1993). We seek here to capitalize on the intuition that, at least early in learning, the learner should search primarily for string-local structure, because most structure is local.1 By penalizing dependencies between two words that are farther apart in the string, we obtain consistent improvements in accuracy of the learned model (§3). We then explore how gradually changing δ over time affects learning (§4): we start out with a 2 Task and Model In this paper we use a simple unlexicalized dependency model due to Klein and Manning (2004). The model is a probabilistic head automaton grammar (Alshawi, 1996) with a “split” form that renders it parseable in cubic time (Eisner, 1997). Let x = hx1 , x2 , ..., xn i be the sentence. x0 is a special “wall” symbol, $, on the left of every sentence. A tree y is defined by a pair of functions yleft and yright (both {0, 1, 2, ..., n} → 2{1,2,...,n} ) that map each word to its sets of left and right dependents, respectively. The graph is constrained to be a projective tree rooted at $: each word except $ has a single parent, and there are no cycles ∗ This work was supported by a Fannie and John Hertz Foundation fellowship to the first author and NSF ITR gra"
P06-1072,W03-2405,0,0.0219645,"Missing"
P06-1072,H92-1030,0,0.0197222,"Missing"
P06-1072,J93-2003,0,0.00569574,"Missing"
P06-1072,W06-2920,0,0.0256923,"Missing"
P06-1072,P97-1003,0,0.225517,"Missing"
P06-1072,H05-1050,1,0.887179,"Missing"
P06-1072,P99-1059,1,0.867493,"Missing"
P06-1072,W05-1504,1,0.871521,"ll; see appendix.) Supervised model selection, which uses a small annotated development set, performs almost as well as the oracle, but unsupervised model selection, which selects the model that maximizes likelihood on an unannotated development set, is often much worse. 0 0.2 One way to bias a learner toward local explanations is to penalize longer attachments. This was done for supervised parsing in different ways by Collins (1997), Klein and Manning (2003), and McDonald et al. (2005), all of whom considered intervening material or coarse distance classes when predicting children in a tree. Eisner and Smith (2005) achieved speed and accuracy improvements by modeling distance directly in a ML-estimated (deficient) generative model. Here we use string distance to measure the length of a dependency link and consider the inclusion of a sum-of-lengths feature in the probabilistic model, for learning only. Keeping our original model, we will simply multiply into the probability of each tree another factor that penalizes long dependencies, giving:  3 -0.2 Figure 1: Test-set F1 performance of models trained by EM with a locality bias at varying δ. Each curve corresponds to a different language and shows perfo"
P06-1072,1997.iwpt-1.10,1,0.64705,"n learning, the learner should search primarily for string-local structure, because most structure is local.1 By penalizing dependencies between two words that are farther apart in the string, we obtain consistent improvements in accuracy of the learned model (§3). We then explore how gradually changing δ over time affects learning (§4): we start out with a 2 Task and Model In this paper we use a simple unlexicalized dependency model due to Klein and Manning (2004). The model is a probabilistic head automaton grammar (Alshawi, 1996) with a “split” form that renders it parseable in cubic time (Eisner, 1997). Let x = hx1 , x2 , ..., xn i be the sentence. x0 is a special “wall” symbol, $, on the left of every sentence. A tree y is defined by a pair of functions yleft and yright (both {0, 1, 2, ..., n} → 2{1,2,...,n} ) that map each word to its sets of left and right dependents, respectively. The graph is constrained to be a projective tree rooted at $: each word except $ has a single parent, and there are no cycles ∗ This work was supported by a Fannie and John Hertz Foundation fellowship to the first author and NSF ITR grant IIS-0313193 to the second author. The views expressed are not necessaril"
P06-1072,P02-1017,0,0.49026,"Missing"
P06-1072,P05-1012,0,0.0792022,"Missing"
P06-1072,W03-2403,0,0.0656734,"Missing"
P06-1072,P04-1062,1,0.903898,"Missing"
P06-1072,P05-1044,1,0.737676,"nd in the appendix. Introduction strong preference for short dependencies, then relax the preference. The new approach, structural annealing, often gives superior performance. An alternative structural bias is explored in §5. This approach views a sentence as a sequence of one or more yields of separate, independent trees. The points of segmentation are a hidden variable, and during learning all possible segmentations are entertained probabilistically. This allows the learner to accept hypotheses that explain the sentences as independent pieces. In §6 we briefly review contrastive estimation (Smith and Eisner, 2005a), relating it to the new method, and show its performance alone and when augmented with structural bias. Inducing a weighted context-free grammar from flat text is a hard problem. A common starting point for weighted grammar induction is the Expectation-Maximization (EM) algorithm (Dempster et al., 1977; Baker, 1979). EM’s mediocre performance (Table 1) reflects two problems. First, it seeks to maximize likelihood, but a grammar that makes the training data likely does not necessarily assign a linguistically defensible syntactic structure. Second, the likelihood surface is not globally conca"
P06-1072,E03-1008,0,0.036507,"Missing"
P06-1072,P95-1026,0,0.0145072,"Missing"
P06-1072,J93-2004,0,\N,Missing
P06-1072,P90-1034,0,\N,Missing
P06-1072,P04-1061,0,\N,Missing
P07-1095,J98-2005,0,0.0314,"side probabilities. Let the PCFG have nonterminal set N, start symbol S ∈ N, terminal alphabet Σ, and rules of the form A → B C and A → x. (We assume Chomsky normal form for clarity; the generalization is straightforward.) Let rA (B C) and rA (x) denote the probabilities of nonterminal A rewriting to child sequence B C or x, respectively. Then ∀A ∈ N: oA = X X oB iC [rB (A C) + rB (C A)] B∈N C∈N  iA 1 if A = S + 0 otherwise X X X = rA (B C)iB iC + rA (x)ix B∈N C∈N ox = X x oA rA (x), ∀x ∈ Σ A∈N ix = 1, ∀x ∈ Σ In most practical applications, the PCFG will be “tight” (Booth and Thompson, 1973; Chi and Geman, 1998). Informally, this means that the probability of a derivation rooted in S failing to terminate is zero. If that is the case, then iA = 1 for all A ∈ N, and the system becomes linear (see also Corazza and Satta, 2006).5 If tightness is not guaranteed, iterative propagation of weights, following Stolcke (1995), works well in our experience for solving the quadratic system, and converges quickly. As in the HMM case, expected counts of arbitrary contiguous tree substructures can be computed as products of probabilities of rules appearing within the structure, factoring in the o value of the struct"
P07-1095,N06-1043,0,0.033968,"tforward.) Let rA (B C) and rA (x) denote the probabilities of nonterminal A rewriting to child sequence B C or x, respectively. Then ∀A ∈ N: oA = X X oB iC [rB (A C) + rB (C A)] B∈N C∈N  iA 1 if A = S + 0 otherwise X X X = rA (B C)iB iC + rA (x)ix B∈N C∈N ox = X x oA rA (x), ∀x ∈ Σ A∈N ix = 1, ∀x ∈ Σ In most practical applications, the PCFG will be “tight” (Booth and Thompson, 1973; Chi and Geman, 1998). Informally, this means that the probability of a derivation rooted in S failing to terminate is zero. If that is the case, then iA = 1 for all A ∈ N, and the system becomes linear (see also Corazza and Satta, 2006).5 If tightness is not guaranteed, iterative propagation of weights, following Stolcke (1995), works well in our experience for solving the quadratic system, and converges quickly. As in the HMM case, expected counts of arbitrary contiguous tree substructures can be computed as products of probabilities of rules appearing within the structure, factoring in the o value of the structure’s root and the i values of the structure’s leaves. 4.2 Optimization To carry out M-estimation, we minimize the function `(w) in Eq. 3. To apply gradient descent or a quasi-Newton numerical optimization method,6 i"
P07-1095,H05-1036,1,0.509031,"ssed in §4.2, we trained using L-BFGS; training continued until relative improvement fell within machine precision or 100 iterations, whichever came first. After training, the value of c is chosen that maximizes F1 accuracy on the tuning set. Runtime Fig. 1 compares the wall time of carefully-timed training runs on a dedicated server. Note that Dyna, a high-level programming language, was used for dynamic programming (in the CRF) and summations (MEMM and pseudolikelihood). The runtime overhead incurred by using Dyna is estimated as a slow-down factor of 3–5 against a handtuned implementation (Eisner et al., 2005), though the slow-down factor is almost certainly less for the MEMM and pseudolikelihood. All training (except the HMM, of course) was done using the R language implementation of L-BFGS. In our implementation, the M-estimator trained substantially faster than the other methods. Of the 64 minutes required to train the M-estimator, 6 minutes were spent precomputing Eq0 (X,Y ) [f (X, Y )] (this need not be repeated if the regularization settings are altered). Accuracy Tab. 1 shows how NP chunking accuracy compares among the models. With HMM features, the M-estimator is about the same as the HMM a"
P07-1095,P01-1042,0,0.0312219,"nce is required during training, any features really are permitted, so long as their expected values can be estimated under the base model q0 . Indeed, M-estimation is considerably easier to implement than conditional estimation. Both require feature counts from the training data; M-estimation replaces repeated calculation and differentiation of normalizing constants with inference or sampling (once) under a base model. So the M-estimator is much faster to train. Generative and discriminative models have been compared and discussed a great deal (Ng and Jordan, 2002), including for NLP models (Johnson, 2001; Klein and Manning, 2002). Sutton and McCallum (2005) present approximate methods that keep a discriminative objective while avoiding full inference. We see M-estimation as a particularly promising method in settings where performance depends on high-dimensional, highly-correlated feature spaces, where the desired features “large,” making discriminative training too time-consuming—a compelling example is machine translation. Further, in some settings a locally-normalized conditional log-linear model (like an MEMM) may be difficult to design; our estimator avoids normalization altogether.8 The"
P07-1095,W02-1002,0,0.0186521,"during training, any features really are permitted, so long as their expected values can be estimated under the base model q0 . Indeed, M-estimation is considerably easier to implement than conditional estimation. Both require feature counts from the training data; M-estimation replaces repeated calculation and differentiation of normalizing constants with inference or sampling (once) under a base model. So the M-estimator is much faster to train. Generative and discriminative models have been compared and discussed a great deal (Ng and Jordan, 2002), including for NLP models (Johnson, 2001; Klein and Manning, 2002). Sutton and McCallum (2005) present approximate methods that keep a discriminative objective while avoiding full inference. We see M-estimation as a particularly promising method in settings where performance depends on high-dimensional, highly-correlated feature spaces, where the desired features “large,” making discriminative training too time-consuming—a compelling example is machine translation. Further, in some settings a locally-normalized conditional log-linear model (like an MEMM) may be difficult to design; our estimator avoids normalization altogether.8 The M-estimator may also be u"
P07-1095,N03-1028,0,0.142734,"Missing"
P07-1095,J95-2002,0,0.0627841,"e B C or x, respectively. Then ∀A ∈ N: oA = X X oB iC [rB (A C) + rB (C A)] B∈N C∈N  iA 1 if A = S + 0 otherwise X X X = rA (B C)iB iC + rA (x)ix B∈N C∈N ox = X x oA rA (x), ∀x ∈ Σ A∈N ix = 1, ∀x ∈ Σ In most practical applications, the PCFG will be “tight” (Booth and Thompson, 1973; Chi and Geman, 1998). Informally, this means that the probability of a derivation rooted in S failing to terminate is zero. If that is the case, then iA = 1 for all A ∈ N, and the system becomes linear (see also Corazza and Satta, 2006).5 If tightness is not guaranteed, iterative propagation of weights, following Stolcke (1995), works well in our experience for solving the quadratic system, and converges quickly. As in the HMM case, expected counts of arbitrary contiguous tree substructures can be computed as products of probabilities of rules appearing within the structure, factoring in the o value of the structure’s root and the i values of the structure’s leaves. 4.2 Optimization To carry out M-estimation, we minimize the function `(w) in Eq. 3. To apply gradient descent or a quasi-Newton numerical optimization method,6 it suffices to specify the fixed quantities 5 The same is true for HMMs: if the probability of"
P07-1095,W00-0726,0,0.143653,"Missing"
P07-1095,N03-1033,0,0.00653133,"3.68 91.79 92.15 90.42 93.86 91.83 91.51 89.64 Table 1: NP chunking accuracy on test data using different training methods. The effects of discriminative training (CRF) and extended feature sets (lower section) are more than additive. compared to what Sha and Pereira report. There are 630,862 such features. Using the original HMM feature set and the extended feature set, we trained four models that can use arbitrary features: conditional random fields (a near-replication of Sha and Pereira, 2003), maximum entropy Markov models (MEMMs; McCallum et al., 2000), pseudolikelihood (Besag, 1975; see Toutanova et al., 2003, for a tagging application), and our M-estimator with the HMM as q0 . CRFs and MEMMs are discriminatively-trained to maximize conditional likelihood (the former is parameterized using a sequence-normalized log-linear model, the latter using a locally-normalized loglinear model). Pseudolikelihood is a consistent estimator for the joint likelihood, like our M-estimator; its objective function is a sum of log probabilities. In each case, we trained seven models for each feature set with quadratic regularizers c ∈ [10−1 , 10], spaced at equal intervals in the log-scale, plus an unregularized mode"
P09-1039,W07-2216,0,0.220657,"that th shown by McDonald et al. (2005), is an instance of the maximal arborescence problem. Combinatorial algorithms (Chu and Liu, 1965; Edmonds, 1967) can solve this problem in cubic time.4 If the dependency parse trees are restricted to be projective, cubic-time algorithms are available via dynamic programming (Eisner, 1996). While in the projective case, the arc-factored assumption can be weakened in certain ways while maintaining polynomial parser runtime (Eisner and Satta, 1999), the same does not happen in the nonprojective case, where finding the highest-scoring tree becomes NP-hard (McDonald and Satta, 2007). Approximate algorithms have been employed to handle models that are not arc-factored (although features are still fairly local): McDonald and Pereira (2006) adopted an approximation based on O(n3 ) projective parsing followed by a hillclimbing algorithm to rearrange arcs, and Smith and Eisner (2008) proposed an algorithm based on loopy belief propagation. 3 fact, fast solvers are available today that make this a practical solution for many problems. Their performance depends on the dimensions and degree of sparsity of the constraint matrix A. Riedel and Clarke (2006) proposed an ILP formulat"
P09-1039,H05-1066,0,0.685035,"Missing"
P09-1039,P08-1108,0,0.628299,". featureHowrepning, 2002; McDonald and Pereira, resentations over the inputparsing (McDonald et al., 2.2ever, Arc Factorization and Locality in the data-driven setting this2005a). can be The goal of this is to further our current partially bywork incorporating feature repThere has adverted been much recent workrich on dependency understanding of the computational nature of nonresentations over the input (McDonald et al., 2005a). parsing using graph-based, transition-based, and projective forfurther both learning and The goalparsing of thisalgorithms work is to our current hybrid methods; see Nivre and McDonald (2008) inference within setting. We start by understanding ofthe thedata-driven computational nature of nonforprojective an overview. Typical graph-based methods investigating and extending the edge-factored model parsing algorithms for both learning and consider linear classifiers of the form of McDonald et the al. data-driven (2005b). Insetting. particular, we apinference within We start by peal to the Matrix Tree Theorem for multi-digraphs investigating and extending the edge-factored model hw (x) = argmaxy∈Y w&gt; f (x,fory),calculat-(1) to McDonald design polynomial-time algorithms of et al. (2005"
P09-1039,C04-1197,0,0.0420515,"), model word valency, and can learn to favor nearly-projective parses. Introduction We evaluate the performance of the new parsers on standard parsing tasks in seven languages. The techniques that we present are also compatible with scenarios where expert knowledge is available, for example in the form of hard or soft firstorder logic constraints (Richardson and Domingos, 2006; Chang et al., 2008). Much attention has recently been devoted to integer linear programming (ILP) formulations of NLP problems, with interesting results in applications like semantic role labeling (Roth and Yih, 2005; Punyakanok et al., 2004), dependency parsing (Riedel and Clarke, 2006), word alignment for machine translation (Lacoste-Julien et al., 2006), summarization (Clarke and Lapata, 2008), and coreference resolution (Denis and Baldridge, 2007), among others. In general, the rationale for the development of ILP formulations is to incorporate non-local features or global constraints, which are often difficult to handle with traditional algorithms. ILP formulations focus more on the modeling of problems, rather than algorithm design. While solving an ILP is NP-hard in general, fast solvers are available today that make it a p"
P09-1039,W06-1616,0,0.30569,"nearly-projective parses. Introduction We evaluate the performance of the new parsers on standard parsing tasks in seven languages. The techniques that we present are also compatible with scenarios where expert knowledge is available, for example in the form of hard or soft firstorder logic constraints (Richardson and Domingos, 2006; Chang et al., 2008). Much attention has recently been devoted to integer linear programming (ILP) formulations of NLP problems, with interesting results in applications like semantic role labeling (Roth and Yih, 2005; Punyakanok et al., 2004), dependency parsing (Riedel and Clarke, 2006), word alignment for machine translation (Lacoste-Julien et al., 2006), summarization (Clarke and Lapata, 2008), and coreference resolution (Denis and Baldridge, 2007), among others. In general, the rationale for the development of ILP formulations is to incorporate non-local features or global constraints, which are often difficult to handle with traditional algorithms. ILP formulations focus more on the modeling of problems, rather than algorithm design. While solving an ILP is NP-hard in general, fast solvers are available today that make it a practical solution for many NLP problems. This"
P09-1039,D08-1016,0,0.497506,"programming (Eisner, 1996). While in the projective case, the arc-factored assumption can be weakened in certain ways while maintaining polynomial parser runtime (Eisner and Satta, 1999), the same does not happen in the nonprojective case, where finding the highest-scoring tree becomes NP-hard (McDonald and Satta, 2007). Approximate algorithms have been employed to handle models that are not arc-factored (although features are still fairly local): McDonald and Pereira (2006) adopted an approximation based on O(n3 ) projective parsing followed by a hillclimbing algorithm to rearrange arcs, and Smith and Eisner (2008) proposed an algorithm based on loopy belief propagation. 3 fact, fast solvers are available today that make this a practical solution for many problems. Their performance depends on the dimensions and degree of sparsity of the constraint matrix A. Riedel and Clarke (2006) proposed an ILP formulation for dependency parsing which refines the arc-factored model by imposing linguistically motivated “hard” constraints that forbid some arc configurations. Their formulation includes an exponential number of constraints—one for each possible cycle. Since it is intractable to throw in all constraints"
P09-1039,W08-2121,0,0.0794721,"Missing"
P09-1039,D07-1003,1,0.197007,"NP-hard in general, fast solvers are available today that make it a practical solution for many NLP problems. This paper presents new, concise ILP formulations for projective and non-projective depen2 2.1 Dependency Parsing Preliminaries A dependency tree is a lightweight syntactic representation that attempts to capture functional relationships between words. Lately, this formalism has been used as an alternative to phrase-based parsing for a variety of tasks, ranging from machine translation (Ding and Palmer, 2005) to relation extraction (Culotta and Sorensen, 2004) and question answering (Wang et al., 2007). Let us first describe formally the set of legal dependency parse trees. Consider a sentence x = 342 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 342–350, c Suntec, Singapore, 2-7 August 2009. 2009 ACL and AFNLP hw0 , . . . , wn i, where wi denotes the word at the ith position, and w0 = $ is a wall symbol. We form the (complete1 ) directed graph D = hV, Ai, with vertices in V = {0, . . . , n} (the i-th vertex corresponding to the i-th word) and arcs in A = V 2 . Using terminology from graph theory, we say that B ⊆ A is an r-arborescence2 of the dire"
P09-1039,D08-1059,0,0.19634,"Missing"
P09-1039,W06-2920,0,0.447326,"Consider 3 (or 30 ) from §3.1. φka = −1, k ∈ V  {0} (18) a∈δ + (0) φka − (23) From the definition of projective arcs in §2.1, we np have that za = 1 if and only if the arc is active (za = 1) and there is some vertex k in the span of a = hi, ji such that ψik = 0. We are led to the following O(|A |· |V |) constraints for hi, ji ∈ A: • Any node consumes its own commodity and no other: X k ∈ V  {0}. zanp , I(a ∈ y and a is nonprojective). i∈V • The root sends one unit of commodity to each node: φka − j, k ∈ V  {0} np For most languages, dependency parse trees tend to be nearly projective (cf. Buchholz and Marsi, 2006). We wish to make our model capable of learning to prefer “nearly” projective parses whenever that behavior is observed in the data. The multicommodity directed flow model of Magnanti and Wolsey (1994) is a refinement of the model described in §3.1 which offers a compact and elegant way to indicate nonprojective arcs, requiring O(n3 ) variables and constraints. In this model, every node k 6= 0 defines a commodity: one unit of commodity k originates at the root node and must be delivered to node k; the variable φkij denotes the flow of commodity k in arc hi, ji. We first replace (4–9) by (18–22"
P09-1039,W08-2102,0,0.0499262,"ed simple higher order features that look at the word, part-of-speech tag, and (if available) morphological information of the words being correlated through the indicator variables. For scalability (and noting that some of the models require O(|V |· |A|) constraints and variables, which, when A = V 2 , grows cubically with the number of words), we first prune the base graph by running a simple algorithm that ranks the k-best candidate parents for each word in the sentence (we set k = 10); this reduces the number of candidate arcs to |A |= kn.11 This strategy is similar to the one employed by Carreras et al. (2008) to prune the search space of the actual parser. The ranker is a local model trained using a max-margin criterion; it is arc-factored and not subject to any structural constraints, so it is very fast. The actual parser was trained via the online structured passive-aggressive algorithm of Crammer et al. (2006); it differs from the 1-best MIRA algorithm of McDonald et al. (2005) by solving a sequence of loss-augmented inference problems.12 The number of iterations was set to 10. The results are summarized in Table 1; for the sake of comparison, we reproduced three strong 8 We used the provided t"
P09-1039,D07-1101,0,0.261932,"the first child of a given word. The ability to handle such “ordered” features is intimately associated with Eisner’s dynamic programming parsing algorithm and with the Markovian assumptions made explicitly by his generative model. We next show how similar features zijk V but this would yield a constraint matrix with O(n4 ) non-zero elements. Instead, we define auxiliary variables βjk and γij : sibl zijk ≥ zij + zik − 1 (14) for all triples hi, j, ki ∈ Rsibl , and zijk if hi, ji and hi, ki are consecutive siblings,   0 otherwise, ( first child zij As shown by McDonald and Pereira (2006) and Carreras (2007), the inclusion of features that correlate sibling and grandparent arcs may be highly beneficial, even if doing so requires resorting to approximate algorithms.7 Define Rsibl , {hi, j, ki |hi, ji ∈ A, hi, ki ∈ A} and Rgrand , {hi, j, ki |hi, ji ∈ A, hj, ki ∈ A}. To include such features in our formulation, we need to add extra variables zsibl , hzr ir∈Rsibl and zgrand , hzr ir∈Rgrand that indicate the presence of sibling and grandparent arcs. Observe that these indicator variables are conjunctions of arc indicator varisibl = z ∧ z and z grand = z ∧ z . ables, i.e., zijk ij ij ik jk ijk Hence,"
P09-1039,P04-1054,0,0.018088,"ather than algorithm design. While solving an ILP is NP-hard in general, fast solvers are available today that make it a practical solution for many NLP problems. This paper presents new, concise ILP formulations for projective and non-projective depen2 2.1 Dependency Parsing Preliminaries A dependency tree is a lightweight syntactic representation that attempts to capture functional relationships between words. Lately, this formalism has been used as an alternative to phrase-based parsing for a variety of tasks, ranging from machine translation (Ding and Palmer, 2005) to relation extraction (Culotta and Sorensen, 2004) and question answering (Wang et al., 2007). Let us first describe formally the set of legal dependency parse trees. Consider a sentence x = 342 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 342–350, c Suntec, Singapore, 2-7 August 2009. 2009 ACL and AFNLP hw0 , . . . , wn i, where wi denotes the word at the ith position, and w0 = $ is a wall symbol. We form the (complete1 ) directed graph D = hV, Ai, with vertices in V = {0, . . . , n} (the i-th vertex corresponding to the i-th word) and arcs in A = V 2 . Using terminology from graph theory, we say t"
P09-1039,N07-1030,0,0.0284769,"e also compatible with scenarios where expert knowledge is available, for example in the form of hard or soft firstorder logic constraints (Richardson and Domingos, 2006; Chang et al., 2008). Much attention has recently been devoted to integer linear programming (ILP) formulations of NLP problems, with interesting results in applications like semantic role labeling (Roth and Yih, 2005; Punyakanok et al., 2004), dependency parsing (Riedel and Clarke, 2006), word alignment for machine translation (Lacoste-Julien et al., 2006), summarization (Clarke and Lapata, 2008), and coreference resolution (Denis and Baldridge, 2007), among others. In general, the rationale for the development of ILP formulations is to incorporate non-local features or global constraints, which are often difficult to handle with traditional algorithms. ILP formulations focus more on the modeling of problems, rather than algorithm design. While solving an ILP is NP-hard in general, fast solvers are available today that make it a practical solution for many NLP problems. This paper presents new, concise ILP formulations for projective and non-projective depen2 2.1 Dependency Parsing Preliminaries A dependency tree is a lightweight syntactic"
P09-1039,P05-1067,0,0.026854,"tions focus more on the modeling of problems, rather than algorithm design. While solving an ILP is NP-hard in general, fast solvers are available today that make it a practical solution for many NLP problems. This paper presents new, concise ILP formulations for projective and non-projective depen2 2.1 Dependency Parsing Preliminaries A dependency tree is a lightweight syntactic representation that attempts to capture functional relationships between words. Lately, this formalism has been used as an alternative to phrase-based parsing for a variety of tasks, ranging from machine translation (Ding and Palmer, 2005) to relation extraction (Culotta and Sorensen, 2004) and question answering (Wang et al., 2007). Let us first describe formally the set of legal dependency parse trees. Consider a sentence x = 342 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 342–350, c Suntec, Singapore, 2-7 August 2009. 2009 ACL and AFNLP hw0 , . . . , wn i, where wi denotes the word at the ith position, and w0 = $ is a wall symbol. We form the (complete1 ) directed graph D = hV, Ai, with vertices in V = {0, . . . , n} (the i-th vertex corresponding to the i-th word) and arcs in A ="
P09-1039,P99-1059,0,0.0534909,"grammar b of Wang and Har work on empiric note include the w ing systems, no showing that the of Wang and Ha note include the showing that th shown by McDonald et al. (2005), is an instance of the maximal arborescence problem. Combinatorial algorithms (Chu and Liu, 1965; Edmonds, 1967) can solve this problem in cubic time.4 If the dependency parse trees are restricted to be projective, cubic-time algorithms are available via dynamic programming (Eisner, 1996). While in the projective case, the arc-factored assumption can be weakened in certain ways while maintaining polynomial parser runtime (Eisner and Satta, 1999), the same does not happen in the nonprojective case, where finding the highest-scoring tree becomes NP-hard (McDonald and Satta, 2007). Approximate algorithms have been employed to handle models that are not arc-factored (although features are still fairly local): McDonald and Pereira (2006) adopted an approximation based on O(n3 ) projective parsing followed by a hillclimbing algorithm to rearrange arcs, and Smith and Eisner (2008) proposed an algorithm based on loopy belief propagation. 3 fact, fast solvers are available today that make this a practical solution for many problems. Their per"
P09-1039,C96-1058,0,0.889999,"ss; the typical loss functhat they can be used in many important learning tions over all possible dependency graphs for a given problems including min-risk decodtionandis inference the Hamming loss, `(y 0 ; y) , |{hi, ji ∈ sentence. To motivate these algorithms, we show ing, training globally normalized log-linear mody 0 that : hi,they ji ∈ /cany}|. Tractability is usually ensured be used in many important learning els, syntactic language modeling, and unsupervised byand strong factorization like the one inference problemsassumptions, including min-risk decodunderlying the arc-factored model (Eisner, 1996; ing, training globally normalized log-linear modMcDonald et al., 2005),modeling, which forbids any feature els, syntactic language and unsupervised that depends on two or more arcs. This induces a decomposition of the feature vector f (x, y) as: 1 The general case where A ⊆ V 2 is also of interest; it arises whenever a constraint or a lexicon forbids some arcs from appearing in dependency tree. It may also arise as a consequence of a first-stage pruning step where some candidate arcs are eliminated; this will be further discussed in §4. 2 Or “directed spanning tree with designated root r.” 3"
P09-1039,P98-1106,0,0.0175876,"0, . . . , n} (the i-th vertex corresponding to the i-th word) and arcs in A = V 2 . Using terminology from graph theory, we say that B ⊆ A is an r-arborescence2 of the directed graph D if hV, Bi is a (directed) tree rooted at r. We define the set of legal dependency parse trees of x (denoted Y(x)) as the set of 0-arborescences of D, i.e., we admit each arborescence as a potential dependency tree. Let y ∈ Y(x) be a legal dependency tree for x; if the arc a = hi, ji ∈ y, we refer to i as the parent of j (denoted i = π(j)) and j as a child of i. We also say that a is projective (in the sense of Kahane et al., 1998) if any vertex k in the span of a is reachable from i (in other words, if for any k satisfying min(i, j) &lt; k &lt; max(i, j), there is a directed path in y from i to k). A dependency tree is called projective if it only contains projective arcs. Fig. 1 illustrates this concept.3 The formulation to be introduced in §3 makes use of the notion of the incidence vector associated with a dependency tree y ∈ Y(x). This is the binary vector z , hza ia∈A with each component defined as za = I(a ∈ y) (here, I(.) denotes the indicator function). Considering simultaneously all incidence vectors of legal depend"
P09-1039,N06-1015,0,0.00916773,"of the new parsers on standard parsing tasks in seven languages. The techniques that we present are also compatible with scenarios where expert knowledge is available, for example in the form of hard or soft firstorder logic constraints (Richardson and Domingos, 2006; Chang et al., 2008). Much attention has recently been devoted to integer linear programming (ILP) formulations of NLP problems, with interesting results in applications like semantic role labeling (Roth and Yih, 2005; Punyakanok et al., 2004), dependency parsing (Riedel and Clarke, 2006), word alignment for machine translation (Lacoste-Julien et al., 2006), summarization (Clarke and Lapata, 2008), and coreference resolution (Denis and Baldridge, 2007), among others. In general, the rationale for the development of ILP formulations is to incorporate non-local features or global constraints, which are often difficult to handle with traditional algorithms. ILP formulations focus more on the modeling of problems, rather than algorithm design. While solving an ILP is NP-hard in general, fast solvers are available today that make it a practical solution for many NLP problems. This paper presents new, concise ILP formulations for projective and non-pr"
P09-1039,D08-1017,1,0.875982,": • Disabled arcs do not carry any flow: φka ≤ za , a ∈ A, k ∈ V (20)  0 0   i ≤ π(k) ≤ j , if i0 &lt; k &lt; j 0 , π(k) &lt; i0 ∨ π(k) &gt; j 0 , if k &lt; i0 or k &gt; j 0   or k = i. • There are exactly n enabled arcs: P a∈A za =n (21) 347 Then, Y(x) will be redefined as the set of projective dependency parse trees. baselines, all of them state-of-the-art parsers based on non-arc-factored models: the second order model of McDonald and Pereira (2006), the hybrid model of Nivre and McDonald (2008), which combines a (labeled) transition-based and a graphbased parser, and a refinement of the latter, due to Martins et al. (2008), which attempts to approximate non-local features.13 We did not reproduce the model of Riedel and Clarke (2006) since the latter is tailored for labeled dependency parsing; however, experiments reported in that paper for Dutch (and extended to other languages in the CoNLL-X task) suggest that their model performs worse than our three baselines. By looking at the middle four columns, we can see that adding non-arc-factored features makes the models more accurate, for all languages. With the exception of Portuguese, the best results are achieved with the full set of features. We can also observ"
P09-1039,E06-1011,0,0.807214,"n solve this problem in cubic time.4 If the dependency parse trees are restricted to be projective, cubic-time algorithms are available via dynamic programming (Eisner, 1996). While in the projective case, the arc-factored assumption can be weakened in certain ways while maintaining polynomial parser runtime (Eisner and Satta, 1999), the same does not happen in the nonprojective case, where finding the highest-scoring tree becomes NP-hard (McDonald and Satta, 2007). Approximate algorithms have been employed to handle models that are not arc-factored (although features are still fairly local): McDonald and Pereira (2006) adopted an approximation based on O(n3 ) projective parsing followed by a hillclimbing algorithm to rearrange arcs, and Smith and Eisner (2008) proposed an algorithm based on loopy belief propagation. 3 fact, fast solvers are available today that make this a practical solution for many problems. Their performance depends on the dimensions and degree of sparsity of the constraint matrix A. Riedel and Clarke (2006) proposed an ILP formulation for dependency parsing which refines the arc-factored model by imposing linguistically motivated “hard” constraints that forbid some arc configurations. T"
P09-1039,C98-1102,0,\N,Missing
P09-1053,W04-3219,0,0.0968852,"We estimate the distributions over dependency labels, POS tags, and named entity classes using the transformed treebank (footnote 4). The distribution over words is taken from the Gigaword corpus (as in §3.4). It is important to note that G0 is designed to give a smoothed estimate of the probability of a particular parsed, named entity-tagged sentence. It is never used for parsing or for generation; it is only used as a component in the generative probability model presented in §2 (Eq. 2). 3.6 4 In all our experiments, we have used the Microsoft Research Paraphrase Corpus (Dolan et al., 2004; Quirk et al., 2004). The corpus contains 5,801 pairs of sentences that have been marked as “equivalent” or “not equivalent.” It was constructed from thousands of news sources on the web. Dolan and Brockett (2005) remark that this corpus was created semi-automatically by first training an SVM classifier on a disjoint annotated 10,000 sentence pair dataset and then applying the SVM on an unseen 49,375 sentence pair corpus, with its output probabilities skewed towards over-identification, i.e., towards generating some false paraphrases. 5,801 out of these 49,375 pairs were randomly selected and presented to human j"
P09-1053,W96-0213,0,0.665591,"ank (see footnote 4). For unobserved cases, the conditional probability is estimated by backing off to the parent POS tag and child direction. We discuss next how to parameterize the probability pkid that appears in Equations 4, 5, and 6. This conditional distribution forms the core of our QGs, and we deviate from earlier research using QGs in defining pkid in a fully generative way. In addition to assuming that dependency parse trees for s and t are observable, we also assume each word wi comes with POS and named entity tags. In our experiments these were obtained automatically using MXPOST (Ratnaparkhi, 1996) and BBN’s Identifinder (Bikel et al., 1999). s t, x P (τ t,j |tj , x(j), τ s ) j∈λt (i)∪ρt (i) x(j)=0 pQ (t |Gp (s)) = p(τ |Gp (τ )) = m X Y |Gp (τ s )) (3) Because the QG is essentially a context-free dependency grammar, we can factor it into recursive steps as follows (let i be an arbitrary index in {1, ..., n}): P (τ t,i |ti , x(i), τ s ) = pval (|λt (i)|, |ρt (i) ||ti ) 4 In our experiments, we use the parser described by McDonald et al. (2005), trained on sections 2–21 of the WSJ Penn Treebank, transformed to dependency trees following Yamada and Matsumoto (2003). (The same treebank data"
P09-1053,W06-3104,0,0.0876301,"niversity Pittsburgh, PA 15213, USA {dipanjan,nasmith}@cs.cmu.edu Abstract task (Zhang and Patrick, 2005; Finch et al., 2005; Wan et al., 2006; Corley and Mihalcea, 2005, inter alia), but do not explicitly model correspondence structure (or “alignment”) between the parts of two sentences. In this paper, we adopt a model that posits correspondence between the words in the two sentences, defining it in loose syntactic terms: if two sentences are paraphrases, we expect their dependency trees to align closely, though some divergences are also expected, with some more likely than others. Following Smith and Eisner (2006), we adopt the view that the syntactic structure of sentences paraphrasing some sentence s should be “inspired” by the structure of s. Because dependency syntax is still only a crude approximation to semantic structure, we augment the model with a lexical semantics component, based on WordNet (Miller, 1995), that models how words are probabilistically altered in generating a paraphrase. This combination of loose syntax and lexical semantics is similar to the “Jeopardy” model of Wang et al. (2007). This syntactic framework represents a major departure from useful and popular surface similarity"
P09-1053,2003.mtsummit-papers.51,0,0.036401,"Missing"
P09-1053,U06-1019,0,0.715078,"generating a paraphrase. This combination of loose syntax and lexical semantics is similar to the “Jeopardy” model of Wang et al. (2007). This syntactic framework represents a major departure from useful and popular surface similarity features, and the latter are difficult to incorporate into our probabilistic model. We use a product of experts (Hinton, 2002) to bring together a logistic regression classifier built from n-gram overlap features and our syntactic model. This combined model leverages complementary strengths of the two approaches, outperforming a strong state-ofthe-art baseline (Wan et al., 2006). This paper is organized as follows. We introduce our probabilistic model in §2. The model makes use of three quasi-synchronous grammar models (Smith and Eisner, 2006, QG, hereafter) as components (one modeling paraphrase, one modeling not-paraphrase, and one a base grammar); these are detailed, along with latent-variable inference and discriminative training algorithms, in §3. We discuss the Microsoft Research Paraphrase Corpus, upon which we conduct experiments, in §4. In §5, we present experiments on paraphrase We present a novel approach to deciding whether two sentences hold a paraphrase"
P09-1053,D07-1003,1,0.749287,"sely, though some divergences are also expected, with some more likely than others. Following Smith and Eisner (2006), we adopt the view that the syntactic structure of sentences paraphrasing some sentence s should be “inspired” by the structure of s. Because dependency syntax is still only a crude approximation to semantic structure, we augment the model with a lexical semantics component, based on WordNet (Miller, 1995), that models how words are probabilistically altered in generating a paraphrase. This combination of loose syntax and lexical semantics is similar to the “Jeopardy” model of Wang et al. (2007). This syntactic framework represents a major departure from useful and popular surface similarity features, and the latter are difficult to incorporate into our probabilistic model. We use a product of experts (Hinton, 2002) to bring together a logistic regression classifier built from n-gram overlap features and our syntactic model. This combined model leverages complementary strengths of the two approaches, outperforming a strong state-ofthe-art baseline (Wan et al., 2006). This paper is organized as follows. We introduce our probabilistic model in §2. The model makes use of three quasi-syn"
P09-1053,J97-3002,0,0.114124,"Missing"
P09-1053,W05-1205,0,0.240934,"Missing"
P09-1053,W03-3023,0,0.0319775,"ained automatically using MXPOST (Ratnaparkhi, 1996) and BBN’s Identifinder (Bikel et al., 1999). s t, x P (τ t,j |tj , x(j), τ s ) j∈λt (i)∪ρt (i) x(j)=0 pQ (t |Gp (s)) = p(τ |Gp (τ )) = m X Y |Gp (τ s )) (3) Because the QG is essentially a context-free dependency grammar, we can factor it into recursive steps as follows (let i be an arbitrary index in {1, ..., n}): P (τ t,i |ti , x(i), τ s ) = pval (|λt (i)|, |ρt (i) ||ti ) 4 In our experiments, we use the parser described by McDonald et al. (2005), trained on sections 2–21 of the WSJ Penn Treebank, transformed to dependency trees following Yamada and Matsumoto (2003). (The same treebank data were also to estimate many of the parameters of our model, as discussed in the text.) Though it leads to a partial “pipeline” approximation of the posterior probability p(c |s, t), we believe that the relatively high quality of English dependency parsing makes this approximation reasonable. 470 For clarity, let j = τpt (i) and let l = x(j). Configuration Description parent-child τps (x(i)) = x(j), appended with τ`s (x(i)) child-parent x(i) = τps (x(j)), appended with τ`s (x(j)) grandparent- τps (τps (x(i))) = x(j), appended with grandchild τ`s (x(i)) siblings τps (x(i"
P09-1053,U05-1023,0,0.132167,"Missing"
P09-1053,N03-1003,0,0.135255,"atures. We evaluate our models on the task of distinguishing true paraphrase pairs from false ones on a standard corpus, giving competitive state-of-the-art performance. 1 Introduction The problem of modeling paraphrase relationships between natural language utterances (McKeown, 1979) has recently attracted interest. For computational linguists, solving this problem may shed light on how best to model the semantics of sentences. For natural language engineers, the problem bears on information management systems like abstractive summarizers that must measure semantic overlap between sentences (Barzilay and Lee, 2003), question answering modules (Marsi and Krahmer, 2005) and machine translation (Callison-Burch et al., 2006). The paraphrase identification problem asks whether two sentences have essentially the same meaning. Although paraphrase identification is defined in semantic terms, it is usually solved using statistical classifiers based on shallow lexical, n-gram, and syntactic “overlap” features. Such overlap features give the best-published classification accuracy for the paraphrase identification 468 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 468–476,"
P09-1053,N06-1003,0,0.050362,"standard corpus, giving competitive state-of-the-art performance. 1 Introduction The problem of modeling paraphrase relationships between natural language utterances (McKeown, 1979) has recently attracted interest. For computational linguists, solving this problem may shed light on how best to model the semantics of sentences. For natural language engineers, the problem bears on information management systems like abstractive summarizers that must measure semantic overlap between sentences (Barzilay and Lee, 2003), question answering modules (Marsi and Krahmer, 2005) and machine translation (Callison-Burch et al., 2006). The paraphrase identification problem asks whether two sentences have essentially the same meaning. Although paraphrase identification is defined in semantic terms, it is usually solved using statistical classifiers based on shallow lexical, n-gram, and syntactic “overlap” features. Such overlap features give the best-published classification accuracy for the paraphrase identification 468 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 468–476, c Suntec, Singapore, 2-7 August 2009. 2009 ACL and AFNLP identification with our model and make comparisons"
P09-1053,W05-1203,0,0.0490343,"Missing"
P09-1053,I05-5002,0,0.0364744,"d corpus (as in §3.4). It is important to note that G0 is designed to give a smoothed estimate of the probability of a particular parsed, named entity-tagged sentence. It is never used for parsing or for generation; it is only used as a component in the generative probability model presented in §2 (Eq. 2). 3.6 4 In all our experiments, we have used the Microsoft Research Paraphrase Corpus (Dolan et al., 2004; Quirk et al., 2004). The corpus contains 5,801 pairs of sentences that have been marked as “equivalent” or “not equivalent.” It was constructed from thousands of news sources on the web. Dolan and Brockett (2005) remark that this corpus was created semi-automatically by first training an SVM classifier on a disjoint annotated 10,000 sentence pair dataset and then applying the SVM on an unseen 49,375 sentence pair corpus, with its output probabilities skewed towards over-identification, i.e., towards generating some false paraphrases. 5,801 out of these 49,375 pairs were randomly selected and presented to human judges for refinement into true and false paraphrases. 3,900 of the pairs were marked as having Discriminative Training D (i) EN (i) Given training data hs1 , s2 , c(i) i , we train i=1 the mode"
P09-1053,C04-1051,0,0.650031,"(s |G0 ) = C 0 (0). We estimate the distributions over dependency labels, POS tags, and named entity classes using the transformed treebank (footnote 4). The distribution over words is taken from the Gigaword corpus (as in §3.4). It is important to note that G0 is designed to give a smoothed estimate of the probability of a particular parsed, named entity-tagged sentence. It is never used for parsing or for generation; it is only used as a component in the generative probability model presented in §2 (Eq. 2). 3.6 4 In all our experiments, we have used the Microsoft Research Paraphrase Corpus (Dolan et al., 2004; Quirk et al., 2004). The corpus contains 5,801 pairs of sentences that have been marked as “equivalent” or “not equivalent.” It was constructed from thousands of news sources on the web. Dolan and Brockett (2005) remark that this corpus was created semi-automatically by first training an SVM classifier on a disjoint annotated 10,000 sentence pair dataset and then applying the SVM on an unseen 49,375 sentence pair corpus, with its output probabilities skewed towards over-identification, i.e., towards generating some false paraphrases. 5,801 out of these 49,375 pairs were randomly selected and"
P09-1053,W07-1401,0,0.0226089,"L Wan et al. SVM and pQ pQ and pL Accuracy 66.49 75.63 75.42 68.64 73.33 73.86 75.36 76.06 80.17 83.42 83.19 Results Recall 100.00 90.00 90.14 96.51 91.10 91.28 87.44 86.05 92.07 96.60 95.29 Table 2: Accuracy, p-class precision, and p-class recall on the test set (N = 1,725). See text for differences in implementation between Wan et al. and our replication; their reported score does not include the full test set. the more intricate QG to the straightforward SVM. First, the QG discovers hidden alignments between words. Alignments have been leveraged in related tasks such as textual entailment (Giampiccolo et al., 2007); they make the model more interpretable in analyzing system output (e.g., Fig. 2). Second, the paraphrases of a sentence can be considered to be monolingual translations. We model the paraphrase problem using a direct machine translation model, thus providing a translation interpretation of the problem. This framework could be extended to permit paraphrase generation, or to exploit other linguistic annotations, such as representations of semantics (see, e.g., Qiu et al., 2006). Nonetheless, the usefulness of surface overlap features is difficult to ignore. We next provide an efficient way to"
P09-1053,W05-1612,0,0.0146939,"uishing true paraphrase pairs from false ones on a standard corpus, giving competitive state-of-the-art performance. 1 Introduction The problem of modeling paraphrase relationships between natural language utterances (McKeown, 1979) has recently attracted interest. For computational linguists, solving this problem may shed light on how best to model the semantics of sentences. For natural language engineers, the problem bears on information management systems like abstractive summarizers that must measure semantic overlap between sentences (Barzilay and Lee, 2003), question answering modules (Marsi and Krahmer, 2005) and machine translation (Callison-Burch et al., 2006). The paraphrase identification problem asks whether two sentences have essentially the same meaning. Although paraphrase identification is defined in semantic terms, it is usually solved using statistical classifiers based on shallow lexical, n-gram, and syntactic “overlap” features. Such overlap features give the best-published classification accuracy for the paraphrase identification 468 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 468–476, c Suntec, Singapore, 2-7 August 2009. 2009 ACL and AFN"
P09-1053,P05-1012,0,0.0848019,"t are observable, we also assume each word wi comes with POS and named entity tags. In our experiments these were obtained automatically using MXPOST (Ratnaparkhi, 1996) and BBN’s Identifinder (Bikel et al., 1999). s t, x P (τ t,j |tj , x(j), τ s ) j∈λt (i)∪ρt (i) x(j)=0 pQ (t |Gp (s)) = p(τ |Gp (τ )) = m X Y |Gp (τ s )) (3) Because the QG is essentially a context-free dependency grammar, we can factor it into recursive steps as follows (let i be an arbitrary index in {1, ..., n}): P (τ t,i |ti , x(i), τ s ) = pval (|λt (i)|, |ρt (i) ||ti ) 4 In our experiments, we use the parser described by McDonald et al. (2005), trained on sections 2–21 of the WSJ Penn Treebank, transformed to dependency trees following Yamada and Matsumoto (2003). (The same treebank data were also to estimate many of the parameters of our model, as discussed in the text.) Though it leads to a partial “pipeline” approximation of the posterior probability p(c |s, t), we believe that the relatively high quality of English dependency parsing makes this approximation reasonable. 470 For clarity, let j = τpt (i) and let l = x(j). Configuration Description parent-child τps (x(i)) = x(j), appended with τ`s (x(i)) child-parent x(i) = τps (x"
P09-1053,P79-1016,0,0.676755,"paraphrase relationship. The model cleanly incorporates both syntax and lexical semantics using quasi-synchronous dependency grammars (Smith and Eisner, 2006). Furthermore, using a product of experts (Hinton, 2002), we combine the model with a complementary logistic regression model based on state-of-the-art lexical overlap features. We evaluate our models on the task of distinguishing true paraphrase pairs from false ones on a standard corpus, giving competitive state-of-the-art performance. 1 Introduction The problem of modeling paraphrase relationships between natural language utterances (McKeown, 1979) has recently attracted interest. For computational linguists, solving this problem may shed light on how best to model the semantics of sentences. For natural language engineers, the problem bears on information management systems like abstractive summarizers that must measure semantic overlap between sentences (Barzilay and Lee, 2003), question answering modules (Marsi and Krahmer, 2005) and machine translation (Callison-Burch et al., 2006). The paraphrase identification problem asks whether two sentences have essentially the same meaning. Although paraphrase identification is defined in sem"
P09-1053,P04-1083,0,0.0205113,"which came first (i.e., which was s and which was s0 ). Both orderings are assumed to be equally probable. For class c, ht, li → ht, liht0 , ki or ht, li → ht0 , kiht, li where t and t0 range over the vocabulary of the target language, and l and k ∈ {0, ..., m} are indices in the source sentence, with 0 denoting null.3 Hard or soft constraints can be applied between l and k in a rule. These constraints imply permissible “configurations.” For example, requiring l 6= 0 and, if k 6= 0 then sk must be a child of sl in the source tree, we can implement a synchronous dependency grammar similar to (Melamed, 2004). Smith and Eisner (2006) used a quasisynchronous grammar to discover the correspondence between words implied by the correspondence between the trees. We follow Wang et al. (2007) in treating the correspondences as latent variables, and in using a WordNet-based lexical semantics model to generate the target words. pQ (s1 , s2 |c) = 0.5 × pQ (s1 |G0 ) × pQ (s2 |Gc (s1 )) + 0.5 × pQ (s2 |G0 ) × pQ (s1 |Gc (s2 ))(2) where c can be p or n; Gp (s) is the QG that generates paraphrases for sentence s, while Gn (s) is the QG that generates sentences that are not paraphrases of sentence s. This latter"
P09-1053,2001.mtsummit-papers.68,0,0.0416791,"Missing"
P09-1053,W06-1603,0,0.617126,"hidden alignments between words. Alignments have been leveraged in related tasks such as textual entailment (Giampiccolo et al., 2007); they make the model more interpretable in analyzing system output (e.g., Fig. 2). Second, the paraphrases of a sentence can be considered to be monolingual translations. We model the paraphrase problem using a direct machine translation model, thus providing a translation interpretation of the problem. This framework could be extended to permit paraphrase generation, or to exploit other linguistic annotations, such as representations of semantics (see, e.g., Qiu et al., 2006). Nonetheless, the usefulness of surface overlap features is difficult to ignore. We next provide an efficient way to combine a surface model with pQ . Tab. 2 shows performance achieved by the baseline SVM and variations on pQ on the test set. We performed a few feature ablation studies, evaluating on the development data. We removed the lexical semantics component of the QG,10 and disallowed the syntactic configurations one by one, to investigate which components of pQ contributes to system performance. The lexical semantics component is critical, as seen by the drop in accuracy from the tabl"
P09-1053,P02-1040,0,\N,Missing
P09-1053,I05-5003,0,\N,Missing
P09-2001,P09-2001,1,0.0512899,"Missing"
P09-2001,P04-1061,0,0.0881175,", observed data x, ˜ r : N → 24r annealing schedule 4 Output: learned parameters α and approximate posterior q(θ, y) t ← 1; repeat E-step: repeat (t+1) E-step: forall i ∈ [r] do: qi (y) ← argmax = i=1 q(y)∈Qi P (t) (t) F 0 ( j6=i λj qi (θ)q(y) + λi qi q(y), α(t) ) (t+1) M-step: forall i ∈ [r] do: qi q(θ)∈Qi ˜ r (t) λ∈4 3 until convergence ; M-step: α(t+1) ← P (t+1) (t+1) argmax F 0 ( ri=1 λi qi (θ)qi (y), α) Experiments We tested our method on the unsupervised learning problem of dependency grammar induction. For the generative model, we used the dependency model with valence as it appears in Klein and Manning (2004). We used the data from the Chinese treebank (Xue et al., 2004). Following standard practice, sentences were stripped of words and punctuation, leaving part-of-speech tags for the unsupervised induction of dependency structure, and sentences of length more than 10 were removed from the set. We experimented with a Dirichlet prior over the parameters and logistic normal priors over the parameters, and found the latter to still be favorable with our method, as in Cohen et al. (2008). We therefore report results with our method only for the logistic normal prior. We do inference on sections 1–270"
P09-2001,P92-1017,0,0.198447,"r by constraining it to be from a mixture family of distributions. We will use x to denote observable random variables, y to denote hidden structure, and θ to denote the to-be-learned parameters of the model (coming from a subset of R` for some `). α will denote the parameters of a prior over θ. The mean-field assumption in the Bayesian setting assumes that the posterior has a factored form: Introduction Learning natural language in an unsupervised way commonly involves the expectation-maximization (EM) algorithm to optimize the parameters of a generative model, often a probabilistic grammar (Pereira and Schabes, 1992). Later approaches include variational EM in a Bayesian setting (Beal and Gharamani, 2003), which has been shown to obtain even better results for various natural language tasks over EM (e.g., Cohen et al., 2008). Variational EM usually makes the mean-field assumption, factoring the posterior over hidden variables into independent distributions. Bishop et al. (1998) showed how to use a less strict assumption: a mixture of factorized distributions. In other work, soft or hard constraints on the posterior during the E-step have been explored in order to improve performance. For example, Smith an"
P09-2001,P06-1072,1,0.948292,"s, 1992). Later approaches include variational EM in a Bayesian setting (Beal and Gharamani, 2003), which has been shown to obtain even better results for various natural language tasks over EM (e.g., Cohen et al., 2008). Variational EM usually makes the mean-field assumption, factoring the posterior over hidden variables into independent distributions. Bishop et al. (1998) showed how to use a less strict assumption: a mixture of factorized distributions. In other work, soft or hard constraints on the posterior during the E-step have been explored in order to improve performance. For example, Smith and Eisner (2006) have penalized the approximate posterior over dependency structures in a natural language grammar induction task to avoid long range dependencies between words. Grac¸a et al. (2007) added linear constraints on expected values of features of the hidden variables in an alignment task. In this paper, we use posterior mixtures to inject bias or prior knowledge into a Bayesian model. q(θ, y) = q(θ)q(y) (1) Traditionally, variational inference with the meanfield assumption alternates between an E-step which optimizes q(y) and then an M-step which optimizes q(θ).1 The mean-field assumption makes inf"
P09-2026,A00-2018,0,0.0079424,"riminative online learning to train feature weights. A key 2 Experimental Paradigm Supervised approaches to sentence compression typically use parallel corpora consisting of original and compressed sentences (paired corpus, henceforth). In this paper, we will refer to these pairs as a 2-tuple <x, y&gt;, where x is the original sentence and y is the compressed sentence. We implemented the M06 system as an experimental framework in which to conduct our investigation. The system uses as input the paired corpus, the corresponding POS tagged corpus, the paired corpus parsed using the Charniak parser (Charniak, 2000), and dependency parses from the MST parser (McDonald et al., 2005). Features are extracted over adjacent pairs of words in the compressed sentence and weights are learnt at training time using the MIRA algorithm (Crammer and Singer, 2003). We decode as follows to find the best compression: Let the score of a compression y for a sentence x be s(x, y). This score is factored using a first-order Markov assumption over the words in the compressed sentence, and is defined by the dot product between a high dimensional feature representation and a corresponding weight vector (for details, refer to M"
P09-2026,N09-2037,1,0.847565,"Missing"
P09-2026,N03-1020,0,0.136449,"Missing"
P09-2026,E06-1038,0,0.0665047,"), and dependency parses from the MST parser (McDonald et al., 2005). Features are extracted over adjacent pairs of words in the compressed sentence and weights are learnt at training time using the MIRA algorithm (Crammer and Singer, 2003). We decode as follows to find the best compression: Let the score of a compression y for a sentence x be s(x, y). This score is factored using a first-order Markov assumption over the words in the compressed sentence, and is defined by the dot product between a high dimensional feature representation and a corresponding weight vector (for details, refer to McDonald, 2006). The equations for decoding are as follows: C[1] 0.0 C[i] max j i C[ j ] s( x, j, i), i 1 101 Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 101–104, c Suntec, Singapore, 4 August 2009. 2009 ACL and AFNLP where C is the dynamic programming table and C[i] represents the highest score for compressions ending at word i for the sentence x. The M06 system takes the best scoring compression from the set of all possible compressions. In the ARC system, the model determines the compression rate and enforces a target compression length by altering the dynamic programming algorithm a"
P09-2026,P05-1012,0,0.0167782,"Experimental Paradigm Supervised approaches to sentence compression typically use parallel corpora consisting of original and compressed sentences (paired corpus, henceforth). In this paper, we will refer to these pairs as a 2-tuple <x, y&gt;, where x is the original sentence and y is the compressed sentence. We implemented the M06 system as an experimental framework in which to conduct our investigation. The system uses as input the paired corpus, the corresponding POS tagged corpus, the paired corpus parsed using the Charniak parser (Charniak, 2000), and dependency parses from the MST parser (McDonald et al., 2005). Features are extracted over adjacent pairs of words in the compressed sentence and weights are learnt at training time using the MIRA algorithm (Crammer and Singer, 2003). We decode as follows to find the best compression: Let the score of a compression y for a sentence x be s(x, y). This score is factored using a first-order Markov assumption over the words in the compressed sentence, and is defined by the dot product between a high dimensional feature representation and a corresponding weight vector (for details, refer to McDonald, 2006). The equations for decoding are as follows: C[1] 0.0"
P09-2026,N03-1026,0,0.0311585,"oduces significantly improved compressions across a range of compression rates compared to existing state-of-the-art approaches. Thus, we name our system for generating compressions the Adjustable Rate Compressor (ARC). Knight and Marcu (2000) (K&M, henceforth) presented two approaches to the sentence compression problem: one using a noisy channel model, the other using a decision-based model. The performances of the two models were comparable though their experiments suggested that the noisy channel model degraded more smoothly than the decision-based model when tested on out-of-domain data. Riezler et al. (2003) applied linguistically rich LFG grammars to a sentence compression system. Turner and Charniak (2005) achieved similar performance to K&M using an unsupervised approach that induced rules from the Penn Treebank. A variety of feature encodings have previously been explored for the problem of sentence compression. Clarke and Lapata (2007) included discourse level features in their framework to leverage context for enhancing coherence. McDonald’s (2006) model (M06, henceforth) is similar to K&M except that it uses discriminative online learning to train feature weights. A key 2 Experimental Para"
P09-2026,D07-1001,0,\N,Missing
P10-1152,N09-1009,1,0.873848,"ns and grammar weights for a probabilistic context-free grammar, the problem approximately solved by “Viterbi training.” We show that solving and even approximating Viterbi training for PCFGs is NP-hard. We motivate the use of uniformat-random initialization for Viterbi EM as an optimal initializer in absence of further information about the correct model parameters, providing an approximate bound on the log-likelihood. 1 Introduction Probabilistic context-free grammars are an essential ingredient in many natural language processing models (Charniak, 1997; Collins, 2003; Johnson et al., 2006; Cohen and Smith, 2009, inter alia). Various algorithms for training such models have been proposed, including unsupervised methods. Many of these are based on the expectationmaximization (EM) algorithm. There are alternatives to EM, and one such alternative is Viterbi EM, also called “hard” EM or “sparse” EM (Neal and Hinton, 1998). Instead of using the parameters (which are maintained in the algorithm’s current state) to find the true posterior over the derivations, Viterbi EM algorithm uses a posterior focused on the Viterbi parse of those parameters. Viterbi EM and variants have been used in various settings in"
P10-1152,J03-4003,0,0.549798,"lihood assignment of hidden derivations and grammar weights for a probabilistic context-free grammar, the problem approximately solved by “Viterbi training.” We show that solving and even approximating Viterbi training for PCFGs is NP-hard. We motivate the use of uniformat-random initialization for Viterbi EM as an optimal initializer in absence of further information about the correct model parameters, providing an approximate bound on the log-likelihood. 1 Introduction Probabilistic context-free grammars are an essential ingredient in many natural language processing models (Charniak, 1997; Collins, 2003; Johnson et al., 2006; Cohen and Smith, 2009, inter alia). Various algorithms for training such models have been proposed, including unsupervised methods. Many of these are based on the expectationmaximization (EM) algorithm. There are alternatives to EM, and one such alternative is Viterbi EM, also called “hard” EM or “sparse” EM (Neal and Hinton, 1998). Instead of using the parameters (which are maintained in the algorithm’s current state) to find the true posterior over the derivations, Viterbi EM algorithm uses a posterior focused on the Viterbi parse of those parameters. Viterbi EM and v"
P10-1152,P08-2007,0,0.39949,"methods. Many of these are based on the expectationmaximization (EM) algorithm. There are alternatives to EM, and one such alternative is Viterbi EM, also called “hard” EM or “sparse” EM (Neal and Hinton, 1998). Instead of using the parameters (which are maintained in the algorithm’s current state) to find the true posterior over the derivations, Viterbi EM algorithm uses a posterior focused on the Viterbi parse of those parameters. Viterbi EM and variants have been used in various settings in natural language processing (Yejin and Cardie, 2007; Wang et al., 2007; Goldwater and Johnson, 2005; DeNero and Klein, 2008; Spitkovsky et al., 2010). Viterbi EM can be understood as a coordinate ascent procedure that locally optimizes a function; we call this optimization goal “Viterbi training.” In this paper, we explore Viterbi training for probabilistic context-free grammars. We first show that under the assumption that P 6= NP, solving and even approximating the Viterbi training problem is hard. This result holds even for hidden Markov models. We extend the main hardness result to the EM algorithm (giving an alternative proof to this known result), as well as the problem of conditional Viterbi training. We th"
P10-1152,N07-1018,0,0.0277163,"ions, we showed that solving Viterbi training is hard, and therefore requires an approximation algorithm. Viterbi EM, which is an example of such algorithm, is dependent on an initialization of either θ to start with an E-step or z to start with an M-step. In the absence of a betterinformed initializer, it is reasonable to initialize z using a uniform distribution over D(G, xi ) for each i. If D(G, xi ) is finite, it can be done efficiently by setting θ = 1 (ignoring the normalization constraint), running the inside algorithm, and sampling from the (unnormalized) posterior given by the chart (Johnson et al., 2007). We turn next to an analysis of this initialization technique that suggests it is well-motivated. The sketch of our result is as follows: we first give an asymptotic upper bound for the loglikelihood of derivations and sentences. This bound, which has an information-theoretic interpretation, depends on a parameter λ, which depends on the distribution from which the derivations were chosen. We then show that this bound is minimized when we pick λ such that this distribution is (conditioned on the sentence) a uniform distribution over derivations. Let q(x) be any distribution over L(G) and θ so"
P10-1152,J99-4005,0,0.0622948,"ns++. They show that their initialization is O(log k)-competitive; i.e., it approximates the optimal clusters assignment by a factor of O(log k). In §7.1, we showed that uniform-at-random initialization is approximately O(|N|Lλ2 /n)-competitive (modulo an additive constant) for CNF grammars, where n is the number of sentences, L is the total length of sentences and λ is a measure for distance between the true distribution and the uniform distribution.5 Many combinatorial problems in NLP involving phrase-structure trees, alignments, and dependency graphs are hard (Sima’an, 1996; Goodman, 1998; Knight, 1999; Casacuberta and de la Higuera, 2000; Lyngsø and Pederson, 2002; Udupa and Maji, 2006; McDonald and Satta, 2007; DeNero and Klein, 2008, inter alia). Of special relevance to this paper is Abe and Warmuth (1992), who showed that the problem of finding maximum likelihood model of probabilistic automata is hard even for a single string and an automaton with two states. Understanding the complexity of NLP problems, we believe, is crucial as we seek effective practical approximations when necessary. 9 Conclusion We described some properties of Viterbi training for probabilistic context-free gramma"
P10-1152,N06-1020,0,0.0609822,"necessary to resort to approximate algorithms like Viterbi EM. Neal and Hinton (1998) use the term “sparse EM” to refer to a version of the EM algorithm where the E-step finds the modes of hidden variables (rather than marginals as in standard EM). Viterbi EM is a variant of this, where the Estep finds the mode for each xi ’s derivation, argmaxz∈D(G,xi ) p(xi , z |θ). We will refer to L(θ, z) = n Y p(xi , zi |θ) (5) i=1 as “the objective function of ViterbiTrain.” Viterbi training and Viterbi EM are closely related to self-training, an important concept in semi-supervised NLP (Charniak, 1997; McClosky et al., 2006a; McClosky et al., 2006b). With selftraining, the model is learned with some seed annotated data, and then iterates by labeling new, unannotated data and adding it to the original annotated training set. McClosky et al. consider selftraining to be “one round of Viterbi EM” with supervised initialization using labeled seed data. We refer the reader to Abney (2007) for more details. 4 Hardness of Viterbi Training We now describe hardness results for Problem 1. We first note that the following problem is known to be NP-hard, and in fact, NP-complete (Sipser, 2006): Problem 2. 3-SAT V Input: A fo"
P10-1152,P06-1043,0,0.0329954,"necessary to resort to approximate algorithms like Viterbi EM. Neal and Hinton (1998) use the term “sparse EM” to refer to a version of the EM algorithm where the E-step finds the modes of hidden variables (rather than marginals as in standard EM). Viterbi EM is a variant of this, where the Estep finds the mode for each xi ’s derivation, argmaxz∈D(G,xi ) p(xi , z |θ). We will refer to L(θ, z) = n Y p(xi , zi |θ) (5) i=1 as “the objective function of ViterbiTrain.” Viterbi training and Viterbi EM are closely related to self-training, an important concept in semi-supervised NLP (Charniak, 1997; McClosky et al., 2006a; McClosky et al., 2006b). With selftraining, the model is learned with some seed annotated data, and then iterates by labeling new, unannotated data and adding it to the original annotated training set. McClosky et al. consider selftraining to be “one round of Viterbi EM” with supervised initialization using labeled seed data. We refer the reader to Abney (2007) for more details. 4 Hardness of Viterbi Training We now describe hardness results for Problem 1. We first note that the following problem is known to be NP-hard, and in fact, NP-complete (Sipser, 2006): Problem 2. 3-SAT V Input: A fo"
P10-1152,W07-2216,0,0.0228016,"clusters assignment by a factor of O(log k). In §7.1, we showed that uniform-at-random initialization is approximately O(|N|Lλ2 /n)-competitive (modulo an additive constant) for CNF grammars, where n is the number of sentences, L is the total length of sentences and λ is a measure for distance between the true distribution and the uniform distribution.5 Many combinatorial problems in NLP involving phrase-structure trees, alignments, and dependency graphs are hard (Sima’an, 1996; Goodman, 1998; Knight, 1999; Casacuberta and de la Higuera, 2000; Lyngsø and Pederson, 2002; Udupa and Maji, 2006; McDonald and Satta, 2007; DeNero and Klein, 2008, inter alia). Of special relevance to this paper is Abe and Warmuth (1992), who showed that the problem of finding maximum likelihood model of probabilistic automata is hard even for a single string and an automaton with two states. Understanding the complexity of NLP problems, we believe, is crucial as we seek effective practical approximations when necessary. 9 Conclusion We described some properties of Viterbi training for probabilistic context-free grammars. We showed that Viterbi training is NP-hard and, in fact, NP-hard to approximate. We gave motivation for unif"
P10-1152,C96-2215,0,0.167451,"Missing"
P10-1152,J07-4003,1,0.775492,"on 1. Indeed, UniformInit uses θ I to initialize the state of Viterbi EM. We note that if θ I was known for a specific grammar, then we could have used it as a direct initializer. However, Condition 1 only guarantees its existence, and does not give a practical way to identify it. In general, as mentioned above, θ = 1 can be used to obtain a weighted CFG that satisfies p(z |θ, x) = 1/|D(G, x)|. Since we require a uniform posterior distribution, the number of derivations of a fixed length is finite. This means that we can converted the weighted CFG with θ = 1 to a PCFG with the same posterior (Smith and Johnson, 2007), and identify the appropriate θ I . 8 Related Work Viterbi training is closely related to the k-means clustering problem, where the objective is to find k centroids for a given set of d-dimensional points such that the sum of distances between the points and the closest centroid is minimized. The analog for Viterbi EM for the k-means problem is the k-means clustering algorithm (Lloyd, 1982), a coordinate ascent algorithm for solving the k-means problem. It works by iterating between an E-likestep, in which each point is assigned the closest centroid, and an M-like-step, in which the centroids"
P10-1152,W10-2902,0,0.272166,"re based on the expectationmaximization (EM) algorithm. There are alternatives to EM, and one such alternative is Viterbi EM, also called “hard” EM or “sparse” EM (Neal and Hinton, 1998). Instead of using the parameters (which are maintained in the algorithm’s current state) to find the true posterior over the derivations, Viterbi EM algorithm uses a posterior focused on the Viterbi parse of those parameters. Viterbi EM and variants have been used in various settings in natural language processing (Yejin and Cardie, 2007; Wang et al., 2007; Goldwater and Johnson, 2005; DeNero and Klein, 2008; Spitkovsky et al., 2010). Viterbi EM can be understood as a coordinate ascent procedure that locally optimizes a function; we call this optimization goal “Viterbi training.” In this paper, we explore Viterbi training for probabilistic context-free grammars. We first show that under the assumption that P 6= NP, solving and even approximating the Viterbi training problem is hard. This result holds even for hidden Markov models. We extend the main hardness result to the EM algorithm (giving an alternative proof to this known result), as well as the problem of conditional Viterbi training. We then describe a “competitive"
P10-1152,E06-1004,0,0.0252116,"proximates the optimal clusters assignment by a factor of O(log k). In §7.1, we showed that uniform-at-random initialization is approximately O(|N|Lλ2 /n)-competitive (modulo an additive constant) for CNF grammars, where n is the number of sentences, L is the total length of sentences and λ is a measure for distance between the true distribution and the uniform distribution.5 Many combinatorial problems in NLP involving phrase-structure trees, alignments, and dependency graphs are hard (Sima’an, 1996; Goodman, 1998; Knight, 1999; Casacuberta and de la Higuera, 2000; Lyngsø and Pederson, 2002; Udupa and Maji, 2006; McDonald and Satta, 2007; DeNero and Klein, 2008, inter alia). Of special relevance to this paper is Abe and Warmuth (1992), who showed that the problem of finding maximum likelihood model of probabilistic automata is hard even for a single string and an automaton with two states. Understanding the complexity of NLP problems, we believe, is crucial as we seek effective practical approximations when necessary. 9 Conclusion We described some properties of Viterbi training for probabilistic context-free grammars. We showed that Viterbi training is NP-hard and, in fact, NP-hard to approximate. W"
P10-1152,D07-1003,1,0.908095,"dels have been proposed, including unsupervised methods. Many of these are based on the expectationmaximization (EM) algorithm. There are alternatives to EM, and one such alternative is Viterbi EM, also called “hard” EM or “sparse” EM (Neal and Hinton, 1998). Instead of using the parameters (which are maintained in the algorithm’s current state) to find the true posterior over the derivations, Viterbi EM algorithm uses a posterior focused on the Viterbi parse of those parameters. Viterbi EM and variants have been used in various settings in natural language processing (Yejin and Cardie, 2007; Wang et al., 2007; Goldwater and Johnson, 2005; DeNero and Klein, 2008; Spitkovsky et al., 2010). Viterbi EM can be understood as a coordinate ascent procedure that locally optimizes a function; we call this optimization goal “Viterbi training.” In this paper, we explore Viterbi training for probabilistic context-free grammars. We first show that under the assumption that P 6= NP, solving and even approximating the Viterbi training problem is hard. This result holds even for hidden Markov models. We extend the main hardness result to the EM algorithm (giving an alternative proof to this known result), as well"
P10-1152,N07-1009,0,0.15986,"hms for training such models have been proposed, including unsupervised methods. Many of these are based on the expectationmaximization (EM) algorithm. There are alternatives to EM, and one such alternative is Viterbi EM, also called “hard” EM or “sparse” EM (Neal and Hinton, 1998). Instead of using the parameters (which are maintained in the algorithm’s current state) to find the true posterior over the derivations, Viterbi EM algorithm uses a posterior focused on the Viterbi parse of those parameters. Viterbi EM and variants have been used in various settings in natural language processing (Yejin and Cardie, 2007; Wang et al., 2007; Goldwater and Johnson, 2005; DeNero and Klein, 2008; Spitkovsky et al., 2010). Viterbi EM can be understood as a coordinate ascent procedure that locally optimizes a function; we call this optimization goal “Viterbi training.” In this paper, we explore Viterbi training for probabilistic context-free grammars. We first show that under the assumption that P 6= NP, solving and even approximating the Viterbi training problem is hard. This result holds even for hidden Markov models. We extend the main hardness result to the EM algorithm (giving an alternative proof to this know"
P10-1152,J93-2004,0,\N,Missing
P10-1152,W08-0704,0,\N,Missing
P10-1152,N09-1012,0,\N,Missing
P10-1152,P07-1094,0,\N,Missing
P10-1152,N09-1036,0,\N,Missing
P10-1152,P08-1046,0,\N,Missing
P10-1152,P09-1012,0,\N,Missing
P10-1152,P04-1061,0,\N,Missing
P10-1152,D07-1072,0,\N,Missing
P10-1152,P96-1024,0,\N,Missing
P11-1042,H05-1009,0,0.0582652,"si like like one of ”. that that is but since since hsi when , how , not 6 θk 3.08 1.19 1.06 0.95 0.92 0.92 0.84 0.83 0.83 0.83 Bigram . h/si ?? hsi please much ? hsi if thank you hsi sorry hsi you please like hsi this θk 2.67 2.25 2.01 1.61 1.58 1.47 1.46 1.45 1.24 1.19 Bigram . h/si hsi this will . are . is . is that have . has . was . will h/si θk 1.87 1.24 1.17 1.16 1.09 1.00 0.97 0.96 0.91 0.88 Related Work The literature contains numerous descriptions of discriminative approaches to word alignment motivated by the desire to be able to incorporate multiple, overlapping knowledge sources (Ayan et al., 2005; Moore, 2005; Taskar et al., 2005; Blunsom and Cohn, 2006; Haghighi et al., 2009; Liu et al., 2010; DeNero and Klein, 2010; Setiawan et al., 2010). This body of work has been an invaluable source of useful features. Several authors have dealt with the problem training log-linear models in an unsuIBM Model 4 alignment Our model&apos;s alignment Figure 2: Example English-Urdu alignment under IBM Model 4 (left) and our discriminative model (right). Model 4 displays two characteristic errors: garbage collection and an overly-strong monotonicity bias. Whereas our model does not exhibit these problems,"
P11-1042,N10-1083,0,0.158266,"Missing"
P11-1042,P06-1009,0,0.395676,"tion scores that are precomputed by looking at the full training data: Dice’s coefficient (discretized), which we use to measure association strength between pairs of source and target word types across sentence pairs (Dice, 1945), IBM Model 1 forward and reverse probabilities, and the geometric mean of the Model 1 forward and reverse probabilities. Finally, we also cluster the source and target vocabularies (Och, 1999) and include class pair indicator features, which can learn generalizations that, e.g., “nouns tend to translate into nouns but not modal verbs.” Positional features. Following Blunsom and Cohn (2006), we include features indicating closeness to the alignment matrix diagonal, aj j h(aj , j, m, n) = m − n . We also conjoin this feature with the source word class type indicator to enable the model to learn that certain word types are more or less likely to favor a location on the diagonal (e.g. Urdu’s sentence-final verbs). Source features. Some words are functional elements that fulfill purely grammatical roles and should not be the “source” of a translation. For example, Romance languages require a preposition in the formation of what could be a noun-noun compound in English, thus, it may"
P11-1042,P08-1024,0,0.0215045,"t, makes no mistakes in the alignment. pervised setting. The contrastive estimation technique proposed by Smith and Eisner (2005) is globally normalized (and thus capable of dealing with arbitrary features), and closely related to the model we developed; however, they do not discuss the problem of word alignment. Berg-Kirkpatrick et al. (2010) learn locally normalized log-linear models in a generative setting. Globally normalized discriminative models with latent variables (Quattoni et al., 2004) have been used for a number of language processing problems, including MT (Dyer and Resnik, 2010; Blunsom et al., 2008a). However, this previous work relied on translation grammars constructed using standard generative word alignment processes. 7 Future Work While we have demonstrated that this model can be substantially useful, it is limited in some important ways which are being addressed in ongoing work. First, training is expensive, and we are exploring alternatives to the conditional likelihood objective that is currently used, such as contrastive neighborhoods advocated by (Smith and Eisner, 2005). Additionally, there is much evidence that non-local features like the source word fertility are (cf. IBM M"
P11-1042,D08-1023,0,0.0170163,"t, makes no mistakes in the alignment. pervised setting. The contrastive estimation technique proposed by Smith and Eisner (2005) is globally normalized (and thus capable of dealing with arbitrary features), and closely related to the model we developed; however, they do not discuss the problem of word alignment. Berg-Kirkpatrick et al. (2010) learn locally normalized log-linear models in a generative setting. Globally normalized discriminative models with latent variables (Quattoni et al., 2004) have been used for a number of language processing problems, including MT (Dyer and Resnik, 2010; Blunsom et al., 2008a). However, this previous work relied on translation grammars constructed using standard generative word alignment processes. 7 Future Work While we have demonstrated that this model can be substantially useful, it is limited in some important ways which are being addressed in ongoing work. First, training is expensive, and we are exploring alternatives to the conditional likelihood objective that is currently used, such as contrastive neighborhoods advocated by (Smith and Eisner, 2005). Additionally, there is much evidence that non-local features like the source word fertility are (cf. IBM M"
P11-1042,bojar-prokopova-2006-czech,0,0.300645,"Missing"
P11-1042,J93-2003,0,0.180904,"r model yields better alignments than generative baselines in a number of language pairs. 1 Introduction Word alignment is an important subtask in statistical machine translation which is typically solved in one of two ways. The more common approach uses a generative translation model that relates bilingual string pairs using a latent alignment variable to designate which source words (or phrases) generate which target words. The parameters in these models can be learned straightforwardly from parallel sentences using EM, and standard inference techniques can recover most probable alignments (Brown et al., 1993). This approach is attractive because it only requires parallel training data. An alternative to the generative approach uses a discriminatively trained 409 alignment model to predict word alignments in the parallel corpus. Discriminative models are attractive because they can incorporate arbitrary, overlapping features, meaning that errors observed in the predictions made by the model can be addressed by engineering new and better features. Unfortunately, both approaches are problematic, but in different ways. In the case of discriminative alignment models, manual alignment data is required f"
P11-1042,J07-2003,0,0.0848504,"n test sets.10 While neither a decrease in the average singleton fertility nor an increase in the number of rules induced guarantees better alignment quality, we believe it is reasonable to assume that they are positively correlated. For the translation experiments in each language pair, we make use of the cdec decoder (Dyer et al., 10 This measure does not assess whether the rule types are good or bad, but it does suggest that the system’s coverage is greater. 2010), inducing a hierarchical phrase based translation grammar from two sets of symmetrized alignments using the method described by Chiang (2007). Additionally, recent work that has demonstrated that extracting rules from n-best alignments has value (Liu et al., 2009; Venugopal et al., 2008). We therefore define a third condition where rules are extracted from the corpus under both the Model 4 and discriminative alignments and merged to form a single grammar. We incorporate a 3-gram language model learned from the target side of the training data as well as 50M supplemental words of monolingual training data consisting of sentences randomly sampled from the English Gigaword, version 4. In the small Chinese-English travel domain experim"
P11-1042,P11-2031,1,0.446686,"Gigaword, version 4. In the small Chinese-English travel domain experiment, we just use the LM estimated from the bitext. The parameters of the translation model were tuned using “hypergraph” minimum error rate training (MERT) to maximize BLEU on a held-out development set (Kumar et al., 2009). Results are reported using case-insensitive BLEU (Papineni et al., 2002), METEOR11 (Lavie and Denkowski, 2009), and TER (Snover et al., 2006), with the number of references varying by task. Since MERT is a nondeterministic optimization algorithm and results can vary considerably between runs, we follow Clark et al. (2011) and report the average score and standard deviation of 5 independent runs, 30 in the case of Chinese-English, since observed variance was higher. 5.2 Experimental Results Czech-English. Czech-English poses problems for word alignment models since, unlike English, Czech words have a complex inflectional morphology, and the syntax permits relatively free word order. For this language pair, we evaluate alignment error rate using the manual alignment corpus described by Bojar and Prokopov´a (2006). Table 2 summarizes the results. Chinese-English. Chinese-English poses a different set of problems"
P11-1042,E09-1020,0,0.0365102,"Missing"
P11-1042,P10-1147,0,0.0270736,"0.83 0.83 Bigram . h/si ?? hsi please much ? hsi if thank you hsi sorry hsi you please like hsi this θk 2.67 2.25 2.01 1.61 1.58 1.47 1.46 1.45 1.24 1.19 Bigram . h/si hsi this will . are . is . is that have . has . was . will h/si θk 1.87 1.24 1.17 1.16 1.09 1.00 0.97 0.96 0.91 0.88 Related Work The literature contains numerous descriptions of discriminative approaches to word alignment motivated by the desire to be able to incorporate multiple, overlapping knowledge sources (Ayan et al., 2005; Moore, 2005; Taskar et al., 2005; Blunsom and Cohn, 2006; Haghighi et al., 2009; Liu et al., 2010; DeNero and Klein, 2010; Setiawan et al., 2010). This body of work has been an invaluable source of useful features. Several authors have dealt with the problem training log-linear models in an unsuIBM Model 4 alignment Our model&apos;s alignment Figure 2: Example English-Urdu alignment under IBM Model 4 (left) and our discriminative model (right). Model 4 displays two characteristic errors: garbage collection and an overly-strong monotonicity bias. Whereas our model does not exhibit these problems, and in fact, makes no mistakes in the alignment. pervised setting. The contrastive estimation technique proposed by Smith a"
P11-1042,N10-1128,1,0.745719,"se problems, and in fact, makes no mistakes in the alignment. pervised setting. The contrastive estimation technique proposed by Smith and Eisner (2005) is globally normalized (and thus capable of dealing with arbitrary features), and closely related to the model we developed; however, they do not discuss the problem of word alignment. Berg-Kirkpatrick et al. (2010) learn locally normalized log-linear models in a generative setting. Globally normalized discriminative models with latent variables (Quattoni et al., 2004) have been used for a number of language processing problems, including MT (Dyer and Resnik, 2010; Blunsom et al., 2008a). However, this previous work relied on translation grammars constructed using standard generative word alignment processes. 7 Future Work While we have demonstrated that this model can be substantially useful, it is limited in some important ways which are being addressed in ongoing work. First, training is expensive, and we are exploring alternatives to the conditional likelihood objective that is currently used, such as contrastive neighborhoods advocated by (Smith and Eisner, 2005). Additionally, there is much evidence that non-local features like the source word fe"
P11-1042,P10-4002,1,0.451451,"Missing"
P11-1042,E09-1037,1,0.831167,"as contrastive neighborhoods advocated by (Smith and Eisner, 2005). Additionally, there is much evidence that non-local features like the source word fertility are (cf. IBM Model 3) useful for translation and alignment modeling. To be truly general, it must be possible to utilize such features. Unfortunately, features like this that depend on global properties of the alignment vector, a, make 417 the inference problem NP-hard, and approximations are necessary. Fortunately, there is much recent work on approximate inference techniques for incorporating nonlocal features (Blunsom et al., 2008b; Gimpel and Smith, 2009; Cromi`eres and Kurohashi, 2009; Weiss and Taskar, 2010), suggesting that this problem too can be solved using established techniques. 8 Conclusion We have introduced a globally normalized, loglinear lexical translation model that can be trained discriminatively using only parallel sentences, which we apply to the problem of word alignment. Our approach addresses two important shortcomings of previous work: (1) that local normalization of generative models constrains the features that can be used, and (2) that previous discriminatively trained word alignment models required supervised alignme"
P11-1042,N06-2013,0,0.0188684,"ring new and better features. Unfortunately, both approaches are problematic, but in different ways. In the case of discriminative alignment models, manual alignment data is required for training, which is problematic for at least three reasons. Manual alignments are notoriously difficult to create and are available only for a handful of language pairs. Second, manual alignments impose a commitment to a particular preprocessing regime; this can be problematic since the optimal segmentation for translation often depends on characteristics of the test set or size of the available training data (Habash and Sadat, 2006) or may be constrained by requirements of other processing components, such parsers. Third, the “correct” alignment annotation for different tasks may vary: for example, relatively denser or sparser alignments may be optimal for different approaches to (downstream) translation model induction (Lopez, 2008; Fraser, 2007). Generative models have a different limitation: the joint probability of a particular setting of the random variables must factorize according to steps in a process that successively “generates” the values of the variables. At each step, the probability of some value being gene"
P11-1042,P09-1104,0,0.020558,"3.08 1.19 1.06 0.95 0.92 0.92 0.84 0.83 0.83 0.83 Bigram . h/si ?? hsi please much ? hsi if thank you hsi sorry hsi you please like hsi this θk 2.67 2.25 2.01 1.61 1.58 1.47 1.46 1.45 1.24 1.19 Bigram . h/si hsi this will . are . is . is that have . has . was . will h/si θk 1.87 1.24 1.17 1.16 1.09 1.00 0.97 0.96 0.91 0.88 Related Work The literature contains numerous descriptions of discriminative approaches to word alignment motivated by the desire to be able to incorporate multiple, overlapping knowledge sources (Ayan et al., 2005; Moore, 2005; Taskar et al., 2005; Blunsom and Cohn, 2006; Haghighi et al., 2009; Liu et al., 2010; DeNero and Klein, 2010; Setiawan et al., 2010). This body of work has been an invaluable source of useful features. Several authors have dealt with the problem training log-linear models in an unsuIBM Model 4 alignment Our model&apos;s alignment Figure 2: Example English-Urdu alignment under IBM Model 4 (left) and our discriminative model (right). Model 4 displays two characteristic errors: garbage collection and an overly-strong monotonicity bias. Whereas our model does not exhibit these problems, and in fact, makes no mistakes in the alignment. pervised setting. The contrastiv"
P11-1042,N03-1017,0,0.0190572,"d confining ourselves to these relatively small corpora reduced the engineering overhead of getting an implementation up and running. Future work will explore the scalability characteristics and limits of the model. 5.1 Methodology For each language pair, we train two log-linear translation models as described above (§3), once with English as the source and once with English as the target language. For a baseline, we use the Giza++ toolkit (Och and Ney, 2003) to learn Model 4, again in both directions. We symmetrize the alignments from both model types using the grow-diag-final-and heuristic (Koehn et al., 2003) producing, in total, six alignment sets. We evaluate them both intrinsically and in terms of their performance in a translation system. Since we only have gold alignments for CzechEnglish (Bojar and Prokopov´a, 2006), we can report alignment error rate (AER; Och and Ney, 2003) only for this pair. However, we offer two further measures that we believe are suggestive and that do not require gold alignments. One is the average alignment “fertility” of source words that occur only a single time in the training data (so-called hapax legomena). This assesses the impact of a typical alignment proble"
P11-1042,P09-1019,1,0.827278,"xtracted from the corpus under both the Model 4 and discriminative alignments and merged to form a single grammar. We incorporate a 3-gram language model learned from the target side of the training data as well as 50M supplemental words of monolingual training data consisting of sentences randomly sampled from the English Gigaword, version 4. In the small Chinese-English travel domain experiment, we just use the LM estimated from the bitext. The parameters of the translation model were tuned using “hypergraph” minimum error rate training (MERT) to maximize BLEU on a held-out development set (Kumar et al., 2009). Results are reported using case-insensitive BLEU (Papineni et al., 2002), METEOR11 (Lavie and Denkowski, 2009), and TER (Snover et al., 2006), with the number of references varying by task. Since MERT is a nondeterministic optimization algorithm and results can vary considerably between runs, we follow Clark et al. (2011) and report the average score and standard deviation of 5 independent runs, 30 in the case of Chinese-English, since observed variance was higher. 5.2 Experimental Results Czech-English. Czech-English poses problems for word alignment models since, unlike English, Czech word"
P11-1042,N09-1069,0,0.0123084,"depicted graph is determined by the features that we use (§4). the gradient of the unregularized objective (Tsuruoka et al., 2009). This method is quite attractive since it is only necessary to represent the active features, meaning impractically large feature spaces can be searched provided the regularization strength is sufficiently high. Additionally, not only has this technique been shown to be very effective for optimizing convex objectives, but evidence suggests that the stochasticity of online algorithms often results in better solutions than batch optimizers for nonconvex objectives (Liang and Klein, 2009). On account of the latent alignment variable in our model, L is non-convex (as is the likelihood objective of the generative variant). To choose the regularization strength β and the initial learning rate η0 ,3 we trained several models on a 10,000-sentence-pair subset of the FrenchEnglish Hansards, and chose values that minimized the alignment error rate, as evaluated on a 447 sentence set of manually created alignments (Mihalcea and Pedersen, 2003). For the remainder of the experiments, we use the values we obtained, β = 0.4 and η0 = 0.3. 3.2 Inference with WFSAs We now describe how to use"
P11-1042,D09-1106,0,0.0217753,"ed guarantees better alignment quality, we believe it is reasonable to assume that they are positively correlated. For the translation experiments in each language pair, we make use of the cdec decoder (Dyer et al., 10 This measure does not assess whether the rule types are good or bad, but it does suggest that the system’s coverage is greater. 2010), inducing a hierarchical phrase based translation grammar from two sets of symmetrized alignments using the method described by Chiang (2007). Additionally, recent work that has demonstrated that extracting rules from n-best alignments has value (Liu et al., 2009; Venugopal et al., 2008). We therefore define a third condition where rules are extracted from the corpus under both the Model 4 and discriminative alignments and merged to form a single grammar. We incorporate a 3-gram language model learned from the target side of the training data as well as 50M supplemental words of monolingual training data consisting of sentences randomly sampled from the English Gigaword, version 4. In the small Chinese-English travel domain experiment, we just use the LM estimated from the bitext. The parameters of the translation model were tuned using “hypergraph” m"
P11-1042,J10-3002,0,0.0438443,"92 0.92 0.84 0.83 0.83 0.83 Bigram . h/si ?? hsi please much ? hsi if thank you hsi sorry hsi you please like hsi this θk 2.67 2.25 2.01 1.61 1.58 1.47 1.46 1.45 1.24 1.19 Bigram . h/si hsi this will . are . is . is that have . has . was . will h/si θk 1.87 1.24 1.17 1.16 1.09 1.00 0.97 0.96 0.91 0.88 Related Work The literature contains numerous descriptions of discriminative approaches to word alignment motivated by the desire to be able to incorporate multiple, overlapping knowledge sources (Ayan et al., 2005; Moore, 2005; Taskar et al., 2005; Blunsom and Cohn, 2006; Haghighi et al., 2009; Liu et al., 2010; DeNero and Klein, 2010; Setiawan et al., 2010). This body of work has been an invaluable source of useful features. Several authors have dealt with the problem training log-linear models in an unsuIBM Model 4 alignment Our model&apos;s alignment Figure 2: Example English-Urdu alignment under IBM Model 4 (left) and our discriminative model (right). Model 4 displays two characteristic errors: garbage collection and an overly-strong monotonicity bias. Whereas our model does not exhibit these problems, and in fact, makes no mistakes in the alignment. pervised setting. The contrastive estimation techn"
P11-1042,C08-1064,0,0.0291766,"only for a handful of language pairs. Second, manual alignments impose a commitment to a particular preprocessing regime; this can be problematic since the optimal segmentation for translation often depends on characteristics of the test set or size of the available training data (Habash and Sadat, 2006) or may be constrained by requirements of other processing components, such parsers. Third, the “correct” alignment annotation for different tasks may vary: for example, relatively denser or sparser alignments may be optimal for different approaches to (downstream) translation model induction (Lopez, 2008; Fraser, 2007). Generative models have a different limitation: the joint probability of a particular setting of the random variables must factorize according to steps in a process that successively “generates” the values of the variables. At each step, the probability of some value being generated may depend only on the generation history (or a subset thereof), and the possible values a variable will take must form a locally normalized conditional probability distribution (CPD). While these locally normalized CPDs may be paProceedings of the 49th Annual Meeting of the Association for Computat"
P11-1042,W03-0301,0,0.0383228,"s, but evidence suggests that the stochasticity of online algorithms often results in better solutions than batch optimizers for nonconvex objectives (Liang and Klein, 2009). On account of the latent alignment variable in our model, L is non-convex (as is the likelihood objective of the generative variant). To choose the regularization strength β and the initial learning rate η0 ,3 we trained several models on a 10,000-sentence-pair subset of the FrenchEnglish Hansards, and chose values that minimized the alignment error rate, as evaluated on a 447 sentence set of manually created alignments (Mihalcea and Pedersen, 2003). For the remainder of the experiments, we use the values we obtained, β = 0.4 and η0 = 0.3. 3.2 Inference with WFSAs We now describe how to use weighted finite-state automata (WFSAs) to compute the quantities neces3.1 Parameter Learning sary for training. We begin by describing the ideal To learn the parameters of our model, we select the WFSA representing the full translation search space, θ ∗ that minimizes the `1 regularized conditional log- which we call the discriminative neighborhood, and likelihood of a set of training data T : then discuss strategies for reducing its size in the X X X"
P11-1042,H05-1011,0,0.332958,"alignment variable a = ha1 , a2 , . . . , an i ∈ [0, m]n , where aj = 0 represents a special null token. X p(t |s, n) = p(t, a |s, n) 2 3 Model In this section, we develop a conditional model p(t |s) that, given a source language sentence s with length m = |s|, assigns probabilities to a target sentence t with length n, where each word tj is an element in the finite target vocabulary Ω. We begin by using the chain rule to factor this probability into two components, a translation model and a length model. p(t |s) = p(t, n |s) = p(t |s, n) × p(n |s) |{z } |{z } translation model length model 1 Moore (2005) likewise uses this example to motivate the need for models that support arbitrary, overlapping features. 410 a So far, our model is identical to that of (Brown et al., 1993); however, we part ways here. Rather than using the chain rule to further decompose this probability and motivate opportunities to make independence assumptions, we use a log-linear model with parameters θ ∈ Rk and feature vector function H that maps each tuple ha, s, t, ni into Rk to model p(t, a |s, n) directly: pθ (t, a |s, n) = Zθ (s, n) = exp θ &gt; H(t, a, s, n) , where Zθ (s, n) X X exp θ &gt; H(t0 , a0 , s, n) t0 ∈Ωn a0"
P11-1042,J03-1002,0,0.00681142,"translation. 9 http://statmt.org/wmt10 414 der in Chinese and Urdu, morphological complexity in Czech, and a non-alphabetic writing system in Chinese), and confining ourselves to these relatively small corpora reduced the engineering overhead of getting an implementation up and running. Future work will explore the scalability characteristics and limits of the model. 5.1 Methodology For each language pair, we train two log-linear translation models as described above (§3), once with English as the source and once with English as the target language. For a baseline, we use the Giza++ toolkit (Och and Ney, 2003) to learn Model 4, again in both directions. We symmetrize the alignments from both model types using the grow-diag-final-and heuristic (Koehn et al., 2003) producing, in total, six alignment sets. We evaluate them both intrinsically and in terms of their performance in a translation system. Since we only have gold alignments for CzechEnglish (Bojar and Prokopov´a, 2006), we can report alignment error rate (AER; Och and Ney, 2003) only for this pair. However, we offer two further measures that we believe are suggestive and that do not require gold alignments. One is the average alignment “fert"
P11-1042,E99-1010,0,0.034305,"phic feature was computed after first applying a heuristic Romanization, which made the orthographic forms somewhat comparable. 413 regardless of length). We also include “global” association scores that are precomputed by looking at the full training data: Dice’s coefficient (discretized), which we use to measure association strength between pairs of source and target word types across sentence pairs (Dice, 1945), IBM Model 1 forward and reverse probabilities, and the geometric mean of the Model 1 forward and reverse probabilities. Finally, we also cluster the source and target vocabularies (Och, 1999) and include class pair indicator features, which can learn generalizations that, e.g., “nouns tend to translate into nouns but not modal verbs.” Positional features. Following Blunsom and Cohn (2006), we include features indicating closeness to the alignment matrix diagonal, aj j h(aj , j, m, n) = m − n . We also conjoin this feature with the source word class type indicator to enable the model to learn that certain word types are more or less likely to favor a location on the diagonal (e.g. Urdu’s sentence-final verbs). Source features. Some words are functional elements that fulfill purely"
P11-1042,P02-1040,0,0.108705,"nments and merged to form a single grammar. We incorporate a 3-gram language model learned from the target side of the training data as well as 50M supplemental words of monolingual training data consisting of sentences randomly sampled from the English Gigaword, version 4. In the small Chinese-English travel domain experiment, we just use the LM estimated from the bitext. The parameters of the translation model were tuned using “hypergraph” minimum error rate training (MERT) to maximize BLEU on a held-out development set (Kumar et al., 2009). Results are reported using case-insensitive BLEU (Papineni et al., 2002), METEOR11 (Lavie and Denkowski, 2009), and TER (Snover et al., 2006), with the number of references varying by task. Since MERT is a nondeterministic optimization algorithm and results can vary considerably between runs, we follow Clark et al. (2011) and report the average score and standard deviation of 5 independent runs, 30 in the case of Chinese-English, since observed variance was higher. 5.2 Experimental Results Czech-English. Czech-English poses problems for word alignment models since, unlike English, Czech words have a complex inflectional morphology, and the syntax permits relativel"
P11-1042,D10-1052,1,0.841806,"?? hsi please much ? hsi if thank you hsi sorry hsi you please like hsi this θk 2.67 2.25 2.01 1.61 1.58 1.47 1.46 1.45 1.24 1.19 Bigram . h/si hsi this will . are . is . is that have . has . was . will h/si θk 1.87 1.24 1.17 1.16 1.09 1.00 0.97 0.96 0.91 0.88 Related Work The literature contains numerous descriptions of discriminative approaches to word alignment motivated by the desire to be able to incorporate multiple, overlapping knowledge sources (Ayan et al., 2005; Moore, 2005; Taskar et al., 2005; Blunsom and Cohn, 2006; Haghighi et al., 2009; Liu et al., 2010; DeNero and Klein, 2010; Setiawan et al., 2010). This body of work has been an invaluable source of useful features. Several authors have dealt with the problem training log-linear models in an unsuIBM Model 4 alignment Our model&apos;s alignment Figure 2: Example English-Urdu alignment under IBM Model 4 (left) and our discriminative model (right). Model 4 displays two characteristic errors: garbage collection and an overly-strong monotonicity bias. Whereas our model does not exhibit these problems, and in fact, makes no mistakes in the alignment. pervised setting. The contrastive estimation technique proposed by Smith and Eisner (2005) is glob"
P11-1042,P05-1044,1,0.911055,"= 0.01) on the translation distributions and use the empirical Bayes (EB) method to infer a point estimate, using variational inference. Table 1: Comparison of alternative definitions Ωs (arrows indicate whether higher or lower is better). P Ωs time (s) ↓ AER ↓ s |Ωs |↓ =Ω 22.4 86.0M 0.0 co-occ. 8.9 0.68M 0.0 Model 1 0.2 0.38M 6.2 EB-Model 1 1.0 0.15M 2.9 Table 1 compares the average per-sentence time required to run the inference algorithm described 5 Future work will explore alternative formulations of the discriminative neighborhood with the goal of further improving inference efficiency. Smith and Eisner (2005) show that good performance on unsupervised syntax learning is possible even when learning from very small discriminative neighborhoods, and we posit that the same holds here. above under these four different definitions of Ωs on a 10,000 sentence subset of the Hansards FrenchEnglish corpus that includes manual word alignments. While our constructions guarantee that all references are reachable even in the reduced neighborhoods, not all alignments between source and target are possible. The last column is the oracle AER. Although EB variant of Model 1 neighborhood is slightly more expensive to"
P11-1042,2006.amta-papers.25,0,0.0389463,"guage model learned from the target side of the training data as well as 50M supplemental words of monolingual training data consisting of sentences randomly sampled from the English Gigaword, version 4. In the small Chinese-English travel domain experiment, we just use the LM estimated from the bitext. The parameters of the translation model were tuned using “hypergraph” minimum error rate training (MERT) to maximize BLEU on a held-out development set (Kumar et al., 2009). Results are reported using case-insensitive BLEU (Papineni et al., 2002), METEOR11 (Lavie and Denkowski, 2009), and TER (Snover et al., 2006), with the number of references varying by task. Since MERT is a nondeterministic optimization algorithm and results can vary considerably between runs, we follow Clark et al. (2011) and report the average score and standard deviation of 5 independent runs, 30 in the case of Chinese-English, since observed variance was higher. 5.2 Experimental Results Czech-English. Czech-English poses problems for word alignment models since, unlike English, Czech words have a complex inflectional morphology, and the syntax permits relatively free word order. For this language pair, we evaluate alignment erro"
P11-1042,takezawa-etal-2002-toward,0,0.0213892,"word translates as itself (for example, a name or a date, which occurs in languages that share the same alphabet) in position j, but then is translated again (as something else) in position j − 1 or j + 1. 5 Experiments We now turn to an empirical assessment of our model. Using various datasets, we evaluate the performance of the models’ intrinsic quality and theirtheir alignments’ contribution to a standard machine translation system. We make use of parallel corpora from languages with very different typologies: a small (0.8M words) Chinese-English corpus from the tourism and travel domain (Takezawa et al., 2002), a corpus of Czech-English news commentary (3.1M words),9 and an Urdu-English corpus (2M words) provided by NIST for the 2009 Open MT Evaluation. These pairs were selected since each poses different alignment challenges (word or8 This is of course what makes history-based language model integration an inference challenge in translation. 9 http://statmt.org/wmt10 414 der in Chinese and Urdu, morphological complexity in Czech, and a non-alphabetic writing system in Chinese), and confining ourselves to these relatively small corpora reduced the engineering overhead of getting an implementation u"
P11-1042,H05-1010,0,0.0995513,"is but since since hsi when , how , not 6 θk 3.08 1.19 1.06 0.95 0.92 0.92 0.84 0.83 0.83 0.83 Bigram . h/si ?? hsi please much ? hsi if thank you hsi sorry hsi you please like hsi this θk 2.67 2.25 2.01 1.61 1.58 1.47 1.46 1.45 1.24 1.19 Bigram . h/si hsi this will . are . is . is that have . has . was . will h/si θk 1.87 1.24 1.17 1.16 1.09 1.00 0.97 0.96 0.91 0.88 Related Work The literature contains numerous descriptions of discriminative approaches to word alignment motivated by the desire to be able to incorporate multiple, overlapping knowledge sources (Ayan et al., 2005; Moore, 2005; Taskar et al., 2005; Blunsom and Cohn, 2006; Haghighi et al., 2009; Liu et al., 2010; DeNero and Klein, 2010; Setiawan et al., 2010). This body of work has been an invaluable source of useful features. Several authors have dealt with the problem training log-linear models in an unsuIBM Model 4 alignment Our model&apos;s alignment Figure 2: Example English-Urdu alignment under IBM Model 4 (left) and our discriminative model (right). Model 4 displays two characteristic errors: garbage collection and an overly-strong monotonicity bias. Whereas our model does not exhibit these problems, and in fact, makes no mistakes in"
P11-1042,P09-1054,0,0.228503,"me using dynamic programming. For example, when the graph has a sequential structure, exact inference can be carried out using the familiar forwardbackward algorithm (Lafferty et al., 2001). Although our features look at more structure than this, they are designed to keep treewidth low, meaning exact inference is still possible with dynamic programming. Figure 1 gives a graphical representation of our model as well as the more familiar generative (directed) variants. The edge set in the depicted graph is determined by the features that we use (§4). the gradient of the unregularized objective (Tsuruoka et al., 2009). This method is quite attractive since it is only necessary to represent the active features, meaning impractically large feature spaces can be searched provided the regularization strength is sufficiently high. Additionally, not only has this technique been shown to be very effective for optimizing convex objectives, but evidence suggests that the stochasticity of online algorithms often results in better solutions than batch optimizers for nonconvex objectives (Liang and Klein, 2009). On account of the latent alignment variable in our model, L is non-convex (as is the likelihood objective o"
P11-1042,2008.amta-papers.18,1,0.874421,"er alignment quality, we believe it is reasonable to assume that they are positively correlated. For the translation experiments in each language pair, we make use of the cdec decoder (Dyer et al., 10 This measure does not assess whether the rule types are good or bad, but it does suggest that the system’s coverage is greater. 2010), inducing a hierarchical phrase based translation grammar from two sets of symmetrized alignments using the method described by Chiang (2007). Additionally, recent work that has demonstrated that extracting rules from n-best alignments has value (Liu et al., 2009; Venugopal et al., 2008). We therefore define a third condition where rules are extracted from the corpus under both the Model 4 and discriminative alignments and merged to form a single grammar. We incorporate a 3-gram language model learned from the target side of the training data as well as 50M supplemental words of monolingual training data consisting of sentences randomly sampled from the English Gigaword, version 4. In the small Chinese-English travel domain experiment, we just use the LM estimated from the bitext. The parameters of the translation model were tuned using “hypergraph” minimum error rate trainin"
P11-1042,C96-2141,0,0.939204,"rization and only depends on hood, the set Ωn ×[0, m]n , such that every path from 2 One way to understand expressiveness is in terms of indethe start state to goal yields a pair ht0 , ai with weight pendence assumptions, of course. Research in graphical models has done much to relate independence assumptions to the complexity of inference algorithms (Koller and Friedman, 2009). 411 3 For the other free parameters of the algorithm, we use the default values recommended by Tsuruoka et al. (2009). s s n n s a1 a2 a3 ... an t1 t2 t3 ... tn a1 s s a2 s t1 Fully directed model (Brown et al., 1993; Vogel et al., 1996; Berg-Kirkpatrick et al., 2010) s a3 ... t3 ... s t2 s an s tn Our model Figure 1: A graphical representation of a conventional generative lexical translation model (left) and our model with an undirected translation model. For clarity, the observed node s (representing the full source sentence) is drawn in multiple locations. The dashed lines indicate a dependency on a deterministic mapping of tj (not its complete value). H(t0 , a, s, n). With our feature set (§4), number of states in this WFSA is O(m × n) since at each target index j, there is a different state for each possible index of th"
P11-1137,W10-1757,0,0.0335859,"Missing"
P11-1137,D10-1124,1,0.267148,"Missing"
P11-1137,N04-1039,0,0.241069,"Missing"
P11-1137,W03-1018,0,0.0271586,"Missing"
P11-1137,W04-3223,0,0.0219742,"Missing"
P11-1144,boas-2002-bilingual,0,0.0377024,"stic models to improve coverage on unseen predicates.2 Expert resources have limited coverage, and FrameNet is no exception. Automatic induction of semantic resources has been a major effort in recent years (Snow et al., 2006; Ponzetto and Strube, 2007, inter alia). In the domain of frame semantics, previous work has sought to extend the coverage of FrameNet by exploiting resources like VerbNet, WordNet, or Wikipedia (Shi and Mihalcea, 2005; Giuglea and Moschitti, 2006; Pennacchiotti et al., 2008; Tonelli and Giuliano, 2009), and projecting entries and annotations within and across languages (Boas, 2002; Fung and Chen, 2004; Pad´o and Lapata, 2005). Although these approaches have increased coverage to various degrees, they rely on other lexicons and resources created by experts. F¨urstenau and Lapata (2009) proposed the use of unlabeled data to improve coverage, but their work was limited to verbs. Bejan (2009) used self-training to improve frame identification and reported improvements, but did not explicitly model unknown targets. In contrast, we use statistics gathered from large volumes of unlabeled data to improve the coverage of a frame-semantic parser on several syntactic categories,"
P11-1144,P11-1061,1,0.101357,"k an approach to word-sense disambiguation due to Niu et al. (2005). Their formulation was transductive, so that the test data was part of the constructed graph, and they did not consider predicate-argument analysis. In contrast, we make use of the smoothed graph during inference in a probabilistic setting, in turn using it for the full frame-semantic parsing task. Recently, Subramanya et al. (2010) proposed the use of a graph over substructures of an underlying sequence model, and used a smoothed graph for domain adaptation of part-of-speech taggers. Subramanya et al.’s model was extended by Das and Petrov (2011) to induce part-of-speech dictionaries for unsupervised learning of taggers. Our semi-supervised learning setting is similar to these two lines of work and, like them, we use the graph to arrive at better final structures, in an inductive setting (i.e., where a parametric model is learned and then separately applied to test data, following most NLP research). 3 Approach Overview Our overall approach to handling unobserved targets consists of four distinct stages. Before going into the details of each stage individually, we provide their overview here: Graph Construction: A graph consisting of"
P11-1144,N10-1138,1,0.0647197,"taining instantiations of semantic frames, and full-text annotations provide supervision for learning frame-semantic parsers. Yet these annotations lack coverage, including only 9,300 annotated target types. Recent papers have tried to address the coverage problem. Johansson and Nugues (2007) used WordNet (Fellbaum, 1998) to expand the list of targets that can evoke frames and trained classifiers to identify the best-suited frame for the newly created targets. In past work, we described an approach where latent variables were used in a probabilistic model to predict frames for unseen targets (Das et al., 2010a).1 Relatedly, for the argument identification subtask, Matsubayashi et al. (2009) proposed a technique for generalization of semantic roles to overcome data sparseness. Unseen targets continue to present a major obstacle to domain-general semantic analysis. In this paper, we address the problem of idenfifying the semantic frames for targets unseen either in FrameNet (including the exemplar sentences) or the collection of full-text annotations released along with the lexicon. Using a standard model for the argument identification stage (Das et al., 2010a), our proposed method improves overall"
P11-1144,erk-pado-2006-shalmaneser,0,0.414117,"Missing"
P11-1144,W03-1007,0,0.136534,"coverage for hitherto unobserved predicates (§6). 2 Background Before going into the details of our model, we provide some background on two topics relevant to this paper: frame-semantic parsing and graph-based learning applied to natural language tasks. 2.1 Frame-semantic Parsing Gildea and Jurafsky (2002) pioneered SRL, and since then there has been much applied research on predicate-argument semantics. Early work on frame-semantic role labeling made use of the exemplar sentences in the FrameNet corpus, each of which is annotated for a single frame and its arguments (Thompson et al., 2003; Fleischman et al., 2003; Shi and Mihalcea, 2004; Erk and Pad´o, 2006, inter alia). Most of this work was done on an older, smaller version of FrameNet. Recently, since the release of full-text annotations in SemEval’07 (Baker et al., 2007), there has been work on identifying multiple frames and their corresponding sets of ar1436 guments in a sentence. The LTH system of Johansson and Nugues (2007) performed the best in the SemEval’07 shared task on frame-semantic parsing. Our probabilistic frame-semantic parser outperforms LTH on that task and dataset (Das et al., 2010a). The current paper builds on those probabilist"
P11-1144,C04-1134,0,0.0378492,"to improve coverage on unseen predicates.2 Expert resources have limited coverage, and FrameNet is no exception. Automatic induction of semantic resources has been a major effort in recent years (Snow et al., 2006; Ponzetto and Strube, 2007, inter alia). In the domain of frame semantics, previous work has sought to extend the coverage of FrameNet by exploiting resources like VerbNet, WordNet, or Wikipedia (Shi and Mihalcea, 2005; Giuglea and Moschitti, 2006; Pennacchiotti et al., 2008; Tonelli and Giuliano, 2009), and projecting entries and annotations within and across languages (Boas, 2002; Fung and Chen, 2004; Pad´o and Lapata, 2005). Although these approaches have increased coverage to various degrees, they rely on other lexicons and resources created by experts. F¨urstenau and Lapata (2009) proposed the use of unlabeled data to improve coverage, but their work was limited to verbs. Bejan (2009) used self-training to improve frame identification and reported improvements, but did not explicitly model unknown targets. In contrast, we use statistics gathered from large volumes of unlabeled data to improve the coverage of a frame-semantic parser on several syntactic categories, in a novel framework"
P11-1144,E09-1026,0,0.022886,"Missing"
P11-1144,J02-3001,0,0.624656,"Missing"
P11-1144,S07-1048,0,0.525432,"he use of two statistical classifiers corresponding to the aforementioned subtasks: the first one to identify the most suitable semantic frame for a marked lexical predicate (target, henceforth) in a sentence, and the second for performing semantic role labeling (SRL) given the frame. The FrameNet lexicon, its exemplar sentences containing instantiations of semantic frames, and full-text annotations provide supervision for learning frame-semantic parsers. Yet these annotations lack coverage, including only 9,300 annotated target types. Recent papers have tried to address the coverage problem. Johansson and Nugues (2007) used WordNet (Fellbaum, 1998) to expand the list of targets that can evoke frames and trained classifiers to identify the best-suited frame for the newly created targets. In past work, we described an approach where latent variables were used in a probabilistic model to predict frames for unseen targets (Das et al., 2010a).1 Relatedly, for the argument identification subtask, Matsubayashi et al. (2009) proposed a technique for generalization of semantic roles to overcome data sparseness. Unseen targets continue to present a major obstacle to domain-general semantic analysis. In this paper, we"
P11-1144,P93-1016,0,0.0368514,"appended with a coarse POS tag, and it resembles the lexical units in the FrameNet lexicon. For example, two targets corresponding to the same lemma would look like boast.N and boast.V. Here, the first target is a noun, while the second is a verb. An example multiword target is chemical weapon.N. We use two resources for graph construction. First, we take all the words and phrases present in the dependency-based thesaurus constructed using syntactic cooccurrence statistics (Lin, 1998).3 To construct this resource, a corpus containing 64 million words was parsed with a fast dependency parser (Lin, 1993; Lin, 1994), and syntactic contexts were used to find similar lexical items for a given word 3 This resource is available at http://webdocs.cs. ualberta.ca/˜lindek/Downloads/sim.tgz POVERTY powerlessness.N rich.A deprivation.N POVERTY wealthy.A SIMILARITY variant.N SIMILARITY similarity.N Figure 2: Excerpt from a graph over targets. Green targets are SIMILARITY POVERTY observed in the FrameNet data. poverty.N resemblance.N inequality.N Above/below them are shown the destitution.N resemble.V most frequently observed frame homelessness.N SIMILARITY unemployment that these targets evoke. The bla"
P11-1144,C94-1079,0,0.12035,"ith a coarse POS tag, and it resembles the lexical units in the FrameNet lexicon. For example, two targets corresponding to the same lemma would look like boast.N and boast.V. Here, the first target is a noun, while the second is a verb. An example multiword target is chemical weapon.N. We use two resources for graph construction. First, we take all the words and phrases present in the dependency-based thesaurus constructed using syntactic cooccurrence statistics (Lin, 1998).3 To construct this resource, a corpus containing 64 million words was parsed with a fast dependency parser (Lin, 1993; Lin, 1994), and syntactic contexts were used to find similar lexical items for a given word 3 This resource is available at http://webdocs.cs. ualberta.ca/˜lindek/Downloads/sim.tgz POVERTY powerlessness.N rich.A deprivation.N POVERTY wealthy.A SIMILARITY variant.N SIMILARITY similarity.N Figure 2: Excerpt from a graph over targets. Green targets are SIMILARITY POVERTY observed in the FrameNet data. poverty.N resemblance.N inequality.N Above/below them are shown the destitution.N resemble.V most frequently observed frame homelessness.N SIMILARITY unemployment that these targets evoke. The black diﬀerence"
P11-1144,P98-2127,0,0.164038,"nstruction We construct a graph with targets as vertices. For us, each target corresponds to a lemmatized word or phrase appended with a coarse POS tag, and it resembles the lexical units in the FrameNet lexicon. For example, two targets corresponding to the same lemma would look like boast.N and boast.V. Here, the first target is a noun, while the second is a verb. An example multiword target is chemical weapon.N. We use two resources for graph construction. First, we take all the words and phrases present in the dependency-based thesaurus constructed using syntactic cooccurrence statistics (Lin, 1998).3 To construct this resource, a corpus containing 64 million words was parsed with a fast dependency parser (Lin, 1993; Lin, 1994), and syntactic contexts were used to find similar lexical items for a given word 3 This resource is available at http://webdocs.cs. ualberta.ca/˜lindek/Downloads/sim.tgz POVERTY powerlessness.N rich.A deprivation.N POVERTY wealthy.A SIMILARITY variant.N SIMILARITY similarity.N Figure 2: Excerpt from a graph over targets. Green targets are SIMILARITY POVERTY observed in the FrameNet data. poverty.N resemblance.N inequality.N Above/below them are shown the destituti"
P11-1144,P09-1003,0,0.0472891,"ide supervision for learning frame-semantic parsers. Yet these annotations lack coverage, including only 9,300 annotated target types. Recent papers have tried to address the coverage problem. Johansson and Nugues (2007) used WordNet (Fellbaum, 1998) to expand the list of targets that can evoke frames and trained classifiers to identify the best-suited frame for the newly created targets. In past work, we described an approach where latent variables were used in a probabilistic model to predict frames for unseen targets (Das et al., 2010a).1 Relatedly, for the argument identification subtask, Matsubayashi et al. (2009) proposed a technique for generalization of semantic roles to overcome data sparseness. Unseen targets continue to present a major obstacle to domain-general semantic analysis. In this paper, we address the problem of idenfifying the semantic frames for targets unseen either in FrameNet (including the exemplar sentences) or the collection of full-text annotations released along with the lexicon. Using a standard model for the argument identification stage (Das et al., 2010a), our proposed method improves overall frame-semantic parsing, especially for unseen targets. To better handle these unse"
P11-1144,P05-1012,0,0.0112118,"exemplar sentences and the training data, we arrived at a set of 877 frames, 1,068 roles,10 and 9,263 targets. Our training split of the full-text annotations contained 3,256 sentences with 19,582 frame annotatations with corresponding roles, while the test set contained 2,420 sentences with 4,458 annotations (the test set contained fewer annotated targets per sentence). We also divide the 55 training documents into 5 parts for crossvalidation (see §6.3). The raw sentences in all the training and test documents were preprocessed using MXPOST (Ratnaparkhi, 1996) and the MST dependency parser (McDonald et al., 2005) following Das et al. (2010a). In this work we assume the frame-evoking targets have been correctly identified in training and test data. 10 Note that the number of listed roles in the lexicon is nearly 9,000, but their number in actual annotations is a lot fewer. 1441 Baselines We compare our model with three baselines. The first baseline is the purely supervised model of Das et al. (2010a) trained on the training split of 55 documents. Note that this is the strongest baseline available for this task;11 we refer to this model as “SEMAFOR.” The second baseline is a semi-supervised selftrained"
P11-1144,P05-1049,0,0.00894607,"xtended in this work. 2.2 Graph-based Semi-Supervised Learning In graph-based semi-supervised learning, one constructs a graph whose vertices are labeled and unlabeled examples. Weighted edges in the graph, connecting pairs of examples/vertices, encode the degree to which they are expected to have the same label (Zhu et al., 2003). Variants of label propagation are used to transfer labels from the labeled to the unlabeled examples. There are several instances of the use of graph-based methods for natural language tasks. Most relevant to our work an approach to word-sense disambiguation due to Niu et al. (2005). Their formulation was transductive, so that the test data was part of the constructed graph, and they did not consider predicate-argument analysis. In contrast, we make use of the smoothed graph during inference in a probabilistic setting, in turn using it for the full frame-semantic parsing task. Recently, Subramanya et al. (2010) proposed the use of a graph over substructures of an underlying sequence model, and used a smoothed graph for domain adaptation of part-of-speech taggers. Subramanya et al.’s model was extended by Das and Petrov (2011) to induce part-of-speech dictionaries for uns"
P11-1144,H05-1108,0,0.0372655,"Missing"
P11-1144,D08-1048,0,0.125485,"Missing"
P11-1144,W96-0213,0,0.0519558,"remaining ones as our test set. After scanning the exemplar sentences and the training data, we arrived at a set of 877 frames, 1,068 roles,10 and 9,263 targets. Our training split of the full-text annotations contained 3,256 sentences with 19,582 frame annotatations with corresponding roles, while the test set contained 2,420 sentences with 4,458 annotations (the test set contained fewer annotated targets per sentence). We also divide the 55 training documents into 5 parts for crossvalidation (see §6.3). The raw sentences in all the training and test documents were preprocessed using MXPOST (Ratnaparkhi, 1996) and the MST dependency parser (McDonald et al., 2005) following Das et al. (2010a). In this work we assume the frame-evoking targets have been correctly identified in training and test data. 10 Note that the number of listed roles in the lexicon is nearly 9,000, but their number in actual annotations is a lot fewer. 1441 Baselines We compare our model with three baselines. The first baseline is the purely supervised model of Das et al. (2010a) trained on the training split of 55 documents. Note that this is the strongest baseline available for this task;11 we refer to this model as “SEMAFOR.”"
P11-1144,W04-2008,0,0.0905875,"observed predicates (§6). 2 Background Before going into the details of our model, we provide some background on two topics relevant to this paper: frame-semantic parsing and graph-based learning applied to natural language tasks. 2.1 Frame-semantic Parsing Gildea and Jurafsky (2002) pioneered SRL, and since then there has been much applied research on predicate-argument semantics. Early work on frame-semantic role labeling made use of the exemplar sentences in the FrameNet corpus, each of which is annotated for a single frame and its arguments (Thompson et al., 2003; Fleischman et al., 2003; Shi and Mihalcea, 2004; Erk and Pad´o, 2006, inter alia). Most of this work was done on an older, smaller version of FrameNet. Recently, since the release of full-text annotations in SemEval’07 (Baker et al., 2007), there has been work on identifying multiple frames and their corresponding sets of ar1436 guments in a sentence. The LTH system of Johansson and Nugues (2007) performed the best in the SemEval’07 shared task on frame-semantic parsing. Our probabilistic frame-semantic parser outperforms LTH on that task and dataset (Das et al., 2010a). The current paper builds on those probabilistic models to improve cov"
P11-1144,P06-1101,0,0.0289538,"al., 2007), there has been work on identifying multiple frames and their corresponding sets of ar1436 guments in a sentence. The LTH system of Johansson and Nugues (2007) performed the best in the SemEval’07 shared task on frame-semantic parsing. Our probabilistic frame-semantic parser outperforms LTH on that task and dataset (Das et al., 2010a). The current paper builds on those probabilistic models to improve coverage on unseen predicates.2 Expert resources have limited coverage, and FrameNet is no exception. Automatic induction of semantic resources has been a major effort in recent years (Snow et al., 2006; Ponzetto and Strube, 2007, inter alia). In the domain of frame semantics, previous work has sought to extend the coverage of FrameNet by exploiting resources like VerbNet, WordNet, or Wikipedia (Shi and Mihalcea, 2005; Giuglea and Moschitti, 2006; Pennacchiotti et al., 2008; Tonelli and Giuliano, 2009), and projecting entries and annotations within and across languages (Boas, 2002; Fung and Chen, 2004; Pad´o and Lapata, 2005). Although these approaches have increased coverage to various degrees, they rely on other lexicons and resources created by experts. F¨urstenau and Lapata (2009) propos"
P11-1144,D10-1017,0,0.352335,"003). Variants of label propagation are used to transfer labels from the labeled to the unlabeled examples. There are several instances of the use of graph-based methods for natural language tasks. Most relevant to our work an approach to word-sense disambiguation due to Niu et al. (2005). Their formulation was transductive, so that the test data was part of the constructed graph, and they did not consider predicate-argument analysis. In contrast, we make use of the smoothed graph during inference in a probabilistic setting, in turn using it for the full frame-semantic parsing task. Recently, Subramanya et al. (2010) proposed the use of a graph over substructures of an underlying sequence model, and used a smoothed graph for domain adaptation of part-of-speech taggers. Subramanya et al.’s model was extended by Das and Petrov (2011) to induce part-of-speech dictionaries for unsupervised learning of taggers. Our semi-supervised learning setting is similar to these two lines of work and, like them, we use the graph to arrive at better final structures, in an inductive setting (i.e., where a parametric model is learned and then separately applied to test data, following most NLP research). 3 Approach Overview"
P11-1144,D09-1029,0,0.0212473,"rms LTH on that task and dataset (Das et al., 2010a). The current paper builds on those probabilistic models to improve coverage on unseen predicates.2 Expert resources have limited coverage, and FrameNet is no exception. Automatic induction of semantic resources has been a major effort in recent years (Snow et al., 2006; Ponzetto and Strube, 2007, inter alia). In the domain of frame semantics, previous work has sought to extend the coverage of FrameNet by exploiting resources like VerbNet, WordNet, or Wikipedia (Shi and Mihalcea, 2005; Giuglea and Moschitti, 2006; Pennacchiotti et al., 2008; Tonelli and Giuliano, 2009), and projecting entries and annotations within and across languages (Boas, 2002; Fung and Chen, 2004; Pad´o and Lapata, 2005). Although these approaches have increased coverage to various degrees, they rely on other lexicons and resources created by experts. F¨urstenau and Lapata (2009) proposed the use of unlabeled data to improve coverage, but their work was limited to verbs. Bejan (2009) used self-training to improve frame identification and reported improvements, but did not explicitly model unknown targets. In contrast, we use statistics gathered from large volumes of unlabeled data to i"
P11-1144,S07-1018,0,\N,Missing
P11-1144,C98-2122,0,\N,Missing
P11-2008,C10-2005,0,0.591162,"toP jailN ., ThankV uO God∧ ., #amen# Figure 1: Example tweets with gold annotations. Underlined tokens show tagger improvements due to features detailed in Section 3 (respectively: TAG D ICT, M ETAPH, and D IST S IM). Introduction The growing popularity of social media and usercreated web content is producing enormous quantities of text in electronic form. The popular microblogging service Twitter (twitter.com) is one particularly fruitful source of user-created content, and a flurry of recent research has aimed to understand and exploit these data (Ritter et al., 2010; Sharifi et al., 2010; Barbosa and Feng, 2010; Asur and Huberman, 2010; O’Connor et al., 2010a; Thelwall et al., 2011). However, the bulk of this work eschews the standard pipeline of tools which might enable a richer linguistic analysis; such tools are typically trained on newstext and have been shown to perform poorly on Twitter (Finin et al., 2010). One of the most fundamental parts of the linguistic pipeline is part-of-speech (POS) tagging, a basic form of syntactic analysis which has countless applications in NLP. Most POS taggers are trained from treebanks in the newswire domain, such as the Wall Street Journal corpus of the Penn T"
P11-2008,W10-0713,0,0.446157,"enormous quantities of text in electronic form. The popular microblogging service Twitter (twitter.com) is one particularly fruitful source of user-created content, and a flurry of recent research has aimed to understand and exploit these data (Ritter et al., 2010; Sharifi et al., 2010; Barbosa and Feng, 2010; Asur and Huberman, 2010; O’Connor et al., 2010a; Thelwall et al., 2011). However, the bulk of this work eschews the standard pipeline of tools which might enable a richer linguistic analysis; such tools are typically trained on newstext and have been shown to perform poorly on Twitter (Finin et al., 2010). One of the most fundamental parts of the linguistic pipeline is part-of-speech (POS) tagging, a basic form of syntactic analysis which has countless applications in NLP. Most POS taggers are trained from treebanks in the newswire domain, such as the Wall Street Journal corpus of the Penn Treebank (PTB; Marcus et al., 1993). Tagging performance degrades on out-of-domain data, and Twitter poses additional challenges due to the conversational nature of the text, the lack of conventional orthography, and 140character limit of each message (“tweet”). Figure 1 shows three tweets which illustrate t"
P11-2008,J93-2004,0,0.0664899,"erman, 2010; O’Connor et al., 2010a; Thelwall et al., 2011). However, the bulk of this work eschews the standard pipeline of tools which might enable a richer linguistic analysis; such tools are typically trained on newstext and have been shown to perform poorly on Twitter (Finin et al., 2010). One of the most fundamental parts of the linguistic pipeline is part-of-speech (POS) tagging, a basic form of syntactic analysis which has countless applications in NLP. Most POS taggers are trained from treebanks in the newswire domain, such as the Wall Street Journal corpus of the Penn Treebank (PTB; Marcus et al., 1993). Tagging performance degrades on out-of-domain data, and Twitter poses additional challenges due to the conversational nature of the text, the lack of conventional orthography, and 140character limit of each message (“tweet”). Figure 1 shows three tweets which illustrate these challenges. In this paper, we produce an English POS tagger that is designed especially for Twitter data. Our contributions are as follows: • we developed a POS tagset for Twitter, • we manually tagged 1,827 tweets, • we developed features for Twitter POS tagging and conducted experiments to evaluate them, and • we prov"
P11-2008,petrov-etal-2012-universal,1,0.297019,"Missing"
P11-2008,N10-1020,0,0.140487,"yD licenseN and& #2$ notR takinV druN booN toP jailN ., ThankV uO God∧ ., #amen# Figure 1: Example tweets with gold annotations. Underlined tokens show tagger improvements due to features detailed in Section 3 (respectively: TAG D ICT, M ETAPH, and D IST S IM). Introduction The growing popularity of social media and usercreated web content is producing enormous quantities of text in electronic form. The popular microblogging service Twitter (twitter.com) is one particularly fruitful source of user-created content, and a flurry of recent research has aimed to understand and exploit these data (Ritter et al., 2010; Sharifi et al., 2010; Barbosa and Feng, 2010; Asur and Huberman, 2010; O’Connor et al., 2010a; Thelwall et al., 2011). However, the bulk of this work eschews the standard pipeline of tools which might enable a richer linguistic analysis; such tools are typically trained on newstext and have been shown to perform poorly on Twitter (Finin et al., 2010). One of the most fundamental parts of the linguistic pipeline is part-of-speech (POS) tagging, a basic form of syntactic analysis which has countless applications in NLP. Most POS taggers are trained from treebanks in the newswire domain, such a"
P11-2008,N10-1100,0,0.180483,"notR takinV druN booN toP jailN ., ThankV uO God∧ ., #amen# Figure 1: Example tweets with gold annotations. Underlined tokens show tagger improvements due to features detailed in Section 3 (respectively: TAG D ICT, M ETAPH, and D IST S IM). Introduction The growing popularity of social media and usercreated web content is producing enormous quantities of text in electronic form. The popular microblogging service Twitter (twitter.com) is one particularly fruitful source of user-created content, and a flurry of recent research has aimed to understand and exploit these data (Ritter et al., 2010; Sharifi et al., 2010; Barbosa and Feng, 2010; Asur and Huberman, 2010; O’Connor et al., 2010a; Thelwall et al., 2011). However, the bulk of this work eschews the standard pipeline of tools which might enable a richer linguistic analysis; such tools are typically trained on newstext and have been shown to perform poorly on Twitter (Finin et al., 2010). One of the most fundamental parts of the linguistic pipeline is part-of-speech (POS) tagging, a basic form of syntactic analysis which has countless applications in NLP. Most POS taggers are trained from treebanks in the newswire domain, such as the Wall Street Jour"
P11-2008,N03-1033,0,0.210842,"Missing"
P11-2008,P10-1040,0,0.263127,"butional similarity. When training data is limited, distributional features from unlabeled text can improve performance (Sch¨utze and Pedersen, 1993). We used 1.9 million tokens from 134,000 unlabeled tweets to construct distributional features from the successor and predecessor probabilities for the 10,000 most common terms. The successor and predecessor transition matrices are horizontally concatenated into a sparse matrix M, which we approximate using a truncated singular value decomposition: M ≈ USVT , where U is limited to 50 columns. Each term’s feature vector is its row in U; following Turian et al. (2010), we standardize and scale the standard deviation to 0.1. M ETAPH : Phonetic normalization. Since Twitter includes many alternate spellings of words, we used the Metaphone algorithm (Philips, 1990)9 to create a coarse phonetic normalization of words to simpler keys. Metaphone consists of 19 rules that rewrite consonants and delete vowels. For example, in our 7 1 α = 100 , C = 10; this score is equivalent to the posterior probability of capitalization with a Beta(0.1, 9.9) prior. 8 Both WSJ and Brown corpora, no case normalization. We also tried adding the WordNet (Fellbaum, 1998) and Moby (War"
P11-2031,W05-0909,1,0.0850933,"m is the average of all mi s, we report the standard deviation over the tuning set as sdev : v u n uX (mi − m)2 sdev = t n−1 0.5 0.4 0.4 0.5 0.6 0.2 i=1 0.1 0.2 0.2 0.1 0.3 0.4 A high sdev value may indicate that the optimizer is struggling with local optima and changing hyperparameters (e.g. more random restarts in MERT) could improve system performance. Table 1: Measured standard deviations of different automatic metrics due to test-set and optimizer variability. sdev is reported only for the tuning objective function BLEU. Results are reported using BLEU (Papineni et al., 2002), M ETEOR5 (Banerjee and Lavie, 2005; Denkowski and Lavie, 2010), and TER (Snover et al., 2006). 4.1 Extraneous variables in one system In this section, we describe and measure (on the example systems just described) three extraneous variables that should be considered when evaluating a translation system. We quantify these variables in terms of standard deviation s, since it is expressed in the same units as the original metric. Refer to Table 1 for the statistics. Local optima effects sdev The first extraneous variable we discuss is the stochasticity of the optimizer. As discussed above, different optimization runs find differ"
P11-2031,J08-1003,0,0.0192515,"Missing"
P11-2031,W08-0304,0,0.0315931,"ther uses in NLP include the MUC-6 evaluation (Chinchor, 1993) and parsing (Cahill et al., 2008). However, these previous methods assume model parameters are elements of the system rather than extraneous variables. Prior work on optimizer noise in MT has focused primarily on reducing optimizer instability (whereas our concern is how to deal with optimizer noise, when it exists). Foster and Kuhn (2009) measured the instability of held-out BLEU scores across 10 MERT runs to improve tune/test set correlation. However, they only briefly mention the implications of the instability on significance. Cer et al. (2008) 177 explored regularization of MERT to improve generalization on test sets. Moore and Quirk (2008) explored strategies for selecting better random “restart points” in optimization. Cer et al. (2010) analyzed the standard deviation over 5 MERT runs when each of several metrics was used as the objective function. 4 Experiments In our experiments, we ran the MERT optimizer to optimize BLEU on a held-out development set many times to obtain a set of optimizer samples on two different pairs of systems (4 configurations total). Each pair consists of a baseline system (System A) and an “experimental"
P11-2031,N10-1080,0,0.00491755,"eous variables. Prior work on optimizer noise in MT has focused primarily on reducing optimizer instability (whereas our concern is how to deal with optimizer noise, when it exists). Foster and Kuhn (2009) measured the instability of held-out BLEU scores across 10 MERT runs to improve tune/test set correlation. However, they only briefly mention the implications of the instability on significance. Cer et al. (2008) 177 explored regularization of MERT to improve generalization on test sets. Moore and Quirk (2008) explored strategies for selecting better random “restart points” in optimization. Cer et al. (2010) analyzed the standard deviation over 5 MERT runs when each of several metrics was used as the objective function. 4 Experiments In our experiments, we ran the MERT optimizer to optimize BLEU on a held-out development set many times to obtain a set of optimizer samples on two different pairs of systems (4 configurations total). Each pair consists of a baseline system (System A) and an “experimental” system (System B), which previous research has suggested will perform better. The first system pair contrasts a baseline phrasebased system (Moses) and experimental hierarchical phrase-based system"
P11-2031,D08-1024,0,0.049401,"initialization and search hyperparameters. Additionally, stochastic optimization and search techniques, such as minimum error rate training (Och, 2003) and Markov chain Monte Carlo methods (Arun et al., 2010),3 constitute a second, more obvious source of noise in the optimization procedure. This variation in the parameter vector affects the quality of the model measured on both development 2 This variation directly affects the output translations, and so it will propagate to both automated metrics as well as human evaluators. 3 Online subgradient techniques such as MIRA (Crammer et al., 2006; Chiang et al., 2008) have an implicit stochastic component as well based on the order of the training examples. 176 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 176–181, c Portland, Oregon, June 19-24, 2011. 2011 Association for Computational Linguistics data and held-out test data, independently of any experimental manipulation. Thus, when trying to determine whether the difference between two measurements is significant, it is necessary to control for variance due to noisy parameter estimates. This can be done by replication of the optimization proce"
P11-2031,J07-2003,0,0.277048,"e MERT optimizer to optimize BLEU on a held-out development set many times to obtain a set of optimizer samples on two different pairs of systems (4 configurations total). Each pair consists of a baseline system (System A) and an “experimental” system (System B), which previous research has suggested will perform better. The first system pair contrasts a baseline phrasebased system (Moses) and experimental hierarchical phrase-based system (Hiero), which were constructed from the Chinese-English BTEC corpus (0.7M words), the later of which was decoded with the cdec decoder (Koehn et al., 2007; Chiang, 2007; Dyer et al., 2010). The second system pair contrasts two German-English Hiero/cdec systems constructed from the WMT11 parallel training data (98M words).4 The baseline system was trained on unsegmented words, and the experimental system was constructed using the most probable segmentation of the German text according to the CRF word segmentation model of Dyer (2009). The ChineseEnglish systems were optimized 300 times, and the German-English systems were optimized 50 times. Our experiments used the default implementation of MERT that accompanies each of the two decoders. The Moses MERT imple"
P11-2031,M93-1008,0,0.427905,"rocessing. Bootstrap techniques (Efron, 1979; Wasserman, 2003) are widespread in many problem areas, including for confidence estimation in speech recognition (Bisani and Ney, 2004), and to determine the significance of MT results (Och, 2003; Koehn, 2004; Zhang et al., 2004; Zhang and Vogel, 2010). Approximate randomization (AR) has been proposed as a more reliable technique for MT significance testing, and evidence suggests that it yields fewer type I errors (i.e., claiming a significant difference where none exists; Riezler and Maxwell, 2005). Other uses in NLP include the MUC-6 evaluation (Chinchor, 1993) and parsing (Cahill et al., 2008). However, these previous methods assume model parameters are elements of the system rather than extraneous variables. Prior work on optimizer noise in MT has focused primarily on reducing optimizer instability (whereas our concern is how to deal with optimizer noise, when it exists). Foster and Kuhn (2009) measured the instability of held-out BLEU scores across 10 MERT runs to improve tune/test set correlation. However, they only briefly mention the implications of the instability on significance. Cer et al. (2008) 177 explored regularization of MERT to impro"
P11-2031,N10-1031,1,0.375888,"i s, we report the standard deviation over the tuning set as sdev : v u n uX (mi − m)2 sdev = t n−1 0.5 0.4 0.4 0.5 0.6 0.2 i=1 0.1 0.2 0.2 0.1 0.3 0.4 A high sdev value may indicate that the optimizer is struggling with local optima and changing hyperparameters (e.g. more random restarts in MERT) could improve system performance. Table 1: Measured standard deviations of different automatic metrics due to test-set and optimizer variability. sdev is reported only for the tuning objective function BLEU. Results are reported using BLEU (Papineni et al., 2002), M ETEOR5 (Banerjee and Lavie, 2005; Denkowski and Lavie, 2010), and TER (Snover et al., 2006). 4.1 Extraneous variables in one system In this section, we describe and measure (on the example systems just described) three extraneous variables that should be considered when evaluating a translation system. We quantify these variables in terms of standard deviation s, since it is expressed in the same units as the original metric. Refer to Table 1 for the statistics. Local optima effects sdev The first extraneous variable we discuss is the stochasticity of the optimizer. As discussed above, different optimization runs find different local maxima. The noise"
P11-2031,P10-4002,1,0.432465,"er to optimize BLEU on a held-out development set many times to obtain a set of optimizer samples on two different pairs of systems (4 configurations total). Each pair consists of a baseline system (System A) and an “experimental” system (System B), which previous research has suggested will perform better. The first system pair contrasts a baseline phrasebased system (Moses) and experimental hierarchical phrase-based system (Hiero), which were constructed from the Chinese-English BTEC corpus (0.7M words), the later of which was decoded with the cdec decoder (Koehn et al., 2007; Chiang, 2007; Dyer et al., 2010). The second system pair contrasts two German-English Hiero/cdec systems constructed from the WMT11 parallel training data (98M words).4 The baseline system was trained on unsegmented words, and the experimental system was constructed using the most probable segmentation of the German text according to the CRF word segmentation model of Dyer (2009). The ChineseEnglish systems were optimized 300 times, and the German-English systems were optimized 50 times. Our experiments used the default implementation of MERT that accompanies each of the two decoders. The Moses MERT implementation uses 20 ra"
P11-2031,N09-1046,1,0.489862,"ebased system (Moses) and experimental hierarchical phrase-based system (Hiero), which were constructed from the Chinese-English BTEC corpus (0.7M words), the later of which was decoded with the cdec decoder (Koehn et al., 2007; Chiang, 2007; Dyer et al., 2010). The second system pair contrasts two German-English Hiero/cdec systems constructed from the WMT11 parallel training data (98M words).4 The baseline system was trained on unsegmented words, and the experimental system was constructed using the most probable segmentation of the German text according to the CRF word segmentation model of Dyer (2009). The ChineseEnglish systems were optimized 300 times, and the German-English systems were optimized 50 times. Our experiments used the default implementation of MERT that accompanies each of the two decoders. The Moses MERT implementation uses 20 random restart points per iteration, drawn uniformly from the default ranges for each feature, and, at each iteration, 200-best lists were extracted with the current weight vector (Bertoldi et al., 2009). The cdec MERT implementation performs inference over the decoder search space which is structured as a hypergraph (Kumar et al., 2009). Rather than"
P11-2031,W09-0439,0,0.0241928,"been proposed as a more reliable technique for MT significance testing, and evidence suggests that it yields fewer type I errors (i.e., claiming a significant difference where none exists; Riezler and Maxwell, 2005). Other uses in NLP include the MUC-6 evaluation (Chinchor, 1993) and parsing (Cahill et al., 2008). However, these previous methods assume model parameters are elements of the system rather than extraneous variables. Prior work on optimizer noise in MT has focused primarily on reducing optimizer instability (whereas our concern is how to deal with optimizer noise, when it exists). Foster and Kuhn (2009) measured the instability of held-out BLEU scores across 10 MERT runs to improve tune/test set correlation. However, they only briefly mention the implications of the instability on significance. Cer et al. (2008) 177 explored regularization of MERT to improve generalization on test sets. Moore and Quirk (2008) explored strategies for selecting better random “restart points” in optimization. Cer et al. (2010) analyzed the standard deviation over 5 MERT runs when each of several metrics was used as the objective function. 4 Experiments In our experiments, we ran the MERT optimizer to optimize B"
P11-2031,P07-2045,1,0.0508851,"periments, we ran the MERT optimizer to optimize BLEU on a held-out development set many times to obtain a set of optimizer samples on two different pairs of systems (4 configurations total). Each pair consists of a baseline system (System A) and an “experimental” system (System B), which previous research has suggested will perform better. The first system pair contrasts a baseline phrasebased system (Moses) and experimental hierarchical phrase-based system (Hiero), which were constructed from the Chinese-English BTEC corpus (0.7M words), the later of which was decoded with the cdec decoder (Koehn et al., 2007; Chiang, 2007; Dyer et al., 2010). The second system pair contrasts two German-English Hiero/cdec systems constructed from the WMT11 parallel training data (98M words).4 The baseline system was trained on unsegmented words, and the experimental system was constructed using the most probable segmentation of the German text according to the CRF word segmentation model of Dyer (2009). The ChineseEnglish systems were optimized 300 times, and the German-English systems were optimized 50 times. Our experiments used the default implementation of MERT that accompanies each of the two decoders. The Mo"
P11-2031,W04-3250,0,0.348961,"ine result paired with a low experimental result could lead to a useful experimental manipulation being incorrectly identified as useless. We now turn to the question of how to reduce the probability falling into this trap. 3 Related Work The use of statistical hypothesis testing has grown apace with the adoption of empirical methods in natural language processing. Bootstrap techniques (Efron, 1979; Wasserman, 2003) are widespread in many problem areas, including for confidence estimation in speech recognition (Bisani and Ney, 2004), and to determine the significance of MT results (Och, 2003; Koehn, 2004; Zhang et al., 2004; Zhang and Vogel, 2010). Approximate randomization (AR) has been proposed as a more reliable technique for MT significance testing, and evidence suggests that it yields fewer type I errors (i.e., claiming a significant difference where none exists; Riezler and Maxwell, 2005). Other uses in NLP include the MUC-6 evaluation (Chinchor, 1993) and parsing (Cahill et al., 2008). However, these previous methods assume model parameters are elements of the system rather than extraneous variables. Prior work on optimizer noise in MT has focused primarily on reducing optimizer instab"
P11-2031,P09-1019,1,0.208024,"egmentation model of Dyer (2009). The ChineseEnglish systems were optimized 300 times, and the German-English systems were optimized 50 times. Our experiments used the default implementation of MERT that accompanies each of the two decoders. The Moses MERT implementation uses 20 random restart points per iteration, drawn uniformly from the default ranges for each feature, and, at each iteration, 200-best lists were extracted with the current weight vector (Bertoldi et al., 2009). The cdec MERT implementation performs inference over the decoder search space which is structured as a hypergraph (Kumar et al., 2009). Rather than using restart points, in addition to optimizing each feature independently, it optimizes in 5 random directions per iteration by constructing a search vector by uniformly sampling each element of the vector from (−1, 1) and then renormalizing so it has length 1. For all systems, the initial weight vector was manually initialized so as to yield reasonable translations. 4 http://statmt.org/wmt11/ System Avg ssel sdev BTEC Chinese-English (n = 300) System A 48.4 1.6 0.2 BLEU ↑ System B 49.9 1.5 0.1 System A 63.3 0.9 MET ↑ System B 63.8 0.9 System A 30.2 1.1 TER ↓ System B 28.7 1.0 W"
P11-2031,C08-1074,0,0.00521439,"8). However, these previous methods assume model parameters are elements of the system rather than extraneous variables. Prior work on optimizer noise in MT has focused primarily on reducing optimizer instability (whereas our concern is how to deal with optimizer noise, when it exists). Foster and Kuhn (2009) measured the instability of held-out BLEU scores across 10 MERT runs to improve tune/test set correlation. However, they only briefly mention the implications of the instability on significance. Cer et al. (2008) 177 explored regularization of MERT to improve generalization on test sets. Moore and Quirk (2008) explored strategies for selecting better random “restart points” in optimization. Cer et al. (2010) analyzed the standard deviation over 5 MERT runs when each of several metrics was used as the objective function. 4 Experiments In our experiments, we ran the MERT optimizer to optimize BLEU on a held-out development set many times to obtain a set of optimizer samples on two different pairs of systems (4 configurations total). Each pair consists of a baseline system (System A) and an “experimental” system (System B), which previous research has suggested will perform better. The first system pa"
P11-2031,P03-1021,0,0.949219,"proves translation quality in comparison to a baseline system. To answer this question, he runs an experiment to evaluate the behavior of the two systems on held-out data. In this paper, we consider how to make such experiments more statistically reliable. We provide a systematic analysis of the effects of optimizer instability—an extraneous variable that is seldom controlled for—on experimental outcomes, and make recommendations for reporting results more accurately. 1 2 Introduction The need for statistical hypothesis testing for machine translation (MT) has been acknowledged since at least Och (2003). In that work, the proposed method was based on bootstrap resampling and was designed to improve the statistical reliability of results by controlling for randomness across test sets. However, there is no consistently used strategy that controls for the effects of unstable estimates of model parameters.1 While the existence of optimizer instability is an acknowledged problem, it is only infrequently discussed in relation to the reliability of experimental results, and, to our knowledge, there has yet to be a systematic study of its effects on 1 We hypothesize that the convention of “trusting”"
P11-2031,P02-1040,0,0.112437,"for the ith optimization run, and m is the average of all mi s, we report the standard deviation over the tuning set as sdev : v u n uX (mi − m)2 sdev = t n−1 0.5 0.4 0.4 0.5 0.6 0.2 i=1 0.1 0.2 0.2 0.1 0.3 0.4 A high sdev value may indicate that the optimizer is struggling with local optima and changing hyperparameters (e.g. more random restarts in MERT) could improve system performance. Table 1: Measured standard deviations of different automatic metrics due to test-set and optimizer variability. sdev is reported only for the tuning objective function BLEU. Results are reported using BLEU (Papineni et al., 2002), M ETEOR5 (Banerjee and Lavie, 2005; Denkowski and Lavie, 2010), and TER (Snover et al., 2006). 4.1 Extraneous variables in one system In this section, we describe and measure (on the example systems just described) three extraneous variables that should be considered when evaluating a translation system. We quantify these variables in terms of standard deviation s, since it is expressed in the same units as the original metric. Refer to Table 1 for the statistics. Local optima effects sdev The first extraneous variable we discuss is the stochasticity of the optimizer. As discussed above, dif"
P11-2031,W05-0908,0,0.060395,"has grown apace with the adoption of empirical methods in natural language processing. Bootstrap techniques (Efron, 1979; Wasserman, 2003) are widespread in many problem areas, including for confidence estimation in speech recognition (Bisani and Ney, 2004), and to determine the significance of MT results (Och, 2003; Koehn, 2004; Zhang et al., 2004; Zhang and Vogel, 2010). Approximate randomization (AR) has been proposed as a more reliable technique for MT significance testing, and evidence suggests that it yields fewer type I errors (i.e., claiming a significant difference where none exists; Riezler and Maxwell, 2005). Other uses in NLP include the MUC-6 evaluation (Chinchor, 1993) and parsing (Cahill et al., 2008). However, these previous methods assume model parameters are elements of the system rather than extraneous variables. Prior work on optimizer noise in MT has focused primarily on reducing optimizer instability (whereas our concern is how to deal with optimizer noise, when it exists). Foster and Kuhn (2009) measured the instability of held-out BLEU scores across 10 MERT runs to improve tune/test set correlation. However, they only briefly mention the implications of the instability on significanc"
P11-2031,2006.amta-papers.25,0,0.190763,"over the tuning set as sdev : v u n uX (mi − m)2 sdev = t n−1 0.5 0.4 0.4 0.5 0.6 0.2 i=1 0.1 0.2 0.2 0.1 0.3 0.4 A high sdev value may indicate that the optimizer is struggling with local optima and changing hyperparameters (e.g. more random restarts in MERT) could improve system performance. Table 1: Measured standard deviations of different automatic metrics due to test-set and optimizer variability. sdev is reported only for the tuning objective function BLEU. Results are reported using BLEU (Papineni et al., 2002), M ETEOR5 (Banerjee and Lavie, 2005; Denkowski and Lavie, 2010), and TER (Snover et al., 2006). 4.1 Extraneous variables in one system In this section, we describe and measure (on the example systems just described) three extraneous variables that should be considered when evaluating a translation system. We quantify these variables in terms of standard deviation s, since it is expressed in the same units as the original metric. Refer to Table 1 for the statistics. Local optima effects sdev The first extraneous variable we discuss is the stochasticity of the optimizer. As discussed above, different optimization runs find different local maxima. The noise due to this variable can depend"
P11-2031,zhang-etal-2004-interpreting,0,0.00388873,"ired with a low experimental result could lead to a useful experimental manipulation being incorrectly identified as useless. We now turn to the question of how to reduce the probability falling into this trap. 3 Related Work The use of statistical hypothesis testing has grown apace with the adoption of empirical methods in natural language processing. Bootstrap techniques (Efron, 1979; Wasserman, 2003) are widespread in many problem areas, including for confidence estimation in speech recognition (Bisani and Ney, 2004), and to determine the significance of MT results (Och, 2003; Koehn, 2004; Zhang et al., 2004; Zhang and Vogel, 2010). Approximate randomization (AR) has been proposed as a more reliable technique for MT significance testing, and evidence suggests that it yields fewer type I errors (i.e., claiming a significant difference where none exists; Riezler and Maxwell, 2005). Other uses in NLP include the MUC-6 evaluation (Chinchor, 1993) and parsing (Cahill et al., 2008). However, these previous methods assume model parameters are elements of the system rather than extraneous variables. Prior work on optimizer noise in MT has focused primarily on reducing optimizer instability (whereas our c"
P12-1072,N01-1007,0,0.0341389,"ffiliated with the Chicago Bears. However, when the model saw a mention token The Bears, it did not have any other columns to put the word token The, and decided to put it in the fourth column although it is not a location. If we added more columns, determiners could become another attribute of the entities that might go into one of these new columns. 692 5 Related Work There has been work that attempts to fill predefined templates using Bayesian nonparametrics (Haghighi and Klein, 2010) and automatically learns template structures using agglomerative clustering (Chambers and Jurafsky, 2011). Charniak (2001) and Elsner et al. (2009) focused specifically on names and discovering their structure, which is a part of the problem we consider here. More similar to our work, Eisenstein et al. (2011) introduced a nonparametric Bayesian approach to extract structured databases of entities. A fundamental difference of our approach from any of the previous work is it maximizes conditional likelihood and thus allows beneficial incorporation of arbitrary features. Our model is focused on the problem of canonicalizing mention strings into their parts, though its r variables (which map mentions to rows) could b"
P12-1072,W11-2202,1,0.398081,"them—is a central part of many NLP applications. We seek an algorithm that infers a set of real-world entities from mentions in a text, mapping each entity mention token to an entity, and discovers general categories of words used in names (e.g., titles and last names). Here, we use a probabilistic model to infer a structured representation of canonical forms of entity attributes through transductive learning from named entity mentions with a small number of seeds (see Table 1). The input is a collection of mentions found by a named entity recognizer, along with their contexts, and, following Eisenstein et al. (2011), the output is a table in which entities are rows (the number of which is not pre-specified) and attribute words are organized into columns. This paper contributes a model that builds on the approach of Eisenstein et al. (2011), but also: • incorporates context of the mention to help with disambiguation and to allow mentions that do not share words to be merged liberally; • conditions against shape features, which improve the assignment of words to columns; • is designed to explicitly handle some noise; and • is learned using elements of Bayesian inference with conditional estimation (see §2)"
P12-1072,N09-1019,0,0.020192,"hicago Bears. However, when the model saw a mention token The Bears, it did not have any other columns to put the word token The, and decided to put it in the fourth column although it is not a location. If we added more columns, determiners could become another attribute of the entities that might go into one of these new columns. 692 5 Related Work There has been work that attempts to fill predefined templates using Bayesian nonparametrics (Haghighi and Klein, 2010) and automatically learns template structures using agglomerative clustering (Chambers and Jurafsky, 2011). Charniak (2001) and Elsner et al. (2009) focused specifically on names and discovering their structure, which is a part of the problem we consider here. More similar to our work, Eisenstein et al. (2011) introduced a nonparametric Bayesian approach to extract structured databases of entities. A fundamental difference of our approach from any of the previous work is it maximizes conditional likelihood and thus allows beneficial incorporation of arbitrary features. Our model is focused on the problem of canonicalizing mention strings into their parts, though its r variables (which map mentions to rows) could be interpreted as (within-"
P12-1072,P05-1045,0,0.0106066,"some features β for both datasets. These values were obtained from preliminary experiments on a smaller sample of the datasets, and updated on the first EM iteration. 689 # source documents # mentions # unique mentions size of mention vocabulary size of context vocabulary Politics 3,000 10,647 528 666 2,934 Sports 700 13,813 884 1,177 2,844 Table 2: Descriptive statistics about the datasets. Association, National Football League, and Major League Baseball) in 2009. Due to the large size of the corpora, we uniformly sampled a subset of documents for each corpus and ran the Stanford NER tagger (Finkel et al., 2005), which tagged named entities mentions as person, location, and organization. We used named entity of type person from the political blogs corpus, while we are interested in person and organization entities for the sports news corpus. Mentions that appear less than five times are discarded. Table 2 summarizes statistics for both datasets of named entity mentions. Reference tables. We use Eisenstein et al.’s manually built 125-entity (282 vocabulary items) reference table for the politics dataset. Each entity in the table is represented by the set of all tokens that apppear in its references, a"
P12-1072,P07-1107,0,0.0318528,"work, Eisenstein et al. (2011) introduced a nonparametric Bayesian approach to extract structured databases of entities. A fundamental difference of our approach from any of the previous work is it maximizes conditional likelihood and thus allows beneficial incorporation of arbitrary features. Our model is focused on the problem of canonicalizing mention strings into their parts, though its r variables (which map mentions to rows) could be interpreted as (within-document and cross-document) coreference resolution, which has been tackled using a range of probabilistic models (Li et al., 2004; Haghighi and Klein, 2007; Poon and Domingos, 2008; Singh et al., 2011). We have not evaluated it as such, believing that further work should be done to integrate appropriate linguistic cues before such an application. 6 Conclusions We presented an improved probabilistic model for canonicalizing named entities into a table. We showed that the model adapts to different tasks depending on its input and seeds, and that it improves over state-of-the-art performance on two corpora. Acknowledgements The authors thank Jacob Eisenstein and Tae Yano for helpful discussions and providing us with the implementation of their mode"
P12-1072,P10-2054,0,0.0251975,"ible improvement might be obtained by providing more context to the model. The sixth row is interesting because Dave Toub is indeed affiliated with the Chicago Bears. However, when the model saw a mention token The Bears, it did not have any other columns to put the word token The, and decided to put it in the fourth column although it is not a location. If we added more columns, determiners could become another attribute of the entities that might go into one of these new columns. 692 5 Related Work There has been work that attempts to fill predefined templates using Bayesian nonparametrics (Haghighi and Klein, 2010) and automatically learns template structures using agglomerative clustering (Chambers and Jurafsky, 2011). Charniak (2001) and Elsner et al. (2009) focused specifically on names and discovering their structure, which is a part of the problem we consider here. More similar to our work, Eisenstein et al. (2011) introduced a nonparametric Bayesian approach to extract structured databases of entities. A fundamental difference of our approach from any of the previous work is it maximizes conditional likelihood and thus allows beneficial incorporation of arbitrary features. Our model is focused on"
P12-1072,P09-1012,0,0.0348344,"lumn index c1 for the first word in the mention, marginalizing out probabilities of other words in the mention. After we sample the column index for the first word, we sample the column index c2 for the second word, fixing the previous word to be in column c1 , and marginalizing out probabilities of c3 , . . . , cL as before. We repeat the above procedure until we reach the last word in the mention. In practice, this can be done efficiently using backward probabilities computed via dynamic programming. This kind of blocked Gibbs sampling was proposed by Jensen et al. (1995) and used in NLP by Mochihashi et al. (2009). We have: p(cm` = c |. . .) ∝ p(cm` = c |f m` , β)p(cm` = c |cm`− = c− ) P  p (c = c | c = c ) + b m` m` + c+ where `− is the preceding word and c− is its sampled index, `+ is the following word and c+ is its possible index, and pb (·) are backward probabilities. Alternatively, we can perform standard Gibbs sampling and drop the dependencies between columns, which makes the model rely more heavily on the features. For completeness, we detail the computations. Featurized log linear distribution. Our model can use arbitrary features to choose a column index. These features are incorporated as"
P12-1072,D08-1068,0,0.0269122,"2011) introduced a nonparametric Bayesian approach to extract structured databases of entities. A fundamental difference of our approach from any of the previous work is it maximizes conditional likelihood and thus allows beneficial incorporation of arbitrary features. Our model is focused on the problem of canonicalizing mention strings into their parts, though its r variables (which map mentions to rows) could be interpreted as (within-document and cross-document) coreference resolution, which has been tackled using a range of probabilistic models (Li et al., 2004; Haghighi and Klein, 2007; Poon and Domingos, 2008; Singh et al., 2011). We have not evaluated it as such, believing that further work should be done to integrate appropriate linguistic cues before such an application. 6 Conclusions We presented an improved probabilistic model for canonicalizing named entities into a table. We showed that the model adapts to different tasks depending on its input and seeds, and that it improves over state-of-the-art performance on two corpora. Acknowledgements The authors thank Jacob Eisenstein and Tae Yano for helpful discussions and providing us with the implementation of their model, Tim Hawes for helpful"
P12-1072,P11-1080,0,0.116393,"ametric Bayesian approach to extract structured databases of entities. A fundamental difference of our approach from any of the previous work is it maximizes conditional likelihood and thus allows beneficial incorporation of arbitrary features. Our model is focused on the problem of canonicalizing mention strings into their parts, though its r variables (which map mentions to rows) could be interpreted as (within-document and cross-document) coreference resolution, which has been tackled using a range of probabilistic models (Li et al., 2004; Haghighi and Klein, 2007; Poon and Domingos, 2008; Singh et al., 2011). We have not evaluated it as such, believing that further work should be done to integrate appropriate linguistic cues before such an application. 6 Conclusions We presented an improved probabilistic model for canonicalizing named entities into a table. We showed that the model adapts to different tasks depending on its input and seeds, and that it improves over state-of-the-art performance on two corpora. Acknowledgements The authors thank Jacob Eisenstein and Tae Yano for helpful discussions and providing us with the implementation of their model, Tim Hawes for helpful discussions, Naomi Sa"
P12-1072,P05-1044,1,0.768347,"iefs. Fixing f is quite different; it is conditioning our model on some observable features of the data, in this case word shape features. We do this to avoid integrating over feature vector values. These choices highlight that the design of a probabilistic model can draw from both Bayesian and discriminative tools. Observing some of x as seeds (˜ x) renders this approach transductive. Exact inference in this model is intractable, so we resort to an approximate inference technique based on Markov Chain Monte Carlo simulation. The optimization of β can be described as “contrastive” estimation (Smith and Eisner, 2005), in which some aspects of the data are conditioned against for computational convenience. The optimization of τ can be described as “empirical Bayesian” estimation (Morris, 1983) in which the parameters of a prior are fit to data. Our overall learning procedure is a Monte Carlo Expectation Maximization algorithm (Wei and Tanner, 1990). 3 Learning and Inference Our learning procedure is an iterative algorithm consisting of two steps. In the E-step, we perform collapsed Gibbs sampling to obtain distributions over row and column indices for every mention, given the current value of the hyperpara"
P12-1072,P11-1098,0,\N,Missing
P12-2050,attardi-etal-2010-resource,0,0.557965,"Missing"
P12-2050,P98-1013,0,0.0338496,"r require prohibitive expertise and effort to apply.2 We therefore turn to supersense tags (SSTs), 40 coarse lexical semantic classes (25 for nouns, 15 for verbs) originating in WordNet. Previously these served as groupings of English lexicon 1 Some ontologies like those in Sekine et al. (2002) and BBN Identifinder (Bikel et al., 1999) include a large selection of classes, which tend to be especially relevant to proper names. 2 E.g., a WordNet (Fellbaum, 1998) sense annotation effort reported by Passonneau et al. (2010) found considerable interannotator variability for some lexemes; FrameNet (Baker et al., 1998) is limited in coverage, even for English; and PropBank (Kingsbury and Palmer, 2002) does not capture semantic relationships across lexemes. We note that the Omega ontology (Philpot et al., 2003) has been used for fine-grained crosslingual annotation (Hovy et al., 2006; Dorr et al., 2010). Figure 1: A sentence from the article “Islamic Golden Age,” with the supersense tagging from one of two annotators. The Arabic is shown left-to-right. entries, but here we have repurposed them as target labels for direct human annotation. Part of the earliest versions of WordNet, the supersense categories (o"
P12-2050,W06-1670,0,0.786017,"Missing"
P12-2050,W03-1022,0,0.0596354,"(Hovy et al., 2006; Dorr et al., 2010). Figure 1: A sentence from the article “Islamic Golden Age,” with the supersense tagging from one of two annotators. The Arabic is shown left-to-right. entries, but here we have repurposed them as target labels for direct human annotation. Part of the earliest versions of WordNet, the supersense categories (originally, “lexicographer classes”) were intended to partition all English noun and verb senses into broad groupings, or semantic fields (Miller, 1990; Fellbaum, 1990). More recently, the task of automatic supersense tagging has emerged for English (Ciaramita and Johnson, 2003; Curran, 2005; Ciaramita and Altun, 2006; Paaß and Reichartz, 2009), as well as for Italian (Picca et al., 2008; Picca et al., 2009; Attardi et al., 2010) and Chinese (Qiu et al., 2011), languages with WordNets mapped to English WordNet.3 In principle, we believe supersenses ought to apply to nouns and verbs in any language, and need not depend on the availability of a semantic lexicon.4 In this work we focus on the noun SSTs, summarized in figure 2 and applied to an Arabic sentence in figure 1. SSTs both refine and relate lexical items: they capture lexical polysemy on the one hand—e.g., 3 N"
P12-2050,P05-1004,0,0.00884402,"al., 2010). Figure 1: A sentence from the article “Islamic Golden Age,” with the supersense tagging from one of two annotators. The Arabic is shown left-to-right. entries, but here we have repurposed them as target labels for direct human annotation. Part of the earliest versions of WordNet, the supersense categories (originally, “lexicographer classes”) were intended to partition all English noun and verb senses into broad groupings, or semantic fields (Miller, 1990; Fellbaum, 1990). More recently, the task of automatic supersense tagging has emerged for English (Ciaramita and Johnson, 2003; Curran, 2005; Ciaramita and Altun, 2006; Paaß and Reichartz, 2009), as well as for Italian (Picca et al., 2008; Picca et al., 2009; Attardi et al., 2010) and Chinese (Qiu et al., 2011), languages with WordNets mapped to English WordNet.3 In principle, we believe supersenses ought to apply to nouns and verbs in any language, and need not depend on the availability of a semantic lexicon.4 In this work we focus on the noun SSTs, summarized in figure 2 and applied to an Arabic sentence in figure 1. SSTs both refine and relate lexical items: they capture lexical polysemy on the one hand—e.g., 3 Note that work"
P12-2050,elkateb-etal-2006-building,0,0.102714,"Missing"
P12-2050,N06-2015,0,0.015385,"in Sekine et al. (2002) and BBN Identifinder (Bikel et al., 1999) include a large selection of classes, which tend to be especially relevant to proper names. 2 E.g., a WordNet (Fellbaum, 1998) sense annotation effort reported by Passonneau et al. (2010) found considerable interannotator variability for some lexemes; FrameNet (Baker et al., 1998) is limited in coverage, even for English; and PropBank (Kingsbury and Palmer, 2002) does not capture semantic relationships across lexemes. We note that the Omega ontology (Philpot et al., 2003) has been used for fine-grained crosslingual annotation (Hovy et al., 2006; Dorr et al., 2010). Figure 1: A sentence from the article “Islamic Golden Age,” with the supersense tagging from one of two annotators. The Arabic is shown left-to-right. entries, but here we have repurposed them as target labels for direct human annotation. Part of the earliest versions of WordNet, the supersense categories (originally, “lexicographer classes”) were intended to partition all English noun and verb senses into broad groupings, or semantic fields (Miller, 1990; Fellbaum, 1990). More recently, the task of automatic supersense tagging has emerged for English (Ciaramita and Johns"
P12-2050,kingsbury-palmer-2002-treebank,0,0.0369554,"upersense tags (SSTs), 40 coarse lexical semantic classes (25 for nouns, 15 for verbs) originating in WordNet. Previously these served as groupings of English lexicon 1 Some ontologies like those in Sekine et al. (2002) and BBN Identifinder (Bikel et al., 1999) include a large selection of classes, which tend to be especially relevant to proper names. 2 E.g., a WordNet (Fellbaum, 1998) sense annotation effort reported by Passonneau et al. (2010) found considerable interannotator variability for some lexemes; FrameNet (Baker et al., 1998) is limited in coverage, even for English; and PropBank (Kingsbury and Palmer, 2002) does not capture semantic relationships across lexemes. We note that the Omega ontology (Philpot et al., 2003) has been used for fine-grained crosslingual annotation (Hovy et al., 2006; Dorr et al., 2010). Figure 1: A sentence from the article “Islamic Golden Age,” with the supersense tagging from one of two annotators. The Arabic is shown left-to-right. entries, but here we have repurposed them as target labels for direct human annotation. Part of the earliest versions of WordNet, the supersense categories (originally, “lexicographer classes”) were intended to partition all English noun and"
P12-2050,C02-1150,0,0.0134343,"SONs. This lumping property might be expected to give too much latitude to annotators; yet we find that in practice, it is possible to elicit reasonable inter-annotator agreement, even for a language other than English. We encapsulate our interpretation of the tags in a set of brief guidelines that aims to be usable by anyone who can read and understand a text in the target language; our annotators had no prior expertise in linguistics or linguistic annotation. Finally, we note that ad hoc categorization schemes not unlike SSTs have been developed for purposes ranging from question answering (Li and Roth, 2002) to animacy hierarchy representation for corpus linguistics (Zaenen et al., 2004). We believe the interpretation of the SSTs adopted here can serve as a single starting point for diverse resource engineering efforts and applications, especially when fine-grained sense annotation is not feasible. 2 Tagging Conventions WordNet’s definitions of the supersenses are terse, and we could find little explicit discussion of the specific rationales behind each category. Thus, we have crafted more specific explanations, summarized for nouns in figure 2. English examples are given, but the guidelines are"
P12-2050,H93-1061,0,0.614672,"applications, especially when fine-grained sense annotation is not feasible. 2 Tagging Conventions WordNet’s definitions of the supersenses are terse, and we could find little explicit discussion of the specific rationales behind each category. Thus, we have crafted more specific explanations, summarized for nouns in figure 2. English examples are given, but the guidelines are intended to be language-neutral. A more systematic breakdown, formulated as a 43-rule decision list, is included with the corpus.5 In developing these guidelines we consulted English WordNet (Fellbaum, 1998) and SemCor (Miller et al., 1993) for examples and synset definitions, occasionally making simplifying decisions where we found distinctions that seemed esoteric or internally inconsistent. Special cases (e.g., multiword expressions, anaphora, figurative 5 For example, one rule states that all man-made structures (buildings, rooms, bridges, etc.) are to be tagged as ARTIFACTs. 254 language) are addressed with additional rules. 3 Arabic Wikipedia Annotation The annotation in this work was on top of a small corpus of Arabic Wikipedia articles that had already been annotated for named entities (Mohit et al., 2012). Here we use t"
P12-2050,E12-1017,1,0.890745,"Missing"
P12-2050,passonneau-etal-2010-word,0,0.021609,"o a full-fledged lexicon/ontology, which may insufficiently cover the language/domain of interest or require prohibitive expertise and effort to apply.2 We therefore turn to supersense tags (SSTs), 40 coarse lexical semantic classes (25 for nouns, 15 for verbs) originating in WordNet. Previously these served as groupings of English lexicon 1 Some ontologies like those in Sekine et al. (2002) and BBN Identifinder (Bikel et al., 1999) include a large selection of classes, which tend to be especially relevant to proper names. 2 E.g., a WordNet (Fellbaum, 1998) sense annotation effort reported by Passonneau et al. (2010) found considerable interannotator variability for some lexemes; FrameNet (Baker et al., 1998) is limited in coverage, even for English; and PropBank (Kingsbury and Palmer, 2002) does not capture semantic relationships across lexemes. We note that the Omega ontology (Philpot et al., 2003) has been used for fine-grained crosslingual annotation (Hovy et al., 2006; Dorr et al., 2010). Figure 1: A sentence from the article “Islamic Golden Age,” with the supersense tagging from one of two annotators. The Arabic is shown left-to-right. entries, but here we have repurposed them as target labels for d"
P12-2050,picca-etal-2008-supersense,0,0.610731,"Missing"
P12-2050,W09-3531,0,0.371255,"two annotators. The Arabic is shown left-to-right. entries, but here we have repurposed them as target labels for direct human annotation. Part of the earliest versions of WordNet, the supersense categories (originally, “lexicographer classes”) were intended to partition all English noun and verb senses into broad groupings, or semantic fields (Miller, 1990; Fellbaum, 1990). More recently, the task of automatic supersense tagging has emerged for English (Ciaramita and Johnson, 2003; Curran, 2005; Ciaramita and Altun, 2006; Paaß and Reichartz, 2009), as well as for Italian (Picca et al., 2008; Picca et al., 2009; Attardi et al., 2010) and Chinese (Qiu et al., 2011), languages with WordNets mapped to English WordNet.3 In principle, we believe supersenses ought to apply to nouns and verbs in any language, and need not depend on the availability of a semantic lexicon.4 In this work we focus on the noun SSTs, summarized in figure 2 and applied to an Arabic sentence in figure 1. SSTs both refine and relate lexical items: they capture lexical polysemy on the one hand—e.g., 3 Note that work in supersense tagging used text with finegrained sense annotations that were then coarsened to SSTs. 4 The noun/verb d"
P12-2050,sekine-etal-2002-extended,0,0.0260826,"s paper describes coarse lexical semantic annotation of Arabic Wikipedia articles subject to these constraints. Traditional lexical semantic representations are either narrow in scope, like named entities,1 or make reference to a full-fledged lexicon/ontology, which may insufficiently cover the language/domain of interest or require prohibitive expertise and effort to apply.2 We therefore turn to supersense tags (SSTs), 40 coarse lexical semantic classes (25 for nouns, 15 for verbs) originating in WordNet. Previously these served as groupings of English lexicon 1 Some ontologies like those in Sekine et al. (2002) and BBN Identifinder (Bikel et al., 1999) include a large selection of classes, which tend to be especially relevant to proper names. 2 E.g., a WordNet (Fellbaum, 1998) sense annotation effort reported by Passonneau et al. (2010) found considerable interannotator variability for some lexemes; FrameNet (Baker et al., 1998) is limited in coverage, even for English; and PropBank (Kingsbury and Palmer, 2002) does not capture semantic relationships across lexemes. We note that the Omega ontology (Philpot et al., 2003) has been used for fine-grained crosslingual annotation (Hovy et al., 2006; Dorr"
P12-2050,W04-0216,0,0.0213529,"Missing"
P12-2050,C98-1013,0,\N,Missing
P13-1035,E12-1065,0,0.176881,"osophers and dramatists have long argued whether the most important element of narrative is plot or character. Under a classical Aristotelian perspective, plot is supreme;1 modern theoretical dramatists and screenwriters disagree.2 Without addressing this debate directly, much computational work on narrative has focused on learning the sequence of events by which a story is defined; in this tradition we might situate seminal work on learning procedural scripts (Schank and Abelson, 1977; Regneri et al., 2010), narrative chains (Chambers and Jurafsky, 2008), and plot structure (Finlayson, 2011; Elsner, 2012; McIntyre and Lapata, 2010; Goyal et al., 2010). We present a complementary perspective that addresses the importance of character in defining 1 “Dramatic action . . . is not with a view to the representation of character: character comes in as subsidiary to the actions . . . The Plot, then, is the first principle, and, as it were, the soul of a tragedy: Character holds the second place.” Poetics I.VI (Aristotle, 335 BCE). 2 “Aristotle was mistaken in his time, and our scholars are mistaken today when they accept his rulings concerning character. Character was a great factor in Aristotle’s ti"
P13-1035,D10-1008,0,0.0305925,"Missing"
P13-1035,W06-1601,0,0.013252,"act with the world and with each other (runs but just misses the train, spills coffee on her boss) – through which they reveal to the viewer those inherent qualities about themselves. This work is inspired by past approaches that infer typed semantic arguments along with narrative schemas (Chambers and Jurafsky, 2009; Regneri et al., 2011), but seeks a more holistic view of character, one that learns from stereotypical attributes in addition to plot events. This work also naturally draws on earlier work on the unsupervised learning of verbal arguments and semantic roles (Pereira et al., 1993; Grenager and Manning, 2006; Titov and Klementiev, 2012) and unsupervised relation discovery (Yao et al., 2011). This character-centric perspective leads to two natural questions. First, can we learn what those standard personas are by how individual characters (who instantiate those types) are portrayed? Second, can we learn the set of attributes and actions by which we recognize those common types? How do we, as viewers, recognize a V ILLIAN? At its most extreme, this perspective reduces to learning the grand archetypes of Joseph Campbell (1949) or Carl Jung (1981), such as the H ERO or T RICKSTER. We seek, however, a"
P13-1035,P10-1158,0,0.174513,"ramatists have long argued whether the most important element of narrative is plot or character. Under a classical Aristotelian perspective, plot is supreme;1 modern theoretical dramatists and screenwriters disagree.2 Without addressing this debate directly, much computational work on narrative has focused on learning the sequence of events by which a story is defined; in this tradition we might situate seminal work on learning procedural scripts (Schank and Abelson, 1977; Regneri et al., 2010), narrative chains (Chambers and Jurafsky, 2008), and plot structure (Finlayson, 2011; Elsner, 2012; McIntyre and Lapata, 2010; Goyal et al., 2010). We present a complementary perspective that addresses the importance of character in defining 1 “Dramatic action . . . is not with a view to the representation of character: character comes in as subsidiary to the actions . . . The Plot, then, is the first principle, and, as it were, the soul of a tragedy: Character holds the second place.” Poetics I.VI (Aristotle, 335 BCE). 2 “Aristotle was mistaken in his time, and our scholars are mistaken today when they accept his rulings concerning character. Character was a great factor in Aristotle’s time, and no fine play ever w"
P13-1035,P08-1090,0,0.0598036,"testbed to help drive future work in this area. 1 Introduction Philosophers and dramatists have long argued whether the most important element of narrative is plot or character. Under a classical Aristotelian perspective, plot is supreme;1 modern theoretical dramatists and screenwriters disagree.2 Without addressing this debate directly, much computational work on narrative has focused on learning the sequence of events by which a story is defined; in this tradition we might situate seminal work on learning procedural scripts (Schank and Abelson, 1977; Regneri et al., 2010), narrative chains (Chambers and Jurafsky, 2008), and plot structure (Finlayson, 2011; Elsner, 2012; McIntyre and Lapata, 2010; Goyal et al., 2010). We present a complementary perspective that addresses the importance of character in defining 1 “Dramatic action . . . is not with a view to the representation of character: character comes in as subsidiary to the actions . . . The Plot, then, is the first principle, and, as it were, the soul of a tragedy: Character holds the second place.” Poetics I.VI (Aristotle, 335 BCE). 2 “Aristotle was mistaken in his time, and our scholars are mistaken today when they accept his rulings concerning charac"
P13-1035,P09-1068,0,0.163591,"aracter types, or personas, we want to see involved (the PROTAGONIST, the LOVE INTER EST , the BEST FRIEND ). After picking this set, we fill out each of these roles with specific attributes (female, 28 years old, klutzy); with this cast of characters, we then sketch out the set of events by which they interact with the world and with each other (runs but just misses the train, spills coffee on her boss) – through which they reveal to the viewer those inherent qualities about themselves. This work is inspired by past approaches that infer typed semantic arguments along with narrative schemas (Chambers and Jurafsky, 2009; Regneri et al., 2011), but seeks a more holistic view of character, one that learns from stereotypical attributes in addition to plot events. This work also naturally draws on earlier work on the unsupervised learning of verbal arguments and semantic roles (Pereira et al., 1993; Grenager and Manning, 2006; Titov and Klementiev, 2012) and unsupervised relation discovery (Yao et al., 2011). This character-centric perspective leads to two natural questions. First, can we learn what those standard personas are by how individual characters (who instantiate those types) are portrayed? Second, can"
P13-1035,P93-1024,0,0.0692685,"ts by which they interact with the world and with each other (runs but just misses the train, spills coffee on her boss) – through which they reveal to the viewer those inherent qualities about themselves. This work is inspired by past approaches that infer typed semantic arguments along with narrative schemas (Chambers and Jurafsky, 2009; Regneri et al., 2011), but seeks a more holistic view of character, one that learns from stereotypical attributes in addition to plot events. This work also naturally draws on earlier work on the unsupervised learning of verbal arguments and semantic roles (Pereira et al., 1993; Grenager and Manning, 2006; Titov and Klementiev, 2012) and unsupervised relation discovery (Yao et al., 2011). This character-centric perspective leads to two natural questions. First, can we learn what those standard personas are by how individual characters (who instantiate those types) are portrayed? Second, can we learn the set of attributes and actions by which we recognize those common types? How do we, as viewers, recognize a V ILLIAN? At its most extreme, this perspective reduces to learning the grand archetypes of Joseph Campbell (1949) or Carl Jung (1981), such as the H ERO or T R"
P13-1035,E12-1003,0,0.0234302,"each other (runs but just misses the train, spills coffee on her boss) – through which they reveal to the viewer those inherent qualities about themselves. This work is inspired by past approaches that infer typed semantic arguments along with narrative schemas (Chambers and Jurafsky, 2009; Regneri et al., 2011), but seeks a more holistic view of character, one that learns from stereotypical attributes in addition to plot events. This work also naturally draws on earlier work on the unsupervised learning of verbal arguments and semantic roles (Pereira et al., 1993; Grenager and Manning, 2006; Titov and Klementiev, 2012) and unsupervised relation discovery (Yao et al., 2011). This character-centric perspective leads to two natural questions. First, can we learn what those standard personas are by how individual characters (who instantiate those types) are portrayed? Second, can we learn the set of attributes and actions by which we recognize those common types? How do we, as viewers, recognize a V ILLIAN? At its most extreme, this perspective reduces to learning the grand archetypes of Joseph Campbell (1949) or Carl Jung (1981), such as the H ERO or T RICKSTER. We seek, however, a more finegrained set that in"
P13-1035,P10-1100,0,0.0366039,"analysis of film, along with a benchmark testbed to help drive future work in this area. 1 Introduction Philosophers and dramatists have long argued whether the most important element of narrative is plot or character. Under a classical Aristotelian perspective, plot is supreme;1 modern theoretical dramatists and screenwriters disagree.2 Without addressing this debate directly, much computational work on narrative has focused on learning the sequence of events by which a story is defined; in this tradition we might situate seminal work on learning procedural scripts (Schank and Abelson, 1977; Regneri et al., 2010), narrative chains (Chambers and Jurafsky, 2008), and plot structure (Finlayson, 2011; Elsner, 2012; McIntyre and Lapata, 2010; Goyal et al., 2010). We present a complementary perspective that addresses the importance of character in defining 1 “Dramatic action . . . is not with a view to the representation of character: character comes in as subsidiary to the actions . . . The Plot, then, is the first principle, and, as it were, the soul of a tragedy: Character holds the second place.” Poetics I.VI (Aristotle, 335 BCE). 2 “Aristotle was mistaken in his time, and our scholars are mistaken toda"
P13-1035,R11-1064,0,0.0150956,"e want to see involved (the PROTAGONIST, the LOVE INTER EST , the BEST FRIEND ). After picking this set, we fill out each of these roles with specific attributes (female, 28 years old, klutzy); with this cast of characters, we then sketch out the set of events by which they interact with the world and with each other (runs but just misses the train, spills coffee on her boss) – through which they reveal to the viewer those inherent qualities about themselves. This work is inspired by past approaches that infer typed semantic arguments along with narrative schemas (Chambers and Jurafsky, 2009; Regneri et al., 2011), but seeks a more holistic view of character, one that learns from stereotypical attributes in addition to plot events. This work also naturally draws on earlier work on the unsupervised learning of verbal arguments and semantic roles (Pereira et al., 1993; Grenager and Manning, 2006; Titov and Klementiev, 2012) and unsupervised relation discovery (Yao et al., 2011). This character-centric perspective leads to two natural questions. First, can we learn what those standard personas are by how individual characters (who instantiate those types) are portrayed? Second, can we learn the set of att"
P13-1035,D11-1135,0,0.0148004,"boss) – through which they reveal to the viewer those inherent qualities about themselves. This work is inspired by past approaches that infer typed semantic arguments along with narrative schemas (Chambers and Jurafsky, 2009; Regneri et al., 2011), but seeks a more holistic view of character, one that learns from stereotypical attributes in addition to plot events. This work also naturally draws on earlier work on the unsupervised learning of verbal arguments and semantic roles (Pereira et al., 1993; Grenager and Manning, 2006; Titov and Klementiev, 2012) and unsupervised relation discovery (Yao et al., 2011). This character-centric perspective leads to two natural questions. First, can we learn what those standard personas are by how individual characters (who instantiate those types) are portrayed? Second, can we learn the set of attributes and actions by which we recognize those common types? How do we, as viewers, recognize a V ILLIAN? At its most extreme, this perspective reduces to learning the grand archetypes of Joseph Campbell (1949) or Carl Jung (1981), such as the H ERO or T RICKSTER. We seek, however, a more finegrained set that includes not only archetypes, but stereotypes as well – c"
P13-1108,P09-1068,0,0.0250564,"on within natural language processing in part due to governmentfunded challenges such as MUC-3 and MUC-4 (Lehnert, 1994), which focused on the extraction of terrorist events, as well as the more recent ACE program. The work in this paper is inspired by unsupervised approaches that seek to discover types of relations and events, instead of assuming them to be pre-specified; this includes research under various headings such as template/frame/event learning (Cheung et al., 2013; Modi et al., 2012; Chambers and Jurafsky, 2011; Li et al., 2010; Bejan, 2008), script learning (Regneri et al., 2010; Chambers and Jurafsky, 2009), relation learning (Yao et al., 2011), open information extraction (Banko et al., 2007; Carlson et al., 2010), verb caseframe learning (Rooth et al., 1999; Gildea, 2002; Grenager and Manning, 2006; Lang and La´ S´eaghdha, 2010; Titov and Klemenpata, 2010; O tiev, 2012), and a version of frame learning called “unsupervised semantic parsing” (Titov and Klementiev, 2011; Poon and Domingos, 2009). Unlike much of the previous literature, we do not learn latent roles/slots. Event extraction is also a large literature, including supervised systems targeting problems similar to MUC and political even"
P13-1108,P11-1098,0,0.0231441,"ral Language Processing Political event extraction from news has also received considerable attention within natural language processing in part due to governmentfunded challenges such as MUC-3 and MUC-4 (Lehnert, 1994), which focused on the extraction of terrorist events, as well as the more recent ACE program. The work in this paper is inspired by unsupervised approaches that seek to discover types of relations and events, instead of assuming them to be pre-specified; this includes research under various headings such as template/frame/event learning (Cheung et al., 2013; Modi et al., 2012; Chambers and Jurafsky, 2011; Li et al., 2010; Bejan, 2008), script learning (Regneri et al., 2010; Chambers and Jurafsky, 2009), relation learning (Yao et al., 2011), open information extraction (Banko et al., 2007; Carlson et al., 2010), verb caseframe learning (Rooth et al., 1999; Gildea, 2002; Grenager and Manning, 2006; Lang and La´ S´eaghdha, 2010; Titov and Klemenpata, 2010; O tiev, 2012), and a version of frame learning called “unsupervised semantic parsing” (Titov and Klementiev, 2011; Poon and Domingos, 2009). Unlike much of the previous literature, we do not learn latent roles/slots. Event extraction is also a"
P13-1108,N13-1104,0,0.0175968,"terns in future work. 6.2 Events in Natural Language Processing Political event extraction from news has also received considerable attention within natural language processing in part due to governmentfunded challenges such as MUC-3 and MUC-4 (Lehnert, 1994), which focused on the extraction of terrorist events, as well as the more recent ACE program. The work in this paper is inspired by unsupervised approaches that seek to discover types of relations and events, instead of assuming them to be pre-specified; this includes research under various headings such as template/frame/event learning (Cheung et al., 2013; Modi et al., 2012; Chambers and Jurafsky, 2011; Li et al., 2010; Bejan, 2008), script learning (Regneri et al., 2010; Chambers and Jurafsky, 2009), relation learning (Yao et al., 2011), open information extraction (Banko et al., 2007; Carlson et al., 2010), verb caseframe learning (Rooth et al., 1999; Gildea, 2002; Grenager and Manning, 2006; Lang and La´ S´eaghdha, 2010; Titov and Klemenpata, 2010; O tiev, 2012), and a version of frame learning called “unsupervised semantic parsing” (Titov and Klementiev, 2011; Poon and Domingos, 2009). Unlike much of the previous literature, we do not lear"
P13-1108,D10-1124,1,0.208531,"Missing"
P13-1108,C02-1132,0,0.0409121,"the more recent ACE program. The work in this paper is inspired by unsupervised approaches that seek to discover types of relations and events, instead of assuming them to be pre-specified; this includes research under various headings such as template/frame/event learning (Cheung et al., 2013; Modi et al., 2012; Chambers and Jurafsky, 2011; Li et al., 2010; Bejan, 2008), script learning (Regneri et al., 2010; Chambers and Jurafsky, 2009), relation learning (Yao et al., 2011), open information extraction (Banko et al., 2007; Carlson et al., 2010), verb caseframe learning (Rooth et al., 1999; Gildea, 2002; Grenager and Manning, 2006; Lang and La´ S´eaghdha, 2010; Titov and Klemenpata, 2010; O tiev, 2012), and a version of frame learning called “unsupervised semantic parsing” (Titov and Klementiev, 2011; Poon and Domingos, 2009). Unlike much of the previous literature, we do not learn latent roles/slots. Event extraction is also a large literature, including supervised systems targeting problems similar to MUC and political events (Piskorski and Atkinson, 2011; Piskorski et al., 2011; Sanfilippo et al., 2008). One can also see this work as a relational ex14 http://eventdata.psu.edu/data.dir/ GD"
P13-1108,W06-1601,0,0.0337098,"nt ACE program. The work in this paper is inspired by unsupervised approaches that seek to discover types of relations and events, instead of assuming them to be pre-specified; this includes research under various headings such as template/frame/event learning (Cheung et al., 2013; Modi et al., 2012; Chambers and Jurafsky, 2011; Li et al., 2010; Bejan, 2008), script learning (Regneri et al., 2010; Chambers and Jurafsky, 2009), relation learning (Yao et al., 2011), open information extraction (Banko et al., 2007; Carlson et al., 2010), verb caseframe learning (Rooth et al., 1999; Gildea, 2002; Grenager and Manning, 2006; Lang and La´ S´eaghdha, 2010; Titov and Klemenpata, 2010; O tiev, 2012), and a version of frame learning called “unsupervised semantic parsing” (Titov and Klementiev, 2011; Poon and Domingos, 2009). Unlike much of the previous literature, we do not learn latent roles/slots. Event extraction is also a large literature, including supervised systems targeting problems similar to MUC and political events (Piskorski and Atkinson, 2011; Piskorski et al., 2011; Sanfilippo et al., 2008). One can also see this work as a relational ex14 http://eventdata.psu.edu/data.dir/ GDELT.html tension of co-occur"
P13-1108,N10-1137,0,0.0387823,"Missing"
P13-1108,Y10-1027,0,0.0271324,"ical event extraction from news has also received considerable attention within natural language processing in part due to governmentfunded challenges such as MUC-3 and MUC-4 (Lehnert, 1994), which focused on the extraction of terrorist events, as well as the more recent ACE program. The work in this paper is inspired by unsupervised approaches that seek to discover types of relations and events, instead of assuming them to be pre-specified; this includes research under various headings such as template/frame/event learning (Cheung et al., 2013; Modi et al., 2012; Chambers and Jurafsky, 2011; Li et al., 2010; Bejan, 2008), script learning (Regneri et al., 2010; Chambers and Jurafsky, 2009), relation learning (Yao et al., 2011), open information extraction (Banko et al., 2007; Carlson et al., 2010), verb caseframe learning (Rooth et al., 1999; Gildea, 2002; Grenager and Manning, 2006; Lang and La´ S´eaghdha, 2010; Titov and Klemenpata, 2010; O tiev, 2012), and a version of frame learning called “unsupervised semantic parsing” (Titov and Klementiev, 2011; Poon and Domingos, 2009). Unlike much of the previous literature, we do not learn latent roles/slots. Event extraction is also a large literature"
P13-1108,W12-1901,0,0.010358,"6.2 Events in Natural Language Processing Political event extraction from news has also received considerable attention within natural language processing in part due to governmentfunded challenges such as MUC-3 and MUC-4 (Lehnert, 1994), which focused on the extraction of terrorist events, as well as the more recent ACE program. The work in this paper is inspired by unsupervised approaches that seek to discover types of relations and events, instead of assuming them to be pre-specified; this includes research under various headings such as template/frame/event learning (Cheung et al., 2013; Modi et al., 2012; Chambers and Jurafsky, 2011; Li et al., 2010; Bejan, 2008), script learning (Regneri et al., 2010; Chambers and Jurafsky, 2009), relation learning (Yao et al., 2011), open information extraction (Banko et al., 2007; Carlson et al., 2010), verb caseframe learning (Rooth et al., 1999; Gildea, 2002; Grenager and Manning, 2006; Lang and La´ S´eaghdha, 2010; Titov and Klemenpata, 2010; O tiev, 2012), and a version of frame learning called “unsupervised semantic parsing” (Titov and Klementiev, 2011; Poon and Domingos, 2009). Unlike much of the previous literature, we do not learn latent roles/slot"
P13-1108,P10-1045,0,0.0166366,"Missing"
P13-1108,D09-1001,0,0.0313609,"nder various headings such as template/frame/event learning (Cheung et al., 2013; Modi et al., 2012; Chambers and Jurafsky, 2011; Li et al., 2010; Bejan, 2008), script learning (Regneri et al., 2010; Chambers and Jurafsky, 2009), relation learning (Yao et al., 2011), open information extraction (Banko et al., 2007; Carlson et al., 2010), verb caseframe learning (Rooth et al., 1999; Gildea, 2002; Grenager and Manning, 2006; Lang and La´ S´eaghdha, 2010; Titov and Klemenpata, 2010; O tiev, 2012), and a version of frame learning called “unsupervised semantic parsing” (Titov and Klementiev, 2011; Poon and Domingos, 2009). Unlike much of the previous literature, we do not learn latent roles/slots. Event extraction is also a large literature, including supervised systems targeting problems similar to MUC and political events (Piskorski and Atkinson, 2011; Piskorski et al., 2011; Sanfilippo et al., 2008). One can also see this work as a relational ex14 http://eventdata.psu.edu/data.dir/ GDELT.html tension of co-occurence-based methods such as Gerrish (2013; ch. 4), Diesner and Carley (2005), Chang et al. (2009), or Newman et al. (2006), which perform bag-of-words-style analysis of text fragments containing co-oc"
P13-1108,P10-1100,0,0.00619,"d considerable attention within natural language processing in part due to governmentfunded challenges such as MUC-3 and MUC-4 (Lehnert, 1994), which focused on the extraction of terrorist events, as well as the more recent ACE program. The work in this paper is inspired by unsupervised approaches that seek to discover types of relations and events, instead of assuming them to be pre-specified; this includes research under various headings such as template/frame/event learning (Cheung et al., 2013; Modi et al., 2012; Chambers and Jurafsky, 2011; Li et al., 2010; Bejan, 2008), script learning (Regneri et al., 2010; Chambers and Jurafsky, 2009), relation learning (Yao et al., 2011), open information extraction (Banko et al., 2007; Carlson et al., 2010), verb caseframe learning (Rooth et al., 1999; Gildea, 2002; Grenager and Manning, 2006; Lang and La´ S´eaghdha, 2010; Titov and Klemenpata, 2010; O tiev, 2012), and a version of frame learning called “unsupervised semantic parsing” (Titov and Klementiev, 2011; Poon and Domingos, 2009). Unlike much of the previous literature, we do not learn latent roles/slots. Event extraction is also a large literature, including supervised systems targeting problems sim"
P13-1108,P99-1014,0,0.0186878,"t events, as well as the more recent ACE program. The work in this paper is inspired by unsupervised approaches that seek to discover types of relations and events, instead of assuming them to be pre-specified; this includes research under various headings such as template/frame/event learning (Cheung et al., 2013; Modi et al., 2012; Chambers and Jurafsky, 2011; Li et al., 2010; Bejan, 2008), script learning (Regneri et al., 2010; Chambers and Jurafsky, 2009), relation learning (Yao et al., 2011), open information extraction (Banko et al., 2007; Carlson et al., 2010), verb caseframe learning (Rooth et al., 1999; Gildea, 2002; Grenager and Manning, 2006; Lang and La´ S´eaghdha, 2010; Titov and Klemenpata, 2010; O tiev, 2012), and a version of frame learning called “unsupervised semantic parsing” (Titov and Klementiev, 2011; Poon and Domingos, 2009). Unlike much of the previous literature, we do not learn latent roles/slots. Event extraction is also a large literature, including supervised systems targeting problems similar to MUC and political events (Piskorski and Atkinson, 2011; Piskorski et al., 2011; Sanfilippo et al., 2008). One can also see this work as a relational ex14 http://eventdata.psu.ed"
P13-1108,P11-1145,0,0.0143218,"ed; this includes research under various headings such as template/frame/event learning (Cheung et al., 2013; Modi et al., 2012; Chambers and Jurafsky, 2011; Li et al., 2010; Bejan, 2008), script learning (Regneri et al., 2010; Chambers and Jurafsky, 2009), relation learning (Yao et al., 2011), open information extraction (Banko et al., 2007; Carlson et al., 2010), verb caseframe learning (Rooth et al., 1999; Gildea, 2002; Grenager and Manning, 2006; Lang and La´ S´eaghdha, 2010; Titov and Klemenpata, 2010; O tiev, 2012), and a version of frame learning called “unsupervised semantic parsing” (Titov and Klementiev, 2011; Poon and Domingos, 2009). Unlike much of the previous literature, we do not learn latent roles/slots. Event extraction is also a large literature, including supervised systems targeting problems similar to MUC and political events (Piskorski and Atkinson, 2011; Piskorski et al., 2011; Sanfilippo et al., 2008). One can also see this work as a relational ex14 http://eventdata.psu.edu/data.dir/ GDELT.html tension of co-occurence-based methods such as Gerrish (2013; ch. 4), Diesner and Carley (2005), Chang et al. (2009), or Newman et al. (2006), which perform bag-of-words-style analysis of text"
P13-1108,E12-1003,0,0.0315777,"Missing"
P13-1108,D11-1135,0,0.0246765,"Missing"
P13-2109,C96-1058,0,0.508512,"1 Introduction • We apply the third-order feature models of Koo and Collins (2010) to non-projective parsing. Dependency parsing has become a prominent approach to syntax in the last few years, with increasingly fast and accurate models being devised (K¨ubler et al., 2009; Huang and Sagae, 2010; Zhang and Nivre, 2011; Rush and Petrov, 2012). In projective parsing, the arcs in the dependency tree are constrained to be nested, and the problem of finding the best tree can be addressed with dynamic programming. This results in cubic-time decoders for arc-factored and sibling second-order models (Eisner, 1996; McDonald and Pereira, 2006), and quartic-time for grandparent models (Carreras, 2007) and third-order models (Koo and Collins, 2010). Recently, Rush and Petrov (2012) trained third-order parsers with vine pruning cascades, achieving runtimes only a small factor slower than first-order systems. Third-order features have also been included in transition systems (Zhang and Nivre, 2011) and graph-based parsers with cube-pruning (Zhang and McDonald, 2012). Unfortunately, non-projective dependency parsers (appropriate for languages with a more flexible word order, such as Czech, Dutch, and German)"
P13-2109,W06-2933,0,0.0918345,"Missing"
P13-2109,P10-1110,0,0.0539693,"proach uses AD3 , an accelerated dual decomposition algorithm which we extend to handle specialized head automata and sequential head bigram models. Experiments in fourteen languages yield parsing speeds competitive to projective parsers, with state-ofthe-art accuracies for the largest datasets (English, Czech, and German). 1 Introduction • We apply the third-order feature models of Koo and Collins (2010) to non-projective parsing. Dependency parsing has become a prominent approach to syntax in the last few years, with increasingly fast and accurate models being devised (K¨ubler et al., 2009; Huang and Sagae, 2010; Zhang and Nivre, 2011; Rush and Petrov, 2012). In projective parsing, the arcs in the dependency tree are constrained to be nested, and the problem of finding the best tree can be addressed with dynamic programming. This results in cubic-time decoders for arc-factored and sibling second-order models (Eisner, 1996; McDonald and Pereira, 2006), and quartic-time for grandparent models (Carreras, 2007) and third-order models (Koo and Collins, 2010). Recently, Rush and Petrov (2012) trained third-order parsers with vine pruning cascades, achieving runtimes only a small factor slower than first-or"
P13-2109,N12-1054,0,0.683007,"ition algorithm which we extend to handle specialized head automata and sequential head bigram models. Experiments in fourteen languages yield parsing speeds competitive to projective parsers, with state-ofthe-art accuracies for the largest datasets (English, Czech, and German). 1 Introduction • We apply the third-order feature models of Koo and Collins (2010) to non-projective parsing. Dependency parsing has become a prominent approach to syntax in the last few years, with increasingly fast and accurate models being devised (K¨ubler et al., 2009; Huang and Sagae, 2010; Zhang and Nivre, 2011; Rush and Petrov, 2012). In projective parsing, the arcs in the dependency tree are constrained to be nested, and the problem of finding the best tree can be addressed with dynamic programming. This results in cubic-time decoders for arc-factored and sibling second-order models (Eisner, 1996; McDonald and Pereira, 2006), and quartic-time for grandparent models (Carreras, 2007) and third-order models (Koo and Collins, 2010). Recently, Rush and Petrov (2012) trained third-order parsers with vine pruning cascades, achieving runtimes only a small factor slower than first-order systems. Third-order features have also bee"
P13-2109,P10-1001,0,0.831259,"ll behind those of projective parsers. This paper bridges the gap above by presenting the following contributions: We present fast, accurate, direct nonprojective dependency parsers with thirdorder features. Our approach uses AD3 , an accelerated dual decomposition algorithm which we extend to handle specialized head automata and sequential head bigram models. Experiments in fourteen languages yield parsing speeds competitive to projective parsers, with state-ofthe-art accuracies for the largest datasets (English, Czech, and German). 1 Introduction • We apply the third-order feature models of Koo and Collins (2010) to non-projective parsing. Dependency parsing has become a prominent approach to syntax in the last few years, with increasingly fast and accurate models being devised (K¨ubler et al., 2009; Huang and Sagae, 2010; Zhang and Nivre, 2011; Rush and Petrov, 2012). In projective parsing, the arcs in the dependency tree are constrained to be nested, and the problem of finding the best tree can be addressed with dynamic programming. This results in cubic-time decoders for arc-factored and sibling second-order models (Eisner, 1996; McDonald and Pereira, 2006), and quartic-time for grandparent models"
P13-2109,D10-1001,0,0.0581119,"s et al. (2011), the problem of obtaining the best-scored tree can be written as follows: PS maximize s=1 fs (z s ) Figure 1: Parts considered in this paper. Firstorder models factor over arcs (Eisner, 1996; McDonald et al., 2005), and second-order models include also consecutive siblings and grandparents (Carreras, 2007). Our parsers add also arbitrary siblings (not necessarily consecutive) and head bigrams, as in Martins et al. (2011), in addition to third-order features for grand- and tri-siblings (Koo and Collins, 2010). problems in a modular and extensible manner (Komodakis et al., 2007; Rush et al., 2010). In this paper, we employ alternating directions dual decomposition (AD3 ; Martins et al., 2011). Like the subgradient algorithm of Rush et al. (2010), AD3 splits the original problem into local subproblems, and seeks an agreement on the overlapping variables. The difference is that the AD3 subproblems have an additional quadratic term to accelerate consensus. Recent analysis (Martins et al., 2012) has shown that: (i) AD3 converges at a faster rate,2 and (ii) the quadratic subproblems can be solved using the same combinatorial machinery that is used in the subgradient algorithm. This opens th"
P13-2109,D10-1125,0,0.826073,"ng is NP-hard beyond arc-factored models (McDonald • This extension is non-trivial since exact dynamic programming is not applicable. Instead, we adapt AD3 , the dual decomposition algorithm proposed by Martins et al. (2011), to handle third-order features, by introducing specialized head automata. • We make our parser substantially faster than the many-components approach of Martins et al. (2011). While AD3 requires solving quadratic subproblems as an intermediate step, recent results (Martins et al., 2012) show that they can be addressed with the same oracles used in the subgradient method (Koo et al., 2010). This enables AD3 to exploit combinatorial subproblems like the the head automata above. Along with this paper, we provide a free distribution of our parsers, including training code.1 2 Dependency Parsing with AD3 Dual decomposition is a class of optimization techniques that tackle the dual of combinatorial 1 Released as TurboParser 2.1, and publicly available at http://www.ark.cs.cmu.edu/TurboParser. 617 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 617–622, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics fo"
P13-2109,D08-1016,0,0.138862,"/TurboParser. 617 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 617–622, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics formed trees. Let {As }Ss=1 be a cover of A, where each As ⊆ A. We assume that the score PS of a parse tree u ∈ Y decomposes as f (u) := s=1 fs (z s ), where each z s := hzs,a ia∈As is a “partial view” of u, and each local score function fs comes from a feature-based linear model. Past work in dependency parsing considered either (i) a few “large” components, such as trees and head automata (Smith and Eisner, 2008; Koo et al., 2010), or (ii) many “small” components, coming from a multi-commodity flow formulation (Martins et al., 2009, 2011). Let Ys ⊆ R|As |denote the set of feasible realizations of z s , i.e., those that are partial views of an actual Q parse tree. A tuple of views hz 1 , . . . , z S i ∈ Ss=1 Ys is said to be globally consistent if zs,a = zs0 ,a holds for every a, s and s0 such that a ∈ As ∩As0 . We assume each parse u ∈ Y corresponds uniquely to a globally consistent tuple of views, and vice-versa. Following Martins et al. (2011), the problem of obtaining the best-scored tree can be w"
P13-2109,P09-1039,1,0.954383,"Fast Third-Order Non-Projective Turbo Parsers Andr´e F. T. Martins∗† Miguel B. Almeida∗† Noah A. Smith# Priberam Labs, Alameda D. Afonso Henriques, 41, 2o , 1000-123 Lisboa, Portugal Instituto de Telecomunicac¸o˜ es, Instituto Superior T´ecnico, 1049-001 Lisboa, Portugal # School of Computer Science, Carnegie Mellon University, Pittsburgh, PA 15213, USA {atm,mba}@priberam.pt, nasmith@cs.cmu.edu ∗ † Abstract and Satta, 2007). Approximate parsers have therefore been introduced, based on belief propagation (Smith and Eisner, 2008), dual decomposition (Koo et al., 2010), or multi-commodity flows (Martins et al., 2009, 2011). These are all instances of turbo parsers, as shown by Martins et al. (2010): the underlying approximations come from the fact that they run global inference in factor graphs ignoring loop effects. While this line of research has led to accuracy gains, none of these parsers use third-order contexts, and their speeds are well behind those of projective parsers. This paper bridges the gap above by presenting the following contributions: We present fast, accurate, direct nonprojective dependency parsers with thirdorder features. Our approach uses AD3 , an accelerated dual decomposition al"
P13-2109,W08-2121,0,0.235101,"Missing"
P13-2109,D10-1004,1,0.919751,"ida∗† Noah A. Smith# Priberam Labs, Alameda D. Afonso Henriques, 41, 2o , 1000-123 Lisboa, Portugal Instituto de Telecomunicac¸o˜ es, Instituto Superior T´ecnico, 1049-001 Lisboa, Portugal # School of Computer Science, Carnegie Mellon University, Pittsburgh, PA 15213, USA {atm,mba}@priberam.pt, nasmith@cs.cmu.edu ∗ † Abstract and Satta, 2007). Approximate parsers have therefore been introduced, based on belief propagation (Smith and Eisner, 2008), dual decomposition (Koo et al., 2010), or multi-commodity flows (Martins et al., 2009, 2011). These are all instances of turbo parsers, as shown by Martins et al. (2010): the underlying approximations come from the fact that they run global inference in factor graphs ignoring loop effects. While this line of research has led to accuracy gains, none of these parsers use third-order contexts, and their speeds are well behind those of projective parsers. This paper bridges the gap above by presenting the following contributions: We present fast, accurate, direct nonprojective dependency parsers with thirdorder features. Our approach uses AD3 , an accelerated dual decomposition algorithm which we extend to handle specialized head automata and sequential head bigr"
P13-2109,W03-3023,0,0.510551,"Missing"
P13-2109,D11-1022,1,0.264644,"der systems. Third-order features have also been included in transition systems (Zhang and Nivre, 2011) and graph-based parsers with cube-pruning (Zhang and McDonald, 2012). Unfortunately, non-projective dependency parsers (appropriate for languages with a more flexible word order, such as Czech, Dutch, and German) lag behind these recent advances. The main obstacle is that non-projective parsing is NP-hard beyond arc-factored models (McDonald • This extension is non-trivial since exact dynamic programming is not applicable. Instead, we adapt AD3 , the dual decomposition algorithm proposed by Martins et al. (2011), to handle third-order features, by introducing specialized head automata. • We make our parser substantially faster than the many-components approach of Martins et al. (2011). While AD3 requires solving quadratic subproblems as an intermediate step, recent results (Martins et al., 2012) show that they can be addressed with the same oracles used in the subgradient method (Koo et al., 2010). This enables AD3 to exploit combinatorial subproblems like the the head automata above. Along with this paper, we provide a free distribution of our parsers, including training code.1 2 Dependency Parsing"
P13-2109,D12-1030,0,0.831572,"em of finding the best tree can be addressed with dynamic programming. This results in cubic-time decoders for arc-factored and sibling second-order models (Eisner, 1996; McDonald and Pereira, 2006), and quartic-time for grandparent models (Carreras, 2007) and third-order models (Koo and Collins, 2010). Recently, Rush and Petrov (2012) trained third-order parsers with vine pruning cascades, achieving runtimes only a small factor slower than first-order systems. Third-order features have also been included in transition systems (Zhang and Nivre, 2011) and graph-based parsers with cube-pruning (Zhang and McDonald, 2012). Unfortunately, non-projective dependency parsers (appropriate for languages with a more flexible word order, such as Czech, Dutch, and German) lag behind these recent advances. The main obstacle is that non-projective parsing is NP-hard beyond arc-factored models (McDonald • This extension is non-trivial since exact dynamic programming is not applicable. Instead, we adapt AD3 , the dual decomposition algorithm proposed by Martins et al. (2011), to handle third-order features, by introducing specialized head automata. • We make our parser substantially faster than the many-components approach"
P13-2109,P11-2033,0,0.121241,"celerated dual decomposition algorithm which we extend to handle specialized head automata and sequential head bigram models. Experiments in fourteen languages yield parsing speeds competitive to projective parsers, with state-ofthe-art accuracies for the largest datasets (English, Czech, and German). 1 Introduction • We apply the third-order feature models of Koo and Collins (2010) to non-projective parsing. Dependency parsing has become a prominent approach to syntax in the last few years, with increasingly fast and accurate models being devised (K¨ubler et al., 2009; Huang and Sagae, 2010; Zhang and Nivre, 2011; Rush and Petrov, 2012). In projective parsing, the arcs in the dependency tree are constrained to be nested, and the problem of finding the best tree can be addressed with dynamic programming. This results in cubic-time decoders for arc-factored and sibling second-order models (Eisner, 1996; McDonald and Pereira, 2006), and quartic-time for grandparent models (Carreras, 2007) and third-order models (Koo and Collins, 2010). Recently, Rush and Petrov (2012) trained third-order parsers with vine pruning cascades, achieving runtimes only a small factor slower than first-order systems. Third-orde"
P13-2109,E06-1011,0,0.644051,"n • We apply the third-order feature models of Koo and Collins (2010) to non-projective parsing. Dependency parsing has become a prominent approach to syntax in the last few years, with increasingly fast and accurate models being devised (K¨ubler et al., 2009; Huang and Sagae, 2010; Zhang and Nivre, 2011; Rush and Petrov, 2012). In projective parsing, the arcs in the dependency tree are constrained to be nested, and the problem of finding the best tree can be addressed with dynamic programming. This results in cubic-time decoders for arc-factored and sibling second-order models (Eisner, 1996; McDonald and Pereira, 2006), and quartic-time for grandparent models (Carreras, 2007) and third-order models (Koo and Collins, 2010). Recently, Rush and Petrov (2012) trained third-order parsers with vine pruning cascades, achieving runtimes only a small factor slower than first-order systems. Third-order features have also been included in transition systems (Zhang and Nivre, 2011) and graph-based parsers with cube-pruning (Zhang and McDonald, 2012). Unfortunately, non-projective dependency parsers (appropriate for languages with a more flexible word order, such as Czech, Dutch, and German) lag behind these recent adva"
P13-2109,W07-2216,0,0.17719,"Missing"
P13-2109,H05-1066,0,0.829343,"⊆ R|As |denote the set of feasible realizations of z s , i.e., those that are partial views of an actual Q parse tree. A tuple of views hz 1 , . . . , z S i ∈ Ss=1 Ys is said to be globally consistent if zs,a = zs0 ,a holds for every a, s and s0 such that a ∈ As ∩As0 . We assume each parse u ∈ Y corresponds uniquely to a globally consistent tuple of views, and vice-versa. Following Martins et al. (2011), the problem of obtaining the best-scored tree can be written as follows: PS maximize s=1 fs (z s ) Figure 1: Parts considered in this paper. Firstorder models factor over arcs (Eisner, 1996; McDonald et al., 2005), and second-order models include also consecutive siblings and grandparents (Carreras, 2007). Our parsers add also arbitrary siblings (not necessarily consecutive) and head bigrams, as in Martins et al. (2011), in addition to third-order features for grand- and tri-siblings (Koo and Collins, 2010). problems in a modular and extensible manner (Komodakis et al., 2007; Rush et al., 2010). In this paper, we employ alternating directions dual decomposition (AD3 ; Martins et al., 2011). Like the subgradient algorithm of Rush et al. (2010), AD3 splits the original problem into local subproblems, and"
P13-2109,W06-2932,0,0.0291159,"2 785 93.52 2,996 92.69 740 86.01 366 85.59 318 91.14 684 76.90 793 Best published UAS UAS Tok/sec 81.12 - Ma11 93.50 - Ma11 91.89 - Ma10 89.46 - Ma11 91.86 - Ma11 85.81 121 Ko10 91.89 - Ma11 92.68 - Ma11 93.72 - Ma11 93.03 79 Ko10 86.95 - Ma11 87.48 - ZM12 91.44 - ZM12 77.55 258 Ko10 RP12 UAS Tok/sec 91.9 3,980 90.9 7,800 90.8 2,880 92.3 8,600 91.5 2,900 90.1 5,320 - ZM12 UAS 93.08 91.35 93.24 91.69 87.48 91.44 - Table 3: Results for the CoNLL-2006 datasets and the non-projective English dataset of CoNLL-2008. “Best Published UAS” includes the most accurate parsers among Nivre et al. (2006), McDonald et al. (2006), Martins et al. (2010, 2011), Koo et al. (2010), Rush and Petrov (2012), Zhang and McDonald (2012). The last two are shown separately in the rightmost columns. work was partially supported by the EU/FEDER programme, QREN/POR Lisboa (Portugal), under the Intelligo project (contract 2012/24803), by a FCT grant PTDC/EEI-SII/2312/2012, and by NSF grant IIS-1054319. In our second experiment (Tab. 3), we used 14 datasets, most of which are non-projective, from the CoNLL 2006 and 2008 shared tasks (Buchholz and Marsi, 2006; Surdeanu et al., 2008). Our third-order model achieved the best reported sco"
P13-2109,W06-2920,0,\N,Missing
P13-2109,D07-1101,0,\N,Missing
P14-1035,P13-1035,1,0.460661,"Missing"
P14-1035,N04-1039,0,0.111087,"Bayesian approach in which the words we observe are generated conditional on a combination of different effects captured in a log-linear (or “maximum entropy”) distribution. Maximum entropy approaches to language modeling have been used since Rosenfeld (1996) to incorporate long-distance information, such as previously-mentioned trigger words, into n-gram language models. This work has since been extended to a Bayesian setting by applying both a Gaussian prior (Chen and Rosenfeld, 2000), which dampens the impact of any individual feature, and sparsity-inducing priors (Kazama and Tsujii, 2003; Goodman, 2004), which can drive many feature weights to 0. The latter have been applied specifically to the problem of estimating word probabilities with sparse additive generative (SAGE) models (Eisenstein et al., 2011), where sparse extra-linguistic effects can influence a word probability in a larger generative setting. In contrast to previous work in which the probability of a word linked to a character is dependent entirely on the character’s latent persona, in our model, we see the probability of a word as dependent on: (i) the background likelihood of the word, (ii) the author, so that a word becomes"
P14-1035,N10-1061,0,0.0321203,". Smith School of Computer Science Carnegie Mellon University Pittsburgh, PA 15213, USA nasmith@cs.cmu.edu Introduction Recent work in NLP has begun to exploit the potential of entity-centric modeling for a variety of tasks: Chambers (2013) places entities at the center of probabilistic frame induction, showing gains over a comparable event-centric model (Cheung et al., 2013); Bamman et al. (2013) explicitly learn character types (or “personas”) in a dataset of Wikipedia movie plot summaries; and entity-centric models form one dominant approach in coreference resolution (Durrett et al., 2013; Haghighi and Klein, 2010). One commonality among all of these very different probabilistic approaches is that each learns statistical regularities about how entities are depicted in text (whether for the sake of learning a set of semantic roles, character types, or linking anaphora to the entities to which they refer). In each case, the text we observe associated with an entity in a document is directly dependent on the class of entity—and only that class. This relationship between entity and text is a theoretical assumption, with important consequences for 2 Literary Background Inferring character is challenging from"
P14-1035,P14-1035,1,0.106949,"personas; two characters with a lower JS divergence are judged to be more similar than two characters with a higher one. As a Baseline, we also evaluate all hypotheses on a model with no latent variables whatsoever, which instead measures similarity as the average JS divergence between the empirical word distributions over each role type. Table 1 presents the results of this comparison; for all models with latent variables, we report the average of 5 sampling runs with different random initializations. Figure 4 provides a synAll 29 hypotheses can be found in a supplementary technical report (Bamman et al., 2014). We emphasize that the full set of hypotheses was locked before the model was estimated. 6 Experiments Part of the motivation of our mixed effects model is to be able to tackle hypothesis class C—by factoring out the influence of a particular author on the learning of personas, we would like to be able to discriminate between characters that all have a common authorial voice. In contrast, the Persona Regression model of Bamman et al. (2013), 376 P 250 100 50 25 10 Model Author/Persona Basic Persona Persona Reg. Author/Persona Basic Persona Persona Reg. Author/Persona Basic Persona Persona Reg"
P14-1035,W03-1018,0,0.0406548,"we adopt a hierarchical Bayesian approach in which the words we observe are generated conditional on a combination of different effects captured in a log-linear (or “maximum entropy”) distribution. Maximum entropy approaches to language modeling have been used since Rosenfeld (1996) to incorporate long-distance information, such as previously-mentioned trigger words, into n-gram language models. This work has since been extended to a Bayesian setting by applying both a Gaussian prior (Chen and Rosenfeld, 2000), which dampens the impact of any individual feature, and sparsity-inducing priors (Kazama and Tsujii, 2003; Goodman, 2004), which can drive many feature weights to 0. The latter have been applied specifically to the problem of estimating word probabilities with sparse additive generative (SAGE) models (Eisenstein et al., 2011), where sparse extra-linguistic effects can influence a word probability in a larger generative setting. In contrast to previous work in which the probability of a word linked to a character is dependent entirely on the character’s latent persona, in our model, we see the probability of a word as dependent on: (i) the background likelihood of the word, (ii) the author, so tha"
P14-1035,J92-4003,0,0.157219,"iscriminative classifier only on the task of resolving pronominal anaphora (i.e., ignoring generic noun phrases such as the paint or the rascal). For this task, we annotated a set of 832 coreference links in 3 books (Pride and Prejudice, The Turn of the Screw, and Heart of Darkness) and featurized coreference/antecedent pairs with: 4 All code is available at http://www.ark.cs.cmu. edu/literaryCharacter 5 Over all 15,099 narratives, the average number of character proper name mentions is 1,673; the average number of gendered singular pronouns (he, she, him, his, her) is 4,641. 6 In comparison, Brown et al. (1992) clusters learned from the same data capture syntactic similarity (placing functionally similar words in the same cluster). 372 1 01110011 → 0 1 0111001111: pair boots shoes gloves leather 0 0111001110: hat coat cap cloak handkerchief 1 0111001101: dress clothes wore worn wear 0 0111001100: dressed costume uniform clad clothed Figure 1: Bitstring representations of neural agglomerative clusters, illustrating the leaf nodes in a binary tree rooted in the prefix 01110011. Bitstring encodings of intermediate nodes and terminal leaves result by following the left (0) and right (1) branches of the"
P14-1035,D13-1185,0,0.0323315,"y generating all text associated with a character, we introduce a model that employs multiple effects to account for the influence of extra-linguistic information (such as author). In an empirical evaluation, we find that this method leads to improved agreement with the preregistered judgments of a literary scholar, complementing the results of alternative models. 1 Noah A. Smith School of Computer Science Carnegie Mellon University Pittsburgh, PA 15213, USA nasmith@cs.cmu.edu Introduction Recent work in NLP has begun to exploit the potential of entity-centric modeling for a variety of tasks: Chambers (2013) places entities at the center of probabilistic frame induction, showing gains over a comparable event-centric model (Cheung et al., 2013); Bamman et al. (2013) explicitly learn character types (or “personas”) in a dataset of Wikipedia movie plot summaries; and entity-centric models form one dominant approach in coreference resolution (Durrett et al., 2013; Haghighi and Klein, 2010). One commonality among all of these very different probabilistic approaches is that each learns statistical regularities about how entities are depicted in text (whether for the sake of learning a set of semantic r"
P14-1035,N13-1104,0,0.0148763,"extra-linguistic information (such as author). In an empirical evaluation, we find that this method leads to improved agreement with the preregistered judgments of a literary scholar, complementing the results of alternative models. 1 Noah A. Smith School of Computer Science Carnegie Mellon University Pittsburgh, PA 15213, USA nasmith@cs.cmu.edu Introduction Recent work in NLP has begun to exploit the potential of entity-centric modeling for a variety of tasks: Chambers (2013) places entities at the center of probabilistic frame induction, showing gains over a comparable event-centric model (Cheung et al., 2013); Bamman et al. (2013) explicitly learn character types (or “personas”) in a dataset of Wikipedia movie plot summaries; and entity-centric models form one dominant approach in coreference resolution (Durrett et al., 2013; Haghighi and Klein, 2010). One commonality among all of these very different probabilistic approaches is that each learns statistical regularities about how entities are depicted in text (whether for the sake of learning a set of semantic roles, character types, or linking anaphora to the entities to which they refer). In each case, the text we observe associated with an enti"
P14-1035,D13-1010,1,0.722808,"ersonas mance and output of our model by preregistering pd,c Character c’s persona a set of 29 hypotheses of varying scope and diffij An index for a hr, wi tuple in the data wj Word cluster ID for tuple j culty and comparing the performance of different rj Role for tuple j ∈ {agent, patient, poss, pred} models in either confirming, or failing to confirm, η Coefficients for the log-linear language model those hypotheses. This kind of evaluation was preµ, λ Laplace mean and scale (for regularizing η) α Dirichlet concentration parameter viously applied to a subjective text measurement problem by Sim et al. (2013). Figure 2: Above: Probabilistic graphical model. Observed variables are shaded, latent variables are clear, and collapsed All hypotheses were created by a literary variables are dotted. Below: Definition of variables. scholar with specialization in the period to not only give an empirical measure of the strengths and weaknesses of different models, but also to observed in each role r for that character: help explore exactly what the different models R Y Y (count(z; pd,−c ) + αz )× P (bj |m, p, r, η) may, or may not, be learning. All preregistered hypotheses establish the degrees of similarity"
P14-1035,N03-1033,0,0.0175018,"g poetry, drama, and nonfiction as well as prose narrative), we extract 32,209 volumes of prose fiction, remove duplicates and fuse multi-volume works to create the final dataset. Since the original texts were produced 2 3 All categories are described using the Stanford typed dependencies (de Marneffe and Manning, 2008), but any syntactic formalism is equally applicable. http://www.hathitrust.org 371 the biggest bottlenecks. Before addressing character inference, we present here a prerequisite NLP pipeline that scales well to book-length documents.4 This pipeline uses the Stanford POS tagger (Toutanova et al., 2003), the linear-time MaltParser (Nivre et al., 2007) for dependency parsing (trained on Stanford typed dependencies), and the Stanford named entity recognizer (Finkel et al., 2005). It includes the following components for clustering character name mentions, resolving pronominal coreference, and reducing vocabulary dimensionality. 3.1 1. The syntactic dependency path from a pronoun to its potential antecedent (e.g., dobj↑pred→↓pred↓nsubj (where → denotes movement across sentence boundaries). 2. The salience of the antecedent character (defined as the count of that character’s named mentions in th"
P14-1035,P13-1012,0,0.0115471,"ative models. 1 Noah A. Smith School of Computer Science Carnegie Mellon University Pittsburgh, PA 15213, USA nasmith@cs.cmu.edu Introduction Recent work in NLP has begun to exploit the potential of entity-centric modeling for a variety of tasks: Chambers (2013) places entities at the center of probabilistic frame induction, showing gains over a comparable event-centric model (Cheung et al., 2013); Bamman et al. (2013) explicitly learn character types (or “personas”) in a dataset of Wikipedia movie plot summaries; and entity-centric models form one dominant approach in coreference resolution (Durrett et al., 2013; Haghighi and Klein, 2010). One commonality among all of these very different probabilistic approaches is that each learns statistical regularities about how entities are depicted in text (whether for the sake of learning a set of semantic roles, character types, or linking anaphora to the entities to which they refer). In each case, the text we observe associated with an entity in a document is directly dependent on the class of entity—and only that class. This relationship between entity and text is a theoretical assumption, with important consequences for 2 Literary Background Inferring ch"
P14-1035,P10-1015,0,0.32407,"Missing"
P14-1035,P05-1045,0,0.0295951,"Since the original texts were produced 2 3 All categories are described using the Stanford typed dependencies (de Marneffe and Manning, 2008), but any syntactic formalism is equally applicable. http://www.hathitrust.org 371 the biggest bottlenecks. Before addressing character inference, we present here a prerequisite NLP pipeline that scales well to book-length documents.4 This pipeline uses the Stanford POS tagger (Toutanova et al., 2003), the linear-time MaltParser (Nivre et al., 2007) for dependency parsing (trained on Stanford typed dependencies), and the Stanford named entity recognizer (Finkel et al., 2005). It includes the following components for clustering character name mentions, resolving pronominal coreference, and reducing vocabulary dimensionality. 3.1 1. The syntactic dependency path from a pronoun to its potential antecedent (e.g., dobj↑pred→↓pred↓nsubj (where → denotes movement across sentence boundaries). 2. The salience of the antecedent character (defined as the count of that character’s named mentions in the previous 500 words). 3. The antecedent part of speech. 4. Whether or not the pronoun and antecedent appear in the same quotation scope (false if one appears in a quotation and"
P14-1074,J92-4003,0,0.418695,"in a non treestructured regularizer, we add an additional lasso penalty for each word type (with hyperparameter λlas ) to also encourage weights of irrelevant words to go to zero. Our LDA regularizer is an instance of sparse group lasso (Friedman et al., 2010). 6 Another possibility is to group the smallest set of words whose total probability given a topic amounts to P (e.g., 0.99). mass of a topic. Preliminary experiments found this not to work well. 789 v0 4.4 Brown Cluster Regularizer Brown clustering is a commonly used unsupervised method for grouping words into a hierarchy of clusters (Brown et al., 1992). Because it uses local information, it tends to discover words with similar syntactic behavior, though semantic groupings are often evident, especially at the more finegrained end of the hierarchy. We incorporate Brown clusters into a regularizer in a similar way to the topical word groups inferred using LDA in §4.3, but here we make use of the hierarchy. Specifically, we construct treestructured groups, one per cluster (i.e., one per node in the hierarchy). The Brown cluster regularizer is: P Ωbrown (w) = N v=1 λv kwv k2 , v2 v1 v5 v4 v6 v7 v3 v10 v11 v12 v13 v14 v15 v8 v9 midﬁelder knee inj"
P14-1074,D11-1139,1,0.837075,"Missing"
P14-1074,D13-1024,0,0.233107,"r Science Carnegie Mellon University Pittsburgh, PA 15213, USA nasmith@cs.cmu.edu Dani Yogatama Language Technologies Institute School of Computer Science Carnegie Mellon University Pittsburgh, PA 15213, USA dyogatama@cs.cmu.edu Abstract about different weights jointly. The most widely explored variant, group lasso (Yuan and Lin, 2006) seeks to avoid large `2 norms for groups of weights. Group lasso has been shown useful in a range of applications, including computational biology (Kim and Xing, 2008), signal processing (Lv et al., 2011), and NLP (Eisenstein et al., 2011; Martins et al., 2011; Nelakanti et al., 2013). For text categorization problems, Yogatama and Smith (2014) proposed groups based on sentences, an idea generalized here to take advantage of richer linguistic information. In this paper, we show how linguistic information of various kinds—parse trees, thematic topics, and hierarchical word clusterings—can be used to construct group lasso variants that impose linguistic bias without introducing any new features. Our experiments demonstrate that structured regularizers can squeeze higher performance out of conventional bag-of-words models on seven out of eight of text categorization tasks tes"
P14-1074,P11-1137,1,0.498072,"nguage Technologies Institute School of Computer Science Carnegie Mellon University Pittsburgh, PA 15213, USA nasmith@cs.cmu.edu Dani Yogatama Language Technologies Institute School of Computer Science Carnegie Mellon University Pittsburgh, PA 15213, USA dyogatama@cs.cmu.edu Abstract about different weights jointly. The most widely explored variant, group lasso (Yuan and Lin, 2006) seeks to avoid large `2 norms for groups of weights. Group lasso has been shown useful in a range of applications, including computational biology (Kim and Xing, 2008), signal processing (Lv et al., 2011), and NLP (Eisenstein et al., 2011; Martins et al., 2011; Nelakanti et al., 2013). For text categorization problems, Yogatama and Smith (2014) proposed groups based on sentences, an idea generalized here to take advantage of richer linguistic information. In this paper, we show how linguistic information of various kinds—parse trees, thematic topics, and hierarchical word clusterings—can be used to construct group lasso variants that impose linguistic bias without introducing any new features. Our experiments demonstrate that structured regularizers can squeeze higher performance out of conventional bag-of-words models on seve"
P14-1074,N07-1051,0,0.0256197,"will be recommended by a Congressional committee (Yano et al., 2012).13 Table 2 summarizes statistics about the datasets used in our experiments. In total, we evaluate our method on eight binary classification tasks. 6.2 Setup In all our experiments, we use unigram features plus an additional bias term which is not regularized. We compare our new regularizers with state-of-the-art methods for document classification: lasso, ridge, and elastic net regularization, as well as the sentence regularizer discussed in §4.1 (Yogatama and Smith, 2014).14 We parsed all corpora using the Berkeley parser (Petrov and Klein, 2007).15 For the LDA regularizers, we ran LDA16 on training documents with K = 1, 000 and R = 10. For the Brown cluster regularizers, we ran Brown clustering17 on training documents with 5, 000 clusters for the topic classification and sentiment analysis datasets, and 1, 000 for the larger text forecasting datasets (since they are bigger datasets that took more time). 13 14 Topic and cluster features. Another way to incorporate LDA topics and Brown clusters into a linear model is by adding them as additional features. For the 20N datasets, we also ran lasso, ridge, and elastic net with additional L"
P14-1074,W09-3607,0,0.0646657,"Missing"
P14-1074,D13-1170,0,0.319182,"es in isolation (such as lasso, ridge, and elastic net regularizers) on a range of datasets for various text prediction problems: topic classification, sentiment analysis, and forecasting. 1 Introduction What is the best way to exploit linguistic information in statistical text processing models? For tasks like text classification, sentiment analysis, and text-driven forecasting, this is an open question, as cheap “bag-of-words” models often perform well. Much recent work in NLP has focused on linguistic feature engineering (Joshi et al., 2010) or representation learning (Glorot et al., 2011; Socher et al., 2013). In this paper, we propose a radical alternative. We embrace the conventional bag-of-words representation of text, instead bringing linguistic bias to bear on regularization. Since the seminal work of Chen and Rosenfeld (2000), the importance of regularization in discriminative models of text— including language modeling, structured prediction, and classification—has been widely recognized. The emphasis, however, has largely been on one specific kind of inductive bias: avoiding large weights (i.e., coefficients in a linear model). Recently, structured (or composite) regularization has been in"
P14-1074,N10-1038,1,0.213941,"ion accuracies compared to standard regularizers that penalize features in isolation (such as lasso, ridge, and elastic net regularizers) on a range of datasets for various text prediction problems: topic classification, sentiment analysis, and forecasting. 1 Introduction What is the best way to exploit linguistic information in statistical text processing models? For tasks like text classification, sentiment analysis, and text-driven forecasting, this is an open question, as cheap “bag-of-words” models often perform well. Much recent work in NLP has focused on linguistic feature engineering (Joshi et al., 2010) or representation learning (Glorot et al., 2011; Socher et al., 2013). In this paper, we propose a radical alternative. We embrace the conventional bag-of-words representation of text, instead bringing linguistic bias to bear on regularization. Since the seminal work of Chen and Rosenfeld (2000), the importance of regularization in discriminative models of text— including language modeling, structured prediction, and classification—has been widely recognized. The emphasis, however, has largely been on one specific kind of inductive bias: avoiding large weights (i.e., coefficients in a linear"
P14-1074,N09-1031,1,0.815694,"e neutral reviews. λg ρ For a tree structured regularizer, we can get speedups by working from the root node towards the leaf nodes when applying the proximal operator in the second step. If g is a node in a tree which is driven to zero, all of its children h that has λh ≤ λg will also be driven to zero. Eq. 3 is a simple update of the dual variable u. Algorithm 1 summarizes our learning procedure.9 Text-driven forecasting. Forecasting from text requires identifying textual correlates of a response variable revealed in the future, most of which will be weak and many of which will be spurious (Kogan et al., 2009). We consider two such problems. The first one is predicting whether a scientific paper will be cited or not within three years of its publication (Yogatama et al., 2011); 7 For the parse tree regularizer, L is the sum, over all training-data word tokens t, of the number of constituents t belongs to. For the LDA regularizer, L = R × K. For the Brown cluster regularizer, L = V − 1. 8 The difference lies in that the squared `2 norm in the penalty penalizes the difference between w and a vector that depends on the current values of u and v. This does not affect the algorithm or its convergence in"
P14-1074,W06-1639,0,0.0777233,"ment analysis. One task in sentiment analysis is predicting the polarity of a piece of text, i.e., whether the author is favorably inclined toward a (usually known) subject of discussion or proposition (Pang and Lee, 2008). Sentiment analysis, even at the coarse level of polarity we consider here, can be confused by negation, stylistic use of irony, and other linguistic phenomena. Our sentiment analysis datasets consist of movie reviews from the Stanford sentiment treebank (Socher et al., 2013),11 and floor speeches by U.S. Congressmen alongside “yea”/“nay” votes on the bill under discussion (Thomas et al., 2006).12 For the Stanford sentiment treebank, we only predict binary classifications (positive or negative) and exclude neutral reviews. λg ρ For a tree structured regularizer, we can get speedups by working from the root node towards the leaf nodes when applying the proximal operator in the second step. If g is a node in a tree which is driven to zero, all of its children h that has λh ≤ λg will also be driven to zero. Eq. 3 is a simple update of the dual variable u. Algorithm 1 summarizes our learning procedure.9 Text-driven forecasting. Forecasting from text requires identifying textual correlat"
P14-1074,N12-1097,1,0.83816,"Missing"
P14-1074,D10-1102,0,0.0792896,"Missing"
P14-1074,D11-1055,1,0.751634,"Missing"
P14-1134,W11-0103,0,0.0422234,"Missing"
P14-1134,W06-1655,0,0.0374261,"Missing"
P14-1134,W13-2322,0,0.53377,"gorithm for finding a maximum spanning, connected subgraph, embedded within a Lagrangian relaxation of an optimization problem that imposes linguistically inspired constraints. Our approach is described in the general framework of structured prediction, allowing future incorporation of additional features and constraints, and may extend to other formalisms as well. Our open-source system, JAMR, is available at: http://github.com/jflanigan/jamr 1 Introduction Semantic parsing is the problem of mapping natural language strings into meaning representations. Abstract Meaning Representation (AMR) (Banarescu et al., 2013; Dorr et al., 1998) is a semantic formalism in which the meaning of a sentence is encoded as a rooted, directed, acyclic graph. Nodes represent concepts, and labeled directed edges represent the relationships between them–see Figure 1 for an example AMR graph. The formalism is based on propositional logic and neo-Davidsonian event representations (Parsons, 1990; Davidson, 1967). Although it does not encode quantifiers, tense, or modality, the set of semantic phenomena included in AMR were selected with natural language applications—in particular, machine translation—in mind. In this paper we"
P14-1134,P13-2131,0,0.425377,"alism is based on propositional logic and neo-Davidsonian event representations (Parsons, 1990; Davidson, 1967). Although it does not encode quantifiers, tense, or modality, the set of semantic phenomena included in AMR were selected with natural language applications—in particular, machine translation—in mind. In this paper we introduce JAMR, the first published system for automatic AMR parsing. The system is based on a statistical model whose parameters are trained discriminatively using annotated sentences in the AMR Bank corpus (Banarescu et al., 2013). We evaluate using the Smatch score (Cai and Knight, 2013), establishing a baseline for future work. The core of JAMR is a two-part algorithm that first identifies concepts using a semi-Markov model and then identifies the relations that obtain between these by searching for the maximum spanning connected subgraph (MSCG) from an edge-labeled, directed graph representing all possible relations between the identified concepts. To solve the latter problem, we introduce an apparently novel O(|V |2 log |V |) algorithm that is similar to the maximum spanning tree (MST) algorithms that are widely used for dependency parsing (McDonald et al., 2005). Our MSCG"
P14-1134,1992.tmi-1.20,1,0.743247,"Missing"
P14-1134,P13-1091,0,0.183068,"Missing"
P14-1134,W02-1001,0,0.542547,"Missing"
P14-1134,S12-1029,1,0.839695,"Missing"
P14-1134,J14-1002,1,0.256753,"Missing"
P14-1134,de-marneffe-etal-2006-generating,0,0.0716777,"Missing"
P14-1134,dorr-etal-1998-thematic,0,0.405692,"Missing"
P14-1134,J02-3001,0,0.064329,"Missing"
P14-1134,C12-1083,0,0.303236,"Missing"
P14-1134,P03-1054,0,0.0983673,"Missing"
P14-1134,D10-1119,0,0.0230971,"Missing"
P14-1134,P11-1060,0,0.0809238,"Missing"
P14-1134,P09-1039,1,0.766485,"Missing"
P14-1134,D11-1022,1,0.524395,"Missing"
P14-1134,P13-2109,1,0.858632,"Missing"
P14-1134,E06-1011,0,0.178173,"Missing"
P14-1134,H05-1066,0,0.221071,"Missing"
P14-1134,J08-2005,0,0.0599703,"Missing"
P14-1134,W09-1119,0,0.154302,"Missing"
P14-1134,W06-1616,0,0.00889884,"Missing"
P14-1134,D07-1071,0,0.0579116,"Missing"
P14-2099,N04-1015,0,0.012771,"licy sections. While we expect that different kinds of websites will likely address different privacy issues, we believe that many policies will discuss roughly the same set of issues. Aligning the policies is a first step in a larger effort to (i) automatically analyze policies to make them less opaque to users and (ii) support legal experts who wish to characterize the state of privacy online and make recommendations (Costante et al., 2012; Ammar et al., 2012; Costante et al., 2013). We are inspired by multiple sequence alignment methods in computational biology (Durbin et al., 1998) and by Barzilay and Lee (2004), who described a hidden Markov model (HMM) for document content where each state corresponds to a distinct topic and generates sentences relevant to that topic according to a language model. We estimate an HMM-like model on our corpus, exploiting similarity across privacy policies to the extent it is evident in the data. In our formulation, each hidden state corresponds to an issue or topic, characterized by a distribution over words and bigrams appearing in privacy policy sections addressing that issue. The transition distribution captures tendencies of privacy policy authors to organize the"
P14-2134,P98-2127,0,0.0302714,"that are shaped by the context in which it is uttered. In a quantitative evaluation on the task of judging geographically informed semantic similarity between representations learned from 1.1 billion words of geo-located tweets, our joint model outperforms comparable independent models that learn meaning in isolation. 1 In this paper, we introduce a method that extends vector-space lexical semantic models to learn representations of geographically situated language. Vector-space models of lexical semantics have been a popular and effective approach to learning representations of word meaning (Lin, 1998; Turney and Pantel, 2010; Reisinger and Mooney, 2010; Socher et al., 2013; Mikolov et al., 2013, inter alia). In bringing in extra-linguistic information to learn word representations, our work falls into the general domain of multimodal learning; while other work has used visual information to improve distributed representations (Andrews et al., 2009; Feng and Lapata, 2010; Bruni et al., 2011; Bruni et al., 2012a; Bruni et al., 2012b; Roller and im Walde, 2013), this work generally exploits information about the object being described (e.g., strawberry and a picture of a strawberry); in cont"
P14-2134,W11-2503,0,0.0139341,"ical semantic models to learn representations of geographically situated language. Vector-space models of lexical semantics have been a popular and effective approach to learning representations of word meaning (Lin, 1998; Turney and Pantel, 2010; Reisinger and Mooney, 2010; Socher et al., 2013; Mikolov et al., 2013, inter alia). In bringing in extra-linguistic information to learn word representations, our work falls into the general domain of multimodal learning; while other work has used visual information to improve distributed representations (Andrews et al., 2009; Feng and Lapata, 2010; Bruni et al., 2011; Bruni et al., 2012a; Bruni et al., 2012b; Roller and im Walde, 2013), this work generally exploits information about the object being described (e.g., strawberry and a picture of a strawberry); in contrast, we use information about the speaker to learn representations that vary according to contextual variables from the speaker’s perspective. Unlike classic multimodal systems that incorporate multiple active modalities (such as gesture) from a user (Oviatt, 2003; Yu and Introduction The vast textual resources used in NLP – newswire, web text, parliamentary proceedings – can encourage a view"
P14-2134,P12-1015,0,0.0172109,"to learn representations of geographically situated language. Vector-space models of lexical semantics have been a popular and effective approach to learning representations of word meaning (Lin, 1998; Turney and Pantel, 2010; Reisinger and Mooney, 2010; Socher et al., 2013; Mikolov et al., 2013, inter alia). In bringing in extra-linguistic information to learn word representations, our work falls into the general domain of multimodal learning; while other work has used visual information to improve distributed representations (Andrews et al., 2009; Feng and Lapata, 2010; Bruni et al., 2011; Bruni et al., 2012a; Bruni et al., 2012b; Roller and im Walde, 2013), this work generally exploits information about the object being described (e.g., strawberry and a picture of a strawberry); in contrast, we use information about the speaker to learn representations that vary according to contextual variables from the speaker’s perspective. Unlike classic multimodal systems that incorporate multiple active modalities (such as gesture) from a user (Oviatt, 2003; Yu and Introduction The vast textual resources used in NLP – newswire, web text, parliamentary proceedings – can encourage a view of language as a dis"
P14-2134,E14-1011,0,0.0267058,"iamentary proceedings – can encourage a view of language as a disembodied phenomenon. The rise of social media, however, with its large volume of text paired with information about its author and social context, reminds us that each word is uttered by a particular person at a particular place and time. In short: language is situated. The coupling of text with demographic information has enabled computational modeling of linguistic variation, including uncovering words and topics that are characteristic of geographical regions (Eisenstein et al., 2010; O’Connor et al., 2010; Hong et al., 2012; Doyle, 2014), learning correlations between words and socioeconomic variables (Rao et al., 2010; Eisenstein et al., 2011; Pennacchiotti and Popescu, 2011; Bamman et al., 2014); and charting how new terms spread geographically (Eisenstein et al., 2012). These models 828 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 828–834, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics ... Main Alabama Alaska Arizona Arkansas W h X o Figure 1: Model. Illustrated are the input dimensions that fire for a single sampl"
P14-2134,N10-1013,0,0.0373229,"ich it is uttered. In a quantitative evaluation on the task of judging geographically informed semantic similarity between representations learned from 1.1 billion words of geo-located tweets, our joint model outperforms comparable independent models that learn meaning in isolation. 1 In this paper, we introduce a method that extends vector-space lexical semantic models to learn representations of geographically situated language. Vector-space models of lexical semantics have been a popular and effective approach to learning representations of word meaning (Lin, 1998; Turney and Pantel, 2010; Reisinger and Mooney, 2010; Socher et al., 2013; Mikolov et al., 2013, inter alia). In bringing in extra-linguistic information to learn word representations, our work falls into the general domain of multimodal learning; while other work has used visual information to improve distributed representations (Andrews et al., 2009; Feng and Lapata, 2010; Bruni et al., 2011; Bruni et al., 2012a; Bruni et al., 2012b; Roller and im Walde, 2013), this work generally exploits information about the object being described (e.g., strawberry and a picture of a strawberry); in contrast, we use information about the speaker to learn r"
P14-2134,D10-1124,1,0.869174,"Missing"
P14-2134,D13-1115,0,0.027385,"Missing"
P14-2134,P11-1137,1,0.872534,"social media, however, with its large volume of text paired with information about its author and social context, reminds us that each word is uttered by a particular person at a particular place and time. In short: language is situated. The coupling of text with demographic information has enabled computational modeling of linguistic variation, including uncovering words and topics that are characteristic of geographical regions (Eisenstein et al., 2010; O’Connor et al., 2010; Hong et al., 2012; Doyle, 2014), learning correlations between words and socioeconomic variables (Rao et al., 2010; Eisenstein et al., 2011; Pennacchiotti and Popescu, 2011; Bamman et al., 2014); and charting how new terms spread geographically (Eisenstein et al., 2012). These models 828 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 828–834, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics ... Main Alabama Alaska Arizona Arkansas W h X o Figure 1: Model. Illustrated are the input dimensions that fire for a single sample, reflecting a particular word (vocabulary item #2) spoken in Alaska, along with a single output. Parameter"
P14-2134,D12-1137,0,0.0173599,"Missing"
P14-2134,D11-1014,0,0.00987513,"e of tokens around a target word, either by linear distance or dependency path). We can invoke the distributional hypothesis for many instances of regional variation by observing that such variants often appear in similar contexts. For example: • my boy’s wicked smart • my boy’s hella smart • my boy’s very smart Here, all three variants can often be seen in an immediately pre-adjectival position (as is common with intensifying adverbs). Given the empirical success of vector-space representations in capturing semantic properties and their success at a variety of NLP tasks (Turian et al., 2010; Socher et al., 2011; Collobert et al., 2011; Socher et al., 2013), we use a simple, but state-of-the-art neural architecture (Mikolov et al., 2013) to learn low-dimensional real-valued repre829 be in the same vector space and can therefore be compared to each other; with individual models (each with different initializations), word vectors across different states may not be directly compared. month, day of week, or other demographic variables of the speaker. Let |C |denote the sum of the cardinalities of all variables in C (i.e., 51 states, including the District of Columbia). Rather than using a single embeddin"
P14-2134,N10-1011,0,0.0193515,"xtends vector-space lexical semantic models to learn representations of geographically situated language. Vector-space models of lexical semantics have been a popular and effective approach to learning representations of word meaning (Lin, 1998; Turney and Pantel, 2010; Reisinger and Mooney, 2010; Socher et al., 2013; Mikolov et al., 2013, inter alia). In bringing in extra-linguistic information to learn word representations, our work falls into the general domain of multimodal learning; while other work has used visual information to improve distributed representations (Andrews et al., 2009; Feng and Lapata, 2010; Bruni et al., 2011; Bruni et al., 2012a; Bruni et al., 2012b; Roller and im Walde, 2013), this work generally exploits information about the object being described (e.g., strawberry and a picture of a strawberry); in contrast, we use information about the speaker to learn representations that vary according to contextual variables from the speaker’s perspective. Unlike classic multimodal systems that incorporate multiple active modalities (such as gesture) from a user (Oviatt, 2003; Yu and Introduction The vast textual resources used in NLP – newswire, web text, parliamentary proceedings – c"
P14-2134,P13-1045,0,0.0613431,"titative evaluation on the task of judging geographically informed semantic similarity between representations learned from 1.1 billion words of geo-located tweets, our joint model outperforms comparable independent models that learn meaning in isolation. 1 In this paper, we introduce a method that extends vector-space lexical semantic models to learn representations of geographically situated language. Vector-space models of lexical semantics have been a popular and effective approach to learning representations of word meaning (Lin, 1998; Turney and Pantel, 2010; Reisinger and Mooney, 2010; Socher et al., 2013; Mikolov et al., 2013, inter alia). In bringing in extra-linguistic information to learn word representations, our work falls into the general domain of multimodal learning; while other work has used visual information to improve distributed representations (Andrews et al., 2009; Feng and Lapata, 2010; Bruni et al., 2011; Bruni et al., 2012a; Bruni et al., 2012b; Roller and im Walde, 2013), this work generally exploits information about the object being described (e.g., strawberry and a picture of a strawberry); in contrast, we use information about the speaker to learn representations that v"
P14-2134,P10-1040,0,0.0103477,"as the bag or sequence of tokens around a target word, either by linear distance or dependency path). We can invoke the distributional hypothesis for many instances of regional variation by observing that such variants often appear in similar contexts. For example: • my boy’s wicked smart • my boy’s hella smart • my boy’s very smart Here, all three variants can often be seen in an immediately pre-adjectival position (as is common with intensifying adverbs). Given the empirical success of vector-space representations in capturing semantic properties and their success at a variety of NLP tasks (Turian et al., 2010; Socher et al., 2011; Collobert et al., 2011; Socher et al., 2013), we use a simple, but state-of-the-art neural architecture (Mikolov et al., 2013) to learn low-dimensional real-valued repre829 be in the same vector space and can therefore be compared to each other; with individual models (each with different initializations), word vectors across different states may not be directly compared. month, day of week, or other demographic variables of the speaker. Let |C |denote the sum of the cardinalities of all variables in C (i.e., 51 states, including the District of Columbia). Rather than us"
P14-2134,P11-1096,0,0.0632653,"Missing"
P14-2134,C98-2122,0,\N,Missing
P14-5021,diaz-de-ilarraza-etal-2004-abar,0,0.067333,"Missing"
P14-5021,J93-2004,0,0.0477748,"Missing"
P14-5021,N03-4009,0,0.0449942,"units, but still requires dependency relations to be labeled. Finally, we note that new approaches to corpus annotation of semantic dependencies also come with rich browser-based annotation interfaces (Banarescu et al., 2013; Abend and Rappoport, 2013). Other Tools In treebanking, a good user interface is essential for annotator productivity and accuracy. Several existing tools support dependency annotation; GFL-Web is the first designed specifically for the FUDG/GFL framework. Some, including WebAnno (Yimam et al., 2013) and brat (Stenetorp et al., 2012), are browser-based, while WordFreak (Morton and LaCivita, 2003), Abar-Hitz (Ilarraza et al., 2004), and TrEd (Pajas and Fabian, 2000–2011) are client-side applications. All offer tree visualizations; to the best of our knowledge, ours is the only dependency annotation interface that has text as the exclu7 Conclusion While the creation of high-quality, highly specified, syntactically annotated corpora is a goal that is out of reach for most languages and genres, GFL-Web facilitates a rapid annotation workflow within a simple framework for dependency syntax. More information on FUDG/GFL is available at http://www.ark.cs.cmu.edu/FUDG/, and the source code fo"
P14-5021,W13-2307,1,0.566479,"Missing"
P14-5021,E12-2021,0,0.0872351,"Missing"
P14-5021,P13-4001,0,0.0377836,"Missing"
P14-5021,P13-1023,0,0.0264748,"013). These in turn require Graphviz (Ellson et al., 2002), which is freely available. Flask provides a built-in server, but can also be deployed in Apache, via WSGI or CGI, etc. 6 Certain elements of the FUDG/GFL framework can be found in other annotation systems, such as the PASSAGE syntactic representation (Vilnat et al., 2010), which allows for grouping of words into units, but still requires dependency relations to be labeled. Finally, we note that new approaches to corpus annotation of semantic dependencies also come with rich browser-based annotation interfaces (Banarescu et al., 2013; Abend and Rappoport, 2013). Other Tools In treebanking, a good user interface is essential for annotator productivity and accuracy. Several existing tools support dependency annotation; GFL-Web is the first designed specifically for the FUDG/GFL framework. Some, including WebAnno (Yimam et al., 2013) and brat (Stenetorp et al., 2012), are browser-based, while WordFreak (Morton and LaCivita, 2003), Abar-Hitz (Ilarraza et al., 2004), and TrEd (Pajas and Fabian, 2000–2011) are client-side applications. All offer tree visualizations; to the best of our knowledge, ours is the only dependency annotation interface that has te"
P14-5021,N13-1049,0,0.0175252,"UDG/GFL annotation. The simple interface provides instantaneous feedback on the wellformedness of a GFL annotation, and by wrapping Schneider et al.’s notation parsing and rendering software, gives a user-friendly visualization of the annotated sentence. The tool itself is lightweight, multi-user, and easily deployed with few software dependencies. Sentences are assigned to annotators via an administrative interface, which also records progress and provides for a text dump of 1 These can be especially effective when some details of the syntax can be predicted automatically with high accuracy (Alkuhlani et al., 2013). 121 Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 121–126, c Baltimore, Maryland USA, June 23-24, 2014. 2014 Association for Computational Linguistics (a) @Bryan_wright11 i lost all my contacts , smh . (b) Texas Rangers are in the World Series ! Rangers !!!!!!!!! http://fb.me/D2LsXBJx Go Figure 1: FUDG annotation graphs for two tweets. • Multiword units may be joined to form composite lexical nodes (e.g., World_Series in figure 1b). These nodes are not annotated with any internal syntactic structure. • Tokens that are used i"
P14-5021,W13-2322,1,0.771883,"GFL (Schneider et al., 2013). These in turn require Graphviz (Ellson et al., 2002), which is freely available. Flask provides a built-in server, but can also be deployed in Apache, via WSGI or CGI, etc. 6 Certain elements of the FUDG/GFL framework can be found in other annotation systems, such as the PASSAGE syntactic representation (Vilnat et al., 2010), which allows for grouping of words into units, but still requires dependency relations to be labeled. Finally, we note that new approaches to corpus annotation of semantic dependencies also come with rich browser-based annotation interfaces (Banarescu et al., 2013; Abend and Rappoport, 2013). Other Tools In treebanking, a good user interface is essential for annotator productivity and accuracy. Several existing tools support dependency annotation; GFL-Web is the first designed specifically for the FUDG/GFL framework. Some, including WebAnno (Yimam et al., 2013) and brat (Stenetorp et al., 2012), are browser-based, while WordFreak (Morton and LaCivita, 2003), Abar-Hitz (Ilarraza et al., 2004), and TrEd (Pajas and Fabian, 2000–2011) are client-side applications. All offer tree visualizations; to the best of our knowledge, ours is the only dependency anno"
P14-5021,2005.eamt-1.11,0,0.0485334,"Missing"
P14-5021,P11-2008,1,0.874453,"Missing"
P14-5021,P09-2056,0,0.0238252,"ies et al., 2012), the Penn Arabic Treebank (Maamouri et al., 2004), and the Prague dependency treebanks (Hajiˇc, 1998; ˇ Cmejrek et al., 2005), have relied on expert linguists to produce carefully-controlled annotated data. Because this process is costly, such annotation projects have been undertaken for only a handful of important languages. Therefore, developing syntactic resources for less-studied, lowerresource, or politically less important languages and genres will require alternative methods. To address this, simplified annotation schemes that trade cost for detail have been proposed (Habash and Roth, 2009).1 Although GFL offers a number of conveniences to annotators, the text-based UI is limiting: the existing tools require constant switching between a text editor and executing commands, and there are no tools for managing a large-scale annotation effort. Additionally, user interface research has found marked preferences for and better performance with graphical tools relative to text-based interfaces—particularly for less computer-savvy users (Staggers and Kobus, 2000). In this paper, we present the GFL-Web tool, a web-based interface for FUDG/GFL annotation. The simple interface provides inst"
P14-5021,vilnat-etal-2010-passage,0,\N,Missing
P15-1033,C14-1076,1,0.893251,"Missing"
P15-1033,P14-2131,0,0.0270437,"ds—both those that are OOV in both the very limited parsing data but present in the pretraining LM, and words that are OOV in both. To ensure we have estimates of the OOVs in the parsing training data, we stochastically replace (with p = 0.5) each singleton word type in the parsing training data with the UNK token in each training iteration. Pretrained word embeddings. A veritable cottage industry exists for creating word embeddings, meaning numerous pretraining options for w ˜ LM are available. However, for syntax modeling problems, embedding approaches which discard order perform less well (Bansal et al., 2014); therefore we used a variant of the skip n-gram model introduced by Ling et al. (2015), named “structured skip n-gram,” where a different set of parameters is used to predict each context word depending on its position relative to the target word. The hyperparameters of the model are the same as in the skip n-gram model defined in word2vec (Mikolov c = tanh (U[h; d; r] + e) . For the relation vector, we use an embedding of the parser action that was applied to construct the relation (i.e., the syntactic relation paired with the direction of attachment). 4 Training Procedure We trained our par"
P15-1033,P82-1020,0,0.861444,"Missing"
P15-1033,D12-1133,0,0.0254088,"Missing"
P15-1033,P08-1067,0,0.025115,"Missing"
P15-1033,D14-1081,0,0.0362549,"Missing"
P15-1033,D14-1082,0,0.815907,"Transition-Based Dependency Parsing with Stack Long Short-Term Memory Chris Dyer♣♠ Miguel Ballesteros♦♠ Wang Ling♠ Austin Matthews♠ Noah A. Smith♠ ♣ Marianas Labs ♦ NLP Group, Pompeu Fabra University ♠ Carnegie Mellon University chris@marianaslabs.com, miguel.ballesteros@upf.edu, {lingwang,austinma,nasmith}@cs.cmu.edu Abstract decisions (Nivre, 2007; Nivre, 2008; Nivre, 2009; Choi and McCallum, 2013; Bohnet and Nivre, 2012), through feature engineering (Zhang and Nivre, 2011; Ballesteros and Nivre, 2014; Chen et al., 2014; Ballesteros and Bohnet, 2014) and more recently using neural networks (Chen and Manning, 2014; Stenetorp, 2013). We propose a technique for learning representations of parser states in transitionbased dependency parsers. Our primary innovation is a new control structure for sequence-to-sequence neural networks— the stack LSTM. Like the conventional stack data structures used in transitionbased parsing, elements can be pushed to or popped from the top of the stack in constant time, but, in addition, an LSTM maintains a continuous space embedding of the stack contents. This lets us formulate an efficient parsing model that captures three facets of a parser’s state: (i) unbounded look-ah"
P15-1033,N15-1142,1,0.133656,"aining LM, and words that are OOV in both. To ensure we have estimates of the OOVs in the parsing training data, we stochastically replace (with p = 0.5) each singleton word type in the parsing training data with the UNK token in each training iteration. Pretrained word embeddings. A veritable cottage industry exists for creating word embeddings, meaning numerous pretraining options for w ˜ LM are available. However, for syntax modeling problems, embedding approaches which discard order perform less well (Bansal et al., 2014); therefore we used a variant of the skip n-gram model introduced by Ling et al. (2015), named “structured skip n-gram,” where a different set of parameters is used to predict each context word depending on its position relative to the target word. The hyperparameters of the model are the same as in the skip n-gram model defined in word2vec (Mikolov c = tanh (U[h; d; r] + e) . For the relation vector, we use an embedding of the parser action that was applied to construct the relation (i.e., the syntactic relation paired with the direction of attachment). 4 Training Procedure We trained our parser to maximize the conditional log-likelihood (Eq. 1) of treebank parses given sentenc"
P15-1033,C14-1078,0,0.0487974,"Missing"
P15-1033,D10-1004,1,0.291438,"Missing"
P15-1033,P13-1104,0,0.0353977,"Missing"
P15-1033,de-marneffe-etal-2006-generating,0,0.0341911,"Missing"
P15-1033,W03-3017,0,0.0269603,"step is constructed incrementally, and therefore parsing and training time remain linear in the length of the input sentence. The technical innovation that lets us do this is a variation of recurrent neural networks with long short-term memory units (LSTMs) which we call stack LSTMs (§2), and which support both reading (pushing) and “forgetting” (popping) inputs. Introduction Transition-based dependency parsing formalizes the parsing problem as a series of decisions that read words sequentially from a buffer and combine them incrementally into syntactic structures (Yamada and Matsumoto, 2003; Nivre, 2003; Nivre, 2004). This formalization is attractive since the number of operations required to build any projective parse tree is linear in the length of the sentence, making transition-based parsing computationally efficient relative to graph- and grammarbased formalisms. The challenge in transitionbased parsing is modeling which action should be taken in each of the unboundedly many states encountered as the parser progresses. This challenge has been addressed by development of alternative transition sets that simplify the modeling problem by making better attachment Our parsing model uses thre"
P15-1033,W04-0308,0,0.20895,"ructed incrementally, and therefore parsing and training time remain linear in the length of the input sentence. The technical innovation that lets us do this is a variation of recurrent neural networks with long short-term memory units (LSTMs) which we call stack LSTMs (§2), and which support both reading (pushing) and “forgetting” (popping) inputs. Introduction Transition-based dependency parsing formalizes the parsing problem as a series of decisions that read words sequentially from a buffer and combine them incrementally into syntactic structures (Yamada and Matsumoto, 2003; Nivre, 2003; Nivre, 2004). This formalization is attractive since the number of operations required to build any projective parse tree is linear in the length of the sentence, making transition-based parsing computationally efficient relative to graph- and grammarbased formalisms. The challenge in transitionbased parsing is modeling which action should be taken in each of the unboundedly many states encountered as the parser progresses. This challenge has been addressed by development of alternative transition sets that simplify the modeling problem by making better attachment Our parsing model uses three stack LSTMs:"
P15-1033,P13-2111,0,0.0261337,"ter points (i.e., the hTOP ), a continuous-space “summary” of the contents of the current stack configuration is available. We refer to this value as the “stack summary.” it = σ(Wix xt + Wih ht−1 + Wic ct−1 + bi ) ft = σ(Wf x xt + Wf h ht−1 + Wf c ct−1 + bf ) ct = ft ct−1 + What does the stack summary look like? Intuitively, elements near the top of the stack will it tanh(Wcx xt + Wch ht−1 + bc ), 1 Ours is not the first deviation from a strict left-toright order: previous variations include bidirectional LSTMs (Graves and Schmidhuber, 2005) and multidimensional LSTMs (Graves et al., 2007). 2 Goldberg et al. (2013) propose a similar stack construction to prevent stack operations from invalidating existing references to the stack in a beam-search parser that must (efficiently) maintain a priority queue of stacks. where σ is the component-wise logistic sigmoid function, and is the component-wise (Hadamard) product. The value ht of the LSTM at each time step is controlled by a third gate (ot ) that is applied to the result of the application of a nonlinearity to the 335 P TO y0 P P TO TO y1 y0 y1 pop ; x1 y0 y1 y2 ; x1 x2 push ; x1 Figure 1: A stack LSTM extends a conventional left-to-right LSTM with the a"
P15-1033,N07-1050,0,0.0449813,"Missing"
P15-1033,J08-4003,0,0.120327,"Missing"
P15-1033,P09-1040,0,0.0683306,"Missing"
P15-1033,P13-1014,0,0.0122661,"represent each input token, we concatenate three vectors: a learned vector representation for each word type (w); a fixed vector representa˜ LM ), and a tion from a neural language model (w learned representation (t) of the POS tag of the token, provided as auxiliary input to the parser. A Our parser is based on the arc-standard transition inventory (Nivre, 2004), given in Figure 3. 5 In general, A(S, B) is the complete set of parser actions discussed in §3.2, but in some cases not all actions are available. For example, when S is empty and words remain in B, a SHIFT operation is obligatory (Sartorio et al., 2013). 337 Stackt (u, u), (v, v), S (u, u), (v, v), S S Buffert B B (u, u), B Action Stackt+1 (gr (u, v), u), S (gr (v, u), v), S (u, u), S REDUCE - RIGHT (r) REDUCE - LEFT (r) SHIFT Buffert+1 B B B Dependency r u→v r u←v — Figure 3: Parser transitions indicating the action applied to the stack and buffer and the resulting stack and buffer states. Bold symbols indicate (learned) embeddings of words and relations, script symbols indicate the corresponding words and relations. linear map (V) is applied to the resulting vector and passed through a component-wise ReLU, et al., 2013), and we set the win"
P15-1033,P14-6005,0,0.0326662,"Missing"
P15-1033,P04-1013,0,0.0390693,"Missing"
P15-1033,P13-1088,0,0.0213712,"ons. linear map (V) is applied to the resulting vector and passed through a component-wise ReLU, et al., 2013), and we set the window size to 5, used a negative sampling rate to 10, and ran 5 epochs through unannotated corpora described in §5.1. ˜ LM ; t] + b} . x = max {0, V[w; w 3.4 This mapping can be shown schematically as in Figure 4. ˜ 2LM w x2 w 2 t2 overhasty UNK JJ ˜ 3LM w w3 Composition Functions Recursive neural network models enable complex phrases to be represented compositionally in terms of their parts and the relations that link them (Socher et al., 2011; Socher et al., 2013c; Hermann and Blunsom, 2013; Socher et al., 2013b). We follow this previous line of work in embedding dependency tree fragments that are present in the stack S in the same vector space as the token embeddings discussed above. A particular challenge here is that a syntactic head may, in general, have an arbitrary number of dependents. To simplify the parameterization of our composition function, we combine headmodifier pairs one at a time, building up more complicated structures in the order they are “reduced” in the parser, as illustrated in Figure 5. Each node in this expanded syntactic tree has a value computed as a f"
P15-1033,P13-1045,0,0.0166315,"nding words and relations. linear map (V) is applied to the resulting vector and passed through a component-wise ReLU, et al., 2013), and we set the window size to 5, used a negative sampling rate to 10, and ran 5 epochs through unannotated corpora described in §5.1. ˜ LM ; t] + b} . x = max {0, V[w; w 3.4 This mapping can be shown schematically as in Figure 4. ˜ 2LM w x2 w 2 t2 overhasty UNK JJ ˜ 3LM w w3 Composition Functions Recursive neural network models enable complex phrases to be represented compositionally in terms of their parts and the relations that link them (Socher et al., 2011; Socher et al., 2013c; Hermann and Blunsom, 2013; Socher et al., 2013b). We follow this previous line of work in embedding dependency tree fragments that are present in the stack S in the same vector space as the token embeddings discussed above. A particular challenge here is that a syntactic head may, in general, have an arbitrary number of dependents. To simplify the parameterization of our composition function, we combine headmodifier pairs one at a time, building up more complicated structures in the order they are “reduced” in the parser, as illustrated in Figure 5. Each node in this expanded syntactic tree"
P15-1033,D13-1170,0,0.0024939,"nding words and relations. linear map (V) is applied to the resulting vector and passed through a component-wise ReLU, et al., 2013), and we set the window size to 5, used a negative sampling rate to 10, and ran 5 epochs through unannotated corpora described in §5.1. ˜ LM ; t] + b} . x = max {0, V[w; w 3.4 This mapping can be shown schematically as in Figure 4. ˜ 2LM w x2 w 2 t2 overhasty UNK JJ ˜ 3LM w w3 Composition Functions Recursive neural network models enable complex phrases to be represented compositionally in terms of their parts and the relations that link them (Socher et al., 2011; Socher et al., 2013c; Hermann and Blunsom, 2013; Socher et al., 2013b). We follow this previous line of work in embedding dependency tree fragments that are present in the stack S in the same vector space as the token embeddings discussed above. A particular challenge here is that a syntactic head may, in general, have an arbitrary number of dependents. To simplify the parameterization of our composition function, we combine headmodifier pairs one at a time, building up more complicated structures in the order they are “reduced” in the parser, as illustrated in Figure 5. Each node in this expanded syntactic tree"
P15-1033,P07-1080,0,0.0306643,"Missing"
P15-1033,N03-1033,0,0.0859622,"Missing"
P15-1033,I05-3027,0,0.0128763,"Missing"
P15-1033,P15-1032,0,0.257466,"Missing"
P15-1033,W03-3023,0,0.0327577,"for prediction at each time step is constructed incrementally, and therefore parsing and training time remain linear in the length of the input sentence. The technical innovation that lets us do this is a variation of recurrent neural networks with long short-term memory units (LSTMs) which we call stack LSTMs (§2), and which support both reading (pushing) and “forgetting” (popping) inputs. Introduction Transition-based dependency parsing formalizes the parsing problem as a series of decisions that read words sequentially from a buffer and combine them incrementally into syntactic structures (Yamada and Matsumoto, 2003; Nivre, 2003; Nivre, 2004). This formalization is attractive since the number of operations required to build any projective parse tree is linear in the length of the sentence, making transition-based parsing computationally efficient relative to graph- and grammarbased formalisms. The challenge in transitionbased parsing is modeling which action should be taken in each of the unboundedly many states encountered as the parser progresses. This challenge has been addressed by development of alternative transition sets that simplify the modeling problem by making better attachment Our parsing mo"
P15-1033,D08-1059,0,0.0189076,"Missing"
P15-1033,P11-2033,0,0.0327321,"Missing"
P15-1033,D14-1109,0,0.0354567,"Missing"
P15-1033,Q14-1017,0,\N,Missing
P15-1144,J08-4004,0,0.041742,"Missing"
P15-1144,P98-1013,0,0.241048,"Missing"
P15-1144,P14-2131,0,0.0164479,"Missing"
P15-1144,E14-1049,1,0.0297678,"., 2013) is fast and widely-used. In this model, each word’s Huffman code is used as an input to a log-linear classifier with a continuous projection layer and words within a given context window are predicted. These vectors were trained on 100 billion words of Google news data and are of length 300.4 Global Context (GC). These vectors are learned using a recursive neural network that incorporates both local and global (documentlevel) context features (Huang et al., 2012). These vectors were trained on the first 1 billion words of English Wikipedia and are of length 50.5 Multilingual (Multi). Faruqui and Dyer (2014) learned vectors by first performing SVD on text in different languages, then applying canonical correlation analysis on pairs of vectors for words that align in parallel corpora. These vectors were trained on WMT-2011 news corpus containing 360 million words and are of length 48.6 B Evaluation Benchmarks Our comparisons of word vector quality consider five benchmark tasks. We now describe the different evaluation benchmarks for word vectors. Word Similarity. We evaluate our word representations on two word similarity tasks. The first is the WS-353 dataset (Finkelstein et al., 2001), which con"
P15-1144,N15-1184,1,0.140197,"Missing"
P15-1144,P14-1046,0,0.0799848,"gularization hyperparameter, and Ω is the regularizer. Here, we use the squared loss for the reconstruction error, but other loss functions could also be used (Lee et al., 2009). To obtain sparse word representations we will impose an `1 V X kxi − Dai k22 + λkai k1 + τ kDk22 (2) i=1 where mi denotes the ith column vector of matrix M. Note that this problem is not convex. We refer to this approach as method A. 2.2 Sparse Nonnegative Vectors Nonnegativity in the feature space has often been shown to correspond to interpretability (Lee and Seung, 1999; Cichocki et al., 2009; Murphy et al., 2012; Fyshe et al., 2014; Fyshe et al., 2015). To obtain nonnegative sparse word vectors, we use a variation of the nonnegative sparse coding method (Hoyer, 2002). Nonnegative sparse coding further constrains the problem in Eq. 2 so that D and ai are nonnegative. Here, we apply this constraint only to the representation vectors {ai }. Thus, the new objective for nonnegative sparse vectors becomes: arg min K×V D∈RL×K ≥0 ,A∈R≥0 V X kxi −Dai k22 +λkai k1 +τ kDk22 i=1 (3) This problem will play a role in our second approach, method B, to which we will return shortly. This nonnegativity constraint can be easily incorporat"
P15-1144,N15-1004,0,0.157051,"rameter, and Ω is the regularizer. Here, we use the squared loss for the reconstruction error, but other loss functions could also be used (Lee et al., 2009). To obtain sparse word representations we will impose an `1 V X kxi − Dai k22 + λkai k1 + τ kDk22 (2) i=1 where mi denotes the ith column vector of matrix M. Note that this problem is not convex. We refer to this approach as method A. 2.2 Sparse Nonnegative Vectors Nonnegativity in the feature space has often been shown to correspond to interpretability (Lee and Seung, 1999; Cichocki et al., 2009; Murphy et al., 2012; Fyshe et al., 2014; Fyshe et al., 2015). To obtain nonnegative sparse word vectors, we use a variation of the nonnegative sparse coding method (Hoyer, 2002). Nonnegative sparse coding further constrains the problem in Eq. 2 so that D and ai are nonnegative. Here, we apply this constraint only to the representation vectors {ai }. Thus, the new objective for nonnegative sparse vectors becomes: arg min K×V D∈RL×K ≥0 ,A∈R≥0 V X kxi −Dai k22 +λkai k1 +τ kDk22 i=1 (3) This problem will play a role in our second approach, method B, to which we will return shortly. This nonnegativity constraint can be easily incorporated during optimizatio"
P15-1144,N04-1039,0,0.0582336,"rk To the best of our knowledge, there has been no prior work on obtaining overcomplete word vector representations that are sparse and categorical. However, overcomplete features have been widely used in image processing, computer vision (Olshausen and Field, 1997; Lewicki and Sejnowski, 2000) and signal processing (Donoho et al., 2006). Nonnegative matrix factorization is often used for interpretable coding of information (Lee and Seung, 1999; Liu et al., 2003; Cichocki et al., 2009). Sparsity constraints are in general useful in NLP problems (Kazama and Tsujii, 2003; Friedman et al., 2008; Goodman, 2004), like POS tagging (Ganchev et al., 2009), dependency parsing (Martins et al., 2011), text classification (Yogatama and Smith, 2014), and representation learning (Bengio et al., 2013). Including sparsity constraints in Bayesian models of lexical semantics like LDA in the form of sparse Dirichlet priors has been shown to be useful for downstream tasks like POStagging (Toutanova and Johnson, 2007), and improving interpretation (Paul and Dredze, 2012; Zhu and Xing, 2012). 1496 V379 V353 V76 V186 V339 V177 V114 V342 V332 V270 V222 V91 V303 V473 V355 V358 V164 V348 V324 V192 V24 V281 V82 V46 V277 V"
P15-1144,S13-1001,0,0.0655251,"Missing"
P15-1144,D14-1012,0,0.017442,"overcomplete representations A and also binarized representations B. Initial vectors are discussed in §A and tasks in §B. We find that on average, across initializing vectors and across all tasks that our sparse overcomplete (A) vectors lead to better performance than either of the alternative transformations. 4 Figure 2: Average performace across all tasks for sparse overcomplete vectors (A) produced by Glove initial vectors, as a function of the ratio of K to L. achieves a binary, sparse vector (B) by applying:  bi,j = 1 if xi,j > 0 0 otherwise (7) The second transformation was proposed by Guo et al. (2014). Here, the original vector length is also preserved, but sparsity is achieved through:   1 if xi,j ≥ M + −1 if xi,j ≤ M − ai,j = (8)  0 otherwise where M + (M − ) is the mean of positive-valued (negative-valued) elements of X. These vectors are, obviously, not binary. Interpretability Our hypothesis is that the dimensions of sparse overcomplete vectors are more interpretable than those of dense word vectors. Following Murphy et al. (2012), we use a word intrusion experiment (Chang et al., 2009) to corroborate this hypothesis. In addition, we conduct qualitative analysis of interpretability"
P15-1144,J15-4004,0,0.0321756,"Missing"
P15-1144,P12-1092,0,0.066485,"ia and English Gigaword and are of length 300.3 3 http://www-nlp.stanford.edu/projects/ glove/ Skip-Gram (SG). The word2vec tool (Mikolov et al., 2013) is fast and widely-used. In this model, each word’s Huffman code is used as an input to a log-linear classifier with a continuous projection layer and words within a given context window are predicted. These vectors were trained on 100 billion words of Google news data and are of length 300.4 Global Context (GC). These vectors are learned using a recursive neural network that incorporates both local and global (documentlevel) context features (Huang et al., 2012). These vectors were trained on the first 1 billion words of English Wikipedia and are of length 50.5 Multilingual (Multi). Faruqui and Dyer (2014) learned vectors by first performing SVD on text in different languages, then applying canonical correlation analysis on pairs of vectors for words that align in parallel corpora. These vectors were trained on WMT-2011 news corpus containing 360 million words and are of length 48.6 B Evaluation Benchmarks Our comparisons of word vector quality consider five benchmark tasks. We now describe the different evaluation benchmarks for word vectors. Word S"
P15-1144,W03-1018,0,0.0399485,"dimension. mostly distinct vectors. 5 Related Work To the best of our knowledge, there has been no prior work on obtaining overcomplete word vector representations that are sparse and categorical. However, overcomplete features have been widely used in image processing, computer vision (Olshausen and Field, 1997; Lewicki and Sejnowski, 2000) and signal processing (Donoho et al., 2006). Nonnegative matrix factorization is often used for interpretable coding of information (Lee and Seung, 1999; Liu et al., 2003; Cichocki et al., 2009). Sparsity constraints are in general useful in NLP problems (Kazama and Tsujii, 2003; Friedman et al., 2008; Goodman, 2004), like POS tagging (Ganchev et al., 2009), dependency parsing (Martins et al., 2011), text classification (Yogatama and Smith, 2014), and representation learning (Bengio et al., 2013). Including sparsity constraints in Bayesian models of lexical semantics like LDA in the form of sparse Dirichlet priors has been shown to be useful for downstream tasks like POStagging (Toutanova and Johnson, 2007), and improving interpretation (Paul and Dredze, 2012; Zhu and Xing, 2012). 1496 V379 V353 V76 V186 V339 V177 V114 V342 V332 V270 V222 V91 V303 V473 V355 V358 V164"
P15-1144,D13-1147,0,0.0458503,"Missing"
P15-1144,D13-1196,0,0.025068,"d accuracy is reported on the test set. 20 Newsgroup Dataset. We consider three binary categorization tasks from the 20 Newsgroups dataset.7 Each task involves categorizing a document according to two related categories with training/dev./test split in accordance with Yogatama and Smith (2014): (1) Sports: baseball vs. hockey (958/239/796) (2) Comp.: IBM vs. Mac (929/239/777) (3) Religion: atheism vs. christian (870/209/717). We use average of the word vectors of a given sentence as features. The classifier is tuned on the dev. set and accuracy is reported on the test set. NP bracketing (NP). Lazaridou et al. (2013) constructed a dataset from the Penn Treebank (Marcus et al., 1993) of noun phrases (NP) of 7 http://qwone.com/˜jason/20Newsgroups length three words, where the first can be an adjective or a noun and the other two are nouns. The task is to predict the correct bracketing in the parse tree for a given noun phrase. For example, local (phone company) and (blood pressure) medicine exhibit right and left bracketing, respectively. We append the word vectors of the three words in the NP in order and use them as features for binary classification. The dataset contains 2,227 noun phrases split into 10"
P15-1144,Q13-1015,0,0.0245514,"Missing"
P15-1144,W14-2406,0,0.0387394,"Missing"
P15-1144,C02-1150,0,0.0787722,"e classes has been split into training, development, and test datasets containing 6,920, 872, and 1,821 sentences, respectively. We use average of the word vectors of a given sentence as feature for classification. The classifier is tuned on the dev. set and accuracy is reported on the test set. Question Classification (TREC). As an aid to question answering, a question may be classified as belonging to one of many question types. The TREC questions dataset involves six different question types, e.g., whether the question is about a location, about a person, or about some numeric information (Li and Roth, 2002). The training dataset consists of 5,452 labeled questions, and the test dataset consists of 500 questions. An average of the word vectors of the input question is used as features and accuracy is reported on the test set. 20 Newsgroup Dataset. We consider three binary categorization tasks from the 20 Newsgroups dataset.7 Each task involves categorizing a document according to two related categories with training/dev./test split in accordance with Yogatama and Smith (2014): (1) Sports: baseball vs. hockey (958/239/796) (2) Comp.: IBM vs. Mac (929/239/777) (3) Religion: atheism vs. christian (8"
P15-1144,J93-2004,0,0.058184,"der three binary categorization tasks from the 20 Newsgroups dataset.7 Each task involves categorizing a document according to two related categories with training/dev./test split in accordance with Yogatama and Smith (2014): (1) Sports: baseball vs. hockey (958/239/796) (2) Comp.: IBM vs. Mac (929/239/777) (3) Religion: atheism vs. christian (870/209/717). We use average of the word vectors of a given sentence as features. The classifier is tuned on the dev. set and accuracy is reported on the test set. NP bracketing (NP). Lazaridou et al. (2013) constructed a dataset from the Penn Treebank (Marcus et al., 1993) of noun phrases (NP) of 7 http://qwone.com/˜jason/20Newsgroups length three words, where the first can be an adjective or a noun and the other two are nouns. The task is to predict the correct bracketing in the parse tree for a given noun phrase. For example, local (phone company) and (blood pressure) medicine exhibit right and left bracketing, respectively. We append the word vectors of the three words in the NP in order and use them as features for binary classification. The dataset contains 2,227 noun phrases split into 10 folds. The classifier is tuned on the first fold and cross-validati"
P15-1144,D11-1139,1,0.497885,"Missing"
P15-1144,P14-1074,1,0.851886,"are sparse and categorical. However, overcomplete features have been widely used in image processing, computer vision (Olshausen and Field, 1997; Lewicki and Sejnowski, 2000) and signal processing (Donoho et al., 2006). Nonnegative matrix factorization is often used for interpretable coding of information (Lee and Seung, 1999; Liu et al., 2003; Cichocki et al., 2009). Sparsity constraints are in general useful in NLP problems (Kazama and Tsujii, 2003; Friedman et al., 2008; Goodman, 2004), like POS tagging (Ganchev et al., 2009), dependency parsing (Martins et al., 2011), text classification (Yogatama and Smith, 2014), and representation learning (Bengio et al., 2013). Including sparsity constraints in Bayesian models of lexical semantics like LDA in the form of sparse Dirichlet priors has been shown to be useful for downstream tasks like POStagging (Toutanova and Johnson, 2007), and improving interpretation (Paul and Dredze, 2012; Zhu and Xing, 2012). 1496 V379 V353 V76 V186 V339 V177 V114 V342 V332 V270 V222 V91 V303 V473 V355 V358 V164 V348 V324 V192 V24 V281 V82 V46 V277 V466 V465 V128 V11 V413 V98 V131 V445 V199 V475 V208 V431 V299 V357 V149 V80 V247 V231 V42 V44 V376 V152 V74 V254 V141 V341 V349 V234"
P15-1144,P14-2089,0,0.0191967,"Missing"
P15-1144,C12-1118,0,0.72512,"is vectors. λ is a regularization hyperparameter, and Ω is the regularizer. Here, we use the squared loss for the reconstruction error, but other loss functions could also be used (Lee et al., 2009). To obtain sparse word representations we will impose an `1 V X kxi − Dai k22 + λkai k1 + τ kDk22 (2) i=1 where mi denotes the ith column vector of matrix M. Note that this problem is not convex. We refer to this approach as method A. 2.2 Sparse Nonnegative Vectors Nonnegativity in the feature space has often been shown to correspond to interpretability (Lee and Seung, 1999; Cichocki et al., 2009; Murphy et al., 2012; Fyshe et al., 2014; Fyshe et al., 2015). To obtain nonnegative sparse word vectors, we use a variation of the nonnegative sparse coding method (Hoyer, 2002). Nonnegative sparse coding further constrains the problem in Eq. 2 so that D and ai are nonnegative. Here, we apply this constraint only to the representation vectors {ai }. Thus, the new objective for nonnegative sparse vectors becomes: arg min K×V D∈RL×K ≥0 ,A∈R≥0 V X kxi −Dai k22 +λkai k1 +τ kDk22 i=1 (3) This problem will play a role in our second approach, method B, to which we will return shortly. This nonnegativity constraint can"
P15-1144,D14-1162,0,0.123387,"ymous reviewers for their feedback. This research was supported in part by the National Science Foundation through grant IIS-1251131 and the Defense Advanced Research Projects Agency through grant FA87501420244. This work was supported in part by the U.S. Army Research Laboratory and the U.S. Army Research Office under contract/grant number W911NF-10-1-0533. A Initial Vector Representations (X) Our experiments consider four publicly available collections of pre-trained word vectors. They vary in the amount of data used and the estimation method. Glove. Global vectors for word representations (Pennington et al., 2014) are trained on aggregated global word-word co-occurrence statistics from a corpus. These vectors were trained on 6 billion words from Wikipedia and English Gigaword and are of length 300.3 3 http://www-nlp.stanford.edu/projects/ glove/ Skip-Gram (SG). The word2vec tool (Mikolov et al., 2013) is fast and widely-used. In this model, each word’s Huffman code is used as an input to a log-linear classifier with a continuous projection layer and words within a given context window are predicted. These vectors were trained on 100 billion words of Google news data and are of length 300.4 Global Conte"
P15-1144,D13-1170,0,0.0032486,"/code.google.com/p/word2vec http://nlp.stanford.edu/˜socherr/ ACL2012_wordVectorsTextFile.zip 6 http://cs.cmu.edu/˜mfaruqui/soft.html 1497 5 A more recent dataset, SimLex-999 (Hill et al., 2014), has been constructed to specifically focus on similarity (rather than relatedness). It contains a balanced set of noun, verb, and adjective pairs. We calculate cosine similarity between the vectors of two words forming a test item and report Spearman’s rank correlation coefficient (Myers and Well, 1995) between the rankings produced by our model against the human rankings. Sentiment Analysis (Senti). Socher et al. (2013) created a treebank of sentences annotated with fine-grained sentiment labels on phrases and sentences from movie review excerpts. The coarse-grained treebank of positive and negative classes has been split into training, development, and test datasets containing 6,920, 872, and 1,821 sentences, respectively. We use average of the word vectors of a given sentence as feature for classification. The classifier is tuned on the dev. set and accuracy is reported on the test set. Question Classification (TREC). As an aid to question answering, a question may be classified as belonging to one of many"
P15-1144,D13-1015,0,0.0389009,"Missing"
P15-1144,C98-1013,0,\N,Missing
P15-1144,P14-1009,0,\N,Missing
P15-2036,S07-1018,0,0.0348331,"nce relationships and also from grouping by the role name (SEMAFOR already incorporates such features). Johansson (2012) reports improvements in SRL for Swedish, by exploiting relationships between both frames and roles. Baldewein et al. (2004) learn latent clusters of roles and role-fillers, reporting mixed results. Our approach is described in §3.2. 2.2 Sentences Frames Overt arguments Exemplars train test and arguments to as many words as possible. Beginning with the SemEval-2007 shared task on FrameNet analysis, frame-semantic parsers have been trained and evaluated on the full-text data (Baker et al., 2007; Das et al., 2014).3 The full-text documents represent a mix of genres, prominently including travel guides and bureaucratic reports about weapons stockpiles. Exemplars: To document a given predicate, lexicographers manually select corpus examples and annotate them only with respect to the predicate in question. These singly-annotated sentences from FrameNet are called lexicographic exemplars. There are over 140,000 sentences containing argument annotations and relative to the FT dataset, these contain an order of magnitude more frame annotations and over two orders of magnitude more sentence"
P15-2036,P98-1013,0,0.803968,"TIVITY_FINISH : complete.v Fra Agent Activity conclude.v finish.v … the people us to stay coursehorizontal and finish the lines job . representing m ING _e OFFreally _ ONwant by hold off.theThin _OFF_ON: hold off.v Pr Ne Agent End_point Desirable_action: ∅ HOLDING finish-v-01 A0 labeled with role names. A1wait.v t op argument spans are (Not shown: July Ba stay-v-01 nk A3 _ UNIT and fill its Unit and August evokeA1C ALENDRIC role.) Paucity of data resources is a challenge for semantic analyses like frame-semantic parsing (Gildea and Jurafsky, 2002; Das et al., 2014) using the FrameNet lexicon (Baker et al., 1998; Fillmore and Baker, 2009).1 Given a sentence, a framesemantic parse maps word tokens to frames they evoke, and for each frame, finds and labels its argument phrases with frame-specific roles. An example appears in figure 1. In this paper, we address this argument identification subtask, a form of semantic role labeling (SRL), a task introduced by Gildea and Jurafsky (2002) using an earlier version of FrameNet. Our contribution addresses the paucity of annotated data for training using standard domain adaptation techniques. We exploit three annotation sources: ‡ t pan tici par ant r cip nce a"
P15-2036,W04-0817,0,0.0210726,"ING.Convict. We say that a parent of a role is one that has either the Inheritance or Subframe relation to it. There are 4,138 Inheritance and 589 Subframe links among role types in FrameNet 1.5. Prior work has considered various ways of grouping role labels together in order to share statistical strength. Matsubayashi et al. (2009) observed small gains from using the Inheritance relationships and also from grouping by the role name (SEMAFOR already incorporates such features). Johansson (2012) reports improvements in SRL for Swedish, by exploiting relationships between both frames and roles. Baldewein et al. (2004) learn latent clusters of roles and role-fillers, reporting mixed results. Our approach is described in §3.2. 2.2 Sentences Frames Overt arguments Exemplars train test and arguments to as many words as possible. Beginning with the SemEval-2007 shared task on FrameNet analysis, frame-semantic parsers have been trained and evaluated on the full-text data (Baker et al., 2007; Das et al., 2014).3 The full-text documents represent a mix of genres, prominently including travel guides and bureaucratic reports about weapons stockpiles. Exemplars: To document a given predicate, lexicographers manually"
P15-2036,bonial-etal-2014-propbank,0,0.057978,"Missing"
P15-2036,W13-5503,0,0.0575005,"Missing"
P15-2036,J14-1002,1,0.781256,"ant, ACTIVITY _ FINISH by Afinish, and H OLD CTIVITY_FINISH : complete.v Fra Agent Activity conclude.v finish.v … the people us to stay coursehorizontal and finish the lines job . representing m ING _e OFFreally _ ONwant by hold off.theThin _OFF_ON: hold off.v Pr Ne Agent End_point Desirable_action: ∅ HOLDING finish-v-01 A0 labeled with role names. A1wait.v t op argument spans are (Not shown: July Ba stay-v-01 nk A3 _ UNIT and fill its Unit and August evokeA1C ALENDRIC role.) Paucity of data resources is a challenge for semantic analyses like frame-semantic parsing (Gildea and Jurafsky, 2002; Das et al., 2014) using the FrameNet lexicon (Baker et al., 1998; Fillmore and Baker, 2009).1 Given a sentence, a framesemantic parse maps word tokens to frames they evoke, and for each frame, finds and labels its argument phrases with frame-specific roles. An example appears in figure 1. In this paper, we address this argument identification subtask, a form of semantic role labeling (SRL), a task introduced by Gildea and Jurafsky (2002) using an earlier version of FrameNet. Our contribution addresses the paucity of annotated data for training using standard domain adaptation techniques. We exploit three annot"
P15-2036,S12-1029,1,0.891611,"model on a source domain, make predictions using that model on the target domain, then use those predictions as additional features while training a new model on the target domain. The source domain model is effectively a form of preprocessing, and the features from its output are known as guide features (Johansson, 2013; Kong et al., 2014).6 In our case, the full text data is our target domain, and PropBank and the exemplars data are our source domains, respectively. For PropBank, we run the SRL system of Illinois Curator 1.1.4 (Pun5 Recent work has improved upon global decoding techniques (Das et al., 2012; Täckström et al., 2015). We expect such improvements to be complementary to the gains due to the added features and data reported here. 6 This is related to the technique of model stacking, where successively richer models are trained by cross-validation on the same dataset (e.g., Cohen and Carvalho, 2005; Nivre and McDonald, 2008; Martins et al., 2008). scorew (a ∣ x, p, f ,r) = w⊺ φ (a,x, p, f ,r) (1) The model parameters w are learned from data (§4). Prediction requires choosing a joint assignment of all arguments of a frame, respecting the constraints that a role may be assigned to at mo"
P15-2036,P11-1144,1,0.598328,"sk.4 Annotations Statistics for the annotations appear in table 1. Full-text (FT): This portion of the FrameNet corpus consists of documents and has about 5,000 sentences for which annotators assigned frames 219 2.3 PropBank PropBank (PB; Palmer et al., 2005) is a lexicon and corpus of predicate–argument structures that takes a shallower approach than FrameNet. FrameNet frames cluster lexical predicates that evoke sim3 Though these were annotated at the document level, and train/development/test splits are by document, the framesemantic parsing is currently restricted to the sentence level. 4 Das and Smith (2011, 2012) investigated semi-supervised techniques using the exemplars and WordNet for frame identification. Hermann et al. (2014) also improve frame identification by mapping frames and predicates into the same continuous vector space, allowing statistical sharing. ilar kinds of scenarios In comparison, PropBank frames are purely lexical and there are no formal relations between different predicates or their roles. PropBank’s sense distinctions are generally coarsergrained than FrameNet’s. Moreover, FrameNet lexical entries cover many different parts of speech, while PropBank focuses on verbs an"
P15-2036,N12-1086,1,0.904091,"Missing"
P15-2036,P07-1033,0,0.0592643,"termined by frame identification). We use the heuristic procedure described by (Das et al., 2014) for extracting candidate argument spans for the predicate; call this spans(x, p, f ). spans always includes a special span denoting an empty or nonovert role, denoted ∅. For each candidate argument a ∈ spans(x, p, f ) and each role r, a binary feature vector φ (a,x, p, f ,r) is extracted. We use the feature extractors from (Das et al., 2014) as a baseline, adding additional ones in our experiments (§3.2– §3.4). Each a is given a real-valued score by a linear model: Domain Adaptation and Exemplars Daumé (2007) proposed a feature augmentation approach that is now widely used in supervised domain adaptation scenarios. We use a variant of this approach. Let Dex denote the exemplars training data, and Dft denote the full text training data. For every feature φ (a,x, p, f ,r) in the base model, we add a new feature φft (⋅) that fires only if φ (⋅) fires and x ∈ Dft . The intuition is that each base feature contributes both a “general” weight and a “domain-specific” weight to the model; thus, it can exhibit a general preference for specific roles, but this general preference can be fine-tuned for the dom"
P15-2036,S15-1005,0,0.0135716,"mance upon combining the best approaches. Both use full-text and exemplars for training; the first uses PropBank SRL as guide features, and the second adds hierarchy features. The best result is the 221 0.2 Acknowledgments 100 50 0 Test Examples 150 0.4 over, the techniques discussed here could be further explored using semi-automatic mappings between lexical resources (such as UBY; Gurevych et al., 2012), and correspondingly, this task could be used to extrinsically validate those mappings. Ours is not the only study to show benefit from heterogeneous annotations for semantic analysis tasks. Feizabadi and Padó (2015), for example, successfully applied similar techniques for SRL of implicit arguments.9 Ultimately, given the diversity of semantic resources, we expect that learning from heterogeneous annotations in different corpora will be necessary to build automatic semantic analyzers that are both accurate and robust. 0 200 400 600 800 1000 1200 1400 Frame Element, ordered by test set frequency 0.8 (a) Frequency of each role appearing in the test set. The authors are grateful to Dipanjan Das for his assistance, and to anonymous reviewers for their helpful feedback. This research has been supported by the"
P15-2036,W03-1007,0,0.023046,"annotate them only with respect to the predicate in question. These singly-annotated sentences from FrameNet are called lexicographic exemplars. There are over 140,000 sentences containing argument annotations and relative to the FT dataset, these contain an order of magnitude more frame annotations and over two orders of magnitude more sentences. As these were manually selected, the rate of overt arguments per frame is noticeably higher than in the FT data. The exemplars formed the basis of early studies of frame-semantic role labeling (e.g., Gildea and Jurafsky, 2002; Thompson et al., 2003; Fleischman et al., 2003; Litkowski, 2004; Kwon et al., 2004). Exemplars have not yet been exploited successfully to improve role labeling performance on the more realistic FT task.4 Annotations Statistics for the annotations appear in table 1. Full-text (FT): This portion of the FrameNet corpus consists of documents and has about 5,000 sentences for which annotators assigned frames 219 2.3 PropBank PropBank (PB; Palmer et al., 2005) is a lexicon and corpus of predicate–argument structures that takes a shallower approach than FrameNet. FrameNet frames cluster lexical predicates that evoke sim3 Though these were annot"
P15-2036,J02-3001,0,0.941676,"A0 want ING is evoked by want, ACTIVITY _ FINISH by Afinish, and H OLD CTIVITY_FINISH : complete.v Fra Agent Activity conclude.v finish.v … the people us to stay coursehorizontal and finish the lines job . representing m ING _e OFFreally _ ONwant by hold off.theThin _OFF_ON: hold off.v Pr Ne Agent End_point Desirable_action: ∅ HOLDING finish-v-01 A0 labeled with role names. A1wait.v t op argument spans are (Not shown: July Ba stay-v-01 nk A3 _ UNIT and fill its Unit and August evokeA1C ALENDRIC role.) Paucity of data resources is a challenge for semantic analyses like frame-semantic parsing (Gildea and Jurafsky, 2002; Das et al., 2014) using the FrameNet lexicon (Baker et al., 1998; Fillmore and Baker, 2009).1 Given a sentence, a framesemantic parse maps word tokens to frames they evoke, and for each frame, finds and labels its argument phrases with frame-specific roles. An example appears in figure 1. In this paper, we address this argument identification subtask, a form of semantic role labeling (SRL), a task introduced by Gildea and Jurafsky (2002) using an earlier version of FrameNet. Our contribution addresses the paucity of annotated data for training using standard domain adaptation techniques. We"
P15-2036,E12-1059,0,0.0207678,"Missing"
P15-2036,P14-1136,0,0.167275,"ts of documents and has about 5,000 sentences for which annotators assigned frames 219 2.3 PropBank PropBank (PB; Palmer et al., 2005) is a lexicon and corpus of predicate–argument structures that takes a shallower approach than FrameNet. FrameNet frames cluster lexical predicates that evoke sim3 Though these were annotated at the document level, and train/development/test splits are by document, the framesemantic parsing is currently restricted to the sentence level. 4 Das and Smith (2011, 2012) investigated semi-supervised techniques using the exemplars and WordNet for frame identification. Hermann et al. (2014) also improve frame identification by mapping frames and predicates into the same continuous vector space, allowing statistical sharing. ilar kinds of scenarios In comparison, PropBank frames are purely lexical and there are no formal relations between different predicates or their roles. PropBank’s sense distinctions are generally coarsergrained than FrameNet’s. Moreover, FrameNet lexical entries cover many different parts of speech, while PropBank focuses on verbs and (as of recently) eventive noun and adjective predicates. An example with PB annotations is shown in figure 2. We use the mode"
P15-2036,N06-2015,0,0.136256,"Missing"
P15-2036,S12-1016,0,0.0167416,"d T RIAL . C RIMINAL _ PROCESS .Defendant, for instance, is mapped to A RREST.Suspect, T RIAL.Defendant, and S ENTENCING.Convict. We say that a parent of a role is one that has either the Inheritance or Subframe relation to it. There are 4,138 Inheritance and 589 Subframe links among role types in FrameNet 1.5. Prior work has considered various ways of grouping role labels together in order to share statistical strength. Matsubayashi et al. (2009) observed small gains from using the Inheritance relationships and also from grouping by the role name (SEMAFOR already incorporates such features). Johansson (2012) reports improvements in SRL for Swedish, by exploiting relationships between both frames and roles. Baldewein et al. (2004) learn latent clusters of roles and role-fillers, reporting mixed results. Our approach is described in §3.2. 2.2 Sentences Frames Overt arguments Exemplars train test and arguments to as many words as possible. Beginning with the SemEval-2007 shared task on FrameNet analysis, frame-semantic parsers have been trained and evaluated on the full-text data (Baker et al., 2007; Das et al., 2014).3 The full-text documents represent a mix of genres, prominently including travel"
P15-2036,N13-1013,0,0.0347516,"r the domain. Regularization encourages the model to use the general version over the domain-specific, if possible. 3.4 Guide Features We experiment with features shared between related roles of related frames in order to capture Another approach to domain adaptation is to train a supervised model on a source domain, make predictions using that model on the target domain, then use those predictions as additional features while training a new model on the target domain. The source domain model is effectively a form of preprocessing, and the features from its output are known as guide features (Johansson, 2013; Kong et al., 2014).6 In our case, the full text data is our target domain, and PropBank and the exemplars data are our source domains, respectively. For PropBank, we run the SRL system of Illinois Curator 1.1.4 (Pun5 Recent work has improved upon global decoding techniques (Das et al., 2012; Täckström et al., 2015). We expect such improvements to be complementary to the gains due to the added features and data reported here. 6 This is related to the technique of model stacking, where successively richer models are trained by cross-validation on the same dataset (e.g., Cohen and Carvalho, 200"
P15-2036,D14-1108,1,0.685009,"ularization encourages the model to use the general version over the domain-specific, if possible. 3.4 Guide Features We experiment with features shared between related roles of related frames in order to capture Another approach to domain adaptation is to train a supervised model on a source domain, make predictions using that model on the target domain, then use those predictions as additional features while training a new model on the target domain. The source domain model is effectively a form of preprocessing, and the features from its output are known as guide features (Johansson, 2013; Kong et al., 2014).6 In our case, the full text data is our target domain, and PropBank and the exemplars data are our source domains, respectively. For PropBank, we run the SRL system of Illinois Curator 1.1.4 (Pun5 Recent work has improved upon global decoding techniques (Das et al., 2012; Täckström et al., 2015). We expect such improvements to be complementary to the gains due to the added features and data reported here. 6 This is related to the technique of model stacking, where successively richer models are trained by cross-validation on the same dataset (e.g., Cohen and Carvalho, 2005; Nivre and McDonal"
P15-2036,C04-1179,0,0.0362007,"dicate in question. These singly-annotated sentences from FrameNet are called lexicographic exemplars. There are over 140,000 sentences containing argument annotations and relative to the FT dataset, these contain an order of magnitude more frame annotations and over two orders of magnitude more sentences. As these were manually selected, the rate of overt arguments per frame is noticeably higher than in the FT data. The exemplars formed the basis of early studies of frame-semantic role labeling (e.g., Gildea and Jurafsky, 2002; Thompson et al., 2003; Fleischman et al., 2003; Litkowski, 2004; Kwon et al., 2004). Exemplars have not yet been exploited successfully to improve role labeling performance on the more realistic FT task.4 Annotations Statistics for the annotations appear in table 1. Full-text (FT): This portion of the FrameNet corpus consists of documents and has about 5,000 sentences for which annotators assigned frames 219 2.3 PropBank PropBank (PB; Palmer et al., 2005) is a lexicon and corpus of predicate–argument structures that takes a shallower approach than FrameNet. FrameNet frames cluster lexical predicates that evoke sim3 Though these were annotated at the document level, and train"
P15-2036,W04-0803,0,0.0508177,"espect to the predicate in question. These singly-annotated sentences from FrameNet are called lexicographic exemplars. There are over 140,000 sentences containing argument annotations and relative to the FT dataset, these contain an order of magnitude more frame annotations and over two orders of magnitude more sentences. As these were manually selected, the rate of overt arguments per frame is noticeably higher than in the FT data. The exemplars formed the basis of early studies of frame-semantic role labeling (e.g., Gildea and Jurafsky, 2002; Thompson et al., 2003; Fleischman et al., 2003; Litkowski, 2004; Kwon et al., 2004). Exemplars have not yet been exploited successfully to improve role labeling performance on the more realistic FT task.4 Annotations Statistics for the annotations appear in table 1. Full-text (FT): This portion of the FrameNet corpus consists of documents and has about 5,000 sentences for which annotators assigned frames 219 2.3 PropBank PropBank (PB; Palmer et al., 2005) is a lexicon and corpus of predicate–argument structures that takes a shallower approach than FrameNet. FrameNet frames cluster lexical predicates that evoke sim3 Though these were annotated at the docum"
P15-2036,D08-1017,1,0.640396,"case, the full text data is our target domain, and PropBank and the exemplars data are our source domains, respectively. For PropBank, we run the SRL system of Illinois Curator 1.1.4 (Pun5 Recent work has improved upon global decoding techniques (Das et al., 2012; Täckström et al., 2015). We expect such improvements to be complementary to the gains due to the added features and data reported here. 6 This is related to the technique of model stacking, where successively richer models are trained by cross-validation on the same dataset (e.g., Cohen and Carvalho, 2005; Nivre and McDonald, 2008; Martins et al., 2008). scorew (a ∣ x, p, f ,r) = w⊺ φ (a,x, p, f ,r) (1) The model parameters w are learned from data (§4). Prediction requires choosing a joint assignment of all arguments of a frame, respecting the constraints that a role may be assigned to at most one span, and spans of overt arguments must not overlap. Beam search, with a beam size of 100, is used to find this argmax.5 3.2 Hierarchy Features 220 yakanok et al., 2008)7 on verbs in the full-text data. For the exemplars, we train baseline SEMAFOR on the exemplars and run it on the full-text data. We use two types of guide features: one encodes the"
P15-2036,P09-1003,0,0.0182029,"ED.Wrongdoer, and so forth. Subframe: This indicates a subevent within a complex event. E.g., the C RIMINAL _ PROCESS frame groups together subframes A RREST, A RRAIGN MENT and T RIAL . C RIMINAL _ PROCESS .Defendant, for instance, is mapped to A RREST.Suspect, T RIAL.Defendant, and S ENTENCING.Convict. We say that a parent of a role is one that has either the Inheritance or Subframe relation to it. There are 4,138 Inheritance and 589 Subframe links among role types in FrameNet 1.5. Prior work has considered various ways of grouping role labels together in order to share statistical strength. Matsubayashi et al. (2009) observed small gains from using the Inheritance relationships and also from grouping by the role name (SEMAFOR already incorporates such features). Johansson (2012) reports improvements in SRL for Swedish, by exploiting relationships between both frames and roles. Baldewein et al. (2004) learn latent clusters of roles and role-fillers, reporting mixed results. Our approach is described in §3.2. 2.2 Sentences Frames Overt arguments Exemplars train test and arguments to as many words as possible. Beginning with the SemEval-2007 shared task on FrameNet analysis, frame-semantic parsers have been"
P15-2036,P08-1108,0,0.0186663,"Missing"
P15-2036,J05-1004,0,0.717357,"Missing"
P15-2036,P15-2067,0,0.0221741,"Missing"
P15-2036,J08-2005,0,0.587921,"Missing"
P15-2036,Q15-1003,0,0.23587,"domain, make predictions using that model on the target domain, then use those predictions as additional features while training a new model on the target domain. The source domain model is effectively a form of preprocessing, and the features from its output are known as guide features (Johansson, 2013; Kong et al., 2014).6 In our case, the full text data is our target domain, and PropBank and the exemplars data are our source domains, respectively. For PropBank, we run the SRL system of Illinois Curator 1.1.4 (Pun5 Recent work has improved upon global decoding techniques (Das et al., 2012; Täckström et al., 2015). We expect such improvements to be complementary to the gains due to the added features and data reported here. 6 This is related to the technique of model stacking, where successively richer models are trained by cross-validation on the same dataset (e.g., Cohen and Carvalho, 2005; Nivre and McDonald, 2008; Martins et al., 2008). scorew (a ∣ x, p, f ,r) = w⊺ φ (a,x, p, f ,r) (1) The model parameters w are learned from data (§4). Prediction requires choosing a joint assignment of all arguments of a frame, respecting the constraints that a role may be assigned to at most one span, and spans of"
P15-2036,C98-1013,0,\N,Missing
P15-2072,I13-1191,0,0.00574217,"nomic and”) which is 502 = 2500. 4 A small secondary experiment, described in supplementary material, was used to test the reliability of this process. 5 Krippendorff’s α is similar to Cohen’s κ, but calculates expected agreement between annotators based on the combined pool of labels provided by all annotators, rather than considering each annotators’s frequency of use separately. Moreover, it can be used for more than two annotators and 441 Krippendorff’s Alpha 1.0 banov et al., 2010; Sim et al., 2013; Iyyer et al., 2014), or “stance” (position for or against an issue) (Walker et al., 2012; Hasan and Ng, 2013). A related line of work is the analysis of subjective language or “scientific” language, which has also been posed in terms of framing (Wiebe et al., 2004; Choi et al., 2012). While the study of ideology, sentiment, and subjectivity are interesting in their own right, we believe that they fail to capture the more nuanced nature of framing, which is often more complex than positive or negative sentiment. In discussions of same-sex marriage, for example, both advocates and opponents may attempt to control whether the issue is perceived as primarily about politics, legality, or ethics. Moreover,"
P15-2072,P14-1105,1,0.122942,"of the non-overlapping part of the annotations (“immigration exert influence over our economic and”) which is 502 = 2500. 4 A small secondary experiment, described in supplementary material, was used to test the reliability of this process. 5 Krippendorff’s α is similar to Cohen’s κ, but calculates expected agreement between annotators based on the combined pool of labels provided by all annotators, rather than considering each annotators’s frequency of use separately. Moreover, it can be used for more than two annotators and 441 Krippendorff’s Alpha 1.0 banov et al., 2010; Sim et al., 2013; Iyyer et al., 2014), or “stance” (position for or against an issue) (Walker et al., 2012; Hasan and Ng, 2013). A related line of work is the analysis of subjective language or “scientific” language, which has also been posed in terms of framing (Wiebe et al., 2004; Choi et al., 2012). While the study of ideology, sentiment, and subjectivity are interesting in their own right, we believe that they fail to capture the more nuanced nature of framing, which is often more complex than positive or negative sentiment. In discussions of same-sex marriage, for example, both advocates and opponents may attempt to control"
P15-2072,J08-4004,0,0.126671,"wspapers (Smith et al., 2013; Smith et al., 2014). While not the same as framing, identifying this sort of text reuse is an important step towards analyzing the “media packages” that social scientists associate with framing. Immigration Smoking Same-sex marriage 0.8 0.6 0.4 0.2 0.0 Stage 1 0 10 Stage 2 20 Stage 3 30 40 Round Figure 4: Chance-corrected inter-annotator agreement on the primary frame. Marker size indicates the number of annotations being compared; α = 1 indicates perfect agreement. ric has been previously recommended for tasks in computational linguistics that involve unitizing (Artstein and Poesio, 2008). For a more complete explanation, see Krippendorff (2004). The pattern of αU values across rounds is very similar to that shown in Figure 4, but not surprisingly, average levels of agreement are much lower. Arguably, this agreement statistic is overly harsh for our purposes. We do not necessarily expect annotators to agree perfectly about where to start and end each annotated span, or how many spans to annotate per article, and our codebook and guidelines offer relatively little guidance on these lowlevel decisions. Nevertheless, it is encouraging that in all cases, average agreement is great"
P15-2072,P10-2047,0,0.0327642,"Missing"
P15-2072,N15-1171,0,0.282768,"decisions give rise to thematic sets of interrelated ideas, imagery, and arguments, which tend to cohere and persist over time. Past work on framing includes many examples of issue-specific studies based on manual content analysis (Baumgartner et al., 2008; Berinsky and Kinder, 2006). While such studies reveal much about the range of opinions on an issue, they do not characterize framing at a level of abstraction that allows comparison across social issues. More recently, there have also been a handful of papers on the computational analysis of framing (Nguyen et al., 2015; Tsur et al., 2015; Baumer et al., 2015). While these papers represent impressive advances, they are still focused on the problem of automating the analysis of framing along a single dimension, or within a particular domain. We propose that framing can be understood as a general aspect of linguistic communication about facts and opinions on any issue. Empirical assessment of this hypothesis requires analyzing framing • A degree of subjectivity in framing analysis is unavoidable. While some variation in annotations is due to mistakes and misunderstandings by annotators (and is to be minimized), much variation is due to valid differen"
P15-2072,W06-2915,0,0.018161,"Missing"
P15-2072,P15-1139,1,0.448476,"e. Theories of framing posit that these decisions give rise to thematic sets of interrelated ideas, imagery, and arguments, which tend to cohere and persist over time. Past work on framing includes many examples of issue-specific studies based on manual content analysis (Baumgartner et al., 2008; Berinsky and Kinder, 2006). While such studies reveal much about the range of opinions on an issue, they do not characterize framing at a level of abstraction that allows comparison across social issues. More recently, there have also been a handful of papers on the computational analysis of framing (Nguyen et al., 2015; Tsur et al., 2015; Baumer et al., 2015). While these papers represent impressive advances, they are still focused on the problem of automating the analysis of framing along a single dimension, or within a particular domain. We propose that framing can be understood as a general aspect of linguistic communication about facts and opinions on any issue. Empirical assessment of this hypothesis requires analyzing framing • A degree of subjectivity in framing analysis is unavoidable. While some variation in annotations is due to mistakes and misunderstandings by annotators (and is to be minimized)"
P15-2072,W12-3809,0,0.194439,"milar to Cohen’s κ, but calculates expected agreement between annotators based on the combined pool of labels provided by all annotators, rather than considering each annotators’s frequency of use separately. Moreover, it can be used for more than two annotators and 441 Krippendorff’s Alpha 1.0 banov et al., 2010; Sim et al., 2013; Iyyer et al., 2014), or “stance” (position for or against an issue) (Walker et al., 2012; Hasan and Ng, 2013). A related line of work is the analysis of subjective language or “scientific” language, which has also been posed in terms of framing (Wiebe et al., 2004; Choi et al., 2012). While the study of ideology, sentiment, and subjectivity are interesting in their own right, we believe that they fail to capture the more nuanced nature of framing, which is often more complex than positive or negative sentiment. In discussions of same-sex marriage, for example, both advocates and opponents may attempt to control whether the issue is perceived as primarily about politics, legality, or ethics. Moreover, we emphasize that framing is an important feature of even seemingly neutral or objective language. A different but equally relevant line of work has focused on text re-use. L"
P15-2072,Q14-1025,0,0.0177153,"he lengths of the parts which do not overlap.7 As with the more common α statistic, αU is a chance-corrected agreement metric scaled such that 1 represents perfect agreement and 0 represents the level of chance. This metInter-annotator Agreement Because our annotation task is complex (selecting potentially overlapping text spans and labeling them), there is no single comprehensive measure of inter-annotator agreement. The simplest aspect of the annotations to compare is the choice of primary frame, which we measure using Krippendorff’s α (Krippendorff, 2012).5 accommodates missing values. See Passonneau and Carpenter (2014) for additional details. 6 Note that this is not a controlled experiment on annotation procedures, but rather a difference observed between two stages of an evolving process. 7 For example, in the example shown in Figure 2, the amount of disagreement on the two Cultural identity annotations would be the square of the length (in characters) of the non-overlapping part of the annotations (“immigration exert influence over our economic and”) which is 502 = 2500. 4 A small secondary experiment, described in supplementary material, was used to test the reliability of this process. 5 Krippendorff’s"
P15-2072,D13-1010,1,0.765748,"th (in characters) of the non-overlapping part of the annotations (“immigration exert influence over our economic and”) which is 502 = 2500. 4 A small secondary experiment, described in supplementary material, was used to test the reliability of this process. 5 Krippendorff’s α is similar to Cohen’s κ, but calculates expected agreement between annotators based on the combined pool of labels provided by all annotators, rather than considering each annotators’s frequency of use separately. Moreover, it can be used for more than two annotators and 441 Krippendorff’s Alpha 1.0 banov et al., 2010; Sim et al., 2013; Iyyer et al., 2014), or “stance” (position for or against an issue) (Walker et al., 2012; Hasan and Ng, 2013). A related line of work is the analysis of subjective language or “scientific” language, which has also been posed in terms of framing (Wiebe et al., 2004; Choi et al., 2012). While the study of ideology, sentiment, and subjectivity are interesting in their own right, we believe that they fail to capture the more nuanced nature of framing, which is often more complex than positive or negative sentiment. In discussions of same-sex marriage, for example, both advocates and opponents ma"
P15-2072,D10-1028,1,0.895827,"Missing"
P15-2072,P15-1157,0,0.419998,"g posit that these decisions give rise to thematic sets of interrelated ideas, imagery, and arguments, which tend to cohere and persist over time. Past work on framing includes many examples of issue-specific studies based on manual content analysis (Baumgartner et al., 2008; Berinsky and Kinder, 2006). While such studies reveal much about the range of opinions on an issue, they do not characterize framing at a level of abstraction that allows comparison across social issues. More recently, there have also been a handful of papers on the computational analysis of framing (Nguyen et al., 2015; Tsur et al., 2015; Baumer et al., 2015). While these papers represent impressive advances, they are still focused on the problem of automating the analysis of framing along a single dimension, or within a particular domain. We propose that framing can be understood as a general aspect of linguistic communication about facts and opinions on any issue. Empirical assessment of this hypothesis requires analyzing framing • A degree of subjectivity in framing analysis is unavoidable. While some variation in annotations is due to mistakes and misunderstandings by annotators (and is to be minimized), much variation is"
P15-2072,N12-1072,0,0.0510175,"nfluence over our economic and”) which is 502 = 2500. 4 A small secondary experiment, described in supplementary material, was used to test the reliability of this process. 5 Krippendorff’s α is similar to Cohen’s κ, but calculates expected agreement between annotators based on the combined pool of labels provided by all annotators, rather than considering each annotators’s frequency of use separately. Moreover, it can be used for more than two annotators and 441 Krippendorff’s Alpha 1.0 banov et al., 2010; Sim et al., 2013; Iyyer et al., 2014), or “stance” (position for or against an issue) (Walker et al., 2012; Hasan and Ng, 2013). A related line of work is the analysis of subjective language or “scientific” language, which has also been posed in terms of framing (Wiebe et al., 2004; Choi et al., 2012). While the study of ideology, sentiment, and subjectivity are interesting in their own right, we believe that they fail to capture the more nuanced nature of framing, which is often more complex than positive or negative sentiment. In discussions of same-sex marriage, for example, both advocates and opponents may attempt to control whether the issue is perceived as primarily about politics, legality,"
P15-2072,J04-3002,0,0.0348045,"ippendorff’s α is similar to Cohen’s κ, but calculates expected agreement between annotators based on the combined pool of labels provided by all annotators, rather than considering each annotators’s frequency of use separately. Moreover, it can be used for more than two annotators and 441 Krippendorff’s Alpha 1.0 banov et al., 2010; Sim et al., 2013; Iyyer et al., 2014), or “stance” (position for or against an issue) (Walker et al., 2012; Hasan and Ng, 2013). A related line of work is the analysis of subjective language or “scientific” language, which has also been posed in terms of framing (Wiebe et al., 2004; Choi et al., 2012). While the study of ideology, sentiment, and subjectivity are interesting in their own right, we believe that they fail to capture the more nuanced nature of framing, which is often more complex than positive or negative sentiment. In discussions of same-sex marriage, for example, both advocates and opponents may attempt to control whether the issue is perceived as primarily about politics, legality, or ethics. Moreover, we emphasize that framing is an important feature of even seemingly neutral or objective language. A different but equally relevant line of work has focus"
P15-2072,D10-1111,0,\N,Missing
P17-1072,P15-2072,1,0.854644,"las Card† Noah A. Smith∗ ∗ Paul G. Allen School of Computer Science & Engineering † School of Computer Science University of Washington Carnegie Mellon University Seattle, WA 98195, USA Pittsburgh, PA 15213, USA chenhao@chenhaot.com dcard@cmu.edu nasmith@cs.washington.edu Abstract units that can be identified as being present or absent in a document. In this work, we consider representing ideas using keywords and topics obtained in an unsupervised fashion, but our way of characterizing the relations between ideas could be applied to many other types of textual representations, such as frames (Card et al., 2015) and hashtags. What does it mean for two ideas to compete in texts, quantitatively? Consider, for example, the issue of immigration. There are two strongly competing narratives about the roughly 11 million people1 who are residing in the United States without permission. One is “illegal aliens”, who “steal” jobs and deny opportunities to legal immigrants; the other is “undocumented immigrants”, who are already part of the fabric of society and deserve a path to citizenship (Merolla et al., 2013). Although prior knowledge suggests that these two narratives compete, it is not immediately obvious"
P17-1072,J90-1003,0,0.446356,"Missing"
P17-1072,D08-1038,0,0.0526117,"each of these ideas in comparison to machine translation is shown in in Fig. 9, which reveals additional detail. est methods” word alignment ) (#1 ace (#1 ) arms-race (#2) arms-race (#23) ead sentiment analysis Figure 8: Top relations between the topics in ACL Anthology. The top 10 words for the rule, forest methods topic are rule, grammar, derivation, span, algorithm, forest, parsing, figure, set, string. 5 We present two strands of related studies in addition to what we have discussed. Trends in ideas. Most studies have so far examined the trends of ideas individually (Michel et al., 2011; Hall et al., 2008; Rule et al., 2015). For instance, Hall et al. (2008) present various trends in our own computational linguistics community, including the rise of statistical machine translation. More recently, rhetorical framing has been used to predict these sorts of patterns (Prabhakaran et al., 2016). An exception is that Shi et al. (2010) use prevalence correlation to analyze lag relations between topics in publications and research grants. Anecdotally, Grudin (2009) observes a “head-tohead” relation between artificial intelligence and human-computer interaction in research funding. However, to our know"
P17-1072,P16-1111,0,0.0207713,"0 words for the rule, forest methods topic are rule, grammar, derivation, span, algorithm, forest, parsing, figure, set, string. 5 We present two strands of related studies in addition to what we have discussed. Trends in ideas. Most studies have so far examined the trends of ideas individually (Michel et al., 2011; Hall et al., 2008; Rule et al., 2015). For instance, Hall et al. (2008) present various trends in our own computational linguistics community, including the rise of statistical machine translation. More recently, rhetorical framing has been used to predict these sorts of patterns (Prabhakaran et al., 2016). An exception is that Shi et al. (2010) use prevalence correlation to analyze lag relations between topics in publications and research grants. Anecdotally, Grudin (2009) observes a “head-tohead” relation between artificial intelligence and human-computer interaction in research funding. However, to our knowledge, our work is the first study to systematically characterize relations between ideas. Representation of ideas. In addition to topics and keywords, studies have also sought to operationalize the “memes” metaphor using quotes and text reuse in the media (Leskovec et al., 2009; Niculae e"
P17-1072,W09-3607,0,0.0455099,"Missing"
P17-1092,P15-2072,1,0.843464,"Missing"
P17-1092,D16-1148,1,0.785607,"e Table 2. On four out of five datasets, our U NLABELED model (line 8) outperforms past methods. In the case of the very large Yelp dataset, our F ULL model (line 9) gives even stronger performance, but not elsewhere, suggesting that it is overparameterized for the smaller datasets. Indeed, on the MFC and Movies tasks, the discourse-ignorant A DDITIVE outperforms the F ULL model. On these datasets, the selected F ULL model had nearly 20 times as many parameters as the U NLABELED model, which in turn had twice as many parameters as the A DDITIVE. 1000 Method Prior work 1. Yang et al. (2016) 2. Card et al. (2016) 3. Yogatama and Smith (2014) 4. Bhatia et al. (2015) 5. Hogenboom et al. (2015) Variants of our model 6. A DDITIVE 7. ROOT 8. U NLABELED 9. F ULL Yelp MFC Debates Movies Bills 71.0 — — — — — 56.8 — — — — — 74.0 — — — — — 82.9 71.9 — — 88.5 — — 68.5 54.3 71.3 71.8 57.6 51.2 58.4 56.3 69.0 60.3 75.7 74.2 82.7 68.7 83.1 79.5 80.1 70.5 78.4 77.0 Table 2: Test-set accuracy across five datasets. Results from prior work are reprinted from the corresponding publications. Boldface marks performance stronger than the previous state of the art. This finding demonstrates the benefit of explicit discourse"
P17-1092,W01-1605,0,0.496864,"Missing"
P17-1092,P14-1002,1,0.409694,": Bhatia et al. (2015) and Hogenboom et al. (2015) applied hand-crafted weighting schemes to the sentences in a document, based on its discourse structure, and showed benefit to sentiment polarity classification. In this paper, we investigate the value of discourse structure for text categorization more broadly, considering five tasks, through the use of a recursive neural network built on an Figure 1: A manually constructed example of the RST (Mann and Thompson, 1988) discourse structure on a text. automatically-derived document parse from a topperforming, open-source discourse parser, DPLP (Ji and Eisenstein, 2014). Our models learn to weight the importance of a document’s sentences, based on their positions and relations in the discourse tree. We introduce a new, unnormalized attention mechanism to this end. Experimental results show that variants of our model outperform prior work on four out of five tasks considered. Our method unsurprisingly underperforms on the fifth task, making predictions about legislative bills—a genre in which discourse conventions are quite different from those in the discourse parser’s training data. Further experiments show the effect of discourse parse quality on text cate"
P17-1092,Q15-1024,1,0.86762,"NLABELED model still uses discourse to bias the model toward some content (that which is closer to the tree’s root). 3.4 Simpler Variants We consider two additional baselines that are even simpler. The first, ROOT, uses the discourse dependency structure only to select the root EDU, which is used to represent the entire text: vroot = eroot . No composition function is needed. This model variant is motivated by work on document summarization (Yoshida et al., 2014), where the 998 pretrained GloVe word embeddings (Pennington et al., 2014), following previous work on neural discourse processing (Ji and Eisenstein, 2015). For larger datasets, we randomly initialized word embeddings and trained them alongside other model parameters. most central EDU is used to represent the whole text. The second variant, A DDITIVE, uses all the EDUs with a simple composition function, and does not depend on discourse structure at all: 1 PN vroot = N i=1 ei , where N is the total number of EDUs. This serves as a baseline to test the benefits of discourse, controlling for other design decisions and implementation choices. Although sentence representations ei are built in a different way from the work of Yang et al. (2016), this"
P17-1092,E17-1117,1,0.560896,"Missing"
P17-1092,J93-2004,0,0.0583071,"Missing"
P17-1092,P04-1035,0,0.0116279,"Missing"
P17-1092,N16-1174,0,0.424161,"waiter serve himself many drinks.]D [He kept running into the bathroom]E [instead of grabbing our bill.]F Introduction Advances in text categorization have the potential to improve systems for analyzing sentiment, inferring authorship or author attributes, making predictions, and many more. Several past researchers have noticed that methods that reason about the relative salience or importance of passages within a text can lead to improvements (Ko et al., 2004). Latent variables (Yessenalina et al., 2010), structured-sparse regularizers (Yogatama and Smith, 2014), and neural attention models (Yang et al., 2016) have all been explored. Discourse structure, which represents the organization of a text as a tree (for an example, see Figure 1), might provide cues for the importance of different parts of a text. Some promising results on sentiment classification tasks support this idea: Bhatia et al. (2015) and Hogenboom et al. (2015) applied hand-crafted weighting schemes to the sentences in a document, based on its discourse structure, and showed benefit to sentiment polarity classification. In this paper, we investigate the value of discourse structure for text categorization more broadly, considering"
P17-1092,N12-1097,1,0.464779,"Missing"
P17-1092,D10-1102,0,0.0750954,"od was amazing] [and I was in love with the spicy pork burrito,]B [the service was really awful.]C [We watched our waiter serve himself many drinks.]D [He kept running into the bathroom]E [instead of grabbing our bill.]F Introduction Advances in text categorization have the potential to improve systems for analyzing sentiment, inferring authorship or author attributes, making predictions, and many more. Several past researchers have noticed that methods that reason about the relative salience or importance of passages within a text can lead to improvements (Ko et al., 2004). Latent variables (Yessenalina et al., 2010), structured-sparse regularizers (Yogatama and Smith, 2014), and neural attention models (Yang et al., 2016) have all been explored. Discourse structure, which represents the organization of a text as a tree (for an example, see Figure 1), might provide cues for the importance of different parts of a text. Some promising results on sentiment classification tasks support this idea: Bhatia et al. (2015) and Hogenboom et al. (2015) applied hand-crafted weighting schemes to the sentences in a document, based on its discourse structure, and showed benefit to sentiment polarity classification. In th"
P17-1092,P14-1074,1,0.749227,"ito,]B [the service was really awful.]C [We watched our waiter serve himself many drinks.]D [He kept running into the bathroom]E [instead of grabbing our bill.]F Introduction Advances in text categorization have the potential to improve systems for analyzing sentiment, inferring authorship or author attributes, making predictions, and many more. Several past researchers have noticed that methods that reason about the relative salience or importance of passages within a text can lead to improvements (Ko et al., 2004). Latent variables (Yessenalina et al., 2010), structured-sparse regularizers (Yogatama and Smith, 2014), and neural attention models (Yang et al., 2016) have all been explored. Discourse structure, which represents the organization of a text as a tree (for an example, see Figure 1), might provide cues for the importance of different parts of a text. Some promising results on sentiment classification tasks support this idea: Bhatia et al. (2015) and Hogenboom et al. (2015) applied hand-crafted weighting schemes to the sentences in a document, based on its discourse structure, and showed benefit to sentiment polarity classification. In this paper, we investigate the value of discourse structure f"
P17-1092,D14-1196,0,0.183545,"ATION relation, A is the nucleus and B is the satellite). The information embedded in RST trees has motivated many applications in NLP research, including document summarization (Marcu, 1999), argumentation mining (Azar, 1999), and sentiment analysis (Bhatia et al., 2015). In most applications, RST trees are built by automatic discourse parsing, due to the expensive cost of manual annotation. In this work, we use a state-of-the-art open-source RST-style discourse parser, DPLP (Ji and Eisenstein, 2014).2 We follow recent work that suggests transforming the RST tree into a dependency structure (Yoshida et al., 2014).3 Figure 2(a) shows the corresponding dependency structure of the RST tree in Figure 1. It is clear that C is the root of the tree, and in fact this clause summarizes the review and suffices to categorize it as negative. This dependency representation of the RST tree offers a Model Our model is a recursive neural network built on a discourse dependency tree. It includes a distributed representation computed for each EDU, and a composition function that combines EDUs and partial trees into larger trees. At the top of the tree, the representation of the complete document is used to make a categ"
P17-1092,D14-1162,0,0.113163,"ia et al. (2015) and the flat document structure used by Yang et al. (2016); the U NLABELED model still uses discourse to bias the model toward some content (that which is closer to the tree’s root). 3.4 Simpler Variants We consider two additional baselines that are even simpler. The first, ROOT, uses the discourse dependency structure only to select the root EDU, which is used to represent the entire text: vroot = eroot . No composition function is needed. This model variant is motivated by work on document summarization (Yoshida et al., 2014), where the 998 pretrained GloVe word embeddings (Pennington et al., 2014), following previous work on neural discourse processing (Ji and Eisenstein, 2015). For larger datasets, we randomly initialized word embeddings and trained them alongside other model parameters. most central EDU is used to represent the whole text. The second variant, A DDITIVE, uses all the EDUs with a simple composition function, and does not depend on discourse structure at all: 1 PN vroot = N i=1 ei , where N is the total number of EDUs. This serves as a baseline to test the benefits of discourse, controlling for other design decisions and implementation choices. Although sentence represe"
P17-1092,prasad-etal-2008-penn,0,0.0249213,"nh ei + X j∈children(i) Once we have vroot of a text, the prediction of its category is given by softmax (Wo vroot + b). We refer to this model as the F ULL model, since it makes use of the entire discourse dependency tree.  αi,j Wri,j vj  , (1) where Wri,j is a relation-specific composition matrix indexed by the relation between i and j, ri,j . αi,j is an “attention” weight, defined as   αi,j = σ e> W v (2) α j , i 3.3 Unlabeled Model The F ULL model based on Equation 1 uses a dependency discourse tree with relations. Because alternate discourse relation labels have been proposed (e.g., Prasad et al., 2008), we seek to measure the effect of these labels. We therefore consider an U NLABELED model based only on the tree structure, without the relations:   X vi = tanh ei + αi,j vj  . (3) where σ is the elementwise sigmoid and Wα contains attention parameters (these are relationindependent). Our attention mechanism differs from prior work (Bahdanau et al., 2015), in which attention weights are normalized to sum to one across competing candidates for attention. Here, αi,j does not depend on node i’s other children. This is motivated by RST, in which the presence of a node does not signify lesser"
P17-1092,D09-1026,0,0.0327395,") domain mismatch between the training corpus for a discourse parser and the domain where the discourse parser is used. For the first factor, discourse parsing is still an active research topic in NLP, and may yet improve. The second factor suggests exploring domain adaptation methods or even direct discourse annotation for genres of interest. 7 Related Work Early work on text categorization often treated text as a bag of words (e.g., Joachims, 1998; Yang and Pedersen, 1997). Representation learning, for example through matrix decomposition (Deerwester et al., 1990) or latent topic variables (Ramage et al., 2009), has been considered to avoid overfitting in the face of sparse data. The assumption that all parts of a text should influence categorization equally persists even as more powerful representation learners are considered. Zhang et al. (2015) treat a text as a sequence of characters, proposing to a deep convolutional neural network to build text representation. Xiao and Cho (2016) extended that architecture by inserting a recurrent neural network layer between the convolutional layer and the classification layer. In contrast, our contributions follow Ko et al. (2004), who sought to weight the i"
P17-1092,Q14-1017,0,0.0114065,"review and suffices to categorize it as negative. This dependency representation of the RST tree offers a Model Our model is a recursive neural network built on a discourse dependency tree. It includes a distributed representation computed for each EDU, and a composition function that combines EDUs and partial trees into larger trees. At the top of the tree, the representation of the complete document is used to make a categorization decision. Our approach is analogous to (and inspired by) the use of recursive neural networks on syntactic dependency trees, with word embeddings at the leaves (Socher et al., 2014). 3.1 Representation of Sentences Let e be the distributed representation of an EDU. We use a bidirectional LSTM on the words’ embeddings within each EDU (details of word embeddings are given in section 4), concatenating the last hidden state vector from the forward LSTM − −) to get e. (→ e ) with that of the backward LSTM (← e There is extensive recent work on architectures for embedding representations of sentences and other short pieces of text, including, for example, (bi)recursive neural networks (Paulus et al., 2014) and convolutional neural networks (Kalchbrenner et al., 2014). Future w"
P17-1092,W06-1639,0,0.0252515,"Missing"
P17-1092,W03-3023,0,0.0286397,"ition function, we seek for (i.) the contribution of the parent node ei to be central; and (ii.) the contribution of each child node ej be determined by its content as well as the discourse relation it holds with the parent. We therefore define 1 There are also a few exceptions in which a relation can be realized with multiple nuclei. 2 https://github.com/jiyfeng/DPLP 3 The transformation is trivial and deterministic given the nucleus-satellite mapping for each relation. The procedure is analogous to the transformation of a headed phrase-structure parse in syntax into a dependency tree (e.g., Yamada and Matsumoto, 2003). 997 C Cont. Exp. A tanh(eC + Exp. D Elab. WC,A E Cont. B F tanh(eA + αA,B WA,B vB ) P j∈{A,D,E} αC,j WC,j vj ) WC,E WC,D tanh(eE + αF,E WF,E vF ) tanh(eD ) WA,B WF,E tanh(eB ) (a) dependency structure tanh(eF ) (b) recursive neural network structure Figure 2: The dependency discourse tree derived from the example RST tree in Figure 1 (a) and the corresponding recursive neural network model on the tree (b).  vi = tanh ei + X j∈children(i) Once we have vroot of a text, the prediction of its category is given by softmax (Wo vroot + b). We refer to this model as the F ULL model, since it makes"
P17-1186,S15-2162,0,0.371028,"λ 1 X 2 min kΘk + L xi , yi ; Θ , Θ 2 N (6) i=1 where Θ is all parameters in the model, and L is the structured hinge loss:     L xi , yi ; Θ = max S xi , y + c y, yi y∈Y(xi ) (7)  − S xi , yi . c is a weighted Hamming distance that trades off between precision and recall (Taskar et al., 2004). Following Martins and Almeida (2014), we encourage recall over precision by using the costs 0.6 for false negative arc predictions and 0.4 for false positives. 3.4 Experiments We evaluate our basic model on the English dataset from SemEval 2015 Task 18 closed track.3 We split as in previous work (Almeida and Martins, 2015; Du et al., 2015), resulting in 33,964 training sentences from §00–19 of the WSJ corpus, 1,692 development sentences from §20, 1,410 sentences from §21 as in-domain test data, and 1,849 sentences sampled from the Brown Corpus as out-of-domain test data. The closed track differs from the open and gold tracks in that it does not allow access to any syntactic analyses. In the open track, additional machine generated syntactic parses are provided, while the gold-track gives access to various goldstandard syntactic analyses. Our model is evaluated with closed track data; it does not have access to"
P17-1186,Q16-1031,1,0.814124,"2014; Taub-Tabib et al., 2015; Pei et al., 2015, inter alia). Our work is perhaps closest to those who used BiLSTMs to encode inputs (Kiperwasser and Goldberg, 2016; Kuncoro et al., 2016; Wang and Chang, 2016; Dozat and Manning, 2017; Ma and Hovy, 2016). Multitask learning in NLP. There have been many efforts in NLP to use joint learning to replace pipelines, motivated by concerns about cascading errors. Collobert and Weston (2008) proposed sharing the same word representation while solving multiple NLP tasks. Zhang and Weiss (2016) use a continuous stacking model for POS tagging and parsing. Ammar et al. (2016) and Guo et al. (2016) explored parameter sharing for multilingual parsing. Johansson (2013) and Kshirsagar et al. (2015) applied ideas from domain adaptation to multitask learning. Successes in multitask learning have been enabled by advances in representation learning as well as earlier explorations of parameter sharing (Ando and Zhang, 2005; Blitzer et al., 2006; Daum´e III, 2007). 6 Conclusion We showed two orthogonal ways to apply deep multitask learning to graph-based parsing. The first shares parameters when encoding tokens in the input with recurrent neural networks, and the second int"
P17-1186,S07-1018,0,0.0118772,"//github.com/ Noahs-ARK/NeurboParser. 1 _and_c arg1 arg2 arg1 poss Last week , shareholders took their money and ran . (a) DM top arg1 arg1 coord arg1 arg2 arg1 arg1 coord Last week , shareholders took their money and ran . (b) PAS top twhen act twhen rstr top conj pat act conj app Last week , shareholders took their money and ran . (c) PSD Figure 1: An example sentence annotated with the three semantic formalisms of the broad-coverage semantic dependency parsing shared tasks. Introduction Labeled directed graphs are a natural and flexible representation for semantics (Copestake et al., 2005; Baker et al., 2007; Surdeanu et al., 2008; Banarescu et al., 2013, inter alia). Their generality over trees, for instance, allows them to represent relational semantics while handling phenomena like coreference and coordination. Even syntactic formalisms are moving toward graphs (de Marneffe et al., 2014). However, full semantic graphs can be expensive to annotate, and efforts are fragmented across competing semantic theories, leading to a limited number of annotations in any one formalism. This makes learning to parse more difficult, especially for powerful but data-hungry machine learning techniques like neur"
P17-1186,W13-2322,0,0.599748,"d_c arg1 arg2 arg1 poss Last week , shareholders took their money and ran . (a) DM top arg1 arg1 coord arg1 arg2 arg1 arg1 coord Last week , shareholders took their money and ran . (b) PAS top twhen act twhen rstr top conj pat act conj app Last week , shareholders took their money and ran . (c) PSD Figure 1: An example sentence annotated with the three semantic formalisms of the broad-coverage semantic dependency parsing shared tasks. Introduction Labeled directed graphs are a natural and flexible representation for semantics (Copestake et al., 2005; Baker et al., 2007; Surdeanu et al., 2008; Banarescu et al., 2013, inter alia). Their generality over trees, for instance, allows them to represent relational semantics while handling phenomena like coreference and coordination. Even syntactic formalisms are moving toward graphs (de Marneffe et al., 2014). However, full semantic graphs can be expensive to annotate, and efforts are fragmented across competing semantic theories, leading to a limited number of annotations in any one formalism. This makes learning to parse more difficult, especially for powerful but data-hungry machine learning techniques like neural networks. In this work, we hypothesize that"
P17-1186,W06-1615,0,0.109482,"vated by concerns about cascading errors. Collobert and Weston (2008) proposed sharing the same word representation while solving multiple NLP tasks. Zhang and Weiss (2016) use a continuous stacking model for POS tagging and parsing. Ammar et al. (2016) and Guo et al. (2016) explored parameter sharing for multilingual parsing. Johansson (2013) and Kshirsagar et al. (2015) applied ideas from domain adaptation to multitask learning. Successes in multitask learning have been enabled by advances in representation learning as well as earlier explorations of parameter sharing (Ando and Zhang, 2005; Blitzer et al., 2006; Daum´e III, 2007). 6 Conclusion We showed two orthogonal ways to apply deep multitask learning to graph-based parsing. The first shares parameters when encoding tokens in the input with recurrent neural networks, and the second introduces interactions between output structures across formalisms. Without using syntactic parsing, these approaches outperform even state-of-the-art semantic dependency parsing systems that use syntax. Because our techniques apply to labeled directed graphs in general, they can easily be extended to incorporate more formalisms, semantic or otherwise. In future work"
P17-1186,D07-1101,0,0.0344661,"alogous. The output representations {ψ (t) } remain task-specific, and the score is still the inner product between the input representation and the output representation. The second variant, which we call SHARED, e and doesn’t use uses only the shared encoder h, (t) task-specific encoders {h }. It can be understood as a special case of FREDA where the dimensions of the task-specific encoders are 0. 4.3 Multitask SDP with Cross-Task Structures In syntactic parsing, higher-order structures have commonly been used to model interactions between multiple adjacent arcs in the same dependency tree (Carreras, 2007; Smith and Eisner, 2008; Martins et al., 2009; Zhang et al., 2014, inter alia). Llu´ıs et al. (2013), in contrast, used second-order structures to jointly model syntactic dependencies and semantic roles. Similarly, we use higher-order structures across tasks instead of within tasks. In this work, we look at interactions between arcs that share the same head and modifier.5 See Figures 2b and 2c for examples of higher-order cross-task structures. Higher-order structure scoring. Borrowing from Lei et al. (2014), we introduce a low-rank tensor scoring strategy that, given a higher-order structure"
P17-1186,copestake-flickinger-2000-open,0,0.263716,"cates (Palmer et al., 2005). Semantic dependency graphs also tend to have higher levels of nonprojectivity than syntactic trees (Oepen et al., 2014). Sentences with graphs containing cycles have been removed from the dataset by the organizers, so all remaining graphs are directed acyclic graphs. Table 1 summarizes some of the dataset’s high-level statistics. Formalisms. Following the SemEval shared tasks, we consider three formalisms. The DM (DELPH-IN MRS) representation comes from DeepBank (Flickinger et al., 2012), which are manually-corrected parses from the LinGO English Resource Grammar (Copestake and Flickinger, 2000). LinGO is a head-driven phrase structure grammar (HPSG; Pollard and Sag, 1994) with minimal recursion semantics (Copestake et al., 2005). The PAS (Predicate-Argument Structures) representation is extracted from the Enju Treebank, which consists of automatic parses from the Enju HPSG parser (Miyao, 2006). PAS annotations are also available for the Penn Chinese Treebank (Xue et al., 2005). The PSD (Prague Semantic Dependencies) representation is extracted from the tectogrammatical layer of the Prague Czech-English Dependency Treebank (Hajiˇc et al., 2012). PSD annotations are also available for"
P17-1186,P07-1033,0,0.0953949,"Missing"
P17-1186,de-marneffe-etal-2014-universal,0,0.0177931,"Missing"
P17-1186,S14-2080,0,0.0796542,"Missing"
P17-1186,S15-2154,0,0.0238899,"i ; Θ , Θ 2 N (6) i=1 where Θ is all parameters in the model, and L is the structured hinge loss:     L xi , yi ; Θ = max S xi , y + c y, yi y∈Y(xi ) (7)  − S xi , yi . c is a weighted Hamming distance that trades off between precision and recall (Taskar et al., 2004). Following Martins and Almeida (2014), we encourage recall over precision by using the costs 0.6 for false negative arc predictions and 0.4 for false positives. 3.4 Experiments We evaluate our basic model on the English dataset from SemEval 2015 Task 18 closed track.3 We split as in previous work (Almeida and Martins, 2015; Du et al., 2015), resulting in 33,964 training sentences from §00–19 of the WSJ corpus, 1,692 development sentences from §20, 1,410 sentences from §21 as in-domain test data, and 1,849 sentences sampled from the Brown Corpus as out-of-domain test data. The closed track differs from the open and gold tracks in that it does not allow access to any syntactic analyses. In the open track, additional machine generated syntactic parses are provided, while the gold-track gives access to various goldstandard syntactic analyses. Our model is evaluated with closed track data; it does not have access to any syntactic ana"
P17-1186,D15-1112,0,0.0810732,"Missing"
P17-1186,P14-1134,1,0.959913,"aning representation (AMR; Banarescu et al., 2013). They abstract over different syntactic realizations of the same or similar meaning (e.g., “She gave me the ball.” vs. “She gave the ball to me.”). Conversely, they attempt to distinguish between different senses even when realized in similar syntactic forms (e.g., “I baked in the kitchen.” vs. “I baked in the sun.”). Structurally, they are labeled directed graphs whose vertices are tokens in the sentence. This is in contrast to AMR whose vertices are abstract concepts, with no explicit alignment to tokens, which makes parsing more difficult (Flanigan et al., 2014). Their arc labels encode broadlyapplicable semantic relations rather than being tailored to any specific downstream application or ontology.1 They are not necessarily trees, because a token may be an argument of more than one predicate (e.g., in “John wants to eat,” John is both the wanter and the would-be eater). Their analyses may optionally leave out non–contentbearing tokens, such as punctuation or the infinitival “to,” or prepositions that simply mark the type of relation holding between other words. But when restricted to content-bearing tokens (including adjectives, adverbs, etc.), the"
P17-1186,J02-3001,0,0.119068,"al 2014 shared task (Oepen et al., 2014), and then extended by Oepen et al. (2015), the broad-coverage semantic depency parsing (SDP) task is centered around three semantic formalisms whose annotations have been converted into bilexical dependencies. See Figure 1 for an example. The formalisms come from varied linguistic traditions, but all three aim to capture predicate-argument relations between content-bearing words in a sentence. While at first glance similar to syntactic dependencies, semantic dependencies have distinct goals and characteristics, more akin to semantic role labeling (SRL; Gildea and Jurafsky, 2002) or the abstract meaning representation (AMR; Banarescu et al., 2013). They abstract over different syntactic realizations of the same or similar meaning (e.g., “She gave me the ball.” vs. “She gave the ball to me.”). Conversely, they attempt to distinguish between different senses even when realized in similar syntactic forms (e.g., “I baked in the kitchen.” vs. “I baked in the sun.”). Structurally, they are labeled directed graphs whose vertices are tokens in the sentence. This is in contrast to AMR whose vertices are abstract concepts, with no explicit alignment to tokens, which makes parsi"
P17-1186,C16-1002,0,0.0303248,"2015; Pei et al., 2015, inter alia). Our work is perhaps closest to those who used BiLSTMs to encode inputs (Kiperwasser and Goldberg, 2016; Kuncoro et al., 2016; Wang and Chang, 2016; Dozat and Manning, 2017; Ma and Hovy, 2016). Multitask learning in NLP. There have been many efforts in NLP to use joint learning to replace pipelines, motivated by concerns about cascading errors. Collobert and Weston (2008) proposed sharing the same word representation while solving multiple NLP tasks. Zhang and Weiss (2016) use a continuous stacking model for POS tagging and parsing. Ammar et al. (2016) and Guo et al. (2016) explored parameter sharing for multilingual parsing. Johansson (2013) and Kshirsagar et al. (2015) applied ideas from domain adaptation to multitask learning. Successes in multitask learning have been enabled by advances in representation learning as well as earlier explorations of parameter sharing (Ando and Zhang, 2005; Blitzer et al., 2006; Daum´e III, 2007). 6 Conclusion We showed two orthogonal ways to apply deep multitask learning to graph-based parsing. The first shares parameters when encoding tokens in the input with recurrent neural networks, and the second introduces interactions b"
P17-1186,J13-4006,0,0.01048,"test set. Table 4: The last columns show the micro-average over the three tasks. † denotes the use of syntactic parses. Bold font indicates best performance among all systems, and underlines indicate statistical significance with Bonferroni correction compared to A&M, 2015 (open), the strongest baseline system. try, which is consistent with the extensive evaluation by Zhang et al. (2016), but we leave the incorporation of syntactic trees to future work. Syntactic parsing could be treated as yet another output task, as explored in Llu´ıs et al. (2013) and in the transition-based frameworks of Henderson et al. (2013) and Swayamdipta et al. (2016). Effects of structural overlap. We hypothesized that the overlap between formalisms would enable multitask learning to be effective; in this section we investigate in more detail how structural overlap affected performance. By looking at undirected overlap between unlabeled arcs, we discover that modeling only arcs in the same direction may have been a design mistake. DM and PAS are more structurally similar to each other than either is to PSD. Table 5 compares the structural similarities between the three for2044 Undirected DM PAS PSD Directed DM PAS PSD DM PAS"
P17-1186,P14-1136,0,0.0169374,"he given sentence. The figure depicts single-layer BiLSTM and MLPs, while in practice we use two layers for both. For unlabeled arc and labeled arc structures, it depends on both the head and the modifier (but not the label, which is captured in the distributed output representation):    φ(i→j) = tanh CUA hi ; hj + bUA , (3b)    ` φ(i → j) = tanh CLA hi ; hj + bLA . (3c) Distributed output representations. NLP researchers have found that embedding discrete output labels into a low dimensional real space is an effective way to capture commonalities among them (Srikumar and Manning, 2014; Hermann et al., 2014; FitzGerald et al., 2015, inter alia). In neural language models (Bengio et al., 2003; Mnih and Hinton, 2007, inter alia) the weights of the output layer could also be regarded as an output embedding. We associate each first-order structure p with a d-dimensional real vector ψ(p) which does not depend on particular words in p. Predicates and unlabeled arcs are each mapped to a single vector: ψ(i→·) = ψ pred , (4a) ψ(i→j) = ψ UA , (4b) and each label gets a vector: ψ(i → j) = ψ LA (`). (4c) s(p) = φ(p) · ψ(p). (5) Scoring. Finally, we use an inner product to score first-order structures: 2 For"
P17-1186,P15-1162,0,0.0661842,"Missing"
P17-1186,N13-1013,0,0.0184244,"ose who used BiLSTMs to encode inputs (Kiperwasser and Goldberg, 2016; Kuncoro et al., 2016; Wang and Chang, 2016; Dozat and Manning, 2017; Ma and Hovy, 2016). Multitask learning in NLP. There have been many efforts in NLP to use joint learning to replace pipelines, motivated by concerns about cascading errors. Collobert and Weston (2008) proposed sharing the same word representation while solving multiple NLP tasks. Zhang and Weiss (2016) use a continuous stacking model for POS tagging and parsing. Ammar et al. (2016) and Guo et al. (2016) explored parameter sharing for multilingual parsing. Johansson (2013) and Kshirsagar et al. (2015) applied ideas from domain adaptation to multitask learning. Successes in multitask learning have been enabled by advances in representation learning as well as earlier explorations of parameter sharing (Ando and Zhang, 2005; Blitzer et al., 2006; Daum´e III, 2007). 6 Conclusion We showed two orthogonal ways to apply deep multitask learning to graph-based parsing. The first shares parameters when encoding tokens in the input with recurrent neural networks, and the second introduces interactions between output structures across formalisms. Without using syntactic pa"
P17-1186,S15-2161,0,0.0852605,"Missing"
P17-1186,P14-1130,0,0.0707597,"n used to model interactions between multiple adjacent arcs in the same dependency tree (Carreras, 2007; Smith and Eisner, 2008; Martins et al., 2009; Zhang et al., 2014, inter alia). Llu´ıs et al. (2013), in contrast, used second-order structures to jointly model syntactic dependencies and semantic roles. Similarly, we use higher-order structures across tasks instead of within tasks. In this work, we look at interactions between arcs that share the same head and modifier.5 See Figures 2b and 2c for examples of higher-order cross-task structures. Higher-order structure scoring. Borrowing from Lei et al. (2014), we introduce a low-rank tensor scoring strategy that, given a higher-order structure p, models interactions between the firstorder structures (i.e., arcs) p is made up of. This approach builds on and extends the parameter sharing techniques in §4.2. It can either follow FREDA or SHARED to get the input representations for first-order structures. We first introduce basic tensor notation. The order of a tensor is the number of its dimensions. The outer product of two vectors forms a secondorder tensor (matrix) where [u ⊗ v]i,j = ui vj . We denote the inner product of two tensors of the same di"
P17-1186,Q13-1018,0,0.228499,"Missing"
P17-1186,S14-2082,0,0.152086,"or examples of local structures. s is a parameterized function, whose parameters (denoted Θ and suppressed here for clarity) will be learned from the training data (§3.3). Since we search over every possible labeled graph (i.e., considering each labeled arc for each pair of words), our approach can be considered a graph-based (or all-pairs) method. The models presented in this work all share this common graph-based approach, differing only in the set of structures they score and in the parameterization of the scoring function s. This approach also underlies state-of-the-art approaches to SDP (Martins and Almeida, 2014). Basic Model Our basic model is inspired by recent successes in neural arc-factored graph-based dependency parsing (Kiperwasser and Goldberg, 2016; Dozat and Manning, 2017; Kuncoro et al., 2016). It borrows heavily from the neural arc-scoring architectures in those works, but decodes with a different algorithm under slightly different constraints. 3.2.1 Basic Structures Our basic model factors over three types of structures (p in Equation 2): • predicate, indicating a predicate word, denoted i→·; • unlabeled arc, representing the existence of an arc from a predicate to an argument, denoted i→"
P17-1186,P13-2109,1,0.896571,"Missing"
P17-1186,P09-1039,1,0.535066,"t) } remain task-specific, and the score is still the inner product between the input representation and the output representation. The second variant, which we call SHARED, e and doesn’t use uses only the shared encoder h, (t) task-specific encoders {h }. It can be understood as a special case of FREDA where the dimensions of the task-specific encoders are 0. 4.3 Multitask SDP with Cross-Task Structures In syntactic parsing, higher-order structures have commonly been used to model interactions between multiple adjacent arcs in the same dependency tree (Carreras, 2007; Smith and Eisner, 2008; Martins et al., 2009; Zhang et al., 2014, inter alia). Llu´ıs et al. (2013), in contrast, used second-order structures to jointly model syntactic dependencies and semantic roles. Similarly, we use higher-order structures across tasks instead of within tasks. In this work, we look at interactions between arcs that share the same head and modifier.5 See Figures 2b and 2c for examples of higher-order cross-task structures. Higher-order structure scoring. Borrowing from Lei et al. (2014), we introduce a low-rank tensor scoring strategy that, given a higher-order structure p, models interactions between the firstorder"
P17-1186,C16-1038,0,0.0213401,"ameter Sharing A common approach when using BiLSTMs for multitask learning is to share the BiLSTM part of the model across tasks, while training specialized classifiers for each task (Søgaard and Goldberg, 2016). In this spirit, we let each task keep its own specialized MLPs, and explore two variants of our model that share parameters at the BiLSTM level. The first variant consists of a set of task-specific BiLSTM encoders as well as a common one that is shared across all tasks. We denote it FREDA. FREDA uses a neural generalization of “frustratingly easy” domain adaptation (Daum´e III, 2007; Kim et al., 2016), where one augments domainspecific features with a shared set of features to capture global patterns. Formally, let {h(t) }t∈T denote the three task-specific encoders. We introe that is shared across all duce another encoder h tasks. Then a new set of input functions {φ(t) }t∈T can be defined as in Equations 3a–3c, for example: ` (t)  (t) (t) φ(t) (i → j) = tanh CLA hi ; hj ;   (8) ei; h e j + b(t) . h LA The predicate and unlabeled arc versions are analogous. The output representations {ψ (t) } remain task-specific, and the score is still the inner product between the input representation"
P17-1186,D11-1022,1,0.934686,"Missing"
P17-1186,P05-1012,0,0.195432,"om “week” to “Last.” We can compare FREDA 3 to FREDA 1 to isolate the effect of modeling higher-order structures. Table 6 shows performance on the development data in both unlabeled and labeled F1 . We can see that FREDA 3’s unlabeled performance improves on DM and PAS, but degrades on PSD. This supports our hypothesis, and suggests that in future work, a more careful selection of structures to model might lead to further improvements. 5 Related Work We note two important strands of related work. Graph-based parsing. Graph-based parsing was originally invented to handle non-projective syntax (McDonald et al., 2005; Koo et al., 2010; Martins et al., 2013, inter alia), but has been adapted to semantic parsing (Flanigan et al., 2014; Martins and Almeida, 2014; Thomson et al., 2014; Kuhlmann, 2014, inter alia). Local structure scoring was traditionally done with linear models over hand-engineered features, but lately, various forms of representation learning have been explored to learn feature combinations (Lei et al., 2014; Taub-Tabib et al., 2015; Pei et al., 2015, inter alia). Our work is perhaps closest to those who used BiLSTMs to encode inputs (Kiperwasser and Goldberg, 2016; Kuncoro et al., 2016; Wa"
P17-1186,Q16-1023,0,0.258747,"om the training data (§3.3). Since we search over every possible labeled graph (i.e., considering each labeled arc for each pair of words), our approach can be considered a graph-based (or all-pairs) method. The models presented in this work all share this common graph-based approach, differing only in the set of structures they score and in the parameterization of the scoring function s. This approach also underlies state-of-the-art approaches to SDP (Martins and Almeida, 2014). Basic Model Our basic model is inspired by recent successes in neural arc-factored graph-based dependency parsing (Kiperwasser and Goldberg, 2016; Dozat and Manning, 2017; Kuncoro et al., 2016). It borrows heavily from the neural arc-scoring architectures in those works, but decodes with a different algorithm under slightly different constraints. 3.2.1 Basic Structures Our basic model factors over three types of structures (p in Equation 2): • predicate, indicating a predicate word, denoted i→·; • unlabeled arc, representing the existence of an arc from a predicate to an argument, denoted i→j; • labeled arc, an arc labeled with a semantic ` role, denoted i → j. Here i and j are word indices in a given sentence, and ` indicates the arc"
P17-1186,D10-1125,0,0.0133576,"can compare FREDA 3 to FREDA 1 to isolate the effect of modeling higher-order structures. Table 6 shows performance on the development data in both unlabeled and labeled F1 . We can see that FREDA 3’s unlabeled performance improves on DM and PAS, but degrades on PSD. This supports our hypothesis, and suggests that in future work, a more careful selection of structures to model might lead to further improvements. 5 Related Work We note two important strands of related work. Graph-based parsing. Graph-based parsing was originally invented to handle non-projective syntax (McDonald et al., 2005; Koo et al., 2010; Martins et al., 2013, inter alia), but has been adapted to semantic parsing (Flanigan et al., 2014; Martins and Almeida, 2014; Thomson et al., 2014; Kuhlmann, 2014, inter alia). Local structure scoring was traditionally done with linear models over hand-engineered features, but lately, various forms of representation learning have been explored to learn feature combinations (Lei et al., 2014; Taub-Tabib et al., 2015; Pei et al., 2015, inter alia). Our work is perhaps closest to those who used BiLSTMs to encode inputs (Kiperwasser and Goldberg, 2016; Kuncoro et al., 2016; Wang and Chang, 2016"
P17-1186,P15-2036,1,0.740334,"to encode inputs (Kiperwasser and Goldberg, 2016; Kuncoro et al., 2016; Wang and Chang, 2016; Dozat and Manning, 2017; Ma and Hovy, 2016). Multitask learning in NLP. There have been many efforts in NLP to use joint learning to replace pipelines, motivated by concerns about cascading errors. Collobert and Weston (2008) proposed sharing the same word representation while solving multiple NLP tasks. Zhang and Weiss (2016) use a continuous stacking model for POS tagging and parsing. Ammar et al. (2016) and Guo et al. (2016) explored parameter sharing for multilingual parsing. Johansson (2013) and Kshirsagar et al. (2015) applied ideas from domain adaptation to multitask learning. Successes in multitask learning have been enabled by advances in representation learning as well as earlier explorations of parameter sharing (Ando and Zhang, 2005; Blitzer et al., 2006; Daum´e III, 2007). 6 Conclusion We showed two orthogonal ways to apply deep multitask learning to graph-based parsing. The first shares parameters when encoding tokens in the input with recurrent neural networks, and the second introduces interactions between output structures across formalisms. Without using syntactic parsing, these approaches outpe"
P17-1186,S14-2068,0,0.0118126,"led F1 . We can see that FREDA 3’s unlabeled performance improves on DM and PAS, but degrades on PSD. This supports our hypothesis, and suggests that in future work, a more careful selection of structures to model might lead to further improvements. 5 Related Work We note two important strands of related work. Graph-based parsing. Graph-based parsing was originally invented to handle non-projective syntax (McDonald et al., 2005; Koo et al., 2010; Martins et al., 2013, inter alia), but has been adapted to semantic parsing (Flanigan et al., 2014; Martins and Almeida, 2014; Thomson et al., 2014; Kuhlmann, 2014, inter alia). Local structure scoring was traditionally done with linear models over hand-engineered features, but lately, various forms of representation learning have been explored to learn feature combinations (Lei et al., 2014; Taub-Tabib et al., 2015; Pei et al., 2015, inter alia). Our work is perhaps closest to those who used BiLSTMs to encode inputs (Kiperwasser and Goldberg, 2016; Kuncoro et al., 2016; Wang and Chang, 2016; Dozat and Manning, 2017; Ma and Hovy, 2016). Multitask learning in NLP. There have been many efforts in NLP to use joint learning to replace pipelines, motivated b"
P17-1186,D16-1180,1,0.929424,"ossible labeled graph (i.e., considering each labeled arc for each pair of words), our approach can be considered a graph-based (or all-pairs) method. The models presented in this work all share this common graph-based approach, differing only in the set of structures they score and in the parameterization of the scoring function s. This approach also underlies state-of-the-art approaches to SDP (Martins and Almeida, 2014). Basic Model Our basic model is inspired by recent successes in neural arc-factored graph-based dependency parsing (Kiperwasser and Goldberg, 2016; Dozat and Manning, 2017; Kuncoro et al., 2016). It borrows heavily from the neural arc-scoring architectures in those works, but decodes with a different algorithm under slightly different constraints. 3.2.1 Basic Structures Our basic model factors over three types of structures (p in Equation 2): • predicate, indicating a predicate word, denoted i→·; • unlabeled arc, representing the existence of an arc from a predicate to an argument, denoted i→j; • labeled arc, an arc labeled with a semantic ` role, denoted i → j. Here i and j are word indices in a given sentence, and ` indicates the arc label. This list corresponds to the most basic s"
P17-1186,D13-1161,0,0.0273124,"ue Czech-English Dependency Treebank (Hajiˇc et al., 2012). PSD annotations are also available for a Czech translation of the WSJ Corpus. In this work, we train and evaluate only on English annotations. Of the three, PAS follows syntax most closely, and prior work has found it the easiest to predict. PSD has the largest set of labels, and parsers 1 This may make another disambiguation step necessary to use these representations in a downstream task, but there is evidence that modeling semantic composition separately from grounding in any ontology is an effective way to achieve broad coverage (Kwiatkowski et al., 2013). 2038 arg1 arg1 3.2 act shareholders took shareholders took act arg1 arg1 arg1 act act shareholders took shareholders took shareholders took (a) First-order. (b) Second-order. (c) Third-order. Figure 2: Examples of local structures. We refer to the number of arcs that a structure contains as its order. have significantly lower performance on it (Oepen et al., 2015). 3 Single-Task SDP Here we introduce our basic model, in which training and prediction for each formalism is kept completely separate. We also lay out basic notation, which will be reused for our multitask extensions. 3.1 Problem F"
P17-1186,S15-2153,0,0.402955,"Missing"
P17-1186,S14-2008,0,0.0468554,"Missing"
P17-1186,J05-1004,0,0.267745,"token may be an argument of more than one predicate (e.g., in “John wants to eat,” John is both the wanter and the would-be eater). Their analyses may optionally leave out non–contentbearing tokens, such as punctuation or the infinitival “to,” or prepositions that simply mark the type of relation holding between other words. But when restricted to content-bearing tokens (including adjectives, adverbs, etc.), the subgraph is connected. In this sense, SDP provides a whole-sentence analysis. This is in contrast to PropBank-style SRL, which gives an analysis of only verbal and nominal predicates (Palmer et al., 2005). Semantic dependency graphs also tend to have higher levels of nonprojectivity than syntactic trees (Oepen et al., 2014). Sentences with graphs containing cycles have been removed from the dataset by the organizers, so all remaining graphs are directed acyclic graphs. Table 1 summarizes some of the dataset’s high-level statistics. Formalisms. Following the SemEval shared tasks, we consider three formalisms. The DM (DELPH-IN MRS) representation comes from DeepBank (Flickinger et al., 2012), which are manually-corrected parses from the LinGO English Resource Grammar (Copestake and Flickinger, 2"
P17-1186,P15-1031,0,0.0130332,"two important strands of related work. Graph-based parsing. Graph-based parsing was originally invented to handle non-projective syntax (McDonald et al., 2005; Koo et al., 2010; Martins et al., 2013, inter alia), but has been adapted to semantic parsing (Flanigan et al., 2014; Martins and Almeida, 2014; Thomson et al., 2014; Kuhlmann, 2014, inter alia). Local structure scoring was traditionally done with linear models over hand-engineered features, but lately, various forms of representation learning have been explored to learn feature combinations (Lei et al., 2014; Taub-Tabib et al., 2015; Pei et al., 2015, inter alia). Our work is perhaps closest to those who used BiLSTMs to encode inputs (Kiperwasser and Goldberg, 2016; Kuncoro et al., 2016; Wang and Chang, 2016; Dozat and Manning, 2017; Ma and Hovy, 2016). Multitask learning in NLP. There have been many efforts in NLP to use joint learning to replace pipelines, motivated by concerns about cascading errors. Collobert and Weston (2008) proposed sharing the same word representation while solving multiple NLP tasks. Zhang and Weiss (2016) use a continuous stacking model for POS tagging and parsing. Ammar et al. (2016) and Guo et al. (2016) explo"
P17-1186,D14-1162,0,0.0823774,".4 Implementation Details Each input token is mapped to a concatenation of three real vectors: a pre-trained word vector; a randomly-initialized word vector; and a randomlyinitialized POS tag vector.8 All three are updated Hyperparameter Pre-trained word embedding dimension Randomly-initialized word embedding dimension POS tag embedding dimension Dimensions of representations φ and ψ MLP layers BiLSTM layers BiLSTM dimensions Rank of tensor r α for word dropout Value 100 25 25 100 2 2 200 100 0.25 Table 3: Hyperparameters used in the experiments. during training. We use 100-dimensional GloVe (Pennington et al., 2014) vectors trained over Wikipedia and Gigaword as pre-trained word embeddings. To deal with out-of-vocabulary words, we apply word dropout (Iyyer et al., 2015) and randomly replace a word w with a special unkα symbol with probability 1+#(w) , where #(w) is the count of w in the training set. Models are trained for up to 30 epochs with Adam (Kingma and Ba, 2015), with β1 = β2 = 0.9, and initial learning rate η0 = 10−3 . The learning rate η is annealed at a rate of 0.5 every 10 epochs (Dozat and Manning, 2017). We apply early-stopping based on the labeled F1 score on the development set.9 We set t"
P17-1186,P16-1147,0,0.0327299,"presentation learning have been explored to learn feature combinations (Lei et al., 2014; Taub-Tabib et al., 2015; Pei et al., 2015, inter alia). Our work is perhaps closest to those who used BiLSTMs to encode inputs (Kiperwasser and Goldberg, 2016; Kuncoro et al., 2016; Wang and Chang, 2016; Dozat and Manning, 2017; Ma and Hovy, 2016). Multitask learning in NLP. There have been many efforts in NLP to use joint learning to replace pipelines, motivated by concerns about cascading errors. Collobert and Weston (2008) proposed sharing the same word representation while solving multiple NLP tasks. Zhang and Weiss (2016) use a continuous stacking model for POS tagging and parsing. Ammar et al. (2016) and Guo et al. (2016) explored parameter sharing for multilingual parsing. Johansson (2013) and Kshirsagar et al. (2015) applied ideas from domain adaptation to multitask learning. Successes in multitask learning have been enabled by advances in representation learning as well as earlier explorations of parameter sharing (Ando and Zhang, 2005; Blitzer et al., 2006; Daum´e III, 2007). 6 Conclusion We showed two orthogonal ways to apply deep multitask learning to graph-based parsing. The first shares parameters whe"
P17-1186,D08-1016,0,0.0359637,"put representations {ψ (t) } remain task-specific, and the score is still the inner product between the input representation and the output representation. The second variant, which we call SHARED, e and doesn’t use uses only the shared encoder h, (t) task-specific encoders {h }. It can be understood as a special case of FREDA where the dimensions of the task-specific encoders are 0. 4.3 Multitask SDP with Cross-Task Structures In syntactic parsing, higher-order structures have commonly been used to model interactions between multiple adjacent arcs in the same dependency tree (Carreras, 2007; Smith and Eisner, 2008; Martins et al., 2009; Zhang et al., 2014, inter alia). Llu´ıs et al. (2013), in contrast, used second-order structures to jointly model syntactic dependencies and semantic roles. Similarly, we use higher-order structures across tasks instead of within tasks. In this work, we look at interactions between arcs that share the same head and modifier.5 See Figures 2b and 2c for examples of higher-order cross-task structures. Higher-order structure scoring. Borrowing from Lei et al. (2014), we introduce a low-rank tensor scoring strategy that, given a higher-order structure p, models interactions"
P17-1186,P16-2038,0,0.158857,"y (t) , φ(t) ), where T = {DM, PAS, PSD}. Our task is now to predict three graphs {y (t) }t∈T for a given input sentence x. Multitask SDP can also be understood as x into a single unified multigraph y = S parsing (t) t∈T y . Similarly to Equations 1–2, we decompose y’s score S(x, y) into a sum of local scores for local structures in y, and we seek a multigraph yˆ that maximizes S(x, y). 4.2 Multitask SDP with Parameter Sharing A common approach when using BiLSTMs for multitask learning is to share the BiLSTM part of the model across tasks, while training specialized classifiers for each task (Søgaard and Goldberg, 2016). In this spirit, we let each task keep its own specialized MLPs, and explore two variants of our model that share parameters at the BiLSTM level. The first variant consists of a set of task-specific BiLSTM encoders as well as a common one that is shared across all tasks. We denote it FREDA. FREDA uses a neural generalization of “frustratingly easy” domain adaptation (Daum´e III, 2007; Kim et al., 2016), where one augments domainspecific features with a shared set of features to capture global patterns. Formally, let {h(t) }t∈T denote the three task-specific encoders. We introe that is shared"
P17-1186,W08-2121,0,0.268899,"Missing"
P17-1186,K16-1019,1,0.84747,"columns show the micro-average over the three tasks. † denotes the use of syntactic parses. Bold font indicates best performance among all systems, and underlines indicate statistical significance with Bonferroni correction compared to A&M, 2015 (open), the strongest baseline system. try, which is consistent with the extensive evaluation by Zhang et al. (2016), but we leave the incorporation of syntactic trees to future work. Syntactic parsing could be treated as yet another output task, as explored in Llu´ıs et al. (2013) and in the transition-based frameworks of Henderson et al. (2013) and Swayamdipta et al. (2016). Effects of structural overlap. We hypothesized that the overlap between formalisms would enable multitask learning to be effective; in this section we investigate in more detail how structural overlap affected performance. By looking at undirected overlap between unlabeled arcs, we discover that modeling only arcs in the same direction may have been a design mistake. DM and PAS are more structurally similar to each other than either is to PSD. Table 5 compares the structural similarities between the three for2044 Undirected DM PAS PSD Directed DM PAS PSD DM PAS PSD 70.0 57.4 67.2 56.3 56.8 5"
P17-1186,N15-1163,0,0.0124791,"s. 5 Related Work We note two important strands of related work. Graph-based parsing. Graph-based parsing was originally invented to handle non-projective syntax (McDonald et al., 2005; Koo et al., 2010; Martins et al., 2013, inter alia), but has been adapted to semantic parsing (Flanigan et al., 2014; Martins and Almeida, 2014; Thomson et al., 2014; Kuhlmann, 2014, inter alia). Local structure scoring was traditionally done with linear models over hand-engineered features, but lately, various forms of representation learning have been explored to learn feature combinations (Lei et al., 2014; Taub-Tabib et al., 2015; Pei et al., 2015, inter alia). Our work is perhaps closest to those who used BiLSTMs to encode inputs (Kiperwasser and Goldberg, 2016; Kuncoro et al., 2016; Wang and Chang, 2016; Dozat and Manning, 2017; Ma and Hovy, 2016). Multitask learning in NLP. There have been many efforts in NLP to use joint learning to replace pipelines, motivated by concerns about cascading errors. Collobert and Weston (2008) proposed sharing the same word representation while solving multiple NLP tasks. Zhang and Weiss (2016) use a continuous stacking model for POS tagging and parsing. Ammar et al. (2016) and Guo e"
P17-1186,S14-2027,1,0.870825,"Missing"
P17-1186,P16-1218,0,0.0195298,"05; Koo et al., 2010; Martins et al., 2013, inter alia), but has been adapted to semantic parsing (Flanigan et al., 2014; Martins and Almeida, 2014; Thomson et al., 2014; Kuhlmann, 2014, inter alia). Local structure scoring was traditionally done with linear models over hand-engineered features, but lately, various forms of representation learning have been explored to learn feature combinations (Lei et al., 2014; Taub-Tabib et al., 2015; Pei et al., 2015, inter alia). Our work is perhaps closest to those who used BiLSTMs to encode inputs (Kiperwasser and Goldberg, 2016; Kuncoro et al., 2016; Wang and Chang, 2016; Dozat and Manning, 2017; Ma and Hovy, 2016). Multitask learning in NLP. There have been many efforts in NLP to use joint learning to replace pipelines, motivated by concerns about cascading errors. Collobert and Weston (2008) proposed sharing the same word representation while solving multiple NLP tasks. Zhang and Weiss (2016) use a continuous stacking model for POS tagging and parsing. Ammar et al. (2016) and Guo et al. (2016) explored parameter sharing for multilingual parsing. Johansson (2013) and Kshirsagar et al. (2015) applied ideas from domain adaptation to multitask learning. Success"
P17-1186,J16-3001,0,0.0655123,"3 FREDA 3 DM PAS PSD Avg. 81.8 81.8 83.8 84.5 87.2 86.9 87.6 88.3 73.3 74.8 76.2 75.3 81.7 82.0 83.3 83.6 84.4 84.9 88.1 88.3 75.4 75.8 83.5 83.9 85.3 85.3 88.4 89.0 76.1 76.4 84.1 84.4 (b) Labeled F1 score on the out-of-domain test set. Table 4: The last columns show the micro-average over the three tasks. † denotes the use of syntactic parses. Bold font indicates best performance among all systems, and underlines indicate statistical significance with Bonferroni correction compared to A&M, 2015 (open), the strongest baseline system. try, which is consistent with the extensive evaluation by Zhang et al. (2016), but we leave the incorporation of syntactic trees to future work. Syntactic parsing could be treated as yet another output task, as explored in Llu´ıs et al. (2013) and in the transition-based frameworks of Henderson et al. (2013) and Swayamdipta et al. (2016). Effects of structural overlap. We hypothesized that the overlap between formalisms would enable multitask learning to be effective; in this section we investigate in more detail how structural overlap affected performance. By looking at undirected overlap between unlabeled arcs, we discover that modeling only arcs in the same directio"
P17-1186,D14-1109,0,0.0111244,"fic, and the score is still the inner product between the input representation and the output representation. The second variant, which we call SHARED, e and doesn’t use uses only the shared encoder h, (t) task-specific encoders {h }. It can be understood as a special case of FREDA where the dimensions of the task-specific encoders are 0. 4.3 Multitask SDP with Cross-Task Structures In syntactic parsing, higher-order structures have commonly been used to model interactions between multiple adjacent arcs in the same dependency tree (Carreras, 2007; Smith and Eisner, 2008; Martins et al., 2009; Zhang et al., 2014, inter alia). Llu´ıs et al. (2013), in contrast, used second-order structures to jointly model syntactic dependencies and semantic roles. Similarly, we use higher-order structures across tasks instead of within tasks. In this work, we look at interactions between arcs that share the same head and modifier.5 See Figures 2b and 2c for examples of higher-order cross-task structures. Higher-order structure scoring. Borrowing from Lei et al. (2014), we introduce a low-rank tensor scoring strategy that, given a higher-order structure p, models interactions between the firstorder structures (i.e., a"
P17-1186,I17-1007,0,\N,Missing
P18-1028,N18-1205,0,0.0616202,"e variants, the long short-term memory (LSTM; Hochreiter and Schmidhuber, 1997) and gated recurrent units (GRU; Cho et al., 2014), have become ubiquitous in NLP algorithms (Goldberg, 2016). Recently, several works introduced simpler versions of RNNs, such as recurrent additive networks (Lee et al., 2017) and Quasi-RNNs (Bradbury et al., 2017). Like SoPa, these models can be seen as points along the bridge between RNNs and CNNs. Other works have studied the expressive power of RNNs, in particular in the context of WFSAs or HMMs (Cleeremans et al., 1989; Giles et al., 1992; Visser et al., 2001; Chen et al., 2018). In this work we relate CNNs to WFSAs, showing that a one-layer CNN with max-pooling can be simulated by a collection of linear-chain WFSAs. Convolutional neural networks. CNNs are prominent feature extractors in NLP, both for generating character-based embeddings (Kim et al., 2016), and as sentence encoders for tasks like text classification (Yin and Sch¨utze, 2015) and machine translation (Gehring et al., 2017). Similarly to SoPa, several recently introduced variants of CNNs support varying window sizes by either allowing several fixed window sizes (Yin and Sch¨utze, 2015) or by supporting"
P18-1028,Q15-1031,0,0.0610826,"the resulting score and the original model score is considered p’s contribution. We then consider the highest contributing patterns, and attach each one with its highest scoring phrase in that document. Table 3 shows example texts along with their most positive and negative contributing phrases. 8 Related Work Weighted finite-state automata. WFSAs and hidden Markov models19 were once popular in automatic speech recognition (Hetherington, 2004; Moore et al., 2006; Hoffmeister et al., 2012) 19 HMMs are a special case of WFSAs (Mohri et al., 2002). and remain popular in morphology (Dreyer, 2011; Cotterell et al., 2015). Most closely related to this work, neural networks have been combined with weighted finite-state transducers to do morphological reinflection (Rastogi et al., 2016). These prior works learn a single FSA or FST, whereas our model learns a collection of simple but complementary FSAs, together encoding a sequence. We are the first to incorporate neural networks both before WFSAs (in their transition scoring functions), and after (in the function that turns their vector of scores into a final prediction), to produce an expressive model that remains interpretable. Recurrent neural networks. The a"
P18-1028,P08-1079,0,0.0278546,"–6 times as many parameters. Figure 3 shows a comparison of all models on the SST and Amazon datasets with varying training set sizes. SoPa is substantially outperforming all baselines, in particular BiLSTM, on small datasets (100 samples). This suggests that SoPa is better fit to learn from small datasets. Ablation analysis. Table 1 also shows an ablation of the differences between SoPa and CNN: max-product semiring with sigmoid vs. max-sum semiring with identity, self-loops, and ✏-transitions. The last line is equivalent to a CNN with 16 Some words may serve as both words and wildcards. See Davidov and Rappoport (2008) for discussion. 17 The number of patterns and their length are hyperparameters tuned on the development data (see Appendix A). 300 Model ROC SST Amazon Highest Scoring Phrases Hard DAN BiLSTM CNN 62.2 (4K) 64.3 (91K) 65.2 (844K) 64.3 (155K) SoPa 66.5 (255K) 85.6 (255K) 90.5 (256K) SoPams1 SoPams1 {sl} SoPams1 {✏} SoPams1 {sl, ✏} 64.4 63.2 64.3 64.0 75.5 (6K) 83.1 (91K) 84.8 (1.5M) 82.2 (62K) 84.8 84.6 83.6 85.0 88.5 (67K) 85.4 (91K) 90.8 (844K) 90.2 (305K) 90.0 89.8 89.7 89.5 Classification Accuracy Table 1: Test classification accuracy (and the number of parameters used). The bottom part"
P18-1028,C10-2028,0,0.0574624,"le method to interpret SoPa (Section 7). This method applies equally well to CNNs. We release our code at https://github.com/ Noahs-ARK/soft_patterns. 2 Background Surface patterns. Patterns (Hearst, 1992) are particularly useful tool in NLP (Lin et al., 2003; Etzioni et al., 2005; Schwartz et al., 2015). The most basic definition of a pattern is a sequence of words and wildcards (e.g., “X is a Y”), which can either be manually defined or extracted from a corpus using cooccurrence statistics. Patterns can then be matched against a specific text span by replacing wildcards with concrete words. Davidov et al. (2010) introduced a flexible notion of patterns, which supports partial matching of the pattern with a given text by skipping some of the words in the pattern, or introducing new words. In their framework, when a sequence of text partially matches a pattern, hard-coded partial scores are assigned to the pattern match. Here, we represent patterns as WFSAs with neural weights, and support these partial matches in a soft manner. WFSAs. We review weighted finite-state automata with ✏-transitions before we move on to our special case in Section 3. A WFSA-✏ with d states over a vocabulary V is formally de"
P18-1028,D14-1181,0,0.0420492,"iting this theoretical restriction is in practice, especially when SoPa is used as a component in a larger network. We defer the investigation of the exact computational properties of SoPa to future work. In the next section, we show that SoPa is an extension of a one-layer CNN, and hence more expressive. 4 SoPa as a CNN Extension A convolutional neural network (CNN; LeCun, 1998) moves a fixed-size sliding window over the document, producing a vector representation for each window. These representations are then often summed, averaged, or max-pooled to produce a document-level representation (Kim, 2014; Yin and Sch¨utze, 2015). In this section, we show that SoPa is an extension of one-layer, max-pooled CNNs. To recover a CNN from a soft pattern with d + 1 states, we first remove self-loops and ✏-transitions, 9 Rational series generalize recognizers of regular languages, which are the special case of the Boolean semiring. 298 max-pooled END states pattern1 states START states pattern2 states word vectors Fielding’s funniest and most likeable book in years Figure 2: State activations of two patterns as they score a document. pattern1 (length three) matches on “in years”. pattern2 (length five"
P18-1028,P02-1001,0,0.0973942,"ontributed equally. funny, magical great a What START 1 2 X 3 ! 4 END ✏ Figure 1: A representation of a surface pattern as a six-state automaton. Self-loops allow for repeatedly inserting words (e.g., “funny”). ✏-transitions allow for dropping words (e.g., “a”). is able to capture a soft notion of surface patterns (e.g., “what a great X !”; Hearst, 1992), where some words may be dropped, inserted, or replaced with similar words (see Figure 1). From a modeling perspective, SoPa is interesting because WFSAs are well-studied and come with efficient and flexible inference algorithms (Mohri, 1997; Eisner, 2002) that SoPa can take advantage of. SoPa defines a set of soft patterns of different lengths, with each pattern represented as a WFSA (Section 3). While the number and lengths of the patterns are hyperparameters, the patterns themselves are learned end-to-end. SoPa then represents a document with a vector that is the aggregate of the scores computed by matching each of the patterns with each span in the document. Because SoPa defines a hidden state that depends on the input token and the previous state, it can be thought of as a simple type of RNN. We show that SoPa is an extension of a onelayer"
P18-1028,W16-5901,0,0.0193662,"nt-level score. 3 SoPa: A Weighted Finite-State Automaton RNN We introduce SoPa, a WFSA-based RNN, which is designed to represent text as collection of surface pattern occurrences. We start by showing how a single pattern can be represented as a WFSA-✏ (Section 3.1). Then we describe how to score a complete document using a pattern (Section 3.2), and how multiple patterns can be used to encode a document (Section 3.3). Finally, we show that SoPa can be seen as a simple variant of an RNN (Section 3.4). 1 The semiring parsing view (Goodman, 1999) has produced unexpected connections in the past (Eisner, 2016). We experiment with max-product and max-sum semirings, but note that our model could be easily updated to use any semiring. 2 In our case, we also use a sparse transition matrix (Section 3.1), which further reduces our runtime to O(dn). 296 3.1 Patterns as WFSAs We describe how a pattern can be represented as a WFSA-✏. We first assume a single pattern. A pattern is a WFSA-✏, but we impose hard constraints on its shape, and its transition weights are given by differentiable functions that have the power to capture concrete words, wildcards, and everything in between. Our model is designed to b"
P18-1028,J99-4004,0,0.124536,"tion 3.2 how phrase-level scores can be summarized into a document-level score. 3 SoPa: A Weighted Finite-State Automaton RNN We introduce SoPa, a WFSA-based RNN, which is designed to represent text as collection of surface pattern occurrences. We start by showing how a single pattern can be represented as a WFSA-✏ (Section 3.1). Then we describe how to score a complete document using a pattern (Section 3.2), and how multiple patterns can be used to encode a document (Section 3.3). Finally, we show that SoPa can be seen as a simple variant of an RNN (Section 3.4). 1 The semiring parsing view (Goodman, 1999) has produced unexpected connections in the past (Eisner, 2016). We experiment with max-product and max-sum semirings, but note that our model could be easily updated to use any semiring. 2 In our case, we also use a sparse transition matrix (Section 3.1), which further reduces our runtime to O(dn). 296 3.1 Patterns as WFSAs We describe how a pattern can be represented as a WFSA-✏. We first assume a single pattern. A pattern is a WFSA-✏, but we impose hard constraints on its shape, and its transition weights are given by differentiable functions that have the power to capture concrete words, w"
P18-1028,C92-2082,0,0.352422,"Ns and CNNs, presenting SoPa (for Soft Patterns), a model that lies in between them. SoPa is a neural version of a weighted finitestate automaton (WFSA), with a restricted set of transitions. Linguistically, SoPa is appealing as it ⇤ The first two authors contributed equally. funny, magical great a What START 1 2 X 3 ! 4 END ✏ Figure 1: A representation of a surface pattern as a six-state automaton. Self-loops allow for repeatedly inserting words (e.g., “funny”). ✏-transitions allow for dropping words (e.g., “a”). is able to capture a soft notion of surface patterns (e.g., “what a great X !”; Hearst, 1992), where some words may be dropped, inserted, or replaced with similar words (see Figure 1). From a modeling perspective, SoPa is interesting because WFSAs are well-studied and come with efficient and flexible inference algorithms (Mohri, 1997; Eisner, 2002) that SoPa can take advantage of. SoPa defines a set of soft patterns of different lengths, with each pattern represented as a WFSA (Section 3). While the number and lengths of the patterns are hyperparameters, the patterns themselves are learned end-to-end. SoPa then represents a document with a vector that is the aggregate of the scores co"
P18-1028,P15-1162,0,0.0497016,"Missing"
P18-1028,D15-1180,0,0.0555662,"WFSAs, showing that a one-layer CNN with max-pooling can be simulated by a collection of linear-chain WFSAs. Convolutional neural networks. CNNs are prominent feature extractors in NLP, both for generating character-based embeddings (Kim et al., 2016), and as sentence encoders for tasks like text classification (Yin and Sch¨utze, 2015) and machine translation (Gehring et al., 2017). Similarly to SoPa, several recently introduced variants of CNNs support varying window sizes by either allowing several fixed window sizes (Yin and Sch¨utze, 2015) or by supporting non-consecutive n-gram matching (Lei et al., 2015; Nguyen and Grishman, 2016). Neural networks and patterns. Some works used patterns as part of a neural network. Schwartz et al. (2016) used pattern contexts for estimating word embeddings, showing improved word similarity results compared to bag-of-word 302 contexts. Shwartz et al. (2016) designed an LSTM representation for dependency patterns, using them to detect hypernymy relations. Here, we learn patterns as a neural version of WFSAs. Interpretability. There have been several efforts to interpret neural models. The weights of the attention mechanism (Bahdanau et al., 2015) are often used"
P18-1028,D16-1011,0,0.0215851,"The weights of the attention mechanism (Bahdanau et al., 2015) are often used to display the words that are most significant for making a prediction. LIME (Ribeiro et al., 2016) is another approach for visualizing neural models (not necessarily textual). Yogatama and Smith (2014) introduced structured sparsity, which encodes linguistic information into the regularization of a model, thus allowing to visualize the contribution of different bag-of-word features. Other works jointly learned to encode text and extract the span which best explains the model’s prediction (Yessenalina et al., 2010; Lei et al., 2016). Li et al. (2016) and K´ad´ar et al. (2017) suggested a method that erases pieces of the text in order to analyze their effect on a neural model’s decisions. Finally, several works presented methods to visualize deep CNNs (Zeiler and Fergus, 2014; Simonyan et al., 2014; Yosinski et al., 2015), focusing on visualizing the different layers of the network, mainly in the context of image and video understanding. We believe these two types of research approaches are complementary: inventing general purpose visualization tools for existing black-box models on the one hand, and on the other, designi"
P18-1028,J97-2003,0,0.355747,"two authors contributed equally. funny, magical great a What START 1 2 X 3 ! 4 END ✏ Figure 1: A representation of a surface pattern as a six-state automaton. Self-loops allow for repeatedly inserting words (e.g., “funny”). ✏-transitions allow for dropping words (e.g., “a”). is able to capture a soft notion of surface patterns (e.g., “what a great X !”; Hearst, 1992), where some words may be dropped, inserted, or replaced with similar words (see Figure 1). From a modeling perspective, SoPa is interesting because WFSAs are well-studied and come with efficient and flexible inference algorithms (Mohri, 1997; Eisner, 2002) that SoPa can take advantage of. SoPa defines a set of soft patterns of different lengths, with each pattern represented as a WFSA (Section 3). While the number and lengths of the patterns are hyperparameters, the patterns themselves are learned end-to-end. SoPa then represents a document with a vector that is the aggregate of the scores computed by matching each of the patterns with each span in the document. Because SoPa defines a hidden state that depends on the input token and the previous state, it can be thought of as a simple type of RNN. We show that SoPa is an extensio"
P18-1028,N16-1098,0,0.0681027,"Missing"
P18-1028,D16-1085,0,0.0167498,"t a one-layer CNN with max-pooling can be simulated by a collection of linear-chain WFSAs. Convolutional neural networks. CNNs are prominent feature extractors in NLP, both for generating character-based embeddings (Kim et al., 2016), and as sentence encoders for tasks like text classification (Yin and Sch¨utze, 2015) and machine translation (Gehring et al., 2017). Similarly to SoPa, several recently introduced variants of CNNs support varying window sizes by either allowing several fixed window sizes (Yin and Sch¨utze, 2015) or by supporting non-consecutive n-gram matching (Lei et al., 2015; Nguyen and Grishman, 2016). Neural networks and patterns. Some works used patterns as part of a neural network. Schwartz et al. (2016) used pattern contexts for estimating word embeddings, showing improved word similarity results compared to bag-of-word 302 contexts. Shwartz et al. (2016) designed an LSTM representation for dependency patterns, using them to detect hypernymy relations. Here, we learn patterns as a neural version of WFSAs. Interpretability. There have been several efforts to interpret neural models. The weights of the attention mechanism (Bahdanau et al., 2015) are often used to display the words that a"
P18-1028,D14-1162,0,0.0887451,"vx + bi ), if j = i + 1 &gt; : 0, otherwise, (3) where ui and wi are vectors of parameters, ai and bi are scalar parameters, vx is a fixed pre-trained word vector for x,4 and E is an encoding function, typically the identity function or sigmoid. ✏-transitions are also parameterized, but don’t consume a token and depend only on the current state: ( E(ci ), if j = i + 1 [T(✏)]i,j = (4) 0, otherwise, where ci is a scalar parameter.5 As we have only 3 To ensure that we start in the START state and end in the END state, we fix ⇡ = [1, 0, . . . , 0] and ⌘ = [0, . . . , 0, 1]. 4 We use GloVe 300d 840B (Pennington et al., 2014). 5 Adding ✏-transitions to WFSAs does not increase their three non-zero diagonals in total, the matrix multiplications in Equation 2 can be implemented using vector operations, and the overall runtimes of Forward and Viterbi are reduced to O(dn).6 Words vs. wildcards. Traditional hard patterns distinguish between words and wildcards. Our model does not explicitly capture the notion of either, but the transition weight function can be interpreted in those terms. Each transition is a logistic regression over the next word vector vx . For example, for a main path out of state i, T has two parame"
P18-1028,N16-1076,0,0.123297,"t scoring phrase in that document. Table 3 shows example texts along with their most positive and negative contributing phrases. 8 Related Work Weighted finite-state automata. WFSAs and hidden Markov models19 were once popular in automatic speech recognition (Hetherington, 2004; Moore et al., 2006; Hoffmeister et al., 2012) 19 HMMs are a special case of WFSAs (Mohri et al., 2002). and remain popular in morphology (Dreyer, 2011; Cotterell et al., 2015). Most closely related to this work, neural networks have been combined with weighted finite-state transducers to do morphological reinflection (Rastogi et al., 2016). These prior works learn a single FSA or FST, whereas our model learns a collection of simple but complementary FSAs, together encoding a sequence. We are the first to incorporate neural networks both before WFSAs (in their transition scoring functions), and after (in the function that turns their vector of scores into a final prediction), to produce an expressive model that remains interpretable. Recurrent neural networks. The ability of RNNs to represent arbitrarily long sequences of embedded tokens has made them attractive to NLP researchers. The most notable variants, the long short-term"
P18-1028,D13-1170,0,0.00747955,"Pa is restricted to operations that follow the semiring laws.11 As a model that is more flexible than a one-layer CNN, but (arguably) less expressive than many RNNs, SoPa lies somewhere on the continuum between these two approaches. Continuing to study the bridge between CNNs and RNNs is an exciting direction for future research. 5 Experiments To evaluate SoPa, we apply it to text classification tasks. Below we describe our datasets and baselines. More details can be found in Appendix A. Datasets. We experiment with three binary classification datasets. • SST. The Stanford Sentiment Treebank (Socher et al., 2013)12 contains roughly 10K movie reviews from Rotten Tomatoes,13 labeled on a scale of 1–5. We consider the binary task, which considers 1 and 2 as negative, and 4 and 5 as positive (ignoring 3s). It is worth noting that this dataset also contains syntactic phrase level annotations, providing a sentiment label to parts of 11 The max-sum semiring corresponds to a linear filter with max-pooling. Other semirings could potentially model more interesting interactions, but we leave this to future work. 12 https://nlp.stanford.edu/sentiment/ index.html 13 http://www.rottentomatoes.com 299 sentences. In"
P18-1028,D15-1243,0,0.016976,"large magnitude and is close to the word vector for some word y (e.g., wi ⇡ 100vy ), and bi is a large negative bias (e.g., bi ⇡ 100), then the transition is essentially matching the specific word y. Whereas if wi has small magnitude (wi ⇡ 0) and bi is a large positive bias (e.g., bi ⇡ 100), then the transition is ignoring the current token and matching a wildcard.7 The transition could also be something in between, for instance by focusing on specific dimensions of a word’s meaning encoded in the vector, such as POS or semantic features like animacy or concreteness (Rubinstein et al., 2015; Tsvetkov et al., 2015). 3.2 Scoring Documents So far we described how to calculate how well a pattern matches a token span exactly (consuming the whole span). To score a complete document, we prefer a score that aggregates over all matches on subspans of the document (similar to “search” instead of “match” in regular expression parlance). We still assume a single pattern. Either the Forward algorithm can be used to calculate the Pexpected count of the pattern in the document, 1ijn pspan (xi:j ), or Viterbi to calculate sdoc (x) = max1ijn sspan (xi:j ), the score of the highest-scoring match. In short document"
P18-1028,N16-3020,0,0.0173392,"terns as part of a neural network. Schwartz et al. (2016) used pattern contexts for estimating word embeddings, showing improved word similarity results compared to bag-of-word 302 contexts. Shwartz et al. (2016) designed an LSTM representation for dependency patterns, using them to detect hypernymy relations. Here, we learn patterns as a neural version of WFSAs. Interpretability. There have been several efforts to interpret neural models. The weights of the attention mechanism (Bahdanau et al., 2015) are often used to display the words that are most significant for making a prediction. LIME (Ribeiro et al., 2016) is another approach for visualizing neural models (not necessarily textual). Yogatama and Smith (2014) introduced structured sparsity, which encodes linguistic information into the regularization of a model, thus allowing to visualize the contribution of different bag-of-word features. Other works jointly learned to encode text and extract the span which best explains the model’s prediction (Yessenalina et al., 2010; Lei et al., 2016). Li et al. (2016) and K´ad´ar et al. (2017) suggested a method that erases pieces of the text in order to analyze their effect on a neural model’s decisions. Fi"
P18-1028,P15-2119,1,0.853947,"rs, wi and bi . If wi has large magnitude and is close to the word vector for some word y (e.g., wi ⇡ 100vy ), and bi is a large negative bias (e.g., bi ⇡ 100), then the transition is essentially matching the specific word y. Whereas if wi has small magnitude (wi ⇡ 0) and bi is a large positive bias (e.g., bi ⇡ 100), then the transition is ignoring the current token and matching a wildcard.7 The transition could also be something in between, for instance by focusing on specific dimensions of a word’s meaning encoded in the vector, such as POS or semantic features like animacy or concreteness (Rubinstein et al., 2015; Tsvetkov et al., 2015). 3.2 Scoring Documents So far we described how to calculate how well a pattern matches a token span exactly (consuming the whole span). To score a complete document, we prefer a score that aggregates over all matches on subspans of the document (similar to “search” instead of “match” in regular expression parlance). We still assume a single pattern. Either the Forward algorithm can be used to calculate the Pexpected count of the pattern in the document, 1ijn pspan (xi:j ), or Viterbi to calculate sdoc (x) = max1ijn sspan (xi:j ), the score of the highest-scoring"
P18-1028,K15-1026,1,0.84818,"tional LSTM and a CNN. Our model performs on par with or better than all baselines on all tasks (Section 6). Moreover, when training with smaller datasets, SoPa is particularly useful, outperforming all models by substantial margins. Finally, building on the connections discovered in this paper, we offer a new, simple method to interpret SoPa (Section 7). This method applies equally well to CNNs. We release our code at https://github.com/ Noahs-ARK/soft_patterns. 2 Background Surface patterns. Patterns (Hearst, 1992) are particularly useful tool in NLP (Lin et al., 2003; Etzioni et al., 2005; Schwartz et al., 2015). The most basic definition of a pattern is a sequence of words and wildcards (e.g., “X is a Y”), which can either be manually defined or extracted from a corpus using cooccurrence statistics. Patterns can then be matched against a specific text span by replacing wildcards with concrete words. Davidov et al. (2010) introduced a flexible notion of patterns, which supports partial matching of the pattern with a given text by skipping some of the words in the pattern, or introducing new words. In their framework, when a sequence of text partially matches a pattern, hard-coded partial scores are a"
P18-1028,N16-1060,1,0.85755,"networks. CNNs are prominent feature extractors in NLP, both for generating character-based embeddings (Kim et al., 2016), and as sentence encoders for tasks like text classification (Yin and Sch¨utze, 2015) and machine translation (Gehring et al., 2017). Similarly to SoPa, several recently introduced variants of CNNs support varying window sizes by either allowing several fixed window sizes (Yin and Sch¨utze, 2015) or by supporting non-consecutive n-gram matching (Lei et al., 2015; Nguyen and Grishman, 2016). Neural networks and patterns. Some works used patterns as part of a neural network. Schwartz et al. (2016) used pattern contexts for estimating word embeddings, showing improved word similarity results compared to bag-of-word 302 contexts. Shwartz et al. (2016) designed an LSTM representation for dependency patterns, using them to detect hypernymy relations. Here, we learn patterns as a neural version of WFSAs. Interpretability. There have been several efforts to interpret neural models. The weights of the attention mechanism (Bahdanau et al., 2015) are often used to display the words that are most significant for making a prediction. LIME (Ribeiro et al., 2016) is another approach for visualizing"
P18-1028,K17-1004,1,0.830825,"2013)14 contains electronics product reviews, a subset of a larger review dataset. Each document in the dataset contains a review and a summary. Following Yogatama et al. (2015), we only use the reviews part, focusing on positive and negative reviews. The number of training/development/test samples is 20K/5K/25K. • ROC. The ROC story cloze task (Mostafazadeh et al., 2016) is a story understanding task.15 The task is composed of four-sentence story prefixes, followed by two competing endings: one that makes the joint five-sentence story coherent, and another that makes it incoherent. Following Schwartz et al. (2017), we treat it as a style detection task: we treat all “right” endings as positive samples and all “wrong” ones as negative, and we ignore the story prefix. We split the development set into train and development (of sizes 3,366 and 374 sentences, respectively), and take the test set as-is (3,742 sentences). Reduced training data. In order to test our model’s ability to learn from small datasets, we also randomly sample 100, 500, 1,000 and 2,500 SST training instances and 100, 500, 1,000, 2,500, 5,000, and 10,000 Amazon training instances. Development and test sets remain the same. Baselines. W"
P18-1028,P16-1226,0,0.0186872,"s like text classification (Yin and Sch¨utze, 2015) and machine translation (Gehring et al., 2017). Similarly to SoPa, several recently introduced variants of CNNs support varying window sizes by either allowing several fixed window sizes (Yin and Sch¨utze, 2015) or by supporting non-consecutive n-gram matching (Lei et al., 2015; Nguyen and Grishman, 2016). Neural networks and patterns. Some works used patterns as part of a neural network. Schwartz et al. (2016) used pattern contexts for estimating word embeddings, showing improved word similarity results compared to bag-of-word 302 contexts. Shwartz et al. (2016) designed an LSTM representation for dependency patterns, using them to detect hypernymy relations. Here, we learn patterns as a neural version of WFSAs. Interpretability. There have been several efforts to interpret neural models. The weights of the attention mechanism (Bahdanau et al., 2015) are often used to display the words that are most significant for making a prediction. LIME (Ribeiro et al., 2016) is another approach for visualizing neural models (not necessarily textual). Yogatama and Smith (2014) introduced structured sparsity, which encodes linguistic information into the regulariz"
P18-1028,D10-1102,0,0.0265513,"o interpret neural models. The weights of the attention mechanism (Bahdanau et al., 2015) are often used to display the words that are most significant for making a prediction. LIME (Ribeiro et al., 2016) is another approach for visualizing neural models (not necessarily textual). Yogatama and Smith (2014) introduced structured sparsity, which encodes linguistic information into the regularization of a model, thus allowing to visualize the contribution of different bag-of-word features. Other works jointly learned to encode text and extract the span which best explains the model’s prediction (Yessenalina et al., 2010; Lei et al., 2016). Li et al. (2016) and K´ad´ar et al. (2017) suggested a method that erases pieces of the text in order to analyze their effect on a neural model’s decisions. Finally, several works presented methods to visualize deep CNNs (Zeiler and Fergus, 2014; Simonyan et al., 2014; Yosinski et al., 2015), focusing on visualizing the different layers of the network, mainly in the context of image and video understanding. We believe these two types of research approaches are complementary: inventing general purpose visualization tools for existing black-box models on the one hand, and on"
P18-1028,K15-1021,0,0.0334286,"Missing"
P18-1028,D15-1251,1,0.848162,"e interesting interactions, but we leave this to future work. 12 https://nlp.stanford.edu/sentiment/ index.html 13 http://www.rottentomatoes.com 299 sentences. In order to experiment in a realistic setup, we only consider the complete sentences, and ignore syntactic annotations at train or test time. The number of training/development/test sentences in the dataset is 6,920/872/1,821. • Amazon. The Amazon Review Corpus (McAuley and Leskovec, 2013)14 contains electronics product reviews, a subset of a larger review dataset. Each document in the dataset contains a review and a summary. Following Yogatama et al. (2015), we only use the reviews part, focusing on positive and negative reviews. The number of training/development/test samples is 20K/5K/25K. • ROC. The ROC story cloze task (Mostafazadeh et al., 2016) is a story understanding task.15 The task is composed of four-sentence story prefixes, followed by two competing endings: one that makes the joint five-sentence story coherent, and another that makes it incoherent. Following Schwartz et al. (2017), we treat it as a style detection task: we treat all “right” endings as positive samples and all “wrong” ones as negative, and we ignore the story prefix."
P18-1028,P14-1074,1,0.820344,"embeddings, showing improved word similarity results compared to bag-of-word 302 contexts. Shwartz et al. (2016) designed an LSTM representation for dependency patterns, using them to detect hypernymy relations. Here, we learn patterns as a neural version of WFSAs. Interpretability. There have been several efforts to interpret neural models. The weights of the attention mechanism (Bahdanau et al., 2015) are often used to display the words that are most significant for making a prediction. LIME (Ribeiro et al., 2016) is another approach for visualizing neural models (not necessarily textual). Yogatama and Smith (2014) introduced structured sparsity, which encodes linguistic information into the regularization of a model, thus allowing to visualize the contribution of different bag-of-word features. Other works jointly learned to encode text and extract the span which best explains the model’s prediction (Yessenalina et al., 2010; Lei et al., 2016). Li et al. (2016) and K´ad´ar et al. (2017) suggested a method that erases pieces of the text in order to analyze their effect on a neural model’s decisions. Finally, several works presented methods to visualize deep CNNs (Zeiler and Fergus, 2014; Simonyan et al."
P18-1028,C16-1329,0,0.0191724,"), and take the test set as-is (3,742 sentences). Reduced training data. In order to test our model’s ability to learn from small datasets, we also randomly sample 100, 500, 1,000 and 2,500 SST training instances and 100, 500, 1,000, 2,500, 5,000, and 10,000 Amazon training instances. Development and test sets remain the same. Baselines. We compare to four baselines: a BiLSTM, a one-layer CNN, DAN (a simple alternative to RNNs) and a feature-based classifier trained with hard-pattern features. • BiLSTM. Bidirectional LSTMs have been successfully used in the past for text classification tasks (Zhou et al., 2016). We learn a one-layer BiLSTM representation of the document, and feed the average of all hidden states to an MLP. • CNN. CNNs are particularly useful for text classification (Kim, 2014). We train a one-layer CNN with max-pooling, and feed the resulting representation to an MLP. 14 http://riejohnson.com/cnn_data.html 15 http://cs.rochester.edu/nlp/ rocstories/ • DAN. We learn a deep averaging network with word dropout (Iyyer et al., 2015), a simple but strong text-classification baseline. • Hard. We train a logistic regression classifier with hard-pattern features. Following Tsur et al. (2010)"
P18-1043,P98-1013,0,0.691869,"Missing"
P18-1043,D17-1091,0,0.0343723,"Missing"
P18-1043,H05-1079,0,0.118776,"Missing"
P18-1043,D15-1075,0,0.0465582,"events the user has experienced, without the user explicitly stating how they are feeling. Similarly, advertisement systems on social media should be able to reason about the emotional reactions of people after events such as mass shootings and remove ads for guns which might increase social distress (Goel and Isaac, 2016). Also, pragmatic inference is a necessary step toward automatic narrative understanding and generation (Tomai and Forbus, 2010; Ding and Riloff, 2016; Ding et al., 2017). However, this type of social commonsense reasoning goes far beyond the widely studied entailment tasks (Bowman et al., 2015; Dagan et al., 2006) and thus falls outside the scope of existing benchmarks. In this paper, we introduce a new task, corpus, Introduction Understanding a narrative requires commonsense reasoning about the mental states of people in relation to events. For example, if “Alex is dragging his feet at work”, pragmatic implications about Alex’s intent are that “Alex wants to avoid doing things” (Figure 1). We can also infer that Alex’s emotional reaction might be feeling “lazy” or “bored”. Furthermore, while not explicitly mentioned, we can infer that people other than Alex are affected by the sit"
P18-1043,S13-1035,0,0.0167626,"we extract are a combination of a verb predicate with partially instantiated arguments. We keep specific arguments together with the predicate, if they appear frequently enough (e.g., PersonX eats pasta for dinner). Otherwise, the arguments are replaced with an untyped blank (e.g., PersonX eats for dinner). In our work, only person mentions are replaced with typed variables, leaving other types to future research. 2.1 Event Extraction We extract phrasal events from three different corpora for broad coverage: the ROC Story training set (Mostafazadeh et al., 2016), the Google Syntactic N-grams (Goldberg and Orwant, 2013), and the Spinn3r corpus (Gordon and Swanson, 2008). We derive events from the set of verb phrases in our corpora, based on syntactic parses (Klein and Manning, 2003). We then replace the predicate subject and other entities with the typed variables (e.g., PersonX, PersonY), and selectively substitute verb arguments with blanks ( ). We use frequency thresholds to select events to annotate (for details, see Appendix A.1). Additionally, we supplement the list of events with all 2,000 verb idioms found in Wiktionary, in order to cover events that are less compositional.2 Our final annotation corp"
P18-1043,K16-1002,0,0.0559548,"Missing"
P18-1043,N15-1113,0,0.0306237,"cult for commonsense inference, perhaps due to the difficulty in composing meaning over nonliteral or noncompositional event descriptions. To further evaluate the geometry of the embedding space, we analyze interpolations between pairs of event phrases (from outside the train set), similar to the homotopic analysis of Bowman et al. (2016). For a handful of event pairs, we decode intents, reactions for PersonX, and reactions for other people from points sampled at equal inter5.1 Processing of Movie Scripts For our portrayal analyses, we use scene descriptions from 772 movie scripts released by Gorinski and Lapata (2015), assigned to over 21,000 characters as done by Sap et al. (2017). We extract events from the scene descriptions, and generate their 10 most probable intent and reaction sequences using our BiRNN sequence model (as in Figure 7). We then categorize generated intents and reactions into groups based on LIWC category scores of the generated output (Tausczik and Pennebaker, 2016).3 The intent and reaction categories are then 3 469 We only consider content word categories: ‘Core Drives 5.2 PersonX hugs ___ , planting a smooch on PersonY&apos;s cheek show affection show love Int ent loving none Female: in"
P18-1043,W14-4012,0,0.0541464,"Missing"
P18-1043,D14-1125,0,0.0578311,"Missing"
P18-1043,D17-1167,0,0.0612405,"Missing"
P18-1043,D13-1149,0,0.0657859,"Missing"
P18-1043,D17-1292,0,0.0441691,"Missing"
P18-1043,J14-1002,1,0.872303,"Missing"
P18-1043,D14-1181,0,0.00637193,"Missing"
P18-1043,P03-1054,0,0.0196713,"y enough (e.g., PersonX eats pasta for dinner). Otherwise, the arguments are replaced with an untyped blank (e.g., PersonX eats for dinner). In our work, only person mentions are replaced with typed variables, leaving other types to future research. 2.1 Event Extraction We extract phrasal events from three different corpora for broad coverage: the ROC Story training set (Mostafazadeh et al., 2016), the Google Syntactic N-grams (Goldberg and Orwant, 2013), and the Spinn3r corpus (Gordon and Swanson, 2008). We derive events from the set of verb phrases in our corpora, based on syntactic parses (Klein and Manning, 2003). We then replace the predicate subject and other entities with the typed variables (e.g., PersonX, PersonY), and selectively substitute verb arguments with blanks ( ). We use frequency thresholds to select events to annotate (for details, see Appendix A.1). Additionally, we supplement the list of events with all 2,000 verb idioms found in Wiktionary, in order to cover events that are less compositional.2 Our final annotation corpus contains nearly 25,000 event phrases, spanning over 1,300 unique verb predicates (Table 2). Inference types The first type of pragmatic inference is about intent."
P18-1043,P16-1137,0,0.0318906,"sk, demonstrating that, given the phrase-level inference dataset, neural encoderdecoder models can successfully compose phrasal embeddings for previously unseen events and reason about the mental states of their participants. 1 464 https://tinyurl.com/event2mind matic or commonsense interpretation. We scope our study to two distinct types of inference: given a phrase that describes an event, we want to reason about the likely intents and emotional reactions of people who caused or affected by the event. This complements prior work on more general commonsense inference (Speer and Havasi, 2012; Li et al., 2016; Zhang et al., 2017), by focusing on the causal relations between events and people’s mental states, which are not well covered by most existing resources. # Unique Events # Unique Verbs Average  ROC Story G. N-grams Spinn3r Idioms 13,627 7,066 2,130 1,916 639 789 388 442 0.57 0.39 0.41 0.42 Total 24,716 1,333 0.45 Source Table 2: Data and annotation agreement statistics for our new phrasal inference corpus. Each event is annotated by three crowdworkers. We collect a wide range of phrasal event descriptions from stories, blogs, and Wiktionary idioms. Compared to prior work on phrasal embeddi"
P18-1043,N09-4007,0,0.0891988,"Missing"
P18-1043,speer-havasi-2012-representing,0,0.036999,"rformance on this new task, demonstrating that, given the phrase-level inference dataset, neural encoderdecoder models can successfully compose phrasal embeddings for previously unseen events and reason about the mental states of their participants. 1 464 https://tinyurl.com/event2mind matic or commonsense interpretation. We scope our study to two distinct types of inference: given a phrase that describes an event, we want to reason about the likely intents and emotional reactions of people who caused or affected by the event. This complements prior work on more general commonsense inference (Speer and Havasi, 2012; Li et al., 2016; Zhang et al., 2017), by focusing on the causal relations between events and people’s mental states, which are not well covered by most existing resources. # Unique Events # Unique Verbs Average  ROC Story G. N-grams Spinn3r Idioms 13,627 7,066 2,130 1,916 639 789 388 442 0.57 0.39 0.41 0.42 Total 24,716 1,333 0.45 Source Table 2: Data and annotation agreement statistics for our new phrasal inference corpus. Each event is annotated by three crowdworkers. We collect a wide range of phrasal event descriptions from stories, blogs, and Wiktionary idioms. Compared to prior work o"
P18-1043,J12-2003,0,0.103728,"Missing"
P18-1043,E14-4025,0,0.105708,"Missing"
P18-1043,N16-1098,0,0.103644,"shown in examples in Table 1. More formally, the phrases we extract are a combination of a verb predicate with partially instantiated arguments. We keep specific arguments together with the predicate, if they appear frequently enough (e.g., PersonX eats pasta for dinner). Otherwise, the arguments are replaced with an untyped blank (e.g., PersonX eats for dinner). In our work, only person mentions are replaced with typed variables, leaving other types to future research. 2.1 Event Extraction We extract phrasal events from three different corpora for broad coverage: the ROC Story training set (Mostafazadeh et al., 2016), the Google Syntactic N-grams (Goldberg and Orwant, 2013), and the Spinn3r corpus (Gordon and Swanson, 2008). We derive events from the set of verb phrases in our corpora, based on syntactic parses (Klein and Manning, 2003). We then replace the predicate subject and other entities with the typed variables (e.g., PersonX, PersonY), and selectively substitute verb arguments with blanks ( ). We use frequency thresholds to select events to annotate (for details, see Appendix A.1). Additionally, we supplement the list of events with all 2,000 verb idioms found in Wiktionary, in order to cover even"
P18-1043,D16-1177,0,0.0562659,"Missing"
P18-1043,P15-2070,0,0.0512399,"Missing"
P18-1043,P17-1153,0,0.0571898,"Missing"
P18-1043,P16-1030,1,0.906195,"Missing"
P18-1043,P17-2022,0,0.0397663,"Missing"
P18-1043,Q15-1034,0,0.0422461,"Missing"
P18-1043,S15-2077,0,0.0631799,"Missing"
P18-1043,D17-1247,1,0.939721,"tly mentioned by the event phrase, and (3) a task formulation that aims to generate the textual descriptions of intents and reactions, instead of classifying their polarities or classifying the inference relations between two given textual descriptions. Furthermore, in order to showcase the practical implications of commonsense inference on events and people’s mental states, we apply our model to modern movie scripts, which provide a new insight into the gender bias in modern films beyond what previous studies have offered (England et al., 2011; Agarwal et al., 2015; Ramakrishna et al., 2017; Sap et al., 2017). The resulting corpus includes around 25,000 event phrases, which combine automatically extracted phrases from stories and blogs with all idiomatic verb phrases listed in the Wiktionary. Our corpus is publicly available.1 2 Dataset One goal of our investigation is to probe whether it is feasible to build computational models that can perform limited, but well-scoped commonsense inference on short free-form text, which we refer to as event phrases. While there has been much prior research on phrase-level paraphrases (Pavlick et al., 2015) and phrase-level entailment (Dagan et al., 2006), relat"
P18-1043,Q15-1025,0,\N,Missing
P18-1043,C98-1013,0,\N,Missing
P18-1043,Q17-1027,0,\N,Missing
P18-1173,W06-1615,0,0.0529895,"hey often require other modifications to maintain well-formedness of the tree. Figure 2 gives an example. 6 Related Work Joint learning in NLP pipelines. To avoid cascading errors, much effort has been devoted to joint decoding in NLP pipelines (Habash and Rambow, 2005; Cohen and Smith, 2007; Goldberg and Tsarfaty, 2008; Lewis et al., 2015; Zhang et al., 2015, inter alia). However, joint inference can sometimes be prohibitively expensive. Recent advances in representation learning facilitate exploration in the joint learning of multiple tasks by sharing parameters (Collobert and Weston, 2008; Blitzer et al., 2006; Finkel and Manning, 2010; Zhang and Weiss, 2016; Hashimoto et al., 2017, inter alia). Differentiable optimization. Gould et al. (2016) review the generic approaches to differentiation in bi-level optimization (Bard, 2010; Kunisch and Pock, 2013). Amos and Kolter (2017) extend their efforts to a class of subdifferentiable quadratic programs. However, they both require that the intermediate objective has an invertible Hessian, limiting their application 1870 in NLP. In another line of work, the steps of a gradient-based optimization procedure are unrolled into a single computation graph (Stoya"
P18-1173,D07-1022,1,0.803076,"Missing"
P18-1173,Q16-1031,1,0.818541,"by randomly sampling an instance from the union of their training data at each step. In order to isolate the effect of backpropagation, we do not share any parameters between the two models.5 Implementation details are summarized in the supplementary materials. 4.1.2 UF LF UF LF – – 89.4 90.4 – – 77.6 78.5 STE 91.8 91.6 92.0 90.8 90.6 91.1 88.4 87.9 88.9 78.1 78.1 78.9 SPIGOT 92.4 91.6 88.6 78.9 N EURBO PARSER F REDA 3 i6=j PIPELINE SA (a) F1 on in-domain test set. DM Model 5 Parameter sharing has proved successful in many related tasks (Collobert and Weston, 2008; Søgaard and Goldberg, 2016; Ammar et al., 2016; Swayamdipta et al., 2016, 2017, inter alia), and could be easily combined with our approach. 6 We drop the third (PAS) because its structure is highly predictable from parts-of-speech, making it less interesting. PSD UF LF UF LF – – 84.5 85.3 – – 75.3 76.4 STE 87.4 87.3 87.7 85.8 85.6 86.4 85.5 84.9 85.8 75.6 75.9 76.6 SPIGOT 87.9 86.7 85.5 77.1 N EURBO PARSER F REDA 3 PIPELINE SA (b) F1 on out-of-domain test set. Table 1: Semantic dependency parsing performance in both unlabeled (UF ) and labeled (LF ) F1 scores. Bold font indicates the best performance. Peng et al. (2017) does not report U"
P18-1173,P17-1092,1,0.806242,"sing gradient-based optimization. Yet traditional NLP often assumed modular stages of processing that formed a pipeline; e.g., text was tokenized, then tagged with parts of speech, then parsed into a phrase-structure or dependency tree, then semantically analyzed. Pipelines, which make “hard” (i.e., discrete) decisions at each stage, appear to be incompatible with neural learning, leading many researchers to abandon earlier-stage processing. Inspired by findings that continue to see benefit from various kinds of linguistic or domain-specific preprocessing (He et al., 2017; Oepen et al., 2017; Ji and Smith, 2017), we argue that pipelines can be treated as layers in neural architectures for NLP tasks. Several solutions are readily available: • Reinforcement learning (most notably the REINFORCE algorithm; Williams, 1992), and structured attention (SA; Kim et al., 2017). These methods replace argmax with a sampling or marginalization operation. We note two potential downsides of these approaches: (i) not all argmax-able operations have corresponding sampling or marginalization methods that are efficient, and (ii) inspection of intermediate outputs, which could benefit error analysis and system improvemen"
P18-1173,W16-5901,0,0.0224627,"ˆ with respect to derivative of each element of z each element of s. Unfortunately, argmax is a piecewise constant function, so its Jacobian is either zero (almost everywhere) or undefined (in the case of ties). One solution, taken in structured attention, is to replace the argmax with marginal inference and ˆ encodes probability a softmax function, so that z distributions over parts (Kim et al., 2017; Liu and Lapata, 2018). As discussed in §1, there are two reasons to avoid this modification. Softmax can only be used when marginal inference is feasible, by sum-product algorithms for example (Eisner, 2016; Friesen and Domingos, 2016); in general marginal inference can be #P-complete. Further, a soft intermediate layer will be less amenable to inspection by anyone wishing to understand and improve the model. In another line of work, argmax is augmented with a strongly-convex penalty on the solutions (Martins and Astudillo, 2016; Amos and Kolter, 2017; Niculae and Blondel, 2017; Niculae et al., 2018; Mensch and Blondel, 2018). However, their approaches require solving a relaxation even when exact decoding is tractable. Also, the penalty will bias the solutions found by the decoder, which may be"
P18-1173,C96-1058,0,0.472955,"We first briefly review the neural network architectures for the two models (§4.1.1), and then introduce the datasets (§4.1.2) and baselines (§4.1.3). 4.1.1 Architectures Syntactic dependency parser. For intermediate syntactic dependencies, we use the unlabeled arc-factored parser of Kiperwasser and Goldberg (2016). It uses bidirectional LSTMs (BiLSTM) to encode the input, followed by a multilayerperceptron (MLP) to score each potential dependency. One notable modification is that we replace their use of Chu-Liu/Edmonds’ algorithm (Chu and Liu, 1965; Edmonds, 1967) with the Eisner algorithm (Eisner, 1996, 2000), since our dataset is in English and mostly projective. Semantic dependency parser. We use the basic model of Peng et al. (2017) (denoted as N EUR BO PARSER ) as the end model. It is a first-order parser, and uses local factors for heads, unlabeled arcs, and labeled arcs. N EURBO PARSER does not use syntax. It first encodes an input sentence with a two-layer BiLSTM, and then computes part scores with two-layer tanh-MLPs. Inference is conducted with AD3 (Martins et al., 2015). To add syntactic features to N EURBO PARSER, we concatenate a token’s contextualized representation to that of"
P18-1173,P10-1074,0,0.0325355,"r modifications to maintain well-formedness of the tree. Figure 2 gives an example. 6 Related Work Joint learning in NLP pipelines. To avoid cascading errors, much effort has been devoted to joint decoding in NLP pipelines (Habash and Rambow, 2005; Cohen and Smith, 2007; Goldberg and Tsarfaty, 2008; Lewis et al., 2015; Zhang et al., 2015, inter alia). However, joint inference can sometimes be prohibitively expensive. Recent advances in representation learning facilitate exploration in the joint learning of multiple tasks by sharing parameters (Collobert and Weston, 2008; Blitzer et al., 2006; Finkel and Manning, 2010; Zhang and Weiss, 2016; Hashimoto et al., 2017, inter alia). Differentiable optimization. Gould et al. (2016) review the generic approaches to differentiation in bi-level optimization (Bard, 2010; Kunisch and Pock, 2013). Amos and Kolter (2017) extend their efforts to a class of subdifferentiable quadratic programs. However, they both require that the intermediate objective has an invertible Hessian, limiting their application 1870 in NLP. In another line of work, the steps of a gradient-based optimization procedure are unrolled into a single computation graph (Stoyanov et al., 2011; Domke, 2"
P18-1173,Q16-1023,0,0.238929,"n the top), and gold syntactic dependency tree (blue arcs at the bottom). A pretrained syntactic parser predicts the same tree as the gold; the semantic parser backpropagates into the intermediate syntactic parser, and changes the dashed blue arcs into dashed red arcs (§5). tic dependency parsing as the end task. We first briefly review the neural network architectures for the two models (§4.1.1), and then introduce the datasets (§4.1.2) and baselines (§4.1.3). 4.1.1 Architectures Syntactic dependency parser. For intermediate syntactic dependencies, we use the unlabeled arc-factored parser of Kiperwasser and Goldberg (2016). It uses bidirectional LSTMs (BiLSTM) to encode the input, followed by a multilayerperceptron (MLP) to score each potential dependency. One notable modification is that we replace their use of Chu-Liu/Edmonds’ algorithm (Chu and Liu, 1965; Edmonds, 1967) with the Eisner algorithm (Eisner, 1996, 2000), since our dataset is in English and mostly projective. Semantic dependency parser. We use the basic model of Peng et al. (2017) (denoted as N EUR BO PARSER ) as the end model. It is a first-order parser, and uses local factors for heads, unlabeled arcs, and labeled arcs. N EURBO PARSER does not"
P18-1173,D15-1169,0,0.0707756,"Missing"
P18-1173,Q18-1005,0,0.0770331,"For backpropascores s and outputs a solution z gation, to calculate gradients for parameters of s, the chain rule defines: ∇s L = J ∇zˆ L, (3) z where the Jacobian matrix J = ∂ˆ ∂s contains the ˆ with respect to derivative of each element of z each element of s. Unfortunately, argmax is a piecewise constant function, so its Jacobian is either zero (almost everywhere) or undefined (in the case of ties). One solution, taken in structured attention, is to replace the argmax with marginal inference and ˆ encodes probability a softmax function, so that z distributions over parts (Kim et al., 2017; Liu and Lapata, 2018). As discussed in §1, there are two reasons to avoid this modification. Softmax can only be used when marginal inference is feasible, by sum-product algorithms for example (Eisner, 2016; Friesen and Domingos, 2016); in general marginal inference can be #P-complete. Further, a soft intermediate layer will be less amenable to inspection by anyone wishing to understand and improve the model. In another line of work, argmax is augmented with a strongly-convex penalty on the solutions (Martins and Astudillo, 2016; Amos and Kolter, 2017; Niculae and Blondel, 2017; Niculae et al., 2018; Mensch and Bl"
P18-1173,P08-1043,0,0.0809498,"Missing"
P18-1173,J93-2004,0,0.0610317,"eresting. PSD UF LF UF LF – – 84.5 85.3 – – 75.3 76.4 STE 87.4 87.3 87.7 85.8 85.6 86.4 85.5 84.9 85.8 75.6 75.9 76.6 SPIGOT 87.9 86.7 85.5 77.1 N EURBO PARSER F REDA 3 PIPELINE SA (b) F1 on out-of-domain test set. Table 1: Semantic dependency parsing performance in both unlabeled (UF ) and labeled (LF ) F1 scores. Bold font indicates the best performance. Peng et al. (2017) does not report UF . 1,849 out-of-domain test instances from the Brown corpus.7 • For syntactic dependencies, we use the Stanford Dependency (de Marneffe and Manning, 2008) conversion of the the Penn Treebank WSJ portion (Marcus et al., 1993). To avoid data leak, we depart from standard split and use §20 and §21 as development and test data, and the remaining sections as training data. The number of training/dev./test instances is 40,265/2,012/1,671. Datasets • For semantic dependencies, we use the English dataset from SemEval 2015 Task 18 (Oepen et al., 2015). Among the three formalisms provided by the shared task, we consider DELPH-IN MRS-derived dependencies (DM) and Prague Semantic Dependencies (PSD).6 It includes §00–19 of the WSJ corpus as training data, §20 and §21 for development and in-domain test data, resulting in a 33,"
P18-1173,P05-1071,0,0.119381,"Missing"
P18-1173,D17-1206,0,0.0281609,"e tree. Figure 2 gives an example. 6 Related Work Joint learning in NLP pipelines. To avoid cascading errors, much effort has been devoted to joint decoding in NLP pipelines (Habash and Rambow, 2005; Cohen and Smith, 2007; Goldberg and Tsarfaty, 2008; Lewis et al., 2015; Zhang et al., 2015, inter alia). However, joint inference can sometimes be prohibitively expensive. Recent advances in representation learning facilitate exploration in the joint learning of multiple tasks by sharing parameters (Collobert and Weston, 2008; Blitzer et al., 2006; Finkel and Manning, 2010; Zhang and Weiss, 2016; Hashimoto et al., 2017, inter alia). Differentiable optimization. Gould et al. (2016) review the generic approaches to differentiation in bi-level optimization (Bard, 2010; Kunisch and Pock, 2013). Amos and Kolter (2017) extend their efforts to a class of subdifferentiable quadratic programs. However, they both require that the intermediate objective has an invertible Hessian, limiting their application 1870 in NLP. In another line of work, the steps of a gradient-based optimization procedure are unrolled into a single computation graph (Stoyanov et al., 2011; Domke, 2012; Goodfellow et al., 2013; Brakel et al., 20"
P18-1173,P17-1044,0,0.0239896,"iable functions that can be trained using gradient-based optimization. Yet traditional NLP often assumed modular stages of processing that formed a pipeline; e.g., text was tokenized, then tagged with parts of speech, then parsed into a phrase-structure or dependency tree, then semantically analyzed. Pipelines, which make “hard” (i.e., discrete) decisions at each stage, appear to be incompatible with neural learning, leading many researchers to abandon earlier-stage processing. Inspired by findings that continue to see benefit from various kinds of linguistic or domain-specific preprocessing (He et al., 2017; Oepen et al., 2017; Ji and Smith, 2017), we argue that pipelines can be treated as layers in neural architectures for NLP tasks. Several solutions are readily available: • Reinforcement learning (most notably the REINFORCE algorithm; Williams, 1992), and structured attention (SA; Kim et al., 2017). These methods replace argmax with a sampling or marginalization operation. We note two potential downsides of these approaches: (i) not all argmax-able operations have corresponding sampling or marginalization methods that are efficient, and (ii) inspection of intermediate outputs, which could ben"
P18-1173,P15-1162,0,0.0704195,"Missing"
P18-1173,P13-2109,1,0.894661,"Missing"
P18-1173,D15-1279,1,0.850605,"= [ h j ; h j ] at each position j as the contextualized token representations. We then concatenate 1867 hj with the representation of its head hHEAD(j) by  e j = [hj ; hHEAD(j) ] = hj ; h DM Model  X zˆσ(i→j) hi  , (7) ˆ ∈ Bn(n−1) is a binary encoding of the tree where z structure predicted by by the intermediate parser. e j anywhere hj would have been We then use h used in N EURBO PARSER. In backpropagation, we compute ∇zˆ L with an automatic differentiation toolkit (DyNet; Neubig et al., 2017). We note that this approach can be generalized to convolutional neural networks over graphs (Mou et al., 2015; Duvenaud et al., 2015; Kipf and Welling, 2017, inter alia), recurrent neural networks along paths (Xu et al., 2015; Roth and Lapata, 2016, inter alia) or dependency trees (Tai et al., 2015). We choose to use concatenations to control the model’s complexity, and thus to better understand which parts of the model work. We refer the readers to Kiperwasser and Goldberg (2016) and Peng et al. (2017) for further details of the parsing models. Training procedure. Following previous work, we minimize structured hinge loss (Tsochantaridis et al., 2004) for both models. We jointly train both models fr"
P18-1173,D13-1170,0,0.026467,"Missing"
P18-1173,P16-2038,0,0.0429373,"n both models from scratch, by randomly sampling an instance from the union of their training data at each step. In order to isolate the effect of backpropagation, we do not share any parameters between the two models.5 Implementation details are summarized in the supplementary materials. 4.1.2 UF LF UF LF – – 89.4 90.4 – – 77.6 78.5 STE 91.8 91.6 92.0 90.8 90.6 91.1 88.4 87.9 88.9 78.1 78.1 78.9 SPIGOT 92.4 91.6 88.6 78.9 N EURBO PARSER F REDA 3 i6=j PIPELINE SA (a) F1 on in-domain test set. DM Model 5 Parameter sharing has proved successful in many related tasks (Collobert and Weston, 2008; Søgaard and Goldberg, 2016; Ammar et al., 2016; Swayamdipta et al., 2016, 2017, inter alia), and could be easily combined with our approach. 6 We drop the third (PAS) because its structure is highly predictable from parts-of-speech, making it less interesting. PSD UF LF UF LF – – 84.5 85.3 – – 75.3 76.4 STE 87.4 87.3 87.7 85.8 85.6 86.4 85.5 84.9 85.8 75.6 75.9 76.6 SPIGOT 87.9 86.7 85.5 77.1 N EURBO PARSER F REDA 3 PIPELINE SA (b) F1 on out-of-domain test set. Table 1: Semantic dependency parsing performance in both unlabeled (UF ) and labeled (LF ) F1 scores. Bold font indicates the best performance. Peng et al. (201"
P18-1173,K16-1019,1,0.866923,"an instance from the union of their training data at each step. In order to isolate the effect of backpropagation, we do not share any parameters between the two models.5 Implementation details are summarized in the supplementary materials. 4.1.2 UF LF UF LF – – 89.4 90.4 – – 77.6 78.5 STE 91.8 91.6 92.0 90.8 90.6 91.1 88.4 87.9 88.9 78.1 78.1 78.9 SPIGOT 92.4 91.6 88.6 78.9 N EURBO PARSER F REDA 3 i6=j PIPELINE SA (a) F1 on in-domain test set. DM Model 5 Parameter sharing has proved successful in many related tasks (Collobert and Weston, 2008; Søgaard and Goldberg, 2016; Ammar et al., 2016; Swayamdipta et al., 2016, 2017, inter alia), and could be easily combined with our approach. 6 We drop the third (PAS) because its structure is highly predictable from parts-of-speech, making it less interesting. PSD UF LF UF LF – – 84.5 85.3 – – 75.3 76.4 STE 87.4 87.3 87.7 85.8 85.6 86.4 85.5 84.9 85.8 75.6 75.9 76.6 SPIGOT 87.9 86.7 85.5 77.1 N EURBO PARSER F REDA 3 PIPELINE SA (b) F1 on out-of-domain test set. Table 1: Semantic dependency parsing performance in both unlabeled (UF ) and labeled (LF ) F1 scores. Bold font indicates the best performance. Peng et al. (2017) does not report UF . 1,849 out-of-domain te"
P18-1173,S15-2153,0,0.308993,"Missing"
P18-1173,L16-1630,0,0.0429082,"Missing"
P18-1173,S14-2008,0,0.0543016,"s et al., 2013). 1866 Line 3 forms a singly constrained QP, and can be solved in O(n) time (Brucker, 1984). Algorithm 2 Projection onto the relaxed polytope PDEP for dependency tree structures. Let bold σ(·→j) denote the index set of arcs incoming to j. For a vector v, we use vσ(·→j) to denote vector [vk ]k∈σ(·→j) . 1: procedure D EP P ROJ(ˆ p) 2: for j = 1, 2, . . . , n do  ˜σ(·→j) ← proj∆n−2 p ˆ σ(·→j) 3: z 4: end for ˜ 5: return z 6: end procedure First-order semantic dependency parsing. Semantic dependency parsing uses labeled bilexical dependencies to represent sentence-level semantics (Oepen et al., 2014, 2015, 2016). Each dependency is represented by a labeled directed arc from a head token to a modifier token, where the arc label encodes broadly applicable semantic relations. Figure 2 diagrams a semantic graph from the DELPH-IN MRS-derived dependencies (DM), together with a syntactic tree. We use a state-of-the-art semantic dependency parser (Peng et al., 2017) that considers three types of parts: heads, unlabeled arcs, and labeled arcs. ` Let σ(i → j) denote the index of the arc from i to j with semantic role `. In addition to [0, 1] constraints, we constrain that the predictions for label"
P18-1173,P17-1186,1,0.869174,". . , n do  ˜σ(·→j) ← proj∆n−2 p ˆ σ(·→j) 3: z 4: end for ˜ 5: return z 6: end procedure First-order semantic dependency parsing. Semantic dependency parsing uses labeled bilexical dependencies to represent sentence-level semantics (Oepen et al., 2014, 2015, 2016). Each dependency is represented by a labeled directed arc from a head token to a modifier token, where the arc label encodes broadly applicable semantic relations. Figure 2 diagrams a semantic graph from the DELPH-IN MRS-derived dependencies (DM), together with a syntactic tree. We use a state-of-the-art semantic dependency parser (Peng et al., 2017) that considers three types of parts: heads, unlabeled arcs, and labeled arcs. ` Let σ(i → j) denote the index of the arc from i to j with semantic role `. In addition to [0, 1] constraints, we constrain that the predictions for labeled arcs sum to the prediction of their associated unlabeled arc: ( ) X d p ` = pσ(i→j) , ∀i 6= j . PSDP p ∈ U σ(i→j) ` (6) This ensures that exactly one label is predicted if and only if its arc is present. The projection onto PSDP can be solved similarly to Algorithm 2. We drop the determinism constraint imposed by Peng et al. (2017) in the backward computation."
P18-1173,N18-1135,1,0.877942,"Missing"
P18-1173,W04-2401,0,0.203195,"Missing"
P18-1173,P16-1113,0,0.0160836,"f its head hHEAD(j) by  e j = [hj ; hHEAD(j) ] = hj ; h DM Model  X zˆσ(i→j) hi  , (7) ˆ ∈ Bn(n−1) is a binary encoding of the tree where z structure predicted by by the intermediate parser. e j anywhere hj would have been We then use h used in N EURBO PARSER. In backpropagation, we compute ∇zˆ L with an automatic differentiation toolkit (DyNet; Neubig et al., 2017). We note that this approach can be generalized to convolutional neural networks over graphs (Mou et al., 2015; Duvenaud et al., 2015; Kipf and Welling, 2017, inter alia), recurrent neural networks along paths (Xu et al., 2015; Roth and Lapata, 2016, inter alia) or dependency trees (Tai et al., 2015). We choose to use concatenations to control the model’s complexity, and thus to better understand which parts of the model work. We refer the readers to Kiperwasser and Goldberg (2016) and Peng et al. (2017) for further details of the parsing models. Training procedure. Following previous work, we minimize structured hinge loss (Tsochantaridis et al., 2004) for both models. We jointly train both models from scratch, by randomly sampling an instance from the union of their training data at each step. In order to isolate the effect of backprop"
P18-1173,P15-1150,0,0.0972052,"Missing"
P18-1173,D15-1206,1,0.831085,"representation of its head hHEAD(j) by  e j = [hj ; hHEAD(j) ] = hj ; h DM Model  X zˆσ(i→j) hi  , (7) ˆ ∈ Bn(n−1) is a binary encoding of the tree where z structure predicted by by the intermediate parser. e j anywhere hj would have been We then use h used in N EURBO PARSER. In backpropagation, we compute ∇zˆ L with an automatic differentiation toolkit (DyNet; Neubig et al., 2017). We note that this approach can be generalized to convolutional neural networks over graphs (Mou et al., 2015; Duvenaud et al., 2015; Kipf and Welling, 2017, inter alia), recurrent neural networks along paths (Xu et al., 2015; Roth and Lapata, 2016, inter alia) or dependency trees (Tai et al., 2015). We choose to use concatenations to control the model’s complexity, and thus to better understand which parts of the model work. We refer the readers to Kiperwasser and Goldberg (2016) and Peng et al. (2017) for further details of the parsing models. Training procedure. Following previous work, we minimize structured hinge loss (Tsochantaridis et al., 2004) for both models. We jointly train both models from scratch, by randomly sampling an instance from the union of their training data at each step. In order to isolate"
P18-1173,J16-3001,0,0.0830221,"Missing"
P18-1173,N15-1005,0,0.048558,"Missing"
P18-1173,P16-1147,0,0.0207504,"n well-formedness of the tree. Figure 2 gives an example. 6 Related Work Joint learning in NLP pipelines. To avoid cascading errors, much effort has been devoted to joint decoding in NLP pipelines (Habash and Rambow, 2005; Cohen and Smith, 2007; Goldberg and Tsarfaty, 2008; Lewis et al., 2015; Zhang et al., 2015, inter alia). However, joint inference can sometimes be prohibitively expensive. Recent advances in representation learning facilitate exploration in the joint learning of multiple tasks by sharing parameters (Collobert and Weston, 2008; Blitzer et al., 2006; Finkel and Manning, 2010; Zhang and Weiss, 2016; Hashimoto et al., 2017, inter alia). Differentiable optimization. Gould et al. (2016) review the generic approaches to differentiation in bi-level optimization (Bard, 2010; Kunisch and Pock, 2013). Amos and Kolter (2017) extend their efforts to a class of subdifferentiable quadratic programs. However, they both require that the intermediate objective has an invertible Hessian, limiting their application 1870 in NLP. In another line of work, the steps of a gradient-based optimization procedure are unrolled into a single computation graph (Stoyanov et al., 2011; Domke, 2012; Goodfellow et al.,"
P18-1173,S14-2082,0,\N,Missing
P18-1173,D14-1162,0,\N,Missing
P18-1189,N18-1034,0,0.0201955,"move up in the space. If we wanted to look at each year individually, we could drop the embedding of years, and learn a sparse set of topic-year interactions, similar to tone in Table 3. 5 Additional Related Work The literature on topic models is vast; in addition to papers cited throughout, other efforts to incorporate metadata into topic models include Dirichletmultinomial regression (DMR; Mimno and McCallum, 2008), Labeled LDA (Ramage et al., 2009), and MedLDA (Zhu et al., 2009). A recent paper also extended DMR by using deep neural networks to embed metadata into a richer document prior (Benton and Dredze, 2018). A separate line of work has pursued parameterizing unsupervised models of documents using neural networks (Hinton and Salakhutdinov, 2038 Base topics (each row is a topic) ice customs agency enforcement homeland population born percent americans english judge case court guilty appeals attorney patrol border miles coast desert boat guard licenses drivers card visa cards applicants island story chinese ellis international guest worker workers bush labor bill benefits bill welfare republican state senate Anti-immigration interactions criminal customs arrested jobs million illegals taxpayers gui"
P18-1189,P15-2072,1,0.819845,"est data), and externally, using a decade of articles from the English Gigaword dataset (Graff and Cieri, 2003). Since our model employs variational methods, the reported perplexity is an upper bound based on the ELBO. As datasets we use the familiar 20 newsgroups, the IMDB corpus of 50,000 movie reviews (Maas et al., 2011), and the UIUC Yahoo answers dataset with 150,000 documents in 15 categories (Chang et al., 2008). For further exploration, we also make use of a corpus of approximately 4,000 timestamped news articles about US immigration, each annotated with pro- or anti-immigration tone (Card et al., 2015). We use the original author-provided implementations of SAGE11 and SLDA,12 while for LDA we use Mallet.13 . Our implementation of SCHOLAR is in TensorFlow, but we have also provided a preliminary PyTorch implementation of the core of our model.14 For additional details about datasets and implementation, please refer to the supplementary material. It is challenging to fairly evaluate the relative computational efficiency of our approach compared to past work (due to the stochastic nature of our ap2036 11 github.com/jacobeisenstein/SAGE github.com/blei-lab/class-slda 13 mallet.cs.umass.edu 14 g"
P18-1189,P15-1077,0,0.0450079,"s of unlabeled data, and may promote greater coherence in inferred topics. The same could also be done for some covariates; for example, we could embed the source of a news article based on its place on the ideological spectrum. Conversely, if we choose to learn these parameters, the learned values (Wy and Wc ) may provide meaningful embeddings of these metadata (see section §4.3). Other variants on topic models have also proposed incorporating word vectors, both as a parallel part of the generative process (Nguyen et al., 2015a), and as an alternative parameterization of topic distributions (Das et al., 2015), but inference is not scalable in either of these models. Because of the generality of the VAE framework, we could Experiments and Results To evaluate and demonstrate the potential of this model, we present a series of experiments below. We first test SCHOLAR without observed metadata, and explore the effects of using regularization and/or word vector initialization, compared to LDA, SAGE, and NVDM (§4.1). We then evaluate our model in terms of predictive performance, in comparison to SLDA and an l2 -regularized logistic regression baseline (§4.2). Finally, we demonstrate the ability to incor"
P18-1189,P17-1033,0,0.0192474,"california gov state Pro-immigration interactions detainees detention center agency english newcomers hispanic city asylum court judge case appeals died authorities desert border bodies green citizenship card citizen apply island school ellis english story workers tech skilled farm labor law welfare students tuition Table 3: Top words for topics (left) and the corresponding anti-immigration (middle) and pro-immigration (right) variations when treating tone as a covariate, with interactions. 2009; Larochelle and Lauly, 2012), including nonBayesian approaches (Cao et al., 2015). More recently, Lau et al. (2017) proposed a neural language model that incorporated topics, and He et al. (2017) developed a scalable alternative to the correlated topic model by simultaneously learning topic embeddings. Others have attempted to extend the reparameterization trick to the Dirichlet and Gamma distributions, either through transformations (Kucukelbir et al., 2016) or a generalization of reparameterization (Ruiz et al., 2016). Black-box and VAE-style inference have been implemented in at least two general purpose tools designed to allow rapid exploration and evaluation of models (Kucukelbir et al., 2015; Tran et"
P18-1189,P11-1015,0,0.0489923,"measured in terms of perplexity; coherence, measured in terms of non-negative point-wise mutual information (NPMI; Chang et al., 2009; Newman et al., 2010), and classification accuracy on test data. For coherence we evaluate NPMI using the top 10 words of each topic, both internally (using test data), and externally, using a decade of articles from the English Gigaword dataset (Graff and Cieri, 2003). Since our model employs variational methods, the reported perplexity is an upper bound based on the ELBO. As datasets we use the familiar 20 newsgroups, the IMDB corpus of 50,000 movie reviews (Maas et al., 2011), and the UIUC Yahoo answers dataset with 150,000 documents in 15 categories (Chang et al., 2008). For further exploration, we also make use of a corpus of approximately 4,000 timestamped news articles about US immigration, each annotated with pro- or anti-immigration tone (Card et al., 2015). We use the original author-provided implementations of SAGE11 and SLDA,12 while for LDA we use Mallet.13 . Our implementation of SCHOLAR is in TensorFlow, but we have also provided a preliminary PyTorch implementation of the core of our model.14 For additional details about datasets and implementation, p"
P18-1189,P15-1139,0,0.166201,"articles about US immigration. 1 Introduction Topic models comprise a family of methods for uncovering latent structure in text corpora, and are widely used tools in the digital humanities, political science, and other related fields (Boyd-Graber et al., 2017). Latent Dirichlet allocation (LDA; Blei et al., 2003) is often used when there is no prior knowledge about a corpus. In the real world, however, most documents have non-textual attributes such as author (Rosen-Zvi et al., 2004), timestamp (Blei and Lafferty, 2006), rating (McAuliffe and Blei, 2008), or ideology (Eisenstein et al., 2011; Nguyen et al., 2015b), which we refer to as metadata. Many customizations of LDA have been developed to incorporate document metadata. Two models of note are supervised LDA (SLDA; McAuliffe and Blei, 2008), which jointly models words and labels (e.g., ratings) as being generated from a latent representation, and sparse additive generative models (SAGE; Eisenstein et al., 2011), which assumes that observed covariates (e.g., author ideology) have a sparse effect on the relative probabilities of words given topics. The structural topic model (STM; Roberts et al., 2014), which adds correlations between topics to SAG"
P18-1189,D09-1026,0,0.088791,"ddings of year-ofpublication (treated as a covariate) from combined model of news articles about immigration. hijackers, and attacks increasing in probability as we move up in the space. If we wanted to look at each year individually, we could drop the embedding of years, and learn a sparse set of topic-year interactions, similar to tone in Table 3. 5 Additional Related Work The literature on topic models is vast; in addition to papers cited throughout, other efforts to incorporate metadata into topic models include Dirichletmultinomial regression (DMR; Mimno and McCallum, 2008), Labeled LDA (Ramage et al., 2009), and MedLDA (Zhu et al., 2009). A recent paper also extended DMR by using deep neural networks to embed metadata into a richer document prior (Benton and Dredze, 2018). A separate line of work has pursued parameterizing unsupervised models of documents using neural networks (Hinton and Salakhutdinov, 2038 Base topics (each row is a topic) ice customs agency enforcement homeland population born percent americans english judge case court guilty appeals attorney patrol border miles coast desert boat guard licenses drivers card visa cards applicants island story chinese ellis international guest"
P18-1189,N10-1012,0,0.179161,"hout observed metadata, and explore the effects of using regularization and/or word vector initialization, compared to LDA, SAGE, and NVDM (§4.1). We then evaluate our model in terms of predictive performance, in comparison to SLDA and an l2 -regularized logistic regression baseline (§4.2). Finally, we demonstrate the ability to incorporate covariates and/or labels in an exploratory data analysis (§4.3). The scores we report are generalization to heldout data, measured in terms of perplexity; coherence, measured in terms of non-negative point-wise mutual information (NPMI; Chang et al., 2009; Newman et al., 2010), and classification accuracy on test data. For coherence we evaluate NPMI using the top 10 words of each topic, both internally (using test data), and externally, using a decade of articles from the English Gigaword dataset (Graff and Cieri, 2003). Since our model employs variational methods, the reported perplexity is an upper bound based on the ELBO. As datasets we use the familiar 20 newsgroups, the IMDB corpus of 50,000 movie reviews (Maas et al., 2011), and the UIUC Yahoo answers dataset with 150,000 documents in 15 categories (Chang et al., 2008). For further exploration, we also make u"
P18-1189,Q15-1022,0,0.14849,"articles about US immigration. 1 Introduction Topic models comprise a family of methods for uncovering latent structure in text corpora, and are widely used tools in the digital humanities, political science, and other related fields (Boyd-Graber et al., 2017). Latent Dirichlet allocation (LDA; Blei et al., 2003) is often used when there is no prior knowledge about a corpus. In the real world, however, most documents have non-textual attributes such as author (Rosen-Zvi et al., 2004), timestamp (Blei and Lafferty, 2006), rating (McAuliffe and Blei, 2008), or ideology (Eisenstein et al., 2011; Nguyen et al., 2015b), which we refer to as metadata. Many customizations of LDA have been developed to incorporate document metadata. Two models of note are supervised LDA (SLDA; McAuliffe and Blei, 2008), which jointly models words and labels (e.g., ratings) as being generated from a latent representation, and sparse additive generative models (SAGE; Eisenstein et al., 2011), which assumes that observed covariates (e.g., author ideology) have a sparse effect on the relative probabilities of words given topics. The structural topic model (STM; Roberts et al., 2014), which adds correlations between topics to SAG"
P18-1189,D11-1055,1,0.844275,"Missing"
P18-1189,Q16-1017,0,\N,Missing
P18-2106,D15-1245,0,0.0307139,"rsing. In addition to sharing features with multilingual word vectors, they use them to find word translations of target language words for additional lexical features. Other polyglot models have been proposed for semantics. Richardson et al. (2018) train on multiple (natural language)-(programming language) pairs to improve a model that translates API text into code signature representations. Duong et al. (2017) treat English and German semantic parsing as a multi-task learning problem and saw improvement over monolingual baselines, especially for small datasets. Most relevant to our work is Johannsen et al. (2015), which trains a polyglot 6 Conclusion In this work, we have explored a straightforward method for polyglot training in SRL: use multilingual word vectors and combine training data across languages. This allows sharing without crosslingual alignments, shared annotation, or parallel data. We demonstrate that a polyglot model can outperform a monolingual one for semantic analysis, particularly for languages with less data. 671 Acknowledgments Sepp Hochreiter and J¨urgen Schmidhuber. 1997. Long short-term memory. Neural Computation, 9(8):1735–1780. We thank Luke Zettlemoyer, Luheng He, and the an"
P18-2106,C12-1089,0,0.0400356,"nguage and English by using pretrained multilingual embeddings. This polyglot model is trained on the union of annotations in the two languages. We use stratified sampling to give the two datasets equal effective weight in training, and we ensure that every training instance is seen at least once per epoch. 4 Pretrained multilingual embeddings. The basis of our polyglot training is the use of pretrained multilingual word vectors, which allow representing entirely distinct vocabularies (such as the tokens of different languages) in a shared representation space, allowing crosslingual learning (Klementiev et al., 2012). We produced multilingual embeddings from the monolingual embeddings using the method of Ammar et al. (2016b): for each non-English language, a small crosslingual dictionary and canonical correlation analysis was used to find a transformation of the non-English vectors into the English vector space (Faruqui and Dyer, 2014). Unlike multilingual word representations, argument label sets are disjoint between language pairs, and correspondences are not clearly defined. Hence, we use separate label representations for each language’s labels. Similarly, while (for example) ENG:look and SPA:mira may"
P18-2106,K17-1041,0,0.422248,", the CoNLL 2009 shared task requires disambiguation of the sense of the predicate, and labeling all its dependent arguments. The shared task assumed predicates have already been identified, hence we do not handle the predicate identification task. Our basic model adapts the span-based dependency SRL model of He et al. (2017). This adaptation treats the dependent arguments as argument spans of length 1. Additionally, BIO consistency constraints are removed from the original model— each token is tagged simply with the argument label or an empty tag. A similar approach has also been proposed by Marcheggiani et al. (2017). The input to the model consists of a sequence of pretrained embeddings for the surface forms of the sentence tokens. Each token embedding is also concatenated with a vector indicating whether the word is a predicate or not. Since the part-ofspeech tags in the CoNLL 2009 dataset are based on a different tagset for each language, we do not use these. Each training instance consists of the annotations for a single predicate. These representations are then passed through a deep, multilayer bidirectional LSTM (Graves, 2013; Hochreiter and Schmidhuber, 1997) with highway connections (Srivastava et"
P18-2106,D17-1159,0,0.0604907,"pta et al. (2016) present a transition-based stack LSTM model that predicts syntax and semantics jointly, as a remedy to the reliance on pipelined models. Guo et al. (2016) and Roth and Lapata (2016) use deep biLSTM architectures which use syntactic information to guide the composition. Marcheggiani et al. (2017) use a simple LSTM model over word tokens to tag semantic dependencies, like our model. Their model predicts a token’s label based on the combination of the token vector and the predicate vector, and saw benefits from using POS tags, both improvements that could be added to our model. Marcheggiani and Titov (2017) apply the recently-developed graph 670 Model CAT CES DEU ENG JPN SPA ZHO Marcheggiani et al. (2017) Best previously reported 80.32 86.00 86.00 80.10 87.60 89.10 78.15 80.30 80.50 81.20 81.20 Monolingual + ENG(simple polyglot) + ENG(language ID) + ENG(language-specific LSTMs) 77.31 79.08 79.05 79.45 84.87 84.82 85.14 84.78 66.71 69.97 69.49 68.30 86.54 – – – 74.99 76.00 75.77 75.88 75.98 76.45 77.32 76.86 81.26 81.50 81.42 81.89 Table 2: Semantic F1 scores (including predicate sense disambiguation) on the CoNLL 2009 dataset. State of the art for Catalan and Japanese is from Zhao et al. (2009),"
P18-2106,Q16-1031,1,0.931097,"differences between the monolingual and polyglot models. Our findings indicate that polyglot training consistently improves label accuracy for common labels. The standard approach to multilingual NLP is to design a single architecture, but tune and train a separate model for each language. While this method allows for customizing the model to the particulars of each language and the available data, it also presents a problem when little data is available: extensive language-specific annotation is required. The reality is that most languages have very little annotated data for most NLP tasks. Ammar et al. (2016a) found that using training data from multiple languages annotated with Universal Dependencies (Nivre et al., 2016), and represented using multilingual word vectors, outperformed monolingual training. Inspired by this, we apply the idea of training one model on multiple languages—which we call polyglot training— to PropBank-style semantic role labeling (SRL). We train several parsers for each language in the CoNLL 2009 dataset (Hajiˇc et al., 2009): a tra667 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 667–672 c Melbourne, Austr"
P18-2106,L16-1262,0,0.0849136,"Missing"
P18-2106,P07-1033,0,0.206471,"Missing"
P18-2106,J05-1004,0,0.143441,"r system on the semantic role labeling portion of the CoNLL-2009 shared task (Hajiˇc et al., 2009), on all seven languages, namely Catalan, Chinese, Czech, English, German, Japanese and Spanish. For each language, certain tokens in each sentence in the dataset are marked as predicates. Each predicate takes as arguments other words in the same sentence, their relationship marked by labeled dependency arcs. Sentences may contain no predicates. Despite the consistency of this format, there are significant differences between the training sets across languages.1 English uses PropBank role labels (Palmer et al., 2005). Catalan, Chinese, English, German, and Spanish include (but are not limited to) labels such as “arg0 -agt” (for “agent”) or “A0 ” that may correspond to some degree to each other and to the English roles. Catalan and Spanish share most labels (being drawn from the same source corpus, AnCora; Taul´e et al., 2008), and English and German share some labels. Czech and Japanese each have their own distinct sets of argument labels, most of which do not have clear correspondences to English or to each other. We also note that, due to semi-automatic projection of annotations to construct the German"
P18-2106,K17-1038,0,0.02369,"olutional networks to SRL, obtaining state of the art results on English and Chinese. All of these approaches are orthogonal to ours, and might benefit from polyglot training. model for frame-semantic parsing. In addition to sharing features with multilingual word vectors, they use them to find word translations of target language words for additional lexical features. Other polyglot models have been proposed for semantics. Richardson et al. (2018) train on multiple (natural language)-(programming language) pairs to improve a model that translates API text into code signature representations. Duong et al. (2017) treat English and German semantic parsing as a multi-task learning problem and saw improvement over monolingual baselines, especially for small datasets. Most relevant to our work is Johannsen et al. (2015), which trains a polyglot 6 Conclusion In this work, we have explored a straightforward method for polyglot training in SRL: use multilingual word vectors and combine training data across languages. This allows sharing without crosslingual alignments, shared annotation, or parallel data. We demonstrate that a polyglot model can outperform a monolingual one for semantic analysis, particularl"
P18-2106,D14-1162,0,0.0809685,"redicate. 3.1 mira.01 may not correspond. Hence, predicate sense representations are also language-specific. 3.3 In the second variant, we concatenate a language ID vector to each multilingual word embedding and predicate indicator feature in the input representation. This vector is randomly initialized and updated in training. These additional parameters provide a small degree of language-specificity in the model, while still sharing most parameters. Monolingual Baseline We use pretrained word embeddings as input to the model. For each of the shared task languages, we produced GloVe vectors (Pennington et al., 2014) from the news, web, and Wikipedia text of the Leipzig Corpora Collection (Goldhahn et al., 2012).2 We trained 300-dimensional vectors, then reduced them to 100 dimensions with principal component analysis for efficiency. 3.2 Language Identification 3.4 Language-Specific LSTMs This third variant takes inspiration from the “frustratingly easy” architecture of Daume III (2007) for domain adaptation. In addition to processing every example with a shared biLSTM as in previous models, we add language-specific biLSTMs that are trained only on the examples belonging to one language. Each of these lan"
P18-2106,E14-1049,0,0.0319071,"ilingual embeddings. The basis of our polyglot training is the use of pretrained multilingual word vectors, which allow representing entirely distinct vocabularies (such as the tokens of different languages) in a shared representation space, allowing crosslingual learning (Klementiev et al., 2012). We produced multilingual embeddings from the monolingual embeddings using the method of Ammar et al. (2016b): for each non-English language, a small crosslingual dictionary and canonical correlation analysis was used to find a transformation of the non-English vectors into the English vector space (Faruqui and Dyer, 2014). Unlike multilingual word representations, argument label sets are disjoint between language pairs, and correspondences are not clearly defined. Hence, we use separate label representations for each language’s labels. Similarly, while (for example) ENG:look and SPA:mira may be semantically connected, the senses look.01 and Experiments We present our results in Table 2. We observe that simple polyglot training improves over monolingual training, with the exception of Czech, where we observe no change in performance. The languages with the fewest training examples (German, Japanese, Catalan) sh"
P18-2106,N18-1066,0,0.0271702,"ES DEU ENG JPN SPA ZHO 93.92 94.09 91.92 91.97 87.95 89.01 92.87 – 85.55 86.17 93.61 93.65 87.93 87.90 Table 5: Unlabeled semantic F1 scores on the CoNLL 2009 dataset. convolutional networks to SRL, obtaining state of the art results on English and Chinese. All of these approaches are orthogonal to ours, and might benefit from polyglot training. model for frame-semantic parsing. In addition to sharing features with multilingual word vectors, they use them to find word translations of target language words for additional lexical features. Other polyglot models have been proposed for semantics. Richardson et al. (2018) train on multiple (natural language)-(programming language) pairs to improve a model that translates API text into code signature representations. Duong et al. (2017) treat English and German semantic parsing as a multi-task learning problem and saw improvement over monolingual baselines, especially for small datasets. Most relevant to our work is Johannsen et al. (2015), which trains a polyglot 6 Conclusion In this work, we have explored a straightforward method for polyglot training in SRL: use multilingual word vectors and combine training data across languages. This allows sharing without"
P18-2106,goldhahn-etal-2012-building,0,0.0277314,"specific. 3.3 In the second variant, we concatenate a language ID vector to each multilingual word embedding and predicate indicator feature in the input representation. This vector is randomly initialized and updated in training. These additional parameters provide a small degree of language-specificity in the model, while still sharing most parameters. Monolingual Baseline We use pretrained word embeddings as input to the model. For each of the shared task languages, we produced GloVe vectors (Pennington et al., 2014) from the news, web, and Wikipedia text of the Leipzig Corpora Collection (Goldhahn et al., 2012).2 We trained 300-dimensional vectors, then reduced them to 100 dimensions with principal component analysis for efficiency. 3.2 Language Identification 3.4 Language-Specific LSTMs This third variant takes inspiration from the “frustratingly easy” architecture of Daume III (2007) for domain adaptation. In addition to processing every example with a shared biLSTM as in previous models, we add language-specific biLSTMs that are trained only on the examples belonging to one language. Each of these languagespecific biLSTMs is two layers deep, and is combined with the shared biSLTM in the input to"
P18-2106,P16-1113,0,0.205232,"and LOC. While the effect sizes are small (consistent with other languages), the overall F1 score on Czech decreases slightly in the polyglot condition. It may be that the Czech dataset is too large to make use of the comparatively small amount of English data, or that differences in the annotation schemes prevent 5 Related Work Recent improvements in multilingual SRL can be attributed to neural architectures. Swayamdipta et al. (2016) present a transition-based stack LSTM model that predicts syntax and semantics jointly, as a remedy to the reliance on pipelined models. Guo et al. (2016) and Roth and Lapata (2016) use deep biLSTM architectures which use syntactic information to guide the composition. Marcheggiani et al. (2017) use a simple LSTM model over word tokens to tag semantic dependencies, like our model. Their model predicts a token’s label based on the combination of the token vector and the predicate vector, and saw benefits from using POS tags, both improvements that could be added to our model. Marcheggiani and Titov (2017) apply the recently-developed graph 670 Model CAT CES DEU ENG JPN SPA ZHO Marcheggiani et al. (2017) Best previously reported 80.32 86.00 86.00 80.10 87.60 89.10 78.15 80"
P18-2106,C16-1120,0,0.0199543,"ment labels, e.g., PAT and LOC. While the effect sizes are small (consistent with other languages), the overall F1 score on Czech decreases slightly in the polyglot condition. It may be that the Czech dataset is too large to make use of the comparatively small amount of English data, or that differences in the annotation schemes prevent 5 Related Work Recent improvements in multilingual SRL can be attributed to neural architectures. Swayamdipta et al. (2016) present a transition-based stack LSTM model that predicts syntax and semantics jointly, as a remedy to the reliance on pipelined models. Guo et al. (2016) and Roth and Lapata (2016) use deep biLSTM architectures which use syntactic information to guide the composition. Marcheggiani et al. (2017) use a simple LSTM model over word tokens to tag semantic dependencies, like our model. Their model predicts a token’s label based on the combination of the token vector and the predicate vector, and saw benefits from using POS tags, both improvements that could be added to our model. Marcheggiani and Titov (2017) apply the recently-developed graph 670 Model CAT CES DEU ENG JPN SPA ZHO Marcheggiani et al. (2017) Best previously reported 80.32 86.00 86.00"
P18-2106,K16-1019,1,0.828164,"uages, and consistently results in overall gains from polyglot training. One exception is in Czech, where polyglot training reduces accuracy on several common argument labels, e.g., PAT and LOC. While the effect sizes are small (consistent with other languages), the overall F1 score on Czech decreases slightly in the polyglot condition. It may be that the Czech dataset is too large to make use of the comparatively small amount of English data, or that differences in the annotation schemes prevent 5 Related Work Recent improvements in multilingual SRL can be attributed to neural architectures. Swayamdipta et al. (2016) present a transition-based stack LSTM model that predicts syntax and semantics jointly, as a remedy to the reliance on pipelined models. Guo et al. (2016) and Roth and Lapata (2016) use deep biLSTM architectures which use syntactic information to guide the composition. Marcheggiani et al. (2017) use a simple LSTM model over word tokens to tag semantic dependencies, like our model. Their model predicts a token’s label based on the combination of the token vector and the predicate vector, and saw benefits from using POS tags, both improvements that could be added to our model. Marcheggiani and"
P18-2106,taule-etal-2008-ancora,0,0.162698,"Missing"
P18-2106,W09-1209,0,0.469257,"ni and Titov (2017) apply the recently-developed graph 670 Model CAT CES DEU ENG JPN SPA ZHO Marcheggiani et al. (2017) Best previously reported 80.32 86.00 86.00 80.10 87.60 89.10 78.15 80.30 80.50 81.20 81.20 Monolingual + ENG(simple polyglot) + ENG(language ID) + ENG(language-specific LSTMs) 77.31 79.08 79.05 79.45 84.87 84.82 85.14 84.78 66.71 69.97 69.49 68.30 86.54 – – – 74.99 76.00 75.77 75.88 75.98 76.45 77.32 76.86 81.26 81.50 81.42 81.89 Table 2: Semantic F1 scores (including predicate sense disambiguation) on the CoNLL 2009 dataset. State of the art for Catalan and Japanese is from Zhao et al. (2009), for German and Spanish from Roth and Lapata (2016), for English and Chinese from Marcheggiani and Titov (2017). Italics indicate use of syntax. arg0 arg1 arg2 arg3 arg4 argL argM Gold label count (CAT) Monolingual CAT F1 + ENG improvement 2117 82.06 +2.75 4296 79.06 +2.58 1713 68.95 +4.53 61 28.89 +18.17 71 42.42 +9.81 49 39.51 +1.35 2968 60.85 +1.10 Gold label count (SPA) Monolingual SPA F1 + ENG improvement 2438 82.44 +0.37 4295 77.93 +0.43 1677 70.24 +1.35 49 28.89 -3.40 82 41.15 -3.48 46 22.50 +4.01 3237 58.89 +1.26 Table 3: Per-label breakdown of F1 scores for Catalan and Spanish. These"
P18-2106,P17-1044,0,0.133241,"by far the fewest training examples (predicate-argument structures); see Table 1. 1 This is expected, as the datasets were annotated independently under diverse formalisms and only later converted into CoNLL format (Hajiˇc et al., 2009). 668 Model Given a sentence with a marked predicate, the CoNLL 2009 shared task requires disambiguation of the sense of the predicate, and labeling all its dependent arguments. The shared task assumed predicates have already been identified, hence we do not handle the predicate identification task. Our basic model adapts the span-based dependency SRL model of He et al. (2017). This adaptation treats the dependent arguments as argument spans of length 1. Additionally, BIO consistency constraints are removed from the original model— each token is tagged simply with the argument label or an empty tag. A similar approach has also been proposed by Marcheggiani et al. (2017). The input to the model consists of a sequence of pretrained embeddings for the surface forms of the sentence tokens. Each token embedding is also concatenated with a vector indicating whether the word is a predicate or not. Since the part-ofspeech tags in the CoNLL 2009 dataset are based on a diffe"
P19-1164,P06-1084,0,0.0264317,"erms of alphabet, word order, or grammar), while still allowing for highly accurate automatic morphological analysis. These languages belong to four different families: (1) Romance languages: Spanish, French, and Italian, all of which have gendered noun-determiner agreement and spaCy morphological analysis support (Honnibal and Montani, 2017). (2) Slavic languages (Cyrillic alphabet): Russian and Ukrainian, for which we use the morphological analyzer developed by Korobov (2015). (3) Semitic languages: Hebrew and Arabic, each with a unique alphabet. For Hebrew, we use the analyzer developed by Adler and Elhadad (2006), while gender inflection in Arabic can be easily identified via the ta marbuta character, which uniquely indicates feminine inflection. (4) Germanic languages: German, for which we Results Our main findings are presented in Tables 2 and 3. For each tested MT system and target language we compute three metrics with respect to their ability to convey the correct gender in the target language. Ultimately, our analyses indicate that all tested MT systems are indeed gender biased. First, the overall system Accuracy is calculated by the percentage of instances in which the translation preserved the"
P19-1164,D17-1042,0,0.0262651,"Missing"
P19-1164,N18-1118,0,0.0150505,"h or “soldat” (soldier) in French, which do not have female inflections. See Table 5 for detailed examples. 4 Discussion Related work This work is most related to several recent efforts which evaluate MT through the use of challenge sets. Similarly to our use WinoMT, these works evaluate MT systems (either manually or automatically) on test sets which are specially created to exhibit certain linguistic phenomena, thus going beyond the traditional BLEU metric (Papineni et al., 2002). These include challenge sets for language-specific idiosyncrasies (Isabelle et al., 2017), discourse phenomena (Bawden et al., 2018), pronoun translation (M¨uller et al., 2018; Webster et al., 2018), or coreference and multiword expressions (Burchardt et al., 2017). Limitations and future work While our work presents the first large-scale evaluation of gender bias in MT, it still suffers from certain limitations which could be addressed in follow up work. First, like some of the challenge sets discussed above, WinoMT is composed of synthetic English sourceside examples. On the one hand, this allows for a controlled experiment environment, while, on the other hand, this might introduce some artificial biases in our data and"
P19-1164,D17-1263,0,0.0335542,"ders, for example “sastre” (tailor) in Spanish or “soldat” (soldier) in French, which do not have female inflections. See Table 5 for detailed examples. 4 Discussion Related work This work is most related to several recent efforts which evaluate MT through the use of challenge sets. Similarly to our use WinoMT, these works evaluate MT systems (either manually or automatically) on test sets which are specially created to exhibit certain linguistic phenomena, thus going beyond the traditional BLEU metric (Papineni et al., 2002). These include challenge sets for language-specific idiosyncrasies (Isabelle et al., 2017), discourse phenomena (Bawden et al., 2018), pronoun translation (M¨uller et al., 2018; Webster et al., 2018), or coreference and multiword expressions (Burchardt et al., 2017). Limitations and future work While our work presents the first large-scale evaluation of gender bias in MT, it still suffers from certain limitations which could be addressed in follow up work. First, like some of the challenge sets discussed above, WinoMT is composed of synthetic English sourceside examples. On the one hand, this allows for a controlled experiment environment, while, on the other hand, this might intro"
P19-1164,W18-6307,0,0.0594926,"Missing"
P19-1164,N13-1073,1,0.665237,"3,888 instances, and is equally balanced between male and female genders, as well as between stereotypical and nonstereotypical gender-role assignments (e.g., a female doctor versus a female nurse). Additional dataset statistics are presented in Table 1. We use WinoMT to estimate the gender-bias of an MT model, M , in target-language L by performing following steps (exemplified in Figure 1): (1) Translate all of the sentences in WinoMT into L using M , thus forming a bilingual corpus of English and the target language L. (2) Align between the source and target translations, using fast align (Dyer et al., 2013), trained on the automatic translations from from step (1). Winogender Experimental Setup MT systems We test six widely used MT models, representing the state of the art in both commercial and academic research: (1) Google Translate,1 (2) Microsoft Translator,2 (3) Amazon Translate,3 (4) SYSTRAN,4 (5) the model of Ott et al. (2018), which recently achieved the best performance on English-to-French translation on the WMT’14 test set, and (6) the model of Edunov et al. (2018), the WMT’18 winner on English-toGerman translation. We query the online API for the first four commercial MT systems, whi"
P19-1164,D18-1045,0,0.016978,"ingual corpus of English and the target language L. (2) Align between the source and target translations, using fast align (Dyer et al., 2013), trained on the automatic translations from from step (1). Winogender Experimental Setup MT systems We test six widely used MT models, representing the state of the art in both commercial and academic research: (1) Google Translate,1 (2) Microsoft Translator,2 (3) Amazon Translate,3 (4) SYSTRAN,4 (5) the model of Ott et al. (2018), which recently achieved the best performance on English-to-French translation on the WMT’14 test set, and (6) the model of Edunov et al. (2018), the WMT’18 winner on English-toGerman translation. We query the online API for the first four commercial MT systems, while for the latter two academic models we use the pretrained models provided by the Fairseq toolkit.5 1680 1 https://translate.google.com https://www.bing.com/translator 3 https://aws.amazon.com/translate 4 http://www.systransoft.com 5 https://github.com/pytorch/fairseq 2 Google Translate Amazon Translate∗ Microsoft Translator SYSTRAN Acc ∆G ∆S Acc ∆G ∆S Acc ∆G ∆S Acc ∆G ∆S ES FR IT 53.1 63.6 39.6 23.4 6.4 32.9 21.3 26.7 21.5 47.3 44.7 39.8 36.8 36.4 39.8 23.2 29.7 17.0 59.4"
P19-1164,D18-1002,0,0.0218372,"e languages, all annotated with ground truth entity gender. Second, similar to any medium size test set, it is clear that WinoMT serves only as a proxy estimation for the phenomenon of gender bias, and would probably be easy to overfit. A larger annotated corpus can perhaps provide a better signal for training. Finally, even though in Section 3.3 we show a very rudimentary debiasing scheme which relies on oracle coreference system, it is clear that this is not applicable in a real-world scenario. While recent research has shown that getting rid of such biases may prove to be very challenging (Elazar and Goldberg, 2018; Gonen and Goldberg, 2019), we hope that this work will serve as a first step for developing more gender-balanced MT models. 5 Conclusions We presented the first large-scale multilingual quantitative evidence for gender bias in MT, showing that on eight diverse target languages, all four tested popular commercial systems and two recent state-of-the-art academic MT models are significantly prone to translate based on gender stereotypes rather than more meaningful context. Our data and code are publicly available at https://github.com/ gabrielStanovsky/mt_gender. Acknowledgments We would like t"
P19-1164,W19-3821,0,0.233151,"Missing"
P19-1164,W19-3621,0,0.0489826,"with ground truth entity gender. Second, similar to any medium size test set, it is clear that WinoMT serves only as a proxy estimation for the phenomenon of gender bias, and would probably be easy to overfit. A larger annotated corpus can perhaps provide a better signal for training. Finally, even though in Section 3.3 we show a very rudimentary debiasing scheme which relies on oracle coreference system, it is clear that this is not applicable in a real-world scenario. While recent research has shown that getting rid of such biases may prove to be very challenging (Elazar and Goldberg, 2018; Gonen and Goldberg, 2019), we hope that this work will serve as a first step for developing more gender-balanced MT models. 5 Conclusions We presented the first large-scale multilingual quantitative evidence for gender bias in MT, showing that on eight diverse target languages, all four tested popular commercial systems and two recent state-of-the-art academic MT models are significantly prone to translate based on gender stereotypes rather than more meaningful context. Our data and code are publicly available at https://github.com/ gabrielStanovsky/mt_gender. Acknowledgments We would like to thank Mark Yatskar, Iz Be"
P19-1164,W18-6301,0,0.0342982,"Missing"
P19-1164,P02-1040,0,0.113146,"emale inflections for professions which were stereotypically associated with one of the genders, for example “sastre” (tailor) in Spanish or “soldat” (soldier) in French, which do not have female inflections. See Table 5 for detailed examples. 4 Discussion Related work This work is most related to several recent efforts which evaluate MT through the use of challenge sets. Similarly to our use WinoMT, these works evaluate MT systems (either manually or automatically) on test sets which are specially created to exhibit certain linguistic phenomena, thus going beyond the traditional BLEU metric (Papineni et al., 2002). These include challenge sets for language-specific idiosyncrasies (Isabelle et al., 2017), discourse phenomena (Bawden et al., 2018), pronoun translation (M¨uller et al., 2018; Webster et al., 2018), or coreference and multiword expressions (Burchardt et al., 2017). Limitations and future work While our work presents the first large-scale evaluation of gender bias in MT, it still suffers from certain limitations which could be addressed in follow up work. First, like some of the challenge sets discussed above, WinoMT is composed of synthetic English sourceside examples. On the one hand, this"
P19-1164,W17-1609,0,0.15079,"Missing"
P19-1164,N18-2002,0,0.232322,"Missing"
P19-1164,Q18-1042,0,0.0532662,"ctions. See Table 5 for detailed examples. 4 Discussion Related work This work is most related to several recent efforts which evaluate MT through the use of challenge sets. Similarly to our use WinoMT, these works evaluate MT systems (either manually or automatically) on test sets which are specially created to exhibit certain linguistic phenomena, thus going beyond the traditional BLEU metric (Papineni et al., 2002). These include challenge sets for language-specific idiosyncrasies (Isabelle et al., 2017), discourse phenomena (Bawden et al., 2018), pronoun translation (M¨uller et al., 2018; Webster et al., 2018), or coreference and multiword expressions (Burchardt et al., 2017). Limitations and future work While our work presents the first large-scale evaluation of gender bias in MT, it still suffers from certain limitations which could be addressed in follow up work. First, like some of the challenge sets discussed above, WinoMT is composed of synthetic English sourceside examples. On the one hand, this allows for a controlled experiment environment, while, on the other hand, this might introduce some artificial biases in our data and evaluation. Ideally, WinoMT could be augmented with natural “in t"
P19-1164,D17-1323,0,0.163501,"Missing"
P19-1164,N18-2003,0,0.250997,"Missing"
P19-1264,W05-0909,0,0.157461,"OUGE. 1 Introduction Automatic text evaluation reduces the need for human evaluations, which can be expensive and time-consuming to collect, particularly when evaluating long, multi-sentence texts. Automatic metrics allow faster measures of progress when training and testing models and easier development of text generation systems. However, existing automatic metrics for evaluating text are problematic. Due to their computational efficiency, metrics based on word-matching are common, such as ROUGE (Lin, 2004) for summarization, BLEU (Papineni et al., 2002) for machine translation, and METEOR (Banerjee and Lavie, 2005) or CIDER (Vedantam et al., 2015) for image captioning. Nevertheless, these metrics of∗ Work done while author was at Microsoft Research. A: The family is on a picnic. They have fun. S+WMS: 5.13 6.3 3.7 6.2 7.6 5.5 5.1 6.1 5.1 B: The children eat lunch and play in the park. Figure 1: An illustration of S + WMS (a sentence mover similarity metric that uses both word and sentence embeddings) between two documents. This metric finds the minimal cost of “moving” both the word embeddings (orange) and the sentence embeddings (blue) in Document A to those in Document B. An arrow’s width is the propor"
P19-1264,J08-1001,0,0.149367,"Missing"
P19-1264,E06-1040,0,0.186976,"Missing"
P19-1264,N18-1033,0,0.0539092,"Missing"
P19-1264,P14-2074,0,0.0626316,"Missing"
P19-1264,W18-2501,0,0.022427,"Missing"
P19-1264,J18-1008,0,0.0174029,"of embedding. Finally, we show in §5 that sentence mover’s similarity metrics can also be used when learning to generate text. Generating summaries using reinforcement learning with sentence mover’s similarity as the reward results in higher quality summaries than those generated using a ROUGE - L or WMD reward, according to both automatic metrics and human evaluations. 2 that evaluates the distance between two sequences (e.g., sentences, paragraphs, etc.), each represented with relative word frequencies. It combines (1) item similarity2 on bag-of-word (BOW) histogram representations of text (Goldberg et al., 2018) with (2) word embedding similarity. For any two documents A and B, WMD is defined as the minimum cost of transforming one document into the other. Each document is represented by the relative frequencies of words it contains, i.e., for the ith word type, dA,i = count(i)/|A| where |A |is the total word count of document A, and dB,i is defined similarly. Now let the ith word be represented by vi ∈ Rm , i.e., an m-length embedding,3 allowing us to define distances between the ith and jth words, denoted ∆(i, j). V is the vocabulary size. We follow Kusner et al. (2015) and use the Euclidean distan"
P19-1264,N19-1169,0,0.0352922,"Missing"
P19-1264,N18-1150,1,0.88414,"Missing"
P19-1264,P18-1060,0,0.284561,"Missing"
P19-1264,P16-1046,0,0.0606545,"Missing"
P19-1264,L18-1269,0,0.0619859,"Missing"
P19-1264,W07-0734,0,0.106654,"Missing"
P19-1264,W04-1013,0,0.156526,"and human evaluations of summaries learned in this way, finding that our approach outperforms ROUGE. 1 Introduction Automatic text evaluation reduces the need for human evaluations, which can be expensive and time-consuming to collect, particularly when evaluating long, multi-sentence texts. Automatic metrics allow faster measures of progress when training and testing models and easier development of text generation systems. However, existing automatic metrics for evaluating text are problematic. Due to their computational efficiency, metrics based on word-matching are common, such as ROUGE (Lin, 2004) for summarization, BLEU (Papineni et al., 2002) for machine translation, and METEOR (Banerjee and Lavie, 2005) or CIDER (Vedantam et al., 2015) for image captioning. Nevertheless, these metrics of∗ Work done while author was at Microsoft Research. A: The family is on a picnic. They have fun. S+WMS: 5.13 6.3 3.7 6.2 7.6 5.5 5.1 6.1 5.1 B: The children eat lunch and play in the park. Figure 1: An illustration of S + WMS (a sentence mover similarity metric that uses both word and sentence embeddings) between two documents. This metric finds the minimal cost of “moving” both the word embeddings ("
P19-1264,D16-1230,0,0.0313795,"that uses both word and sentence embeddings) between two documents. This metric finds the minimal cost of “moving” both the word embeddings (orange) and the sentence embeddings (blue) in Document A to those in Document B. An arrow’s width is the proportion of the embedding’s weight being moved, and its label is the Euclidean distance. Here we show only the highest weighted connections. ten fail to capture information that has been reworded or reordered from the reference text, as shown in Kilickaya et al. (2017) and Table 1.1 They have also been found to correlate weakly with human judgments (Liu et al., 2016; Novikova et al., 2017). To avoid these shortcomings, word mover’s distance (WMD; Kusner et al., 2015) can be used to evaluate text in a continuous space using pretrained word embeddings instead of relying on exact word matching. WMD has been used successfully for tasks including image caption evaluation (Kilickaya et al., 2017), automatic essay evaluation (Tashu and Horv´ath, 2018), and affect detection (Alshahrani et al., 2017). This bag-ofembeddings approach is flexible but fails to reflect the grouping of words and ideas, a shortcoming that becomes more problematic as the length of the do"
P19-1264,L18-1008,0,0.0299579,"Missing"
P19-1264,D17-1238,0,0.0626451,"d and sentence embeddings) between two documents. This metric finds the minimal cost of “moving” both the word embeddings (orange) and the sentence embeddings (blue) in Document A to those in Document B. An arrow’s width is the proportion of the embedding’s weight being moved, and its label is the Euclidean distance. Here we show only the highest weighted connections. ten fail to capture information that has been reworded or reordered from the reference text, as shown in Kilickaya et al. (2017) and Table 1.1 They have also been found to correlate weakly with human judgments (Liu et al., 2016; Novikova et al., 2017). To avoid these shortcomings, word mover’s distance (WMD; Kusner et al., 2015) can be used to evaluate text in a continuous space using pretrained word embeddings instead of relying on exact word matching. WMD has been used successfully for tasks including image caption evaluation (Kilickaya et al., 2017), automatic essay evaluation (Tashu and Horv´ath, 2018), and affect detection (Alshahrani et al., 2017). This bag-ofembeddings approach is flexible but fails to reflect the grouping of words and ideas, a shortcoming that becomes more problematic as the length of the document grows. We modify"
P19-1264,P02-1040,0,0.110817,"learned in this way, finding that our approach outperforms ROUGE. 1 Introduction Automatic text evaluation reduces the need for human evaluations, which can be expensive and time-consuming to collect, particularly when evaluating long, multi-sentence texts. Automatic metrics allow faster measures of progress when training and testing models and easier development of text generation systems. However, existing automatic metrics for evaluating text are problematic. Due to their computational efficiency, metrics based on word-matching are common, such as ROUGE (Lin, 2004) for summarization, BLEU (Papineni et al., 2002) for machine translation, and METEOR (Banerjee and Lavie, 2005) or CIDER (Vedantam et al., 2015) for image captioning. Nevertheless, these metrics of∗ Work done while author was at Microsoft Research. A: The family is on a picnic. They have fun. S+WMS: 5.13 6.3 3.7 6.2 7.6 5.5 5.1 6.1 5.1 B: The children eat lunch and play in the park. Figure 1: An illustration of S + WMS (a sentence mover similarity metric that uses both word and sentence embeddings) between two documents. This metric finds the minimal cost of “moving” both the word embeddings (orange) and the sentence embeddings (blue) in Do"
P19-1264,D17-1103,0,0.0322114,"Missing"
P19-1264,D14-1162,0,0.0809008,"Missing"
P19-1264,N18-1202,0,0.111763,"Missing"
P19-1264,W11-2704,0,0.0771538,"Missing"
P19-1264,W06-1422,0,0.0705409,"Missing"
P19-1264,J09-4008,0,0.0417624,"Missing"
P19-1264,P17-1099,0,0.161561,"Missing"
P19-1264,W17-5306,0,0.0409033,"Missing"
P19-1264,D18-1482,0,0.074386,"Missing"
P19-1264,D17-1207,0,0.0507763,"Missing"
P19-1282,D18-1537,0,0.0347956,"re many variants of attention (Vaswani et al., 2017), each for1 Code is available at https://github.com/ serrano-s/attn-tests. mulation consists of the same high-level goal: calculating nonnegative weights for each input component (e.g., word) that together sum to 1, multiplying those weights by their corresponding representations, and summing the resulting vectors into a single fixed-length representation. Since attention calculates a distribution over inputs, prior work has used attention as a tool for interpretation of model decisions (Wang et al., 2016; Lee et al., 2017; Lin et al., 2017; Ghaeini et al., 2018). The existence of so much work on visualizing attention weights is a testament to attention’s popularity in this regard; to name just a few examples of these weights being examined to understand a model, recent work has focused on goals from explaining and debugging the current system’s decision (Lee et al., 2017; Ding et al., 2017) to distilling important traits of a dataset (Yang et al., 2017; Habernal et al., 2018). Despite this, existing work on interpretability is only beginning to assess what computed attention weights actually communicate. In an independent and contemporaneous study, J"
P19-1282,P11-1052,0,0.0336226,", which is unsurprising since I is all words in the input text, so most attention weights are very small. in a truly useful ranking of importance would comprise a minimal necessary set of information for making the model’s decision. The idea of a minimal set of inputs necessary to uphold a decision is not new; Li et al. (2016) use reinforcement learning to attempt to construct such a minimal set of words, Lei et al. (2016) train an encoder to constrain the input prior to classification, and much of the work that has been done on extractive summarization takes this concept as a starting point (Lin and Bilmes, 2011). However, such work has focused on approximating minimal sets, instead of evaluating the ability of other importance-determining “shortcuts” (such as attention weight orderings) to identify them. Nguyen (2018) leveraged the idea of minimal sets in a much more similar way to our work, comparing different input importance orderings. Concretely, to assess the validity of an importance ranking method (e.g., attention), we begin erasing representations from the top of the ranking downward until the model’s decision changes. Ideally, we would then enumerate all possible subsets of that instance’s c"
P19-1282,N18-1036,0,0.0172527,"ntion calculates a distribution over inputs, prior work has used attention as a tool for interpretation of model decisions (Wang et al., 2016; Lee et al., 2017; Lin et al., 2017; Ghaeini et al., 2018). The existence of so much work on visualizing attention weights is a testament to attention’s popularity in this regard; to name just a few examples of these weights being examined to understand a model, recent work has focused on goals from explaining and debugging the current system’s decision (Lee et al., 2017; Ding et al., 2017) to distilling important traits of a dataset (Yang et al., 2017; Habernal et al., 2018). Despite this, existing work on interpretability is only beginning to assess what computed attention weights actually communicate. In an independent and contemporaneous study, Jain and Wallace (2019) explore whether attention mechanisms can identify the relative importance of inputs to the full model, finding them to be highly inconsistent predictors. In this work, we apply a different analysis based on intermediate representation erasure to assess whether attention weights can instead be relied upon to explain the relative importance of the inputs to the attention layer itself. We find simil"
P19-1282,D14-1181,0,\N,Missing
P19-1282,D16-1053,0,\N,Missing
P19-1282,N15-1004,0,\N,Missing
P19-1282,N16-1174,0,\N,Missing
P19-1282,W16-1601,0,\N,Missing
P19-1282,D16-1058,0,\N,Missing
P19-1282,P17-1092,1,\N,Missing
P19-1282,P17-1106,0,\N,Missing
P19-1282,D17-1211,0,\N,Missing
P19-1282,D17-2021,0,\N,Missing
P19-1282,N18-1097,0,\N,Missing
P19-1282,N19-1357,0,\N,Missing
P19-1282,Q18-1005,0,\N,Missing
P19-1282,N16-3020,0,\N,Missing
P19-1590,P13-1162,0,0.0243683,"there is a huge amount of unlabeled text available for some languages, such as English, this scale of data is not available for all languages. Indomain data availability, of course, varies by domain. For many researchers, especially outside of STEM fields, computation may also be a scarce resource, such that training contextual embeddings from scratch, or even incorporating them into a model could be prohibitively expensive. Moreover, even when such pretrained models are available, they inevitably come with potentially undesirable biases baked in, based on the data on which they were trained (Recasens et al., 2013; Bolukbasi et al., 2016; Zhao et al., 2019). 2 http://github.com/allenai/vampire Particularly for social science applications, it may be preferable to exclude such confounders by only working with in-domain or curated data. Given these constraints and limitations, we seek an approach to semi-supervised learning that can leverage in-domain unlabeled data, achieve high accuracy with only a handful of labeled instances, and can run efficiently on a CPU. 2.2 Semi-supervised Learning Many approaches to semi-supervised learning have been developed for NLP, including variants of bootstrapping (Charn"
P19-1590,P11-1015,0,\N,Missing
P19-1590,D11-1024,0,\N,Missing
P19-1590,N10-1012,0,\N,Missing
P19-1590,P15-1162,0,\N,Missing
P19-1590,E14-1056,0,\N,Missing
P19-1590,D14-1162,0,\N,Missing
P19-1590,D18-1179,0,\N,Missing
P19-1590,N19-1064,0,\N,Missing
P19-1590,K16-1002,0,\N,Missing
P19-1590,N19-1423,0,\N,Missing
P19-1590,N06-1020,0,\N,Missing
P19-1590,P18-1189,1,\N,Missing
Q14-1015,D10-1124,1,0.880985,"Missing"
Q14-1015,N09-1058,0,0.0314007,"adaptation of the model to data in real time. In online learning, streaming examples are processed only when they arrive. Online learning also eliminates the need to store large amounts of data in memory. Strictly speaking, online learning is distinct from stochastic learning, which for language models built on massive datasets has been explored by Hoffman et al. (2013) and Wang et al. (2011). Those techniques are still for static modeling. Language modeling for streaming datasets in the context of machine translation was considered by Levenberg and Osborne (2009) and Levenberg et al. (2010). Goyal et al. (2009) introduced a streaming algorithm for large scale language modeling by approximating ngram frequency counts. We propose a general online learning algorithm for language modeling that draws inspiration from regret minimization in sequential predictions (Cesa-Bianchi and Lugosi, 2006) and on181 Transactions of the Association for Computational Linguistics, 2 (2014) 181–192. Action Editor: Eric Fosler-Lussier. c Submitted 10/2013; Revised 2/2014; Published 4/2014. 2014 Association for Computational Linguistics. line variational algorithms (Sato, 2001; Honkela and Valpola, 2003). To our knowledge,"
Q14-1015,D09-1079,0,0.0148112,"nonstationarity and dynamics in the data that necessitate adaptation of the model to data in real time. In online learning, streaming examples are processed only when they arrive. Online learning also eliminates the need to store large amounts of data in memory. Strictly speaking, online learning is distinct from stochastic learning, which for language models built on massive datasets has been explored by Hoffman et al. (2013) and Wang et al. (2011). Those techniques are still for static modeling. Language modeling for streaming datasets in the context of machine translation was considered by Levenberg and Osborne (2009) and Levenberg et al. (2010). Goyal et al. (2009) introduced a streaming algorithm for large scale language modeling by approximating ngram frequency counts. We propose a general online learning algorithm for language modeling that draws inspiration from regret minimization in sequential predictions (Cesa-Bianchi and Lugosi, 2006) and on181 Transactions of the Association for Computational Linguistics, 2 (2014) 181–192. Action Editor: Eric Fosler-Lussier. c Submitted 10/2013; Revised 2/2014; Published 4/2014. 2014 Association for Computational Linguistics. line variational algorithms (Sato, 20"
Q14-1015,N10-1062,0,0.0210279,"he data that necessitate adaptation of the model to data in real time. In online learning, streaming examples are processed only when they arrive. Online learning also eliminates the need to store large amounts of data in memory. Strictly speaking, online learning is distinct from stochastic learning, which for language models built on massive datasets has been explored by Hoffman et al. (2013) and Wang et al. (2011). Those techniques are still for static modeling. Language modeling for streaming datasets in the context of machine translation was considered by Levenberg and Osborne (2009) and Levenberg et al. (2010). Goyal et al. (2009) introduced a streaming algorithm for large scale language modeling by approximating ngram frequency counts. We propose a general online learning algorithm for language modeling that draws inspiration from regret minimization in sequential predictions (Cesa-Bianchi and Lugosi, 2006) and on181 Transactions of the Association for Computational Linguistics, 2 (2014) 181–192. Action Editor: Eric Fosler-Lussier. c Submitted 10/2013; Revised 2/2014; Published 4/2014. 2014 Association for Computational Linguistics. line variational algorithms (Sato, 2001; Honkela and Valpola, 200"
Q14-1016,W13-1016,0,0.0856848,"Missing"
Q14-1016,I11-1130,0,0.0168818,"tic functions. Within MWEs, we distinguish (a) proper names and (b) lexical idioms. The latter have proved themselves a “pain in the neck for NLP” (Sag et al., 2002). Automatic and efficient detection of MWEs, though far from solved, would have diverse appli193 Transactions of the Association for Computational Linguistics, 2 (2014) 193–206. Action Editor: Joakim Nivre. c Submitted 12/2013; Revised 1/2014; Published 4/2014. 2014 Association for Computational Linguistics. cations including machine translation (Carpuat and Diab, 2010), information retrieval (Newman et al., 2012), opinion mining (Berend, 2011), and second language learning (Ellis et al., 2008). It is difficult to establish any comprehensive taxonomy of multiword idioms, let alone develop linguistic criteria and corpus resources that cut across these types. Consequently, the voluminous literature on MWEs in computational linguistics—see §7, Baldwin and Kim (2010), and Ramisch (2012) for surveys—has been fragmented, looking (for example) at subclasses of phrasal verbs or nominal compounds in isolation. To the extent that MWEs have been annotated in existing corpora, it has usually been as a secondary aspect of some other scheme. Trad"
Q14-1016,W06-1620,0,0.105697,"rpus, broaden the scope of the MWE identification task to include many varieties of MWEs at once, including explicit marking of gaps and a strength distinction. By contrast, the aforementioned identification systems, as well as some MWE-enhanced syntactic parsers (e.g., Green et al., 2012), have been restricted to contiguous MWEs. However, Green et al. (2011) allow gaps to be described as constituents in a syntax tree. Gimpel and Smith’s (2011) shallow, gappy language model allows arbitrary token groupings within a sentence, whereas our model imposes projectivity and nesting constraints (§3). Blunsom and Baldwin (2006) present a sequence model for HPSG supertagging, and evaluate performance on discontinuous MWEs, though the sequence model treats the non-adjacent component supertags like other labels—it cannot enforce that they mutually require one another, as we do via the gappy tagging scheme (§3.1). The lexicon lookup procedures of Bejˇcek et al. (2013) can match gappy MWEs, but are nonstatistical and extremely error-prone when tuned for high oracle recall. Another major thread of research has pursued unsupervised discovery of multiword types from raw corpora, such as with statistical association measures"
Q14-1016,J92-4003,0,0.037196,"ramming, allowing gaps of up to length 2.13 Unsupervised word clusters. Distributional clustering on large (unlabeled) corpora can produce lexical generalizations that are useful for syntactic and semantic analysis tasks (e.g.: Miller et al., 2004; Koo et al., 2008; Turian et al., 2010; Owoputi et al., 2013; Grave et al., 2013). We were interested to see whether a similar pattern would hold for MWE identification, given that MWEs are concerned with what is lexically idiosyncratic—i.e., backing off from specific lexemes to word classes may lose the MWE-relevant information. Brown clustering14 (Brown et al., 1992) 12 http://www.phrases.net/ and postech.ac.kr/~oyz/doc/idiom.html http://home. 13 Each top-level lexical expression (single- or multiword) incurs a cost of 1; each expression within a gap has cost 1.25. 14 With Liang’s (2005) implementation: https://github. com/percyliang/brown-cluster. We obtain 1,000 clusters 199 on the 21-million-word Yelp Academic Dataset15 (which is similar in genre to the annotated web reviews data) gives us a hard clustering of word types. To our tagger, we add features mapping the previous, current, and next token to Brown cluster IDs. The feature for the current token"
Q14-1016,N10-1029,0,0.148488,"units in the lexicon. As figure 1 illustrates, MWEs occupy diverse syntactic and semantic functions. Within MWEs, we distinguish (a) proper names and (b) lexical idioms. The latter have proved themselves a “pain in the neck for NLP” (Sag et al., 2002). Automatic and efficient detection of MWEs, though far from solved, would have diverse appli193 Transactions of the Association for Computational Linguistics, 2 (2014) 193–206. Action Editor: Joakim Nivre. c Submitted 12/2013; Revised 1/2014; Published 4/2014. 2014 Association for Computational Linguistics. cations including machine translation (Carpuat and Diab, 2010), information retrieval (Newman et al., 2012), opinion mining (Berend, 2011), and second language learning (Ellis et al., 2008). It is difficult to establish any comprehensive taxonomy of multiword idioms, let alone develop linguistic criteria and corpus resources that cut across these types. Consequently, the voluminous literature on MWEs in computational linguistics—see §7, Baldwin and Kim (2010), and Ramisch (2012) for surveys—has been fragmented, looking (for example) at subclasses of phrasal verbs or nominal compounds in isolation. To the extent that MWEs have been annotated in existing c"
Q14-1016,W06-1670,0,0.0891639,"ng on a preexisting lexical resource. The annotations are shallow, not relying explicitly on syntax (though in principle they could be mapped onto the parses in the Web Treebank). In terms of modeling, the use of machine learning classification (Hashimoto and Kawahara, 2008; Shigeto et al., 2013) and specifically BIO sequence tagging (Diab and Bhutada, 2009; Constant and Sigogne, 2011; Constant et al., 2012; Vincze et al., 2013) for contextual recognition of MWEs is not new. Lexical semantic classification tasks like named entity recognition (e.g., Ratinov and Roth, 2009), supersense tagging (Ciaramita and Altun, 2006; Paaß and Reichartz, 2009), and index term identification (Newman et al., 2012) also involve chunking of certain MWEs. But our discriminative models, facilitated by the new corpus, broaden the scope of the MWE identification task to include many varieties of MWEs at once, including explicit marking of gaps and a strength distinction. By contrast, the aforementioned identification systems, as well as some MWE-enhanced syntactic parsers (e.g., Green et al., 2012), have been restricted to contiguous MWEs. However, Green et al. (2011) allow gaps to be described as constituents in a syntax tree. G"
Q14-1016,W02-1001,0,0.0210912,"expression, not the gap’s contents. 5 Model With the above representations we model MWE identification as sequence tagging, one of the paradigms that has been used previously for identifying contiguous MWEs (Constant and Sigogne, 2011, see §7).8 Constraints on legal tag bigrams are sufficient to ensure the full tagging is well-formed subject to the regular expressions in figure 2; we enforce these 8 Hierarchical modeling based on our representations is left to future work. constraints in our experiments.9 In NLP, conditional random fields (Lafferty et al., 2001) and the structured perceptron (Collins, 2002) are popular techniques for discriminative sequence modeling with a convex loss function. We choose the second approach for its speed: learning and inference depend mainly on the runtime of the Viterbi algorithm, whose asymptotic complexity is linear in the length of the input and (with a first-order Markov assumption) quadratic in the number of tags. Below, we review the structured perceptron and discuss our cost function, features, and experimental setup. 5.1 Cost-Augmented Structured Perceptron The structured perceptron’s (Collins, 2002) learning procedure, algorithm 1, generalizes the clas"
Q14-1016,W11-0809,0,0.0955959,"like their out-of-gap counterparts. Gappy, 2-level (8 tags). 8 tags are required to encode the 2-level scheme with gaps: {O o B b ¯ I¯ ı˜ I˜ ı}. Variants of the inside tag are marked for strength of the incoming link—this applies gap-externally (capitalized tags) and gap-internally (lowercase tags). If ¯ I or ˜ I immediately follows a gap, its diacritic reflects the strength of the gappy expression, not the gap’s contents. 5 Model With the above representations we model MWE identification as sequence tagging, one of the paradigms that has been used previously for identifying contiguous MWEs (Constant and Sigogne, 2011, see §7).8 Constraints on legal tag bigrams are sufficient to ensure the full tagging is well-formed subject to the regular expressions in figure 2; we enforce these 8 Hierarchical modeling based on our representations is left to future work. constraints in our experiments.9 In NLP, conditional random fields (Lafferty et al., 2001) and the structured perceptron (Collins, 2002) are popular techniques for discriminative sequence modeling with a convex loss function. We choose the second approach for its speed: learning and inference depend mainly on the runtime of the Viterbi algorithm, whose a"
Q14-1016,P12-1022,0,0.521376,",y′ ) + cost(y,y′ ,x)) if yˆ ≠ y then w ← w + g(x,y) − g(x, yˆ ) w ← w +tg(x,y) −tg(x, yˆ ) end t ← t +1 end end Output: w − (w/t) Algorithm 1: Training with the averaged perceptron. (Adapted from Daumé, 2006, p. 19.) N experiments showed that a cost function penalizing all recall errors—i.e., with ρ⟦y∗ ≠ O ∧ y′ = O⟧ as the second term, as in Mohit et al.—tended to append additional tokens to high-confidence MWEs (such as proper names) rather than encourage new MWEs, which would require positing at least two new nonoutside tags. 5.2 Features Basic features. These are largely based on those of Constant et al. (2012): they look at word unigrams and bigrams, character prefixes and suffixes, and POS tags, as well as lexicon entries that match lemmas10 of multiple words in the sentence. Appendix A lists the basic features in detail. Some of the basic features make use of lexicons. We use or construct 10 lists of English MWEs: all multiword entries in WordNet (Fellbaum, 1998); all multiword chunks in SemCor (Miller et al., 1993); all multiword entries in English Wiktionary;11 the WikiMwe dataset mined from English Wikipedia (Hartmann et al., 2012); the SAID database of phrasal lexical idioms (Kuiper et al., 2"
Q14-1016,W09-2903,0,0.21043,"edParalellFX corpus (Vincze, 2012), and the Prague ˇ Czech-English Dependency Treebank (Cmejrek et al., 202 2005). The difference is that Schneider et al. (2014) pursued a comprehensive annotation approach rather than targeting specific varieties of MWEs or relying on a preexisting lexical resource. The annotations are shallow, not relying explicitly on syntax (though in principle they could be mapped onto the parses in the Web Treebank). In terms of modeling, the use of machine learning classification (Hashimoto and Kawahara, 2008; Shigeto et al., 2013) and specifically BIO sequence tagging (Diab and Bhutada, 2009; Constant and Sigogne, 2011; Constant et al., 2012; Vincze et al., 2013) for contextual recognition of MWEs is not new. Lexical semantic classification tasks like named entity recognition (e.g., Ratinov and Roth, 2009), supersense tagging (Ciaramita and Altun, 2006; Paaß and Reichartz, 2009), and index term identification (Newman et al., 2012) also involve chunking of certain MWEs. But our discriminative models, facilitated by the new corpus, broaden the scope of the MWE identification task to include many varieties of MWEs at once, including explicit marking of gaps and a strength distinctio"
Q14-1016,I13-1168,0,0.0572292,"in termine the number of training iterations (epochs) M places to eat in Baltimore (because eat in, meaning by early stopping—that is, after each iteration, we use ‘eat at home,’ is listed in WordNet). The supervised the model to decode the held-out data, and when that approach has learned not to trust WordNet too much accuracy ceases to improve, use the previous model. due to this sort of ambiguity. Downstream applicaThe two hyperparameters are the number of iterations tions that currently use lexicon matching for MWE and the value of the recall cost hyperparameter (ρ). identification (e.g., Ghoneim and Diab, 2013) likely Both are tuned via cross-validation on train; we use stand to benefit from our statistical approach. the multiple of 50 that maximizes average link-based F1 . The chosen values are shown in table 3. Experi- 6.2 How best to exploit MWE lexicons (type-level information)? ments were managed with the ducttape tool.18 For statistical tagging (right portion of table 2), using 6 Results more preexisting (out-of-domain) lexicons generally We experimentally address the following questions improves recall; precision also improves a bit. A lexicon of MWEs occurring in the non-held-out to probe an"
Q14-1016,W11-2165,1,0.894392,"Missing"
Q14-1016,W13-3511,0,0.0198865,"nd one or more of the lexicons, lookup proceeds as follows: we enumerate entries whose lemma sequences match a sequence of lemmatized tokens, and build a lattice of possible analyses over the sentence. We find the shortest path (i.e., using as few expressions as possible) with dynamic programming, allowing gaps of up to length 2.13 Unsupervised word clusters. Distributional clustering on large (unlabeled) corpora can produce lexical generalizations that are useful for syntactic and semantic analysis tasks (e.g.: Miller et al., 2004; Koo et al., 2008; Turian et al., 2010; Owoputi et al., 2013; Grave et al., 2013). We were interested to see whether a similar pattern would hold for MWE identification, given that MWEs are concerned with what is lexically idiosyncratic—i.e., backing off from specific lexemes to word classes may lose the MWE-relevant information. Brown clustering14 (Brown et al., 1992) 12 http://www.phrases.net/ and postech.ac.kr/~oyz/doc/idiom.html http://home. 13 Each top-level lexical expression (single- or multiword) incurs a cost of 1; each expression within a gap has cost 1.25. 14 With Liang’s (2005) implementation: https://github. com/percyliang/brown-cluster. We obtain 1,000 cluste"
Q14-1016,D11-1067,0,0.0185646,"Missing"
Q14-1016,hajic-etal-2012-announcing,0,0.022231,"Missing"
Q14-1016,D08-1104,0,0.0477308,"such as SemCor (Miller et al., 1993), the French Treebank (Abeillé et al., 2003), the SzegedParalellFX corpus (Vincze, 2012), and the Prague ˇ Czech-English Dependency Treebank (Cmejrek et al., 202 2005). The difference is that Schneider et al. (2014) pursued a comprehensive annotation approach rather than targeting specific varieties of MWEs or relying on a preexisting lexical resource. The annotations are shallow, not relying explicitly on syntax (though in principle they could be mapped onto the parses in the Web Treebank). In terms of modeling, the use of machine learning classification (Hashimoto and Kawahara, 2008; Shigeto et al., 2013) and specifically BIO sequence tagging (Diab and Bhutada, 2009; Constant and Sigogne, 2011; Constant et al., 2012; Vincze et al., 2013) for contextual recognition of MWEs is not new. Lexical semantic classification tasks like named entity recognition (e.g., Ratinov and Roth, 2009), supersense tagging (Ciaramita and Altun, 2006; Paaß and Reichartz, 2009), and index term identification (Newman et al., 2012) also involve chunking of certain MWEs. But our discriminative models, facilitated by the new corpus, broaden the scope of the MWE identification task to include many va"
Q14-1016,P08-1068,0,0.0247046,"f which may be variables like &lt;something>. Given a sentence and one or more of the lexicons, lookup proceeds as follows: we enumerate entries whose lemma sequences match a sequence of lemmatized tokens, and build a lattice of possible analyses over the sentence. We find the shortest path (i.e., using as few expressions as possible) with dynamic programming, allowing gaps of up to length 2.13 Unsupervised word clusters. Distributional clustering on large (unlabeled) corpora can produce lexical generalizations that are useful for syntactic and semantic analysis tasks (e.g.: Miller et al., 2004; Koo et al., 2008; Turian et al., 2010; Owoputi et al., 2013; Grave et al., 2013). We were interested to see whether a similar pattern would hold for MWE identification, given that MWEs are concerned with what is lexically idiosyncratic—i.e., backing off from specific lexemes to word classes may lose the MWE-relevant information. Brown clustering14 (Brown et al., 1992) 12 http://www.phrases.net/ and postech.ac.kr/~oyz/doc/idiom.html http://home. 13 Each top-level lexical expression (single- or multiword) incurs a cost of 1; each expression within a gap has cost 1.25. 14 With Liang’s (2005) implementation: http"
Q14-1016,J93-2004,0,0.0472323,"valuate our models below. The corpus is publicly available as a benchmark for further research.1 Data. The documents in the corpus are online user reviews of restaurants, medical providers, retailers, automotive services, pet care services, etc. Marked by conversational and opinionated language, this genre is fertile ground for colloquial idioms (Nunberg et al., 1994; Moon, 1998). The 723 reviews (55,000 words, 3,800 sentences) in the English Web Treebank (WTB; Bies et al., 2012b) were collected by Google, tokenized, and annotated with phrase structure trees in the style of the Penn Treebank (Marcus et al., 1993). MWE annotators used the sentence and word tokenizations supplied by the treebank.2 Annotation scheme. The annotation scheme itself was designed to be as simple as possible. It consists of grouping together the tokens in each sentence that belong to the same MWE instance. While annotation guidelines provide examples of MWE groupings in a wide range of constructions, the annotator is not 1 2 http://www.ark.cs.cmu.edu/LexSem/ Because we use treebank data, syntactic parses are available to assist in post hoc analysis. Syntactic information was not shown to annotators. # of constituent tokens 2 3"
Q14-1016,W97-0311,0,0.106691,"MWEs, though the sequence model treats the non-adjacent component supertags like other labels—it cannot enforce that they mutually require one another, as we do via the gappy tagging scheme (§3.1). The lexicon lookup procedures of Bejˇcek et al. (2013) can match gappy MWEs, but are nonstatistical and extremely error-prone when tuned for high oracle recall. Another major thread of research has pursued unsupervised discovery of multiword types from raw corpora, such as with statistical association measures (Church et al., 1991; Pecina, 2010; Ramisch et al., 2012, inter alia), parallel corpora (Melamed, 1997; Moirón and Tiedemann, 2006; Tsvetkov and Wintner, 2010), or a combination thereof (Tsvetkov and Wintner, 2011); this may be followed by a lookupand-classify approach to contextual identification (Ramisch et al., 2010). Though preliminary experiments with our models did not show benefit to incorporating such automatically constructed lexicons, we hope these two perspectives can be brought together in future work. 8 Conclusion This article has presented the first supervised model for identifying heterogeneous multiword expressions in English text. Our feature-rich discriminative sequence tagge"
Q14-1016,H93-1061,0,0.216655,"such as proper names) rather than encourage new MWEs, which would require positing at least two new nonoutside tags. 5.2 Features Basic features. These are largely based on those of Constant et al. (2012): they look at word unigrams and bigrams, character prefixes and suffixes, and POS tags, as well as lexicon entries that match lemmas10 of multiple words in the sentence. Appendix A lists the basic features in detail. Some of the basic features make use of lexicons. We use or construct 10 lists of English MWEs: all multiword entries in WordNet (Fellbaum, 1998); all multiword chunks in SemCor (Miller et al., 1993); all multiword entries in English Wiktionary;11 the WikiMwe dataset mined from English Wikipedia (Hartmann et al., 2012); the SAID database of phrasal lexical idioms (Kuiper et al., 2003); the named entities and other MWEs in the WSJ corpus on the English side of the CEDT (Hajiˇc et al., 2012); 10 The WordNet API in NLTK (Bird et al., 2009) was used for lemmatization. 11 http://en.wiktionary.org; data obtained from https://toolserver.org/~enwikt/definitions/ enwikt-defs-20130814-en.tsv.gz L OOKUP preexising lexicons none WordNet + SemCor 6 lexicons 10 lexicons entries 0 71k 420k 437k best con"
Q14-1016,N04-1043,0,0.0119357,"f word lemmas, some of which may be variables like &lt;something>. Given a sentence and one or more of the lexicons, lookup proceeds as follows: we enumerate entries whose lemma sequences match a sequence of lemmatized tokens, and build a lattice of possible analyses over the sentence. We find the shortest path (i.e., using as few expressions as possible) with dynamic programming, allowing gaps of up to length 2.13 Unsupervised word clusters. Distributional clustering on large (unlabeled) corpora can produce lexical generalizations that are useful for syntactic and semantic analysis tasks (e.g.: Miller et al., 2004; Koo et al., 2008; Turian et al., 2010; Owoputi et al., 2013; Grave et al., 2013). We were interested to see whether a similar pattern would hold for MWE identification, given that MWEs are concerned with what is lexically idiosyncratic—i.e., backing off from specific lexemes to word classes may lose the MWE-relevant information. Brown clustering14 (Brown et al., 1992) 12 http://www.phrases.net/ and postech.ac.kr/~oyz/doc/idiom.html http://home. 13 Each top-level lexical expression (single- or multiword) incurs a cost of 1; each expression within a gap has cost 1.25. 14 With Liang’s (2005) im"
Q14-1016,E12-1017,1,0.651813,"sion of the structured perceptron that is sensitive to different kinds of errors during training. When recall is the bigger obstacle, we can adopt the following cost function: given a sentence x, its gold labeling y∗ , and a candidate labeling y′ , ∗ ′ ∣y∗ ∣ cost(y ,y ,x) = ∑ c(y∗j ,y′j ) where j=1 ′ c(y∗ ,y′ ) = ⟦y∗ ≠ y ⟧ + ρ⟦y∗ ∈ {B, b} ∧ y′ ∈ {O, o}⟧ A single nonnegative hyperparameter, ρ, controls the tradeoff between recall and accuracy; higher ρ biases the model in favor of recall (possibly hurting accuracy and precision). This is a slight variant of the recall-oriented cost function of Mohit et al. (2012). The difference is that we only penalize beginning-of-expression recall errors. Preliminary 9 The 8-tag scheme licenses 42 tag bigrams: sequences such as B O and o ¯ ı are prohibited. There are also constraints on the allowed tags at the beginning and end of the sequence. 198 Input: data ⟨⟨x(n) ,y(n) ⟩⟩n=1 ; number of iterations M w←0 w←0 t ←1 for m = 1 to M do for n = 1 to N do ⟨x,y⟩ ← ⟨x(n) ,y(n) ⟩ yˆ ← argmaxy′ (w⊺ g(x,y′ ) + cost(y,y′ ,x)) if yˆ ≠ y then w ← w + g(x,y) − g(x, yˆ ) w ← w +tg(x,y) −tg(x, yˆ ) end t ← t +1 end end Output: w − (w/t) Algorithm 1: Training with the averaged per"
Q14-1016,W06-2405,0,0.0359021,"he sequence model treats the non-adjacent component supertags like other labels—it cannot enforce that they mutually require one another, as we do via the gappy tagging scheme (§3.1). The lexicon lookup procedures of Bejˇcek et al. (2013) can match gappy MWEs, but are nonstatistical and extremely error-prone when tuned for high oracle recall. Another major thread of research has pursued unsupervised discovery of multiword types from raw corpora, such as with statistical association measures (Church et al., 1991; Pecina, 2010; Ramisch et al., 2012, inter alia), parallel corpora (Melamed, 1997; Moirón and Tiedemann, 2006; Tsvetkov and Wintner, 2010), or a combination thereof (Tsvetkov and Wintner, 2011); this may be followed by a lookupand-classify approach to contextual identification (Ramisch et al., 2010). Though preliminary experiments with our models did not show benefit to incorporating such automatically constructed lexicons, we hope these two perspectives can be brought together in future work. 8 Conclusion This article has presented the first supervised model for identifying heterogeneous multiword expressions in English text. Our feature-rich discriminative sequence tagger performs shallow chunking"
Q14-1016,C12-1127,0,0.308867,"WEs occupy diverse syntactic and semantic functions. Within MWEs, we distinguish (a) proper names and (b) lexical idioms. The latter have proved themselves a “pain in the neck for NLP” (Sag et al., 2002). Automatic and efficient detection of MWEs, though far from solved, would have diverse appli193 Transactions of the Association for Computational Linguistics, 2 (2014) 193–206. Action Editor: Joakim Nivre. c Submitted 12/2013; Revised 1/2014; Published 4/2014. 2014 Association for Computational Linguistics. cations including machine translation (Carpuat and Diab, 2010), information retrieval (Newman et al., 2012), opinion mining (Berend, 2011), and second language learning (Ellis et al., 2008). It is difficult to establish any comprehensive taxonomy of multiword idioms, let alone develop linguistic criteria and corpus resources that cut across these types. Consequently, the voluminous literature on MWEs in computational linguistics—see §7, Baldwin and Kim (2010), and Ramisch (2012) for surveys—has been fragmented, looking (for example) at subclasses of phrasal verbs or nominal compounds in isolation. To the extent that MWEs have been annotated in existing corpora, it has usually been as a secondary as"
Q14-1016,N13-1039,1,0.165809,"Missing"
Q14-1016,W12-3311,0,0.0171944,"ction Editor: Joakim Nivre. c Submitted 12/2013; Revised 1/2014; Published 4/2014. 2014 Association for Computational Linguistics. cations including machine translation (Carpuat and Diab, 2010), information retrieval (Newman et al., 2012), opinion mining (Berend, 2011), and second language learning (Ellis et al., 2008). It is difficult to establish any comprehensive taxonomy of multiword idioms, let alone develop linguistic criteria and corpus resources that cut across these types. Consequently, the voluminous literature on MWEs in computational linguistics—see §7, Baldwin and Kim (2010), and Ramisch (2012) for surveys—has been fragmented, looking (for example) at subclasses of phrasal verbs or nominal compounds in isolation. To the extent that MWEs have been annotated in existing corpora, it has usually been as a secondary aspect of some other scheme. Traditionally, such resources have prioritized certain kinds of MWEs to the exclusion of others, so they are not appropriate for evaluating general-purpose identification systems. In this article, we briefly review a shallow form of analysis for MWEs that is neutral to expression type, and that facilitates free text annotation without requiring a"
Q14-1016,W12-3301,0,0.207661,"Missing"
Q14-1016,ramisch-etal-2010-mwetoolkit,0,0.0636479,"lookup procedures of Bejˇcek et al. (2013) can match gappy MWEs, but are nonstatistical and extremely error-prone when tuned for high oracle recall. Another major thread of research has pursued unsupervised discovery of multiword types from raw corpora, such as with statistical association measures (Church et al., 1991; Pecina, 2010; Ramisch et al., 2012, inter alia), parallel corpora (Melamed, 1997; Moirón and Tiedemann, 2006; Tsvetkov and Wintner, 2010), or a combination thereof (Tsvetkov and Wintner, 2011); this may be followed by a lookupand-classify approach to contextual identification (Ramisch et al., 2010). Though preliminary experiments with our models did not show benefit to incorporating such automatically constructed lexicons, we hope these two perspectives can be brought together in future work. 8 Conclusion This article has presented the first supervised model for identifying heterogeneous multiword expressions in English text. Our feature-rich discriminative sequence tagger performs shallow chunking with a novel scheme that allows for MWEs containing gaps, and includes a strength distinction to separate highly idiomatic expressions from collocations. It is trained and evaluated on a corp"
Q14-1016,W95-0107,0,0.0576853,"O gappy, he was willing to budge a little on the price which means a lot to me . ¯ O ˜ ¯ ˜ ˜ 2-level O O O O B b ¯ ı I O O B I I I I O (O∣BI+ )+ ¯I ˜]+ )+ (O∣B[I (O∣B(o∣bi+ ∣I)∗ I+ )+ ¯I ˜])∗ [I ¯I ˜]+ )+ ¯ı ˜]+ ∣[I (O∣B(o∣b[ı Figure 2: Examples and regular expressions for the 4 tagging schemes. Strong links are depicted with solid arcs, and weak links with dotted arcs. The bottom analysis was provided by an annotator; the ones above are simplifications. score because F1 = F1↑ = F1↓ . This method applies to both the link-based and exact match evaluation criteria. 4 Tagging Schemes Following (Ramshaw and Marcus, 1995), shallow analysis is often modeled as a sequence-chunking task, with tags containing chunk-positional information. The BIO scheme and variants (e.g., BILOU; Ratinov and Roth, 2009) are standard for tasks like named entity recognition, supersense tagging, and shallow parsing. The language of derivations licensed by the grammars in §3 allows for a tag-based encoding of MWE analyses with only bigram constraints. We describe 4 tagging schemes for MWE identification, starting with BIO and working up to more expressive variants. They are depicted in figure 2. No gaps, 1-level (3 tags). This is the"
Q14-1016,W09-1119,0,0.264965,"+ ¯I ˜])∗ [I ¯I ˜]+ )+ ¯ı ˜]+ ∣[I (O∣B(o∣b[ı Figure 2: Examples and regular expressions for the 4 tagging schemes. Strong links are depicted with solid arcs, and weak links with dotted arcs. The bottom analysis was provided by an annotator; the ones above are simplifications. score because F1 = F1↑ = F1↓ . This method applies to both the link-based and exact match evaluation criteria. 4 Tagging Schemes Following (Ramshaw and Marcus, 1995), shallow analysis is often modeled as a sequence-chunking task, with tags containing chunk-positional information. The BIO scheme and variants (e.g., BILOU; Ratinov and Roth, 2009) are standard for tasks like named entity recognition, supersense tagging, and shallow parsing. The language of derivations licensed by the grammars in §3 allows for a tag-based encoding of MWE analyses with only bigram constraints. We describe 4 tagging schemes for MWE identification, starting with BIO and working up to more expressive variants. They are depicted in figure 2. No gaps, 1-level (3 tags). This is the standard contiguous chunking representation from Ramshaw and Marcus (1995) using the tags {O B I}. O is for tokens outside any chunk; B marks tokens beginning a chunk; and I marks o"
Q14-1016,D11-1141,0,0.0107976,"Missing"
Q14-1016,schneider-etal-2014-comprehensive,1,0.374841,"rpora, it has usually been as a secondary aspect of some other scheme. Traditionally, such resources have prioritized certain kinds of MWEs to the exclusion of others, so they are not appropriate for evaluating general-purpose identification systems. In this article, we briefly review a shallow form of analysis for MWEs that is neutral to expression type, and that facilitates free text annotation without requiring a prespecified MWE lexicon (§2). The scheme applies to gappy (discontinuous) as well as contiguous expressions, and allows for a qualitative distinction of association strengths. In Schneider et al. (2014) we have applied this scheme to fully annotate a 55,000-word corpus of English web reviews (Bies et al., 2012a), a conversational genre in which colloquial idioms are highly salient. This article’s main contribution is to show that the representation— constrained according to linguistically motivated assumptions (§3)—can be transformed into a sequence tagging scheme that resembles standard approaches in named entity recognition and other text chunking tasks (§4). Along these lines, we develop a discriminative, structured model of MWEs in context (§5) and train, evaluate, and examine it on the"
Q14-1016,N03-1033,0,0.00694519,"Missing"
Q14-1016,C10-2144,0,0.0511466,"non-adjacent component supertags like other labels—it cannot enforce that they mutually require one another, as we do via the gappy tagging scheme (§3.1). The lexicon lookup procedures of Bejˇcek et al. (2013) can match gappy MWEs, but are nonstatistical and extremely error-prone when tuned for high oracle recall. Another major thread of research has pursued unsupervised discovery of multiword types from raw corpora, such as with statistical association measures (Church et al., 1991; Pecina, 2010; Ramisch et al., 2012, inter alia), parallel corpora (Melamed, 1997; Moirón and Tiedemann, 2006; Tsvetkov and Wintner, 2010), or a combination thereof (Tsvetkov and Wintner, 2011); this may be followed by a lookupand-classify approach to contextual identification (Ramisch et al., 2010). Though preliminary experiments with our models did not show benefit to incorporating such automatically constructed lexicons, we hope these two perspectives can be brought together in future work. 8 Conclusion This article has presented the first supervised model for identifying heterogeneous multiword expressions in English text. Our feature-rich discriminative sequence tagger performs shallow chunking with a novel scheme that allo"
Q14-1016,D11-1077,0,0.0260984,"cannot enforce that they mutually require one another, as we do via the gappy tagging scheme (§3.1). The lexicon lookup procedures of Bejˇcek et al. (2013) can match gappy MWEs, but are nonstatistical and extremely error-prone when tuned for high oracle recall. Another major thread of research has pursued unsupervised discovery of multiword types from raw corpora, such as with statistical association measures (Church et al., 1991; Pecina, 2010; Ramisch et al., 2012, inter alia), parallel corpora (Melamed, 1997; Moirón and Tiedemann, 2006; Tsvetkov and Wintner, 2010), or a combination thereof (Tsvetkov and Wintner, 2011); this may be followed by a lookupand-classify approach to contextual identification (Ramisch et al., 2010). Though preliminary experiments with our models did not show benefit to incorporating such automatically constructed lexicons, we hope these two perspectives can be brought together in future work. 8 Conclusion This article has presented the first supervised model for identifying heterogeneous multiword expressions in English text. Our feature-rich discriminative sequence tagger performs shallow chunking with a novel scheme that allows for MWEs containing gaps, and includes a strength di"
Q14-1016,W11-0807,0,0.0247747,"ppy expressions or the strong/weak distinction, we would expect it to do no better when trained with the full tagset than with the simplified tagset. However, there is some loss in performance as the tagset for learning is simplified, which suggests that gappiness and strength are being learned to an extent. 7 Related Work Our annotated corpus (Schneider et al., 2014) joins several resources that indicate certain varieties of MWEs: lexicons such as WordNet (Fellbaum, 1998), SAID (Kuiper et al., 2003), and WikiMwe (Hartmann et al., 2012); targeted lists (Baldwin, 2005, 2008; Cook et al., 2008; Tu and Roth, 2011, 2012); websites like Wiktionary and Phrases.net; and large-scale corpora such as SemCor (Miller et al., 1993), the French Treebank (Abeillé et al., 2003), the SzegedParalellFX corpus (Vincze, 2012), and the Prague ˇ Czech-English Dependency Treebank (Cmejrek et al., 202 2005). The difference is that Schneider et al. (2014) pursued a comprehensive annotation approach rather than targeting specific varieties of MWEs or relying on a preexisting lexical resource. The annotations are shallow, not relying explicitly on syntax (though in principle they could be mapped onto the parses in the Web Tre"
Q14-1016,S12-1010,0,0.0570731,"Missing"
Q14-1016,P10-1040,0,0.00760394,"iables like &lt;something>. Given a sentence and one or more of the lexicons, lookup proceeds as follows: we enumerate entries whose lemma sequences match a sequence of lemmatized tokens, and build a lattice of possible analyses over the sentence. We find the shortest path (i.e., using as few expressions as possible) with dynamic programming, allowing gaps of up to length 2.13 Unsupervised word clusters. Distributional clustering on large (unlabeled) corpora can produce lexical generalizations that are useful for syntactic and semantic analysis tasks (e.g.: Miller et al., 2004; Koo et al., 2008; Turian et al., 2010; Owoputi et al., 2013; Grave et al., 2013). We were interested to see whether a similar pattern would hold for MWE identification, given that MWEs are concerned with what is lexically idiosyncratic—i.e., backing off from specific lexemes to word classes may lose the MWE-relevant information. Brown clustering14 (Brown et al., 1992) 12 http://www.phrases.net/ and postech.ac.kr/~oyz/doc/idiom.html http://home. 13 Each top-level lexical expression (single- or multiword) incurs a cost of 1; each expression within a gap has cost 1.25. 14 With Liang’s (2005) implementation: https://github. com/percy"
Q14-1016,2005.eamt-1.11,0,0.0139397,"Missing"
Q14-1016,M95-1005,0,0.0417178,"on1 the price which means4 a43 lot43 to4 me4 . Subscripts denote strong MW groups and superscripts weak MW groups; unmarked tokens serve as single-word expressions. The MW groups are thus {budge, on}, {a, little}, {a, lot}, and {means, {a, lot}, to, me}. As should be evident from the grammar, the projectivity and gap-nesting constraints apply here just as in the 1-level scheme. 3.2 Evaluation Matching criteria. Given that most tokens do not belong to an MWE, to evaluate MWE identification we adopt a precision/recall-based measure from the coreference resolution literature. The MUC criterion (Vilain et al., 1995) measures precision and recall great gateways never1 before1 , so23 far23 as23 Hudson knew2 , seen1 by Europeans was annotated in another corpus. 4 This was violated 6 times in our annotated data: modifiers within gaps are sometimes collocated with the gappy expression, as in on12 a12 tight1 budget12 and have12 little1 doubt12 . 196 of links in terms of groups (units) implied by the transitive closure over those links.5 It can be defined as follows: Let a Ð b denote a link between two elements in the gold standard, and aÐb ˆ denote a link in the system prediction. Let the ∗ operator denote the"
Q14-1016,vincze-2012-light,0,0.02716,"set for learning is simplified, which suggests that gappiness and strength are being learned to an extent. 7 Related Work Our annotated corpus (Schneider et al., 2014) joins several resources that indicate certain varieties of MWEs: lexicons such as WordNet (Fellbaum, 1998), SAID (Kuiper et al., 2003), and WikiMwe (Hartmann et al., 2012); targeted lists (Baldwin, 2005, 2008; Cook et al., 2008; Tu and Roth, 2011, 2012); websites like Wiktionary and Phrases.net; and large-scale corpora such as SemCor (Miller et al., 1993), the French Treebank (Abeillé et al., 2003), the SzegedParalellFX corpus (Vincze, 2012), and the Prague ˇ Czech-English Dependency Treebank (Cmejrek et al., 202 2005). The difference is that Schneider et al. (2014) pursued a comprehensive annotation approach rather than targeting specific varieties of MWEs or relying on a preexisting lexical resource. The annotations are shallow, not relying explicitly on syntax (though in principle they could be mapped onto the parses in the Web Treebank). In terms of modeling, the use of machine learning classification (Hashimoto and Kawahara, 2008; Shigeto et al., 2013) and specifically BIO sequence tagging (Diab and Bhutada, 2009; Constant a"
Q14-1016,J13-1009,0,\N,Missing
Q14-1016,W13-1021,0,\N,Missing
Q14-1029,P14-1035,1,0.0533871,"Missing"
Q14-1029,N07-4013,0,0.0323342,"Missing"
Q14-1029,P08-1092,0,0.211931,"Missing"
Q14-1029,E06-1002,0,0.0744382,"Missing"
Q14-1029,P08-1090,0,0.419924,"Missing"
Q14-1029,P09-1068,0,0.211534,"Missing"
Q14-1029,P07-2044,0,0.051892,"Missing"
Q14-1029,D13-1185,0,0.163245,"Missing"
Q14-1029,N13-1104,0,0.0263112,"Missing"
Q14-1029,D07-1074,0,0.0535691,"Missing"
Q14-1029,P10-1015,0,0.0341515,"Missing"
Q14-1029,D11-1142,0,0.0145687,"Missing"
Q14-1029,P05-1045,0,0.0106523,"Missing"
Q14-1029,H93-1026,0,0.338049,"Missing"
Q14-1029,P06-1095,0,0.111292,"Missing"
Q14-1029,W12-1901,0,0.0554933,"Missing"
Q14-1029,N03-1033,0,0.0702563,"Missing"
Q14-1029,S07-1014,0,0.0251174,"Missing"
Q14-1029,D11-1135,0,0.0363269,"Missing"
Q16-1031,W14-3902,0,0.0352708,"Missing"
Q16-1031,W06-2920,0,0.165241,"l., 2013; Nivre et al., 2015b; Agi´c et al., 2015; Nivre et al., 2015a) has created an opportunity to develop a parser that is capable of parsing sentences in multiple languages, addressing these theoretical and practical concerns.3 A multilingual parser can potentially replace an array of language-specific monolingually-trained parsers 2 While our parser can be used to parse input with codeswitching, we have not evaluated this capability due to the lack of appropriate data. 3 Although multilingual dependency treebanks have been available for a decade via the 2006 and 2007 CoNLL shared tasks (Buchholz and Marsi, 2006; Nivre et al., 2007), the treebank of each language was annotated independently and with its own annotation conventions. 431 Transactions of the Association for Computational Linguistics, vol. 4, pp. 431–444, 2016. Action Editor: David Chiang. Submission batch: 3/2016; Revision batch: 5/2016; Published 7/2016. c 2016 Association for Computational Linguistics. Distributed under a CC-BY 4.0 license. (for languages with a large treebank). The same approach has been used in low-resource scenarios (with no treebank or a small treebank in the target language), where indirect supervision from auxili"
Q16-1031,D14-1082,0,0.821854,"languages,4 optionally predicting the part-of-speech (POS) tags and input language. The parser is trained on the union of available universal dependency annotations in different languages. Our approach integrates and critically relies on several recent developments related to dependency parsing: universal POS tagsets (Petrov et al., 2012), cross-lingual word clusters (Täckström et al., 2012), selective sharing (Naseem et al., 2012), universal dependency annotations (McDonald et al., 2013; Nivre et al., 2015b; Agi´c et al., 2015; Nivre et al., 2015a), advances in neural network architectures (Chen and Manning, 2014; Dyer et al., 2015), and multilingual word embeddings (Gardner et al., 2015; Guo et al., 2016; Ammar et al., 2016). We show that our parser compares favorably to strong baselines trained on the same treebanks in three data scenarios: when the target language has a large treebank (Table 3), a small treebank (Table 7), or no treebank (Table 8). Our parser is publicly available.5 2 Overview Our goal is to train a dependency parser for a set of target languages Lt , given universal dependency annotations in a set of source languages Ls . Ideally, we would like to have training data in all target"
Q16-1031,D11-1005,1,0.955701,"language was annotated independently and with its own annotation conventions. 431 Transactions of the Association for Computational Linguistics, vol. 4, pp. 431–444, 2016. Action Editor: David Chiang. Submission batch: 3/2016; Revision batch: 5/2016; Published 7/2016. c 2016 Association for Computational Linguistics. Distributed under a CC-BY 4.0 license. (for languages with a large treebank). The same approach has been used in low-resource scenarios (with no treebank or a small treebank in the target language), where indirect supervision from auxiliary languages improves the parsing quality (Cohen et al., 2011; McDonald et al., 2011; Zhang and Barzilay, 2015; Duong et al., 2015a; Duong et al., 2015b; Guo et al., 2016), but these models may sacrifice accuracy on source languages with a large treebank. In this paper, we describe a model that works well for both low-resource and high-resource scenarios. We propose a parsing architecture that takes as input sentences in several languages,4 optionally predicting the part-of-speech (POS) tags and input language. The parser is trained on the union of available universal dependency annotations in different languages. Our approach integrates and critically"
Q16-1031,P15-2139,0,0.546315,"entions. 431 Transactions of the Association for Computational Linguistics, vol. 4, pp. 431–444, 2016. Action Editor: David Chiang. Submission batch: 3/2016; Revision batch: 5/2016; Published 7/2016. c 2016 Association for Computational Linguistics. Distributed under a CC-BY 4.0 license. (for languages with a large treebank). The same approach has been used in low-resource scenarios (with no treebank or a small treebank in the target language), where indirect supervision from auxiliary languages improves the parsing quality (Cohen et al., 2011; McDonald et al., 2011; Zhang and Barzilay, 2015; Duong et al., 2015a; Duong et al., 2015b; Guo et al., 2016), but these models may sacrifice accuracy on source languages with a large treebank. In this paper, we describe a model that works well for both low-resource and high-resource scenarios. We propose a parsing architecture that takes as input sentences in several languages,4 optionally predicting the part-of-speech (POS) tags and input language. The parser is trained on the union of available universal dependency annotations in different languages. Our approach integrates and critically relies on several recent developments related to dependency parsing:"
Q16-1031,D15-1040,0,0.568546,"entions. 431 Transactions of the Association for Computational Linguistics, vol. 4, pp. 431–444, 2016. Action Editor: David Chiang. Submission batch: 3/2016; Revision batch: 5/2016; Published 7/2016. c 2016 Association for Computational Linguistics. Distributed under a CC-BY 4.0 license. (for languages with a large treebank). The same approach has been used in low-resource scenarios (with no treebank or a small treebank in the target language), where indirect supervision from auxiliary languages improves the parsing quality (Cohen et al., 2011; McDonald et al., 2011; Zhang and Barzilay, 2015; Duong et al., 2015a; Duong et al., 2015b; Guo et al., 2016), but these models may sacrifice accuracy on source languages with a large treebank. In this paper, we describe a model that works well for both low-resource and high-resource scenarios. We propose a parsing architecture that takes as input sentences in several languages,4 optionally predicting the part-of-speech (POS) tags and input language. The parser is trained on the union of available universal dependency annotations in different languages. Our approach integrates and critically relies on several recent developments related to dependency parsing:"
Q16-1031,D12-1001,0,0.0806483,"ework, while proposing a novel neural architecture for embedding language typology (see §3.4), and use a variant of word dropout (Iyyer et al., 2015) for consuming noisy structured inputs. We also show how to replace an array of monolingually trained parsers with one multilinguallytrained parser without sacrificing accuracy, which is related to Vilares et al. (2016). Neural network parsing models which preceded Dyer et al. (2015) include Henderson (2003), Titov and Henderson (2007), Henderson and Titov (2010) and Chen and Manning (2014). Related to lexical features in cross-lingual parsing is Durrett et al. (2012) who defined lexico-syntactic features based on bilingual lexicons. Other related work include Östling (2015), which may be used to induce more useful typological properties to inform multilingual parsing. Another popular approach for cross-lingual supervision is to project annotations from the source language to the target language via a parallel cor441 pus (Yarowsky et al., 2001; Hwa et al., 2005) or via automatically-translated sentences (Tiedemann et al., 2014). Ma and Xia (2014) used entropy regularization to learn from both parallel data (with projected annotations) and unlabeled data in"
Q16-1031,P15-1033,1,0.28604,"predicting the part-of-speech (POS) tags and input language. The parser is trained on the union of available universal dependency annotations in different languages. Our approach integrates and critically relies on several recent developments related to dependency parsing: universal POS tagsets (Petrov et al., 2012), cross-lingual word clusters (Täckström et al., 2012), selective sharing (Naseem et al., 2012), universal dependency annotations (McDonald et al., 2013; Nivre et al., 2015b; Agi´c et al., 2015; Nivre et al., 2015a), advances in neural network architectures (Chen and Manning, 2014; Dyer et al., 2015), and multilingual word embeddings (Gardner et al., 2015; Guo et al., 2016; Ammar et al., 2016). We show that our parser compares favorably to strong baselines trained on the same treebanks in three data scenarios: when the target language has a large treebank (Table 3), a small treebank (Table 7), or no treebank (Table 8). Our parser is publicly available.5 2 Overview Our goal is to train a dependency parser for a set of target languages Lt , given universal dependency annotations in a set of source languages Ls . Ideally, we would like to have training data in all target languages (i.e., Lt"
Q16-1031,P15-1119,0,0.573596,"parsing with M A LOPA, we start with a simple delexicalized model where the token representation only consists of learned embeddings of coarse POS tags, which are shared across all languages to enable model transfer. In the following subsections, we enhance the token representation in M A LOPA to include lexical embeddings, language embeddings, and fine-grained POS embeddings. 3.3 Lexical Embeddings Previous work has shown that sacrificing lexical features amounts to a substantial decrease in the performance of a dependency parser (Cohen et al., 2011; Täckström et al., 2012; Tiedemann, 2015; Guo et al., 2015). Therefore, we extend the token representation in M A LOPA by concatenating learned embeddings of multilingual word clusters, and pretrained multilingual embeddings of word types. Multilingual Brown clusters. Before training the parser, we estimate Brown clusters of English words and project them via word alignments to words in other languages. This is similar to the ‘projected clusters’ method in Täckström et al. (2012). To go from Brown clusters to embeddings, we ignore the hierarchy within Brown clusters and assign a unique parameter vector to each cluster. Multilingual word embeddings. We"
Q16-1031,E89-1018,0,0.221127,"Missing"
Q16-1031,N03-1014,0,0.100262,"ers between two languages, and used an `2 regularizer to tie the lexical embeddings of translationallyequivalent words. We incorporate these ideas in our framework, while proposing a novel neural architecture for embedding language typology (see §3.4), and use a variant of word dropout (Iyyer et al., 2015) for consuming noisy structured inputs. We also show how to replace an array of monolingually trained parsers with one multilinguallytrained parser without sacrificing accuracy, which is related to Vilares et al. (2016). Neural network parsing models which preceded Dyer et al. (2015) include Henderson (2003), Titov and Henderson (2007), Henderson and Titov (2010) and Chen and Manning (2014). Related to lexical features in cross-lingual parsing is Durrett et al. (2012) who defined lexico-syntactic features based on bilingual lexicons. Other related work include Östling (2015), which may be used to induce more useful typological properties to inform multilingual parsing. Another popular approach for cross-lingual supervision is to project annotations from the source language to the target language via a parallel cor441 pus (Yarowsky et al., 2001; Hwa et al., 2005) or via automatically-translated se"
Q16-1031,P15-1162,0,0.195853,"efines a categorical distribution over possible POS tags. For parsing, we construct the token representation by further concatenating the embeddings of predicted POS tags. This token representation feeds into the stack-LSTM modules of the buffer and stack components of the transition-based parser. This multi-task learning setup enables us to predict both POS tags and dependency trees in the same model. We note that pretrained word embeddings, cluster embeddings and language embeddings are shared for tagging and parsing. Block dropout. We use an independently developed variant of word dropout (Iyyer et al., 2015), which we call block dropout. The token representation used for parsing includes the embedding of predicted POS tags, which may be incorrect. We introduce another modification which makes the parser more robust to incorrect POS tag predictions, by stochastically zeroing out the entire embedding of the POS tag. While training the parser, we replace the POS embedding vector e with another vector (of the same dimensionality) stochastically computed as: e0 = (1 − b)/µ × e, where b ∈ {0, 1} is a Bernoulli-distributed random variable with parameter µ which is initialized to 1.0 (i.e., always dropou"
Q16-1031,P12-3005,0,0.0572083,"Missing"
Q16-1031,P14-1126,0,0.0336688,"erson and Titov (2010) and Chen and Manning (2014). Related to lexical features in cross-lingual parsing is Durrett et al. (2012) who defined lexico-syntactic features based on bilingual lexicons. Other related work include Östling (2015), which may be used to induce more useful typological properties to inform multilingual parsing. Another popular approach for cross-lingual supervision is to project annotations from the source language to the target language via a parallel cor441 pus (Yarowsky et al., 2001; Hwa et al., 2005) or via automatically-translated sentences (Tiedemann et al., 2014). Ma and Xia (2014) used entropy regularization to learn from both parallel data (with projected annotations) and unlabeled data in the target language. Rasooli and Collins (2015) trained an array of target-language parsers on fully annotated trees, by iteratively decoding sentences in the target language with incomplete annotations. One research direction worth pursuing is to find synergies between the model transfer approach and annotation projection approach. 6 Conclusion We presented M A LOPA, a single parser trained on a multilingual set of treebanks. We showed that this parser, equipped with language embed"
Q16-1031,D11-1006,0,0.536368,"ed independently and with its own annotation conventions. 431 Transactions of the Association for Computational Linguistics, vol. 4, pp. 431–444, 2016. Action Editor: David Chiang. Submission batch: 3/2016; Revision batch: 5/2016; Published 7/2016. c 2016 Association for Computational Linguistics. Distributed under a CC-BY 4.0 license. (for languages with a large treebank). The same approach has been used in low-resource scenarios (with no treebank or a small treebank in the target language), where indirect supervision from auxiliary languages improves the parsing quality (Cohen et al., 2011; McDonald et al., 2011; Zhang and Barzilay, 2015; Duong et al., 2015a; Duong et al., 2015b; Guo et al., 2016), but these models may sacrifice accuracy on source languages with a large treebank. In this paper, we describe a model that works well for both low-resource and high-resource scenarios. We propose a parsing architecture that takes as input sentences in several languages,4 optionally predicting the part-of-speech (POS) tags and input language. The parser is trained on the union of available universal dependency annotations in different languages. Our approach integrates and critically relies on several recen"
Q16-1031,P12-1066,0,0.677145,"eebank. In this paper, we describe a model that works well for both low-resource and high-resource scenarios. We propose a parsing architecture that takes as input sentences in several languages,4 optionally predicting the part-of-speech (POS) tags and input language. The parser is trained on the union of available universal dependency annotations in different languages. Our approach integrates and critically relies on several recent developments related to dependency parsing: universal POS tagsets (Petrov et al., 2012), cross-lingual word clusters (Täckström et al., 2012), selective sharing (Naseem et al., 2012), universal dependency annotations (McDonald et al., 2013; Nivre et al., 2015b; Agi´c et al., 2015; Nivre et al., 2015a), advances in neural network architectures (Chen and Manning, 2014; Dyer et al., 2015), and multilingual word embeddings (Gardner et al., 2015; Guo et al., 2016; Ammar et al., 2016). We show that our parser compares favorably to strong baselines trained on the same treebanks in three data scenarios: when the target language has a large treebank (Table 3), a small treebank (Table 7), or no treebank (Table 8). Our parser is publicly available.5 2 Overview Our goal is to train a"
Q16-1031,P05-1013,0,0.0543947,"nsition-based Parsing with S-LSTMs This section briefly reviews Dyer et al.’s S-LSTM parser, which we modify in the following sections. The core parser can be understood as the sequential manipulation of three data structures: • a buffer (from which we read the token sequence), The parser uses the arc-standard transition system (Nivre, 2004).11 At each timestep t, a transition action is applied that alters these data structures according to Table 2. 11 In a preprocessing step, we transform nonprojective trees in the training treebanks to pseudo-projective trees using the “baseline” scheme in (Nivre and Nilsson, 2005). We evaluate against the original nonprojective test set. 433 Token Representations The vector representations of input tokens feed into the stack-LSTM modules of the buffer and the stack. 12 A stack-LSTM module is used to compute the vector representation for each data structure, as detailed in Dyer et al. (2015). 13 The total number of actions is 1+2× the number of unique dependency labels in the treebank used for training, but we only consider actions which meet the arc-standard preconditions in Fig. 2. Stackt u, v, S u, v, S S Buffert B B u, B Action REDUCE - RIGHT (r) REDUCE - LEFT (r) S"
Q16-1031,W04-0308,0,0.0841575,"action in every time step until a complete parse tree is produced. The following sections describe our extensions of the core parser. More details about the core parser can be found in Dyer et al. (2015). • a list of actions previously taken by the parser. 3.2 3.1 Transition-based Parsing with S-LSTMs This section briefly reviews Dyer et al.’s S-LSTM parser, which we modify in the following sections. The core parser can be understood as the sequential manipulation of three data structures: • a buffer (from which we read the token sequence), The parser uses the arc-standard transition system (Nivre, 2004).11 At each timestep t, a transition action is applied that alters these data structures according to Table 2. 11 In a preprocessing step, we transform nonprojective trees in the training treebanks to pseudo-projective trees using the “baseline” scheme in (Nivre and Nilsson, 2005). We evaluate against the original nonprojective test set. 433 Token Representations The vector representations of input tokens feed into the stack-LSTM modules of the buffer and the stack. 12 A stack-LSTM module is used to compute the vector representation for each data structure, as detailed in Dyer et al. (2015). 1"
Q16-1031,P15-2034,0,0.047877,"word dropout (Iyyer et al., 2015) for consuming noisy structured inputs. We also show how to replace an array of monolingually trained parsers with one multilinguallytrained parser without sacrificing accuracy, which is related to Vilares et al. (2016). Neural network parsing models which preceded Dyer et al. (2015) include Henderson (2003), Titov and Henderson (2007), Henderson and Titov (2010) and Chen and Manning (2014). Related to lexical features in cross-lingual parsing is Durrett et al. (2012) who defined lexico-syntactic features based on bilingual lexicons. Other related work include Östling (2015), which may be used to induce more useful typological properties to inform multilingual parsing. Another popular approach for cross-lingual supervision is to project annotations from the source language to the target language via a parallel cor441 pus (Yarowsky et al., 2001; Hwa et al., 2005) or via automatically-translated sentences (Tiedemann et al., 2014). Ma and Xia (2014) used entropy regularization to learn from both parallel data (with projected annotations) and unlabeled data in the target language. Rasooli and Collins (2015) trained an array of target-language parsers on fully annotat"
Q16-1031,petrov-etal-2012-universal,0,0.079337,"Guo et al., 2016), but these models may sacrifice accuracy on source languages with a large treebank. In this paper, we describe a model that works well for both low-resource and high-resource scenarios. We propose a parsing architecture that takes as input sentences in several languages,4 optionally predicting the part-of-speech (POS) tags and input language. The parser is trained on the union of available universal dependency annotations in different languages. Our approach integrates and critically relies on several recent developments related to dependency parsing: universal POS tagsets (Petrov et al., 2012), cross-lingual word clusters (Täckström et al., 2012), selective sharing (Naseem et al., 2012), universal dependency annotations (McDonald et al., 2013; Nivre et al., 2015b; Agi´c et al., 2015; Nivre et al., 2015a), advances in neural network architectures (Chen and Manning, 2014; Dyer et al., 2015), and multilingual word embeddings (Gardner et al., 2015; Guo et al., 2016; Ammar et al., 2016). We show that our parser compares favorably to strong baselines trained on the same treebanks in three data scenarios: when the target language has a large treebank (Table 3), a small treebank (Table 7),"
Q16-1031,D15-1039,0,0.0550973,"ntactic features based on bilingual lexicons. Other related work include Östling (2015), which may be used to induce more useful typological properties to inform multilingual parsing. Another popular approach for cross-lingual supervision is to project annotations from the source language to the target language via a parallel cor441 pus (Yarowsky et al., 2001; Hwa et al., 2005) or via automatically-translated sentences (Tiedemann et al., 2014). Ma and Xia (2014) used entropy regularization to learn from both parallel data (with projected annotations) and unlabeled data in the target language. Rasooli and Collins (2015) trained an array of target-language parsers on fully annotated trees, by iteratively decoding sentences in the target language with incomplete annotations. One research direction worth pursuing is to find synergies between the model transfer approach and annotation projection approach. 6 Conclusion We presented M A LOPA, a single parser trained on a multilingual set of treebanks. We showed that this parser, equipped with language embeddings and fine-grained POS embeddings, on average outperforms monolingually-trained parsers for target languages with a treebank. This pattern of results is qui"
Q16-1031,P15-1165,0,0.11785,"Missing"
Q16-1031,N12-1052,0,0.735129,"accuracy on source languages with a large treebank. In this paper, we describe a model that works well for both low-resource and high-resource scenarios. We propose a parsing architecture that takes as input sentences in several languages,4 optionally predicting the part-of-speech (POS) tags and input language. The parser is trained on the union of available universal dependency annotations in different languages. Our approach integrates and critically relies on several recent developments related to dependency parsing: universal POS tagsets (Petrov et al., 2012), cross-lingual word clusters (Täckström et al., 2012), selective sharing (Naseem et al., 2012), universal dependency annotations (McDonald et al., 2013; Nivre et al., 2015b; Agi´c et al., 2015; Nivre et al., 2015a), advances in neural network architectures (Chen and Manning, 2014; Dyer et al., 2015), and multilingual word embeddings (Gardner et al., 2015; Guo et al., 2016; Ammar et al., 2016). We show that our parser compares favorably to strong baselines trained on the same treebanks in three data scenarios: when the target language has a large treebank (Table 3), a small treebank (Table 7), or no treebank (Table 8). Our parser is publicly avai"
Q16-1031,Q13-1001,0,0.0298107,"OPA de 54.1 55.9 57.1 es 68.3 73.0 74.6 target language fr it pt 68.8 69.4 72.5 71.0 71.2 78.6 73.9 72.5 77.0 average sv 62.5 69.5 68.1 65.9 69.3 70.5 Table 8: Dependency parsing: labeled attachment scores (LAS) for multi-source transfer parsers in the simulated low-resource scenario where Lt ∩ Ls = ∅. who trained a parser on a source language treebank then applied it to parse sentences in a target language. Cohen et al. (2011) and McDonald et al. (2011) trained unlexicalized parsers on treebanks of multiple source languages and applied the parser to different languages. Naseem et al. (2012), Täckström et al. (2013), and Zhang and Barzilay (2015) used language typology to improve model transfer. To add lexical information, Täckström et al. (2012) used multilingual word clusters, while Xiao and Guo (2014), Guo et al. (2015), Søgaard et al. (2015) and Guo et al. (2016) used multilingual word embeddings. Duong et al. (2015b) used a neural network based model, sharing most of the parameters between two languages, and used an `2 regularizer to tie the lexical embeddings of translationallyequivalent words. We incorporate these ideas in our framework, while proposing a novel neural architecture for embedding la"
Q16-1031,W14-1614,0,0.110791,"nd Henderson (2007), Henderson and Titov (2010) and Chen and Manning (2014). Related to lexical features in cross-lingual parsing is Durrett et al. (2012) who defined lexico-syntactic features based on bilingual lexicons. Other related work include Östling (2015), which may be used to induce more useful typological properties to inform multilingual parsing. Another popular approach for cross-lingual supervision is to project annotations from the source language to the target language via a parallel cor441 pus (Yarowsky et al., 2001; Hwa et al., 2005) or via automatically-translated sentences (Tiedemann et al., 2014). Ma and Xia (2014) used entropy regularization to learn from both parallel data (with projected annotations) and unlabeled data in the target language. Rasooli and Collins (2015) trained an array of target-language parsers on fully annotated trees, by iteratively decoding sentences in the target language with incomplete annotations. One research direction worth pursuing is to find synergies between the model transfer approach and annotation projection approach. 6 Conclusion We presented M A LOPA, a single parser trained on a multilingual set of treebanks. We showed that this parser, equipped"
Q16-1031,W15-2137,0,0.0388388,"For multilingual parsing with M A LOPA, we start with a simple delexicalized model where the token representation only consists of learned embeddings of coarse POS tags, which are shared across all languages to enable model transfer. In the following subsections, we enhance the token representation in M A LOPA to include lexical embeddings, language embeddings, and fine-grained POS embeddings. 3.3 Lexical Embeddings Previous work has shown that sacrificing lexical features amounts to a substantial decrease in the performance of a dependency parser (Cohen et al., 2011; Täckström et al., 2012; Tiedemann, 2015; Guo et al., 2015). Therefore, we extend the token representation in M A LOPA by concatenating learned embeddings of multilingual word clusters, and pretrained multilingual embeddings of word types. Multilingual Brown clusters. Before training the parser, we estimate Brown clusters of English words and project them via word alignments to words in other languages. This is similar to the ‘projected clusters’ method in Täckström et al. (2012). To go from Brown clusters to embeddings, we ignore the hierarchy within Brown clusters and assign a unique parameter vector to each cluster. Multilingual"
Q16-1031,P07-1080,0,0.0567686,"nguages, and used an `2 regularizer to tie the lexical embeddings of translationallyequivalent words. We incorporate these ideas in our framework, while proposing a novel neural architecture for embedding language typology (see §3.4), and use a variant of word dropout (Iyyer et al., 2015) for consuming noisy structured inputs. We also show how to replace an array of monolingually trained parsers with one multilinguallytrained parser without sacrificing accuracy, which is related to Vilares et al. (2016). Neural network parsing models which preceded Dyer et al. (2015) include Henderson (2003), Titov and Henderson (2007), Henderson and Titov (2010) and Chen and Manning (2014). Related to lexical features in cross-lingual parsing is Durrett et al. (2012) who defined lexico-syntactic features based on bilingual lexicons. Other related work include Östling (2015), which may be used to induce more useful typological properties to inform multilingual parsing. Another popular approach for cross-lingual supervision is to project annotations from the source language to the target language via a parallel cor441 pus (Yarowsky et al., 2001; Hwa et al., 2005) or via automatically-translated sentences (Tiedemann et al., 2"
Q16-1031,P16-2069,0,0.0633887,"Missing"
Q16-1031,W14-1613,0,0.0268074,"nt scores (LAS) for multi-source transfer parsers in the simulated low-resource scenario where Lt ∩ Ls = ∅. who trained a parser on a source language treebank then applied it to parse sentences in a target language. Cohen et al. (2011) and McDonald et al. (2011) trained unlexicalized parsers on treebanks of multiple source languages and applied the parser to different languages. Naseem et al. (2012), Täckström et al. (2013), and Zhang and Barzilay (2015) used language typology to improve model transfer. To add lexical information, Täckström et al. (2012) used multilingual word clusters, while Xiao and Guo (2014), Guo et al. (2015), Søgaard et al. (2015) and Guo et al. (2016) used multilingual word embeddings. Duong et al. (2015b) used a neural network based model, sharing most of the parameters between two languages, and used an `2 regularizer to tie the lexical embeddings of translationallyequivalent words. We incorporate these ideas in our framework, while proposing a novel neural architecture for embedding language typology (see §3.4), and use a variant of word dropout (Iyyer et al., 2015) for consuming noisy structured inputs. We also show how to replace an array of monolingually trained parsers"
Q16-1031,H01-1035,0,0.0721877,"k parsing models which preceded Dyer et al. (2015) include Henderson (2003), Titov and Henderson (2007), Henderson and Titov (2010) and Chen and Manning (2014). Related to lexical features in cross-lingual parsing is Durrett et al. (2012) who defined lexico-syntactic features based on bilingual lexicons. Other related work include Östling (2015), which may be used to induce more useful typological properties to inform multilingual parsing. Another popular approach for cross-lingual supervision is to project annotations from the source language to the target language via a parallel cor441 pus (Yarowsky et al., 2001; Hwa et al., 2005) or via automatically-translated sentences (Tiedemann et al., 2014). Ma and Xia (2014) used entropy regularization to learn from both parallel data (with projected annotations) and unlabeled data in the target language. Rasooli and Collins (2015) trained an array of target-language parsers on fully annotated trees, by iteratively decoding sentences in the target language with incomplete annotations. One research direction worth pursuing is to find synergies between the model transfer approach and annotation projection approach. 6 Conclusion We presented M A LOPA, a single pa"
Q16-1031,I08-3008,0,0.044074,"train the parser on six other languages in the Google universal dependency treebanks version 2.029 (de, en, es, fr, it, pt, sv, excluding whichever is the target language), and we use gold coarse POS tags. Our parser uses the same word embeddings and word clusters used in Guo et al. (2016), and does not use any typology information.30 The results in Table 8 show that, on average, our parser outperforms both baselines by more than 1 point in LAS, and gives the best LAS results in four (out of six) languages. 5 Related Work Our work builds on the model transfer approach, which was pioneered by Zeman and Resnik (2008) 29 https://github.com/ryanmcd/uni-dep-tb/ In preliminary experiments, we found language embeddings to hurt the performance of the parser for target languages without a treebank. 30 LAS Zhang and Barzilay (2015) Guo et al. (2016) M A LOPA de 54.1 55.9 57.1 es 68.3 73.0 74.6 target language fr it pt 68.8 69.4 72.5 71.0 71.2 78.6 73.9 72.5 77.0 average sv 62.5 69.5 68.1 65.9 69.3 70.5 Table 8: Dependency parsing: labeled attachment scores (LAS) for multi-source transfer parsers in the simulated low-resource scenario where Lt ∩ Ls = ∅. who trained a parser on a source language treebank then appli"
Q16-1031,D15-1213,0,0.342662,"th its own annotation conventions. 431 Transactions of the Association for Computational Linguistics, vol. 4, pp. 431–444, 2016. Action Editor: David Chiang. Submission batch: 3/2016; Revision batch: 5/2016; Published 7/2016. c 2016 Association for Computational Linguistics. Distributed under a CC-BY 4.0 license. (for languages with a large treebank). The same approach has been used in low-resource scenarios (with no treebank or a small treebank in the target language), where indirect supervision from auxiliary languages improves the parsing quality (Cohen et al., 2011; McDonald et al., 2011; Zhang and Barzilay, 2015; Duong et al., 2015a; Duong et al., 2015b; Guo et al., 2016), but these models may sacrifice accuracy on source languages with a large treebank. In this paper, we describe a model that works well for both low-resource and high-resource scenarios. We propose a parsing architecture that takes as input sentences in several languages,4 optionally predicting the part-of-speech (POS) tags and input language. The parser is trained on the union of available universal dependency annotations in different languages. Our approach integrates and critically relies on several recent developments related to"
Q16-1031,D15-1127,0,\N,Missing
Q19-1031,N18-1094,0,0.660573,"debater’s success, but they do not emphasize the role of the individual debater. There has been some recent work on studying individual debaters. Durmus and Cardie (2019) analyze users and find that a user’s success and improvement depend on their social network features. We take another approach by estimating each user’s skill level in each debate they participate in by considering their debate history. These estimates reveal features correlated with skill and the importance of particular debates over time. Our study is based on debates from an online debate forum, Debate.org, introduced by Durmus and Cardie (2018) and discussed in §2. This Web site is composed of primarily textbased debates and attracts a large number of users to debate regularly. Our model of skill builds on the Elo (1978) rating system, designed for rating players in two-player games (§3). Our preliminary analysis using Elo scores suggests that user skill is not static; debaters in the Debate.org forum tend Online debates allow people to express their persuasive abilities and provide exciting opportunities for understanding persuasion. Prior studies have focused on studying persuasion in debate content, but without accounting for eac"
Q19-1031,E17-1070,0,0.135798,"er time, Zhang et al. (2017) study how one document may improve over time through annotated revisions. Where our work examines users’ linguistic change across multiple debates, they focus on how a user improves a single document over multiple revisions. Online debates. There has also been recent work in characterizing specific arguments in online settings in contrast to our focus on the debaters themselves. For example, Somasundaran and Wiebe (2009), Walker et al. (2012), Qiu et al. (2015), and Sridhar et al. (2015) built systems for identifying the stances users take in online debate forums. Lukin et al. (2017) studied how persuasiveness of arguments depends on personality factors of the audience. Other researchers have focused on annotation tasks. For example, Park and Cardie (2014) annotated online user comments to identify and classify different propositions. Hidey et al. (2017) annotated comments from the changemyview subreddit, a community where participants ask the community to change a view they hold. Likewise, Anand et al. (2011) annotated online blogs with a classification of persuasive tactics. Inspired by Aristotle’s three modes of persuasion (ethos, pathos, and logos) their work annotate"
Q19-1031,D16-1129,0,0.168404,"pends on personality factors of the audience. Other researchers have focused on annotation tasks. For example, Park and Cardie (2014) annotated online user comments to identify and classify different propositions. Hidey et al. (2017) annotated comments from the changemyview subreddit, a community where participants ask the community to change a view they hold. Likewise, Anand et al. (2011) annotated online blogs with a classification of persuasive tactics. Inspired by Aristotle’s three modes of persuasion (ethos, pathos, and logos) their work annotates claims and premises within the comments. Habernal and Gurevych (2016) used crowdsourcing to study what makes an argument more convincing. They paired two similar arguments and asked annotators which one was more convincing. This framework allowed them to study the flaws in the less convincing arguments. The annotations they produced offer a rich understanding of arguments which, though costly, can be useful as future work. 9 Acknowledgments We thank Esin Durmus for her help with the DDO corpus. We also thank the anonymous reviewers and the action editor for their helpful comments and suggestions that helped improve the paper. This research was supported in part"
Q19-1031,J17-1004,0,0.0129286,"se of debaters over the course of their debating activity to show that the best users improve while the bottom users stagnate. Our approach sets the stage for future explorations of the role of history profile, discourse, more fine-grained sentiment features, or notions of topic in persuasion. also studied political debates and found that a debater’s tendency to switch topics correlates with their public perception. Argumentation has also been studied extensively in student persuasive essays and web discourse (Persing and Ng, 2015; Ong et al., 2014; Song et al., 2014; Stab and Gurevych, 2014; Habernal and Gurevych, 2017; Lippi and Torroni, 2016). Most relevant to our work on how users improve over time, Zhang et al. (2017) study how one document may improve over time through annotated revisions. Where our work examines users’ linguistic change across multiple debates, they focus on how a user improves a single document over multiple revisions. Online debates. There has also been recent work in characterizing specific arguments in online settings in contrast to our focus on the debaters themselves. For example, Somasundaran and Wiebe (2009), Walker et al. (2012), Qiu et al. (2015), and Sridhar et al. (2015) b"
Q19-1031,W14-2104,0,0.0298289,"pothesis that users improve, we explicitly track the feature use of debaters over the course of their debating activity to show that the best users improve while the bottom users stagnate. Our approach sets the stage for future explorations of the role of history profile, discourse, more fine-grained sentiment features, or notions of topic in persuasion. also studied political debates and found that a debater’s tendency to switch topics correlates with their public perception. Argumentation has also been studied extensively in student persuasive essays and web discourse (Persing and Ng, 2015; Ong et al., 2014; Song et al., 2014; Stab and Gurevych, 2014; Habernal and Gurevych, 2017; Lippi and Torroni, 2016). Most relevant to our work on how users improve over time, Zhang et al. (2017) study how one document may improve over time through annotated revisions. Where our work examines users’ linguistic change across multiple debates, they focus on how a user improves a single document over multiple revisions. Online debates. There has also been recent work in characterizing specific arguments in online settings in contrast to our focus on the debaters themselves. For example, Somasundaran and Wiebe (20"
Q19-1031,W14-2105,0,0.0298088,"es, they focus on how a user improves a single document over multiple revisions. Online debates. There has also been recent work in characterizing specific arguments in online settings in contrast to our focus on the debaters themselves. For example, Somasundaran and Wiebe (2009), Walker et al. (2012), Qiu et al. (2015), and Sridhar et al. (2015) built systems for identifying the stances users take in online debate forums. Lukin et al. (2017) studied how persuasiveness of arguments depends on personality factors of the audience. Other researchers have focused on annotation tasks. For example, Park and Cardie (2014) annotated online user comments to identify and classify different propositions. Hidey et al. (2017) annotated comments from the changemyview subreddit, a community where participants ask the community to change a view they hold. Likewise, Anand et al. (2011) annotated online blogs with a classification of persuasive tactics. Inspired by Aristotle’s three modes of persuasion (ethos, pathos, and logos) their work annotates claims and premises within the comments. Habernal and Gurevych (2016) used crowdsourcing to study what makes an argument more convincing. They paired two similar arguments an"
Q19-1031,P15-1053,0,0.018893,"ones. To verify our hypothesis that users improve, we explicitly track the feature use of debaters over the course of their debating activity to show that the best users improve while the bottom users stagnate. Our approach sets the stage for future explorations of the role of history profile, discourse, more fine-grained sentiment features, or notions of topic in persuasion. also studied political debates and found that a debater’s tendency to switch topics correlates with their public perception. Argumentation has also been studied extensively in student persuasive essays and web discourse (Persing and Ng, 2015; Ong et al., 2014; Song et al., 2014; Stab and Gurevych, 2014; Habernal and Gurevych, 2017; Lippi and Torroni, 2016). Most relevant to our work on how users improve over time, Zhang et al. (2017) study how one document may improve over time through annotated revisions. Where our work examines users’ linguistic change across multiple debates, they focus on how a user improves a single document over multiple revisions. Online debates. There has also been recent work in characterizing specific arguments in online settings in contrast to our focus on the debaters themselves. For example, Somasund"
Q19-1031,D17-1261,0,0.106146,"sets. This notion of upset is not used in the forecasting task. 6 In addition to requiring 10 debates instead of 5 per user, as in the rest of our analysis, we also do not subject the opponents to the same requirements. 540 By observing debate outcomes alongside the two participants’ histories, we can estimate the parameters of such a probability model. Elo provides a baseline; rather than opaque scores associated with individual users at different times, we seek to explain the probability of winning through linguistic features of past debate content. Unlike previous work (Zhang et al., 2016; Potash and Rumshisky, 2017; Tan et al., 2018), we do not use the content of the current debate (dA t ) to predict its outcome; rather, we forecast the outcome of the debate as if the debate has not yet occurred. To our knowledge, this is the first work to derive scores for current skill levels based on observing participants’ behavior over time. In the remainder of this section, we discuss features of past debates and ways of aggregating them. the contender rounds of that debate were written by his opponent). Table 2 shows the full list of features. Hedging with fightin’ words. We introduce one novel feature for our wo"
Q19-1031,W17-5102,0,0.137536,"has also been recent work in characterizing specific arguments in online settings in contrast to our focus on the debaters themselves. For example, Somasundaran and Wiebe (2009), Walker et al. (2012), Qiu et al. (2015), and Sridhar et al. (2015) built systems for identifying the stances users take in online debate forums. Lukin et al. (2017) studied how persuasiveness of arguments depends on personality factors of the audience. Other researchers have focused on annotation tasks. For example, Park and Cardie (2014) annotated online user comments to identify and classify different propositions. Hidey et al. (2017) annotated comments from the changemyview subreddit, a community where participants ask the community to change a view they hold. Likewise, Anand et al. (2011) annotated online blogs with a classification of persuasive tactics. Inspired by Aristotle’s three modes of persuasion (ethos, pathos, and logos) their work annotates claims and premises within the comments. Habernal and Gurevych (2016) used crowdsourcing to study what makes an argument more convincing. They paired two similar arguments and asked annotators which one was more convincing. This framework allowed them to study the flaws in"
Q19-1031,D14-1157,0,0.0212336,"ts such as phrasing or linguistic accommodation. For example, Danescu-Niculescu-Mizil et al. (2012) showed that, in a pool of people vying to become an administrator of a Web site, those who were promoted tended to coordinate more than those who were not. Similarly, other work defines and discusses power relations over discussion threads such as emails (Prabhakaran and Rambow, 2014, 2013). Additionally, Tan et al. (2018) explored how debate quotes are selected by news media. They found that linguistic and interactive factors of an utterance are predictive of whether or not it would be quoted. Prabhakaran et al. (2014) Related Work In addition to the most relevant studies mentioned so far, our work is related to three broad areas: skill estimation, argumentation mining, and studies of online debates. 547 around predicting which of two debaters will be most convincing to observers predisposed to be unconvinced. We find that linguistic profiles on their own are similarly predictive to the classic Elo method, which does not parameterize skill according to attributes of a participant or their behavior, but only models wins and losses. Moreover, we show that our findings are robust to topic of debate and frequen"
Q19-1031,P14-1017,1,0.862502,"n@colorado.edu Abstract clear signal, a win or loss, indicating whether or not a debater was successful against the adversary. This work aims to quantify the skill level of each debater in an online community and also investigates what factors contribute to expertise. Although persuasion has generated interest in the natural language processing community, most researchers have not tried to quantify the persuasiveness of a particular speaker. Instead, they estimate how persuasive a text is, using linguistic features such as the author’s choice of wording or how they interact with the audience (Tan et al., 2014, 2016; Althoff et al., 2014; Danescu-Niculescu-Mizil et al., 2012). Previous research has also established that debaters’ content and interactions both contribute to the success of the persuader (Tan et al., 2016; Zhang et al., 2016; Wang et al., 2017). These works have found textual factors that contribute to a debater’s success, but they do not emphasize the role of the individual debater. There has been some recent work on studying individual debaters. Durmus and Cardie (2019) analyze users and find that a user’s success and improvement depend on their social network features. We take anot"
Q19-1031,I13-1025,0,0.0772681,"Missing"
Q19-1031,P14-2056,0,0.0263745,"icant change over time except for the length of their rounds and negative emotional cue use. In those cases, the worst users seem to worsen over time. 8 Argumentation and persuasion. Past studies have noted the persuasiveness of stylistic effects such as phrasing or linguistic accommodation. For example, Danescu-Niculescu-Mizil et al. (2012) showed that, in a pool of people vying to become an administrator of a Web site, those who were promoted tended to coordinate more than those who were not. Similarly, other work defines and discusses power relations over discussion threads such as emails (Prabhakaran and Rambow, 2014, 2013). Additionally, Tan et al. (2018) explored how debate quotes are selected by news media. They found that linguistic and interactive factors of an utterance are predictive of whether or not it would be quoted. Prabhakaran et al. (2014) Related Work In addition to the most relevant studies mentioned so far, our work is related to three broad areas: skill estimation, argumentation mining, and studies of online debates. 547 around predicting which of two debaters will be most convincing to observers predisposed to be unconvinced. We find that linguistic profiles on their own are similarly p"
Q19-1031,N12-1072,0,0.0296196,"g et al., 2014; Stab and Gurevych, 2014; Habernal and Gurevych, 2017; Lippi and Torroni, 2016). Most relevant to our work on how users improve over time, Zhang et al. (2017) study how one document may improve over time through annotated revisions. Where our work examines users’ linguistic change across multiple debates, they focus on how a user improves a single document over multiple revisions. Online debates. There has also been recent work in characterizing specific arguments in online settings in contrast to our focus on the debaters themselves. For example, Somasundaran and Wiebe (2009), Walker et al. (2012), Qiu et al. (2015), and Sridhar et al. (2015) built systems for identifying the stances users take in online debate forums. Lukin et al. (2017) studied how persuasiveness of arguments depends on personality factors of the audience. Other researchers have focused on annotation tasks. For example, Park and Cardie (2014) annotated online user comments to identify and classify different propositions. Hidey et al. (2017) annotated comments from the changemyview subreddit, a community where participants ask the community to change a view they hold. Likewise, Anand et al. (2011) annotated online blo"
Q19-1031,P09-1026,0,0.0461398,"Ng, 2015; Ong et al., 2014; Song et al., 2014; Stab and Gurevych, 2014; Habernal and Gurevych, 2017; Lippi and Torroni, 2016). Most relevant to our work on how users improve over time, Zhang et al. (2017) study how one document may improve over time through annotated revisions. Where our work examines users’ linguistic change across multiple debates, they focus on how a user improves a single document over multiple revisions. Online debates. There has also been recent work in characterizing specific arguments in online settings in contrast to our focus on the debaters themselves. For example, Somasundaran and Wiebe (2009), Walker et al. (2012), Qiu et al. (2015), and Sridhar et al. (2015) built systems for identifying the stances users take in online debate forums. Lukin et al. (2017) studied how persuasiveness of arguments depends on personality factors of the audience. Other researchers have focused on annotation tasks. For example, Park and Cardie (2014) annotated online user comments to identify and classify different propositions. Hidey et al. (2017) annotated comments from the changemyview subreddit, a community where participants ask the community to change a view they hold. Likewise, Anand et al. (2011"
Q19-1031,Q17-1016,0,0.0819711,"ute to expertise. Although persuasion has generated interest in the natural language processing community, most researchers have not tried to quantify the persuasiveness of a particular speaker. Instead, they estimate how persuasive a text is, using linguistic features such as the author’s choice of wording or how they interact with the audience (Tan et al., 2014, 2016; Althoff et al., 2014; Danescu-Niculescu-Mizil et al., 2012). Previous research has also established that debaters’ content and interactions both contribute to the success of the persuader (Tan et al., 2016; Zhang et al., 2016; Wang et al., 2017). These works have found textual factors that contribute to a debater’s success, but they do not emphasize the role of the individual debater. There has been some recent work on studying individual debaters. Durmus and Cardie (2019) analyze users and find that a user’s success and improvement depend on their social network features. We take another approach by estimating each user’s skill level in each debate they participate in by considering their debate history. These estimates reveal features correlated with skill and the importance of particular debates over time. Our study is based on de"
Q19-1031,W14-2110,0,0.0194655,"s improve, we explicitly track the feature use of debaters over the course of their debating activity to show that the best users improve while the bottom users stagnate. Our approach sets the stage for future explorations of the role of history profile, discourse, more fine-grained sentiment features, or notions of topic in persuasion. also studied political debates and found that a debater’s tendency to switch topics correlates with their public perception. Argumentation has also been studied extensively in student persuasive essays and web discourse (Persing and Ng, 2015; Ong et al., 2014; Song et al., 2014; Stab and Gurevych, 2014; Habernal and Gurevych, 2017; Lippi and Torroni, 2016). Most relevant to our work on how users improve over time, Zhang et al. (2017) study how one document may improve over time through annotated revisions. Where our work examines users’ linguistic change across multiple debates, they focus on how a user improves a single document over multiple revisions. Online debates. There has also been recent work in characterizing specific arguments in online settings in contrast to our focus on the debaters themselves. For example, Somasundaran and Wiebe (2009), Walker et al."
Q19-1031,P17-1144,0,0.0311229,"ers stagnate. Our approach sets the stage for future explorations of the role of history profile, discourse, more fine-grained sentiment features, or notions of topic in persuasion. also studied political debates and found that a debater’s tendency to switch topics correlates with their public perception. Argumentation has also been studied extensively in student persuasive essays and web discourse (Persing and Ng, 2015; Ong et al., 2014; Song et al., 2014; Stab and Gurevych, 2014; Habernal and Gurevych, 2017; Lippi and Torroni, 2016). Most relevant to our work on how users improve over time, Zhang et al. (2017) study how one document may improve over time through annotated revisions. Where our work examines users’ linguistic change across multiple debates, they focus on how a user improves a single document over multiple revisions. Online debates. There has also been recent work in characterizing specific arguments in online settings in contrast to our focus on the debaters themselves. For example, Somasundaran and Wiebe (2009), Walker et al. (2012), Qiu et al. (2015), and Sridhar et al. (2015) built systems for identifying the stances users take in online debate forums. Lukin et al. (2017) studied"
Q19-1031,P15-1012,0,0.0235261,"rnal and Gurevych, 2017; Lippi and Torroni, 2016). Most relevant to our work on how users improve over time, Zhang et al. (2017) study how one document may improve over time through annotated revisions. Where our work examines users’ linguistic change across multiple debates, they focus on how a user improves a single document over multiple revisions. Online debates. There has also been recent work in characterizing specific arguments in online settings in contrast to our focus on the debaters themselves. For example, Somasundaran and Wiebe (2009), Walker et al. (2012), Qiu et al. (2015), and Sridhar et al. (2015) built systems for identifying the stances users take in online debate forums. Lukin et al. (2017) studied how persuasiveness of arguments depends on personality factors of the audience. Other researchers have focused on annotation tasks. For example, Park and Cardie (2014) annotated online user comments to identify and classify different propositions. Hidey et al. (2017) annotated comments from the changemyview subreddit, a community where participants ask the community to change a view they hold. Likewise, Anand et al. (2011) annotated online blogs with a classification of persuasive tactics"
Q19-1031,N16-1017,0,0.318813,"what factors contribute to expertise. Although persuasion has generated interest in the natural language processing community, most researchers have not tried to quantify the persuasiveness of a particular speaker. Instead, they estimate how persuasive a text is, using linguistic features such as the author’s choice of wording or how they interact with the audience (Tan et al., 2014, 2016; Althoff et al., 2014; Danescu-Niculescu-Mizil et al., 2012). Previous research has also established that debaters’ content and interactions both contribute to the success of the persuader (Tan et al., 2016; Zhang et al., 2016; Wang et al., 2017). These works have found textual factors that contribute to a debater’s success, but they do not emphasize the role of the individual debater. There has been some recent work on studying individual debaters. Durmus and Cardie (2019) analyze users and find that a user’s success and improvement depend on their social network features. We take another approach by estimating each user’s skill level in each debate they participate in by considering their debate history. These estimates reveal features correlated with skill and the importance of particular debates over time. Our"
Q19-1031,D14-1006,0,0.0352902,"citly track the feature use of debaters over the course of their debating activity to show that the best users improve while the bottom users stagnate. Our approach sets the stage for future explorations of the role of history profile, discourse, more fine-grained sentiment features, or notions of topic in persuasion. also studied political debates and found that a debater’s tendency to switch topics correlates with their public perception. Argumentation has also been studied extensively in student persuasive essays and web discourse (Persing and Ng, 2015; Ong et al., 2014; Song et al., 2014; Stab and Gurevych, 2014; Habernal and Gurevych, 2017; Lippi and Torroni, 2016). Most relevant to our work on how users improve over time, Zhang et al. (2017) study how one document may improve over time through annotated revisions. Where our work examines users’ linguistic change across multiple debates, they focus on how a user improves a single document over multiple revisions. Online debates. There has also been recent work in characterizing specific arguments in online settings in contrast to our focus on the debaters themselves. For example, Somasundaran and Wiebe (2009), Walker et al. (2012), Qiu et al. (2015)"
S10-1059,N10-1138,1,0.246183,"rnegie Mellon University, Pittsburgh, PA 15213, USA {desaic@andrew,dipanjan@cs,nschneid@cs,nasmith@cs}.cmu.edu Abstract we detect null instantiations via a simple two-stage pipeline: the first stage predicts whether a given role is null-instantiated, and the second stage (§4) predicts how it is null-instantiated, if it is not overt. We report performance on the SemEval 2010 test set under the full-SRL and NI-only conditions. This paper describes the SEMAFOR system’s performance in the SemEval 2010 task on linking events and their participants in discourse. Our entry is based upon SEMAFOR 1.0 (Das et al., 2010a), a frame-semantic probabilistic parser built from log-linear models. The extended system models null instantiations, including non-local argument reference. Performance is evaluated on the task data with and without gold-standard overt arguments. In both settings, it fares the best of the submitted systems with respect to recall and F1 . 1 2 Data The SemEval 2007 task on frame-semantic parsing (Baker et al., 2007) provided a small (about 50,000 words and 2,000 sentences) dataset of news text, travel guides, and bureaucratic accounts of weapons stockpiles. Sentences in this dataset were full"
S10-1059,S07-1018,0,\N,Missing
S10-1059,S10-1008,0,\N,Missing
S12-1029,P11-1048,0,0.0143799,"emantic frame abstractions, and employ explicit role labels. A common trait in prior work has been the use of a two-stage model that identifies arguments first, then labels them. They are treated jointly here, unlike what has typically been done in PropBank-style SRL (M`arquez et al., 2008). Dual decomposition: Rush et al. (2010) proposed subgradient-based dual decomposition as a way of combining models which are tractable individually, but not jointly, by solving a relaxation of the original problem. This was followed by work adopting this method for syntax and translation (Koo et al., 2010; Auli and Lopez, 2011; DeNero and Macherey, 2011; Rush and Collins, 2011; Chang and Collins, 2011). Recently, Martins et al. (2011b) showed that the success of subgradient-based dual decomposition strongly relies on breaking down the original problem into a “good” decomposition, i.e., one with few overlapping components. This leaves out many declarative constrained problems, for which such a good decomposition is not readily available. For those, Martins et al. (2011b) proposed the AD3 al216 gorithm, which retains the modularity of previous methods, but can handle thousands of small overlapping components. Exact d"
S12-1029,S07-1018,0,0.0523873,"xact solution. Two observations are noteworthy. First, the optimal value of the relaxed problem (Eq. 11) provides an upper bound to the original problem (Eq. 2). This is because Eq. 2 has the additional integer constraint on the variables. In particular, any feasible dual point provides an upper bound to the original 214 4.1 Experiments and Results Dataset, Preprocessing, and Learning In our experiments, we use FrameNet 1.5, which contains a lexicon of 877 frames and 1,068 role labels, and 78 documents with multiple predicateargument annotations (a superset of the SemEval shared task dataset; Baker et al., 2007). We used the same split as Das and Smith (2011), with 55 documents for training (containing 19,582 frame annotations) and 23 for testing (with 4,458 annotations). We randomly selected 4,462 predicates in the training set as development data. The raw sentences in all the training and test documents were preprocessed using MXPOST (Ratnaparkhi, 1996) and the MST dependency parser (McDonald et al., 2005). The state-of-the-art system for this task is SEMAFOR, an open source tool (Das et al., 2010a)5 that provides a baseline benchmark for our new algorithm. We use the components of SEMAFOR as-is to"
S12-1029,D11-1003,0,0.0195552,"it in prior work has been the use of a two-stage model that identifies arguments first, then labels them. They are treated jointly here, unlike what has typically been done in PropBank-style SRL (M`arquez et al., 2008). Dual decomposition: Rush et al. (2010) proposed subgradient-based dual decomposition as a way of combining models which are tractable individually, but not jointly, by solving a relaxation of the original problem. This was followed by work adopting this method for syntax and translation (Koo et al., 2010; Auli and Lopez, 2011; DeNero and Macherey, 2011; Rush and Collins, 2011; Chang and Collins, 2011). Recently, Martins et al. (2011b) showed that the success of subgradient-based dual decomposition strongly relies on breaking down the original problem into a “good” decomposition, i.e., one with few overlapping components. This leaves out many declarative constrained problems, for which such a good decomposition is not readily available. For those, Martins et al. (2011b) proposed the AD3 al216 gorithm, which retains the modularity of previous methods, but can handle thousands of small overlapping components. Exact decoding: This paper contributes an exact branch-and-bound technique wrapped a"
S12-1029,S10-1059,1,0.488928,"Missing"
S12-1029,P11-1144,1,0.396255,"First, the optimal value of the relaxed problem (Eq. 11) provides an upper bound to the original problem (Eq. 2). This is because Eq. 2 has the additional integer constraint on the variables. In particular, any feasible dual point provides an upper bound to the original 214 4.1 Experiments and Results Dataset, Preprocessing, and Learning In our experiments, we use FrameNet 1.5, which contains a lexicon of 877 frames and 1,068 role labels, and 78 documents with multiple predicateargument annotations (a superset of the SemEval shared task dataset; Baker et al., 2007). We used the same split as Das and Smith (2011), with 55 documents for training (containing 19,582 frame annotations) and 23 for testing (with 4,458 annotations). We randomly selected 4,462 predicates in the training set as development data. The raw sentences in all the training and test documents were preprocessed using MXPOST (Ratnaparkhi, 1996) and the MST dependency parser (McDonald et al., 2005). The state-of-the-art system for this task is SEMAFOR, an open source tool (Das et al., 2010a)5 that provides a baseline benchmark for our new algorithm. We use the components of SEMAFOR as-is to define the features h and train the weights ψ u"
S12-1029,N10-1138,1,0.116028,"cuments with multiple predicateargument annotations (a superset of the SemEval shared task dataset; Baker et al., 2007). We used the same split as Das and Smith (2011), with 55 documents for training (containing 19,582 frame annotations) and 23 for testing (with 4,458 annotations). We randomly selected 4,462 predicates in the training set as development data. The raw sentences in all the training and test documents were preprocessed using MXPOST (Ratnaparkhi, 1996) and the MST dependency parser (McDonald et al., 2005). The state-of-the-art system for this task is SEMAFOR, an open source tool (Das et al., 2010a)5 that provides a baseline benchmark for our new algorithm. We use the components of SEMAFOR as-is to define the features h and train the weights ψ used in the scoring function c. We also use its 5 http://www.ark.cs.cmu.edu/SEMAFOR heuristic mechanism to find potential spans St for a given predicate t. SEMAFOR learns weights using `2 -penalized log-likelihood; we augmented its dev set-tuning procedure to tune both the regularization strength and the AD3 penalty strength ρ. We initialize ρ = 0.1 and follow Martins et al. (2011b) in dynamically adjusting it. Note that we do not use SEMAFOR’s a"
S12-1029,P11-1043,0,0.0256002,"ions, and employ explicit role labels. A common trait in prior work has been the use of a two-stage model that identifies arguments first, then labels them. They are treated jointly here, unlike what has typically been done in PropBank-style SRL (M`arquez et al., 2008). Dual decomposition: Rush et al. (2010) proposed subgradient-based dual decomposition as a way of combining models which are tractable individually, but not jointly, by solving a relaxation of the original problem. This was followed by work adopting this method for syntax and translation (Koo et al., 2010; Auli and Lopez, 2011; DeNero and Macherey, 2011; Rush and Collins, 2011; Chang and Collins, 2011). Recently, Martins et al. (2011b) showed that the success of subgradient-based dual decomposition strongly relies on breaking down the original problem into a “good” decomposition, i.e., one with few overlapping components. This leaves out many declarative constrained problems, for which such a good decomposition is not readily available. For those, Martins et al. (2011b) proposed the AD3 al216 gorithm, which retains the modularity of previous methods, but can handle thousands of small overlapping components. Exact decoding: This paper contrib"
S12-1029,P10-1160,0,0.0220157,"of effort has been spent by the optimization community to devise efficient generic solvers. An example is CPLEX, a state-of-the-art solver for mixed integer programming that we employ as a baseline to solve the ILP in Eq. 2 as well as its LP relaxation in Eq. 11. Like many of the best implementations, CPLEX is proprietary. 4 We noticed in the annotated data, in some cases, the “requires” constraint is violated by the FrameNet annotators. This happens mostly when one of the required roles is absent in the sentence containing the predicate, but is rather instantiated in an earlier sentence; see Gerber and Chai (2010). We apply the hard constraint in Eq. 10, though extending our algorithm to seek arguments outside the sentence is straightforward (Chen et al., 2010). 2.2 Linguistic Constraints from FrameNet a pair of roles that share the “requires” relationship. Although enforcing the four different sets of constraints above is intuitive from a general linguistic perspective, we ground their use in definitive linguistic information present in the FrameNet lexicon (Fillmore et al., 2003). FrameNet, along with lists of semantic frames, associated semantic roles, and predicates that could evoke the frames, giv"
S12-1029,J02-3001,0,0.124019,"“requires” and “excludes” constraints of §2. Finally decoding time (without feature computation steps) on the whole test set is shown in the last column averaged over 5 runs. 5 Related Work Semantic role labeling: Most SRL systems use conventions from PropBank (Kingsbury and Palmer, 2002) and NomBank (Meyers et al., 2004), which store information about verbal and nominal predicates and corresponding symbolic and meaningspecific semantic roles. A separate line of work, including this paper, investigates SRL systems that use FrameNet conventions; while less popular, these systems, pioneered by Gildea and Jurafsky (2002), consider predicates of a wider variety of syntactic categories, use semantic frame abstractions, and employ explicit role labels. A common trait in prior work has been the use of a two-stage model that identifies arguments first, then labels them. They are treated jointly here, unlike what has typically been done in PropBank-style SRL (M`arquez et al., 2008). Dual decomposition: Rush et al. (2010) proposed subgradient-based dual decomposition as a way of combining models which are tractable individually, but not jointly, by solving a relaxation of the original problem. This was followed by w"
S12-1029,S07-1048,0,0.0706484,"Missing"
S12-1029,kingsbury-palmer-2002-treebank,0,0.514983,"using FrameNet, these interactions have been largely ignored, though they 1 have the potential to improve the quality and consistency of semantic analysis. In this paper, we present an algorithm that finds the full collection of arguments of a predicate given its semantic frame. Although we work within the conventions of FrameNet, our approach is generalizable to other semantic role labeling (SRL) frameworks. We model this argument identification task as constrained optimization, where the constraints come from expert knowledge encoded in a lexicon. Following prior work on PropBank-style SRL (Kingsbury and Palmer, 2002) that dealt with similar constrained problems (Punyakanok et al., 2004; Punyakanok et al., 2008, inter alia), we incorporate this declarative knowledge in an integer linear program (ILP). Because general-purpose ILP solvers are proprietary and do not fully exploit the structure of the problem, we turn to a class of optimization techniques called dual decomposition (Komodakis et al., 2007; Rush et al., 2010; Martins et al., 2011a). We derive a modular, extensible, parallelizable approach in which semantic constraints map not just to declarative components of the algorithm, but also to procedura"
S12-1029,D10-1125,0,0.0857449,"categories, use semantic frame abstractions, and employ explicit role labels. A common trait in prior work has been the use of a two-stage model that identifies arguments first, then labels them. They are treated jointly here, unlike what has typically been done in PropBank-style SRL (M`arquez et al., 2008). Dual decomposition: Rush et al. (2010) proposed subgradient-based dual decomposition as a way of combining models which are tractable individually, but not jointly, by solving a relaxation of the original problem. This was followed by work adopting this method for syntax and translation (Koo et al., 2010; Auli and Lopez, 2011; DeNero and Macherey, 2011; Rush and Collins, 2011; Chang and Collins, 2011). Recently, Martins et al. (2011b) showed that the success of subgradient-based dual decomposition strongly relies on breaking down the original problem into a “good” decomposition, i.e., one with few overlapping components. This leaves out many declarative constrained problems, for which such a good decomposition is not readily available. For those, Martins et al. (2011b) proposed the AD3 al216 gorithm, which retains the modularity of previous methods, but can handle thousands of small overlappi"
S12-1029,J08-2001,0,0.0417426,"Missing"
S12-1029,D10-1004,1,0.572151,"rtner 1 and Partner 2 that share the “requires” relationship. In fact, out of 877 frames in FrameNet 1.5, the lexicon’s latest edition, 204 frames have at least a pair of roles that share the “excludes” relationship, and 54 list at least 2.3 Constraints as Factors in a Graphical Model 212 The LP in Eq. 11 can be represented as a maximum a posteriori (MAP) inference problem in an undirected graphical model. In the factor graph, each component of z corresponds to a binary variable, and each instantiation of a constraint in Eqs. 3, 5, 8 and 10 corresponds to a factor. Smith and Eisner (2008) and Martins et al. (2010) used such a representation to impose constraints in a dependency parsing problem; the latter discussed the equivalence of linear programs and factor graphs for representing discrete optimization problems. Each of our constraints take standard factor forms we can describe using the terminology of Smith and Eisner (2008) and Martins et al. (2010). The uniqueness constraint in Eq. 3 corresponds to an X OR factor, while the overlap constraint in Eq. 5 corresponds to an AT M OST O NE factor. The constraints in Eq. 8 enforcing the “excludes” relationship can be represented with an O R factor. Final"
S12-1029,D11-1022,1,0.770273,"entification task as constrained optimization, where the constraints come from expert knowledge encoded in a lexicon. Following prior work on PropBank-style SRL (Kingsbury and Palmer, 2002) that dealt with similar constrained problems (Punyakanok et al., 2004; Punyakanok et al., 2008, inter alia), we incorporate this declarative knowledge in an integer linear program (ILP). Because general-purpose ILP solvers are proprietary and do not fully exploit the structure of the problem, we turn to a class of optimization techniques called dual decomposition (Komodakis et al., 2007; Rush et al., 2010; Martins et al., 2011a). We derive a modular, extensible, parallelizable approach in which semantic constraints map not just to declarative components of the algorithm, but also to procedural ones, in the form of “workers.” While dual decomposition algorithms only solve a relaxation of the original problem, we make a novel contribution by wrapping the algorithm in a branch-andbound search procedure, resulting in exact solutions. We experimentally find that our algorithm achieves accuracy comparable to a state-of-the-art system, while respecting all imposed linguistic constraints. In comparison to inexact beam sear"
S12-1029,P05-1012,0,0.0760088,"ments, we use FrameNet 1.5, which contains a lexicon of 877 frames and 1,068 role labels, and 78 documents with multiple predicateargument annotations (a superset of the SemEval shared task dataset; Baker et al., 2007). We used the same split as Das and Smith (2011), with 55 documents for training (containing 19,582 frame annotations) and 23 for testing (with 4,458 annotations). We randomly selected 4,462 predicates in the training set as development data. The raw sentences in all the training and test documents were preprocessed using MXPOST (Ratnaparkhi, 1996) and the MST dependency parser (McDonald et al., 2005). The state-of-the-art system for this task is SEMAFOR, an open source tool (Das et al., 2010a)5 that provides a baseline benchmark for our new algorithm. We use the components of SEMAFOR as-is to define the features h and train the weights ψ used in the scoring function c. We also use its 5 http://www.ark.cs.cmu.edu/SEMAFOR heuristic mechanism to find potential spans St for a given predicate t. SEMAFOR learns weights using `2 -penalized log-likelihood; we augmented its dev set-tuning procedure to tune both the regularization strength and the AD3 penalty strength ρ. We initialize ρ = 0.1 and f"
S12-1029,W04-2705,0,0.0292647,".17 ± 0.01 4.78 ± 0.04 Table 1: Comparison of decoding strategies in §4.2. We evaluate in terms of precision, recall and F1 score on a test set containing 4,458 predicates. We also compute the number of structural violations each model makes: number of overlapping arguments and violations of the “requires” and “excludes” constraints of §2. Finally decoding time (without feature computation steps) on the whole test set is shown in the last column averaged over 5 runs. 5 Related Work Semantic role labeling: Most SRL systems use conventions from PropBank (Kingsbury and Palmer, 2002) and NomBank (Meyers et al., 2004), which store information about verbal and nominal predicates and corresponding symbolic and meaningspecific semantic roles. A separate line of work, including this paper, investigates SRL systems that use FrameNet conventions; while less popular, these systems, pioneered by Gildea and Jurafsky (2002), consider predicates of a wider variety of syntactic categories, use semantic frame abstractions, and employ explicit role labels. A common trait in prior work has been the use of a two-stage model that identifies arguments first, then labels them. They are treated jointly here, unlike what has t"
S12-1029,C04-1197,0,0.149314,"1 have the potential to improve the quality and consistency of semantic analysis. In this paper, we present an algorithm that finds the full collection of arguments of a predicate given its semantic frame. Although we work within the conventions of FrameNet, our approach is generalizable to other semantic role labeling (SRL) frameworks. We model this argument identification task as constrained optimization, where the constraints come from expert knowledge encoded in a lexicon. Following prior work on PropBank-style SRL (Kingsbury and Palmer, 2002) that dealt with similar constrained problems (Punyakanok et al., 2004; Punyakanok et al., 2008, inter alia), we incorporate this declarative knowledge in an integer linear program (ILP). Because general-purpose ILP solvers are proprietary and do not fully exploit the structure of the problem, we turn to a class of optimization techniques called dual decomposition (Komodakis et al., 2007; Rush et al., 2010; Martins et al., 2011a). We derive a modular, extensible, parallelizable approach in which semantic constraints map not just to declarative components of the algorithm, but also to procedural ones, in the form of “workers.” While dual decomposition algorithms"
S12-1029,J08-2005,0,0.890274,"mprove the quality and consistency of semantic analysis. In this paper, we present an algorithm that finds the full collection of arguments of a predicate given its semantic frame. Although we work within the conventions of FrameNet, our approach is generalizable to other semantic role labeling (SRL) frameworks. We model this argument identification task as constrained optimization, where the constraints come from expert knowledge encoded in a lexicon. Following prior work on PropBank-style SRL (Kingsbury and Palmer, 2002) that dealt with similar constrained problems (Punyakanok et al., 2004; Punyakanok et al., 2008, inter alia), we incorporate this declarative knowledge in an integer linear program (ILP). Because general-purpose ILP solvers are proprietary and do not fully exploit the structure of the problem, we turn to a class of optimization techniques called dual decomposition (Komodakis et al., 2007; Rush et al., 2010; Martins et al., 2011a). We derive a modular, extensible, parallelizable approach in which semantic constraints map not just to declarative components of the algorithm, but also to procedural ones, in the form of “workers.” While dual decomposition algorithms only solve a relaxation o"
S12-1029,W96-0213,0,0.0498838,"Dataset, Preprocessing, and Learning In our experiments, we use FrameNet 1.5, which contains a lexicon of 877 frames and 1,068 role labels, and 78 documents with multiple predicateargument annotations (a superset of the SemEval shared task dataset; Baker et al., 2007). We used the same split as Das and Smith (2011), with 55 documents for training (containing 19,582 frame annotations) and 23 for testing (with 4,458 annotations). We randomly selected 4,462 predicates in the training set as development data. The raw sentences in all the training and test documents were preprocessed using MXPOST (Ratnaparkhi, 1996) and the MST dependency parser (McDonald et al., 2005). The state-of-the-art system for this task is SEMAFOR, an open source tool (Das et al., 2010a)5 that provides a baseline benchmark for our new algorithm. We use the components of SEMAFOR as-is to define the features h and train the weights ψ used in the scoring function c. We also use its 5 http://www.ark.cs.cmu.edu/SEMAFOR heuristic mechanism to find potential spans St for a given predicate t. SEMAFOR learns weights using `2 -penalized log-likelihood; we augmented its dev set-tuning procedure to tune both the regularization strength and t"
S12-1029,P11-1008,0,0.0134412,"ole labels. A common trait in prior work has been the use of a two-stage model that identifies arguments first, then labels them. They are treated jointly here, unlike what has typically been done in PropBank-style SRL (M`arquez et al., 2008). Dual decomposition: Rush et al. (2010) proposed subgradient-based dual decomposition as a way of combining models which are tractable individually, but not jointly, by solving a relaxation of the original problem. This was followed by work adopting this method for syntax and translation (Koo et al., 2010; Auli and Lopez, 2011; DeNero and Macherey, 2011; Rush and Collins, 2011; Chang and Collins, 2011). Recently, Martins et al. (2011b) showed that the success of subgradient-based dual decomposition strongly relies on breaking down the original problem into a “good” decomposition, i.e., one with few overlapping components. This leaves out many declarative constrained problems, for which such a good decomposition is not readily available. For those, Martins et al. (2011b) proposed the AD3 al216 gorithm, which retains the modularity of previous methods, but can handle thousands of small overlapping components. Exact decoding: This paper contributes an exact branch-and"
S12-1029,D10-1001,0,0.260942,"el this argument identification task as constrained optimization, where the constraints come from expert knowledge encoded in a lexicon. Following prior work on PropBank-style SRL (Kingsbury and Palmer, 2002) that dealt with similar constrained problems (Punyakanok et al., 2004; Punyakanok et al., 2008, inter alia), we incorporate this declarative knowledge in an integer linear program (ILP). Because general-purpose ILP solvers are proprietary and do not fully exploit the structure of the problem, we turn to a class of optimization techniques called dual decomposition (Komodakis et al., 2007; Rush et al., 2010; Martins et al., 2011a). We derive a modular, extensible, parallelizable approach in which semantic constraints map not just to declarative components of the algorithm, but also to procedural ones, in the form of “workers.” While dual decomposition algorithms only solve a relaxation of the original problem, we make a novel contribution by wrapping the algorithm in a branch-andbound search procedure, resulting in exact solutions. We experimentally find that our algorithm achieves accuracy comparable to a state-of-the-art system, while respecting all imposed linguistic constraints. In compariso"
S12-1029,D08-1016,0,0.0185648,"tners, also has two roles Partner 1 and Partner 2 that share the “requires” relationship. In fact, out of 877 frames in FrameNet 1.5, the lexicon’s latest edition, 204 frames have at least a pair of roles that share the “excludes” relationship, and 54 list at least 2.3 Constraints as Factors in a Graphical Model 212 The LP in Eq. 11 can be represented as a maximum a posteriori (MAP) inference problem in an undirected graphical model. In the factor graph, each component of z corresponds to a binary variable, and each instantiation of a constraint in Eqs. 3, 5, 8 and 10 corresponds to a factor. Smith and Eisner (2008) and Martins et al. (2010) used such a representation to impose constraints in a dependency parsing problem; the latter discussed the equivalence of linear programs and factor graphs for representing discrete optimization problems. Each of our constraints take standard factor forms we can describe using the terminology of Smith and Eisner (2008) and Martins et al. (2010). The uniqueness constraint in Eq. 3 corresponds to an X OR factor, while the overlap constraint in Eq. 5 corresponds to an AT M OST O NE factor. The constraints in Eq. 8 enforcing the “excludes” relationship can be represented"
S12-1029,P05-1073,0,0.0878624,"t line imposes constraints on the mapping between roles and spans; these are motivated on linguistic grounds and are described next.3 Uniqueness: Each role r is filled by at most one span in St . This constraint can be expressed by: P ∀r ∈ Rf , s∈St zr,s = 1. (3) There are O(|Rf |) such constraints. Note that since St contains the null span ∅, non-overt roles are also captured using the above constraints. Such a constraint is used extensively in prior literature (Punyakanok et al., 2008, §3.4.1). Overlap: SRL systems commonly constrain roles to be filled by non-overlapping spans. For example, Toutanova et al. (2005) used dynamic programming over a phrase structure tree to prevent overlaps between arguments, and Punyakanok et al. (2008) used 3 Note that equality constraints a · z = b can be transformed into double-side inequalities a · z ≤ b and −a · z ≤ −b. constraints in an ILP to respect this requirement. Inspired by the latter, we require that each input sentence position of x be covered by at most one argument. For each role r ∈ Rf , we define: Gr (i) = {s |s ∈ St , s covers position i in x}. (4) We can define our overlap constraints in terms of Gr as follows, for every sentence position i: P P ∀i ∈"
S14-2027,J92-4003,0,0.0715867,"t at index i, the top classifier’s features included t’s POS tag, i, those two conjoined, and the depth of t in the syntactic dependency tree. 4 Word vectors: Features derived from 64-dimensional vectors from (Faruqui and Dyer, 2014), including the concatenation, difference, inner product, and elementwise multiplication of the two vectors associated with a parent-child edge. We also trained a Random Forest on the word vectors using Liaw and Wiener’s (2002) R implementation. The predicted labels were then used as features in L OGISTIC E DGE. Brown clusters Features derived from Brown clusters (Brown et al., 1992) trained on a large corpus of web data. Parent, child, and conjoined parent-child edge features from cluster prefixes of length 2, 4, 6, 8, 10, and 12. Conjunctions of those features with the POS tags of the parent and child tokens. Active/passive: Active/passive voice feature (as in Johansson and Nugues (2008)) conjoined with both the Linear Distance features and the Subcategorization Sequence features. Voice information may already be captured by features from the Stanford dependency–style parses, which include passivization information in arc labels such as nsubjpass and auxpass (de Marneff"
S14-2027,W08-1301,0,0.0702255,"Missing"
S14-2027,E14-1049,1,0.763349,"., nodes where there is at 178 least one outbound edge) are possible candidates to be “top”; the classifier probabilities are evaluated, and the highest-scoring node is chosen to be “top.” This is suboptimal, since some graphs have multiple tops (in PCEDT this is more common); but selection rules based on probability thresholds gave worse F1 performance on the dev set. For a given token t at index i, the top classifier’s features included t’s POS tag, i, those two conjoined, and the depth of t in the syntactic dependency tree. 4 Word vectors: Features derived from 64-dimensional vectors from (Faruqui and Dyer, 2014), including the concatenation, difference, inner product, and elementwise multiplication of the two vectors associated with a parent-child edge. We also trained a Random Forest on the word vectors using Liaw and Wiener’s (2002) R implementation. The predicted labels were then used as features in L OGISTIC E DGE. Brown clusters Features derived from Brown clusters (Brown et al., 1992) trained on a large corpus of web data. Parent, child, and conjoined parent-child edge features from cluster prefixes of length 2, 4, 6, 8, 10, and 12. Conjunctions of those features with the POS tags of the parent"
S14-2027,P14-1134,1,0.844279,"f length 2, 4, 6, 8, 10, and 12. Conjunctions of those features with the POS tags of the parent and child tokens. Active/passive: Active/passive voice feature (as in Johansson and Nugues (2008)) conjoined with both the Linear Distance features and the Subcategorization Sequence features. Voice information may already be captured by features from the Stanford dependency–style parses, which include passivization information in arc labels such as nsubjpass and auxpass (de Marneffe and Manning, 2008). Connectivity constraint: Enforcing that the graph is connected (ignoring singletons), similar to Flanigan et al. (2014). Almost all semantic dependency graphs in the training data are connected (ignoring singletons), but we found that enforcing this constraint significantly hurt precision. Tree constraint: Enforces that the graph is a tree. Unsurprisingly, we found that enforcing a tree constraint hurt performance. Negative Results We followed a forward-selection process during feature engineering. For each potential feature, we tested the current feature set versus the current feature set plus the new potential feature. If the new feature did not improve performance, we did not add it. We list in table 2 some"
S14-2027,D08-1008,0,0.0230174,"se multiplication of the two vectors associated with a parent-child edge. We also trained a Random Forest on the word vectors using Liaw and Wiener’s (2002) R implementation. The predicted labels were then used as features in L OGISTIC E DGE. Brown clusters Features derived from Brown clusters (Brown et al., 1992) trained on a large corpus of web data. Parent, child, and conjoined parent-child edge features from cluster prefixes of length 2, 4, 6, 8, 10, and 12. Conjunctions of those features with the POS tags of the parent and child tokens. Active/passive: Active/passive voice feature (as in Johansson and Nugues (2008)) conjoined with both the Linear Distance features and the Subcategorization Sequence features. Voice information may already be captured by features from the Stanford dependency–style parses, which include passivization information in arc labels such as nsubjpass and auxpass (de Marneffe and Manning, 2008). Connectivity constraint: Enforcing that the graph is connected (ignoring singletons), similar to Flanigan et al. (2014). Almost all semantic dependency graphs in the training data are connected (ignoring singletons), but we found that enforcing this constraint significantly hurt precision."
S14-2027,P13-2109,1,0.846354,"es and constraints giving negative results. Conclusion and Future Work We found that feature-rich discriminative models perform well at the task of mapping from sentences to semantic dependency parses. While our final approach is fairly standard for work in parsing, we note here additional features and constraints which did not appear to help (contrary to expectation). There are a number of clear extensions to this work that could improve performance. While an edge-factored model allows for efficient inference, there is much to be gained from higher-order features (McDonald and Pereira, 2006; Martins et al., 2013). The amount of information shared DM PAS PCEDT Average LP 0.8446 0.9078 0.7681 0.8402 LR 0.8348 0.8851 0.7072 0.8090 LF 0.8397 0.8963 0.7364 0.8241 LM 0.0875 0.2604 0.0712 0.1397 Table 3: Labeled precision (LP), recall (LR), F1 (LF), and whole-sentence match (LM) on the held-out test data. between the three formalisms suggests that a multitask learning (Evgeniou and Pontil, 2004) framework could lead to gains. And finally, there is additional structure in the formalisms which could be exploited (such as the deterministic processes by which an original PCEDT tree annotation was converted into"
S14-2027,E06-1011,0,0.0537858,"ur scores. 6 Table 2: Features and constraints giving negative results. Conclusion and Future Work We found that feature-rich discriminative models perform well at the task of mapping from sentences to semantic dependency parses. While our final approach is fairly standard for work in parsing, we note here additional features and constraints which did not appear to help (contrary to expectation). There are a number of clear extensions to this work that could improve performance. While an edge-factored model allows for efficient inference, there is much to be gained from higher-order features (McDonald and Pereira, 2006; Martins et al., 2013). The amount of information shared DM PAS PCEDT Average LP 0.8446 0.9078 0.7681 0.8402 LR 0.8348 0.8851 0.7072 0.8090 LF 0.8397 0.8963 0.7364 0.8241 LM 0.0875 0.2604 0.0712 0.1397 Table 3: Labeled precision (LP), recall (LR), F1 (LF), and whole-sentence match (LM) on the held-out test data. between the three formalisms suggests that a multitask learning (Evgeniou and Pontil, 2004) framework could lead to gains. And finally, there is additional structure in the formalisms which could be exploited (such as the deterministic processes by which an original PCEDT tree annotat"
S14-2027,S14-2008,0,0.0475934,"Missing"
S16-1143,J92-4003,0,0.613457,"ce, lemma, POS tag, MWE tag, offset of parent, and supersense label (if applicable). The blind test set consists of 1,000 sentences from three sources: online reviews from the TrustPilot corpus (Hovy et al., 2015), tweets from the Tweebank corpus (Kong et al., 2014) and TED talk transcripts from the IWSLT MT evaluation campaigns, obtained from the WIT3 archive (Cettolo et al., 2012). The shared task has three data conditions: supervised closed, semi-supervised closed, and open. In the supervised closed condition, only the labeled data, the English WordNet lexicon, a provided Brown clustering (Brown et al., 1992) on the 21million-word Yelp Academic Dataset5 (Schneider et al., 2014), and any of the ARK Tweet NLP clusters6 are allowed. The semi-supervised closed condition adds the Yelp Academic Dataset to the resources. The open condition allows the use of any available resources. We have participated in the supervised closed and open conditions. The evaluation is based on F1 score for MWE identification, supersense labeling, and their combination.7 5 https://www.yelp.com/academic_dataset http://www.cs.cmu.edu/˜ark/TweetNLP/ #resources 7 For details, see http://dimsum16.github.io Models 3.1 Input Featur"
S16-1143,S14-1001,0,0.116519,"tions. Label constraints on tag bigrams ensure a globally consistent tagging. Our experiments show that 2-CRF outperforms a zero-order baseline, the structured perceptron used Task Description For completeness, we briefly review the shared task. The shared task training dataset, called “Detecting Minimal Semantic Units and their Meanings” (DiMSUM) (Schneider et al., 2016),1 consists of sentences with multiword expression (MWE) and supersense annotations. The data combine and harmonize the STREUSLE 2.1 corpus of web reviews (Schneider and Smith, 2015)2 and Ritter and Lowlands Twitter datasets (Johannsen et al., 2014).3 Similar to prior work (Schneider and Smith, 2015), the annotation for MWEs extends the conventional BIO scheme (Ramshaw and Marcus, 1995) to include gappy MWEs with one level of nesting.4 Segmentations are represented using six tags; the lower-case variants indicate that an expression is within another MWE’s gap. • O and o: single word expression • B and b: the first word of a MWE 1 https://github.com/dimsum16/ dimsum-data/blob/1.5/README.md 2 http://www.cs.cmu.edu/˜ark/LexSem 3 https://github.com/coastalcph/ supersense-data-twitter 4 Unlike in Schneider and Smith (2015), there is no notion"
S16-1143,D14-1108,1,0.819268,"t in 170 potential labels: I, i, and each of B, b, O and o paired with one of the 41 supersenses and no supersense (2 + 4 × 42 = 170). Only 110 of these are attested in the training data, and these are the combinations our approach considers. There are 4,799 sentences in the training data. For each token, the dataset provides its offset in the sentence, lemma, POS tag, MWE tag, offset of parent, and supersense label (if applicable). The blind test set consists of 1,000 sentences from three sources: online reviews from the TrustPilot corpus (Hovy et al., 2015), tweets from the Tweebank corpus (Kong et al., 2014) and TED talk transcripts from the IWSLT MT evaluation campaigns, obtained from the WIT3 archive (Cettolo et al., 2012). The shared task has three data conditions: supervised closed, semi-supervised closed, and open. In the supervised closed condition, only the labeled data, the English WordNet lexicon, a provided Brown clustering (Brown et al., 1992) on the 21million-word Yelp Academic Dataset5 (Schneider et al., 2014), and any of the ARK Tweet NLP clusters6 are allowed. The semi-supervised closed condition adds the Yelp Academic Dataset to the resources. The open condition allows the use of"
S16-1143,W95-0107,0,0.644064,"eline, the structured perceptron used Task Description For completeness, we briefly review the shared task. The shared task training dataset, called “Detecting Minimal Semantic Units and their Meanings” (DiMSUM) (Schneider et al., 2016),1 consists of sentences with multiword expression (MWE) and supersense annotations. The data combine and harmonize the STREUSLE 2.1 corpus of web reviews (Schneider and Smith, 2015)2 and Ritter and Lowlands Twitter datasets (Johannsen et al., 2014).3 Similar to prior work (Schneider and Smith, 2015), the annotation for MWEs extends the conventional BIO scheme (Ramshaw and Marcus, 1995) to include gappy MWEs with one level of nesting.4 Segmentations are represented using six tags; the lower-case variants indicate that an expression is within another MWE’s gap. • O and o: single word expression • B and b: the first word of a MWE 1 https://github.com/dimsum16/ dimsum-data/blob/1.5/README.md 2 http://www.cs.cmu.edu/˜ark/LexSem 3 https://github.com/coastalcph/ supersense-data-twitter 4 Unlike in Schneider and Smith (2015), there is no notion of weak and strong MWEs. 931 Proceedings of SemEval-2016, pages 931–936, c San Diego, California, June 16-17, 2016. 2016 Association for Co"
S16-1143,N15-1177,1,0.924065,"ee Computer Science and Engineering University of Washington Seattle, WA 98195, USA {hosseini,nasmith,suinlee}@cs.washington.edu Abstract by Schneider and Smith (2015), and a conventional CRF (§4). For SemEval 2016 Task 10: Detecting Minimal Semantic Units and their Meanings, we submitted a CRF for the closed condition and a 2CRF (incompletely trained) for the open condition, achieving first and second place, respectively. We describe our entry to SemEval 2016 Task 10: Detecting Minimal Semantic Units and their Meanings. Our approach uses a discriminative first-order sequence model similar to Schneider and Smith (2015). The chief novelty in our approach is a factorization of the labels into multiword expression and supersense labels, and restricting first-order dependencies within these two parts. Our submitted models achieved first place in the closed competition (CRF) and second place in the open competition (2-CRF). 1 2 Introduction Schneider and Smith (2015) argued that the problems of segmenting a piece of text into minimal semantic units, and of labeling those units with semantic classes (e.g., supersenses), are intimately connected. We propose to use a double-chained conditional random field (which w"
S16-1143,Q14-1016,1,0.939121,"l (if applicable). The blind test set consists of 1,000 sentences from three sources: online reviews from the TrustPilot corpus (Hovy et al., 2015), tweets from the Tweebank corpus (Kong et al., 2014) and TED talk transcripts from the IWSLT MT evaluation campaigns, obtained from the WIT3 archive (Cettolo et al., 2012). The shared task has three data conditions: supervised closed, semi-supervised closed, and open. In the supervised closed condition, only the labeled data, the English WordNet lexicon, a provided Brown clustering (Brown et al., 1992) on the 21million-word Yelp Academic Dataset5 (Schneider et al., 2014), and any of the ARK Tweet NLP clusters6 are allowed. The semi-supervised closed condition adds the Yelp Academic Dataset to the resources. The open condition allows the use of any available resources. We have participated in the supervised closed and open conditions. The evaluation is based on F1 score for MWE identification, supersense labeling, and their combination.7 5 https://www.yelp.com/academic_dataset http://www.cs.cmu.edu/˜ark/TweetNLP/ #resources 7 For details, see http://dimsum16.github.io Models 3.1 Input Features For the open condition, we use all features introduced in Schneider"
S16-1143,S16-1084,0,0.0189922,"eatures and labels and between the labels of the consecutive words. The 2CRF models local dependencies between MWE and supersense sequences with two parallel chains of labels, restricting direct interaction between the two to local, single-word positions. Label constraints on tag bigrams ensure a globally consistent tagging. Our experiments show that 2-CRF outperforms a zero-order baseline, the structured perceptron used Task Description For completeness, we briefly review the shared task. The shared task training dataset, called “Detecting Minimal Semantic Units and their Meanings” (DiMSUM) (Schneider et al., 2016),1 consists of sentences with multiword expression (MWE) and supersense annotations. The data combine and harmonize the STREUSLE 2.1 corpus of web reviews (Schneider and Smith, 2015)2 and Ritter and Lowlands Twitter datasets (Johannsen et al., 2014).3 Similar to prior work (Schneider and Smith, 2015), the annotation for MWEs extends the conventional BIO scheme (Ramshaw and Marcus, 1995) to include gappy MWEs with one level of nesting.4 Segmentations are represented using six tags; the lower-case variants indicate that an expression is within another MWE’s gap. • O and o: single word expression"
S16-1143,C00-2137,0,0.23155,"Missing"
S16-1186,P13-2131,0,0.308975,"Missing"
S16-1186,W02-1001,0,0.141888,"ke predictions as follows: yˆ = arg max w · f (x, y 0 ) y 0 ∈Y(x) To train the model parameters w, a function of the training data is minimized with respect to w. This function is a sum of individual training examples’ losses L, plus a regularizer: X L(D; w) = L(xi , yi ; w) + λkwk2 (xi ,yi )∈D    C(xi ,yi ) }| { z     L(xi , yi ; w) = −  lim max w · f (xi , y) + α ·  min cost(yi , y 00 ) −cost(yi , y) 00 α→∞ y∈Y(xi ) + max y 0 ∈Y(xi )  y ∈Y(xi )  w · f (xi , y 0 ) + cost(yi , y 0 ) (1) Figure 1: Infinite ramp loss. Typical loss functions are the structured perceptron loss (Collins, 2002): L(xi , yi ; w) = −w · f (xi , yi ) + max w · f (xi , y) y∈Y(xi ) (2) and the structured SVM loss (Taskar et al., 2003; Tsochantaridis et al., 2004), which incorporates margin using a cost function:1 L(xi , yi ; w) = −w · f (xi , yi ) + max y∈Y(xi ) w · f (xi , y) + cost(yi , y)  (3) Both (2) and (3) are problematic if example i is unreachable, i.e., yi ∈ / Y(xi ), due to imperfect data or an imperfect definition of Y. In this case, the model is trying to learn an output it cannot produce. In some applications, the features f (xi , yi ) cannot even be computed for these examples. This proble"
S16-1186,P14-1134,1,0.736854,"ss function for structured prediction called infinite ramp, which is a generalization of the structured SVM to problems with unreachable training instances. 1 • Frame file lookup: for every word in the input sentence, if the lemma matches the name of a frame in the AMR frame files (with sense tag removed), we add the lemma concatenated with “-01” as a candidate concept fragment. • Lemma: for every word in the input sentence, we add the lemma of the word as a candidate concept fragment. Introduction Our entry to the SemEval 2016 Shared Task 8 is a set of improvements to the system presented in Flanigan et al. (2014). The improvements are: a novel training loss function for structured prediction, which we call “infinite ramp,” new sources for concepts, improved features, and improvements to the rule-based aligner in Flanigan et al. (2014). The overall architecture of the system and the decoding algorithms for concept identification and relation identification are unchanged from Flanigan et al. (2014), and we refer readers seeking a complete understanding of the system to that paper. 2 New Concept Fragment Sources and Features The concept identification stage relies on a function called clex in Section 3 o"
S16-1186,N12-1023,1,0.842131,"d to ramp loss (Collobert et al., 2006; Chapelle et al., 2009; Keshet and McAllester, 2011): L(xi , yi ; w) =  w · f (xi , y) − α · cost(yi , y) y∈Y(xi )  + max w · f (xi , y 0 ) + cost(yi , y 0 ) − max y 0 ∈Y(xi ) (5) The parameter α is often set to zero, and controls the “height” of the ramp, which is α + 1. Taking α → ∞ in Eq. 5 corresponds roughly to Eq. 1, hence the name “infinite ramp loss”. However, Eq. 1 also includes C(xi , yi ) term to make the limit well defined even when miny∈Y(xi ) cost(yi , y) 6= 0. Like infinite ramp loss, ramp loss also handles unreachable training examples (Gimpel and Smith, 2012), but we have found ramp loss to be more difficult to optimize than infinite ramp loss in practice due to local minima. Both loss functions are nonconvex. However, infinite ramp loss is convex if arg miny∈Y(xi ) cost(yi , y) is unique. the AMR annotation scheme between the production of the LDC2015E86 training data and the SemEval test set. During that time, there were changes to the concept senses and the concept frame files. Because the improvements in our parser were due to boosting recall in concept identification (and using the frame files to our advantage), our approach does not show as"
schneider-etal-2014-comprehensive,W11-0807,0,\N,Missing
schneider-etal-2014-comprehensive,S12-1021,0,\N,Missing
schneider-etal-2014-comprehensive,S12-1010,0,\N,Missing
schneider-etal-2014-comprehensive,M95-1005,0,\N,Missing
schneider-etal-2014-comprehensive,W06-1670,0,\N,Missing
schneider-etal-2014-comprehensive,H93-1061,0,\N,Missing
schneider-etal-2014-comprehensive,D11-1067,0,\N,Missing
schneider-etal-2014-comprehensive,J13-1009,0,\N,Missing
schneider-etal-2014-comprehensive,N10-1029,0,\N,Missing
schneider-etal-2014-comprehensive,Q14-1016,1,\N,Missing
schneider-etal-2014-comprehensive,P12-1022,0,\N,Missing
schneider-etal-2014-comprehensive,vincze-2012-light,0,\N,Missing
schneider-etal-2014-comprehensive,C12-1127,0,\N,Missing
smith-jahr-2000-cairo,J93-2003,0,\N,Missing
smith-jahr-2000-cairo,P97-1047,0,\N,Missing
W02-1013,P01-1030,0,0.0314924,"Missing"
W02-1013,1999.mtsummit-1.79,0,0.105611,"Missing"
W02-1013,J00-2004,0,0.0722033,"Missing"
W02-1013,W00-0504,0,0.0263396,"Missing"
W02-1013,P99-1068,0,0.0938247,"Missing"
W02-1013,W99-0626,0,0.179233,"Missing"
W02-1013,H94-1028,0,\N,Missing
W04-3207,J00-1004,0,0.0292845,"Missing"
W04-3207,J90-2002,0,0.0283338,"Missing"
W04-3207,J93-2003,0,0.00752304,"Missing"
W04-3207,2003.mtsummit-papers.6,0,0.0904424,"Missing"
W04-3207,P03-1012,0,0.0208107,"Missing"
W04-3207,J94-4004,0,0.0257474,"Missing"
W04-3207,P99-1059,0,0.0198494,"ncluding our bilingual parsing algorithm. We close by reviewing prior work in areas related to this paper (§5). 2 Bilingual parsing The joint model used by our bilingual parser is an instance of a stochastic bilingual multitext grammar (2MTG), formally defined by Melamed (2003). The 2MTG formalism generates two strings such that each syntactic constituent—including individual words—in one side of the bitext corresponds either to a constituent in the other side or to ∅. Melamed defines bilexicalized MTG (L2 MTG), which is a synchronous extension of bilexical grammars such as those described in Eisner and Satta (1999) and applies the latter’s algorithmic speedups to L2 MTG-parsing. Our formalism is not a precise fit to either unlexicalized MTG or L2 MTG since we posit lexical dependency structure only in one of the languages (English). The primary rationale for this is that we are dealing with only a small quantity of labeled data in language L and therefore do not expect to be able to accurately estimate its lexical affinities. Further, synchronous parsing is in practice computationally expensive, and eliminating lexicalization on one side reduces the run-time of the parser from O(n8 ) to O(n7 ). Our pars"
W04-3207,P03-2041,0,0.0405277,"Missing"
W04-3207,W02-1039,0,0.0365437,"Missing"
W04-3207,P03-1011,0,0.0250551,"Missing"
W04-3207,P02-1050,0,0.101719,"We evaluate our bilingual parser on the Penn Korean Treebank and against several baseline systems and show improvements parsing Korean with very limited labeled data. 1 Introduction Consider the problem of parsing a language L for which annotated resources like treebanks are scarce. Suppose we have a small amount of text data with syntactic annotations and a fairly large corpus of parallel text, for which the other language (e.g., English) is not resourceimpoverished. How might we exploit English parsers to improve syntactic analysis tools for this language? One idea (Yarowsky and Ngai, 2001; Hwa et al., 2002) is to project English analysis onto L data, “through” word-aligned parallel text. To do this, we might use an English parser to analyze the English side of the parallel text and a word-alignment algorithm to induce word correspondences. By positing a coupling of English syntax with L syntax, we can induce structure on the L side of the parallel text that is in some sense isomorphic to the English parse. We might take the projection idea a step farther. A statistical English parser can tell us much more than the hypothesized best parse. It can be used to find every parse admitted by a grammar,"
W04-3207,P99-1069,0,0.0460784,"Missing"
W04-3207,N03-1021,0,0.0210817,"f joint English-parse, L-parse, and word-alignment inference. In §3 we describe parameter estimation for each of the factored models, including novel applications of log-linear models to English dependency parsing and Korean morphological analysis. §4 presents Korean parsing results with various monolingual and bilingual algorithms, including our bilingual parsing algorithm. We close by reviewing prior work in areas related to this paper (§5). 2 Bilingual parsing The joint model used by our bilingual parser is an instance of a stochastic bilingual multitext grammar (2MTG), formally defined by Melamed (2003). The 2MTG formalism generates two strings such that each syntactic constituent—including individual words—in one side of the bitext corresponds either to a constituent in the other side or to ∅. Melamed defines bilexicalized MTG (L2 MTG), which is a synchronous extension of bilexical grammars such as those described in Eisner and Satta (1999) and applies the latter’s algorithmic speedups to L2 MTG-parsing. Our formalism is not a precise fit to either unlexicalized MTG or L2 MTG since we posit lexical dependency structure only in one of the languages (English). The primary rationale for this i"
W04-3207,J03-1002,0,0.00410479,"Missing"
W04-3207,P02-1035,0,0.0608816,"Missing"
W04-3207,W02-2207,0,0.0439253,"Missing"
W04-3207,C90-3045,0,0.0272605,"Missing"
W04-3207,N03-1031,0,0.0192336,"Missing"
W04-3207,J97-3002,0,0.2764,"Missing"
W04-3207,W00-1208,0,0.0458292,"Missing"
W04-3207,P01-1067,0,0.0767938,"Missing"
W04-3207,N01-1026,0,0.108151,"te parameter estimation. We evaluate our bilingual parser on the Penn Korean Treebank and against several baseline systems and show improvements parsing Korean with very limited labeled data. 1 Introduction Consider the problem of parsing a language L for which annotated resources like treebanks are scarce. Suppose we have a small amount of text data with syntactic annotations and a fairly large corpus of parallel text, for which the other language (e.g., English) is not resourceimpoverished. How might we exploit English parsers to improve syntactic analysis tools for this language? One idea (Yarowsky and Ngai, 2001; Hwa et al., 2002) is to project English analysis onto L data, “through” word-aligned parallel text. To do this, we might use an English parser to analyze the English side of the parallel text and a word-alignment algorithm to induce word correspondences. By positing a coupling of English syntax with L syntax, we can induce structure on the L side of the parallel text that is in some sense isomorphic to the English parse. We might take the projection idea a step farther. A statistical English parser can tell us much more than the hypothesized best parse. It can be used to find every parse adm"
W04-3207,J98-4003,0,\N,Missing
W04-3207,W90-0102,0,\N,Missing
W05-1504,W04-3224,0,0.014868,"cussion and Elliott Dr´abek and Markus Dreyer for insights on (respectively) Chinese and German parsing. They also thank an anonymous reviewer for suggesting the German experiments. 1 In a phrase-structure parse, if phrase X headed by word token x is a subconstituent of phrase Y headed by word token y 6= x, then x is said to depend on y. In a more powerful compositional formalism like LTAG or CCG, dependencies can be extracted from the derivation tree. 2 It has recently been questioned whether these “bilexical” features actually contribute much to parsing performance (Klein and Manning, 2003; Bikel, 2004), at least when one has only a million words of training. typical parsing features in that they cannot be determined from tree-local information. Though lengths are not usually considered, we will see that bilexical dynamic-programming parsing algorithms can easily consider them as they build the parse. Soft constraints. Like any other feature of trees, dependency lengths can be explicitly used as features in a probability model that chooses among trees. Such a model will tend to disfavor long dependencies (at least of some kinds), as these are empirically rare. In the first part of the paper,"
W05-1504,E99-1016,0,0.0192424,"ly yields a superset of the original context-free language. Subset and superset approximations of (weighted) CFLs by (weighted) regular languages, usually by preventing center-embedding, have been widely explored; Nederhof (2000) gives a thorough review. We limit all dependency lengths (not just centerembedding).27 Further, we derive weights from a modified treebank rather than by approximating the true weights. And though regular grammar approximations are useful for other purposes, we argue that for parsing it is more efficient to perform the approximation in the parser, not in the grammar. Brants (1999) described a parser that encoded the grammar as a set of cascaded Markov models. The decoder was applied iteratively, with each iteration transforming the best (or n-best) output from the previous one until only the root symbol remained. This is a greedy variant of CFG parsing where the grammar is in Backus-Naur form. Bertsch and Nederhof (1999) gave a linear-time recognition algorithm for the recognition of the regular closure of deterministic context-free languages. Our result is related; instead of a closure of deterministic CFLs, we deal in a closure of CFLs that are assumed (by the parser"
W05-1504,J98-2004,0,0.0703624,"d the best feasible parse, as constructed from the gold-standard parse by grafting: see §4.2). Runtime is measured as the number of items per word   27 Of course, this still allows right-branching or leftbranching to unbounded depth. 20 H H X y XX y X  , (i.e., @ @ , , , , @ @ ) built by the agenda parser. The “soft constraint” point marked with × represents the p(∆ |d, h, c)-augmented model from §3. Second, fast approximate parsing may play a role in more accurate parsing. It might be used to rapidly compute approximate outside-probability estimates to prioritize best-first search (e.g., Caraballo and Charniak, 1998). It might also be used to speed up the early iterations of training a weighted parsing model, which for modern training methods tends to require repeated parsing (either for the best parse, as by Taskar et al., 2004, or all parses, as by Miyao and Tsujii, 2002). Third, it would be useful to investigate algorithmic techniques and empirical benefits for limiting dependency length in more powerful grammar formalisms. Our runtime reduction from O(n3 ) → O(nk 2 ) for a length-k bound applies only to a “split” bilexical grammar.28 Various kinds of synchronous grammars, in particular, are becoming i"
W05-1504,P97-1003,0,0.541457,"ct parses exhibit a “shortdependency preference”: a word’s dependents tend to be close to it in the string.3 If the j th word of a sentence depends on the ith word, then |i−j |tends to be 3 In this paper, we consider only a crude notion of “closeness”: the number of intervening words. Other distance measures could be substituted or added (following the literature on heavy-shift and sentence comprehension), including the phonological, morphological, syntactic, or referential (given/new) complexity of the intervening material (Gibson, 1998). In parsing, the most relevant previous work is due to Collins (1997), who considered three binary features of the intervening material: did it contain (a) any word tokens at all, (b) any verbs, (c) any commas or colons? Note that (b) is effective because it measures the length of a dependency in terms of the number of alternative attachment sites that the dependent skipped over, a notion that could be generalized. Similarly, McDonald et al. (2005) separately considered each of the intervening POS tags. 30 Proceedings of the Ninth International Workshop on Parsing Technologies (IWPT), pages 30–41, c Vancouver, October 2005. 2005 Association for Computational Li"
W05-1504,H05-1036,1,0.259835,") will assemble the resulting fragments into a vine parse. In this case, ATTACH -R IGHT should also be restricted to h &gt; 0, to prevent duplicate derivations. The runtime is O(nk(k + t0 )tg 2 ), dominated by the ATTACH rules; the rules in (b) require only O(nktg 2 + ngtt0 ) time. Each algorithm is specified as a collection of deductive inference rules. Once one has derived all antecedent items above the horizontal line and any side conditions to the right of the line, one may derive the consequent item below the line. Weighted agenda-based deduction is handled in the usual way (Nederhof, 2003; Eisner et al., 2005). The probabilities governing the automaton Lw , 0 namely p(start at q), p(q −w→r |q), and p(stop | q), are respectively associated with the axiomatic T REE -R IGHT: q −w→r ∈ Rw @ @ 0:$ n accept F E ND -V INE: 0 @ @ 0:$ S TART-V INE: q ∈ init(R$ ) q q −→r ∈ R$ w i @ @ i:w rX y X @  @ q @ @ i h0 : w 0 G RAFT-V INE: h:w q h0 : w 0 j :x 0:$ H H S EAL -L EFT:   i @ @ F q ∈ final(Rw ) h:w F h : w h0 : w 0 C OMPLETE -R IGHT: q F h:w F   j :x @  @ i:w i:w q @  @ q T REE -L EFT: i:w 0:$ F h:w q h:w   q h:w r   h0 : w 0 i:w @  @ i−1 i q 0:$ @ @ q i h0 : w 0 T REE -S TART: i F C OMPLETE"
W05-1504,P99-1059,1,0.657514,"r dependency lengths. We confine ourselves (throughout the paper) to parsing part-ofspeech (POS) tag sequences. This allows us to ignore data sparseness, out-of-vocabulary, smoothing, and pruning issues, but it means that our accuracy measures are not state-of-the-art. Our techniques could be straightforwardly adapted to (bi)lexicalized parsers on actual word sequences, though not necessarily with the same success. 3.1 Grammar Formalism Throughout this paper we will use split bilexical grammars, or SBGs (Eisner, 2000), a notationally simpler variant of split head-automaton grammars, or SHAGs (Eisner and Satta, 1999). The formalism is context-free. We define here a probabilistic version,6 which we use for the baseline models in our experiments. They are only baselines because the SBG generative process does not take note of dependency length. An SBG is an tuple G = (Σ, $, L, R). Σ is an alphabet of words. (In our experiments, we parse only POS tag sequences, so Σ is actually an alphabet of tags.) $ 6∈ Σ is a distinguished root symbol; ¯ = Σ ∪ {$}. L and R are functions from Σ ¯ let Σ to probabilistic -free finite-state automata over Σ. ¯ the SBG specifies “left” and Thus, for each w ∈ Σ, “right” probabil"
W05-1504,P90-1034,0,0.0134826,"abstractly as the average number of items (i.e., @ @ , built per word. Model size is measured as the number of nonzero parameters.) either of which would allow the non-regular language {an bcn : 0 < n < ∞}. It does allow arbitrarily deep right- or left-branching structures. 4.1 Vine Grammars The tighter the bound on dependency length, the fewer parse trees we allow and the faster we can find them using the algorithm of Fig. 2a. If the bound is too tight to allow the correct parse of some sentence, we would still like to allow an accurate partial parse: a sequence of accurate parse fragments (Hindle, 1990; Abney, 1991; Appelt et al., 1993; Chen, 1995; Grefenstette, 1996). Furthermore, we would like to use the fact that some fragment sequences are presumably more likely than others. Our partial parses will look like the one in Fig. 1b. where 4 subtrees rather than 1 are dependent on $. This is easy to arrange in the SBG formalism. We merely need to construct our SBG so that the automaton R$ is now permitted to generate multiple children—the roots of parse fragments. This R$ is a probabilistic finite-state automaton that describes legal or likely root sequences in Σ∗ . In our experiments in this"
W05-1504,C90-3029,0,0.0480492,"ncouver, October 2005. 2005 Association for Computational Linguistics small. This implies that neither i nor j is modified by complex phrases that fall between i and j. In terms of phrase structure, it implies that the phrases modifying word i from a given side tend to be (1) few in number, (2) ordered so that the longer phrases fall farther from i, and (3) internally structured so that the bulk of each phrase falls on the side of j away from i. These principles can be blamed for several linguistic phenomena. (1) helps explain the “late closure” or “attach low” heuristic (e.g., Frazier, 1979; Hobbs and Bear, 1990): a modifier such as a PP is more likely to attach to the closest appropriate head. (2) helps account for heavy-shift: when an NP is long and complex, take NP out, put NP on the table, and give NP to Mary are likely to be rephrased as take out NP, put on the table NP, and give Mary NP. (3) explains certain non-canonical word orders: in English, a noun’s left modifier must become a right modifier if and only if it is right-heavy (a taller politician vs. a politician taller than all her rivals4 ), and a verb’s left modifier may extrapose its rightheavy portion (An aardvark walked in who had circ"
W05-1504,P04-1061,0,0.0172847,"w’s left dependents are conditionally independent of one another given w. In model C (the best), each automaton }−→}  has an extra state q0 that allows the first (closest) dependent to be chosen differently from the rest. Model B is a compromise:7 it is like model A, but each type w ∈ Σ may have an elevated or reduced probability of having no dependents at all. This is accomplished by using automata }−→}  as in model C, which allows the stopping probabilities p(STOP |q0 ) and p(STOP |q1 ) to differ, but tying the conditional dis7 It is equivalent to the “dependency model with valence” of Klein and Manning (2004). 32 3.3 3.4 Length-Sensitive Models Parsing Algorithm Fig. 2a gives a variant of Eisner and Satta’s (1999) SHAG parsing algorithm, adapted to SBGs, which are easier to understand.9 (We will modify this algorithm later in §4.) The algorithm obtains O(n3 ) runtime, despite the need to track the position of head words, by exploiting the conditional independence between a head’s left children and right children. It builds “half-constituents” denoted by @ @ (a head word together with some modifying phrases on the right, i.e., wα1 . . . αr ) and (a head word together with some modifying phrases on"
W05-1504,P05-1012,0,0.280182,"shift and sentence comprehension), including the phonological, morphological, syntactic, or referential (given/new) complexity of the intervening material (Gibson, 1998). In parsing, the most relevant previous work is due to Collins (1997), who considered three binary features of the intervening material: did it contain (a) any word tokens at all, (b) any verbs, (c) any commas or colons? Note that (b) is effective because it measures the length of a dependency in terms of the number of alternative attachment sites that the dependent skipped over, a notion that could be generalized. Similarly, McDonald et al. (2005) separately considered each of the intervening POS tags. 30 Proceedings of the Ninth International Workshop on Parsing Technologies (IWPT), pages 30–41, c Vancouver, October 2005. 2005 Association for Computational Linguistics small. This implies that neither i nor j is modified by complex phrases that fall between i and j. In terms of phrase structure, it implies that the phrases modifying word i from a given side tend to be (1) few in number, (2) ordered so that the longer phrases fall farther from i, and (3) internally structured so that the bulk of each phrase falls on the side of j away f"
W05-1504,J00-1003,0,0.0198261,"ecific bounds 0.6 0.55 0.5 k=1 0 20 15 20 40 60 runtime (items/word) 80 100 1 Chinese 0.9 0.8 0.7 15 0.6 2 0.5 0 20 40 20 3 Model C, baseline soft constraint single bound type-specific bounds 60 80 k=1 0.4 100 120 140 160 runtime (items/word) 1 German 0.95 0.9 0.85 0.8 Future Work F 6 1 0.95 F language. Our “vines” then let us concatenate several strings in this subset, which typically yields a superset of the original context-free language. Subset and superset approximations of (weighted) CFLs by (weighted) regular languages, usually by preventing center-embedding, have been widely explored; Nederhof (2000) gives a thorough review. We limit all dependency lengths (not just centerembedding).27 Further, we derive weights from a modified treebank rather than by approximating the true weights. And though regular grammar approximations are useful for other purposes, we argue that for parsing it is more efficient to perform the approximation in the parser, not in the grammar. Brants (1999) described a parser that encoded the grammar as a set of cascaded Markov models. The decoder was applied iteratively, with each iteration transforming the best (or n-best) output from the previous one until only the"
W05-1504,J03-1006,0,0.150199,"combines @ @ + , the annotations on @ @ and suffice to determine the dependency’s length ∆ = |h − h0 |, direction d = sign(h − h0 ), head word w, and child word w0 .12 So the additional cost of such a dependency, e.g. p(∆ |d, w, w0 ), can be included as the weight of an extra antecedent to the rule, and so included in  H  H the weight of the resulting or . To execute the inference rules in Fig. 2a, we use a prioritized agenda. Derived items such as  H  H @ @ , , , and are prioritized by their Viterbi-inside probabilities. This is known as uniform-cost search or shortest-hyperpath search (Nederhof, 2003). We halt as soon as a full parse (the accept item) pops from the agenda, since uniform-cost search (as a special case of the A∗ algorithm) guarantees this to be the maximumprobability parse. No other pruning is done. 11 Confusion-set parsing may be regarded as parsing a particular lattice with n states and ng arcs. The algorithm can be generalized to lattice parsing, in which case it has runtime O(m2 (n + t0 )t) for a lattice of n states and m arcs. Roughly, h : w is replaced by an arc, while i is replaced by a state and i − 1 is replaced by the same state. 12 For general lattice parsing, it"
W05-1504,2003.mtsummit-semit.11,0,0.0249507,"sing (either for the best parse, as by Taskar et al., 2004, or all parses, as by Miyao and Tsujii, 2002). Third, it would be useful to investigate algorithmic techniques and empirical benefits for limiting dependency length in more powerful grammar formalisms. Our runtime reduction from O(n3 ) → O(nk 2 ) for a length-k bound applies only to a “split” bilexical grammar.28 Various kinds of synchronous grammars, in particular, are becoming important in statistical machine translation. Their high runtime complexity might be reduced by limiting monolingual dependency length (for a related idea see Schafer and Yarowsky, 2003). Finally, consider the possibility of limiting dependency length during grammar induction. We reason that a learner might start with simple structures that focus on local relationships, and gradually relax this restriction to allow more complex models. 7 Conclusion We have described a novel reason for identifying headword-to-headword dependencies while parsing: to consider their length. We have demonstrated that simple bilexical parsers of English, Chinese, and German can exploit a “short-dependency preference.” Notably, soft constraints on dependency length can improve both speed and accurac"
W05-1504,W04-3201,0,0.0306737,"ed depth. 20 H H X y XX y X  , (i.e., @ @ , , , , @ @ ) built by the agenda parser. The “soft constraint” point marked with × represents the p(∆ |d, h, c)-augmented model from §3. Second, fast approximate parsing may play a role in more accurate parsing. It might be used to rapidly compute approximate outside-probability estimates to prioritize best-first search (e.g., Caraballo and Charniak, 1998). It might also be used to speed up the early iterations of training a weighted parsing model, which for modern training methods tends to require repeated parsing (either for the best parse, as by Taskar et al., 2004, or all parses, as by Miyao and Tsujii, 2002). Third, it would be useful to investigate algorithmic techniques and empirical benefits for limiting dependency length in more powerful grammar formalisms. Our runtime reduction from O(n3 ) → O(nk 2 ) for a length-k bound applies only to a “split” bilexical grammar.28 Various kinds of synchronous grammars, in particular, are becoming important in statistical machine translation. Their high runtime complexity might be reduced by limiting monolingual dependency length (for a related idea see Schafer and Yarowsky, 2003). Finally, consider the possibi"
W05-1504,P03-1054,0,\N,Missing
W05-1504,P95-1031,0,\N,Missing
W06-2929,P96-1023,0,0.0130127,"set D. In this work, the graph is constrained to be a projective tree rooted at $: each word except $ has a single parent, and there are no cycles or crossing dependencies. Using a simple dynamic program to find the minimum-error projective parse, we find that assuming projectivity need not harm accuracy very much (Tab. 1, col. 3). 3 Unlabeled Parsing The first component of our system is an unlabeled parser that, given a sentence, finds the U best unlabeled trees under a probabilistic model using a bottom-up dynamic programming algorithm.2 The model is a probabilistic head automaton grammar (Alshawi, 1996) that assumes conditional indepen1 We used words and fine tags in our parser and labeler, with coarse tags in one backoff model. Other features are used in reranking; we never used the given morphological features or the “projective” annotations offered in the training data. 2 The execution model we use is best-first, exhaustive search, as described in Eisner et al. (2004). All of our dynamic programming algorithms are implemented concisely in the Dyna language. 201 Proceedings of the 10th Conference on Computational Natural Language Learning (CoNLL-X), c pages 201–205, New York City, June 200"
W06-2929,P05-1022,0,0.0897925,"e system consists of three phases: a probabilistic vine parser (Eisner and N. Smith, 2005) that produces unlabeled dependency trees, a probabilistic relation-labeling model, and a discriminative minimum risk reranker (D. Smith and Eisner, 2006). The system is designed for fast training and decoding and for high precision. We describe sources of crosslingual error and ways to ameliorate them. We then provide a detailed error analysis of parses produced for sentences in German (much training data) and Arabic (little training data). 1 Introduction Standard state-of-the-art parsing systems (e.g., Charniak and Johnson, 2005) typically involve two passes. First, a parser produces a list of the most likely n parse trees under a generative, probabilistic model (usually some flavor of PCFG). A discriminative reranker then chooses among trees in this list by using an extended feature set (Collins, 2000). This paradigm has many advantages: PCFGs are fast to train, can be very robust, and perform better as more data is made available; and rerankers train quickly (compared to discriminative models), require few parameters, and permit arbitrary features. We describe such a system for dependency parsing. Our shared task en"
W06-2929,P99-1059,0,0.0617705,"Missing"
W06-2929,W05-1504,1,0.743677,"Missing"
W06-2929,1997.iwpt-1.10,0,0.159707,"Missing"
W06-2929,W05-1506,0,0.02202,"show oracle, 1-best, and reranked performance on the test set at different stages of the system. Boldface marks oracle performance that, given perfect downstream modules, would supercede the best system. Italics mark the few cases where the reranker increased error rate. Columns 8–10 show labeled accuracy; column 10 gives the final shared task evaluation scores. dence between the left yield and the right yield of a given head, given the head (Eisner, 1997).3 The best known parsing algorithm for such a model is O(n3 ) (Eisner and Satta, 1999). The U -best list is generated using Algorithm 3 of Huang and Chiang (2005). 3.1 Vine parsing (dependency length bounds) Following Eisner and N. Smith (2005), we also impose a bound on the string distance between every 3 To empirically test this assumption across languages, we measured the mutual information between different features of yleft (j) and yright (j), given xj . (Mutual information is a statistic that equals zero iff conditional independence holds.) A detailed discussion, while interesting, is omitted for space, but we highlight some of our findings. First, unsurprisingly, the splithead assumption appears to be less valid for languages with freer word ord"
W06-2929,H05-1066,0,0.181384,"Missing"
W06-2929,P06-2101,1,0.782705,"beled labeling model is a major source of error. 4 Our infrastructure provides a concise, interpreted language for expressing the models to be mixed, so large-scale combination and comparison are possible. 203 5 We tested first-order Markov models that conditioned on parent or MRSSS dependency labels. 5 Reranking We train a log-linear model combining many feature scores (see below), including the log-probabilities from the parser and labeler. Training minimizes the expected error under the model; we use deterministic annealing to smooth the error surface and avoid local minima (Rose, 1998; D. Smith and Eisner, 2006). We reserved 200 sentences in each language for training the reranker, plus 200 for choosing among rerankers trained on different feature sets and different (U × L)-best lists.6 Features Our reranking features predict tags, labels, lemmata, suffixes and other information given all or some of the following non-local conditioning context: bigrams and trigrams of tags or dependency labels; parent and grandparent dependency labels; subcategorization frames (in terms of tags or dependency labels); the occurrence of certain tags between head and child; surface features like the lemma7 and the 3-cha"
W06-2929,W03-2403,0,\N,Missing
W06-2929,dzeroski-etal-2006-towards,0,\N,Missing
W06-2929,W03-2405,0,\N,Missing
W06-2929,afonso-etal-2002-floresta,0,\N,Missing
W08-0302,W05-0909,0,0.0437062,"dering model, a lexicalized reordering model, and an n-gram language model, SRILM implementation (Stolcke, 2002) with modified KneserNey smoothing (Chen and Goodman, 1998). Minimum error-rate (MER) training (Och, 2003) was applied to obtain weights (λm in Equation 2) for these features. A recaser is trained on the target side of the parallel corpus using the script provided with Moses. All output is recased and detokenized prior to evaluation. Evaluation We evaluate translation output using three automatic evaluation measures: BLEU (Papineni et al., 2002), NIST (Doddington, 2002), and METEOR (Banerjee and Lavie, 2005, version 0.6).5 All measures used were the case-sensitive, corpuslevel versions. The version of BLEU used was that provided by NIST. Significance was tested using a paired bootstrap (Koehn, 2004) with 1000 samples (p < 0.05).6 4 http://www.statmt.org/wmt08 METEOR details: For English, we use exact matching, Porter stemming, and WordNet synonym matching. For German, we use exact matching and Porter stemming. These are the same settings that were used to evaluate systems for the WMT07 shared task. 6 Code implementing this test for these metrics can be freely downloaded at http://www.ark.cs.cmu."
W08-0302,W04-3224,0,0.00884977,"ional challenge (§4). Specific modifications to the standard training and evaluation paradigm are presented in §5. Experimental results are reported in §6. 2 Related Work Stroppa et al. (2007) added souce-side context features to a phrase-based translation system, including conditional probabilities of the same form that we use. They consider up to two words and/or POS tags of context on either side. Because of the aforementioned data sparseness problem, they use a decision3 An illustrative example is the debate over the use of bilexicalized grammar rules in statistical parsing (Gildea, 2001; Bikel, 2004). 10 tree classifier that implicitly smooths relative frequency estimates. The method improved over a standard phrase-based baseline trained on small amounts of data (< 50K sentence pairs) for Italian → English and Chinese → English. We explore a significantly larger space of context features, a smoothing method that more naturally fits into the widely used, errordriven linear model, and report a more comprehensive experimental evaluation (including feature comparison and scaling up to very large datasets). Recent research on the use of word-sense disambiguation in machine translation also poi"
W08-0302,D07-1090,0,0.0218763,"seto-English and English-to-German translation, and also describe our entry in the WMT-08 shared task based on this approach. 1 hˆ e, a ˆi = argmax score(e, a, f ) he,ai M X λm hm (e, a, f ) = argmax he,ai m=1 Introduction Machine translation (MT) by statistical modeling of bilingual phrases is one of the most successful approaches in the past few years. Phrase-based MT systems are straightforward to train from parallel corpora (Koehn et al., 2003) and, like the original IBM models (Brown et al., 1990), benefit from standard language models built on large monolingual, target-language corpora (Brants et al., 2007). Many of these systems perform well in competitive evaluations and scale well to large-data situations (1) (2) where a represents the segmentation of e and f into phrases and a correspondence between phrases, and each hm is a R-valued feature with learned weight λm . The translation is typically found using beam search (Koehn et al., 2003). The weights hλ1 , ..., λM i are typically learned to directly minimize a standard evaluation criterion on development data (e.g., the BLEU score; Papineni et al., (2002)) using numerical search (Och, 2003). Many features are used in phrase-based MT, but ne"
W08-0302,J90-2002,0,0.404921,"scales well to large data scenarios. We report significant improvements in automatic evaluation scores for Chineseto-English and English-to-German translation, and also describe our entry in the WMT-08 shared task based on this approach. 1 hˆ e, a ˆi = argmax score(e, a, f ) he,ai M X λm hm (e, a, f ) = argmax he,ai m=1 Introduction Machine translation (MT) by statistical modeling of bilingual phrases is one of the most successful approaches in the past few years. Phrase-based MT systems are straightforward to train from parallel corpora (Koehn et al., 2003) and, like the original IBM models (Brown et al., 1990), benefit from standard language models built on large monolingual, target-language corpora (Brants et al., 2007). Many of these systems perform well in competitive evaluations and scale well to large-data situations (1) (2) where a represents the segmentation of e and f into phrases and a correspondence between phrases, and each hm is a R-valued feature with learned weight λm . The translation is typically found using beam search (Koehn et al., 2003). The weights hλ1 , ..., λM i are typically learned to directly minimize a standard evaluation criterion on development data (e.g., the BLEU scor"
W08-0302,D07-1007,0,0.0344733,"luding feature comparison and scaling up to very large datasets). Recent research on the use of word-sense disambiguation in machine translation also points toward our approach. For example, Vickrey et al. (2005) built classifiers inspired by those used in word sense disambiguation to fill in blanks in a partially-completed translation. Gim´enez and M`arquez (2007) extended the work by considering phrases and moved to full translation instead of filling in target-side blanks. They trained an SVM for each source language phrase using local features of the sentences in which the phrases appear. Carpuat and Wu (2007) and Chan et al. (2007) embedded state-of-the-art word sense disambiguation modules into statistical MT systems, achieving performance improvements under several automatic measures for Chinese → English translation. Our approach is also reminiscent of examplebased machine translation (Nagao, 1984; Somers, 1999; Carl and Way, 2003), which has for many years emphasized use of the context in which source phrases appear when translating them. Indeed, like the example-based community, we do not begin with any set of assumptions about which kinds of phrases require additional disambiguation (cf. the"
W08-0302,P07-1005,0,0.103979,"ble tools (Koehn et al., 2007). We follow the approach of Koehn et al. (2003), in which we translate a source-language sentence f into the target-language sentence e ˆ that maximizes a linear combination of features and weights:1 We explore the augmentation of statistical machine translation models with features of the context of each phrase to be translated. This work extends several existing threads of research in statistical MT, including the use of context in example-based machine translation (Carl and Way, 2003) and the incorporation of word sense disambiguation into a translation model (Chan et al., 2007). The context features we consider use surrounding words and part-of-speech tags, local syntactic structure, and other properties of the source language sentence to help predict each phrase’s translation. Our approach requires very little computation beyond the standard phrase extraction algorithm and scales well to large data scenarios. We report significant improvements in automatic evaluation scores for Chineseto-English and English-to-German translation, and also describe our entry in the WMT-08 shared task based on this approach. 1 hˆ e, a ˆi = argmax score(e, a, f ) he,ai M X λm hm (e, a"
W08-0302,P05-1033,0,0.0104973,"9 0.2051 5.957 0.2782 0.2003 5.889 0.2720 Context 0.2039 5.941 0.2784 0.2088 6.036 0.2826 0.2016 5.956 0.2772 Table 5: English → German shared task system results using WMT-08 Europarl parallel data for training, dev06 for tuning, and three test sets, including the final 2008 test set. The row labeled “Context” uses the top-performing feature set {2 POS on left, 1 word on right}. Boldface marks scores that are significantly higher than the baseline. models, including the lexicalized reordering model and the lexical translation model in the Moses MT system, or hierarchical or syntactic models (Chiang, 2005). Additional linguistic analysis (e.g., morphological disambiguation, named entity recognition, semantic role labeling) can be used to define new context features. 8 Conclusion We have described a straightforward, scalable method for improving phrase translation models by modeling features of a phrase’s source-side context. Our method allows incorporation of features from any kind of source-side annotation and barely affects the decoding algorithm. Experiments show performance rivaling or exceeding strong, state-of-the-art baselines on standard translation tasks. Automatic feature selection ca"
W08-0302,W01-0521,0,0.00620467,"ose a computational challenge (§4). Specific modifications to the standard training and evaluation paradigm are presented in §5. Experimental results are reported in §6. 2 Related Work Stroppa et al. (2007) added souce-side context features to a phrase-based translation system, including conditional probabilities of the same form that we use. They consider up to two words and/or POS tags of context on either side. Because of the aforementioned data sparseness problem, they use a decision3 An illustrative example is the debate over the use of bilexicalized grammar rules in statistical parsing (Gildea, 2001; Bikel, 2004). 10 tree classifier that implicitly smooths relative frequency estimates. The method improved over a standard phrase-based baseline trained on small amounts of data (< 50K sentence pairs) for Italian → English and Chinese → English. We explore a significantly larger space of context features, a smoothing method that more naturally fits into the widely used, errordriven linear model, and report a more comprehensive experimental evaluation (including feature comparison and scaling up to very large datasets). Recent research on the use of word-sense disambiguation in machine transl"
W08-0302,W07-0719,0,0.18129,"Missing"
W08-0302,N03-1017,0,0.0123898,"on beyond the standard phrase extraction algorithm and scales well to large data scenarios. We report significant improvements in automatic evaluation scores for Chineseto-English and English-to-German translation, and also describe our entry in the WMT-08 shared task based on this approach. 1 hˆ e, a ˆi = argmax score(e, a, f ) he,ai M X λm hm (e, a, f ) = argmax he,ai m=1 Introduction Machine translation (MT) by statistical modeling of bilingual phrases is one of the most successful approaches in the past few years. Phrase-based MT systems are straightforward to train from parallel corpora (Koehn et al., 2003) and, like the original IBM models (Brown et al., 1990), benefit from standard language models built on large monolingual, target-language corpora (Brants et al., 2007). Many of these systems perform well in competitive evaluations and scale well to large-data situations (1) (2) where a represents the segmentation of e and f into phrases and a correspondence between phrases, and each hm is a R-valued feature with learned weight λm . The translation is typically found using beam search (Koehn et al., 2003). The weights hλ1 , ..., λM i are typically learned to directly minimize a standard evalua"
W08-0302,P07-2045,0,0.048035,"e baseline system. 6 Experiments In this section we present experimental results using our context-endowed phrase translation model with a variety of different context features, on Chinese → English, German → English, and English → GerContext features None Lexical Shallow Lexical + Shallow Syntactic Positional Chinese → English (UN) BLEU NIST METEOR 0.3715 7.918 0.6486 0.4030 8.367 0.6716 0.3807 7.981 0.6523 0.4030 8.403 0.6703 0.3823 7.992 0.6531 0.3775 7.958 0.6510 man translation tasks. Dataset details are given in Appendices A (Chinese) and B (German). Baseline We use the Moses MT system (Koehn et al., 2007) as a baseline and closely follow the example training procedure given for the WMT-07 and WMT-08 shared tasks.4 In particular, we perform word alignment in each direction using GIZA++ (Och and Ney, 2003), apply the “grow-diag-finaland” heuristic for symmetrization and use a maximum phrase length of 7. In addition to the two phrase translation conditionals p(e |f ) and p(f | e), we use lexical translation probabilities in each direction, a word penalty, a phrase penalty, a lengthbased reordering model, a lexicalized reordering model, and an n-gram language model, SRILM implementation (Stolcke,"
W08-0302,W04-3250,0,0.0547816,"Och, 2003) was applied to obtain weights (λm in Equation 2) for these features. A recaser is trained on the target side of the parallel corpus using the script provided with Moses. All output is recased and detokenized prior to evaluation. Evaluation We evaluate translation output using three automatic evaluation measures: BLEU (Papineni et al., 2002), NIST (Doddington, 2002), and METEOR (Banerjee and Lavie, 2005, version 0.6).5 All measures used were the case-sensitive, corpuslevel versions. The version of BLEU used was that provided by NIST. Significance was tested using a paired bootstrap (Koehn, 2004) with 1000 samples (p < 0.05).6 4 http://www.statmt.org/wmt08 METEOR details: For English, we use exact matching, Porter stemming, and WordNet synonym matching. For German, we use exact matching and Porter stemming. These are the same settings that were used to evaluate systems for the WMT07 shared task. 6 Code implementing this test for these metrics can be freely downloaded at http://www.ark.cs.cmu.edu/MT. 5 Table 2: Chinese → English experiments: training and testing on UN data. Boldface marks scores significantly higher than “None.” 13 Chinese → English Testing on UN Testing on News (NIST"
W08-0302,J06-4004,0,0.0340549,"Missing"
W08-0302,J03-1002,0,0.00179854,"an → English, and English → GerContext features None Lexical Shallow Lexical + Shallow Syntactic Positional Chinese → English (UN) BLEU NIST METEOR 0.3715 7.918 0.6486 0.4030 8.367 0.6716 0.3807 7.981 0.6523 0.4030 8.403 0.6703 0.3823 7.992 0.6531 0.3775 7.958 0.6510 man translation tasks. Dataset details are given in Appendices A (Chinese) and B (German). Baseline We use the Moses MT system (Koehn et al., 2007) as a baseline and closely follow the example training procedure given for the WMT-07 and WMT-08 shared tasks.4 In particular, we perform word alignment in each direction using GIZA++ (Och and Ney, 2003), apply the “grow-diag-finaland” heuristic for symmetrization and use a maximum phrase length of 7. In addition to the two phrase translation conditionals p(e |f ) and p(f | e), we use lexical translation probabilities in each direction, a word penalty, a phrase penalty, a lengthbased reordering model, a lexicalized reordering model, and an n-gram language model, SRILM implementation (Stolcke, 2002) with modified KneserNey smoothing (Chen and Goodman, 1998). Minimum error-rate (MER) training (Och, 2003) was applied to obtain weights (λm in Equation 2) for these features. A recaser is trained o"
W08-0302,P03-1021,0,0.337031,"ge monolingual, target-language corpora (Brants et al., 2007). Many of these systems perform well in competitive evaluations and scale well to large-data situations (1) (2) where a represents the segmentation of e and f into phrases and a correspondence between phrases, and each hm is a R-valued feature with learned weight λm . The translation is typically found using beam search (Koehn et al., 2003). The weights hλ1 , ..., λM i are typically learned to directly minimize a standard evaluation criterion on development data (e.g., the BLEU score; Papineni et al., (2002)) using numerical search (Och, 2003). Many features are used in phrase-based MT, but nearly ubiquitous are estimates of the conditional translation probabilities p(eji |f `k ) and p(f `k |eji ) for each phrase pair heji , f lk i in the candidate sentence pair.2 In this paper, we add and evaluate fea1 In the statistical MT literature, this is often referred to as a “log-linear model,” but since the score is normalized during neither parameter training nor decoding, and is never interpreted as a log-probability, it is essentially a linear combination of feature functions. Since many of the features are actually probabilities, this"
W08-0302,P02-1040,0,0.10804,"enefit from standard language models built on large monolingual, target-language corpora (Brants et al., 2007). Many of these systems perform well in competitive evaluations and scale well to large-data situations (1) (2) where a represents the segmentation of e and f into phrases and a correspondence between phrases, and each hm is a R-valued feature with learned weight λm . The translation is typically found using beam search (Koehn et al., 2003). The weights hλ1 , ..., λM i are typically learned to directly minimize a standard evaluation criterion on development data (e.g., the BLEU score; Papineni et al., (2002)) using numerical search (Och, 2003). Many features are used in phrase-based MT, but nearly ubiquitous are estimates of the conditional translation probabilities p(eji |f `k ) and p(f `k |eji ) for each phrase pair heji , f lk i in the candidate sentence pair.2 In this paper, we add and evaluate fea1 In the statistical MT literature, this is often referred to as a “log-linear model,” but since the score is normalized during neither parameter training nor decoding, and is never interpreted as a log-probability, it is essentially a linear combination of feature functions. Since many of the featu"
W08-0302,2007.tmi-papers.28,0,0.530763,"Missing"
W08-0302,H05-1097,0,0.130546,"mooths relative frequency estimates. The method improved over a standard phrase-based baseline trained on small amounts of data (< 50K sentence pairs) for Italian → English and Chinese → English. We explore a significantly larger space of context features, a smoothing method that more naturally fits into the widely used, errordriven linear model, and report a more comprehensive experimental evaluation (including feature comparison and scaling up to very large datasets). Recent research on the use of word-sense disambiguation in machine translation also points toward our approach. For example, Vickrey et al. (2005) built classifiers inspired by those used in word sense disambiguation to fill in blanks in a partially-completed translation. Gim´enez and M`arquez (2007) extended the work by considering phrases and moved to full translation instead of filling in target-side blanks. They trained an SVM for each source language phrase using local features of the sentences in which the phrases appear. Carpuat and Wu (2007) and Chan et al. (2007) embedded state-of-the-art word sense disambiguation modules into statistical MT systems, achieving performance improvements under several automatic measures for Chines"
W08-0302,D08-1076,0,\N,Missing
W09-1801,C08-1018,0,0.134658,"Missing"
W09-1801,P02-1057,0,0.377314,"Missing"
W09-1801,W03-0501,0,0.620052,"; Carbonell and Goldstein, 1998; Radev et al., 2000). Most of this work aims only to extract relevant sentences from the original documents and present them as the summary; this simplification of the problem yields scalable solutions. Some attention has been devoted by the NLP community to the related problem of sentence compression (Knight and Marcu, 2000): given a long sentence, how to maximally compress it into a grammatical sentence that still preserves all the relevant information? While sentence compression is a promising framework with applications, for example, in headline generation (Dorr et al., 2003; Jin, 2003), little work has been done to include it as a module in document summarization systems. Most existing approaches (with some exceptions, like the vine-growth model of Daum´e, 2006) use a two-stage architecture, either by first extracting a certain number of salient sentences and then feeding them into a sentence compressor, or by first compressing all sentences and extracting later. However, regardless of which operation is performed first—compression or extraction—two-step “pipeline” approaches may fail to find overall-optimal solutions; often the summaries are not better that the"
W09-1801,W02-0406,0,0.0137543,"Missing"
W09-1801,W03-1101,0,0.236222,"systems. Most existing approaches (with some exceptions, like the vine-growth model of Daum´e, 2006) use a two-stage architecture, either by first extracting a certain number of salient sentences and then feeding them into a sentence compressor, or by first compressing all sentences and extracting later. However, regardless of which operation is performed first—compression or extraction—two-step “pipeline” approaches may fail to find overall-optimal solutions; often the summaries are not better that the ones produced by extractive summarization. On the other hand, a pilot study carried out by Lin (2003) suggests that summarization systems that perform sentence compression have the potential to beat pure extractive systems if they model cross-sentence effects. In this work, we address this issue by merging the tasks of sentence extraction and sentence compression into a global optimization problem. A careful design of the objective function encourages “sparse solutions,” i.e., solutions that involve only a small number of sentences whose compressions are to be included in the summary. Our contributions are: • We cast joint sentence extraction and compression as an integer linear program (ILP)"
W09-1801,H05-1066,0,0.023209,"Missing"
W09-1801,E06-1038,0,0.568619,"a short, informative and grammatical sentence. Such a sentence compressor is given a sentence t , hw1 , . . . , wN i as input and outputs a subsequence of length L, c , hwj1 , . . . , wjL i, with 1 ≤ j1 &lt; . . . &lt; jL ≤ N . We may represent this output as a binary vector s of length N , where sj = 1 iff word wj is included in the compression. Note that there are O(2N ) possible subsequences. 3.1 2000; Daum´e and Marcu, 2002), heuristic methods that parse the sentence and then trim constituents according to linguistic criteria (Dorr et al., 2003; Zajic et al., 2006), a pure discriminative model (McDonald, 2006), and an ILP formulation (Clarke and Lapata, 2008). We next give an overview of the two latter approaches. McDonald (2006) uses the outputs of two parsers (a phrase-based and a dependency parser) as features in a discriminative model that decomposes over pairs of consecutive words. Formally, given a sentence t = hw1 , . . . , wN i, the score of a compression c = hwj1 , . . . , wjL i decomposes as: αi θ &gt; f (t, 0, i) + βi θ &gt; f (t, i, n + 1) + i=1 N −1 X N X γij θ &gt; f (t, i, j), (10) i=1 j=i+1 where αi , βi , and γij are additional binary variables with the following meanings: • αi = 1 iff word"
W09-1801,W02-0401,0,0.060482,"oach for document summarization that jointly performs sentence extraction and compression by solving an integer linear program. We report favorable experimental results on newswire data. 1 Introduction Automatic text summarization dates back to the 1950s and 1960s (Luhn, 1958; Baxendale, 1958; Edmundson, 1969). Today, the proliferation of digital information makes research on summarization technologies more important than ever before. In the last two decades, machine learning techniques have been employed in extractive summarization of single documents (Kupiec et al., 1995; Aone et al., 1999; Osborne, 2002) and multiple documents (Radev and McKeown, 1998; Carbonell and Goldstein, 1998; Radev et al., 2000). Most of this work aims only to extract relevant sentences from the original documents and present them as the summary; this simplification of the problem yields scalable solutions. Some attention has been devoted by the NLP community to the related problem of sentence compression (Knight and Marcu, 2000): given a long sentence, how to maximally compress it into a grammatical sentence that still preserves all the relevant information? While sentence compression is a promising framework with app"
W09-1801,J98-3005,0,0.0563716,"intly performs sentence extraction and compression by solving an integer linear program. We report favorable experimental results on newswire data. 1 Introduction Automatic text summarization dates back to the 1950s and 1960s (Luhn, 1958; Baxendale, 1958; Edmundson, 1969). Today, the proliferation of digital information makes research on summarization technologies more important than ever before. In the last two decades, machine learning techniques have been employed in extractive summarization of single documents (Kupiec et al., 1995; Aone et al., 1999; Osborne, 2002) and multiple documents (Radev and McKeown, 1998; Carbonell and Goldstein, 1998; Radev et al., 2000). Most of this work aims only to extract relevant sentences from the original documents and present them as the summary; this simplification of the problem yields scalable solutions. Some attention has been devoted by the NLP community to the related problem of sentence compression (Knight and Marcu, 2000): given a long sentence, how to maximally compress it into a grammatical sentence that still preserves all the relevant information? While sentence compression is a promising framework with applications, for example, in headline generation ("
W09-1801,W00-0403,0,0.0196396,"lving an integer linear program. We report favorable experimental results on newswire data. 1 Introduction Automatic text summarization dates back to the 1950s and 1960s (Luhn, 1958; Baxendale, 1958; Edmundson, 1969). Today, the proliferation of digital information makes research on summarization technologies more important than ever before. In the last two decades, machine learning techniques have been employed in extractive summarization of single documents (Kupiec et al., 1995; Aone et al., 1999; Osborne, 2002) and multiple documents (Radev and McKeown, 1998; Carbonell and Goldstein, 1998; Radev et al., 2000). Most of this work aims only to extract relevant sentences from the original documents and present them as the summary; this simplification of the problem yields scalable solutions. Some attention has been devoted by the NLP community to the related problem of sentence compression (Knight and Marcu, 2000): given a long sentence, how to maximally compress it into a grammatical sentence that still preserves all the relevant information? While sentence compression is a promising framework with applications, for example, in headline generation (Dorr et al., 2003; Jin, 2003), little work has been"
W09-1801,J03-4003,0,\N,Missing
W10-0705,D09-1030,0,0.108104,"Missing"
W10-0705,N10-1086,1,0.798635,"ific ratings can be used to train statistical rankers to improve systems’ final output, or to evaluate technologies that generate natural language. We discuss the question rating scheme we developed, assess the quality of the ratings that we gathered through Amazon Mechanical Turk, and show evidence that these ratings can be used to improve question generation. 1 Introduction This paper discusses the use of Amazon Mechanical Turk (MTurk) to rate computer-generated reading comprehension questions about Wikipedia articles. We have developed a question generation system (Heilman and Smith, 2009; Heilman and Smith, 2010) that uses the overgenerate-and-rank paradigm (Langkilde and Knight, 1998). In the the overgenerate-and-rank approach, many systemgenerated outputs are ranked in order to select higher quality outputs. While the approach has had considerable success in natural language generation (Langkilde and Knight, 1998; Walker et al., 2001), it often requires human labels on system output for the purpose of learning to rank. We employ MTurk to reduce the time and cost of acquiring these labels. For many problems, large labeled datasets do not exist. One alternative is to build rule-based systems, but it i"
W10-0705,P98-1116,0,0.0168434,"s’ final output, or to evaluate technologies that generate natural language. We discuss the question rating scheme we developed, assess the quality of the ratings that we gathered through Amazon Mechanical Turk, and show evidence that these ratings can be used to improve question generation. 1 Introduction This paper discusses the use of Amazon Mechanical Turk (MTurk) to rate computer-generated reading comprehension questions about Wikipedia articles. We have developed a question generation system (Heilman and Smith, 2009; Heilman and Smith, 2010) that uses the overgenerate-and-rank paradigm (Langkilde and Knight, 1998). In the the overgenerate-and-rank approach, many systemgenerated outputs are ranked in order to select higher quality outputs. While the approach has had considerable success in natural language generation (Langkilde and Knight, 1998; Walker et al., 2001), it often requires human labels on system output for the purpose of learning to rank. We employ MTurk to reduce the time and cost of acquiring these labels. For many problems, large labeled datasets do not exist. One alternative is to build rule-based systems, but it is often difficult and time-consuming to accurately encode relevant linguis"
W10-0705,W04-1013,0,0.0348758,"MTurk ratings to train a learned ranking component, we could also use MTurk ratings to evaluate the final top-ranked output of our system. More generally, MTurk can be a useful evaluation tool for systems that output natural language (e.g., systems for natural language generation, summarization, translation). For example, Callison-Burch (2009) used MTurk to evaluate machine translations. MTurk facilitates the efficient measurement and understanding of errors made by such technologies, and could be used to complement automatic evaluation metrics such as BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004). It is true that, for our task, MTurk workers annotate computer-generated rather than humangenerated natural language. Thus, the data will not be as generally useful as other types of annotations, such as parse trees, which could be used to build general purpose syntactic parsers. However, for the reasons described above, we believe the use of MTurk to rate computer-generated output can be useful for the training, development, and evaluation of language technologies. The remainder of the paper is organized as follows: §2 and §3 briefly describe the question generation system and corpora used"
W10-0705,P02-1040,0,0.0877714,", 1995); such In addition to using MTurk ratings to train a learned ranking component, we could also use MTurk ratings to evaluate the final top-ranked output of our system. More generally, MTurk can be a useful evaluation tool for systems that output natural language (e.g., systems for natural language generation, summarization, translation). For example, Callison-Burch (2009) used MTurk to evaluate machine translations. MTurk facilitates the efficient measurement and understanding of errors made by such technologies, and could be used to complement automatic evaluation metrics such as BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004). It is true that, for our task, MTurk workers annotate computer-generated rather than humangenerated natural language. Thus, the data will not be as generally useful as other types of annotations, such as parse trees, which could be used to build general purpose syntactic parsers. However, for the reasons described above, we believe the use of MTurk to rate computer-generated output can be useful for the training, development, and evaluation of language technologies. The remainder of the paper is organized as follows: §2 and §3 briefly describe the question generation sy"
W10-0705,D08-1027,0,0.0948387,"Missing"
W10-0705,N01-1003,0,0.0164312,"improve question generation. 1 Introduction This paper discusses the use of Amazon Mechanical Turk (MTurk) to rate computer-generated reading comprehension questions about Wikipedia articles. We have developed a question generation system (Heilman and Smith, 2009; Heilman and Smith, 2010) that uses the overgenerate-and-rank paradigm (Langkilde and Knight, 1998). In the the overgenerate-and-rank approach, many systemgenerated outputs are ranked in order to select higher quality outputs. While the approach has had considerable success in natural language generation (Langkilde and Knight, 1998; Walker et al., 2001), it often requires human labels on system output for the purpose of learning to rank. We employ MTurk to reduce the time and cost of acquiring these labels. For many problems, large labeled datasets do not exist. One alternative is to build rule-based systems, but it is often difficult and time-consuming to accurately encode relevant linguistic knowledge in rules. Another alternative, unsupervised or semisupervised learning, usually requires clever formulations of bias that guide the learning process (Carroll and Charniak, 1992; Yarowsky, 1995); such In addition to using MTurk ratings to trai"
W10-0705,P95-1026,0,0.0131608,"age generation (Langkilde and Knight, 1998; Walker et al., 2001), it often requires human labels on system output for the purpose of learning to rank. We employ MTurk to reduce the time and cost of acquiring these labels. For many problems, large labeled datasets do not exist. One alternative is to build rule-based systems, but it is often difficult and time-consuming to accurately encode relevant linguistic knowledge in rules. Another alternative, unsupervised or semisupervised learning, usually requires clever formulations of bias that guide the learning process (Carroll and Charniak, 1992; Yarowsky, 1995); such In addition to using MTurk ratings to train a learned ranking component, we could also use MTurk ratings to evaluate the final top-ranked output of our system. More generally, MTurk can be a useful evaluation tool for systems that output natural language (e.g., systems for natural language generation, summarization, translation). For example, Callison-Burch (2009) used MTurk to evaluate machine translations. MTurk facilitates the efficient measurement and understanding of errors made by such technologies, and could be used to complement automatic evaluation metrics such as BLEU (Papinen"
W10-0705,C98-1112,0,\N,Missing
W10-0723,J93-1003,0,0.105507,"entences this way so that the collection has variety, while including enough examples for individual categories. Our goal was to gather at least 1,000 annotated sentences; ultimately we collected 1,041. The categories are as follows. “Sticky” partisan bigrams. One likely indicator of bias is the use of terms that are particular to one side or the other in a debate (Monroe et al., 2008). In order to identify such terms, we independently created two lists of “sticky” (i.e., strongly associated) bigrams in liberal and conservative subcorpora, measuring association using the log-likelihood ratio (Dunning, 1993) and omitting bigrams containing stopwords.11 We identified a bigram as “liberal” if it was among the top 1,000 bigrams from the liberal blogs, as measured by strength of association, and was also not among the top 1,000 bigrams on the conservative side. The reverse definition yielded the “conservative” bigrams. The resulting liberal list contained 495 bigrams, and the conservative list contained 539. We then manually filtered cases that were clearly remnant HTML tags and other markup, arriving at lists of 433 and 535, respectively. Table 1 shows the strongest weighted bigrams. As an example,"
W10-0723,N09-1057,1,0.839043,"egories of words from Pennebaker’s LIWC dictionary: Negative Emotion, Positive Emotion, Causation, and Anger.12 The following is one example of a biased sentence in our dataset that matched these lexicons, in this case the Anger category; the match is in bold. A bunch of ugly facts are nailing the biggest scare story in history. The five most frequent matches in the corpus for each category are as follows.13 Negative Emotion: war attack* problem* numb* argu* Positive Emotion: like well good party* secur* Causation: how because lead* make why Anger: war attack* argu* fight* threat* Kill verbs. Greene and Resnik (2009) discuss the relevance of syntactic structure to the perception of sentiment. For example, their psycholinguistic experiments would predict that when comparing Millions of people starved under Stalin (inchoative) with Stalin starved millions of people (transitive), the latter will be perceived as more negative toward Stalin, because the transitive syntactic frame tends to be connected with semantic properties such as intended action by the subject and change of state in the object. “Kill verbs” provide particularly strong examples of such phenomena, because they exhibit a large set of semantic"
W10-0723,P99-1032,0,0.110884,"Missing"
W10-0723,N07-1033,0,0.0141441,"a sentence showed bias, and if so, in which political direction and through which word tokens. We also asked annotators questions about their own political views. We conducted a preliminary analysis of the data, exploring how different groups perceive bias in different blogs, and showing some lexical indicators strongly associated with perceived bias. 1 • The nature of the bias (very liberal, moderately liberal, moderately conservative, very conservative, biased but not sure which direction); and • Which words in the sentence give away the author’s bias, similar to “rationale” annotations in Zaidan et al. (2007). For example, a participant might identify a moderate liberal bias in this sentence, Introduction Bias and framing are central topics in the study of communications, media, and political discourse (Scheufele, 1999; Entman, 2007), but they have received relatively little attention in computational linguistics. What are the linguistic indicators of bias? Are there lexical, syntactic, topical, or other clues that can be computationally modeled and automatically detected? Here we use Amazon Mechanical Turk (MTurk) to engage in a systematic, empirical study of linguistic indicators of bias in the"
W10-2925,J93-2004,0,0.0383684,"Missing"
W10-2925,J94-2001,0,0.150122,"Missing"
W10-2925,W03-0301,0,0.0211998,"Missing"
W10-2925,W03-0419,0,0.0128423,"Missing"
W10-2925,J93-2003,0,0.0351098,"rgence criterion is left unspecified here. Langford et al. (2009) present convergence rates via regret bounds, which are linear in D. The convergence rate √ of asynchronous stochastic gradient descent is O( T D), where T is the total number of updates made. In addition to the situation in which function components are chosen uniformly at random, Langford et al. provide results for several other scenarios, including the case in which an adversary supplies the training examples in whatever ordering he chooses. of training examples. For example, for simple word alignment models like IBM Model 1 (Brown et al., 1993), only parameters corresponding to words appearing in the particular subsample of sentence pairs are needed. The error introduced when making asynchronous updates should intuitively be less severe in these cases, where different mini-batches use small and mostly nonoverlapping subsets of θ. 5.1 Below we experiment with optimization of both convex and non-convex functions, using fixed step sizes and decreasing step size formulas, and consider several values of D. Even when exploring regions of the experimental space that are not yet supported by theoretical results, asynchronous algorithms perf"
W10-2925,D08-1024,0,0.0252101,"essors in systems without shared memory). Nonetheless, this simple approach is widely used in practice; approaches in which the gradient computation is distributed via MapReduce have recently been described in machine learning and NLP (Chu et al., 2006; Dyer et al., 2008; Wolfe et al., 2008). Mann et al. (2009) compare this framework to one in which each processor maintains a separate parameter vector which is updated independently of the others. At the end of learning, the parameter vectors are averaged or a vote is taken during prediction. A similar parameter-averaging approach was taken by Chiang et al. (2008) when parallelizing MIRA (Crammer et al., 2006). In this paper, we restrict our attention to distributed frameworks which maintain and update a single copy of the parameters θ. The use of multiple parameter vectors is essentially orthogonal to the framework we discuss here and we leave the integration of the two ideas for future exploration. 4 (4) 5 Distributed Asynchronous Mini-Batch Optimization An asynchronous framework may use multiple processors more efficiently and minimize idle time (Nedic et al., 2001; Langford et al., 2009). In this setting, the master sends θ and a mini-batch Mk to e"
W10-2925,W08-0333,0,0.0135295,"-batch is shorter than the time for a full batch, mini-batch algorithms make far more updates and some processor cycles will be wasted in computing each one. Also, more mini-batches imply that more time will be lost due to per-mini-batch overhead (e.g., waiting for synchronization locks in sharedmemory systems, or sending data and θ to the processors in systems without shared memory). Nonetheless, this simple approach is widely used in practice; approaches in which the gradient computation is distributed via MapReduce have recently been described in machine learning and NLP (Chu et al., 2006; Dyer et al., 2008; Wolfe et al., 2008). Mann et al. (2009) compare this framework to one in which each processor maintains a separate parameter vector which is updated independently of the others. At the end of learning, the parameter vectors are averaged or a vote is taken during prediction. A similar parameter-averaging approach was taken by Chiang et al. (2008) when parallelizing MIRA (Crammer et al., 2006). In this paper, we restrict our attention to distributed frameworks which maintain and update a single copy of the parameters θ. The use of multiple parameter vectors is essentially orthogonal to the fra"
W10-2925,P08-1109,0,0.312135,"nguage Technologies Institute Carnegie Mellon Univeristy Pittsburgh, PA 15213, USA {kgimpel,dipanjan,nasmith}@cs.cmu.edu Abstract Online algorithms offer fast convergence rates and scalability to large datasets, but distributed computing is a more natural fit for algorithms that require a lot of computation—e.g., processing a large batch of training examples—to be done between updates. Typically, distributed online learning has been done in a synchronous setting, meaning that a mini-batch of data is divided among multiple CPUs, and the model is updated when they have all completed processing (Finkel et al., 2008). Each mini-batch is processed only after the previous one has completed. Synchronous frameworks are appealing in that they simulate the same algorithms that work on a single processor, but they have the drawback that the benefits of parallelism are only obtainable within one mini-batch iteration. Moreover, empirical evaluations suggest that online methods only converge faster than batch algorithms when using very small mini-batches (Liang and Klein, 2009). In this case, synchronous parallelization will not offer much benefit. In this paper, we focus our attention on asynchronous algorithms th"
W10-2925,D07-1031,0,0.0117089,"s,9 resulting in a total of 10 runs for each algorithm. We ran each for six hours, saving models every five minutes. After training completed, using each model we decoded the entire training data using posterior decoding and computed the log-likelihood. The results for 5 initial models and two example orderings are shown in Figure 6. We evaluated tagging performance using many-to-1 accuracy, which is obtained by mapping the HMM states to gold standard POS tags so as to maximize accuracy, where multiple states can be mapped to the same tag. This is the metric used by Liang and Klein (2009) and Johnson (2007), who report figures comparable to ours. The asynchronous algorithm converges much faster than the single-node algorithm, allowing a tagger to be trained from the Penn Treebank in less than two hours using a single machine. Furthermore, the 4-processor synchronous algorithm improves only marginally EM converges faster as m decreases. With large mini-batches, load-balancing becomes less important as there will be less variation in per-minibatch observed runtime. These results suggest that asynchronous mini-batch algorithms will be most useful for learning problems in which small minibatches wor"
W10-2925,D07-1033,0,0.0101298,"ingle−processor 86 0 2 4 6 Wall clock time (hours) 8 10 Figure 2: NER: (Top) Synchronous optimization improves very little when moving from 2 to 4 processors due to the need for load-balancing, leaving some processors idle for stretches of time. (Bottom) Asynchronous optimization does not require load balancing and therefore improves when moving from 2 to 4 processors because each processor is in nearconstant use. All curves use a mini-batch size of 4 and the “Single-processor” curve is identical in the two plots. Named Entity Recognition Our NER CRF used a standard set of features, following Kazama and Torisawa (2007), along with token shape features like those in Collins (2002) and simple gazetteer features; a feature was included if and only it occurred at least once in training data (total 1.3M). We used a diagonal Gaussian prior with a variance of 1.0 for each weight. We compared SGD on a single processor to distributed synchronous SGD and distributed asynchronous SGD. For all experiments, we used a fixed step size of 0.01 and chose each training example for each mini-batch uniformly at random from the full data set.3 We report performance by 3 88 plotting test-set accuracy against wall-time over 12 ho"
W10-2925,N03-1017,0,0.00503619,"k and NER is the size of the mini-batch used, so we experimented with several values for the mini-batch size m. Figure 5 shows the results. As m decreases, a larger fraction of time is spent updating parameters; this slows observed convergence time even when using the sparse update rule. It can be seen that, though synchronous and asynchronous stepwise EM converge at the same rate with a large minibatch size (m = 10000), asynchronous stepwise We trained IBM Model 1 in both directions. To align test data, we symmetrized both directional Viterbi alignments using the “grow-diag-final” heuristic (Koehn et al., 2003). We evaluated our models using alignment error rate (AER). Experiments on a Single Machine We followed Liang and Klein (2009) in using synchronous (mini-batch) stepwise EM on a single processor for this task. We used the same learning rate formula (η (t) = (t + 2)−q , with 0.5 &lt; q ≤ 1). We also used asynchronous stepwise EM by using the same update rule, but gathered sufficient statistics on 4 processors of a single machine in parallel, analogous to our asynchronous method from §5. Whenever a processor was done gathering the expected counts for its mini-batch, it updated the sufficient statis"
W10-2925,N09-1069,0,0.450305,"setting, meaning that a mini-batch of data is divided among multiple CPUs, and the model is updated when they have all completed processing (Finkel et al., 2008). Each mini-batch is processed only after the previous one has completed. Synchronous frameworks are appealing in that they simulate the same algorithms that work on a single processor, but they have the drawback that the benefits of parallelism are only obtainable within one mini-batch iteration. Moreover, empirical evaluations suggest that online methods only converge faster than batch algorithms when using very small mini-batches (Liang and Klein, 2009). In this case, synchronous parallelization will not offer much benefit. In this paper, we focus our attention on asynchronous algorithms that generalize those presented by Nedic et al. (2001) and Langford et al. (2009). In these algorithms, multiple mini-batches are processed simultaneously, each using potentially different and typically stale parameters. The key advantage of an asynchronous framework is that it allows processors to remain in near-constant use, preventing them from wasting cycles waiting for other processors to complete their portion of the current mini-batch. In this way, as"
W10-2925,W03-1013,0,\N,Missing
W10-2925,P02-1034,0,\N,Missing
W10-2925,P02-1062,0,\N,Missing
W11-1515,cieri-etal-2004-fisher,0,0.0966874,"construed along a variety of dimensions, including age, gender, socioeconomic status and political affiliation. A person is a member of a multiplicity of communities, and thus the person’s identity and language are influenced by many factors. In this paper we focus on the relationship between age and language use. Recently, machine learning Our first contribution to this literature is an investigation of age prediction using a multi-corpus approach. We present results and analysis across three very different corpora: a blog corpus (Schler et al., 2006), a transcribed telephone speech corpus (Cieri et al., 2004) and posts from an online forum on breast cancer. By using the domain adaptation approach of Daum´e III (2007), we train a model on all these corpora together and separate the global features from corpus-specific features that are associated with age. A second contribution is the investigation of age prediction with age modeled as a continuous variable rather than as a categorical variable. Most prior research on age prediction has framed this as a two-class or three-class classification problem (e.g., Schler et al., 2006 and Garera and Yarowsky, 2009). In our work, modeling age as a continuou"
W11-1515,P07-1033,0,0.0151607,"Missing"
W11-1515,P07-1104,0,0.0431598,"model: yˆ = β0 + x&gt; β where β0 and β are the parameters to estimate. Usually, the parameters are learned by minimizing the sum of squared errors. In order to strive for a model with high explanatory value, we use a linear regression model with Lasso (also called L1 ) regularization (Tibshirani, 1996). This minimizes the sum of P squared errors, but in addition adds a penalty term λ m j=1 |βj |. λ is a constant and can be found by optimizing over the development data. As a result, this method delivers sparse models. We use OWLQN to optimize the regularized empirical risk (Andrew and Gao, 2007; Gao et al., 2007). We evaluate the models by reporting the correlation and mean absolute error (MAE). Besides experimenting with the joint model, we are also interested in the performance using only the discovered global features. This can be achieved by applying the weights for the global features directly as learned by the joint model, or retraining the model on the individual datasets using only the global features. In summary, we have the following models: 4.2 4.4 Joint model To discover which features are important across datasets and which are corpus-specific, we train a model on the data of all corpora"
W11-1515,P09-1080,0,0.474427,"., 2006), a transcribed telephone speech corpus (Cieri et al., 2004) and posts from an online forum on breast cancer. By using the domain adaptation approach of Daum´e III (2007), we train a model on all these corpora together and separate the global features from corpus-specific features that are associated with age. A second contribution is the investigation of age prediction with age modeled as a continuous variable rather than as a categorical variable. Most prior research on age prediction has framed this as a two-class or three-class classification problem (e.g., Schler et al., 2006 and Garera and Yarowsky, 2009). In our work, modeling age as a continuous variable is interesting not only as a more realistic representation of age, but also for practical benefits of joint modeling of age across corpora since the bound115 Proceedings of the 5th ACL-HLT Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities, pages 115–123, c Portland, OR, USA, 24 June 2011. 2011 Association for Computational Linguistics aries for discretizing age into a categorical variable in prior work have been chosen heuristically and in a corpus-dependent way, making it hard to compare performance acro"
W11-1515,N03-1033,0,0.0106484,"rained on the three corpora individually. • JOINT: Model trained on all three corpora with features represented as in Daum´e III (2007). • JOINT-Global: Using the learned JOINT model but only keeping the global features. • JOINT-Global-Retrained: Using the discovered global features by the JOINT model, but retrained on each specific dataset. Features 4.4.1 Textual features We explore the following textual features; all features are frequency counts normalized by the length (number of tokens) of the document. • Unigrams. • POS unigrams and bigrams. Text is tagged using the Stanford POS tagger (Toutanova et al., 2003). • LIWC (Pennebaker et al., 2001). This is a word counting program that captures word classes such as inclusion words (LIWC-incl: “with,” “and,” “include,” etc.), causation words (LIWCcause: “because,” “hence,” etc.), and stylistic characteristics such as percentage of words longer than 6 letters (LIWC-Sixltr). of training instances, the noisy text, and the fact that the ages lie very close to each other. Predicted age Figure 2: Scatterplot of true and predicted age. 90 80 70 60 50 40 30 20 10 0 -10 -20 Overall, the joint model using all features performed best (condition 10). In Figure 2 a p"
W11-2139,2007.mtsummit-papers.3,0,0.0309223,"e with millions of features and avoid overfitting, perhaps by eliminating most of the features and keeping only the most valuable (which would also keep the model compact). 3 Similar features have been proposed for use in discriminative monolingual parsing models (Taskar et al., 2004). 339 Furthermore, we would like to be able to still target the BLEU measure of translation quality during learning. While large-scale discriminative training for machine translation is a widely studied problem (Hopkins and May, 2011; Li and Eisner, 2009; Devlin, 2009; Blunsom et al., 2008; Watanabe et al., 2007; Arun and Koehn, 2007; Liang et al., 2006), no tractable algorithm exists for learning a large number of feature weights while directly optimizing a corpus-level metric like BLEU. Rather than resorting to a decomposable approximation, we have explored a new two-phase training algorithm in development of this system. The two-phase algorithm works as follows. In phase 1, we use a non-BLEU objective to train a translation model that includes the large feature set. Then, we use this model to compute a small number of coarse “summary features,” which summarize the “opinion” of the first model about a translation hypoth"
W11-2139,P08-1024,0,0.69537,"we require a learning algorithm that can cope with millions of features and avoid overfitting, perhaps by eliminating most of the features and keeping only the most valuable (which would also keep the model compact). 3 Similar features have been proposed for use in discriminative monolingual parsing models (Taskar et al., 2004). 339 Furthermore, we would like to be able to still target the BLEU measure of translation quality during learning. While large-scale discriminative training for machine translation is a widely studied problem (Hopkins and May, 2011; Li and Eisner, 2009; Devlin, 2009; Blunsom et al., 2008; Watanabe et al., 2007; Arun and Koehn, 2007; Liang et al., 2006), no tractable algorithm exists for learning a large number of feature weights while directly optimizing a corpus-level metric like BLEU. Rather than resorting to a decomposable approximation, we have explored a new two-phase training algorithm in development of this system. The two-phase algorithm works as follows. In phase 1, we use a non-BLEU objective to train a translation model that includes the large feature set. Then, we use this model to compute a small number of coarse “summary features,” which summarize the “opinion”"
W11-2139,J92-4003,0,0.294899,"Missing"
W11-2139,J93-2003,0,0.0182861,"ve use of “closed” compounds (compound words written as a single orthographic token), we use a CRF segmentation model of to evaluate the probability of all possible segmentations, encoding the most probable ones compactly in a lattice (Dyer, 2009). For the purposes of grammar induction, the single most probable segmentation of each word in the source side of the parallel training data under the model was inferred. The parallel data were aligned using the Giza++ implementation of IBM Model 4 run in both directions and then symmetrized using the grow-diag-final-and heuristic (Och and Ney, 2002; Brown et al., 1993; Koehn et al., 2003). The aligned corpus was encoded as a suffix array (Lopez, 2008) and lattice-specific grammars (containing just the rules that are capable of matching spans in the input lattice) were extracted for each sentence in the test and development sets, using the heuristics recommended by Chiang (2007). A 4-gram modified Kneser-Ney language model (Chen and Goodman, 1996) was constructed using the SRI language modeling toolkit (Stolcke, 2002) from the English side of the parallel text, the monolingual English data, and the English version 4 Gigaword corpus (Parker et al., 2009). Si"
W11-2139,W11-2103,0,0.0265339,"ogical analyzers). An exception is a compound segmentation model used for preprocessing that was trained on a corpus of manually segmented German. Aside from this, no further manually annotated data was used, and we suspect many of the improvements described here can be had in other language pairs. Despite avoiding language-specific resources and using only the training data provided by the workshop, an extensive manual evaluation determined that the outputs produced were of significantly higher quality than both statistical and rule-based systems that made use of language-specific resources (Callison-Burch et al., 2011). 2 Baseline system and data Our translation system is based on a hierarchical phrase-based translation model (Chiang, 2007), as implemented in the cdec decoder (Dyer et al., 2010). Since German is a language that makes productive use of “closed” compounds (compound words written as a single orthographic token), we use a CRF segmentation model of to evaluate the probability of all possible segmentations, encoding the most probable ones compactly in a lattice (Dyer, 2009). For the purposes of grammar induction, the single most probable segmentation of each word in the source side of the paralle"
W11-2139,P96-1041,0,0.137601,"g data under the model was inferred. The parallel data were aligned using the Giza++ implementation of IBM Model 4 run in both directions and then symmetrized using the grow-diag-final-and heuristic (Och and Ney, 2002; Brown et al., 1993; Koehn et al., 2003). The aligned corpus was encoded as a suffix array (Lopez, 2008) and lattice-specific grammars (containing just the rules that are capable of matching spans in the input lattice) were extracted for each sentence in the test and development sets, using the heuristics recommended by Chiang (2007). A 4-gram modified Kneser-Ney language model (Chen and Goodman, 1996) was constructed using the SRI language modeling toolkit (Stolcke, 2002) from the English side of the parallel text, the monolingual English data, and the English version 4 Gigaword corpus (Parker et al., 2009). Since there were many duplicate segments in the training data (much 338 of which was crawled from the web), duplicate segments and segments longer than 100 words were removed. Inference was carried out using the language modeling library described by Heafield (2011). The newstest-2009 set (with the 500 longest segments removed) was used for development,2 and newstest-2010 was used as a"
W11-2139,D08-1024,0,0.0449065,"ormation in the form of features can lead to improved performance (Chiang, 2010; Gimpel and Smith, 2009; Marton and Resnik, 2008). Syntactic features that are computed by assessing the overlap of the translation parse with a linguistic parse can be understood to improve translation because they lead to a better model of what a “correct” parse of the source sentence is under the translation grammar. Like the “soft syntactic features” used in pre2 Removing long segments substantially reduces training time and does not appear to negatively affect performance. vious work (Marton and Resnik, 2008; Chiang et al., 2008), we propose features to assess the tree structure induced during translation. However, unlike that work, we do not rely on linguistic source parses, but instead only make use of features that are directly computable from the source sentence and the parse structure being considered in the decoder. In particular, we take inspiration from the model of Klein and Manning (2002), which models constituency in terms of the contexts that rule productions occur in. Additionally, we make use of salient aspects of the spans being dominated by a nonterminal, such as the words at the beginning and end of t"
W11-2139,J07-2003,0,0.877323,"ments to our core hierarchical phrase-based translation system, including: feature engineering to improve modeling of the derivation structure of translations; better handing of OOVs; and using development set translations into other languages to create additional pseudoreferences for training. 1 Introduction We describe the German-English translation system submitted to the shared translation task in the Sixth Workshop on Machine Translation (WMT11) by the ARK research group at Carnegie Mellon University.1 The core translation system is a hierarchical phrase-based machine translation system (Chiang, 2007) that has been extended in several ways described in this paper. Some of our innovations focus on modeling. Since German and English word orders can diverge considerably, particularly in non-matrix clauses, we focused on feature engineering to improve the modeling of long-distance relationships, which are poorly captured in standard hierarchical phrasebased translation models. To do so, we developed features that assess the goodness of the source 1 http://www.ark.cs.cmu.edu On the training side, we had two improvements over our baseline system. First, we were inspired by the work of Madnani (2"
W11-2139,P10-1146,0,0.026468,"and Byrne, 2004). 3 Source parse structure modeling Improving phrase-based translation systems is challenging in part because our intuitions about what makes a “good” phrase or translation derivation are often poor. For example, restricting phrases and rules to be consistent with syntactic constituents consistently harms performance (Chiang, 2007; Galley et al., 2006; Koehn et al., 2003), although our intuitions might suggest this is a reasonable thing to do. On the other hand, it has been shown that incorporating syntactic information in the form of features can lead to improved performance (Chiang, 2010; Gimpel and Smith, 2009; Marton and Resnik, 2008). Syntactic features that are computed by assessing the overlap of the translation parse with a linguistic parse can be understood to improve translation because they lead to a better model of what a “correct” parse of the source sentence is under the translation grammar. Like the “soft syntactic features” used in pre2 Removing long segments substantially reduces training time and does not appear to negatively affect performance. vious work (Marton and Resnik, 2008; Chiang et al., 2008), we propose features to assess the tree structure induced"
W11-2139,P10-4002,1,0.856164,"ted data was used, and we suspect many of the improvements described here can be had in other language pairs. Despite avoiding language-specific resources and using only the training data provided by the workshop, an extensive manual evaluation determined that the outputs produced were of significantly higher quality than both statistical and rule-based systems that made use of language-specific resources (Callison-Burch et al., 2011). 2 Baseline system and data Our translation system is based on a hierarchical phrase-based translation model (Chiang, 2007), as implemented in the cdec decoder (Dyer et al., 2010). Since German is a language that makes productive use of “closed” compounds (compound words written as a single orthographic token), we use a CRF segmentation model of to evaluate the probability of all possible segmentations, encoding the most probable ones compactly in a lattice (Dyer, 2009). For the purposes of grammar induction, the single most probable segmentation of each word in the source side of the parallel training data under the model was inferred. The parallel data were aligned using the Giza++ implementation of IBM Model 4 run in both directions and then symmetrized using the gr"
W11-2139,N09-1046,1,0.856464,"y higher quality than both statistical and rule-based systems that made use of language-specific resources (Callison-Burch et al., 2011). 2 Baseline system and data Our translation system is based on a hierarchical phrase-based translation model (Chiang, 2007), as implemented in the cdec decoder (Dyer et al., 2010). Since German is a language that makes productive use of “closed” compounds (compound words written as a single orthographic token), we use a CRF segmentation model of to evaluate the probability of all possible segmentations, encoding the most probable ones compactly in a lattice (Dyer, 2009). For the purposes of grammar induction, the single most probable segmentation of each word in the source side of the parallel training data under the model was inferred. The parallel data were aligned using the Giza++ implementation of IBM Model 4 run in both directions and then symmetrized using the grow-diag-final-and heuristic (Och and Ney, 2002; Brown et al., 1993; Koehn et al., 2003). The aligned corpus was encoded as a suffix array (Lopez, 2008) and lattice-specific grammars (containing just the rules that are capable of matching spans in the input lattice) were extracted for each sente"
W11-2139,W09-0439,0,0.105102,"the goodness of the source 1 http://www.ark.cs.cmu.edu On the training side, we had two improvements over our baseline system. First, we were inspired by the work of Madnani (2010), who showed that when training to optimize BLEU (Papineni et al., 2002), overfitting is reduced by supplementing a single human-generated reference translation with additional computer-generated references. We generated supplementary pseudo-references for our development set (which is translated into many languages, but once) by using MT output from a secondary Spanish-English translation system. Second, following Foster and Kuhn (2009), we used a secondary development set to select from among many optimization runs, which further improved generalization. We largely sought techniques that did not require language-specific resources (e.g., treebanks, POS 337 Proceedings of the 6th Workshop on Statistical Machine Translation, pages 337–343, c Edinburgh, Scotland, UK, July 30–31, 2011. 2011 Association for Computational Linguistics annotations, morphological analyzers). An exception is a compound segmentation model used for preprocessing that was trained on a corpus of manually segmented German. Aside from this, no further manu"
W11-2139,P06-1121,0,0.0545557,"segment-level MBR decoding with 1 − BLEU as the loss function, approximated over a 500-best list for each sentence. This reliably results in a small but consistent improvement in translation quality, but is much more time consuming to compute (Kumar and Byrne, 2004). 3 Source parse structure modeling Improving phrase-based translation systems is challenging in part because our intuitions about what makes a “good” phrase or translation derivation are often poor. For example, restricting phrases and rules to be consistent with syntactic constituents consistently harms performance (Chiang, 2007; Galley et al., 2006; Koehn et al., 2003), although our intuitions might suggest this is a reasonable thing to do. On the other hand, it has been shown that incorporating syntactic information in the form of features can lead to improved performance (Chiang, 2010; Gimpel and Smith, 2009; Marton and Resnik, 2008). Syntactic features that are computed by assessing the overlap of the translation parse with a linguistic parse can be understood to improve translation because they lead to a better model of what a “correct” parse of the source sentence is under the translation grammar. Like the “soft syntactic features”"
W11-2139,D09-1023,1,0.847272,"4). 3 Source parse structure modeling Improving phrase-based translation systems is challenging in part because our intuitions about what makes a “good” phrase or translation derivation are often poor. For example, restricting phrases and rules to be consistent with syntactic constituents consistently harms performance (Chiang, 2007; Galley et al., 2006; Koehn et al., 2003), although our intuitions might suggest this is a reasonable thing to do. On the other hand, it has been shown that incorporating syntactic information in the form of features can lead to improved performance (Chiang, 2010; Gimpel and Smith, 2009; Marton and Resnik, 2008). Syntactic features that are computed by assessing the overlap of the translation parse with a linguistic parse can be understood to improve translation because they lead to a better model of what a “correct” parse of the source sentence is under the translation grammar. Like the “soft syntactic features” used in pre2 Removing long segments substantially reduces training time and does not appear to negatively affect performance. vious work (Marton and Resnik, 2008; Chiang et al., 2008), we propose features to assess the tree structure induced during translation. Howe"
W11-2139,W11-2123,0,0.0277226,"development sets, using the heuristics recommended by Chiang (2007). A 4-gram modified Kneser-Ney language model (Chen and Goodman, 1996) was constructed using the SRI language modeling toolkit (Stolcke, 2002) from the English side of the parallel text, the monolingual English data, and the English version 4 Gigaword corpus (Parker et al., 2009). Since there were many duplicate segments in the training data (much 338 of which was crawled from the web), duplicate segments and segments longer than 100 words were removed. Inference was carried out using the language modeling library described by Heafield (2011). The newstest-2009 set (with the 500 longest segments removed) was used for development,2 and newstest-2010 was used as a development test set. Results in this paper are reported on the devtest set using uncased BLEU4 with a single reference translation. Minimum error rate training (Och, 2003) was used to optimize the parameters of the system to maximize BLEU on the development data, and inference was performed over a pruned hypergraph representation of the translation hypothesis space (Kumar et al., 2009). For the experiments reported in this paper, Viterbi (max-derivation) decoding was used"
W11-2139,D11-1125,0,0.0365813,"that MERT can not be used to infer their weights. Instead, we require a learning algorithm that can cope with millions of features and avoid overfitting, perhaps by eliminating most of the features and keeping only the most valuable (which would also keep the model compact). 3 Similar features have been proposed for use in discriminative monolingual parsing models (Taskar et al., 2004). 339 Furthermore, we would like to be able to still target the BLEU measure of translation quality during learning. While large-scale discriminative training for machine translation is a widely studied problem (Hopkins and May, 2011; Li and Eisner, 2009; Devlin, 2009; Blunsom et al., 2008; Watanabe et al., 2007; Arun and Koehn, 2007; Liang et al., 2006), no tractable algorithm exists for learning a large number of feature weights while directly optimizing a corpus-level metric like BLEU. Rather than resorting to a decomposable approximation, we have explored a new two-phase training algorithm in development of this system. The two-phase algorithm works as follows. In phase 1, we use a non-BLEU objective to train a translation model that includes the large feature set. Then, we use this model to compute a small number of"
W11-2139,P02-1017,0,0.0707841,"nder the translation grammar. Like the “soft syntactic features” used in pre2 Removing long segments substantially reduces training time and does not appear to negatively affect performance. vious work (Marton and Resnik, 2008; Chiang et al., 2008), we propose features to assess the tree structure induced during translation. However, unlike that work, we do not rely on linguistic source parses, but instead only make use of features that are directly computable from the source sentence and the parse structure being considered in the decoder. In particular, we take inspiration from the model of Klein and Manning (2002), which models constituency in terms of the contexts that rule productions occur in. Additionally, we make use of salient aspects of the spans being dominated by a nonterminal, such as the words at the beginning and end of the span, and the length of the span. Importantly, the features do not rely on the target words being predicted, but only look at the structure of the translation derivation. As such, they can be understood as monolingual parse features.3 Table 1 lists the feature templates that were used. Template CTX :fi−1 , fj CTX :fi−1 , fj , x CTX :fi−1 , fj , x, (j − i) LU :fi−1 LB :fi"
W11-2139,N03-1017,0,0.162752,"ompounds (compound words written as a single orthographic token), we use a CRF segmentation model of to evaluate the probability of all possible segmentations, encoding the most probable ones compactly in a lattice (Dyer, 2009). For the purposes of grammar induction, the single most probable segmentation of each word in the source side of the parallel training data under the model was inferred. The parallel data were aligned using the Giza++ implementation of IBM Model 4 run in both directions and then symmetrized using the grow-diag-final-and heuristic (Och and Ney, 2002; Brown et al., 1993; Koehn et al., 2003). The aligned corpus was encoded as a suffix array (Lopez, 2008) and lattice-specific grammars (containing just the rules that are capable of matching spans in the input lattice) were extracted for each sentence in the test and development sets, using the heuristics recommended by Chiang (2007). A 4-gram modified Kneser-Ney language model (Chen and Goodman, 1996) was constructed using the SRI language modeling toolkit (Stolcke, 2002) from the English side of the parallel text, the monolingual English data, and the English version 4 Gigaword corpus (Parker et al., 2009). Since there were many d"
W11-2139,N04-1022,0,0.0377714,") was used to optimize the parameters of the system to maximize BLEU on the development data, and inference was performed over a pruned hypergraph representation of the translation hypothesis space (Kumar et al., 2009). For the experiments reported in this paper, Viterbi (max-derivation) decoding was used. The system submitted for manual evaluation used segment-level MBR decoding with 1 − BLEU as the loss function, approximated over a 500-best list for each sentence. This reliably results in a small but consistent improvement in translation quality, but is much more time consuming to compute (Kumar and Byrne, 2004). 3 Source parse structure modeling Improving phrase-based translation systems is challenging in part because our intuitions about what makes a “good” phrase or translation derivation are often poor. For example, restricting phrases and rules to be consistent with syntactic constituents consistently harms performance (Chiang, 2007; Galley et al., 2006; Koehn et al., 2003), although our intuitions might suggest this is a reasonable thing to do. On the other hand, it has been shown that incorporating syntactic information in the form of features can lead to improved performance (Chiang, 2010; Gi"
W11-2139,P09-1019,1,0.890376,"ds were removed. Inference was carried out using the language modeling library described by Heafield (2011). The newstest-2009 set (with the 500 longest segments removed) was used for development,2 and newstest-2010 was used as a development test set. Results in this paper are reported on the devtest set using uncased BLEU4 with a single reference translation. Minimum error rate training (Och, 2003) was used to optimize the parameters of the system to maximize BLEU on the development data, and inference was performed over a pruned hypergraph representation of the translation hypothesis space (Kumar et al., 2009). For the experiments reported in this paper, Viterbi (max-derivation) decoding was used. The system submitted for manual evaluation used segment-level MBR decoding with 1 − BLEU as the loss function, approximated over a 500-best list for each sentence. This reliably results in a small but consistent improvement in translation quality, but is much more time consuming to compute (Kumar and Byrne, 2004). 3 Source parse structure modeling Improving phrase-based translation systems is challenging in part because our intuitions about what makes a “good” phrase or translation derivation are often po"
W11-2139,D09-1005,0,0.0209007,"ed to infer their weights. Instead, we require a learning algorithm that can cope with millions of features and avoid overfitting, perhaps by eliminating most of the features and keeping only the most valuable (which would also keep the model compact). 3 Similar features have been proposed for use in discriminative monolingual parsing models (Taskar et al., 2004). 339 Furthermore, we would like to be able to still target the BLEU measure of translation quality during learning. While large-scale discriminative training for machine translation is a widely studied problem (Hopkins and May, 2011; Li and Eisner, 2009; Devlin, 2009; Blunsom et al., 2008; Watanabe et al., 2007; Arun and Koehn, 2007; Liang et al., 2006), no tractable algorithm exists for learning a large number of feature weights while directly optimizing a corpus-level metric like BLEU. Rather than resorting to a decomposable approximation, we have explored a new two-phase training algorithm in development of this system. The two-phase algorithm works as follows. In phase 1, we use a non-BLEU objective to train a translation model that includes the large feature set. Then, we use this model to compute a small number of coarse “summary featu"
W11-2139,N09-1069,0,0.0173381,"a variable representing the unobserved synchronous parses giving rise to the pair of sentences hf, ei, and where R(θ) is a penalty that favors less complex models. Since we not only want to prevent over P fitting but also want a small model, we use R(θ) = k |θk |, the `1 norm, which forces many parameters to be exactly 0. Although L is not convex in θ (on account of the latent derivation variable), we make use of an online stochastic gradient descent algorithm that imposes an `1 penalty on the objective (Tsuruoka et al., 2009). Online algorithms are often effective for non-convex objectives (Liang and Klein, 2009). We selected 12,500 sentences randomly from the news-commentary portion of the training data to use to train the latent variable model. Using the standard rule extraction heuristics (Chiang, 2007), 9,967 of the sentence pairs could be derived.4 In addition to the parse features describe above, the standard phrase features (relative frequency and lexical translation probabilities), and a rule count feature were included. Training was run for 48 hours on a single machine, which resulted in 8 passes through the training data, instantiating over 8M unique features. The regularization strength λ w"
W11-2139,P06-1096,0,0.14682,"Missing"
W11-2139,C08-1064,0,0.0343443,"e a CRF segmentation model of to evaluate the probability of all possible segmentations, encoding the most probable ones compactly in a lattice (Dyer, 2009). For the purposes of grammar induction, the single most probable segmentation of each word in the source side of the parallel training data under the model was inferred. The parallel data were aligned using the Giza++ implementation of IBM Model 4 run in both directions and then symmetrized using the grow-diag-final-and heuristic (Och and Ney, 2002; Brown et al., 1993; Koehn et al., 2003). The aligned corpus was encoded as a suffix array (Lopez, 2008) and lattice-specific grammars (containing just the rules that are capable of matching spans in the input lattice) were extracted for each sentence in the test and development sets, using the heuristics recommended by Chiang (2007). A 4-gram modified Kneser-Ney language model (Chen and Goodman, 1996) was constructed using the SRI language modeling toolkit (Stolcke, 2002) from the English side of the parallel text, the monolingual English data, and the English version 4 Gigaword corpus (Parker et al., 2009). Since there were many duplicate segments in the training data (much 338 of which was cr"
W11-2139,P08-1114,0,0.022175,"ture modeling Improving phrase-based translation systems is challenging in part because our intuitions about what makes a “good” phrase or translation derivation are often poor. For example, restricting phrases and rules to be consistent with syntactic constituents consistently harms performance (Chiang, 2007; Galley et al., 2006; Koehn et al., 2003), although our intuitions might suggest this is a reasonable thing to do. On the other hand, it has been shown that incorporating syntactic information in the form of features can lead to improved performance (Chiang, 2010; Gimpel and Smith, 2009; Marton and Resnik, 2008). Syntactic features that are computed by assessing the overlap of the translation parse with a linguistic parse can be understood to improve translation because they lead to a better model of what a “correct” parse of the source sentence is under the translation grammar. Like the “soft syntactic features” used in pre2 Removing long segments substantially reduces training time and does not appear to negatively affect performance. vious work (Marton and Resnik, 2008; Chiang et al., 2008), we propose features to assess the tree structure induced during translation. However, unlike that work, we"
W11-2139,P02-1038,0,0.0739731,"that makes productive use of “closed” compounds (compound words written as a single orthographic token), we use a CRF segmentation model of to evaluate the probability of all possible segmentations, encoding the most probable ones compactly in a lattice (Dyer, 2009). For the purposes of grammar induction, the single most probable segmentation of each word in the source side of the parallel training data under the model was inferred. The parallel data were aligned using the Giza++ implementation of IBM Model 4 run in both directions and then symmetrized using the grow-diag-final-and heuristic (Och and Ney, 2002; Brown et al., 1993; Koehn et al., 2003). The aligned corpus was encoded as a suffix array (Lopez, 2008) and lattice-specific grammars (containing just the rules that are capable of matching spans in the input lattice) were extracted for each sentence in the test and development sets, using the heuristics recommended by Chiang (2007). A 4-gram modified Kneser-Ney language model (Chen and Goodman, 1996) was constructed using the SRI language modeling toolkit (Stolcke, 2002) from the English side of the parallel text, the monolingual English data, and the English version 4 Gigaword corpus (Park"
W11-2139,P03-1021,0,0.537349,"version 4 Gigaword corpus (Parker et al., 2009). Since there were many duplicate segments in the training data (much 338 of which was crawled from the web), duplicate segments and segments longer than 100 words were removed. Inference was carried out using the language modeling library described by Heafield (2011). The newstest-2009 set (with the 500 longest segments removed) was used for development,2 and newstest-2010 was used as a development test set. Results in this paper are reported on the devtest set using uncased BLEU4 with a single reference translation. Minimum error rate training (Och, 2003) was used to optimize the parameters of the system to maximize BLEU on the development data, and inference was performed over a pruned hypergraph representation of the translation hypothesis space (Kumar et al., 2009). For the experiments reported in this paper, Viterbi (max-derivation) decoding was used. The system submitted for manual evaluation used segment-level MBR decoding with 1 − BLEU as the loss function, approximated over a 500-best list for each sentence. This reliably results in a small but consistent improvement in translation quality, but is much more time consuming to compute (K"
W11-2139,P02-1040,0,0.0824535,"his paper. Some of our innovations focus on modeling. Since German and English word orders can diverge considerably, particularly in non-matrix clauses, we focused on feature engineering to improve the modeling of long-distance relationships, which are poorly captured in standard hierarchical phrasebased translation models. To do so, we developed features that assess the goodness of the source 1 http://www.ark.cs.cmu.edu On the training side, we had two improvements over our baseline system. First, we were inspired by the work of Madnani (2010), who showed that when training to optimize BLEU (Papineni et al., 2002), overfitting is reduced by supplementing a single human-generated reference translation with additional computer-generated references. We generated supplementary pseudo-references for our development set (which is translated into many languages, but once) by using MT output from a secondary Spanish-English translation system. Second, following Foster and Kuhn (2009), we used a secondary development set to select from among many optimization runs, which further improved generalization. We largely sought techniques that did not require language-specific resources (e.g., treebanks, POS 337 Proce"
W11-2139,W04-3201,0,0.116735,"training corpus, it is replaced by a special unknown token. The SMALLCAPS prefixes prevent accidental feature collisions. 3.1 Two-phase discriminative learning The parse features just introduced are numerous and sparse, which means that MERT can not be used to infer their weights. Instead, we require a learning algorithm that can cope with millions of features and avoid overfitting, perhaps by eliminating most of the features and keeping only the most valuable (which would also keep the model compact). 3 Similar features have been proposed for use in discriminative monolingual parsing models (Taskar et al., 2004). 339 Furthermore, we would like to be able to still target the BLEU measure of translation quality during learning. While large-scale discriminative training for machine translation is a widely studied problem (Hopkins and May, 2011; Li and Eisner, 2009; Devlin, 2009; Blunsom et al., 2008; Watanabe et al., 2007; Arun and Koehn, 2007; Liang et al., 2006), no tractable algorithm exists for learning a large number of feature weights while directly optimizing a corpus-level metric like BLEU. Rather than resorting to a decomposable approximation, we have explored a new two-phase training algorithm"
W11-2139,P09-1054,0,0.0541831,"− X log hf,ei∈T where pθ (e, d |f) = X pθ (e, d |f) d exp θ&gt; h(f, e, d) Z(f) , where d is a variable representing the unobserved synchronous parses giving rise to the pair of sentences hf, ei, and where R(θ) is a penalty that favors less complex models. Since we not only want to prevent over P fitting but also want a small model, we use R(θ) = k |θk |, the `1 norm, which forces many parameters to be exactly 0. Although L is not convex in θ (on account of the latent derivation variable), we make use of an online stochastic gradient descent algorithm that imposes an `1 penalty on the objective (Tsuruoka et al., 2009). Online algorithms are often effective for non-convex objectives (Liang and Klein, 2009). We selected 12,500 sentences randomly from the news-commentary portion of the training data to use to train the latent variable model. Using the standard rule extraction heuristics (Chiang, 2007), 9,967 of the sentence pairs could be derived.4 In addition to the parse features describe above, the standard phrase features (relative frequency and lexical translation probabilities), and a rule count feature were included. Training was run for 48 hours on a single machine, which resulted in 8 passes through"
W11-2139,P10-1040,0,0.0101623,"prove long range reordering quality. To further support the modeling of larger spans, we incorporated a 7-gram class-based language model. Automatic word clusters are attractive because they can be learned for any language without supervised data, and, unlike part-of-speech annotations, each word is in only a single class, which simplifies inference. We performed Brown clustering (Brown et al., 1992) on 900k sentences from our language modeling data (including the news commentary corpus and a subset of Gigaword). We obtained 1,000 clusters using an implementation provided by Liang (2005),6 as Turian et al. (2010) found that relatively large numbers clusters gave better performance for information extraction tasks. We then replaced words with their clusters in our language modeling data and built a 7-gram LM with Witten-Bell smoothing (Witten and Bell, 1991).7 The last two rows of Ta6 http://www.cs.berkeley.edu/˜pliang/ software 7 The distributional assumptions made by the more commonly used Kneser-Ney estimator do not hold in the wordble 2 shows that in conjunction with the source parse features, a slight improvement comes from including the 7-gram LM. 4 Non-translating tokens When two languages share"
W11-2139,D07-1080,0,0.0804662,"algorithm that can cope with millions of features and avoid overfitting, perhaps by eliminating most of the features and keeping only the most valuable (which would also keep the model compact). 3 Similar features have been proposed for use in discriminative monolingual parsing models (Taskar et al., 2004). 339 Furthermore, we would like to be able to still target the BLEU measure of translation quality during learning. While large-scale discriminative training for machine translation is a widely studied problem (Hopkins and May, 2011; Li and Eisner, 2009; Devlin, 2009; Blunsom et al., 2008; Watanabe et al., 2007; Arun and Koehn, 2007; Liang et al., 2006), no tractable algorithm exists for learning a large number of feature weights while directly optimizing a corpus-level metric like BLEU. Rather than resorting to a decomposable approximation, we have explored a new two-phase training algorithm in development of this system. The two-phase algorithm works as follows. In phase 1, we use a non-BLEU objective to train a translation model that includes the large feature set. Then, we use this model to compute a small number of coarse “summary features,” which summarize the “opinion” of the first model abou"
W11-2165,2007.mtsummit-papers.3,0,0.0302107,"previous work used heuristics or local statistical tests to extract patterns from corpora. In this paper, we present probabilistic models that generate text using gappy patterns of arbitrary length and with arbitrarily-many gaps. We exploit nonparametric priors and use Bayesian inference to discover the most salient gappy patterns in monolingual and parallel text. We first inspect these patterns manually and discuss the categories of phenomena that they capture. We also add them as features in a discriminatively-trained phrase-based MT system, using standard techniques to train their weights (Arun and Koehn, 2007; Watanabe et al., 2007) and incorporate them during decoding (Chiang, 2007). We present experiments for SpanishEnglish and Chinese-English translation, reporting encouraging preliminary results. A growing body of machine translation research aims to exploit lexical patterns (e.g., ngrams and phrase pairs) with gaps (Simard et al., 2005; Chiang, 2005; Xiong et al., 2011). Typically, these “gappy patterns” are discovered using heuristics based on word alignments or local statistics such as mutual information. In this paper, we develop generative models of monolingual and parallel text that buil"
W11-2165,P11-1131,0,0.0145795,"ibution over patterns: wCj ∼ ππππππππππππ( ) = either __ or Mult(µ). If the words wCj are not consistent Galley and Manning (2010) proposed ways of incorwith the color assignments, i.e., wrong number of porating phrase pairs with gaps into standard left-toit provides either too little or too much . words or gaps, gaps not in the correct locations, right decoding algorithms familiar to phrase-based it 's neither particularly complicated nor novel . repeat this step. and N -gram-based MT; both used heuristics to exnato must either say "" yes "" or "" no "" to the baltic states . tract phrase pairs. Bansal et al. (2011) presented a Thus, the probability of generating number of words good scientific ideas formulated in bad english either die or get repackaged . model and training procedure for word alignment n, words w1:n , color assignments c1:n , and number that uses phrase pairs with gaps. They use a semi- of colors m is Markov model with an enlarged dynamic programp(w1:n , c1:n , m |β, µ) ming state in order to represent alignment between    n Y m gappy phrases. Their model permits up to one gap 1 β n −β 1 1 = e pµ (π(Cj )) per phrase while our models permit an arbitrary Z n! n m j=1 number. (1) 3 M"
W11-2165,J92-4003,0,0.405129,"Missing"
W11-2165,W10-1703,0,0.127737,"ew one, and for an unaligned source word wi0 , c0i can be any source color, including a new one. The full equations for sampling can be easily derived using the equations from §3. 5 Evaluation We conducted evaluation to determine (1) what types of phenomena are captured by the most probable patterns discovered by our models, and (2) whether including the patterns as features can improve translation quality. 5.1 5.1.1 Qualitative Evaluation Monolingual Model Since inference is computationally expensive, we used the 126K-sentence English news commentary corpus provided for the WMT shared tasks (Callison-Burch et al., 2010). We ran Gibbs sampling for 600 iterations through the data, discarding the first 300 samples for burn-in and computing statistics of the patterns using the remaining 300 samples. Each iteration took approximately 3 minutes on a single 2.2GHz CPU. When looking primarily at the most frequent patterns, we found that this list did not vary much when only using half of the data instead. We set ν = 3 and α = 100; we found these hyperparameters to have only minor effects on the results. Since many frequent patterns include the period (.), we found it useful to constrain the model to treat this token"
W11-2165,W08-0336,0,0.0147491,"modeling toolkit (Stolcke, 2002) with modified Kneser-Ney smoothing (Chen and Goodman, 1998). Language models were trained on the target side of the parallel corpus as well as the first 5 million additional sentences from the extra English monolingual newswire data provided for the shared tasks. We used news-test2008 for tuning and news-test2009 for testing. We also consider Chinese-English (ZH→EN) and followed a similar training procedure as above. We used 303K sentence pairs from the FBIS corpus (LDC2003E14) and segmented the Chinese data using the Stanford Chinese segmenter in “CTB” mode (Chang et al., 2008), giving us 7.9M Chinese words and 9.4M English words. A trigram language model was estimated using modified KneserNey smoothing from the English side of the parallel corpus concatenated with 200M words of randomlyselected sentences from the Gigaword v4 corpus (excluding the NY Times and LA Times). We used NIST MT03 for tuning and NIST MT05 for testing. For evaluation, we used case-insensitive IBM BLEU (Papineni et al., 2001). 5.2.1 Training and Decoding Unlike n-gram language models, our models have latent structure (the color assignments), making it difficult to compute the probability of a"
W11-2165,D08-1024,0,0.0390457,"Missing"
W11-2165,P05-1033,0,0.352732,"text. We first inspect these patterns manually and discuss the categories of phenomena that they capture. We also add them as features in a discriminatively-trained phrase-based MT system, using standard techniques to train their weights (Arun and Koehn, 2007; Watanabe et al., 2007) and incorporate them during decoding (Chiang, 2007). We present experiments for SpanishEnglish and Chinese-English translation, reporting encouraging preliminary results. A growing body of machine translation research aims to exploit lexical patterns (e.g., ngrams and phrase pairs) with gaps (Simard et al., 2005; Chiang, 2005; Xiong et al., 2011). Typically, these “gappy patterns” are discovered using heuristics based on word alignments or local statistics such as mutual information. In this paper, we develop generative models of monolingual and parallel text that build sentences using gappy patterns of arbitrary length and with arbitrarily many gaps. We exploit Bayesian nonparametrics and collapsed Gibbs sampling to discover salient patterns in a corpus. We evaluate the patterns qualitatively and also add them as features to an MT system, reporting promising preliminary results. 2 1 Introduction Beginning with th"
W11-2165,J07-2003,0,0.310873,"orpora. In this paper, we present probabilistic models that generate text using gappy patterns of arbitrary length and with arbitrarily-many gaps. We exploit nonparametric priors and use Bayesian inference to discover the most salient gappy patterns in monolingual and parallel text. We first inspect these patterns manually and discuss the categories of phenomena that they capture. We also add them as features in a discriminatively-trained phrase-based MT system, using standard techniques to train their weights (Arun and Koehn, 2007; Watanabe et al., 2007) and incorporate them during decoding (Chiang, 2007). We present experiments for SpanishEnglish and Chinese-English translation, reporting encouraging preliminary results. A growing body of machine translation research aims to exploit lexical patterns (e.g., ngrams and phrase pairs) with gaps (Simard et al., 2005; Chiang, 2005; Xiong et al., 2011). Typically, these “gappy patterns” are discovered using heuristics based on word alignments or local statistics such as mutual information. In this paper, we develop generative models of monolingual and parallel text that build sentences using gappy patterns of arbitrary length and with arbitrarily ma"
W11-2165,2009.eamt-1.10,0,0.0147609,"to a phrase-based MT system that scores new words based on all potential triggers from previous parts of the derivation. We are not aware of prior work that uses generative modeling and Bayesian nonparametrics to discover these same types of patterns automatically; doing so allows us to discover larger patterns with more words and gaps if they are warranted by the data. In addition to the gappy phrase-based (Simard et al., 2005) and hierarchical phrase-based (Chiang, 2005) models mentioned earlier, other researchers have explored the use of bilingual gappy structures for machine translation. Crego and Yvon (2009) and 512 Proceedings of the 6th Workshop on Statistical Machine Translation, pages 512–522, c Edinburgh, Scotland, UK, July 30–31, 2011. 2011 Association for Computational Linguistics nato must either say "" yes "" or "" no "" to the baltic states . π( ) = nato π( ) = say π( ) = to the π( ) = must π( ) = "" __ "" __ "" __ "" π( ) = baltic states π( ) = either __ or π( ) = yes __ no π( ) = . Figure 1: A sentence from the news commentary corpus, along with color assignments for the words and the π function for each color. 1. Sample the number of words: n ∼ Poisson(β) 2. Sample the number of unique color"
W11-2165,N10-1140,0,0.0296996,"_ or π( ) = yes __ no π( ) = . Figure 1: A sentence from the news commentary corpus, along with color assignments for the words and the π function for each color. 1. Sample the number of words: n ∼ Poisson(β) 2. Sample the number of unique colors in the sentence given n: m ∼ Uniform(1, n) 3. For each word index i = 1 . . . n, sample the color of word i: ci ∼ Uniform(1, m). If any of the m colors has no words, repeat this step. 4. For each color j = 1 . . . m, sample from a multinomial distribution over patterns: wCj ∼ ππππππππππππ( ) = either __ or Mult(µ). If the words wCj are not consistent Galley and Manning (2010) proposed ways of incorwith the color assignments, i.e., wrong number of porating phrase pairs with gaps into standard left-toit provides either too little or too much . words or gaps, gaps not in the correct locations, right decoding algorithms familiar to phrase-based it 's neither particularly complicated nor novel . repeat this step. and N -gram-based MT; both used heuristics to exnato must either say "" yes "" or "" no "" to the baltic states . tract phrase pairs. Bansal et al. (2011) presented a Thus, the probability of generating number of words good scientific ideas formulated in bad engli"
W11-2165,P07-1019,0,0.0196628,"been shown to be effective for MT (Liang et al., 2006; Arun and Koehn, 2007; Watanabe et al., 2007; Chiang et al., 2008). Though not shown in the algorithm, in practice we store the BLEU-best translation on each k-best list from all previous iterations and use it as e+ if it has a higher BLEU score than any on the k-best list on the current iteration. At decoding time, we follow a procedure similar to training: we generate lattices for each source sentence using Moses with its standard set of features and using weights λM . We rescore the lattices using λ∗ and use cube pruning (Chiang, 2007; Huang and Chiang, 2007) to incorporate the gappy pattern features with weights θ ∗ . Cube pruning is necessary because the pattern features may match anywhere in the translation; thus they are non-local in the phrase lattice and require approximate inference. 5.3 ES→EN 25.64 25.85 Training Algorithm Comparison Before adding pattern features, we evaluate our training algorithm by comparing it to MERT using the same standard Moses features. As the ini520 5.4 Feature Preparation We chose monolingual and bilingual pattern features using the posterior samples obtained via the inference procedures described above. We rank"
W11-2165,N03-1017,0,0.028193,"patterns” are discovered using heuristics based on word alignments or local statistics such as mutual information. In this paper, we develop generative models of monolingual and parallel text that build sentences using gappy patterns of arbitrary length and with arbitrarily many gaps. We exploit Bayesian nonparametrics and collapsed Gibbs sampling to discover salient patterns in a corpus. We evaluate the patterns qualitatively and also add them as features to an MT system, reporting promising preliminary results. 2 1 Introduction Beginning with the success of phrase-based translation models (Koehn et al., 2003), a trend arose of modeling larger and increasingly complex structural units in translation. One thread of work has focused on the use of lexical patterns with gaps. Simard et al. (2005) proposed using phrase pairs with gaps in a phrase-based translation model, providing a heuristic method to extract gappy phrase pairs from wordaligned parallel corpora. The widely-used hierarchical phrase-based translation framework was introduced by Chiang (2005) and also relies on a simple heuristic for phrase pair extraction. On the monolingual side, researchers have taken inspiration from trigger-based lan"
W11-2165,P07-2045,0,0.015552,"e subject in English when translating from Spanish, as Spanish often drops the subject when it is clear from context, e.g., “we are(estamos)”. Also, one probable pattern for German-English was “the of the(des)” (des is aligned to the final the). The German determiner des is in the genitive case, so this pattern helps to encourage its object to also be in the genitive case when translated. 5.2 Quantitative Evaluation We consider the Spanish-to-English (ES→EN) translation task from the ACL-2010 Workshop on Statistical Machine Translation (Callison-Burch et al., 2010). We trained a Moses system (Koehn et al., 2007) following the baseline training instructions for the shared task.7 In particular, we performed word alignment in each direction using GIZA++ (Och and Ney, 2003), used the “grow-diag-final-and” heuristic for symmetrization, and extracted phrase pairs up to a maximum length of seven. After filtering sentence pairs with one sentence longer than 50 words, we ended up with 1.45M sentence pairs of Europarl data and 91K sentence pairs of news commentary data. Language models (N = 5) were estimated using the SRI language modeling toolkit (Stolcke, 2002) with modified Kneser-Ney smoothing (Chen and Go"
W11-2165,P06-1096,0,0.0610772,"Missing"
W11-2165,J03-1002,0,0.0135678,"particular prepositions). There are also many probable patterns without gaps, shown at the bottom of Table 4. From these patterns we can see that our models can also be used to find collocations, but we note that these are discovered in the context of the gappy patterns. That is, due to the use of latent variables in our models (the color assignments), there is a natural trading-off effect whereby the gappy patterns encourage particular non-gappy patterns to be used, and vice versa. 5.1.2 Bilingual Model We use the news commentary corpus for each language and take the intersection of GIZA++ (Och and Ney, 2003) word alignments in each direction, thereby ensuring that they are 1-to-1 alignments. We ran Gibbs sampling for 300 iterations, averaging pattern counts from the last 200. We set α = 100, λ = 3, and γ = 0.5. We ran the model in 3 conditions: source words, target words; source clusters, target clusters; and source clusters, target words. We 6 We filter Brown cluster patterns in which every cluster is a singleton, since these patterns are typically already accounted for in the lexical patterns. Rank 2 6 28 178 239 8 12 21 23 43 46 149 172 180 5 9 19 40 45 50 56 68 98 131 1 15 30 47 62 72 73 113"
W11-2165,P03-1021,0,0.0604284,", we used case-insensitive IBM BLEU (Papineni et al., 2001). 5.2.1 Training and Decoding Unlike n-gram language models, our models have latent structure (the color assignments), making it difficult to compute the probability of a translation during decoding. We leave this problem for future work and instead simply add a feature for each of the most probable patterns discovered by our models. Each feature counts the number of occurrences of its pattern in the translation. We wish to add thousands of features to our model, but the standard training algorithm – minimum error rate training (MERT; Och, 2003) – cannot handle large numbers of features. So, we leverage recent work on feature-rich training for MT using online discriminative learning algorithms. Our training procedure is shown as Algorithm 1. We find it convenient to notationally distinguish feature weights for the standard Moses features (λ) from weights for our pattern features (θ). We use h(e) to denote the feature vector for translation e. The function Bi (t) returns the sentence BLEU score for translation t given reference ei (i.e., treating the sentence pair as a corpus).8 MERT is run to convergence on the tuning set to obtain w"
W11-2165,2001.mtsummit-papers.68,0,0.013434,"training procedure as above. We used 303K sentence pairs from the FBIS corpus (LDC2003E14) and segmented the Chinese data using the Stanford Chinese segmenter in “CTB” mode (Chang et al., 2008), giving us 7.9M Chinese words and 9.4M English words. A trigram language model was estimated using modified KneserNey smoothing from the English side of the parallel corpus concatenated with 200M words of randomlyselected sentences from the Gigaword v4 corpus (excluding the NY Times and LA Times). We used NIST MT03 for tuning and NIST MT05 for testing. For evaluation, we used case-insensitive IBM BLEU (Papineni et al., 2001). 5.2.1 Training and Decoding Unlike n-gram language models, our models have latent structure (the color assignments), making it difficult to compute the probability of a translation during decoding. We leave this problem for future work and instead simply add a feature for each of the most probable patterns discovered by our models. Each feature counts the number of occurrences of its pattern in the translation. We wish to add thousands of features to our model, but the standard training algorithm – minimum error rate training (MERT; Och, 2003) – cannot handle large numbers of features. So, w"
W11-2165,H05-1095,0,0.0483087,"Missing"
W11-2165,W02-1021,0,0.0431779,"work on feature-rich training for MT using online discriminative learning algorithms. Our training procedure is shown as Algorithm 1. We find it convenient to notationally distinguish feature weights for the standard Moses features (λ) from weights for our pattern features (θ). We use h(e) to denote the feature vector for translation e. The function Bi (t) returns the sentence BLEU score for translation t given reference ei (i.e., treating the sentence pair as a corpus).8 MERT is run to convergence on the tuning set to obtain weights for the standard Moses features (line 1). Phrase lattices (Ueffing et al., 2002) are generated for all source sentences in the tuning set using the trained weights λM (line 2). The lattices are used within a modified version of the margininfused relaxed algorithm (MIRA; Crammer et al., 2006) for structured max-margin learning (lines 515). A k-best list is extracted from the current lattice (line 7), then the translations on the k-best list with the highest and lowest sentence-level BLEU scores are found (lines 8 and 9). The step size is then computed using the standard MIRA formula (lines 1011) and the update is made (line 12). The returned weights are averaged over all u"
W11-2165,D07-1080,0,0.0178299,"ristics or local statistical tests to extract patterns from corpora. In this paper, we present probabilistic models that generate text using gappy patterns of arbitrary length and with arbitrarily-many gaps. We exploit nonparametric priors and use Bayesian inference to discover the most salient gappy patterns in monolingual and parallel text. We first inspect these patterns manually and discuss the categories of phenomena that they capture. We also add them as features in a discriminatively-trained phrase-based MT system, using standard techniques to train their weights (Arun and Koehn, 2007; Watanabe et al., 2007) and incorporate them during decoding (Chiang, 2007). We present experiments for SpanishEnglish and Chinese-English translation, reporting encouraging preliminary results. A growing body of machine translation research aims to exploit lexical patterns (e.g., ngrams and phrase pairs) with gaps (Simard et al., 2005; Chiang, 2005; Xiong et al., 2011). Typically, these “gappy patterns” are discovered using heuristics based on word alignments or local statistics such as mutual information. In this paper, we develop generative models of monolingual and parallel text that build sentences using gappy"
W11-2165,P11-1129,0,0.0733406,"t inspect these patterns manually and discuss the categories of phenomena that they capture. We also add them as features in a discriminatively-trained phrase-based MT system, using standard techniques to train their weights (Arun and Koehn, 2007; Watanabe et al., 2007) and incorporate them during decoding (Chiang, 2007). We present experiments for SpanishEnglish and Chinese-English translation, reporting encouraging preliminary results. A growing body of machine translation research aims to exploit lexical patterns (e.g., ngrams and phrase pairs) with gaps (Simard et al., 2005; Chiang, 2005; Xiong et al., 2011). Typically, these “gappy patterns” are discovered using heuristics based on word alignments or local statistics such as mutual information. In this paper, we develop generative models of monolingual and parallel text that build sentences using gappy patterns of arbitrary length and with arbitrarily many gaps. We exploit Bayesian nonparametrics and collapsed Gibbs sampling to discover salient patterns in a corpus. We evaluate the patterns qualitatively and also add them as features to an MT system, reporting promising preliminary results. 2 1 Introduction Beginning with the success of phrase-b"
W11-2165,P02-1040,0,\N,Missing
W11-2165,D08-1076,0,\N,Missing
W11-2202,S07-1012,0,0.0908542,"Missing"
W11-2202,D08-1031,0,0.0174584,"rence resolution (Li et al., 2004; Haghighi and Klein, 2007; Poon and Domingos, 2008; Singh et al., 2011). The problem we consider is related to cross-document coreference, although we take on the additional challenge of providing a canonicalized name for each referent (the corresponding table row), and in inferring a structured representation of entity names (the table columns). For this reason, our evaluation focuses on the induced table of entities, rather than the clustering of mention strings. The best coreference systems depend on carefully crafted, problem-specific linguistic features (Bengtson and Roth, 2008) and external knowledge (Haghighi and Klein, 2010b). Future work might consider how to exploit such features for the more holistic information extraction setting. 10 9 Conclusion This paper presents a Bayesian nonparametric approach to recover structured records from text. Using only a small set of prototype records, we are able to recover an accurate table that jointly identifies entities and internal name structure. In our view, the main advantage of a Bayesian approach compared to more heuristic alternatives is that it facilitates incorporation of additional information sources when availab"
W11-2202,W98-1118,0,0.0439867,"rk. Name Segmentation and Structure A related stream of research focuses specifically on names: identifying them in raw text, discovering their structure, and matching names that refer to the same entity. We do not undertake the problem of named entity recognition (Tjong Kim Sang, 2002), but rather apply an existing NER system as a preprocessing step (Finkel et al., 2005). Typical NER systems do not attempt to discover the internal structure of names or a database of canonical names, although they often use prefabricated “gazetteers” of names and name parts as features to improve performance (Borthwick et al., 1998; Sarawagi and Cohen, 2005). Charniak (2001) shows that it is possible to learn a model of name structure, either by using coreference information as labeled data, or by leveraging a small set of hand-crafted constraints. Elsner et al. (2009) develop a nonparametric Bayesian model of name structure using adaptor grammars, which they use to distinguish types of names (e.g., people, places, and organizations). Li et al. (2004) use a set of manuallycrafted “transformations” of name parts to build a model of how a name might be rendered in multiple different ways. While each of these approaches be"
W11-2202,P11-1098,0,0.0389599,"text to fill in the fields of manually-defined templates, thus populating databases of events or re9 lations (McNamee and Dang, 2009). While early approaches focused on surface-level methods such as wrapper induction (Kushmerick et al., 1997), more recent work in this area includes Bayesian nonparametrics to select the number of rows in the database (Haghighi and Klein, 2010a). However, even in such nonparametric work, the form of the template and the number of slots are fixed in advance. Our approach differs in that the number of fields and their meaning is learned from data. Recent work by Chambers and Jurafsky (2011) approaches a related problem, applying agglomerative clustering over sentences to detect events, and then clustering syntactic constituents to induce the relevant fields of each event entity. As described in Section 6, our method performs well against an agglomerative clustering baseline, though a more comprehensive comparison of the two approaches is an important step for future work. Name Segmentation and Structure A related stream of research focuses specifically on names: identifying them in raw text, discovering their structure, and matching names that refer to the same entity. We do not"
W11-2202,N01-1007,0,0.441624,"m of research focuses specifically on names: identifying them in raw text, discovering their structure, and matching names that refer to the same entity. We do not undertake the problem of named entity recognition (Tjong Kim Sang, 2002), but rather apply an existing NER system as a preprocessing step (Finkel et al., 2005). Typical NER systems do not attempt to discover the internal structure of names or a database of canonical names, although they often use prefabricated “gazetteers” of names and name parts as features to improve performance (Borthwick et al., 1998; Sarawagi and Cohen, 2005). Charniak (2001) shows that it is possible to learn a model of name structure, either by using coreference information as labeled data, or by leveraging a small set of hand-crafted constraints. Elsner et al. (2009) develop a nonparametric Bayesian model of name structure using adaptor grammars, which they use to distinguish types of names (e.g., people, places, and organizations). Li et al. (2004) use a set of manuallycrafted “transformations” of name parts to build a model of how a name might be rendered in multiple different ways. While each of these approaches bears on one or more facets of the problem tha"
W11-2202,doddington-etal-2004-automatic,0,0.0381729,"ave many more: for example, the entity Barack Obama has known names: {Barack, Obama, Sen., Mr.}. Metrics We evaluate the recall and precision of a system’s response set by matching against the reference set. The first step is to create a bipartite matching between response and reference entities.3 Using a cost function that quantifies the sim2 Recent work exploiting Wikipedia disambiguation pages for evaluating cross-document coreference suggests an appealing alternative for future work (Singh et al., 2011). 3 Bipartite matchings are typical in information extraction evaluation metrics (e.g., Doddington et al., 2004). Systems The initial seed set for our system consists of a partial annotation of five entities (Table 1) — larger seed sets did not improve performance. We run the inference procedure described in the previous section for 20,000 iterations, and then obtain a final database by taking the intersection of the in¯ obtained at every 100 iterations, startferred tables x ing with iteration 15,000. To account for variance across Markov chains, we perform three different runs. We evaluate a non-temporal version of our model (as described in Sections 3 and 4), and a temporal version with 5 epochs. For"
W11-2202,S07-1058,0,0.0623979,"that appear in names (such as titles and first names). We are aware of no existing system that performs all three of these tasks jointly. We evaluate on a dataset of political blogs, measuring our system’s ability to discover a set of reference entities (recall) while maintaining a compact number of rows and columns (precision). With as few as five partially-complete prototype examples, our approach gives accurate tables that match well against a manually-annotated reference list. Our method outperforms a baseline singlelink clustering approach inspired by one of the most successful entries (Elmacioglu et al., 2007) in the SEMEVAL “Web People Search” shared task (Artiles et al., 2007). 2 Task Definition In this work, we assume that a bag of M mentions in text have been identified. The mth mention wm is a sequence of contiguous word tokens (its length is denoted Nm ) understood to refer to a real-world entity. The entities (and the mapping of mentions to entities) are not known in advance. While our focus in this paper is names of people, the task is defined in a more generic way. Formally, the task is to construct a table x where rows correspond to entities and columns to functional fields. The number of"
W11-2202,N09-1019,0,0.0814631,"d entity recognition (Tjong Kim Sang, 2002), but rather apply an existing NER system as a preprocessing step (Finkel et al., 2005). Typical NER systems do not attempt to discover the internal structure of names or a database of canonical names, although they often use prefabricated “gazetteers” of names and name parts as features to improve performance (Borthwick et al., 1998; Sarawagi and Cohen, 2005). Charniak (2001) shows that it is possible to learn a model of name structure, either by using coreference information as labeled data, or by leveraging a small set of hand-crafted constraints. Elsner et al. (2009) develop a nonparametric Bayesian model of name structure using adaptor grammars, which they use to distinguish types of names (e.g., people, places, and organizations). Li et al. (2004) use a set of manuallycrafted “transformations” of name parts to build a model of how a name might be rendered in multiple different ways. While each of these approaches bears on one or more facets of the problem that we consider here, none provides a holistic treatment of name disambiguation and structure. Resolving Mentions to Entities The problem of resolving mentions to entities has been approach from a var"
W11-2202,P05-1045,0,0.0716754,"the entities which are mentioned in raw text. We annotate a new dataset of blog text for this purpose, and design precision and recall metrics to reward systems that recover as much of the reference set as possible, while avoiding spurious entities and fields. We also perform a qualitative analysis, noting the areas where our method outperforms string matching approaches, and where there is need for further improvement. Data Evaluation was performed on a corpus of blogs describing United States politics in 2008 (Eisenstein and Xing, 2010). We ran the Stanford Named Entity Recognition system (Finkel et al., 2005) to obtain a set of 25,000 candidate mentions which the system judged to be names of people. We then pruned strings that appeared fewer than four times and eliminated strings with more than seven tokens (these were usually errors). The resulting dataset has 19,247 mentions comprising 45,466 word tokens, and 813 unique mention strings. Gold standard We develop a reference set of 100 entities for evaluation. This set was created by sorting the unique name strings in the training set by frequency, and manually merging strings that reference the same entity. We also manually discarded strings from"
W11-2202,P07-1107,0,0.0705131,"ns and abbreviations lead to imperfect matches in different references to the same publication. In our task, we consider name mentions in raw text; such mentions are short, and may not offer as many redundant clues for linkage as bibliographic references. In natural language processing, coreference resolution is the task of grouping entity mentions (strings), in one or more documents, based on their common referents in the world. Although much of coreference resolution has on the single document setting, there has been some recent work on crossdocument coreference resolution (Li et al., 2004; Haghighi and Klein, 2007; Poon and Domingos, 2008; Singh et al., 2011). The problem we consider is related to cross-document coreference, although we take on the additional challenge of providing a canonicalized name for each referent (the corresponding table row), and in inferring a structured representation of entity names (the table columns). For this reason, our evaluation focuses on the induced table of entities, rather than the clustering of mention strings. The best coreference systems depend on carefully crafted, problem-specific linguistic features (Bengtson and Roth, 2008) and external knowledge (Haghighi a"
W11-2202,P10-2054,0,0.24949,"in an improvement. Larger scale MetropolisHastings moves, such as split-merge or type-based sampling (Liang et al., 2010) may help. 8 Related Work Information Extraction A tradition of research in information extraction focuses on processing raw text to fill in the fields of manually-defined templates, thus populating databases of events or re9 lations (McNamee and Dang, 2009). While early approaches focused on surface-level methods such as wrapper induction (Kushmerick et al., 1997), more recent work in this area includes Bayesian nonparametrics to select the number of rows in the database (Haghighi and Klein, 2010a). However, even in such nonparametric work, the form of the template and the number of slots are fixed in advance. Our approach differs in that the number of fields and their meaning is learned from data. Recent work by Chambers and Jurafsky (2011) approaches a related problem, applying agglomerative clustering over sentences to detect events, and then clustering syntactic constituents to induce the relevant fields of each event entity. As described in Section 6, our method performs well against an agglomerative clustering baseline, though a more comprehensive comparison of the two approache"
W11-2202,N10-1061,0,0.177867,"in an improvement. Larger scale MetropolisHastings moves, such as split-merge or type-based sampling (Liang et al., 2010) may help. 8 Related Work Information Extraction A tradition of research in information extraction focuses on processing raw text to fill in the fields of manually-defined templates, thus populating databases of events or re9 lations (McNamee and Dang, 2009). While early approaches focused on surface-level methods such as wrapper induction (Kushmerick et al., 1997), more recent work in this area includes Bayesian nonparametrics to select the number of rows in the database (Haghighi and Klein, 2010a). However, even in such nonparametric work, the form of the template and the number of slots are fixed in advance. Our approach differs in that the number of fields and their meaning is learned from data. Recent work by Chambers and Jurafsky (2011) approaches a related problem, applying agglomerative clustering over sentences to detect events, and then clustering syntactic constituents to induce the relevant fields of each event entity. As described in Section 6, our method performs well against an agglomerative clustering baseline, though a more comprehensive comparison of the two approache"
W11-2202,D08-1068,0,0.0585186,"to imperfect matches in different references to the same publication. In our task, we consider name mentions in raw text; such mentions are short, and may not offer as many redundant clues for linkage as bibliographic references. In natural language processing, coreference resolution is the task of grouping entity mentions (strings), in one or more documents, based on their common referents in the world. Although much of coreference resolution has on the single document setting, there has been some recent work on crossdocument coreference resolution (Li et al., 2004; Haghighi and Klein, 2007; Poon and Domingos, 2008; Singh et al., 2011). The problem we consider is related to cross-document coreference, although we take on the additional challenge of providing a canonicalized name for each referent (the corresponding table row), and in inferring a structured representation of entity names (the table columns). For this reason, our evaluation focuses on the induced table of entities, rather than the clustering of mention strings. The best coreference systems depend on carefully crafted, problem-specific linguistic features (Bengtson and Roth, 2008) and external knowledge (Haghighi and Klein, 2010b). Future"
W11-2202,P11-1080,0,0.346535,"for the 100 entities. Most entities only include first and last names, though the most frequent entities have many more: for example, the entity Barack Obama has known names: {Barack, Obama, Sen., Mr.}. Metrics We evaluate the recall and precision of a system’s response set by matching against the reference set. The first step is to create a bipartite matching between response and reference entities.3 Using a cost function that quantifies the sim2 Recent work exploiting Wikipedia disambiguation pages for evaluating cross-document coreference suggests an appealing alternative for future work (Singh et al., 2011). 3 Bipartite matchings are typical in information extraction evaluation metrics (e.g., Doddington et al., 2004). Systems The initial seed set for our system consists of a partial annotation of five entities (Table 1) — larger seed sets did not improve performance. We run the inference procedure described in the previous section for 20,000 iterations, and then obtain a final database by taking the intersection of the in¯ obtained at every 100 iterations, startferred tables x ing with iteration 15,000. To account for variance across Markov chains, we perform three different runs. We evaluate a"
W11-2202,W02-2024,0,0.109779,"ng over sentences to detect events, and then clustering syntactic constituents to induce the relevant fields of each event entity. As described in Section 6, our method performs well against an agglomerative clustering baseline, though a more comprehensive comparison of the two approaches is an important step for future work. Name Segmentation and Structure A related stream of research focuses specifically on names: identifying them in raw text, discovering their structure, and matching names that refer to the same entity. We do not undertake the problem of named entity recognition (Tjong Kim Sang, 2002), but rather apply an existing NER system as a preprocessing step (Finkel et al., 2005). Typical NER systems do not attempt to discover the internal structure of names or a database of canonical names, although they often use prefabricated “gazetteers” of names and name parts as features to improve performance (Borthwick et al., 1998; Sarawagi and Cohen, 2005). Charniak (2001) shows that it is possible to learn a model of name structure, either by using coreference information as labeled data, or by leveraging a small set of hand-crafted constraints. Elsner et al. (2009) develop a nonparametri"
W11-2202,N10-1082,0,\N,Missing
W11-2208,N10-1083,0,0.327563,"vere problem of local maxima, and suggest an alternative – using contrastive estimation for estimation of the parameters. Our experiments show that estimating the parameters this way, using overlapping features with joint MRFs performs better than previous work on the 1984 dataset. 1 Introduction This paper considers unsupervised learning of linguistic structure—specifically, parts of speech—in parallel text data. This setting, and more generally the multilingual learning scenario, has been found advantageous for a variety of unsupervised NLP tasks (Snyder et al., 2008; Cohen and Smith, 2010; Berg-Kirkpatrick et al., 2010; Das and Petrov, 2011). We consider globally normalized Markov random fields (MRFs) as an alternative to directed models based on multinomial distributions or locally normalized log-linear distributions. This alternate parameterization allows us to introduce correlated features that, at least in principle, depend on any parts of the hidden structure. Such models, sometimes called “undirected,” are widespread in supervised NLP; the most notable instances are conditional random fields (Lafferty et al., 2001), which have enabled rich feature engineering to incorporate knowledge and improve perfo"
W11-2208,D11-1005,1,0.858908,"sented in WFSTs. Das and Petrov (2011) also consider the problem of unsupervised bilingual POS induction. They make use of independent conventional HMM monolingual tagging models that are parameterized with feature-rich log-linear models (Berg-Kirkpatrick et al., 2010). However, training is constrained with tag dictionaries inferred using bilingual contexts derived from aligned parallel data. In this way, the complex inference and modeling challenges associated with a bilingual tagging model are avoided. Finally, multilingual POS induction has also been considered without using parallel data. Cohen et al. (2011) present a multilingual estimation technique for part-of-speech tagging (and grammar induction), where the lack of parallel data is compensated by the use of labeled data for some languages and unlabeled data for other languages. 3 Model Our model is a Markov random field whose random variables correspond to words in two parallel sentences and POS tags for those words. Let s = hs1 , . . . , sNs i and t = ht1 , . . . , tNt i denote the two word sequences; these correspond to Ns + Nt observed random variables.1 Let x and y denote the sequences of POS tags for s and t, respectively. These are the"
W11-2208,P11-1061,0,0.230006,"nd suggest an alternative – using contrastive estimation for estimation of the parameters. Our experiments show that estimating the parameters this way, using overlapping features with joint MRFs performs better than previous work on the 1984 dataset. 1 Introduction This paper considers unsupervised learning of linguistic structure—specifically, parts of speech—in parallel text data. This setting, and more generally the multilingual learning scenario, has been found advantageous for a variety of unsupervised NLP tasks (Snyder et al., 2008; Cohen and Smith, 2010; Berg-Kirkpatrick et al., 2010; Das and Petrov, 2011). We consider globally normalized Markov random fields (MRFs) as an alternative to directed models based on multinomial distributions or locally normalized log-linear distributions. This alternate parameterization allows us to introduce correlated features that, at least in principle, depend on any parts of the hidden structure. Such models, sometimes called “undirected,” are widespread in supervised NLP; the most notable instances are conditional random fields (Lafferty et al., 2001), which have enabled rich feature engineering to incorporate knowledge and improve performance. We conjecture t"
W11-2208,erjavec-2004-multext,0,0.0739108,"Missing"
W11-2208,P07-1094,0,0.0711216,"task of unsupervised bilingual POS induction was originally suggested and explored by Snyder et al. (2008). Their work proposes a joint model over pairs of tag sequences and words that can be understood as a pair of hidden Markov models (HMMs) Proceedings of EMNLP 2011, Conference on Empirical Methods in Natural Language Processing, pages 64–71, c Edinburgh, Scotland, UK, July 27–31, 2011. 2011 Association for Computational Linguistics in which aligned words share states (a fixed and observable word alignment is assumed). Figure 1 gives an example for a French-English sentence pair. Following Goldwater and Griffiths (2007), the transition, emission and coupling parameters are governed by Dirichlet priors, and a token-level collapsed Gibbs sampler is used for inference. The hyperparameters of the prior distributions are inferred from data in an empirical Bayesian fashion. Why repeat x1/y1 X2/y2 that catastrophe ? x4/y5 x5/y6 catastrophe ? x3 Pourquoi répéter y3 y4 la même Figure 1: Bilingual Directed POS induction model When word alignments are monotonic (i.e., there are no crossing links in the alignment graph), the model of Snyder et al. is straightforward to construct. However, crossing alignment links pose a"
W11-2208,N06-1041,0,0.692952,"model. Their solution is to eliminate such alignment pairs (their algorithm for doing so is discussed below). Unfortunately, this is a potentially a serious loss of information. Crossing alignments often correspond to systematic word order differences between languages (e.g., SVO vs. SOV languages). As such, leaving them out prevents useful information about entire subsets of POS types from exploiting of bilingual context. In the monolingual setting, Smith and Eisner (2005) showed similarly that a POS induction model can be improved with spelling features (prefixes and suffixes of words), and Haghighi and Klein (2006) describe an MRF-based monolingual POS induction model that uses features. An example of such a monolingual model is shown in Figure 2. Both papers developed different approximations of the computationally expensive partition function. Haghighi and Klein (2006) approximated by ignoring all sentences of length greater than some maximum, and the “contrastive estimation” of Smith and Eisner (2005) approximates the partition function with a set 65 A N V V Economic discrepancies are growing Figure 2: Monolingual MRF tag model (Haghighi and Klein, 2006) of automatically distorted training examples w"
W11-2208,2005.mtsummit-papers.11,0,0.106634,"Missing"
W11-2208,J93-2004,0,0.0361661,"Missing"
W11-2208,J03-1002,0,0.00473676,"Missing"
W11-2208,P05-1044,1,0.952248,"However, crossing alignment links pose a problem: they induce cycles in the tag sequence graph, which corresponds to an ill-defined probability model. Their solution is to eliminate such alignment pairs (their algorithm for doing so is discussed below). Unfortunately, this is a potentially a serious loss of information. Crossing alignments often correspond to systematic word order differences between languages (e.g., SVO vs. SOV languages). As such, leaving them out prevents useful information about entire subsets of POS types from exploiting of bilingual context. In the monolingual setting, Smith and Eisner (2005) showed similarly that a POS induction model can be improved with spelling features (prefixes and suffixes of words), and Haghighi and Klein (2006) describe an MRF-based monolingual POS induction model that uses features. An example of such a monolingual model is shown in Figure 2. Both papers developed different approximations of the computationally expensive partition function. Haghighi and Klein (2006) approximated by ignoring all sentences of length greater than some maximum, and the “contrastive estimation” of Smith and Eisner (2005) approximates the partition function with a set 65 A N V"
W11-2208,D08-1109,0,0.502551,"likelihood with joint MRFs suffers from a severe problem of local maxima, and suggest an alternative – using contrastive estimation for estimation of the parameters. Our experiments show that estimating the parameters this way, using overlapping features with joint MRFs performs better than previous work on the 1984 dataset. 1 Introduction This paper considers unsupervised learning of linguistic structure—specifically, parts of speech—in parallel text data. This setting, and more generally the multilingual learning scenario, has been found advantageous for a variety of unsupervised NLP tasks (Snyder et al., 2008; Cohen and Smith, 2010; Berg-Kirkpatrick et al., 2010; Das and Petrov, 2011). We consider globally normalized Markov random fields (MRFs) as an alternative to directed models based on multinomial distributions or locally normalized log-linear distributions. This alternate parameterization allows us to introduce correlated features that, at least in principle, depend on any parts of the hidden structure. Such models, sometimes called “undirected,” are widespread in supervised NLP; the most notable instances are conditional random fields (Lafferty et al., 2001), which have enabled rich feature"
W11-2208,N03-1033,0,\N,Missing
W11-2208,W06-2920,0,\N,Missing
W11-2208,N09-1009,1,\N,Missing
W11-2208,H94-1020,0,\N,Missing
W12-3203,P11-1135,0,0.0202624,"are not clearly an accurate description of his work at that time. More fine-grained modeling of time and also accounting for the death and birth of factions might ameliorate 29 these inconsistencies with our background knowledge about Charniak. The model finds that Aravind Joshi was associated with the tagging/parsing faction in the 1990s and in recent years moved back towards discourse (Prasad et al., 2008). David Yarowsky, known for his early work on word sense disambiguation, has since focused on applying word sense disambiguation techniques in a multilingual context (Garera et al., 2009; Bergsma et al., 2011). As mentioned in the previous section, we observe that the extended model is able to capture Daniel Marcu’s shift from discourse-related work to MT with his work in phrase-based statistical MT (Marcu and Wong, 2002). 5 Related Work A number of algorithms use topic modeling to analyze the text in the articles. Topic models such as latent Dirichlet allocation (Blei et al., 2003) and its variations have been increasingly used to study trends in scientific literature (McCallum et al., 2006; Dietz et al., 2007; Hall et al., 2008; Gerrish and Blei, 2010), predict citation information (McNee et al.,"
W12-3203,A00-2018,0,0.0344748,"3,874, 3,786, and 8,105 papers, respectively. Here we used G = 20 factions for faster runtime, leading to diminished interpretability, though the sparsity of the deviation vectors mitigates this problem somewhat. Figure 4 shows graphical plots of selected authors and their faction membership posteriors over time (drawn from the final E-step). With a simple extension of the original model, we can learn shifts in the subject area the author is publishing about. Consider Eugene Charniak: the model observed a major change in faction alignment around 2000, when one of the popular Charniak parsers (Charniak, 2000) was released; this is somewhat later than Charniak’s interests shifted, and the earlier faction’s words are not clearly an accurate description of his work at that time. More fine-grained modeling of time and also accounting for the death and birth of factions might ameliorate 29 these inconsistencies with our background knowledge about Charniak. The model finds that Aravind Joshi was associated with the tagging/parsing faction in the 1990s and in recent years moved back towards discourse (Prasad et al., 2008). David Yarowsky, known for his early work on word sense disambiguation, has since f"
W12-3203,W09-1117,0,0.0192651,"rlier faction’s words are not clearly an accurate description of his work at that time. More fine-grained modeling of time and also accounting for the death and birth of factions might ameliorate 29 these inconsistencies with our background knowledge about Charniak. The model finds that Aravind Joshi was associated with the tagging/parsing faction in the 1990s and in recent years moved back towards discourse (Prasad et al., 2008). David Yarowsky, known for his early work on word sense disambiguation, has since focused on applying word sense disambiguation techniques in a multilingual context (Garera et al., 2009; Bergsma et al., 2011). As mentioned in the previous section, we observe that the extended model is able to capture Daniel Marcu’s shift from discourse-related work to MT with his work in phrase-based statistical MT (Marcu and Wong, 2002). 5 Related Work A number of algorithms use topic modeling to analyze the text in the articles. Topic models such as latent Dirichlet allocation (Blei et al., 2003) and its variations have been increasingly used to study trends in scientific literature (McCallum et al., 2006; Dietz et al., 2007; Hall et al., 2008; Gerrish and Blei, 2010), predict citation inf"
W12-3203,D08-1038,0,0.252651,"thology. We extend the model to reveal changes in some authors’ faction memberships over time. 1 Introduction The ACL Anthology presents an excellent dataset for studying both the language and the social connections in our evolving research field. Extensive studies using techniques from the field of bibliometrics have been applied to this dataset (Radev et al., 2009a), quantifying the importance and impact factor of both authors and articles in the community. Moreover, recent work has leveraged the availability of digitized publications to study trends and influences within the ACL community (Hall et al., 2008; Gerrish and Blei, 2010; Yogatama et al., 2011) and to analyze academic collaborations (Johri et al., 2011). To the best of our knowledge, however, existing work has mainly pursued “macroscopic” investigations of the interaction of authors in collaboration, citation networks, or the textual content of whole papers. We seek to complement these results with a David A. Smith Department of Computer Science University of Massachusetts Amherst, MA 01003, USA dasmith@cs.umass.edu “microscopic” investigation of authors’ interactions by considering the individual sentences authors use to cite each oth"
W12-3203,W11-1516,0,0.40676,"ion The ACL Anthology presents an excellent dataset for studying both the language and the social connections in our evolving research field. Extensive studies using techniques from the field of bibliometrics have been applied to this dataset (Radev et al., 2009a), quantifying the importance and impact factor of both authors and articles in the community. Moreover, recent work has leveraged the availability of digitized publications to study trends and influences within the ACL community (Hall et al., 2008; Gerrish and Blei, 2010; Yogatama et al., 2011) and to analyze academic collaborations (Johri et al., 2011). To the best of our knowledge, however, existing work has mainly pursued “macroscopic” investigations of the interaction of authors in collaboration, citation networks, or the textual content of whole papers. We seek to complement these results with a David A. Smith Department of Computer Science University of Massachusetts Amherst, MA 01003, USA dasmith@cs.umass.edu “microscopic” investigation of authors’ interactions by considering the individual sentences authors use to cite each other. In this paper, we present a joint model of who cites whom in computational linguistics, and also of how"
W12-3203,W02-1018,0,0.12387,"cal MT and parsing factions in the bottom left exhibit higher citation activity amongst each other. In addition, we note that factions tend to self-cite more often than out of their own factions; this is unsurprising given the prior we selected. The IFC between discourse and MT2 (as shown by the edge thickness in figure 2) is higher than expected, given our prior knowledge of the computational linguistics community. Further investigation revealed that, Daniel Marcu, posited by our model to be a member of the discourse faction, has coauthored numerous highly cited papers in MT in recent years (Marcu and Wong, 2002). However, the model split the translation field, which fragmented the counts of MT related citation words. Thus, assigning Daniel Marcu to the discourse faction, which also has a less diverse citation vocabulary, is more probable than assigning him to one of the MT factions. In §4.6, we consider a model of factions over time to mitigate this problem. 4.5 Comparison to Graph Clustering Work in the field of bibliometrics has largely focused on using the link structure of citation networks to study higher level structures. See Osareh (1996) for a review. Popular methods include bibliographic cou"
W12-3203,prasad-etal-2008-penn,0,0.00873884,"major change in faction alignment around 2000, when one of the popular Charniak parsers (Charniak, 2000) was released; this is somewhat later than Charniak’s interests shifted, and the earlier faction’s words are not clearly an accurate description of his work at that time. More fine-grained modeling of time and also accounting for the death and birth of factions might ameliorate 29 these inconsistencies with our background knowledge about Charniak. The model finds that Aravind Joshi was associated with the tagging/parsing faction in the 1990s and in recent years moved back towards discourse (Prasad et al., 2008). David Yarowsky, known for his early work on word sense disambiguation, has since focused on applying word sense disambiguation techniques in a multilingual context (Garera et al., 2009; Bergsma et al., 2011). As mentioned in the previous section, we observe that the extended model is able to capture Daniel Marcu’s shift from discourse-related work to MT with his work in phrase-based statistical MT (Marcu and Wong, 2002). 5 Related Work A number of algorithms use topic modeling to analyze the text in the articles. Topic models such as latent Dirichlet allocation (Blei et al., 2003) and its va"
W12-3203,W09-3607,0,0.248063,"re closely within their faction, cite within the faction using language distinct from citation outside the faction, and be largely understandable through the language used when cited from without. We conduct an exploratory data analysis on the ACL Anthology. We extend the model to reveal changes in some authors’ faction memberships over time. 1 Introduction The ACL Anthology presents an excellent dataset for studying both the language and the social connections in our evolving research field. Extensive studies using techniques from the field of bibliometrics have been applied to this dataset (Radev et al., 2009a), quantifying the importance and impact factor of both authors and articles in the community. Moreover, recent work has leveraged the availability of digitized publications to study trends and influences within the ACL community (Hall et al., 2008; Gerrish and Blei, 2010; Yogatama et al., 2011) and to analyze academic collaborations (Johri et al., 2011). To the best of our knowledge, however, existing work has mainly pursued “macroscopic” investigations of the interaction of authors in collaboration, citation networks, or the textual content of whole papers. We seek to complement these resul"
W12-3203,D11-1055,1,0.883961,"Missing"
W12-4410,P10-4002,1,0.876198,"ce labeling model requires a runtime that is quadratic in the number of labels. Since our labels are character n-grams in the target language, we must cope with thousands of labels. To make the most of each inference call during training, we apply a mini-batch training algorithm which converges quickly. 66 Finally, we wish to consider some global features that would render exact inference intractable. We therefore use a reranking model (Collins, 2000). We demonstrate the performance benefits of these modifications on the Arabic-English transliteration task, using the open-source library cdec (Dyer et al., 2010)1 for learning and prediction. 2 Problem Description In the NEWS 2012 workshop, the task is to generate a list of ten transliterations in a specified target language for each named entity (in a known source language) in the test set. A training set is provided for each language pair. An entry in the training set comprises a named entity in the source language and one or more transliterations in the target language. Zhang et al. (2012) provides a detailed description of the shared task. 3 Approach 3.1 Character Alignment In order to extract source-target character mappings, we use m2m-aligner ("
W12-4410,N09-1046,1,0.852255,"possibility is to create multiple independent training inputs for each input x, one for each correct transliteration in Y ∗ (x). Using this approach, with K different transliterations, the CRF training objective will attempt to assign probability 1 K to each correct transliteration, and 0 to all others (modulo regularization). Alternatively, we can train the model to maximize the marginal probability assigned by the model to the set of correct labels Y ∗ = {y1 , . . . , yK }. That is, we assume a set of training data {(xj , Yj∗ )}`j=1 and replace the standard CRF objective with the following (Dyer, 2009):5 P P maxλ `j=1 log y∈Y ∗ pλ (y |xj ) − C||λ||22 (4) j This learning objective has more flexibility. It can maximize the likelihood of the training data by giving uniform probability to each reference transliteration for a given x, but it does not have to. In effect, we do not care how probability mass is distributed among the correct labels. Our hope is that if some transliterations are difficult to model—perhaps because they are incorrect—the model will be able to disregard them. To calculate the marginal probability for each xj , we represent Y ∗ (x) as a label lattice, which is supported"
W12-4410,I08-6006,0,0.276473,"raining objective that optimizes toward any of a set of possible correct labels (since more than one transliteration is often possible for a particular input), and a k-best reranking stage to incorporate nonlocal features. This paper presents results on the Arabic-English transliteration task of the NEWS 2012 workshop. 1 Introduction Transliteration is the transformation of a piece of text from one language’s writing system into another. Since the transformation is mostly explained as local substitutions, deletions, and insertions, we treat word transliteration as a sequence labeling problem (Ganesh et al., 2008; Reddy and Waxmonsky, 2009), using linear-chain conditional random fields as our model (Lafferty et al., 2001; Sha and Pereira, 2003). We tailor this model to the transliteration task in several ways. First, for the Arabic-English task, each Arabic input is paired with multiple valid English transliteration outputs, any of which is judged to be correct. To effectively exploit these multiple references during learning, we use a training objective in which the model may favor some correct transliterations over the others. Computationally efficient inference is achieved by encoding the reference"
W12-4410,W05-1506,0,0.0350846,"n estimate of plen (|y || |x|) assuming a multinomial distribution with parameters estimated using transliteration pairs of the training set. The probabilistic model for each of the global features is trained using training data provided for the shared task. The reranking score is a linear combination of log pcrf (y |x), log pcharLM (y), log pclassLM (y) and log plen (|y x|). Linear coefficients are optimized using simulated annealing, optimizing accuracy of the 1-best transliteration in a development set. k-best lists are extracted from the CRF trellis using the lazy enumeration algorithm of Huang and Chiang (2005). 4 Experiments We tested on the NEWS 2012 Arabic-English dataset. The train, development, and test sets consist of 27,177, 1,292, and 1,296 source named entities, respectively, with an average 9.6 references per name in each case. Table 4 summarizes our results using the ACC score (Zhang et al., 2012) (i.e., word accuracy in top-1). “Basic CRF” is the model with mini-batch learning and represents multiple reference transliterations as independent training examples. We manually tuned the number of training examples and LBFGS iterations per mini-batch to five and eight, respectively. “CRF w/lat"
W12-4410,N07-1047,0,0.0465979,"1 for learning and prediction. 2 Problem Description In the NEWS 2012 workshop, the task is to generate a list of ten transliterations in a specified target language for each named entity (in a known source language) in the test set. A training set is provided for each language pair. An entry in the training set comprises a named entity in the source language and one or more transliterations in the target language. Zhang et al. (2012) provides a detailed description of the shared task. 3 Approach 3.1 Character Alignment In order to extract source-target character mappings, we use m2m-aligner (Jiampojamarn et al., 2007),2 which implements a forward-backward algorithm to sum over probabilities of possible character sequence mappings, and uses Expectation Maximization to learn mapping probabilities. We allow source characters to be deleted, but not target characters. Parameters -maxX and -maxY are tuned on a devevelopment set. Our running example is the Arabic name EAdl (in Buckwalter’s ASCII-based encoding of Arabic) with two English transliterations: ADEL and ’ADIL. The character alignment for the two pairs is shown in Fig. 1. 1 2 http://www.cdec-decoder.org http://code.google.com/p/m2m-aligner Proceedings o"
W12-4410,W09-3520,0,0.114285,"t optimizes toward any of a set of possible correct labels (since more than one transliteration is often possible for a particular input), and a k-best reranking stage to incorporate nonlocal features. This paper presents results on the Arabic-English transliteration task of the NEWS 2012 workshop. 1 Introduction Transliteration is the transformation of a piece of text from one language’s writing system into another. Since the transformation is mostly explained as local substitutions, deletions, and insertions, we treat word transliteration as a sequence labeling problem (Ganesh et al., 2008; Reddy and Waxmonsky, 2009), using linear-chain conditional random fields as our model (Lafferty et al., 2001; Sha and Pereira, 2003). We tailor this model to the transliteration task in several ways. First, for the Arabic-English task, each Arabic input is paired with multiple valid English transliteration outputs, any of which is judged to be correct. To effectively exploit these multiple references during learning, we use a training objective in which the model may favor some correct transliterations over the others. Computationally efficient inference is achieved by encoding the references in a lattice. Second, infe"
W12-4410,N03-1028,0,0.0773347,"ble for a particular input), and a k-best reranking stage to incorporate nonlocal features. This paper presents results on the Arabic-English transliteration task of the NEWS 2012 workshop. 1 Introduction Transliteration is the transformation of a piece of text from one language’s writing system into another. Since the transformation is mostly explained as local substitutions, deletions, and insertions, we treat word transliteration as a sequence labeling problem (Ganesh et al., 2008; Reddy and Waxmonsky, 2009), using linear-chain conditional random fields as our model (Lafferty et al., 2001; Sha and Pereira, 2003). We tailor this model to the transliteration task in several ways. First, for the Arabic-English task, each Arabic input is paired with multiple valid English transliteration outputs, any of which is judged to be correct. To effectively exploit these multiple references during learning, we use a training objective in which the model may favor some correct transliterations over the others. Computationally efficient inference is achieved by encoding the references in a lattice. Second, inference for our first-order sequence labeling model requires a runtime that is quadratic in the number of la"
W12-4410,W12-4401,0,\N,Missing
W13-2307,2020.lrec-1.643,0,0.293721,"Missing"
W13-2307,P99-1010,0,0.418476,"fficiently mature annotation conventions, or actual ambiguity in the sentence. On the other hand, annotators may be indifferent to certain phenomena. This can happen for a variety of reasons: • Some projects may only need annotations of specific constructions. For example, building a semantic resource for events may require annotation of syntactic verb-argument relations, but not internal noun phrase structure. • As a project matures, it may be more useful to annotate only infrequent lexical items. • Semisupervised learning from partial annotations may be sufficient to learn complete parsers (Hwa, 1999; Clark and Curran, 2006). • Beginning annotators may wish to focus on easily understood syntactic phenomena. • Different members of a project may wish to specialize in different syntactic phenomena, reducing training cost and cognitive load. Rather than treating annotations as invalid unless and until they are complete trees, we formally represent and reason about partial parse structures. Annotators produce annotations, which encode constraints on the (inferred) analysis, the parse structure, of a sentence. We say that a valid annotation supports (is compatible with) one or more analyses. Bo"
W13-2307,N06-1019,0,0.236476,"mature annotation conventions, or actual ambiguity in the sentence. On the other hand, annotators may be indifferent to certain phenomena. This can happen for a variety of reasons: • Some projects may only need annotations of specific constructions. For example, building a semantic resource for events may require annotation of syntactic verb-argument relations, but not internal noun phrase structure. • As a project matures, it may be more useful to annotate only infrequent lexical items. • Semisupervised learning from partial annotations may be sufficient to learn complete parsers (Hwa, 1999; Clark and Curran, 2006). • Beginning annotators may wish to focus on easily understood syntactic phenomena. • Different members of a project may wish to specialize in different syntactic phenomena, reducing training cost and cognitive load. Rather than treating annotations as invalid unless and until they are complete trees, we formally represent and reason about partial parse structures. Annotators produce annotations, which encode constraints on the (inferred) analysis, the parse structure, of a sentence. We say that a valid annotation supports (is compatible with) one or more analyses. Both annotations and analys"
W13-2307,W07-2416,0,0.0563019,"ator labels are as in table 2. Per-annotator com (with lexical reconciliation) and inter-annotator softComPrec are aggregated over sentences by arithmetic mean. less burdensome, and the specialized annotations did prove complementary to each other.19 5.4 Treebank Comparison Though the annotators in our study were native speakers well acquainted with representations of English syntax, we sought to quantify their agreement with the expert treebankers who created the EWTB (the source of the Reviews sentences). We converted the EWTB’s constituent parses to dependencies via the PennConverter tool (Johansson and Nugues, 2007),20 then removed punctuation. Agreement with the converted treebank parses appears in the bottom two rows of table 3. Because the EWTB commits to a single analysis, precision scores are quite lopsided. Most of its attachments are consistent with our annotations (softComPrec &gt; 0.9), but these allow many additional analyses (hence the scores below 0.5). Annotator Specialization As an experiment in using underspecification for labor division, two of the annotators of Reviews data were assigned specific linguistic phenomena to focus on. Annotator “D” was tasked with the internal structure of base"
W13-2307,P06-2066,0,0.0538705,"nd compare underspecified annotations. 1 Introduction Computational representations for natural language syntax are borne of competing design considerations. When designing such representations, there may be a tradeoff between parsimony and expressiveness. A range of linguistic theories attract support due to differing purposes and aesthetic principles (Chomsky, 1957; Tesnière, 1959; Hudson, 1984; Sgall et al., 1986; Mel’ˇcuk, 1988, inter alia). Formalisms concerned with tractable computation may care chiefly about learnability or parsing efficiency (Shieber, 1992; Sleator and Temperly, 1993; Kuhlmann and Nivre, 2006). Further considerations may include psychological and evolutionary plausibility (Croft, 2001; Tomasello, 2003; Steels et al., 2011; Fossum and Levy, 2012), integration with other representations such as semantics (Steedman, 2000; Bergen and Chang, 2005), or suitability for particular applications (e.g., translation). Here we elevate ease of annotation as a primary design concern for a syntactic annotation formalism. Currently, a lack of annotated data is a huge bottleneck for robust NLP, standing in the way of parsers for social media text (Foster et al., 2011) and many low-resourced language"
W13-2307,W12-1706,0,0.033779,"When designing such representations, there may be a tradeoff between parsimony and expressiveness. A range of linguistic theories attract support due to differing purposes and aesthetic principles (Chomsky, 1957; Tesnière, 1959; Hudson, 1984; Sgall et al., 1986; Mel’ˇcuk, 1988, inter alia). Formalisms concerned with tractable computation may care chiefly about learnability or parsing efficiency (Shieber, 1992; Sleator and Temperly, 1993; Kuhlmann and Nivre, 2006). Further considerations may include psychological and evolutionary plausibility (Croft, 2001; Tomasello, 2003; Steels et al., 2011; Fossum and Levy, 2012), integration with other representations such as semantics (Steedman, 2000; Bergen and Chang, 2005), or suitability for particular applications (e.g., translation). Here we elevate ease of annotation as a primary design concern for a syntactic annotation formalism. Currently, a lack of annotated data is a huge bottleneck for robust NLP, standing in the way of parsers for social media text (Foster et al., 2011) and many low-resourced languages (to name two examples). Traditional syntactic annotation projects like the Penn Treebank (Marcus ∗ 2 A Dependency Grammar for Annotation Although depende"
W13-2307,J93-2004,0,0.0444543,"portion of the English Web Treebank 14 Malagasy is a VOS Austronesian language spoken by 15 million people, mostly in Madagascar. Kinyarwanda is an SVO Bantu language spoken by 12 million people mostly in Rwanda. All annotations were done by native speakers of English. The Kinyarwanda and Malagasy annotators had basic proficiency in these languages. 15 As a point of comparison, during the Penn Treebank project, annotators corrected the syntactic bracketings produced by a high-quality hand-written parser (Fidditch) and achieved a rate of only 375 tokens/hour using a specialized GUI interface (Marcus et al., 1993). 16 Included with the data and software release (footnote 1). 57 com thus reduces the commitment averages for each annotation—to a greater extent for annotator “A” (.96 in table 2 vs. .82 in table 3) because “A” marked more multiwords. An analysis fully compatible with both annotations exists for only 27/60 sentences; the finer-grained softComPrec measure (§4.2), however, offers insight into the balance between commitment and agreement. Qualitatively, we observe three leading causes of incompatibilities (disagreements): obvious annotator mistakes (such as the marked as a head); inconsistent h"
W13-2307,1993.iwpt-1.22,0,0.0410599,"lop algorithms to evaluate and compare underspecified annotations. 1 Introduction Computational representations for natural language syntax are borne of competing design considerations. When designing such representations, there may be a tradeoff between parsimony and expressiveness. A range of linguistic theories attract support due to differing purposes and aesthetic principles (Chomsky, 1957; Tesnière, 1959; Hudson, 1984; Sgall et al., 1986; Mel’ˇcuk, 1988, inter alia). Formalisms concerned with tractable computation may care chiefly about learnability or parsing efficiency (Shieber, 1992; Sleator and Temperly, 1993; Kuhlmann and Nivre, 2006). Further considerations may include psychological and evolutionary plausibility (Croft, 2001; Tomasello, 2003; Steels et al., 2011; Fossum and Levy, 2012), integration with other representations such as semantics (Steedman, 2000; Bergen and Chang, 2005), or suitability for particular applications (e.g., translation). Here we elevate ease of annotation as a primary design concern for a syntactic annotation formalism. Currently, a lack of annotated data is a huge bottleneck for robust NLP, standing in the way of parsers for social media text (Foster et al., 2011) and"
W13-2307,D07-1014,1,0.853574,"edges that are known to be incompatible with the annotation before searching for spanning trees. Our “upward-downward” method for constructing a graph of supported edges first enumerates a set of candidate top nodes for every fudge expression, then uses that information to infer a set of supported parents for every node.12 The supported edge graph then consists of vertices lexnodes(A) ∪ {root} and edges S 0 0 v∈lexnodes(A) {(v → v ) ∀ v ∈ suppParentsA (v)}. From this graph we can count all directed spanning trees in cubic time using Kirchhoff’s matrix tree theorem (Chaiken and Kleitman, 1978; Smith and Smith, 2007; Margoliash, 2010).13 If some lexical node has no supported parents, this reflects conflicting constraints in the annotation, and no spanning tree will be found. Promiscuity will tend to be higher for longer sentences. To control for this, we define a second quantity, the annotation’s commitment quotient (commitment being the opposite of promiscuity), 4.2 Inter-Annotator Agreement FUDG can encode flat groupings and coreference at the lexical level, as well as syntactic structure over lexical items. Inter-annotator agreement can be measured separately for each of these facets. Pilot annotator"
W13-2307,D08-1027,1,0.400347,"Missing"
W13-2307,N13-1039,1,0.775169,"Missing"
W13-2307,W13-2307,1,0.0512826,"Missing"
W14-1615,J99-1004,0,0.168391,"istributions are constructed to build in additional inductive bias. 3.1 The first idea subsumes the complexity measure used by Baldridge, but accomplishes the goal naturally by letting the probabilities decrease as the category grows. The rate of decay is governed by the pterm parameter: the marginal probability of generating a terminal (atomic) category in each expansion. A higher pterm means a stronger emphasis on simplicity. The probability distribution over categories is guaranteed to be proper so long as pterm > 12 since the probability of the depth of a tree will decrease geometrically (Chi, 1999). The second idea is a natural extension of the complexity concept and is particularly relevant when features are used. The original complexity measure treated all atoms uniformly, but e.g. we would expect NPexpl / N to be less likely than NP / N since it contains the more specialized, and thus rarer, atom NPexpl . We define the distribution patom (a) as the prior over atomic categories. Due to our weak, type-only supervision, we have to estimate patom from just the tag dictionary and raw corpus, without frequency data. Our goal is to estimate the number of each atom in the supertags that shou"
W14-1615,P11-1061,0,0.050044,"Missing"
W14-1615,D12-1075,1,0.875226,"es using FFBS. To sample a tagging for a sentence x, the strategy is to inductively compute, for each token xi starting with i = 0 and going “forward”, the probability of generating x0 , x1 , . . . , xi via any tag sequence that ends with yi = u: Emission Prior Means (φ0t ) For each supertag type t, φ0t is the mean distribution over words it emits. While Baldridge’s approach used a uniform emission initialization, treating all words as equally likely, we can, again, induce token-level corpus-specific information:5 To set φ0t , we use a variant and simplification of the procedure introduced by Garrette and Baldridge (2012) that takes advantage of our prior over categories PG . Assuming that C(w) is the count of word type w in the raw corpus, TD(w) is the set of supertags associated with word type w in the tag dictionary, and TD(t) is the set of known word types associated with supertag t, the count of word/tag pairs for known words (words appearing in the tag dictionary) is estimated by uniformly distributing a word’s (δ-smoothed) raw counts over its tag dictionary entries: ( C(w)+δ |TD (w) |if t ∈ TD (w) Cknown (t, w) = 0 otherwise For unknown words, we first use the idea of tag “openness” to estimate the like"
W14-1615,N13-1014,1,0.879438,"Missing"
W14-1615,P13-1057,1,0.870138,"Missing"
W14-1615,P08-1085,0,0.018326,"s work that used similar biases to initialize an EM-based learner. Additional gains are obtained by further shaping the prior with corpus-specific information that is extracted automatically from raw text and a tag dictionary. 1 Noah A. Smith† Introduction Unsupervised part-of-speech (POS) induction is a classic problem in NLP. Many proposed solutions are based on Hidden Markov models (HMMs), with various improvements obtainable through: inductive bias in the form of tag dictionaries (Kupiec, 1992; Merialdo, 1994), sparsity constraints (Lee et al., 2010), careful initialization of parameters (Goldberg et al., 2008), feature based representations (Berg-Kirkpatrick et al., 2010; Smith and Eisner, 2005), and priors on model parameters (Johnson, 2007; Goldwater and Griffiths, 2007; Blunsom and Cohn, 2011, inter alia). When tag dictionaries are available, a situation we will call type-supervision, POS induction from unlabeled corpora can be relatively successful; however, as the number of possible tags increases, performance drops (Ravi and Knight, 141 Proceedings of the Eighteenth Conference on Computational Language Learning, pages 141–150, c Baltimore, Maryland USA, June 26-27 2014. 2014 Association for C"
W14-1615,C08-1008,1,0.200491,". Therefore, it is unsurprising that the unsupervised (or even weaklysupervised) learning of a Combinatory Categorial Grammar (CCG) supertagger, which labels each word with one of a large (possibly unbounded) number of structured categories called supertags, is a considerable challenge. Despite the apparent complexity of the task, supertag sequences have regularities due to universal properties of the CCG formalism (§2) that can be used to reduce the complexity of the problem; previous work showed promising results by using these regularities to initialize an HMM that is then refined with EM (Baldridge, 2008). Here, we exploit CCG’s category structure to motivate a novel prior over HMM parameters for use in Bayesian learning (§3). This prior encourages (i) crosslinguistically common tag types, (ii) tag bigrams that can combine using CCG’s combinators, and (iii) sparse transition distributions. We also go beyond the use of these universals to show how additional, corpus-specific information can be automatically extracted from a combination of the tag dictionary and raw data, and how that information can be combined with the universal knowledge for integration into the model to improve the prior. We"
W14-1615,P07-1094,0,0.241725,"that is extracted automatically from raw text and a tag dictionary. 1 Noah A. Smith† Introduction Unsupervised part-of-speech (POS) induction is a classic problem in NLP. Many proposed solutions are based on Hidden Markov models (HMMs), with various improvements obtainable through: inductive bias in the form of tag dictionaries (Kupiec, 1992; Merialdo, 1994), sparsity constraints (Lee et al., 2010), careful initialization of parameters (Goldberg et al., 2008), feature based representations (Berg-Kirkpatrick et al., 2010; Smith and Eisner, 2005), and priors on model parameters (Johnson, 2007; Goldwater and Griffiths, 2007; Blunsom and Cohn, 2011, inter alia). When tag dictionaries are available, a situation we will call type-supervision, POS induction from unlabeled corpora can be relatively successful; however, as the number of possible tags increases, performance drops (Ravi and Knight, 141 Proceedings of the Eighteenth Conference on Computational Language Learning, pages 141–150, c Baltimore, Maryland USA, June 26-27 2014. 2014 Association for Computational Linguistics 2 X/Y Y X/Y Y  Y /Z CCG and Supertagging CCG (Steedman, 2000; Steedman and Baldridge, 2011) is a grammar formalism in which each lexical t"
W14-1615,J99-2004,0,0.0338277,"ase, etc), or a complex category formed from the combination of two categories by one of two slash operators. In CCG, complex categories indicate a grammatical relationship between the two operands. For example, the category ( SNP )/ NP might describe a transitive verb, looking first to its right (indicated by /) for an object, then to its left () for a subject, to produce a sentence. Further, atomic categories may be augmented with features, such as Sdcl , to restrict the set of atoms with which they may unify. The task of assigning a category to each word in a text is called supertagging (Bangalore and Joshi, 1999). Because they are recursively defined, there is an infinite number of potential CCG categories (though in practice it is limited by the number of actual grammatical contexts). As a result, the number of supertags appearing in a corpus far exceeds the number of POS tags (see Table 1). Since supertags specify the grammatical context of a token, and high frequency words appear in many contexts, CCG grammars tend to have very high lexical ambiguity, with frequent word types associating with a large number of categories. This ambiguity has made type-supervised supertagger learning very difficult b"
W14-1615,J07-3004,0,0.183225,"5k 60k 9k ambiguity type token 3.75 13.11 56.98 296.18 96.58 323.37 178.88 426.13 dev tokens — 128k 59k 5k test tokens — 127k 85k 5k Table 1: Statistics for the various corpora used. CCGBank is English, CCG-CTB is Chinese, and TUT is Italian. The number of tags includes only those tags found in the tag dictionary (TD). Ambiguity rates are the average number of entries in the unpruned tag dictionary for each word in the raw corpus. English POS statistics are shown only for comparison; only CCG experiments were run. πt ∼ Dir hαπ · πt0 (u) + C(t, u)iu∈T To evaluate our approach, we used CCGBank (Hockenmaier and Steedman, 2007), which is a transformation of the English Penn Treebank (Marcus et al., 1993); the CTB-CCG (Tse and Curran, 2010) transformation of the Penn Chinese Treebank (Xue et al., 2005); and the CCG-TUT corpus (Bos et al., 2009), built from the TUT corpus of Italian text (Bosco et al., 2000). Statistics on the size and ambiguity of these datasets are shown in Table 1. For CCGBank, sections 00–15 were used for extracting the tag dictionary, 16–18 for the raw corpus, 19–21 for development data, and 22–24 for test data. For TUT, the first 150 sentences of each of the CIVIL LAW and NEWSPAPER sections were"
W14-1615,N10-1083,0,0.0462617,"Missing"
W14-1615,N09-1036,0,0.0282852,"ld be automatically extracted from the weak supervision that is available: the raw corpus and the tag dictionary. This allows us to combine the cross-linguistic properties of the CCG formalism with corpus- or language-specific information in the data into a single, unified Bayesian prior. Our model uses a relatively large number of parameters, e.g., pterm , pfw , pmod , patom , in the prior. Here, we fixed each to a single value (i.e., a “fully Bayesian” approach). Future work might explore sensitivity to these choices, or empirical Bayesian or maximum a posteriori inference for their values (Johnson and Goldwater, 2009). Related Work Ravi et al. (2010) also improved upon the work by Baldridge (2008) by using integer linear programming to find a minimal model of supertag transitions, thereby generating a better starting point for EM than the grammatical constraints alone could provide. This approach is complementary to the work presented here, and because we have shown that our work yields gains under tag dictionaries of various levels of cleanliness, it is probable that employing minimization to set the base distribution for sampling could lead to still higher gains. On the Bayesian side, Van Gael et al. (20"
W14-1615,Q13-1007,0,0.0671236,"ere, and because we have shown that our work yields gains under tag dictionaries of various levels of cleanliness, it is probable that employing minimization to set the base distribution for sampling could lead to still higher gains. On the Bayesian side, Van Gael et al. (2009) used a non-parametric, infinite HMM for truly unsupervised POS-tagger learning (Van Gael et al., 2008; Beal et al., 2001). While their model is not restricted to the standard set of POS tags, and may learn a more fine-grained set of labels, the induced labels are arbitrary and not grounded in any grammatical formalism. Bisk and Hockenmaier (2013) developed an approach to CCG grammar induction that does not use a tag dictionary. Like ours, their procedure learns from general properties of the CCG formalism. However, while our work is intended to produce categories that match those used in a particular training corpus, however complex they might be, their work produces categories in a simplified form of CCG in which N and S are the only atoms 148 In this work, as in most type-supervised work, the tag dictionary was automatically extracted from an existing tagged corpus. However, a tag dictionary could instead be automatically induced vi"
W14-1615,D07-1031,0,0.022783,"fic information that is extracted automatically from raw text and a tag dictionary. 1 Noah A. Smith† Introduction Unsupervised part-of-speech (POS) induction is a classic problem in NLP. Many proposed solutions are based on Hidden Markov models (HMMs), with various improvements obtainable through: inductive bias in the form of tag dictionaries (Kupiec, 1992; Merialdo, 1994), sparsity constraints (Lee et al., 2010), careful initialization of parameters (Goldberg et al., 2008), feature based representations (Berg-Kirkpatrick et al., 2010; Smith and Eisner, 2005), and priors on model parameters (Johnson, 2007; Goldwater and Griffiths, 2007; Blunsom and Cohn, 2011, inter alia). When tag dictionaries are available, a situation we will call type-supervision, POS induction from unlabeled corpora can be relatively successful; however, as the number of possible tags increases, performance drops (Ravi and Knight, 141 Proceedings of the Eighteenth Conference on Computational Language Learning, pages 141–150, c Baltimore, Maryland USA, June 26-27 2014. 2014 Association for Computational Linguistics 2 X/Y Y X/Y Y  Y /Z CCG and Supertagging CCG (Steedman, 2000; Steedman and Baldridge, 2011) is a grammar fo"
W14-1615,P11-1087,0,0.0131501,"y from raw text and a tag dictionary. 1 Noah A. Smith† Introduction Unsupervised part-of-speech (POS) induction is a classic problem in NLP. Many proposed solutions are based on Hidden Markov models (HMMs), with various improvements obtainable through: inductive bias in the form of tag dictionaries (Kupiec, 1992; Merialdo, 1994), sparsity constraints (Lee et al., 2010), careful initialization of parameters (Goldberg et al., 2008), feature based representations (Berg-Kirkpatrick et al., 2010; Smith and Eisner, 2005), and priors on model parameters (Johnson, 2007; Goldwater and Griffiths, 2007; Blunsom and Cohn, 2011, inter alia). When tag dictionaries are available, a situation we will call type-supervision, POS induction from unlabeled corpora can be relatively successful; however, as the number of possible tags increases, performance drops (Ravi and Knight, 141 Proceedings of the Eighteenth Conference on Computational Language Learning, pages 141–150, c Baltimore, Maryland USA, June 26-27 2014. 2014 Association for Computational Linguistics 2 X/Y Y X/Y Y  Y /Z CCG and Supertagging CCG (Steedman, 2000; Steedman and Baldridge, 2011) is a grammar formalism in which each lexical token is associated with"
W14-1615,D10-1083,0,0.016251,"show that it yields substantial improvements over previous work that used similar biases to initialize an EM-based learner. Additional gains are obtained by further shaping the prior with corpus-specific information that is extracted automatically from raw text and a tag dictionary. 1 Noah A. Smith† Introduction Unsupervised part-of-speech (POS) induction is a classic problem in NLP. Many proposed solutions are based on Hidden Markov models (HMMs), with various improvements obtainable through: inductive bias in the form of tag dictionaries (Kupiec, 1992; Merialdo, 1994), sparsity constraints (Lee et al., 2010), careful initialization of parameters (Goldberg et al., 2008), feature based representations (Berg-Kirkpatrick et al., 2010; Smith and Eisner, 2005), and priors on model parameters (Johnson, 2007; Goldwater and Griffiths, 2007; Blunsom and Cohn, 2011, inter alia). When tag dictionaries are available, a situation we will call type-supervision, POS induction from unlabeled corpora can be relatively successful; however, as the number of possible tags increases, performance drops (Ravi and Knight, 141 Proceedings of the Eighteenth Conference on Computational Language Learning, pages 141–150, c Ba"
W14-1615,J93-2004,0,0.0456766,"ens — 128k 59k 5k test tokens — 127k 85k 5k Table 1: Statistics for the various corpora used. CCGBank is English, CCG-CTB is Chinese, and TUT is Italian. The number of tags includes only those tags found in the tag dictionary (TD). Ambiguity rates are the average number of entries in the unpruned tag dictionary for each word in the raw corpus. English POS statistics are shown only for comparison; only CCG experiments were run. πt ∼ Dir hαπ · πt0 (u) + C(t, u)iu∈T To evaluate our approach, we used CCGBank (Hockenmaier and Steedman, 2007), which is a transformation of the English Penn Treebank (Marcus et al., 1993); the CTB-CCG (Tse and Curran, 2010) transformation of the Penn Chinese Treebank (Xue et al., 2005); and the CCG-TUT corpus (Bos et al., 2009), built from the TUT corpus of Italian text (Bosco et al., 2000). Statistics on the size and ambiguity of these datasets are shown in Table 1. For CCGBank, sections 00–15 were used for extracting the tag dictionary, 16–18 for the raw corpus, 19–21 for development data, and 22–24 for test data. For TUT, the first 150 sentences of each of the CIVIL LAW and NEWSPAPER sections were used for raw data, the next sentences 150– 249 of each was used for developme"
W14-1615,bosco-etal-2000-building,0,0.500961,"the tag dictionary (TD). Ambiguity rates are the average number of entries in the unpruned tag dictionary for each word in the raw corpus. English POS statistics are shown only for comparison; only CCG experiments were run. πt ∼ Dir hαπ · πt0 (u) + C(t, u)iu∈T To evaluate our approach, we used CCGBank (Hockenmaier and Steedman, 2007), which is a transformation of the English Penn Treebank (Marcus et al., 1993); the CTB-CCG (Tse and Curran, 2010) transformation of the Penn Chinese Treebank (Xue et al., 2005); and the CCG-TUT corpus (Bos et al., 2009), built from the TUT corpus of Italian text (Bosco et al., 2000). Statistics on the size and ambiguity of these datasets are shown in Table 1. For CCGBank, sections 00–15 were used for extracting the tag dictionary, 16–18 for the raw corpus, 19–21 for development data, and 22–24 for test data. For TUT, the first 150 sentences of each of the CIVIL LAW and NEWSPAPER sections were used for raw data, the next sentences 150– 249 of each was used for development, and the sentences 250–349 were used for test; the remaining data, 457 sentences from CIVIL LAW and 548 from NEWSPAPER, plus the much smaller 132sentence JRC ACQUIS data, was used for the tag dictionary."
W14-1615,J94-2001,0,0.352076,"of the CCG formalism. Empirically, we show that it yields substantial improvements over previous work that used similar biases to initialize an EM-based learner. Additional gains are obtained by further shaping the prior with corpus-specific information that is extracted automatically from raw text and a tag dictionary. 1 Noah A. Smith† Introduction Unsupervised part-of-speech (POS) induction is a classic problem in NLP. Many proposed solutions are based on Hidden Markov models (HMMs), with various improvements obtainable through: inductive bias in the form of tag dictionaries (Kupiec, 1992; Merialdo, 1994), sparsity constraints (Lee et al., 2010), careful initialization of parameters (Goldberg et al., 2008), feature based representations (Berg-Kirkpatrick et al., 2010; Smith and Eisner, 2005), and priors on model parameters (Johnson, 2007; Goldwater and Griffiths, 2007; Blunsom and Cohn, 2011, inter alia). When tag dictionaries are available, a situation we will call type-supervision, POS induction from unlabeled corpora can be relatively successful; however, as the number of possible tags increases, performance drops (Ravi and Knight, 141 Proceedings of the Eighteenth Conference on Computation"
W14-1615,P09-1057,0,0.0422068,"Missing"
W14-1615,P10-1051,1,0.81725,"supervision that is available: the raw corpus and the tag dictionary. This allows us to combine the cross-linguistic properties of the CCG formalism with corpus- or language-specific information in the data into a single, unified Bayesian prior. Our model uses a relatively large number of parameters, e.g., pterm , pfw , pmod , patom , in the prior. Here, we fixed each to a single value (i.e., a “fully Bayesian” approach). Future work might explore sensitivity to these choices, or empirical Bayesian or maximum a posteriori inference for their values (Johnson and Goldwater, 2009). Related Work Ravi et al. (2010) also improved upon the work by Baldridge (2008) by using integer linear programming to find a minimal model of supertag transitions, thereby generating a better starting point for EM than the grammatical constraints alone could provide. This approach is complementary to the work presented here, and because we have shown that our work yields gains under tag dictionaries of various levels of cleanliness, it is probable that employing minimization to set the base distribution for sampling could lead to still higher gains. On the Bayesian side, Van Gael et al. (2009) used a non-parametric, infini"
W14-1615,P05-1044,1,0.751187,"e obtained by further shaping the prior with corpus-specific information that is extracted automatically from raw text and a tag dictionary. 1 Noah A. Smith† Introduction Unsupervised part-of-speech (POS) induction is a classic problem in NLP. Many proposed solutions are based on Hidden Markov models (HMMs), with various improvements obtainable through: inductive bias in the form of tag dictionaries (Kupiec, 1992; Merialdo, 1994), sparsity constraints (Lee et al., 2010), careful initialization of parameters (Goldberg et al., 2008), feature based representations (Berg-Kirkpatrick et al., 2010; Smith and Eisner, 2005), and priors on model parameters (Johnson, 2007; Goldwater and Griffiths, 2007; Blunsom and Cohn, 2011, inter alia). When tag dictionaries are available, a situation we will call type-supervision, POS induction from unlabeled corpora can be relatively successful; however, as the number of possible tags increases, performance drops (Ravi and Knight, 141 Proceedings of the Eighteenth Conference on Computational Language Learning, pages 141–150, c Baltimore, Maryland USA, June 26-27 2014. 2014 Association for Computational Linguistics 2 X/Y Y X/Y Y  Y /Z CCG and Supertagging CCG (Steedman, 2000"
W14-1615,C10-1122,0,0.194953,"k 85k 5k Table 1: Statistics for the various corpora used. CCGBank is English, CCG-CTB is Chinese, and TUT is Italian. The number of tags includes only those tags found in the tag dictionary (TD). Ambiguity rates are the average number of entries in the unpruned tag dictionary for each word in the raw corpus. English POS statistics are shown only for comparison; only CCG experiments were run. πt ∼ Dir hαπ · πt0 (u) + C(t, u)iu∈T To evaluate our approach, we used CCGBank (Hockenmaier and Steedman, 2007), which is a transformation of the English Penn Treebank (Marcus et al., 1993); the CTB-CCG (Tse and Curran, 2010) transformation of the Penn Chinese Treebank (Xue et al., 2005); and the CCG-TUT corpus (Bos et al., 2009), built from the TUT corpus of Italian text (Bosco et al., 2000). Statistics on the size and ambiguity of these datasets are shown in Table 1. For CCGBank, sections 00–15 were used for extracting the tag dictionary, 16–18 for the raw corpus, 19–21 for development data, and 22–24 for test data. For TUT, the first 150 sentences of each of the CIVIL LAW and NEWSPAPER sections were used for raw data, the next sentences 150– 249 of each was used for development, and the sentences 250–349 were u"
W14-1615,D09-1071,0,0.0533631,"Missing"
W14-1615,W12-3127,0,0.0384305,"Missing"
W16-5807,D15-1041,1,0.856653,"n studied by Mandal et al. (2015) in the context of question answering and by King and Abney (2013). Both used primarily character n-gram features. Several other studies have investigated the use of character sequence models in language processing. These techniques were first applied only to create word embeddings (dos Santos and Zadrozny, 2015; dos Santos and Guimaraes, 2015) and then later extended to have the word embeddings feed directly into a word-level RNN. Applications include partof-speech (POS) tagging (Ling et al., 2015b), language modeling (Ling et al., 2015a), dependency parsing (Ballesteros et al., 2015), translation (Ling et al., 2015b), and slot filling text analysis (Jaech et al., 2016a). A more extensive discussion of related work on language ID and character sequence models can be found in Jaech et al. (2016b). 6 Conclusion We present C2V2L, a hierarchical neural model for language ID that preforms competitively on challenging word-level language ID tasks. Without feature engineering, we achieved the best performance in two common categories and good results in two others. Future work could include adapting C2V2L for other sequence labeling tasks, having shown that the current architectu"
W16-5807,W15-3904,0,0.0252191,"results.html 63 English-Spanish and English-Nepali in the EMNLP 2014 Language Identitication in Code-Switched Data (Solorio et al., 2014), using an RNN with skipgram word embeddings and character n-gram features. Word-level language ID has also been studied by Mandal et al. (2015) in the context of question answering and by King and Abney (2013). Both used primarily character n-gram features. Several other studies have investigated the use of character sequence models in language processing. These techniques were first applied only to create word embeddings (dos Santos and Zadrozny, 2015; dos Santos and Guimaraes, 2015) and then later extended to have the word embeddings feed directly into a word-level RNN. Applications include partof-speech (POS) tagging (Ling et al., 2015b), language modeling (Ling et al., 2015a), dependency parsing (Ballesteros et al., 2015), translation (Ling et al., 2015b), and slot filling text analysis (Jaech et al., 2016a). A more extensive discussion of related work on language ID and character sequence models can be found in Jaech et al. (2016b). 6 Conclusion We present C2V2L, a hierarchical neural model for language ID that preforms competitively on challenging word-level language"
W16-5807,W16-6212,1,0.914402,"in social media text where informal styles, closely related language pairs, and code-switching are common. Progress on language ID is needed especially since downstream tasks, like translation or semantic parsing, depend on it. Continuous representations for language data, which have produced new states of the art for language modeling (Mikolov et al., 2010), machine translation (Bahdanau et al., 2015), and other tasks, can be useful for language ID. For the Language Identification in Code-Switched Data shared task (LICS 2016), we submitted a hierarchical characterword model closely following Jaech et al. (2016b), focusing on word-level language ID. Our discussion of the model closely follows that paper. This model, which we call C2V2L (“character to vector to language”) is hierarchical in the sense that it explicitly builds a continuous representation for each word from its character sequence, capturing orthographic and morphology-related patterns, Model Our model has two main components, though they are trained together, end-to-end. The first, “char2vec,” applies a convolutional neural network (CNN) to a whitespace-delimited word’s Unicode character sequence, providing a word vector. The second is"
W16-5807,N13-1131,0,0.0417997,"ious work on the text domain mostly uses word or character ngram features combined with linear classifiers (Hurtado et al., 2014; Gamallo et al., 2014). Chang and Lin (2014) outperformed the top results for 2 Full results can be found at http://care4lang1. seas.gwu.edu/cs2/results.html 63 English-Spanish and English-Nepali in the EMNLP 2014 Language Identitication in Code-Switched Data (Solorio et al., 2014), using an RNN with skipgram word embeddings and character n-gram features. Word-level language ID has also been studied by Mandal et al. (2015) in the context of question answering and by King and Abney (2013). Both used primarily character n-gram features. Several other studies have investigated the use of character sequence models in language processing. These techniques were first applied only to create word embeddings (dos Santos and Zadrozny, 2015; dos Santos and Guimaraes, 2015) and then later extended to have the word embeddings feed directly into a word-level RNN. Applications include partof-speech (POS) tagging (Ling et al., 2015b), language modeling (Ling et al., 2015a), dependency parsing (Ballesteros et al., 2015), translation (Ling et al., 2015b), and slot filling text analysis (Jaech"
W16-5807,D15-1176,0,0.165722,"nsion of y is 3n2 , corresponding to the number of filters used. Similar to Kim et al. (2016) who use a highway network after the max-pooling layer, we apply a residual network layer. The residual network uses a matrix W ∈ R3n2 ×3n2 and bias vector b3 to create the vector z = y + fR (y) where fR (y) = ReLU(Wy + b3 ). The resulting vector z is used as a word embedding vector in the word-level LSTM portion of the model. There are three differences between our version of the model and the one described by Kim et al. (2016). First, we use two layers of convolution instead of just one, inspired by Ling et al. (2015a) which uses a 2-layer LSTM for character modeling. Second, we use the ReLU function as a nonlinearity as opposed to the tanh function. ReLU has been highly successful in computer vision in conjunction with convolutional layers (Jarrett et al., 2009). Finally, we use a residual network layer instead of a highway network layer after the max-pooling step, to reduce the model size. 2.2 Sentence-level Context The sequence of word embedding vectors is processed by a bi-LSTM, which outputs a sequence of 61 Figure 1: C2V2L model architecture. The model takes the word “esfuezo,” a misspelling of the"
W16-6212,N10-1027,0,0.0248549,"pose a challenge to language identification. We introduce a hierarchical model that learns character and contextualized word-level representations for language identification. Our method performs well against strong baselines, and can also reveal code-switching. 1 Introduction Language identification (language ID), despite being described as a solved problem more than ten years ago (McNamee, 2005), remains a difficult problem. Particularly when working with short texts, informal styles, or closely related language pairs, it is an active area of research (Gella et al., 2014; Wang et al., 2015; Baldwin and Lui, 2010). These difficult cases are often found in social media content. Progress on language ID is needed especially since downstream tasks, like translation and semantic parsing, depend on correct language ID. This paper brings continuous representations for language data, which have produced new states of the art for language modeling (Mikolov et al., 2010), machine translation (Bahdanau et al., 2015), and other tasks, to language ID. We adapt a hierarchical character-word neural architecture from Kim et al. (2016), demonstrating that it works well for language ID. Our model, which we call C2V2L (“"
W16-6212,D15-1041,1,0.824864,"rs, published datasets quickly go out of date when the tweets in question are no longer available online, making it difficult to compare against prior work. Several other studies have investigated the use of character sequence models in language processing. These techniques were first used only to create word embeddings (dos Santos and Zadrozny, 2015; dos Santos and Guimaraes, 2015) and then later extended to have the word embeddings feed directly into a word-level RNN. Applications include partof-speech tagging (Ling et al., 2015b), language modeling (Ling et al., 2015a), dependency parsing (Ballesteros et al., 2015), translation (Ling et al., 2015b), and slot filling text analysis (Jaech et al., 2016a). The work is divided in terms of whether the character sequence is modeled with an LSTM or CNN, though virtually all now leverage the resulting word vectors in a word-level RNN. We are not aware of prior results comparing LSTMs and CNNs on a specific task, but the reduction in model size com91 Chang and Lin (2014) outperformed the top results for English-Spanish and English-Nepali in the EMNLP 2014 Language Identification in CodeSwitched Data (Solorio et al., 2014), using an RNN with skipgram word embeddin"
W16-6212,W12-2108,0,0.193237,"ds from the training data and their cosine similarities for inputs “couldn’t”, “@maria_sanchez”, and “noite”. Model langid.py 5-gram LM C2V2L (ours) F1 87.9 93.8 91.2 pared to word-only systems is reported to be much higher for LSTM architectures. All analyses report that the greatest improvements in performance from character sequence models are for infrequent and previously unseen words, as expected. Table 6: F1 scores on the Twitter70 dataset. with information from the Twitter social graph improves language ID on TweetLID from 74.7 to 76.6 F1 , only slightly better than our result of 76.2. Bergsma et al. (2012) created their own multilingual Twitter dataset and tested both a discriminative model based on n-grams plus hand-crafted features and a compression-based classifier. Since the Twitter API requires researchers to re-download tweets based on their identifiers, published datasets quickly go out of date when the tweets in question are no longer available online, making it difficult to compare against prior work. Several other studies have investigated the use of character sequence models in language processing. These techniques were first used only to create word embeddings (dos Santos and Zadroz"
W16-6212,W15-3904,0,0.0325553,"own multilingual Twitter dataset and tested both a discriminative model based on n-grams plus hand-crafted features and a compression-based classifier. Since the Twitter API requires researchers to re-download tweets based on their identifiers, published datasets quickly go out of date when the tweets in question are no longer available online, making it difficult to compare against prior work. Several other studies have investigated the use of character sequence models in language processing. These techniques were first used only to create word embeddings (dos Santos and Zadrozny, 2015; dos Santos and Guimaraes, 2015) and then later extended to have the word embeddings feed directly into a word-level RNN. Applications include partof-speech tagging (Ling et al., 2015b), language modeling (Ling et al., 2015a), dependency parsing (Ballesteros et al., 2015), translation (Ling et al., 2015b), and slot filling text analysis (Jaech et al., 2016a). The work is divided in terms of whether the character sequence is modeled with an LSTM or CNN, though virtually all now leverage the resulting word vectors in a word-level RNN. We are not aware of prior results comparing LSTMs and CNNs on a specific task, but the reduct"
W16-6212,W14-5151,0,0.015758,"s’ brevity and unconventional spelling pose a challenge to language identification. We introduce a hierarchical model that learns character and contextualized word-level representations for language identification. Our method performs well against strong baselines, and can also reveal code-switching. 1 Introduction Language identification (language ID), despite being described as a solved problem more than ten years ago (McNamee, 2005), remains a difficult problem. Particularly when working with short texts, informal styles, or closely related language pairs, it is an active area of research (Gella et al., 2014; Wang et al., 2015; Baldwin and Lui, 2010). These difficult cases are often found in social media content. Progress on language ID is needed especially since downstream tasks, like translation and semantic parsing, depend on correct language ID. This paper brings continuous representations for language data, which have produced new states of the art for language modeling (Mikolov et al., 2010), machine translation (Bahdanau et al., 2015), and other tasks, to language ID. We adapt a hierarchical character-word neural architecture from Kim et al. (2016), demonstrating that it works well for lan"
W16-6212,N16-1155,0,0.0356935,"Missing"
W16-6212,D15-1240,1,0.849284,"d in the Wikipedia data, is ignored, then there is a net performance gain. In Table 5, we show the top seven neighbors to selected input words based on cosine similarity. In the left column we see that words with similar features, such as the presence of the “n’t” contraction, can be grouped together by char2vec. In the middle column, an out-of-vocabulary username is supplied and similar usernames are retrieved. When working with n-gram features, removing usernames is common, but some previous work demonstrates that they still carry useful information for predicting the language of the tweet (Jaech and Ostendorf, 2015). The third example,“noite” (Portuguese for “night”), shows that the word embeddings are largely invariant to changes in punctuation and capitalization. 5.3 Twitter70 We compare C2V2L to langid.py and the 5gram language model on the Twitter70 dataset; see Table 6. Although the 5-gram model achieves the best performance, the results are virtually identical to those for C2V2L except for the closely-related Bosnian-Croatian language pair. The lowest performance for all the models is on closely related language pairs. For example, using the C2V2L model, the F1 score for Danish is only 62.7 due to"
W16-6212,W16-5807,1,0.837176,"able online, making it difficult to compare against prior work. Several other studies have investigated the use of character sequence models in language processing. These techniques were first used only to create word embeddings (dos Santos and Zadrozny, 2015; dos Santos and Guimaraes, 2015) and then later extended to have the word embeddings feed directly into a word-level RNN. Applications include partof-speech tagging (Ling et al., 2015b), language modeling (Ling et al., 2015a), dependency parsing (Ballesteros et al., 2015), translation (Ling et al., 2015b), and slot filling text analysis (Jaech et al., 2016a). The work is divided in terms of whether the character sequence is modeled with an LSTM or CNN, though virtually all now leverage the resulting word vectors in a word-level RNN. We are not aware of prior results comparing LSTMs and CNNs on a specific task, but the reduction in model size com91 Chang and Lin (2014) outperformed the top results for English-Spanish and English-Nepali in the EMNLP 2014 Language Identification in CodeSwitched Data (Solorio et al., 2014), using an RNN with skipgram word embeddings and character ngram features. Word-level language ID has also been studied by Manda"
W16-6212,N13-1131,0,0.0833342,"quence is modeled with an LSTM or CNN, though virtually all now leverage the resulting word vectors in a word-level RNN. We are not aware of prior results comparing LSTMs and CNNs on a specific task, but the reduction in model size com91 Chang and Lin (2014) outperformed the top results for English-Spanish and English-Nepali in the EMNLP 2014 Language Identification in CodeSwitched Data (Solorio et al., 2014), using an RNN with skipgram word embeddings and character ngram features. Word-level language ID has also been studied by Mandal et al. (2015) in the context of question answering and by King and Abney (2013). Both used primarily character n-gram features, which are well motivated for code-switching tasks since the presence of multiple languages increases the odds of encountering a previously unseen word. 7 Conclusion We present C2V2L, a hierarchical neural model for language ID that outperforms previous work on the challenging TweetLID task. We also find that smoothed character n-gram language models can work well as classifiers for language ID for short texts. Without feature engineering, our n-gram baseline beat eleven out of the twelve submissions in the TweetLID shared task, and gives the bes"
W16-6212,D15-1176,0,0.126313,"etwork layers allow values from the previous layer to pass through unchanged but the residual layer is preferred in our case because it uses half as many parameters (He et al., 2015). The residual network uses a matrix W ∈ R3n2 ×3n2 and bias vector b3 to create the vector z = y + fR (y) where fR (y) = ReLU(Wy + b3 ). The resulting vector z is used as a word embedding vector in the word-level LSTM portion of the model. There are three differences between our version of the model and the one described by Kim et al. (2016). First, we use two layers of convolution instead of just one, inspired by Ling et al. (2015a) 85 who used a 2-layer LSTM for character modeling. Second, we use the ReLU function as a nonlinearity as opposed to the tanh function. ReLU has been highly successful in computer vision applications in conjunction with convolutional layers (Jarrett et al., 2009). Finally, we use a residual network layer instead of a highway network layer after the maxpooling step, to reduce the model size. Figure 1: C2V2L model architecture. The model takes the (misspelled) word “esfuezo,” and produces a word vector via the two CNN layers and the residual layer. The word vectors are then combined via the LS"
W16-6212,P12-3005,0,0.113067,"ayers, the size of the word-level LSTM vector, and the dropout rate. The selected values are listed in Table 2. 5 Experiments For all the studies below on language identification, we compare to two baselines: i) langid.py, a popular open-source language ID package, and ii) a classifier using n-gram character language models. For the TweetLID dataset, additional comparisons are included as described next. In addition, we test our model’s word-level performance on a codeswitching dataset. The first baseline, based on the langid.py package, uses a naïve Bayes classifier over byte ngram features (Lui and Baldwin, 2012). The pretrained model distributed with the package is designed to perform well on a wide range of domains, and achieved high performance on “microblog messages” (tweets) in the original paper. langid.py uses feature selection for domain adaptation and to reduce the model size; thus, retraining it on indomain data as we do in this paper does not provide an entirely fair comparison. However, we include it for its popularity and importance. The second baseline is built from character ngram language models. It assigns each tweet according to language `∗ = arg max` p(tweet |`), i.e., applying Baye"
W16-6212,W15-1703,0,0.0134668,"ventional spelling pose a challenge to language identification. We introduce a hierarchical model that learns character and contextualized word-level representations for language identification. Our method performs well against strong baselines, and can also reveal code-switching. 1 Introduction Language identification (language ID), despite being described as a solved problem more than ten years ago (McNamee, 2005), remains a difficult problem. Particularly when working with short texts, informal styles, or closely related language pairs, it is an active area of research (Gella et al., 2014; Wang et al., 2015; Baldwin and Lui, 2010). These difficult cases are often found in social media content. Progress on language ID is needed especially since downstream tasks, like translation and semantic parsing, depend on correct language ID. This paper brings continuous representations for language data, which have produced new states of the art for language modeling (Mikolov et al., 2010), machine translation (Bahdanau et al., 2015), and other tasks, to language ID. We adapt a hierarchical character-word neural architecture from Kim et al. (2016), demonstrating that it works well for language ID. Our model"
W16-6212,N15-1109,0,\N,Missing
W17-0907,P82-1020,0,0.813873,"Missing"
W17-0907,W07-0602,0,0.0823614,". (2017). 2 System Description We design a system that predicts, given a pair of story endings, which is the right one and which is the wrong one. Our system applies a linear classifier guided by several types of features to solve the task. We describe the system in detail below. 52 Proceedings of the 2nd Workshop on Linking Models of Lexical, Sentential and Discourse-level Semantics, pages 52–55, c Valencia, Spain, April 3, 2017. 2017 Association for Computational Linguistics 2.1 Model (Argamon et al., 2003; Schler et al., 2006; Bamman et al., 2014), and native language (Koppel et al., 2005; Tsur and Rappoport, 2007; Bergsma et al., 2012). We add the following classification features to capture style differences between the two endings. These features are computed on the story endings alone (right or wrong), and do not consider, either at train or at test time, the first four (shared) sentences of each story. We train a binary logistic regression classifier to distinguish between right and wrong stories. We use the set of right stories as positive samples and the set of wrong stories as negative samples. At test time, for a given pair, we consider the classification results of both candidates. If our cla"
W17-0907,N16-1098,0,0.0403179,"Missing"
W17-0907,W11-1515,1,0.892048,"Missing"
W17-0907,P11-1077,0,0.0656157,"rring less than 3 times by a special out-of-vocabulary character, yielding a vocabulary size of 21,582. Only during training, we apply a (1) The intuition is that a correct ending should be unsurprising (to the model) given the four preceding sentences of the story (the numerator), controlling for the inherent surprise of the words in that ending (the denominator).1 Stylistic features. We hypothesize that right and wrong endings might be distinguishable using style features. We adopt style features that have been shown useful in the past in tasks such as detection of age (Schler et al., 2006; Rosenthal and McKeown, 2011; Nguyen et al., 2011), gender 2 www.nltk.org/api/nltk.tokenize.html www.tensorflow.org 4 We train on both the Spring 2016 and the Winter 2017 datasets, a total of roughly 100K stories. 3 1 Note that taking the logarithm of the expression in Equation 1 gives the pointwise mutual information between the story and the ending, under the language model. 53 Model DSSM (Mostafazadeh et al., 2016) LexVec (Salle et al., 2016) RNNLM features Stylistic features Combined (Style + RNNLM) Human judgment Acc. 0.585 0.599 0.677 0.724 0.752 1.000 style on the authors, which is expressed in the different style"
W17-0907,D13-1193,1,0.0470041,", we keep them. If not, the label whose posterior probability is lower is reversed. We describe the classification features below. 2.2 • Length. The number of words in the sentence. Features We use two types of features, designed to capture different aspects of the problem. We use neural language model features to leverage corpus level word distributions, specifically longer term sequence probabilities. We use stylistic features to capture differences in writing between coherent story endings and incoherent ones. • Word n-grams. We use sequences of 1– 5 words. Following Tsur et al. (2010) and Schwartz et al. (2013), we distinguish between high frequency and low frequency words. Specifically, we replace content words, which are often low frequency, with their part-of-speech tags (Nouns, Verbs, Adjectives, and Adverbs). Language model features. We experiment with state-of-the-art text comprehension models, specifically an LSTM (Hochreiter and Schmidhuber, 1997) recurrent neural network language model (RNNLM; Mikolov et al., 2010). Our RNNLM is used to generate two different probabilities: pθ (ending), which is the language model probability of the fifth sentence alone and pθ (ending |story), which is the"
W17-0907,K17-1004,1,0.671994,"Missing"
W18-1206,P16-1225,0,0.0942725,"Missing"
W18-1206,N16-1038,0,0.0678252,"Missing"
W18-1206,D14-1162,0,0.101227,"ure vectors that encode the submorphemes occurring in a surface form. We use sparse regularization to select relevant features from this model, which enables it to automatically choose a subset of the submorpheme features that predict the vectors (our predicted phonesthemes). Specifically, we regularize our linear regression model with the elastic net (Zou and Hastie, 2005). We used scikit-learn (Pedregosa et al., 2011) to train our models, and we tune the L1 and L2 regularization strengths on held-out error in 5-fold cross-validation. 3 Data For our experiments, we use 300-dimensional GloVe (Pennington et al., 2014) English word embeddings trained on the cased Common Crawl. Many of the terms in the set of pretrained vectors are not English words. As a first attempt toward removing non-English words and named entities, we discard types that are not alphabetical or not completely lowercased. In addition, it’s unlikely that rare words or very common words will contribute to the formation of sound-meaning associations (Hutchins, 1998). To further filter these rare or common words (and remove additional nonEnglish types), we remove types that either occur less than 1000 times in the Gigaword corpus or in more"
W18-3024,P17-1080,0,0.0468882,"t LSTMs are able to count infinitely, since their cell states are unbounded, while GRUs cannot count infinitely since the activations are constrained to a finite range. One avenue of future work could compare the performance of LSTMs and GRUs on the memorization task. Past studies have also investigated what information RNNs encode by directly examining hidden unit activations (Karpathy et al., 2016; Li et al., 2016; Shi et al., 2016a, among others) and by training an auxiliary classifier to predict various properties of interest from hidden state vectors (Shi et al., 2016b; Adi et al., 2017; Belinkov et al., 2017, among others). Validation Test 0 10 20 30 # Epochs 40 50 Figure 4: Model validation and test accuracy over time during training. Validation improves faster than test, indicating that the model exploits linguistic properties of the data during training. Figure 4 shows that models trained on the unigram and language datasets converge to high validation accuracy faster than high test accuracy. This suggests that models trained on data with linguistic attributes first learn to do well on the training data by exploiting the properties of language and not truly memorizing. Perhaps the model genera"
W18-3024,D16-1248,0,0.206153,"ical structure of language 1 This distribution is adversarial with respect to the Zipfian and natural language training sets. 180 Proceedings of the 3rd Workshop on Representation Learning for NLP, pages 180–186 c Melbourne, Australia, July 20, 2018. 2018 Association for Computational Linguistics paths for the model to minimize its loss, thus offering more training signal than the uniform case, in which the only way to reduce the loss is to learn the memorization function. We further inspect how the LSTM solves the memorization task, and find that some hidden units count the number of inputs. Shi et al. (2016a) analyzed LSTM encoder-decoder translation models and found that similar counting neurons regulate the length of generated translations. Since LSTMs better memorize (and thus better count) on language data than on non-language data, and counting plays a role in encoder-decoder models, our work could also lead to improved training for sequence-to-sequence models in non-language applications (e.g., Schwaller et al., 2017). vocabulary. Our goal is to evaluate the memorization ability of the LSTM, so we freeze the weights of the embedding matrix and the linear output projection during training."
W18-3024,D16-1159,0,0.258984,"ical structure of language 1 This distribution is adversarial with respect to the Zipfian and natural language training sets. 180 Proceedings of the 3rd Workshop on Representation Learning for NLP, pages 180–186 c Melbourne, Australia, July 20, 2018. 2018 Association for Computational Linguistics paths for the model to minimize its loss, thus offering more training signal than the uniform case, in which the only way to reduce the loss is to learn the memorization function. We further inspect how the LSTM solves the memorization task, and find that some hidden units count the number of inputs. Shi et al. (2016a) analyzed LSTM encoder-decoder translation models and found that similar counting neurons regulate the length of generated translations. Since LSTMs better memorize (and thus better count) on language data than on non-language data, and counting plays a role in encoder-decoder models, our work could also lead to improved training for sequence-to-sequence models in non-language applications (e.g., Schwaller et al., 2017). vocabulary. Our goal is to evaluate the memorization ability of the LSTM, so we freeze the weights of the embedding matrix and the linear output projection during training."
W18-3024,P18-2117,0,0.0961116,"counting frequent words or phrases. In the uniform setting, the model has only one path to success: true memorization, and it cannot find an effective way to reduce the loss. In other words, linguistic structure and the patterns of language may provide additional signals that correlate with the label and facilitate learning the memorization task. 183 Accuracy 100 90 80 70 60 50 40 30 20 10 0 ture of language. Blevins et al. (2018) show that the internal representations of LSTMs encode syntactic information, even when trained without explicit syntactic supervision. Also related is the work of Weiss et al. (2018), who demonstrate that LSTMs are able to count infinitely, since their cell states are unbounded, while GRUs cannot count infinitely since the activations are constrained to a finite range. One avenue of future work could compare the performance of LSTMs and GRUs on the memorization task. Past studies have also investigated what information RNNs encode by directly examining hidden unit activations (Karpathy et al., 2016; Li et al., 2016; Shi et al., 2016a, among others) and by training an auxiliary classifier to predict various properties of interest from hidden state vectors (Shi et al., 2016"
W18-3024,N18-1108,0,0.051157,"Missing"
W18-3024,P82-1020,0,0.756047,"Missing"
W18-3024,N16-1082,0,0.0475318,"t the internal representations of LSTMs encode syntactic information, even when trained without explicit syntactic supervision. Also related is the work of Weiss et al. (2018), who demonstrate that LSTMs are able to count infinitely, since their cell states are unbounded, while GRUs cannot count infinitely since the activations are constrained to a finite range. One avenue of future work could compare the performance of LSTMs and GRUs on the memorization task. Past studies have also investigated what information RNNs encode by directly examining hidden unit activations (Karpathy et al., 2016; Li et al., 2016; Shi et al., 2016a, among others) and by training an auxiliary classifier to predict various properties of interest from hidden state vectors (Shi et al., 2016b; Adi et al., 2017; Belinkov et al., 2017, among others). Validation Test 0 10 20 30 # Epochs 40 50 Figure 4: Model validation and test accuracy over time during training. Validation improves faster than test, indicating that the model exploits linguistic properties of the data during training. Figure 4 shows that models trained on the unigram and language datasets converge to high validation accuracy faster than high test accuracy. Th"
W18-3024,Q16-1037,0,0.0939639,"Missing"
W18-3024,J93-2004,0,0.0609389,"set the forget gate to 0 and the input gate to 1).3 All input sequences at train and test time are of equal length. To explore the effect of sequence length on LSTM task performance, we experiment with different input sequence lengths (10, 20, 40, 60, . . . , 300). 3 • In the uniform setup, each token in the training dataset is randomly sampled from a uniform distribution over the vocabulary. • In the unigram setup, we modify the uniform data by integrating the Zipfian token frequencies found in natural language data. The input sequences are taken from a modified version of the Penn Treebank (Marcus et al., 1993) with randomly permuted tokens. • In the 5gram, 10gram, and 50gram settings, we seek to augment the unigram setting with Markovian dependencies. We generate the dataset by grouping the tokens of the Penn Treebank into 5, 10, or 50-length chunks and randomly permuting these chunks. Experimental Setup We modify the linguistic properties of the training data and observe the effects on model performance. Further details are found in Appendix A, and we release code for reproducing our results.4 • In the language setup, we assess the effect of using real language. The input sequences here are taken"
W19-4302,marelli-etal-2014-sick,0,0.0276838,"Missing"
W19-4302,I05-5002,0,0.162595,"se, the sentence representation is typically provided as input to a linear classifier ( ). LM pretraining with has also been successfully applied to sentence-level tasks. Howard and Ruder (2018, ULMFiT) propose techniques for fine-tuning a LM, including triangular learning rate schedules and discriminative finetuning, which uses lower learning rates for lower layers. Radford et al. (2018) extend LM- to additional sentence and sentence-pair tasks. PD For paraphrase detection (i.e., decide whether two sentences are semantically equivalent), we use the Microsoft Research Paraphrase Corpus (MRPC; Dolan and Brockett, 2005). SA We use the binary version of the Stanford Sentiment Treebank (SST-2; Socher et al., 2013), providing sentiment labels (negative or positive) for sentences of movie reviews. NLI We use both the broad-domain MultiNLI dataset (Williams et al., 2018) and Sentences Involving Compositional Knowledge (SICK-E; Marelli et al., 2014). STS We employ the Semantic Textual Similarity Benchmark (STS-B; Cer et al., 2017) and SICKR (Marelli et al., 2014). Both datasets provide a similarity value from 1 to 5 for each sentence pair. 3.2 We now describe how we adapt ELMo and BERT to these tasks. For we requi"
W19-4302,D16-1046,0,0.0375898,", 2015), which employs a next-sentence prediction objective similar to BERT. Both ELMo and BERT outperform the sentence embedding method significantly, except on the semantic textual similarity tasks (STS) where Skipthoughts is similar to ELMo. The overall performance of and shows small differences except for a few notable cases. For ELMo, we find the largest differences for sentence pair tasks where consistently outperforms . For BERT, we obtain nearly the opposite result: significantly outperforms on all STS tasks, with much smaller differences for the others. 5 Discussion Past work in NLP (Mou et al., 2016) showed that similar pretraining tasks transfer better.1 In computer vision (CV), Yosinski et al. (2014) similarly found that the transferability of features decreases as the distance between the pretraining and target task increases. In this vein, Skip-thoughts—and Quick-thoughts (Logeswaran and Lee, 2018), which has similar performance— which use a next-sentence prediction objective Analyses Modelling pairwise interactions LSTMs consider each token sequentially, while Transformers can relate each token to every other in each layer (Vaswani et al., 2017). This might facilitate with Transforme"
W19-4302,P18-1031,1,0.923864,"y improved over noncontextual vectors. notations of newswire across four different entity types (PER, LOC, ORG, MISC). Sentence embedding methods Such methods learn sentence representations via different pretraining objectives such as previous/next sentence prediction (Kiros et al., 2015; Logeswaran and Lee, 2018), NLI (Conneau et al., 2017), or a combination of objectives (Subramanian et al., 2018). During the adaptation phase, the sentence representation is typically provided as input to a linear classifier ( ). LM pretraining with has also been successfully applied to sentence-level tasks. Howard and Ruder (2018, ULMFiT) propose techniques for fine-tuning a LM, including triangular learning rate schedules and discriminative finetuning, which uses lower learning rates for lower layers. Radford et al. (2018) extend LM- to additional sentence and sentence-pair tasks. PD For paraphrase detection (i.e., decide whether two sentences are semantically equivalent), we use the Microsoft Research Paraphrase Corpus (MRPC; Dolan and Brockett, 2005). SA We use the binary version of the Stanford Sentiment Treebank (SST-2; Socher et al., 2013), providing sentiment labels (negative or positive) for sentences of movie"
W19-4302,D14-1181,0,0.00879252,"duce universal representations suitable for any downstream task. The first two authors contributed equally. Sebastian is now affiliated with DeepMind. 7 Proceedings of the 4th Workshop on Representation Learning for NLP (RepL4NLP-2019), pages 7–14 c Florence, Italy, August 2, 2019. 2019 Association for Computational Linguistics Word representations Pretrained word vectors (Turian et al., 2010; Pennington et al., 2014) have been an essential component in state-of-the-art NLP systems. Word representations are often fixed and fed into a task specific model ( ), although can provide improvements (Kim, 2014). Recently, contextual word representations learned supervisedly (e.g., through MT; McCann et al., 2017) or unsupervisedly (typically through language modeling; Peters et al., 2018) have significantly improved over noncontextual vectors. notations of newswire across four different entity types (PER, LOC, ORG, MISC). Sentence embedding methods Such methods learn sentence representations via different pretraining objectives such as previous/next sentence prediction (Kiros et al., 2015; Logeswaran and Lee, 2018), NLI (Conneau et al., 2017), or a combination of objectives (Subramanian et al., 2018"
W19-4302,D14-1162,0,0.0766308,"Missing"
W19-4302,N18-1202,1,0.932679,"f adaptation guidelines for the NLP practitioner. 1 Any Any Any ELMo BERT Any Add many task parameters Any Add minimal task parameters Hyper-parameters Seq. / clas. Sent. pair Sent. pair and use use have similar performance computationally cheaper as features only need to be computed once. On the other hand, is convenient as it may allow us to adapt a general-purpose representation to many different tasks. Gaining a better understanding of the adaptation phase is key in making the most use out of pretrained representations. To this end, we compare two state-of-the-art pretrained models, ELMo (Peters et al., 2018) and BERT (Devlin et al., 2018) using both and across seven diverse tasks including named entity recognition, natural language inference (NLI), and paraphrase detection. We seek to characterize the conditions under which one approach substantially outperforms the other, and whether it is dependent on the pretraining objective or target task. We find that and have comparable performance in most cases, except when the source and target tasks are either highly similar or highly dissimilar. We furthermore shed light on the practical challenges of adaptation and provide a set of guidelines to the N"
W19-4302,N03-1017,0,0.0208465,"ch the model learns a generalpurpose representation of inputs, and adaptation, in which the representation is transferred to a new task. Most previous work in NLP has focused on pretraining objectives for learning word or sentence representations (Mikolov et al., 2013; Kiros et al., 2015). Few works, however, have focused on the adaptation phase. There are two main paradigms for adaptation: feature extraction and fine-tuning. In feature extraction ( ) the model’s weights are ‘frozen’ and the pretrained representations are used in a downstream model similar to classic feature-based approaches (Koehn et al., 2003). Alternatively, a pretrained model’s parameters can be unfrozen and fine-tuned ( ) on a new task (Dai and Le, 2015). Both have benefits: enables use of task-specific model architectures and may be † Any Any Any Guidelines Table 1: This paper’s guidelines for using feature extraction ( ) and fine-tuning ( ) with ELMo and BERT. Seq.: sequence labeling. Clas.: classification. Sent. pair: sentence pair tasks. Introduction ? Conditions Adapt. Task 2 Pretraining and Adaptation In this work, we focus on pretraining tasks that seek to induce universal representations suitable for any downstream task."
W19-4302,N19-5004,1,0.763114,"ition, natural language inference (NLI), and paraphrase detection. We seek to characterize the conditions under which one approach substantially outperforms the other, and whether it is dependent on the pretraining objective or target task. We find that and have comparable performance in most cases, except when the source and target tasks are either highly similar or highly dissimilar. We furthermore shed light on the practical challenges of adaptation and provide a set of guidelines to the NLP practitioner, as summarized in Table 1. Sequential inductive transfer learning (Pan and Yang, 2010; Ruder, 2019) consists of two stages: pretraining, in which the model learns a generalpurpose representation of inputs, and adaptation, in which the representation is transferred to a new task. Most previous work in NLP has focused on pretraining objectives for learning word or sentence representations (Mikolov et al., 2013; Kiros et al., 2015). Few works, however, have focused on the adaptation phase. There are two main paradigms for adaptation: feature extraction and fine-tuning. In feature extraction ( ) the model’s weights are ‘frozen’ and the pretrained representations are used in a downstream model s"
W19-4302,D17-1038,1,0.632924,"JS div TE GO TR FI SL 84.4 -1.1 0.21 86.7 -0.2 0.18 86.1 -0.6 0.14 84.5 0.4 0.09 80.9 -0.6 0.09 Table 6: Accuracy of feature extraction ( ) and difference compared to fine-tuning ( ) with BERT-base trained on training data of different MNLI domains and evaluated on corresponding dev sets. TE: telephone. FI: fiction. TR: travel. GO: government. SL: slate. Impact of Target Domain Pretrained language model representations are intended to be universal. However, the target domain might still impact the adaptation performance. We calculate the Jensen-Shannon divergence based on term distributions (Ruder and Plank, 2017) between the domains used to train BERT (books and Wikipedia) and each MNLI domain. We show results in Table 6. We find no significant correlation. At least for this task, the distance of the source and target domains does not seem to have a major impact on the adaptation performance. Impact of additional parameters We evaluate whether adding parameters is useful for both adaptation settings on NER. We add a CRF layer (as used in ) and a BiLSTM with a CRF layer (as used in ) to both and show results in Table 5. We find that additional parameters are key for , but hurt performance with .3 In ad"
W19-4302,D13-1170,0,0.00970417,"aining with has also been successfully applied to sentence-level tasks. Howard and Ruder (2018, ULMFiT) propose techniques for fine-tuning a LM, including triangular learning rate schedules and discriminative finetuning, which uses lower learning rates for lower layers. Radford et al. (2018) extend LM- to additional sentence and sentence-pair tasks. PD For paraphrase detection (i.e., decide whether two sentences are semantically equivalent), we use the Microsoft Research Paraphrase Corpus (MRPC; Dolan and Brockett, 2005). SA We use the binary version of the Stanford Sentiment Treebank (SST-2; Socher et al., 2013), providing sentiment labels (negative or positive) for sentences of movie reviews. NLI We use both the broad-domain MultiNLI dataset (Williams et al., 2018) and Sentences Involving Compositional Knowledge (SICK-E; Marelli et al., 2014). STS We employ the Semantic Textual Similarity Benchmark (STS-B; Cer et al., 2017) and SICKR (Marelli et al., 2014). Both datasets provide a similarity value from 1 to 5 for each sentence pair. 3.2 We now describe how we adapt ELMo and BERT to these tasks. For we require a task-specific architecture, while for we need a task-specific output layer. For fair comp"
W19-4302,W18-5446,0,0.0773103,"Missing"
W19-4302,N18-1101,0,0.0234461,"triangular learning rate schedules and discriminative finetuning, which uses lower learning rates for lower layers. Radford et al. (2018) extend LM- to additional sentence and sentence-pair tasks. PD For paraphrase detection (i.e., decide whether two sentences are semantically equivalent), we use the Microsoft Research Paraphrase Corpus (MRPC; Dolan and Brockett, 2005). SA We use the binary version of the Stanford Sentiment Treebank (SST-2; Socher et al., 2013), providing sentiment labels (negative or positive) for sentences of movie reviews. NLI We use both the broad-domain MultiNLI dataset (Williams et al., 2018) and Sentences Involving Compositional Knowledge (SICK-E; Marelli et al., 2014). STS We employ the Semantic Textual Similarity Benchmark (STS-B; Cer et al., 2017) and SICKR (Marelli et al., 2014). Both datasets provide a similarity value from 1 to 5 for each sentence pair. 3.2 We now describe how we adapt ELMo and BERT to these tasks. For we require a task-specific architecture, while for we need a task-specific output layer. For fair comparison, we conduct an extensive hyper-parameter search for each task. Feature extraction ( ) For both ELMo and BERT, we extract contextual representations of"
W19-4302,W03-0419,0,\N,Missing
W19-4302,P10-1040,0,\N,Missing
W19-4302,N16-1030,0,\N,Missing
W19-4302,P17-1152,0,\N,Missing
W19-4302,S17-2001,0,\N,Missing
W19-4302,D17-1169,0,\N,Missing
W19-4302,W18-2501,1,\N,Missing
