2020.findings-emnlp.358,Effects of Naturalistic Variation in Goal-Oriented Dialog,2020,-1,-1,2,0,4161,jatin ganhotra,Findings of the Association for Computational Linguistics: EMNLP 2020,0,"Existing benchmarks used to evaluate the performance of end-to-end neural dialog systems lack a key component: natural variation present in human conversations. Most datasets are constructed through crowdsourcing, where the crowd workers follow a fixed template of instructions while enacting the role of a user/agent. This results in straight-forward, somewhat routine, and mostly trouble-free conversations, as crowd workers do not think to represent the full range of actions that occur naturally with real users. In this work, we investigate the impact of naturalistic variation on two goal-oriented datasets: bAbI dialog task and Stanford Multi-Domain Dataset (SMD). We also propose new and more effective testbeds for both datasets, by introducing naturalistic variation by the user. We observe that there is a significant drop in performance (more than 60{\%} in Ent. F1 on SMD and 85{\%} in per-dialog accuracy on bAbI task) of recent state-of-the-art end-to-end neural methods such as BossNet and GLMP on both datasets."
D15-1151,An Improved Tag Dictionary for Faster Part-of-Speech Tagging,2015,11,6,1,1,19878,robert moore,Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,0,"Ratnaparkhi (1996) introduced a method of inferring a tag dictionary from annotated data to speed up part-of-speech tagging by limiting the set of possible tags for each word. While Ratnaparkhixe2x80x99s tag dictionary makes tagging faster but less accurate, an alternative tag dictionary that we recently proposed (Moore, 2014) makes tagging as fast as with Ratnaparkhixe2x80x99s tag dictionary, but with no decrease in accuracy. In this paper, we show that a very simple semi-supervised variant of Ratnaparkhixe2x80x99s method results in a much tighter tag dictionary than either Ratnaparkhixe2x80x99s or our previous method, with accuracy as high as with our previous tag dictionary but much faster taggingxe2x80x94more than 100,000 tokens per second in Perl. 1 Overview"
C14-1110,Fast High-Accuracy Part-of-Speech Tagging by Independent Classifiers,2014,20,6,1,1,19878,robert moore,"Proceedings of {COLING} 2014, the 25th International Conference on Computational Linguistics: Technical Papers",0,"Part-of-speech (POS) taggers can be quite accurate, but for practical use, accuracy often has to be sacrificed for speed. For example, the maintainers of the Stanford tagger (Toutanova et al., 2003; Manning, 2011) recommend tagging with a model whose per tag error rate is 17% higher, relatively, than their most accurate model, to gain a factor of 10 or more in speed. In this paper, we treat POS tagging as a single-token independent multiclass classification task. We show that by using a rich feature set we can obtain high tagging accuracy within this framework, and by employing some novel feature-weight-combination and hypothesis-pruning techniques we can also get very fast tagging with this model. A prototype tagger implemented in Perl is tested and found to be at least 8 times faster than any publicly available tagger reported to have comparable accuracy on the standard Penn Treebank Wall Street Journal test set."
W12-3125,On Hierarchical Re-ordering and Permutation Parsing for Phrase-based Decoding,2012,18,16,2,0,3520,colin cherry,Proceedings of the Seventh Workshop on Statistical Machine Translation,0,"The addition of a deterministic permutation parser can provide valuable hierarchical information to a phrase-based statistical machine translation (PBSMT) system. Permutation parsers have been used to implement hierarchical re-ordering models (Galley and Manning, 2008) and to enforce inversion transduction grammar (ITG) constraints (Feng et al., 2010). We present a number of theoretical results regarding the use of permutation parsers in PBSMT. In particular, we show that an existing ITG constraint (Zens et al., 2004) does not prevent all non-ITG permutations, and we demonstrate that the hierarchical reordering model can produce analyses during decoding that are inconsistent with analyses made during training. Experimentally, we verify the utility of hierarchical re-ordering, and compare several theoretically-motivated variants in terms of both translation quality and the syntactic complexity of their output."
P11-1131,Gappy Phrasal Alignment By Agreement,2011,18,11,3,0,717,mohit bansal,Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,1,"We propose a principled and efficient phrase-to-phrase alignment model, useful in machine translation as well as other related natural language processing problems. In a hidden semi-Markov model, word-to-phrase and phrase-to-word translations are modeled directly by the system. Agreement between two directional models encourages the selection of parsimonious phrasal alignments, avoiding the overfitting commonly encountered in unsupervised training with multi-word units. Expanding the state space to include gappy phrases (such as French ne * pas) makes the alignment space more symmetric; thus, it allows agreement between discontinuous alignments. The resulting system shows substantial improvements in both alignment quality and translation quality over word-based Hidden Markov Models, while maintaining asymptotically equivalent runtime."
P10-2041,Intelligent Selection of Language Model Training Data,2010,8,310,1,1,19878,robert moore,Proceedings of the {ACL} 2010 Conference Short Papers,0,"We address the problem of selecting non-domain-specific language model training data to build auxiliary language models for use in tasks such as machine translation. Our approach is based on comparing the cross-entropy, according to domain-specific and non-domain-specifc language models, for each sentence of the text source used to produce the latter language model. We show that this produces better language models, trained on less data, than both random data selection and two other previously proposed methods."
W09-0109,What Do Computational Linguists Need to Know about Linguistics?,2009,0,2,1,1,19878,robert moore,"Proceedings of the {EACL} 2009 Workshop on the Interaction between Linguistics and Computational Linguistics: Virtuous, Vicious or Vacuous?",0,"In this position paper, we argue that although the data-driven, empirical paradigm for computational linguistics seems to be the best way forward at the moment, a thorough grounding in descriptive linguistics is still needed to do competent work in the field. Examples are given of how knowledge of linguistic phenomena leads to understanding the limitations of particular statistical models and to better feature selection for such models."
P09-2088,Improved Smoothing for N-gram Language Models Based on Ordinary Counts,2009,6,12,1,1,19878,robert moore,Proceedings of the {ACL}-{IJCNLP} 2009 Conference Short Papers,0,"Kneser-Ney (1995) smoothing and its variants are generally recognized as having the best perplexity of any known method for estimating N-gram language models. Kneser-Ney smoothing, however, requires nonstandard N-gram counts for the lower-order models used to smooth the highest-order model. For some applications, this makes Kneser-Ney smoothing inappropriate or inconvenient. In this paper, we introduce a new smoothing method based on ordinary counts that outperforms all of the previous ordinary-count methods we have tested, with the new method eliminating most of the gap between Kneser-Ney and those methods."
D09-1078,"Less is More: Significance-Based {N}-gram Selection for Smaller, Better Language Models",2009,11,9,1,1,19878,robert moore,Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,0,"The recent availability of large corpora for training N-gram language models has shown the utility of models of higher order than just trigrams. In this paper, we investigate methods to control the increase in model size resulting from applying standard methods at higher orders. We introduce significance-based N-gram selection, which not only reduces model size, but also improves perplexity for several smoothing methods, including Katz backoff and absolute discounting. We also show that, when combined with a new smoothing method and a novel variant of weighted-difference pruning, our selection method performs better in the trade-off between model size and perplexity than the best pruning method we found for modified Kneser-Ney smoothing."
P08-1012,{B}ayesian Learning of Non-Compositional Phrases with Synchronous Parsing,2008,17,63,3,0,7671,hao zhang,Proceedings of ACL-08: HLT,1,"We combine the strengths of Bayesian modeling and synchronous grammar in unsupervised learning of basic translation phrase pairs. The structured space of a synchronous grammar is a natural fit for phrase pair probability estimation, though the search space can be prohibitively large. Therefore we explore efficient algorithms for pruning this space that lead to empirically effective results. Incorporating a sparse prior using Variational Bayes, biases the models toward generalizable, parsimonious parameter sets, leading to significant improvements in word alignment. This preference for sparse solutions together with effective pruning methods forms a phrase alignment regimen that produces better end-to-end translations than standard word alignment approaches."
D08-1011,Indirect-{HMM}-based Hypothesis Alignment for Combining Outputs from Machine Translation Systems,2008,28,73,5,0,730,xiaodong he,Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing,0,"This paper presents a new hypothesis alignment method for combining outputs of multiple machine translation (MT) systems. An indirect hidden Markov model (IHMM) is proposed to address the synonym matching and word ordering issues in hypothesis alignment. Unlike traditional HMMs whose parameters are trained via maximum likelihood estimation (MLE), the parameters of the IHMM are estimated indirectly from a variety of sources including word semantic similarity, word surface similarity, and a distance-based distortion penalty. The IHMM-based method significantly outperforms the state-of-the-art TER-based alignment model in our experiments on NIST benchmark datasets. Our combined SMT system using the proposed method achieved the best Chinese-to-English translation result in the constrained training track of the 2008 NIST Open MT Evaluation."
C08-1074,Random Restarts in Minimum Error Rate Training for Statistical Machine Translation,2008,6,41,1,1,19878,robert moore,Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008),0,"Och's (2003) minimum error rate training (MERT) procedure is the most commonly used method for training feature weights in statistical machine translation (SMT) models. The use of multiple randomized starting points in MERT is a well-established practice, although there seems to be no published systematic study of its benefits. We compare several ways of performing random restarts with MERT. We find that all of our random restart methods outperform MERT without random restarts, and we develop some refinements of random restarts that are superior to the most common approach with regard to resulting model quality and training time."
W07-0715,An Iteratively-Trained Segmentation-Free Phrase Translation Model for Statistical Machine Translation,2007,13,18,1,1,19878,robert moore,Proceedings of the Second Workshop on Statistical Machine Translation,0,"Attempts to estimate phrase translation probablities for statistical machine translation using iteratively-trained models have repeatedly failed to produce translations as good as those obtained by estimating phrase translation probablities from surface statistics of bilingual word alignments as described by Koehn, et al. (2003). We propose a new iteratively-trained phrase translation model that produces translations of quality equal to or better than those produced by Koehn, et al.'s model. Moreover, with the new model, translation quality degrades much more slowly as pruning is tightend to reduce translation time."
N07-2053,Selective Phrase Pair Extraction for Improved Statistical Machine Translation,2007,10,8,2,0,7794,luke zettlemoyer,"Human Language Technologies 2007: The Conference of the North {A}merican Chapter of the Association for Computational Linguistics; Companion Volume, Short Papers",0,"Phrase-based statistical machine translation systems depend heavily on the knowledge represented in their phrase translation tables. However, the phrase pairs included in these tables are typically selected using simple heuristics that potentially leave much room for improvement. In this paper, we present a technique for selecting the phrase pairs to include in phrase translation tables based on their estimated quality according to a translation model. This method not only reduces the size of the phrase translation table, but also improves translation quality as measured by the BLEU metric."
2007.mtsummit-papers.43,Faster beam-search decoding for phrasal statistical machine translation,2007,29,51,1,1,19878,robert moore,Proceedings of Machine Translation Summit XI: Papers,0,"Improved systems, methods and apparatuses are provided for fast beam-search decoding for phrasal statistical machine translation. The provided techniques incorporate a front-loaded distortion penalty estimate for future estimated distortion penalty and/or early pruning to reduce the search space. The improvements result in up to an order of magnitude increase in translation speed for statistical machine translation systems. The disclosed details enable various refinements and modifications according to decoder and system design considerations."
P06-1065,Improved Discriminative Bilingual Word Alignment,2006,14,69,1,1,19878,robert moore,Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics,1,"For many years, statistical machine translation relied on generative models to provide bilingual word alignments. In 2005, several independent efforts showed that discriminative models could be used to enhance or replace the standard generative approach. Building on this work, we demonstrate substantial improvement in word-alignment accuracy, partly though improved training methods, but predominantly through selection of more and better features. Our best model produces the lowest alignment error rate yet reported on Canadian Hansards bilingual data."
W05-0801,Association-Based Bilingual Word Alignment,2005,23,59,1,1,19878,robert moore,Proceedings of the {ACL} Workshop on Building and Using Parallel Texts,0,"Bilingual word alignment forms the foundation of current work on statistical machine translation. Standard word-alignment methods involve the use of probabilistic generative models that are complex to implement and slow to train. In this paper we show that it is possible to approach the alignment accuracy of the standard models using algorithms that are much faster, and in some ways simpler, based on basic word-association statistics."
H05-1011,A Discriminative Framework for Bilingual Word Alignment,2005,11,135,1,1,19878,robert moore,Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing,0,"Bilingual word alignment forms the foundation of most approaches to statistical machine translation. Current word alignment methods are predominantly based on generative models. In this paper, we demonstrate a discriminative approach to training simple word alignment models that are comparable in accuracy to the more complex generative models normally used. These models have the the advantages that they are easy to add features to and they allow fast optimization of model parameters using small amounts of annotated data."
W04-3243,On Log-Likelihood-Ratios and the Significance of Rare Events,2004,6,71,1,1,19878,robert moore,Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing,0,"We address the issue of judging the significance of rare events as it typically arises in statistical naturallanguage processing. We first define a general approach to the problem, and we empirically compare results obtained using log-likelihood-ratios and Fisherxe2x80x99s exact test, applied to measuring strength of bilingual word associations."
P04-1066,Improving {IBM} Word Alignment Model 1,2004,16,86,1,1,19878,robert moore,Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics ({ACL}-04),1,"We investigate a number of simple methods for improving the word-alignment accuracy of IBM Model 1. We demonstrate reduction in alignment error rate of approximately 30% resulting from (1) giving extra weight to the probability of alignment to the null word, (2) smoothing probability estimates for rare words, and (3) using a simple heuristic estimation method to initialize, or replace, EM training of model parameters."
ringger-etal-2004-using,Using the {P}enn {T}reebank to Evaluate Non-Treebank Parsers,2004,12,13,2,1,30821,eric ringger,Proceedings of the Fourth International Conference on Language Resources and Evaluation ({LREC}{'}04),0,"This paper describes a method for conducting evaluations of Treebank and non-Treebank parsers alike against the English language U. Penn Treebank (Marcus et al., 1993) using a metric that focuses on the accuracy of relatively non-controversial aspects of parse structure. Our conjecture is that if we focus on maximal projections of heads (MPH), we are likely to find much broader agreement than if we try to evaluate based on order of attachment. We hope that this method may find wider acceptance and be useful in establishing a generally applicable framework for evaluation in natural language parsing. We employ this method in an evaluation of NLPWin (Heidorn, 2000), a parser developed at Microsoft Research without reference to the Penn Treebank, and, for comparison, the well-known statistical Treebank parser of Charniak (2000)."
C04-1080,Part-of-Speech Tagging in Context,2004,17,92,2,0,4720,michele banko,{COLING} 2004: Proceedings of the 20th International Conference on Computational Linguistics,0,"We present a new HMM tagger that exploits context on both sides of a word to be tagged, and evaluate it in both the unsupervised and supervised case. Along the way, we present the first comprehensive comparison of unsupervised methods for part-of-speech tagging, noting that published results to date have not been comparable across corpora or lexicons. Observing that the quality of the lexicon greatly impacts the accuracy that can be achieved by the algorithms, we present a method of HMM training that improves accuracy when training of lexical probabilities is unstable. Finally, we show how this new tagger achieves state-of-the-art results in a supervised, non-training intensive framework."
C04-1097,Linguistically Informed Statistical Models of Constituent Structure for Ordering in Sentence Realization,2004,20,39,3,1,30821,eric ringger,{COLING} 2004: Proceedings of the 20th International Conference on Computational Linguistics,0,"We present several statistical models of syntactic constituent order for sentence realization. We compare several models, including simple joint models inspired by existing statistical parsing models, and several novel conditional models. The conditional models leverage a large set of linguistic features without manual feature selection. We apply and evaluate the models in sentence realization for French and German and find that a particular conditional model outperforms all others. We employ a version of that model in an evaluation on unordered trees from the Penn TreeBank. We offer this result on standard data as a reference-point for evaluations of ordering in sentence realization."
E03-1035,Learning Translations of Named-Entity Phrases from Parallel Corpora,2003,9,57,1,1,19878,robert moore,10th Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,"We develop a new approach to learning phrase translations from parallel corpora, and show that it performs with very high coverage and accuracy in choosing French translations of English named-entity phrases in a test corpus of software manuals. Analysis of a subset of our results suggests that the method should also perform well on more general phrase translation tasks."
W02-2105,An Overview of Amalgam: A Machine-learned Generation Module,2002,16,48,4,0,159,simon corstonoliver,Proceedings of the International Natural Language Generation Conference,0,"We present an overview of Amalgam, a sentence realization module that combines machine-learned and knowledgeengineered components to produce natural language sentences from logical form inputs. We describe the decomposition of the task of sentence realization into a linguistically informed series of steps, with particular attention to the linguistic issues that arise in German. We report on the evaluation of component steps and of the overall system."
W02-2106,"A Complete, Efficient Sentence-Realization Algorithm for Unification Grammar",2002,11,11,1,1,19878,robert moore,Proceedings of the International Natural Language Generation Conference,0,"This paper describes an efficient sentencerealization algorithm that is complete for a very general class of unification grammars. Under fairly modest constraints on the grammar, the algorithm is shown to have polynomial time complexity for generation of sentences whose logical form exactly matches the goal logical form. The algorithm can be extended to handle what is arguably the most important subcase of the logical-form equivalence problem, permutation of logical conjunction. With this extension the algorithm is no longer polynomial, but it seems to be about as efficient as the nature of the problem permits."
P02-1004,Machine-learned contexts for linguistic operations in {G}erman sentence realization,2002,13,13,4,0.638298,15131,michael gamon,Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics,1,"We show that it is possible to learn the contexts for linguistic operations which map a semantic representation to a surface syntactic tree in sentence realization with high accuracy. We cast the problem of learning the contexts for the linguistic operations as classification tasks, and apply straightforward machine learning techniques, such as decision tree learning. The training data consist of linguistic features extracted from syntactic and semantic representations produced by a linguistic analysis system. The target features are extracted from links to surface syntax trees. Our evidence consists of four examples from the German sentence realization system code-named Amalgam: case assignment, assignment of verb position features, extraposition, and syntactic aggregation."
P02-1019,Pronunciation Modeling for Improved Spelling Correction,2002,10,154,2,0,9781,kristina toutanova,Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics,1,This paper presents a method for incorporating word pronunciation information in a noisy channel model for spelling correction. The proposed method builds an explicit error model for word pronunciations. By modeling pronunciation similarities between words we achieve a substantial performance improvement over the previous best performing models for spelling correction.
C02-1036,{E}xtraposition: A Case Study in {G}erman Sentence Realization,2002,4,14,4,0.638298,15131,michael gamon,{COLING} 2002: The 19th International Conference on Computational Linguistics,0,"We profile the occurrence of clausal extraposition in corpora from different domains and demonstrate that extraposition is a pervasive phenomenon in German that must be addressed in German sentence realization. We present two different approaches to the modeling of extraposition, both based on machine learned decision tree classifiers. The two approaches differ in their view of the movement operation: one approach models multi-step movement through intermediate nodes to the ultimate target node, while the other approach models one-step movement to the target node. We compare the resulting models, trained on data from two domains and discuss the differences between the two types of models and between the results obtained in the different domains."
moore-2002-fast,Fast and accurate sentence alignment of bilingual corpora,2002,10,208,1,1,19878,robert moore,Proceedings of the 5th Conference of the Association for Machine Translation in the Americas: Technical Papers,0,"We present a new method for aligning sentences with their translations in a parallel bilingual corpus. Previous approaches have generally been based either on sentence length or word correspondences. Sentence-length-based methods are relatively fast and fairly accurate. Word-correspondence-based methods are generally more accurate but much slower, and usually depend on cognates or a bilingual lexicon. Our method adapts and combines these approaches, achieving high accuracy at a modest computational cost, and requiring no knowledge of the languages or the corpus beyond division into words and sentences."
W01-1411,Towards a Simple and Accurate Statistical Approach to Learning Translation Relationships among Words,2001,13,37,1,1,19878,robert moore,Proceedings of the {ACL} 2001 Workshop on Data-Driven Methods in Machine Translation,0,"We report on a project to derive word translation relationships automatically from parallel corpora. Our effort is distinguished by the use of simpler, faster models than those used in previous high-accuracy approaches. Our methods achieve accuracy on single-word translations that seems comparable to any work previously reported, up to nearly 60% coverage of word types, and they perform particularly well on a class of multi-word compounds of special interest to our translation effort."
W00-1603,Time as a Measure of Parsing Efficiency,2000,8,6,1,1,19878,robert moore,Proceedings of the {COLING}-2000 Workshop on Efficiency In Large-Scale Parsing Systems,0,"Charniak and his colleagues have proposed implementation-independent metrics as a way of comparing the efficiency of parsing algorithms implemented on different platforms, in different languages, and with different degrees of incidental optimization. We argue that there are easily immaginable circumstances in which their proposed metrics would mask significant differences in efficiency; we point out that their data do not, in fact, support the usability of such metrics for comparing the efficiency of different algorithms; and we analyze data for a similar metric to try to quantify the degree of variation one might expect between such metrics and actual parse time. Finally, we propose a methodology for making cross-platform comparisons through the use of reference parser implementations."
P00-1037,An Improved Error Model for Noisy Channel Spelling Correction,2000,16,389,2,0,51339,eric brill,Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics,1,"The noisy channel model has been applied to a wide range of problems, including spelling correction. These models consist of two components: a source model and a channel model. Very little research has gone into improving the channel model for spelling correction. This paper describes a new channel model for spelling correction, based on generic string to string edits. Using this model gives significant performance improvements compared to previously proposed models."
A00-2033,Removing Left Recursion from Context-Free Grammars,2000,8,36,1,1,19878,robert moore,1st Meeting of the North {A}merican Chapter of the Association for Computational Linguistics,0,"A long-standing issue regarding algorithms that manipulate context-free grammars (CFGs) in a top-down left-to-right fashion is that left recursion can lead to nontermination. An algorithm is known that transforms any CFG into an equivalent nonleft-recursive CFG, but the resulting grammars are often too large for practical use. We present a new method for removing left recursion from CFGs that is both theoretically superior to the standard algorithm, and produces very compact non-left-recursive CFGs in practice."
2000.iwpt-1.18,Improved Left-corner Chart Parsing for Large Context-free Grammars,2000,-1,-1,1,1,19878,robert moore,Proceedings of the Sixth International Workshop on Parsing Technologies,0,"We develop an improved form of left-corner chart parsing for large context-free grammars, introducing improvements that result in significant speed-ups more compared to previously-known variants of left corner parsing. We also compare our method to several other major parsing approaches, and find that our improved left-corner parsing method outperforms each of these across a range of grammars. Finally, we also describe a new technique for minimizing the extra information needed to efficiently recover parses from the data structures built in the course of parsing."
P99-1024,The {C}ommand{T}alk Spoken Dialogue System,1999,16,234,5,0,16906,amanda stent,Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics,1,None
P94-1016,Interleaving Syntax and Semantics in an Efficient Bottom-Up Parser,1994,12,31,2,1,46824,john dowding,32nd Annual Meeting of the Association for Computational Linguistics,1,"We describe an efficient bottom-up parser that interleaves syntactic and semantic structure building. Two techniques are presented for reducing search by reducing local ambiguity: Limited left context constraints are used to reduce local syntactic ambiguity, and deferred sortal-constraint application is used to reduce local semantic ambiguity. We experimentally evaluate these techniques, and show dramatic reductions in both number of chart edges and total parsing time. The robust processing capabilities of the parser are demonstrated in its use in improving the accuracy of a speech recognizer."
H94-1022,Semantic Evaluation for Spoken-Language Systems,1994,1,12,1,1,19878,robert moore,"{H}uman {L}anguage {T}echnology: Proceedings of a Workshop held at {P}lainsboro, {N}ew {J}ersey, {M}arch 8-11, 1994",0,"Development has begun on a semantic evaluation (SemEval) methodology and infrastructure for the ARPA Spoken Language Program. SemEval is an attempt to define a task-independent technology-based evaluation for language-understanding systems consisting of three parts: word-sense identification, predicate-argument structure determination, and identification of coreference relations. An initial spoken-language SemEval on ATIS data is planned for November/December 1994, concurrent with the next ATIS CAS (database answer) evaluation."
H94-1115,Combining Linguistic and Statistical Technology for Improved Spoken Language Understanding,1994,0,0,2,0,56429,michael cohen,"{H}uman {L}anguage {T}echnology: Proceedings of a Workshop held at {P}lainsboro, {N}ew {J}ersey, {M}arch 8-11, 1994",0,"The goal of this project is to develop technology for spoken language understanding which is highly accurate, robust, and fast, is easily ported to new domains, environments. and channels, and quickly adapts to new speakers. The system combines the DECIPHER speech recognition system with the Gemini natural language understanding system."
C94-1099,A Tool for Collecting Domain Dependent Sortal Constraints From Corpora,1994,6,3,4,0,56280,francois andry,{COLING} 1994 Volume 1: The 15th {I}nternational {C}onference on {C}omputational {L}inguistics,0,"In this paper, we describe a tool designed to generate semi-automatically the sortal constraints specific to a domain to be used in a natural language (NL) understanding system. This tool is evaluated using the SRI Gemini NL understanding system in the ATIS domain."
P93-1008,{GEMINI}: A Natural Language System for Spoken-Language Understanding,1993,24,125,6,1,46824,john dowding,31st Annual Meeting of the Association for Computational Linguistics,1,"The demands on a natural language understanding system used for spoken language differ somewhat from the demands of text processing. For processing spoken language, there is a tension between the system being as robust as necessary, and as constrained as possible. The robust system will a t tempt to find as sensible an interpretation as possible, even in the presence of performance errors by the speaker, or recognition errors by the speech recognizer. In contrast, in order to provide language constraints to a speech recognizer, a system should be able to detect that a recognized string is not a sentence of English, and disprefer that recognition hypothesis from the speech recognizer. If the coupling is to be tight, with parsing and recognition interleaved, then the parser should be able to enforce as many constraints as possible for partial utterances. The approach taken in Gemini is to tightly constrain language recognition to limit overgeneration, but to extend the language analysis to recognize certain characteristic patterns of spoken utterances (but not generally thought of as part of grammar) and to recognize specific types of performance errors by the speaker."
H93-1008,{G}emini: A Natural Language System for Spoken-Language Understanding,1993,24,125,6,1,46824,john dowding,"{H}uman {L}anguage {T}echnology: Proceedings of a Workshop Held at Plainsboro, New Jersey, March 21-24, 1993",0,"The demands on a natural language understanding system used for spoken language differ somewhat from the demands of text processing. For processing spoken language, there is a tension between the system being as robust as necessary, and as constrained as possible. The robust system will a t tempt to find as sensible an interpretation as possible, even in the presence of performance errors by the speaker, or recognition errors by the speech recognizer. In contrast, in order to provide language constraints to a speech recognizer, a system should be able to detect that a recognized string is not a sentence of English, and disprefer that recognition hypothesis from the speech recognizer. If the coupling is to be tight, with parsing and recognition interleaved, then the parser should be able to enforce as many constraints as possible for partial utterances. The approach taken in Gemini is to tightly constrain language recognition to limit overgeneration, but to extend the language analysis to recognize certain characteristic patterns of spoken utterances (but not generally thought of as part of grammar) and to recognize specific types of performance errors by the speaker."
H93-1024,Session 4: Natural Language,1993,-1,-1,1,1,19878,robert moore,"{H}uman {L}anguage {T}echnology: Proceedings of a Workshop Held at Plainsboro, New Jersey, March 21-24, 1993",0,None
H93-1108,A Real-Time Spoken-Language System for Interactive Problem Solving,1993,0,0,2,0.538199,54006,patti price,"{H}uman {L}anguage {T}echnology: Proceedings of a Workshop Held at Plainsboro, New Jersey, March 21-24, 1993",0,"Abstract : Under this effort, SRI has developed spoken-language technology for interactive problem solving, featuring real-time performance for up to several thousand word vocabularies, high semantic accuracy, habitability within the domain, and robustness to many sources of variability. Although the technology is suitable for many applications, efforts to date have focussed on developing an Air Travel Information System (ATIS) prototype application. SRI's ATIS system has been evaluated in four ARPA benchmark evaluations, and has consistently been at or near the top in performance. These achievements are the result of SRI's technical progress in speech recognition, natural-language processing, and speech and natural-language integration."
H92-1120,A Real-Time Spoken-Language System for Interactive Problem Solving,1992,-1,-1,2,0.538199,54006,patti price,"Speech and Natural Language: Proceedings of a Workshop Held at Harriman, New York, {F}ebruary 23-26, 1992",0,None
H91-1034,A Template Matcher for Robust {NL} Interpretation,1991,6,52,4,0,56999,eric jackson,"Speech and Natural Language: Proceedings of a Workshop Held at Pacific Grove, California, {F}ebruary 19-22, 1991",0,"In this paper, we describe the Template Matcher, a system built at SRI to provide robust natural-language interpretation in the Air Travel Information System (ATIS) domain. The system appears to be robust to both speech recognition errors and unanticipated or difficult locutions used by speakers. We explain the motivation for the Template Matcher, describe in general terms how it works in comparison with similar systems, and examine its performance. We discuss some limitations of this approach, and sketch a plan for integrating the Template Matcher with an analytic parser, which we believe will combine the advantages of both."
H91-1036,Efficient Bottom-Up Parsing,1991,5,16,1,1,19878,robert moore,"Speech and Natural Language: Proceedings of a Workshop Held at Pacific Grove, California, {F}ebruary 19-22, 1991",0,"This paper describes a series of experiments aimed at producing a bottom-up parser that will produce partial parses suitable for use in robust interpretation and still be reasonably efficient. In the course of these experiments, we improved parse times by a factor of 18 over our first attempt, ending with a system that was twice as fast as our previous parser, which relied on strong top-down constraints. The major algorithmic variations we tried are described along with the corresponding performance results."
H91-1099,{SRI}{'}s Real-Time Spoken Language System,1991,0,0,2,0.555556,54006,patti price,"Speech and Natural Language: Proceedings of a Workshop Held at Pacific Grove, California, {F}ebruary 19-22, 1991",0,"This project involves the integration of speech and natural-language processing for spoken language systems (SLS). The goal of this project, to develop a multi-modal interface to the Official Airline Guide database, is being developed along two overlapping research and development lines: one focussed on an SLS kernel for database query, and the other on the full interactive system."
J90-1004,Semantic-Head-Driven Generation,1990,0,0,4,0.376654,12905,stuart shieber,Computational Linguistics,0,We present an algorithm for generating strings from logical form encodings that improves upon previous algorithms in that it places fewer restrictions on the class of grammars to which it is applic...
H90-1031,{SRI}{'}s Experience with the {ATIS} Evaluation,1990,0,131,1,1,19878,robert moore,"Speech and Natural Language: Proceedings of a Workshop Held at Hidden Valley, {P}ennsylvania, June 24-27,1990",0,"SRI International participated in the June 1990 Air Travel Information System (ATIS) natural-language evaluation. This report briefly describes the system that SRI used in the evaluation, analyzes SRI's results, and makes some recommendations for changes in the database structure and data collection system to be used for future ATIS evaluations."
H90-1096,A Real-Time Spoken-Language System Interactive Problem-Solving,1990,0,0,2,0.555556,54006,patti price,"Speech and Natural Language: Proceedings of a Workshop Held at Hidden Valley, {P}ennsylvania, June 24-27,1990",0,"SRI is developing a system to improve complex problem-solving through the use of interactive spoken language in conjunction with other media. This requires real-time performance, large vocabulary, high semantic accuracy and habitability, as well as robustness to expected and unexpected variability. SRI's spoken-language system is being developed in the air travel planning domain along two overlapping research and development lines, one focussed on an SLS kernel for database query, and the other on the full interactive system."
P89-1002,A Semantic-Head-Driven Generation Algorithm for Unification-Based Formalisms,1989,16,78,3,0.376654,12905,stuart shieber,27th Annual Meeting of the Association for Computational Linguistics,1,"We present an algorithm for generating strings from logical form encodings that improves upon previous algorithms in that it places fewer restrictions on the class of grammars to which it is applicable. In particular, unlike an Earley deduction generator (Shieber, 1988), it allows use of semantically nonmonotonic grammars, yet unlike topdown methods, it also permits left-recursion. The enabling design feature of the algorithm is its implicit traversal of the analysis tree for the string being generated in a semantic-head-driven fashion."
P89-1005,Unification-Based Semantic Interpretation,1989,5,35,1,1,19878,robert moore,27th Annual Meeting of the Association for Computational Linguistics,1,"We show how unification can be used to specify the semantic interpretation of natural-language expressions, including problematical constructions involving long-distance dependencies. We also sketch a theoretical foundation for unification-based semantic interpretation, and compare the unification-based approach with more conventional techniques based on the lambda calculus."
H89-1043,Integrating Speech and Natural-Language Processing,1989,1,26,1,1,19878,robert moore,"Speech and Natural Language: Proceedings of a Workshop Held at Philadelphia, {P}ennsylvania, {F}ebruary 21-23, 1989",0,"SRI has developed a new architecture for integrating speech and natural-language processing that applies linguistic constraints during recognition by incrementally expanding the state-transition network embodied in a unification grammar. We compare this dynamic-grammar-network (DGN) approach to its principal alternative, word-lattice parsing, presenting preliminary experimental results that suggest the DGN approach requires much less computation time than word-lattice parsing, while maintaining a very tractable recognition search space."
P82-1007,Natural-Language Access to Databases{--}Theoretical/Technical Issues,1982,7,2,1,1,19878,robert moore,20th Annual Meeting of the Association for Computational Linguistics,1,"In responding to the guidelines established by the session chairman of this panel, three of the five topics he set forth will be discussed. These include aggregate functions and quantity questions, querying semantically complex fields, and multi-file queries. As we will make clear in the sequel, the transformational apparatus utilized in the TQA Question Answering System provides a principled basis for handling these and many other problems in natural language access to databases."
C82-1015,{DIALOGIC}: A Core Natural-Language Processing System,1982,6,149,6,0,30392,barbara grosz,{C}oling 1982: Proceedings of the {N}inth {I}nternational {C}onference on {C}omputational {L}inguistics,0,"The DIALOGIC system translates English sentences into representations of their literal meaning in the context of an utterance. These representations, or logical forms, are intended to be a purely formal language that is as close as possible to the structure of natural language, while providing the semantic compositionality necessary for meaning-dependent computational processing. The design of DIALOGIC (and of its constituent modules) was influenced by the goal of using it as the core language-processing component in a variety of systems, some of which are transportable to new domains of application."
P81-1028,Problems in Logical Form,1981,10,59,1,1,19878,robert moore,19th Annual Meeting of the Association for Computational Linguistics,1,"Abstract : Most current theories of natural-language processing propose that the assimilation of an utterance involves producing an expression or structure that in some sense represents the literal meaning of the utterance. It is often maintained that understanding what an utterance literally means consists in being able to recover such a representation. In philosophy and linguistics this sort of representation is usually said to display the logical form of an utterance. This paper surveys some of the key problems that arise in defining a system of representation for the logical forms of English sentences and suggests possible approaches to their solution. The author first looks at some general issues relating to the notion of logical form, explaining why it makes sense to define such a notion only for sentences in context, not in isolation, and then discusses the relationship between research on logical form and work on knowledge representation in artificial intelligence. The rest of the paper is devoted to examining specific problems in logical form. These include the following: quantifiers; events, actions and processes; time and space; collective entities and substances; propositional attitudes and modalities; and questions and imperatives."
