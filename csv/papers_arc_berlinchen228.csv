2021.rocling-1.7,A Study on Contextualized Language Modeling for Machine Reading Comprehension,2021,-1,-1,3,0,2320,chinying wu,Proceedings of the 33rd Conference on Computational Linguistics and Speech Processing (ROCLING 2021),0,"With the recent breakthrough of deep learning technologies, research on machine reading comprehension (MRC) has attracted much attention and found its versatile applications in many use cases. MRC is an important natural language processing (NLP) task aiming to assess the ability of a machine to understand natural language expressions, which is typically operationalized by first asking questions based on a given text paragraph and then receiving machine-generated answers in accordance with the given context paragraph and questions. In this paper, we leverage two novel pretrained language models built on top of Bidirectional Encoder Representations from Transformers (BERT), namely BERT-wwm and MacBERT, to develop effective MRC methods. In addition, we also seek to investigate whether additional incorporation of the categorical information about a context paragraph can benefit MRC or not, which is achieved based on performing context paragraph clustering on the training dataset. On the other hand, an ensemble learning approach is proposed to harness the synergistic power of the aforementioned two BERT-based models so as to further promote MRC performance."
2021.rocling-1.14,A Preliminary Study on Environmental Sound Classification Leveraging Large-Scale Pretrained Model and Semi-Supervised Learning,2021,-1,-1,5,0,2340,yousheng tsao,Proceedings of the 33rd Conference on Computational Linguistics and Speech Processing (ROCLING 2021),0,"With the widespread commercialization of smart devices, research on environmental sound classification has gained more and more attention in recent years. In this paper, we set out to make effective use of large-scale audio pretrained model and semi-supervised model training paradigm for environmental sound classification. To this end, an environmental sound classification method is first put forward, whose component model is built on top a large-scale audio pretrained model. Further, to simulate a low-resource sound classification setting where only limited supervised examples are made available, we instantiate the notion of transfer learning with a recently proposed training algorithm (namely, FixMatch) and a data augmentation method (namely, SpecAugment) to achieve the goal of semi-supervised model training. Experiments conducted on bench-mark dataset UrbanSound8K reveal that our classification method can lead to an accuracy improvement of 2.4{\%} in relation to a current baseline method."
2021.rocling-1.17,Exploring the Integration of {E}2{E} {ASR} and Pronunciation Modeling for {E}nglish Mispronunciation Detection,2021,-1,-1,4,0,2347,hsinwei wang,Proceedings of the 33rd Conference on Computational Linguistics and Speech Processing (ROCLING 2021),0,"There has been increasing demand to develop effective computer-assisted language training (CAPT) systems, which can provide feedback on mispronunciations and facilitate second-language (L2) learners to improve their speaking proficiency through repeated practice. Due to the shortage of non-native speech for training the automatic speech recognition (ASR) module of a CAPT system, the corresponding mispronunciation detection performance is often affected by imperfect ASR. Recognizing this importance, we in this paper put forward a two-stage mispronunciation detection method. In the first stage, the speech uttered by an L2 learner is processed by an end-to-end ASR module to produce N-best phone sequence hypotheses. In the second stage, these hypotheses are fed into a pronunciation model which seeks to faithfully predict the phone sequence hypothesis that is most likely pronounced by the learner, so as to improve the performance of mispronunciation detection. Empirical experiments conducted a English benchmark dataset seem to confirm the utility of our method."
2020.rocling-1.15,Multi-view Attention-based Speech Enhancement Model for Noise-robust Automatic Speech Recognition,2020,-1,-1,3,0,15596,fuan chao,Proceedings of the 32nd Conference on Computational Linguistics and Speech Processing (ROCLING 2020),0,None
2020.rocling-1.17,Innovative Pretrained-based Reranking Language Models for N-best Speech Recognition Lists,2020,-1,-1,2,0,15598,shihhsuan chiu,Proceedings of the 32nd Conference on Computational Linguistics and Speech Processing (ROCLING 2020),0,None
2020.rocling-1.24,A Study on Contextualized Language Modeling for {FAQ} Retrieval,2020,-1,-1,3,0,15611,wenting tseng,Proceedings of the 32nd Conference on Computational Linguistics and Speech Processing (ROCLING 2020),0,None
2020.rocling-1.27,Exploiting Text Prompts for the Development of an End-to-End Computer-Assisted Pronunciation Training System,2020,-1,-1,3,0,15614,yusen cheng,Proceedings of the 32nd Conference on Computational Linguistics and Speech Processing (ROCLING 2020),0,None
2020.rocling-1.31,Exploring Disparate Language Model Combination Strategies for {M}andarin-{E}nglish Code-Switching {ASR},2020,-1,-1,2,0,2331,weiting lin,Proceedings of the 32nd Conference on Computational Linguistics and Speech Processing (ROCLING 2020),0,None
2020.ijclclp-1.2,åºæ¼ç«¯å°ç«¯æ¨¡ååæè¡ä¹èªé³æä»¶æè¦ (Spoken Document Summarization Using End-to-End Modeling Techniques),2020,-1,-1,4,0,19035,tzuen liu,"International Journal of Computational Linguistics {\\&} {C}hinese Language Processing, Volume 25, Number 1, June 2020",0,None
P19-1439,Can You Tell Me How to Get Past Sesame Street? Sentence-Level Pretraining Beyond Language Modeling,2019,0,13,12,0,9746,alex wang,Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,1,"Natural language understanding has recently seen a surge of progress with the use of sentence encoders like ELMo (Peters et al., 2018a) and BERT (Devlin et al., 2019) which are pretrained on variants of language modeling. We conduct the first large-scale systematic study of candidate pretraining tasks, comparing 19 different tasks both as alternatives and complements to language modeling. Our primary results support the use language modeling, especially when combined with pretraining on additional labeled-data tasks. However, our results are mixed across pretraining tasks and show some concerning trends: In ELMo{'}s pretrain-then-freeze paradigm, random baselines are worryingly strong and results vary strikingly across target tasks. In addition, fine-tuning BERT on an intermediate task often negatively impacts downstream transfer. In a more positive trend, we see modest gains from multitask training, suggesting the development of more sophisticated multitask and transfer learning techniques as an avenue for further research."
2019.rocling-1.21,ä½¿ç¨çæå°æç¶²è·¯æ¼å¼·å¥å¼èªåèªé³è¾¨è­çæç¨(Exploiting Generative Adversarial Network for Robustness Automatic Speech Recognition),2019,-1,-1,4,0,27237,mingjhang yang,Proceedings of the 31st Conference on Computational Linguistics and Speech Processing (ROCLING 2019),0,None
2019.rocling-1.26,æ¢ç©¶ç«¯å°ç«¯èªé³è¾¨è­æ¼ç¼é³æª¢æ¸¬èè¨ºæ·(Investigating on Computer-Assisted Pronunciation Training Leveraging End-to-End Speech Recognition Techniques),2019,-1,-1,4,1,27243,hsiujui chang,Proceedings of the 31st Conference on Computational Linguistics and Speech Processing (ROCLING 2019),0,None
2019.rocling-1.31,åºæ¼éå±¤å¼ç·¨ç¢¼æ¶æ§ä¹ææ¬å¯è®æ§é æ¸¬(A Hierarchical Encoding Framework for Text Readability Prediction),2019,-1,-1,4,0,2343,shiyan weng,Proceedings of the 31st Conference on Computational Linguistics and Speech Processing (ROCLING 2019),0,None
2019.ijclclp-1.3,æ¢ç©¶ç«¯å°ç«¯æ··åæ¨¡åæ¶æ§æ¼è¯èªèªé³è¾¨è­ (An Investigation of Hybrid {CTC}-Attention Modeling in {M}andarin Speech Recognition),2019,-1,-1,4,1,27243,hsiujui chang,"International Journal of Computational Linguistics {\\&} {C}hinese Language Processing, Volume 24, Number 1, June 2019",0,None
O18-1004,"æè­°èªé³è¾¨è­ä½¿ç¨èªè\
è³è¨ä¹èªè¨æ¨¡åèª¿é©æè¡ (On the Use of Speaker-Aware Language Model Adaptation Techniques for Meeting Speech Recognition ) [In {C}hinese]",2018,0,0,5,0,29220,yingwen chen,Proceedings of the 30th Conference on Computational Linguistics and Speech Processing ({ROCLING} 2018),0,None
O18-1007,æ¢è¨è²å­¸æ¨¡åçåä½µæè¡èåç£ç£éå¥å¼è¨ç·´æ¼æè­°èªé³è¾¨è­ä¹ç ç©¶ (Investigating acoustic model combination and semi-supervised discriminative training for meeting speech recognition) [In {C}hinese],2018,0,0,2,1,2341,tienhong lo,Proceedings of the 30th Conference on Computational Linguistics and Speech Processing ({ROCLING} 2018),0,None
O18-1010,æ¢è¨éå¥å¼è¨ç·´è²å­¸æ¨¡åä¹é¡ç¥ç¶ç¶²è·¯æ¶æ§ååªåæ¹æ³çæ¹é² (Discriminative Training of Acoustic Models Leveraging Improved Neural Network Architecture and Optimization Method) [In {C}hinese],2018,0,0,4,1,27368,weicheng chao,Proceedings of the 30th Conference on Computational Linguistics and Speech Processing ({ROCLING} 2018),0,None
O18-1012,æ¢ç´¢çµåå¿«éææ¬åå·ç©ç¥ç¶ç¶²è·¯æ¼å¯è®æ§æ¨¡åä¹å»ºç« (Exploring Combination of {F}ast{T}ext and Convolutional Neural Networks for Building Readability Models) [In {C}hinese],2018,0,0,2,1,27254,houchiang tseng,Proceedings of the 30th Conference on Computational Linguistics and Speech Processing ({ROCLING} 2018),0,None
2018.ijclclp-2.2,çµåéå¥å¼è¨ç·´èæ¨¡ååä½µæ¼åç£ç£å¼èªé³è¾¨è­ä¹ç ç©¶ (Leveraging Discriminative Training and Model Combination for Semi-supervised Speech Recognition),2018,-1,-1,2,1,2341,tienhong lo,"International Journal of Computational Linguistics {\\&} {C}hinese Language Processing, Volume 23, Number 2, December 2018",0,None
O17-3002,èªé³æä»¶æª¢ç´¢ä½¿ç¨é¡ç¥ç¶ç¶²è·¯æè¡ (On the Use of Neural Network Modeling Techniques for Spoken Document Retrieval) [In {C}hinese],2017,0,0,5,1,2341,tienhong lo,"International Journal of Computational Linguistics {\\&} {C}hinese Language Processing, Volume 22, Number 2, {D}ecember 2017-Special Issue on Selected Papers from {ROCLING} {XXIX}",0,None
O17-3004,æ¢ç©¶ä½¿ç¨åºæ¼é¡ç¥ç¶ç¶²è·¯ä¹ç¹å¾µæ¼ææ¬å¯è®æ§åé¡ (Exploring the Use of Neural Network based Features for Text Readability Classification) [In {C}hinese],2017,0,0,2,1,27254,houchiang tseng,"International Journal of Computational Linguistics {\\&} {C}hinese Language Processing, Volume 22, Number 2, {D}ecember 2017-Special Issue on Selected Papers from {ROCLING} {XXIX}",0,None
O17-2001,ç¶ä»£éç£ç£å¼æ¹æ³ä¹æ¯è¼æ¼ç¯éå¼èªé³æè¦ (An Empirical Comparison of Contemporary Unsupervised Approaches for Extractive Speech Summarization) [In {C}hinese],2017,0,0,4,0.980284,19036,shihhung liu,"International Journal of Computational Linguistics {\\&} {C}hinese Language Processing, Volume 22, Number 1, June 2017",0,None
O17-1011,æ¢ç©¶ä¸åé åæä»¶ä¹å¯è®æ§åæ (Exploring Readability Analysis on Multi-Domain Texts) [In {C}hinese],2017,0,0,3,1,27254,houchiang tseng,Proceedings of the 29th Conference on Computational Linguistics and Speech Processing ({ROCLING} 2017),0,None
O17-1015,ä½¿ç¨æ¥è©¢æåæ¢ç´¢èé¡ç¥ç¶ç¶²è·¯æ¼èªé³æä»¶æª¢ç´¢ä¹ç ç©¶ (Exploring Query Intent and Neural Network modeling Techniques for Spoken Document Retrieval) [In {C}hinese],2017,0,0,3,1,2341,tienhong lo,Proceedings of the 29th Conference on Computational Linguistics and Speech Processing ({ROCLING} 2017),0,None
O17-1032,"åºåæ¨è¨èé\
å°æ¹æ³ç¨æ¼èªé³è¾¨è­é¯èª¤åµæ¸¬åä¿®æ­£ (On the Use of Sequence Labeling and Matching Methods for {ASR} Error Detection and Correction) [In {C}hinese]",2017,0,0,5,0,32729,chiahua wu,Proceedings of the 29th Conference on Computational Linguistics and Speech Processing ({ROCLING} 2017),0,None
O16-3003,"ä½¿ç¨å­å\
¸å­¸ç¿æ³æ¼å¼·å¥æ§èªé³è¾¨è­ (The Use of Dictionary Learning Approach for Robustness Speech Recognition) [In {C}hinese]",2016,0,0,4,1,2348,bicheng yan,"International Journal of Computational Linguistics {\\&} {C}hinese Language Processing, Volume 21, Number 2, {D}ecember 2016",0,None
O16-3004,è©ä¼°å°ºåº¦ç¸éæä½³åæ¹æ³æ¼è¯èªé¯èª¤ç¼é³æª¢æ¸¬ä¹ç ç©¶ (Evaluation Metric-related Optimization Methods for {M}andarin Mispronunciation Detection) [In {C}hinese],2016,0,0,6,1,34556,yaochi hsu,"International Journal of Computational Linguistics {\\&} {C}hinese Language Processing, Volume 21, Number 2, {D}ecember 2016",0,None
O16-3006,èåå¤ä»»åå­¸ç¿é¡ç¥ç¶ç¶²è·¯è²å­¸æ¨¡åè¨ç·´æ¼æè­°èªé³è¾¨è­ä¹ç ç©¶ (Leveraging Multi-Task Learning with Neural Network Based Acoustic Modeling for Improved Meeting Speech Recognition) [In {C}hinese],2016,0,0,6,1,34557,minghan yang,"International Journal of Computational Linguistics {\\&} {C}hinese Language Processing, Volume 21, Number 2, {D}ecember 2016",0,None
O16-1001,è©ä¼°å°ºåº¦ç¸éæä½³åæ¹æ³æ¼è¯èªé¯èª¤ç¼é³æª¢æ¸¬ä¹ç ç©¶(Evaluation Metric-related Optimization Methods for {M}andarin Mispronunciation Detection) [In {C}hinese],2016,0,0,5,1,34556,yaochi hsu,Proceedings of the 28th Conference on Computational Linguistics and Speech Processing ({ROCLING} 2016),0,None
O16-1002,èåå¤ä»»åå­¸ç¿é¡ç¥ç¶ç¶²è·¯è²å­¸æ¨¡åè¨ç·´æ¼æè­°èªé³è¾¨è­ä¹ç ç©¶(Leveraging Multi-task Learning with Neural Network Based Acoustic Modeling for Improved Meeting Speech Recognition) [In {C}hinese],2016,0,0,5,1,34557,minghan yang,Proceedings of the 28th Conference on Computational Linguistics and Speech Processing ({ROCLING} 2016),0,None
O16-1003,"ä½¿ç¨å­å\
¸å­¸ç¿æ³æ¼å¼·å¥æ§èªé³è¾¨è­(The Use of Dictionary Learning Approach for Robustness Speech Recognition) [In {C}hinese]",2016,0,0,4,1,2348,bicheng yan,Proceedings of the 28th Conference on Computational Linguistics and Speech Processing ({ROCLING} 2016),0,None
O16-1012,éç¨åºåå°åºåçææ¶æ§æ¼éå¯«å¼èªåæè¦(Exploiting Sequence-to-Sequence Generation Framework for Automatic Abstractive Summarization)[In {C}hinese],2016,0,0,6,0.701754,25636,yulun hsieh,Proceedings of the 28th Conference on Computational Linguistics and Speech Processing ({ROCLING} 2016),0,None
O16-1024,åºæ¼æ·±å±¤é¡ç¥ç¶ç¶²è·¯åè¡¨ç¤ºå­¸ç¿æè¡ä¹æä»¶å¯è®æ§åé¡(Classification of Text Readability Based on Deep Neural Network and Representation Learning Techniques)[In {C}hinese],2016,0,0,4,1,27254,houchiang tseng,Proceedings of the 28th Conference on Computational Linguistics and Speech Processing ({ROCLING} 2016),0,None
C16-1035,Learning to Distill: The Essence Vector Modeling Framework,2016,25,5,3,1,2312,kuanyu chen,"Proceedings of {COLING} 2016, the 26th International Conference on Computational Linguistics: Technical Papers",0,"In the context of natural language processing, representation learning has emerged as a newly active research subject because of its excellent performance in many applications. Learning representations of words is a pioneering study in this school of research. However, paragraph (or sentence and document) embedding learning is more suitable/reasonable for some tasks, such as sentiment classification and document summarization. Nevertheless, as far as we are aware, there is only a dearth of research focusing on launching unsupervised paragraph embedding methods. Classic paragraph embedding methods infer the representation of a given paragraph by considering all of the words occurring in the paragraph. Consequently, those stop or function words that occur frequently may mislead the embedding learning process to produce a misty paragraph representation. Motivated by these observations, our major contributions are twofold. First, we propose a novel unsupervised paragraph embedding method, named the essence vector (EV) model, which aims at not only distilling the most representative information from a paragraph but also excluding the general background information to produce a more informative low-dimensional vector representation for the paragraph. We evaluate the proposed EV model on benchmark sentiment classification and multi-document summarization tasks. The experimental results demonstrate the effectiveness and applicability of the proposed embedding method. Second, in view of the increasing importance of spoken content processing, an extension of the EV model, named the denoising essence vector (D-EV) model, is proposed. The D-EV model not only inherits the advantages of the EV model but also can infer a more robust representation for a given spoken paragraph against imperfect speech recognition. The utility of the D-EV model is evaluated on a spoken document summarization task, confirming the effectiveness of the proposed embedding method in relation to several well-practiced and state-of-the-art summarization methods."
O15-3004,ç¯éå¼èªé³æä»¶æè¦ä½¿ç¨è¡¨ç¤ºæ³å­¸ç¿æè¡ (Extractive Spoken Document Summarization with Representation Learning Techniques) [In {C}hinese],2015,0,0,5,1,32687,kaiwun shih,"International Journal of Computational Linguistics {\\&} {C}hinese Language Processing, Volume 20, Number 2, {D}ecember 2015 - Special Issue on Selected Papers from {ROCLING} {XXVII}",0,None
O15-3005,èª¿è®é »è­åè§£æè¡æ¼å¼·å¥èªé³è¾¨è­ä¹ç ç©¶ (Investigating Modulation Spectrum Factorization Techniques for Robust Speech Recognition) [In {C}hinese],2015,0,0,5,0,37547,tinghao chang,"International Journal of Computational Linguistics {\\&} {C}hinese Language Processing, Volume 20, Number 2, {D}ecember 2015 - Special Issue on Selected Papers from {ROCLING} {XXVII}",0,None
O15-1001,è¡¨ç¤ºæ³å­¸ç¿æè¡æ¼ç¯éå¼èªé³æä»¶æè¦ä¹ç ç©¶(A Study on Representation Learning Techniques for Extractive Spoken Document Summarization) [In {C}hinese],2015,0,0,2,1,32687,kaiwun shih,Proceedings of the 27th Conference on Computational Linguistics and Speech Processing ({ROCLING} 2015),0,None
O15-1002,ä½¿ç¨è©åéè¡¨ç¤ºèæ¦å¿µè³è¨æ¼ä¸­æå¤§è©å½é£çºèªé³è¾¨è­ä¹èªè¨æ¨¡åèª¿é©(Exploring Word Embedding and Concept Information for Language Model Adaptation in {M}andarin Large Vocabulary Continuous Speech Recognition) [In {C}hinese],2015,0,0,4,0,37556,ssucheng chen,Proceedings of the 27th Conference on Computational Linguistics and Speech Processing ({ROCLING} 2015),0,None
O15-1008,å¯è®æ§é æ¸¬æ¼ä¸­å°å­¸åèªææç§æ¸ååªè¯èª²å¤è®ç©ä¹ç ç©¶(A Study of Readability Prediction on Elementary and Secondary {C}hinese Textbooks and Excellent Extracurricular Reading Materials) [In {C}hinese],2015,0,0,4,0,37566,yinian liu,Proceedings of the 27th Conference on Computational Linguistics and Speech Processing ({ROCLING} 2015),0,None
O15-1010,èª¿è®é »è­åè§£ä¹æ¹è¯æ¼å¼·å¥æ§èªé³è¾¨è­(Several Refinements of Modulation Spectrum Factorization for Robust Speech Recognition) [In {C}hinese],2015,0,0,5,0,37547,tinghao chang,Proceedings of the 27th Conference on Computational Linguistics and Speech Processing ({ROCLING} 2015),0,None
O15-1011,èåå¤ç¨®æ·±å±¤é¡ç¥ç¶ç¶²è·¯è²å­¸æ¨¡åèåé¡æè¡æ¼è¯èªé¯èª¤ç¼é³æª¢æ¸¬ä¹ç ç©¶(Exploring Combinations of Various Deep Neural Network based Acoustic Models and Classification Techniques for {M}andarin Mispro-nunciation Detection)[In {C}hinese],2015,0,0,6,1,34556,yaochi hsu,Proceedings of the 27th Conference on Computational Linguistics and Speech Processing ({ROCLING} 2015),0,None
O14-5004,ä½¿ç¨æ¦å¿µè³è¨æ¼ä¸­æå¤§è©å½é£çºèªé³è¾¨è­ä¹ç ç©¶ (Exploring Concept Information for {M}andarin Large Vocabulary Continuous Speech Recognition) [In {C}hinese],2014,0,0,3,0,39216,pohan hao,"International Journal of Computational Linguistics {\\&} {C}hinese Language Processing, Volume 19, Number 4, {D}ecember 2014 - Special Issue on Selected Papers from {ROCLING} {XXVI}",0,"Language modeling (LM) is part and parcel of automatic speech recognition (ASR), since it can assist ASR to constrain the acoustic analysis, guide the search through multiple candidate word strings, and quantify the acceptability of the final output hypothesis given an input utterance. This paper investigates and develops language model adaptation techniques for use in ASR and its main contribution is two-fold. First, we propose a novel concept language modeling (CLM) approach to rendering the relationships between a search history and an upcoming word. Second, the instantiations of CLM are constructed with different levels of lexical granularities, such as words and document clusters. In addition, we also explore the incorporation of word proximity cues into the model formulation of CLM, getting around the xe2x80x9cbag-of-wordsxe2x80x9d assumption. A series of experiments conducted on a Mandarin large vocabulary continuous speech recognition (LVCSR) task demonstrate that our proposed language models can offer substantial improvements over the baseline N-gram system, and achieve performance competitive to, or better than, some state-of-the-art language model adaptation methods."
O14-1001,éç¨æ¦å¿µæ¨¡ååæè¡æ¼ä¸­æå¤§è©å½é£çºèªé³è¾¨è­ä¹èªè¨æ¨¡åèª¿é© (Leveraging Concept Modeling Techniques for Language Model Adaptation in {M}andarin Large Vocabulary Continuous Speech Recognition) [In {C}hinese],2014,0,0,3,0,39216,pohan hao,Proceedings of the 26th Conference on Computational Linguistics and Speech Processing ({ROCLING} 2014),0,None
O14-1002,æ¢ç©¶æ°ç©èªå¥æ¨¡ååæè¡æ¼ç¯éå¼èªé³æè¦ (Investigating Novel Sentence Modeling Techniques for Extractive Speech Summarization) [In {C}hinese],2014,0,0,4,1,19036,shihhung liu,Proceedings of the 26th Conference on Computational Linguistics and Speech Processing ({ROCLING} 2014),0,None
D14-1156,Leveraging Effective Query Modeling Techniques for Speech Recognition and Summarization,2014,47,7,3,1,2312,kuanyu chen,Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP}),0,"Statistical language modeling (LM) that purports to quantify the acceptability of a given piece of text has long been an interesting yet challenging research area. In particular, language modeling for information retrieval (IR) has enjoyed remarkable empirical success; one emerging stream of the LM approach for IR is to employ the pseudo-relevance feedback process to enhance the representation of an input query so as to improve retrieval effectiveness. This paper presents a continuation of such a general line of research and the main contribution is threefold. First, we propose a principled framework which can unify the relationships among several widely-used query modeling formulations. Second, on top of the successfully developed framework, we propose an extended query modeling formulation by incorporating critical query-specific information cues to guide the model estimation. Third, we further adopt and formalize such a framework to the speech recognition and summarization tasks. A series of empirical experiments reveal the feasibility of such an LM framework and the performance merits of the deduced models on these two tasks."
O13-1001,æ¹è¯èªå¥æ¨¡åæè¡æ¼ç¯éå¼èªé³æè¦ä¹ç ç©¶ (Improved Sentence Modeling Techniques for Extractive Speech Summarization) [In {C}hinese],2013,0,0,5,1,19036,shihhung liu,Proceedings of the 25th Conference on Computational Linguistics and Speech Processing ({ROCLING} 2013),0,None
O13-1014,æ¹è¯èª¿è®é »è­çµ±è¨åç­åæ³æ¼å¼·å¥æ§èªé³è¾¨è­ä¹ç ç©¶ (Improved Modulation Spectrum Histogram Equalization for Robust Speech Recognition) [In {C}hinese],2013,0,0,2,0,32732,yuchen kao,Proceedings of the 25th Conference on Computational Linguistics and Speech Processing ({ROCLING} 2013),0,None
O12-5005,èªé³è¾¨è­ä½¿ç¨çµ±è¨åç­åæ¹æ³ (Speech Recognition Leveraging Histogram Equalization Methods) [In {C}hinese],2012,-1,-1,3,0,42731,hsinju hsieh,"International Journal of Computational Linguistics {\\&} {C}hinese Language Processing, Volume 17, Number 4, {D}ecember 2012-Special Issue on Selected Papers from {ROCLING} {XXIV}",0,None
O12-2004,A Comparative Study of Methods for Topic Modeling in Spoken Document Retrieval,2012,20,0,2,1,42746,shihhsiang lin,"International Journal of Computational Linguistics {\\&} {C}hinese Language Processing, Volume 17, Number 1, March 2012",0,"Topic modeling for information retrieval (IR) has attracted significant attention and demonstrated good performance in a wide variety of tasks over the years. In this paper, we first present a comprehensive comparison of various topic modeling approaches, including the so-called document topic models (DTM) and word topic models (WTM), for Chinese spoken document retrieval (SDR). Moreover, different granularities of index features, including words, subword units, and their combinations, are also exploited to work in conjunction with various extensions of topic modeling presented in this paper, so as to alleviate SDR performance degradation caused by speech recognition errors. All of the experiments were performed on the TDT Chinese collection."
O12-1001,æ¹è¯å¼çµ±è¨åç­åæ³å¼·éµæ§èªé³è¾¨è­ä¹ç ç©¶ (Improved Histogram Equalization Methods for Robust Speech Recognition) [In {C}hinese],2012,0,0,3,0,42731,hsinju hsieh,Proceedings of the 24th Conference on Computational Linguistics and Speech Processing ({ROCLING} 2012),0,None
O12-1008,éè¿´å¼é¡ç¥ç¶ç¶²è·¯èªè¨æ¨¡åæç¨é¡å¤è³è¨æ¼èªé³è¾¨è­ä¹ç ç©¶ (Recurrent Neural Network-based Language Modeling with Extra Information Cues for Speech Recognition) [In {C}hinese],2012,0,0,4,0,42755,bangxuan huang,Proceedings of the 24th Conference on Computational Linguistics and Speech Processing ({ROCLING} 2012),0,None
O11-2001,æ©çå¼èª¿è®é »è­åè§£æ¼å¼·å¥æ§èªé³è¾¨è­ (Probabilistic Modulation Spectrum Factorization for Robust Speech Recognition) [In {C}hinese],2011,0,0,3,0,44706,wenyi chu,{ROCLING} 2011 Poster Papers,0,None
O11-1001,å¯¦è­æ¢ç©¶å¤ç¨®éå¥å¼èªè¨æ¨¡åæ¼èªé³è¾¨è­ä¹ç ç©¶ (Empirical Comparisons of Various Discriminative Language Models for Speech Recognition) [In {C}hinese],2011,27,1,4,0,44725,minhsuan lai,Proceedings of the 23rd Conference on Computational Linguistics and Speech Processing ({ROCLING} 2011),0,None
I11-1149,An Effective and Robust Framework for Transliteration Exploration,2011,20,0,4,0,40145,eaee jan,Proceedings of 5th International Joint Conference on Natural Language Processing,0,"Transliteration is the process of proper name translation based on pronunciation. It is an important process in many multilingual natural language tasks. A common and essential component of transliteration approaches is a verification mechanism that tests if the two names in different languages are translations of each other. Although many transliteration systems have verification as a component, verification as a stand-alone problem is relatively new. In this paper, we propose a simple, effective and robust training framework for the task of verification. We show the many applications of the verification techniques. Our proposed method can operate on both phonemic and orthographic inputs. Our best results show that a simple, straightforward orthographic representation is sufficient and no complex training method is needed. It is effective because it achieves remarkable accuracies. It is robust because it is language-independent. We show that on Chinese and Korean our technique achieves equal error rate well below 1% and around 1% for Japanese using 2009 and 2010 NEWS transliteration generation share task dataset. Our results also show that the orthographic system outperforms the phonemic system. This is especially encouraging because the orthographic inputs are easier to generate and secondly, one does not need to resort to more complex training algorithm to achieve excellent results. This approach is integrated for proper name based cross lingual information retrieval without translation. 1"
P10-1009,A Risk Minimization Framework for Extractive Speech Summarization,2010,31,18,2,1,42746,shihhsiang lin,Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,1,"In this paper, we formulate extractive summarization as a risk minimization problem and propose a unified probabilistic framework that naturally combines supervised and unsupervised summarization models to inherit their individual merits as well as to overcome their inherent limitations. In addition, the introduction of various loss functions also provides the summarization framework with a flexible but systematic way to render the redundancy and coherence relationships among sentences and between sentences and the whole document, respectively. Experiments on speech summarization show that the methods deduced from our framework are very competitive with existing summarization approaches."
O10-1003,éå¥å¼èªè¨æ¨¡åæ¼èªé³è¾¨è­çµæéæ°æåºä¹ç ç©¶ (Exploiting Discriminative Language Models for Reranking Speech Recognition Hypotheses) [In {C}hinese],2010,0,0,3,0,45746,chiawen liu,Proceedings of the 22nd Conference on Computational Linguistics and Speech Processing ({ROCLING} 2010),0,None
O10-1005,æ´åééè³è¨æ¼éå¥å¼è²å­¸æ¨¡åè¨ç·´æ¹æ³ä¹æ¯è¼ç ç©¶ (A Comparative Study on Margin-Based Discriminative Training of Acoustic Models) [In {C}hinese],2010,0,0,2,0,45749,yuengtien lo,Proceedings of the 22nd Conference on Computational Linguistics and Speech Processing ({ROCLING} 2010),0,None
O09-1001,ç¸ä¼¼åº¦æ¯çå¼éå¥åææç¨æ¼å¤§è©å½é£çºèªé³è¾¨è­ (Likelihood Ratio Based Discriminant Analysis for Large Vocabulary Continuous Speech Recognition) [In {C}hinese],2009,0,0,2,0,32683,hungshin lee,Proceedings of the 21st Conference on Computational Linguistics and Speech Processing,0,None
O09-1014,ä¸»é¡èªè¨æ¨¡åæ¼å¤§è©å½é£çºèªé³è¾¨è­ä¹ç ç©¶ (On the Use of Topic Models for Large-Vocabulary Continuous Speech Recognition) [In {C}hinese],2009,0,0,2,1,2312,kuanyu chen,Proceedings of the 21st Conference on Computational Linguistics and Speech Processing,0,None
O08-5005,Improved Minimum Phone Error based Discriminative Training of Acoustic Models for {M}andarin Large Vocabulary Continuous Speech Recognition,2008,23,3,4,1,19036,shihhung liu,"International Journal of Computational Linguistics {\\&} {C}hinese Language Processing, Volume 13, Number 3, September 2008: Special Issue on Selected Papers from {ROCLING} {XIX}",0,"This paper considers minimum phone error (MPE) based discriminative training of acoustic models for Mandarin broadcast news recognition. We present a new phone accuracy function based on the frame-level accuracy of hypothesized phone arcs instead of using the raw phone accuracy function of MPE training. Moreover, a novel data selection approach based on the frame-level normalized entropy of Gaussian posterior probabilities obtained from the word lattice of the training utterance is explored. It has the merit of making the training algorithm focus much more on the training statistics of those frame samples that center nearly around the decision boundary for better discrimination. The underlying characteristics of the presented approaches are extensively investigated, and their performance is verified by comparison with the standard MPE training approach as well as the other related work. Experiments conducted on broadcast news collected in Taiwan demonstrate that the integration of the frame-level phone accuracy calculation and data selection yields slight but consistent improvements over the baseline system."
O07-4006,A Comparative Study of Histogram Equalization ({HEQ}) for Robust Speech Recognition,2007,26,9,3,1,42746,shihhsiang lin,"International Journal of Computational Linguistics {\\&} {C}hinese Language Processing, Volume 12, Number 2, June 2007",0,"The performance of current automatic speech recognition (ASR) systems often deteriorates radically when the input speech is corrupted by various kinds of noise sources. Quite a few techniques have been proposed to improve ASR robustness over the past several years. Histogram equalization (HEQ) is one of the most efficient techniques that have been used to reduce the mismatch between training and test acoustic conditions. This paper presents a comparative study of various HEQ approaches for robust ASR. Two representative HEQ approaches, namely, the table-based histogram equalization (THEQ) and the quantile-based histogram equalization (QHEQ), were first investigated. Then, a polynomial-fit histogram equalization (PHEQ) approach, exploring the use of the data fitting scheme to efficiently approximate the inverse of the cumulative density function of training speech for HEQ, was proposed. Moreover, the temporal average (TA) operation was also performed on the feature vector components to alleviate the influence of sharp peaks and valleys caused by non-stationary noises. All the experiments were carried out on the Aurora 2 database and task. Very encouraging results were initially demonstrated. The best recognition performance was achieved by combing PHEQ with TA. Relative word error rate reductions of 68% and 40% over the MFCC-based baseline system, respectively, for clean- and multi- condition training, were obtained."
O07-1006,æ¹åä»¥æå°åé³ç´ é¯èª¤çºåºç¤çéå¥å¼è²å­¸æ¨¡åè¨ç·´æ¼ä¸­æé£çºèªé³è¾¨è­ä¹ç ç©¶ (Improved Minimum Phone Error based Discriminative Training of Acoustic Models for {C}hinese Continuous Speech Reconigtion) [In {C}hinese],2007,0,0,3,1,19036,shihhung liu,Proceedings of the 19th Conference on Computational Linguistics and Speech Processing,0,None
O06-4002,An Empirical Study of Word Error Minimization Approaches for {M}andarin Large Vocabulary Continuous Speech Recognition,2006,27,0,4,1,50005,jenwei kuo,"International Journal of Computational Linguistics {\\&} {C}hinese Language Processing, Volume 11, Number 3, September 2006: Special Issue on Selected Papers from {ROCLING} {XVII}",0,"This paper presents an empirical study of word error minimization approaches for Mandarin large vocabulary continuous speech recognition (LVCSR). First, the minimum phone error (MPE) criterion, which is one of the most popular discriminative training criteria, is extensively investigated for both acoustic model training and adaptation in a Mandarin LVCSR system. Second, the word error minimization (WEM) criterion, used to rescore N-best word strings, is appropriately modified for a Mandarin LVCSR system. Finally, a series of speech recognition experiments is conducted on the MATBN Mandarin Chinese broadcast news corpus. The experiment results demonstrate that the MPE training approach reduces the character error rate (CER) by 12% for a system initially trained with the maximum likelihood (ML) approach. Meanwhile, for unsupervised acoustic model adaptation, MPE-based linear regression (MPELR) adaptation outperforms conventional maximum likelihood linear regression (MLLR) in terms of CER reduction. When the WEM decoding approach is used for N-best rescoring, a slight performance gain over the conventional maximum a posteriori (MAP) decoding method is also observed."
O06-1011,çµ±è¨åç­åæ³æ¼éè¨èªé³è¾¨è­ä¹é²ä¸æ­¥ç ç©¶ (An Improved Histogram Equalization Approach for Robust Speech Recognition) [In {C}hinese],2006,0,0,3,1,42746,shihhsiang lin,Proceedings of the 18th Conference on Computational Linguistics and Speech Processing,0,None
O05-3004,{MATBN}: A {M}andarin {C}hinese Broadcast News Corpus,2005,-1,-1,2,0.772067,2311,hsinmin wang,"International Journal of Computational Linguistics {\\&} {C}hinese Language Processing, Volume 10, Number 2, June 2005: Special Issue on Annotated Speech Corpora",0,None
O05-2001,Lightly Supervised and Data-Driven Approaches to {M}andarin Broadcast News Transcription,2005,0,4,1,1,2322,berlin chen,"International Journal of Computational Linguistics {\\&} {C}hinese Language Processing, Volume 10, Number 1, March 2005",0,None
O05-1025,é¢¨éªæå°åæºåå¨ä¸­æå¤§è©å½é£çºèªé³è¾¨è­ä¹ç ç©¶ (Risk Minimization Criterion for {M}andarin Large Vocabulary Continuous Speech Recognition) [In {C}hinese],2005,0,0,3,1,50005,jenwei kuo,Proceedings of the 17th Conference on Computational Linguistics and Speech Processing,0,None
O04-1003,éç£ç£å¼å­¸ç¿æ¼ä¸­æé»è¦æ°èèªåè½å¯«ä¹åæ­¥æç¨ (Unsupervised Learning for {C}hinese Broadcast News Transcription) [In {C}hinese],2004,0,0,3,1,50005,jenwei kuo,Proceedings of the 16th Conference on Computational Linguistics and Speech Processing,0,None
H01-1050,{M}andarin-{E}nglish Information: Investigating Translingual Speech Retrieval,2001,20,17,2,0,36519,helen meng,Proceedings of the First International Conference on Human Language Technology Research,0,"This paper describes the Mandarin-English Information (MEI) project, where we investigated the problem of cross-language spoken document retrieval (CL-SDR), and developed one of the first English-Chinese CL-SDR systems. Our system accepts an entire English news story (text) as query, and retrieves relevant Chinese broadcast news stories (audio) from the document collection. Hence this is a cross-language and cross-media retrieval task. We applied a multi-scale approach to our problem, which unifies the use of phrases, words and subwords in retrieval. The English queries are translated into Chinese by means of a dictionary-based approach, where we have integrated phrase-based translation with word-by-word translation. Untranslatable named entities are transliterated by a novel subword translation technique. The multi-scale approach can be divided into three subtasks -- multi-scale query formulation, multi-scale audio indexing (by speech recognition) and multi-scale retrieval. Experimental results demonstrate that the use of phrase-based translation and subword translation gave performance gains, and multi-scale retrieval outperforms word-based retrieval."
