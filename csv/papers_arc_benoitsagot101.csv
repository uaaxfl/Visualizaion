2021.wnut-1.47,Can Character-based Language Models Improve Downstream Task Performances In Low-Resource And Noisy Language Scenarios?,2021,-1,-1,2,0,249,arij riabi,Proceedings of the Seventh Workshop on Noisy User-generated Text (W-NUT 2021),0,"Recent impressive improvements in NLP, largely based on the success of contextual neural language models, have been mostly demonstrated on at most a couple dozen high- resource languages. Building language mod- els and, more generally, NLP systems for non- standardized and low-resource languages remains a challenging task. In this work, we fo- cus on North-African colloquial dialectal Arabic written using an extension of the Latin script, called NArabizi, found mostly on social media and messaging communication. In this low-resource scenario with data display- ing a high level of variability, we compare the downstream performance of a character-based language model on part-of-speech tagging and dependency parsing to that of monolingual and multilingual models. We show that a character-based model trained on only 99k sentences of NArabizi and fined-tuned on a small treebank of this language leads to performance close to those obtained with the same architecture pre- trained on large multilingual and monolingual models. Confirming these results a on much larger data set of noisy French user-generated content, we argue that such character-based language models can be an asset for NLP in low-resource and high language variability set- tings."
2021.naacl-main.38,When Being Unseen from m{BERT} is just the Beginning: Handling New Languages With Multilingual Language Models,2021,-1,-1,3,1,3345,benjamin muller,Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"Transfer learning based on pretraining language models on a large amount of raw data has become a new norm to reach state-of-the-art performance in NLP. Still, it remains unclear how this approach should be applied for unseen languages that are not covered by any available large-scale multilingual language model and for which only a small amount of raw data is generally available. In this work, by comparing multilingual and monolingual models, we show that such models behave in multiple ways on unseen languages. Some languages greatly benefit from transfer learning and behave similarly to closely related high resource languages whereas others apparently do not. Focusing on the latter, we show that this failure to transfer is largely related to the impact of the script used to write such languages. We show that transliterating those languages significantly improves the potential of large-scale multilingual language models on downstream tasks. This result provides a promising direction towards making these massively multilingual models useful for a new set of unseen languages."
2021.findings-acl.75,Can Cognate Prediction Be Modelled as a Low-Resource Machine Translation Task?,2021,-1,-1,3,1,7686,clementine fourrier,Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021,0,None
2021.emnlp-main.562,Synthetic Data Augmentation for Zero-Shot Cross-Lingual Question Answering,2021,-1,-1,4,0,249,arij riabi,Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,0,"Coupled with the availability of large scale datasets, deep learning architectures have enabled rapid progress on the Question Answering task. However, most of those datasets are in English, and the performances of state-of-the-art multilingual models are significantly lower when evaluated on non-English data. Due to high data collection costs, it is not realistic to obtain annotated data for each language one desires to support. We propose a method to improve the Cross-lingual Question Answering performance without requiring additional annotated data, leveraging Question Generation models to produce synthetic samples in a cross-lingual fashion. We show that the proposed method allows to significantly outperform the baselines trained on English data only. We report a new state-of-the-art on four datasets: MLQA, XQuAD, SQuAD-it and PIAF (fr)."
2021.eacl-main.189,"First Align, then Predict: Understanding the Cross-Lingual Ability of Multilingual {BERT}",2021,-1,-1,3,1,3345,benjamin muller,Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume,0,"Multilingual pretrained language models have demonstrated remarkable zero-shot cross-lingual transfer capabilities. Such transfer emerges by fine-tuning on a task of interest in one language and evaluating on a distinct language, not seen during the fine-tuning. Despite promising results, we still lack a proper understanding of the source of this transfer. Using a novel layer ablation technique and analyses of the model{'}s internal representations, we show that multilingual BERT, a popular multilingual language model, can be viewed as the stacking of two sub-networks: a multilingual encoder followed by a task-specific language-agnostic predictor. While the encoder is crucial for cross-lingual transfer and remains mostly unchanged during fine-tuning, the task predictor has little importance on the transfer and can be reinitialized during fine-tuning. We present extensive experiments with three distinct tasks, seventeen typologically diverse languages and multiple domains to support our hypothesis."
2020.lt4hala-1.12,Comparing Statistical and Neural Models for Learning Sound Correspondences,2020,-1,-1,2,1,7686,clementine fourrier,Proceedings of LT4HALA 2020 - 1st Workshop on Language Technologies for Historical and Ancient Languages,0,"Cognate prediction and proto-form reconstruction are key tasks in computational historical linguistics that rely on the study of sound change regularity. Solving these tasks appears to be very similar to machine translation, though methods from that field have barely been applied to historical linguistics. Therefore, in this paper, we investigate the learnability of sound correspondences between a proto-language and daughter languages for two machine-translation-inspired models, one statistical, the other neural. We first carry out our experiments on plausible artificial languages, without noise, in order to study the role of each parameter on the algorithms respective performance under almost perfect conditions. We then study real languages, namely Latin, Italian and Spanish, to see if those performances generalise well. We show that both model types manage to learn sound changes despite data scarcity, although the best performing model type depends on several parameters such as the size of the training data, the ambiguity, and the prediction direction."
2020.lrec-1.392,Methodological Aspects of Developing and Managing an Etymological Lexical Resource: Introducing {E}tym{DB}-2.0,2020,-1,-1,2,1,7686,clementine fourrier,Proceedings of the 12th Language Resources and Evaluation Conference,0,"Diachronic lexical information is not only important in the field of historical linguistics, but is also increasingly used in NLP, most recently for machine translation of low resource languages. Therefore, there is a need for fine-grained, large-coverage and accurate etymological lexical resources. In this paper, we propose a set of guidelines to generate such resources, for each step of the life-cycle of an etymological lexicon: creation, update, evaluation, dissemination, and exploitation. To illustrate the guidelines, we introduce EtymDB 2.0, an etymological database automatically generated from the Wiktionary, which contains 1.8 million lexemes, linked by more than 700,000 fine-grained etymological relations, across 2,536 living and dead languages. We also introduce use cases for which EtymDB 2.0 could represent a key resource, such as phylogenetic tree generation, low resource machine translation or medieval languages study."
2020.lrec-1.393,{OF}r{L}ex: A Computational Morphological and Syntactic Lexicon for {O}ld {F}rench,2020,-1,-1,2,0,5289,gael guibon,Proceedings of the 12th Language Resources and Evaluation Conference,0,"In this paper we describe our work on the development and enrichment of OFrLex, a freely available, large-coverage morphological and syntactic Old French lexicon. We rely on several heterogeneous language resources to extract structured and exploitable information. The extraction follows a semi-automatic procedure with substantial manual steps to respond to difficulties encountered while aligning lexical entries from distinct language resources. OFrLex aims at improving natural language processing tasks on Old French such as part-of-speech tagging and dependency parsing. We provide quantitative information on OFrLex and discuss its reliability. We also describe and evaluate a semi-automatic, word-embedding-based lexical enrichment process aimed at increasing the accuracy of the resource. Results of this extension technique will be manually validated in the near future, a step that will take advantage of OFrLex{'}s viewing, searching and editing interface, which is already accessible online."
2020.lrec-1.569,Establishing a New State-of-the-Art for {F}rench Named Entity Recognition,2020,-1,-1,5,0,17806,pedro suarez,Proceedings of the 12th Language Resources and Evaluation Conference,0,"The French TreeBank developed at the University Paris 7 is the main source of morphosyntactic and syntactic annotations for French. However, it does not include explicit information related to named entities, which are among the most useful information for several natural language processing tasks and applications. Moreover, no large-scale French corpus with named entity annotations contain referential information, which complement the type and the span of each mention with an indication of the entity it refers to. We have manually annotated the French TreeBank with such information, after an automatic pre-annotation step. We sketch the underlying annotation guidelines and we provide a few figures about the resulting annotations."
2020.lrec-1.577,Controllable Sentence Simplification,2020,-1,-1,3,1,17824,louis martin,Proceedings of the 12th Language Resources and Evaluation Conference,0,"Text simplification aims at making a text easier to read and understand by simplifying grammar and structure while keeping the underlying information identical. It is often considered an all-purpose generic task where the same simplification is suitable for all; however multiple audiences can benefit from simplified text in different ways. We adapt a discrete parametrization mechanism that provides explicit control on simplification systems based on Sequence-to-Sequence models. As a result, users can condition the simplifications returned by a model on attributes such as length, amount of paraphrasing, lexical complexity and syntactic complexity. We also show that carefully chosen values of these attributes allow out-of-the-box Sequence-to-Sequence models to outperform their standard counterparts on simplification benchmarks. Our model, which we call ACCESS (as shorthand for AudienCe-CEntric Sentence Simplification), establishes the state of the art at 41.87 SARI on the WikiLarge test set, a +1.42 improvement over the best previously reported score."
2020.jeptalnrecital-taln.5,Les mod{\\`e}les de langue contextuels Camembert pour le fran{\\c{c}}ais : impact de la taille et de l{'}h{\\'e}t{\\'e}rog{\\'e}n{\\'e}it{\\'e} des donn{\\'e}es d{'}entrainement ({C} {AMEM} {BERT} Contextual Language Models for {F}rench: Impact of Training Data Size and Heterogeneity ),2020,-1,-1,7,1,17824,louis martin,"Actes de la 6e conf{\\'e}rence conjointe Journ{\\'e}es d'{\\'E}tudes sur la Parole (JEP, 33e {\\'e}dition), Traitement Automatique des Langues Naturelles (TALN, 27e {\\'e}dition), Rencontre des {\\'E}tudiants Chercheurs en Informatique pour le Traitement Automatique des Langues (R{\\'E}CITAL, 22e {\\'e}dition). Volume 2 : Traitement Automatique des Langues Naturelles",0,"Les mod{\`e}les de langue neuronaux contextuels sont d{\'e}sormais omnipr{\'e}sents en traitement automatique des langues. Jusqu{'}{\`a} r{\'e}cemment, la plupart des mod{\`e}les disponibles ont {\'e}t{\'e} entra{\^\i}n{\'e}s soit sur des donn{\'e}es en anglais, soit sur la concat{\'e}nation de donn{\'e}es dans plusieurs langues. L{'}utilisation pratique de ces mod{\`e}les {---} dans toutes les langues sauf l{'}anglais {---} {\'e}tait donc limit{\'e}e. La sortie r{\'e}cente de plusieurs mod{\`e}les monolingues fond{\'e}s sur BERT (Devlin et al., 2019), notamment pour le fran{\c{c}}ais, a d{\'e}montr{\'e} l{'}int{\'e}r{\^e}t de ces mod{\`e}les en am{\'e}liorant l{'}{\'e}tat de l{'}art pour toutes les t{\^a}ches {\'e}valu{\'e}es. Dans cet article, {\`a} partir d{'}exp{\'e}riences men{\'e}es sur CamemBERT (Martin et al., 2019), nous montrons que l{'}utilisation de donn{\'e}es {\`a} haute variabilit{\'e} est pr{\'e}f{\'e}rable {\`a} des donn{\'e}es plus uniformes. De fa{\c{c}}on plus surprenante, nous montrons que l{'}utilisation d{'}un ensemble relativement petit de donn{\'e}es issues du web (4Go) donne des r{\'e}sultats aussi bons que ceux obtenus {\`a} partir d{'}ensembles de donn{\'e}es plus grands de deux ordres de grandeurs (138Go)."
2020.cmlc-1.3,{F}rench Contextualized Word-Embeddings with a sip of {C}a{B}e{R}net: a New {F}rench Balanced Reference Corpus,2020,-1,-1,3,0,21816,murielle popafabre,Proceedings of the 8th Workshop on Challenges in the Management of Large Corpora,0,"This paper investigates the impact of different types and size of training corpora on language models. By asking the fundamental question of quality versus quantity, we compare four French corpora by pre-training four different ELMos and evaluating them on dependency parsing, POS-tagging and Named Entities Recognition downstream tasks. We present and asses the relevance of a new balanced French corpus, CaBeRnet, that features a representative range of language usage, including a balanced variety of genres (oral transcriptions, newspapers, popular magazines, technical reports, fiction, academic texts), in oral and written styles. We hypothesize that a linguistically representative corpus will allow the language models to be more efficient, and therefore yield better evaluation scores on different evaluation sets and tasks. This paper offers three main contributions: (1) two newly built corpora: (a) CaBeRnet, a French Balanced Reference Corpus and (b) CBT-fr a domain-specific corpus having both oral and written style in youth literature, (2) five versions of ELMo pre-trained on differently built corpora, and (3) a whole array of computational results on downstream tasks that deepen our understanding of the effects of corpus balance and register in NLP evaluation."
2020.acl-main.107,Building a User-Generated Content {N}orth-{A}frican {A}rabizi Treebank: Tackling Hell,2020,-1,-1,7,0,167,djame seddah,Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,1,"We introduce the first treebank for a romanized user-generated content variety of Algerian, a North-African Arabic dialect known for its frequent usage of code-switching. Made of 1500 sentences, fully annotated in morpho-syntax and Universal Dependency syntax, with full translation at both the word and the sentence levels, this treebank is made freely available. It is supplemented with 50k unlabeled sentences collected from Common Crawl and web-crawled data using intensive data-mining techniques. Preliminary experiments demonstrate its usefulness for POS tagging and dependency parsing. We believe that what we present in this paper is useful beyond the low-resource language community. This is the first time that enough unlabeled and annotated data is provided for an emerging user-generated content dialectal language with rich morphology and code switching, making it an challenging test-bed for most recent NLP approaches."
2020.acl-main.156,A Monolingual Approach to Contextualized Word Embeddings for Mid-Resource Languages,2020,-1,-1,3,0,17806,pedro suarez,Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,1,"We use the multilingual OSCAR corpus, extracted from Common Crawl via language classification, filtering and cleaning, to train monolingual contextualized word embeddings (ELMo) for five mid-resource languages. We then compare the performance of OSCAR-based and Wikipedia-based ELMo embeddings for these languages on the part-of-speech tagging and parsing tasks. We show that, despite the noise in the Common-Crawl-based OSCAR data, embeddings trained on OSCAR perform much better than monolingual embeddings trained on Wikipedia. They actually equal or improve the current state of the art in tagging and parsing for all five languages. In particular, they also improve over multilingual Wikipedia-based contextual embeddings (multilingual BERT), which almost always constitutes the previous state of the art, thereby showing that the benefit of a larger, more diverse corpus surpasses the cross-lingual benefit of multilingual embedding architectures."
2020.acl-main.424,{ASSET}: {A} Dataset for Tuning and Evaluation of Sentence Simplification Models with Multiple Rewriting Transformations,2020,44,1,5,0,1658,fernando alvamanchego,Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,1,"In order to simplify a sentence, human editors perform multiple rewriting transformations: they split it into several shorter sentences, paraphrase words (i.e. replacing complex words or phrases by simpler synonyms), reorder components, and/or delete information deemed unnecessary. Despite these varied range of possible text alterations, current models for automatic sentence simplification are evaluated using datasets that are focused on a single transformation, such as lexical paraphrasing or splitting. This makes it impossible to understand the ability of simplification models in more realistic settings. To alleviate this limitation, this paper introduces ASSET, a new dataset for assessing sentence simplification in English. ASSET is a crowdsourced multi-reference corpus where each simplification was produced by executing several rewriting transformations. Through quantitative and qualitative experiments, we show that simplifications in ASSET are better at capturing characteristics of simplicity when compared to other standard evaluation datasets for the task. Furthermore, we motivate the need for developing better methods for automatic evaluation using ASSET, since we show that current popular metrics may not be suitable when multiple simplification transformations are performed."
2020.acl-main.645,{C}amem{BERT}: a Tasty {F}rench Language Model,2020,-1,-1,8,1,17824,louis martin,Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,1,"Pretrained language models are now ubiquitous in Natural Language Processing. Despite their success, most available models have either been trained on English data or on the concatenation of data in multiple languages. This makes practical use of such models {--}in all languages except English{--} very limited. In this paper, we investigate the feasibility of training monolingual Transformer-based language models for other languages, taking French as an example and evaluating our language models on part-of-speech tagging, dependency parsing, named entity recognition and natural language inference tasks. We show that the use of web crawled data is preferable to the use of Wikipedia data. More surprisingly, we show that a relatively small web crawled dataset (4GB) leads to results that are as good as those obtained using larger datasets (130+GB). Our best performing model CamemBERT reaches or improves the state of the art in all four downstream tasks."
P19-1356,What Does {BERT} Learn about the Structure of Language?,2019,0,47,2,1,12015,ganesh jawahar,Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,1,"BERT is a recent language representation model that has surprisingly performed well in diverse language understanding benchmarks. This result indicates the possibility that BERT networks capture structural information about language. In this work, we provide novel support for this claim by performing a series of experiments to unpack the elements of English language structure learned by BERT. Our findings are fourfold. BERT{'}s phrasal representation captures the phrase-level information in the lower layers. The intermediate layers of BERT compose a rich hierarchy of linguistic information, starting with surface features at the bottom, syntactic features in the middle followed by semantic features at the top. BERT requires deeper layers while tracking subject-verb agreement to handle long-term dependency problem. Finally, the compositional scheme underlying BERT mimics classical, tree-like structures."
D19-5539,Enhancing {BERT} for Lexical Normalization,2019,0,0,2,1,3345,benjamin muller,Proceedings of the 5th Workshop on Noisy User-generated Text (W-NUT 2019),0,"Language model-based pre-trained representations have become ubiquitous in natural language processing. They have been shown to significantly improve the performance of neural models on a great variety of tasks. However, it remains unclear how useful those general models can be in handling non-canonical text. In this article, focusing on User Generated Content (UGC), we study the ability of BERT to perform lexical normalisation. Our contribution is simple: by framing lexical normalisation as a token prediction task, by enhancing its architecture and by carefully fine-tuning it, we show that BERT can be a competitive lexical normalisation model without the need of any UGC resources aside from 3,000 training sentences. To the best of our knowledge, it is the first work done in adapting and analysing the ability of this model to handle noisy UGC data."
2019.jeptalnrecital-court.12,D{\\'e}veloppement d{'}un lexique morphologique et syntaxique de l{'}ancien fran{\\c{c}}ais (Development of a morphological and syntactic lexicon of {O}ld {F}rench),2019,-1,-1,1,1,250,benoit sagot,Actes de la Conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles (TALN) PFIA 2019. Volume II : Articles courts,0,"Nous d{\'e}crivons dans cet article notre travail de d{\'e}veloppement d{'}un lexique morphologique et syntaxique {\`a} grande {\'e}chelle de l{'}ancien fran{\c{c}}ais pour le traitement automatique des langues. Nous nous sommes appuy{\'e}s sur des ressources dictionnairiques et lexicales dans lesquelles l{'}extraction d{'}informations structur{\'e}es et exploitables a n{\'e}cessit{\'e} des d{\'e}veloppements sp{\'e}cifiques. De plus, la mise en correspondance d{'}informations provenant de ces diff{\'e}rentes sources a soulev{\'e} des difficult{\'e}s. Nous donnons quelques indications quantitatives sur le lexique obtenu, et discutons de sa fiabilit{\'e} dans sa version actuelle et des perspectives d{'}am{\'e}lioration permises par l{'}existence d{'}une premi{\`e}re version, notamment au travers de l{'}analyse automatique de donn{\'e}es textuelles."
W18-7005,Reference-less Quality Estimation of Text Simplification Systems,2018,0,6,6,1,17824,louis martin,Proceedings of the 1st Workshop on Automatic Text Adaptation ({ATA}),0,"The evaluation of text simplification (TS) systems remains an open challenge. As the task has common points with machine translation (MT), TS is often evaluated using MT metrics such as BLEU. However, such metrics require high quality reference data, which is rarely available for TS. TS has the advantage over MT of being a monolingual task, which allows for direct comparisons to be made between the simplified text and its original version. In this paper, we compare multiple approaches to reference-less quality estimation of sentence-level text simplification systems, based on the dataset used for the QATS 2016 shared task. We distinguish three different dimensions: gram-maticality, meaning preservation and simplicity. We show that n-gram-based MT metrics such as BLEU and METEOR correlate the most with human judgment of grammaticality and meaning preservation, whereas simplicity is best evaluated by basic length-based metrics."
L18-1292,A multilingual collection of {C}o{NLL}-{U}-compatible morphological lexicons,2018,0,1,1,1,250,benoit sagot,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,"We introduce UDLexicons, a multilingual collection of morphological lexicons that follow the guidelines and format of the Universal Dependencies initiative. We describe the three approaches we use to create 53 morphological lexicons covering 38 languages, based on existing resources. These lexicons, which are freely available, have already proven useful for improving part-of-speech tagging accuracy in state-of-the-art architectures."
L18-1608,{C}o{NLL}-{UL}: Universal Morphological Lattices for {U}niversal {D}ependency Parsing,2018,0,2,5,0,25397,amir more,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,"Following the development of the universal dependencies (UD) framework and the CoNLL 2017 Shared Task on end-to-end UD parsing, we address the need for a universal representation of morphological analysis which on the one hand can capture a range of different alternative morphological analyses of surface tokens, and on the other hand is compatible with the segmentation and morphological annotation guidelines prescribed for UD treebanks. We propose the CoNLL universal lattices (CoNLL-UL) format, a new annotation format for word lattices that represent morphological analyses, and provide resources that obey this format for a range of typologically different languages. The resources we provide are harmonized with the two-level representation and morphological annotation in their respective UD v2 treebanks, thus enabling research on universal models for morphological and syntactic parsing , in both pipeline and joint settings, and presenting new opportunities in the development of UD resources for low-resource languages."
L18-1718,Cheating a Parser to Death: Data-driven Cross-Treebank Annotation Transfer,2018,0,0,3,0,167,djame seddah,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,"We present an efficient and accurate method for transferring annotations between two different treebanks of the same language. This method led to the creation of a new instance of the French Treebank (Abeille et al., 2003), which follows the Universal Dependency annotation scheme and which was proposed to the participants of the CoNLL 2017 Universal Dependency parsing shared task (Zeman et al., 2017). Strong results from an evaluation on our gold standard (94.75% of LAS, 99.40% UAS on the test set) demonstrate the quality of this new annotated data set and validate our approach."
K18-2023,{ELM}o{L}ex: Connecting {ELM}o and Lexicon Features for Dependency Parsing,2018,0,1,6,1,12015,ganesh jawahar,Proceedings of the {C}o{NLL} 2018 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies,0,"In this paper, we present the details of the neural dependency parser and the neural tagger submitted by our team {`}ParisNLP{'} to the CoNLL 2018 Shared Task on parsing from raw text to Universal Dependencies. We augment the deep Biaffine (BiAF) parser (Dozat and Manning, 2016) with novel features to perform competitively: we utilize an indomain version of ELMo features (Peters et al., 2018) which provide context-dependent word representations; we utilize disambiguated, embedded, morphosyntactic features from lexicons (Sagot, 2018), which complements the existing feature set. Henceforth, we call our system {`}ELMoLex{'}. In addition to incorporating character embeddings, ELMoLex benefits from pre-trained word vectors, ELMo and morphosyntactic features (whenever available) to correctly handle rare or unknown words which are prevalent in languages with complex morphology. ELMoLex ranked 11th by Labeled Attachment Score metric (70.64{\%}), Morphology-aware LAS metric (55.74{\%}) and ranked 9th by Bilexical dependency metric (60.70{\%})."
W17-6304,Improving neural tagging with lexical information,2017,0,4,1,1,250,benoit sagot,Proceedings of the 15th International Conference on Parsing Technologies,0,"Neural part-of-speech tagging has achieved competitive results with the incorporation of character-based and pre-trained word embeddings. In this paper, we show that a state-of-the-art bi-LSTM tagger can benefit from using information from morphosyntactic lexicons as additional input. The tagger, trained on several dozen languages, shows a consistent, average improvement when using lexical information, even when also using character-based embeddings, thus showing the complementarity of the different sources of lexical information. The improvements are particularly important for the smaller datasets."
W17-2212,Speeding up corpus development for linguistic research: language documentation and acquisition in {R}omansh Tuatschin,2017,5,0,2,0,29833,geraldine walther,"Proceedings of the Joint {SIGHUM} Workshop on Computational Linguistics for Cultural Heritage, Social Sciences, Humanities and Literature",0,"In this paper, we present ongoing work for developing language resources and basic NLP tools for an undocumented variety of Romansh, in the context of a language documentation and language acquisition project. Our tools are meant to improve the speed and reliability of corpus annotations for noisy data involving large amounts of code-switching, occurrences of child-speech and orthographic noise. Being able to increase the efficiency of language resource development for language documentation and acquisition research also constitutes a step towards solving the data sparsity issues with which researchers have been struggling."
W17-0805,Annotating omission in statement pairs,2017,11,0,3,0.186723,20634,hector alonso,Proceedings of the 11th Linguistic Annotation Workshop,0,"We focus on the identification of omission in statement pairs. We compare three annotation schemes, namely two different crowdsourcing schemes and manual expert annotation. We show that the simplest of the two crowdsourcing approaches yields a better annotation quality than the more complex one. We use a dedicated classifier to assess whether the annotators{'} behavior can be explained by straightforward linguistic features. The classifier benefits from a modeling that uses lexical information beyond length and overlap measures. However, for our task, we argue that expert and not crowdsourcing-based annotation is the best compromise between annotation cost and quality."
K17-3026,The {P}aris{NLP} entry at the {C}on{LL} {UD} Shared Task 2017: A Tale of a {\\#}{P}arsing{T}ragedy,2017,4,0,2,0,17825,eric clergerie,Proceedings of the {C}o{NLL} 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies,0,"We present the ParisNLP entry at the UD CoNLL 2017 parsing shared task. In addition to the UDpipe models provided, we built our own data-driven tokenization models, sentence segmenter and lexicon-based morphological analyzers. All of these were used with a range of different parsing models (neural or not, feature-rich or not, transition or graph-based, etc.) and the best combination for each language was selected. Unfortunately, a glitch in the shared task{'}s Matrix led our model selector to run generic, weakly lexicalized models, tailored for surprise languages, instead of our dataset-specific models. Because of this {\#}ParsingTragedy, we officially ranked 27th, whereas our real models finally unofficially ranked 6th."
2017.jeptalnrecital-long.12,Construction automatique d{'}une base de donn{\\'e}es {\\'e}tymologiques {\\`a} partir du wiktionary (Automatic construction of an etymological database using {W}iktionary),2017,-1,-1,1,1,250,benoit sagot,Actes des 24{\\`e}me Conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Volume 1 - Articles longs,0,"Les ressources lexicales {\'e}lectroniques ne contiennent quasiment jamais d{'}informations {\'e}tymologiques. De telles informations, convenablement formalis{\'e}es, permettraient pourtant de d{\'e}velopper des outils automatiques au service de la linguistique historique et comparative, ainsi que d{'}am{\'e}liorer significativement le traitement automatique de langues anciennes. Nous d{\'e}crivons ici le processus que nous avons mis en {\oe}uvre pour extraire des donn{\'e}es {\'e}tymologiques {\`a} partir des notices {\'e}tymologiques du wiktionary, r{\'e}dig{\'e}es en anglais. Nous avons ainsi produit une base multilingue de pr{\`e}s d{'}un million de lex{\`e}mes et une base de plus d{'}un demi-million de relations {\'e}tymologiques entre lex{\`e}mes."
W16-3905,From Noisy Questions to {M}inecraft Texts: Annotation Challenges in Extreme Syntax Scenario,2016,24,0,3,0.186723,20634,hector alonso,Proceedings of the 2nd Workshop on Noisy User-generated Text ({WNUT}),0,"User-generated content presents many challenges for its automatic processing. While many of them do come from out-of-vocabulary effects, others spawn from different linguistic phenomena such as unusual syntax. In this work we present a French three-domain data set made up of question headlines from a cooking forum, game chat logs and associated forums from two popular online games (MINECRAFT {\&} LEAGUE OF LEGENDS). We chose these domains because they encompass different degrees of lexical and syntactic compliance with canonical language. We conduct an automatic and manual evaluation of the difficulties of processing these domains for part-of-speech prediction, and introduce a pilot study to determine whether dependency analysis lends itself well to annotate these data. We also discuss the development cost of our data set."
2016.jeptalnrecital-poster.16,{\\'E}tiquetage multilingue en parties du discours avec {ME}lt (Multilingual part-of-speech tagging with {ME}lt),2016,-1,-1,1,1,250,benoit sagot,Actes de la conf{\\'e}rence conjointe JEP-TALN-RECITAL 2016. volume 2 : TALN (Posters),0,"Nous pr{\'e}sentons des travaux r{\'e}cents r{\'e}alis{\'e}s autour de MElt, syst{\`e}me discriminant d{'}{\'e}tiquetage en parties du discours. MElt met l{'}accent sur l{'}exploitation optimale d{'}informations lexicales externes pour am{\'e}liorer les performances des {\'e}tiqueteurs par rapport aux mod{\`e}les entra{\^\i}n{\'e}s seulement sur des corpus annot{\'e}s. Nous avons entra{\^\i}n{\'e} MElt sur plus d{'}une quarantaine de jeux de donn{\'e}es couvrant plus d{'}une trentaine de langues. Compar{\'e} au syst{\`e}me {\'e}tat-de-l{'}art MarMoT, MElt obtient en moyenne des r{\'e}sultats l{\'e}g{\`e}rement moins bons en l{'}absence de lexique externe, mais meilleurs lorsque de telles ressources sont disponibles, produisant ainsi des {\'e}tiqueteurs {\'e}tat-de-l{'}art pour plusieurs langues."
W14-0608,Automated Error Detection in Digitized Cultural Heritage Documents,2014,25,0,2,0.952381,22563,kata gabor,"Proceedings of the 8th Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities ({L}a{T}e{CH})",0,"The work reported in this paper aims at performance optimization in the digitization of documents pertaining to the cultural heritage domain. A hybrid method is roposed, combining statistical classification algorithms and linguistic knowledge to automatize post-OCR error detection and correction. The current paper deals with the integration of linguistic modules and their impact on error detection."
sagot-2014-delex,"{D}e{L}ex, a freely-avaible, large-scale and linguistically grounded morphological lexicon for {G}erman",2014,13,0,1,1,250,benoit sagot,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"We introduce DeLex, a freely-avaible, large-scale and linguistically grounded morphological lexicon for German developed within the Alexina framework. We extracted lexical information from the German wiktionary and developed a morphological inflection grammar for German, based on a linguistically sound model of inflectional morphology. Although the developement of DeLex involved some manual work, we show that is represents a good tradeoff between development cost, lexical coverage and resource accuracy."
baranes-sagot-2014-language,A Language-independent Approach to Extracting Derivational Relations from an Inflectional Lexicon,2014,18,3,2,0,18767,marion baranes,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"In this paper, we describe and evaluate an unsupervised method for acquiring pairs of lexical entries belonging to the same morphological family, i.e., derivationally related words, starting from a purely inflectional lexicon. Our approach relies on transformation rules that relate lexical entries with the one another, and which are automatically extracted from the inflected lexicon based on surface form analogies and on part-of-speech information. It is generic enough to be applied to any language with a mainly concatenative derivational morphology. Results were obtained and evaluated on English, French, German and Spanish. Precision results are satisfying, and our French results favorably compare with another resource, although its construction relied on manually developed lexicographic information whereas our approach only requires an inflectional lexicon."
candito-etal-2014-developing,Developing a {F}rench {F}rame{N}et: Methodology and First results,2014,23,6,11,0,16504,marie candito,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"The Asfalda project aims to develop a French corpus with frame-based semantic annotations and automatic tools for shallow semantic analysis. We present the first part of the project: focusing on a set of notional domains, we delimited a subset of English frames, adapted them to French data when necessary, and developed the corresponding French lexicon. We believe that working domain by domain helped us to enforce the coherence of the resulting resource, and also has the advantage that, though the number of frames is limited (around a hundred), we obtain full coverage within a given domain."
hanoka-sagot-2014-open,An Open-Source Heavily Multilingual Translation Graph Extracted from Wiktionaries and Parallel Corpora,2014,14,0,2,0,39870,valerie hanoka,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"This paper describes YaMTG (Yet another Multilingual Translation Graph), a new open-source heavily multilingual translation database (over 664 languages represented) built using several sources, namely various wiktionaries and the OPUS parallel corpora (Tiedemann, 2009). We detail the translation extraction process for 21 wiktionary language editions, and provide an evaluation of the translations contained in YaMTG."
scherrer-sagot-2014-language,A language-independent and fully unsupervised approach to lexicon induction and part-of-speech tagging for closely related languages,2014,17,2,2,0.474554,263,yves scherrer,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"In this paper, we describe our generic approach for transferring part-of-speech annotations from a resourced language towards an etymologically closely related non-resourced language, without using any bilingual (i.e., parallel) data. We first induce a translation lexicon from monolingual corpora, based on cognate detection followed by cross-lingual contextual similarity. Second, POS information is transferred from the resourced language along translation pairs to the non-resourced language and used for tagging the corpus. We evaluate our methods on three language families, consisting of five Romance languages, three Germanic languages and five Slavic languages. We obtain tagging accuracies of up to 91.6{\%}."
F14-2007,Sub-categorization in {`}pour{'} and lexical syntax (Sous-cat{\\'e}gorisation en pour et syntaxe lexicale) [in {F}rench],2014,-1,-1,1,1,250,benoit sagot,Proceedings of TALN 2014 (Volume 2: Short Papers),0,None
F14-2009,Named Entity Recognition and Correction in {OCR}ized Corpora (D{\\'e}tection et correction automatique d{'}entit{\\'e}s nomm{\\'e}es dans des corpus {OCR}is{\\'e}s) [in {F}rench],2014,-1,-1,1,1,250,benoit sagot,Proceedings of TALN 2014 (Volume 2: Short Papers),0,None
F14-1013,Analogy-based Text Normalization : the case of unknowns words (Normalisation de textes par analogie: le cas des mots inconnus) [in {F}rench],2014,-1,-1,2,0,18767,marion baranes,Proceedings of TALN 2014 (Volume 1: Long Papers),0,None
W13-5306,Lexicon induction and part-of-speech tagging of non-resourced languages without any bilingual resources,2013,19,5,2,0.474554,263,yves scherrer,Proceedings of the Workshop on Adaptation of Language Resources and Tools for Closely Related Languages and Language Variants,0,"We introduce a generic approach for transferring part-of-speech annotations from a resourced language to a non-resourced but etymologically close language. We first infer a bilingual lexicon between the two languages with methods based on character similarity, frequency similarity and context similarity. We then assign part-of-speech tags to these bilingual lexicon entries and annotate the remaining words on the basis of suffix analogy. We evaluate our approach on five language pairs of the Iberic peninsula, reaching up to 95% of precision on the lexicon induction task and up to 85% of tagging accuracy."
W13-4402,Can {MDL} Improve Unsupervised {C}hinese Word Segmentation?,2013,13,3,2,1,15790,pierre magistry,Proceedings of the Seventh {SIGHAN} Workshop on {C}hinese Language Processing,0,"It is often assumed that Minimum Descrip- tion Length (MDL) is a good criterion for unsupervised word segmentation. In this paper, we introduce a new approach to unsupervised word segmentation of Man- darin Chinese, that leads to segmentations whose Description Length is lower than what can be obtained using other algo- rithms previously proposed in the litera- ture. Suprisingly, we show that this lower Description Length does not necessarily corresponds to better segmentation results. Finally, we show that we can use very basic linguistic knowledge to coerce the MDL towards a linguistically plausible hypoth- esis and obtain better results than any pre- viously proposed method for unsupervised Chinese word segmentation with minimal human effort."
N13-1024,Enforcing Subcategorization Constraints in a Parser Using Sub-parses Recombining,2013,26,9,3,0,35499,seyed mirroshandel,Proceedings of the 2013 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"Treebanks are not large enough to adequately model subcategorization frames of predicative lexemes, which is an important source of lexico-syntactic constraints for parsing. As a consequence, parsers trained on such treebanks usually make mistakes when selecting the arguments of predicative lexemes. In this paper, we propose an original way to correct subcategorization errors by combining sub-parses of a sentence S that appear in the list of the n-best parses of S. The subcategorization information comes from three different resources, the first one is extracted from a treebank, the second one is computed on a large corpora and the third one is an existing syntactic lexicon. Experiments on the French Treebank showed a 15.24% reduction of erroneous subcategorization frames (SF) selections for verbs as well as a relative decrease of the error rate of 4% Labeled Accuracy Score on the state of the art parser on this treebank."
F13-1030,Dynamic extension of a {F}rench morphological lexicon based a text stream (Extension dynamique de lexiques morphologiques pour le fran{\\c{c}}ais {\\`a} partir d{'}un flux textuel) [in {F}rench],2013,-1,-1,1,1,250,benoit sagot,Proceedings of TALN 2013 (Volume 1: Long Papers),0,None
W12-5107,Dictionary-ontology cross-enrichment,2012,0,0,4,0,42104,emmanuel eckard,Proceedings of the 3rd Workshop on Cognitive Aspects of the Lexicon,0,None
W12-3408,Statistical Parsing of {S}panish and Data Driven Lemmatization,2012,15,5,2,0,5824,joseph roux,Proceedings of the {ACL} 2012 Joint Workshop on Statistical Parsing and Semantic Processing of Morphologically Rich Languages,0,"Although parsing performances have greatly improved in the last years, grammar inference from treebanks for morphologically rich lan- guages, especially from small treebanks, is still a challenging task. In this paper we in- vestigate how state-of-the-art parsing perfor- mances can be achieved on Spanish, a lan- guage with a rich verbal morphology, with a non-lexicalized parser trained on a treebank containing only around 2,800 trees. We rely on accurate part-of-speech tagging and data- driven lemmatization in order to cope with lexical data sparseness. Providing state-of- the-art results on Spanish, our methodology is applicable to other languages."
W12-3007,Population of a Knowledge Base for News Metadata from Unstructured Text and Web Data,2012,11,6,2,1,42263,rosa stern,Proceedings of the Joint Workshop on Automatic Knowledge Base Construction and Web-scale Knowledge Extraction ({AKBC}-{WEKEX}),0,"We present a practical use case of knowledge base (KB) population at the French news agency AFP. The target KB instances are entities relevant for news production and content enrichment. In order to acquire uniquely identified entities over news wires, i.e. textual data, and integrate the resulting KB in the Linked Data framework, a series of data models need to be aligned: Web data resources are harvested for creating a wide coverage entity database, which is in turn used to link entities to their mentions in French news wires. Finally, the extracted entities are selected for instantiation in the target KB. We describe our methodology along with the resources created and used for the target KB population."
W12-0508,A Joint Named Entity Recognition and Entity Linking System,2012,16,6,2,1,42263,rosa stern,Proceedings of the Workshop on Innovative Hybrid Approaches to the Processing of Textual Data,0,"We present a joint system for named entity recognition (NER) and entity linking (EL), allowing for named entities mentions extracted from textual data to be matched to uniquely identifiable entities. Our approach relies on combined NER modules which transfer the disambiguation step to the EL component, where referential knowledge about entities can be used to select a correct entity reading. Hybridation is a main feature of our system, as we have performed experiments combining two types of NER, based respectively on symbolic and statistical techniques. Furthermore, the statistical EL module relies on entity knowledge acquired over a large news corpus using a simple rule-base disambiguation tool. An implementation of our system is described, along with experiments and evaluation results on French news wires. Linking accuracy reaches up to 87%, and the NER F-score up to 83%."
P12-2075,Unsupervized Word Segmentation: the Case for {M}andarin {C}hinese,2012,16,18,2,1,15790,pierre magistry,Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"In this paper, we present an unsupervized segmentation system tested on Mandarin Chinese. Following Harris's Hypothesis in Kempe (1999) and Tanaka-Ishii's (2005) reformulation, we base our work on the Variation of Branching Entropy. We improve on (Jin and Tanaka-Ishii, 2006) by adding normalization and viterbi-decoding. This enable us to remove most of the thresholds and parameters from their model and to reach near state-of-the-art results (Wang et al., 2011) with a simpler system. We provide evaluation on different corpora available from the Segmentation bake-off II (Emerson, 2005) and define a more precise topline for the task using cross-trained supervized system available off-the-shelf (Zhang and Clark, 2010; Zhao and Kit, 2008; Huang and Zhao, 2007)"
apidianaki-sagot-2012-applying,Applying cross-lingual {WSD} to wordnet development,2012,20,3,2,0,2673,marianna apidianaki,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"The automatic development of semantic resources constitutes an important challenge in the NLP community. The methods used generally exploit existing large-scale resources, such as Princeton WordNet, often combined with information extracted from multilingual resources and parallel corpora. In this paper we show how Cross-Lingual Word Sense Disambiguation can be applied to wordnet development. We apply the proposed method to WOLF, a free wordnet for French still under construction, in order to fill synsets that did not contain any literal yet and increase its coverage."
tolone-etal-2012-evaluating,Evaluating and improving syntactic lexica by plugging them within a parser,2012,17,1,2,0,43065,elsa tolone,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"We present some evaluation results for four French syntactic lexica, obtained through their conversion to the Alexina format used by the Lefff lexicon, and their integration within the large-coverage TAG-based FRMG parser. The evaluations are run on two test corpora, annotated with two distinct annotation formats, namely EASy/Passage chunks and relations and CoNLL dependencies. The information provided by the evaluation results provide valuable feedback about the four lexica. Moreover, when coupled with error mining techniques, they allow us to identify how these lexica might be improved."
gabor-etal-2012-boosting,Boosting the Coverage of a Semantic Lexicon by Automatically Extracted Event Nominalizations,2012,18,0,3,0.952381,22563,kata gabor,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"In this article, we present a distributional analysis method for extracting nominalization relations from monolingual corpora. The acquisition method makes use of distributional and morphological information to select nominalization candidates. We explain how the learning is performed on a dependency annotated corpus and describe the nominalization results. Furthermore, we show how these results served to enrich an existing lexical resource, the WOLF (Wordnet Libre du Franc{\^A}{\c{}}ais). We present the techniques that we developed in order to integrate the new information into WOLF, based on both its structure and content. Finally, we evaluate the validity of the automatically obtained information and the correctness of its integration into the semantic resource. The method proved to be useful for boosting the coverage of WOLF and presents the advantage of filling verbal synsets, which are particularly difficult to handle due to the high level of verbal polysemy."
sagot-stern-2012-aleda,"Aleda, a free large-scale entity database for {F}rench",2012,7,6,1,1,250,benoit sagot,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"Named entity recognition, which focuses on the identification of the span and type of named entity mentions in texts, has drawn the attention of the NLP community for a long time. However, many real-life applications need to know which real entity each mention refers to. For such a purpose, often refered to as entity resolution and linking, an inventory of entities is required in order to constitute a reference. In this paper, we describe how we extracted such a resource for French from freely available resources (the French Wikipedia and the GeoNames database). We describe the results of an instrinsic evaluation of the resulting entity database, named Aleda, as well as those of a task-based evaluation in the context of a named entity detection system. We also compare it with the NLGbAse database (Charton and Torres-Moreno, 2010), a resource with similar objectives."
sagot-fiser-2012-cleaning,Cleaning noisy wordnets,2012,9,3,1,1,250,benoit sagot,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"Automatic approaches to creating and extending wordnets, which have become very popular in the past decade, inadvertently result in noisy synsets. This is why we propose an approach to detect synset outliers in order to eliminate the noise and improve accuracy of the developed wordnets, so that they become more useful lexico-semantic resources for natural language applications. The approach compares the words that appear in the synset and its surroundings with the contexts of the literals in question they are used in based on large monolingual corpora. By fine-tuning the outlier threshold we can influence how many outlier candidates will be eliminated. Although the proposed approach is language-independent we test it on Slovene and French that were created automatically from bilingual resources and contain plenty of disambiguation errors. Manual evaluation of the results shows that by applying a threshold similar to the estimated error rate in the respective wordnets, 67{\%} of the proposed outlier candidates are indeed incorrect for French and a 64{\%} for Slovene. This is a big improvement compared to the estimated overall error rates in the resources, which are 12{\%} for French and 15{\%} for Slovene."
hanoka-sagot-2012-wordnet,{W}ordnet extension made simple: A multilingual lexicon-based approach using wiki resources,2012,11,3,2,0,39870,valerie hanoka,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"In this paper, we propose a simple methodology for building or extending wordnets using easily extractible lexical knowledge from Wiktionary and Wikipedia. This method relies on a large multilingual translation/synonym graph in many languages as well as synset-aligned wordnets. It guesses frequent and polysemous literals that are difficult to find using other methods by looking at back-translations in the graph, showing that the use of a heavily multilingual lexicon can be a way to mitigate the lack of wide coverage bilingual lexicon for wordnet creation or extension. We evaluate our approach on French by applying it for extending WOLF, a freely available French wordnet."
F12-2008,{TCOF}-{POS} : un corpus libre de fran{\\c{c}}ais parl{\\'e} annot{\\'e} en morphosyntaxe ({TCOF}-{POS} : A Freely Available {POS}-Tagged Corpus of Spoken {F}rench) [in {F}rench],2012,-1,-1,3,0,30098,christophe benzitoun,"Proceedings of the Joint Conference JEP-TALN-RECITAL 2012, volume 2: TALN",0,None
F12-2050,Annotation r{\\'e}f{\\'e}rentielle du Corpus Arbor{\\'e} de {P}aris 7 en entit{\\'e}s nomm{\\'e}es (Referential named entity annotation of the {P}aris 7 {F}rench {T}ree{B}ank) [in {F}rench],2012,0,0,1,1,250,benoit sagot,"Proceedings of the Joint Conference JEP-TALN-RECITAL 2012, volume 2: TALN",0,None
C12-1149,The {F}rench {S}ocial {M}edia {B}ank: a Treebank of Noisy User Generated Content,2012,34,19,2,0.378351,167,djame seddah,Proceedings of {COLING} 2012,0,"In recent years, statistical parsers have reached high performance levels on well-edited texts. Domain adaptation techniques have improved parsing results on text genres differing from the journalistic data most parsers are trained on. However, such corpora usually comply with standard linguistic, spelling and typographic conventions. In the meantime, the emergence of Web 2.0 communication media has caused the apparition of new types of online textual data. Although valuable, e.g., in terms of data mining and sentiment analysis, such user-generated content rarely complies with standard conventions: they are noisy. This prevents most NLP tools, especially treebank based parsers, from performing well on such data. For this reason, we have developed the French Social Media Bank, the first user-generated content treebank for French, a morphologically rich language (MRL). The first release of this resource contains 1,700 sentences from various Web 2.0 sources, including data specifically chosen for their high noisiness. We describe here how we created this treebank and expose the methodology we used for fully annotating it. We also provide baseline POS tagging and statistical constituency parsing results, which are lower by far than usual results on edited texts. This highlights the high difficulty of automatically processing such noisy data in a MRL."
2011.jeptalnrecital-long.12,Un turc m{\\'e}canique pour les ressources linguistiques : critique de la myriadisation du travail parcellis{\\'e} ({M}echanical {T}urk for linguistic resources: review of the crowdsourcing of parceled work),2011,-1,-1,1,1,250,benoit sagot,Actes de la 18e conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Articles longs,0,"Cet article est une prise de position concernant les plate-formes de type Amazon Mechanical Turk, dont l{'}utilisation est en plein essor depuis quelques ann{\'e}es dans le traitement automatique des langues. Ces plateformes de travail en ligne permettent, selon le discours qui pr{\'e}vaut dans les articles du domaine, de faire d{\'e}velopper toutes sortes de ressources linguistiques de qualit{\'e}, pour un prix imbattable et en un temps tr{\`e}s r{\'e}duit, par des gens pour qui il s{'}agit d{'}un passe-temps. Nous allons ici d{\'e}montrer que la situation est loin d{'}{\^e}tre aussi id{\'e}ale, que ce soit sur le plan de la qualit{\'e}, du prix, du statut des travailleurs ou de l{'}{\'e}thique. Nous rappellerons ensuite les solutions alternatives d{\'e}j{\`a} existantes ou propos{\'e}es. Notre but est ici double : informer les chercheurs, afin qu{'}ils fassent leur choix en toute connaissance de cause, et proposer des solutions pratiques et organisationnelles pour am{\'e}liorer le d{\'e}veloppement de nouvelles ressources linguistiques en limitant les risques de d{\'e}rives {\'e}thiques et l{\'e}gales, sans que cela se fasse au prix de leur co{\^u}t ou de leur qualit{\'e}."
2011.jeptalnrecital-long.23,Segmentation et induction de lexique non-supervis{\\'e}es du mandarin (Unsupervised segmentation and induction of mandarin lexicon),2011,-1,-1,2,1,15790,pierre magistry,Actes de la 18e conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Articles longs,0,"Pour la plupart des langues utilisant l{'}alphabet latin, le d{\'e}coupage d{'}un texte selon les espaces et les symboles de ponctuation est une bonne approximation d{'}un d{\'e}coupage en unit{\'e}s lexicales. Bien que cette approximation cache de nombreuses difficult{\'e}s, elles sont sans comparaison avec celles que l{'}on rencontre lorsque l{'}on veut traiter des langues qui, comme le chinois mandarin, n{'}utilisent pas l{'}espace. Un grand nombre de syst{\`e}mes de segmentation ont {\'e}t{\'e} propos{\'e}s parmi lesquels certains adoptent une approche non-supervis{\'e}e motiv{\'e}e linguistiquement. Cependant les m{\'e}thodes d{'}{\'e}valuation commun{\'e}ment utilis{\'e}es ne rendent pas compte de toutes les propri{\'e}t{\'e}s de tels syst{\`e}mes. Dans cet article, nous montrons qu{'}un mod{\`e}le simple qui repose sur une reformulation en termes d{'}entropie d{'}une hypoth{\`e}se ind{\'e}pendante de la langue {\'e}nonc{\'e}e par Harris (1955), permet de segmenter un corpus et d{'}en extraire un lexique. Test{\'e} sur le corpus de l{'}Academia Sinica, notre syst{\`e}me permet l{'}induction d{'}une segmentation et d{'}un lexique qui ont de bonnes propri{\'e}t{\'e}s intrins{\`e}ques et dont les caract{\'e}ristiques sont similaires {\`a} celles du lexique sous-jacent au corpus segment{\'e} manuellement. De plus, on constate une certaine corr{\'e}lation entre les r{\'e}sultats du mod{\`e}le de segmentation et les structures syntaxiques fournies par une sous-partie arbor{\'e}e corpus."
2011.jeptalnrecital-court.4,Coop{\\'e}ration de m{\\'e}thodes statistiques et symboliques pour l{'}adaptation non-supervis{\\'e}e d{'}un syst{\\`e}me d{'}{\\'e}tiquetage en entit{\\'e}s nomm{\\'e}es (Statistical and symbolic methods cooperation for the unsupervised adaptation of a named entity recognition system),2011,-1,-1,2,0,17997,frederic bechet,Actes de la 18e conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Articles courts,0,"La d{\'e}tection et le typage des entit{\'e}s nomm{\'e}es sont des t{\^a}ches pour lesquelles ont {\'e}t{\'e} d{\'e}velopp{\'e}s {\`a} la fois des syst{\`e}mes symboliques et probabilistes. Nous pr{\'e}sentons les r{\'e}sultats d{'}une exp{\'e}rience visant {\`a} faire interagir le syst{\`e}me {\`a} base de r{\`e}gles NP, d{\'e}velopp{\'e} sur des corpus provenant de l{'}AFP, int{\'e}grant la base d{'}entit{\'e}s Aleda et qui a une bonne pr{\'e}cision, et le syst{\`e}me LIANE, entra{\^\i}n{\'e} sur des transcriptions de l{'}oral provenant du corpus ESTER et qui a un bon rappel. Nous montrons qu{'}on peut adapter {\`a} un nouveau type de corpus, de mani{\`e}re non supervis{\'e}e, un syst{\`e}me probabiliste tel que LIANE gr{\^a}ce {\`a} des corpus volumineux annot{\'e}s automatiquement par NP. Cette adaptation ne n{\'e}cessite aucune annotation manuelle suppl{\'e}mentaire et illustre la compl{\'e}mentarit{\'e} des m{\'e}thodes num{\'e}riques et symboliques pour la r{\'e}solution de t{\^a}ches linguistiques."
2011.jeptalnrecital-court.12,Construction d{'}un lexique des adjectifs d{\\'e}nominaux (Construction of a lexicon of denominal adjectives),2011,-1,-1,2,0,32743,jana strnadova,Actes de la 18e conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Articles courts,0,"Apr{\`e}s une br{\`e}ve analyse linguistique des adjectifs d{\'e}nominaux en fran{\c{c}}ais, nous d{\'e}crivons le processus automatique que nous avons mis en place {\`a} partir de lexiques et de corpus volumineux pour construire un lexique d{'}adjectifs d{\'e}nominaux d{\'e}riv{\'e}s de mani{\`e}re r{\'e}guli{\`e}re. Nous estimons {\`a} la fois la pr{\'e}cision et la couverture du lexique d{\'e}rivationnel obtenu. {\`A} terme, ce lexique librement disponible aura {\'e}t{\'e} valid{\'e} manuellement et contiendra {\'e}galement les adjectifs d{\'e}nominaux {\`a} base suppl{\'e}tive."
2011.jeptalnrecital-court.13,"D{\\'e}veloppement de ressources pour le persan : {P}er{L}ex 2, nouveau lexique morphologique et {ME}ltfa, {\\'e}tiqueteur morphosyntaxique (Development of resources for {P}ersian: {P}er{L}ex 2, a new morphological lexicon and {ME}ltfa, a morphosyntactic tagger)",2011,-1,-1,1,1,250,benoit sagot,Actes de la 18e conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Articles courts,0,"Nous pr{\'e}sentons une nouvelle version de PerLex, lexique morphologique du persan, une version corrig{\'e}e et partiellement r{\'e}annot{\'e}e du corpus {\'e}tiquet{\'e} BijanKhan (BijanKhan, 2004) et MEltfa, un nouvel {\'e}tiqueteur morphosyntaxique librement disponible pour le persan. Apr{\`e}s avoir d{\'e}velopp{\'e} une premi{\`e}re version de PerLex (Sagot {\&} Walther, 2010), nous en proposons donc ici une version am{\'e}lior{\'e}e. Outre une validation manuelle partielle, PerLex 2 repose d{\'e}sormais sur un inventaire de cat{\'e}gories linguistiquement motiv{\'e}. Nous avons {\'e}galement d{\'e}velopp{\'e} une nouvelle version du corpus BijanKhan : elle contient des corrections significatives de la tokenisation ainsi qu{'}un r{\'e}{\'e}tiquetage {\`a} l{'}aide des nouvelles cat{\'e}gories. Cette nouvelle version du corpus a enfin {\'e}t{\'e} utilis{\'e}e pour l{'}entra{\^\i}nement de MEltfa, notre {\'e}tiqueteur morphosyntaxique pour le persan librement disponible, s{'}appuyant {\`a} la fois sur ce nouvel inventaire de cat{\'e}gories, sur PerLex 2 et sur le syst{\`e}me d{'}{\'e}tiquetage MElt (Denis {\&} Sagot, 2009)."
W10-4413,"Control Verb, Argument Cluster Coordination and Multi Component {TAG}",2010,0,1,2,0.453441,167,djame seddah,Proceedings of the 10th International Workshop on Tree Adjoining Grammar and Related Frameworks ({TAG}+10),0,None
W10-1807,Influence of Pre-Annotation on {POS}-Tagged Corpus Development,2010,14,35,2,0.277778,10472,karen fort,Proceedings of the Fourth Linguistic Annotation Workshop,0,"This article details a series of carefully designed experiments aiming at evaluating the influence of automatic pre-annotation on the manual part-of-speech annotation of a corpus, both from the quality and the time points of view, with a specific attention drawn to biases. For this purpose, we manually annotated parts of the Penn Treebank corpus (Marcus et al., 1993) under various experimental setups, either from scratch or using various pre-annotations. These experiments confirm and detail the gain in quality observed before (Marcus et al., 1993; Dandapat et al., 2009; Rehbein et al., 2009), while showing that biases do appear and should be taken into account. They finally demonstrate that even a not so accurate tagger can help improving annotation speed."
P10-1054,Optimal Rank Reduction for Linear Context-Free Rewriting Systems with Fan-Out Two,2010,13,9,1,1,250,benoit sagot,Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,1,Linear Context-Free Rewriting Systems (LCFRSs) are a grammar formalism capable of modeling discontinuous phrases. Many parsing applications use LCFRSs where the fan-out (a measure of the discontinuity of phrases) does not exceed 2. We present an efficient algorithm for optimal reduction of the length of production right-hand side in LCFRSs with fan-out at most 2. This results in asymptotical running time improvement for known parsing algorithms for this class.
sagot-etal-2010-lexicon,A Lexicon of {F}rench Quotation Verbs for Automatic Quotation Extraction,2010,11,6,1,1,250,benoit sagot,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"Quotation extraction is an important information extraction task, especially when dealing with news wires. Quotations can be found in various configurations. In this paper, we focus on direct quotations introduced by a parenthetical clause, headed by a ''``quotation verb''''. Our study is based on a large French news wire corpus from the Agence France-Presse. We introduce and motivate an analysis at the discursive level of such quotations, which differs from the syntactic analyses generally proposed. We show how we enriched the Lefff syntactic lexicon so that it provides an account for quotation verbs heading a quotation parenthetical, especially those extracted from a news wire corpus. We also sketch how these lexical entries can be extended to the discursive level in order to model quotations introduced in a parenthetical clause in a complete way."
sagot-walther-2010-morphological,A Morphological Lexicon for the {P}ersian Language,2010,12,14,1,1,250,benoit sagot,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"We introduce PerLex, a large-coverage and freely-available morphological lexicon for the Persian language. We describe the main features of the Persian morphology, and the way we have represented it within the Alexina formalism, on which PerLex is based. We focus on the methodology we used for constructing lexical entries from various sources, as well as the problems related to typographic normalisation. The resulting lexicon shows a satisfying coverage on a reference corpus and should therefore be a good starting point for developing a syntactic lexicon for the Persian language."
sagot-2010-lefff,"The Lefff, a Freely Available and Large-coverage Morphological and Syntactic Lexicon for {F}rench",2010,28,114,1,1,250,benoit sagot,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"In this paper, we introduce the Lefff, a freely available, accurate and large-coverage morphological and syntactic lexicon for French, used in many NLP tools such as large-coverage parsers. We first describe Alexina, the lexical framework in which the Lefff is developed as well as the linguistic notions and formalisms it is based on. Next, we describe the various sources of lexical data we used for building the Lefff, in particular semi-automatic lexical development techniques and conversion and merging of existing resources. Finally, we illustrate the coverage and precision of the resource by comparing it with other resources and by assessing its impact in various NLP tools."
2010.jeptalnrecital-long.3,Exploitation d{'}une ressource lexicale pour la construction d{'}un {\\'e}tiqueteur morpho-syntaxique {\\'e}tat-de-l{'}art du fran{\\c{c}}ais,2010,-1,-1,2,0.740741,11461,pascal denis,Actes de la 17e conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Articles longs,0,"Cet article pr{\'e}sente MEltfr, un {\'e}tiqueteur morpho-syntaxique automatique du fran{\c{c}}ais. Il repose sur un mod{\`e}le probabiliste s{\'e}quentiel qui b{\'e}n{\'e}ficie d{'}informations issues d{'}un lexique exog{\`e}ne, {\`a} savoir le Lefff. Evalu{\'e} sur le FTB, MEltfr atteint un taux de pr{\'e}cision de 97.75{\%} (91.36{\%} sur les mots inconnus) sur un jeu de 29 {\'e}tiquettes. Ceci correspond {\`a} une diminution du taux d{'}erreur de 18{\%} (36.1{\%} sur les mots inconnus) par rapport au m{\^e}me mod{\`e}le sans couplage avec le Lefff. Nous {\'e}tudions plus en d{\'e}tail la contribution de cette ressource, au travers de deux s{\'e}ries d{'}exp{\'e}riences. Celles-ci font appara{\^\i}tre en particulier que la contribution des traits issus du Lefff est de permettre une meilleure couverture, ainsi qu{'}une mod{\'e}lisation plus fine du contexte droit des mots."
2010.jeptalnrecital-long.40,D{\\'e}veloppement de ressources pour le persan: lexique morphologique et cha{\\^\\i}ne de traitements de surface,2010,-1,-1,1,1,250,benoit sagot,Actes de la 17e conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Articles longs,0,"Nous pr{\'e}sentons PerLex, un lexique morphologique du persan {\`a} large couverture et librement disponible, accompagn{\'e} d{'}une cha{\^\i}ne de traitements de surface pour cette langue. Nous d{\'e}crivons quelques caract{\'e}ristiques de la morphologie du persan, et la fa{\c{c}}on dont nous l{'}avons repr{\'e}sent{\'e}e dans le formalisme lexical Alexina, sur lequel repose PerLex. Nous insistons sur la m{\'e}thodologie que nous avons employ{\'e}e pour construire les entr{\'e}es lexicales {\`a} partir de diverses sources, ainsi que sur les probl{\`e}mes li{\'e}s {\`a} la normalisation typographique. Le lexique obtenu a une couverture satisfaisante sur un corpus de r{\'e}f{\'e}rence, et devrait donc constituer un bon point de d{\'e}part pour le d{\'e}veloppement d{'}un lexique syntaxique du persan."
2010.jeptalnrecital-court.5,Ponctuations fortes abusives,2010,8,0,2,0,28085,laurence danlos,Actes de la 17e conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Articles courts,0,"Certaines ponctuations fortes sont Â« abusivement Â» utilis{\'e}es {\`a} la place de ponctuations faibles, d{\'e}bouchant sur des phrases graphiques qui ne sont pas des phrases grammaticales. Cet article pr{\'e}sente une {\'e}tude sur corpus de ce ph{\'e}nom{\`e}ne et une {\'e}bauche d{'}outil pour rep{\'e}rer automatiquement les ponctuations fortes abusives."
2010.jeptalnrecital-court.15,Traitement des inconnus : une approche syst{\\'e}matique de l{'}incompl{\\'e}tude lexicale,2010,-1,-1,4,0,44937,helena blancafort,Actes de la 17e conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Articles courts,0,"Cet article aborde le ph{\'e}nom{\`e}ne de l{'}incompl{\'e}tude des ressources lexicales, c{'}est-{\`a}-dire la probl{\'e}matique des inconnus, dans un contexte de traitement automatique. Nous proposons tout d{'}abord une d{\'e}finition op{\'e}rationnelle de la notion d{'}inconnu. Nous d{\'e}crivons ensuite une typologie des diff{\'e}rentes classes d{'}inconnus, motiv{\'e}e par des consid{\'e}rations linguistiques et applicatives ainsi que par l{'}annotation des inconnus d{'}un petit corpus selon notre typologie. Cette typologie sera mise en oeuvre et valid{\'e}e par l{'}annotation d{'}un corpus important de l{'}Agence France-Presse dans le cadre du projet EDyLex."
2010.jeptalnrecital-court.23,D{\\'e}tection et r{\\'e}solution d{'}entit{\\'e}s nomm{\\'e}es dans des d{\\'e}p{\\^e}ches d{'}agence,2010,-1,-1,2,1,42263,rosa stern,Actes de la 17e conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Articles courts,0,"Nous pr{\'e}sentons NP, un syst{\`e}me de reconnaissance d{'}entit{\'e}s nomm{\'e}es. Comprenant un module de r{\'e}solution, il permet d{'}associer {\`a} chaque occurrence d{'}entit{\'e} le r{\'e}f{\'e}rent qu{'}elle d{\'e}signe parmi les entr{\'e}es d{'}un r{\'e}f{\'e}rentiel d{\'e}di{\'e}. NP apporte ainsi des informations pertinentes pour l{'}exploitation de l{'}extraction d{'}entit{\'e}s nomm{\'e}es en contexte applicatif. Ce syst{\`e}me fait l{'}objet d{'}une {\'e}valuation gr{\^a}ce au d{\'e}veloppement d{'}un corpus annot{\'e} manuellement et adapt{\'e} aux t{\^a}ches de d{\'e}tection et de r{\'e}solution."
Y09-1013,Coupling an Annotated Corpus and a Morphosyntactic Lexicon for State-of-the-Art {POS} Tagging with Less Human Effort,2009,13,83,2,0.740741,11461,pascal denis,"Proceedings of the 23rd Pacific Asia Conference on Language, Information and Computation, Volume 1",0,"This paper investigates how to best couple hand-annotated data with information extracted from an external lexical resource to improve POS tagging performance. Focusing on French tagging, we introduce a maximum entropy conditional sequence tagging system that is enriched with information extracted from a morphological resource. This system gives a 97.7% accuracy on the French Treebank, an error reduction of 23% (28% on unknown words) over the same tagger without lexical information. We also conduct experiments on datasets and lexicons of varying sizes in order to assess the best trade-off between annotating data vs. developing a lexicon. We find that the use of a lexicon improves the quality of the tagger at any stage of development of either resource, and that for fixed performance levels the availability of the full lexicon consistently reduces the need for supervised data by at least one half."
W09-4619,Building a morphological and syntactic lexicon by merging various linguistic resources,2009,11,4,2,0,46758,miguel molinero,Proceedings of the 17th Nordic Conference of Computational Linguistics ({NODALIDA} 2009),0,"This paper shows how large-coverage morphological and syntactic NLP lexicons can be developed by interpreting, converting to a common format and merging existing lexical resources. Applied on Spanish, this allowed us to build a morphological and syntactic lexicon, the Leffe. It relies on the Alexina framework, originally developed together 200with the French lexicon Lefff. We describe how the input resources --two morphological and two syntactic lexicons -- were converted into Alexina lexicons and merged. A preliminary evaluation shows that merging different sources of lexical information is indeed a good approach to improve the development speed, the coverage and the precision of linguistic resources."
W09-3818,Constructing parse forests that include exactly the n-best {PCFG} trees,2009,11,4,3,0,46836,pierre boullier,Proceedings of the 11th International Conference on Parsing Technologies ({IWPT}{'}09),0,"This paper describes and compares two algorithms that take as input a shared PCFG parse forest and produce shared forests that contain exactly the n most likely trees of the initial forest. Such forests are suitable for subsequent processing, such as (some types of) reranking or LFG f-structure computation, that can be performed ontop of a shared forest, but that may have a high (e.g., exponential) complexity w.r.t. the number of trees contained in the forest. We evaluate the performances of both algorithms on real-scale NLP forests generated with a PCFG extracted from the Penn Treebank."
W09-3841,Parsing Directed Acyclic Graphs with Range Concatenation Grammars,2009,5,0,2,0,46836,pierre boullier,Proceedings of the 11th International Conference on Parsing Technologies ({IWPT}{'}09),0,"Range Concatenation Grammars (RCGs) are a syntactic formalism which possesses many attractive properties. It is more powerful than Linear Context-Free Rewriting Systems, though this power is not reached to the detriment of efficiency since its sentences can always be parsed in polynomial time. If the input, instead of a string, is a Directed Acyclic Graph (DAG), only simple RCGs can still be parsed in polynomial time. For non-linear RCGs, this polynomial parsing time cannot be guaranteed anymore. In this paper, we show how the standard parsing algorithm can be adapted for parsing DAGs with RCGs, both in the linear (simple) and in the non-linear case."
R09-1049,A Morphological and Syntactic Wide-coverage Lexicon for {S}panish: The Leffe,2009,12,17,2,0,46758,miguel molinero,Proceedings of the International Conference {RANLP}-2009,0,"In this paper, we introduce the Lde Formas Flexionadas del Espaxcbx9c (Leffe), a wide-coverage morphological and syntactic Spanish lexicon based on the Alexina lexical framework. We explain how the Leffe has been created by merging together several heterogeneous lexicons and how the Alexina lexical framework has been applied to Spanish. We also introduce a semi-automatic technique based on a tagger to detect the lexicon's deficiencies. A preliminary evaluation shows the potential of the Leffe and the relevance of both creation and extension processes."
R09-1058,Towards Efficient Production of Linguistic Resources: the {V}ictoria Project,2009,9,2,3,1,3003,lionel nicolas,Proceedings of the International Conference {RANLP}-2009,0,"In order to produce efficient Natural Language Pro- cessing (NLP) tools, reliable linguistic resources are a preliminary requirement. When available for a given language, the resources are generally far below the ex- pectations in terms of quality, coverage or usability. This paper presents a project whose ambition is to en- hance the production capacities of linguistic resources through the creation and intensive use of intercon- nected acquisition and correction tools, inter-lingual transfer processes and a collaborative online develop- ment framework."
N09-2047,{MICA}: A Probabilistic Dependency Parser Based on Tree Insertion Grammars (Application Note),2009,16,20,5,0,4704,srinivas bangalore,"Proceedings of Human Language Technologies: The 2009 Annual Conference of the North {A}merican Chapter of the Association for Computational Linguistics, Companion Volume: Short Papers",0,"MICA is a dependency parser which returns deep dependency representations, is fast, has state-of-the-art performance, and is freely available."
2009.jeptalnrecital-long.23,Trouver et confondre les coupables : un processus sophistiqu{\\'e} de correction de lexique,2009,0,1,2,1,3003,lionel nicolas,Actes de la 16{\\`e}me conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Articles longs,0,"La couverture d{'}un analyseur syntaxique d{\'e}pend avant tout de la grammaire et du lexique sur lequel il repose. Le d{\'e}veloppement d{'}un lexique complet et pr{\'e}cis est une t{\^a}che ardue et de longue haleine, surtout lorsque le lexique atteint un certain niveau de qualit{\'e} et de couverture. Dans cet article, nous pr{\'e}sentons un processus capable de d{\'e}tecter automatiquement les entr{\'e}es manquantes ou incompl{\`e}tes d{'}un lexique, et de sugg{\'e}rer des corrections pour ces entr{\'e}es. La d{\'e}tection se r{\'e}alise au moyen de deux techniques reposant soit sur un mod{\`e}le statistique, soit sur les informations fournies par un {\'e}tiqueteur syntaxique. Les hypoth{\`e}ses de corrections pour les entr{\'e}es lexicales d{\'e}tect{\'e}es sont g{\'e}n{\'e}r{\'e}es en {\'e}tudiant les modifications qui permettent d{'}am{\'e}liorer le taux d{'}analyse des phrases dans lesquelles ces entr{\'e}es apparaissent. Le processus global met en oeuvre plusieurs techniques utilisant divers outils tels que des {\'e}tiqueteurs et des analyseurs syntaxiques ou des classifieurs d{'}entropie. Son application au Lefff , un lexique morphologique et syntaxique {\`a} large couverture du fran{\c{c}}ais, nous a d{\'e}j{\`a} permis de r{\'e}aliser des am{\'e}liorations notables."
2009.jeptalnrecital-court.20,Int{\\'e}grer les tables du Lexique-Grammaire {\\`a} un analyseur syntaxique robuste {\\`a} grande {\\'e}chelle,2009,0,5,1,1,250,benoit sagot,Actes de la 16{\\`e}me conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Articles courts,0,"Dans cet article, nous montrons comment nous avons converti les tables du Lexique-Grammaire en un format TAL, celui du lexique Lefff, permettant ainsi son int{\'e}gration dans l{'}analyseur syntaxique FRMG. Nous pr{\'e}sentons les fondements linguistiques de ce processus de conversion et le lexique obtenu. Nous validons le lexique obtenu en {\'e}valuant l{'}analyseur syntaxique FRMG sur le corpus de r{\'e}f{\'e}rence de la campagne EASy selon qu{'}il utilise les entr{\'e}es verbales du Lefff ou celles des tables des verbes du Lexique-Grammaire ainsi converties."
C08-1080,Computer Aided Correction and Extension of a Syntactic Wide-Coverage Lexicon,2008,9,10,2,1,3003,lionel nicolas,Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008),0,"The effectiveness of parsers based on manually created resources, namely a grammar and a lexicon, rely mostly on the quality of these resources. Thus, increasing the parser coverage and precision usually implies improving these two resources. Their manual improvement is a time consuming and complex task: identifying which resource is the true culprit for a given mistake is not always obvious, as well as finding the mistake and correcting it.n n Some techniques, like van Noord (2004) or Sagot and Villemonte de La Clergerie (2006), bring a convenient way to automatically identify forms having potentially erroneous entries in a lexicon. We have integrated and extended such techniques in a wider process which, thanks to the grammar ability to tell how these forms could be used as part of correct parses, is able to propose lexical corrections for the identified entries.n n We present in this paper an implementation of this process and discuss the main results we have obtained on a syntactic wide-coverage French lexicon."
2008.jeptalnrecital-long.18,Construction d{'}un wordnet libre du fran{\\c{c}}ais {\\`a} partir de ressources multilingues,2008,-1,-1,1,1,250,benoit sagot,Actes de la 15{\\`e}me conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Articles longs,0,"Cet article d{\'e}crit la construction d{'}un Wordnet Libre du Fran{\c{c}}ais (WOLF) {\`a} partir du Princeton WordNet et de diverses ressources multilingues. Les lex{\`e}mes polys{\'e}miques ont {\'e}t{\'e} trait{\'e}s au moyen d{'}une approche reposant sur l{'}alignement en mots d{'}un corpus parall{\`e}le en cinq langues. Le lexique multilingue extrait a {\'e}t{\'e} d{\'e}sambigu{\""\i}s{\'e} s{\'e}mantiquement {\`a} l{'}aide des wordnets des langues concern{\'e}es. Par ailleurs, une approche bilingue a {\'e}t{\'e} suffisante pour construire de nouvelles entr{\'e}es {\`a} partir des lex{\`e}mes monos{\'e}miques. Nous avons pour cela extrait des lexiques bilingues {\`a} partir deWikip{\'e}dia et de th{\'e}saurus. Le wordnet obtenu a {\'e}t{\'e} {\'e}valu{\'e} par rapport au wordnet fran{\c{c}}ais issu du projet EuroWordNet. Les r{\'e}sultats sont encourageants, et des applications sont d{'}ores et d{\'e}j{\`a} envisag{\'e}es."
W07-2213,Are Very Large Context-Free Grammars Tractable?,2007,21,4,2,0.672215,46836,pierre boullier,Proceedings of the Tenth International Conference on Parsing Technologies,0,"In this paper, we present a method which, in practice, allows to use parsers for languages defined by very large context-free grammars (over a million symbol occurrences). The idea is to split the parsing process in two passes. A first pass computes a sub-grammar which is a specialized part of the large grammar selected by the input text and various filtering strategies. The second pass is a traditional parser which works with the sub-grammar and the input text. This approach is validated by practical experiments performed on a Earley-like parser running on a test set with two large context-free grammars."
2007.jeptalnrecital-long.21,Comparaison du Lexique-Grammaire des verbes pleins et de {DICOVALENCE} : vers une int{\\'e}gration dans le Lefff,2007,10,4,2,0,28085,laurence danlos,Actes de la 14{\\`e}me conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Articles longs,0,"Cet article compare le Lexique-Grammaire des verbes pleins et DICOVALENCE, deux ressources lexicales syntaxiques pour le fran{\c{c}}ais d{\'e}velopp{\'e}es par des linguistes depuis de nombreuses ann{\'e}es. Nous {\'e}tudions en particulier les divergences et les empi{\`e}tements des mod{\`e}les lexicaux sous-jacents. Puis nous pr{\'e}sentons le Lefff , lexique syntaxique {\`a} grande {\'e}chelle pour le TAL, et son propre mod{\`e}le lexical. Nous montrons que ce mod{\`e}le est {\`a} m{\^e}me d{'}int{\'e}grer les informations lexicales pr{\'e}sentes dans le Lexique-Grammaire et dans DICOVALENCE. Nous pr{\'e}sentons les r{\'e}sultats des premiers travaux effectu{\'e}s en ce sens, avec pour objectif {\`a} terme la constitution d{'}un lexique syntaxique de r{\'e}f{\'e}rence pour le TAL."
W06-1522,Modeling and Analysis of Elliptic Coordination by Dynamic Exploitation of Derivation Forests in {LTAG} Parsing,2006,13,3,2,0.453441,167,djame seddah,Proceedings of the Eighth International Workshop on Tree Adjoining Grammar and Related Formalisms,0,"In this paper, we introduce a generic approach to elliptic coordination modeling through the parsing of Ltag grammars. We show that erased lexical items can be replaced during parsing by informations gathered in the other member of the coordinate structure and used as a guide at the derivation level. Moreover, we show how this approach can be indeed implemented as a light extension of the LTAG formalism throuh a so-called xe2x80x9cfusionxe2x80x9d operation and by the use of tree schemata during parsing in order to obtain a dependency graph."
P06-1042,Error Mining in Parsing Results,2006,5,30,1,1,250,benoit sagot,Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics,1,"We introduce an error mining technique for automatically detecting errors in resources that are used in parsing systems. We applied this technique on parsing results produced on several million words by two distinct parsing systems, which share the syntactic lexicon and the pre-parsing processing chain. We were thus able to identify missing and erroneous information in these resources."
sagot-boullier-2006-deep,Deep non-probabilistic parsing of large corpora,2006,1,1,1,1,250,benoit sagot,Proceedings of the Fifth International Conference on Language Resources and Evaluation ({LREC}{'}06),0,"This paper reports a large-scale non-probabilistic parsing experiment with a deep LFG parser. We briefly introduce the parser we used, named SXLFG, and the resources that were used together with it. Then we report quantitative results about the parsing of a multi-million word journalistic corpus. We show that we can parse more than 6 million words in less than 12 hours, only 6.7{\%} of all sentences reaching the 1s timeout. This shows that deep large-coverage non-probabilistic parsers can be efficient enough to parse very large corpora in a reasonable amount of time."
sagot-etal-2006-lefff,"The Lefff 2 syntactic lexicon for {F}rench: architecture, acquisition, use",2006,6,74,1,1,250,benoit sagot,Proceedings of the Fifth International Conference on Language Resources and Evaluation ({LREC}{'}06),0,"In this paper, we introduce a new lexical resource for French which is freely available as the second version of the Lefff (Lexique des formes fl{\'e}chies du fran{\c{c}}ais - Lexicon of French inflected forms). It is a wide-coverage morphosyntactic and syntactic lexicon, whose architecture relies on properties inheritance, which makes it more compact and more easily maintainable and allows to describe lexical entries independantly from the formalisms it is used for. For these two reasons, we define it as a meta-lexicon. We describe its architecture, several automatic or semi-automatic approaches we use to acquire, correct and/or enrich such a lexicon, as well as the way it is used both with an LFG parser and with a TAG parser based on a meta-grammar, so as to build two large-coverage parsers for French. The web site of the Lefff is http://www.lefff.net/."
2006.jeptalnrecital-poster.25,Mod{\\'e}lisation et analyse des coordinations elliptiques par l{'}exploitation dynamique des for{\\^e}ts de d{\\'e}rivation,2006,-1,-1,2,0.453441,167,djame seddah,Actes de la 13{\\`e}me conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Posters,0,"Nous pr{\'e}sentons dans cet article une approche g{\'e}n{\'e}rale pour la mod{\'e}lisation et l{'}analyse syntaxique des coordinations elliptiques. Nous montrons que les lex{\`e}mes {\'e}lid{\'e}s peuvent {\^e}tre remplac{\'e}s, au cours de l{'}analyse, par des informations qui proviennent de l{'}autre membre de la coordination, utilis{\'e} comme guide au niveau des d{\'e}rivations. De plus, nous montrons comment cette approche peut {\^e}tre effectivement mise en oeuvre par une l{\'e}g{\`e}re extension des Grammaires d{'}Arbres Adjoints Lexicalis{\'e}es (LTAG) {\`a} travers une op{\'e}ration dite de fusion. Nous d{\'e}crivons les algorithmes de d{\'e}rivation n{\'e}cessaires pour l{'}analyse de constructions coordonn{\'e}es pouvant comporter un nombre quelconque d{'}ellipses."
2006.jeptalnrecital-long.26,Trouver le coupable : Fouille d{'}erreurs sur des sorties d{'}analyseurs syntaxiques,2006,-1,-1,1,1,250,benoit sagot,Actes de la 13{\\`e}me conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Articles longs,0,"Nous pr{\'e}sentons une m{\'e}thode de fouille d{'}erreurs pour d{\'e}tecter automatiquement des erreurs dans les ressources utilis{\'e}es par les syst{\`e}mes d{'}analyse syntaxique. Nous avons mis en oeuvre cette m{\'e}thode sur le r{\'e}sultat de l{'}analyse de plusieurs millions de mots par deux syst{\`e}mes d{'}analyse diff{\'e}rents qui ont toutefois en commun le lexique syntaxique et la cha{\^\i}ne de traitement pr{\'e}-syntaxique. Nous avons pu identifier ainsi des inexactitudes et des incompl{\'e}tudes dans les ressources utilis{\'e}es. En particulier, la comparaison des r{\'e}sultats obtenus sur les sorties des deux analyseurs sur un m{\^e}me corpus nous a permis d{'}isoler les probl{\`e}mes issus des ressources partag{\'e}es de ceux issus des grammaires."
W05-1501,Efficient and Robust {LFG} Parsing: {S}x{LFG},2005,10,17,2,0.672215,46836,pierre boullier,Proceedings of the Ninth International Workshop on Parsing Technology,0,"In this paper, we introduce a new parser, called SxLfg, based on the Lexical-Functional Grammars formalism (LFG). We describe the underlying context-free parser and how functional structures are efficiently computed on top of the CFG shared forest thanks to computation sharing, lazy evaluation, and compact data representation. We then present various error recovery techniques we implemented in order to build a robust parser. Finally, we offer concrete results when SxLfg is used with an existing grammar for French. We show that our parser is both efficient and robust, although the grammar is very ambiguous."
2005.jeptalnrecital-long.11,Cha{\\^\\i}nes de traitement syntaxique,2005,-1,-1,3,0.672215,46836,pierre boullier,Actes de la 12{\\`e}me conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Articles longs,0,"Cet article expose l{'}ensemble des outils que nous avons mis en oeuvre pour la campagne EASy d{'}{\'e}valuation d{'}analyse syntaxique. Nous commen{\c{c}}ons par un aper{\c{c}}u du lexique morphologique et syntaxique utilis{\'e}. Puis nous d{\'e}crivons bri{\`e}vement les propri{\'e}t{\'e}s de notre cha{\^\i}ne de traitement pr{\'e}-syntaxique qui permet de g{\'e}rer des corpus tout-venant. Nous pr{\'e}sentons alors les deux syst{\`e}mes d{'}analyse que nous avons utilis{\'e}s, un analyseur TAG issu d{'}une m{\'e}ta-grammaire et un analyseur LFG. Nous comparons ces deux syst{\`e}mes en indiquant leurs points communs, comme l{'}utilisation intensive du partage de calcul et des repr{\'e}sentations compactes de l{'}information, mais {\'e}galement leurs diff{\'e}rences, au niveau des formalismes, des grammaires et des analyseurs. Nous d{\'e}crivons ensuite le processus de post-traitement, qui nous a permis d{'}extraire de nos analyses les informations demand{\'e}es par la campagne EASy. Nous terminons par une {\'e}valuation quantitative de nos architectures."
2005.jeptalnrecital-court.4,Un analyseur {LFG} efficace pour le fran{\\c{c}}ais : {SXLFG},2005,0,1,2,0.672215,46836,pierre boullier,Actes de la 12{\\`e}me conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Articles courts,0,"Dans cet article, nous proposons un nouvel analyseur syntaxique, qui repose sur une variante du mod{\`e}le Lexical-Functional Grammars (Grammaires Lexicales Fonctionnelles) ou LFG. Cet analyseur LFG accepte en entr{\'e}e un treillis de mots et calcule ses structures fonctionnelles sur une for{\^e}t partag{\'e}e. Nous pr{\'e}sentons {\'e}galement les diff{\'e}rentes techniques de rattrapage d{'}erreurs que nous avons mises en oeuvre. Puis nous {\'e}valuons cet analyseur sur une grammaire {\`a} large couverture du fran{\c{c}}ais dans le cadre d{'}une utilisation {\`a} grande {\'e}chelle sur corpus vari{\'e}s. Nous montrons que cet analyseur est {\`a} la fois efficace et robuste."
2005.jeptalnrecital-court.19,Les M{\\'e}ta-{RCG}: description et mise en oeuvre,2005,-1,-1,1,1,250,benoit sagot,Actes de la 12{\\`e}me conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Articles courts,0,"Nous pr{\'e}sentons dans cet article un nouveau formalisme linguistique qui repose sur les Grammaires {\`a} Concat{\'e}nation d{'}Intervalles (RCG), appel{\'e} M{\'e}ta-RCG. Nous exposons tout d{'}abord pourquoi la non-lin{\'e}arit{\'e} permet une repr{\'e}sentation ad{\'e}quate des ph{\'e}nom{\`e}nes linguistiques, et en particulier de l{'}interaction entre les diff{\'e}rents niveaux de description. Puis nous pr{\'e}sentons les M{\'e}ta-RCG et les concepts linguistiques suppl{\'e}mentaires qu{'}elles mettent en oeuvre, tout en restant convertibles en RCG classiques. Nous montrons que les analyses classiques (constituants, d{\'e}pendances, topologie, s{\'e}mantique pr{\'e}dicat-arguments) peuvent {\^e}tre obtenues par projection partielle d{'}une analyse M{\'e}ta-RCG compl{\`e}te. Enfin, nous d{\'e}crivons la grammaire du fran{\c{c}}ais que nous d{\'e}veloppons dans ce nouveau formalisme et l{'}analyseur efficace qui en d{\'e}coule. Nous illustrons alors la notion de projection partielle sur un exemple."
clement-etal-2004-morphology,Morphology Based Automatic Acquisition of Large-coverage Lexica,2004,6,49,2,0,43391,lionel clement,Proceedings of the Fourth International Conference on Language Resources and Evaluation ({LREC}{'}04),0,"In this article, we introduce a new technique for constructing wide-coverage morphological lexica from large corpora and morphological knowledge, with an application to French. Basically, it relies on the idea that the existence of a hypothetical lemma can be guessed if several different words found in the corpus are best interpreted as morphological variants of this lemma. We first validated our technique by extracting verbs and adjectives on a general French corpus of 25 million words. Compared with other lexical resources available for French, our results are very satisfying, since we cover many words, often derived words, that are not always present in other lexica. Application of our algorithm to the acquisition of domain-specific adjectives on a botanic corpus gave also very good results, thus demonstrating its usability to extract domain-specific lexica. Moreover, it is generalizable to any language with a substantial morphology."
2004.jeptalnrecital-long.36,Les Grammaires {\\`a} Concat{\\'e}nation d{'}Intervalles ({RCG}) comme formalisme grammatical pour la linguistique,2004,-1,-1,1,1,250,benoit sagot,Actes de la 11{\\`e}me conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Articles longs,0,"Le but de cet article est de montrer pourquoi les Grammaires {\`a} Concat{\'e}nation d{'}Intervalles (Range Concatenation Grammars, ou RCG) sont un formalisme particuli{\`e}rement bien adapt{\'e} {\`a} la description du langage naturel. Nous expliquons d{'}abord que la puissance n{\'e}cessaire pour d{\'e}crire le langage naturel est celle de PTIME. Ensuite, parmi les formalismes grammaticaux ayant cette puissance d{'}expression, nous justifions le choix des RCG. Enfin, apr{\`e}s un aper{\c{c}}u de leur d{\'e}finition et de leurs propri{\'e}t{\'e}s, nous montrons comment leur utilisation comme grammaires linguistiques permet de traiter des ph{\'e}nom{\`e}nes syntagmatiques complexes, de r{\'e}aliser simultan{\'e}ment l{'}analyse syntaxique et la v{\'e}rification des diverses contraintes (morphosyntaxiques, s{\'e}mantique lexicale), et de construire dynamiquement des grammaires linguistiques modulaires."
