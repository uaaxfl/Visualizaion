2004.eamt-1.3,W03-2201,1,0.926972,"tion and disambiguation of the source text – aspects which could be significantly improved by named entity recognition. We further suggest an automatic method for distinguishing and lexical differences in MT output that could have applications in automated MT evaluation for morphologically rich languages. 1. different knowledge sources, in a similar way to WSD (Stevenson and Wilks, 2001). But the cross-level nature of this problem also suggests that improvement in MT quality could be achieved through improving related aspects of the source-text analysis, such as Named Entity (NE) recognition (Babych and Hartley, 2003; Somers, 2003:524). For the purposes of this discussion, we assimilate proper nouns to NEs and investigate NE recognition as a possible solution to the PCD problem insofar as it might enable the selection of the correct strategy. Accurate NE recognition is important for the general quality of MT for the following reasons: 1. The translation of the same token may be different depending on whether the token is a common noun or part of an NE, e.g. in Russian if a common name is a part of an organization name, a “do-not-translate” or “transliterate” strategy should be used instead of a default tr"
2004.eamt-1.3,M95-1017,0,0.0475425,"Missing"
2004.eamt-1.3,2001.mtsummit-papers.68,0,0.0644277,"Missing"
2005.mtsummit-posters.13,P04-1079,1,0.83624,"Missing"
2005.mtsummit-posters.13,babych-hartley-2004-modelling,1,0.86669,"Missing"
2005.mtsummit-posters.13,J82-2006,0,0.586519,"for concepts). The difference in the degree of analytism may explain the differences in the parameters for French and Spanish whitepaper texts. It should be also noted that within a particular language ‘typological distance’ between sublanguages (or text-types) could be different: it is intuitively plausible that the colloquial style of emails in German is very different from the style of legal documents, such as the whitepaper – in terms of lexicon and syntax – and that such a distance is possibly greater than between English or French emails and the whitepaper texts in those languages (cf. Kittredge, 1982). This could provide a clue as to why there is a difference in regression parameters across text types in German, but there is no such difference in English, French or Italian. However, the most important and interesting result of our experiment is the very fact that the regression parameters do vary across text types and target languages (TLs), so they cannot be re-used for previously untested combinations of TLs/texttypes. This means that knowing the regression line parameters for a certain combination of these evaluation-external factors is not helpful for predicting human evaluation scores"
2005.mtsummit-posters.13,P02-1040,0,0.0786338,"ial 30 directed language pairs. The source texts consisted of a collection of emails and an EU whitepaper, text types given as representative of the end users’ translation inputs. The quality attribute we focused on in the human evaluations was adequacy – the extent to which the information content of the original, source text is judged to be preserved in the translation produced by the MT system. This decision reflected the projected use of the service for gisting and transactional correspondence rather than for publication. The n-gram metrics we used for the automated evaluations were BLEU (Papineni et al, 2002) developed at IBM and a weighted n-gram metric WNM (Babych, 2004). The correlations between the human judgements and the scores produced by both automated metrics were found to be highly reliable, but not in themselves sufficient for an automated score to be extrapolated to a human score. This prediction depends on two parameters of the regression line, namely target language and text type. Abstract The use of n-gram metrics to evaluate the output of MT systems is widespread. Typically, they are used in system development, where an increase in the score is taken to represent an improvement in"
2005.mtsummit-posters.13,1994.amta-1.25,0,0.841117,"Missing"
2007.mtsummit-papers.31,2005.mtsummit-posters.13,1,0.855467,"Missing"
2007.mtsummit-papers.31,C04-1016,1,0.902404,"Missing"
2007.mtsummit-papers.31,P04-1079,0,0.030327,"Missing"
2007.mtsummit-papers.31,hamon-etal-2006-cesta,0,0.0721614,"Missing"
2007.mtsummit-papers.31,hamon-rajman-2006-x,0,0.0228193,"Missing"
2007.mtsummit-papers.31,2001.mtsummit-papers.68,0,0.074963,"Missing"
2007.mtsummit-papers.31,2005.mtsummit-papers.16,1,0.863637,"Missing"
2007.mtsummit-papers.31,2001.mtsummit-eval.10,0,0.108512,"Missing"
2007.mtsummit-papers.31,1994.amta-1.25,0,0.240679,"Missing"
2007.mtsummit-papers.31,zhang-etal-2004-interpreting,0,0.0229871,"Missing"
2007.mtsummit-papers.31,P02-1040,0,\N,Missing
2007.mtsummit-papers.5,2005.mtsummit-osmtw.4,0,0.0172384,"compare the output quality of a direct MT process with that of a pivot MT process. MT between closely related languages has been very successful, achieving near-publishable quality (which needs very little or no post-editing) for a number of historically and structurally-related languages, such as Czech and Slovak (Hajic et al., 2000b), Catalan and Spanish (Alonso, 2005), Ukrainian and Russian (Gryaznukhina, 2004). Such engines explore similarities between the related languages (Dvorak et al., 2006) and typically rely on shallow processing techniques and knowledge-light linguistic resources (Armentano-Oller et al., 2005). High quality makes such MT systems useful in the pivot-based MT framework, which we take here to mean that the text is translated in several stages via one or more intermediate natural languages, or pivots. Overall translation quality crucially depends on the quality of the weakest link in the pipeline, which is usually the stage between more distant languages. From an engineering perspective, therefore, it is beneficial to use the best available MT system for that stage, even if there is no access to its source code. The only existing reference to an approach involving pivot-based translati"
2007.mtsummit-papers.5,C04-1016,1,0.899644,"terview and an article by a British diplomat). Table 1 presents the characteristics of the corpus. Language Texts Paras Sentences Ukrainian 35 1449 4675 Russian 35 1449 4528 English 35 1449 3513 Table 1: Parameters of MT evaluation corpus Words 64575 65181 68445 The size of our corpus is almost twice that of the DARPA 94 MT evaluation corpus of 36k words (White et al., 1994), which has been widely used for such tasks and has been shown to be sufficient for automated MT evaluation methods (e.g., BLEU) to ensure high correlation with human evaluation scores for translation adequacy and fluency (Babych et al., 2004). The corpus was aligned on the paragraph level and MT-translated into English using commercial MT systems available for Ukrainian, Russian and English. Table 2 gives the characterisitics of the MT systems used for the experiment. MT Pragma Plaj-Ruta Version / Dev 2.0 (2002) Trident Soft ProMT XP 5.0 (2003) ProLingLtd. 3.0 (2002) ProMT Systran 5.0 (2004) Systran S.A. Source L Ukrainian Russian Ukrainian Target L English Russian English Russian Russian English German French Russian German French English Table 2: MT systems The quality of MT was measured using the standard BLEU metric (Papineni"
2007.mtsummit-papers.5,P04-1079,1,0.835977,"erisitics of the MT systems used for the experiment. MT Pragma Plaj-Ruta Version / Dev 2.0 (2002) Trident Soft ProMT XP 5.0 (2003) ProLingLtd. 3.0 (2002) ProMT Systran 5.0 (2004) Systran S.A. Source L Ukrainian Russian Ukrainian Target L English Russian English Russian Russian English German French Russian German French English Table 2: MT systems The quality of MT was measured using the standard BLEU metric (Papineni et al., 2002), as well as the less commonly used WNM (Weighted N-gram Model), which on large corpus has been shown to produce a better correlation with human adequacy judgments (Babych and Hartley, 2004). BLEU and WNM are in some sense complementary, measuring different quality parameters: WNM assigns salience scores (similar to tf.idf) to Ngrams, which rewards matches of those content words that are most important for the general text structure. So its correlation with adequacy can be expected to be higher. BLEU, however, is a better predictor for fluency, since it does not disregard matching sequences of function words. BLEU was computed with one reference and N-gram size 5 (BLEUr1n5). The automated scores were computed for direct translation from Russian and Ukrainian into English, then fo"
2007.mtsummit-papers.5,2005.mtsummit-posters.13,1,0.882248,"both automated scores the best direct translation quality for English–Russian direction is achieved by ProMT (which is not surprising for a mainstream translation direction developed by a wellresourced Russian team working for many years). Thirdly, BLEU scores for closely related translation (ua&gt;ru) are much higher than for distant translation (ua&gt;en and ru&gt;en). Even though BLEU scores for translation into different languages (English vs. Russian) are not directly comparable – the difference in scores does not necessarily correspond to a difference in human judgment about translation quality (Babych et al., 2005) – there is still no doubt that for MT between closely related languages the number of N-gram matches between MT output and human reference is much higher, especially for longer N-grams. Interestingly, the distribution of BLEU scores for Ngrams of different length is different for MT between closely related languages and MT for distant languages. Chart 3 illustrates these distributions for N-grams N=1 to N=5. The most surprising fact is not the even greater Ngram precision for closely related translation, but the different rates of decline in precision for longer N-grams: the decline is close"
2007.mtsummit-papers.5,feldman-etal-2006-cross,0,0.0385578,"Missing"
2007.mtsummit-papers.5,A00-1002,0,0.315821,"anecdotal experience, but to our knowledge there has been no published evaluation of the actual drop in quality. The method proposed in this paper is novel in two respects. First, our pivot is closely related to the source language. Second, we use a parallel corpus to evaluate and compare the output quality of a direct MT process with that of a pivot MT process. MT between closely related languages has been very successful, achieving near-publishable quality (which needs very little or no post-editing) for a number of historically and structurally-related languages, such as Czech and Slovak (Hajic et al., 2000b), Catalan and Spanish (Alonso, 2005), Ukrainian and Russian (Gryaznukhina, 2004). Such engines explore similarities between the related languages (Dvorak et al., 2006) and typically rely on shallow processing techniques and knowledge-light linguistic resources (Armentano-Oller et al., 2005). High quality makes such MT systems useful in the pivot-based MT framework, which we take here to mean that the text is translated in several stages via one or more intermediate natural languages, or pivots. Overall translation quality crucially depends on the quality of the weakest link in the pipeline,"
2007.mtsummit-papers.5,2005.mtsummit-papers.11,0,0.0357083,"for better-resourced languages. This bottleneck can be opened by using Statistical Machine Translation (Och and Ney, 2003), (Marcu and Wong, 2002), which can be trained on parallel corpora for any language pair. However, development of a good quality SMT system requires the use of large collections of parallel texts aligned at the sentence level, amounting to at least several million words. At the same time, parallel corpora of this size tend to be very rare, especially for under-resourced languages. Even for well-resourced languages such resources also tend to be specialised, e.g. Europarl (Koehn, 2005), which covers the language of debates in the European Parliament, so their performance degrades significantly when the system is applied to a slightly different domain, e.g. news (Babych et al., 2007). In this paper we investigate the performance of translation from an under-resourced language into English via a closely-related, or cognate, pivot language with well-developed translation resources. Typically any language can be used as the pivot if it covers the bridge for a language pair that is not available in a given MT system. For instance, if no system translating from French to Japanese"
2007.mtsummit-papers.5,J03-1002,0,0.00373206,"eater than for others. There are commercial systems for translation into English from well-resourced languages, such as French or Russian, that can achieve acceptable quality for many practical applications of machine translation. At the same time there are many more languages for which good quality translation resources are not available. For some of those languages MT systems have occasionally been developed, but their lexical and syntactic coverage is very far from what has been achieved for better-resourced languages. This bottleneck can be opened by using Statistical Machine Translation (Och and Ney, 2003), (Marcu and Wong, 2002), which can be trained on parallel corpora for any language pair. However, development of a good quality SMT system requires the use of large collections of parallel texts aligned at the sentence level, amounting to at least several million words. At the same time, parallel corpora of this size tend to be very rare, especially for under-resourced languages. Even for well-resourced languages such resources also tend to be specialised, e.g. Europarl (Koehn, 2005), which covers the language of debates in the European Parliament, so their performance degrades significantly"
2007.mtsummit-papers.5,P02-1040,0,0.0820725,"l., 2004). The corpus was aligned on the paragraph level and MT-translated into English using commercial MT systems available for Ukrainian, Russian and English. Table 2 gives the characterisitics of the MT systems used for the experiment. MT Pragma Plaj-Ruta Version / Dev 2.0 (2002) Trident Soft ProMT XP 5.0 (2003) ProLingLtd. 3.0 (2002) ProMT Systran 5.0 (2004) Systran S.A. Source L Ukrainian Russian Ukrainian Target L English Russian English Russian Russian English German French Russian German French English Table 2: MT systems The quality of MT was measured using the standard BLEU metric (Papineni et al., 2002), as well as the less commonly used WNM (Weighted N-gram Model), which on large corpus has been shown to produce a better correlation with human adequacy judgments (Babych and Hartley, 2004). BLEU and WNM are in some sense complementary, measuring different quality parameters: WNM assigns salience scores (similar to tf.idf) to Ngrams, which rewards matches of those content words that are most important for the general text structure. So its correlation with adequacy can be expected to be higher. BLEU, however, is a better predictor for fluency, since it does not disregard matching sequences of"
2007.mtsummit-papers.5,P99-1067,0,0.0607218,"stem for translation into the related pivot. Existing research with Czech and Slovak (Hajic et al., 2000a) shows that simple transfer systems operating at the word level can produce reasonable results for closely related languages. The induction of transfer rules for closely related languages can be achieved using comparable corpora: bootstrapping from a small initial bilingual lexicon or the set of orthographic cognates, the system can identify words of the two languages that occur in contexts with a large number of words that are known mutual translations from the seed lexicon. As shown in (Rapp, 1999) this automatic procedure can produce a reliable bilingual lexicon without resorting to parallel corpora. This procedure relies on the availability of morphological resources and sufficiently large comparable corpora (of the size of 20-100 million words). The feasibility of semi-automatic acquisition of such corpora has already been demonstrated (Sharoff, 2006). Experiments with creating taggers and lemmatisers (Feldman et al., 2006) also show that it is possible to bootstrap a sufficiently accurate tagger on the basis of existing resources for cognate languages. This opens the possibility to"
2007.mtsummit-papers.5,1994.amta-1.25,0,0.213082,"Missing"
2007.tc-1.3,P98-1117,0,0.0129615,"semantic field categories. Often a lexical item is mapped to multiple semantic categories, reflecting its potential multiple senses. In such cases, the tags are arranged by the order of likelihood of meanings, with the most prominent first. 3 Objective evaluation In the objective evaluation we tested the performance of our system on a selection of indirect translation problems, extracted from a parallel corpus consisting mostly of articles from English and Russian newspapers (118,497 words in the R-E direction, 589,055 words in the E-R direction). It was aligned at the sentence level by JAPA (Langlais et al., 1998), and further at the word level by GIZA++ (Och and Ney, 2003). 3.1 Comparative performance The intuition behind the objective evaluation experiment is that the capacity of ASSIST to find indirect translation equivalents in comparable corpora can be compared with the results of automatic alignment of parallel texts used in translation models in SMT: one of the major 6/10 advantages of the SMT paradigm is its ability to reuse indirect equivalents found in parallel corpora (equivalents that may never come up in hand-crafted dictionaries). Thus, automatically generated GIZA++ dictionaries with wor"
2007.tc-1.3,J03-1002,0,0.00770091,"tch, but also because of the paucity of suitable aligned (parallel) corpora. The approach adopted here includes the use of comparable corpora in source and target languages, i.e. corpora of texts dealing with similar subject matter and intended for similar readerships. These are relatively easy to create, by ‘harvesting’ them from the internet, for example, and there are no alignment costs. The greatest challenge is to generate a list of solutions that translators will find usable and to rank them such that the best are at the top. While ASSIST is unlike statistical machine translation (SMT – Och and Ney, 2003), where lexical selection is effected by a translation model based on aligned, parallel corpora, the novel techniques it has developed are exploitable in the SMT paradigm. It also differs from 2/10 now traditional uses of comparable corpora for detecting translation equivalents (Rapp, 1999) or extracting terms (Grefenstette, 2002) which exhibit a one-to-one correspondence irrespective of the context. ASSIST addresses difficulties with expressions from the general lexicon, whose translation is context-dependent. 2 Methodology The software acts as a decision support system for nologies for extra"
2007.tc-1.3,2001.mtsummit-papers.68,0,0.0213128,"actory condition, bad state of repair, badly in need of repair, and so on. The objective evaluation shows that the system has been able to find the suggestion used by a particular translator for the problem studied. It does not tell us whether the system has found some other translations suitable for the context. Such legitimate translation variation implies that the performance of a system should be studied on the basis of multiple reference translations. For the purposes of evaluating a fully automatic MT tool, the typical practice of using just two reference translations may be sufficient (Papineni, et al, 2001). However, in the context of a translator’s amanuensis which deals with expressions difficult for human translators, it is reasonable to work with a larger range of acceptable target expressions. With this in mind we evaluated the performance of the tool with a panel of 12 professional translators, members of ITI and the Chartered Institute of Linguists. Test materials were provided in which problematic expressions were highlighted and the translators were asked to find suitable suggestions produced by the tool for these expressions and rank their usability on a scale from 1 to 5 (‘not accepta"
2007.tc-1.3,P99-1067,0,0.0508505,"create, by ‘harvesting’ them from the internet, for example, and there are no alignment costs. The greatest challenge is to generate a list of solutions that translators will find usable and to rank them such that the best are at the top. While ASSIST is unlike statistical machine translation (SMT – Och and Ney, 2003), where lexical selection is effected by a translation model based on aligned, parallel corpora, the novel techniques it has developed are exploitable in the SMT paradigm. It also differs from 2/10 now traditional uses of comparable corpora for detecting translation equivalents (Rapp, 1999) or extracting terms (Grefenstette, 2002) which exhibit a one-to-one correspondence irrespective of the context. ASSIST addresses difficulties with expressions from the general lexicon, whose translation is context-dependent. 2 Methodology The software acts as a decision support system for nologies for extracting indirect translation equivalents following subsections we give the user perspective on ogy underlying each of its sub-tasks. Explanations of vided by (Babych et al., 2007). 2.1 translators. It integrates different techfrom large comparable corpora. In the the system and describe the m"
2007.tc-1.3,rapp-2004-freely,0,0.0655345,"Missing"
2007.tc-1.3,P06-2095,1,0.807661,"the following: Children attend schools that are in poor repair and lacking basic essentials Thus ASSIST supports translators in making decisions about indirect translation equivalents in a number of ways: it suggests possible structural and lexical transformations for contextual descriptors; it verifies which translation variants co-occur in the TL corpus; and it illustrates the use of the transformed TL lexical descriptors in actual contexts. 2.2 Generating and ranking translation equivalents The method for generating translation equivalents is a generalisation of one used in previous work (Sharoff et al., 2006) on extracting equivalents for continuous multiword expressions (MWEs). Essentially, the method expands the search queries for each word and its dictionary translations with entries from thesauri automatically computed from the corpora. It then checks which combinations are possible in the TL corpus or corpora. These potential translation equivalents are now ranked by their distributional similarity to the original query and presented to the user. In this way, the range of retrievable equivalents has been extended from a relatively limited range of two-word constructions which mirror POS categ"
2009.eamt-1.6,2007.mtsummit-papers.5,1,0.845111,"Missing"
2012.eamt-1.57,2002.tc-1.7,0,0.0680251,"in the software domain. More specific studies have been undertaken to identify those rules which have the greatest impact on the usability of MT output (e.g., O’Brien and Roturier, 2007). © 2012 European Association for Machine Translation. 237 Overwhelmingly, controlled language studies have focused on English as source language. This is not to say that CL varieties do not exist for languages other than English. Among recent work, Barthe (1998) relates the process of developing GIFAS, the ‘rationalised’ French counterpart of the AECMA documentation standard for the aerospace industry, while Lieske et al. (2002) describe a controlled German. In the case of Japanese, the application of the CL notion dates back to (Nagao and Tanaka, 1984), who describe a framework for assisting authors in producing what they termed ‘machinereadable’ Japanese. Yoshida (1987) outlines a framework for designing a ‘standardised’ Japanese for MT. Kaji (1999) offers a few Japanese examples. More recent computational work has focused on automatic re-writing of what we can term ‘MT-intractable’ Japanese (e.g., Shirai, 1998; Matsuyoshi et al., 2004). Since such re-writing is a machine-internal process, these studies are not nec"
2012.eamt-1.57,1999.mtsummit-1.6,0,0.0603831,"is not to say that CL varieties do not exist for languages other than English. Among recent work, Barthe (1998) relates the process of developing GIFAS, the ‘rationalised’ French counterpart of the AECMA documentation standard for the aerospace industry, while Lieske et al. (2002) describe a controlled German. In the case of Japanese, the application of the CL notion dates back to (Nagao and Tanaka, 1984), who describe a framework for assisting authors in producing what they termed ‘machinereadable’ Japanese. Yoshida (1987) outlines a framework for designing a ‘standardised’ Japanese for MT. Kaji (1999) offers a few Japanese examples. More recent computational work has focused on automatic re-writing of what we can term ‘MT-intractable’ Japanese (e.g., Shirai, 1998; Matsuyoshi et al., 2004). Since such re-writing is a machine-internal process, these studies are not necessarily directly applicable to guiding the authoring of human-readable texts. Morita and Ishida (2011) provide protocols to enable monolingual users to converge on a correct Japanese/English machine translation, but no a priori writing or editing rules are proposed. The proposals in (Sato et al., 2003) are motivated by persona"
2012.eamt-1.9,1999.mtsummit-1.6,0,0.094763,"Missing"
2012.tc-1.1,P07-2002,1,0.672352,"ide-by-side with differences highlighted. Babych, Hartley, Kageura, Thomas, Utiyama 5 MNH-TT: a collaborative platform for translator training Translating and the Computer 34 29-30 November 2012, London, UK Figure 1: Dictionary lookup in QRedit MNH assumes translators work voluntarily and not upon request by customers, so the management of overall workflow in commercial settings is not explicitly provided, although it can be simulated using existing MNH functions. Translation work is carried out using the translation-aid editor QRedit, a two-pane editor which provides the following functions (Abekawa and Kageura, 2007):      lookup of dictionaries and terminologies (idiom variants are matched to their canonical forms; multi-word units and idioms are prioritised) seamless connection to bilingual corpora (TMs) seamless connection to Wikipedia monolingual and bilingual entries seamless connection to Google webpage and dictionary search registration of terms. These functions are triggered by mouse actions starting from the relevant words or phrases in the SL text area. Throughout these actions, the keyboard remains active in the TL text area in order to improve the efficiency of translation (Figure 1). The"
2012.tc-1.1,abekawa-kageura-2008-constructing,1,0.654492,"contribution. The poster may also specify their role and the object or ‘prop’ to which they are referring; these include: translation-brief, set-of-targetdocuments, research-data, glossary, tms, mt-raw-output, as well as text spans such as sentence or word. Figure 3 illustrates the bulletin board displaying a number of interactions between a project manager and volunteers signing up to play various roles in the project. Figure 3: Interaction in MNH-TT structured by dialogue act and referencing role 2.4 MNH-TT: revision categorisation The third extension provides a set of categories based on (Abekawa and Kageura, 2008; Castagnoli et al., 2006; Secara, 2005) to allow revisers to motivate and justify individual revisions (Table 3). The defined categories are grouped thematically.     content- revisions bear on the perceived transfer of ideas between the source and the target document lexis- revisions bear on the choice of words and terms grammar- revisions bear on the well-formedness of the target document text- revisions relate to departures from the conventions holding for the genre of the target document, or to clumsiness, or to a lack of cohesion. Revision categories are represented as a menu. Each t"
2012.tc-1.1,abekawa-etal-2010-community,1,0.806611,"Missing"
2012.tc-1.1,W07-0732,0,0.148105,"Missing"
2012.tc-1.1,2012.tc-1.1,1,0.0530913,"Missing"
2012.tc-1.1,2009.mtsummit-posters.22,1,0.370012,"Missing"
2012.tc-1.1,2009.tc-1.4,1,0.205718,"Missing"
2015.mtsummit-papers.8,2012.eamt-1.57,1,0.940682,"ocuments drawing on existing wisdom about technical writing in Japanese. Since the study chiefly referred to writing guidelines intended for human understandability or readability, the overall efficacy of the rules with MT was not significant. Thus, there remains much room for investigating other patterns impactProceedings of MT Summit XV, vol.1: MT Researchers&apos; Track Miami, Oct 30 - Nov 3, 2015 |p. 91 ing on MT performance within the municipal domain. O’Brien (2006) argued persuasively for the need to tune CL rule sets to language pair and MT system. The results of evaluation experiments by (Hartley et al., 2012) also suggested there were differences between rule-based machine translation (RBMT) and statistical machine translation (SMT) systems in terms of the impact of specific CL rules on their performance, although the MT systems as such were not the focus of their investigation. In short, it is still uncertain to what extent CL rules can be effectively generalised across MT systems or how much improvement can be attained if we compile rules specifically tuned to a given system. Practical deployment of CL requires that the readability of the source text (ST) should not be compromised in the interes"
2015.mtsummit-papers.8,J14-1005,0,0.0847801,"der of this paper is structured as follows. We describe related work in Section 2. In Section 3, we discuss how we constructed our CL rules, while Section 4 explains our experimental setup for human evaluation to assess the rules. We present our results accompanied with a discussion in Section 5. Section 6 concludes with implications for future work. 2 Related studies Controlled (natural) language or C(N)L is ‘a constructed language that is based on a certain natural language, being more restrictive concerning lexicon, syntax, and/or semantics while preserving most of its natural properties’ (Kuhn, 2014, p.123). A number of English CL rule sets have been proposed to improve MT performance as well as human comprehension, and they have been actually implemented, mainly in technical documentation (e.g., Kamprath et al., 1998; Nyberg et al., 2003). Evaluation experiments on CL for MT have also been undertaken to assess machine translatability and post-editing productivity (Pym, 1990; Bernth and Gdaniec, 2001; O’Brien and Roturier, 2007; Aikawa et al., 2007), showing evidence of the effectiveness of CL. In the case of Japanese CL, Nagao et al. (1984) devised a controlled grammar to syntactically"
babych-etal-2004-calibrating,P02-1040,0,\N,Missing
babych-etal-2004-calibrating,2001.mtsummit-papers.3,0,\N,Missing
babych-etal-2008-generalising,rapp-2004-freely,0,\N,Missing
babych-etal-2008-generalising,W06-2405,0,\N,Missing
babych-etal-2008-generalising,P07-1018,1,\N,Missing
babych-etal-2008-generalising,P06-1085,0,\N,Missing
babych-etal-2008-generalising,P06-1011,0,\N,Missing
babych-etal-2008-generalising,P07-1084,0,\N,Missing
babych-etal-2008-generalising,2005.mtsummit-papers.11,0,\N,Missing
babych-etal-2008-generalising,sharoff-2006-uniform,1,\N,Missing
babych-hartley-2004-modelling,P02-1040,0,\N,Missing
babych-hartley-2004-modelling,2001.mtsummit-papers.3,0,\N,Missing
babych-hartley-2008-sensitivity,E06-1032,0,\N,Missing
babych-hartley-2008-sensitivity,C04-1016,1,\N,Missing
babych-hartley-2008-sensitivity,P02-1040,0,\N,Missing
babych-hartley-2008-sensitivity,W06-1606,0,\N,Missing
bateman-hartley-2000-target,J97-1004,0,\N,Missing
bateman-hartley-2000-target,W96-0501,0,\N,Missing
bateman-hartley-2000-target,C96-1043,0,\N,Missing
bateman-hartley-2000-target,C96-2120,0,\N,Missing
bateman-hartley-2000-target,C96-2167,0,\N,Missing
bateman-hartley-2000-target,H89-1022,0,\N,Missing
bateman-hartley-2000-target,A97-1039,0,\N,Missing
C04-1016,2001.mtsummit-papers.3,0,0.0253953,"kes a fairer comparison between the MT systems evaluated on different corpora. The translation complexity metric was integrated into two automated MT evaluation packages – BLEU and the Weighted N-gram model. The extended MT evaluation tools are available from the first author’s web site: http://www.comp.leeds.ac.uk/bogdan/evalMT.html 1 Introduction Automated evaluation tools for MT systems aim at producing scores that are consistent with the results of human assessment of translation quality parameters, such as adequacy and fluency. Automated metrics such as BLEU (Papineni et al., 2002), RED (Akiba et al, 2001), Weighted N-gram model (WNM) (Babych, 2004), syntactic relation / semantic vector model (Rajman and Hartley, 2001) have been shown to correlate closely with scoring or ranking by different human evaluation parameters. Automated evaluation is much quicker and cheaper than human evaluation. Another advantage of the scores produced by automated MT evaluation tools is that intuitive human scores depend on the exact formulation of an evaluation task, on the granularity of the measuring scale and on the relative quality of the presented translation variants: human judges may adjust their evaluation"
C04-1016,E03-1029,0,0.0221245,"Missing"
C04-1016,P02-1040,0,0.0721199,"es. The suggested approach makes a fairer comparison between the MT systems evaluated on different corpora. The translation complexity metric was integrated into two automated MT evaluation packages – BLEU and the Weighted N-gram model. The extended MT evaluation tools are available from the first author’s web site: http://www.comp.leeds.ac.uk/bogdan/evalMT.html 1 Introduction Automated evaluation tools for MT systems aim at producing scores that are consistent with the results of human assessment of translation quality parameters, such as adequacy and fluency. Automated metrics such as BLEU (Papineni et al., 2002), RED (Akiba et al, 2001), Weighted N-gram model (WNM) (Babych, 2004), syntactic relation / semantic vector model (Rajman and Hartley, 2001) have been shown to correlate closely with scoring or ranking by different human evaluation parameters. Automated evaluation is much quicker and cheaper than human evaluation. Another advantage of the scores produced by automated MT evaluation tools is that intuitive human scores depend on the exact formulation of an evaluation task, on the granularity of the measuring scale and on the relative quality of the presented translation variants: human judges ma"
C04-1016,2001.mtsummit-eval.6,0,0.0307525,"tric was integrated into two automated MT evaluation packages – BLEU and the Weighted N-gram model. The extended MT evaluation tools are available from the first author’s web site: http://www.comp.leeds.ac.uk/bogdan/evalMT.html 1 Introduction Automated evaluation tools for MT systems aim at producing scores that are consistent with the results of human assessment of translation quality parameters, such as adequacy and fluency. Automated metrics such as BLEU (Papineni et al., 2002), RED (Akiba et al, 2001), Weighted N-gram model (WNM) (Babych, 2004), syntactic relation / semantic vector model (Rajman and Hartley, 2001) have been shown to correlate closely with scoring or ranking by different human evaluation parameters. Automated evaluation is much quicker and cheaper than human evaluation. Another advantage of the scores produced by automated MT evaluation tools is that intuitive human scores depend on the exact formulation of an evaluation task, on the granularity of the measuring scale and on the relative quality of the presented translation variants: human judges may adjust their evaluation scale in order to discriminate between slightly better and slightly worse variants – but only those variants which"
C04-1016,1994.amta-1.25,0,0.796466,"Missing"
C04-1016,E03-1004,0,\N,Missing
C16-2008,W02-2117,1,0.549993,"Missing"
C16-2008,J14-1005,0,0.0222758,"TUAL, which aims to help writers create multilingual texts. The highlighted feature of the system is that it enables machine translation (MT) to generate outputs appropriate to their functional context within the target document. Our system is operational online, implementing core mechanisms for document structuring and controlled writing. These include a topic template and a controlled language authoring assistant, linked to our statistical MT system. 1 Introduction For improved machine translatability, a wide variety of controlled language (CL) rule sets have been proposed (Kittredge, 2003; Kuhn, 2014). Evidence of reduced post-editing costs when a CL is employed is provided (Bernth and Gdaniec, 2001; O’Brien and Roturier, 2007), and several controlled authoring support tools, such as Acrolinx1 and MAXIT2 , have been developed. The fundamental limitation of the CLs proposed hitherto is, however, that they are defined at the level of the sentence rather than at the level of the document (Hartley and Paris, 2001). In fact, the notion of functional document element (see Section 2.1) does figure in some CL rule sets. ASD Simplified Technical English (ASD, 2013), for example, specifies writing p"
C16-2008,2003.eamt-1.10,0,0.0917482,"pic template is the core interface for authoring self-contained topics in a structured manner. The left pane in Figure 3 provides the basic DITA Task topic structure for composing municipal procedural documents. • CL authoring assistant analyses each sentence in the text box and highlights any segment that violates a local CL rule or controlled terminology, together with diagnostic comments and suggestions for rewriting (shown at bottom centre in Figure 3) (Miyata et al., 2016). In addition, we have implemented a preliminary rewriting support function with several of the features advocated by Mitamura et al. (2003). For a particular CL-noncompliant segment, the function offers alternative expressions; clicking one of the suggestions automatically replaces the offending segment in the text box above. • Pre-translation processing automatically modifies source segments in the background following transformation rules defined for each functional element, and then MT produces the translation and back-translation at the same time. 3 We used a Japanese morphological analyser MeCab. http://taku910.github.io/mecab/ 37 MT and back translation DITA task topic CL authoring assistant Figure 3: Task topic template fo"
C16-2008,2015.mtsummit-papers.8,1,0.857596,"Missing"
C96-1050,P95-1018,0,0.0219139,"Missing"
elliott-etal-2004-fluency,P02-1040,0,\N,Missing
elliott-etal-2004-fluency,takezawa-etal-2002-toward,0,\N,Missing
elliott-etal-2004-fluency,vanni-miller-2002-scaling,0,\N,Missing
elliott-etal-2004-fluency,2001.mtsummit-eval.6,0,\N,Missing
elliott-etal-2004-fluency,rajman-hartley-2002-automatic,1,\N,Missing
elliott-etal-2004-fluency,2001.mtsummit-papers.3,0,\N,Missing
kurella-etal-2008-corpus,ciobanu-etal-2006-using,1,\N,Missing
kurella-etal-2008-corpus,baroni-bernardini-2004-bootcat,0,\N,Missing
P06-2095,P98-2127,0,0.00486332,"languages. Unlike aligned parallel corpora, comparable corpora provide a model for each individual language, while dictionaries, which can serve as a bridge, are inadequate for the task in question, because the problem we want to address involves precisely translation equivalents that are not listed there. Therefore, a specific query needs first to be generalised in order to then retrieve a suitable candidate from a set of candidates. One way to generalise the query is by using similarity classes, i.e. groups of words with lexically similar behaviour. In his work on distributional similarity (Lin, 1998) designed a parser to identify grammatical relationships between words. However, broad-coverage parsers suitable for processing BNC-like corpora are not available for many languages. Another, resource-light approach treats the context as a bag of words (BoW) and detects the similarity of contexts on the basis of collocations in a window of a certain size, typically 3-4 words, e.g. (Rapp, 2004). Even if using a parser can increase precision in identification of contexts in the case of long-distance dependencies (e.g. to cook Alice a whole meal), we can find a reasonable set of relevant terms re"
P06-2095,J03-1002,0,0.00684622,"Missing"
P06-2095,C00-2090,0,0.0263067,"working on an option to identify semantic contexts by means of ‘semantic signatures’ obtained from a broad-coverage semantic parser, such as USAS (Rayson et al., 2004). The semantic tagset used by USAS is a languageindependent multi-tier structure with 21 major discourse fields, subdivided into 232 sub-categories (such as I1.1- = Money: lack; A5.1- = Evaluation: bad), which can be used to detect the semantic context. Identification of semantically similar situations can be also improved by the use of segment-matching algorithms as employed in Example-Based MT (EBMT) and translation memories (Planas and Furuse, 2000; Carl and Way, 2003). The proposed model looks similar to some implementations of statistical machine translation (SMT), which typically uses a parallel corpus for its translation model, and then finds the best possible recombination that fits into the target language model (Och and Ney, 2003). Just like an MT system, our tool can find translation equivalents for queries which are not explicitly coded as entries in system dictionaries. However, from the user perspective it resembles a dynamic dictionary or thesaurus: it translates difficult words and phrases, not entire sentences. The main th"
P06-2095,rapp-2004-freely,0,0.485094,"uitable candidate from a set of candidates. One way to generalise the query is by using similarity classes, i.e. groups of words with lexically similar behaviour. In his work on distributional similarity (Lin, 1998) designed a parser to identify grammatical relationships between words. However, broad-coverage parsers suitable for processing BNC-like corpora are not available for many languages. Another, resource-light approach treats the context as a bag of words (BoW) and detects the similarity of contexts on the basis of collocations in a window of a certain size, typically 3-4 words, e.g. (Rapp, 2004). Even if using a parser can increase precision in identification of contexts in the case of long-distance dependencies (e.g. to cook Alice a whole meal), we can find a reasonable set of relevant terms returned using the BoW approach, cf. the results of human evaluation for English and German by (Rapp, 2004). Finding translations in comparable corpora The proposed model finds potential translation equivalents in four steps, which include 1. expansion of words in the original expression using related words; 2. translation of the resultant set using existing bilingual dictionaries; 3. further ex"
P06-2095,P04-1079,1,0.82229,"terpretation of the results The results were surprising in so far as for the majority of problems translators preferred very different translation solutions and did not agree in their scores for the same solutions. For instance, concrete plan in Table 3 received the score 1 from translator t1 and 5 from t2. In general, the translators very often picked up on different opportunities presented by the suggestions from the lists, and most suggestions were equally legitimate ways of conveying the intended content, cf. the study of legitimate translation variation with respect to the BLEU score in (Babych and Hartley, 2004). In this respect it may be unfair to compute average scores for each potential solution, since for most interesting cases the scores do not fit into the normal distribution model. So averaging scores would mask the potential usability of really inventive solutions. In this case it is more reasonable to evaluate two sets of solutions – the one generated by ASSIST and the other found in dictionaries – but not each solution individually. In order to do that for each translation problem the best scores given by each translator in each of these two sets were selected. This way of generalising data"
P06-2095,C98-2122,0,\N,Missing
P07-1018,P98-1117,0,0.0155735,"tic field categories. Often a lexical item is mapped to multiple semantic categories, reflecting its potential multiple senses. In such cases, the tags are arranged by the order of likelihood of meanings, with the most prominent first. 3 Objective evaluation In the objective evaluation we tested the performance of our system on a selection of indirect translation problems, extracted from a parallel corpus consisting mostly of articles from English and Russian newspapers (118,497 words in the R-E direction, 589,055 words in the E-R direction). It has been aligned on the sentence level by JAPA (Langlais et al., 1998), and further on the word level by GIZA++ (Och and Ney, 2003). 3.1 Comparative performance The intuition behind the objective evaluation experiment is that the capacity of our tool to find indirect translation equivalents in comparable corpora can be compared with the results of automatic alignment of parallel texts used in translation models in SMT: one of the major advantages of the SMT paradigm is its ability to reuse indirect equivalents found in parallel corpora (equivalents that may never come up in hand-crafted dictionaries). Thus, automatically generated GIZA++ dictionaries with word a"
P07-1018,J03-1002,0,0.0109802,"ions are indirect in that they involve lexical shifts or POS transformations. Finding such translations is a hard task that can benefit from automated assistance. 'Mining' such indirect equivalents is difficult, precisely because of the structural mismatch, but also because of the paucity of suitable aligned corpora. The approach adopted here includes the use of comparable corpora in source and target languages, which are relatively easy to create. The challenge is to generate a list of usable solutions and to rank them such that the best are at the top. Thus the present system is unlike SMT (Och and Ney, 2003), where lexical selection is effected by a translation model based on aligned, parallel corpora, but the novel techniques it has developed are exploitable in the SMT paradigm. It also differs from now traditional uses of comparable corpora for detecting translation equivalents (Rapp, 1999) or extracting terminology (Grefenstette, 2002), which allows a one-to-one correspondence irrespective of the context. Our system addresses difficulties in expressions in the general lexicon, whose translation is context-dependent. The structure of the paper is as follows. In Section 2 we present the method w"
P07-1018,2001.mtsummit-papers.68,0,0.017632,"ression плохо отремонтированные. It is also possible to translate it as unsatisfactory condition, bad state of repair, badly in need of repair, and so on. The objective evaluation shows that the system has been able to find the suggestion used by a particular translator for the problem studied. It does not tell us whether the system has found some other translations suitable for the context. Such legitimate translation variation implies that the performance of a system should be studied on the basis of multiple reference translations, though typically just two reference translations are used (Papineni, et al, 2001). This might be enough for the purposes of a fully automatic MT tool, but in the context of a translator's amanuensis which deals with expressions difficult for human translators, it is reasonable to work with a larger range of acceptable target expressions. With this in mind we evaluated the performance of the tool with a panel of 12 professional translators. Problematic expressions were highlighted and the translators were asked to find suitable suggestions produced by the tool for these expressions and rank their usability on a scale from 1 to 5 (not acceptable to fully idiomatic, so 1 mean"
P07-1018,P99-1067,0,0.112298,"able aligned corpora. The approach adopted here includes the use of comparable corpora in source and target languages, which are relatively easy to create. The challenge is to generate a list of usable solutions and to rank them such that the best are at the top. Thus the present system is unlike SMT (Och and Ney, 2003), where lexical selection is effected by a translation model based on aligned, parallel corpora, but the novel techniques it has developed are exploitable in the SMT paradigm. It also differs from now traditional uses of comparable corpora for detecting translation equivalents (Rapp, 1999) or extracting terminology (Grefenstette, 2002), which allows a one-to-one correspondence irrespective of the context. Our system addresses difficulties in expressions in the general lexicon, whose translation is context-dependent. The structure of the paper is as follows. In Section 2 we present the method we use for mining translation equivalents. In Section 3 we present the results of an objective evaluation of the quality of suggestions produced by the system by comparing our output against a parallel corpus. Finally, in Section 4 we present a subjective evaluation focusing on the integrat"
P07-1018,rapp-2004-freely,0,0.174703,"general lexicon, which do not have established equivalents, but not yet for terminology. It relies on a high-quality bilingual dictionary (en-ru ~30k, ru-en ~50K words, combining ORD and the core part of Multitran) and large comparable corpora (~200M En, ~70M Ru) of news texts. For each of the SL query terms q the system generates its dictionary translation Tr(q) and its similarity class S(q) – a set of words with a similar distribution in a monolingual corpus. Similarity is measured as the cosine between collocation vectors, whose dimensionality is reduced by SVD using the implementation by Rapp (2004). The descriptor and each word in the similarity class are then translated into the TL using ORD or the Multitran dictionary, resulting in {Tr(q)∪ Tr(S(q))}. On the TL side we also generate similarity classes, 138 but only for dictionary translations of query terms Tr(q) (not for Tr(S(q)), which can make output too noisy). We refer to the resulting set of TL words as a translation class T. T = {Tr(q) ∪ Tr(S(q)) ∪ S(Tr(q))} Translation classes approximate lexical and structural transformations which can potentially be applied to each of the query terms. Automatically computed similarity classes"
P07-1018,P06-2095,1,0.875587,"transformation. The resulting translation may be the following: Children attend schools that are in poor repair and lacking basic essentials Thus our system supports translators in making decisions about indirect translation equivalents in a number of ways: it suggests possible structural and lexical transformations for contextual descriptors; it verifies which translation variants co-occur in the TL corpus; and it illustrates the use of the transformed TL lexical descriptors in actual contexts. 2.2 Generating translation equivalents We have generalised the method used in our previous study (Sharoff et al., 2006) for extracting equivalents for continuous multiword expressions (MWEs). Essentially, the method expands the search space for each word and its dictionary translations with entries from automatically computed thesauri, and then checks which combinations are possible in target corpora. These potential translation equivalents are then ranked by their similarity to the original query and presented to the user. The range of retrievable equivalents is now extended from a relatively limited range of two-word constructions which mirror POS categories in SL and TL to a much wider set of co-occurring l"
P07-1018,P02-1040,0,\N,Missing
P07-1018,C98-1113,0,\N,Missing
P96-1026,W94-0308,1,0.837463,"that the patterns of realisations uncovered in our analysis follow the principle of good technical writing practice known as the minimalist approach, e.g., (Carroll, 1994; Hammond, 1994). Moreover, we observe that our corpus does not exhibit shortcomings identified in a Systemic Functional analysis of English software manuals (Plum et al., 1990), such as a high incidence of agentless passive and a failure to distinguish the function of informing from that of instructing. Other work has focused on the cross-linguistic realisations of two specific semantic relations (generation and enablement) (Delin et al., 1994; Delia et 197 al., 1996), in a more general corpus of instructions for household appliances. Our work focuses on the single application domain of software instructions. However, it takes into consideration the whole task structure and looks at the realisation of semantic elements as found in the knowledge base, instead of two semantic relations not explicitly present in the underlying semantic model. 10 Conclusion In this paper we have shown how genre and task structure provide two essential sources of control over the text generation process. Genre does so by constraining the selection of th"
P96-1026,C96-1050,1,0.873674,"Missing"
P96-1026,J82-2006,0,0.144411,"Furthermore, we know that Macintosh documentation undergoes thorough local quality control. It certainly conforms to the principles of good documentation established by current research on technical documentation and on the needs of end-users, e.g., (Carroll, 1994; Hammond, 1994), in that it supplies clear and concise information for the task at hand. Finally, we have been assured by French users of the software that they consider this particular manual to be well written and to bear no unnatural trace of its origins. Technical manuals within a specific domain constitute a sublanguage, e.g., (Kittredge, 1982; Sager et al., 1980). An important defining property of a sublanguage is that of closure, both lexieal and syntactic. Lexical closure has been demonstrated by, for example, (Kittredge, 1987), who shows that after as few as the first 2000 words of a sublanguage text, the number of new word types increases little if at all. Other work, e.g., (Biber, 1988; Biber, 1989) and (Grishman and Kittredge, 1986) illustrates the property of syntactic closure, which means that generally available constructions just do not occur in this or that sublanguage. In the light of these results, we considered a cor"
P96-1026,W94-0307,0,0.164563,"two chapters which provide the user with generic instructions for performing relevant tasks, and descriptions of the commands available within MacWrite. The overlap in information between the two chapters offers opportunities to observe differences in the linguistic expressions of the same task structure elements in different contexts. features Our lexico-grammatical coding was done using the networks and features of the Nigel grammar (Halliday, 1985). We focused on four main concerns, guided by previous work on instructional texts, e.g., (Lehrberger, 1986; Plum et at., 1990; Ghadessy, 1993; Kosseim and Lapalme, 1994). • Relations between processes: to determine whether textual cohesion was achieved through conjunctives or through relations implicit in the task structure elements. Among the features considered were clause dependency and conjunction type. • Agency: to see whether the actor performing or enabling a particular action is clearly identified, and whether the reader is explicitly addressed. We coded here for features such as voice and agent types. • Mood, modality and polarity: to find out the extent to which actions are presented to the reader as being desirable, possible, mandatory, or prohibit"
rajman-hartley-2002-automatic,besancon-rajman-2002-evaluation,1,\N,Missing
rajman-hartley-2002-automatic,P02-1040,0,\N,Missing
rajman-hartley-2002-automatic,2001.mtsummit-eval.6,1,\N,Missing
sharoff-etal-2006-using,rapp-2004-freely,0,\N,Missing
sharoff-etal-2006-using,bennison-bowker-2000-designing,0,\N,Missing
sharoff-etal-2006-using,C00-2090,0,\N,Missing
sharoff-etal-2006-using,P02-1040,0,\N,Missing
sharoff-etal-2006-using,P04-1079,1,\N,Missing
sharoff-etal-2006-using,sharoff-2006-uniform,1,\N,Missing
W01-0815,P98-1006,0,0.0133926,"ourse mediated by (c); that is, we focused on the issue of creating an accurate model. This is an easier issue than that of the fidelity of the output text to the model (b), while the representations in (d) are too remote from one another to permit useful comparison. To measure the correspondence between the actual models and the desired/target models, we adopted the Generation String Accuracy (GSA) metric (Bangalore, Rambow and Whittaker, 2000; Bangalore and Rambow, 2000) used in evaluating the output of a NLG system. It extends the simple Word Accuracy metric suggested in the MT literature (Alshawi et al., 1998), based on the string edit distance between some reference text and the output of the system. As it stands, this metric fails to account for some of the special properties of the text generation task, which involves ordering word tokens. Thus, corrections may involve reordering tokens. In order not to penalise a misplaced constituent twice—as both a deletion and an insertion—the generation accuracy metric treats the deletion (D) of a token from one location and its insertion (I) at another location as a single movement (M). The remaining deletions, insertions, and substitutions (S) are counted"
W01-0815,C00-1007,0,0.014121,"edium oven for 1.5 hours.” and “To cook a goose: First pluck the goose. Then put it in a medium oven for 1.5 hours.” We focused on (a), which was of course mediated by (c); that is, we focused on the issue of creating an accurate model. This is an easier issue than that of the fidelity of the output text to the model (b), while the representations in (d) are too remote from one another to permit useful comparison. To measure the correspondence between the actual models and the desired/target models, we adopted the Generation String Accuracy (GSA) metric (Bangalore, Rambow and Whittaker, 2000; Bangalore and Rambow, 2000) used in evaluating the output of a NLG system. It extends the simple Word Accuracy metric suggested in the MT literature (Alshawi et al., 1998), based on the string edit distance between some reference text and the output of the system. As it stands, this metric fails to account for some of the special properties of the text generation task, which involves ordering word tokens. Thus, corrections may involve reordering tokens. In order not to penalise a misplaced constituent twice—as both a deletion and an insertion—the generation accuracy metric treats the deletion (D) of a token from one loc"
W01-0815,W00-1401,0,0.184146,"Missing"
W01-0815,A94-1001,0,0.0318942,". As such, they are obvious candidates as the standard against which to measure the content of the texts that are generated from them. We first consider the case of feedback presented in graphical mode, and then the option of textual feedback, using the WYSIWYM technology (Power and Scott, 1998; Scott, Power and Evans, 1998). We go on to make recommendations concerning the desirable properties of the feedback text. 4 Graphical representations of content Symbolic authoring systems typically make use of graphical representations of the content of the domain model—for example, conceptual graphs (Caldwell and Korelsky, 1994). Once trained in the language of the interface, the domain specialist uses standard text-editing devices such as menu selection and navigation with a cursor, together with standard text-editing actions (e.g., select, copy, paste, delete) to create and edit the content specification of the text to be generated in one or several selected languages. The user of AGILE, conceived to be a specialist in the domain of the particular software for which the manual is required (i.e., CAD/CAM), models the procedures for how to use the software. AGILE’s graphical user interface (Hartley, Power et al., 200"
W01-0815,C00-1069,0,0.0281267,"from natural language generation (NLG) or from machine translation (MT): indeed, they could result from the same language generator. Given this, it may be natural to assume that NLG could appropriately adopt evaluation methods developed for its more mature sister, MT. However, while this holds true for issues related to intelligibility (the second critical question), it does not apply as readily to issues of fidelity (the first question). We go beyond our recent experience of evaluating the AGILE system for producing multilingual versions of software user manuals (Hartley, Scott et al., 2000; Kruijff et al., 2000) and raise some open questions about how best to evaluate the faithfulness of an output text with respect to its input specification. 2 Evaluating intelligibility The use of rating scales to assess the intelligibility of MT output has been widespread since the early days in the field. Typically, monolingual raters assign a score to each sentence in the output text. However, this does not amount to an agreed methodology, since the number of points on the scale and their definition have varied considerably. For example, Carroll (1966) used a nine-point scale where point 1 was defined as “hopeles"
W01-0815,J98-3004,0,0.1511,"the particular domain model that serves as input to the generation system. This model may have been provided directly by an artificial agent, such as an expert system. Alternatively, it may have been constructed by a human agent as the intended instantiation of their mental model. Yet, whatever its origins, directly comparing this intermediate representation to the output text is problematic. A recent survey of complete NLG systems (Cahill et al., 1999) found that half of the 18 systems examined accepted input directly from another system1. A typical example is the Caption Generation System (Mittal et al., 1998), which produces paragraph-sized captions to accompany the complex graphics generated by SAGE (Roth et al., 1994). The input to generation includes definitions of the graphical constituents that are used to by SAGE to convey information: “spaces (e.g., charts, maps, tables), graphemes (e.g., labels, marks, bars), their properties (e.g., color, shape) and encoders—the frames of reference that enable their properties to be interpreted/translated back to data values (e.g., axes, graphical keys).”2 For obvious reasons, this does not readily lend itself to direct comparison with the generated text"
W01-0815,J85-2001,0,0.0958368,"with respect to its input specification. 2 Evaluating intelligibility The use of rating scales to assess the intelligibility of MT output has been widespread since the early days in the field. Typically, monolingual raters assign a score to each sentence in the output text. However, this does not amount to an agreed methodology, since the number of points on the scale and their definition have varied considerably. For example, Carroll (1966) used a nine-point scale where point 1 was defined as “hopelessly unintelligible” and point 9 as “perfectly clear and intelligible”; Nagao and colleagues (Nagao et al., 1985), in contrast, used a five-point scale, while Arnold and his colleagues (Arnold et al., 1994) suggest a four-point discrimination. In evaluating the intelligibility of the AGILE output, we asked professional translators and authors who were native speakers of the languages concerned—Bulgarian, Czech and Russian—to score individual text fragments on a four-point scale. The evaluators were also asked to give a summative assessment of the output’s suitability as the first draft of a manual. In a single pass, AGILE is capable of generating several types of text, each constituting a section of a ty"
W01-0815,P98-2173,1,0.787219,"knowledge representation language of the 1 By complete systems, we refer to systems that determine both “what to say” and “how to say it”, taking as input a specification that is not a hand-crafted simulation of some intermediate representation. 2 Mittal et al., 1998, pg. 438. 3 See Scott, Power and Evans, 1998. domain model. As such, they are obvious candidates as the standard against which to measure the content of the texts that are generated from them. We first consider the case of feedback presented in graphical mode, and then the option of textual feedback, using the WYSIWYM technology (Power and Scott, 1998; Scott, Power and Evans, 1998). We go on to make recommendations concerning the desirable properties of the feedback text. 4 Graphical representations of content Symbolic authoring systems typically make use of graphical representations of the content of the domain model—for example, conceptual graphs (Caldwell and Korelsky, 1994). Once trained in the language of the interface, the domain specialist uses standard text-editing devices such as menu selection and navigation with a cursor, together with standard text-editing actions (e.g., select, copy, paste, delete) to create and edit the conte"
W01-0815,W98-1427,1,\N,Missing
W01-0815,C98-2168,1,\N,Missing
W01-0815,C98-1006,0,\N,Missing
W03-2201,1998.amta-tutorials.5,0,\N,Missing
W03-2201,P02-1040,0,\N,Missing
W03-2201,M95-1017,0,\N,Missing
W03-2201,P02-1051,0,\N,Missing
W03-2201,white-etal-2000-determining,0,\N,Missing
W03-2201,2001.mtsummit-papers.3,0,\N,Missing
W03-2201,A97-2017,0,\N,Missing
W12-0114,P07-1018,1,0.869022,"translation quality. (III) Extension to other languages: Structural similarity and translation by pivot languages is used to obtain extension to further languages: High-quality translation between closely related languages (e.g., Russian and Ukrainian or Portuguese and Spanish) can be achieved with relatively simple resources (using linguistic similarity, but also homomorphism assumptions with respect to parallel text, if available), while greater efforts are put into ensuring better-quality translation between more distant languages (e.g. German and Russian). According to our prior research (Babych et al., 2007b) the pipeline between languages of different similarity results in improved translation quality for a larger number of language pairs (e.g., MT from Portuguese or Ukrainian into German is easier if there are highquality analysis and transfer modules for Spanish and Russian into German (respectively). Of course, (III) draws heavily on the detailed analysis and MT systems that the industrial partner in HyghTra provides for a number of languages. In the following sections we give more details of the work currently done with regard to (I) and with regard to parts of (II): the creation of a new M"
W12-0114,2007.mtsummit-papers.5,1,0.957669,"Missing"
W12-0114,E06-1032,0,0.0769042,"Missing"
W12-0114,2001.mtsummit-papers.18,1,0.831384,"able and are unlikely to become available in the future. Also, SMT tends to disregard important classificatory knowledge (such as morphosyntactic, categorical and lexical class features), which can be provided and used relatively easily within non-statistical representations. On the other hand, advantages of RBMT are that its (grammar and lexical) rules and information are understandable by humans and can be exploited for a lot of applications outside of translation (dictionaries, text understanding, dialogue systems, etc.). The slot grammar approach used in Lingenio systems (cf. McCord 1989, Eberle 2001) is a prime example of such linguistically rich representations that can be used for a number of different applications. Fig.1 shows this by a visualization of (an excerpt of) the entry for the ambiguous German verb einstellen in the database that underlies (a) the Lingenio translation products, where it links up with corresponding set of the transfer rules, and (b) Lingenio’s dictionary product TranslateDict, which is primarily intended for human translators. 101 Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 101–112, c Avign"
W12-0114,eberle-etal-2012-tool,1,0.868581,"Missing"
W12-0114,A94-1016,0,0.083521,"ation and statistical extension and training): (a) We start out with declarative analysis and generation components of the considered languages, and with basic bilingual dictionaries connecting to one another the entries of relatively small vocabularies comprising the most frequent words of each language in a given translation pair (cf. Fig 1 a). (b) Having completed this phase, we extend the dictionaries and train the analysis-, transfer- and generation-components of the rule-based core systems using monolingual and bilingual corpora. 1 A prominent early example is Frederking and colleagues (Frederking & Nirenburg, 1994). For an overview of hybrid MT till the late nineties see Streiter et al. (1999). More recent approaches include Groves & Way (2006a, 2006b). Commercial implementations include AppTek (http://www.apptek.com) and Language Weaver (http://www.languageweaver.com). An ongoing MT important project investigating hybrid methods is EuroMatrixPlus (http://www.euromatrixplus.net/) 102 (II) Error detection and improvement cycle: (a) We automatically discover the most frequent problematic grammatical constructions and multiword expressions for commercial RBMT and SMT systems using automatic construction-ba"
W12-0114,W97-0119,0,0.0461742,"Missing"
W12-0114,J93-1004,0,0.311866,"Missing"
W12-0114,2004.eamt-1.9,0,0.0664357,"Missing"
W12-0114,2006.eamt-1.15,0,0.0619246,"Missing"
W12-0114,habash-dorr-2002-handling,0,0.0417473,"Missing"
W12-0114,J06-4003,0,0.0132216,"Sentence Alignment; Melamed, 1999). For segmentation of text we use corresponding Lingenio-tools (unpublished).2 For word alignment Giza++ (Och & Ney, 2003) is the standard tool. Given a word alignment, the extraction of a (SMT) dictionary is relatively straightforward. With the exception of sentence segmentation, these algorithms are largely language independent and can be used for all of the languages that we consider. We did this for a number of language pairs on the basis of the 2 If these cannot be applied because of lack of information about a language, we intend to use the algorithm by Kiss & Strunk (2006). An open-source implementation of parts of the Kiss & Strunk algorithm is available from Patrick Tschorn at http://www.denkselbst.de/sentrick/index.html. 104 Europarl-texts considered (as stored in our database). In order to optimize the results we use the dictionaries of step 1 as set of cognates (cf. Simard at al 1992, Gough & Way 2004), as well as other words easily obtainable from the internet that can be used for this purpose (like company names and other named entities with cross-language identity and terminology translations). Using the morphology component of the new language and the"
W12-0114,2005.mtsummit-papers.11,0,0.0671921,"Missing"
W12-0114,J99-1003,0,0.113762,"Missing"
W12-0114,J05-4003,0,0.0183273,"quire parallel and comparable corpora As our parallel corpus, we use the Europarl. The size of the current version is up to 40 million words per language, and several of the languages we are currently considering are covered. Also, we make use of other parallel corpora such as the Canadian Hansards (Proceedings of the Canadian Parliament) for the English–French language pair. For non-EU Languages (mainly Russian), we intend to conduct a pilot study to establish the feasibility of retrieving parallel corpora from the web, a problem for which various approaches have been proposed (Resnik, 1999; Munteanu & Marcu, 2005; Wu & Fung, 2005). In addition to the parallel corpora, we will need large monolingual corpora in the future (at least 200 million words) for each of the six languages. Here, we intend to use newspaper corpora supplemented with text collections downloadable from the web. The corpora are stored in a database that allows for assigning analyses of different depth and nature to the sentences and for alignment between the sentences and their analyses. The architecture of this database and the corresponding analysis and evaluation frontend is described in (Eberle et al 2010, 2012). Section Results"
W12-0114,P02-1038,0,0.0217034,"Missing"
W12-0114,J03-1002,0,0.00439785,"enerationoriented representations from grammar models and statistical combinatorial properties of annotated features. Step 3: Generating dictionary extensions from parallel corpora Based on parallel corpora, dictionaries can be derived using established techniques of automatic sentence alignment and word alignment. For sentence alignment, the length-based Gale & Church aligner (1993) can be used, or – alternatively – Dan Melamed’s GSA-algorithm (Geometric Sentence Alignment; Melamed, 1999). For segmentation of text we use corresponding Lingenio-tools (unpublished).2 For word alignment Giza++ (Och & Ney, 2003) is the standard tool. Given a word alignment, the extraction of a (SMT) dictionary is relatively straightforward. With the exception of sentence segmentation, these algorithms are largely language independent and can be used for all of the languages that we consider. We did this for a number of language pairs on the basis of the 2 If these cannot be applied because of lack of information about a language, we intend to use the algorithm by Kiss & Strunk (2006). An open-source implementation of parts of the Kiss & Strunk algorithm is available from Patrick Tschorn at http://www.denkselbst.de/se"
W12-0114,P02-1040,0,0.0873178,"Missing"
W12-0114,P95-1050,1,0.218313,"Missing"
W12-0114,rapp-2004-freely,1,0.85454,"Missing"
W12-0114,P99-1068,0,0.0503705,"ps. Step 1: Acquire parallel and comparable corpora As our parallel corpus, we use the Europarl. The size of the current version is up to 40 million words per language, and several of the languages we are currently considering are covered. Also, we make use of other parallel corpora such as the Canadian Hansards (Proceedings of the Canadian Parliament) for the English–French language pair. For non-EU Languages (mainly Russian), we intend to conduct a pilot study to establish the feasibility of retrieving parallel corpora from the web, a problem for which various approaches have been proposed (Resnik, 1999; Munteanu & Marcu, 2005; Wu & Fung, 2005). In addition to the parallel corpora, we will need large monolingual corpora in the future (at least 200 million words) for each of the six languages. Here, we intend to use newspaper corpora supplemented with text collections downloadable from the web. The corpora are stored in a database that allows for assigning analyses of different depth and nature to the sentences and for alignment between the sentences and their analyses. The architecture of this database and the corresponding analysis and evaluation frontend is described in (Eberle et al 2010,"
W12-0114,C90-3044,0,0.0668576,"Missing"
W12-0114,P06-2095,1,0.899134,"f a manually compiled kernel does not show 105 an ambiguity problem of similar significance), and as experience shows that most low frequency words in a full-size lexicon tend to be unambiguous, the ambiguity problem is reduced further for the words investigated and extracted by this comparison method. Step 5: Expanding dictionaries comparable corpora (multiword units) using In order to account for technical terms, idioms, collocations, and typical short phrases, an important feature of an MT lexicon is a high coverage of multiword units. Very recent work conducted at the University of Leeds (Sharoff et al., 2006) shows that dictionary entries for such multiword units can be derived from comparable corpora if a dictionary of single words is available. It could even be shown that this methodology can be superior to deriving multiword-units from parallel corpora (Babych et al., 2007). This is a major breakthrough as comparable corpora are far easier to acquire than parallel corpora. It even opens up the possibility of building domainspecific dictionaries by using texts from different domains. The outline of the algorithm is as follows: • Extract collocations from a corpus of the source language (Smadja,"
W12-0114,sharoff-2006-uniform,1,0.886741,"in a machinetranslated corpus In a later work package of the project, we will run a large parallel corpus through available (competitive) MT engines, which will be enhanced by automatic dictionaries developed during the previous stages. On the source-language side of the corpus we will automatically generate lists of frequent multiword expressions (MWEs) and grammatical constructions using the methodology proposed in (Sharoff et al., 2006). For each of the identified MWEs and constructions we will generate a parallel concordance using open-source CSAR architecture developed by the Leeds team (Sharoff, 2006). The concordance will be generated by running queries to the sentencealigned parallel corpora and will return lists of corresponding sentences from gold-standard human translations and corresponding sentences generated by MT. Each of these concordances will be automatically evaluated using standard MT evaluation metrics, such as BLEU. Under these settings parallel concordances will be used as standard MT evaluation corpora in an automated MT evaluation scenario. Normally BLEU gives reliable results for MT corpora over 7000 words. However, in (Babych and Hartley, 2009; Babych and Hartley, 2008"
W12-0114,J93-1007,0,0.0374134,"., 2006) shows that dictionary entries for such multiword units can be derived from comparable corpora if a dictionary of single words is available. It could even be shown that this methodology can be superior to deriving multiword-units from parallel corpora (Babych et al., 2007). This is a major breakthrough as comparable corpora are far easier to acquire than parallel corpora. It even opens up the possibility of building domainspecific dictionaries by using texts from different domains. The outline of the algorithm is as follows: • Extract collocations from a corpus of the source language (Smadja, 1993) • To translate a collocation, look up all its words using any dictionary • Generate all possible permutations (sequences) of the word translations • Count the occurrence frequencies of these sequences in a corpus of the target language and test for significance • Consider the most significant sequence to be the translation of the source language collocation Of course, in later steps of the project, we will experiment on filtering these sequences by exploiting structural knowledge similarly to what was described in the two previous steps. This can be obtained on the basis of the declarative an"
W12-0114,2007.mtsummit-aptme.6,0,0.11913,"Missing"
W12-0114,I05-1023,0,\N,Missing
W12-0114,P99-1067,1,\N,Missing
W12-0114,W02-0902,0,\N,Missing
W12-0114,baroni-bernardini-2004-bootcat,0,\N,Missing
W17-0807,C08-1113,0,0.0380051,"ot an issue” Table 3: Our decision tree for classifying a given issue: we do not produce questions for distinguishing X1/X2/X3, and X8/X10/X11/X12/X13, considering that their definitions are clear enough. fer) and LA (language) issues, described in §2. The priority of the former over the latter, implicitly assumed in the MeLLANGE typology, is also largely preserved; the sole exceptions are X7 (incorrect translations of terms) and X4b (too literal). Table 2 also shows that our typology includes the following three issue types that are not covered by MQM. of partial/incomplete annotation, e.g., Tsuboi et al. (2008), our procedure is nevertheless different from these in the sense that we leave issues “unannotated” only when identical ones are already annotated. 5 Intrinsic Evaluation of the Scheme It is hard to make a fair and unbiased comparison between different annotation schemes that target the same phenomena, employing the same assessors. We thus evaluated whether our issue classification scheme leads to sufficiently high level of inter-assessor agreement, regarding those poor results described in §2 as baselines, and analyzed the tendencies of disagreements and the distribution of issues. • X6 (ind"
W17-0807,N06-2015,0,0.376172,"nglish and Japanese. Neither have their applicability to translations produced by less advanced learners, such as undergraduate students, been fully examined. Aiming at (i) a consistent human assessment, (ii) of English-to-Japanese translations, (iii) produced by learner translators, we manually constructed a scheme for classifying identified issues. We first collected English-to-Japanese translations from learners in order to assure and validate the applicability of our scheme (§3). We then manually created an issue typology and a decision tree through an application of the OntoNotes method (Hovy et al., 2006), i.e., an iteration of assessing learners’ translations and updating the typology and decision tree (§4). We adopted an existing typology, that of MNH-TT (Babych et al., 2012), as the starting point, because its origin (Castagnoli et al., 2006) was tailored to assessing university student learners’ translations and its applicability across several European languages had been demonstrated. We evaluated our scheme with inter-assessor agreement, employing four assessors and an undergraduate learner translator (§5). 4 http://www.atanet.org/certification/ aboutexams_error.php 5 SAE J2450, the stan"
W17-0807,P02-1040,0,0.114813,"arity of issues depend Introduction Assessing and assuring translation quality is one of the main concerns for translation services, machine translation (MT) industries, and translation teaching institutions.1 The assessment process for a given pair of source document (SD) and its translation, i.e., target document (TD), consists of two tasks. The first task is to identify erroneous text spans in the TD. In professional settings, when assessors consider a text span in a TD as erroneous, 2 While automated metrics for MT quality evaluation are often presented as objective, many, including BLEU (Papineni et al., 2002), rely on comparison with a one or more human reference translations whose quality and subjectivity are merely assumed and not independently validated. 3 http://www.qt21.eu/launchpad/content/ multidimensional-quality-metrics 1 These include both private companies and translationrelated departments in colleges and universities. 57 Proceedings of the 11th Linguistic Annotation Workshop, pages 57–66, c Valencia, Spain, April 3, 2017. 2017 Association for Computational Linguistics on the purpose of translations and the aim of human assessments (e.g., formative or summative). However, the typology"
W94-0308,W93-0203,1,0.701901,"Missing"
W94-0308,P92-1016,0,0.0503603,"Missing"
Y10-1089,baroni-bernardini-2004-bootcat,0,0.0396684,"ompetence can be achieved only once an adequate discipline-specific vocabulary has been acquired. Thus, a prime application of such corpora for both language learning and translation is the extraction of terminology – single- and multiword terms – to create vocabulary lists of frequent words and collocations. Single-word terms are 773 774 Workshop on Advanced Corpus Solutions Figure 3: Comparison of frequencies for 情报 and 信息. detected by loglikelihood scores for their frequencies against a reference corpus, while for multiword terms we use an adaptation of the commonly-used Bootcat algorithm (Baroni and Bernardini, 2004). The corpora and derived word lists have provided a basis for teaching business Russian to British students. 4.4 Affix-based searches Initially the affix function was developed with learners of Russian in mind and was intended for verbal prefixes alone. However, it has a much broader application in teaching and research and is relevant to other languages, including English, German, Chinese and Japanese. The function now works also for suffix-based searches and is applicable to parts of speech other than verbs. Users may either enter a word and search for the prefixes or suffixes that are used"
Y10-1089,W08-0909,0,0.0139766,"entic texts of various genres and therefore no attention has gone into selecting texts of an “appropriate” level. Consequently, students at the beginner and (lower-)intermediate level have been highly restricted in what they can use corpora for. Prior research in grading texts by their difficulty relied on assessing it from the viewpoint of native speakers of English, normally in the context of US schools or the US army (DuBay, 2004). In recent years there have been attempts to address the needs of learners of English as a foreign language, e.g., (Kotani et al., 2008; Kilgarriff et al., 2008; Heilman et al., 2008), but some of these studies relied on the availability of syntactic parsers or WordNet, and none of them addressed the needs of learners of other languages. In prior research (Sharoff et al., 2008) we established parameters for assessing the difficulty of texts and individual sentences in several languages (English, Chinese, German and Russian) by comparing the parameters associated with texts judged to be more or less difficult by language tutors for these languages. These parameters were detected by using a Principal Component Analysis (PCA) transform of a large set of features, which were e"
Y10-1089,sharoff-2006-uniform,1,0.911297,"in detail how we have implemented the following functions: • • • • • searches using metadata and statistics for metadata; automatic genre identification; advanced definition of shallow patterns; operations with frequency lists, including affix-based searches; classification of concordance lines according to their level of difficulty and appropriateness for language learners. We have tested the tools with a range of corpora and languages, including representative webderived corpora for Arabic, Chinese, English, French, German, Greek, Italian, Japanese, Polish, Portuguese, Russian and Spanish (Sharoff, 2006a), as well more specific collections, such as newswire or business corpora. In this paper, we present examples from Chinese, Japanese and Russian. 2 Outline of the project IntelliText was conceived in order to allow humanities researchers, including those with little or no experience of working with electronic corpora, to make use of advanced methods of text collection and analysis. Rather than producing a new product we are developing our existing tools by enhancing the range of their functions and their usability. We are doing so by liaising with humanities researchers who represent several"
Y10-1089,sharoff-etal-2010-web,1,0.751656,"traditional corpora, such as the BNC, contain fairly extensive annotation of their texts according to domains, audience types and genres (Lee, 2001). This information is normally not available for corpora collected from the Web. Even in traditional corpora, important genre or register distinctions may not be made at all or may be made in incompatible ways, rendering it impossible to show, for example, the difference between expressions of requests or suggestions in English and Japanese (e.g., -ましょ、-ませんか) in a given register. We rely here on our current work on automatic genre classification (Sharoff et al., 2010), which can achieve reasonable accuracy provided that we have a manually annotated, topically diverse training sample of approximately 20-30 documents per genre or stylistic class on which to train the probabilistic classifier. The training documents within each genre need to cover a variety of topics. If this is not the case, even if a probabilistic classifier can achieve high accuracy in genre identification on the training set, it is more likely to be able discriminate between topics rather than genres in the rest of the corpus. This facility has been incorporated to display elaborate styli"
