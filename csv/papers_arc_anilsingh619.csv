2020.wnut-1.60,{NLPRL} at {WNUT}-2020 Task 2: {ELM}o-based System for Identification of {COVID}-19 Tweets,2020,-1,-1,4,0,13737,rajesh mundotiya,Proceedings of the Sixth Workshop on Noisy User-generated Text (W-NUT 2020),0,"The Coronavirus pandemic has been a dominating news on social media for the last many months. Efforts are being made to reduce its spread and reduce the casualties as well as new infections. For this purpose, the information about the infected people and their related symptoms, as available on social media, such as Twitter, can help in prevention and taking precautions. This is an example of using noisy text processing for disaster management. This paper discusses the NLPRL results in Shared Task-2 of WNUT-2020 workshop. We have considered this problem as a binary classification problem and have used a pre-trained ELMo embedding with GRU units. This approach helps classify the tweets with accuracy as 80.85{\%} and 78.54{\%} as F1-score on the provided test dataset. The experimental code is available online."
2020.wmt-1.44,Transformer-based Neural Machine Translation System for {H}indi {--} {M}arathi: {WMT}20 Shared Task,2020,-1,-1,4,1,7177,amit kumar,Proceedings of the Fifth Conference on Machine Translation,0,"This paper reports the results for the Machine Translation (MT) system submitted by the NLPRL team for the Hindi {--} Marathi Similar Translation Task at WMT 2020. We apply the Transformer-based Neural Machine Translation (NMT) approach on both translation directions for this language pair. The trained model is evaluated on the corpus provided by shared task organizers, using BLEU, RIBES, and TER scores. There were a total of 23 systems submitted for Marathi to Hindi and 21 systems submitted for Hindi to Marathi in the shared task. Out of these, our submission ranked 6th and 9th, respectively."
2020.wmt-1.126,{NLPRL} System for Very Low Resource Supervised Machine Translation,2020,-1,-1,4,0,13738,rupjyoti baruah,Proceedings of the Fifth Conference on Machine Translation,0,"This paper describes the results of the system that we used for the WMT20 very low resource (VLR) supervised MT shared task. For our experiments, we use a byte-level version of BPE, which requires a base vocabulary of size 256 only. BPE based models are a kind of sub-word models. Such models try to address the Out of Vocabulary (OOV) word problem by performing word segmentation so that segments correspond to morphological units. They are also reported to work across different languages, especially similar languages due to their sub-word nature. Based on BLEU cased score, our NLPRL systems ranked ninth for HSB to GER and tenth in GER to HSB translation scenario."
2020.paclic-1.54,Attention-based Domain Adaption Using Transfer Learning for Part-of-Speech Tagging: An Experiment on the {H}indi language,2020,-1,-1,4,0,13737,rajesh mundotiya,"Proceedings of the 34th Pacific Asia Conference on Language, Information and Computation",0,None
2020.loresmt-1.6,Unsupervised Approach for Zero-Shot Experiments: {B}hojpuri{--}{H}indi and {M}agahi{--}{H}indi@{L}o{R}es{MT} 2020,2020,-1,-1,3,1,7177,amit kumar,Proceedings of the 3rd Workshop on Technologies for MT of Low Resource Languages,0,"This paper reports a Machine Translation (MT) system submitted by the NLPRL team for the Bhojpuri{--}Hindi and Magahi{--}Hindi language pairs at LoResMT 2020 shared task. We used an unsupervised domain adaptation approach that gives promising results for zero or extremely low resource languages. Task organizers provide the development and the test sets for evaluation and the monolingual data for training. Our approach is a hybrid approach of domain adaptation and back-translation. Metrics used to evaluate the trained model are BLEU, RIBES, Precision, Recall and F-measure. Our approach gives relatively promising results, with a wide range, of 19.5, 13.71, 2.54, and 3.16 BLEU points for Bhojpuri to Hindi, Magahi to Hindi, Hindi to Bhojpuri and Hindi to Magahi language pairs, respectively."
2020.icon-main.31,Parsing {I}ndian {E}nglish News Headlines,2020,-1,-1,3,0,19142,samapika roy,Proceedings of the 17th International Conference on Natural Language Processing (ICON),0,"Parsing news Headlines is one of the difficult tasks of Natural Language Processing. It is mostly because news Headlines (NHs) are not complete grammatical sentences. News editors use all sorts of tricks to grab readers{'} attention, for instance, unusual capitalization as in the headline{'} Ear SHOT ashok rajagopalan{'}; some are world knowledge demanding like {`}Church reformation celebrated{'} where the {`}Church reformation{'} refers to a historical event and not a piece of news about an ordinary church. The lack of transparency in NHs can be linguistic, cultural, social, or contextual. The lack of space provided for a news headline has led to creative liberty. Though many works like news value extraction, summary generation, emotion classification of NHs have been going on, parsing them had been a tough challenge. Linguists have also been interested in NHs for creativity in the language used by bending traditional grammar rules. Researchers have conducted studies on news reportage, discourse analysis of NHs, and many more. While the creativity seen in NHs is fascinating for language researchers, it poses a computational challenge for Natural Language Processing researchers. This paper presents an outline of the ongoing doctoral research on the parsing of Indian English NHs. The ultimate aim of this research is to provide a module that will generate correctly parsed NHs. The intention is to enhance the broad applicability of newspaper corpus for future Natural Language Processing applications."
2020.aacl-srw.24,Generating Inflectional Errors for Grammatical Error Correction in {H}indi,2020,-1,-1,4,0,23212,ankur sonawane,Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing: Student Research Workshop,0,"Automated grammatical error correction has been explored as an important research problem within NLP, with the majority of the work being done on English and similar resource-rich languages. Grammar correction using neural networks is a data-heavy task, with the recent state of the art models requiring datasets with millions of annotated sentences for proper training. It is difficult to find such resources for Indic languages due to their relative lack of digitized content and complex morphology, compared to English. We address this problem by generating a large corpus of artificial inflectional errors for training GEC models. Moreover, to evaluate the performance of models trained on this dataset, we create a corpus of real Hindi errors extracted from Wikipedia edits. Analyzing this dataset with a modified version of the ERRANT error annotation toolkit, we find that inflectional errors are very common in this language. Finally, we produce the initial baseline results using state of the art methods developed for English."
W19-2006,{SWOW}-8500: Word Association task for Intrinsic Evaluation of Word Embeddings,2019,-1,-1,3,0,3392,avijit thawani,Proceedings of the 3rd Workshop on Evaluating Vector Space Representations for {NLP},0,"Downstream evaluation of pretrained word embeddings is expensive, more so for tasks where current state of the art models are very large architectures. Intrinsic evaluation using word similarity or analogy datasets, on the other hand, suffers from several disadvantages. We propose a novel intrinsic evaluation task employing large word association datasets (particularly the Small World of Words dataset). We observe correlations not just between performances on SWOW-8500 and previously proposed intrinsic tasks of word similarity prediction, but also with downstream tasks (eg. Text Classification and Natural Language Inference). Most importantly, we report better confidence intervals for scores on our word association task, with no fall in correlation with downstream performance."
D19-5222,{NLPRL} at {WAT}2019: Transformer-based {T}amil {--} {E}nglish Indic Task Neural Machine Translation System,2019,0,0,2,1,7177,amit kumar,Proceedings of the 6th Workshop on Asian Translation,0,This paper describes the Machine Translation system for Tamil-English Indic Task organized at WAT 2019. We use Transformer- based architecture for Neural Machine Translation.
W18-6116,Language Identification in Code-Mixed Data using Multichannel Neural Networks and Context Capture,2018,0,0,2,0,27812,soumil mandal,Proceedings of the 2018 {EMNLP} Workshop W-{NUT}: The 4th Workshop on Noisy User-generated Text,0,"An accurate language identification tool is an absolute necessity for building complex NLP systems to be used on code-mixed data. Lot of work has been recently done on the same, but there{'}s still room for improvement. Inspired from the recent advancements in neural network architectures for computer vision tasks, we have implemented multichannel neural networks combining CNN and LSTM for word level language identification of code-mixed data. Combining this with a Bi-LSTM-CRF context capture module, accuracies of 93.28{\%} and 93.32{\%} is achieved on our two testing sets."
W18-3921,{IIT} ({BHU}) System for {I}ndo-{A}ryan Language Identification ({ILI}) at {V}ar{D}ial 2018,2018,0,2,4,0,28225,divyanshu gupta,"Proceedings of the Fifth Workshop on {NLP} for Similar Languages, Varieties and Dialects ({V}ar{D}ial 2018)",0,"Text language Identification is a Natural Language Processing task of identifying and recognizing a given language out of many different languages from a piece of text. This paper describes our submission to the ILI 2018 shared-task, which includes the identification of 5 closely related Indo-Aryan languages. We developed a word-level LSTM(Long Short-term Memory) model, a specific type of Recurrent Neural Network model, for this task. Given a sentence, our model embeds each word of the sentence and convert into its trainable word embedding, feeds them into our LSTM network and finally predict the language. We obtained an F1 macro score of 0.836, ranking 5th in the task."
W18-3220,{IIT} ({BHU}) Submission for the {ACL} Shared Task on Named Entity Recognition on Code-switched Data,2018,0,4,3,0,28354,shashwat trivedi,Proceedings of the Third Workshop on Computational Approaches to Linguistic Code-Switching,0,"This paper describes the best performing system for the shared task on Named Entity Recognition (NER) on code-switched data for the language pair Spanish-English (ENG-SPA). We introduce a gated neural architecture for the NER task. Our final model achieves an F1 score of 63.76{\%}, outperforming the baseline by 10{\%}."
W18-0914,Di-{LSTM} Contrast : A Deep Neural Network for Metaphor Detection,2018,0,3,2,0,26156,krishnkant swarnkar,Proceedings of the Workshop on Figurative Language Processing,0,"The contrast between the contextual and general meaning of a word serves as an important clue for detecting its metaphoricity. In this paper, we present a deep neural architecture for metaphor detection which exploits this contrast. Additionally, we also use cost-sensitive learning by re-weighting examples, and baseline features like concreteness ratings, POS and WordNet-based features. The best performing system of ours achieves an overall F1 score of 0.570 on All POS category and 0.605 on the Verbs category at the Metaphor Shared Task 2018."
S18-1104,{NLPRL}-{IITBHU} at {S}em{E}val-2018 Task 3: Combining Linguistic Features and Emoji pre-trained {CNN} for Irony Detection in Tweets,2018,0,4,3,0,28355,harsh rangwani,Proceedings of The 12th International Workshop on Semantic Evaluation,0,"This paper describes our participation in SemEval 2018 Task 3 on Irony Detection in Tweets. We combine linguistic features with pre-trained activations of a neural network. The CNN is trained on the emoji prediction task. We combine the two feature sets and feed them into an XGBoost Classifier for classification. Subtask-A involves classification of tweets into ironic and non-ironic instances whereas Subtask-B involves classification of the tweet into - non-ironic, verbal irony, situational irony or other verbal irony. It is observed that combining features from these two different feature spaces improves our system results. We leverage the SMOTE algorithm to handle the problem of class imbalance in Subtask-B. Our final model achieves an F1-score of 0.65 and 0.47 on Subtask-A and Subtask-B respectively. Our system ranks 4th on both tasks respectively, outperforming the baseline by 6{\%} on Subtask-A and 14{\%} on Subtask-B."
K18-3005,Experiments on Morphological Reinflection: {C}o{NLL}-2018 Shared Task,2018,0,1,2,0,26377,rishabh jain,Proceedings of the {C}o{NLL}{--}{SIGMORPHON} 2018 Shared Task: Universal Morphological Reinflection,0,None
C18-1247,How emotional are you? Neural Architectures for Emotion Intensity Prediction in Microblogs,2018,0,0,3,0,9790,devang kulshreshtha,Proceedings of the 27th International Conference on Computational Linguistics,0,"Social media based micro-blogging sites like Twitter have become a common source of real-time information (impacting organizations and their strategies, and are used for expressing emotions and opinions. Automated analysis of such content therefore rises in importance. To this end, we explore the viability of using deep neural networks on the specific task of emotion intensity prediction in tweets. We propose a neural architecture combining convolutional and fully connected layers in a non-sequential manner - done for the first time in context of natural language based tasks. Combined with lexicon-based features along with transfer learning, our model achieves state-of-the-art performance, outperforming the previous system by 0.044 or 4.4{\%} Pearson correlation on the WASSA{'}17 EmoInt shared task dataset. We investigate the performance of deep multi-task learning models trained for all emotions at once in a unified architecture and get encouraging results. Experiments performed on evaluating correlation between emotion pairs offer interesting insights into the relationship between them."
W17-7504,Reference Scope Identification for Citances Using Convolutional Neural Networks,2017,0,1,4,0,31181,saurav jha,Proceedings of the 14th International Conference on Natural Language Processing ({ICON}-2017),0,None
W17-7559,Neural Morphological Disambiguation Using Surface and Contextual Morphological Awareness,2017,0,1,2,0,26959,akhilesh sudhakar,Proceedings of the 14th International Conference on Natural Language Processing ({ICON}-2017),0,None
W17-4007,Word Transduction for Addressing the {OOV} Problem in Machine Translation for Similar Resource-Scarce Languages,2017,12,1,2,0,31733,shashikant sharma,Proceedings of the 13th International Conference on Finite State Methods and Natural Language Processing ({FSMNLP} 2017),0,None
W17-0912,{IIT} ({BHU}): System Description for {LSDS}em{'}17 Shared Task,2017,0,1,2,0.833333,4535,pranav goel,"Proceedings of the 2nd Workshop on Linking Models of Lexical, Sentential and Discourse-level Semantics",0,This paper describes an ensemble system submitted as part of the LSDSem Shared Task 2017 - the Story Cloze Test. The main conclusion from our results is that an approach based on semantic similarity alone may not be enough for this task. We test various approaches and compare them with two ensemble systems. One is based on voting and the other on logistic regression based classifier. Our final system is able to outperform the previous state of the art for the Story Cloze test. Another very interesting observation is the performance of sentiment based approach which works almost as well on its own as our final ensemble system.
K17-2007,Experiments on Morphological Reinflection: {C}o{NLL}-2017 Shared Task,2017,4,0,2,0,26959,akhilesh sudhakar,Proceedings of the {C}o{NLL} {SIGMORPHON} 2017 Shared Task: Universal Morphological Reinflection,0,None
I17-4003,{IJCNLP}-2017 Task 3: Review Opinion Diversification ({R}ev{O}pi{D}-2017),2017,0,0,1,1,13740,anil singh,"Proceedings of the {IJCNLP} 2017, Shared Tasks",0,"Unlike Entity Disambiguation in web search results, Opinion Disambiguation is a relatively unexplored topic. RevOpiD shared task at IJCNLP-2107 aimed to attract attention towards this research problem. In this paper, we summarize the first run of this task and introduce a new dataset that we have annotated for the purpose of evaluating Opinion Mining, Summarization and Disambiguation methods."
K16-2015,{IIT} ({BHU}) Submission on the {C}o{NLL}-2016 Shared Task: Shallow Discourse Parsing using Semantic Lexicons,2016,0,1,3,0,35469,manpreet kaur,Proceedings of the {C}o{NLL}-16 shared task,0,None
K15-2009,Shallow Discourse Parsing with Syntactic and (a Few) Semantic Features,2015,8,0,4,0,37715,shubham mukherjee,Proceedings of the Nineteenth Conference on Computational Natural Language Learning - Shared Task,0,"Discourse parsing is a challenging task and is crucial for discourse analysis. In this paper, we focus on labelling argument spans of discourse connectives and sense identification in the CoNLL-2015 shared task setting. We have used syntactic features and have also tried a few semantic features. We employ a pipeline of classifiers, where the best features and parameters were selected for each individual classifier, based on experimental evaluation. We could only get results somewhat better than of the baseline on the overall task, but the results over some of the sub-tasks are encouraging. Our initial efforts at using semantic features do not seem to help."
W14-5208,{SSF}: A Common Representation Scheme for Language Analysis for Language Technology Infrastructure Development,2014,18,6,4,0,36415,akshar bharati,Proceedings of the Workshop on Open Infrastructures and Analysis Frameworks for {HLT},0,"We describe a representation scheme and an analysis engine using that scheme, both of which have been used to develop infrastructure for HLT. The Shakti Standard Format is a readable and robust representation scheme for analysis frameworks and other purposes. The representation is highly extensible. This representation scheme, based on the blackboard architectural model, allows a very wide variety of linguistic and non-linguistic information to be stored in one place and operated upon by any number of processing modules. We show how it has been successfully used for building machine translation systems for several language pairs using the same architecture. It has also been used for creation of language resources such as treebanks and for different kinds of annotation interfaces. There is even a query language designed for this representation. Easily wrappable into XML, it can be used equally well for distributed computing."
W13-2250,{LIMSI} Submission for the {WMT}{`}13 Quality Estimation Task: an Experiment with N-Gram Posteriors,2013,18,2,1,1,13740,anil singh,Proceedings of the Eighth Workshop on Statistical Machine Translation,0,This paper describes the machine learning algorithm and the features used by LIMSI for the Quality Estimation Shared Task. Our submission mainly aims at evaluating the usefulness for quality estimation of ngram posterior probabilities that quantify the probability for a given n-gram to be part of the system output.
F13-2028,A corpus of post-edited translations (Un corpus d{'}erreurs de traduction) [in {F}rench],2013,-1,-1,2,0,168,guillaume wisniewski,Proceedings of TALN 2013 (Volume 2: Short Papers),0,None
2013.mtsummit-papers.15,"Design and Analysis of a Large Corpus of Post-Edited Translations: Quality Estimation, Failure Analysis and the Variability of Post-Edition",2013,-1,-1,2,0,168,guillaume wisniewski,Proceedings of Machine Translation Summit XIV: Papers,0,None
agarwal-etal-2012-gui,A {GUI} to Detect and Correct Errors in {H}indi Dependency Treebank,2012,14,4,3,0,43181,rahul agarwal,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"A treebank is an important resource for developing many NLP based tools. Errors in the treebank may lead to error in the tools that use it. It is essential to ensure the quality of a treebank before it can be deployed for other purposes. Automatic (or semi-automatic) detection of errors in the treebank can reduce the manual work required to find and remove errors. Usually, the errors found automatically are manually corrected by the annotators. There is not much work reported so far on error correction tools which helps the annotators in correcting errors efficiently. In this paper, we present such an error correction tool that is an extension of the error detection method described earlier (Ambati et al., 2010; Ambati et al., 2011; Agarwal et al., 2012)."
singh-2012-concise,A Concise Query Language with Search and Transform Operations for Corpora with Multiple Levels of Annotation,2012,18,4,1,1,13740,anil singh,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"The usefulness of annotated corpora is greatly increased if there is an associated tool that can allow various kinds of operations to be performed in a simple way. Different kinds of annotation frameworks and many query languages for them have been proposed, including some to deal with multiple layers of annotation. We present here an easy to learn query language for a particular kind of annotation framework based on Âthreaded trees', which are somewhere between the complete order of a tree and the anarchy of a graph. Through 'typed' threads, they can allow multiple levels of annotation in the same document. Our language has a simple, intuitive and concise syntax and high expressive power. It allows not only to search for complicated patterns with short queries but also allows data manipulation and specification of arbitrary return values. Many of the commonly used tasks that otherwise require writing programs, can be performed with one or more queries. We compare the language with some others and try to evaluate it."
singh-ambati-2010-integrated,An Integrated Digital Tool for Accessing Language Resources,2010,8,5,1,1,13740,anil singh,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"Language resources can be classified under several categories. To be able to query and operate on all (or most of) these categories using a single digital tool would be very helpful for a large number of researchers working on languages. We describe such a tool in this paper. It is different from other such tools in that it allows querying and transformation on different kinds of resources (such as corpora, lexicon and language models) with the same framework. Search options can be given based on the kind of resource being queried. It is possible to select a matched resource and open it for editing in the specialized interfaces with which that resource is associated. The tool also allows the extracted or modified data to be saved separately, apart from having the usual facilities like displaying the results in KeyWord-In-Context (KWIC) format. We also present the notation used for querying and transformation, which is comparable to but different from the Corpus Query Language (CQL)."
kolachina-etal-2010-grammar,Grammar Extraction from Treebanks for {H}indi and {T}elugu,2010,13,2,3,0,21982,prasanth kolachina,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"Grammars play an important role in many Natural Language Processing (NLP) applications. The traditional approach to creating grammars manually, besides being labor-intensive, has several limitations. With the availability of large scale syntactically annotated treebanks, it is now possible to automatically extract an approximate grammar of a language in any of the existing formalisms from a corresponding treebank. In this paper, we present a basic approach to extract grammars from dependency treebanks of two Indian languages, Hindi and Telugu. The process of grammar extraction requires a generalization mechanism. Towards this end, we explore an approach which relies on generalization of argument structure over the verbs based on their syntactic similarity. Such a generalization counters the effect of data sparseness in the treebanks. A grammar extracted using this system can not only expand already existing knowledge bases for NLP tasks such as parsing, but also aid in the creation of grammars for languages where none exist. Further, we show that the grammar extraction process can help in identifying annotation errors and thus aid in the task of the treebank validation."
R09-1064,From Bag of Languages to Family Trees From Noisy Corpus,2009,15,6,2,0,21002,taraka rama,Proceedings of the International Conference {RANLP}-2009,0,"In this paper, we use corpus-based measures for constructing phylogenetic trees and try to address some questions about the validity of doing this and applicability to linguistic areas as against language families. We experiment with four corpus based distance measures for constructing phylogenetic trees. Three of these measures were earlier tried for estimating language distances. We use a fourth measure based on phonetic and orthographic feature n-grams. We compare the trees obtained using these measures and present our observations."
N09-3016,Modeling Letter-to-Phoneme Conversion as a Phrase Based Statistical Machine Translation Problem with {M}inimum {E}rror {R}ate Training,2009,25,26,2,0,21002,taraka rama,"Proceedings of Human Language Technologies: The 2009 Annual Conference of the North {A}merican Chapter of the Association for Computational Linguistics, Companion Volume: Student Research Workshop and Doctoral Consortium",0,Letter-to-phoneme conversion plays an important role in several applications. It can be a difficult task because the mapping from letters to phonemes can be many-to-many. We present a language independent letter-to-phoneme conversion approach which is based on the popular phrase based Statistical Machine Translation techniques. The results of our experiments clearly demonstrate that such techniques can be used effectively for letter-to-phoneme conversion. Our results show an overall improvement of 5.8% over the baseline and are comparable to the state of the art. We also propose a measure to estimate the difficulty level of L2P task for a language.
singh-etal-2008-estimating,Estimating the Resource Adaption Cost from a Resource Rich Language to a Similar Resource Poor Language,2008,8,1,1,1,13740,anil singh,Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08),0,"Developing resources which can be used for Natural Language Processing is an extremely difficult task for any language, but is even more so for less privileged (or less computerized) languages. One way to overcome this difficulty is to adapt the resources of a linguistically close resource rich language. In this paper we discuss how the cost of such adaption can be estimated using subjective and objective measures of linguistic similarity for allocating financial resources, time, manpower etc. Since this is the first work of its kind, the method described in this paper should be seen as only a preliminary method, indicative of how better methods can be developed. Corpora of several less computerized languages had to be collected for the work described in the paper, which was difficult because for many of these varieties there is not much electronic data available. Even if it is, it is in non-standard encodings, which means that we had to build encoding converters for these varieties. The varieties we have focused on are some of the varieties spoken in the South Asian region."
I08-5003,Named Entity Recognition for South and South {E}ast {A}sian Languages: Taking Stock,2008,30,23,1,1,13740,anil singh,Proceedings of the {IJCNLP}-08 Workshop on Named Entity Recognition for South and South East {A}sian Languages,0,"In this paper we first present a brief discussion of the problem of Named Entity Recognition (NER) in the context of the IJCNLP workshop on NER for South and South East Asian (SSEA) languages 1 . We also present a short report on the development of a named entity annotated corpus in five South Asian language, namely Hindi, Bengali, Telugu, Oriya and Urdu. We present some details about a new named entity tagset used for this corpus and describe the annotation guidelines. Since the corpus was used for a shared task, we also explain the evaluation measures used for the task. We then present the results of our experiments on a baseline which uses a maximum entropy based approach. Finally, we give an overview of the papers to be presented at the workshop, including those from the shared task track. We discuss the results obtained by teams participating in the task and compare their results with the baseline results."
I08-3004,Natural Language Processing for Less Privileged Languages: Where do we come from? Where are we going?,2008,17,3,1,1,13740,anil singh,Proceedings of the {IJCNLP}-08 Workshop on {NLP} for Less Privileged Languages,0,"In the context of the IJCNLP workshop on Natural Language Processing (NLP) for Less Privileged Languages, we discuss the obstacles to research on such languages. We also briefly discuss the ways to make progress in removing these obstacles. We mention some previous work and comment on the papers selected for the workshop."
I08-2141,A Mechanism to Provide Language-Encoding Support and an {NLP} Friendly Editor,2008,3,6,1,1,13740,anil singh,Proceedings of the Third International Joint Conference on Natural Language Processing: Volume-{II},0,"Many languages of the world (some with very large numbers of native speakers) are not yet supported on computers. In this paper we first present a simple method to provide an extra layer of easily customizable language-encoding support for less computerized languages. We then describe an editor called Sanchay Editor, which uses this method and also has many other facilities useful for those using less computerized languages for simple text editing or for Natural Language Processing purposes, especially for annotation."
I08-1009,A More Discerning and Adaptable Multilingual Transliteration Mechanism for {I}ndian Languages,2008,14,27,2,0,48028,harshit surana,Proceedings of the Third International Joint Conference on Natural Language Processing: Volume-{I},0,"Transliteration is the process of transcribing words from a source script to a target script. These words can be content words or proper nouns. They may be of local or foreign origin. In this paper we present a more discerning method which applies different techniques based on the word origin. The techniques used also take into account the properties of the scripts. Our approach does not require training data on the target side, while it uses more sophisticated techniques on the source side. Fuzzy string matching is used to compensate for lack of training on the target side. We have evaluated on two Indian languages and have achieved substantially better results (increase of up to 0.44 in MRR) than the baseline and comparable to the state of the art. Our experiments clearly show that word origin is an important factor in achieving higher accuracy in transliteration."
W07-1306,Can Corpus Based Measures be Used for Comparative Study of Languages?,2007,11,18,1,1,13740,anil singh,Proceedings of Ninth Meeting of the {ACL} Special Interest Group in Computational Morphology and Phonology,0,"Quantitative measurement of inter-language distance is a useful technique for studying diachronic and synchronic relations between languages. Such measures have been used successfully for purposes like deriving language taxonomies and language reconstruction, but they have mostly been applied to handcrafted word lists. Can we instead use corpus based measures for comparative study of languages? In this paper we try to answer this question. We use three corpus based measures and present the results obtained from them and show how these results relate to linguistic and historical knowledge. We argue that the answer is yes and that such studies can provide or validate linguistic and computational insights."
W06-1109,Study of Some Distance Measures for Language and Encoding Identification,2006,10,20,1,1,13740,anil singh,Proceedings of the Workshop on Linguistic Distances,0,"To determine how close two language models (e.g., n-grams models) are, we can use several distance measures. If we can represent the models as distributions, then the similarity is basically the similarity of distributions. And a number of measures are based on information theoretic approach. In this paper we present some experiments on using such similarity measures for an old Natural Language Processing (NLP) problem. One of the measures considered is perhaps a novel one, which we have called mutual cross entropy. Other measures are either well known or based on well known measures, but the results obtained with them vis-avis one-another might help in gaining an insight into how similarity measures work in practice.n n The first step in processing a text is to identify the language and encoding of its contents. This is a practical problem since for many languages, there are no universally followed text encoding standards. The method we have used in this paper for language and encoding identification uses pruned character n-grams, alone as well augmented with word n-grams. This method seems to give results comparable to other methods."
W05-0816,"Comparison, Selection and Use of Sentence Alignment Algorithms for New Language Pairs",2005,15,19,1,1,13740,anil singh,Proceedings of the {ACL} Workshop on Building and Using Parallel Texts,0,"Several algorithms are available for sentence alignment, but there is a lack of systematic evaluation and comparison of these algorithms under different conditions. In most cases, the factors which can significantly affect the performance of a sentence alignment algorithm have not been considered while evaluating. We have used a method for evaluation that can give a better estimate about a sentence alignment algorithm's performance, so that the best one can be selected. We have compared four approaches using this method. These have mostly been tried on European language pairs. We have evaluated manually-checked and validated English-Hindi aligned parallel corpora under different conditions. We also suggest some guidelines on actual alignment."
