2021.iwslt-1.20,{ON}-{TRAC}{'} systems for the {IWSLT} 2021 low-resource speech translation and multilingual speech translation shared tasks,2021,-1,-1,9,0,5778,hang le,Proceedings of the 18th International Conference on Spoken Language Translation (IWSLT 2021),0,"This paper describes the ON-TRAC Consortium translation systems developed for two challenge tracks featured in the Evaluation Campaign of IWSLT 2021, low-resource speech translation and multilingual speech translation. The ON-TRAC Consortium is composed of researchers from three French academic laboratories and an industrial partner: LIA (Avignon Universit{\'e}), LIG (Universit{\'e} Grenoble Alpes), LIUM (Le Mans Universit{\'e}), and researchers from Airbus. A pipeline approach was explored for the low-resource speech translation task, using a hybrid HMM/TDNN automatic speech recognition system fed by wav2vec features, coupled to an NMT system. For the multilingual speech translation task, we investigated the us of a dual-decoder Transformer that jointly transcribes and translates an input speech. This model was trained in order to translate from multiple source languages to multiple target ones."
2020.lrec-1.197,{A}llo{S}at: A New Call Center {F}rench Corpus for Satisfaction and Frustration Analysis,2020,0,0,3,0,17018,manon macary,Proceedings of the 12th Language Resources and Evaluation Conference,0,"We present a new corpus, named AlloSat, composed of real-life call center conversations in French that is continuously annotated in frustration and satisfaction. This corpus has been set up to develop new systems able to model the continuous aspect of semantic and paralinguistic information at the conversation level. The present work focuses on the paralinguistic level, more precisely on the expression of emotions. In the call center industry, the conversation usually aims at solving the caller{'}s request. As far as we know, most emotional databases contain static annotations in discrete categories or in dimensions such as activation or valence. We hypothesize that these dimensions are not task-related enough. Moreover, static annotations do not enable to explore the temporal evolution of emotional states. To solve this issue, we propose a corpus with a rich annotation scheme enabling a real-time investigation of the axis frustration / satisfaction. AlloSat regroups 303 conversations with a total of approximately 37 hours of audio, all recorded in real-life environments collected by Allo-Media (an intelligent call tracking company). First regression experiments, with audio features, show that the evolution of frustration / satisfaction axis can be retrieved automatically at the conversation level."
2020.lrec-1.529,"A Multimodal Educational Corpus of Oral Courses: Annotation, Analysis and Case Study",2020,0,0,2,1,5782,salima mdhaffar,Proceedings of the 12th Language Resources and Evaluation Conference,0,"This corpus is part of the PASTEL (Performing Automated Speech Transcription for Enhancing Learning) project aiming to explore the potential of synchronous speech transcription and application in specific teaching situations. It includes 10 hours of different lectures, manually transcribed and segmented. The main interest of this corpus lies in its multimodal aspect: in addition to speech, the courses were filmed and the written presentation supports (slides) are made available. The dataset may then serve researches in multiple fields, from speech and language to image and video processing. The dataset will be freely available to the research community. In this paper, we first describe in details the annotation protocol, including a detailed analysis of the manually labeled data. Then, we propose some possible use cases of the corpus with baseline results. The use cases concern scientific fields from both speech and text processing, with language model adaptation, thematic segmentation and transcription to slide alignment."
2020.lrec-1.556,Where are we in Named Entity Recognition from Speech?,2020,0,0,3,1,17778,antoine caubriere,Proceedings of the 12th Language Resources and Evaluation Conference,0,"Named entity recognition (NER) from speech is usually made through a pipeline process that consists in (i) processing audio using an automatic speech recognition system (ASR) and (ii) applying a NER to the ASR outputs. The latest data available for named entity extraction from speech in French were produced during the ETAPE evaluation campaign in 2012. Since the publication of ETAPE{'}s campaign results, major improvements were done on NER and ASR systems, especially with the development of neural approaches for both of these components. In addition, recent studies have shown the capability of End-to-End (E2E) approach for NER / SLU tasks. In this paper, we propose a study of the improvements made in speech recognition and named entity recognition for pipeline approaches. For this type of systems, we propose an original 3-pass approach. We also explore the capability of an E2E system to do structured NER. Finally, we compare the performances of ETAPE{'}s systems (state-of-the-art systems in 2012) with the performances obtained using current technologies. The results show the interest of the E2E approach, which however remains below an updated pipeline approach."
2020.lrec-1.610,Toward Qualitative Evaluation of Embeddings for {A}rabic Sentiment Analysis,2020,-1,-1,4,1,17875,amira barhoumi,Proceedings of the 12th Language Resources and Evaluation Conference,0,"In this paper, we propose several protocols to evaluate specific embeddings for Arabic sentiment analysis (SA) task. In fact, Arabic language is characterized by its agglutination and morphological richness contributing to great sparsity that could affect embedding quality. This work presents a study that compares embeddings based on words and lemmas in SA frame. We propose first to study the evolution of embedding models trained with different types of corpora (polar and non polar) and explore the variation between embeddings by observing the sentiment stability of neighbors in embedding spaces. Then, we evaluate embeddings with a neural architecture based on convolutional neural network (CNN). We make available our pre-trained embeddings to Arabic NLP research community with free to use. We provide also for free resources used to evaluate our embeddings. Experiments are done on the Large Arabic-Book Reviews (LABR) corpus in binary (positive/negative) classification frame. Our best result reaches 91.9{\%}, that is higher than the best previous published one (91.5{\%})."
2020.lrec-1.829,Align then Summarize: Automatic Alignment Methods for Summarization Corpus Creation,2020,-1,-1,3,0,18266,paul tardy,Proceedings of the 12th Language Resources and Evaluation Conference,0,"Summarizing texts is not a straightforward task. Before even considering text summarization, one should determine what kind of summary is expected. How much should the information be compressed? Is it relevant to reformulate or should the summary stick to the original phrasing? State-of-the-art on automatic text summarization mostly revolves around news articles. We suggest that considering a wider variety of tasks would lead to an improvement in the field, in terms of generalization and robustness. We explore meeting summarization: generating reports from automatic transcriptions. Our work consists in segmenting and aligning transcriptions with respect to reports, to get a suitable dataset for neural summarization. Using a bootstrapping approach, we provide pre-alignments that are corrected by human annotators, making a validation set against which we evaluate automatic models. This consistently reduces annotators{'} efforts by providing iteratively better pre-alignment and maximizes the corpus size by using annotations from our automatic alignment models. Evaluation is conducted on publicmeetings, a novel corpus of aligned public meetings. We report automatic alignment and summarization performances on this corpus and show that automatic alignment is relevant for data annotation since it leads to large improvement of almost +4 on all ROUGE scores on the summarization task."
2020.jeptalnrecital-jep.8,O{\\`u} en sommes-nous dans la reconnaissance des entit{\\'e}s nomm{\\'e}es structur{\\'e}es {\\`a} partir de la parole ? (Where are we in Named Entity Recognition from speech ?),2020,-1,-1,3,1,17778,antoine caubriere,"Actes de la 6e conf{\\'e}rence conjointe Journ{\\'e}es d'{\\'E}tudes sur la Parole (JEP, 33e {\\'e}dition), Traitement Automatique des Langues Naturelles (TALN, 27e {\\'e}dition), Rencontre des {\\'E}tudiants Chercheurs en Informatique pour le Traitement Automatique des Langues (R{\\'E}CITAL, 22e {\\'e}dition). Volume 1 : Journ{\\'e}es d'{\\'E}tudes sur la Parole",0,"La reconnaissance des entit{\'e}s nomm{\'e}es (REN) {\`a} partir de la parole est traditionnellement effectu{\'e}e par l{'}interm{\'e}diaire d{'}une cha{\^\i}ne de composants, exploitant un syst{\`e}me de reconnaissance de la parole (RAP), puis un syst{\`e}me de REN appliqu{\'e} sur les transcriptions automatiques. Les derni{\`e}res donn{\'e}es disponibles pour la REN structur{\'e}es {\`a} partir de la parole en fran{\c{c}}ais proviennent de la campagne d{'}{\'e}valuation ETAPE en 2012. Depuis la publication des r{\'e}sultats, des am{\'e}liorations majeures ont {\'e}t{\'e} r{\'e}alis{\'e}es pour les syst{\`e}mes de REN et de RAP. Notamment avec le d{\'e}veloppement des syst{\`e}mes neuronaux. De plus, certains travaux montrent l{'}int{\'e}r{\^e}t des approches de bout en bout pour la t{\^a}che de REN dans la parole. Nous proposons une {\'e}tude des am{\'e}liorations en RAP et REN dans le cadre d{'}une cha{\^\i}ne de composants, ainsi qu{'}une nouvelle approche en trois {\'e}tapes. Nous explorons aussi les capacit{\'e}s d{'}une approche bout en bout pour la REN structur{\'e}es. Enfin, nous comparons ces deux types d{'}approches {\`a} l{'}{\'e}tat de l{'}art de la campagne ETAPE. Nos r{\'e}sultats montrent l{'}int{\'e}r{\^e}t de l{'}approche bout en bout, qui reste toutefois en de{\c{c}}{\`a} d{'}une cha{\^\i}ne de composants enti{\`e}rement mise {\`a} jour."
2020.jeptalnrecital-jep.43,Pr{\\'e}diction continue de la satisfaction et de la frustration dans des conversations de centre d{'}appels ({A}llo{S}at : A New Call Center {F}rench Corpus for Affect Analysis),2020,-1,-1,3,0,17018,manon macary,"Actes de la 6e conf{\\'e}rence conjointe Journ{\\'e}es d'{\\'E}tudes sur la Parole (JEP, 33e {\\'e}dition), Traitement Automatique des Langues Naturelles (TALN, 27e {\\'e}dition), Rencontre des {\\'E}tudiants Chercheurs en Informatique pour le Traitement Automatique des Langues (R{\\'E}CITAL, 22e {\\'e}dition). Volume 1 : Journ{\\'e}es d'{\\'E}tudes sur la Parole",0,"Nous pr{\'e}sentons un nouveau corpus, nomm{\'e} AlloSat, compos{\'e} de conversations en fran{\c{c}}ais extraites de centre d{'}appels, annot{\'e}es de fa{\c{c}}on continue en frustration et satisfaction. Dans le contexte des centres d{'}appels, une conversation vise g{\'e}n{\'e}ralement {\`a} r{\'e}soudre la demande de l{'}appelant. Ce corpus a {\'e}t{\'e} mis en place afin de d{\'e}velopper de nouveaux syst{\`e}mes capables de mod{\'e}liser l{'}aspect continu de l{'}information s{\'e}mantique et para-linguistique au niveau conversationnel. Nous nous concentrons sur le niveau para-linguistique, plus pr{\'e}cis{\'e}ment sur l{'}expression des {\'e}motions. {\`A} notre connaissance, la plupart des corpus {\'e}motionnels contiennent des annotations en cat{\'e}gories discr{\`e}tes ou dans des dimensions continues telles que l{'}activation ou la valence. Nous supposons que ces dimensions ne sont pas suffisamment li{\'e}es {\`a} notre contexte. Pour r{\'e}soudre ce probl{\`e}me, nous proposons un corpus permettant une connaissance en temps r{\'e}el de l{'}axe frustration/satisfaction. AlloSat regroupe 303 conversations pour un total d{'}environ 37 heures d{'}audio, toutes enregistr{\'e}es dans des environnements r{\'e}els, collect{\'e}es par Allo-Media (une soci{\'e}t{\'e} sp{\'e}cialis{\'e}e dans l{'}analyse automatique d{'}appels). Les premi{\`e}res exp{\'e}riences de classification montrent que l{'}{\'e}volution de l{'}axe frustration/satisfaction peut {\^e}tre pr{\'e}dite automatiquement par conversation."
2020.iwslt-1.2,{ON}-{TRAC} Consortium for End-to-End and Simultaneous Speech Translation Challenge Tasks at {IWSLT} 2020,2020,28,0,7,0,5712,maha elbayad,Proceedings of the 17th International Conference on Spoken Language Translation,0,"This paper describes the ON-TRAC Consortium translation systems developed for two challenge tracks featured in the Evaluation Campaign of IWSLT 2020, offline speech translation and simultaneous speech translation. ON-TRAC Consortium is composed of researchers from three French academic laboratories: LIA (Avignon Universit{\'e}), LIG (Universit{\'e} Grenoble Alpes), and LIUM (Le Mans Universit{\'e}). Attention-based encoder-decoder models, trained end-to-end, were used for our submissions to the offline speech translation track. Our contributions focused on data augmentation and ensembling of multiple models. In the simultaneous speech translation track, we build on Transformer-based wait-k models for the text-to-text subtask. For speech-to-text simultaneous translation, we attach a wait-k MT system to a hybrid ASR system. We propose an algorithm to control the latency of the ASR+MT cascade and achieve a good latency-quality trade-off on both subtasks."
2019.jeptalnrecital-long.6,Curriculum d{'}apprentissage : reconnaissance d{'}entit{\\'e}s nomm{\\'e}es pour l{'}extraction de concepts s{\\'e}mantiques (Curriculum learning : named entity recognition for semantic concept extraction),2019,-1,-1,3,1,17778,antoine caubriere,Actes de la Conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles (TALN) PFIA 2019. Volume I : Articles longs,0,"Dans cet article, nous pr{\'e}sentons une approche de bout en bout d{'}extraction de concepts s{\'e}mantiques de la parole. En particulier, nous mettons en avant l{'}apport d{'}une cha{\^\i}ne d{'}apprentissage successif pilot{\'e}e par une strat{\'e}gie de curriculum d{'}apprentissage. Dans la cha{\^\i}ne d{'}apprentissage mise en place, nous exploitons des donn{\'e}es fran{\c{c}}aises annot{\'e}es en entit{\'e}s nomm{\'e}es que nous supposons {\^e}tre des concepts plus g{\'e}n{\'e}riques que les concepts s{\'e}mantiques li{\'e}s {\`a} une application informatique sp{\'e}cifique. Dans cette {\'e}tude, il s{'}agit d{'}extraire des concepts s{\'e}mantiques dans le cadre de la t{\^a}che MEDIA. Pour renforcer le syst{\`e}me propos{\'e}, nous exploitons aussi des strat{\'e}gies d{'}augmentation de donn{\'e}es, un mod{\`e}le de langage 5-gramme, ainsi qu{'}un mode {\'e}toile aidant le syst{\`e}me {\`a} se concentrer sur les concepts et leurs valeurs lors de l{'}apprentissage. Les r{\'e}sultats montrent un int{\'e}r{\^e}t {\`a} l{'}utilisation des donn{\'e}es d{'}entit{\'e}s nomm{\'e}es, permettant un gain relatif allant jusqu{'}{\`a} 6,5 {\%}."
2019.jeptalnrecital-court.2,Apport de l{'}adaptation automatique des mod{\\`e}les de langage pour la reconnaissance de la parole: {\\'e}valuation qualitative extrins{\\`e}que dans un contexte de traitement de cours magistraux (Contribution of automatic adaptation of language models for speech recognition : extrinsic qualitative evaluation in a context of educational courses),2019,-1,-1,2,1,5782,salima mdhaffar,Actes de la Conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles (TALN) PFIA 2019. Volume II : Articles courts,0,"Malgr{\'e} les faiblesses connues de cette m{\'e}trique, les performances de diff{\'e}rents syst{\`e}mes de reconnaissance automatique de la parole sont g{\'e}n{\'e}ralement compar{\'e}es {\`a} l{'}aide du taux d{'}erreur sur les mots. Les transcriptions automatiques de ces syst{\`e}mes sont de plus en plus exploitables et utilis{\'e}es dans des syst{\`e}mes complexes de traitement automatique du langage naturel, par exemple pour la traduction automatique, l{'}indexation, la recherche documentaire... Des {\'e}tudes r{\'e}centes ont propos{\'e} des m{\'e}triques permettant de comparer la qualit{\'e} des transcriptions automatiques de diff{\'e}rents syst{\`e}mes en fonction de la t{\^a}che vis{\'e}e. Dans cette {\'e}tude nous souhaitons mesurer, qualitativement, l{'}apport de l{'}adaptation automatique des mod{\`e}les de langage au domaine vis{\'e} par un cours magistral. Les transcriptions du discours de l{'}enseignant peuvent servir de support {\`a} la navigation dans le document vid{\'e}o du cours magistral ou permettre l{'}enrichissement de son contenu p{\'e}dagogique. C{'}est {\`a}-travers le prisme de ces deux t{\^a}ches que nous {\'e}valuons l{'}apport de l{'}adaptation du mod{\`e}le de langage. Les exp{\'e}riences ont {\'e}t{\'e} men{\'e}es sur un corpus de cours magistraux et montrent combien le taux d{'}erreur sur les mots est une m{\'e}trique insuffisante qui masque les apports effectifs de l{'}adaptation des mod{\`e}les de langage."
2019.jeptalnrecital-court.24,Plongements lexicaux sp{\\'e}cifiques {\\`a} la langue arabe : application {\\`a} l{'}analyse d{'}opinions ({A}rabic-specific embedddings : application in Sentiment Analysis),2019,-1,-1,4,1,17875,amira barhoumi,Actes de la Conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles (TALN) PFIA 2019. Volume II : Articles courts,0,"Nous nous int{\'e}ressons, dans cet article, {\`a} la t{\^a}che d{'}analyse d{'}opinions en arabe. Nous {\'e}tudions la sp{\'e}cificit{\'e} de la langue arabe pour la d{\'e}tection de polarit{\'e}. Nous nous focalisons ici sur les caract{\'e}ristiques d{'}agglutination et de richesse morphologique de cette langue. Nous avons particuli{\`e}rement {\'e}tudi{\'e} diff{\'e}rentes repr{\'e}sentations d{'}unit{\'e} lexicale : token, lemme et light stemme. Nous avons construit et test{\'e} des espaces continus de ces diff{\'e}rentes repr{\'e}sentations lexicales. Nous avons mesur{\'e} l{'}apport de tels types de representations vectorielles dans notre cadre sp{\'e}cifique. Les performances du r{\'e}seau CNN montrent un gain significatif de 2{\%} par rapport {\`a} l{'}{\'e}tat de l{'}art."
L18-1329,{F}r{N}ews{L}ink : a corpus linking {TV} Broadcast News Segments and Press Articles,2018,0,0,6,1,17732,nathalie camelin,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,None
L18-1499,Simulating {ASR} errors for training {SLU} systems,2018,0,11,4,0,30072,edwin simonnet,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,"This paper presents an approach to simulate automatic speech recognition (ASR) errors from manual transcriptions and describes how it can be used to improve the performance of spoken language understanding (SLU) systems. In particular, we point out that this noising process is very usefull to obtain a more robust SLU system to ASR errors in case of insufficient training data or more if ASR transcriptions are not available during the training of the SLU model. The proposed method is based on the use of both acoustic and linguistic word embeddings in order to define a similarity measure between words dedicated to predict ASR confusions. Actually, we assume that words acoustically and linguistically close are the ones confused by an ASR system. By using this similarity measure in order to randomly substitute correct words by potentially confusing words in manual annotations used to train CRF-or neural-based SLU systems, we augment the training corpus with these new noisy data. Experiments were carried on the French MEDIA corpus focusing on hotel reservation. They show that this approach significantly improves SLU system performance with a relative reduction of 21.2% of concept/value error rate (CVER), particularly when the SLU system is based on a neural approach (reduction of 22.4% of CVER). A comparison to a naive noising approach shows that the proposed noising approach is particularly relevant."
L18-1500,Evaluation of Feature-Space Speaker Adaptation for End-to-End Acoustic Models,2018,0,5,2,1,5781,natalia tomashenko,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,"This paper investigates speaker adaptation techniques for bidirectional long short term memory (BLSTM) recurrent neural networkn  based acoustic models (AMs) trained with the connectionist temporal classification (CTC) objective function.n BLSTM-CTC AMs play an important role in end-to-end automatic speech recognition systems.n However, there is a lack of research in speaker adaptation algorithms for these models. We explore three different feature-space adaptation approaches for CTC AMs: feature-space maximum linear regression, i-vector based adaptation, and maximum a posteriori adaptation using GMM-derived features.n Experimental results on the TED-LIUM corpus demonstrate that speaker adaptation, applied in combination with data augmentation techniques, provides, in an unsupervised adaptation mode, for different test sets, up to 11--20% of relative word error rate reduction over the baseline model built on the raw filter-bank features. In addition, the adaptation behavior is compared for BLSTM-CTC AMs and time-delay neural network AMs trained with the cross-entropy criterion."
2018.jeptalnrecital-court.3,Des repr{\\'e}sentations continues de mots pour l{'}analyse d{'}opinions en arabe: une {\\'e}tude qualitative (Word embeddings for {A}rabic sentiment analysis : a qualitative study),2018,-1,-1,3,1,17875,amira barhoumi,"Actes de la Conf{\\'e}rence TALN. Volume 1 - Articles longs, articles courts de TALN",0,"Nous nous int{\'e}ressons, dans cet article, {\`a} la d{\'e}tection d{'}opinions dans la langue arabe. Ces derni{\`e}res ann{\'e}es, l{'}utilisation de l{'}apprentissage profond a am{\'e}lior{\'e} des performances de nombreux syst{\`e}mes automatiques dans une grande vari{\'e}t{\'e} de domaines (analyse d{'}images, reconnaissance de la parole, traduction automatique, . . .) et {\'e}galement celui de l{'}analyse d{'}opinions en anglais. Ainsi, nous avons {\'e}tudi{\'e} l{'}apport de deux architectures (CNN et LSTM) dans notre cadre sp{\'e}cifique. Nous avons {\'e}galement test{\'e} et compar{\'e} plusieurs types de repr{\'e}sentations continues de mots (embeddings) disponibles en langue arabe, qui ont permis d{'}obtenir de bons r{\'e}sultats. Nous avons analys{\'e} les erreurs de notre syst{\`e}me et la pertinence de ces embeddings. Cette analyse m{\`e}ne {\`a} plusieurs perspectives int{\'e}ressantes de travail, au sujet notamment de la constitution automatique de ressources expert et d{'}une construction pertinente des embeddings sp{\'e}cifiques {\`a} la t{\^a}che d{'}analyse d{'}opinions."
2018.jeptalnrecital-court.25,Le corpus {PASTEL} pour le traitement automatique de cours magistraux ({PASTEL} corpus for automatic processing of lectures),2018,-1,-1,3,1,5782,salima mdhaffar,"Actes de la Conf{\\'e}rence TALN. Volume 1 - Articles longs, articles courts de TALN",0,"Le projet PASTEL {\'e}tudie l{'}acceptabilit{\'e} et l{'}utilisabilit{\'e} des transcriptions automatiques dans le cadre d{'}enseignements magistraux. Il s{'}agit d{'}outiller les apprenants pour enrichir de mani{\`e}re synchrone et automatique les informations auxquelles ils peuvent avoir acc{\`e}s durant la s{\'e}ance. Cet enrichissement s{'}appuie sur des traitements automatiques du langage naturel effectu{\'e}s sur les transcriptions automatiques. Nous pr{\'e}sentons dans cet article un travail portant sur l{'}annotation d{'}enregistrements de cours magistraux enregistr{\'e}s dans le cadre du projet CominOpenCourseware. Ces annotations visent {\`a} effectuer des exp{\'e}riences de transcription automatique, segmentation th{\'e}matique, appariement automatique en temps r{\'e}el avec des ressources externes... Ce corpus comprend plus de neuf heures de parole annot{\'e}es. Nous pr{\'e}sentons {\'e}galement des exp{\'e}riences pr{\'e}liminaires r{\'e}alis{\'e}es pour {\'e}valuer l{'}adaptation automatique de notre syst{\`e}me de reconnaissance de la parole."
W17-1307,Sentiment Analysis of {T}unisian Dialects: Linguistic Ressources and Experiments,2017,0,18,3,0,32055,salima medhaffar,Proceedings of the Third {A}rabic Natural Language Processing Workshop,0,"Dialectal Arabic (DA) is significantly different from the Arabic language taught in schools and used in written communication and formal speech (broadcast news, religion, politics, etc.). There are many existing researches in the field of Arabic language Sentiment Analysis (SA); however, they are generally restricted to Modern Standard Arabic (MSA) or some dialects of economic or political interest. In this paper we are interested in the SA of the Tunisian Dialect. We utilize Machine Learning techniques to determine the polarity of comments written in Tunisian Dialect. First, we evaluate the SA systems performances with models trained using freely available MSA and Multi-dialectal data sets. We then collect and annotate a Tunisian Dialect corpus of 17.000 comments from Facebook. This corpus allows us a significant accuracy improvement compared to the best model trained on other Arabic dialects or MSA data. We believe that this first freely available corpus will be valuable to researchers working in the field of Tunisian Sentiment Analysis and similar areas."
W16-2511,Evaluation of acoustic word embeddings,2016,14,5,2,1,862,sahar ghannay,Proceedings of the 1st Workshop on Evaluating Vector-Space Representations for {NLP},0,"Recently, researchers in speech recognition have started to reconsider using whole words as the basic modeling unit, instead of phonetic units. These systems rely on a function that embeds an arbitrary or fixed dimensional speech segments to a vector in a fixed-dimensional space, named acoustic word embedding. Thus, speech segments of words that sound similarly will be projected in a close area in a continuous space. This paper focuses on the evaluation of acoustic word embeddings. We propose two approaches to evaluate the intrinsic performances of acoustic word embeddings in comparison to orthographic representations in order to evaluate whether they capture discriminative phonetic information. Since French language is targeted in experiments, a particular focus is made on homophone words."
L16-1046,Word Embedding Evaluation and Combination,2016,0,31,3,1,862,sahar ghannay,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"Word embeddings have been successfully used in several natural language processing tasks (NLP) and speech processing. Different approaches have been introduced to calculate word embeddings through neural networks. In the literature, many studies focused on word embedding evaluation, but for our knowledge, there are still some gaps. This paper presents a study focusing on a rigorous comparison of the performances of different kinds of word embeddings. These performances are evaluated on different NLP and linguistic tasks, while all the word embeddings are estimated on the same training data using the same vocabulary, the same number of dimensions, and other similar characteristics. The evaluation results reported in this paper match those in the literature, since they point out that the improvements achieved by a word embedding in one task are not consistently observed across all tasks. For that reason, this paper investigates and evaluates approaches to combine word embeddings in order to take advantage of their complementarity, and to look for the effective word embeddings that can achieve good performances on all tasks. As a conclusion, this paper provides new perceptions of intrinsic qualities of the famous word embedding families, which can be different from the ones provided by works previously published in the scientific literature."
L16-1166,Enhancing The {RATP}-{DECODA} Corpus With Linguistic Annotations For Performing A Large Range Of {NLP} Tasks,2016,9,3,4,1,18649,carole lailler,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"In this article, we present the RATP-DECODA Corpus which is composed by a set of 67 hours of speech from telephone conversations of a Customer Care Service (CCS). This corpus is already available on line at http://sldr.org/sldr000847/fr in its first version. However, many enhancements have been made in order to allow the development of automatic techniques to transcript conversations and to capture their meaning. These enhancements fall into two categories: firstly, we have increased the size of the corpus with manual transcriptions from a new operational day; secondly we have added new linguistic annotations to the whole corpus (either manually or through an automatic processing) in order to perform various linguistic tasks from syntactic and semantic parsing to dialog act tagging and dialog summarization."
2016.jeptalnrecital-jep.38,Exploration de param{\\`e}tres acoustiques d{\\'e}riv{\\'e}s de {GMM} pour l{'}adaptation non supervis{\\'e}e de mod{\\`e}les acoustiques {\\`a} base de r{\\'e}seaux de neurones profonds (Exploring {GMM}-derived features for unsupervised adaptation of deep neural network acoustic models),2016,-1,-1,4,1,5781,natalia tomashenko,Actes de la conf{\\'e}rence conjointe JEP-TALN-RECITAL 2016. volume 1 : JEP,0,"L{'}{\'e}tude pr{\'e}sent{\'e}e dans cet article am{\'e}liore une m{\'e}thode r{\'e}cemment propos{\'e}e pour l{'}adaptation de mod{\`e}les acoustiques markoviens coupl{\'e}s {\`a} un r{\'e}seau de neurones profond (DNN-HMM). Cette m{\'e}thode d{'}adaptation utilise des param{\`e}tres acoustiques d{\'e}riv{\'e}s de mixtures de mod{\`e}les Gaussiens (GMM-derived features, GMMD ). L{'}am{\'e}lioration provient de l{'}emploi de scores et de mesures de confiance calcul{\'e}s {\`a} partir de graphes construits dans le cadre d{'}un algorithme d{'}adaptation conventionnel dit de maximum a posteriori (MAP). Une version modifi{\'e}e de l{'}adaptation MAP est appliqu{\'e}e sur le mod{\`e}le GMM auxiliaire utilis{\'e} dans une proc{\'e}dure d{'}apprentissage adaptatif au locuteur (speaker adaptative training, SAT) lors de l{'}apprentissage du DNN. Des exp{\'e}riences men{\'e}es sur le corpus Wall Street Journal (WSJ0) montrent que la technique d{'}adaptation non supervis{\'e}e propos{\'e}e dans cet article permet une r{\'e}duction relative de 8, 4{\%} du taux d{'}erreurs sur les mots (WER), par rapport aux r{\'e}sultats obtenus avec des mod{\`e}les DNN-HMM ind{\'e}pendants du locuteur utilisant des param{\`e}tres acoustiques plus conventionnels."
2016.jeptalnrecital-jep.72,Des R{\\'e}seaux de Neurones avec M{\\'e}canisme d{'}Attention pour la Compr{\\'e}hension de la Parole (Exploring the use of Attention-Based Recurrent Neural Networks For Spoken Language Understanding ),2016,-1,-1,4,0,30072,edwin simonnet,Actes de la conf{\\'e}rence conjointe JEP-TALN-RECITAL 2016. volume 1 : JEP,0,"L{'}{\'e}tude porte sur l{'}apport d{'}un r{\'e}seau de neurones r{\'e}current (Recurrent Neural Network RNN) bidirectionnel encodeur/d{\'e}codeur avec m{\'e}canisme d{'}attention pour une t{\^a}che de compr{\'e}hension de la parole. Les premi{\`e}res exp{\'e}riences faites sur le corpus ATIS confirment la qualit{\'e} du syst{\`e}me RNN {\'e}tat de l{'}art utilis{\'e} pour cet article, en comparant les r{\'e}sultats obtenus {\`a} ceux r{\'e}cemment publi{\'e}s dans la litt{\'e}rature. Des exp{\'e}riences suppl{\'e}mentaires montrent que les RNNs avec m{\'e}canisme d{'}attention obtiennent de meilleures performances que les RNNs r{\'e}cemment propos{\'e}s pour la t{\^a}che d{'}{\'e}tiquetage en concepts s{\'e}mantiques. Sur le corpus MEDIA, un corpus fran{\c{c}}ais {\'e}tat de l{'}art pour la compr{\'e}hension d{\'e}di{\'e} {\`a} la r{\'e}servation d{'}h{\^o}tel et aux informations touristiques, les exp{\'e}riences montrent qu{'}un RNN bidirectionnel atteint une f-mesure de 79,51 tandis que le m{\^e}me syst{\`e}me int{\'e}grant le m{\'e}canisme d{'}attention permet d{'}atteindre une f-mesure de 80,27."
2016.jeptalnrecital-jep.81,Utilisation des repr{\\'e}sentations continues des mots et des param{\\`e}tres prosodiques pour la d{\\'e}tection d{'}erreurs dans les transcriptions automatiques de la parole (Combining continuous word representation and prosodic features for {ASR} error detection),2016,-1,-1,2,1,862,sahar ghannay,Actes de la conf{\\'e}rence conjointe JEP-TALN-RECITAL 2016. volume 1 : JEP,0,"R{\'e}cemment, l{'}utilisation des repr{\'e}sentations continues de mots a connu beaucoup de succ{\`e}s dans plusieurs t{\^a}ches de traitement du langage naturel. Dans cet article, nous proposons d{'}{\'e}tudier leur utilisation dans une architecture neuronale pour la t{\^a}che de d{\'e}tection des erreurs au sein de transcriptions automatiques de la parole. Nous avons {\'e}galement exp{\'e}riment{\'e} et {\'e}valu{\'e} l{'}utilisation de param{\`e}tres prosodiques en suppl{\'e}ments des param{\`e}tres classiques (lexicaux, syntaxiques, . . .). La principale contribution de cet article porte sur la combinaison de diff{\'e}rentes repr{\'e}sentations continues de mots : plusieurs approches de combinaison sont propos{\'e}es et {\'e}valu{\'e}es afin de tirer profit de leurs compl{\'e}mentarit{\'e}s. Les exp{\'e}riences sont effectu{\'e}es sur des transcriptions automatiques du corpus ETAPE g{\'e}n{\'e}r{\'e}es par le syst{\`e}me de reconnaissance automatique du LIUM. Les r{\'e}sultats obtenus sont meilleurs que ceux d{'}un syst{\`e}me {\'e}tat de l{'}art bas{\'e} sur les champs al{\'e}atoires conditionnels. Pour terminer, nous montrons que la mesure de confiance produite est particuli{\`e}rement bien calibr{\'e}e selon une {\'e}valuation en terme d{'}Entropie Crois{\'e}e Normalis{\'e}e (NCE)."
2015.jeptalnrecital-court.28,Utilisation d{'}annotations s{\\'e}mantiques pour la validation automatique d{'}hypoth{\\`e}ses dans des conversations t{\\'e}l{\\'e}phoniques,2015,-1,-1,2,1,18649,carole lailler,Actes de la 22e conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Articles courts,0,"Les travaux pr{\'e}sent{\'e}s portent sur l{'}extraction automatique d{'}unit{\'e}s s{\'e}mantiques et l{'}{\'e}valuation de leur pertinence pour des conversations t{\'e}l{\'e}phoniques. Le corpus utilis{\'e} est le corpus fran{\c{c}}ais DECODA. L{'}objectif de la t{\^a}che est de permettre l{'}{\'e}tiquetage automatique en th{\`e}me de chaque conversation. Compte tenu du caract{\`e}re spontan{\'e} de ce type de conversations et de la taille du corpus, nous proposons de recourir {\`a} une strat{\'e}gie semi-supervis{\'e}e fond{\'e}e sur la construction d{'}une ontologie et d{'}un apprentissage actif simple : un annotateur humain analyse non seulement les listes d{'}unit{\'e}s s{\'e}mantiques candidates menant au th{\`e}me mais {\'e}tudie {\'e}galement une petite quantit{\'e} de conversations. La pertinence de la relation unissant les unit{\'e}s s{\'e}mantiques conserv{\'e}es, le sous-th{\`e}me issu de l{'}ontologie et le th{\`e}me annot{\'e} est {\'e}valu{\'e}e par un DNN, prenant en compte une repr{\'e}sentation vectorielle du document. L{'}int{\'e}gration des unit{\'e}s s{\'e}mantiques retenues dans le processus de classification en th{\`e}me am{\'e}liore les performances."
2015.jeptalnrecital-court.33,Segmentation et Titrage Automatique de Journaux T{\\'e}l{\\'e}vis{\\'e}s,2015,-1,-1,4,0.952381,24123,abdessalam bouchekif,Actes de la 22e conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Articles courts,0,"Dans cet article, nous nous int{\'e}ressons au titrage automatique des segments issus de la segmentation th{\'e}matique de journaux t{\'e}l{\'e}vis{\'e}s. Nous proposons d{'}associer un segment {\`a} un article de presse {\'e}crite collect{\'e} le jour m{\^e}me de la diffusion du journal. La t{\^a}che consiste {\`a} apparier un segment {\`a} un article de presse {\`a} l{'}aide d{'}une mesure de similarit{\'e}. Cette approche soul{\`e}ve plusieurs probl{\`e}mes, comme la s{\'e}lection des articles candidats, une bonne repr{\'e}sentation du segment et des articles, le choix d{'}une mesure de similarit{\'e} robuste aux impr{\'e}cisions de la segmentation. Des exp{\'e}riences sont men{\'e}es sur un corpus vari{\'e} de journaux t{\'e}l{\'e}vis{\'e}s fran{\c{c}}ais collect{\'e}s pendant une semaine, conjointement avec des articles aspir{\'e}s {\`a} partir de la page d{'}accueil de Google Actualit{\'e}s. Nous introduisons une m{\'e}trique d{'}{\'e}valuation refl{\'e}tant la qualit{\'e} de la segmentation, du titrage ainsi que la qualit{\'e} conjointe de la segmentation et du titrage. L{'}approche donne de bonnes performances et se r{\'e}v{\`e}le robuste {\`a} la segmentation th{\'e}matique."
2015.iwslt-evaluation.7,The {LIUM} {ASR} and {SLT} systems for {IWSLT} 2015,2015,14,0,5,0,38016,mercedes martinez,Proceedings of the 12th International Workshop on Spoken Language Translation: Evaluation Campaign,0,"This paper describes the Automatic Speech Recognition and Spoken Language Translation systems developed by the LIUM for the IWSLT 2015 evaluation campaign. We participated in two of the proposed tasks, namely the Automatic Speech Recognition task (ASR) in German and the English to French Spoken Language Translation task (SLT). We present the approaches and specificities found in our systems, as well as our results from the evaluation campaign."
W14-5212,{EUMSSI}: a Platform for Multimodal Analysis and Recommendation using {UIMA},2014,6,2,5,0,14059,jens grivolla,Proceedings of the Workshop on Open Infrastructures and Analysis Frameworks for {HLT},0,"The EUMSSI project (Event Understanding through Multimodal Social Stream Interpretation) aims at developing technologies for aggregating data presented as unstructured information in sources of very different nature. The multimodal analytics will help organize, classify and cluster cross-media streams, by enriching its associated metadata in an interactive manner, so that the data resulting from analysing one media helps reinforce the aggregation of information from other media, in a cross-modal semantic representation framework. Once all the available descriptive information has been collected, an interpretation component will dynamically reason over the semantic representation in order to derive implicit knowledge. Finally the enriched information will be fed to a hybrid recommendation system, which will be at the basis of two well-motivated use-cases. In this paper we give a brief overview of EUMSSIxe2x80x99s main goals and how we are approaching its implementation using UIMA to integrate and combine various layers of annotations coming from different sources."
rousseau-etal-2014-enhancing,Enhancing the {TED}-{LIUM} Corpus with Selected Data for Language Modeling and More {TED} Talks,2014,10,79,3,1,17019,anthony rousseau,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"In this paper, we present improvements made to the TED-LIUM corpus we released in 2012. These enhancements fall into two categories. First, we describe how we filtered publicly available monolingual data and used it to estimate well-suited language models (LMs), using open-source tools. Then, we describe the process of selection we applied to new acoustic data from TED talks, providing additions to our previously released corpus. Finally, we report some experiments we made around these improvements."
masmoudi-etal-2014-corpus,A Corpus and Phonetic Dictionary for {T}unisian {A}rabic Speech Recognition,2014,12,13,3,0,25328,abir masmoudi,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"In this paper we describe an effort to create a corpus and phonetic dictionary for Tunisian Arabic Automatic Speech Recognition (ASR). The corpus, named TARIC (Tunisian Arabic Railway Interaction Corpus) has a collection of audio recordings and transcriptions from dialogues in the Tunisian Railway Transport Network. The phonetic (or pronunciation) dictionary is an important ASR component that serves as an intermediary between acoustic models and language models in ASR systems. The method proposed in this paper, to automatically generate a phonetic dictionary, is rule based. For that reason, we define a set of pronunciation rules and a lexicon of exceptions. To determine the performance of our phonetic rules, we chose to evaluate our pronunciation dictionary on two types of corpora. The word error rate of word grapheme-to-phoneme mapping is around 9{\%}."
2014.iwslt-evaluation.14,{LIUM} {E}nglish-to-{F}rench spoken language translation system and the Vecsys/{LIUM} automatic speech recognition system for {I}talian language for {IWSLT} 2014,2014,11,1,4,1,17019,anthony rousseau,Proceedings of the 11th International Workshop on Spoken Language Translation: Evaluation Campaign,0,"This paper describes the Spoken Language Translation system developed by the LIUM for the IWSLT 2014 evaluation campaign. We participated in two of the proposed tasks: (i) the Automatic Speech Recognition task (ASR) in two languages, Italian with the Vecsys company, and English alone, (ii) the English to French Spoken Language Translation task (SLT). We present the approaches and specificities found in our systems, as well as the results from the evaluation campaign."
rousseau-etal-2012-ted,{TED}-{LIUM}: an Automatic Speech Recognition dedicated corpus,2012,6,85,3,1,17019,anthony rousseau,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"This paper presents the corpus developed by the LIUM for Automatic Speech Recognition (ASR), based on the TED Talks. This corpus was built during the IWSLT 2011 Evaluation Campaign, and is composed of 118 hours of speech with its accompanying automatically aligned transcripts. We describe the content of the corpus, how the data was collected and processed, how it will be publicly available and how we built an ASR system using this data leading to a WER score of 17.4 {\%}. The official results we obtained at the IWSLT 2011 evaluation campaign are also discussed."
lefevre-etal-2012-leveraging,Leveraging study of robustness and portability of spoken language understanding systems across languages and domains: the {PORTMEDIA} corpora,2012,20,9,4,0,27634,fabrice lefevre,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"The PORTMEDIA project is intended to develop new corpora for the evaluation of spoken language understanding systems. The newly collected data are in the field of human-machine dialogue systems for tourist information in French in line with the MEDIA corpus. Transcriptions and semantic annotations, obtained by low-cost procedures, are provided to allow a thorough evaluation of the systems' capabilities in terms of robustness and portability across languages and domains. A new test set with some adaptation data is prepared for each case: in Italian as an example of a new language, for ticket reservation as an example of a new domain. Finally the work is complemented by the proposition of a new high level semantic annotation scheme well-suited to dialogue data."
F12-1055,Segmentation et Regroupement en Locuteurs d{'}une collection de documents audio (Cross-show speaker diarization) [in {F}rench],2012,0,0,4,0,43500,gregor dupuy,"Proceedings of the Joint Conference JEP-TALN-RECITAL 2012, volume 1: JEP",0,None
F12-1098,Robustesse et portabilit{\\'e}s multilingue et multi-domaines des syst{\\`e}mes de compr{\\'e}hension de la parole : les corpus du projet {P}ort{M}edia (Robustness and portability of spoken language understanding systems among languages and domains : the {PORTMEDIA} project) [in {F}rench],2012,0,1,4,0,27634,fabrice lefevre,"Proceedings of the Joint Conference JEP-TALN-RECITAL 2012, volume 1: JEP",0,None
F12-1100,Avanc{\\'e}es dans le domaine de la transcription automatique par d{\\'e}codage guid{\\'e} (Improvements on driven decoding system combination) [in {F}rench],2012,0,0,2,1,13777,fethi bougares,"Proceedings of the Joint Conference JEP-TALN-RECITAL 2012, volume 1: JEP",0,None
F12-1104,Combinaison d{'}approches pour la reconnaissance du r{\\^o}le des locuteurs (Combination of approaches for speaker role recognition) [in {F}rench],2012,-1,-1,3,0,16966,richard dufour,"Proceedings of the Joint Conference JEP-TALN-RECITAL 2012, volume 1: JEP",0,None
2011.iwslt-evaluation.10,{LIUM}{'}s systems for the {IWSLT} 2011 speech translation tasks,2011,18,33,5,1,17019,anthony rousseau,Proceedings of the 8th International Workshop on Spoken Language Translation: Evaluation Campaign,0,"This paper describes the three systems developed by the LIUM for the IWSLT 2011 evaluation campaign. We participated in three of the proposed tasks, namely the Automatic Speech Recognition task (ASR), the ASR system combination task (ASR{\_}SC) and the Spoken Language Translation task (SLT), since these tasks are all related to speech translation. We present the approaches and specificities we developed on each task."
esteve-etal-2010-epac,The {EPAC} Corpus: Manual and Automatic Annotations of Conversational Speech in {F}rench Broadcast News,2010,10,39,1,1,5785,yannick esteve,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"This paper presents the EPAC corpus which is composed by a set of 100 hours of conversational speech manually transcribed and by the outputs of automatic tools (automatic segmentation, transcription, POS tagging, etc.) applied on the entire French ESTER 1 audio corpus: this concerns about 1700 hours of audio recordings from radiophonic shows. This corpus was built during the EPAC project funded by the French Research Agency (ANR) from 2007 to 2010. This corpus increases significantly the amount of French manually transcribed audio recordings easily available and it is now included as a part of the ESTER 1 corpus in the ELRA catalog without additional cost. By providing a large set of automatic outputs of speech processing tools, the EPAC corpus should be useful to researchers who want to work on such data without having to develop and deal with such tools. These automatic annotations are various: segmentation and speaker diarization, one-best hypotheses from the LIUM automatic speech recognition system with confidence measures, but also word-lattices and confusion networks, named entities, part-of-speech tags, chunks, etc. The 100 hours of speech manually transcribed were split into three data sets in order to get an official training corpus, an official development corpus and an official test corpus. These data sets were used to develop and to evaluate some automatic tools which have been used to process the 1700 hours of audio recording. For example, on the EPAC test data set our ASR system yields a word error rate equals to 17.25{\%}."
2010.iwslt-evaluation.14,{LIUM}{'}s statistical machine translation system for {IWSLT} 2010,2010,8,0,4,1,17019,anthony rousseau,Proceedings of the 7th International Workshop on Spoken Language Translation: Evaluation Campaign,0,"This paper describes the two systems developed by the LIUM laboratory for the 2010 IWSLT evaluation campaign. We participated to the new English to French TALK task. We developed two systems, one for each evaluation condition, both being statistical phrase-based systems using the the Moses toolkit. Several approaches were investigated."
2009.iwslt-evaluation.10,{LIUM}{'}s statistical machine translation system for {IWSLT} 2009,2009,12,1,3,0.389698,5770,holger schwenk,Proceedings of the 6th International Workshop on Spoken Language Translation: Evaluation Campaign,0,"This paper describes the systems developed by the LIUM laboratory for the 2009 IWSLT evaluation. We participated in the Arabic and Chinese to English BTEC tasks. We developed three different systems: a statistical phrase-based system using the Moses toolkit, an Statistical Post-Editing system and a hierarchical phrase-based system based on Joshua. A continuous space language model was deployed to improve the modeling of the target language. These systems are combined by a confusion network based approach."
laurent-etal-2008-combined,Combined Systems for Automatic Phonetic Transcription of Proper Nouns,2008,7,2,4,1,17728,antoine laurent,Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08),0,"Large vocabulary automatic speech recognition (ASR) technologies perform well in known, controlled contexts. However recognition of proper nouns is commonly considered as a difficult task. Accurate phonetic transcription of a proper noun is difficult to obtain, although it can be one of the most important resources for a recognition system. In this article, we propose methods of automatic phonetic transcription applied to proper nouns. The methods are based on combinations of the rule-based phonetic transcription generator LIA{\_}PHON and an acoustic-phonetic decoding system. On the ESTER corpus, we observed that the combined systems obtain better results than our reference system (LIA{\_}PHON). The WER (Word Error Rate) decreased on segments of speech containing proper nouns, without affecting negatively the results on the rest of the corpus. On the same corpus, the Proper Noun Error Rate (PNER, which is a WER computed on proper nouns only), decreased with our new system."
bazillon-etal-2008-manual,Manual vs Assisted Transcription of Prepared and Spontaneous Speech,2008,6,17,2,0,39875,thierry bazillon,Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08),0,"Our paper focuses on the gain which can be achieved on human transcription of spontaneous and prepared speech, by using the assistance of an ASR system. This experiment has shown interesting results, first about the duration of the transcription task itself: even with the combination of prepared speech + ASR, an experimented annotator needs approximately 4 hours to transcribe 1 hours of audio data. Then, using an ASR system is mostly time-saving, although this gain is much more significant on prepared speech: assisted transcriptions are up to 4 times faster than manual ones. This ratio falls to 2 with spontaneous speech, because of ASR limits for these data. Detailed results reveal interesting correlations between the transcription task and phenomena such as Word Error Rate, telephonic or non-native speech turns, the number of fillers or propers nouns. The latter make spelling correction very time-consuming with prepared speech because of their frequency. As a consequence, watching for low averages of proper nouns may be a way to detect spontaneous speech."
2008.iwslt-evaluation.9,The {LIUM} {A}rabic/{E}nglish statistical machine translation system for {IWSLT} 2008.,2008,19,3,2,0.389698,5770,holger schwenk,Proceedings of the 5th International Workshop on Spoken Language Translation: Evaluation Campaign,0,This paper describes the system developed by the LIUM laboratory for the 2008 IWSLT evaluation. We only participated in the Arabic/English BTEC task. We developed a statistical phrase-based system using the Moses toolkit and SYSTRAN{'}s rule-based translation system to perform a morphological decomposition of the Arabic words. A continuous space language model was deployed to improve the modeling of the target language. Both approaches achieved significant improvements in the BLEU score. The system achieves a score of 49.4 on the test set of the 2008 IWSLT evaluation.
mauclair-etal-2006-automatic,Automatic Detection of Well Recognized Words in Automatic Speech Transcriptions,2006,14,13,2,0,18674,julie mauclair,Proceedings of the Fifth International Conference on Language Resources and Evaluation ({LREC}{'}06),0,"This work adresses the use of confidence measures for extracting well recognized words with very low error rate from automatically transcribed segments in a unsupervised way. We present and compare several confidence measures and propose a method to merge them into a new one. We study its capabilities on extracting correct recognized word-segments compared to the amount of rejected words. We apply this fusion measure to select audio segments composed of words with a high confidence score. These segments come from an automatic transcription of french broadcast news given by our speech recognition system based on the CMU Sphinx3.3 decoder. Injecting new data resulting from unsupervised treatments of raw audio recordings in the training corpus of acoustic models gives statistically significant improvement (95{\%} confident interval) in terms of word error rate. Experiments have been carried out on the corpus used during ESTER, the french evaluation campaign."
2001.jeptalnrecital-poster.1,Mod{\\`e}les de langage hi{\\'e}rarchiques pour les applications de dialogue en parole spontan{\\'e}e,2001,3,0,2,0,17997,frederic bechet,Actes de la 8{\\`e}me conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Posters,0,"Le cadre de cette {\'e}tude concerne les syst{\`e}mes de dialogue via le t{\'e}l{\'e}phone entre un serveur de donn{\'e}es et un utilisateur. Nous nous int{\'e}resserons au cas de dialogues non contraints o{\`u} l{'}utilisateur {\`a} toute libert{\'e} pour formuler ses requ{\^e}tes. G{\'e}n{\'e}ralement, le module de Reconnaissance Automatique de la Parole (RAP) de tels serveurs utilise un seul Mod{\`e}le de Langage (ML) de type bigramme ou trigramme pour mod{\'e}liser l{'}ensemble des interventions possibles de l{'}utilisateur. Ces ML sont appris sur des corpus de phrases retranscrites {\`a} partir de sessions entre le serveur et plusieurs utilisateurs. Nous proposons dans cette {\'e}tude une m{\'e}thode de segmentation de corpus d{'}apprentissage de dialogue utilisant une strat{\'e}gie mixte bas{\'e}e {\`a} la fois sur des connaissances explicites mais aussi sur l{'}optimisation d{'}un crit{\`e}re statistique. Nous montrons qu{'}un gain en terme de perplexit{\'e} et de taux d{'}erreurs/mot peut {\^e}tre constat{\'e} en utilisant un ensemble de sous mod{\`e}les de langage issus de la segmentation plut{\^o}t qu{'}un mod{\`e}le unique appris sur l{'}ensemble du corpus."
