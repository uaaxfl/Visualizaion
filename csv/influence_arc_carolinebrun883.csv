2003.jeptalnrecital-poster.14,W00-1404,1,0.892008,"Missing"
2003.jeptalnrecital-poster.14,C02-1128,1,0.835828,"Missing"
2003.jeptalnrecital-poster.14,P98-2173,0,0.0482442,"Missing"
2004.jeptalnrecital-long.15,2001.mtsummit-papers.27,0,0.0252904,"ans la mesure où plusieurs composantes rentrent en jeu. Nous avons ici choisi d’évaluer dans un premier temps les performances de la première partie du traitement (Extraction d’information ciblée). Cette évaluation effectuée sur une collection de 30 textes du domaine qui n’ont pas été préalablement traités par le système a donné les résultats suivants en terme de précision et de rappel. De part l’approche que Précision .96 Rappel FScore .65 .77 nous avons adoptée toute l’information extraite apparaît nécessairement dans les résumés finaux. Par ailleurs, nous adoptons les critères définis par (Hartley et al. 2001) pour l’évaluation d’une application de génération multilingue et nous pouvons faire les constatations suivantes: L’acceptabilité des résumés produites est optimale dans la mesure où la combinatoire des expressions linguistiques possibles est contrôlée et prévue à priori dans la grammaire MDA. Pour ces mêmes raisons le critère de grammaticalité des résumés obtenus est également optimale. Enfin, la notion de couverture est en fait un reflet des résultats de la phrase d’extraction d’information, puisque tout prédicat extrait dans la première phase aura une réalisation en langue naturelle. Le tra"
2004.jeptalnrecital-long.15,W00-1404,1,0.791706,"Missing"
2004.jeptalnrecital-long.15,2003.jeptalnrecital-long.13,1,0.519437,"Missing"
2008.jeptalnrecital-long.25,W02-1109,0,0.0443786,"Missing"
2008.jeptalnrecital-long.27,A97-1012,0,0.0549443,"Missing"
2008.jeptalnrecital-long.27,2002.jeptalnrecital-long.5,0,0.126154,"Missing"
2008.jeptalnrecital-long.27,S07-1109,1,0.884037,"Missing"
2008.jeptalnrecital-long.27,2003.jeptalnrecital-long.13,0,0.122142,"Missing"
2009.jeptalnrecital-long.26,S07-1109,1,0.87275,"Missing"
2009.jeptalnrecital-long.26,2002.jeptalnrecital-long.17,0,0.127398,"Missing"
2009.jeptalnrecital-long.26,P98-2164,0,0.0966954,"Missing"
2009.jeptalnrecital-long.26,A00-2009,0,0.0611593,"Missing"
2010.jeptalnrecital-court.20,S07-1109,1,0.885924,"Missing"
2011.jeptalnrecital-court.46,N07-1039,0,0.0387486,"Missing"
2011.jeptalnrecital-court.46,esuli-sebastiani-2006-sentiwordnet,0,0.0876625,"Missing"
2011.jeptalnrecital-court.46,W02-1011,0,0.0131084,"Missing"
2011.jeptalnrecital-court.46,H05-1043,0,0.145393,"Missing"
2015.jeptalnrecital-court.34,W01-1819,1,0.677662,"Missing"
2015.jeptalnrecital-court.34,C12-2017,1,0.86233,"Missing"
2015.jeptalnrecital-court.34,S14-2076,0,0.0558988,"Missing"
2015.jeptalnrecital-court.34,W10-0204,0,0.087475,"Missing"
2015.jeptalnrecital-court.34,S14-2038,0,0.0362529,"Missing"
2015.jeptalnrecital-court.34,S14-2036,0,0.0472768,"Missing"
2018.jeptalnrecital-court.39,W12-3709,0,0.0595569,"Missing"
2018.jeptalnrecital-court.39,balahur-etal-2014-resource,0,0.0254524,"Missing"
2018.jeptalnrecital-court.39,banea-etal-2008-bootstrapping,0,0.103654,"Missing"
2018.jeptalnrecital-court.39,S16-1044,1,0.887613,"Missing"
2018.jeptalnrecital-court.39,N13-1073,0,0.0330903,"Missing"
2018.jeptalnrecital-court.39,P08-4006,0,0.0192652,"Missing"
2018.jeptalnrecital-court.39,S15-2128,0,0.0612622,"Missing"
2018.jeptalnrecital-court.39,D17-1263,0,0.0315199,"Missing"
2018.jeptalnrecital-court.39,S14-2076,0,0.0695289,"Missing"
2018.jeptalnrecital-court.39,S16-1174,0,0.0514102,"Missing"
2018.jeptalnrecital-court.39,P07-1123,0,0.128589,"Missing"
2018.jeptalnrecital-court.39,J03-1002,0,0.00966314,"Missing"
2018.jeptalnrecital-court.39,S16-1002,0,0.0775628,"Missing"
2018.jeptalnrecital-court.39,S14-2038,0,0.0679031,"Missing"
2018.jeptalnrecital-court.39,S14-2036,0,0.0608222,"Missing"
2018.jeptalnrecital-court.39,P09-1027,0,0.128348,"Missing"
2021.emnlp-demo.13,W06-0901,0,0.0435747,"Missing"
2021.emnlp-demo.13,C18-1139,0,0.0229882,"Missing"
2021.emnlp-demo.13,P19-1279,0,0.0316276,"Missing"
2021.emnlp-demo.13,D19-5719,0,0.0271156,"Missing"
2021.emnlp-demo.13,P18-1147,0,0.0313219,"Missing"
2021.emnlp-demo.13,2021.ccl-1.108,0,0.019499,"Missing"
2021.emnlp-demo.13,N15-1054,0,0.0585745,"Missing"
2021.emnlp-demo.13,E12-2021,0,0.120543,"Missing"
2021.emnlp-demo.13,D19-5723,0,0.0257897,"Missing"
2021.emnlp-demo.13,2020.emnlp-main.523,0,0.0263971,"Missing"
2021.emnlp-demo.13,D19-5716,0,0.0246629,"Missing"
2021.emnlp-demo.13,2020.coling-main.7,0,0.0279385,"lied is absent or because the query keywords are not on a real POI comment. matched, which can be frustrating due to the waste The proposed IE task goes beyond traditional of time. Named Entity Recognition (NER), which merely Finally, a few sites propose pre-selected flat cat- assigns semantic categories to entity mentions (Akegories for navigation, as illustrated in figure 3. bik et al., 2018; Pawar et al., 2017), even when the This method provides fast access to information set of categories is fine-grained and hierarchical by clicking on the categories, and improves pre- (Mai et al., 2018; Zhang et al., 2020). In contrast, selected term navigation. However, since the cat- SCP labels also contain information on the semanegories are not structured, they can be ambiguous. tic content of the entities or concepts within the 107 4 Figure 4: An example of SCP tagging. The semantic navigation system has two main components: the IE component, which identifies mentions and labels them with SCPs off-line, and a navigation component, which allows exploring the reviews based on the structured classes. We describe these two components in the following subsections. 4.1 mentions, but at the same time also on thei"
C00-1020,W97-0810,0,0.0873414,"Missing"
C00-1020,P98-1051,0,0.0275041,"Missing"
C00-1020,P91-1034,0,0.0926353,"Missing"
C00-1020,H93-1051,0,0.0854775,"Missing"
C00-1020,P96-1006,0,0.0802246,"Missing"
C00-1020,W97-0213,0,0.0617704,"Missing"
C00-1020,J98-1001,0,0.0284053,"Missing"
C00-1020,P95-1026,0,0.0745653,"Missing"
C00-1020,P98-2228,0,0.0297172,"Missing"
C00-1020,C90-2067,0,\N,Missing
C00-1020,C98-2223,0,\N,Missing
C00-1020,C98-1050,0,\N,Missing
C12-2017,baccianella-etal-2010-sentiwordnet,0,0.0320881,"Missing"
C12-2017,N07-1039,0,0.191025,"Missing"
C12-2017,R11-1054,1,0.812554,"Missing"
C12-2017,W02-1011,0,0.0138719,"Missing"
C12-2017,W03-1014,0,0.166454,"Missing"
C12-2017,H05-1044,0,0.0555438,"o WordNet synsets. Many works try to classify polar adjectives, like for example (Vegnaduzzo 2004) who proposes a distributional method to classify polarity adjective using a small seed of polar adjectives. For French, (Vernier and Monceaux 2010) present a learning method relying on the indexing of Web documents by a search engine and large number of linguistically motivated requests automatically sent. There is considerably less attempts to address the problem of associating polarities to larger expressions, and in particular pairs of words in a given syntactic relation, as we propose here. (Wilson et al. 2005), noticed that polar vocabulary have a “prior polarity” that can change according to the context (negation, diminishers such as “little”, “less”,etc). They learn such contexts by performing classification using various features and an annotated corpus. In the present paper, we focus on different kind of patterns (noun-adj) and also use a different methodology since we only use the marks given to reviews by users and data automatically annotated with our rule-based system to perform the clustering step. (Riloff et al. 2003) propose a bootstrapping process that learns linguistically rich extract"
C14-1166,J92-4003,0,0.0595589,"ed ANR project called Imagiweb, whose goal is to develop tools to analyse the brand image of entities (persons or companies) on social media. More specifically, one of the focus of the project is to analyse the brand image of politicians on Twitter. Therefore, data about the two main candidates (F. Hollande and N. Sarkozy) in the last French presidential election in May 2012 have been crawled from Twitter, using Twitter API, from 6 months before to 6 months after the elections. Our unlabeled Twitter data is a sub-set of this corpus. We obtained hierarchical word clusters via Brown Clustering (Brown et al., 1992) on a large set of unlabeled tweets. This algorithm generates a hard clustering, each word belongs to exactly one cluster. The input to the algorithm is a sequence of words wi , . . . , wn . Initially, the algorithm starts with each word in its own cluster. As long as there are at least two clusters left, the algorithm merges the two clusters that maximize the resulting cluster quality. The quality is defined on the class-based bigram language model as follows, where C maps a word w to its class C(w). p(wi |w1 , . . . , wi−1 ) = p(C(wi )|C(wi−1 ))p(wi |C(wi )) We ended up with 500 clusters (th"
C14-1166,W09-3821,0,0.141344,"Missing"
C14-1166,N13-1037,0,0.0702957,"Missing"
C14-1166,W10-0713,0,0.0205691,"the language that the technology expects. For example, (Han and Baldwin, 2011) propose the lexical normalization of short text messages, such as tweets, based on string and distributional similarity. They describe a method to identify and normalize ill-formed words. Word similarity and context are exploited to select the best candidate for noisy tokens. 2.2 Domain adaptation The other approach is instead to adapt the tools to fit the text. A series of papers has followed the mold of ”NLP for Twitter,” including POS tagging (Gimpel et al., 2011; Owoputi et al., 2013), named entity recognition (Finin et al., 2010; Ritter et al., 2011; Xiaohua et al., 2011), parsing (Foster et al., 2011), dialog modeling (Ritter et al., 2010) and summarization (Hutton and Kalita, 2010). These works adapt various parts of the natural language processing pipeline for social media text, and make use of a range of techniques (Preprocessing, New labeled data, New annotation schemes, Self training, Distributional features, Distance supervision) (Eisenstein, 2013). Recently, Seddah et al. (2012) followed the second approach on French social media content and provided new labeled data and annotation schemes. They applied the M"
C14-1166,P05-1045,0,0.0394001,"Missing"
C14-1166,I11-1100,0,0.109152,"Missing"
C14-1166,P11-2008,0,0.268256,"Missing"
C14-1166,P11-1038,0,0.0322358,"atterns. These are the result of unintentional errors, dialectal variation, conversational ellipsis, topic diversity, and creative use of language and orthography (Eisenstein, 2013) The language technology research community proposes two approaches to deal with noisy texts, namely normalization and domain adaptation, which are briefly described here. 2.1 Normalization One way to deal with ill-formed language is to turn it into a well-formed language as a pre-processing task: ”normalizing” social media or SMS messages to better conform to the language that the technology expects. For example, (Han and Baldwin, 2011) propose the lexical normalization of short text messages, such as tweets, based on string and distributional similarity. They describe a method to identify and normalize ill-formed words. Word similarity and context are exploited to select the best candidate for noisy tokens. 2.2 Domain adaptation The other approach is instead to adapt the tools to fit the text. A series of papers has followed the mold of ”NLP for Twitter,” including POS tagging (Gimpel et al., 2011; Owoputi et al., 2013), named entity recognition (Finin et al., 2010; Ritter et al., 2011; Xiaohua et al., 2011), parsing (Foste"
C14-1166,N10-1100,0,0.0303963,"based on string and distributional similarity. They describe a method to identify and normalize ill-formed words. Word similarity and context are exploited to select the best candidate for noisy tokens. 2.2 Domain adaptation The other approach is instead to adapt the tools to fit the text. A series of papers has followed the mold of ”NLP for Twitter,” including POS tagging (Gimpel et al., 2011; Owoputi et al., 2013), named entity recognition (Finin et al., 2010; Ritter et al., 2011; Xiaohua et al., 2011), parsing (Foster et al., 2011), dialog modeling (Ritter et al., 2010) and summarization (Hutton and Kalita, 2010). These works adapt various parts of the natural language processing pipeline for social media text, and make use of a range of techniques (Preprocessing, New labeled data, New annotation schemes, Self training, Distributional features, Distance supervision) (Eisenstein, 2013). Recently, Seddah et al. (2012) followed the second approach on French social media content and provided new labeled data and annotation schemes. They applied the MElt POS tagger (Denis and Sagot, 2012) embedded within text normalization and correction to noisy user generated texts and presented baseline POS tagging and"
C14-1166,P10-1052,0,0.18459,"Missing"
C14-1166,J93-2004,0,0.0484176,"Missing"
C14-1166,N04-1043,0,0.0538799,"”@XXXxxx9xxx”) . Return a vector of Unicode matching of the string wi (e.g., ”@DJRyan1der” → ”[64 − 68 − 74 − 82 − 121 − 97 − 110 − 49 − 100 − 101 − 114]”) . Return the first n character of x (n-gram prefix), where 1 ≤ n ≤ 10 . Return the last n character of x (n-gram suffix), where 1 ≤ n ≤ 10 5.2 Word Clustering Feature Templates To bridge the gap between high and low frequency words, we employed word clustering to acquire knowledge about paradigmatic lexical relations from large-scale texts. Our work is inspired by the suc1767 cessful application of word clustering in supervised NLP models (Miller et al., 2004; Turian et al., 2010; Ritter et al., 2011; Owoputi et al., 2013). Various clustering techniques have been proposed, some of which, for example, perform automatic word clustering optimizing a maximum likelihood criterion with iterative clustering algorithms. In this work, we focus on distributional word clustering, based on the assumption that the words that appear in similar contexts (especially surrounding words) tend to have similar meanings. 5.2.1 Brown Clustering We used our unlabeled Twitter corpus (4M tweets) to improve our tagger performance. This corpus has been extracted in the frame"
C14-1166,N13-1039,0,0.0708227,"Missing"
C14-1166,W96-0213,0,0.597095,"ext. Therefore, without being ambiguous, some words are usually abbreviated with a special spelling. For example, c t usually means c’´etait (it was); qil denotes qu’ il (that it/he). Our tagger is based on sequence labeling models (CRF), enabling arbitrary local features to be integrated into a log-linear model. We employed three categories of feature templates to deal with syntactic variations on social media contents and alleviating the data sparseness problem. 5.1 Basic Feature Templates The feature templates we use here are a superset of the largely language independent features used by (Ratnaparkhi, 1996; Toutanova and Manning, 2000; Toutanova et al., 2003). These features fall into two main categories. A first set of features tries to capture the lexical form of the word being tagged: it includes prefixes and suffixes (of at most 10 characters) from the current word, together with binary features based on the presence of special characters such as numbers, hyphens, and uppercase letters, within wi . A second set of features directly models the context of the current word and tag: it includes the previous tag, surrounding word forms in a 5 tokens window. The detailed list of feature templates"
C14-1166,N10-1020,0,0.0348693,"of short text messages, such as tweets, based on string and distributional similarity. They describe a method to identify and normalize ill-formed words. Word similarity and context are exploited to select the best candidate for noisy tokens. 2.2 Domain adaptation The other approach is instead to adapt the tools to fit the text. A series of papers has followed the mold of ”NLP for Twitter,” including POS tagging (Gimpel et al., 2011; Owoputi et al., 2013), named entity recognition (Finin et al., 2010; Ritter et al., 2011; Xiaohua et al., 2011), parsing (Foster et al., 2011), dialog modeling (Ritter et al., 2010) and summarization (Hutton and Kalita, 2010). These works adapt various parts of the natural language processing pipeline for social media text, and make use of a range of techniques (Preprocessing, New labeled data, New annotation schemes, Self training, Distributional features, Distance supervision) (Eisenstein, 2013). Recently, Seddah et al. (2012) followed the second approach on French social media content and provided new labeled data and annotation schemes. They applied the MElt POS tagger (Denis and Sagot, 2012) embedded within text normalization and correction to noisy user generated t"
C14-1166,D11-1141,0,0.598568,"e technology expects. For example, (Han and Baldwin, 2011) propose the lexical normalization of short text messages, such as tweets, based on string and distributional similarity. They describe a method to identify and normalize ill-formed words. Word similarity and context are exploited to select the best candidate for noisy tokens. 2.2 Domain adaptation The other approach is instead to adapt the tools to fit the text. A series of papers has followed the mold of ”NLP for Twitter,” including POS tagging (Gimpel et al., 2011; Owoputi et al., 2013), named entity recognition (Finin et al., 2010; Ritter et al., 2011; Xiaohua et al., 2011), parsing (Foster et al., 2011), dialog modeling (Ritter et al., 2010) and summarization (Hutton and Kalita, 2010). These works adapt various parts of the natural language processing pipeline for social media text, and make use of a range of techniques (Preprocessing, New labeled data, New annotation schemes, Self training, Distributional features, Distance supervision) (Eisenstein, 2013). Recently, Seddah et al. (2012) followed the second approach on French social media content and provided new labeled data and annotation schemes. They applied the MElt POS tagger (Denis"
C14-1166,C12-1149,0,0.416995,"ational Linguistics: Technical Papers, pages 1764–1772, Dublin, Ireland, August 23-29 2014. words, we have applied unsupervised techniques to enrich the feature set. Finally, we have evaluated our tagger performance with different configurations on annotated corpora from French social media. We will first present related work in Part-Of-Speech tagging (Section 2) on noisy data like social media content. In Section 3, the annotated dataset and its characteristics (e.g., tag set) are described. Section 4 presents the result of applying the MElt POS tagger to user generated text as our baseline (Seddah et al., 2012). In Section 5, we explain how we design and implement our POS tagger. Section 6 is devoted to experiments and performance of our tagger. Section 7 describes the evaluation of the new tagger on English social media texts. Conclusion and future work are given in Section 8. 2 Related work Online conversational texts, typified by micro-blogs, chat, and text messages, are a challenge for natural language processing. Unlike the highly edited genres for which conventional NLP tools have been developed, conversational texts contain many non-standard lexical items and syntactic patterns. These are the"
C14-1166,W00-1308,0,0.415761,"hout being ambiguous, some words are usually abbreviated with a special spelling. For example, c t usually means c’´etait (it was); qil denotes qu’ il (that it/he). Our tagger is based on sequence labeling models (CRF), enabling arbitrary local features to be integrated into a log-linear model. We employed three categories of feature templates to deal with syntactic variations on social media contents and alleviating the data sparseness problem. 5.1 Basic Feature Templates The feature templates we use here are a superset of the largely language independent features used by (Ratnaparkhi, 1996; Toutanova and Manning, 2000; Toutanova et al., 2003). These features fall into two main categories. A first set of features tries to capture the lexical form of the word being tagged: it includes prefixes and suffixes (of at most 10 characters) from the current word, together with binary features based on the presence of special characters such as numbers, hyphens, and uppercase letters, within wi . A second set of features directly models the context of the current word and tag: it includes the previous tag, surrounding word forms in a 5 tokens window. The detailed list of feature templates we used in this category is"
C14-1166,N03-1033,0,0.151533,"rds are usually abbreviated with a special spelling. For example, c t usually means c’´etait (it was); qil denotes qu’ il (that it/he). Our tagger is based on sequence labeling models (CRF), enabling arbitrary local features to be integrated into a log-linear model. We employed three categories of feature templates to deal with syntactic variations on social media contents and alleviating the data sparseness problem. 5.1 Basic Feature Templates The feature templates we use here are a superset of the largely language independent features used by (Ratnaparkhi, 1996; Toutanova and Manning, 2000; Toutanova et al., 2003). These features fall into two main categories. A first set of features tries to capture the lexical form of the word being tagged: it includes prefixes and suffixes (of at most 10 characters) from the current word, together with binary features based on the presence of special characters such as numbers, hyphens, and uppercase letters, within wi . A second set of features directly models the context of the current word and tag: it includes the previous tag, surrounding word forms in a 5 tokens window. The detailed list of feature templates we used in this category is shown in Table 2.1 Contex"
C14-1166,P09-1054,0,0.0236546,"els. It is based on maxent models, maximum entropy Markov models and linear-chain CRF and proposes various optimization and regularization methods to improve both the computational complexity and the prediction performance of standard models. Wapiti has been ranked first on the sequence tagging task for more than a year on MLcomp5 web site. 6.1 Training and parameter regularization In the training of log-linear models, regularization is normally required to prevent the model from over fitting on the training data. The two most common regularization methods are called L1 and L2 regularization (Tsuruoka et al., 2009). Wapiti uses the elastic-net penalty of the form: ρ2 ρ1 ∗ |θ|1 + ∗ ||θ||22 2 and it is implemented with 3 different algorithms: Orthant-Wise Limited-memory Quasi-Newton (OWLQN: L-BFGS), Stochastic Gradient Descent (SGD) and Block Coordinate Descent. We trained with L-BFGS, a classical Quasi-Newton optimization algorithm with limited memory which minimizes the regularized objective and uses elastic net regularization. Using even a very small L1 penalty excludes many irrelevant or highly noisy features. We carried out a grid search for the regularization values, assessing with F-measure and acc"
C14-1166,P10-1040,0,0.0204093,"rn a vector of Unicode matching of the string wi (e.g., ”@DJRyan1der” → ”[64 − 68 − 74 − 82 − 121 − 97 − 110 − 49 − 100 − 101 − 114]”) . Return the first n character of x (n-gram prefix), where 1 ≤ n ≤ 10 . Return the last n character of x (n-gram suffix), where 1 ≤ n ≤ 10 5.2 Word Clustering Feature Templates To bridge the gap between high and low frequency words, we employed word clustering to acquire knowledge about paradigmatic lexical relations from large-scale texts. Our work is inspired by the suc1767 cessful application of word clustering in supervised NLP models (Miller et al., 2004; Turian et al., 2010; Ritter et al., 2011; Owoputi et al., 2013). Various clustering techniques have been proposed, some of which, for example, perform automatic word clustering optimizing a maximum likelihood criterion with iterative clustering algorithms. In this work, we focus on distributional word clustering, based on the assumption that the words that appear in similar contexts (especially surrounding words) tend to have similar meanings. 5.2.1 Brown Clustering We used our unlabeled Twitter corpus (4M tweets) to improve our tagger performance. This corpus has been extracted in the framework of a French gove"
C14-1166,P11-1037,0,0.0233483,"For example, (Han and Baldwin, 2011) propose the lexical normalization of short text messages, such as tweets, based on string and distributional similarity. They describe a method to identify and normalize ill-formed words. Word similarity and context are exploited to select the best candidate for noisy tokens. 2.2 Domain adaptation The other approach is instead to adapt the tools to fit the text. A series of papers has followed the mold of ”NLP for Twitter,” including POS tagging (Gimpel et al., 2011; Owoputi et al., 2013), named entity recognition (Finin et al., 2010; Ritter et al., 2011; Xiaohua et al., 2011), parsing (Foster et al., 2011), dialog modeling (Ritter et al., 2010) and summarization (Hutton and Kalita, 2010). These works adapt various parts of the natural language processing pipeline for social media text, and make use of a range of techniques (Preprocessing, New labeled data, New annotation schemes, Self training, Distributional features, Distance supervision) (Eisenstein, 2013). Recently, Seddah et al. (2012) followed the second approach on French social media content and provided new labeled data and annotation schemes. They applied the MElt POS tagger (Denis and Sagot, 2012) embed"
C98-1030,E95-1021,0,0.0131296,"Linguistic Environment), (Maxwell and Kaplan, 1996), which provides automatic parsing and generation, as well as an interface to the preprocessing tools we are describing. 2 Terminology Extraction The first stage of this work was to extract terminology from our corpus. This corpus is a small French technical text of 742 sentences (7000 words). As we have at our disposal parallel aligned English/French texts, we use the English translation to decide when a potential term is actually a term. The terminology we are dealing with is mainly nominal. To perform this extraction task, we use a tagger (Chanod and Tapanainen, 1995) to disambiguate the French text, and then extract the following syntactic patterns, N Prep N, N N , N A, A N, which are good candidates to be terms. These candidates are considered as terms when the corresponding English translation is a unit, or when their translation differs from a word to word translation. For example, we extract the following terms: In this particular case, each word is a token. But several words can be a unit, for example compounds, or multiword expressions. Here are some examples of the desired tokenization, where terms are treated as units: (3) (1) vitesses rampantes ("
C98-1030,C92-1025,0,0.0156972,"gs of the second word. The two morphological analyzers for the two variations are both unioned into the basic morphological analyzer for French we use for morphology. The result is the transducer we use following tokenization and completing input preprocessing. An example of compound analysis is given here: • B o t h parts vary in n u m b e r : roue motrice, roues motrices This is of course not general for French compounds; there are other variation patterns, however it is reliable enough for the technical manual we are dealing with. Other inflectional schemes and exceptions are described in (Kartunnen et al., 1992) and (Quint, 1997), and can be easily added to the regular grammar if needed. A cascade of regular rules is applied on the different parts of the compound to build the morphological analyzer of the whole compound. For example, roue motrice is marked with the diacritic +DPL, for double plural and then, a first rule which just copies the morphological tags from the end to the middle is applied if the diacritic is present in the right context: roue 0 0 -motrice+DPL+Fem+PL roue+Fern+PL-motrice 0 +Fem+PL Figure 1: First rule A second rule is applied to the output of the preceding one and &quot;realizes&quot;"
C98-1030,C94-1066,0,0.0316535,"Missing"
D15-1130,W01-1819,0,0.0138749,"Missing"
D15-1130,D11-1033,0,0.0202411,"t) is a marker for the Agreeableness trait. These different forms do not naturally translate into other languages. It is doubtful that even a human translator would always pay attention to such subtleties. In investigating whether these cues are preserved when a text is translated, we are also beginning to address the question of consistency in cues between languages. MT systems do not explicitly consider demographic or personality traits. Instead, they often exploit “indomain” data to create translation models that are adapted for the domain of interest (Lu et al., 2007; Foster et al., 2010; Axelrod et al., 2011; Gong et al., 2012; Mirkin and Besacier, 2014). The term “domain” has a wide interpretation in the MT literature and may refer to topic, dialect, genre or style (Chen et al., 2013). However, to the best of our knowledge, MT domain adaptation does not extend to consider demographic or personality traits of the author. Gender in translation has been researched extensively; in human translation studies, it has been shown that the gender of translators impact the translation. In SMT, phrase-based models (Koehn et al., 2003) can correctly pick-up translations of gender-inflected words, and rule-ba"
D15-1130,W12-3709,0,0.033159,"is work was mostly done while the first author was at Xerox Research Centre Europe. labelling is time consuming, requiring the completion of psychometric questionnaires which may be considered invasive by many. An alternative is the use of machine translation (MT) to bootstrap corpora in resource poor languages, and to translate the user’s content into a single language before modeling. Translated text, either manually or automatically generated, is known to have different characteristics than native text. Yet, MT was shown to be of use within traditional NLP tasks such as sentiment analysis (Balahur and Turchi, 2012). We explore the utility of MT for classification of demographic and personality traits. MT models, even domain-specific, are user-generic. Thus, the linguistic signals of user traits which are conveyed in the original language may not be preserved over translation. In other words, the attributes on which we wish to rely for modelling may be lost. This concern is perhaps most observable with gender, a trait of the speaker that is encoded in the morphology of many languages, though not in English. Gendered translation was the topic of research for many years. However, the gender of the author i"
D15-1130,2012.eamt-1.60,0,0.0608977,"Missing"
D15-1130,2014.iwslt-evaluation.1,0,0.0579022,"Missing"
D15-1130,P96-1041,0,0.285608,"-source phrase-based MT system,3 was used to train translation models and translate the data. 2 Version 7, www.statmt.org/europarl We used version 3.0, downloaded on 16 Feb 2015 from www.statmt.org/moses. 3 Preprocessing We used the standard Moses tools to preprocess the data, including tokenization, lowercasing and removal of sentence pairs where at least one of the sentences is empty or longer than 80 tokens. Recasing and Language models We used SRILM (Stolcke, 2002) version 1.7.1 to train 5gram language models on the target side of the parallel corpus, with modified Kneser-Ney discounting (Chen and Goodman, 1996). A recasing model was trained from the same corpus, with a 3-gram KenLM language model (Heafield, 2011). Tuning We tuned the translation models using MERT (Och, 2003), using the development set of the above mentioned campaign (dev2010), consisting of 887 sentence pairs for each language pair. Translation and post-processing Each of the tweets of the PAN training set was preprocessed in the same fashion as the training data. It was then translated with the trained model of the corresponding language pair, and finally underwent quick post-processing, namely recasing and detokenization. 4 Data P"
D15-1130,P13-1126,0,0.0153437,"on to such subtleties. In investigating whether these cues are preserved when a text is translated, we are also beginning to address the question of consistency in cues between languages. MT systems do not explicitly consider demographic or personality traits. Instead, they often exploit “indomain” data to create translation models that are adapted for the domain of interest (Lu et al., 2007; Foster et al., 2010; Axelrod et al., 2011; Gong et al., 2012; Mirkin and Besacier, 2014). The term “domain” has a wide interpretation in the MT literature and may refer to topic, dialect, genre or style (Chen et al., 2013). However, to the best of our knowledge, MT domain adaptation does not extend to consider demographic or personality traits of the author. Gender in translation has been researched extensively; in human translation studies, it has been shown that the gender of translators impact the translation. In SMT, phrase-based models (Koehn et al., 2003) can correctly pick-up translations of gender-inflected words, and rule-based MT systems and factored models (Koehn and Hoang, 2007) provide more explicit ways for gender translation. Yet, most SMT systems are unaware of the gender of the author, neither"
D15-1130,D13-1114,0,0.0296783,"d 29 as female. TED61en In order to account for any potential effect of length, we created a subset of the en-fr corpus, that is of the same size of the fr-en dataset. We matched files from the French side of the en-fr corpus to each of those in the fr-en for gender and length (in tokens). The French en-fr files were truncated after the nearest line break to the desired size; the corresponding English en-fr files were truncated at the same point. 5 Experiments It has been shown that standard approaches to gender classification on English texts can be sub-optimal for non-English language data (Ciot et al., 2013). However, state-of-the-art classification results are not our focus; rather, our intention is to understand the impact 5 Annotation data is available at cm.xrce.xerox.com www.ted.com/watch/tedx-talks 7 developers.google.com/youtube 8 Accessed on 23/2/15. 6 of translation on classification of socio-demographic and personality traits. Therefore, we fix our models with a set of parameters selected via cross-validation (CV) on the native language: the occurrence threshold is set to 5 and the SVD dimensionality to 500. 5.1 PAN For each of the three non-English languages of PAN 15 we train classifi"
D15-1130,D10-1044,0,0.0108077,"(e.g. don’t vs. do not) is a marker for the Agreeableness trait. These different forms do not naturally translate into other languages. It is doubtful that even a human translator would always pay attention to such subtleties. In investigating whether these cues are preserved when a text is translated, we are also beginning to address the question of consistency in cues between languages. MT systems do not explicitly consider demographic or personality traits. Instead, they often exploit “indomain” data to create translation models that are adapted for the domain of interest (Lu et al., 2007; Foster et al., 2010; Axelrod et al., 2011; Gong et al., 2012; Mirkin and Besacier, 2014). The term “domain” has a wide interpretation in the MT literature and may refer to topic, dialect, genre or style (Chen et al., 2013). However, to the best of our knowledge, MT domain adaptation does not extend to consider demographic or personality traits of the author. Gender in translation has been researched extensively; in human translation studies, it has been shown that the gender of translators impact the translation. In SMT, phrase-based models (Koehn et al., 2003) can correctly pick-up translations of gender-inflec"
D15-1130,W12-1508,0,0.027667,"uages. Hovy (2015) extends this work to other languages and NLP tasks. Using demographically-informed word embeddings, they show improvements in sentiment analysis, topic classification and trait detection. None of these works, however, addressed cross-lingual issues. Yet, personality projection goes beyond automatic detection of traits – there is also human perception to be considered. The casual reader may not be aware of personality related linguistic cues. Yet, studies have shown that traits can be reliably detected following cold readings of texts from unknown authors (Mehl et al., 2006; Gill et al., 2012) without such explicit knowledge. Although personality projection in different languages is under-explored, it has been shown that the relationship between language use and personality traits varies between domains (Nowson and Gill, 2014). Thus, while it would seem that there are cues which translate directly between languages, this may not always be the case. In English, for example, women tend to use firstperson pronouns such as “I ” more than men (Newman et al., 2008); but this does not guarantee a gender-based usage difference for, say, “je” in French. Furthermore, what happens with more s"
D15-1130,2012.iwslt-papers.20,0,0.0168403,"Agreeableness trait. These different forms do not naturally translate into other languages. It is doubtful that even a human translator would always pay attention to such subtleties. In investigating whether these cues are preserved when a text is translated, we are also beginning to address the question of consistency in cues between languages. MT systems do not explicitly consider demographic or personality traits. Instead, they often exploit “indomain” data to create translation models that are adapted for the domain of interest (Lu et al., 2007; Foster et al., 2010; Axelrod et al., 2011; Gong et al., 2012; Mirkin and Besacier, 2014). The term “domain” has a wide interpretation in the MT literature and may refer to topic, dialect, genre or style (Chen et al., 2013). However, to the best of our knowledge, MT domain adaptation does not extend to consider demographic or personality traits of the author. Gender in translation has been researched extensively; in human translation studies, it has been shown that the gender of translators impact the translation. In SMT, phrase-based models (Koehn et al., 2003) can correctly pick-up translations of gender-inflected words, and rule-based MT systems and"
D15-1130,D12-1039,0,0.0147191,"(see Section 4.2). The full details of our approach to text processing, translation and classification can be found in our technical paper at the PAN workshop (Nowson et al., 2015); in the interests of space a compressed version is presented here. 1103 3.3 Preprocessing and feature extraction We use the multilingual parser described by AitMokhtar et al. (2001) to preprocess the texts and extract a wide range of features. The parser has been customized to handle social media data, e.g. by detecting hashtags, mentions, and emoticons. For English, we have integrated a normalization dictionary by Han et al. (2012) in the preprocessing. The English and French grammars also include a polarity lexicon to recognize sentiment bearing words or expressions. The features we extract include: 1-, 2-, 3-grams of surface, normalized and lemmatized forms; part-of-speech tagged forms, and n-grams of POS; named entities (places, persons, organization, dates, time expressions), emoticons, hashtags, mentions and URLs. 3.4 Learning framework To train classification models we first prune features with a frequency threshold. Next, the remaining set of features is compressed using truncated singular value decomposition (SV"
D15-1130,W11-2123,0,0.0187669,"tatmt.org/europarl We used version 3.0, downloaded on 16 Feb 2015 from www.statmt.org/moses. 3 Preprocessing We used the standard Moses tools to preprocess the data, including tokenization, lowercasing and removal of sentence pairs where at least one of the sentences is empty or longer than 80 tokens. Recasing and Language models We used SRILM (Stolcke, 2002) version 1.7.1 to train 5gram language models on the target side of the parallel corpus, with modified Kneser-Ney discounting (Chen and Goodman, 1996). A recasing model was trained from the same corpus, with a 3-gram KenLM language model (Heafield, 2011). Tuning We tuned the translation models using MERT (Och, 2003), using the development set of the above mentioned campaign (dev2010), consisting of 887 sentence pairs for each language pair. Translation and post-processing Each of the tweets of the PAN training set was preprocessed in the same fashion as the training data. It was then translated with the trained model of the corresponding language pair, and finally underwent quick post-processing, namely recasing and detokenization. 4 Data Personality-tagged datasets in multiple languages are scarce. We used two datasets, with content from twi"
D15-1130,P15-1073,0,0.0285892,"nslation was the topic of research for many years. However, the gender of the author is largely ignored by MT systems, and specifically statistical ones, that would often arbitrarily (or rather statistically-based) translate into one gender form or another. Other demographic and personality traits have not yet been investigated. One way to address this concern is personalized translation, or author-aware translation.1 The first step toward this goal would be to consider the author traits in the model. Such an approach has already shown to be useful for several NLP tasks (Volkova et al., 2013; Hovy, 2015). However, before embarking on this challenging task, we explore if the above concerns are founded by addressing the research question: does MT has an impact on the classification of demographic and personality traits? 2 Background Oberlander and Nowson (2006) motivated their study of computational personality recognition by arguing that automatically understanding an author’s personality would permit the personalization of sentiment analysis. Such personalized NLP has recently been 1 In this work we investigate MT awareness of the author; in (Mirkin and Meunier, 2015) we address the task of r"
D15-1130,D07-1091,0,0.0112998,"cier, 2014). The term “domain” has a wide interpretation in the MT literature and may refer to topic, dialect, genre or style (Chen et al., 2013). However, to the best of our knowledge, MT domain adaptation does not extend to consider demographic or personality traits of the author. Gender in translation has been researched extensively; in human translation studies, it has been shown that the gender of translators impact the translation. In SMT, phrase-based models (Koehn et al., 2003) can correctly pick-up translations of gender-inflected words, and rule-based MT systems and factored models (Koehn and Hoang, 2007) provide more explicit ways for gender translation. Yet, most SMT systems are unaware of the gender of the author, neither in the training nor in the test data, and are therefore unable to adapt their translation beyond the local inflectional level; in particular when no such evidence exists, as in English. To a much greater extent, this is the case with other demographics, such as age, and with personality traits. 3 Methodology 3.1 Hypothesis The hypothesis of our broader vision is that personalized MT or author-aware translation is an important necessity. We believe the human understanding o"
D15-1130,N03-1017,0,0.0143422,"dapted for the domain of interest (Lu et al., 2007; Foster et al., 2010; Axelrod et al., 2011; Gong et al., 2012; Mirkin and Besacier, 2014). The term “domain” has a wide interpretation in the MT literature and may refer to topic, dialect, genre or style (Chen et al., 2013). However, to the best of our knowledge, MT domain adaptation does not extend to consider demographic or personality traits of the author. Gender in translation has been researched extensively; in human translation studies, it has been shown that the gender of translators impact the translation. In SMT, phrase-based models (Koehn et al., 2003) can correctly pick-up translations of gender-inflected words, and rule-based MT systems and factored models (Koehn and Hoang, 2007) provide more explicit ways for gender translation. Yet, most SMT systems are unaware of the gender of the author, neither in the training nor in the test data, and are therefore unable to adapt their translation beyond the local inflectional level; in particular when no such evidence exists, as in English. To a much greater extent, this is the case with other demographics, such as age, and with personality traits. 3 Methodology 3.1 Hypothesis The hypothesis of ou"
D15-1130,P07-2045,0,0.0359655,"Missing"
D15-1130,2005.mtsummit-papers.11,0,0.0741045,"Missing"
D15-1130,D07-1036,0,0.0117236,"of contractions (e.g. don’t vs. do not) is a marker for the Agreeableness trait. These different forms do not naturally translate into other languages. It is doubtful that even a human translator would always pay attention to such subtleties. In investigating whether these cues are preserved when a text is translated, we are also beginning to address the question of consistency in cues between languages. MT systems do not explicitly consider demographic or personality traits. Instead, they often exploit “indomain” data to create translation models that are adapted for the domain of interest (Lu et al., 2007; Foster et al., 2010; Axelrod et al., 2011; Gong et al., 2012; Mirkin and Besacier, 2014). The term “domain” has a wide interpretation in the MT literature and may refer to topic, dialect, genre or style (Chen et al., 2013). However, to the best of our knowledge, MT domain adaptation does not extend to consider demographic or personality traits of the author. Gender in translation has been researched extensively; in human translation studies, it has been shown that the gender of translators impact the translation. In SMT, phrase-based models (Koehn et al., 2003) can correctly pick-up translat"
D15-1130,2014.amta-researchers.23,1,0.855552,"t. These different forms do not naturally translate into other languages. It is doubtful that even a human translator would always pay attention to such subtleties. In investigating whether these cues are preserved when a text is translated, we are also beginning to address the question of consistency in cues between languages. MT systems do not explicitly consider demographic or personality traits. Instead, they often exploit “indomain” data to create translation models that are adapted for the domain of interest (Lu et al., 2007; Foster et al., 2010; Axelrod et al., 2011; Gong et al., 2012; Mirkin and Besacier, 2014). The term “domain” has a wide interpretation in the MT literature and may refer to topic, dialect, genre or style (Chen et al., 2013). However, to the best of our knowledge, MT domain adaptation does not extend to consider demographic or personality traits of the author. Gender in translation has been researched extensively; in human translation studies, it has been shown that the gender of translators impact the translation. In SMT, phrase-based models (Koehn et al., 2003) can correctly pick-up translations of gender-inflected words, and rule-based MT systems and factored models (Koehn and H"
D15-1130,D15-1238,1,0.7022,"veral NLP tasks (Volkova et al., 2013; Hovy, 2015). However, before embarking on this challenging task, we explore if the above concerns are founded by addressing the research question: does MT has an impact on the classification of demographic and personality traits? 2 Background Oberlander and Nowson (2006) motivated their study of computational personality recognition by arguing that automatically understanding an author’s personality would permit the personalization of sentiment analysis. Such personalized NLP has recently been 1 In this work we investigate MT awareness of the author; in (Mirkin and Meunier, 2015) we address the task of readeraware MT. 1102 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1102–1108, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational Linguistics. explored by Volkova et al. (2013). They incorporated age and gender features for sentiment analysis, and show improvements in three different languages. Hovy (2015) extends this work to other languages and NLP tasks. Using demographically-informed word embeddings, they show improvements in sentiment analysis, topic classification and trait detection. None of"
D15-1130,D13-1187,0,0.101509,"English. Gendered translation was the topic of research for many years. However, the gender of the author is largely ignored by MT systems, and specifically statistical ones, that would often arbitrarily (or rather statistically-based) translate into one gender form or another. Other demographic and personality traits have not yet been investigated. One way to address this concern is personalized translation, or author-aware translation.1 The first step toward this goal would be to consider the author traits in the model. Such an approach has already shown to be useful for several NLP tasks (Volkova et al., 2013; Hovy, 2015). However, before embarking on this challenging task, we explore if the above concerns are founded by addressing the research question: does MT has an impact on the classification of demographic and personality traits? 2 Background Oberlander and Nowson (2006) motivated their study of computational personality recognition by arguing that automatically understanding an author’s personality would permit the personalization of sentiment analysis. Such personalized NLP has recently been 1 In this work we investigate MT awareness of the author; in (Mirkin and Meunier, 2015) we address"
D15-1130,P06-2081,1,0.583243,"rm or another. Other demographic and personality traits have not yet been investigated. One way to address this concern is personalized translation, or author-aware translation.1 The first step toward this goal would be to consider the author traits in the model. Such an approach has already shown to be useful for several NLP tasks (Volkova et al., 2013; Hovy, 2015). However, before embarking on this challenging task, we explore if the above concerns are founded by addressing the research question: does MT has an impact on the classification of demographic and personality traits? 2 Background Oberlander and Nowson (2006) motivated their study of computational personality recognition by arguing that automatically understanding an author’s personality would permit the personalization of sentiment analysis. Such personalized NLP has recently been 1 In this work we investigate MT awareness of the author; in (Mirkin and Meunier, 2015) we address the task of readeraware MT. 1102 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1102–1108, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational Linguistics. explored by Volkova et al. (2013). They incor"
D15-1130,P03-1021,0,0.0395734,"om www.statmt.org/moses. 3 Preprocessing We used the standard Moses tools to preprocess the data, including tokenization, lowercasing and removal of sentence pairs where at least one of the sentences is empty or longer than 80 tokens. Recasing and Language models We used SRILM (Stolcke, 2002) version 1.7.1 to train 5gram language models on the target side of the parallel corpus, with modified Kneser-Ney discounting (Chen and Goodman, 1996). A recasing model was trained from the same corpus, with a 3-gram KenLM language model (Heafield, 2011). Tuning We tuned the translation models using MERT (Och, 2003), using the development set of the above mentioned campaign (dev2010), consisting of 887 sentence pairs for each language pair. Translation and post-processing Each of the tweets of the PAN training set was preprocessed in the same fashion as the training data. It was then translated with the trained model of the corresponding language pair, and finally underwent quick post-processing, namely recasing and detokenization. 4 Data Personality-tagged datasets in multiple languages are scarce. We used two datasets, with content from twitter and TED talks, as described in this section. 4.1 PAN The f"
D15-1130,P02-1040,0,0.0962447,"Missing"
E03-2003,C00-1036,1,0.780552,"s being discussed, we developed grammars for English as well as French realization, each containing about 380 rules. it I &apos;beneficiary Project I I from .s0,-,i0sweism Report: Inarne0fGene The first step of prototype design was to specify the structure and content of the experiment reports. With the help of the grammar writers, the biological experts produced guidelines, both at the level of semantic content and of the textual expressions to be used. It was then followed by DCM formalizing these descriptions and implementing them in the MDA formalism. Details about this formalism are given in (Dymetman et al. 2000) and (Brun et al. 2000). During the formalization and implementation phase, XRCE used its previously developed methodology of first modeling the document macro-structure (similar to a DTD 8 ), then its context-free micro-structure (what types of content choices are possible at a given point in the document), and finally the dependencies between different content elements (example: some experimental observations lead to certain obligatory choices concerning the sequel of the experiment). To perform this formalization/implementation phase, a biological expert and a grammar developer worked in ta"
E03-2003,C02-1128,1,0.788367,"e written report. It was then exciting to discover that the computational model underlying MDA was very adapted, not only to the description of the written report, but also to the fine-grained fbrmalization of the experimental protocol itself In this way, we have gradually moved to a view of MDA as a convenient tool for integrating the formalization of the This difference has several decisive theoretical and practical consequences, in particular for the connection between these systems and XML-based authoring, as well as for the definability of such notions as life/death of authoring choices (Dymetman 2002). 6 http://www.xrce.xerox.com/competencies/contentanalvsis/dcm/mda.en.html 7 http://www.xrce.xerox.com/comnetencies/contentanalysis/dcm/demo/mda-demo.html 5 Start experimental protocol with its associated textual documentation. ILI&Z X-C.I Intorface for Multilingual Document Authoring File Edit - Windows Traces Ir 0 English Edda hie — P 3 The realization 3 ene nmary: of the The following figures show some screenshots of the prototype in use. The author interacts with menus associated with underlined items and may also enter free text in dedicated boxes. 8 I IffIuT&apos;ID•f I • Bactena Ifi • Expre"
E03-2003,P98-2173,0,0.0470403,"Missing"
E03-2003,W00-1404,1,\N,Missing
E03-2003,C98-2168,0,\N,Missing
F12-2045,R11-1054,1,0.871678,"Missing"
F12-2045,D09-1062,0,0.0522502,"Missing"
F12-2045,2008.jeptalnrecital-court.10,1,0.772953,"Missing"
F12-2045,P97-1023,0,0.0781073,"Missing"
F12-2045,C04-1200,0,0.234436,"Missing"
F12-2045,W02-1011,0,0.022455,"Missing"
F12-2045,strapparava-valitutti-2004-wordnet,0,0.100864,"Missing"
F12-2045,J11-2001,0,0.0864517,"Missing"
F12-2045,P02-1053,0,0.0111525,"Missing"
F14-2015,W12-3704,0,0.0250525,"Missing"
F14-2015,R11-1054,1,0.888529,"Missing"
F14-2015,C12-2017,1,0.885661,"Missing"
F14-2015,C10-2028,0,0.112126,"Missing"
F14-2015,W11-0705,0,0.131027,"Missing"
F14-2015,maynard-greenwood-2014-cares,0,0.0529334,"Missing"
N12-3002,R11-1054,1,0.805398,"Missing"
N12-3002,N06-1026,0,0.0765461,"Missing"
P98-1030,E95-1021,0,0.0129303,"Linguistic Environment), (Maxwell and Kaplan, 1996), which provides automatic parsing and generation, as well as an interface to the preprocessing tools we are describing. 2 Terminology Extraction The first stage of this work was to extract terminology from our corpus. This corpus is a small French technical text of 742 sentences (7000 words). As we have at our disposal parallel aligned English/French texts, we use the English translation to decide when a potential term is actually a term. The terminology we are dealing with is mainly nominal. To perform this extraction task, we use a tagger (Chanod and Tapanainen, 1995) to disambiguate the French text, and then extract the following syntactic patterns, N Prep N, N N , N A, A N, which are good candidates to be terms. These candidates are considered as terms when the corresponding English translation is a unit, or when their translation differs from a word to word translation. For example, we extract the following terms: In this particular case, each word is a token. But several words can be a unit, for example compounds, or multiword expressions. Here are some examples of the desired tokenization, where terms are treated as units: (1) vitesses rampantes (cree"
P98-1030,C92-1025,0,0.0150408,"the parsing process, just after the tokenization process, is a two-level finite-state transducer (Chanod, 1994). This lexical transducer links the surface form of a string to its morphological analysis, i.e. its canonical form and some characterizing morphological tags. Some examples are given in 5. (5) &gt;veut vouloir+IndP+SG+P3+Verb &gt;animaux animal+Masc+PL+Noun animal+Masc+PL+Adj The compound terms have to be integrated into this transducer. This is done by developing a local regular grammar which describes the compound morphological variation, according to the inflectional model proposed in (Kartunnen et al., 1992). The hypothesis is that only the two main parts of the compounds are able to vary. i.e. N1 or A1, and N2 or A2. in the patterns .VI prep N2, N1 N2, A1 N2, and ,VI A2. In our corpus, we identify two kinds of morphological variations: • The first part varies in n u m b e r : gyrophare de toit. gyrophares de toit rdgime moteur, rggirnes moteur rule just deletes the tags of the second word. The two morphological analyzers for the two variations are both unioned into the basic morphological analyzer for French we use for morphology. The result is the transducer we use following tokenization and co"
P98-1030,C94-1066,0,0.0138674,"ot completely fixed, and there is an intuition that you won&apos;t loose many good anal~This integration has been done by Fr6d~rique Segond. 196 yses by treating them as single tokens. Moreover, terminology can be semi or fully automatically extracted. Our goal in the present paper is to compare efficiency and syntactic coverage of a French LFG grammar on a technical text, with and without terminology recognition in the preprocessing stage. The preprocessing consists mainly in two stages: tokenization and morphological analysis. Both stages are performed by use of finite-state lexical transducers (Kartunnen, 1994). In the following, we describe the insertion of terminology in these finite-state transducers, as well as the consequences of such an insertion on the syntactic analysis, in terms of number of valid analyses produced, parsing time and nature of the results. We are part of a project, which aims at developing LFG grammars, (Bresnan and Kaplan, 1982), in parallel for French, English and German, (Butt et al., To appear). The grammar is developed in a computational environment called XLE (Xerox Linguistic Environment), (Maxwell and Kaplan, 1996), which provides automatic parsing and generation, as"
R11-1054,esuli-sebastiani-2006-sentiwordnet,0,0.0230948,"Missing"
R11-1054,P97-1023,0,0.0244458,"Missing"
R11-1054,W02-1011,0,0.0132955,"rs and scanners. The following section describes in details the building of the opinion detection system, which makes an intensive use of syntactic information. Finally, we present a preliminary evaluation of the performances of this system and conclude on our perspectives. 2 State of the Art Besides works about lexical resources acquisition for opinion mining, discussed in section 4.3.2, two main types of works can be distinguished: those aiming at classifying texts according to an overall polarity (positive, negative and sometimes neutral), generally based on supervised approaches (such as (Pang et al. 2002), or (Charton and Acuna-Agost 2007)), and those aiming at extracting precise information about positive or negative aspects of a given product or topic. The latter consider that the main concept (e.g. a product) is related to several features (e.g. quality, print speed and resolution for a printer), that can be evaluated separately. Our system belongs to this category. In this case, the goal is to identify related features and opinions expressed about these features. Three sub-tasks are considered: feature extraction, discovery of opinions about these features, and eventually production of a s"
R11-1054,H05-1043,0,0.204955,"d to several features (e.g. quality, print speed and resolution for a printer), that can be evaluated separately. Our system belongs to this category. In this case, the goal is to identify related features and opinions expressed about these features. Three sub-tasks are considered: feature extraction, discovery of opinions about these features, and eventually production of a summary of the information associated with a given feature. In order to extract features, methods are generally based on frequency criteria coupled with linguistically-based heuristic, see for example (Yi et al. 2003) or (Popescu and Etzioni 2005). In order to extract opinions about features, a wide range of methods have been proposed: (Hu and Liu 2004) extract the linguistic segments containing a concept and count the polarity of the polar vocabulary present in the same segment. (Vernier et al. 2009) propose a symbolic method to detect and categorize opinions locally expressed in a set of multi-domain 392 Proceedings of Recent Advances in Natural Language Processing, pages 392–398, Hissar, Bulgaria, 12-14 September 2011. blogs. Some systems use syntactic dependencies to link source and target of the opinion as in (Kim and Hovy 2006) o"
R11-1054,1998.amta-tutorials.1,0,0.205075,"Missing"
R11-1054,W03-1017,0,\N,Missing
R11-1054,H05-2017,0,\N,Missing
S07-1109,W03-1606,1,0.80705,"during the processing (mainly chunks, see (Abney, 1991)). Moreover, together with surface syntactic relations, the parser calculates more sophisticated relations using derivational morphologic properties, deep syntactic properties 2 , and some limited lexical semantic coding (Levin&apos;s verb class alternations, see (Levin, 1993)), and some elements of the Framenet 3 classification, (Ruppenhofer et al., 2006)). These deep syntactic relations correspond roughly to the agent-experiencer roles that is subsumed by the SUBJ-N relation and to the patient-theme role subsumed by the OBJ-N relation, see (Brun and Hagège, 2003). Not only verbs bear these relations but also deverbal nouns with their corresponding arguments. Here is an example of an output (chunks and deep syntactic relations): Lebanon still wanted to see the implementation of a UN resolution TOP{SC{NP{Lebanon} FV{still wanted}} IV{to see} NP{the implementation} PP{of NP{a UN resolution}} .} MOD_PRE(wanted,still) MOD_PRE(resolution,UN) MOD_POST(implementation,resolution) COUNTRY(Lebanon) ORGANISATION(UN) EXPERIENCER_PRE(wanted,Lebanon) EXPERIENCER(see,Lebanon) CONTENT(see,implementation) EMBED_INFINIT(see,wanted) OBJ-N(implement,resolution) 1.2 Adapta"
S07-1109,P98-2127,0,0.061332,"of-sign are occurring with Germany, France, someone, government, president. For each Named Entity annotation, the hybrid method consists in using symbolic annotation if there is (§1.2), else using distributional annotation (§1.3) as presented below. Method: We constructed a distributional space with the 100M-word BNC. We prepared the corpus by lemmatizing and then parsing with the same robust parser than for the symbolic approach (XIP, see section 3.1). It allows us to identify triple instances. Each triple have the form w1.R.w2 where w1 and w2 are lexical units and R is a syntactic relation (Lin, 1998; Kilgarriff & al. 2004). Our approach can be distinguished from classical distributional approach by different points. First, we use triple occurrences to build a distributional space (one triple implies two contexts and two lexical units), but we use the transpose of the classical space: each point xi of this space is a syntactical context (with the form R.w.), each dimension j is a lexical units, and each value xi(j) is the frequency of corresponding triple occurrences. Second, our lexical units are words but also complex nominal groups or verbal groups. Third, contexts can be simple contex"
S07-1109,S07-1007,0,\N,Missing
S07-1109,C98-2122,0,\N,Missing
S14-2149,W01-1819,1,0.604917,"a robust parser that provides information to feed different classifiers with linguistic features dedicated to aspect categories and aspect categories polarity classification. We mainly present the work which has been done on the restaurant domain1 for the four subtasks, aspect term and category detection and aspect term and category polarity. 1 Description of the System 2.1 Existing System In order to tackle the Semeval’14 Task 4, (Pontiki et al., 2014), we used our existing aspectbased opinion detection system. The opinion detection system we built relies on a robust deep syntactic parser, (Ait-Mokhtar et al., 2001), as a fundamental component, from which semantic relations of opinion are calculated. Parsing here includes tokenization, morpho-syntactic analysis, tagging which is performed via a combination of hand-written rules and HMM, Named Entity Detection, chunking and finally, extraction of dependency relations between lexical nodes. These relations are labeled with deep syntactic functions. More precisely, a predicate (verbal or nominal) is linked with what we call its deep subject (SUBJN), its deep object (OBJ-N), and modifiers. In addition, the parser calculates more sophisticated and complex rel"
S14-2149,N07-1039,0,0.0118288,"components. The system outputs a semantic dependency called SENTIMENT which can be binary, i.e. linking opinionated terms and their targets, or unary, i.e. just the polar term in case the target of the Introduction Aspect Based Sentiment Analysis aims at discovering the opinions or sentiments expressed by a user on the different aspects of a given entity ((Hu and Liu, 2004); (Liu, 2012)). A wide range of methods and techniques have been proposed to address this task, among which systems that use syntactic dependencies to link source and target of the opinion, such as in (Kim and Hovy, 2004), (Bloom et al., 2007), or (Wu et al., 2009). We have developed a system that belongs to this family, (Brun, 2011), as we believe that syntactic processing of complex phenomena (negation, comparison, ...) is a crucial step to perform aspect-based opinion mining. In this paper, we describe the adaptations we have made to this system for SemEval, and the way it is applied to category and polarity classification. This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4"
S14-2149,R11-1054,1,0.846289,"nal morphologic properties, deep syntactic properties (subject and object of infinitives in the context of control verbs), and some limited lexical semantic coding. Syntactic relations already extracted by a general dependency grammar, lexical information about word polarities, sub categorization information and syntactic dependencies are all combined within our robust parser to extract the semantic relations. The polarity lexicon has been built using existing resources and also by applying classification techniques over large corpora, while the semantic extraction rules are handcrafted, see (Brun, 2011) and (Brun, 2012) for the complete description of these different components. The system outputs a semantic dependency called SENTIMENT which can be binary, i.e. linking opinionated terms and their targets, or unary, i.e. just the polar term in case the target of the Introduction Aspect Based Sentiment Analysis aims at discovering the opinions or sentiments expressed by a user on the different aspects of a given entity ((Hu and Liu, 2004); (Liu, 2012)). A wide range of methods and techniques have been proposed to address this task, among which systems that use syntactic dependencies to link so"
S14-2149,C12-2017,1,0.828542,"roperties, deep syntactic properties (subject and object of infinitives in the context of control verbs), and some limited lexical semantic coding. Syntactic relations already extracted by a general dependency grammar, lexical information about word polarities, sub categorization information and syntactic dependencies are all combined within our robust parser to extract the semantic relations. The polarity lexicon has been built using existing resources and also by applying classification techniques over large corpora, while the semantic extraction rules are handcrafted, see (Brun, 2011) and (Brun, 2012) for the complete description of these different components. The system outputs a semantic dependency called SENTIMENT which can be binary, i.e. linking opinionated terms and their targets, or unary, i.e. just the polar term in case the target of the Introduction Aspect Based Sentiment Analysis aims at discovering the opinions or sentiments expressed by a user on the different aspects of a given entity ((Hu and Liu, 2004); (Liu, 2012)). A wide range of methods and techniques have been proposed to address this task, among which systems that use syntactic dependencies to link source and target o"
S14-2149,C04-1200,0,0.0504607,"on of these different components. The system outputs a semantic dependency called SENTIMENT which can be binary, i.e. linking opinionated terms and their targets, or unary, i.e. just the polar term in case the target of the Introduction Aspect Based Sentiment Analysis aims at discovering the opinions or sentiments expressed by a user on the different aspects of a given entity ((Hu and Liu, 2004); (Liu, 2012)). A wide range of methods and techniques have been proposed to address this task, among which systems that use syntactic dependencies to link source and target of the opinion, such as in (Kim and Hovy, 2004), (Bloom et al., 2007), or (Wu et al., 2009). We have developed a system that belongs to this family, (Brun, 2011), as we believe that syntactic processing of complex phenomena (negation, comparison, ...) is a crucial step to perform aspect-based opinion mining. In this paper, we describe the adaptations we have made to this system for SemEval, and the way it is applied to category and polarity classification. This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecom"
S14-2149,S14-2004,0,0.203151,"the system. In this paper, we present the system we have developed for the SemEval-2014 Task 4 dedicated to Aspect-Based Sentiment Analysis. The system is based on a robust parser that provides information to feed different classifiers with linguistic features dedicated to aspect categories and aspect categories polarity classification. We mainly present the work which has been done on the restaurant domain1 for the four subtasks, aspect term and category detection and aspect term and category polarity. 1 Description of the System 2.1 Existing System In order to tackle the Semeval’14 Task 4, (Pontiki et al., 2014), we used our existing aspectbased opinion detection system. The opinion detection system we built relies on a robust deep syntactic parser, (Ait-Mokhtar et al., 2001), as a fundamental component, from which semantic relations of opinion are calculated. Parsing here includes tokenization, morpho-syntactic analysis, tagging which is performed via a combination of hand-written rules and HMM, Named Entity Detection, chunking and finally, extraction of dependency relations between lexical nodes. These relations are labeled with deep syntactic functions. More precisely, a predicate (verbal or nomin"
S14-2149,D09-1159,0,0.0282616,"tputs a semantic dependency called SENTIMENT which can be binary, i.e. linking opinionated terms and their targets, or unary, i.e. just the polar term in case the target of the Introduction Aspect Based Sentiment Analysis aims at discovering the opinions or sentiments expressed by a user on the different aspects of a given entity ((Hu and Liu, 2004); (Liu, 2012)). A wide range of methods and techniques have been proposed to address this task, among which systems that use syntactic dependencies to link source and target of the opinion, such as in (Kim and Hovy, 2004), (Bloom et al., 2007), or (Wu et al., 2009). We have developed a system that belongs to this family, (Brun, 2011), as we believe that syntactic processing of complex phenomena (negation, comparison, ...) is a crucial step to perform aspect-based opinion mining. In this paper, we describe the adaptations we have made to this system for SemEval, and the way it is applied to category and polarity classification. This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/ 1 We have not perf"
S16-1044,S14-2149,1,0.79509,"ults obtained for both languages. 1 Introduction and Related Work Sentiment Analysis is an important topic in natural language processing, and Aspect Based Sentiment Analysis (ABSA), i.e. detection of sentiments expressed on different aspects of a given entity, constitute a very interesting but quite challenging task (Liu, 2012; Ganu et al., 2009). ABSA is a task first introduced at SemEval in 2014 (Pontiki et al., 2014), continued in 2015 (Pontiki et al., 2015) and now, in 2016 (Pontiki et al., 2016). Our team has participated to the first edition, with good results on the restaurant domain (Brun et al., 2014) and decided to reiterate the participation in 2016, on the same domain but on English and French, as the challenge has become multilingual. While relatively similar, the task has evolved since 2014: aspect targets and categories are annotated together instead of separately; only opinionated terms (Opinion Target Expressions, OTE) are annotated, and aspect categories are finer grained (12 classes instead of 5), which makes the subtasks even more challenging. In the previous challenges, most systems, including ours, use state-of-the art machine learning algorithms such as SVMs (Wagner et al., 2"
S16-1044,R11-1054,1,0.829573,"This robust parser has been already used for the Aspect Based Sentiment Analysis of SemEval 14 (Brun et al., 2014). We have designed and adapted a semantic extraction component that extracts semantic information about aspect targets and their polarities on top of the parser described before. For this task, syntactic dependencies, lexical information about word polarities and semantic classes, sub-categorization information are all combined within the parser to extract semantic relations associated to aspect targets. We already have developed a component that extracts sentiment relations (see (Brun, 2011) for the complete description of this component), taking into account contexts and scope of the polar predicates. This semantic component makes use of a polar lexicon associating polarities (only positive and negative) to words, and a semantic lexicon associating lexical semantic features (FOOD, DRINK, AMBI ENCE , SERVICE , RESTAURANT , PRICE , STYLE ) to words. For the present challenge, we used the English and French version of the grammars, and complemented the existing domain lexicons with lexical information extracted from the training corpus. We use this parser as a “feature factory”, th"
S16-1044,S14-2145,0,0.0142214,"Missing"
S16-1044,S15-2128,0,0.0564645,"he challenge has become multilingual. While relatively similar, the task has evolved since 2014: aspect targets and categories are annotated together instead of separately; only opinionated terms (Opinion Target Expressions, OTE) are annotated, and aspect categories are finer grained (12 classes instead of 5), which makes the subtasks even more challenging. In the previous challenges, most systems, including ours, use state-of-the art machine learning algorithms such as SVMs (Wagner et al., 2014; Kiritchenko et al., 2014; Brun et al., 2014; Brychc´ın et al., 2014) or CRFs (Toh and Wang, 2014; Hamdan et al., 2015), with lexical information, bigrams and POS as features. In 2014, (Kiritchenko et al., 2014) had particularly good results on aspect category and aspect polarity detection, using SVMs combined with rich linguistic features including dependency parsing. In 2015, the system presented by (Saias, 2015) reported the best result for polarity classification, using a maximum entropy classifier, having bag-of-words, lemmas, bigrams after verbs, and punctuation based features, along with sentiment lexicon-based features. Our system shares whith these ones the use of syntactic features to address the dif"
S16-1044,S14-2076,0,0.0373655,"decided to reiterate the participation in 2016, on the same domain but on English and French, as the challenge has become multilingual. While relatively similar, the task has evolved since 2014: aspect targets and categories are annotated together instead of separately; only opinionated terms (Opinion Target Expressions, OTE) are annotated, and aspect categories are finer grained (12 classes instead of 5), which makes the subtasks even more challenging. In the previous challenges, most systems, including ours, use state-of-the art machine learning algorithms such as SVMs (Wagner et al., 2014; Kiritchenko et al., 2014; Brun et al., 2014; Brychc´ın et al., 2014) or CRFs (Toh and Wang, 2014; Hamdan et al., 2015), with lexical information, bigrams and POS as features. In 2014, (Kiritchenko et al., 2014) had particularly good results on aspect category and aspect polarity detection, using SVMs combined with rich linguistic features including dependency parsing. In 2015, the system presented by (Saias, 2015) reported the best result for polarity classification, using a maximum entropy classifier, having bag-of-words, lemmas, bigrams after verbs, and punctuation based features, along with sentiment lexicon-based"
S16-1044,S14-2004,0,0.115191,"larity classification. We describe the different components of the system, based on composite models combining sophisticated linguistic features with Machine Learning algorithms, and report the results obtained for both languages. 1 Introduction and Related Work Sentiment Analysis is an important topic in natural language processing, and Aspect Based Sentiment Analysis (ABSA), i.e. detection of sentiments expressed on different aspects of a given entity, constitute a very interesting but quite challenging task (Liu, 2012; Ganu et al., 2009). ABSA is a task first introduced at SemEval in 2014 (Pontiki et al., 2014), continued in 2015 (Pontiki et al., 2015) and now, in 2016 (Pontiki et al., 2016). Our team has participated to the first edition, with good results on the restaurant domain (Brun et al., 2014) and decided to reiterate the participation in 2016, on the same domain but on English and French, as the challenge has become multilingual. While relatively similar, the task has evolved since 2014: aspect targets and categories are annotated together instead of separately; only opinionated terms (Opinion Target Expressions, OTE) are annotated, and aspect categories are finer grained (12 classes instea"
S16-1044,S15-2082,0,0.0830128,"Missing"
S16-1044,S15-2130,0,0.114512,"5), which makes the subtasks even more challenging. In the previous challenges, most systems, including ours, use state-of-the art machine learning algorithms such as SVMs (Wagner et al., 2014; Kiritchenko et al., 2014; Brun et al., 2014; Brychc´ın et al., 2014) or CRFs (Toh and Wang, 2014; Hamdan et al., 2015), with lexical information, bigrams and POS as features. In 2014, (Kiritchenko et al., 2014) had particularly good results on aspect category and aspect polarity detection, using SVMs combined with rich linguistic features including dependency parsing. In 2015, the system presented by (Saias, 2015) reported the best result for polarity classification, using a maximum entropy classifier, having bag-of-words, lemmas, bigrams after verbs, and punctuation based features, along with sentiment lexicon-based features. Our system shares whith these ones the use of syntactic features to address the different ABSA tasks. For the present challenge, we addressed substask 1, which implies target terms detection, aspect category and polarity classification. In the remaining of the paper, we describe the different components of our system which combine rich linguistic features and machine learning alg"
S16-1044,S14-2038,0,0.0355737,"ish and French, as the challenge has become multilingual. While relatively similar, the task has evolved since 2014: aspect targets and categories are annotated together instead of separately; only opinionated terms (Opinion Target Expressions, OTE) are annotated, and aspect categories are finer grained (12 classes instead of 5), which makes the subtasks even more challenging. In the previous challenges, most systems, including ours, use state-of-the art machine learning algorithms such as SVMs (Wagner et al., 2014; Kiritchenko et al., 2014; Brun et al., 2014; Brychc´ın et al., 2014) or CRFs (Toh and Wang, 2014; Hamdan et al., 2015), with lexical information, bigrams and POS as features. In 2014, (Kiritchenko et al., 2014) had particularly good results on aspect category and aspect polarity detection, using SVMs combined with rich linguistic features including dependency parsing. In 2015, the system presented by (Saias, 2015) reported the best result for polarity classification, using a maximum entropy classifier, having bag-of-words, lemmas, bigrams after verbs, and punctuation based features, along with sentiment lexicon-based features. Our system shares whith these ones the use of syntactic featu"
S16-1044,S14-2036,0,0.0154609,"un et al., 2014) and decided to reiterate the participation in 2016, on the same domain but on English and French, as the challenge has become multilingual. While relatively similar, the task has evolved since 2014: aspect targets and categories are annotated together instead of separately; only opinionated terms (Opinion Target Expressions, OTE) are annotated, and aspect categories are finer grained (12 classes instead of 5), which makes the subtasks even more challenging. In the previous challenges, most systems, including ours, use state-of-the art machine learning algorithms such as SVMs (Wagner et al., 2014; Kiritchenko et al., 2014; Brun et al., 2014; Brychc´ın et al., 2014) or CRFs (Toh and Wang, 2014; Hamdan et al., 2015), with lexical information, bigrams and POS as features. In 2014, (Kiritchenko et al., 2014) had particularly good results on aspect category and aspect polarity detection, using SVMs combined with rich linguistic features including dependency parsing. In 2015, the system presented by (Saias, 2015) reported the best result for polarity classification, using a maximum entropy classifier, having bag-of-words, lemmas, bigrams after verbs, and punctuation based features, along wi"
velcin-etal-2014-investigating,P04-1035,0,\N,Missing
velcin-etal-2014-investigating,stoyanov-cardie-2008-annotating,0,\N,Missing
velcin-etal-2014-investigating,P12-3020,0,\N,Missing
W00-1404,C96-1043,0,0.129913,"tures: and that author choices be all under the explicit conFirst, the authoring process is monolingual, but trol of the DTD and reflected in the document structhe results are multilingual. At each point of the proture. Such a view, which is argued for in a related cess the author can view in his/her own language the paper (Dymetman et el., 2000), emphasizes the link between ~ML`d~cumeqt~a~a9ring`~aad;mu~ti~nguaL;~,~.~te~t:~s/h~hasa~u~h~rex~:~.~aa~a~d~rea~Èwhere~he ..: text authoring/generation (Power and Scott, 1998; text still needs refinement are highlighted. Menus Hartley and Paris, 1997; Coch, 1996): the choices for selecting a refinement are also presented to the made by the author are treated as a kind of inauthor is his/her own language. Thus, the author is terlingua (specific to the class of documents being always overtly working in the language s/he nows, modelled), and it is the responsibility of appropribut is implicitly building a language-independent ate ""rendering"" mechanisms to produce actual text representation of the document content. From this from these choices ill tile different languages 3 under representation, the system builds multilingual texts consideration, in any o"
W00-1404,C00-1036,1,0.412979,"Missing"
W00-1404,P98-2173,0,0.159268,"ent Authoring system has be altogether eliminated from the document content, the following main features: and that author choices be all under the explicit conFirst, the authoring process is monolingual, but trol of the DTD and reflected in the document structhe results are multilingual. At each point of the proture. Such a view, which is argued for in a related cess the author can view in his/her own language the paper (Dymetman et el., 2000), emphasizes the link between ~ML`d~cumeqt~a~a9ring`~aad;mu~ti~nguaL;~,~.~te~t:~s/h~hasa~u~h~rex~:~.~aa~a~d~rea~Èwhere~he ..: text authoring/generation (Power and Scott, 1998; text still needs refinement are highlighted. Menus Hartley and Paris, 1997; Coch, 1996): the choices for selecting a refinement are also presented to the made by the author are treated as a kind of inauthor is his/her own language. Thus, the author is terlingua (specific to the class of documents being always overtly working in the language s/he nows, modelled), and it is the responsibility of appropribut is implicitly building a language-independent ate ""rendering"" mechanisms to produce actual text representation of the document content. From this from these choices ill tile different langu"
W00-1404,C98-2168,0,\N,Missing
W03-1606,P01-1008,0,0.0398764,"added if there is a negation. 3 Paraphrase detection Paraphrasing means to be able, from some input text that convey a certain meaning, to express the same meaning in a different way. This subject has recently been receiving an increasing interest. For instance, Takahashi et. al. (Takahashi et al., 2000) developed a lexico-structural paraphrasing system. Kaji et al. developed a system which is able to produce verbal paraphrase using dictionary definitions (Kaji et al., 2000) and Barzilay and McKeown showed how, using parallel corpora of English literary translations, they extract paraphrases (Barzilay and McKeown, 2001). Paraphrase detection is a useful step in many NLP applications. For instance, in multi-document summarization, paraphrase detection helps to identify similar text segments in order that the summary become more concise (McKeown et al., 1999). Paraphrase detection can also be used to augment recall in different IE systems. In our experiment, paraphrase detection is a step in normalization, as we want to instantiate the same way the predicates presented above when the informative content is similar. For instance, we want to obtain the same normalized predicate for the two utterances ProductX is"
W03-1606,C94-1042,0,0.0157534,"he verb and the noun (last line). An important feature is that our parser always provides a unique analysis (determinism), this analysis being potentially underspecified. 3.2 General morpho-syntactic normalization The morpho-syntactic normalizer is a general module that is neither corpus- nor application-dedicated. It consists of hand-made rules that apply to the syntactic representation produced by our parser. It uses well known syntactic equivalences such as passiveactive transformation and verb alternations proposed in Levin. It also exploits the classification given by the COMLEX lexicon (Grishman et al., 1994) in order to calculate the deep-subject of infinitive verbs. For instance the utterance Antimony ores are mixed with other metals is finally represented with a set of normalized syntactic relations expressing that the normalized subject (SUBJ-N) of the verb mix is unknown, and that mix has two second actants (OBJ-N) ore and metal : SUBJ-N(mix,SOMEONE) OBJ-N(mix,ore) OBJ-N(mix,metal) For this example, both passive transformation and reciprocal alternation transformation have been applied on the set of dependencies produced by the general parser. Deep syntactic rules are expressed using the same"
W03-1606,P02-1028,0,\N,Missing
W12-0608,R11-1054,1,0.824146,". These semantic relations are intermediary steps to instantiate the five place predicates which are compliant with the aforementioned model. Having syntactic relations already extracted by a general dependency grammar, we use the robust parser by combining lexical information about word polarities, subcategorization information and syntactic dependencies to extract the semantic relations that will then instantiate this model. There exist other systems, such as the one described in (Kim and Hovy, 2006), that use syntactic dependencies to link the source and target of the opinions. Our system (Brun, 2011) belongs to this family, since we believe that the syntactic processing of complex phenomena (negation, comparison and anaphora) is a necessary step in order to perform feature-based opinion mining. Another characteristic of our system is that it respects a two-level architecture; it relies on a generic level, applicable to all domains and corpora, and on a domain-dependent level, adapted for each sub-domain of application. Moreover, our system includes a semantic mapping between polar vocabulary and the features it corresponds to. For instance, the opinion word “fast” is mapped to the feature"
W12-0608,P07-1053,0,0.210703,"user. 1 Introduction Social media has enabled web users to interact through social platforms, express their opinions, comment and review various products/items. Such user-generated content has been analysed from a social as well as content-oriented point of view. For instance, social network analysis techniques have been used to identify user roles (Agarwal et al., 2008; Domingos and Richardson, 2001; Fisher et al., 2006; Zhang et al., 2007) and text or opinion mining techniques have been applied to identify positive/negative tendencies within user online review comments (Ding and Liu, 2007; Ghose et al., 2007; Hu and Liu, 2004; Leskovec et al., 2010). In the applicative context, recommender systems (Adomavicius and Tuzhilin, 2005) make use of the opinion information (such as in star-rating systems) and recommend items (movies, products, news articles, etc.) or social elements (i.e. propositions to connect with other people or communities), that are likely to be of interest to a specific user. Typically, a recommender system compares a user profile with some reference characteristics, and seeks to predict the “preference” or “rating” that a user would give to an item not yet considered. These chara"
W12-0608,N06-1026,0,0.0295775,"ponent, in order to extract deep syntactic dependencies, from which semantic relations of opinion are calculated. These semantic relations are intermediary steps to instantiate the five place predicates which are compliant with the aforementioned model. Having syntactic relations already extracted by a general dependency grammar, we use the robust parser by combining lexical information about word polarities, subcategorization information and syntactic dependencies to extract the semantic relations that will then instantiate this model. There exist other systems, such as the one described in (Kim and Hovy, 2006), that use syntactic dependencies to link the source and target of the opinions. Our system (Brun, 2011) belongs to this family, since we believe that the syntactic processing of complex phenomena (negation, comparison and anaphora) is a necessary step in order to perform feature-based opinion mining. Another characteristic of our system is that it respects a two-level architecture; it relies on a generic level, applicable to all domains and corpora, and on a domain-dependent level, adapted for each sub-domain of application. Moreover, our system includes a semantic mapping between polar vocab"
W12-1009,E95-1021,0,0.0137013,"interpretation is conditioned by the recapitalization steps which transmits information about recapitalization (via features within XIP) that triggers query-specific pos disambiguation rules. statuettes hommes jouant avec un chien (French query) coupe apollon (French query) architecture musique (French query) statue haut relief grecque du 5 siecle (French query) david playing harp fpr saul (English query) stained glass angel (English query) Standard techniques for part-of-speech tagging include rule based methods and statistical methods, mainly based on hidden Markov models (see for example (Chanod and Tapanainen, 1995)). In this case, it would be possible to recompute the probabilities on a corpus of queries manually annotated. However, the correction of part-of-speech tags in the context of queries is easy to develop with a small set of rules. We focus on English and French, and in queries, the main problems come from the ambiguity beThe recapitalization (1) has been implemented within the preprocessing components of XIP within finite state transducers (see (Karttunen, 2000)). The second point (2) is done directly within XIP in the part-of-speech tagging process, with a contextual rule. For example, the an"
W12-1009,D07-1086,0,0.0266721,"ction between a pair of query words to identify the border between the segments in the query (Jones et al., 2006; Guo et 55 Proceedings of the 6th EACL Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities, pages 55–64, c Avignon, France, 24 April 2012. 2012 Association for Computational Linguistics for NE recognition) and doesn’t require access to the document collection. al., 2008). Tan and Peng (2008) propose a generative language model enriched with Wikipedia to identify “concepts” rather than simply “frequency-based” patterns. The segmentation proposed by Bergsma and Wang (2007) is closer to the notion of NP chunking. They propose a machine-learned query segmentation system trained on manually annotated set of 500 AOL queries. However, in this work PoS tagging is used as one of the features in query segmentation and is done with a generic PoS tagger, non adapted for queries. PoS tagging is an important part of query processing and used in many information analytics tasks (query reformulation, query segmentation, etc.). However very few works address query-oriented PoS tagging. Allan and Raghavan (2002) consider that PoS tagging might be ambiguous for short queries an"
W12-1009,D08-1107,0,\N,Missing
W16-6205,W01-1819,0,0.46302,"Missing"
W16-6205,I13-1077,0,0.187423,"tails of a personal experience and sharing information to further develop social relationships. 2 3 best understand affective expressions • that to fully understand expressions of emotion in support forums, a fine-grained annotation scheme is required which takes into account the social function of such expressions. Related Work As reported earlier, polarity-based studies in the healthcare domain have considerable value. One work squarely in the public policy domain sought to classify tweets related to the recent health care reform in the US into positive and negative (Speriosu et al., 2011). Ali et al. (2013) experimented with data from multiple forums for people with hearingloss. They use the subjectivity lexicon of Wilson et al. (2005) and count-based syntactic features (e.g. number of adjectives, adverbs, etc.). This approach outperformed a baseline bag-of-words model, highlighting the importance of subjective lexica for text analysis in health domain. Ofek et al. (2013) use a dynamic sentiment lexicon to improve sentiment analysis in an online community for cancer survivors. Sokolova and Bobicev (2013) took the lexicon approach further: they defined a more fine-grained annotation scheme (see S"
W16-6205,W16-6205,1,0.0529494,"ence-level classification. 4.3.1 Na¨ıve Labelling The most trivial approach to label sentences is for each sentence to inherit the label of the post in which it is present. Following this method, we obtain the distribution as reported in Table 1 We run the 5-class classification scenario on MedSenti-sent using the same conditions and the previous best feature set; the results are shown in 29 R 0.157 0.360 0.088 0.225 0.598 0.286 F 0.188 0.351 0.117 0.243 0.509 0.289 Table 3: Precision, Recall and F1 for Sentence-level classification We also explore the model performance with the error matrix (Navindgi et al., 2016). Our main observation is that the drop in performance of the four subjective categories is largely due to misclassification of sentences as FACT. Sentences in this category are the majority in MedSenti-sent. However, the proportional differences with MedSenti do not seem to be not enough to explain the significant changes. A more likely explanation is simply that the errors arise because – at the very least – there can be FACTlike sentences in any post. At the time of creation, annotators were asked to label “the most dominant sentiment in the whole post” (Sokolova and Bobicev, 2013, p. 636)."
W16-6205,S14-2004,0,0.0605817,"opinion of government policies (Speriosu et al., 2011). However, the level of affective expressions in a support forum setting is considerably more complex than a traditional positivenegative polarity spectrum. More than just a more-fined grained labelling scheme, we also need a deeper understanding on the language being used. Much sentiment analysis research has focused on classifying the overall sentiment of documents onto a positive-negative spectrum (Hu and Liu, 2004). Recently, research work targeting finer grained analysis has emerged, such as aspect-based sentiment analysis (Liu, 2012; Pontiki et al., 2014), or semantic role labelling of emotions (Mohammad et al., 2014). This relatively new trend in social media analytics enables the detection of not simply binary sentiment, but more nuanced sentiments and mixed feelings. Such affective expressions often serve a social purpose (Rothman and Magee, 2016). With this in mind, we explore a dataset drawn from a health-related support forum, labelled for a variety of expressed sentiments. Here, we do not necessarily seek state-of-the-art performance, but use this task to argue for two key positions: • that sub-document level analysis is required to 26"
W16-6205,R13-1083,0,0.147853,"related to the recent health care reform in the US into positive and negative (Speriosu et al., 2011). Ali et al. (2013) experimented with data from multiple forums for people with hearingloss. They use the subjectivity lexicon of Wilson et al. (2005) and count-based syntactic features (e.g. number of adjectives, adverbs, etc.). This approach outperformed a baseline bag-of-words model, highlighting the importance of subjective lexica for text analysis in health domain. Ofek et al. (2013) use a dynamic sentiment lexicon to improve sentiment analysis in an online community for cancer survivors. Sokolova and Bobicev (2013) took the lexicon approach further: they defined a more fine-grained annotation scheme (see Section 3) and labelled data from an IVF-related forum. Their category-specific set of lexicons performed better, at 6-class classification, than a generic subjectivity lexicon. In selecting their data, Sokolova and Bobicev (2013) – as Ali et al. (2013) and others have done – tapped into the domain of on-line support communities. Eastin and LaRose (2005) showed that people who seek support on-line – be it emotional or informational support – typically find it. Informational support is based on sharing k"
W16-6205,R15-1078,0,0.263177,"e expression – is always in this situation directed to another participant. This makes the interpersonal expression loadings both overloaded both in terms of classification and polarity. These relationships, in many ways, make machine modelling therein overly noisy. Of course, it is fair to say that one direction of work in such a social domain that we did not explore is context. The original authors report subsequently on incorporating context into their experiments: both in terms of the position within a discussion of a post (Bobicev and Sokolova, 2015) and the posting history of an author (Sokolova and Bobicev, 2015). In this work we have eschewed context, though acknowledge that it is significantly important: in the ENCO-FACT sample above, for example, context may enable a better understanding that the ENCO sentence is in response to another ENCO statement, while the FACT is a response to a direct question. In this sense, there is a clear motivation to understand document-level relationships at the sentence level. 31 Opinion/evaluation Giving advice Giving emotional support Requesting clarification Community building Personal attacks Subcategory professional experience personal experience emotional expre"
W16-6205,W11-2207,0,0.379233,"ch users generate emotion-rich content, exchange factual information about elements such as ∗ Work carried out while all authors at Xerox Research Centre Europe. treatments or hospitals, and provide emotional support to others (Bringay et al., 2014). This sharing through open discussion is known to be considerably beneficial (Pennebaker et al., 2001). Understanding affective language in the healthcare domain is an effective application of natural language technologies. Sentiment mining on platforms such as Twitter, for example, is a quick method to gauge public opinion of government policies (Speriosu et al., 2011). However, the level of affective expressions in a support forum setting is considerably more complex than a traditional positivenegative polarity spectrum. More than just a more-fined grained labelling scheme, we also need a deeper understanding on the language being used. Much sentiment analysis research has focused on classifying the overall sentiment of documents onto a positive-negative spectrum (Hu and Liu, 2004). Recently, research work targeting finer grained analysis has emerged, such as aspect-based sentiment analysis (Liu, 2012; Pontiki et al., 2014), or semantic role labelling of e"
W16-6205,S07-1013,0,0.261159,"phiris, 2007). In addition to 27 Data 3.1 Data Source The data used here1 is that of Bobicev and Sokolova (2015), an extension of the data described in Sokolova and Bobicev (2013). Data was collected from discussion threads on a sub-forum of an In Vitro Fertilization (IVF) medical forum2 used by participants who belong to a specific age-group (over 35s). The dataset (henceforth MedSenti) originally contained 1321 posts across 80 different topics. 3.2 Annotation Details There are two approaches to annotation of subjective aspects of communication: from the perspective of a reader’s perception (Strapparava and Mihalcea, 2007) or that of the author (Balahur and Steinberger, 2009). In labelling MedSenti Sokolova and Bobicev (2013) opted for the reader-centric model and hence asked the annotators to analyse a post’s sentiment as if they were other discussion participants. This is an important differentiation for automated classification style tasks - models are built to predict how people will understand the emotion expressed, as opposed to the emotion or sentiment an author feels they are conveying. The annotation scheme was evolved over multiple rounds of data exploration, and ultimately three sentiment categories"
W16-6205,H05-1044,0,0.0647244,"pressions • that to fully understand expressions of emotion in support forums, a fine-grained annotation scheme is required which takes into account the social function of such expressions. Related Work As reported earlier, polarity-based studies in the healthcare domain have considerable value. One work squarely in the public policy domain sought to classify tweets related to the recent health care reform in the US into positive and negative (Speriosu et al., 2011). Ali et al. (2013) experimented with data from multiple forums for people with hearingloss. They use the subjectivity lexicon of Wilson et al. (2005) and count-based syntactic features (e.g. number of adjectives, adverbs, etc.). This approach outperformed a baseline bag-of-words model, highlighting the importance of subjective lexica for text analysis in health domain. Ofek et al. (2013) use a dynamic sentiment lexicon to improve sentiment analysis in an online community for cancer survivors. Sokolova and Bobicev (2013) took the lexicon approach further: they defined a more fine-grained annotation scheme (see Section 3) and labelled data from an IVF-related forum. Their category-specific set of lexicons performed better, at 6-class classif"
W16-6205,W06-0301,0,\N,Missing
W16-6205,R13-1054,0,\N,Missing
W18-6217,W16-1620,0,0.0234836,"ect-polarity tuples. The performance on <OTE, Aspect, Polarity&gt; tuples drops down to F1 of 37.1. This evaluation procedure allows us to get an idea on what would be “real-world” system performance, and also indicates the capacities and limitations of the system. 6.2 Weakly Supervised Lexical Acquisition Among other components, our system relies on semantic lexical ressources encoding domain aspect and polarity vocabulary, that were developed semi-automatically, based on SemEval2016 training datasets. In order to enrich these lexicons, we have adapted a semantic clustering method described in (Pelevina et al., 2016)7 . The core idea of this approach is to induce a sense inventory from existing word embeddings via clustering of ego-networks of related words. An ego network consists of a single node (ego) together with the nodes they are connected to and the edges between the connected nodes. Words referring to the same sense tend to have a large number of connections, and to be clustered together. The clustering is done with the Chinese Whispers algorithm (Biemann, 2006). In the case of the present experiments, we initialize the algorithm with a set of seed words together with their semantic aspect (e.g c"
W18-6217,S15-2082,0,0.166929,"Missing"
W18-6217,S15-2128,0,0.0218553,"by an ontology associated to the entity. ABSA includes therefore to identify aspects of an entity, and the sentiment expressed by the writer of the comment about different aspects. For example, from a sentence extracted from a review about a museum, an ABSA system could extract the following information: This museum hosts remarkable collections, however, prices are quite high and the attendants are not always friendly. 2 Related Work Most of the systems dedicated to ABSA use machine learning algorithms such as SVMs (Wagner et al., 2014; Kiritchenko et al., 2014), or CRFs (Toh and Wang, 2014; Hamdan et al., 2015), which are often combined with semantic lexical information, n-gram models, and sometimes more fine-grained syntactic or semantic information. For example, (Kumar et al., 2016) proposed a very efficient system on different languages of SemEval2016. The system use information extracted from dependency graphs and distributional thesaurus learned on the different domains and ”collections”: aspect=museum#collection, polarity=positive; ”prices”: aspect=museum#price, polarity=negative; ”attendants”: aspect=museum#service, polarity=negative; ABSA receives now a specific interest from the scientific"
W18-6217,S14-2004,0,0.0941078,"n-gram models, and sometimes more fine-grained syntactic or semantic information. For example, (Kumar et al., 2016) proposed a very efficient system on different languages of SemEval2016. The system use information extracted from dependency graphs and distributional thesaurus learned on the different domains and ”collections”: aspect=museum#collection, polarity=positive; ”prices”: aspect=museum#price, polarity=negative; ”attendants”: aspect=museum#service, polarity=negative; ABSA receives now a specific interest from the scientific community, especially with the SemEval dedicated challenges, (Pontiki et al., 2014), (Pontiki et al., 2015), (Pontiki et al., 2016), that provided a framework to design and evaluate ABSA ∗ Both authors contributed equally. 116 Proceedings of the 9th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis, pages 116–122 c Brussels, Belgium, October 31, 2018. 2018 Association for Computational Linguistics https://doi.org/10.18653/v1/P17 Dataset Semeval Foursquare languages of the challenge. Deep Learning methods are also emerging: for example, (Ruder et al., 2016) proposed a method using multiple filters CNNs and obtained competitive results o"
W18-6217,S14-2076,0,0.0245231,"or the picture quality of a camera, and can be described by an ontology associated to the entity. ABSA includes therefore to identify aspects of an entity, and the sentiment expressed by the writer of the comment about different aspects. For example, from a sentence extracted from a review about a museum, an ABSA system could extract the following information: This museum hosts remarkable collections, however, prices are quite high and the attendants are not always friendly. 2 Related Work Most of the systems dedicated to ABSA use machine learning algorithms such as SVMs (Wagner et al., 2014; Kiritchenko et al., 2014), or CRFs (Toh and Wang, 2014; Hamdan et al., 2015), which are often combined with semantic lexical information, n-gram models, and sometimes more fine-grained syntactic or semantic information. For example, (Kumar et al., 2016) proposed a very efficient system on different languages of SemEval2016. The system use information extracted from dependency graphs and distributional thesaurus learned on the different domains and ”collections”: aspect=museum#collection, polarity=positive; ”prices”: aspect=museum#price, polarity=negative; ”attendants”: aspect=museum#service, polarity=negative; ABSA re"
W18-6217,S16-1053,0,0.0259805,"Missing"
W18-6217,S16-1174,0,0.0210854,"ts. For example, from a sentence extracted from a review about a museum, an ABSA system could extract the following information: This museum hosts remarkable collections, however, prices are quite high and the attendants are not always friendly. 2 Related Work Most of the systems dedicated to ABSA use machine learning algorithms such as SVMs (Wagner et al., 2014; Kiritchenko et al., 2014), or CRFs (Toh and Wang, 2014; Hamdan et al., 2015), which are often combined with semantic lexical information, n-gram models, and sometimes more fine-grained syntactic or semantic information. For example, (Kumar et al., 2016) proposed a very efficient system on different languages of SemEval2016. The system use information extracted from dependency graphs and distributional thesaurus learned on the different domains and ”collections”: aspect=museum#collection, polarity=positive; ”prices”: aspect=museum#price, polarity=negative; ”attendants”: aspect=museum#service, polarity=negative; ABSA receives now a specific interest from the scientific community, especially with the SemEval dedicated challenges, (Pontiki et al., 2014), (Pontiki et al., 2015), (Pontiki et al., 2016), that provided a framework to design and eval"
W18-6217,E12-2021,0,0.0102186,"s ABSA real-world performances, we manually annotated a completely new dataset from Foursquare1 comments. We have access to about 215K user reviews of restaurants all over the world in English2 . The reviews were written during the period between 2009 to 2018. From these reviews, we randomly selected 585 samples, which contain 1006 sentences and annotate these sentences with the SemEval2016 annotation guidelines for the restaurant domain. The annotations have been performed by a single annotator, expert linguist with a very good knowledge of the SemEval2016 annotation guidelines, using BRAT, (Stenetorp et al., 2012). Each sentence contains annotations about: 1. Opinion Target Expression (OTE), i.e. the linguistic expression (term) used in the text to refer to the reviewed entity, annotated as “NULL” if the aspect is implicit; 2. Aspect Categories, i.e. the semantic categories of the opinionated aspects, which are part of a predefined ontology (12 semantic classes for the restaurant domain from <text&gt;Their sake list was extensive, but we were looking for Purple Haze, which wasn’t listed but made for us upon request!</text&gt; <Opinions&gt; <Opinion target=&quot;sake list&quot; category=&quot;DRINKS#STYLE_OPTIONS&quot; polarity=&quot;po"
W18-6217,P14-5010,0,0.00247156,"with CRF++4 toolkit), trained with some standard features (POS, lemma, presence of upper-case letters, features combining syntactic/semantic dependencies with semantic lexicons, embedding-based features). Aspect and polarity classification components rely on the same features as for OTE, excluding embedding-based features, but extended with bi-grams features. In addition, polarity classifier feature representation is extended with entity and attribute of aspect category (e.g. RESTAURANT#PRICES results in two additional features: (restaurant, prices)). Classification is performed with CoreNLP (Manning et al., 2014) implementation of Maximum Entropy. Thus, we evaluate separately the OTE detection, aspect detection and finally, we evaluate the polarity of opinion detection on the ground truth of phase A. The advantage of this evaluation procedure is of course to assess the quality of the systems on each of the different subtasks involved in the full ABSA system. However, these measures do not reflect the overall results such systems would obtain on the full chain of annotations starting from raw data, in end-to-end application settings. Therefore, we also propose to evaluate the results obtained with the"
W18-6217,S14-2038,0,0.0232777,"and can be described by an ontology associated to the entity. ABSA includes therefore to identify aspects of an entity, and the sentiment expressed by the writer of the comment about different aspects. For example, from a sentence extracted from a review about a museum, an ABSA system could extract the following information: This museum hosts remarkable collections, however, prices are quite high and the attendants are not always friendly. 2 Related Work Most of the systems dedicated to ABSA use machine learning algorithms such as SVMs (Wagner et al., 2014; Kiritchenko et al., 2014), or CRFs (Toh and Wang, 2014; Hamdan et al., 2015), which are often combined with semantic lexical information, n-gram models, and sometimes more fine-grained syntactic or semantic information. For example, (Kumar et al., 2016) proposed a very efficient system on different languages of SemEval2016. The system use information extracted from dependency graphs and distributional thesaurus learned on the different domains and ”collections”: aspect=museum#collection, polarity=positive; ”prices”: aspect=museum#price, polarity=negative; ”attendants”: aspect=museum#service, polarity=negative; ABSA receives now a specific interes"
W18-6217,S14-2036,0,0.0264394,"ce for a restaurant, or the picture quality of a camera, and can be described by an ontology associated to the entity. ABSA includes therefore to identify aspects of an entity, and the sentiment expressed by the writer of the comment about different aspects. For example, from a sentence extracted from a review about a museum, an ABSA system could extract the following information: This museum hosts remarkable collections, however, prices are quite high and the attendants are not always friendly. 2 Related Work Most of the systems dedicated to ABSA use machine learning algorithms such as SVMs (Wagner et al., 2014; Kiritchenko et al., 2014), or CRFs (Toh and Wang, 2014; Hamdan et al., 2015), which are often combined with semantic lexical information, n-gram models, and sometimes more fine-grained syntactic or semantic information. For example, (Kumar et al., 2016) proposed a very efficient system on different languages of SemEval2016. The system use information extracted from dependency graphs and distributional thesaurus learned on the different domains and ”collections”: aspect=museum#collection, polarity=positive; ”prices”: aspect=museum#price, polarity=negative; ”attendants”: aspect=museum#service,"
