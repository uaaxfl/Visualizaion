2020.acl-main.190,P19-1487,0,0.0239655,"n v(x) that test if the given chemical-disease pair follows certain relations in CTD (e.g., if they are in the ctd-therapy dictionary). Table 5 shows that as expected, other sources of information can complement language explanations in ExpBERT. 2109 5 Related work Many other works have used language to guide model training. As mentioned above, semantic parsers have been used to convert language explanations into features (Srivastava et al., 2017) and noisy labels on unlabeled data (Hancock et al., 2018; Wang et al., 2019). Rather than using language to define a global collection of features, Rajani et al. (2019) and Camburu et al. (2018) use instance-level explanations to train models that generate their own explanations. Zaidan and Eisner (2008) ask annotators to highlight important words, then learn a generative model over parameters given these rationales. Others have also used language to directly produce parameters of a classifier (Ba et al., 2015) and as part of the parameter space of a classifier (Andreas et al., 2017). While the above works consider learning from static language supervision, Li et al. (2016) and Weston (2016) learn from language supervision in an interactive setting. In a rel"
2020.acl-main.190,D17-1161,0,0.346631,"over the inductive biases of a model. In this paper, we explore using natural language explanations (Figure 1) to generate features that can augment modern neural representations. This imbues representations with inductive biases corresponding to the explanations, thereby restoring some degree of control while maintaining their expressive power. Prior work on training models with explanations use semantic parsers to interpret explanations: the parser converts each explanation into an executable logical form that is executable over the input sentence and uses the resulting outputs as features (Srivastava et al., 2017) or as noisy labels on unlabeled data (Hancock et al., 2018). However, semantic parsers can typically only parse low-level statements like “‘wife’ appears between {o1 } and {o2 } and the last word of {o1 } is the same as the last word of {o2 }” (Hancock et al., 2018). We remove these limitations by using modern distributed language representations, instead of semantic parsers, to interpret language explanations. Our approach, ExpBERT (Figure 2), uses BERT (Devlin et al., 2019) fine-tuned on the MultiNLI natural language inference dataset (Williams et al., 2018) to produce features that “interp"
2020.acl-main.190,P17-1086,1,0.816499,"(2018) use instance-level explanations to train models that generate their own explanations. Zaidan and Eisner (2008) ask annotators to highlight important words, then learn a generative model over parameters given these rationales. Others have also used language to directly produce parameters of a classifier (Ba et al., 2015) and as part of the parameter space of a classifier (Andreas et al., 2017). While the above works consider learning from static language supervision, Li et al. (2016) and Weston (2016) learn from language supervision in an interactive setting. In a related line of work, Wang et al. (2017), users teach a system high-level concepts via language. 6 Discussion Recent progress in general-purpose language representation models like BERT open up new opportunities to incorporate language into learning. In this work, we show how using these models with natural language explanations can allow us to leverage a richer set of explanations than if we were constrained to only use explanations that can be programmatically evaluated, e.g., through ngram matching (BERT+Patterns) or semantic parsing (BERT+SemParser). The ability to incorporate prior knowledge of the “right” inductive biases into"
2020.acl-main.190,N18-1101,0,0.187154,"resulting outputs as features (Srivastava et al., 2017) or as noisy labels on unlabeled data (Hancock et al., 2018). However, semantic parsers can typically only parse low-level statements like “‘wife’ appears between {o1 } and {o2 } and the last word of {o1 } is the same as the last word of {o2 }” (Hancock et al., 2018). We remove these limitations by using modern distributed language representations, instead of semantic parsers, to interpret language explanations. Our approach, ExpBERT (Figure 2), uses BERT (Devlin et al., 2019) fine-tuned on the MultiNLI natural language inference dataset (Williams et al., 2018) to produce features that “interpret” each explanation on an input. We then use these features to augment the input representation. Just as a semantic parser grounds an explanation by converting it into a logical form and then executing it, the features produced by BERT can be seen as a soft “execution” of the explanation on the input. 2106 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 2106–2113 c July 5 - 10, 2020. 2020 Association for Computational Linguistics wordpiece-tokenized versions of the explanation ej (hypothesis) and the instance x ("
2020.acl-main.190,D08-1004,0,\N,Missing
2020.acl-main.190,D17-1004,0,\N,Missing
2020.acl-main.190,N18-1197,0,\N,Missing
2020.acl-main.190,N19-1423,0,\N,Missing
2020.acl-main.225,P02-1040,0,\N,Missing
2020.acl-main.225,P08-1090,0,\N,Missing
2020.acl-main.225,D15-1195,0,\N,Missing
2020.acl-main.225,N16-1098,0,\N,Missing
2020.acl-main.225,P18-1082,0,\N,Missing
2020.acl-main.225,N18-1204,0,\N,Missing
2020.acl-main.225,W19-2304,0,\N,Missing
2020.acl-main.225,P16-1162,0,\N,Missing
2020.acl-main.225,W19-2405,0,\N,Missing
2020.acl-main.225,K19-1079,0,\N,Missing
2020.acl-main.225,2020.emnlp-main.420,0,\N,Missing
2020.acl-main.245,D13-1170,0,0.0179261,"Missing"
2020.acl-main.245,N18-1101,0,0.0770576,"Missing"
2020.acl-main.245,I05-5002,0,\N,Missing
2020.acl-main.245,D14-1162,0,\N,Missing
2020.acl-main.245,P18-1079,0,\N,Missing
2020.acl-main.245,P18-2006,0,\N,Missing
2020.acl-main.245,C18-1055,0,\N,Missing
2020.acl-main.245,P19-1478,0,\N,Missing
2020.acl-main.245,N19-1326,0,\N,Missing
2020.acl-main.245,P19-1561,0,\N,Missing
2020.acl-main.245,N19-1423,0,\N,Missing
2020.acl-main.245,D19-1423,1,\N,Missing
2020.acl-main.245,D19-1419,0,\N,Missing
2020.acl-main.436,P18-1175,1,0.904086,"Missing"
2020.acl-main.436,L16-1197,0,0.021273,"ine. However, they used separate encoders for the support and query examples, with only the support encoder trained to predict language, resulting in overfitting of the query encoder. ShapeWorld. First, we use the ShapeWorld (Kuhnle and Copestake, 2017) dataset used by Andreas et al. (2018), which consists of 9000 training, 1000 validation, and 4000 test tasks (Figure 2).3 Each task contains a single support set of K = 4 images representing a visual concept with an associated (artificial) English language description, generated with a minimal recursion semantics representation of the concept (Copestake et al., 2016). Each concept is a spatial relation between two objects, each object optionally qualified by color and/or shape, with 2-3 distractor shapes present. The task is to predict whether a query image x belongs to the concept. For ease of comparison, we report results with models identical to Andreas et al. (2018), where fθ is the final convolutional layer of a fixed ImageNetpretrained VGG-16 (Simonyan and Zisserman, 2015) fed through two fully-connected layers: fθ (x) = FC(ReLU(FC(VGG-16(x)))). (6) However, because fixed ImageNet representations may not be the most appropriate choice for artificial"
2020.acl-main.436,D14-1162,0,0.0915277,"Missing"
2020.acl-main.436,P19-1487,0,0.018048,"etrieval, but do not directly ground language explanations. Srivastava et al. (2017) explore a supervision setting similar to ours, except in simple text and symbolic domains where descriptions can be easily converted to executable logical forms via semantic parsing. Another line of work studies the generation of natural language explanations for interpretability across language (e.g. entailment; Camburu et al., 2018) and vision (Hendricks et al., 2016, 2018) tasks, but here we examine whether predicting language can actually improve task performance; similar ideas have been explored in text (Rajani et al., 2019) and reinforcement learning (Bahdanau et al., 2019; Goyal et al., 2019) domains. 3 Language-shaped learning We are interested in settings where language explanations can help learn representations that generalize more efficiently across tasks, especially when training data for each task is scarce and there are many spurious hypotheses consistent with the input. Thus, we study the few-shot (meta-)learning setting, where a model must learn from a set of train tasks, each with limited data, and then generalize to unseen tasks in the same domain. Specifically, in N -way, K-shot learning, a task (t"
2020.acl-main.436,D17-1161,0,0.109248,"ain data (cf. Hancock et al., 2018). Our setting can be viewed as an instance of learning using privileged information (LUPI; Vapnik and Vashist, 2009), where richer supervision augments a model only during training. In this framework, learning with attributes and other domain-specific rationales has been tackled extensively (Zaidan et al., 2007; Donahue and Grauman, 2011; Tokmakov et al., 2019); language less so. Gordo and Larlus (2017) use METEOR scores between captions as a similarity measure for specializing embeddings for image retrieval, but do not directly ground language explanations. Srivastava et al. (2017) explore a supervision setting similar to ours, except in simple text and symbolic domains where descriptions can be easily converted to executable logical forms via semantic parsing. Another line of work studies the generation of natural language explanations for interpretability across language (e.g. entailment; Camburu et al., 2018) and vision (Hendricks et al., 2016, 2018) tasks, but here we examine whether predicting language can actually improve task performance; similar ideas have been explored in text (Rajani et al., 2019) and reinforcement learning (Bahdanau et al., 2019; Goyal et al."
2020.acl-main.436,N07-1033,0,0.0755434,"(Xing et al., 2019) learning. Unlike past work, we have no language at test time and test tasks differ from training tasks, so language from training cannot be used as additional class information (cf. He and Peng, 2017) or weak supervision for labeling additional in-domain data (cf. Hancock et al., 2018). Our setting can be viewed as an instance of learning using privileged information (LUPI; Vapnik and Vashist, 2009), where richer supervision augments a model only during training. In this framework, learning with attributes and other domain-specific rationales has been tackled extensively (Zaidan et al., 2007; Donahue and Grauman, 2011; Tokmakov et al., 2019); language less so. Gordo and Larlus (2017) use METEOR scores between captions as a similarity measure for specializing embeddings for image retrieval, but do not directly ground language explanations. Srivastava et al. (2017) explore a supervision setting similar to ours, except in simple text and symbolic domains where descriptions can be easily converted to executable logical forms via semantic parsing. Another line of work studies the generation of natural language explanations for interpretability across language (e.g. entailment; Camburu"
2020.acl-main.503,W06-1615,0,0.244158,"etection. 2.1 Extrapolation to out-of-domain data Extrapolating from training data to test data from a different distribution is an important challenge for current NLP models (Yogatama et al., 2019). Models trained on many domains may still struggle to generalize to new domains, as these may involve new types of questions or require different reasoning skills (Talmor and Berant, 2019; Fisch et al., 2019). Related work on domain adaptation also tries to generalize to new distributions, but assumes some knowledge about the test distribution, such as unlabeled examples or a few labeled examples (Blitzer et al., 2006; Daume III, 2007); we assume no such access to the test distribution, but instead make the weaker assumption of access to samples from a different OOD distribution. 2.2 Selective prediction Selective prediction, in which a model can either predict or abstain on each test example, is a longstanding research area in machine learning (Chow, 1957; El-Yaniv and Wiener, 2010; Geifman and El-Yaniv, 2017). In NLP, Dong et al. (2018) use a calibrator to obtain better confidence estimates for semantic parsing. Rodriguez et al. (2019) use a similar approach to decide when to answer QuizBowl questions. T"
2020.acl-main.503,D19-5801,1,0.831452,"predict whether the QA model was correct on any given example. The calibrator’s training data consists of both previously held-out source data and known OOD data. Finally, the combined selective QA system is tested on a mixture of test data from the source distribution and an unknown OOD distribution. Introduction Question answering (QA) models have achieved impressive performance when trained and tested on examples from the same dataset, but tend to perform poorly on examples that are out-of-domain (OOD) (Jia and Liang, 2017; Chen et al., 2017; Yogatama et al., 2019; Talmor and Berant, 2019; Fisch et al., 2019). Deployed QA systems in search engines and personal assistants need to gracefully handle OOD inputs, as users often ask questions that fall outside of the system’s training distribution. While the ideal system would correctly answer all OOD questions, such perfection is not attainable given limited training data (Geiger et al., 2019). Instead, we aim for a more achievable yet still challenging goal: models should abstain when they are likely to err, thus avoiding showing wrong answers to users. This general goal motivates the setting of selective prediction, in which a model outputs both a pr"
2020.acl-main.503,D15-1075,0,0.0156941,"ombines two important goals for real-world systems: knowing when to abstain, and handling distribution shift at test time. We show that models are overconfident on OOD examples, leading to poor performance in the our setting, but training a calibrator using other OOD data can help correct for this problem. While we focus on question answering, our framework is general and extends to any prediction task for which graceful handling of out-of-domain inputs is necessary. Across many tasks, NLP models struggle on out-of-domain inputs. Models trained on standard natural language inference datasets (Bowman et al., 2015) generalize poorly to other distributions (Thorne et al., 2018; Naik et al., 2018). Achieving high accuracy on out-of-domain data may not even be possible if the test data requires abilities that are not learnable from the training data (Geiger et al., 2019). Adversarially chosen ungrammatical text can also cause catastrophic errors (Wallace et al., 2019; Cheng et al., 2020). In all these cases, a more intelligent model would recognize that it should abstain on these inputs. Traditional NLU systems typically have a natural ability to abstain. SHRDLU recognizes statements that it cannot parse,"
2020.acl-main.503,D19-5817,0,0.0438933,"Missing"
2020.acl-main.503,P17-1171,0,0.0448876,"l is trained only on source data. Then, a calibrator is trained to predict whether the QA model was correct on any given example. The calibrator’s training data consists of both previously held-out source data and known OOD data. Finally, the combined selective QA system is tested on a mixture of test data from the source distribution and an unknown OOD distribution. Introduction Question answering (QA) models have achieved impressive performance when trained and tested on examples from the same dataset, but tend to perform poorly on examples that are out-of-domain (OOD) (Jia and Liang, 2017; Chen et al., 2017; Yogatama et al., 2019; Talmor and Berant, 2019; Fisch et al., 2019). Deployed QA systems in search engines and personal assistants need to gracefully handle OOD inputs, as users often ask questions that fall outside of the system’s training distribution. While the ideal system would correctly answer all OOD questions, such perfection is not attainable given limited training data (Geiger et al., 2019). Instead, we aim for a more achievable yet still challenging goal: models should abstain when they are likely to err, thus avoiding showing wrong answers to users. This general goal motivates th"
2020.acl-main.503,D19-1456,0,0.211379,"(QA) models have achieved impressive performance when trained and tested on examples from the same dataset, but tend to perform poorly on examples that are out-of-domain (OOD) (Jia and Liang, 2017; Chen et al., 2017; Yogatama et al., 2019; Talmor and Berant, 2019; Fisch et al., 2019). Deployed QA systems in search engines and personal assistants need to gracefully handle OOD inputs, as users often ask questions that fall outside of the system’s training distribution. While the ideal system would correctly answer all OOD questions, such perfection is not attainable given limited training data (Geiger et al., 2019). Instead, we aim for a more achievable yet still challenging goal: models should abstain when they are likely to err, thus avoiding showing wrong answers to users. This general goal motivates the setting of selective prediction, in which a model outputs both a prediction and a scalar confidence, and abstains on inputs where its confidence is low (El-Yaniv and Wiener, 2010; Geifman and El-Yaniv, 2017). In this paper, we propose the setting of selective question answering under domain shift, which captures two important aspects of real-world QA: (i) test data often diverges from the training di"
2020.acl-main.503,N19-1405,0,0.106857,"Missing"
2020.acl-main.503,P07-1033,0,0.446189,"Missing"
2020.acl-main.503,N19-1423,0,0.060798,"otQA, we focused on multi-hop questions by selecting only “hard” examples, as defined by Yang et al. (2018). In each experiment, two different OOD datasets are chosen as qknown and qunk . All results are averaged over all 20 such combinations, unless otherwise specified. We sample 2,000 examples from qknown for Dcalib , and 4,000 SQuAD and 4,000 qunk examples for Dtest . We evaluate using exact match (EM) accuracy, as defined by SQuAD (Rajpurkar et al., 2016). Additional details can be found in Appendix A.1. QA model. For our QA model, we use the BERTbase SQuAD 1.1 model trained for 2 epochs (Devlin et al., 2019). We train six models total: one fsrc and five fsrc+known ’s, one for each OOD dataset. Selective prediction methods. For test-time dropout, we use K = 30 different dropout masks, as in Dong et al. (2018). For our calibrator, we use the random forest implementation from Scikitlearn (Pedregosa et al., 2011). We train on 1,600 SQuAD examples and 1,600 known OOD examples, and use the remaining 400 SQuAD and 400 known OOD examples as a validation set to tune calibrator hyperparameters via grid search. We average our results over 10 random splits of this data. When training the calibrator only on p"
2020.acl-main.503,P18-1069,0,0.415774,"diction (Hendrycks and Gimpel, 2017; Lakshminarayanan et al., 2017). We find that MaxProb gives good confidence estimates on in-domain data, but is overconfident on OOD data. Therefore, MaxProb performs poorly in mixed settings: it does not abstain enough on OOD examples, relative to in-domain examples. We correct for MaxProb’s overconfidence by using known OOD data to train a calibrator—a classifier trained to predict whether the original QA model is correct or incorrect on a given example (Platt, 1999; Zadrozny and Elkan, 2002). While prior work in NLP trains a calibrator on in-domain data (Dong et al., 2018), we show this does not generalize to unknown OOD data as well as training on a mixture of in-domain and known OOD data. Figure 1 illustrates the problem setup and how the calibrator uses known OOD data. We use a simple random forest calibrator over features derived from the input example and the model’s softmax outputs. We conduct extensive experiments using SQuAD (Rajpurkar et al., 2016) as the source distribution and five other QA datasets as different OOD distributions. We average across all 20 choices of using one as the unknown OOD dataset and another as the known OOD dataset, and test o"
2020.acl-main.503,D17-1215,1,0.852926,"tor. First, a QA model is trained only on source data. Then, a calibrator is trained to predict whether the QA model was correct on any given example. The calibrator’s training data consists of both previously held-out source data and known OOD data. Finally, the combined selective QA system is tested on a mixture of test data from the source distribution and an unknown OOD distribution. Introduction Question answering (QA) models have achieved impressive performance when trained and tested on examples from the same dataset, but tend to perform poorly on examples that are out-of-domain (OOD) (Jia and Liang, 2017; Chen et al., 2017; Yogatama et al., 2019; Talmor and Berant, 2019; Fisch et al., 2019). Deployed QA systems in search engines and personal assistants need to gracefully handle OOD inputs, as users often ask questions that fall outside of the system’s training distribution. While the ideal system would correctly answer all OOD questions, such perfection is not attainable given limited training data (Geiger et al., 2019). Instead, we aim for a more achievable yet still challenging goal: models should abstain when they are likely to err, thus avoiding showing wrong answers to users. This genera"
2020.acl-main.503,P17-1147,0,0.14129,"Missing"
2020.acl-main.503,N07-1066,0,0.432455,"d to avoid costly errors (Feng et al., 2019). In computational chemistry, Toplak et al. (2014) use 5685 selective prediction techniques to estimate the set of (possibly out-of-domain) molecules for which a reactivity classifier is reliable. To the best of our knowledge, our work is the first to study selective prediction under domain shift in NLP. Answer validation. Traditional pipelined systems for open-domain QA often have dedicated systems for answer validation—judging whether a proposed answer is correct. These systems often rely on external knowledge about entities (Magnini et al., 2002; Ko et al., 2007). Knowing when to abstain has been part of past QA shared tasks like RespubliQA (Pe˜nas et al., 2009) and QA4MRE (Pe˜nas et al., 2013). IBM’s Watson system for Jeopardy also uses a pipelined approach for answer validation (Gondek et al., 2012). Our work differs by focusing on modern neural QA systems trained end-to-end, rather than pipelined systems, and by viewing the problem of abstention in QA through the lens of selective prediction. Outlier detection. We distinguish selective prediction under domain shift from outlier detection, the task of detecting out-of-domain examples (Sch¨olkopf et"
2020.acl-main.503,Q19-1026,0,0.192155,"Missing"
2020.acl-main.503,P18-2124,1,0.887353,"judged on their ability to rank correct predictions higher than incorrect predictions (El-Yaniv and Wiener, 2010). In contrast, calibration error depends on the absolute confidence scores. Nonetheless, we will find it useful to analyze calibration in Section 5.3, as miscalibration on some examples but not others does imply poor relative ordering, and therefore poor selective prediction. Ovadia et al. (2019) observe increases in calibration error under domain shift. Identifying unanswerable questions. In SQuAD 2.0, models must recognize when a paragraph does not entail an answer to a question (Rajpurkar et al., 2018). Sentence selection systems must rank passages that answer a question higher than passages that do not (Wang et al., 2007; Yang et al., 2015). In these cases, the goal is to “abstain” when no system (or person) could infer an answer to the given question using the given passage. In contrast, in selective prediction, the model should abstain when it would give a wrong answer if forced to make a prediction. Problem Setup Selective Prediction Given an input x, the selective prediction task is to output (ˆ y , c) where yˆ ∈ Y (x), the set of answer candidates, and c ∈ R denotes the model’s confid"
2020.acl-main.503,D16-1264,1,0.940682,"ampled. Second, qunk is an unknown OOD distribution, representing out-of-domain data encountered at test time. The test dataset Dtest is sampled from ptest , a mixture of psource and qunk : ptest = αpsource + (1 − α)qunk (1) for α ∈ (0, 1). We choose α = 12 , and examine the effect of changing this ratio in Section 5.8. Third, qknown is a known OOD distribution, representing examples not in psource but from which the system developer has a small dataset Dcalib . 3.3 Selective Question Answering While our framework is general, we focus on extractive question answering, as exemplified by SQuAD (Rajpurkar et al., 2016), due to its practical importance and the diverse array of available QA datasets in the same format. The input x is a passage-question pair (p, q), and the set of answer candidates Y (x) is all spans of the passage p. A base model f defines a probability distribution f (y |x) over Y (x). All selective prediction methods we consider choose yˆ = arg maxy0 ∈Y (x) f (y 0 | x), but differ in their associated confidence c. 4 4.2 Test-time Dropout For neural networks, another standard approach to estimate confidence is to use dropout at test time. Gal and Ghahramani (2016) showed that dropout gives g"
2020.acl-main.503,P02-1054,0,0.496089,"ve prediction is needed to avoid costly errors (Feng et al., 2019). In computational chemistry, Toplak et al. (2014) use 5685 selective prediction techniques to estimate the set of (possibly out-of-domain) molecules for which a reactivity classifier is reliable. To the best of our knowledge, our work is the first to study selective prediction under domain shift in NLP. Answer validation. Traditional pipelined systems for open-domain QA often have dedicated systems for answer validation—judging whether a proposed answer is correct. These systems often rely on external knowledge about entities (Magnini et al., 2002; Ko et al., 2007). Knowing when to abstain has been part of past QA shared tasks like RespubliQA (Pe˜nas et al., 2009) and QA4MRE (Pe˜nas et al., 2013). IBM’s Watson system for Jeopardy also uses a pipelined approach for answer validation (Gondek et al., 2012). Our work differs by focusing on modern neural QA systems trained end-to-end, rather than pipelined systems, and by viewing the problem of abstention in QA through the lens of selective prediction. Outlier detection. We distinguish selective prediction under domain shift from outlier detection, the task of detecting out-of-domain exampl"
2020.acl-main.503,P19-1416,0,0.0295312,"Missing"
2020.acl-main.503,C18-1198,0,0.0250914,"dling distribution shift at test time. We show that models are overconfident on OOD examples, leading to poor performance in the our setting, but training a calibrator using other OOD data can help correct for this problem. While we focus on question answering, our framework is general and extends to any prediction task for which graceful handling of out-of-domain inputs is necessary. Across many tasks, NLP models struggle on out-of-domain inputs. Models trained on standard natural language inference datasets (Bowman et al., 2015) generalize poorly to other distributions (Thorne et al., 2018; Naik et al., 2018). Achieving high accuracy on out-of-domain data may not even be possible if the test data requires abilities that are not learnable from the training data (Geiger et al., 2019). Adversarially chosen ungrammatical text can also cause catastrophic errors (Wallace et al., 2019; Cheng et al., 2020). In all these cases, a more intelligent model would recognize that it should abstain on these inputs. Traditional NLU systems typically have a natural ability to abstain. SHRDLU recognizes statements that it cannot parse, or that it finds ambiguous (Winograd, 1972). QUALM answers reading comprehension q"
2020.acl-main.503,D19-1432,1,0.88463,"Missing"
2020.acl-main.503,P19-1485,0,0.18546,"calibrator is trained to predict whether the QA model was correct on any given example. The calibrator’s training data consists of both previously held-out source data and known OOD data. Finally, the combined selective QA system is tested on a mixture of test data from the source distribution and an unknown OOD distribution. Introduction Question answering (QA) models have achieved impressive performance when trained and tested on examples from the same dataset, but tend to perform poorly on examples that are out-of-domain (OOD) (Jia and Liang, 2017; Chen et al., 2017; Yogatama et al., 2019; Talmor and Berant, 2019; Fisch et al., 2019). Deployed QA systems in search engines and personal assistants need to gracefully handle OOD inputs, as users often ask questions that fall outside of the system’s training distribution. While the ideal system would correctly answer all OOD questions, such perfection is not attainable given limited training data (Geiger et al., 2019). Instead, we aim for a more achievable yet still challenging goal: models should abstain when they are likely to err, thus avoiding showing wrong answers to users. This general goal motivates the setting of selective prediction, in which a mo"
2020.acl-main.503,N18-1074,0,0.0154851,"n to abstain, and handling distribution shift at test time. We show that models are overconfident on OOD examples, leading to poor performance in the our setting, but training a calibrator using other OOD data can help correct for this problem. While we focus on question answering, our framework is general and extends to any prediction task for which graceful handling of out-of-domain inputs is necessary. Across many tasks, NLP models struggle on out-of-domain inputs. Models trained on standard natural language inference datasets (Bowman et al., 2015) generalize poorly to other distributions (Thorne et al., 2018; Naik et al., 2018). Achieving high accuracy on out-of-domain data may not even be possible if the test data requires abilities that are not learnable from the training data (Geiger et al., 2019). Adversarially chosen ungrammatical text can also cause catastrophic errors (Wallace et al., 2019; Cheng et al., 2020). In all these cases, a more intelligent model would recognize that it should abstain on these inputs. Traditional NLU systems typically have a natural ability to abstain. SHRDLU recognizes statements that it cannot parse, or that it finds ambiguous (Winograd, 1972). QUALM answers rea"
2020.acl-main.503,W17-2623,0,0.1939,"Missing"
2020.acl-main.503,D19-1221,0,0.0291337,"ork is general and extends to any prediction task for which graceful handling of out-of-domain inputs is necessary. Across many tasks, NLP models struggle on out-of-domain inputs. Models trained on standard natural language inference datasets (Bowman et al., 2015) generalize poorly to other distributions (Thorne et al., 2018; Naik et al., 2018). Achieving high accuracy on out-of-domain data may not even be possible if the test data requires abilities that are not learnable from the training data (Geiger et al., 2019). Adversarially chosen ungrammatical text can also cause catastrophic errors (Wallace et al., 2019; Cheng et al., 2020). In all these cases, a more intelligent model would recognize that it should abstain on these inputs. Traditional NLU systems typically have a natural ability to abstain. SHRDLU recognizes statements that it cannot parse, or that it finds ambiguous (Winograd, 1972). QUALM answers reading comprehension questions by constructing reasoning chains, and abstains if it cannot find one that supports an answer (Lehnert, 1977). NLP systems deployed in real-world settings inevitably encounter a mixture of familiar and unfamiliar inputs. Our work provides a framework to study how mo"
2020.acl-main.503,D07-1003,0,0.0913727,"calibration error depends on the absolute confidence scores. Nonetheless, we will find it useful to analyze calibration in Section 5.3, as miscalibration on some examples but not others does imply poor relative ordering, and therefore poor selective prediction. Ovadia et al. (2019) observe increases in calibration error under domain shift. Identifying unanswerable questions. In SQuAD 2.0, models must recognize when a paragraph does not entail an answer to a question (Rajpurkar et al., 2018). Sentence selection systems must rank passages that answer a question higher than passages that do not (Wang et al., 2007; Yang et al., 2015). In these cases, the goal is to “abstain” when no system (or person) could infer an answer to the given question using the given passage. In contrast, in selective prediction, the model should abstain when it would give a wrong answer if forced to make a prediction. Problem Setup Selective Prediction Given an input x, the selective prediction task is to output (ˆ y , c) where yˆ ∈ Y (x), the set of answer candidates, and c ∈ R denotes the model’s confidence. Given a threshold γ ∈ R, the overall system predicts yˆ if c ≥ γ and abstain otherwise. The risk-coverage curve prov"
2020.acl-main.503,H89-1033,0,0.762782,"distributions (Thorne et al., 2018; Naik et al., 2018). Achieving high accuracy on out-of-domain data may not even be possible if the test data requires abilities that are not learnable from the training data (Geiger et al., 2019). Adversarially chosen ungrammatical text can also cause catastrophic errors (Wallace et al., 2019; Cheng et al., 2020). In all these cases, a more intelligent model would recognize that it should abstain on these inputs. Traditional NLU systems typically have a natural ability to abstain. SHRDLU recognizes statements that it cannot parse, or that it finds ambiguous (Winograd, 1972). QUALM answers reading comprehension questions by constructing reasoning chains, and abstains if it cannot find one that supports an answer (Lehnert, 1977). NLP systems deployed in real-world settings inevitably encounter a mixture of familiar and unfamiliar inputs. Our work provides a framework to study how models can more judiciously abstain in these challenging environments. Reproducibility. All code, data and experiments are available on the Codalab platform at https: //bit.ly/35inCah. Acknowledgments. This work was supported by the DARPA ASED program under FA8650-18-27882. We thank Anany"
2020.acl-main.503,D15-1237,0,0.0495215,"epends on the absolute confidence scores. Nonetheless, we will find it useful to analyze calibration in Section 5.3, as miscalibration on some examples but not others does imply poor relative ordering, and therefore poor selective prediction. Ovadia et al. (2019) observe increases in calibration error under domain shift. Identifying unanswerable questions. In SQuAD 2.0, models must recognize when a paragraph does not entail an answer to a question (Rajpurkar et al., 2018). Sentence selection systems must rank passages that answer a question higher than passages that do not (Wang et al., 2007; Yang et al., 2015). In these cases, the goal is to “abstain” when no system (or person) could infer an answer to the given question using the given passage. In contrast, in selective prediction, the model should abstain when it would give a wrong answer if forced to make a prediction. Problem Setup Selective Prediction Given an input x, the selective prediction task is to output (ˆ y , c) where yˆ ∈ Y (x), the set of answer candidates, and c ∈ R denotes the model’s confidence. Given a threshold γ ∈ R, the overall system predicts yˆ if c ≥ γ and abstain otherwise. The risk-coverage curve provides a standard way"
2020.acl-main.503,D18-1259,0,0.0805305,"described in Section 4.2, and also add cDropoutVar as a feature. As discussed above, dropout features are costly to compute and assume white-box access to the model, but may result in better confidence estimates. Both of these variables can be changed independently, leading to four configurations. 5 sages and questions (e.g., whether questions are written based on passages, or passages retrieved based on questions). We used the preprocessed data from the MRQA 2019 shared task (Fisch et al., 2019). For HotpotQA, we focused on multi-hop questions by selecting only “hard” examples, as defined by Yang et al. (2018). In each experiment, two different OOD datasets are chosen as qknown and qunk . All results are averaged over all 20 such combinations, unless otherwise specified. We sample 2,000 examples from qknown for Dcalib , and 4,000 SQuAD and 4,000 qunk examples for Dtest . We evaluate using exact match (EM) accuracy, as defined by SQuAD (Rajpurkar et al., 2016). Additional details can be found in Appendix A.1. QA model. For our QA model, we use the BERTbase SQuAD 1.1 model trained for 2 epochs (Devlin et al., 2019). We train six models total: one fsrc and five fsrc+known ’s, one for each OOD dataset."
2020.blackboxnlp-1.26,D16-1032,0,0.0299536,"to terminate. We explore training probe to find this linear separator, and are able to predict where sequences end with high accuracy (Table 3). In our other experiments, we did not see such a simple possible solution, but can speculate as to what it would require. In particular, the length manifold and length attractor behaviors seem to indicate that length extrapolation fails because the conditions for stopping are tracked in these models more or less in terms of absolute linear position. As such, it is possible that a successful parameterization may make use of an implicit checklist model (Kiddon et al., 2016), that checks off which parts of the input have been accounted for in the output. Conclusion In this work, we studied a decision often overlooked in NLP: modeling the probability of ending the generative process through a special token in a neural decoder’s output vocabulary. We trained neural models to predict this special EOS token and studied how this objective affected their behavior and representations across three diverse tasks. Our quantitative evaluations took place in an oracle setting in which we forced all models to generate until the optimal sequence length at test time. Under this"
2020.blackboxnlp-1.26,P17-4012,0,0.0659463,"Missing"
2020.findings-emnlp.305,P19-1612,0,0.0284188,"nking and mine hard negative examples, an approach related to adaptive retrieval. However, they have abundant labeled data, whereas we study data collection with a limited labeling budget. Work in information retrieval often attempts to maximize precision across all pairs of test objects. Machine learning models are commonly used to re-rank candidate pairs from an upstream retriever (Chen et al., 2017; Nogueira and Cho, 2019), while our method learns embeddings to improve the initial retrieval step. Distant supervision has been used to train end-to-end retrieval models for question answering (Lee et al., 2019), but does not extend to other tasks like paraphrase detection. Other work on duplicate question detection on community QA forums trains on labels generated by forum users (dos Santos et al., 2015). Hoogeveen et al. (2016) show that these datasets tend to have many false negatives and suggests additional labeling to correct this problem; active learning provides one way to choose informative pairs to label. Extreme label imbalance is an important challenge in many non-pairwise NLP tasks, including document classification (Lewis et al., 2004) and relation extraction (Zhang et al., 2017). Most p"
2020.findings-emnlp.305,D19-1410,0,0.0293867,"BERT embeds x1 and x2 independently and predicts pθ (y = 1 |x) based on vector-space similarity. More precisely,   eθ (x1 ) · eθ (x2 ) pθ (y = 1 |x) = σ w · +b , keθ (x1 )kkeθ (x2 )k (7) where σ is the sigmoid function, w &gt; 0 and b are learnable parameters, and eθ : X1 ∪ X2 → Rd is a learnable embedding function. In other words, we compute the cosine similarity of the embeddings of x1 and x2 , and predict y using a logistic regression model with cosine similarity as its only feature. We define eθ as the final layer output of a BERT model (Devlin et al., 2019) mean-pooled across all tokens (Reimers and Gurevych, 2019).5 Gillick et al. (2019) used a similar model for entity linking. 5 Although WikiQA involves an asymmetric relationship between questions and sentences, we use the same encoder for both. This is still expressive enough for WikiQA, since the set 4.2.2 Finding points to query Next, we show how to choose the batch Bi of points to query, given a model pθ (y |x) trained on data from batches B1 , . . . , Bi−1 . Recall that uncertainty sampling chooses the points x for which for which pθ (y = 1 |x) is closest to 12 , and adaptive retrieval chooses the points x with largest pθ (y = 1 |x). Since the se"
2020.findings-emnlp.305,K17-1034,0,0.0209613,"ization (Geiger et al., 2019), and how to collect training data in a way that improves generalization. Evaluating on extremely imbalanced all-pairs data has several advantages over other tests of generalization. Our examples are realistic and natural, unlike adversarial perturbations (Ebrahimi et al., 2018; Alzantot et al., 2018), and diverse, unlike hand-crafted tests of specific phenomena (Glockner et al., 2018; Naik et al., 2018; McCoy et al., 2019). Since we allow querying the label of any training example, generalization to our test data is achievable, while out-of-domain generalization (Levy et al., 2017; Yogatama et al., 2019; Talmor and Berant, 2019) may be statistically impossible. Our work thus offers a natural, challenging, and practically relevant testbed to study both generalization and data collection. Reproducibility. Code and data needed to reproduce all results can be found on the CodaLab platform at https://bit.ly/2GzJAgM. 3408 Acknowledgments This work was supported by a PECASE Award, NSF Award Grant no. 1805310, and NSF Graduate Fellowship DGE-1656518. We thank Chris Manning, Michael Xie, Dallas Card, and other members of the Stanford NLP Group for their helpful comments. Refere"
2020.findings-emnlp.305,K17-1004,0,0.0132545,"d that the frequency of questions in QQP leaks information about the label. Evaluating on all pairs avoids such artifacts, as every test utterance appears in the same number of examples. Zhang et al. (2019) re-weight the original dataset to avoid these biases, but re-weighting cannot compensate for the absence of some types of negative examples, unlike active learning. Many pairwise datasets are generated by asking crowdworkers to generate part or all of the input x (Bowman et al., 2015; Mostafazadeh et al., 2016). Having crowdworkers generate text increases the risk of introducing artifacts (Schwartz et al., 2017; Poliak et al., 2018), while our pool-based approach considers the entire distribution of utterance pairs. We use active learning, specifically uncertainty sampling (Lewis and Gale, 1994), to create a balanced training set that leads to models that generalize to the full imbalanced distribution. Ertekin et al. (2007) argues that active learning is capable of providing balanced classes to the learning algorithm by selecting examples close to the decision boundary. Furthermore, active learning can generalize to the full distribution, both empirically (Settles, 2009; Yang and Loog, 2018) and the"
2020.findings-emnlp.305,D17-1122,0,0.045162,"Missing"
2020.findings-emnlp.305,P19-1485,0,0.0256628,"collect training data in a way that improves generalization. Evaluating on extremely imbalanced all-pairs data has several advantages over other tests of generalization. Our examples are realistic and natural, unlike adversarial perturbations (Ebrahimi et al., 2018; Alzantot et al., 2018), and diverse, unlike hand-crafted tests of specific phenomena (Glockner et al., 2018; Naik et al., 2018; McCoy et al., 2019). Since we allow querying the label of any training example, generalization to our test data is achievable, while out-of-domain generalization (Levy et al., 2017; Yogatama et al., 2019; Talmor and Berant, 2019) may be statistically impossible. Our work thus offers a natural, challenging, and practically relevant testbed to study both generalization and data collection. Reproducibility. Code and data needed to reproduce all results can be found on the CodaLab platform at https://bit.ly/2GzJAgM. 3408 Acknowledgments This work was supported by a PECASE Award, NSF Award Grant no. 1805310, and NSF Graduate Fellowship DGE-1656518. We thank Chris Manning, Michael Xie, Dallas Card, and other members of the Stanford NLP Group for their helpful comments. References M. Alzantot, Y. Sharma, A. Elgohary, B. Ho,"
2020.findings-emnlp.305,P19-1334,0,0.0241116,"and Long, 2013; Mussmann and Liang, 2018). Finally, this paper addresses two central concerns in NLP today: How to construct fair but challenging tests of generalization (Geiger et al., 2019), and how to collect training data in a way that improves generalization. Evaluating on extremely imbalanced all-pairs data has several advantages over other tests of generalization. Our examples are realistic and natural, unlike adversarial perturbations (Ebrahimi et al., 2018; Alzantot et al., 2018), and diverse, unlike hand-crafted tests of specific phenomena (Glockner et al., 2018; Naik et al., 2018; McCoy et al., 2019). Since we allow querying the label of any training example, generalization to our test data is achievable, while out-of-domain generalization (Levy et al., 2017; Yogatama et al., 2019; Talmor and Berant, 2019) may be statistically impossible. Our work thus offers a natural, challenging, and practically relevant testbed to study both generalization and data collection. Reproducibility. Code and data needed to reproduce all results can be found on the CodaLab platform at https://bit.ly/2GzJAgM. 3408 Acknowledgments This work was supported by a PECASE Award, NSF Award Grant no. 1805310, and NSF"
2020.findings-emnlp.305,D15-1237,0,0.0967801,"Missing"
2020.findings-emnlp.305,N16-1098,0,0.0259577,"e find pre-trained embeddings effective for seed set collection in pairwise tasks. Zhang et al. (2019) found that the frequency of questions in QQP leaks information about the label. Evaluating on all pairs avoids such artifacts, as every test utterance appears in the same number of examples. Zhang et al. (2019) re-weight the original dataset to avoid these biases, but re-weighting cannot compensate for the absence of some types of negative examples, unlike active learning. Many pairwise datasets are generated by asking crowdworkers to generate part or all of the input x (Bowman et al., 2015; Mostafazadeh et al., 2016). Having crowdworkers generate text increases the risk of introducing artifacts (Schwartz et al., 2017; Poliak et al., 2018), while our pool-based approach considers the entire distribution of utterance pairs. We use active learning, specifically uncertainty sampling (Lewis and Gale, 1994), to create a balanced training set that leads to models that generalize to the full imbalanced distribution. Ertekin et al. (2007) argues that active learning is capable of providing balanced classes to the learning algorithm by selecting examples close to the decision boundary. Furthermore, active learning"
2020.findings-emnlp.305,C18-1198,0,0.0225096,"t al., 2007; Balcan and Long, 2013; Mussmann and Liang, 2018). Finally, this paper addresses two central concerns in NLP today: How to construct fair but challenging tests of generalization (Geiger et al., 2019), and how to collect training data in a way that improves generalization. Evaluating on extremely imbalanced all-pairs data has several advantages over other tests of generalization. Our examples are realistic and natural, unlike adversarial perturbations (Ebrahimi et al., 2018; Alzantot et al., 2018), and diverse, unlike hand-crafted tests of specific phenomena (Glockner et al., 2018; Naik et al., 2018; McCoy et al., 2019). Since we allow querying the label of any training example, generalization to our test data is achievable, while out-of-domain generalization (Levy et al., 2017; Yogatama et al., 2019; Talmor and Berant, 2019) may be statistically impossible. Our work thus offers a natural, challenging, and practically relevant testbed to study both generalization and data collection. Reproducibility. Code and data needed to reproduce all results can be found on the CodaLab platform at https://bit.ly/2GzJAgM. 3408 Acknowledgments This work was supported by a PECASE Award, NSF Award Grant"
2020.findings-emnlp.305,S18-2023,0,0.0199051,"questions in QQP leaks information about the label. Evaluating on all pairs avoids such artifacts, as every test utterance appears in the same number of examples. Zhang et al. (2019) re-weight the original dataset to avoid these biases, but re-weighting cannot compensate for the absence of some types of negative examples, unlike active learning. Many pairwise datasets are generated by asking crowdworkers to generate part or all of the input x (Bowman et al., 2015; Mostafazadeh et al., 2016). Having crowdworkers generate text increases the risk of introducing artifacts (Schwartz et al., 2017; Poliak et al., 2018), while our pool-based approach considers the entire distribution of utterance pairs. We use active learning, specifically uncertainty sampling (Lewis and Gale, 1994), to create a balanced training set that leads to models that generalize to the full imbalanced distribution. Ertekin et al. (2007) argues that active learning is capable of providing balanced classes to the learning algorithm by selecting examples close to the decision boundary. Furthermore, active learning can generalize to the full distribution, both empirically (Settles, 2009; Yang and Loog, 2018) and theoretically (Balcan et"
2020.findings-emnlp.305,P19-1435,0,0.025955,"ce is an important challenge in many non-pairwise NLP tasks, including document classification (Lewis et al., 2004) and relation extraction (Zhang et al., 2017). Most prior work focuses on sampling a fixed training dataset (Chawla et al., 2004; Sun et al., 2009; Dendamrongvit and Kubat, 2009), whereas our work explores data collection. Attenberg and Provost (2010) find stratified sampling outperforms active learning in non-pairwise imbalanced tasks, primarily due to the difficulty of finding a useful seed set. We find pre-trained embeddings effective for seed set collection in pairwise tasks. Zhang et al. (2019) found that the frequency of questions in QQP leaks information about the label. Evaluating on all pairs avoids such artifacts, as every test utterance appears in the same number of examples. Zhang et al. (2019) re-weight the original dataset to avoid these biases, but re-weighting cannot compensate for the absence of some types of negative examples, unlike active learning. Many pairwise datasets are generated by asking crowdworkers to generate part or all of the input x (Bowman et al., 2015; Mostafazadeh et al., 2016). Having crowdworkers generate text increases the risk of introducing artifa"
2020.intexsempar-1.4,P16-1002,1,0.849721,"”) into lower-level utterances that it can (e.g. “go to the coffee mug and pick it up”, “go to the sink and put it inside”, etc. — see Figure 1). To map language to executable behavior, Wang et al. (2017) and Thomason et al. (2019) built adaptive NLIs that leverage grammar-based parsers that allow reliable one-shot generalization but lack lexical flexibility. For example, a grammar-based system that understands how to “wash the coffee mug” may not generalize to “clean the mug.” Meanwhile, recent semantic parsers are based primarily on neural sequence-to-sequence models (Dong and Lapata, 2016; Jia and Liang, 2016; Guu et al., 2017). While these models excel from a lexical flexibility perspective, they lack the ability to perform reliable one-shot generalization: it is difficult to train them to generalize from individual examples (Koehn and Knowles, 2017). In this paper we propose a new interactive NLI that is lexically flexible and can reliably and efficiently perform one-shot generalization. We introduce a novel exemplar-based neural network semantic parser that first abstracts away entities (e.g. “wash the coffee mug” → “wash the &lt;obj&gt;”), allowing for generalization to previously taught utterances"
2020.intexsempar-1.4,D11-1039,0,0.036017,"introduced LIA, a programmable personal assistant that learned from user-provided condition-action rules. Furthermore, Weigelt et al. (2020) introduce an approach for teaching systems new programmatic functions from language that explicitly reasons about whether utterances contain “teaching intents,” a mechanism that is similar to our procedure for returning NOT-SURE. Once these “teaching intents” have been identified, they are parsed into corresponding code blocks that can then be executed. Other work leverages conversations to learn new concepts, generating queries for users to respond to (Artzi and Zettlemoyer, 2011; Thomason et al., 2019). Notably, Thomason et al. (2019) used this conversational structure in a robotics setting similar to ours, but focused on learning new percepts, rather than structural abstractions. Yao et al. (2019) defined a similar conversational system for Text-to-SQL models that decides when intervention is needed, and generates a clarification question accordingly. each other, to facilitate performing more complex behaviors. In robotics, this might translate to building systems for cooking, perhaps taking inspiration from Epic Kitchens (Damen et al., 2018), where the set of high-"
2020.intexsempar-1.4,Q13-1005,0,0.0333472,"terances to accomplish the task (or give up). Instead, we argue that NLIs need to be dynamic and adaptive, learning interactively from user feedback Introduction As robots are deployed in collaborative applications like healthcare and household assistance (Scassellati et al., 2012; Knepper et al., 2013), there is a growing need for reliable human-robot communication. One such communication modality that is both user-friendly and versatile is natural language; to this end, we focus on robust natural language interfaces (NLIs) that can map utterances to executable behavior (Tellex et al., 2011; Artzi and Zettlemoyer, 2013; Thomason et al., 2015; Arumugam et al., 2017; Shridhar et al., 2020). 23 Proceedings of the First Workshop on Interactive and Executable Semantic Parsing, pages 23–33 c Online, November 19, 2020. 2020 Association for Computational Linguistics https://doi.org/10.18653/v1/P17 High-Level Task 1 Clean & Place (Mug, CounterTop) High-Level Task 2 Clean & Place (Tomato, DiningTable) “Clean and put the tomato on the table” —&gt; I’m sorry - I don’t understand! “Wash the coﬀee mug” —&gt; I’m sorry - I don’t understand! Teaching ”Go to the mug and pick it up” —&gt; GOTO Mug; PICKUP Mug ”Go to the sink and put"
2020.intexsempar-1.4,W17-2809,1,0.799718,"ey are confident the system will understand (mirroring our observed results). This suggests future work in building more reliable methods for one-shot generalization and interpretability, providing users with a clear picture of what the model has learned. General Instruction Following. Other work looks at instruction following for robotics tasks outside the semantic parsing paradigm, for example by mapping language directly to sequences of actions (Anderson et al., 2018; Fried et al., 2018; Shridhar et al., 2020), mapping language to representations of reward functions (Arumugam et al., 2017; Karamcheti et al., 2017), or learning languageconditioned policies via reinforcement learning (Hermann et al., 2017; Chaplot et al., 2018). 6 Discussion & Lessons Learned Towards More Complex Settings. Our analysis in Section 4.2 suggests that situating our system in a more complex setting might allow us to truly see the benefits of learning by decomposition. One such setting is Voxelurn (Wang et al., 2017), with its open-ended tasks that allow for the definition of multiple different high-level abstractions with compositional richness. In contrast, the tasks in this work are linear, with similar sequences of primiti"
2020.intexsempar-1.4,W17-3204,0,0.189278,"tive NLIs that leverage grammar-based parsers that allow reliable one-shot generalization but lack lexical flexibility. For example, a grammar-based system that understands how to “wash the coffee mug” may not generalize to “clean the mug.” Meanwhile, recent semantic parsers are based primarily on neural sequence-to-sequence models (Dong and Lapata, 2016; Jia and Liang, 2016; Guu et al., 2017). While these models excel from a lexical flexibility perspective, they lack the ability to perform reliable one-shot generalization: it is difficult to train them to generalize from individual examples (Koehn and Knowles, 2017). In this paper we propose a new interactive NLI that is lexically flexible and can reliably and efficiently perform one-shot generalization. We introduce a novel exemplar-based neural network semantic parser that first abstracts away entities (e.g. “wash the coffee mug” → “wash the &lt;obj&gt;”), allowing for generalization to previously taught utterances with novel object combinations. Our parser then retrieves the corresponding “lifted” utterance and respective program (exemplar) from the training examples based on a learned metric (implemented as a neural network), giving us the lexical flexibil"
2020.intexsempar-1.4,D18-2025,0,0.01921,"t context. Learning from Interaction. Closest to our work is Voxelurn (Wang et al., 2017), and its close predecessor SHRDLURN (Wang et al., 2016). Voxelurn defined an open-ended environment where the goal was to build arbitrary voxel structures using language instructions. We take inspiration from its teaching procedure where users decompose highlevel utterances into low-level actions in the context 30 of a grammar-based parser. Other work uses alternative modes of interaction to teach new behaviors. Srivastava et al. (2017) used natural language explanations to teach new concepts. Relatedly, Labutov et al. (2018) introduced LIA, a programmable personal assistant that learned from user-provided condition-action rules. Furthermore, Weigelt et al. (2020) introduce an approach for teaching systems new programmatic functions from language that explicitly reasons about whether utterances contain “teaching intents,” a mechanism that is similar to our procedure for returning NOT-SURE. Once these “teaching intents” have been identified, they are parsed into corresponding code blocks that can then be executed. Other work leverages conversations to learn new concepts, generating queries for users to respond to ("
2020.intexsempar-1.4,P16-1004,0,0.0190473,"g. “wash the coffee mug”) into lower-level utterances that it can (e.g. “go to the coffee mug and pick it up”, “go to the sink and put it inside”, etc. — see Figure 1). To map language to executable behavior, Wang et al. (2017) and Thomason et al. (2019) built adaptive NLIs that leverage grammar-based parsers that allow reliable one-shot generalization but lack lexical flexibility. For example, a grammar-based system that understands how to “wash the coffee mug” may not generalize to “clean the mug.” Meanwhile, recent semantic parsers are based primarily on neural sequence-to-sequence models (Dong and Lapata, 2016; Jia and Liang, 2016; Guu et al., 2017). While these models excel from a lexical flexibility perspective, they lack the ability to perform reliable one-shot generalization: it is difficult to train them to generalize from individual examples (Koehn and Knowles, 2017). In this paper we propose a new interactive NLI that is lexically flexible and can reliably and efficiently perform one-shot generalization. We introduce a novel exemplar-based neural network semantic parser that first abstracts away entities (e.g. “wash the coffee mug” → “wash the &lt;obj&gt;”), allowing for generalization to previous"
2020.intexsempar-1.4,P11-1060,1,0.767522,"Missing"
2020.intexsempar-1.4,P16-1138,1,0.905353,"Missing"
2020.intexsempar-1.4,2020.findings-emnlp.305,1,0.696433,"comparison with every training example. We streamline this by using the structure of our embedding space — as the classifier outputs the scaled cosine similarity between two utterances, we store the embeddings φ(fi ) for each training utterance (fi , qi ) in our dataset, then use an approximate nearest neighbors algorithm to find the the set of utterances that are “close-enough”; we use the corresponding lifted programs to form the output set Q. We formalize what it means for an utterance to be “close-enough” in the following paragraph. We note that this procedure is similar to C OSINE BERT (Mussman et al., 2020), a model used for active learning on pairwise language tasks. canonical utterances (Wang et al., 2015), or a core grammar (Wang et al., 2017). We strip stop words (the, up, down, on, off, of, in, to, then, a, an, back, front, out, from, with, inside, outside, below, above, top) from f prior to feeding to our parser to make our model more robust to minor lexical variation. 3.1.3 Setting a Threshold. One of the desiderata of our system is returning NOT-SURE for utterances it is not confident about. To do this, we set a threshold τ such that if kφ(f ) − φ(f 0 )k2 ≥ τ , return NOT-SURE. Note that"
2020.intexsempar-1.4,P17-1097,1,0.810085,"tterances that it can (e.g. “go to the coffee mug and pick it up”, “go to the sink and put it inside”, etc. — see Figure 1). To map language to executable behavior, Wang et al. (2017) and Thomason et al. (2019) built adaptive NLIs that leverage grammar-based parsers that allow reliable one-shot generalization but lack lexical flexibility. For example, a grammar-based system that understands how to “wash the coffee mug” may not generalize to “clean the mug.” Meanwhile, recent semantic parsers are based primarily on neural sequence-to-sequence models (Dong and Lapata, 2016; Jia and Liang, 2016; Guu et al., 2017). While these models excel from a lexical flexibility perspective, they lack the ability to perform reliable one-shot generalization: it is difficult to train them to generalize from individual examples (Koehn and Knowles, 2017). In this paper we propose a new interactive NLI that is lexically flexible and can reliably and efficiently perform one-shot generalization. We introduce a novel exemplar-based neural network semantic parser that first abstracts away entities (e.g. “wash the coffee mug” → “wash the &lt;obj&gt;”), allowing for generalization to previously taught utterances with novel object c"
2020.intexsempar-1.4,D19-1547,0,0.0157973,"licitly reasons about whether utterances contain “teaching intents,” a mechanism that is similar to our procedure for returning NOT-SURE. Once these “teaching intents” have been identified, they are parsed into corresponding code blocks that can then be executed. Other work leverages conversations to learn new concepts, generating queries for users to respond to (Artzi and Zettlemoyer, 2011; Thomason et al., 2019). Notably, Thomason et al. (2019) used this conversational structure in a robotics setting similar to ours, but focused on learning new percepts, rather than structural abstractions. Yao et al. (2019) defined a similar conversational system for Text-to-SQL models that decides when intervention is needed, and generates a clarification question accordingly. each other, to facilitate performing more complex behaviors. In robotics, this might translate to building systems for cooking, perhaps taking inspiration from Epic Kitchens (Damen et al., 2018), where the set of high-level objectives (general recipes to follow, kitchen behaviors to imitate) is much larger, but where individual subtasks (low-level abstractions like slicing a vegetable, stirring a pot) are very common and generalizable. Ot"
2020.intexsempar-1.4,D14-1162,0,0.0821923,"Missing"
2020.intexsempar-1.4,D19-1204,0,0.0406323,"Missing"
2020.intexsempar-1.4,D17-1161,0,0.017846,"ic parser handles linguistic context, while our entity resolver and reranker handle environment context. Learning from Interaction. Closest to our work is Voxelurn (Wang et al., 2017), and its close predecessor SHRDLURN (Wang et al., 2016). Voxelurn defined an open-ended environment where the goal was to build arbitrary voxel structures using language instructions. We take inspiration from its teaching procedure where users decompose highlevel utterances into low-level actions in the context 30 of a grammar-based parser. Other work uses alternative modes of interaction to teach new behaviors. Srivastava et al. (2017) used natural language explanations to teach new concepts. Relatedly, Labutov et al. (2018) introduced LIA, a programmable personal assistant that learned from user-provided condition-action rules. Furthermore, Weigelt et al. (2020) introduce an approach for teaching systems new programmatic functions from language that explicitly reasons about whether utterances contain “teaching intents,” a mechanism that is similar to our procedure for returning NOT-SURE. Once these “teaching intents” have been identified, they are parsed into corresponding code blocks that can then be executed. Other work"
2020.intexsempar-1.4,D07-1071,0,0.222637,"Missing"
2020.intexsempar-1.4,P17-1086,1,0.900723,"by decomposing it into other utterances the system can understand (illustrated by brackets above), which eventually get mapped to low-level actions that are executed. This induced mapping of high-level utterance to low-level actions forms an example that we use to update our semantic parser online. Because our semantic parser is capable of reliable one-shot generalization, users can leverage these decompositions when completing the next task. to index and perform more complicated behaviors. In this work, we explore building NLIs for simulated robotics that learn from real humans. Inspired by Wang et al. (2017), we leverage the idea of learning from decomposition to learn new abstractions. Just like how a human interactively teaches a new task to a friend by breaking it down, users interactively teach our system by simplifying utterances that the system cannot understand (e.g. “wash the coffee mug”) into lower-level utterances that it can (e.g. “go to the coffee mug and pick it up”, “go to the sink and put it inside”, etc. — see Figure 1). To map language to executable behavior, Wang et al. (2017) and Thomason et al. (2019) built adaptive NLIs that leverage grammar-based parsers that allow reliable"
2020.intexsempar-1.4,P16-1224,1,0.904703,"Missing"
2020.intexsempar-1.4,P15-1129,1,0.82971,"— as the classifier outputs the scaled cosine similarity between two utterances, we store the embeddings φ(fi ) for each training utterance (fi , qi ) in our dataset, then use an approximate nearest neighbors algorithm to find the the set of utterances that are “close-enough”; we use the corresponding lifted programs to form the output set Q. We formalize what it means for an utterance to be “close-enough” in the following paragraph. We note that this procedure is similar to C OSINE BERT (Mussman et al., 2020), a model used for active learning on pairwise language tasks. canonical utterances (Wang et al., 2015), or a core grammar (Wang et al., 2017). We strip stop words (the, up, down, on, off, of, in, to, then, a, an, back, front, out, from, with, inside, outside, below, above, top) from f prior to feeding to our parser to make our model more robust to minor lexical variation. 3.1.3 Setting a Threshold. One of the desiderata of our system is returning NOT-SURE for utterances it is not confident about. To do this, we set a threshold τ such that if kφ(f ) − φ(f 0 )k2 ≥ τ , return NOT-SURE. Note that this is equivalent to to thresholding the probability output by pθ which is monotonic in the cosine di"
2020.intexsempar-1.4,2020.acl-main.395,0,0.0429719,"016). Voxelurn defined an open-ended environment where the goal was to build arbitrary voxel structures using language instructions. We take inspiration from its teaching procedure where users decompose highlevel utterances into low-level actions in the context 30 of a grammar-based parser. Other work uses alternative modes of interaction to teach new behaviors. Srivastava et al. (2017) used natural language explanations to teach new concepts. Relatedly, Labutov et al. (2018) introduced LIA, a programmable personal assistant that learned from user-provided condition-action rules. Furthermore, Weigelt et al. (2020) introduce an approach for teaching systems new programmatic functions from language that explicitly reasons about whether utterances contain “teaching intents,” a mechanism that is similar to our procedure for returning NOT-SURE. Once these “teaching intents” have been identified, they are parsed into corresponding code blocks that can then be executed. Other work leverages conversations to learn new concepts, generating queries for users to respond to (Artzi and Zettlemoyer, 2011; Thomason et al., 2019). Notably, Thomason et al. (2019) used this conversational structure in a robotics setting"
2020.tacl-1.36,P96-1009,0,0.132698,"uces the same result, or an alternative interpretation that is also contextually appropriate. Count 8 Related work 3 The view of dialogue as an interactive process of shared plan synthesis dates back to Grosz and Sidner’s earliest work on discourse structure (1986; 1988). That work represents the state of a dialogue as a predicate recognizing whether a desired piece of information has been communicated or change in world state effected. Goals can be refined via questions and corrections from both users and agents. The only systems to attempt full versions of this shared-plans framework (e.g., Allen et al., 1996; Rich et al., 2001) required inputs that could be parsed under a predefined grammar. Subsequent research on dialogue understanding has largely focused on two simpler subtasks: Contextual semantic parsing approaches focus on complex language understanding without reasoning about underspecified goals or agent initiative. Here the prototypical problem is iterated question answering (Hemphill et al., 1990; Yu et al., 2019b), in which the user asks a sequence of questions corresponding to database queries, and results of query execution are presented as structured result sets. Vlachos and Clark (2"
2020.tacl-1.36,D18-1547,0,0.165132,"65 1,052 3,315 Dataflow inline .729 .696 .665 .606 .642 .533 .574 .465 .697 .631 .565 .474 Dataflow inline refer inline both TRADE Table 2: SMCalFlow results. Agent action accuracy is significantly higher than a baseline without metacomputation, especially on turns that involve a reference (Ref. Turns) or revision (Rev. Turns) to earlier turns in the dialogue (p &lt; 10−6 , McNemar’s test). Joint Goal Dialogue Prefix .467 .447 .467 .454 .220 .202 .205 .168 3.07 2.97 2.90 2.73 Table 3: MultiWOZ 2.1 test set results. TRADE (Wu et al., 2019) results are from the public implementation. “Joint Goal” (Budzianowski et al., 2018) is average dialogue state exact-match, “Dialogue” is average dialogue-level exact-match, and “Prefix” is the average number of turns before an incorrect prediction. Within each column, the best result is boldfaced, along with all results that are not significantly worse (p &lt; 0.05, paired permutation test). Moreover, all of “Dataflow,” “inline refer,” and “inline both” have higher dialogue accuracy than TRADE (p &lt; 0.005). els that train on inlined metacomputation. These experiments make it possible to evaluate the importance of explicit dataflow manipulation compared to a standard contextual s"
2020.tacl-1.36,D19-1459,0,0.0502204,"bling side-byside comparisons and experiments with alternative representations. We provide full conversion scripts for MultiWOZ. 567 9 teractive dialogues. It is assumed that any user intent can be represented with a flat structure consisting of a categorical dialogue act and a mapping between a fixed set of slots and string-valued fillers. Existing fine-grained dialogue act schemes (Stolcke et al., 2000) can distinguish among a range of communicative intents not modeled by our approach, and slot-filling representations have historically been easier to predict (Zue et al., 1994) and annotate (Byrne et al., 2019). But while recent variants support interaction between related slots (Budzianowski et al., 2018) and fixed-depth hierarchies of slots (Gupta et al., 2018), modern slot-filling approaches remain limited in their support for semantic compositionality. By contrast, our approach supports user requests corresponding to general compositional programs. Conclusions We have presented a representational framework for task-oriented dialogue modeling based on dataflow graphs, in which dialogue agents predict a sequence of compositional updates to a graphical state representation. This approach makes it p"
2020.tacl-1.36,P17-1167,0,0.0181575,"bout underspecified goals or agent initiative. Here the prototypical problem is iterated question answering (Hemphill et al., 1990; Yu et al., 2019b), in which the user asks a sequence of questions corresponding to database queries, and results of query execution are presented as structured result sets. Vlachos and Clark (2014) describe a semantic parsing representation targeted at more general dialogue problems. Most existing methods interpret context-dependent user questions (What is the next flight to Atlanta? When does it land?) by learning to copy subtrees (Zettlemoyer and Collins, 2009; Iyyer et al., 2017; Suhr et al., 2018) or tokens (Zhang et al., 2019) from previously-generated queries. In contrast, our approach reifies reuse with explicit graph operators. Slot-filling approaches (Pieraccini et al., 1992) model simpler utterances in the context of full, inTable 4: Manual classification of 100 model errors on the SMCalFlow dataset. The largest categories are underprediction (omitting steps from agent programs), entity linking (errors in extraction of entities from user utterances, fencing (classifying a user request as outof-scope), and ambiguity (user utterances with multiple possible inter"
2020.tacl-1.36,P17-4012,0,0.0122901,"propriate type constraint, provided that the reference resolution heuristic would retrieve the correct string from earlier in the dataflow. This covers references like the same day. Otherwise, our re-annotation retains the literal string value. Data statistics are shown in Table 1. To the best of our knowledge, SMCalFlow is the largest annotated task-oriented dialogue dataset to date. Compared to MultiWOZ, it features a larger user vocabulary, a more complex space of statemanipulation primitives, and a long tail of agent programs built from numerous function calls and deep composition. 7 NMT (Klein et al., 2017) pointer-generator network (See et al., 2017), a sequence-to-sequence model that can copy tokens from the source sequence while decoding. Our goal is to demonstrate that dataflow-based representations benefit standard neural model architectures. Dataflowspecific modeling might improve on this baseline, and we leave this as a challenge for future work. For each user turn i, we linearize the target program into a sequence of tokens zi . This must be predicted from the dialogue context— namely the concatenated source sequence xi−c zi−c · · · xi−1 zi−1 xi (for SMCalFlow) or xi−c yi−c · · · xi−1 yi"
2020.tacl-1.36,J94-4002,0,0.175555,"tate representations. While a complete description of dataflow-based language generation is beyond the scope of this paper, we briefly describe the components of the generation system relevant to the understanding system presented here. 3 Reference resolution In a dialogue, entities that have been introduced once may be referred to again. In dataflow dialogues, the entities available for reference are given by the nodes in the dataflow graph. Entities are salient to conversation participants to different degrees, and their relative salience determines the ways in which they may be referenced (Lappin and Leass, 1994). For example, it generally refers to the most salient non-human entity, while more specific expressions like the Friday meeting are needed to refer to accessible but less salient entities. Not all references to entities are overt: if the agent says “You have a meeting tomorrow” and the user responds “What time?”, the agent must predict the implicit reference to a salient event. 559 Dataflow pointers We have seen that refer is used to find referents for referring expressions. In general, these referents may be existing dataflow nodes or new subgraphs for newly mentioned entities. We now give m"
2020.tacl-1.36,J86-3001,0,0.780827,"Missing"
2020.tacl-1.36,W18-6322,0,0.0167485,"ser utterances with multiple possible interpretations). See §7 for discussion. Error analysis Beyond the quantitative results shown in Tables 2–3, we manually analyzed 100 SMCalFlow turns where our model mispredicted. Table 4 breaks down the errors by type. Three categories involve straightforward parsing errors. In underprediction errors, the model fails to predict some computation (e.g., a search constraint or property extractor) specified in the user request. This behavior is not specific to our system: under-length predictions are also welldocumented in neural machine translation systems (Murray and Chiang, 2018). In entity linking errors, the model correctly identifies the presence of an entity mention in the input utterance, but uses it incorrectly in the input plan. Sometimes the entity that appears in the plan is hallucinated, appearing nowhere in the utterance; sometimes the entity is cast to a wrong type (e.g., locations interpreted as event names) used in the wrong field or extracted with wrong boundaries. In fencing errors, the model interprets an out-of-scope user utterance as an interpretable command, or vice-versa versions of the full dataset, and inlined and non-inlined versions of our mod"
2020.tacl-1.36,D18-1300,0,0.0282209,"It is assumed that any user intent can be represented with a flat structure consisting of a categorical dialogue act and a mapping between a fixed set of slots and string-valued fillers. Existing fine-grained dialogue act schemes (Stolcke et al., 2000) can distinguish among a range of communicative intents not modeled by our approach, and slot-filling representations have historically been easier to predict (Zue et al., 1994) and annotate (Byrne et al., 2019). But while recent variants support interaction between related slots (Budzianowski et al., 2018) and fixed-depth hierarchies of slots (Gupta et al., 2018), modern slot-filling approaches remain limited in their support for semantic compositionality. By contrast, our approach supports user requests corresponding to general compositional programs. Conclusions We have presented a representational framework for task-oriented dialogue modeling based on dataflow graphs, in which dialogue agents predict a sequence of compositional updates to a graphical state representation. This approach makes it possible to represent and learn from complex, natural dialogues. Future work might focus on improving prediction by introducing learned implementations of r"
2020.tacl-1.36,H90-1021,0,0.582952,"ted or change in world state effected. Goals can be refined via questions and corrections from both users and agents. The only systems to attempt full versions of this shared-plans framework (e.g., Allen et al., 1996; Rich et al., 2001) required inputs that could be parsed under a predefined grammar. Subsequent research on dialogue understanding has largely focused on two simpler subtasks: Contextual semantic parsing approaches focus on complex language understanding without reasoning about underspecified goals or agent initiative. Here the prototypical problem is iterated question answering (Hemphill et al., 1990; Yu et al., 2019b), in which the user asks a sequence of questions corresponding to database queries, and results of query execution are presented as structured result sets. Vlachos and Clark (2014) describe a semantic parsing representation targeted at more general dialogue problems. Most existing methods interpret context-dependent user questions (What is the next flight to Atlanta? When does it land?) by learning to copy subtrees (Zettlemoyer and Collins, 2009; Iyyer et al., 2017; Suhr et al., 2018) or tokens (Zhang et al., 2019) from previously-generated queries. In contrast, our approach"
2020.tacl-1.36,D14-1162,0,0.0839852,"with a separator token that indicates the speaker (user or agent). Our formulation of context for MultiWOZ is standard (e.g., Wu et al., 2019). We take the source and target vocabularies to consist of all words that occur in (respectively) the source and target sequences in training data, as just defined. The model is trained using the Adam optimizer (Kingma and Ba, 2015) with the maximum likelihood objective. We use 0.001 as the learning rate. Training ends when there have been two different epochs that increased the development loss. We use Glove800B-300d (cased) and Glove6B300d (uncased) (Pennington et al., 2014) to initialize the vocabulary embeddings for the SMCalFlow and MultiWoZ experiments, respectively. The context window size c, hidden layer size d, number of hidden layers l, and dropout rates r are selected based on the agent action accuracy (for SMCalFlow) or dialogue-level exact match (for MultiWoZ) on the development set from {2, 4, 10}, {256, 300, 320, 384}, {1, 2, 3}, {0.3, 0.5, 0.7} respectively. Approximate 1-best decoding uses a beam of size 5. Quantitative evaluation Table 2 shows results for the SMCalFlow dataset. We report program accuracy: specifically, exact-match accuracy of the"
2020.tacl-1.36,P17-1062,0,0.0717515,"Missing"
2020.tacl-1.36,P19-1078,0,0.0158945,"work. For each user turn i, we linearize the target program into a sequence of tokens zi . This must be predicted from the dialogue context— namely the concatenated source sequence xi−c zi−c · · · xi−1 zi−1 xi (for SMCalFlow) or xi−c yi−c · · · xi−1 yi−1 xi (for MultiWOZ 2.1). Here c is a context window size, xj is the user utterance at user turn j, yj is the agent’s naturallanguage response, and zj is the linearized agent program. Each sequence xj , yj , or zj begins with a separator token that indicates the speaker (user or agent). Our formulation of context for MultiWOZ is standard (e.g., Wu et al., 2019). We take the source and target vocabularies to consist of all words that occur in (respectively) the source and target sequences in training data, as just defined. The model is trained using the Adam optimizer (Kingma and Ba, 2015) with the maximum likelihood objective. We use 0.001 as the learning rate. Training ends when there have been two different epochs that increased the development loss. We use Glove800B-300d (cased) and Glove6B300d (uncased) (Pennington et al., 2014) to initialize the vocabulary embeddings for the SMCalFlow and MultiWoZ experiments, respectively. The context window s"
2020.tacl-1.36,P17-1099,0,0.0599554,"Missing"
2020.tacl-1.36,J00-3003,0,0.766051,"Missing"
2020.tacl-1.36,N18-1203,0,0.0172756,"goals or agent initiative. Here the prototypical problem is iterated question answering (Hemphill et al., 1990; Yu et al., 2019b), in which the user asks a sequence of questions corresponding to database queries, and results of query execution are presented as structured result sets. Vlachos and Clark (2014) describe a semantic parsing representation targeted at more general dialogue problems. Most existing methods interpret context-dependent user questions (What is the next flight to Atlanta? When does it land?) by learning to copy subtrees (Zettlemoyer and Collins, 2009; Iyyer et al., 2017; Suhr et al., 2018) or tokens (Zhang et al., 2019) from previously-generated queries. In contrast, our approach reifies reuse with explicit graph operators. Slot-filling approaches (Pieraccini et al., 1992) model simpler utterances in the context of full, inTable 4: Manual classification of 100 model errors on the SMCalFlow dataset. The largest categories are underprediction (omitting steps from agent programs), entity linking (errors in extraction of entities from user utterances, fencing (classifying a user request as outof-scope), and ambiguity (user utterances with multiple possible interpretations). See §7"
2020.tacl-1.36,P19-1443,0,0.056047,"state effected. Goals can be refined via questions and corrections from both users and agents. The only systems to attempt full versions of this shared-plans framework (e.g., Allen et al., 1996; Rich et al., 2001) required inputs that could be parsed under a predefined grammar. Subsequent research on dialogue understanding has largely focused on two simpler subtasks: Contextual semantic parsing approaches focus on complex language understanding without reasoning about underspecified goals or agent initiative. Here the prototypical problem is iterated question answering (Hemphill et al., 1990; Yu et al., 2019b), in which the user asks a sequence of questions corresponding to database queries, and results of query execution are presented as structured result sets. Vlachos and Clark (2014) describe a semantic parsing representation targeted at more general dialogue problems. Most existing methods interpret context-dependent user questions (What is the next flight to Atlanta? When does it land?) by learning to copy subtrees (Zettlemoyer and Collins, 2009; Iyyer et al., 2017; Suhr et al., 2018) or tokens (Zhang et al., 2019) from previously-generated queries. In contrast, our approach reifies reuse wi"
2020.tacl-1.36,Q14-1042,0,0.0241672,", Allen et al., 1996; Rich et al., 2001) required inputs that could be parsed under a predefined grammar. Subsequent research on dialogue understanding has largely focused on two simpler subtasks: Contextual semantic parsing approaches focus on complex language understanding without reasoning about underspecified goals or agent initiative. Here the prototypical problem is iterated question answering (Hemphill et al., 1990; Yu et al., 2019b), in which the user asks a sequence of questions corresponding to database queries, and results of query execution are presented as structured result sets. Vlachos and Clark (2014) describe a semantic parsing representation targeted at more general dialogue problems. Most existing methods interpret context-dependent user questions (What is the next flight to Atlanta? When does it land?) by learning to copy subtrees (Zettlemoyer and Collins, 2009; Iyyer et al., 2017; Suhr et al., 2018) or tokens (Zhang et al., 2019) from previously-generated queries. In contrast, our approach reifies reuse with explicit graph operators. Slot-filling approaches (Pieraccini et al., 1992) model simpler utterances in the context of full, inTable 4: Manual classification of 100 model errors o"
2020.tacl-1.36,E17-1042,0,0.105146,"Missing"
2020.tacl-1.36,P09-1110,0,0.038465,"derstanding without reasoning about underspecified goals or agent initiative. Here the prototypical problem is iterated question answering (Hemphill et al., 1990; Yu et al., 2019b), in which the user asks a sequence of questions corresponding to database queries, and results of query execution are presented as structured result sets. Vlachos and Clark (2014) describe a semantic parsing representation targeted at more general dialogue problems. Most existing methods interpret context-dependent user questions (What is the next flight to Atlanta? When does it land?) by learning to copy subtrees (Zettlemoyer and Collins, 2009; Iyyer et al., 2017; Suhr et al., 2018) or tokens (Zhang et al., 2019) from previously-generated queries. In contrast, our approach reifies reuse with explicit graph operators. Slot-filling approaches (Pieraccini et al., 1992) model simpler utterances in the context of full, inTable 4: Manual classification of 100 model errors on the SMCalFlow dataset. The largest categories are underprediction (omitting steps from agent programs), entity linking (errors in extraction of entities from user utterances, fencing (classifying a user request as outof-scope), and ambiguity (user utterances with mul"
2020.tacl-1.36,W16-3601,0,0.0677409,"Missing"
2020.tacl-1.36,H94-1037,0,0.573739,"odel’s test set predictions, enabling side-byside comparisons and experiments with alternative representations. We provide full conversion scripts for MultiWOZ. 567 9 teractive dialogues. It is assumed that any user intent can be represented with a flat structure consisting of a categorical dialogue act and a mapping between a fixed set of slots and string-valued fillers. Existing fine-grained dialogue act schemes (Stolcke et al., 2000) can distinguish among a range of communicative intents not modeled by our approach, and slot-filling representations have historically been easier to predict (Zue et al., 1994) and annotate (Byrne et al., 2019). But while recent variants support interaction between related slots (Budzianowski et al., 2018) and fixed-depth hierarchies of slots (Gupta et al., 2018), modern slot-filling approaches remain limited in their support for semantic compositionality. By contrast, our approach supports user requests corresponding to general compositional programs. Conclusions We have presented a representational framework for task-oriented dialogue modeling based on dataflow graphs, in which dialogue agents predict a sequence of compositional updates to a graphical state repres"
2021.acl-long.353,E06-1040,0,0.0750714,"Missing"
2021.acl-long.353,N19-1423,0,0.0913987,"Missing"
2021.acl-long.353,W17-3518,0,0.0321664,"e the matrix Pθ [i, :] = MLPθ (Pθ0 [i, :]) by a smaller matrix (Pθ0 ) composed with a large feedforward neural network (MLPθ ). Now, the trainable parameters include Pθ0 and the parameters of MLPθ . Note that Pθ and Pθ0 has the same number of rows (i.e., the prefix length), but different number of columns.4 Once training is complete, these reparametrization parameters can be dropped, and only the prefix (Pθ ) needs to be saved. 5 Experimental Setup 5.1 Datasets and Metrics We evaluate on three standard neural generation datasets for the table-to-text task: E2E (Novikova et al., 2017), WebNLG (Gardent et al., 2017), and DART (Radev et al., 2020), as shown in Table 1. The datasets are ordered by increasing complexity and size. E2E only has 1 domain (i.e. restaurant reviews); WebNLG has 14 domains, and DART is open-domain, using open-domain tables from Wikipedia. For evaluation, we report the metrics using the official evaluation scripts (see details in Appendix A.1). For the summarization task, we use the XSUM (Narayan et al., 2018) dataset, which is an abstractive summarization dataset on news articles. We report ROUGE-1, ROUGE-2 and ROUGE-L. 5.2 Methods For table-to-text generation, we compare prefixtu"
2021.acl-long.353,2021.acl-long.381,0,0.177252,"s no straightforward way to apply these controllable generation techniques to enforce fine-grained control over generated contents, as demanded by tasks like table-to-text and summarization. P*-tuning. Prefix tuning is an instance of a new class of methods that has emerged, which we call p*-tuning (since the other prominent instances, ptuning and prompt-tuning, also start with p), all based on the idea of optimizing a continuous prefix or prompt. Concurrent with our work, Qin and Eisner (2021) learn mixtures of soft fill-in-the-blank prompts to elicit knowledge from LMs such as BERT and BART. Hambardzumyan et al. (2021) learns task-specific embeddings that adapts BERT for sentiment classification. Both works show that tuning soft prompts outperforms previous work, which optimizes over discrete prompts. P-tuning (Liu et al., 2021) shows that jointly updating the prompt embeddings and LM parameters improves GPT-2’s performance on natural language understanding tasks, in both few-shot and full data settings. In a followup work, Prompt-tuning (Lester et al., 2021) simplifies our approach and applies it to T5 (Raffel et al., 2020), demonstrating that the performance gap between fine-tuning and p*tuning vanishes a"
2021.acl-long.353,2020.tacl-1.28,0,0.191809,"Missing"
2021.acl-long.353,2020.inlg-1.14,0,0.24164,"performance when trained on full datasets, prefix-tuning and fine-tuning are comparable for table-to-text (§6.1), while prefix-tuning suffers a small degradation for summarization (§6.2). In low-data settings, prefix-tuning outperforms finetuning on both tasks (§6.3). Prefix-tuning also extrapolates better to tables (for table-to-text) and articles (for summarization) with unseen topics (§6.4). 2 Related Work Fine-tuning for natural language generation. Current state-of-the-art systems for natural language generation (NLG) are based on fine-tuning pretrained LMs. For table-to-text generation, Kale (2020) fine-tunes a sequence-to-sequence model (T5; Raffel et al., 2020). For extractive and abstractive summarization, researchers fine-tune masked language models (e.g., BERT; Devlin et al., 2019) and encode-decoder models (e.g., BART; Lewis et al., 2020), respectively (Zhong et al., 2020; Liu and Lapata, 2019; Raffel et al., 2020). For other conditional NLG tasks such as machine translation and dialogue generation, fine-tuning is also the prevalent paradigm (Zhang et al., 2020c; Stickland et al., 2020; Zhu et al., 2020; Liu et al., 2020). In this paper, we focus on table-to-text using GPT-2 and s"
2021.acl-long.353,W07-0734,0,0.186271,"Missing"
2021.acl-long.353,2021.emnlp-main.243,0,0.336004,"rrent with our work, Qin and Eisner (2021) learn mixtures of soft fill-in-the-blank prompts to elicit knowledge from LMs such as BERT and BART. Hambardzumyan et al. (2021) learns task-specific embeddings that adapts BERT for sentiment classification. Both works show that tuning soft prompts outperforms previous work, which optimizes over discrete prompts. P-tuning (Liu et al., 2021) shows that jointly updating the prompt embeddings and LM parameters improves GPT-2’s performance on natural language understanding tasks, in both few-shot and full data settings. In a followup work, Prompt-tuning (Lester et al., 2021) simplifies our approach and applies it to T5 (Raffel et al., 2020), demonstrating that the performance gap between fine-tuning and p*tuning vanishes as the model size grows. 3 Problem Statement Consider a conditional generation task where the input x is a context and the output y is a sequence of tokens. We focus on two tasks, shown in Figure 2 (right): In table-to-text, x corresponds to a linearized data table and y is a textual description; in summarization, x is an article and y is a summary. 3.1 Autoregressive LM Assume we have an autoregressive neural language model pφ (y |x) parametrize"
2021.acl-long.353,2020.acl-main.703,0,0.528021,"ing on both tasks (§6.3). Prefix-tuning also extrapolates better to tables (for table-to-text) and articles (for summarization) with unseen topics (§6.4). 2 Related Work Fine-tuning for natural language generation. Current state-of-the-art systems for natural language generation (NLG) are based on fine-tuning pretrained LMs. For table-to-text generation, Kale (2020) fine-tunes a sequence-to-sequence model (T5; Raffel et al., 2020). For extractive and abstractive summarization, researchers fine-tune masked language models (e.g., BERT; Devlin et al., 2019) and encode-decoder models (e.g., BART; Lewis et al., 2020), respectively (Zhong et al., 2020; Liu and Lapata, 2019; Raffel et al., 2020). For other conditional NLG tasks such as machine translation and dialogue generation, fine-tuning is also the prevalent paradigm (Zhang et al., 2020c; Stickland et al., 2020; Zhu et al., 2020; Liu et al., 2020). In this paper, we focus on table-to-text using GPT-2 and summarization using BART, but prefix-tuning in principle can be applied to other generation tasks and pretrained models, such as masked LMs. Lightweight fine-tuning. Prefix-tuning falls under the broad class of lightweight fine-tuning methods, which fr"
2021.acl-long.353,W04-1013,0,0.0685606,"Missing"
2021.acl-long.353,D18-1206,0,0.0166297,"saved. 5 Experimental Setup 5.1 Datasets and Metrics We evaluate on three standard neural generation datasets for the table-to-text task: E2E (Novikova et al., 2017), WebNLG (Gardent et al., 2017), and DART (Radev et al., 2020), as shown in Table 1. The datasets are ordered by increasing complexity and size. E2E only has 1 domain (i.e. restaurant reviews); WebNLG has 14 domains, and DART is open-domain, using open-domain tables from Wikipedia. For evaluation, we report the metrics using the official evaluation scripts (see details in Appendix A.1). For the summarization task, we use the XSUM (Narayan et al., 2018) dataset, which is an abstractive summarization dataset on news articles. We report ROUGE-1, ROUGE-2 and ROUGE-L. 5.2 Methods For table-to-text generation, we compare prefixtuning with three other methods: full fine-tuning 3 We find in preliminary experiments that directly optimizing the prefix is very sensitive to initialization. 4 Pθ has dimensions |Pidx |× dim(hi ) while Pθ has dimensions |Pidx |× k. We choose k = 512 for table-to-text and 800 for summarization. MLPθ maps from k to dim(hi ). #examples input length output length E2E WebNLG DART 50K 22K 82K 28.5 49.6 38.8 27.8 30.7 27.3 XSUM"
2021.acl-long.353,W17-5525,0,0.0318707,"performance.3 So we reparametrize the matrix Pθ [i, :] = MLPθ (Pθ0 [i, :]) by a smaller matrix (Pθ0 ) composed with a large feedforward neural network (MLPθ ). Now, the trainable parameters include Pθ0 and the parameters of MLPθ . Note that Pθ and Pθ0 has the same number of rows (i.e., the prefix length), but different number of columns.4 Once training is complete, these reparametrization parameters can be dropped, and only the prefix (Pθ ) needs to be saved. 5 Experimental Setup 5.1 Datasets and Metrics We evaluate on three standard neural generation datasets for the table-to-text task: E2E (Novikova et al., 2017), WebNLG (Gardent et al., 2017), and DART (Radev et al., 2020), as shown in Table 1. The datasets are ordered by increasing complexity and size. E2E only has 1 domain (i.e. restaurant reviews); WebNLG has 14 domains, and DART is open-domain, using open-domain tables from Wikipedia. For evaluation, we report the metrics using the official evaluation scripts (see details in Appendix A.1). For the summarization task, we use the XSUM (Narayan et al., 2018) dataset, which is an abstractive summarization dataset on news articles. We report ROUGE-1, ROUGE-2 and ROUGE-L. 5.2 Methods For table-to-text"
2021.acl-long.353,2020.acl-main.704,0,0.0425012,"Missing"
2021.acl-long.353,P02-1040,0,0.110333,"Missing"
2021.emnlp-main.122,2021.acl-long.145,0,0.0243822,"Ontonotes v5 corpus (Weischedel et al., 2013), recreating the splits used in the CoNLL 2012 shared task, as verified against the split statistics provided by Strubell et al. (2017).78 Since Ontonotes is annotated with constituency parses, not Universal Dependencies, we use the converter provided in CoreNLP (Schuster and Manning, In this work, we intentionally avoid claims as to the “correct” functional family V to be used in conditional probing. Some work has argued for simple probe families (Hewitt and Liang, 2019; Alain and Bengio, 2016), others for complex families (Pimentel et al., 2020b; Hou and Sachan, 2021). 7 In order to provide word vectors for each token in the Pimentel et al. (2020a) argues for choosing mulcorpus, we heuristically align the subword tokenizations of tiple points along an axis of expressivity, while RoBERTa with the corpus-specified tokens through characterlevel alignments, following Tenney et al. (2019). Cao et al. (2021) define the family through the 8 Ontonotes uses the destructive Penn Treebank tokenizaweights of the neural network. Other work pertion (like replacing brackets { with -LCB- (Marcus et al., forms structural analysis of representations without 1993)). We perfo"
2021.emnlp-main.122,P14-5010,1,0.00829756,"ned Conditional 0.15 Bits 0.21 Bits Baselined Conditional 0.65 1 2 3 4 5 6 7 8 9 10 11 12 0.23 upos xpos dep rel ner sst2 0.70 0.20 0.19 Baselined Conditional φ1 φ2 φ1 φ2 0.20 0.20 0.99 0.24 0.18 0.16 0.16 0.81 0.23 0.13 0.22 0.21 1.00 0.25 0.17 0.20 0.20 0.87 0.24 0.13 Table 1: Results on ELMo, reported in bits of Vinformation; higher is better. φi refers to layer i. 0.10 0.05 0.18 Baselined Conditional 1 2 3 4 5 6 7 8 9 10 11 12 Model layer 0.00 1 2 3 4 5 6 7 8 9 10 11 12 Model layer Figure 1: Probing results on RoBERTa. Results are reported in bits of V-information; higher is better. 2016; Manning et al., 2014). For the sentiment annotation, we use the binary GLUE version (Wang et al., 2019) of the the Stanford Sentiment Treebank corpus (Socher et al., 2013). All results are reported on the development sets. Models. We evaluate the popular RoBERTa model (Liu et al., 2019), as provided by the HuggingFace Transformers package (Wolf et al., 2020), as well as the ELMo model (Peters et al., 2018a), as provided by the AllenNLP package (Gardner et al., 2017). When multiple RoBERTa subwords are aligned to a single corpus token, we average the subword vector representations. Probe families. For all of our ex"
2021.emnlp-main.122,J93-2004,0,0.0794903,"Missing"
2021.emnlp-main.122,2020.lrec-1.497,1,0.870655,"Missing"
2021.emnlp-main.122,N18-1202,0,0.716085,"the 2021 Conference on Empirical Methods in Natural Language Processing, pages 1626–1639 c November 7–11, 2021. 2021 Association for Computational Linguistics provides an estimate of conditional V-information IV (repr → property |baseline). In a case study, we answer an open question posed by Hewitt and Liang (2019): how are the aspects of linguistic properties that aren’t explainable by the input layer accessible across the rest of the layers of the network? We find that the partof-speech information not attributable to the input layer remains accessible much deeper into the layers of ELMo (Peters et al., 2018a) and RoBERTa (Liu et al., 2019) than the overall property, a fact previously obscured by the gradual loss across layers of the aspects attributable to the input layer. For the other properties, conditioning on the input layer does not change the trends across layers. Conditional V-information Probing 2 In this section, we describe probing methods and introduce conditional probing. We then review Vinformation and use it to ground probing. 2.1 Probing setup We start with some notation. Let X ∈ X be a random variable taking the value of a sequence of tokens. Let φ(X) be a representation resulti"
2021.emnlp-main.122,D18-1179,0,0.251491,"the 2021 Conference on Empirical Methods in Natural Language Processing, pages 1626–1639 c November 7–11, 2021. 2021 Association for Computational Linguistics provides an estimate of conditional V-information IV (repr → property |baseline). In a case study, we answer an open question posed by Hewitt and Liang (2019): how are the aspects of linguistic properties that aren’t explainable by the input layer accessible across the rest of the layers of the network? We find that the partof-speech information not attributable to the input layer remains accessible much deeper into the layers of ELMo (Peters et al., 2018a) and RoBERTa (Liu et al., 2019) than the overall property, a fact previously obscured by the gradual loss across layers of the aspects attributable to the input layer. For the other properties, conditioning on the input layer does not change the trends across layers. Conditional V-information Probing 2 In this section, we describe probing methods and introduce conditional probing. We then review Vinformation and use it to ground probing. 2.1 Probing setup We start with some notation. Let X ∈ X be a random variable taking the value of a sequence of tokens. Let φ(X) be a representation resulti"
2021.emnlp-main.122,2020.emnlp-main.254,0,0.49658,"ins two probes: (1) on just the baseline, and (2) on the concatenation of the baseline and the representation. The performance of probe (1) is then subtracted from that of probe (2). We call this process conditional probing. Intuitively, the representation is not penalized for lacking aspects of the property accessible in the baseline. We then theoretically ground our probing methodology in V-information, a theory of usable information introduced by Xu et al. (2020) that we additionally extend to multiple predictive variables. We use V-information instead of mutual information (Shannon, 1948; Pimentel et al., 2020b) because any injective deterministic transformation of the input has the same mutual information as the input. For example, a representation that maps each unique sentence to a unique integer must have the same mutual information with any property as does BERT’s representation of that sentence, yet the latter is more useful. In contrast, V-information is defined with respect to a family of functions V that map one random variable to (a probability distribution over) another. V-information can be constructed by deterministic transformations that make a property more accessible to the function"
2021.emnlp-main.122,2020.acl-main.420,0,0.281148,"ins two probes: (1) on just the baseline, and (2) on the concatenation of the baseline and the representation. The performance of probe (1) is then subtracted from that of probe (2). We call this process conditional probing. Intuitively, the representation is not penalized for lacking aspects of the property accessible in the baseline. We then theoretically ground our probing methodology in V-information, a theory of usable information introduced by Xu et al. (2020) that we additionally extend to multiple predictive variables. We use V-information instead of mutual information (Shannon, 1948; Pimentel et al., 2020b) because any injective deterministic transformation of the input has the same mutual information as the input. For example, a representation that maps each unique sentence to a unique integer must have the same mutual information with any property as does BERT’s representation of that sentence, yet the latter is more useful. In contrast, V-information is defined with respect to a family of functions V that map one random variable to (a probability distribution over) another. V-information can be constructed by deterministic transformations that make a property more accessible to the function"
2021.emnlp-main.122,2020.acl-demos.14,1,0.883433,"Missing"
2021.emnlp-main.122,2020.tacl-1.54,0,0.0222857,"the latter is more useful. In contrast, V-information is defined with respect to a family of functions V that map one random variable to (a probability distribution over) another. V-information can be constructed by deterministic transformations that make a property more accessible to the functions in the family. We show that conditional probing Neural language models have become the foundation for modern NLP systems (Devlin et al., 2019; Radford et al., 2018), but what they understand about language, and how they represent that knowledge, is still poorly understood (Belinkov and Glass, 2019; Rogers et al., 2020). The probing methodology grapples with these questions by relating neural representations to well-understood properties. Probing analyzes a representation by using it as input into a supervised classifier, which is trained to predict a property, such as part-of-speech (Shi et al., 2016; Ettinger et al., 2016; Alain and Bengio, 2016; Adi et al., 2017; Belinkov, 2021). 1 One suggests that a representation encodes a Our code is available at https://github.com/ property of interest if probing that representation john-hewitt/conditional-probing. 1626 Proceedings of the 2021 Conference on Empirical"
2021.emnlp-main.611,D19-1435,0,0.110603,"Ng, 2018; Junczys-Dowmunt Break-It-Fix-It (BIFI; Yasunaga and Liang et al., 2018). These methods rely on a combination (2021)) is a recent method to obtain realistic paired of human-labeled data (i.e., hbad, goodi pairs) data from unlabeled data, which has shown promise (Nicholls, 2003; Yannakoudakis et al., 2011; Bryant in the task of source code repair. The idea of BIFI et al., 2019) and synthetic data, which are generated is that using an initial fixer (e.g., trained on synby corrupting good sentences into hsynthetic bad, thetic data) and a critic that tells if an input is bad goodi pairs (Awasthi et al., 2019; Kiyono et al., or good (e.g., compiler, which checks if code has an 2019). Human-labeled pairs are representative error), BIFI iteratively trains the fixer and a breaker of real human errors but are expensive to obtain, to generate better paired data. Specifically, BIFI (1) while synthetic pairs are cheap but are unrealistic, applies the fixer to bad examples and keeps outputs deviating from the distribution of grammatical accepted by the critic, (2) trains a breaker on the re7752 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 7752–7763 c Novemb"
2021.emnlp-main.611,W19-4406,0,0.129222,"We hence compare probabilities in local neighborhood of sentences. Concretely, LM-Critic is defined by two components, an LM (e.g., GPT2) and a neighborhood function (e.g., edit distance 1), and deems a sentence to be grammatical if the LM assigns it the highest probability in its local neighborhood (Figure 1; local optimum criterion). Using this LM-Critic, we apply BIFI to the GEC task. Notably, our approach, both the LM-Critic and GEC learning, does not require labeled data. We evaluate our proposed approach on GEC benchmarks across multiple domains, CoNLL-2014 (Ng et al., 2014), BEA-2019 (Bryant et al., 2019), GMEG-yahoo, and GMEG-wiki (Napoles et al., 2019). We achieve strong performance in the unsupervised setting (i.e., no labeled data), outperforming the baseline fixer trained on synthetic data by 7.7 F0.5 on average. We also evaluate in the supervised setting, where we take the stateof-the-art model GECToR (Omelianchuk et al., 2020) as the baseline fixer, and further fine-tune it by applying our approach using unlabeled data. We achieve 65.8 / 72.9 F0.5 on CoNLL-2014 / BEA-2019, outperforming GECToR by 0.5 F0.5 . Our results also suggest that while existing BIFI assumed access to an oracle cr"
2021.emnlp-main.611,P17-1074,0,0.160619,"ta), and (ii) we can verify if the “bad”-side and “good”-side of the generated pairs are actually “bad” and “good” (Eq 8, 10; red font), which improves the correctness of generated training data compared to vanilla backtranslation (Sennrich et al., 2016; Lample et al., 2018). We refer readers to Yasunaga and Liang (2021) for more details. 4.2 Experiments We study our proposed approach (BIFI with LMCritic) on GEC benchmarks, in both unsupervised and supervised settings. hoo!Answers. For CoNLL-2014, we use the official M2 scorer (Dahlmeier and Ng, 2012), and for others we use the ERRANT metric (Bryant et al., 2017). We describe the training data separately for unsupervised (§4.2.2) and supervised (§4.2.3) settings. 4.2.2 Unsupervised setting Setup and data. We consider the setup with no labeled training data. Existing GEC works (e.g., Awasthi et al. (2019); Omelianchuk et al. (2020)) prepare synthetic paired data by heuristically corrupting sentences from the One-billion-word corpus (Chelba et al., 2013). We follow the same procedure, and train an encoder-decoder Transformer (Vaswani et al., 2017) on this synthetic data to be our baseline fixer. The size of the synthetic data is 9M pairs. We then apply"
2021.emnlp-main.611,2020.emnlp-main.389,0,0.0430094,"ervised setting, there is a distributional shift problem—the synthetically-generated paired data does not match the distribution of grammatical errors humans make. BIFI is a solution for obtaining realistic paired data in an unsupervised way, but it requires a critic. This led us to design a critic for GEC in this work. We note that LM-Critic is not meant to replace existing evaluation metrics for GEC, but rather is an approximate critic to assess grammaticality and help the learning of GEC. Separately, several works (Tenney et al., 2019; Hewitt and Manning, 2019; Yasunaga and Lafferty, 2019; Cao et al., 2020) induce grammar or syntactic structures from LMs, suggesting that LMs can learn about grammaticality in an unsupervised way. As this capacity is likely to grow with the size of LMs (Radford et al., 2019; Brown et al., 2020; Kaplan et al., 2020), we think that how to leverage pretrained LMs for GEC will become an increasingly important research problem. 6 Conclusion We presented LM-Critic, a method that uses a pretrained language model (LM) as a critic for assessing sentence grammaticality. Using LM-Critic and the BIFI algorithm, we learn grammatical error correction (GEC) by generating realist"
2021.emnlp-main.611,W19-3702,0,0.0238724,"pairs. Automatic text evaluation. Popular metrics used to assess the quality of text in GEC include GLEU (Napoles et al., 2015, 2017), M2 (Dahlmeier and Ng, 2012), ERRANT (Bryant et al., 2017) and I-measure (Felice and Briscoe, 2015). While these methods require reference text to compare to, LM-Critic does not. Several prior works also study reference-less methods to assess grammaticality of text: Wan et al. (2005); Mutton et al. (2007); Vadlapudi and Katragadda (2010) use part-ofspeech (POS) tagger or parser predictions to score grammaticality; Napoles et al. (2016); Warstadt et al. (2018); Katinskaia et al. (2019); Niu and Penn (2020) train grammatical error detection (GED) or acceptability judgement systems. However, these works require POS taggers, parsers or GED systems trained on labeled data, which may not scale or generalize well beyond the domain of training data. In contrast, LM-Critic only requires an LM, which is unsupervised and can be pretrained on various domains of unlabeled corpora. Pretrained LM for text evaluation. Several works use pretrained LMs for text evaluation. For reference-based metrics, Zhang et al. (2020) use an GEC model outputs. In Table 7, we analyze LM’s embeddings to me"
2021.emnlp-main.611,P18-1097,0,0.081451,"first perform a simple check to make sure that LMs’ probability score correlates with grammaticality (§3.3.2). We then study the performance of LM-Critic judging grammaticality (§3.3.3). The analysis we conduct in this section is just an intrinsic evaluation of LM-Critic. Our main goal is to use LM-Critic with BIFI for learning GEC, which we describe and evaluate in §4. 3.3.1 Evaluation data To gain insights into how well LM-Critic judges grammaticality, we prepare a simple evaluation data consisting of (xbad ,xgood ) sentence pairs. As experimenting with multiple datasets is desired in GEC (Ge et al., 2018), we construct a combined evaluation set from the dev sets of multiple GEC benchmarks, GMEG-wiki (Napoles et al., 2019), GMEG-yahoo, and BEA-2019 (Bryant et al., 2019), which span the domains of Wikipedia, Yahoo!Answers, and essay/learner English. Specifically, we sampled ∼600 labeled pairs of (xbad ,xgood ) in total from the three benchmarks. We filter out examples where xbad = xgood in this process. We acknowledge that while we use annotated (xbad ,xgood ) pairs for the evaluation here, this does not fully match the way LM-Critic will be used in BIFI (§4), where the critic is run on unlabele"
2021.emnlp-main.611,W19-4413,0,0.0220828,"but our model (“+BIFI”) succeeds. We find that the baseline tends to make unnecessary edits (e.g., changing verb inflection or articles), due to the heuristics used when generating synthetic data. In contrast, BIFI achieves higher precision. 5 Related work and discussion Grammatical error correction (GEC). GEC models are commonly trained from human-labeled data (Nicholls, 2003; Dahlmeier et al., 2013; Yannakoudakis et al., 2011; Bryant et al., 2019), or synthetic data generated by heuristically corrupting unlabeled sentences (Awasthi et al., 2019; Zhao et al., 2019; Grundkiewicz et al., 2019; Katsumata and Komachi, 2019; Omelianchuk et al., 2020). Several works aim to improve the methods for generating paired data, such as learning a breaker from existing labeled data (Lichtarge et al., 2019), applying backtranslation (Sennrich et al., 2016) to GEC (Xie et al., 2018; Kiyono et al., 2019), and synthesizing extra paired data by comparing model predictions and references (Ge et al., 2018). Different from the above works, our method (i) does not require labeled data (works for both unsupervised and supervised settings), and (ii) uses LM-Critic to filter the “bad”-side and “good”-side of generated pairs. Automati"
2021.emnlp-main.611,W19-4427,0,0.0599848,"Good ✘ Bad (grammatical) (ungrammatical) Good Fixer She likes cats. (b) Idea behind LM-Critic: Local optimum criterion Figure 1: Illustration of LM-Critic. (a) In this work, we train a fixer for grammatical error correction (GEC) by leveraging LM-Critic that assesses the grammaticality. (b) LM-Critic deems a sentence to be grammatical if a pretrained language model (e.g., GPT2) assigns it a higher probability than candidates in its local neighborhood (e.g., edit distance 1). Grammatical error correction (GEC) is the task of fixing grammatical errors in text, such as typos, errors humans make (Grundkiewicz et al., 2019). tense and article mistakes. Recent works cast GEC How to obtain inexpensive yet realistic paired data as a translation problem, using encoder-decoder to improve GEC remains a key challenge, especially models to map bad (ungrammatical) sentences in domains or languages with no labeled GEC data into good (grammatical) sentences (Yuan and (Napoles et al., 2019; Náplava and Straka, 2019). Briscoe, 2016; Xie et al., 2016; Ji et al., 2017; Chollampatt and Ng, 2018; Junczys-Dowmunt Break-It-Fix-It (BIFI; Yasunaga and Liang et al., 2018). These methods rely on a combination (2021)) is a recent metho"
2021.emnlp-main.611,N19-1419,0,0.0198379,"t we work on unsupervised learning of GEC. In the unsupervised setting, there is a distributional shift problem—the synthetically-generated paired data does not match the distribution of grammatical errors humans make. BIFI is a solution for obtaining realistic paired data in an unsupervised way, but it requires a critic. This led us to design a critic for GEC in this work. We note that LM-Critic is not meant to replace existing evaluation metrics for GEC, but rather is an approximate critic to assess grammaticality and help the learning of GEC. Separately, several works (Tenney et al., 2019; Hewitt and Manning, 2019; Yasunaga and Lafferty, 2019; Cao et al., 2020) induce grammar or syntactic structures from LMs, suggesting that LMs can learn about grammaticality in an unsupervised way. As this capacity is likely to grow with the size of LMs (Radford et al., 2019; Brown et al., 2020; Kaplan et al., 2020), we think that how to leverage pretrained LMs for GEC will become an increasingly important research problem. 6 Conclusion We presented LM-Critic, a method that uses a pretrained language model (LM) as a critic for assessing sentence grammaticality. Using LM-Critic and the BIFI algorithm, we learn grammati"
2021.emnlp-main.611,P17-1070,0,0.019247,"rhood (e.g., edit distance 1). Grammatical error correction (GEC) is the task of fixing grammatical errors in text, such as typos, errors humans make (Grundkiewicz et al., 2019). tense and article mistakes. Recent works cast GEC How to obtain inexpensive yet realistic paired data as a translation problem, using encoder-decoder to improve GEC remains a key challenge, especially models to map bad (ungrammatical) sentences in domains or languages with no labeled GEC data into good (grammatical) sentences (Yuan and (Napoles et al., 2019; Náplava and Straka, 2019). Briscoe, 2016; Xie et al., 2016; Ji et al., 2017; Chollampatt and Ng, 2018; Junczys-Dowmunt Break-It-Fix-It (BIFI; Yasunaga and Liang et al., 2018). These methods rely on a combination (2021)) is a recent method to obtain realistic paired of human-labeled data (i.e., hbad, goodi pairs) data from unlabeled data, which has shown promise (Nicholls, 2003; Yannakoudakis et al., 2011; Bryant in the task of source code repair. The idea of BIFI et al., 2019) and synthetic data, which are generated is that using an initial fixer (e.g., trained on synby corrupting good sentences into hsynthetic bad, thetic data) and a critic that tells if an input is"
2021.emnlp-main.611,2020.acl-main.245,1,0.834102,"d let B(x) be samples from b(x). To check 1 We acknowledge that this assumption may not hold in some cases, e.g., an ungrammatical sentence may have no correction (“asdfghgfdsa”—just a random typo?) or multiple corrections (“The cat sleep.”—change “sleep” to the present tense or past?). We accept this assumption considering that it is often sufficient in common GEC datasets, and leave the relaxation of the assumption for future work. • ED1. Given a sentence, we generate edit-distance one (ED1) perturbations in the character space. Following prior works in typo generation (Pruthi et al., 2019; Jones et al., 2020), we randomly insert a lowercase letter, delete a character, replace a character, or swap two adjacent characters. • ED1 + Word-level heuristics (all). ED1 can cover most of the character-level typos but may not cover word-level grammatical errors, such as missing an article. Besides ED1, here we include heuristics for word-level perturbations used in Awasthi et al. (2019), which randomly inserts, deletes, or replaces a word based on its dictionary. Please refer to Awasthi et al. for more details. • ED1 + Word-level heuristics. We noticed that the above word-level heuristics include perturbati"
2021.emnlp-main.611,N18-1055,0,0.0497046,"Missing"
2021.emnlp-main.611,K18-1031,0,0.01907,"D systems trained on labeled data, which may not scale or generalize well beyond the domain of training data. In contrast, LM-Critic only requires an LM, which is unsupervised and can be pretrained on various domains of unlabeled corpora. Pretrained LM for text evaluation. Several works use pretrained LMs for text evaluation. For reference-based metrics, Zhang et al. (2020) use an GEC model outputs. In Table 7, we analyze LM’s embeddings to measure the similarity between examples where the baseline fixer trained on input text and reference text. For reference-less 7759 metrics, several works (Kann et al., 2018; Stahlberg et al., 2019) use an LM’s probability as a fluency score of text. While this provides a continuous score for fluency, it in itself cannot classify grammatical / ungrammatical sentences. Our LM-Critic goes a step further to consider the local optimum criterion for classifying grammaticality. The reason we want a classifier (critic) is that we work on unsupervised learning of GEC. In the unsupervised setting, there is a distributional shift problem—the synthetically-generated paired data does not match the distribution of grammatical errors humans make. BIFI is a solution for obtaini"
2021.emnlp-main.611,W19-4414,0,0.0295874,"Missing"
2021.emnlp-main.611,D19-1119,0,0.0561286,"EC. Here, we propose to apply BIFI to the GEC task by using LM-Critic as the critic (§4.1), and evaluate this approach on GEC benchmarks (§4.2). The difference from the original BIFI is that our task is GEC rather than code repair, and we use an approximate critic (i.e., LM-Critic) instead of an oracle critic (i.e., compiler). 4.1 Approach Our goal is to learn a fixer f that maps an ungrammatical sentence xbad into the grammatical version xgood . A common method to obtain paired data for GEC from unlabeled text is to heuristically corrupt good sentences (synthetic data) (Awasthi et al., 2019; Kiyono et al., 2019). However, such synthetic errors do not match the distributions of real grammatical errors humans make, which may result in accuracy drops (Daume III and Marcu, 2006). To mitigate this mismatch, BIFI aims to obtain more realistic paired data and train the fixer on it. Specifically, BIFI takes as inputs: We also analyze when LM-Critic fails. When • Critic c, for which we use LM-Critic LM-Critic predicts a false “good” (labeled “bad” • Unlabeled data Dunlabel . Using the critic c, but predicted “good”), it is commonly because of examples in Dunlabel can be split into bad ones 7756 Dbad = {x |x ∈"
2021.emnlp-main.611,2020.acl-main.703,0,0.029371,"s likely to contain both ungrammatical and grammatical sentences. Hence, we take 10M sentences in total from the Yahoo!Answers corpus (Zhang et al., 2015) and the Wikipedia histories data (Grundkiewicz and JunczysDowmunt, 2014) for which we take sentences prior to revisions.2 This unlabeled data is in the domains of two of our benchmarks (GMEG-wiki and GMEGyahoo) but not of CoNLL-2014 and BEA-2019. Implementation details. The encoder-decoder Transformer architecture has 12 layers, 16 attention heads and hidden state size of 768. The model parameters are initialized with the BART-base release (Lewis et al., 2020), and then optimized by Adam (Kingma and Ba, 2015), with batch size of 512 sequences, learning rate 0.0001, and gradient clipping 1.0 (Pascanu et al., 2013), on a single GTX Titan X GPU. For generation, we use beam search with beam size 10. We run the BIFI algorithm for K = 1 round. The total training time takes 2 days. Results. Table 4 shows the results on the four GEC benchmarks. “Transformers” is our baseline fixer, trained on the synthetic paired data. Our proposed approach (“+BIFI”) outperforms the baseline by substantial margins across the benchmarks, e.g., +8 F0.5 on GMEG-wiki and yahoo"
2021.emnlp-main.611,N19-1333,0,0.0147313,"synthetic data. In contrast, BIFI achieves higher precision. 5 Related work and discussion Grammatical error correction (GEC). GEC models are commonly trained from human-labeled data (Nicholls, 2003; Dahlmeier et al., 2013; Yannakoudakis et al., 2011; Bryant et al., 2019), or synthetic data generated by heuristically corrupting unlabeled sentences (Awasthi et al., 2019; Zhao et al., 2019; Grundkiewicz et al., 2019; Katsumata and Komachi, 2019; Omelianchuk et al., 2020). Several works aim to improve the methods for generating paired data, such as learning a breaker from existing labeled data (Lichtarge et al., 2019), applying backtranslation (Sennrich et al., 2016) to GEC (Xie et al., 2018; Kiyono et al., 2019), and synthesizing extra paired data by comparing model predictions and references (Ge et al., 2018). Different from the above works, our method (i) does not require labeled data (works for both unsupervised and supervised settings), and (ii) uses LM-Critic to filter the “bad”-side and “good”-side of generated pairs. Automatic text evaluation. Popular metrics used to assess the quality of text in GEC include GLEU (Napoles et al., 2015, 2017), M2 (Dahlmeier and Ng, 2012), ERRANT (Bryant et al., 2017"
2021.emnlp-main.611,I11-1017,0,0.0434841,"or the perturbation function in LM-Critic (§3.2) to further improve the critic, which may in turn help BIFI as well as GEC performance, creating a positive loop of learning. 4.2.3 Supervised setting GEC result F0.5 GEC system 50 40 0 1k 10k no BIFI BIFI 100k 1,000k Labeled training data Figure 3: GEC results (y-axis) when varying the amount of labeled data available for training (x-axis). BIFI is particularly helpful in low-resource regimes. BEA-2019 task, which is the combination of: • NUS Corpus of Learner English (NUCLE) (Dahlmeier et al., 2013) • Lang-8 Corpus of Learner English (Lang-8) (Mizumoto et al., 2011; Tajiri et al., 2012) • FCE dataset (Yannakoudakis et al., 2011) • Write & Improve + LOCNESS Corpus (W&I + LOCNESS) (Bryant et al., 2019) They are all in the domain of CoNLL-2014 and BEA-2019 (learner/essay English). The total size of the labeled data is 1M pairs. We then apply the BIFI training on top of GECToR. As our unlabeled data to be used for BIFI, we use 10M sentences taken from Yahoo! Answers and Wikipedia histories (same as §4.2.2). Implementation details. We use the same hyperparameters and training procedures for GECToR as in Omelianchuk et al. (2020). We run the BIFI algorithm fo"
2021.emnlp-main.611,P07-1044,0,0.0377998,", our method (i) does not require labeled data (works for both unsupervised and supervised settings), and (ii) uses LM-Critic to filter the “bad”-side and “good”-side of generated pairs. Automatic text evaluation. Popular metrics used to assess the quality of text in GEC include GLEU (Napoles et al., 2015, 2017), M2 (Dahlmeier and Ng, 2012), ERRANT (Bryant et al., 2017) and I-measure (Felice and Briscoe, 2015). While these methods require reference text to compare to, LM-Critic does not. Several prior works also study reference-less methods to assess grammaticality of text: Wan et al. (2005); Mutton et al. (2007); Vadlapudi and Katragadda (2010) use part-ofspeech (POS) tagger or parser predictions to score grammaticality; Napoles et al. (2016); Warstadt et al. (2018); Katinskaia et al. (2019); Niu and Penn (2020) train grammatical error detection (GED) or acceptability judgement systems. However, these works require POS taggers, parsers or GED systems trained on labeled data, which may not scale or generalize well beyond the domain of training data. In contrast, LM-Critic only requires an LM, which is unsupervised and can be pretrained on various domains of unlabeled corpora. Pretrained LM for text ev"
2021.emnlp-main.611,D19-5545,0,0.0118135,"it a higher probability than candidates in its local neighborhood (e.g., edit distance 1). Grammatical error correction (GEC) is the task of fixing grammatical errors in text, such as typos, errors humans make (Grundkiewicz et al., 2019). tense and article mistakes. Recent works cast GEC How to obtain inexpensive yet realistic paired data as a translation problem, using encoder-decoder to improve GEC remains a key challenge, especially models to map bad (ungrammatical) sentences in domains or languages with no labeled GEC data into good (grammatical) sentences (Yuan and (Napoles et al., 2019; Náplava and Straka, 2019). Briscoe, 2016; Xie et al., 2016; Ji et al., 2017; Chollampatt and Ng, 2018; Junczys-Dowmunt Break-It-Fix-It (BIFI; Yasunaga and Liang et al., 2018). These methods rely on a combination (2021)) is a recent method to obtain realistic paired of human-labeled data (i.e., hbad, goodi pairs) data from unlabeled data, which has shown promise (Nicholls, 2003; Yannakoudakis et al., 2011; Bryant in the task of source code repair. The idea of BIFI et al., 2019) and synthetic data, which are generated is that using an initial fixer (e.g., trained on synby corrupting good sentences into hsynthetic bad, t"
2021.emnlp-main.611,Q19-1032,0,0.0403722,"Missing"
2021.emnlp-main.611,P15-2097,0,0.0230356,"data, such as learning a breaker from existing labeled data (Lichtarge et al., 2019), applying backtranslation (Sennrich et al., 2016) to GEC (Xie et al., 2018; Kiyono et al., 2019), and synthesizing extra paired data by comparing model predictions and references (Ge et al., 2018). Different from the above works, our method (i) does not require labeled data (works for both unsupervised and supervised settings), and (ii) uses LM-Critic to filter the “bad”-side and “good”-side of generated pairs. Automatic text evaluation. Popular metrics used to assess the quality of text in GEC include GLEU (Napoles et al., 2015, 2017), M2 (Dahlmeier and Ng, 2012), ERRANT (Bryant et al., 2017) and I-measure (Felice and Briscoe, 2015). While these methods require reference text to compare to, LM-Critic does not. Several prior works also study reference-less methods to assess grammaticality of text: Wan et al. (2005); Mutton et al. (2007); Vadlapudi and Katragadda (2010) use part-ofspeech (POS) tagger or parser predictions to score grammaticality; Napoles et al. (2016); Warstadt et al. (2018); Katinskaia et al. (2019); Niu and Penn (2020) train grammatical error detection (GED) or acceptability judgement systems. Howev"
2021.emnlp-main.611,D16-1228,0,0.0193416,"ter the “bad”-side and “good”-side of generated pairs. Automatic text evaluation. Popular metrics used to assess the quality of text in GEC include GLEU (Napoles et al., 2015, 2017), M2 (Dahlmeier and Ng, 2012), ERRANT (Bryant et al., 2017) and I-measure (Felice and Briscoe, 2015). While these methods require reference text to compare to, LM-Critic does not. Several prior works also study reference-less methods to assess grammaticality of text: Wan et al. (2005); Mutton et al. (2007); Vadlapudi and Katragadda (2010) use part-ofspeech (POS) tagger or parser predictions to score grammaticality; Napoles et al. (2016); Warstadt et al. (2018); Katinskaia et al. (2019); Niu and Penn (2020) train grammatical error detection (GED) or acceptability judgement systems. However, these works require POS taggers, parsers or GED systems trained on labeled data, which may not scale or generalize well beyond the domain of training data. In contrast, LM-Critic only requires an LM, which is unsupervised and can be pretrained on various domains of unlabeled corpora. Pretrained LM for text evaluation. Several works use pretrained LMs for text evaluation. For reference-based metrics, Zhang et al. (2020) use an GEC model out"
2021.emnlp-main.611,P12-2039,0,0.0172614,"ction in LM-Critic (§3.2) to further improve the critic, which may in turn help BIFI as well as GEC performance, creating a positive loop of learning. 4.2.3 Supervised setting GEC result F0.5 GEC system 50 40 0 1k 10k no BIFI BIFI 100k 1,000k Labeled training data Figure 3: GEC results (y-axis) when varying the amount of labeled data available for training (x-axis). BIFI is particularly helpful in low-resource regimes. BEA-2019 task, which is the combination of: • NUS Corpus of Learner English (NUCLE) (Dahlmeier et al., 2013) • Lang-8 Corpus of Learner English (Lang-8) (Mizumoto et al., 2011; Tajiri et al., 2012) • FCE dataset (Yannakoudakis et al., 2011) • Write & Improve + LOCNESS Corpus (W&I + LOCNESS) (Bryant et al., 2019) They are all in the domain of CoNLL-2014 and BEA-2019 (learner/essay English). The total size of the labeled data is 1M pairs. We then apply the BIFI training on top of GECToR. As our unlabeled data to be used for BIFI, we use 10M sentences taken from Yahoo! Answers and Wikipedia histories (same as §4.2.2). Implementation details. We use the same hyperparameters and training procedures for GECToR as in Omelianchuk et al. (2020). We run the BIFI algorithm for K = 1 round. The tot"
2021.emnlp-main.611,E17-2037,0,0.0424313,"Missing"
2021.emnlp-main.611,W14-1701,0,0.136512,"tence has more common words. We hence compare probabilities in local neighborhood of sentences. Concretely, LM-Critic is defined by two components, an LM (e.g., GPT2) and a neighborhood function (e.g., edit distance 1), and deems a sentence to be grammatical if the LM assigns it the highest probability in its local neighborhood (Figure 1; local optimum criterion). Using this LM-Critic, we apply BIFI to the GEC task. Notably, our approach, both the LM-Critic and GEC learning, does not require labeled data. We evaluate our proposed approach on GEC benchmarks across multiple domains, CoNLL-2014 (Ng et al., 2014), BEA-2019 (Bryant et al., 2019), GMEG-yahoo, and GMEG-wiki (Napoles et al., 2019). We achieve strong performance in the unsupervised setting (i.e., no labeled data), outperforming the baseline fixer trained on synthetic data by 7.7 F0.5 on average. We also evaluate in the supervised setting, where we take the stateof-the-art model GECToR (Omelianchuk et al., 2020) as the baseline fixer, and further fine-tune it by applying our approach using unlabeled data. We achieve 65.8 / 72.9 F0.5 on CoNLL-2014 / BEA-2019, outperforming GECToR by 0.5 F0.5 . Our results also suggest that while existing BIF"
2021.emnlp-main.611,2020.eval4nlp-1.11,0,0.0184943,"luation. Popular metrics used to assess the quality of text in GEC include GLEU (Napoles et al., 2015, 2017), M2 (Dahlmeier and Ng, 2012), ERRANT (Bryant et al., 2017) and I-measure (Felice and Briscoe, 2015). While these methods require reference text to compare to, LM-Critic does not. Several prior works also study reference-less methods to assess grammaticality of text: Wan et al. (2005); Mutton et al. (2007); Vadlapudi and Katragadda (2010) use part-ofspeech (POS) tagger or parser predictions to score grammaticality; Napoles et al. (2016); Warstadt et al. (2018); Katinskaia et al. (2019); Niu and Penn (2020) train grammatical error detection (GED) or acceptability judgement systems. However, these works require POS taggers, parsers or GED systems trained on labeled data, which may not scale or generalize well beyond the domain of training data. In contrast, LM-Critic only requires an LM, which is unsupervised and can be pretrained on various domains of unlabeled corpora. Pretrained LM for text evaluation. Several works use pretrained LMs for text evaluation. For reference-based metrics, Zhang et al. (2020) use an GEC model outputs. In Table 7, we analyze LM’s embeddings to measure the similarity"
2021.emnlp-main.611,2020.bea-1.16,0,0.387105,"terion). Using this LM-Critic, we apply BIFI to the GEC task. Notably, our approach, both the LM-Critic and GEC learning, does not require labeled data. We evaluate our proposed approach on GEC benchmarks across multiple domains, CoNLL-2014 (Ng et al., 2014), BEA-2019 (Bryant et al., 2019), GMEG-yahoo, and GMEG-wiki (Napoles et al., 2019). We achieve strong performance in the unsupervised setting (i.e., no labeled data), outperforming the baseline fixer trained on synthetic data by 7.7 F0.5 on average. We also evaluate in the supervised setting, where we take the stateof-the-art model GECToR (Omelianchuk et al., 2020) as the baseline fixer, and further fine-tune it by applying our approach using unlabeled data. We achieve 65.8 / 72.9 F0.5 on CoNLL-2014 / BEA-2019, outperforming GECToR by 0.5 F0.5 . Our results also suggest that while existing BIFI assumed access to an oracle critic (i.e., compiler), an approximate critic (i.e., LM-Critic) can also help to improve model learning. same intended meaning). A GEC model (fixer) f aims to learn this mapping, typically using a paired dataset Dpair = {(xbad (i) ,xgood (i) )}. In particular, we call it labeled if the pairs are human-annotated. In contrast, we call u"
2021.emnlp-main.611,P19-1452,0,0.0123303,"ifier (critic) is that we work on unsupervised learning of GEC. In the unsupervised setting, there is a distributional shift problem—the synthetically-generated paired data does not match the distribution of grammatical errors humans make. BIFI is a solution for obtaining realistic paired data in an unsupervised way, but it requires a critic. This led us to design a critic for GEC in this work. We note that LM-Critic is not meant to replace existing evaluation metrics for GEC, but rather is an approximate critic to assess grammaticality and help the learning of GEC. Separately, several works (Tenney et al., 2019; Hewitt and Manning, 2019; Yasunaga and Lafferty, 2019; Cao et al., 2020) induce grammar or syntactic structures from LMs, suggesting that LMs can learn about grammaticality in an unsupervised way. As this capacity is likely to grow with the size of LMs (Radford et al., 2019; Brown et al., 2020; Kaplan et al., 2020), we think that how to leverage pretrained LMs for GEC will become an increasingly important research problem. 6 Conclusion We presented LM-Critic, a method that uses a pretrained language model (LM) as a critic for assessing sentence grammaticality. Using LM-Critic and the BIFI al"
2021.emnlp-main.611,N10-3002,0,0.0124542,"not require labeled data (works for both unsupervised and supervised settings), and (ii) uses LM-Critic to filter the “bad”-side and “good”-side of generated pairs. Automatic text evaluation. Popular metrics used to assess the quality of text in GEC include GLEU (Napoles et al., 2015, 2017), M2 (Dahlmeier and Ng, 2012), ERRANT (Bryant et al., 2017) and I-measure (Felice and Briscoe, 2015). While these methods require reference text to compare to, LM-Critic does not. Several prior works also study reference-less methods to assess grammaticality of text: Wan et al. (2005); Mutton et al. (2007); Vadlapudi and Katragadda (2010) use part-ofspeech (POS) tagger or parser predictions to score grammaticality; Napoles et al. (2016); Warstadt et al. (2018); Katinskaia et al. (2019); Niu and Penn (2020) train grammatical error detection (GED) or acceptability judgement systems. However, these works require POS taggers, parsers or GED systems trained on labeled data, which may not scale or generalize well beyond the domain of training data. In contrast, LM-Critic only requires an LM, which is unsupervised and can be pretrained on various domains of unlabeled corpora. Pretrained LM for text evaluation. Several works use pretr"
2021.emnlp-main.611,D19-1221,0,0.0633337,"Missing"
2021.emnlp-main.611,W05-1628,0,0.0383013,"rom the above works, our method (i) does not require labeled data (works for both unsupervised and supervised settings), and (ii) uses LM-Critic to filter the “bad”-side and “good”-side of generated pairs. Automatic text evaluation. Popular metrics used to assess the quality of text in GEC include GLEU (Napoles et al., 2015, 2017), M2 (Dahlmeier and Ng, 2012), ERRANT (Bryant et al., 2017) and I-measure (Felice and Briscoe, 2015). While these methods require reference text to compare to, LM-Critic does not. Several prior works also study reference-less methods to assess grammaticality of text: Wan et al. (2005); Mutton et al. (2007); Vadlapudi and Katragadda (2010) use part-ofspeech (POS) tagger or parser predictions to score grammaticality; Napoles et al. (2016); Warstadt et al. (2018); Katinskaia et al. (2019); Niu and Penn (2020) train grammatical error detection (GED) or acceptability judgement systems. However, these works require POS taggers, parsers or GED systems trained on labeled data, which may not scale or generalize well beyond the domain of training data. In contrast, LM-Critic only requires an LM, which is unsupervised and can be pretrained on various domains of unlabeled corpora. Pre"
2021.emnlp-main.611,P19-1561,0,0.0158593,"tion funcˆ tion b, and let B(x) be samples from b(x). To check 1 We acknowledge that this assumption may not hold in some cases, e.g., an ungrammatical sentence may have no correction (“asdfghgfdsa”—just a random typo?) or multiple corrections (“The cat sleep.”—change “sleep” to the present tense or past?). We accept this assumption considering that it is often sufficient in common GEC datasets, and leave the relaxation of the assumption for future work. • ED1. Given a sentence, we generate edit-distance one (ED1) perturbations in the character space. Following prior works in typo generation (Pruthi et al., 2019; Jones et al., 2020), we randomly insert a lowercase letter, delete a character, replace a character, or swap two adjacent characters. • ED1 + Word-level heuristics (all). ED1 can cover most of the character-level typos but may not cover word-level grammatical errors, such as missing an article. Besides ED1, here we include heuristics for word-level perturbations used in Awasthi et al. (2019), which randomly inserts, deletes, or replaces a word based on its dictionary. Please refer to Awasthi et al. for more details. • ED1 + Word-level heuristics. We noticed that the above word-level heuristi"
2021.emnlp-main.611,N18-1057,0,0.0177175,"iscussion Grammatical error correction (GEC). GEC models are commonly trained from human-labeled data (Nicholls, 2003; Dahlmeier et al., 2013; Yannakoudakis et al., 2011; Bryant et al., 2019), or synthetic data generated by heuristically corrupting unlabeled sentences (Awasthi et al., 2019; Zhao et al., 2019; Grundkiewicz et al., 2019; Katsumata and Komachi, 2019; Omelianchuk et al., 2020). Several works aim to improve the methods for generating paired data, such as learning a breaker from existing labeled data (Lichtarge et al., 2019), applying backtranslation (Sennrich et al., 2016) to GEC (Xie et al., 2018; Kiyono et al., 2019), and synthesizing extra paired data by comparing model predictions and references (Ge et al., 2018). Different from the above works, our method (i) does not require labeled data (works for both unsupervised and supervised settings), and (ii) uses LM-Critic to filter the “bad”-side and “good”-side of generated pairs. Automatic text evaluation. Popular metrics used to assess the quality of text in GEC include GLEU (Napoles et al., 2015, 2017), M2 (Dahlmeier and Ng, 2012), ERRANT (Bryant et al., 2017) and I-measure (Felice and Briscoe, 2015). While these methods require ref"
2021.emnlp-main.611,P11-1019,0,0.538914,"decoder to improve GEC remains a key challenge, especially models to map bad (ungrammatical) sentences in domains or languages with no labeled GEC data into good (grammatical) sentences (Yuan and (Napoles et al., 2019; Náplava and Straka, 2019). Briscoe, 2016; Xie et al., 2016; Ji et al., 2017; Chollampatt and Ng, 2018; Junczys-Dowmunt Break-It-Fix-It (BIFI; Yasunaga and Liang et al., 2018). These methods rely on a combination (2021)) is a recent method to obtain realistic paired of human-labeled data (i.e., hbad, goodi pairs) data from unlabeled data, which has shown promise (Nicholls, 2003; Yannakoudakis et al., 2011; Bryant in the task of source code repair. The idea of BIFI et al., 2019) and synthetic data, which are generated is that using an initial fixer (e.g., trained on synby corrupting good sentences into hsynthetic bad, thetic data) and a critic that tells if an input is bad goodi pairs (Awasthi et al., 2019; Kiyono et al., or good (e.g., compiler, which checks if code has an 2019). Human-labeled pairs are representative error), BIFI iteratively trains the fixer and a breaker of real human errors but are expensive to obtain, to generate better paired data. Specifically, BIFI (1) while synthetic p"
2021.emnlp-main.611,P16-1009,0,0.154577,"decoder model that maps “good”-side examples to “bad”-side examples in paired data P, and T RAINbad→good (P) does the reverse. Red font indicates the use of critic. The key intuition of BIFI is that thanks to the critic, (i) we can extract Dbad from the unlabeled data Dunlabel and incorporate realistic grammatical errors into our data (as opposed to the synthetic data), and (ii) we can verify if the “bad”-side and “good”-side of the generated pairs are actually “bad” and “good” (Eq 8, 10; red font), which improves the correctness of generated training data compared to vanilla backtranslation (Sennrich et al., 2016; Lample et al., 2018). We refer readers to Yasunaga and Liang (2021) for more details. 4.2 Experiments We study our proposed approach (BIFI with LMCritic) on GEC benchmarks, in both unsupervised and supervised settings. hoo!Answers. For CoNLL-2014, we use the official M2 scorer (Dahlmeier and Ng, 2012), and for others we use the ERRANT metric (Bryant et al., 2017). We describe the training data separately for unsupervised (§4.2.2) and supervised (§4.2.3) settings. 4.2.2 Unsupervised setting Setup and data. We consider the setup with no labeled training data. Existing GEC works (e.g., Awasthi"
2021.emnlp-main.611,N19-1406,0,0.0159922,"n labeled data, which may not scale or generalize well beyond the domain of training data. In contrast, LM-Critic only requires an LM, which is unsupervised and can be pretrained on various domains of unlabeled corpora. Pretrained LM for text evaluation. Several works use pretrained LMs for text evaluation. For reference-based metrics, Zhang et al. (2020) use an GEC model outputs. In Table 7, we analyze LM’s embeddings to measure the similarity between examples where the baseline fixer trained on input text and reference text. For reference-less 7759 metrics, several works (Kann et al., 2018; Stahlberg et al., 2019) use an LM’s probability as a fluency score of text. While this provides a continuous score for fluency, it in itself cannot classify grammatical / ungrammatical sentences. Our LM-Critic goes a step further to consider the local optimum criterion for classifying grammaticality. The reason we want a classifier (critic) is that we work on unsupervised learning of GEC. In the unsupervised setting, there is a distributional shift problem—the synthetically-generated paired data does not match the distribution of grammatical errors humans make. BIFI is a solution for obtaining realistic paired data"
2021.emnlp-main.611,N16-1042,0,0.0606999,"Missing"
2021.emnlp-main.611,N19-1014,0,0.015778,"alistic. synthetic data (“Transformer”) fails but our model (“+BIFI”) succeeds. We find that the baseline tends to make unnecessary edits (e.g., changing verb inflection or articles), due to the heuristics used when generating synthetic data. In contrast, BIFI achieves higher precision. 5 Related work and discussion Grammatical error correction (GEC). GEC models are commonly trained from human-labeled data (Nicholls, 2003; Dahlmeier et al., 2013; Yannakoudakis et al., 2011; Bryant et al., 2019), or synthetic data generated by heuristically corrupting unlabeled sentences (Awasthi et al., 2019; Zhao et al., 2019; Grundkiewicz et al., 2019; Katsumata and Komachi, 2019; Omelianchuk et al., 2020). Several works aim to improve the methods for generating paired data, such as learning a breaker from existing labeled data (Lichtarge et al., 2019), applying backtranslation (Sennrich et al., 2016) to GEC (Xie et al., 2018; Kiyono et al., 2019), and synthesizing extra paired data by comparing model predictions and references (Ge et al., 2018). Different from the above works, our method (i) does not require labeled data (works for both unsupervised and supervised settings), and (ii) uses LM-Critic to filter the"
2021.naacl-main.345,N19-1423,0,0.0154595,"Missing"
2021.naacl-main.345,D08-1094,0,0.200717,"Missing"
2021.naacl-main.345,E14-1057,0,0.378713,"ud et al., 2015; Hintz and Biemann, tutes for a target word in a context. For writing, 2016; Zhou et al., 2019; Arefyev et al., 2020) lexical substitution systems can assist humans considers the task of replacing a target word in by suggesting words that humans cannot eascontext with appropriate substitutes. There are ily think of. However, existing benchmarks two widely-used English benchmarks for this depend on human recall as the only source of data, and therefore lack coverage of the task: S EM E VAL (McCarthy and Navigli, 2007) substitutes that would be most helpful to huand C O I N C O (Kremer et al., 2014). For both mans. Furthermore, annotators often provide benchmarks, data was collected by asking human substitutes of low quality, which are not actuannotators to think of substitutes from memory. ally appropriate in the given context. We colBecause lexical substitution was originally lect higher-coverage and higher-quality data proposed as a means for evaluating word sense by framing lexical substitution as a classificadisambiguation systems (McCarthy, 2002), this tion problem, guided by the intuition that it is easier for humans to judge the appropriatedata collection strategy was designed to"
2021.naacl-main.345,J10-3003,0,0.0608061,"Missing"
2021.naacl-main.345,W02-0816,0,0.664728,"fore lack coverage of the task: S EM E VAL (McCarthy and Navigli, 2007) substitutes that would be most helpful to huand C O I N C O (Kremer et al., 2014). For both mans. Furthermore, annotators often provide benchmarks, data was collected by asking human substitutes of low quality, which are not actuannotators to think of substitutes from memory. ally appropriate in the given context. We colBecause lexical substitution was originally lect higher-coverage and higher-quality data proposed as a means for evaluating word sense by framing lexical substitution as a classificadisambiguation systems (McCarthy, 2002), this tion problem, guided by the intuition that it is easier for humans to judge the appropriatedata collection strategy was designed to avoid a ness of candidate substitutes than conjure them bias towards any particular word sense inventory. from memory. To this end, we use a contextIn this work, we consider a different use case free thesaurus to produce candidates and rely for lexical substitution: writing assistance. For on human judgement to determine contextual this use case, we are interested in evaluating a appropriateness. Compared to the previous system’s ability to produce appropri"
2021.naacl-main.345,S07-1009,0,0.545747,"et al., 2013; Kremer et al., stitution, the task of finding appropriate substi2014; Melamud et al., 2015; Hintz and Biemann, tutes for a target word in a context. For writing, 2016; Zhou et al., 2019; Arefyev et al., 2020) lexical substitution systems can assist humans considers the task of replacing a target word in by suggesting words that humans cannot eascontext with appropriate substitutes. There are ily think of. However, existing benchmarks two widely-used English benchmarks for this depend on human recall as the only source of data, and therefore lack coverage of the task: S EM E VAL (McCarthy and Navigli, 2007) substitutes that would be most helpful to huand C O I N C O (Kremer et al., 2014). For both mans. Furthermore, annotators often provide benchmarks, data was collected by asking human substitutes of low quality, which are not actuannotators to think of substitutes from memory. ally appropriate in the given context. We colBecause lexical substitution was originally lect higher-coverage and higher-quality data proposed as a means for evaluating word sense by framing lexical substitution as a classificadisambiguation systems (McCarthy, 2002), this tion problem, guided by the intuition that it is"
2021.naacl-main.345,N15-1050,0,0.0467382,"Missing"
2021.naacl-main.345,P15-2070,0,0.0483446,"Missing"
2021.naacl-main.345,D14-1162,0,0.0851655,"Missing"
2021.naacl-main.45,P19-1615,0,0.040577,"Missing"
2021.naacl-main.45,C16-1236,0,0.03282,"Missing"
2021.naacl-main.45,D18-1454,0,0.0428511,"Missing"
2021.naacl-main.45,D13-1160,1,0.76616,"Missing"
2021.naacl-main.45,P19-1470,1,0.900584,"Missing"
2021.naacl-main.45,D19-1282,0,0.293533,"- score-aware 75.56 L=7 75.96 L=7 75.96 answering tasks (Liu et al., 2019; Raffel et al., the given QA context) and their few-hop neighbors. 2020). However, while LMs have a broad coverage However, this introduces many entity nodes that of knowledge, they do not empirically perform well are semantically irrelevant to the QA context, on structured reasoning (e.g., handling negation) especially when the number of topic entities or hops (Kassner and Schütze, 2020). On the other hand, increases. Additionally, existing LM+KG methods KGs are more suited for structured reasoning (Ren for reasoning (Lin et al., 2019; Wang et al., 2019a; et al., 2020; Ren and Leskovec, 2020) and enable Feng et al., 2020; Lv et al., 2020) treat the QA explainable predictions e.g., by providing reasoning context and KG as two separate modalities. They paths (Lin et al., 2019), but may lack coverage and individually apply LMs to the QA context and graph 535 1 Introduction GNN Attention & Message (§3.3) Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 535–546 June 6–11, 2021. ©2021 Association for Computational Linguistics GNN"
2021.naacl-main.45,2021.ccl-1.108,0,0.0878775,"Missing"
2021.naacl-main.45,2020.emnlp-main.99,0,0.285223,"l., the given QA context) and their few-hop neighbors. 2020). However, while LMs have a broad coverage However, this introduces many entity nodes that of knowledge, they do not empirically perform well are semantically irrelevant to the QA context, on structured reasoning (e.g., handling negation) especially when the number of topic entities or hops (Kassner and Schütze, 2020). On the other hand, increases. Additionally, existing LM+KG methods KGs are more suited for structured reasoning (Ren for reasoning (Lin et al., 2019; Wang et al., 2019a; et al., 2020; Ren and Leskovec, 2020) and enable Feng et al., 2020; Lv et al., 2020) treat the QA explainable predictions e.g., by providing reasoning context and KG as two separate modalities. They paths (Lin et al., 2019), but may lack coverage and individually apply LMs to the QA context and graph 535 1 Introduction GNN Attention & Message (§3.3) Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 535–546 June 6–11, 2021. ©2021 Association for Computational Linguistics GNN Pooling (§3) QA context LM Encoding MLP [q; a] KG Retrieval Joint Graph (§3.1) Z Probab"
2021.naacl-main.45,D19-6003,0,0.119229,"Missing"
2021.naacl-main.45,D18-1260,0,0.134559,"agNet (Lin et al., 2019) and MHGRN (Feng et al., 2020) in Table 1. As we handle edges of different relation types using different edge embeddings instead of designing an independent graph networks for each relation as in RGCN (Schlichtkrull et al., 2018) or MHGRN, the time complexity of our method is constant with respect to the number of relations and linear with respect to the number of nodes. We achieve the same space complexity as MHGRN (Feng et al., 2020). 4 4.1 Experiments Datasets We evaluate QA-GNN on two question answering datasets: CommonsenseQA (Talmor et al., 2019) and OpenBookQA (Mihaylov et al., 2018). CommonsenseQA is a 5-way multiple choice QA task that requires reasoning with commonsense knowledge, containing 12,102 questions. The test set of CommonsenseQA is not publicly available, mst = fm (h(`) (4) s , us , rst ), and model predictions can only be evaluated once where fm : R2.5D → RD is a linear transformation. every two weeks via the official leaderboard. Hence, 538 where us ,ut ∈ {0,1}|T |are one-hot vectors indicating the node types of s and t, est ∈ {0,1}|R |is a one-hot vector indicating the relation type of edge (s,t), fu : R|T |→ RD/2 is a linear transformation, and fr : R|R|+"
2021.naacl-main.45,D15-1038,1,0.81451,"on the performing joint reasoning over the language and the CommonsenseQA and OpenBookQA datasets, knowledge graph (green box). and show its improvement over existing LM Graph Connection (§3.1) Dev Acc. Contextualization (§3.2) and LM+KG models, as well as its capability to No edge between Z and KG nodes No contextualization 74.81 w/ contextual embedding 76.38 perform interpretable and structured reasoning, Connect Z to all KG nodes w/ relevance score (final system) to QA entity nodes (final system) 76.54 e.g., correctly handling negation in questions. Connect Z be noisy (Bordes et al., 2013; Guu et al., 2015). w/ both Dev Ac 75.56 76.31 76.54 76.52 How to reason effectively with both sources of Dev Acc. GNN Layers (§3.3) Dev Acc. knowledge remains an important open problem. Node type, relation, score-aware (final system) 76.54 L=3 75.53 L=4 76.34 - type-aware Combining LMs and KGs 75.41 Question answering systems must be able to access for reasoning (henceL = 5 (final system) 76.54 - relation-aware 75.61 L=6 76.21 relevant knowledge and reason over it. Typically, forth, LM+KG) presents 75.56 two challenges:L =given - score-aware 7 75.96 knowledge can be implicitly encoded in large a QA context (e."
2021.naacl-main.45,P18-1076,0,0.0361117,"Missing"
2021.naacl-main.45,D19-5804,0,0.047242,"Missing"
2021.naacl-main.45,P17-1147,0,0.0620972,"Missing"
2021.naacl-main.45,2020.acl-main.698,0,0.0185558,"lation-aware 75.61mentioned L = 6many question 76.21 L = 6 in 76.21 demonstrated remarkable success in by taking topic entities (KG entities - score-aware 75.15 - score-aware 75.56 L=7 75.96 L=7 75.96 answering tasks (Liu et al., 2019; Raffel et al., the given QA context) and their few-hop neighbors. 2020). However, while LMs have a broad coverage However, this introduces many entity nodes that of knowledge, they do not empirically perform well are semantically irrelevant to the QA context, on structured reasoning (e.g., handling negation) especially when the number of topic entities or hops (Kassner and Schütze, 2020). On the other hand, increases. Additionally, existing LM+KG methods KGs are more suited for structured reasoning (Ren for reasoning (Lin et al., 2019; Wang et al., 2019a; et al., 2020; Ren and Leskovec, 2020) and enable Feng et al., 2020; Lv et al., 2020) treat the QA explainable predictions e.g., by providing reasoning context and KG as two separate modalities. They paths (Lin et al., 2019), but may lack coverage and individually apply LMs to the QA context and graph 535 1 Introduction GNN Attention & Message (§3.3) Proceedings of the 2021 Conference of the North American Chapter of the Asso"
2021.naacl-main.45,N19-1368,0,0.0509063,"Missing"
2021.naacl-main.45,D19-1250,0,0.0606114,"Missing"
2021.naacl-main.45,D16-1264,1,0.813323,"Missing"
2021.naacl-main.45,N13-1008,0,0.0530867,"nder Nos. OAC-1835598 (CINES), OAC1934578 (HDR), CCF-1918940 (Expeditions), IIS-2030477 (RAPID); Stanford Data Science Initiative, Wu Tsai Neuro-sciences Institute, Chan Zuckerberg Biohub, Amazon, JP-Morgan Chase, Docomo, Hitachi, JD.com, KDDI, NVIDIA, Dell, Toshiba, and United Health Group. Hongyu Ren is supported by Masason Foundation Fellowship and the Apple PhD Fellowship. Jure Leskovec is a Chan Zuckerberg Biohub investigator. Knowledge representations. Several works study joint representations of external textual knowledge (e.g., Wikipedia articles) and structured knowledge (e.g., KGs) (Riedel et al., 2013; Toutanova et al., 2015; Xiong et al., 2019; Sun et al., 2019; Wang et al., 2019b). The primary distinction of our joint graph representation is that we construct a graph connecting each question and KG rather than textual and structural knowledge, approaching a complementary problem to the above works. Graph neural networks (GNNs). GNNs have been shown to be effective for modeling graphbased data. Several works use GNNs to model the structure of text (Yasunaga et al., 2017; Zhang et al., 2018; Yasunaga and Liang, 2020) or KGs (Wang et al., 2020a). In contrast to these works, QA-GNN jointly m"
2021.naacl-main.45,2020.findings-emnlp.369,0,0.544593,"with a vanilla fine-tuned LM, which does not use the KG. We use RoBERTa-large (Liu et al., 2019) for CommonsenseQA, and RoBERTa-large and AristoRoBERTa2 (Clark et al., 2019) for 2 OpenBookQA provides an extra corpus of scientific facts in a textual form. AristoRoBERTa uses the facts corresponding to each question, prepared by Clark et al. (2019), as an Methods Test RoBERTa (Liu et al., 2019) RoBERTa+FreeLB (Zhu et al., 2020) (ensemble) RoBERTa+HyKAS (Ma et al., 2019) RoBERTa+KE (ensemble) RoBERTa+KEDGN (ensemble) XLNet+GraphReason (Lv et al., 2020) RoBERTa+MHGRN (Feng et al., 2020) Albert+PG (Wang et al., 2020b) Albert (Lan et al., 2020) (ensemble) UnifiedQA* (Khashabi et al., 2020) 72.1 73.1 73.2 73.3 74.4 75.3 75.4 75.6 76.5 79.1 RoBERTa + QA-GNN (Ours) 76.1 Table 3: Test accuracy on CommonsenseQA’s official leaderboard. The top system, UnifiedQA (11B parameters) is 30x larger than our model. OpenBookQA. Existing LM+KG models. We compare with existing LM+KG methods, which share the same high-level framework as ours but use different modules to reason on the KG in place of QA-GNN (“yellow box” in Figure2): (1) Relation Network (RN) (Santoro et al., 2017), (2) RGCN (Schlichtkrull et al., 2018), (3)"
2021.naacl-main.45,P19-1417,0,0.0178077,"R), CCF-1918940 (Expeditions), IIS-2030477 (RAPID); Stanford Data Science Initiative, Wu Tsai Neuro-sciences Institute, Chan Zuckerberg Biohub, Amazon, JP-Morgan Chase, Docomo, Hitachi, JD.com, KDDI, NVIDIA, Dell, Toshiba, and United Health Group. Hongyu Ren is supported by Masason Foundation Fellowship and the Apple PhD Fellowship. Jure Leskovec is a Chan Zuckerberg Biohub investigator. Knowledge representations. Several works study joint representations of external textual knowledge (e.g., Wikipedia articles) and structured knowledge (e.g., KGs) (Riedel et al., 2013; Toutanova et al., 2015; Xiong et al., 2019; Sun et al., 2019; Wang et al., 2019b). The primary distinction of our joint graph representation is that we construct a graph connecting each question and KG rather than textual and structural knowledge, approaching a complementary problem to the above works. Graph neural networks (GNNs). GNNs have been shown to be effective for modeling graphbased data. Several works use GNNs to model the structure of text (Yasunaga et al., 2017; Zhang et al., 2018; Yasunaga and Liang, 2020) or KGs (Wang et al., 2020a). In contrast to these works, QA-GNN jointly models the language and KG. Graph Attention N"
2021.naacl-main.45,D19-1242,0,0.0337729,"editions), IIS-2030477 (RAPID); Stanford Data Science Initiative, Wu Tsai Neuro-sciences Institute, Chan Zuckerberg Biohub, Amazon, JP-Morgan Chase, Docomo, Hitachi, JD.com, KDDI, NVIDIA, Dell, Toshiba, and United Health Group. Hongyu Ren is supported by Masason Foundation Fellowship and the Apple PhD Fellowship. Jure Leskovec is a Chan Zuckerberg Biohub investigator. Knowledge representations. Several works study joint representations of external textual knowledge (e.g., Wikipedia articles) and structured knowledge (e.g., KGs) (Riedel et al., 2013; Toutanova et al., 2015; Xiong et al., 2019; Sun et al., 2019; Wang et al., 2019b). The primary distinction of our joint graph representation is that we construct a graph connecting each question and KG rather than textual and structural knowledge, approaching a complementary problem to the above works. Graph neural networks (GNNs). GNNs have been shown to be effective for modeling graphbased data. Several works use GNNs to model the structure of text (Yasunaga et al., 2017; Zhang et al., 2018; Yasunaga and Liang, 2020) or KGs (Wang et al., 2020a). In contrast to these works, QA-GNN jointly models the language and KG. Graph Attention Networks (GATs) (Ve"
2021.naacl-main.45,P19-1226,0,0.0354944,"Missing"
2021.naacl-main.45,D18-1259,0,0.0605156,"Missing"
2021.naacl-main.45,D18-1455,0,0.0382151,"Missing"
2021.naacl-main.45,N19-1421,0,0.081014,"ethod and compare with prior works, KagNet (Lin et al., 2019) and MHGRN (Feng et al., 2020) in Table 1. As we handle edges of different relation types using different edge embeddings instead of designing an independent graph networks for each relation as in RGCN (Schlichtkrull et al., 2018) or MHGRN, the time complexity of our method is constant with respect to the number of relations and linear with respect to the number of nodes. We achieve the same space complexity as MHGRN (Feng et al., 2020). 4 4.1 Experiments Datasets We evaluate QA-GNN on two question answering datasets: CommonsenseQA (Talmor et al., 2019) and OpenBookQA (Mihaylov et al., 2018). CommonsenseQA is a 5-way multiple choice QA task that requires reasoning with commonsense knowledge, containing 12,102 questions. The test set of CommonsenseQA is not publicly available, mst = fm (h(`) (4) s , us , rst ), and model predictions can only be evaluated once where fm : R2.5D → RD is a linear transformation. every two weeks via the official leaderboard. Hence, 538 where us ,ut ∈ {0,1}|T |are one-hot vectors indicating the node types of s and t, est ∈ {0,1}|R |is a one-hot vector indicating the relation type of edge (s,t), fu : R|T |→ RD/2 is"
2021.naacl-main.45,K17-1045,1,0.805684,"study joint representations of external textual knowledge (e.g., Wikipedia articles) and structured knowledge (e.g., KGs) (Riedel et al., 2013; Toutanova et al., 2015; Xiong et al., 2019; Sun et al., 2019; Wang et al., 2019b). The primary distinction of our joint graph representation is that we construct a graph connecting each question and KG rather than textual and structural knowledge, approaching a complementary problem to the above works. Graph neural networks (GNNs). GNNs have been shown to be effective for modeling graphbased data. Several works use GNNs to model the structure of text (Yasunaga et al., 2017; Zhang et al., 2018; Yasunaga and Liang, 2020) or KGs (Wang et al., 2020a). In contrast to these works, QA-GNN jointly models the language and KG. Graph Attention Networks (GATs) (Veliˇckovi´c et al., 2018) perform attention-based message passing to induce graph representations. We build on this framework, and further condition the GNN on the language input by introducing a QA context node (§3.1), KG node relevance scoring (§3.2), and joint update of the KG and language representations (§3.3). 6 Conclusion We presented QA-GNN, an end-to-end question answering model that leverages LMs and KGs."
2021.naacl-main.45,P16-2033,0,0.023203,"Missing"
2021.naacl-main.45,D18-1425,1,0.894443,"Missing"
2021.naacl-main.45,D18-1244,0,0.0302663,"ions of external textual knowledge (e.g., Wikipedia articles) and structured knowledge (e.g., KGs) (Riedel et al., 2013; Toutanova et al., 2015; Xiong et al., 2019; Sun et al., 2019; Wang et al., 2019b). The primary distinction of our joint graph representation is that we construct a graph connecting each question and KG rather than textual and structural knowledge, approaching a complementary problem to the above works. Graph neural networks (GNNs). GNNs have been shown to be effective for modeling graphbased data. Several works use GNNs to model the structure of text (Yasunaga et al., 2017; Zhang et al., 2018; Yasunaga and Liang, 2020) or KGs (Wang et al., 2020a). In contrast to these works, QA-GNN jointly models the language and KG. Graph Attention Networks (GATs) (Veliˇckovi´c et al., 2018) perform attention-based message passing to induce graph representations. We build on this framework, and further condition the GNN on the language input by introducing a QA context node (§3.1), KG node relevance scoring (§3.2), and joint update of the KG and language representations (§3.3). 6 Conclusion We presented QA-GNN, an end-to-end question answering model that leverages LMs and KGs. Our key innovations"
D07-1072,A00-2018,0,0.055178,"typically a poor model as it makes overly strong independence assumptions. As a result, many generative approaches to parsing construct refinements of the treebank grammar which are more suitable for the modeling task. Lexical methods split each pre-terminal symbol into many subsymbols, one for each word, and then focus on smoothing sparse lexical statis688 Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational c Natural Language Learning, pp. 688–697, Prague, June 2007. 2007 Association for Computational Linguistics tics (Collins, 1999; Charniak, 2000). Unlexicalized methods refine the grammar in a more conservative fashion, splitting each non-terminal or pre-terminal symbol into a much smaller number of subsymbols (Klein and Manning, 2003; Matsuzaki et al., 2005; Petrov et al., 2006). We apply our HDP-PCFG-GR model to automatically learn the number of subsymbols for each symbol. 2 Models based on Dirichlet processes In document clustering, for example, each data point xi is a document represented by its termfrequency vector. Each component (cluster) z has multinomial parameters φz which specifies a distribution F (·; φz ) over words. It is"
D07-1072,P07-1035,0,0.551821,"Missing"
D07-1072,P06-1085,0,0.0253881,"treebank parsing (Charniak, 1996; Collins, 1999). An important question when learning PCFGs is how many grammar symbols to allocate to the learning algorithm based on the amount of available data. The question of “how many clusters (symbols)?” has been tackled in the Bayesian nonparametrics literature via Dirichlet process (DP) mixture models (Antoniak, 1974). DP mixture models have since been extended to hierarchical Dirichlet processes (HDPs) and HDP-HMMs (Teh et al., 2006; Beal et al., 2002) and applied to many different types of clustering/induction problems in NLP (Johnson et al., 2006; Goldwater et al., 2006). In this paper, we present the hierarchical Dirichlet process PCFG (HDP-PCFG). a nonparametric As models increase in complexity, so does the uncertainty over parameter estimates. In this regime, point estimates are unreliable since they do not take into account the fact that there are different amounts of uncertainty in the various components of the parameters. The HDP-PCFG is a Bayesian model which naturally handles this uncertainty. We present an efficient variational inference algorithm for the HDP-PCFG based on a structured mean-field approximation of the true posterior over parameters. T"
D07-1072,P03-1054,1,0.0589178,"ore suitable for the modeling task. Lexical methods split each pre-terminal symbol into many subsymbols, one for each word, and then focus on smoothing sparse lexical statis688 Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational c Natural Language Learning, pp. 688–697, Prague, June 2007. 2007 Association for Computational Linguistics tics (Collins, 1999; Charniak, 2000). Unlexicalized methods refine the grammar in a more conservative fashion, splitting each non-terminal or pre-terminal symbol into a much smaller number of subsymbols (Klein and Manning, 2003; Matsuzaki et al., 2005; Petrov et al., 2006). We apply our HDP-PCFG-GR model to automatically learn the number of subsymbols for each symbol. 2 Models based on Dirichlet processes In document clustering, for example, each data point xi is a document represented by its termfrequency vector. Each component (cluster) z has multinomial parameters φz which specifies a distribution F (·; φz ) over words. It is customary to use a conjugate Dirichlet prior G0 = Dirichlet(α0 , . . . , α0 ) over the multinomial parameters, which can be interpreted as adding α0 − 1 pseudocounts for each word. 2.2 At th"
D07-1072,P05-1010,0,0.344389,"ling task. Lexical methods split each pre-terminal symbol into many subsymbols, one for each word, and then focus on smoothing sparse lexical statis688 Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational c Natural Language Learning, pp. 688–697, Prague, June 2007. 2007 Association for Computational Linguistics tics (Collins, 1999; Charniak, 2000). Unlexicalized methods refine the grammar in a more conservative fashion, splitting each non-terminal or pre-terminal symbol into a much smaller number of subsymbols (Klein and Manning, 2003; Matsuzaki et al., 2005; Petrov et al., 2006). We apply our HDP-PCFG-GR model to automatically learn the number of subsymbols for each symbol. 2 Models based on Dirichlet processes In document clustering, for example, each data point xi is a document represented by its termfrequency vector. Each component (cluster) z has multinomial parameters φz which specifies a distribution F (·; φz ) over words. It is customary to use a conjugate Dirichlet prior G0 = Dirichlet(α0 , . . . , α0 ) over the multinomial parameters, which can be interpreted as adding α0 − 1 pseudocounts for each word. 2.2 At the heart of the HDP-PCFG"
D07-1072,P06-1055,1,0.592342,"ds split each pre-terminal symbol into many subsymbols, one for each word, and then focus on smoothing sparse lexical statis688 Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational c Natural Language Learning, pp. 688–697, Prague, June 2007. 2007 Association for Computational Linguistics tics (Collins, 1999; Charniak, 2000). Unlexicalized methods refine the grammar in a more conservative fashion, splitting each non-terminal or pre-terminal symbol into a much smaller number of subsymbols (Klein and Manning, 2003; Matsuzaki et al., 2005; Petrov et al., 2006). We apply our HDP-PCFG-GR model to automatically learn the number of subsymbols for each symbol. 2 Models based on Dirichlet processes In document clustering, for example, each data point xi is a document represented by its termfrequency vector. Each component (cluster) z has multinomial parameters φz which specifies a distribution F (·; φz ) over words. It is customary to use a conjugate Dirichlet prior G0 = Dirichlet(α0 , . . . , α0 ) over the multinomial parameters, which can be interpreted as adding α0 − 1 pseudocounts for each word. 2.2 At the heart of the HDP-PCFG is the Dirichlet proce"
D07-1072,J03-4003,0,\N,Missing
D07-1093,X93-1024,0,0.298158,"are fully observed while the vl and ib forms are automatically reconstructed. Figure 6 also shows a specific example of the evolution of the Latin VERBUM (word/verb), along with the specific edits employed by the model. While quantitative evaluation such as measuring edit distance is helpful for comparing results, it is also illuminating to consider the plausibility of the learned parameters in a historical light, which we do here briefly. In particular, we consider rules on the branch between la and vl, for which we have historical evidence. For example, documents such as the Appendix Probi (Baehrens, 1922) provide indications of orthographic confusions which resulted from the growing gap between Classical Latin and Vulgar Latin phonology around the 3rd and 4th centuries AD. The Appendix lists common misspellings of Latin words, from which phonological changes can be inferred. On the la to vl branch, rules for word-final deletion of classical case markers dominate the list (rules ranks 1 and 3 for deletion of final /s/, ranks 2 and 4 for deletion of final /m/). It is indeed likely that these were generally eliminated in Vulgar Latin. For the deletion of the /m/, the Appendix Probi contains pairs"
D07-1093,J03-1002,0,0.00339804,"corpus we created for these experiments. 4.1 Corpus In order to train and evaluate our system, we compiled a corpus of Romance cognate words. The raw data was taken from three sources: the wiktionary.org website, a Bible parallel corpus (Resnik et al., 1999) and the Europarl corpus (Koehn, 2002). From an XML dump of the Wiktionary data, we extracted multilingual translations, which provide a list of word tuples in a large number of languages, including a few ancient languages. The Europarl and the biblical data were processed and aligned in the standard way, using combined GIZA++ alignments (Och and Ney, 2003). We performed our experiments with four languages from the Romance family (Latin, Italian, Spanish, and Portuguese). For each of these languages, we used a simple in-house rule-based system to convert the words into their IPA representations.2 After augmenting our alignments with the transitive closure3 of the Europarl, Bible and Wiktionary data, we filtered out non-cognate words by thresholding the ratio of edit distance to word length.4 The preprocessing is constraining in that we require that all the elements of a tuple to be cognates, which leaves out a significant portion of the data beh"
D07-1093,W97-1101,0,0.0355457,"sadvantages: it does not capture the fact that the cognate is closer between Spanish and Portuguese than between French and Spanish, nor do the resulting models let us conclude anything about the regular processes which caused these languages to diverge. Also, the existing cognate data has been curated at a relatively high cost. In our work, we track each word using an automatically obtained cognate list. While our cognates may be noisier, we compensate by modeling phonological changes rather than boolean mutations in cognate sets. There has been other computational work in this broad domain. Venkataraman et al. (1997) describe an information theoretic measure of the distance between two dialects of Chinese. Like our approach, they use a probabilistic edit model as a formalization of the phonological process. However, they do not consider the question of reconstruction or inference in multi-node phylogenies, nor do they present a learning algorithm for such models. Finally, for the specific application of cognate prediction in machine translation, essentially transliteration, there have been several approaches, including Kondrak (2002). However, the phenomena of interest, and therefore the models, are extre"
D10-1040,P09-1010,0,0.0379509,"ing, pages 410–419, c MIT, Massachusetts, USA, 9-11 October 2010. 2010 Association for Computational Linguistics fall out naturally as consequences of the model. An effective way to empirically explore the pragmatic aspects of language is to work in the grounded setting, where the basic idea is to map language to some representation of the non-linguistic world (Yu and Ballard, 2004; Feldman and Narayanan, 2004; Fleischman and Roy, 2007; Chen and Mooney, 2008; Frank et al., 2009; Liang et al., 2009). Along similar lines, past work has also focused on interpreting natural language instructions (Branavan et al., 2009; Eisenstein et al., 2009; Kollar et al., 2010), which takes into account the goal of the communication. This work differs from ours in that it does not clarify the formal relationship between pragmatics and the interpretation task. Pragmatics has also been studied in the context of dialog systems. For instance, DeVault and Stone (2007) present a model of collaborative language between multiple agents that takes into account contextual ambiguities. We present our pragmatic model in a grounded setting where a speaker must describe a target object to a listener via spatial description (such as i"
D10-1040,D09-1100,0,0.0224414,"IT, Massachusetts, USA, 9-11 October 2010. 2010 Association for Computational Linguistics fall out naturally as consequences of the model. An effective way to empirically explore the pragmatic aspects of language is to work in the grounded setting, where the basic idea is to map language to some representation of the non-linguistic world (Yu and Ballard, 2004; Feldman and Narayanan, 2004; Fleischman and Roy, 2007; Chen and Mooney, 2008; Frank et al., 2009; Liang et al., 2009). Along similar lines, past work has also focused on interpreting natural language instructions (Branavan et al., 2009; Eisenstein et al., 2009; Kollar et al., 2010), which takes into account the goal of the communication. This work differs from ours in that it does not clarify the formal relationship between pragmatics and the interpretation task. Pragmatics has also been studied in the context of dialog systems. For instance, DeVault and Stone (2007) present a model of collaborative language between multiple agents that takes into account contextual ambiguities. We present our pragmatic model in a grounded setting where a speaker must describe a target object to a listener via spatial description (such as in the example given above"
D10-1040,P09-1011,1,0.755924,"We show that Gricean maxims 410 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 410–419, c MIT, Massachusetts, USA, 9-11 October 2010. 2010 Association for Computational Linguistics fall out naturally as consequences of the model. An effective way to empirically explore the pragmatic aspects of language is to work in the grounded setting, where the basic idea is to map language to some representation of the non-linguistic world (Yu and Ballard, 2004; Feldman and Narayanan, 2004; Fleischman and Roy, 2007; Chen and Mooney, 2008; Frank et al., 2009; Liang et al., 2009). Along similar lines, past work has also focused on interpreting natural language instructions (Branavan et al., 2009; Eisenstein et al., 2009; Kollar et al., 2010), which takes into account the goal of the communication. This work differs from ours in that it does not clarify the formal relationship between pragmatics and the interpretation task. Pragmatics has also been studied in the context of dialog systems. For instance, DeVault and Stone (2007) present a model of collaborative language between multiple agents that takes into account contextual ambiguities. We present our pragmatic mode"
D10-1040,P07-1121,0,0.0107279,"ight choose one of the following two utterances: (a) right of O2 (b) on O3 Although both utterances are semantically correct, (a) is ambiguous between O1 and O3, whereas (b) unambiguously identifies O1 as the target object, and should therefore be preferred over (a). In this paper, we present a game-theoretic model that captures this communication-oriented aspect of language interpretation and generation. Successful communication can be broken down into semantics and pragmatics. Most computational work on interpreting language focuses on compositional semantics (Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Piantadosi et al., 2008), which is concerned with verifying the truth of a sentence. However, what is missing from this truthoriented view is the pragmatic aspect of language— that language is used to accomplish an end goal, as exemplified by speech acts (Austin, 1962). Indeed, although both utterances (a) and (b) are semantically valid, only (b) is pragmatically felicitous: (a) is ambiguous and therefore violates the Gricean maxim of manner (Grice, 1975). To capture this maxim, we develop a model of pragmatics based on game theory, in the spirit of J¨ager (2008) but extended to the stochast"
D10-1049,N04-1015,0,0.069203,"weather forecasts, obtaining results comparable to state-ofthe-art domain-specific systems both in terms of BLEU scores and human evaluation. 1 Introduction In this paper, we focus on the problem of generating descriptive text given a world state represented by a set of database records. While existing generation systems can be engineered to obtain good performance on particular domains (e.g., Dale et al. (2003), Green (2006), Turner et al. (2009), Reiter et al. (2005), inter alia), it is often difficult to adapt them across different domains. Furthermore, content selection (what to say: see Barzilay and Lee (2004), Foster and White (2004), inter alia) and surface realization (how to say it: see Ratnaparkhi (2002), Wong and Mooney (2007), Chen and Mooney (2008), Lu et al. (2009), etc.) are typically handled separately. Our goal is to build a simple, flexible system which is domain-independent and performs content selection and surface realization in a unified framework. Dan Klein UC Berkeley Berkeley, CA 94720 klein@cs.berkeley.edu We operate in a setting in which we are only given examples consisting of (i) a set of database records (input) and (ii) example human-generated text describing some of those"
D10-1049,P06-1130,0,0.0327956,"Missing"
D10-1049,W04-0601,0,0.0188729,"ning results comparable to state-ofthe-art domain-specific systems both in terms of BLEU scores and human evaluation. 1 Introduction In this paper, we focus on the problem of generating descriptive text given a world state represented by a set of database records. While existing generation systems can be engineered to obtain good performance on particular domains (e.g., Dale et al. (2003), Green (2006), Turner et al. (2009), Reiter et al. (2005), inter alia), it is often difficult to adapt them across different domains. Furthermore, content selection (what to say: see Barzilay and Lee (2004), Foster and White (2004), inter alia) and surface realization (how to say it: see Ratnaparkhi (2002), Wong and Mooney (2007), Chen and Mooney (2008), Lu et al. (2009), etc.) are typically handled separately. Our goal is to build a simple, flexible system which is domain-independent and performs content selection and surface realization in a unified framework. Dan Klein UC Berkeley Berkeley, CA 94720 klein@cs.berkeley.edu We operate in a setting in which we are only given examples consisting of (i) a set of database records (input) and (ii) example human-generated text describing some of those records (output). We use"
D10-1049,W06-1417,0,0.0965803,"decisions, arranged hierarchically and each trained discriminatively. We deployed our system in three different domains—Robocup sportscasting, technical weather forecasts, and common weather forecasts, obtaining results comparable to state-ofthe-art domain-specific systems both in terms of BLEU scores and human evaluation. 1 Introduction In this paper, we focus on the problem of generating descriptive text given a world state represented by a set of database records. While existing generation systems can be engineered to obtain good performance on particular domains (e.g., Dale et al. (2003), Green (2006), Turner et al. (2009), Reiter et al. (2005), inter alia), it is often difficult to adapt them across different domains. Furthermore, content selection (what to say: see Barzilay and Lee (2004), Foster and White (2004), inter alia) and surface realization (how to say it: see Ratnaparkhi (2002), Wong and Mooney (2007), Chen and Mooney (2008), Lu et al. (2009), etc.) are typically handled separately. Our goal is to build a simple, flexible system which is domain-independent and performs content selection and surface realization in a unified framework. Dan Klein UC Berkeley Berkeley, CA 94720 kle"
D10-1049,P02-1003,0,0.0172526,"(White et al., 2007), HPSG (Nakanishi et al., 2005), and LFG (Cahill and van Genabith, 2006). In each of the above cases, the decomposable structure of the tree/grammar enables tractability. However, we saw that it was important to include features that captured long-range dependencies. Our model is also similar in spirit to Ratnaparkhi (2002) in the use of non-local features, but we operate at three levels of hierarchy to include both content selection and surface realization. One issue that arises with long-range dependencies is the lack of efficient algorithms for finding the optimal text. Koller and Striegnitz (2002) perform surface realization of a flat semantics, which is NPhard, so they recast the problem as non-projective dependency parsing. Ratnaparkhi (2002) uses beam search to find an approximate solution. We found that a greedy approach obtained better results than beam search; Belz (2008) found greedy approaches to be effective as well. 7 Conclusion We have developed a simple yet powerful generation system that combines both content selection and surface realization in a domain independent way. Despite our approach being domain-independent, we were able to obtain performance comparable to the sta"
D10-1049,P09-1011,1,0.0757203,"and surface realization (how to say it: see Ratnaparkhi (2002), Wong and Mooney (2007), Chen and Mooney (2008), Lu et al. (2009), etc.) are typically handled separately. Our goal is to build a simple, flexible system which is domain-independent and performs content selection and surface realization in a unified framework. Dan Klein UC Berkeley Berkeley, CA 94720 klein@cs.berkeley.edu We operate in a setting in which we are only given examples consisting of (i) a set of database records (input) and (ii) example human-generated text describing some of those records (output). We use the model of Liang et al. (2009) to automatically induce the correspondences between words in the text and the actual database records mentioned. We break up the full generation process into a sequence of local decisions, training a log-linear classifier for each type of decision. We use a simple but expressive set of domain-independent features, where each decision is allowed to depend on the entire history of previous decisions, as in the model of Ratnaparkhi (2002). These long-range contextual dependencies turn out to be critical for accurate generation. More specifically, our model is defined in terms of three types of d"
D10-1049,D09-1042,0,0.0478593,"e focus on the problem of generating descriptive text given a world state represented by a set of database records. While existing generation systems can be engineered to obtain good performance on particular domains (e.g., Dale et al. (2003), Green (2006), Turner et al. (2009), Reiter et al. (2005), inter alia), it is often difficult to adapt them across different domains. Furthermore, content selection (what to say: see Barzilay and Lee (2004), Foster and White (2004), inter alia) and surface realization (how to say it: see Ratnaparkhi (2002), Wong and Mooney (2007), Chen and Mooney (2008), Lu et al. (2009), etc.) are typically handled separately. Our goal is to build a simple, flexible system which is domain-independent and performs content selection and surface realization in a unified framework. Dan Klein UC Berkeley Berkeley, CA 94720 klein@cs.berkeley.edu We operate in a setting in which we are only given examples consisting of (i) a set of database records (input) and (ii) example human-generated text describing some of those records (output). We use the model of Liang et al. (2009) to automatically induce the correspondences between words in the text and the actual database records mentio"
D10-1049,W05-1510,0,0.0559178,"form, represented as a tree, into a parse of the text. Soricut and Marcu (2006) uses tree structures called WIDLexpressions (the acronym corresponds to four operations akin to the rewrite rules of a grammar) to represent the realization process, and, like our approach, operates in a log-linear framework. Belz (2008) and Belz and Kow (2009) also perform surface realization from a PCFG-like grammar. Lu et al. (2009) 511 uses a conditional random field model over trees. Other authors have performed surface realization using various grammar formalisms, for instance CCG (White et al., 2007), HPSG (Nakanishi et al., 2005), and LFG (Cahill and van Genabith, 2006). In each of the above cases, the decomposable structure of the tree/grammar enables tractability. However, we saw that it was important to include features that captured long-range dependencies. Our model is also similar in spirit to Ratnaparkhi (2002) in the use of non-local features, but we operate at three levels of hierarchy to include both content selection and surface realization. One issue that arises with long-range dependencies is the lack of efficient algorithms for finding the optimal text. Koller and Striegnitz (2002) perform surface realiz"
D10-1049,P02-1040,0,0.0895557,"T IME, fields that span numbers and wind directions; and for ROBOCUP, fields that span words starting with purple or pink. For each record ri , we define Ti so that BASE(Ti ) and C OARSE(Ti ) are the corresponding two extracted templates. We restrict Fi to the set of abstracted fields in the C OARSE template 5 Experiments We now present an empirical evaluation of our system on our three domains—ROBOCUP, S UM T IME, and W EATHER G OV. 5.1 Evaluation Metrics Automatic Evaluation To evaluate surface realization (or, combined content selection and surface realization), we measured the BLEU score (Papineni et al., 2002) (the precision of 4-grams with a brevity penalty) of the system-generated output with respect to the human-generated output. To evaluate macro content selection, we measured the F1 score (the harmonic mean of precision and recall) of the set of records chosen with respect to the human-annotated set of records. Human Evaluation We conducted a human evaluation using Amazon Mechanical Turk. For each domain, we chose 100 scenarios randomly from the test set. We ran each system under consideration on each of these scenarios, and presented each resulting output to 10 evaluators.2 Evaluators were gi"
D10-1049,P06-1139,0,0.119784,"troduces a generative model of the text given the world state, and in some ways is similar in spirit to our model. Although that model is capable of generation in principle, it was designed for unsupervised induction of hidden alignments (which is exactly what we use it for). Even if combined with a language model, generated text was much worse than our baseline. The prominent approach for surface realization is rendering the text from a grammar. Wong and Mooney (2007) and Chen and Mooney (2008) use synchronous grammars that map a logical form, represented as a tree, into a parse of the text. Soricut and Marcu (2006) uses tree structures called WIDLexpressions (the acronym corresponds to four operations akin to the rewrite rules of a grammar) to represent the realization process, and, like our approach, operates in a log-linear framework. Belz (2008) and Belz and Kow (2009) also perform surface realization from a PCFG-like grammar. Lu et al. (2009) 511 uses a conditional random field model over trees. Other authors have performed surface realization using various grammar formalisms, for instance CCG (White et al., 2007), HPSG (Nakanishi et al., 2005), and LFG (Cahill and van Genabith, 2006). In each of th"
D10-1049,W09-0607,0,0.0541974,"anged hierarchically and each trained discriminatively. We deployed our system in three different domains—Robocup sportscasting, technical weather forecasts, and common weather forecasts, obtaining results comparable to state-ofthe-art domain-specific systems both in terms of BLEU scores and human evaluation. 1 Introduction In this paper, we focus on the problem of generating descriptive text given a world state represented by a set of database records. While existing generation systems can be engineered to obtain good performance on particular domains (e.g., Dale et al. (2003), Green (2006), Turner et al. (2009), Reiter et al. (2005), inter alia), it is often difficult to adapt them across different domains. Furthermore, content selection (what to say: see Barzilay and Lee (2004), Foster and White (2004), inter alia) and surface realization (how to say it: see Ratnaparkhi (2002), Wong and Mooney (2007), Chen and Mooney (2008), Lu et al. (2009), etc.) are typically handled separately. Our goal is to build a simple, flexible system which is domain-independent and performs content selection and surface realization in a unified framework. Dan Klein UC Berkeley Berkeley, CA 94720 klein@cs.berkeley.edu We"
D10-1049,2007.mtsummit-ucnlg.4,0,0.0747759,"rammars that map a logical form, represented as a tree, into a parse of the text. Soricut and Marcu (2006) uses tree structures called WIDLexpressions (the acronym corresponds to four operations akin to the rewrite rules of a grammar) to represent the realization process, and, like our approach, operates in a log-linear framework. Belz (2008) and Belz and Kow (2009) also perform surface realization from a PCFG-like grammar. Lu et al. (2009) 511 uses a conditional random field model over trees. Other authors have performed surface realization using various grammar formalisms, for instance CCG (White et al., 2007), HPSG (Nakanishi et al., 2005), and LFG (Cahill and van Genabith, 2006). In each of the above cases, the decomposable structure of the tree/grammar enables tractability. However, we saw that it was important to include features that captured long-range dependencies. Our model is also similar in spirit to Ratnaparkhi (2002) in the use of non-local features, but we operate at three levels of hierarchy to include both content selection and surface realization. One issue that arises with long-range dependencies is the lack of efficient algorithms for finding the optimal text. Koller and Striegnit"
D10-1049,P07-1121,0,0.0310771,"uman evaluation. 1 Introduction In this paper, we focus on the problem of generating descriptive text given a world state represented by a set of database records. While existing generation systems can be engineered to obtain good performance on particular domains (e.g., Dale et al. (2003), Green (2006), Turner et al. (2009), Reiter et al. (2005), inter alia), it is often difficult to adapt them across different domains. Furthermore, content selection (what to say: see Barzilay and Lee (2004), Foster and White (2004), inter alia) and surface realization (how to say it: see Ratnaparkhi (2002), Wong and Mooney (2007), Chen and Mooney (2008), Lu et al. (2009), etc.) are typically handled separately. Our goal is to build a simple, flexible system which is domain-independent and performs content selection and surface realization in a unified framework. Dan Klein UC Berkeley Berkeley, CA 94720 klein@cs.berkeley.edu We operate in a setting in which we are only given examples consisting of (i) a set of database records (input) and (ii) example human-generated text describing some of those records (output). We use the model of Liang et al. (2009) to automatically induce the correspondences between words in the t"
D10-1049,W09-0603,0,\N,Missing
D13-1117,1993.eamt-1.1,0,0.444029,"Missing"
D13-1117,P05-1045,1,0.0174749,"linear-chain CRFs on two sequence tagging tasks: the CoNLL 2003 NER shared task (Tjong Kim Sang and De Meulder, 2003) and the SANCL 2012 POS tagging task (Petrov and McDonald, 2012) . The standard CoNLL-2003 English shared task benchmark dataset (Tjong Kim Sang and De Meulder, 2003) is a collection of documents from Reuters newswire articles, annotated with four entity types: Person, Location, Organization, and Miscellaneous. We predicted the label sequence Y = {LOC, MISC, ORG, PER, O}T without considering the BIO tags. For training the CRF model, we used a comprehensive set of features from Finkel et al. (2005) that gives state-of-the-art results on this task. A total number of 437906 features were generated on the CoNLL-2003 training dataset. The most important features are: • The word, word shape, and letter n-grams (up to 6gram) at current position • The prediction, word, and word shape of the previous and next position • Previous word shape in conjunction with current word shape • Disjunctive word set of the previous and next 4 positions • Capitalization pattern in a 3 word window • Previous two words in conjunction with the word shape of the previous word • The current word matched against a li"
D13-1117,P06-1027,0,0.0803156,"Missing"
D13-1117,P05-1003,0,0.0148756,"rediction, without actually having to generate noised data. We focus on dropout noise (Hinton et al., 2012), a recently popularized form of artificial feature noise where a random subset of features is omitted independently for each training example. Dropout and its variants have been shown to outperform L2 regularization on various tasks (Hinton et al., 2012; Wang and Manning, 2013; Wan et al., 2013). Dropout is is similar in spirit to feature bagging in the deliberate removal of features, but performs the removal in a preset way rather than randomly (Bryll et al., 2003; Sutton et al., 2005; Smith et al., 2005). 1170 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1170–1179, c Seattle, Washington, USA, 18-21 October 2013. 2013 Association for Computational Linguistics Our approach is based on a second-order approximation to feature noising developed among others by Bishop (1995) and Wager et al. (2013), which allows us to convert dropout noise into a form of adaptive regularization. This method is suitable for structured prediction in log-linear models where second derivatives are computable. In particular, it can be used for multiclass classification wi"
D13-1117,W03-0419,0,0.0238872,"Missing"
D13-1160,D11-1039,0,0.0140545,"ion We focus on the problem of semantic parsing natural language utterances into logical forms that can be executed to produce denotations. Traditional semantic parsers (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Kwiatkowski et al., 2010) have two limitations: (i) they require annotated logical forms as supervision, and (ii) they operate in limited domains with a small number of logical predicates. Recent developments aim to lift these limitations, either by reducing the amount of supervision (Clarke et al., 2010; Liang et al., 2011; Goldwasser et al., 2011; Artzi and Zettlemoyer, 2011) or by increasing the number of logical predicates (Cai and Yates, 2013). The goal of this paper is to do both: learn a semantic parser without annotated logical forms that scales to the large number of predicates on Freebase. At the lexical level, a major challenge in semantic parsing is mapping natural language phrases (e.g., “attend”) to logical predicates (e.g., Education). While limited-domain semantic parsers are able to learn the lexicon from per-example supervision (Kwiatkowski et al., 2011; Liang et al., 2011), at large scale they have inadequate coverage (Cai and Yates, 2013). Previo"
D13-1160,Q13-1005,0,0.724997,"the place of death of Walt Disney, ignoring the matrix verb live. 5 Discussion Our work intersects with two strands of work. The first involves learning models of semantics guided by denotations or interactions with the world. Besides semantic parsing for querying databases (Popescu et al., 2003; Clarke et al., 2010; Liang et al., 2011), previous work has looked at interpreting natural language for performing programming tasks (Kushman and Barzilay, 2013; Lei et al., 2013), playing computer games (Branavan et al., 2010; Branavan et al., 2011), following navigational instructions (Chen, 2012; Artzi and Zettlemoyer, 2013), and interacting in the real world via perception (Matuszek et al., 2012; Tellex et al., 2011; Krishnamurthy and Kollar, 2013). Our system uses denotations rather than logical forms as a training signal, but also benefits from denotation features, which becomes possible in the grounded setting. The second body of work involves connecting natural language and open-domain databases. Sev˜ Oracle score is the fraction of examples for which D(x) contains any derivation with the correct denotation. 8 1542 eral works perform relation extraction using distant supervision from a knowledge base (Riedel"
D13-1160,P10-1129,0,0.00937516,"Missing"
D13-1160,P11-1028,0,0.00622826,". For example, in “Where did Walt Disney live before he died?” it returns the place of death of Walt Disney, ignoring the matrix verb live. 5 Discussion Our work intersects with two strands of work. The first involves learning models of semantics guided by denotations or interactions with the world. Besides semantic parsing for querying databases (Popescu et al., 2003; Clarke et al., 2010; Liang et al., 2011), previous work has looked at interpreting natural language for performing programming tasks (Kushman and Barzilay, 2013; Lei et al., 2013), playing computer games (Branavan et al., 2010; Branavan et al., 2011), following navigational instructions (Chen, 2012; Artzi and Zettlemoyer, 2013), and interacting in the real world via perception (Matuszek et al., 2012; Tellex et al., 2011; Krishnamurthy and Kollar, 2013). Our system uses denotations rather than logical forms as a training signal, but also benefits from denotation features, which becomes possible in the grounded setting. The second body of work involves connecting natural language and open-domain databases. Sev˜ Oracle score is the fraction of examples for which D(x) contains any derivation with the correct denotation. 8 1542 eral works perf"
D13-1160,P12-1014,0,0.00683997,"s. First, we introduce features indicating when a word of a given POS tag is skipped, which could capture the fact that skipping auxiliaries is generally acceptable, while skipping proper nouns is not. Second, we introduce features on the POS tags involved in a composition, inspired by dependency parsing (McDonald et al., 2005). Specifically, when we combine 1538 logical forms z1 and z2 via a join or bridging, we include a feature on the POS tag of (the first word spanned by) z1 conjoined with the POS tag corresponding to z2 . Rather than using head-modifier information from dependency trees (Branavan et al., 2012; Krishnamurthy and Mitchell, 2012; Cai and Yates, 2013; Poon, 2013), we can learn the appropriate relationships tailored for downstream accuracy. For example, the phrase “located” is aligned to the predicate ContainedBy. POS features can detect that if “located” precedes a noun phrase (“What is located in Beijing?”), then the noun phrase is the object of the predicate, and if it follows the noun phrase (“Where is Beijing located?”), then it is in subject position. Note that our three operations (intersection, join, and bridging) are quite permissive, and we rely on features, which encode soft"
D13-1160,P13-1042,0,0.526067,"ty In this paper, we train a semantic parser that scales up to Freebase. Instead of relying on annotated logical forms, which is especially expensive to obtain at large scale, we learn from question-answer pairs. The main challenge in this setting is narrowing down the huge number of possible logical predicates for a given question. We tackle this problem in two ways: First, we build a coarse mapping from phrases to predicates using a knowledge base and a large text corpus. Second, we use a bridging operation to generate additional predicates based on neighboring predicates. On the dataset of Cai and Yates (2013), despite not having annotated logical forms, our system outperforms their state-of-the-art parser. Additionally, we collected a more realistic and challenging dataset of question-answer pairs and improves over a natural baseline. Execute on Database Type.University u Education.BarackObama Type.University bridging Education alignment BarackObama alignment Which college did Obama go to ? Figure 1: Our task is to map questions to answers via latent logical forms. To narrow down the space of logical predicates, we use a (i) coarse alignment based on Freebase and a text corpus and (ii) a bridging"
D13-1160,chang-manning-2012-sutime,0,0.0420154,"les (e1 , r, e2 ) (e.g., (“Obama”, “was also born in”, “August 1961”)) 4 Freebase associates each entity with a set of types using the Type property. 1536 were extracted from ClueWeb09 using the ReVerb open IE system (Fader et al., 2011). Lin et al. (2012) released a subset of these triples5 where they were able to substitute the subject arguments with KB entities. We downloaded their dataset and heuristically replaced object arguments with KB entities by walking on the Freebase graph from subject KB entities and performing simple string matching. In addition, we normalized dates with SUTime (Chang and Manning, 2012). We lemmatize and normalize each text phrase r ∈ R1 and augment it with a type signature [t1 , t2 ] to deal with polysemy (“born in” could either map to PlaceOfBirth or DateOfBirth). We add an entity pair (e1 , e2 ) to the extension of F(r[t1 , t2 ]) if the (Freebase) type of e1 (e2 ) is t1 (t2 ). For example, (BarackObama, 1961) is added to F(“born in”[Person, Date]). We perform a similar procedure that uses a Hearst-like pattern (Hearst, 1992) to map phrases to unary predicates. If a text phrase r ∈ R1 matches the pattern “(is|was a|the) x IN”, where IN is a preposition, then we add e1 to F"
D13-1160,P12-1045,0,0.00683741,"” it returns the place of death of Walt Disney, ignoring the matrix verb live. 5 Discussion Our work intersects with two strands of work. The first involves learning models of semantics guided by denotations or interactions with the world. Besides semantic parsing for querying databases (Popescu et al., 2003; Clarke et al., 2010; Liang et al., 2011), previous work has looked at interpreting natural language for performing programming tasks (Kushman and Barzilay, 2013; Lei et al., 2013), playing computer games (Branavan et al., 2010; Branavan et al., 2011), following navigational instructions (Chen, 2012; Artzi and Zettlemoyer, 2013), and interacting in the real world via perception (Matuszek et al., 2012; Tellex et al., 2011; Krishnamurthy and Kollar, 2013). Our system uses denotations rather than logical forms as a training signal, but also benefits from denotation features, which becomes possible in the grounded setting. The second body of work involves connecting natural language and open-domain databases. Sev˜ Oracle score is the fraction of examples for which D(x) contains any derivation with the correct denotation. 8 1542 eral works perform relation extraction using distant supervision"
D13-1160,W10-2903,0,0.0679367,"rates predicates compatible with neighboring predicates. Introduction We focus on the problem of semantic parsing natural language utterances into logical forms that can be executed to produce denotations. Traditional semantic parsers (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Kwiatkowski et al., 2010) have two limitations: (i) they require annotated logical forms as supervision, and (ii) they operate in limited domains with a small number of logical predicates. Recent developments aim to lift these limitations, either by reducing the amount of supervision (Clarke et al., 2010; Liang et al., 2011; Goldwasser et al., 2011; Artzi and Zettlemoyer, 2011) or by increasing the number of logical predicates (Cai and Yates, 2013). The goal of this paper is to do both: learn a semantic parser without annotated logical forms that scales to the large number of predicates on Freebase. At the lexical level, a major challenge in semantic parsing is mapping natural language phrases (e.g., “attend”) to logical predicates (e.g., Education). While limited-domain semantic parsers are able to learn the lexicon from per-example supervision (Kwiatkowski et al., 2011; Liang et al., 2011),"
D13-1160,D11-1142,0,0.269794,"typed4 phrases R1 (e.g., “born in”[Person,Location]) and predicates R2 (e.g., PlaceOfBirth). For each r ∈ R1 ∪ R2 , we create its extension F(r), which is a set of co-occurring entitypairs (e.g., F(“born in”[Person,Location]) = {(BarackObama, Honolulu), . . . }. The lexicon is generated based on the overlap F(r1 ) ∩ F(r2 ), for r1 ∈ R1 and r2 ∈ R2 . Typed phrases 15 million triples (e1 , r, e2 ) (e.g., (“Obama”, “was also born in”, “August 1961”)) 4 Freebase associates each entity with a set of types using the Type property. 1536 were extracted from ClueWeb09 using the ReVerb open IE system (Fader et al., 2011). Lin et al. (2012) released a subset of these triples5 where they were able to substitute the subject arguments with KB entities. We downloaded their dataset and heuristically replaced object arguments with KB entities by walking on the Freebase graph from subject KB entities and performing simple string matching. In addition, we normalized dates with SUTime (Chang and Manning, 2012). We lemmatize and normalize each text phrase r ∈ R1 and augment it with a type signature [t1 , t2 ] to deal with polysemy (“born in” could either map to PlaceOfBirth or DateOfBirth). We add an entity pair (e1 , e"
D13-1160,P13-1158,0,0.717148,"phrases, whereas a lexicon concerns specific phrases, thus aggregating over facts. On the question answering side, recent methods have made progress in building semantic parsers for the open domain, but still require a fair amount of manual effort (Yahya et al., 2012; Unger et al., 2012; Cai and Yates, 2013). Our system reduces the amount of supervision and has a more extensive evaluation on a new dataset. Finally, although Freebase has thousands of properties, open information extraction (Banko et al., 2007; Fader et al., 2011; Masaum et al., 2012) and associated question answering systems (Fader et al., 2013) work over an even larger open-ended set of properties. The drawback of this regime is that the noise and the difficulty in canonicalization make it hard to perform reliable composition, thereby nullifying one of the key benefits of semantic parsing. An interesting midpoint involves keeping the structured knowledge base but augmenting the predicates, for example using random walks (Lao et al., 2011) or Markov logic (Zhang et al., 2012). This would allow us to map atomic words (e.g., “wife”) to composite predicates (e.g., λx.Marriage.Spouse.(Gender.Female ux)). Learning these composite predicat"
D13-1160,P11-1149,0,0.0503237,"ing predicates. Introduction We focus on the problem of semantic parsing natural language utterances into logical forms that can be executed to produce denotations. Traditional semantic parsers (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Kwiatkowski et al., 2010) have two limitations: (i) they require annotated logical forms as supervision, and (ii) they operate in limited domains with a small number of logical predicates. Recent developments aim to lift these limitations, either by reducing the amount of supervision (Clarke et al., 2010; Liang et al., 2011; Goldwasser et al., 2011; Artzi and Zettlemoyer, 2011) or by increasing the number of logical predicates (Cai and Yates, 2013). The goal of this paper is to do both: learn a semantic parser without annotated logical forms that scales to the large number of predicates on Freebase. At the lexical level, a major challenge in semantic parsing is mapping natural language phrases (e.g., “attend”) to logical predicates (e.g., Education). While limited-domain semantic parsers are able to learn the lexicon from per-example supervision (Kwiatkowski et al., 2011; Liang et al., 2011), at large scale they have inadequate coverage"
D13-1160,C92-2082,0,0.0941625,"alking on the Freebase graph from subject KB entities and performing simple string matching. In addition, we normalized dates with SUTime (Chang and Manning, 2012). We lemmatize and normalize each text phrase r ∈ R1 and augment it with a type signature [t1 , t2 ] to deal with polysemy (“born in” could either map to PlaceOfBirth or DateOfBirth). We add an entity pair (e1 , e2 ) to the extension of F(r[t1 , t2 ]) if the (Freebase) type of e1 (e2 ) is t1 (t2 ). For example, (BarackObama, 1961) is added to F(“born in”[Person, Date]). We perform a similar procedure that uses a Hearst-like pattern (Hearst, 1992) to map phrases to unary predicates. If a text phrase r ∈ R1 matches the pattern “(is|was a|the) x IN”, where IN is a preposition, then we add e1 to F(x). For (Honolulu, “is a city in”, Hawaii), we extract x = “city 00 and add Honolulu to F(“city”). From the initial 15M triples, we extracted 55,081 typed binary phrases (9,456 untyped) and 6,299 unary phrases. Logical predicates Binary logical predicates contain (i) all KB properties6 and (ii) concatenations of two properties p1 .p2 if the intermediate type represents an event (e.g., the married to relation is represented by Marriage.Spouse). F"
D13-1160,P11-1055,0,0.579056,"ia perception (Matuszek et al., 2012; Tellex et al., 2011; Krishnamurthy and Kollar, 2013). Our system uses denotations rather than logical forms as a training signal, but also benefits from denotation features, which becomes possible in the grounded setting. The second body of work involves connecting natural language and open-domain databases. Sev˜ Oracle score is the fraction of examples for which D(x) contains any derivation with the correct denotation. 8 1542 eral works perform relation extraction using distant supervision from a knowledge base (Riedel et al., 2010; Carlson et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012). While similar in spirit to our alignment procedure for building the lexicon, one difference is that relation extraction cares about facts, aggregating over phrases, whereas a lexicon concerns specific phrases, thus aggregating over facts. On the question answering side, recent methods have made progress in building semantic parsers for the open domain, but still require a fair amount of manual effort (Yahya et al., 2012; Unger et al., 2012; Cai and Yates, 2013). Our system reduces the amount of supervision and has a more extensive evaluation on a new dataset. Finally,"
D13-1160,Q13-1016,0,0.0899754,". The first involves learning models of semantics guided by denotations or interactions with the world. Besides semantic parsing for querying databases (Popescu et al., 2003; Clarke et al., 2010; Liang et al., 2011), previous work has looked at interpreting natural language for performing programming tasks (Kushman and Barzilay, 2013; Lei et al., 2013), playing computer games (Branavan et al., 2010; Branavan et al., 2011), following navigational instructions (Chen, 2012; Artzi and Zettlemoyer, 2013), and interacting in the real world via perception (Matuszek et al., 2012; Tellex et al., 2011; Krishnamurthy and Kollar, 2013). Our system uses denotations rather than logical forms as a training signal, but also benefits from denotation features, which becomes possible in the grounded setting. The second body of work involves connecting natural language and open-domain databases. Sev˜ Oracle score is the fraction of examples for which D(x) contains any derivation with the correct denotation. 8 1542 eral works perform relation extraction using distant supervision from a knowledge base (Riedel et al., 2010; Carlson et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012). While similar in spirit to our alignment pr"
D13-1160,D12-1069,0,0.282435,"thout annotated logical forms that scales to the large number of predicates on Freebase. At the lexical level, a major challenge in semantic parsing is mapping natural language phrases (e.g., “attend”) to logical predicates (e.g., Education). While limited-domain semantic parsers are able to learn the lexicon from per-example supervision (Kwiatkowski et al., 2011; Liang et al., 2011), at large scale they have inadequate coverage (Cai and Yates, 2013). Previous work on semantic parsing on Freebase uses a combination of manual rules (Yahya et al., 2012; Unger et al., 2012), distant supervision (Krishnamurthy and Mitchell, 2012), and schema 1533 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1533–1544, c Seattle, Washington, USA, 18-21 October 2013. 2013 Association for Computational Linguistics matching (Cai and Yates, 2013). We use a large amount of web text and a knowledge base to build a coarse alignment between phrases and predicates— an approach similar in spirit to Cai and Yates (2013). However, this alignment only allows us to generate a subset of the desired predicates. Aligning light verbs (e.g., “go”) and prepositions is not very informative due to polysemy, a"
D13-1160,N13-1103,0,0.181619,"h the type Person. (iii) The system sometimes incorrectly draws verbs from subordinate clauses. For example, in “Where did Walt Disney live before he died?” it returns the place of death of Walt Disney, ignoring the matrix verb live. 5 Discussion Our work intersects with two strands of work. The first involves learning models of semantics guided by denotations or interactions with the world. Besides semantic parsing for querying databases (Popescu et al., 2003; Clarke et al., 2010; Liang et al., 2011), previous work has looked at interpreting natural language for performing programming tasks (Kushman and Barzilay, 2013; Lei et al., 2013), playing computer games (Branavan et al., 2010; Branavan et al., 2011), following navigational instructions (Chen, 2012; Artzi and Zettlemoyer, 2013), and interacting in the real world via perception (Matuszek et al., 2012; Tellex et al., 2011; Krishnamurthy and Kollar, 2013). Our system uses denotations rather than logical forms as a training signal, but also benefits from denotation features, which becomes possible in the grounded setting. The second body of work involves connecting natural language and open-domain databases. Sev˜ Oracle score is the fraction of examples"
D13-1160,D10-1119,0,0.0609899,"nt BarackObama alignment Which college did Obama go to ? Figure 1: Our task is to map questions to answers via latent logical forms. To narrow down the space of logical predicates, we use a (i) coarse alignment based on Freebase and a text corpus and (ii) a bridging operation that generates predicates compatible with neighboring predicates. Introduction We focus on the problem of semantic parsing natural language utterances into logical forms that can be executed to produce denotations. Traditional semantic parsers (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Kwiatkowski et al., 2010) have two limitations: (i) they require annotated logical forms as supervision, and (ii) they operate in limited domains with a small number of logical predicates. Recent developments aim to lift these limitations, either by reducing the amount of supervision (Clarke et al., 2010; Liang et al., 2011; Goldwasser et al., 2011; Artzi and Zettlemoyer, 2011) or by increasing the number of logical predicates (Cai and Yates, 2013). The goal of this paper is to do both: learn a semantic parser without annotated logical forms that scales to the large number of predicates on Freebase. At the lexical lev"
D13-1160,D11-1140,0,0.133847,"the amount of supervision (Clarke et al., 2010; Liang et al., 2011; Goldwasser et al., 2011; Artzi and Zettlemoyer, 2011) or by increasing the number of logical predicates (Cai and Yates, 2013). The goal of this paper is to do both: learn a semantic parser without annotated logical forms that scales to the large number of predicates on Freebase. At the lexical level, a major challenge in semantic parsing is mapping natural language phrases (e.g., “attend”) to logical predicates (e.g., Education). While limited-domain semantic parsers are able to learn the lexicon from per-example supervision (Kwiatkowski et al., 2011; Liang et al., 2011), at large scale they have inadequate coverage (Cai and Yates, 2013). Previous work on semantic parsing on Freebase uses a combination of manual rules (Yahya et al., 2012; Unger et al., 2012), distant supervision (Krishnamurthy and Mitchell, 2012), and schema 1533 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1533–1544, c Seattle, Washington, USA, 18-21 October 2013. 2013 Association for Computational Linguistics matching (Cai and Yates, 2013). We use a large amount of web text and a knowledge base to build a coarse alignment"
D13-1160,D11-1049,0,0.54058,"taset. Finally, although Freebase has thousands of properties, open information extraction (Banko et al., 2007; Fader et al., 2011; Masaum et al., 2012) and associated question answering systems (Fader et al., 2013) work over an even larger open-ended set of properties. The drawback of this regime is that the noise and the difficulty in canonicalization make it hard to perform reliable composition, thereby nullifying one of the key benefits of semantic parsing. An interesting midpoint involves keeping the structured knowledge base but augmenting the predicates, for example using random walks (Lao et al., 2011) or Markov logic (Zhang et al., 2012). This would allow us to map atomic words (e.g., “wife”) to composite predicates (e.g., λx.Marriage.Spouse.(Gender.Female ux)). Learning these composite predicates would drastically increase the possible space of logical forms, but we believe that the methods proposed in this paper— alignment via distant supervision and bridging—can provide some traction on this problem. Acknowledgments We would like to thank Thomas Lin, Mausam and Oren Etzioni for providing us with open IE triples that are partially-linked to Freebase, and also Arun Chaganty for helpful co"
D13-1160,P13-1127,0,0.0118652,"system sometimes incorrectly draws verbs from subordinate clauses. For example, in “Where did Walt Disney live before he died?” it returns the place of death of Walt Disney, ignoring the matrix verb live. 5 Discussion Our work intersects with two strands of work. The first involves learning models of semantics guided by denotations or interactions with the world. Besides semantic parsing for querying databases (Popescu et al., 2003; Clarke et al., 2010; Liang et al., 2011), previous work has looked at interpreting natural language for performing programming tasks (Kushman and Barzilay, 2013; Lei et al., 2013), playing computer games (Branavan et al., 2010; Branavan et al., 2011), following navigational instructions (Chen, 2012; Artzi and Zettlemoyer, 2013), and interacting in the real world via perception (Matuszek et al., 2012; Tellex et al., 2011; Krishnamurthy and Kollar, 2013). Our system uses denotations rather than logical forms as a training signal, but also benefits from denotation features, which becomes possible in the grounded setting. The second body of work involves connecting natural language and open-domain databases. Sev˜ Oracle score is the fraction of examples for which D(x) cont"
D13-1160,P11-1060,1,0.607238,"atible with neighboring predicates. Introduction We focus on the problem of semantic parsing natural language utterances into logical forms that can be executed to produce denotations. Traditional semantic parsers (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Kwiatkowski et al., 2010) have two limitations: (i) they require annotated logical forms as supervision, and (ii) they operate in limited domains with a small number of logical predicates. Recent developments aim to lift these limitations, either by reducing the amount of supervision (Clarke et al., 2010; Liang et al., 2011; Goldwasser et al., 2011; Artzi and Zettlemoyer, 2011) or by increasing the number of logical predicates (Cai and Yates, 2013). The goal of this paper is to do both: learn a semantic parser without annotated logical forms that scales to the large number of predicates on Freebase. At the lexical level, a major challenge in semantic parsing is mapping natural language phrases (e.g., “attend”) to logical predicates (e.g., Education). While limited-domain semantic parsers are able to learn the lexicon from per-example supervision (Kwiatkowski et al., 2011; Liang et al., 2011), at large scale they"
D13-1160,J13-2005,1,0.70636,"ps new questions x to answers y via latent logical forms z and the knowledge base K. 1534 2.1 Knowledge base Let E denote a set of entities (e.g., BarackObama), and let P denote a set of properties (e.g., PlaceOfBirth). A knowledge base K is a set of assertions (e1 , p, e2 ) ∈ E × P × E (e.g., (BarackObama, PlaceOfBirth, Honolulu)). We use the Freebase knowledge base (Google, 2013), which has 41M non-numeric entities, 19K properties, and 596M assertions.1 2.2 Logical forms To query the knowledge base, we use a logical language called Lambda Dependency-Based Compositional Semantics (λ-DCS)—see Liang (2013) for details. For the purposes of this paper, we use a restricted subset called simple λ-DCS, which we will define below for the sake of completeness. The chief motivation of λ-DCS is to produce logical forms that are simpler than lambda calculus forms. For example, λx.∃a.p1 (x, a) ∧ ∃b.p2 (a, b) ∧ p3 (b, e) is expressed compactly in λ-DCS as p1 .p2 .p3 .e. Like DCS (Liang et al., 2011), λ-DCS makes existential quantification implicit, thereby reducing the number of variables. Variables are only used for anaphora and building composite binary predicates; these do not appear in simple λ-DCS. Ea"
D13-1160,W12-3016,0,0.00702754,".g., “born in”[Person,Location]) and predicates R2 (e.g., PlaceOfBirth). For each r ∈ R1 ∪ R2 , we create its extension F(r), which is a set of co-occurring entitypairs (e.g., F(“born in”[Person,Location]) = {(BarackObama, Honolulu), . . . }. The lexicon is generated based on the overlap F(r1 ) ∩ F(r2 ), for r1 ∈ R1 and r2 ∈ R2 . Typed phrases 15 million triples (e1 , r, e2 ) (e.g., (“Obama”, “was also born in”, “August 1961”)) 4 Freebase associates each entity with a set of types using the Type property. 1536 were extracted from ClueWeb09 using the ReVerb open IE system (Fader et al., 2011). Lin et al. (2012) released a subset of these triples5 where they were able to substitute the subject arguments with KB entities. We downloaded their dataset and heuristically replaced object arguments with KB entities by walking on the Freebase graph from subject KB entities and performing simple string matching. In addition, we normalized dates with SUTime (Chang and Manning, 2012). We lemmatize and normalize each text phrase r ∈ R1 and augment it with a type signature [t1 , t2 ] to deal with polysemy (“born in” could either map to PlaceOfBirth or DateOfBirth). We add an entity pair (e1 , e2 ) to the extensio"
D13-1160,D12-1048,0,0.440057,"is that relation extraction cares about facts, aggregating over phrases, whereas a lexicon concerns specific phrases, thus aggregating over facts. On the question answering side, recent methods have made progress in building semantic parsers for the open domain, but still require a fair amount of manual effort (Yahya et al., 2012; Unger et al., 2012; Cai and Yates, 2013). Our system reduces the amount of supervision and has a more extensive evaluation on a new dataset. Finally, although Freebase has thousands of properties, open information extraction (Banko et al., 2007; Fader et al., 2011; Masaum et al., 2012) and associated question answering systems (Fader et al., 2013) work over an even larger open-ended set of properties. The drawback of this regime is that the noise and the difficulty in canonicalization make it hard to perform reliable composition, thereby nullifying one of the key benefits of semantic parsing. An interesting midpoint involves keeping the structured knowledge base but augmenting the predicates, for example using random walks (Lao et al., 2011) or Markov logic (Zhang et al., 2012). This would allow us to map atomic words (e.g., “wife”) to composite predicates (e.g., λx.Marriag"
D13-1160,P05-1012,0,0.014856,"h can only represent one-sided preferences for having more or fewer of a given operation. Indicator features stabilize the model, preferring derivations with a well-balanced inventory of operations. Part-of-speech tag features To guide the composition of predicates, we use POS tags in two ways. First, we introduce features indicating when a word of a given POS tag is skipped, which could capture the fact that skipping auxiliaries is generally acceptable, while skipping proper nouns is not. Second, we introduce features on the POS tags involved in a composition, inspired by dependency parsing (McDonald et al., 2005). Specifically, when we combine 1538 logical forms z1 and z2 via a join or bridging, we include a feature on the POS tag of (the first word spanned by) z1 conjoined with the POS tag corresponding to z2 . Rather than using head-modifier information from dependency trees (Branavan et al., 2012; Krishnamurthy and Mitchell, 2012; Cai and Yates, 2013; Poon, 2013), we can learn the appropriate relationships tailored for downstream accuracy. For example, the phrase “located” is aligned to the predicate ContainedBy. POS features can detect that if “located” precedes a noun phrase (“What is located in"
D13-1160,P13-1092,0,0.28632,"kipped, which could capture the fact that skipping auxiliaries is generally acceptable, while skipping proper nouns is not. Second, we introduce features on the POS tags involved in a composition, inspired by dependency parsing (McDonald et al., 2005). Specifically, when we combine 1538 logical forms z1 and z2 via a join or bridging, we include a feature on the POS tag of (the first word spanned by) z1 conjoined with the POS tag corresponding to z2 . Rather than using head-modifier information from dependency trees (Branavan et al., 2012; Krishnamurthy and Mitchell, 2012; Cai and Yates, 2013; Poon, 2013), we can learn the appropriate relationships tailored for downstream accuracy. For example, the phrase “located” is aligned to the predicate ContainedBy. POS features can detect that if “located” precedes a noun phrase (“What is located in Beijing?”), then the noun phrase is the object of the predicate, and if it follows the noun phrase (“Where is Beijing located?”), then it is in subject position. Note that our three operations (intersection, join, and bridging) are quite permissive, and we rely on features, which encode soft, overlapping rules. In contrast, CCG-based methods (Kwiatkowski et"
D13-1160,D12-1042,0,0.527661,"et al., 2012; Tellex et al., 2011; Krishnamurthy and Kollar, 2013). Our system uses denotations rather than logical forms as a training signal, but also benefits from denotation features, which becomes possible in the grounded setting. The second body of work involves connecting natural language and open-domain databases. Sev˜ Oracle score is the fraction of examples for which D(x) contains any derivation with the correct denotation. 8 1542 eral works perform relation extraction using distant supervision from a knowledge base (Riedel et al., 2010; Carlson et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012). While similar in spirit to our alignment procedure for building the lexicon, one difference is that relation extraction cares about facts, aggregating over phrases, whereas a lexicon concerns specific phrases, thus aggregating over facts. On the question answering side, recent methods have made progress in building semantic parsers for the open domain, but still require a fair amount of manual effort (Yahya et al., 2012; Unger et al., 2012; Cai and Yates, 2013). Our system reduces the amount of supervision and has a more extensive evaluation on a new dataset. Finally, although Freebase has t"
D13-1160,P07-1121,0,0.0270625,"dging Education alignment BarackObama alignment Which college did Obama go to ? Figure 1: Our task is to map questions to answers via latent logical forms. To narrow down the space of logical predicates, we use a (i) coarse alignment based on Freebase and a text corpus and (ii) a bridging operation that generates predicates compatible with neighboring predicates. Introduction We focus on the problem of semantic parsing natural language utterances into logical forms that can be executed to produce denotations. Traditional semantic parsers (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Kwiatkowski et al., 2010) have two limitations: (i) they require annotated logical forms as supervision, and (ii) they operate in limited domains with a small number of logical predicates. Recent developments aim to lift these limitations, either by reducing the amount of supervision (Clarke et al., 2010; Liang et al., 2011; Goldwasser et al., 2011; Artzi and Zettlemoyer, 2011) or by increasing the number of logical predicates (Cai and Yates, 2013). The goal of this paper is to do both: learn a semantic parser without annotated logical forms that scales to the large number of predicates on F"
D13-1160,D12-1035,0,0.0209998,"e goal of this paper is to do both: learn a semantic parser without annotated logical forms that scales to the large number of predicates on Freebase. At the lexical level, a major challenge in semantic parsing is mapping natural language phrases (e.g., “attend”) to logical predicates (e.g., Education). While limited-domain semantic parsers are able to learn the lexicon from per-example supervision (Kwiatkowski et al., 2011; Liang et al., 2011), at large scale they have inadequate coverage (Cai and Yates, 2013). Previous work on semantic parsing on Freebase uses a combination of manual rules (Yahya et al., 2012; Unger et al., 2012), distant supervision (Krishnamurthy and Mitchell, 2012), and schema 1533 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1533–1544, c Seattle, Washington, USA, 18-21 October 2013. 2013 Association for Computational Linguistics matching (Cai and Yates, 2013). We use a large amount of web text and a knowledge base to build a coarse alignment between phrases and predicates— an approach similar in spirit to Cai and Yates (2013). However, this alignment only allows us to generate a subset of the desired predicates. Aligning light v"
D15-1038,D14-1162,0,0.090852,"66.0 98.9 75.6 56.3 51.6 98.5 78.2 92.6 71.7 99.0 87.4 Freebase Ded. Ind. 49.3 49.4 82.1 70.6 49.3 50.2 84.5 72.8 85.3 72.4 87.5 76.3 such as the bilinear model can outperform stateof-the-art models after compositional training. Socher et al. (2013) proposed parametrizing each entity vector as the average of vectors of words in the entity (wtad lincoln = 21 (wtad + wlincoln ), and pretraining these word vectors using the method of Turian et al. (2010). Table 5 reports results when using this approach in conjunction with compositional training. We initialized all models with word vectors from Pennington et al. (2014). We found that compositionally trained models outperform the neural tensor network (NTN) on WordNet, while being only slightly behind on Freebase. (We did not use word vectors in any of our other experiments.) When the strategy of averaging word vectors to form entity vectors is not applied, our compositional models are significantly better on WordNet and slightly better on Freebase. It is worth noting that in many domains, entity names are not lexically meaningful, so word vector averaging is not Table 3: Deduction and induction. We compare mean quantile performance of single-edge training ("
D15-1038,D14-1067,0,0.0152406,"se to xt for all valid (s, t) pairs. Distributional compositional semantics. Previous works have explored compositional vector space representations in the context of logic and sentence interpretation. In Socher et al. (2012), a matrix is associated with each word of a sentence, and can be used to recursively modify the meaning of nearby constituents. Grefenstette (2013) ex2. The model’s representation of the path type p and relation r should capture that spatial relationship; that is, x> s Wr1 . . . Wrk ≈ xt im> plies xs Wr ≈ xt , or simply Wr1 . . . Wrk ≈ Wr . 325 sify nodes in the network. Bordes et al. (2014) embed paths as a sum of relation vectors for question answering. Our approach is unique in modeling the denotation of each intermediate step of a path query, and using this information to regularize the spatial arrangement of entity vectors. 8 Discussion We introduced the task of answering path queries on an incomplete knowledge base, and presented a general technique for compositionalizing a broad class of vector space models. Our experiments show that compositional training leads to state-ofthe-art performance on both path query answering and knowledge base completion. There are several key"
D15-1038,N13-1008,0,0.798724,"t have a daughter). Introduction Broad-coverage knowledge bases such as Freebase (Bollacker et al., 2008) support a rich array of reasoning and question answering applications, but they are known to suffer from incomplete coverage (Min et al., 2013). For example, as of May 2015, Freebase has an entity Tad Lincoln (Abraham Lincoln’s son), but does not have his ethnicity. An elegant solution to incompleteness is using vector space representations: Controlling the dimensionality of the vector space forces generalization to new facts (Nickel et al., 2011; Nickel et al., 2012; Socher et al., 2013; Riedel et al., 2013; Neelakantan et al., 2015). In the example, we would hope to infer Tad’s ethnicity from the ethnicity of his parents. In this paper, we present a scheme to answer path queries on knowledge bases by “compositionalizing” a broad class of vector space models that have been used for knowledge base completion (see Figure 1). At a high level, we interpret the base vector space model as implementing a soft edge traversal operator. This operator can then be recursively applied to predict paths. Our interpretation suggests a new compositional training objective that encourages better modeling of 318 P"
D15-1038,D12-1110,0,0.0277406,"any composable model. (15) where JpK is the set of entity pairs connected by p, and similarly for JrK. Intuitively, one way for the model to implicitly learn and exploit such a Horn clause would be to satisfy the following two criteria: 1. The model should ensure a consistent spatial relationship between entity pairs that are related by the path type p; that is, keeping x> s Wr1 . . . Wrk close to xt for all valid (s, t) pairs. Distributional compositional semantics. Previous works have explored compositional vector space representations in the context of logic and sentence interpretation. In Socher et al. (2012), a matrix is associated with each word of a sentence, and can be used to recursively modify the meaning of nearby constituents. Grefenstette (2013) ex2. The model’s representation of the path type p and relation r should capture that spatial relationship; that is, x> s Wr1 . . . Wrk ≈ xt im> plies xs Wr ≈ xt , or simply Wr1 . . . Wrk ≈ Wr . 325 sify nodes in the network. Bordes et al. (2014) embed paths as a sum of relation vectors for question answering. Our approach is unique in modeling the denotation of each intermediate step of a path query, and using this information to regularize the s"
D15-1038,D14-1044,0,0.357785,"Missing"
D15-1038,Q14-1017,0,0.0248271,"rence. Figure 4: We divide paths of length 2 into high precision (> 0.3), low precision (≤ 0.3), and not co-occuring with r. Here r = nationality. Each box plot shows the min, max, and first and third quartiles of ∆dist(p, r). As hypothesized, compositional training results in large decreases in dist(p, r) for high precision paths p, modest decreases for low precision paths, and little to no decreases for irrelevant paths. plored the ability of tensors to simulate logical calculi. Bowman et al. (2014) showed that recursive neural networks can learn to distinguish important semantic relations. Socher et al. (2014) found that compositional models were powerful enough to describe and retrieve images. We demonstrate that compositional representations are also useful in the context of knowledge base querying and completion. In the aforementioned work, compositional models produce vectors which represent truth values, sentiment or image features. In our approach, vectors represent sets of entities constituting the denotation of a knowledge base query. Reproducibility Our code, data, and experiments are available on the CodaLab platform at https://www.codalab.org/worksheets/ 0xfcace41fdeec45f3bc6ddf31107b829"
D15-1038,S13-1001,0,0.0591363,"ly learn and exploit such a Horn clause would be to satisfy the following two criteria: 1. The model should ensure a consistent spatial relationship between entity pairs that are related by the path type p; that is, keeping x> s Wr1 . . . Wrk close to xt for all valid (s, t) pairs. Distributional compositional semantics. Previous works have explored compositional vector space representations in the context of logic and sentence interpretation. In Socher et al. (2012), a matrix is associated with each word of a sentence, and can be used to recursively modify the meaning of nearby constituents. Grefenstette (2013) ex2. The model’s representation of the path type p and relation r should capture that spatial relationship; that is, x> s Wr1 . . . Wrk ≈ xt im> plies xs Wr ≈ xt , or simply Wr1 . . . Wrk ≈ Wr . 325 sify nodes in the network. Bordes et al. (2014) embed paths as a sum of relation vectors for question answering. Our approach is unique in modeling the denotation of each intermediate step of a path query, and using this information to regularize the spatial arrangement of entity vectors. 8 Discussion We introduced the task of answering path queries on an incomplete knowledge base, and presented a"
D15-1038,P10-1040,0,0.0285034,"o work at the institution X works at, which is not just X.) Path query task Bilinear Bi-Diag TransE S INGLE C OMP S INGLE C OMP S INGLE C OMP WordNet Ded. Ind. 96.9 66.0 98.9 75.6 56.3 51.6 98.5 78.2 92.6 71.7 99.0 87.4 Freebase Ded. Ind. 49.3 49.4 82.1 70.6 49.3 50.2 84.5 72.8 85.3 72.4 87.5 76.3 such as the bilinear model can outperform stateof-the-art models after compositional training. Socher et al. (2013) proposed parametrizing each entity vector as the average of vectors of words in the entity (wtad lincoln = 21 (wtad + wlincoln ), and pretraining these word vectors using the method of Turian et al. (2010). Table 5 reports results when using this approach in conjunction with compositional training. We initialized all models with word vectors from Pennington et al. (2014). We found that compositionally trained models outperform the neural tensor network (NTN) on WordNet, while being only slightly behind on Freebase. (We did not use word vectors in any of our other experiments.) When the strategy of averaging word vectors to form entity vectors is not applied, our compositional models are significantly better on WordNet and slightly better on Freebase. It is worth noting that in many domains, ent"
D15-1038,D11-1049,0,0.358891,"completion? Table 2 reveals that C OMP also performs better on the single-edge task of knowledge base completion. This is somewhat surprising, since S INGLE is trained on a training set which distributionally matches the test set, whereas C OMP is not. However, C OMP’s better performance on path queries suggests that there must be another factor at play. At a high level, training on paths must be providing some form of structural regularization which reduces cascading errors. Indeed, paths in a knowledge graph have proven to be important features for predicting the existence of single edges (Lao et al., 2011; Neelakantan et al., 2015). For example, consider the following Horn clause: (14) t∈JqK When all entities in JqK are ranked above all incorrect entities, RQ is 1. In Figure 3, we illustrate how RQ changes over the course of a query. parents (x, y) ∧ location (y, z) 324 ⇒ place of birth (x, z) , We have already seen empirically that S INGLE does not meet criterion 1, because cascading errors cause it to put incorrect entity vectors xt0 closer to x> s Wr1 . . . Wrk than the correct entity. C OMP mitigates these errors. To empirically verify that C OMP also does a better job of meeting criterion"
D15-1038,N13-1095,0,0.0168123,"o support compositional queries (Ullman, 1985). For example, we might ask what the ethnicity of Abraham Lincoln’s daughter would be. This can be formulated as a path query on the knowledge graph, and we would like a method that can answer this efficiently, while generalizing over missing facts and even missing or hypothetical entities (Abraham Lincoln did not in fact have a daughter). Introduction Broad-coverage knowledge bases such as Freebase (Bollacker et al., 2008) support a rich array of reasoning and question answering applications, but they are known to suffer from incomplete coverage (Min et al., 2013). For example, as of May 2015, Freebase has an entity Tad Lincoln (Abraham Lincoln’s son), but does not have his ethnicity. An elegant solution to incompleteness is using vector space representations: Controlling the dimensionality of the vector space forces generalization to new facts (Nickel et al., 2011; Nickel et al., 2012; Socher et al., 2013; Riedel et al., 2013; Neelakantan et al., 2015). In the example, we would hope to infer Tad’s ethnicity from the ethnicity of his parents. In this paper, we present a scheme to answer path queries on knowledge bases by “compositionalizing” a broad cl"
D15-1038,P15-1016,0,0.710329,"ntroduction Broad-coverage knowledge bases such as Freebase (Bollacker et al., 2008) support a rich array of reasoning and question answering applications, but they are known to suffer from incomplete coverage (Min et al., 2013). For example, as of May 2015, Freebase has an entity Tad Lincoln (Abraham Lincoln’s son), but does not have his ethnicity. An elegant solution to incompleteness is using vector space representations: Controlling the dimensionality of the vector space forces generalization to new facts (Nickel et al., 2011; Nickel et al., 2012; Socher et al., 2013; Riedel et al., 2013; Neelakantan et al., 2015). In the example, we would hope to infer Tad’s ethnicity from the ethnicity of his parents. In this paper, we present a scheme to answer path queries on knowledge bases by “compositionalizing” a broad class of vector space models that have been used for knowledge base completion (see Figure 1). At a high level, we interpret the base vector space model as implementing a soft edge traversal operator. This operator can then be recursively applied to predict paths. Our interpretation suggests a new compositional training objective that encourages better modeling of 318 Proceedings of the 2015 Conf"
D16-1264,D14-1159,0,0.0090355,"age “precipitation ... falls under gravity”, then reason that “under” refers to a cause (not location), and thus determine the correct answer: “gravity”. How can we get a machine to make progress on the challenging task of reading comprehension? Historically, large, realistic datasets have played a critical role for driving fields forward—famous examples include ImageNet for object recognition (Deng et al., 2009) and the Penn Treebank for syntactic parsing (Marcus et al., 1993). Existing datasets for RC have one of two shortcomings: (i) those that are high in quality (Richardson et al., 2013; Berant et al., 2014) are too small for training modern data-intensive models, while (ii) those that are large (Hermann et al., 2015; Hill et al., 2015) are semi-synthetic and do not share the same characteristics as explicit reading comprehension questions. To address the need for a large and high-quality reading comprehension dataset, we present the Stan2383 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2383–2392, c Austin, Texas, November 1-5, 2016. 2016 Association for Computational Linguistics ford Question Answering Dataset v1.0 (SQuAD), freely available at htt"
D16-1264,W02-1033,0,0.043035,"ut their task is sentence selection, while ours requires selecting a specific span in the sentence. Selecting the span of text that answers a question is similar to answer extraction, the final step in the open-domain QA pipeline, methods for which include bootstrapping surface patterns (Ravichandran and Hovy, 2002), using dependency trees (Shen and Klakow, 2006), and using a factor graph over multiple sentences (Sun et al., 2013). One key difference between our RC setting and answer extraction is that answer extraction typically exploits the fact that the answer occurs in multiple documents (Brill et al., 2002), which is more lenient than in our setting, where a system only has access to a single reading passage. Cloze datasets. Recently, researchers have constructed cloze datasets, in which the goal is to predict the missing word (often a named entity) in a passage. Since these datasets can be automatically generated from naturally occurring data, they can be extremely large. The Children’s Book Test (CBT) (Hill et al., 2015), for example, involves predicting a blanked-out word of a sentence given the 20 previous sentences. Hermann et al. (2015) constructed a corpus of cloze style questions by blan"
D16-1264,P16-1223,0,0.247782,"the goal is to predict the missing word (often a named entity) in a passage. Since these datasets can be automatically generated from naturally occurring data, they can be extremely large. The Children’s Book Test (CBT) (Hill et al., 2015), for example, involves predicting a blanked-out word of a sentence given the 20 previous sentences. Hermann et al. (2015) constructed a corpus of cloze style questions by blanking out entities in abstractive summaries of CNN / Daily News articles; the goal is to fill in the entity based on the original article. While the size of this dataset is impressive, Chen et al. (2016) showed that the dataset requires less reasoning than previously thought, and 2385 Figure 2: The crowd-facing web interface used to collect the dataset encourages crowdworkers to use their own words while asking questions. concluded that performance is almost saturated. One difference between SQuAD questions and cloze-style queries is that answers to cloze queries are single words or entities, while answers in SQuAD often include non-entities and can be much longer phrases. Another difference is that SQuAD focuses on questions whose answers are entailed by the passage, whereas the answers to c"
D16-1264,P99-1042,0,0.502279,"RC, fill in single entity RC, fill in single word 1.4M 1479 688K Table 1: A survey of several reading comprehension and question answering datasets. SQuAD is much larger than all datasets except the semi-synthetic cloze-style datasets, and it is similar to TREC-QA in the open-endedness of the answers. 2 Existing Datasets We begin with a survey of existing reading comprehension and question answering (QA) datasets, highlighting a variety of task formulation and creation strategies (see Table 1 for an overview). Reading comprehension. A data-driven approach to reading comprehension goes back to Hirschman et al. (1999), who curated a dataset of 600 real 3rd– 6th grade reading comprehension questions. Their pattern matching baseline was subsequently improved by a rule-based system (Riloff and Thelen, 2000) and a logistic regression model (Ng et al., 2000). More recently, Richardson et al. (2013) curated MCTest, which contains 660 stories created by crowdworkers, with 4 questions per story and 4 answer choices per question. Because many of the questions require commonsense reasoning and reasoning across multiple sentences, the dataset remains quite challenging, though there has been noticeable progress (Naras"
D16-1264,D14-1058,0,0.00413803,"tion. Because many of the questions require commonsense reasoning and reasoning across multiple sentences, the dataset remains quite challenging, though there has been noticeable progress (Narasimhan and Barzilay, 2015; Sachan et al., 2015; Wang et al., 2015). Both curated datasets, although real and difficult, are too small to support very expressive statistical models. Some datasets focus on deeper reasoning abilities. Algebra word problems require understanding a story well enough to turn it into a system of equations, which can be easily solved to produce the answer (Kushman et al., 2014; Hosseini et al., 2014). BAbI (Weston et al., 2015), a fully synthetic RC dataset, is stratified by different types of reasoning required to solve each task. Clark and Etzioni (2016) describe the task of solving 4th grade science exams, and stress the need to reason with world knowledge. Open-domain question answering. The goal of open-domain QA is to answer a question from a large collection of documents. The annual evaluations at the Text REtreival Conference (TREC) (Voorhees and Tice, 2000) led to many advances in open-domain QA, many of which were used in IBM Watson for Jeopardy! (Ferrucci et al., 2013). Recentl"
D16-1264,P14-1026,0,0.00481522,"nswer choices per question. Because many of the questions require commonsense reasoning and reasoning across multiple sentences, the dataset remains quite challenging, though there has been noticeable progress (Narasimhan and Barzilay, 2015; Sachan et al., 2015; Wang et al., 2015). Both curated datasets, although real and difficult, are too small to support very expressive statistical models. Some datasets focus on deeper reasoning abilities. Algebra word problems require understanding a story well enough to turn it into a system of equations, which can be easily solved to produce the answer (Kushman et al., 2014; Hosseini et al., 2014). BAbI (Weston et al., 2015), a fully synthetic RC dataset, is stratified by different types of reasoning required to solve each task. Clark and Etzioni (2016) describe the task of solving 4th grade science exams, and stress the need to reason with world knowledge. Open-domain question answering. The goal of open-domain QA is to answer a question from a large collection of documents. The annual evaluations at the Text REtreival Conference (TREC) (Voorhees and Tice, 2000) led to many advances in open-domain QA, many of which were used in IBM Watson for Jeopardy! (Ferrucc"
D16-1264,J93-2004,0,0.0811487,"pitation to fall?” posed on the passage in Figure 1. In order to answer the question, one might first locate the relevant part of the passage “precipitation ... falls under gravity”, then reason that “under” refers to a cause (not location), and thus determine the correct answer: “gravity”. How can we get a machine to make progress on the challenging task of reading comprehension? Historically, large, realistic datasets have played a critical role for driving fields forward—famous examples include ImageNet for object recognition (Deng et al., 2009) and the Penn Treebank for syntactic parsing (Marcus et al., 1993). Existing datasets for RC have one of two shortcomings: (i) those that are high in quality (Richardson et al., 2013; Berant et al., 2014) are too small for training modern data-intensive models, while (ii) those that are large (Hermann et al., 2015; Hill et al., 2015) are semi-synthetic and do not share the same characteristics as explicit reading comprehension questions. To address the need for a large and high-quality reading comprehension dataset, we present the Stan2383 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2383–2392, c Austin, Texas"
D16-1264,P15-1121,0,0.00695705,"1999), who curated a dataset of 600 real 3rd– 6th grade reading comprehension questions. Their pattern matching baseline was subsequently improved by a rule-based system (Riloff and Thelen, 2000) and a logistic regression model (Ng et al., 2000). More recently, Richardson et al. (2013) curated MCTest, which contains 660 stories created by crowdworkers, with 4 questions per story and 4 answer choices per question. Because many of the questions require commonsense reasoning and reasoning across multiple sentences, the dataset remains quite challenging, though there has been noticeable progress (Narasimhan and Barzilay, 2015; Sachan et al., 2015; Wang et al., 2015). Both curated datasets, although real and difficult, are too small to support very expressive statistical models. Some datasets focus on deeper reasoning abilities. Algebra word problems require understanding a story well enough to turn it into a system of equations, which can be easily solved to produce the answer (Kushman et al., 2014; Hosseini et al., 2014). BAbI (Weston et al., 2015), a fully synthetic RC dataset, is stratified by different types of reasoning required to solve each task. Clark and Etzioni (2016) describe the task of solving 4th gra"
D16-1264,W00-1316,0,0.120987,"imilar to TREC-QA in the open-endedness of the answers. 2 Existing Datasets We begin with a survey of existing reading comprehension and question answering (QA) datasets, highlighting a variety of task formulation and creation strategies (see Table 1 for an overview). Reading comprehension. A data-driven approach to reading comprehension goes back to Hirschman et al. (1999), who curated a dataset of 600 real 3rd– 6th grade reading comprehension questions. Their pattern matching baseline was subsequently improved by a rule-based system (Riloff and Thelen, 2000) and a logistic regression model (Ng et al., 2000). More recently, Richardson et al. (2013) curated MCTest, which contains 660 stories created by crowdworkers, with 4 questions per story and 4 answer choices per question. Because many of the questions require commonsense reasoning and reasoning across multiple sentences, the dataset remains quite challenging, though there has been noticeable progress (Narasimhan and Barzilay, 2015; Sachan et al., 2015; Wang et al., 2015). Both curated datasets, although real and difficult, are too small to support very expressive statistical models. Some datasets focus on deeper reasoning abilities. Algebra w"
D16-1264,P02-1006,0,0.0333395,"ons at the Text REtreival Conference (TREC) (Voorhees and Tice, 2000) led to many advances in open-domain QA, many of which were used in IBM Watson for Jeopardy! (Ferrucci et al., 2013). Recently, Yang et al. (2015) created the WikiQA dataset, which, like SQuAD, use Wikipedia passages as a source of answers, but their task is sentence selection, while ours requires selecting a specific span in the sentence. Selecting the span of text that answers a question is similar to answer extraction, the final step in the open-domain QA pipeline, methods for which include bootstrapping surface patterns (Ravichandran and Hovy, 2002), using dependency trees (Shen and Klakow, 2006), and using a factor graph over multiple sentences (Sun et al., 2013). One key difference between our RC setting and answer extraction is that answer extraction typically exploits the fact that the answer occurs in multiple documents (Brill et al., 2002), which is more lenient than in our setting, where a system only has access to a single reading passage. Cloze datasets. Recently, researchers have constructed cloze datasets, in which the goal is to predict the missing word (often a named entity) in a passage. Since these datasets can be automati"
D16-1264,D13-1020,0,0.489393,"relevant part of the passage “precipitation ... falls under gravity”, then reason that “under” refers to a cause (not location), and thus determine the correct answer: “gravity”. How can we get a machine to make progress on the challenging task of reading comprehension? Historically, large, realistic datasets have played a critical role for driving fields forward—famous examples include ImageNet for object recognition (Deng et al., 2009) and the Penn Treebank for syntactic parsing (Marcus et al., 1993). Existing datasets for RC have one of two shortcomings: (i) those that are high in quality (Richardson et al., 2013; Berant et al., 2014) are too small for training modern data-intensive models, while (ii) those that are large (Hermann et al., 2015; Hill et al., 2015) are semi-synthetic and do not share the same characteristics as explicit reading comprehension questions. To address the need for a large and high-quality reading comprehension dataset, we present the Stan2383 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2383–2392, c Austin, Texas, November 1-5, 2016. 2016 Association for Computational Linguistics ford Question Answering Dataset v1.0 (SQuAD), f"
D16-1264,W00-0603,0,0.0450828,"xcept the semi-synthetic cloze-style datasets, and it is similar to TREC-QA in the open-endedness of the answers. 2 Existing Datasets We begin with a survey of existing reading comprehension and question answering (QA) datasets, highlighting a variety of task formulation and creation strategies (see Table 1 for an overview). Reading comprehension. A data-driven approach to reading comprehension goes back to Hirschman et al. (1999), who curated a dataset of 600 real 3rd– 6th grade reading comprehension questions. Their pattern matching baseline was subsequently improved by a rule-based system (Riloff and Thelen, 2000) and a logistic regression model (Ng et al., 2000). More recently, Richardson et al. (2013) curated MCTest, which contains 660 stories created by crowdworkers, with 4 questions per story and 4 answer choices per question. Because many of the questions require commonsense reasoning and reasoning across multiple sentences, the dataset remains quite challenging, though there has been noticeable progress (Narasimhan and Barzilay, 2015; Sachan et al., 2015; Wang et al., 2015). Both curated datasets, although real and difficult, are too small to support very expressive statistical models. Some datas"
D16-1264,P15-1024,0,0.0360804,"600 real 3rd– 6th grade reading comprehension questions. Their pattern matching baseline was subsequently improved by a rule-based system (Riloff and Thelen, 2000) and a logistic regression model (Ng et al., 2000). More recently, Richardson et al. (2013) curated MCTest, which contains 660 stories created by crowdworkers, with 4 questions per story and 4 answer choices per question. Because many of the questions require commonsense reasoning and reasoning across multiple sentences, the dataset remains quite challenging, though there has been noticeable progress (Narasimhan and Barzilay, 2015; Sachan et al., 2015; Wang et al., 2015). Both curated datasets, although real and difficult, are too small to support very expressive statistical models. Some datasets focus on deeper reasoning abilities. Algebra word problems require understanding a story well enough to turn it into a system of equations, which can be easily solved to produce the answer (Kushman et al., 2014; Hosseini et al., 2014). BAbI (Weston et al., 2015), a fully synthetic RC dataset, is stratified by different types of reasoning required to solve each task. Clark and Etzioni (2016) describe the task of solving 4th grade science exams, and"
D16-1264,P06-1112,0,0.0516596,"and Tice, 2000) led to many advances in open-domain QA, many of which were used in IBM Watson for Jeopardy! (Ferrucci et al., 2013). Recently, Yang et al. (2015) created the WikiQA dataset, which, like SQuAD, use Wikipedia passages as a source of answers, but their task is sentence selection, while ours requires selecting a specific span in the sentence. Selecting the span of text that answers a question is similar to answer extraction, the final step in the open-domain QA pipeline, methods for which include bootstrapping surface patterns (Ravichandran and Hovy, 2002), using dependency trees (Shen and Klakow, 2006), and using a factor graph over multiple sentences (Sun et al., 2013). One key difference between our RC setting and answer extraction is that answer extraction typically exploits the fact that the answer occurs in multiple documents (Brill et al., 2002), which is more lenient than in our setting, where a system only has access to a single reading passage. Cloze datasets. Recently, researchers have constructed cloze datasets, in which the goal is to predict the missing word (often a named entity) in a passage. Since these datasets can be automatically generated from naturally occurring data, t"
D16-1264,P15-2115,0,0.0129053,"ade reading comprehension questions. Their pattern matching baseline was subsequently improved by a rule-based system (Riloff and Thelen, 2000) and a logistic regression model (Ng et al., 2000). More recently, Richardson et al. (2013) curated MCTest, which contains 660 stories created by crowdworkers, with 4 questions per story and 4 answer choices per question. Because many of the questions require commonsense reasoning and reasoning across multiple sentences, the dataset remains quite challenging, though there has been noticeable progress (Narasimhan and Barzilay, 2015; Sachan et al., 2015; Wang et al., 2015). Both curated datasets, although real and difficult, are too small to support very expressive statistical models. Some datasets focus on deeper reasoning abilities. Algebra word problems require understanding a story well enough to turn it into a system of equations, which can be easily solved to produce the answer (Kushman et al., 2014; Hosseini et al., 2014). BAbI (Weston et al., 2015), a fully synthetic RC dataset, is stratified by different types of reasoning required to solve each task. Clark and Etzioni (2016) describe the task of solving 4th grade science exams, and stress the need to"
D16-1264,D15-1237,0,0.526803,"AbI (Weston et al., 2015), a fully synthetic RC dataset, is stratified by different types of reasoning required to solve each task. Clark and Etzioni (2016) describe the task of solving 4th grade science exams, and stress the need to reason with world knowledge. Open-domain question answering. The goal of open-domain QA is to answer a question from a large collection of documents. The annual evaluations at the Text REtreival Conference (TREC) (Voorhees and Tice, 2000) led to many advances in open-domain QA, many of which were used in IBM Watson for Jeopardy! (Ferrucci et al., 2013). Recently, Yang et al. (2015) created the WikiQA dataset, which, like SQuAD, use Wikipedia passages as a source of answers, but their task is sentence selection, while ours requires selecting a specific span in the sentence. Selecting the span of text that answers a question is similar to answer extraction, the final step in the open-domain QA pipeline, methods for which include bootstrapping surface patterns (Ravichandran and Hovy, 2002), using dependency trees (Shen and Klakow, 2006), and using a factor graph over multiple sentences (Sun et al., 2013). One key difference between our RC setting and answer extraction is t"
D17-1109,N16-1097,0,0.0220822,"Missing"
D17-1109,D14-1164,1,0.894392,"Missing"
D17-1109,D13-1160,1,0.494161,"ve manner. 1 Relation instances Knowledge Base Linked Entities Debbie Reynolds her title entertainer Fisher daughter child of Twitter Figure 1: An example describing entities and relations in knowledge base population. Harnessing the wealth of information present in unstructured text online has been a long standing goal for the natural language processing community. In particular, knowledge base population seeks to automatically construct a knowledge base consisting of relations between entities from a document corpus. Knowledge bases have found many applications including question answering (Berant et al., 2013; Fader et al., 2014; Authors contributed equally. Debbie Reynolds, per:parents, Fisher + Introduction ∗ Debbie Reynolds , title, entertainer Reddy et al., 2014), automated reasoning (Kalyanpur et al., 2012) and dialogue (Han et al., 2015). Evaluating these systems remains a challenge as it is not economically feasible to exhaustively annotate every possible candidate relation from a sufficiently large corpus. As a result, a poolingbased methodology is used in practice to construct datasets, similar to them methodology used in information retrieval (Jones and Rijsbergen, 1975; Harman, 1993). F"
D17-1109,W15-4616,0,0.0287141,"of information present in unstructured text online has been a long standing goal for the natural language processing community. In particular, knowledge base population seeks to automatically construct a knowledge base consisting of relations between entities from a document corpus. Knowledge bases have found many applications including question answering (Berant et al., 2013; Fader et al., 2014; Authors contributed equally. Debbie Reynolds, per:parents, Fisher + Introduction ∗ Debbie Reynolds , title, entertainer Reddy et al., 2014), automated reasoning (Kalyanpur et al., 2012) and dialogue (Han et al., 2015). Evaluating these systems remains a challenge as it is not economically feasible to exhaustively annotate every possible candidate relation from a sufficiently large corpus. As a result, a poolingbased methodology is used in practice to construct datasets, similar to them methodology used in information retrieval (Jones and Rijsbergen, 1975; Harman, 1993). For instance, at the annual NIST TAC KBP evaluation, all relations predicted by participating systems are pooled together, annotated and released as a dataset for researchers to develop and evaluate their systems on. However, during develop"
D17-1109,D15-1076,0,0.0293716,"Missing"
D17-1109,N16-1104,0,0.101056,"Missing"
D17-1109,P14-5010,1,0.0251035,"Missing"
D17-1109,D16-1106,0,0.0216071,"Missing"
D17-1109,P11-1138,0,0.0166738,"Missing"
D17-1109,Q14-1030,0,0.0136056,"bing entities and relations in knowledge base population. Harnessing the wealth of information present in unstructured text online has been a long standing goal for the natural language processing community. In particular, knowledge base population seeks to automatically construct a knowledge base consisting of relations between entities from a document corpus. Knowledge bases have found many applications including question answering (Berant et al., 2013; Fader et al., 2014; Authors contributed equally. Debbie Reynolds, per:parents, Fisher + Introduction ∗ Debbie Reynolds , title, entertainer Reddy et al., 2014), automated reasoning (Kalyanpur et al., 2012) and dialogue (Han et al., 2015). Evaluating these systems remains a challenge as it is not economically feasible to exhaustively annotate every possible candidate relation from a sufficiently large corpus. As a result, a poolingbased methodology is used in practice to construct datasets, similar to them methodology used in information retrieval (Jones and Rijsbergen, 1975; Harman, 1993). For instance, at the annual NIST TAC KBP evaluation, all relations predicted by participating systems are pooled together, annotated and released as a dataset for"
D17-1109,P14-1122,0,0.027738,"Missing"
D17-1125,D13-1160,1,0.956708,"an average of 8,400 partial logical forms per example. Searching for consistent logical forms is thus a major computational bottleneck. In this paper, we propose macro grammars to bias the search towards structurally sensible logical forms. To illustrate the key idea, suppose we managed to parse the utterance “Who ranked right after Turkey?” in the context of Table 1 into the following consistent logical form (in lambda DCS) (Section 2.1): Introduction We consider the task of learning a semantic parser for question answering from questionanswer pairs (Clarke et al., 2010; Liang et al., 2011; Berant et al., 2013; Artzi and Zettlemoyer, 2013; Pasupat and Liang, 2015). To train such a parser, the learning algorithm must somehow search for consistent logical forms (i.e., logical forms that execute to the correct answer denotation). Typically, the search space is defined by a compositional grammar over logical forms (e.g., a context-free grammar), which we will refer to as the base grammar. To cover logical forms that answer complex questions, the base grammar must be quite general and compositional, leading to a huge search space that contains many useless logical forms. For example, the parser of Pasup"
D17-1125,P14-1133,1,0.899668,"Missing"
D17-1125,Q15-1039,1,0.880847,"Missing"
D17-1125,W10-2903,0,0.0487175,"h beam size 100) generates and featurizes an average of 8,400 partial logical forms per example. Searching for consistent logical forms is thus a major computational bottleneck. In this paper, we propose macro grammars to bias the search towards structurally sensible logical forms. To illustrate the key idea, suppose we managed to parse the utterance “Who ranked right after Turkey?” in the context of Table 1 into the following consistent logical form (in lambda DCS) (Section 2.1): Introduction We consider the task of learning a semantic parser for question answering from questionanswer pairs (Clarke et al., 2010; Liang et al., 2011; Berant et al., 2013; Artzi and Zettlemoyer, 2013; Pasupat and Liang, 2015). To train such a parser, the learning algorithm must somehow search for consistent logical forms (i.e., logical forms that execute to the correct answer denotation). Typically, the search space is defined by a compositional grammar over logical forms (e.g., a context-free grammar), which we will refer to as the base grammar. To cover logical forms that answer complex questions, the base grammar must be quite general and compositional, leading to a huge search space that contains many useless logica"
D17-1125,P16-1004,0,0.100157,"complete logical forms. Parsers define composition based on a grammar formalism such as Combinatory Categorial Grammar (CCG) (Zettlemoyer and Collins, 2007; Kwiatkowski et al., 2011, 2013; Kushman and Barzilay, 2013; Krishnamurthy and Kollar, 2013), Synchronous CFG (Wong and Mooney, 2007), and CFG (Kate and Mooney, 2006; Chen and Mooney, 2011; Berant et al., 2013; Desai et al., 2016), while others use the syntactic structure of the utterance to guide composition (Poon and Domingos, 2009; Reddy et al., 2016). Recent neural semantic parsers allow any sequence of logical tokens to be generated (Dong and Lapata, 2016; Jia and Liang, 2016; Kocisk´y et al., 2016; Neelakantan et al., 2016; Liang et al., 2017; Guu et al., 2017). The flexibility of these composition methods allows arbitrary logical forms to be generated, but at the cost of a vastly increased search space. Whether we have annotated logical forms or not has dramatic implications on what type of approach will work. When logical forms are available, one can perform grammar induction to mine grammar rules without search (Kwiatkowski et al., 2010). When only annotated denotations are available, as in our setting, one must use a base grammar to defin"
D17-1125,D10-1119,0,0.377105,"Missing"
D17-1125,P13-1158,0,0.0672776,"Missing"
D17-1125,D11-1140,0,0.219561,"logical forms or not has dramatic implications on what type of approach will work. When logical forms are available, one can perform grammar induction to mine grammar rules without search (Kwiatkowski et al., 2010). When only annotated denotations are available, as in our setting, one must use a base grammar to define the output space of logical forms. Usually these base grammars come with many restrictions to guard against combinatorial explosion (Pasupat and Liang, 2015). 1221 Previous work on higher-order unification for lexicon induction (Kwiatkowski et al., 2010) using factored lexicons (Kwiatkowski et al., 2011) also learns logical form macros with an online algorithm. The result is a lexicon where each entry contains a logical form template and a set of possible phrases for triggering the template. In contrast, we have avoided binding grammar rules to particular phrases in order to handle lexical variations. Instead, we use a more flexible mechanism—holistic triggering—to determine which rules to fire. This allows us to generate logical forms for utterances containing unseen lexical paraphrases or where the triggering is spread throughout the sentence. For example, the question “Who is X, John or Y”"
D17-1125,P17-1097,1,0.834548,"rammar (CCG) (Zettlemoyer and Collins, 2007; Kwiatkowski et al., 2011, 2013; Kushman and Barzilay, 2013; Krishnamurthy and Kollar, 2013), Synchronous CFG (Wong and Mooney, 2007), and CFG (Kate and Mooney, 2006; Chen and Mooney, 2011; Berant et al., 2013; Desai et al., 2016), while others use the syntactic structure of the utterance to guide composition (Poon and Domingos, 2009; Reddy et al., 2016). Recent neural semantic parsers allow any sequence of logical tokens to be generated (Dong and Lapata, 2016; Jia and Liang, 2016; Kocisk´y et al., 2016; Neelakantan et al., 2016; Liang et al., 2017; Guu et al., 2017). The flexibility of these composition methods allows arbitrary logical forms to be generated, but at the cost of a vastly increased search space. Whether we have annotated logical forms or not has dramatic implications on what type of approach will work. When logical forms are available, one can perform grammar induction to mine grammar rules without search (Kwiatkowski et al., 2010). When only annotated denotations are available, as in our setting, one must use a base grammar to define the output space of logical forms. Usually these base grammars come with many restrictions to guard against"
D17-1125,P17-1003,0,0.0584651,"inatory Categorial Grammar (CCG) (Zettlemoyer and Collins, 2007; Kwiatkowski et al., 2011, 2013; Kushman and Barzilay, 2013; Krishnamurthy and Kollar, 2013), Synchronous CFG (Wong and Mooney, 2007), and CFG (Kate and Mooney, 2006; Chen and Mooney, 2011; Berant et al., 2013; Desai et al., 2016), while others use the syntactic structure of the utterance to guide composition (Poon and Domingos, 2009; Reddy et al., 2016). Recent neural semantic parsers allow any sequence of logical tokens to be generated (Dong and Lapata, 2016; Jia and Liang, 2016; Kocisk´y et al., 2016; Neelakantan et al., 2016; Liang et al., 2017; Guu et al., 2017). The flexibility of these composition methods allows arbitrary logical forms to be generated, but at the cost of a vastly increased search space. Whether we have annotated logical forms or not has dramatic implications on what type of approach will work. When logical forms are available, one can perform grammar induction to mine grammar rules without search (Kwiatkowski et al., 2010). When only annotated denotations are available, as in our setting, one must use a base grammar to define the output space of logical forms. Usually these base grammars come with many restrictio"
D17-1125,J13-2005,1,0.864942,"train a semantic parser from a training set of utterance-denotation pairs. 2.1 Knowledge base and logical forms A knowledge base refers to a collection of entities and relations. For the running example “Who ranked right after Turkey?”, we use Table 1 from Wikipedia as the knowledge base. Table cells (e.g., Turkey) and rows (e.g., r3 = the 3rd row) are treated as entities. Relations connect entities: for example, the relation Nation maps r3 to Turkey, and a special relation Next maps r3 to r4 . A logical form z is a small program that can be executed on the knowledge base. We use lambda DCS (Liang, 2013) as the language of logical forms. The smallest units of lambda DCS are entities (e.g., Turkey) and relations (e.g., Nation). Larger logical forms are composed from smaller ones, and the denotation of the new logical form can be computed from denotations of its constituents. For example, applying the join operation on Nation and Turkey gives Nation.Turkey, whose denotation is JNation.TurkeyKw = {r3 }, which corresponds to the 3rd row of the table. The partial logical form Nation.Turkey can then be used to construct a larger logical form: z = R[Nation].R[Next].Nation.Turkey, (1) where R[·] repr"
D17-1125,P16-1002,1,0.86672,". Parsers define composition based on a grammar formalism such as Combinatory Categorial Grammar (CCG) (Zettlemoyer and Collins, 2007; Kwiatkowski et al., 2011, 2013; Kushman and Barzilay, 2013; Krishnamurthy and Kollar, 2013), Synchronous CFG (Wong and Mooney, 2007), and CFG (Kate and Mooney, 2006; Chen and Mooney, 2011; Berant et al., 2013; Desai et al., 2016), while others use the syntactic structure of the utterance to guide composition (Poon and Domingos, 2009; Reddy et al., 2016). Recent neural semantic parsers allow any sequence of logical tokens to be generated (Dong and Lapata, 2016; Jia and Liang, 2016; Kocisk´y et al., 2016; Neelakantan et al., 2016; Liang et al., 2017; Guu et al., 2017). The flexibility of these composition methods allows arbitrary logical forms to be generated, but at the cost of a vastly increased search space. Whether we have annotated logical forms or not has dramatic implications on what type of approach will work. When logical forms are available, one can perform grammar induction to mine grammar rules without search (Kwiatkowski et al., 2010). When only annotated denotations are available, as in our setting, one must use a base grammar to define the output space of"
D17-1125,P06-1115,0,0.133584,"Missing"
D17-1125,D16-1116,0,0.118554,"Missing"
D17-1125,Q13-1016,0,0.155888,"Missing"
D17-1125,N13-1103,0,0.0330158,"Missing"
D17-1125,P11-1060,1,0.89778,"rates and featurizes an average of 8,400 partial logical forms per example. Searching for consistent logical forms is thus a major computational bottleneck. In this paper, we propose macro grammars to bias the search towards structurally sensible logical forms. To illustrate the key idea, suppose we managed to parse the utterance “Who ranked right after Turkey?” in the context of Table 1 into the following consistent logical form (in lambda DCS) (Section 2.1): Introduction We consider the task of learning a semantic parser for question answering from questionanswer pairs (Clarke et al., 2010; Liang et al., 2011; Berant et al., 2013; Artzi and Zettlemoyer, 2013; Pasupat and Liang, 2015). To train such a parser, the learning algorithm must somehow search for consistent logical forms (i.e., logical forms that execute to the correct answer denotation). Typically, the search space is defined by a compositional grammar over logical forms (e.g., a context-free grammar), which we will refer to as the base grammar. To cover logical forms that answer complex questions, the base grammar must be quite general and compositional, leading to a huge search space that contains many useless logical forms. For example"
D17-1125,P15-1142,1,0.581203,"mple. Searching for consistent logical forms is thus a major computational bottleneck. In this paper, we propose macro grammars to bias the search towards structurally sensible logical forms. To illustrate the key idea, suppose we managed to parse the utterance “Who ranked right after Turkey?” in the context of Table 1 into the following consistent logical form (in lambda DCS) (Section 2.1): Introduction We consider the task of learning a semantic parser for question answering from questionanswer pairs (Clarke et al., 2010; Liang et al., 2011; Berant et al., 2013; Artzi and Zettlemoyer, 2013; Pasupat and Liang, 2015). To train such a parser, the learning algorithm must somehow search for consistent logical forms (i.e., logical forms that execute to the correct answer denotation). Typically, the search space is defined by a compositional grammar over logical forms (e.g., a context-free grammar), which we will refer to as the base grammar. To cover logical forms that answer complex questions, the base grammar must be quite general and compositional, leading to a huge search space that contains many useless logical forms. For example, the parser of Pasupat and Liang (2015) on R[Nation].R[Next].Nation.Turkey,"
D17-1125,D09-1001,0,0.0390491,"traditional semantic parser maps natural language phrases into partial logical forms and composes these partial logical forms into complete logical forms. Parsers define composition based on a grammar formalism such as Combinatory Categorial Grammar (CCG) (Zettlemoyer and Collins, 2007; Kwiatkowski et al., 2011, 2013; Kushman and Barzilay, 2013; Krishnamurthy and Kollar, 2013), Synchronous CFG (Wong and Mooney, 2007), and CFG (Kate and Mooney, 2006; Chen and Mooney, 2011; Berant et al., 2013; Desai et al., 2016), while others use the syntactic structure of the utterance to guide composition (Poon and Domingos, 2009; Reddy et al., 2016). Recent neural semantic parsers allow any sequence of logical tokens to be generated (Dong and Lapata, 2016; Jia and Liang, 2016; Kocisk´y et al., 2016; Neelakantan et al., 2016; Liang et al., 2017; Guu et al., 2017). The flexibility of these composition methods allows arbitrary logical forms to be generated, but at the cost of a vastly increased search space. Whether we have annotated logical forms or not has dramatic implications on what type of approach will work. When logical forms are available, one can perform grammar induction to mine grammar rules without search ("
D17-1125,Q16-1010,0,0.0249954,"Missing"
D17-1125,P07-1121,0,0.669047,"Missing"
D17-1125,D07-1071,0,0.530108,"Missing"
D17-1125,D13-1161,0,0.0653958,"Missing"
D17-1215,K16-1002,0,0.0136481,"generating new adversarial examples at every iteration; this is feasible for images, where fast gradient-based adversaries exist, but is infeasible for domains where only slower adversaries are available. We contrast adversarial evaluation, as studied in this work, with generative adversarial models. While related in name, the two have very different goals. Generative adversarial models pit a generative model, whose goal is to generate realistic outputs, against a discriminative model, whose goal is to distinguish the generator’s outputs from real data (Smith, 2012; Goodfellow et al., 2014). Bowman et al. (2016) and Li et al. (2017) used such a setup for sentence and dialogue generation, respectively. Our setup also involves a generator and a discriminator in an adversarial relationship; however, our discriminative system is tasked with finding the right answer, not distinguishing the generated examples from real ones, and our goal is to evaluate the discriminative system, not to train the generative one. While we use adversaries as a way to evaluate language understanding, robustness to adversarial attacks may also be its own goal for tasks such as spam detection. Dalvi et al. (2004) formulated such"
D17-1215,P16-1002,1,0.641548,"ading comprehension, other adversarial methods may prove more effective on other tasks. As discussed previously, paraphrase generation systems (Madnani and Dorr, 2010) could be used for adversarial evaluation on a wide range of language tasks. Building on our intuition that existing models are overly stable, we could apply meaningaltering perturbations to inputs on tasks like machine translation, and adversarially choose ones for which the model’s output does not change. We could also adversarially generate new examples by combining multiple existing ones, in the spirit of Data Recombination (Jia and Liang, 2016). The Build It, Break It shared task (Bender et al., 2017) encourages researchers to adversarially design minimal pairs to fool sentiment analysis and semantic role labeling systems. Progress on building systems that truly understand language is only possible if our evaluation metrics can distinguish real intelligent behavior from shallow pattern matching. To this end, we have released scripts to run A DD S ENT on any SQuAD system, as well as code for A DDA NY. We hope that our work will motivate the development of more sophisticated models that understand language at a deeper level. Acknowled"
D17-1215,P14-5010,0,0.00178185,"Chicago Tadakatsu moved the city of Chicago to in 1881. AddAny Randomly initialize d words: (Step 4) Fix errors with crowdworkers, verify resulting sentences with other crowdworkers spring attention income getting reached Greedily change one word spring attention income other reached Repeat many times Adversary Adds: tesla move move other george Model Predicts: george Adversary Adds: Tadakatsu moved to the city of Chicago in 1881. Model Predicts: Chicago Figure 2: An illustration of the A DD S ENT and A DDA NY adversaries. of 26 types, corresponding to NER and POS tags from Stanford CoreNLP (Manning et al., 2014), plus a few custom categories (e.g., abbreviations), and manually associate a fake answer with each type. Given the original answer to a question, we compute its type and return the corresponding fake answer. In our running example, the correct answer was not tagged as a named entity, and has the POS tag NNP, which corresponds to the fake answer “Central Park.” In Step 3, we combine the altered question and fake answer into declarative form, using a set of roughly 50 manually-defined rules over CoreNLP constituency parses. For example, “What ABC division handles domestic television distributi"
D17-1215,P16-1144,0,0.0282472,"a classifier and an adversary, and analyzed optimal strategies for each player. Lowd and Meek (2005) described an efficient attack by which an adversary can reverseengineer the weights of a linear classifier, in order to then generate adversarial inputs. In contrast with these methods, we do not make strong structural assumptions about our classifiers. Other work has proposed harder test datasets for various tasks. Levesque (2013) proposed the Winograd Schema challenge, in which computers must resolve coreference resolution problems that were handcrafted to require extensive world knowledge. Paperno et al. (2016) constructed the LAMBADA dataset, which tests the ability of language models to handle long-range dependencies. Their method relies on the availability of a large initial dataset, from which they distill a difficult subset; such initial data may be unavailable for many tasks. Rimell et al. (2009) showed that dependency parsers that seem very accurate by standard metrics perform poorly on a subset of the test data that has unbounded dependency constructions. Such evaluation schemes can only test models on phenomena that are moderately frequent in the test distribution; by perturbing test exampl"
D17-1215,D14-1162,0,0.100023,"wer to confuse models. Figure 2 illustrates these two main adversaries. 3.3.1 A DD S ENT A DD S ENT uses a four-step procedure to generate sentences that look similar to the question, but do not actually contradict the correct answer. Refer to Figure 2 for an illustration of these steps. In Step 1, we apply semantics-altering perturbations to the question, in order to guarantee that the resulting adversarial sentence is compatible. We replace nouns and adjectives with antonyms from WordNet (Fellbaum, 1998), and change named entities and numbers to the nearest word in GloVe word vector space2 (Pennington et al., 2014) with the same part of speech.3 If no words are changed during this step, the adversary gives up and immediately returns the original example. For example, given the question “What ABC division handles domestic television distribution?”, we would change “ABC” to “NBC” (a nearby word in vector space) and “domestic” to “foreign” (a WordNet antonym), resulting in the question, “What NBC division handles foreign television distribution?” In Step 2, we create a fake answer that has the same “type” as the original answer. We define a set 2 We use 100-dimensional GloVe vectors trained on Wikipedia an"
D17-1215,D16-1264,1,0.244512,"n (Wang et al., 2016), RaSOR (Lee et al., 2017), Dynamic Chunk Reader (DCR) (Yu et al., 2016), and the Logistic Regression Baseline (Rajpurkar et al., 2016). We did not run these models during development, so they serve as a held-out set that validates the generality of our approach. 2.3 Standard Evaluation Given a model f that takes in paragraph-question pairs (p, q) and outputs an answer a ˆ, the standard accuracy over a test set Dtest is simply def Acc(f ) = 1 |Dtest | X v((p, q, a), f ), (p,q,a)∈Dtest where v is the F1 score between the true answer a and the predicted answer f (p, q) (see Rajpurkar et al. (2016) for details). 3 3.1 Adversarial Evaluation General Framework A model that relies on superficial cues without understanding language can do well according to average F1 score, if these cues happen to be predictive most of the time. Weissenborn et al. (2017) argue that many SQuAD questions can be answered with heuristics based on type and keyword-matching. To determine whether existing models have learned much beyond such simple patterns, we introduce adversaries that confuse deficient models by altering test examples. Consider the example in Figure 1: the BiDAF Ensemble model originally gives"
D17-1215,D09-1085,0,0.0480966,"make strong structural assumptions about our classifiers. Other work has proposed harder test datasets for various tasks. Levesque (2013) proposed the Winograd Schema challenge, in which computers must resolve coreference resolution problems that were handcrafted to require extensive world knowledge. Paperno et al. (2016) constructed the LAMBADA dataset, which tests the ability of language models to handle long-range dependencies. Their method relies on the availability of a large initial dataset, from which they distill a difficult subset; such initial data may be unavailable for many tasks. Rimell et al. (2009) showed that dependency parsers that seem very accurate by standard metrics perform poorly on a subset of the test data that has unbounded dependency constructions. Such evaluation schemes can only test models on phenomena that are moderately frequent in the test distribution; by perturbing test examples, we can introduce out-of-distribution phenomena while still leveraging prior data collection efforts. While concatenative adversaries are well-suited to reading comprehension, other adversarial methods may prove more effective on other tasks. As discussed previously, paraphrase generation syst"
D17-1215,D17-1230,0,0.0491787,"al examples at every iteration; this is feasible for images, where fast gradient-based adversaries exist, but is infeasible for domains where only slower adversaries are available. We contrast adversarial evaluation, as studied in this work, with generative adversarial models. While related in name, the two have very different goals. Generative adversarial models pit a generative model, whose goal is to generate realistic outputs, against a discriminative model, whose goal is to distinguish the generator’s outputs from real data (Smith, 2012; Goodfellow et al., 2014). Bowman et al. (2016) and Li et al. (2017) used such a setup for sentence and dialogue generation, respectively. Our setup also involves a generator and a discriminator in an adversarial relationship; however, our discriminative system is tasked with finding the right answer, not distinguishing the generated examples from real ones, and our goal is to evaluate the discriminative system, not to train the generative one. While we use adversaries as a way to evaluate language understanding, robustness to adversarial attacks may also be its own goal for tasks such as spam detection. Dalvi et al. (2004) formulated such tasks as a game betw"
D17-1215,D17-1085,0,0.0202495,"used on two published model architectures: BiDAF (Seo et al., 2016) and Match-LSTM (Wang and Jiang, 2016). Both are deep learning architectures that predict a probability distribution over the correct answer. Each model has a single and an ensemble version, yielding four systems in total. We also validate our major findings on twelve other published models with publicly available test-time code: ReasoNet Single and Ensemble versions (Shen et al., 2017), Mnemonic Reader Single and Ensemble versions (Hu et al., 2017), Structural Embedding of Dependency Trees (SEDT) Single and Ensemble versions (Liu et al., 2017), jNet (Zhang et al., 2017), Ruminating Reader (Gong and Bowman, 2017), MultiPerspective Context Matching (MPCM) Single version (Wang et al., 2016), RaSOR (Lee et al., 2017), Dynamic Chunk Reader (DCR) (Yu et al., 2016), and the Logistic Regression Baseline (Rajpurkar et al., 2016). We did not run these models during development, so they serve as a held-out set that validates the generality of our approach. 2.3 Standard Evaluation Given a model f that takes in paragraph-question pairs (p, q) and outputs an answer a ˆ, the standard accuracy over a test set Dtest is simply def Acc(f ) = 1 |Dtest"
D17-1215,J10-3003,0,0.070524,"e valid—a human would judge a0 as the correct answer to q 0 given p0 . Second, (p0 , q 0 , a0 ) should be somehow “close” to the original example (p, q, a). 3.2 Semantics-preserving Adversaries In image classification, adversarial examples are commonly generated by adding an imperceptible amount of noise to the input (Szegedy et al., 2014; Goodfellow et al., 2015). These perturbations do not change the semantics of the image, but they can change the predictions of models that are oversensitive to semantics-preserving changes. For language, the direct analogue would be to paraphrase the input (Madnani and Dorr, 2010). However, high-precision paraphrase generation is challenging, as most edits to a sentence do actually change its meaning. 3.3 Concatenative Adversaries Instead of relying on paraphrasing, we use perturbations that do alter semantics to build concatenative adversaries, which generate examples of the form (p + s, q, a) for some sentence s. In other words, concatenative adversaries add a new sentence to the end of the paragraph, and leave the question and answer unchanged. Valid adversarial examples are precisely those for which s does not contradict the correct answer; we refer to such sentenc"
D17-1215,K17-1028,0,0.0408075,"e generality of our approach. 2.3 Standard Evaluation Given a model f that takes in paragraph-question pairs (p, q) and outputs an answer a ˆ, the standard accuracy over a test set Dtest is simply def Acc(f ) = 1 |Dtest | X v((p, q, a), f ), (p,q,a)∈Dtest where v is the F1 score between the true answer a and the predicted answer f (p, q) (see Rajpurkar et al. (2016) for details). 3 3.1 Adversarial Evaluation General Framework A model that relies on superficial cues without understanding language can do well according to average F1 score, if these cues happen to be predictive most of the time. Weissenborn et al. (2017) argue that many SQuAD questions can be answered with heuristics based on type and keyword-matching. To determine whether existing models have learned much beyond such simple patterns, we introduce adversaries that confuse deficient models by altering test examples. Consider the example in Figure 1: the BiDAF Ensemble model originally gives the right answer, but gets confused when an adversarial distracting sentence is added to the paragraph. We define an adversary A to be a function that takes in an example (p, q, a), optionally with a model f , and returns a new example (p0 , q 0 , a0 ). The"
D18-1008,D14-1159,1,0.817362,"and discourse parsing on the other. The former produces predicate-argument representations of individual facts in a text (Baker et al., 1998; Gildea and Jurafsky, 2002; Palmer et al., 2005); the latter identifies discourse relations between syntactic clauses (Taboada and Mann, 2006; Prasad et al., 2007; Pitler et al., 2009; Prasad et al., 2010; Surdeanu et al., 2015). TAP first maps from syntax to a set of SRL-style representations, and then identifies structurallyconstrained, higher-order relations among them. It is in this sense reminiscent of, but distinct from, work on causal processes by Berant et al. (2014). Related Work Analogy. In the cognitive science literature, analogy is a general form of relational reasoning unique to human cognition (Tversky and Gati, 1978; Holyoak and Thagard, 1996; Goldstone and Son, 2005; Penn et al., 2008; Holyoak, 2012). Our model of textual analogy is particularly influenced by Structure Mapping Theory (Falkenhainer et al., 1989; Gentner and Markman, 1997), an influential cognitive model of analogy as a structurepreserving map between concepts. Within the NLP community, there has been Numbers in NLP. There has been some work on understanding numbers in text. This i"
D18-1008,P16-1055,1,0.768267,"and Son, 2005; Penn et al., 2008; Holyoak, 2012). Our model of textual analogy is particularly influenced by Structure Mapping Theory (Falkenhainer et al., 1989; Gentner and Markman, 1997), an influential cognitive model of analogy as a structurepreserving map between concepts. Within the NLP community, there has been Numbers in NLP. There has been some work on understanding numbers in text. This includes quantitative reasoning (Kushman et al., 2014; Roy et al., 2015), numerical information extraction (Madaan et al., 2016), and techniques for making numbers more easily interpretable in text (Chaganty and Liang, 2016; Kim et al., 2016). If pursued further, the application of plotting 89 References quantitative text that we discuss in this paper could help to clarify quantitative text on the web (Larkin and Simon, 1987; Barrio et al., 2016). Collin F Baker, Charles J Fillmore, and John B Lowe. 1998. The Berkeley FrameNet Project. In Proceedings of the 17th International Conference on Computational Linguistics-Volume 1, pages 86–90. Neural modeling. Recent work has shown the promise of sophisticated neural models on semantic role labeling (He et al., 2017). Similar to other such sequence prediction models,"
D18-1008,D12-1062,0,0.0197056,"he most likely role for each span and edge and then discarding any edges and spans that violate the wellformedness (1) and typing constraints (2). We then enforce transitivity constraints (4) by incrementally building a cluster of analogous and equivalent spans. We then resolve the unique facts constraint (3) by keeping only the span with highest FACT edge score. Finally, for every cluster of analogous VALUE spans, we check that the analogy constraint (5) holds and if not, discard the cluster. We also implement an optimal decoder that encodes the TAP constraints as an ILP (Roth and Yih, 2004; Do et al., 2012). The ILP tries to find an optimal decoding according to the model, subject to hard constraints imposed on the solution space. For example, we require that solutions satisfy the ‘connected spans’ constraint: which defines a joint distribution over per-token role labels. We thus obtain spans from this distribution corresponding to vertices of the graph described in Section 4 by merging contiguous rolelabels in the maximum likelihood label sequence predicted by the CRF. Edge prediction with PATH M AX features. For edge prediction, we use the spans identified above to construct span and edge embe"
D18-1008,J02-3001,0,0.03861,"system. Note that with the imposition of global constraints reflecting the structure of analogy, the system yields well-formed charts. Without these constraints, generated charts either have multiple yaxis values assigned to the same x-axis value, or have floating y-axis values with no grounding on the x-axis. 8 Discourse and Information Extraction. TAP is an information extraction task that synthesizes ideas from semantic role labeling on the one hand and discourse parsing on the other. The former produces predicate-argument representations of individual facts in a text (Baker et al., 1998; Gildea and Jurafsky, 2002; Palmer et al., 2005); the latter identifies discourse relations between syntactic clauses (Taboada and Mann, 2006; Prasad et al., 2007; Pitler et al., 2009; Prasad et al., 2010; Surdeanu et al., 2015). TAP first maps from syntax to a set of SRL-style representations, and then identifies structurallyconstrained, higher-order relations among them. It is in this sense reminiscent of, but distinct from, work on causal processes by Berant et al. (2014). Related Work Analogy. In the cognitive science literature, analogy is a general form of relational reasoning unique to human cognition (Tversky a"
D18-1008,P17-1044,0,0.0145695,"numbers more easily interpretable in text (Chaganty and Liang, 2016; Kim et al., 2016). If pursued further, the application of plotting 89 References quantitative text that we discuss in this paper could help to clarify quantitative text on the web (Larkin and Simon, 1987; Barrio et al., 2016). Collin F Baker, Charles J Fillmore, and John B Lowe. 1998. The Berkeley FrameNet Project. In Proceedings of the 17th International Conference on Computational Linguistics-Volume 1, pages 86–90. Neural modeling. Recent work has shown the promise of sophisticated neural models on semantic role labeling (He et al., 2017). Similar to other such sequence prediction models, e.g., those for named entity recognition (Lample et al., 2016) or semantic role labeling (Zhou and Xu, 2015), our span prediction utilizes a neural CRF. Our model also has an edge-prediction component, which benefits from a simplified version of the PathLSTM model of Roth and Lapata (2016). Our edge-prediction model also uses an embedding concatenation component, which was inspired by recent work on neural coreference resolution (Lee et al., 2017). He et al. (2017) also impose semantic constraints during prediction, but use A∗ search instead"
D18-1008,P14-5010,1,0.00906289,"raphs as defined in Section 4. Given a sentence, the neural model predicts a distribution over role-labeled spans with edges denoting semantic relations between them. Then, we use an ILP to decode while enforcing the TAP constraints defined in Section 4. Figure 4 presents an overview of the architecture. Context-sensitive word embeddings. We first encode the words in a sentence by embedding each token using fixed word embeddings. We also concatenate a few linguistic features to the word embeddings, such as named entity tags and dependency relations. These features are generated using CoreNLP (Manning et al., 2014) and represented by randomly-initialized, learned embeddings for symbols together with the fixed word embedding of each token’s dependency head and the dependency path length between adjacent tokens. The token embeddings are then passed through several stacked convolutional layers (Kim, 2014). While the first convolutional layer can only capture local information, subsequent layers allow for longer-distance reasoning. def l ∈ LR = {FACT, EQUIVALENCE, ANALOGY}. For G so defined to encode a set of valid TAP frames, it must satisfy certain constraints: 1. Well-formedness constraints. For any two"
D18-1008,N13-1090,0,0.0224717,"Missing"
D18-1008,D14-1181,0,0.00241209,". Context-sensitive word embeddings. We first encode the words in a sentence by embedding each token using fixed word embeddings. We also concatenate a few linguistic features to the word embeddings, such as named entity tags and dependency relations. These features are generated using CoreNLP (Manning et al., 2014) and represented by randomly-initialized, learned embeddings for symbols together with the fixed word embedding of each token’s dependency head and the dependency path length between adjacent tokens. The token embeddings are then passed through several stacked convolutional layers (Kim, 2014). While the first convolutional layer can only capture local information, subsequent layers allow for longer-distance reasoning. def l ∈ LR = {FACT, EQUIVALENCE, ANALOGY}. For G so defined to encode a set of valid TAP frames, it must satisfy certain constraints: 1. Well-formedness constraints. For any two vertices v, v 0 ∈ V , their associated spans must not overlap. Furthermore, every vertex must participate in at least one FACT edge, i.e., no disconnected vertices. 2. Typing constraints. FACT relations are always drawn from a VALUE vertex to a nonVALUE vertex. ANALOGY and EQUIVA LENCE are on"
D18-1008,P14-1026,0,0.0350441,"ognitive science literature, analogy is a general form of relational reasoning unique to human cognition (Tversky and Gati, 1978; Holyoak and Thagard, 1996; Goldstone and Son, 2005; Penn et al., 2008; Holyoak, 2012). Our model of textual analogy is particularly influenced by Structure Mapping Theory (Falkenhainer et al., 1989; Gentner and Markman, 1997), an influential cognitive model of analogy as a structurepreserving map between concepts. Within the NLP community, there has been Numbers in NLP. There has been some work on understanding numbers in text. This includes quantitative reasoning (Kushman et al., 2014; Roy et al., 2015), numerical information extraction (Madaan et al., 2016), and techniques for making numbers more easily interpretable in text (Chaganty and Liang, 2016; Kim et al., 2016). If pursued further, the application of plotting 89 References quantitative text that we discuss in this paper could help to clarify quantitative text on the web (Larkin and Simon, 1987; Barrio et al., 2016). Collin F Baker, Charles J Fillmore, and John B Lowe. 1998. The Berkeley FrameNet Project. In Proceedings of the 17th International Conference on Computational Linguistics-Volume 1, pages 86–90. Neural"
D18-1008,J05-1004,0,0.011188,"imposition of global constraints reflecting the structure of analogy, the system yields well-formed charts. Without these constraints, generated charts either have multiple yaxis values assigned to the same x-axis value, or have floating y-axis values with no grounding on the x-axis. 8 Discourse and Information Extraction. TAP is an information extraction task that synthesizes ideas from semantic role labeling on the one hand and discourse parsing on the other. The former produces predicate-argument representations of individual facts in a text (Baker et al., 1998; Gildea and Jurafsky, 2002; Palmer et al., 2005); the latter identifies discourse relations between syntactic clauses (Taboada and Mann, 2006; Prasad et al., 2007; Pitler et al., 2009; Prasad et al., 2010; Surdeanu et al., 2015). TAP first maps from syntax to a set of SRL-style representations, and then identifies structurallyconstrained, higher-order relations among them. It is in this sense reminiscent of, but distinct from, work on causal processes by Berant et al. (2014). Related Work Analogy. In the cognitive science literature, analogy is a general form of relational reasoning unique to human cognition (Tversky and Gati, 1978; Holyoak"
D18-1008,D14-1162,1,0.105743,"rning a sentence embedding or hidden layers, the log-linear model simply uses a CRF to predict span labels directly from fixed input features, and then uses a single sigmoid layer to predict edge labels from deterministic edge embeddings, emn . For the neural models, we used three convolutional layers for sentence embedding with a filter size of 3. Every layer other than the input layer used a hidden dimension of 50 with ReLU nonlinearities. We introduced a single dropout layer (p = 0.5) between every two layers in the network (including at the input). We used 50-dimensional GloVe embeddings (Pennington et al., 2014) learned from Wikipedia 2014 and Gigaword 5 as pre-trained word embeddings, and initialized the embeddings for the features randomly. We chose relatively low input- and hidden-vector dimension because of the size of our data. The network was trained for 15 epochs using ADADELTA (Zeiler, 2012) with a learning rate of 1.0. All models were implemented in PyTorch (Paszke et al., 2017). 7 Model Model Log-linear (all feats.) Neural (no feats.) Neural (all feats.) w/o NER w/o dep. w/o CRF P R F1 42.8 41.7 41.5 41.6 41.2 36.1 82.3 79.1 79.2 79.1 77.5 73.1 56.3 54.6 54.4 54.5 53.8 48.3 Table 4: Perform"
D18-1008,N16-1030,0,0.0241819,"Missing"
D18-1008,P09-1077,0,0.0142039,"generated charts either have multiple yaxis values assigned to the same x-axis value, or have floating y-axis values with no grounding on the x-axis. 8 Discourse and Information Extraction. TAP is an information extraction task that synthesizes ideas from semantic role labeling on the one hand and discourse parsing on the other. The former produces predicate-argument representations of individual facts in a text (Baker et al., 1998; Gildea and Jurafsky, 2002; Palmer et al., 2005); the latter identifies discourse relations between syntactic clauses (Taboada and Mann, 2006; Prasad et al., 2007; Pitler et al., 2009; Prasad et al., 2010; Surdeanu et al., 2015). TAP first maps from syntax to a set of SRL-style representations, and then identifies structurallyconstrained, higher-order relations among them. It is in this sense reminiscent of, but distinct from, work on causal processes by Berant et al. (2014). Related Work Analogy. In the cognitive science literature, analogy is a general form of relational reasoning unique to human cognition (Tversky and Gati, 1978; Holyoak and Thagard, 1996; Goldstone and Son, 2005; Penn et al., 2008; Holyoak, 2012). Our model of textual analogy is particularly influenced"
D18-1008,D17-1018,0,0.0241294,"Missing"
D18-1008,prasad-etal-2010-exploiting,0,0.0244782,"er have multiple yaxis values assigned to the same x-axis value, or have floating y-axis values with no grounding on the x-axis. 8 Discourse and Information Extraction. TAP is an information extraction task that synthesizes ideas from semantic role labeling on the one hand and discourse parsing on the other. The former produces predicate-argument representations of individual facts in a text (Baker et al., 1998; Gildea and Jurafsky, 2002; Palmer et al., 2005); the latter identifies discourse relations between syntactic clauses (Taboada and Mann, 2006; Prasad et al., 2007; Pitler et al., 2009; Prasad et al., 2010; Surdeanu et al., 2015). TAP first maps from syntax to a set of SRL-style representations, and then identifies structurallyconstrained, higher-order relations among them. It is in this sense reminiscent of, but distinct from, work on causal processes by Berant et al. (2014). Related Work Analogy. In the cognitive science literature, analogy is a general form of relational reasoning unique to human cognition (Tversky and Gati, 1978; Holyoak and Thagard, 1996; Goldstone and Son, 2005; Penn et al., 2008; Holyoak, 2012). Our model of textual analogy is particularly influenced by Structure Mapping"
D18-1008,W04-2401,0,0.0606951,"e begin by picking the most likely role for each span and edge and then discarding any edges and spans that violate the wellformedness (1) and typing constraints (2). We then enforce transitivity constraints (4) by incrementally building a cluster of analogous and equivalent spans. We then resolve the unique facts constraint (3) by keeping only the span with highest FACT edge score. Finally, for every cluster of analogous VALUE spans, we check that the analogy constraint (5) holds and if not, discard the cluster. We also implement an optimal decoder that encodes the TAP constraints as an ILP (Roth and Yih, 2004; Do et al., 2012). The ILP tries to find an optimal decoding according to the model, subject to hard constraints imposed on the solution space. For example, we require that solutions satisfy the ‘connected spans’ constraint: which defines a joint distribution over per-token role labels. We thus obtain spans from this distribution corresponding to vertices of the graph described in Section 4 by merging contiguous rolelabels in the maximum likelihood label sequence predicted by the CRF. Edge prediction with PATH M AX features. For edge prediction, we use the spans identified above to construct"
D18-1008,P16-1113,0,0.0219536,"Missing"
D18-1008,Q15-1001,0,0.0128036,"ature, analogy is a general form of relational reasoning unique to human cognition (Tversky and Gati, 1978; Holyoak and Thagard, 1996; Goldstone and Son, 2005; Penn et al., 2008; Holyoak, 2012). Our model of textual analogy is particularly influenced by Structure Mapping Theory (Falkenhainer et al., 1989; Gentner and Markman, 1997), an influential cognitive model of analogy as a structurepreserving map between concepts. Within the NLP community, there has been Numbers in NLP. There has been some work on understanding numbers in text. This includes quantitative reasoning (Kushman et al., 2014; Roy et al., 2015), numerical information extraction (Madaan et al., 2016), and techniques for making numbers more easily interpretable in text (Chaganty and Liang, 2016; Kim et al., 2016). If pursued further, the application of plotting 89 References quantitative text that we discuss in this paper could help to clarify quantitative text on the web (Larkin and Simon, 1987; Barrio et al., 2016). Collin F Baker, Charles J Fillmore, and John B Lowe. 1998. The Berkeley FrameNet Project. In Proceedings of the 17th International Conference on Computational Linguistics-Volume 1, pages 86–90. Neural modeling. Recent wo"
D18-1008,N15-3001,0,0.0187186,"s values assigned to the same x-axis value, or have floating y-axis values with no grounding on the x-axis. 8 Discourse and Information Extraction. TAP is an information extraction task that synthesizes ideas from semantic role labeling on the one hand and discourse parsing on the other. The former produces predicate-argument representations of individual facts in a text (Baker et al., 1998; Gildea and Jurafsky, 2002; Palmer et al., 2005); the latter identifies discourse relations between syntactic clauses (Taboada and Mann, 2006; Prasad et al., 2007; Pitler et al., 2009; Prasad et al., 2010; Surdeanu et al., 2015). TAP first maps from syntax to a set of SRL-style representations, and then identifies structurallyconstrained, higher-order relations among them. It is in this sense reminiscent of, but distinct from, work on causal processes by Berant et al. (2014). Related Work Analogy. In the cognitive science literature, analogy is a general form of relational reasoning unique to human cognition (Tversky and Gati, 1978; Holyoak and Thagard, 1996; Goldstone and Son, 2005; Penn et al., 2008; Holyoak, 2012). Our model of textual analogy is particularly influenced by Structure Mapping Theory (Falkenhainer et"
D18-1008,Q13-1029,0,0.052752,"Missing"
D18-1008,P15-1109,0,0.0332622,"Missing"
D18-1008,P98-1013,0,\N,Missing
D18-1008,C98-1013,0,\N,Missing
D18-1241,P18-1078,0,0.0356121,"r all other dialog acts (neither for affirmation and don’t follow up for continuation). Transition matrix We divide the supporting text into 12 chunks (with a special chunk for no answer) and use the transition matrix (computed from the training set) in Figure 5b to select an answer given the position of the previous answer. This baseline does not output other dialog acts. 5.2 Upper bounds Gold NA + TM This is the same transition matrix (TM) baseline as before, except that for questions whose gold annotations are no answer, we always output no answer. et al., 2016, BiDAF) with self-attention (Clark and Gardner, 2018) and contextualized embeddings.16 A token for no answer is appended to s to enable its prediction following Levy et al. (2017). Additionally, we modify the model for our task to also predict dialog acts, placing a classifier over the same representation used to predict the end position of the predicted span. BiDAF++ w/ k-ctx As BiDAF++ does not model any dialog context, we modify the passage and question embedding processes to consider the dialog history. We consider context from the previous k QA pairs.17 • Passage embedding We explicitly identify the previous k answers within the section tex"
D18-1241,D17-1070,0,0.0153231,"ng Naively prepending the previous k questions to the current question did not show gains in initial experiments. We opt instead to simply encode the dialog turn number within the question embedding. 5.4 Results Table 4 summarizes our results (each cell displays dev/test scores), where dialog acts are Yes/No (affirmation) and Follow up (continuation). For comparison to other datasets, we report F1 without filtering low-agreement QA pairs (F1’). Pretrained InferSent To test the importance of lexical matching in our dataset, we output the sentence in s whose pretrained InferSent representation (Conneau et al., 2017) has the highest cosine similarity to that of the question. Sanity check Overall, the poor sanity check results imply that is very challenging. Of these, following the transition matrix (TM) gives the best performance, reinforcing the observation that the dialog context plays a significant role in the task. Feature-rich logistic regression We train a logistic regression using Vowpal Wabbit (Langford et al., 2007) to select answer sentences. We use simple matching features (e.g., n-gram overlap between questions and candidate answers), bias features (position and length of a candidate), and con"
D18-1241,H94-1010,0,0.675779,"Missing"
D18-1241,K17-1034,1,0.798034,"ext into 12 chunks (with a special chunk for no answer) and use the transition matrix (computed from the training set) in Figure 5b to select an answer given the position of the previous answer. This baseline does not output other dialog acts. 5.2 Upper bounds Gold NA + TM This is the same transition matrix (TM) baseline as before, except that for questions whose gold annotations are no answer, we always output no answer. et al., 2016, BiDAF) with self-attention (Clark and Gardner, 2018) and contextualized embeddings.16 A token for no answer is appended to s to enable its prediction following Levy et al. (2017). Additionally, we modify the model for our task to also predict dialog acts, placing a classifier over the same representation used to predict the end position of the predicted span. BiDAF++ w/ k-ctx As BiDAF++ does not model any dialog context, we modify the passage and question embedding processes to consider the dialog history. We consider context from the previous k QA pairs.17 • Passage embedding We explicitly identify the previous k answers within the section text by concatenating marker embeddings to the existing word embeddings. Gold sentence + NA To see if can be treated as an answer"
D18-1241,D17-1259,0,0.0131978,"of CoQA answers are named entities or short noun phrases, much like those in SQuAD. In contrast, the asymmetric nature of forces students to ask more exploratory questions whose answers can be potentially be followed up on.19 Dialog fits into an increasing interest in open domain dialog, mostly studied in the context of social chit-chat (Li et al., 2016; Ritter et al., 2011; Fang et al., 2017; Ghazvininejad et al., 2018). Most related to our effort is visual dialog (Das et al., 2017), which relies on images as evidence instead of text. More explicit goal driven scenarios, such as bargaining (Lewis et al., 2017) and item guessing (He et al., 2017) have also been explored, but the language is more constrained than in . Information-seeking dialog specifically was studied in Stede and Schlangen (2004). 7 Conclusion In this paper, we introduce , a large scale dataset of information-seeking dialogs over sections from Wikipedia articles. Our data collection process, which takes the form of a teacher-student interaction between two crowd workers, encourages questions that are highly contextual, openended, and even unanswerable from the text. Our baselines, which include top performers on existing machine co"
D18-1241,D16-1127,0,0.0402491,"xt (such as traffic laws) by interacting with a user through dialog. Also concurrently, Reddy et al. (2018) propose conversational question answering (CoQA) from text but allow both students and questioners to see the evidence. As a result, a large percentage of CoQA answers are named entities or short noun phrases, much like those in SQuAD. In contrast, the asymmetric nature of forces students to ask more exploratory questions whose answers can be potentially be followed up on.19 Dialog fits into an increasing interest in open domain dialog, mostly studied in the context of social chit-chat (Li et al., 2016; Ritter et al., 2011; Fang et al., 2017; Ghazvininejad et al., 2018). Most related to our effort is visual dialog (Das et al., 2017), which relies on images as evidence instead of text. More explicit goal driven scenarios, such as bargaining (Lewis et al., 2017) and item guessing (He et al., 2017) have also been explored, but the language is more constrained than in . Information-seeking dialog specifically was studied in Stede and Schlangen (2004). 7 Conclusion In this paper, we introduce , a large scale dataset of information-seeking dialogs over sections from Wikipedia articles. Our data c"
D18-1241,P18-2124,1,0.903645,"the evidence text, as in existing reading comprehension tasks (Rajpurkar et al., 2016). Additionally, the teacher uses dialog acts to provide the student with feedback (e.g., “ask a follow up ques2174 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2174–2184 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics Dataset QuAC CoQA (Reddy et al., 2018) CSQA (Saha et al., 2018) CQA (Talmor and Berant, 2018) SQA (Iyyer et al., 2017) NarrativeQA (Kocisk´y et al., 2017) TriviaQA (Joshi et al., 2017) SQuAD 2.0 (Rajpurkar et al., 2018) MS Marco (Nguyen et al., 2016) NewsQA (Trischler et al., 2016) Multi turn Textbased Dialog Acts Simple Evaluation Unanswerable Questions Asker Can’t See Evidence 4 4 4 4 4 4 4 4 4 4 7 7 7 7 7 4 7 4 7 4 4 4 4 4 7 7 7 7 7 7 7 7 7 4 7 4 4 7 4 4 7 4 4 4 7 7 7 7 4 4 4 7 7 4 7 4 4 7 4 4 Table 1: Comparison of the QUAC dataset to other question answering datasets. tion”), which makes the dialogs more productive. We collect the dataset in an interactive setting where two crowd workers play the roles of teacher and student. To encourage natural and diverse questions, we do not follow previous dialogst"
D18-1241,P17-1162,1,0.821322,"short noun phrases, much like those in SQuAD. In contrast, the asymmetric nature of forces students to ask more exploratory questions whose answers can be potentially be followed up on.19 Dialog fits into an increasing interest in open domain dialog, mostly studied in the context of social chit-chat (Li et al., 2016; Ritter et al., 2011; Fang et al., 2017; Ghazvininejad et al., 2018). Most related to our effort is visual dialog (Das et al., 2017), which relies on images as evidence instead of text. More explicit goal driven scenarios, such as bargaining (Lewis et al., 2017) and item guessing (He et al., 2017) have also been explored, but the language is more constrained than in . Information-seeking dialog specifically was studied in Stede and Schlangen (2004). 7 Conclusion In this paper, we introduce , a large scale dataset of information-seeking dialogs over sections from Wikipedia articles. Our data collection process, which takes the form of a teacher-student interaction between two crowd workers, encourages questions that are highly contextual, openended, and even unanswerable from the text. Our baselines, which include top performers on existing machine comprehension datasets, significantly"
D18-1241,D16-1264,1,0.889651,", who does not see the section text, asks questions. The teacher provides a response in the form of a text span (or No answer ), optionally yes or no ( Yes / No ), and encouragement about continuing a ¯ , or should line of questioning (should, ,→ , could ,→ not 6,→ ask a follow-up question). Wikipedia page), which only the teacher can access. Given just the section’s heading, “Origin & History”, the student aims to learn as much as possible about its contents by asking questions. The teacher answers these questions with spans from the evidence text, as in existing reading comprehension tasks (Rajpurkar et al., 2016). Additionally, the teacher uses dialog acts to provide the student with feedback (e.g., “ask a follow up ques2174 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2174–2184 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics Dataset QuAC CoQA (Reddy et al., 2018) CSQA (Saha et al., 2018) CQA (Talmor and Berant, 2018) SQA (Iyyer et al., 2017) NarrativeQA (Kocisk´y et al., 2017) TriviaQA (Joshi et al., 2017) SQuAD 2.0 (Rajpurkar et al., 2018) MS Marco (Nguyen et al., 2016) NewsQA (Trischler et al., 2016)"
D18-1241,P17-1167,1,0.880503,"ible about its contents by asking questions. The teacher answers these questions with spans from the evidence text, as in existing reading comprehension tasks (Rajpurkar et al., 2016). Additionally, the teacher uses dialog acts to provide the student with feedback (e.g., “ask a follow up ques2174 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2174–2184 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics Dataset QuAC CoQA (Reddy et al., 2018) CSQA (Saha et al., 2018) CQA (Talmor and Berant, 2018) SQA (Iyyer et al., 2017) NarrativeQA (Kocisk´y et al., 2017) TriviaQA (Joshi et al., 2017) SQuAD 2.0 (Rajpurkar et al., 2018) MS Marco (Nguyen et al., 2016) NewsQA (Trischler et al., 2016) Multi turn Textbased Dialog Acts Simple Evaluation Unanswerable Questions Asker Can’t See Evidence 4 4 4 4 4 4 4 4 4 4 7 7 7 7 7 4 7 4 7 4 4 4 4 4 7 7 7 7 7 7 7 7 7 4 7 4 4 7 4 4 7 4 4 4 7 7 7 7 4 4 4 7 7 4 7 4 4 7 4 4 Table 1: Comparison of the QUAC dataset to other question answering datasets. tion”), which makes the dialogs more productive. We collect the dataset in an interactive setting where two crowd workers play the roles o"
D18-1241,P17-1147,1,0.902703,"hese questions with spans from the evidence text, as in existing reading comprehension tasks (Rajpurkar et al., 2016). Additionally, the teacher uses dialog acts to provide the student with feedback (e.g., “ask a follow up ques2174 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2174–2184 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics Dataset QuAC CoQA (Reddy et al., 2018) CSQA (Saha et al., 2018) CQA (Talmor and Berant, 2018) SQA (Iyyer et al., 2017) NarrativeQA (Kocisk´y et al., 2017) TriviaQA (Joshi et al., 2017) SQuAD 2.0 (Rajpurkar et al., 2018) MS Marco (Nguyen et al., 2016) NewsQA (Trischler et al., 2016) Multi turn Textbased Dialog Acts Simple Evaluation Unanswerable Questions Asker Can’t See Evidence 4 4 4 4 4 4 4 4 4 4 7 7 7 7 7 4 7 4 7 4 4 4 4 4 7 7 7 7 7 7 7 7 7 4 7 4 4 7 4 4 7 4 4 4 7 7 7 7 4 4 4 7 7 4 7 4 4 7 4 4 Table 1: Comparison of the QUAC dataset to other question answering datasets. tion”), which makes the dialogs more productive. We collect the dataset in an interactive setting where two crowd workers play the roles of teacher and student. To encourage natural and diverse questions,"
D18-1241,D11-1054,0,0.0542229,"ic laws) by interacting with a user through dialog. Also concurrently, Reddy et al. (2018) propose conversational question answering (CoQA) from text but allow both students and questioners to see the evidence. As a result, a large percentage of CoQA answers are named entities or short noun phrases, much like those in SQuAD. In contrast, the asymmetric nature of forces students to ask more exploratory questions whose answers can be potentially be followed up on.19 Dialog fits into an increasing interest in open domain dialog, mostly studied in the context of social chit-chat (Li et al., 2016; Ritter et al., 2011; Fang et al., 2017; Ghazvininejad et al., 2018). Most related to our effort is visual dialog (Das et al., 2017), which relies on images as evidence instead of text. More explicit goal driven scenarios, such as bargaining (Lewis et al., 2017) and item guessing (He et al., 2017) have also been explored, but the language is more constrained than in . Information-seeking dialog specifically was studied in Stede and Schlangen (2004). 7 Conclusion In this paper, we introduce , a large scale dataset of information-seeking dialogs over sections from Wikipedia articles. Our data collection process, wh"
D18-1241,D18-1233,0,0.203396,"are the first to incorporate these into information-seeking dialog. Sequential QA Our work is similar to sequential question answering against knowledge bases (Iyyer et al., 2017) and the web (Talmor and Berant, 2018), but instead of decomposing a single question into smaller questions, we rely on the curiosity of the student to generate a sequence of questions. Such open information seeking was studied in semantic parsing on knowledge bases (Dahl et al., 1994) and more recently with modern approaches (Saha et al., 2018), but with questions paraphrased from templates. Concurrent to our work, Saeidi et al. (2018) proposed a task of generating and answering yes/no questions for rule focused text (such as traffic laws) by interacting with a user through dialog. Also concurrently, Reddy et al. (2018) propose conversational question answering (CoQA) from text but allow both students and questioners to see the evidence. As a result, a large percentage of CoQA answers are named entities or short noun phrases, much like those in SQuAD. In contrast, the asymmetric nature of forces students to ask more exploratory questions whose answers can be potentially be followed up on.19 Dialog fits into an increasing in"
D18-1241,N18-1059,0,0.1584,"aims to learn as much as possible about its contents by asking questions. The teacher answers these questions with spans from the evidence text, as in existing reading comprehension tasks (Rajpurkar et al., 2016). Additionally, the teacher uses dialog acts to provide the student with feedback (e.g., “ask a follow up ques2174 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2174–2184 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics Dataset QuAC CoQA (Reddy et al., 2018) CSQA (Saha et al., 2018) CQA (Talmor and Berant, 2018) SQA (Iyyer et al., 2017) NarrativeQA (Kocisk´y et al., 2017) TriviaQA (Joshi et al., 2017) SQuAD 2.0 (Rajpurkar et al., 2018) MS Marco (Nguyen et al., 2016) NewsQA (Trischler et al., 2016) Multi turn Textbased Dialog Acts Simple Evaluation Unanswerable Questions Asker Can’t See Evidence 4 4 4 4 4 4 4 4 4 4 7 7 7 7 7 4 7 4 7 4 4 4 4 4 7 7 7 7 7 7 7 7 7 4 7 4 4 7 4 4 7 4 4 4 7 7 7 7 4 4 4 7 7 4 7 4 4 7 4 4 Table 1: Comparison of the QUAC dataset to other question answering datasets. tion”), which makes the dialogs more productive. We collect the dataset in an interactive setting where two crowd"
D18-1256,L16-1432,0,0.203786,"t zt conditioned on past dialogue acts z&lt;t . (3) The generator then produces a response conditioned on both the predicted coarse dialogue act zt and the dialogue history x&lt;t . Importantly, unlike in traditional systems, coarse dialogue acts only capture the rough shape of a dialogue, not the full meaning of its utterances, e.g., inform does not specify the answer to the question. specific knowledge. Existing human-human negotiation datasets are grounded in closed-domain games with a fixed set of objects such as Settlers of Catan (lumber, coal, brick, wheat, and sheep) (Afantenos et al., 2012; Asher et al., 2016) or item division (book, hat, and ball) (DeVault et al., 2015; Lewis et al., 2017). These objects lack the richness of the real world. To study human negotiation in more open-ended settings that involve real goods, we scraped postings of items for sale from craigslist.org as our negotiation scenario. By hiring workers on Amazon Mechanical Turk (AMT) to play the role of buyers and sellers, we collected a new dataset (C RAIGSLIST BARGAIN) of negotiation dialogues.1 Compared to existing datasets, our more realistic scenario invites richer negotiation behavior involving open-ended aspects such as"
D18-1256,E17-2029,0,0.0373631,"ring ways to replace these modules with neural networks and end-to-end training while still having a logical backbone (Wen et al., 2017a; Bordes and Weston, 2017; He et al., 2017). Our work is closely related to the Hybrid Code Network (Williams et al., 2017), but the key difference is that Williams et al. (2017) uses a neural dialogue state, whereas we keep a structured, interpretable dialogue state which allows for stronger top-down control. Another line of work tackles this problem by introducing latent stochastic variables to model the dialogue state (Wen et al., 2017b; Zhao et al., 2017; Cao and Clark, 2017). While the latent discrete variable allows for post-hoc discovery of dialogue acts and increased utterance diversity, it does not provide controllability over the dialogue strategy. Our work is also related to a large body of literature on dialogue policies in negotiation (English and Heeman, 2005; Efstathiou and Lemon, 2014; Hiraoka et al., 2015; Cao et al., 2018). These work mostly focus on learning good negotiation policies in a domain-specific action space, whereas our model operates in an open-ended space of natural language. An interesting future direction is to connect with game theory"
D18-1256,P17-1045,0,0.0897131,"car so I can go to work, but all I have is 6000, any more and I won’t be able to feed my children.”). Most past work in NLP on negotiation focuses on strategy (dialogue management) with either no natural language (Cuay´ahuitl et al., 2015; Cao et al., 2018) or canned responses (Keizer et al., 2017; Traum et al., 2008). Recently, end-to-end neural models (Lewis et al., 2017; He et al., 2017) are used to simultaneously learn dialogue strategy and language realization from human-human dialogues, following the trend of using neural network models on both goal-oriented dialogue (Wen et al., 2017a; Dhingra et al., 2017) and opendomain dialogue (Sordoni et al., 2015; Li et al., 2017; Lowe et al., 2017). However, these models have two problems: (i) it is hard to control and interpret the strategies, and (ii) directly optimizing the agent’s goal through reinforcement learning often leads to degenerate solutions where the utterances become ungrammatical (Lewis et al., 2017) or repetitive (Li et al., 2016). To alleviate these problems, our key idea is to decouple strategy and generation, which gives us control over the strategy such that we can achieve different negotiation goals (e.g., maximizing utility, achiev"
D18-1256,W14-4308,0,0.0249999,"neural dialogue state, whereas we keep a structured, interpretable dialogue state which allows for stronger top-down control. Another line of work tackles this problem by introducing latent stochastic variables to model the dialogue state (Wen et al., 2017b; Zhao et al., 2017; Cao and Clark, 2017). While the latent discrete variable allows for post-hoc discovery of dialogue acts and increased utterance diversity, it does not provide controllability over the dialogue strategy. Our work is also related to a large body of literature on dialogue policies in negotiation (English and Heeman, 2005; Efstathiou and Lemon, 2014; Hiraoka et al., 2015; Cao et al., 2018). These work mostly focus on learning good negotiation policies in a domain-specific action space, whereas our model operates in an open-ended space of natural language. An interesting future direction is to connect with game theory (Brams, 2003) for complex multi-issue bargaining. Another direction is learning to generate persuasive utterances, e.g., through framing (Takuya et al., 2014) or accounting for the social and cultural context (Elnaz et al., 2012). To conclude, we have introduced C RAIGSLISTBARGAIN, a rich dataset of human-human negotiation d"
D18-1256,H05-1127,0,0.052462,"liams et al. (2017) uses a neural dialogue state, whereas we keep a structured, interpretable dialogue state which allows for stronger top-down control. Another line of work tackles this problem by introducing latent stochastic variables to model the dialogue state (Wen et al., 2017b; Zhao et al., 2017; Cao and Clark, 2017). While the latent discrete variable allows for post-hoc discovery of dialogue acts and increased utterance diversity, it does not provide controllability over the dialogue strategy. Our work is also related to a large body of literature on dialogue policies in negotiation (English and Heeman, 2005; Efstathiou and Lemon, 2014; Hiraoka et al., 2015; Cao et al., 2018). These work mostly focus on learning good negotiation policies in a domain-specific action space, whereas our model operates in an open-ended space of natural language. An interesting future direction is to connect with game theory (Brams, 2003) for complex multi-issue bargaining. Another direction is learning to generate persuasive utterances, e.g., through framing (Takuya et al., 2014) or accounting for the social and cultural context (Elnaz et al., 2012). To conclude, we have introduced C RAIGSLISTBARGAIN, a rich dataset"
D18-1256,P17-1162,1,0.912223,"Missing"
D18-1256,W15-4605,0,0.0179976,"eas we keep a structured, interpretable dialogue state which allows for stronger top-down control. Another line of work tackles this problem by introducing latent stochastic variables to model the dialogue state (Wen et al., 2017b; Zhao et al., 2017; Cao and Clark, 2017). While the latent discrete variable allows for post-hoc discovery of dialogue acts and increased utterance diversity, it does not provide controllability over the dialogue strategy. Our work is also related to a large body of literature on dialogue policies in negotiation (English and Heeman, 2005; Efstathiou and Lemon, 2014; Hiraoka et al., 2015; Cao et al., 2018). These work mostly focus on learning good negotiation policies in a domain-specific action space, whereas our model operates in an open-ended space of natural language. An interesting future direction is to connect with game theory (Brams, 2003) for complex multi-issue bargaining. Another direction is learning to generate persuasive utterances, e.g., through framing (Takuya et al., 2014) or accounting for the social and cultural context (Elnaz et al., 2012). To conclude, we have introduced C RAIGSLISTBARGAIN, a rich dataset of human-human negotiation dialogues. We have also"
D18-1256,E17-2077,0,0.129821,"ystems achieve higher task success rate and more human-like negotiation behavior than previous approaches. 1 Introduction A good negotiator needs to decide on the strategy for achieving a certain goal (e.g., proposing $6000) and the realization of that strategy via generation of natural language (e.g., “I really need a car so I can go to work, but all I have is 6000, any more and I won’t be able to feed my children.”). Most past work in NLP on negotiation focuses on strategy (dialogue management) with either no natural language (Cuay´ahuitl et al., 2015; Cao et al., 2018) or canned responses (Keizer et al., 2017; Traum et al., 2008). Recently, end-to-end neural models (Lewis et al., 2017; He et al., 2017) are used to simultaneously learn dialogue strategy and language realization from human-human dialogues, following the trend of using neural network models on both goal-oriented dialogue (Wen et al., 2017a; Dhingra et al., 2017) and opendomain dialogue (Sordoni et al., 2015; Li et al., 2017; Lowe et al., 2017). However, these models have two problems: (i) it is hard to control and interpret the strategies, and (ii) directly optimizing the agent’s goal through reinforcement learning often leads to deg"
D18-1256,D17-1259,0,0.203926,"or than previous approaches. 1 Introduction A good negotiator needs to decide on the strategy for achieving a certain goal (e.g., proposing $6000) and the realization of that strategy via generation of natural language (e.g., “I really need a car so I can go to work, but all I have is 6000, any more and I won’t be able to feed my children.”). Most past work in NLP on negotiation focuses on strategy (dialogue management) with either no natural language (Cuay´ahuitl et al., 2015; Cao et al., 2018) or canned responses (Keizer et al., 2017; Traum et al., 2008). Recently, end-to-end neural models (Lewis et al., 2017; He et al., 2017) are used to simultaneously learn dialogue strategy and language realization from human-human dialogues, following the trend of using neural network models on both goal-oriented dialogue (Wen et al., 2017a; Dhingra et al., 2017) and opendomain dialogue (Sordoni et al., 2015; Li et al., 2017; Lowe et al., 2017). However, these models have two problems: (i) it is hard to control and interpret the strategies, and (ii) directly optimizing the agent’s goal through reinforcement learning often leads to degenerate solutions where the utterances become ungrammatical (Lewis et al., 20"
D18-1256,D16-1127,0,0.508493,"are used to simultaneously learn dialogue strategy and language realization from human-human dialogues, following the trend of using neural network models on both goal-oriented dialogue (Wen et al., 2017a; Dhingra et al., 2017) and opendomain dialogue (Sordoni et al., 2015; Li et al., 2017; Lowe et al., 2017). However, these models have two problems: (i) it is hard to control and interpret the strategies, and (ii) directly optimizing the agent’s goal through reinforcement learning often leads to degenerate solutions where the utterances become ungrammatical (Lewis et al., 2017) or repetitive (Li et al., 2016). To alleviate these problems, our key idea is to decouple strategy and generation, which gives us control over the strategy such that we can achieve different negotiation goals (e.g., maximizing utility, achieving a fair deal) with the same language generator. Our framework consists of three components shown in Figure 1: First, the parser identifies keywords and entities to map each utterance to a coarse dialogue act capturing the highlevel strategic move. Then, the dialogue manager chooses a responding dialogue act based on a sequence-to-sequence model over coarse dialogue acts learned from"
D18-1256,D17-1230,0,0.0436776,"be able to feed my children.”). Most past work in NLP on negotiation focuses on strategy (dialogue management) with either no natural language (Cuay´ahuitl et al., 2015; Cao et al., 2018) or canned responses (Keizer et al., 2017; Traum et al., 2008). Recently, end-to-end neural models (Lewis et al., 2017; He et al., 2017) are used to simultaneously learn dialogue strategy and language realization from human-human dialogues, following the trend of using neural network models on both goal-oriented dialogue (Wen et al., 2017a; Dhingra et al., 2017) and opendomain dialogue (Sordoni et al., 2015; Li et al., 2017; Lowe et al., 2017). However, these models have two problems: (i) it is hard to control and interpret the strategies, and (ii) directly optimizing the agent’s goal through reinforcement learning often leads to degenerate solutions where the utterances become ungrammatical (Lewis et al., 2017) or repetitive (Li et al., 2016). To alleviate these problems, our key idea is to decouple strategy and generation, which gives us control over the strategy such that we can achieve different negotiation goals (e.g., maximizing utility, achieving a fair deal) with the same language generator. Our framewor"
D18-1256,D14-1162,0,0.0810364,"Missing"
D18-1256,N15-1020,0,0.0337923,", any more and I won’t be able to feed my children.”). Most past work in NLP on negotiation focuses on strategy (dialogue management) with either no natural language (Cuay´ahuitl et al., 2015; Cao et al., 2018) or canned responses (Keizer et al., 2017; Traum et al., 2008). Recently, end-to-end neural models (Lewis et al., 2017; He et al., 2017) are used to simultaneously learn dialogue strategy and language realization from human-human dialogues, following the trend of using neural network models on both goal-oriented dialogue (Wen et al., 2017a; Dhingra et al., 2017) and opendomain dialogue (Sordoni et al., 2015; Li et al., 2017; Lowe et al., 2017). However, these models have two problems: (i) it is hard to control and interpret the strategies, and (ii) directly optimizing the agent’s goal through reinforcement learning often leads to degenerate solutions where the utterances become ungrammatical (Lewis et al., 2017) or repetitive (Li et al., 2016). To alleviate these problems, our key idea is to decouple strategy and generation, which gives us control over the strategy such that we can achieve different negotiation goals (e.g., maximizing utility, achieving a fair deal) with the same language genera"
D18-1256,C14-1161,0,0.0286234,"controllability over the dialogue strategy. Our work is also related to a large body of literature on dialogue policies in negotiation (English and Heeman, 2005; Efstathiou and Lemon, 2014; Hiraoka et al., 2015; Cao et al., 2018). These work mostly focus on learning good negotiation policies in a domain-specific action space, whereas our model operates in an open-ended space of natural language. An interesting future direction is to connect with game theory (Brams, 2003) for complex multi-issue bargaining. Another direction is learning to generate persuasive utterances, e.g., through framing (Takuya et al., 2014) or accounting for the social and cultural context (Elnaz et al., 2012). To conclude, we have introduced C RAIGSLISTBARGAIN, a rich dataset of human-human negotiation dialogues. We have also presented a modular approach based on coarse dialogue acts that models a rough strategic backbone as well allowing for open-ended generation. We hope this work will spur more research in hybrid approaches that can work in open-ended, goal-oriented settings. Acknowledgments. This work is supported by DARPA Communicating with Computers (CwC) program under ARO prime contract no. W911NF15-1-0462. We thank memb"
D18-1256,E17-1042,0,0.165377,", “I really need a car so I can go to work, but all I have is 6000, any more and I won’t be able to feed my children.”). Most past work in NLP on negotiation focuses on strategy (dialogue management) with either no natural language (Cuay´ahuitl et al., 2015; Cao et al., 2018) or canned responses (Keizer et al., 2017; Traum et al., 2008). Recently, end-to-end neural models (Lewis et al., 2017; He et al., 2017) are used to simultaneously learn dialogue strategy and language realization from human-human dialogues, following the trend of using neural network models on both goal-oriented dialogue (Wen et al., 2017a; Dhingra et al., 2017) and opendomain dialogue (Sordoni et al., 2015; Li et al., 2017; Lowe et al., 2017). However, these models have two problems: (i) it is hard to control and interpret the strategies, and (ii) directly optimizing the agent’s goal through reinforcement learning often leads to degenerate solutions where the utterances become ungrammatical (Lewis et al., 2017) or repetitive (Li et al., 2016). To alleviate these problems, our key idea is to decouple strategy and generation, which gives us control over the strategy such that we can achieve different negotiation goals (e.g., ma"
D18-1256,P17-1062,0,0.0338578,"icher and more diverse language than prior datasets. Our dataset calls for systems that can handle both strategic decision-making and open-ended text generation. Traditional goal-oriented dialogue systems build a pipeline of modules (Young et al., 2013; Williams et al., 2016). Due to the laborious dialogue state design and annotation, recent work has been exploring ways to replace these modules with neural networks and end-to-end training while still having a logical backbone (Wen et al., 2017a; Bordes and Weston, 2017; He et al., 2017). Our work is closely related to the Hybrid Code Network (Williams et al., 2017), but the key difference is that Williams et al. (2017) uses a neural dialogue state, whereas we keep a structured, interpretable dialogue state which allows for stronger top-down control. Another line of work tackles this problem by introducing latent stochastic variables to model the dialogue state (Wen et al., 2017b; Zhao et al., 2017; Cao and Clark, 2017). While the latent discrete variable allows for post-hoc discovery of dialogue acts and increased utterance diversity, it does not provide controllability over the dialogue strategy. Our work is also related to a large body of literature o"
D18-1256,P17-1061,0,0.0227551,"work has been exploring ways to replace these modules with neural networks and end-to-end training while still having a logical backbone (Wen et al., 2017a; Bordes and Weston, 2017; He et al., 2017). Our work is closely related to the Hybrid Code Network (Williams et al., 2017), but the key difference is that Williams et al. (2017) uses a neural dialogue state, whereas we keep a structured, interpretable dialogue state which allows for stronger top-down control. Another line of work tackles this problem by introducing latent stochastic variables to model the dialogue state (Wen et al., 2017b; Zhao et al., 2017; Cao and Clark, 2017). While the latent discrete variable allows for post-hoc discovery of dialogue acts and increased utterance diversity, it does not provide controllability over the dialogue strategy. Our work is also related to a large body of literature on dialogue policies in negotiation (English and Heeman, 2005; Efstathiou and Lemon, 2014; Hiraoka et al., 2015; Cao et al., 2018). These work mostly focus on learning good negotiation policies in a domain-specific action space, whereas our model operates in an open-ended space of natural language. An interesting future direction is to co"
D18-1540,D15-1138,0,0.110416,"s, and form inputs), as illustrated in Figure 1. While some commands refer to an element’s text directly, many others require more complex reasoning with the various aspects of web pages: the text, attributes, styles, structural data from the document object model (DOM), and spatial data from the rendered web page. Our task is inspired by the semantic parsing literature, which aims to map natural language utterances into actions such as database queries and object manipulation (Zelle and Mooney, 1996; Chen and Mooney, 2011; Artzi and Zettlemoyer, 2013; Berant et al., 2013; Misra et al., 2015; Andreas and Klein, 2015). While these actions usually act on an environment with a fixed and known schema, web pages contain a larger variety of structures, making the task more open-ended. At the same time, our task can be viewed as a reference game (Golland et al., 2010; Smith et al., 2013; Andreas and Klein, 2016), where the system has to select an object given a natural language reference. The diversity of attributes in web page elements, along with the need to use context to interpret elements, makes web pages particularly interesting. Identifying elements via natural language has several real-world applications"
D18-1540,D16-1125,0,0.119786,"the rendered web page. Our task is inspired by the semantic parsing literature, which aims to map natural language utterances into actions such as database queries and object manipulation (Zelle and Mooney, 1996; Chen and Mooney, 2011; Artzi and Zettlemoyer, 2013; Berant et al., 2013; Misra et al., 2015; Andreas and Klein, 2015). While these actions usually act on an environment with a fixed and known schema, web pages contain a larger variety of structures, making the task more open-ended. At the same time, our task can be viewed as a reference game (Golland et al., 2010; Smith et al., 2013; Andreas and Klein, 2016), where the system has to select an object given a natural language reference. The diversity of attributes in web page elements, along with the need to use context to interpret elements, makes web pages particularly interesting. Identifying elements via natural language has several real-world applications. The main one is providing a voice interface for interacting with web pages, which is especially useful as an assistive technology for the visually impaired (Zajicek et al., 1998; Ashok et al., 2014). Another use case is browser automation: natural language commands are less brittle than CSS"
D18-1540,Q13-1005,0,0.333756,"ng natural language commands to web page elements (e.g., links, buttons, and form inputs), as illustrated in Figure 1. While some commands refer to an element’s text directly, many others require more complex reasoning with the various aspects of web pages: the text, attributes, styles, structural data from the document object model (DOM), and spatial data from the rendered web page. Our task is inspired by the semantic parsing literature, which aims to map natural language utterances into actions such as database queries and object manipulation (Zelle and Mooney, 1996; Chen and Mooney, 2011; Artzi and Zettlemoyer, 2013; Berant et al., 2013; Misra et al., 2015; Andreas and Klein, 2015). While these actions usually act on an environment with a fixed and known schema, web pages contain a larger variety of structures, making the task more open-ended. At the same time, our task can be viewed as a reference game (Golland et al., 2010; Smith et al., 2013; Andreas and Klein, 2016), where the system has to select an object given a natural language reference. The diversity of attributes in web page elements, along with the need to use context to interpret elements, makes web pages particularly interesting. Identifyin"
D18-1540,D13-1160,1,0.949104,"to web page elements (e.g., links, buttons, and form inputs), as illustrated in Figure 1. While some commands refer to an element’s text directly, many others require more complex reasoning with the various aspects of web pages: the text, attributes, styles, structural data from the document object model (DOM), and spatial data from the rendered web page. Our task is inspired by the semantic parsing literature, which aims to map natural language utterances into actions such as database queries and object manipulation (Zelle and Mooney, 1996; Chen and Mooney, 2011; Artzi and Zettlemoyer, 2013; Berant et al., 2013; Misra et al., 2015; Andreas and Klein, 2015). While these actions usually act on an environment with a fixed and known schema, web pages contain a larger variety of structures, making the task more open-ended. At the same time, our task can be viewed as a reference game (Golland et al., 2010; Smith et al., 2013; Andreas and Klein, 2016), where the system has to select an object given a natural language reference. The diversity of attributes in web page elements, along with the need to use context to interpret elements, makes web pages particularly interesting. Identifying elements via natura"
D18-1540,P09-1010,0,0.287073,"017), database queries (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2007; Berant et al., 2013; Yih et al., 2015), navigation (Artzi and Zettlemoyer, 2013; Janner et al., 2018), and object manipulation (Tellex et al., 2011; Andreas and Klein, 2015; Guu et al., 2017; Fried et al., 2018). For web pages and graphical user interfaces, there are previous works on using natural language to perform computations on web tables (Pasupat and Liang, 2015; Zhong et al., 2017) and submit web forms (Shi et al., 2017). Our task is similar to previous works on interpreting instructions on user interfaces (Branavan et al., 2009, 2010; Liu et al., 2018). While their works focuses on learning from distant supervision, we consider shallower interactions but on a much broader domain. Previous work also explores the reverse problem of generating natural language description of objects (Vinyals et al., 2014; Karpathy and FeiFei, 2015; Zarriaiß and Schlangen, 2017). We hope that our dataset could also be useful for exploring the reverse task of describing actions on web pages. Reference games. In a reference game, the system has to select the correct object referenced by the given utterance (Frank and Goodman, 2012). Previ"
D18-1540,P10-1129,0,0.0815094,"Missing"
D18-1540,D14-1223,0,0.0253974,"e reverse problem of generating natural language description of objects (Vinyals et al., 2014; Karpathy and FeiFei, 2015; Zarriaiß and Schlangen, 2017). We hope that our dataset could also be useful for exploring the reverse task of describing actions on web pages. Reference games. In a reference game, the system has to select the correct object referenced by the given utterance (Frank and Goodman, 2012). Previous work on reference games focuses on a small number of objects with similar properties, and applies pragmatics to handle ambiguous utterance (Golland et al., 2010; Smith et al., 2013; Çelikyilmaz et al., 2014; Andreas and Klein, 2016; Yu et al., 2017). Our task can be viewed as a reference game with several challenges: higher number of objects, diverse object properties, and the need to interpret objects based on their contexts. Interacting with web pages. Automated scripts are used to interact with web elements. While most scripts reference elements with logical selectors (e.g., CSS and XPath), there have been several alternatives such as images (Yeh et al., 2009) and simple natural language utterances (Soh, 2017). Some other interfaces for navigating web pages include keystrokes (Spalteholz et a"
D18-1540,N18-1177,0,0.0451951,"ents instead of more obscure ones. 5 Related work and discussion Mapping natural language to actions. Previous work on semantic parsing learns to perform actions described by natural language utterances in various environments. Examples of such actions include API calls (Young et al., 2013; Su et al., 2017; Bordes and Weston, 2017), database queries (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2007; Berant et al., 2013; Yih et al., 2015), navigation (Artzi and Zettlemoyer, 2013; Janner et al., 2018), and object manipulation (Tellex et al., 2011; Andreas and Klein, 2015; Guu et al., 2017; Fried et al., 2018). For web pages and graphical user interfaces, there are previous works on using natural language to perform computations on web tables (Pasupat and Liang, 2015; Zhong et al., 2017) and submit web forms (Shi et al., 2017). Our task is similar to previous works on interpreting instructions on user interfaces (Branavan et al., 2009, 2010; Liu et al., 2018). While their works focuses on learning from distant supervision, we consider shallower interactions but on a much broader domain. Previous work also explores the reverse problem of generating natural language description of objects (Vinyals et"
D18-1540,D10-1040,1,0.951696,"object model (DOM), and spatial data from the rendered web page. Our task is inspired by the semantic parsing literature, which aims to map natural language utterances into actions such as database queries and object manipulation (Zelle and Mooney, 1996; Chen and Mooney, 2011; Artzi and Zettlemoyer, 2013; Berant et al., 2013; Misra et al., 2015; Andreas and Klein, 2015). While these actions usually act on an environment with a fixed and known schema, web pages contain a larger variety of structures, making the task more open-ended. At the same time, our task can be viewed as a reference game (Golland et al., 2010; Smith et al., 2013; Andreas and Klein, 2016), where the system has to select an object given a natural language reference. The diversity of attributes in web page elements, along with the need to use context to interpret elements, makes web pages particularly interesting. Identifying elements via natural language has several real-world applications. The main one is providing a voice interface for interacting with web pages, which is especially useful as an assistive technology for the visually impaired (Zajicek et al., 1998; Ashok et al., 2014). Another use case is browser automation: natura"
D18-1540,P17-1097,1,0.810694,"uch prominent elements instead of more obscure ones. 5 Related work and discussion Mapping natural language to actions. Previous work on semantic parsing learns to perform actions described by natural language utterances in various environments. Examples of such actions include API calls (Young et al., 2013; Su et al., 2017; Bordes and Weston, 2017), database queries (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2007; Berant et al., 2013; Yih et al., 2015), navigation (Artzi and Zettlemoyer, 2013; Janner et al., 2018), and object manipulation (Tellex et al., 2011; Andreas and Klein, 2015; Guu et al., 2017; Fried et al., 2018). For web pages and graphical user interfaces, there are previous works on using natural language to perform computations on web tables (Pasupat and Liang, 2015; Zhong et al., 2017) and submit web forms (Shi et al., 2017). Our task is similar to previous works on interpreting instructions on user interfaces (Branavan et al., 2009, 2010; Liu et al., 2018). While their works focuses on learning from distant supervision, we consider shallower interactions but on a much broader domain. Previous work also explores the reverse problem of generating natural language description o"
D18-1540,Q18-1004,0,0.101652,"dates. To provide a natural interface for users, the model should arguably learn to predict such prominent elements instead of more obscure ones. 5 Related work and discussion Mapping natural language to actions. Previous work on semantic parsing learns to perform actions described by natural language utterances in various environments. Examples of such actions include API calls (Young et al., 2013; Su et al., 2017; Bordes and Weston, 2017), database queries (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2007; Berant et al., 2013; Yih et al., 2015), navigation (Artzi and Zettlemoyer, 2013; Janner et al., 2018), and object manipulation (Tellex et al., 2011; Andreas and Klein, 2015; Guu et al., 2017; Fried et al., 2018). For web pages and graphical user interfaces, there are previous works on using natural language to perform computations on web tables (Pasupat and Liang, 2015; Zhong et al., 2017) and submit web forms (Shi et al., 2017). Our task is similar to previous works on interpreting instructions on user interfaces (Branavan et al., 2009, 2010; Liu et al., 2018). While their works focuses on learning from distant supervision, we consider shallower interactions but on a much broader domain. Pre"
D18-1540,P15-1096,1,0.845525,"(e.g., links, buttons, and form inputs), as illustrated in Figure 1. While some commands refer to an element’s text directly, many others require more complex reasoning with the various aspects of web pages: the text, attributes, styles, structural data from the document object model (DOM), and spatial data from the rendered web page. Our task is inspired by the semantic parsing literature, which aims to map natural language utterances into actions such as database queries and object manipulation (Zelle and Mooney, 1996; Chen and Mooney, 2011; Artzi and Zettlemoyer, 2013; Berant et al., 2013; Misra et al., 2015; Andreas and Klein, 2015). While these actions usually act on an environment with a fixed and known schema, web pages contain a larger variety of structures, making the task more open-ended. At the same time, our task can be viewed as a reference game (Golland et al., 2010; Smith et al., 2013; Andreas and Klein, 2016), where the system has to select an object given a natural language reference. The diversity of attributes in web page elements, along with the need to use context to interpret elements, makes web pages particularly interesting. Identifying elements via natural language has sever"
D18-1540,P15-1142,1,0.882183,"tions described by natural language utterances in various environments. Examples of such actions include API calls (Young et al., 2013; Su et al., 2017; Bordes and Weston, 2017), database queries (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2007; Berant et al., 2013; Yih et al., 2015), navigation (Artzi and Zettlemoyer, 2013; Janner et al., 2018), and object manipulation (Tellex et al., 2011; Andreas and Klein, 2015; Guu et al., 2017; Fried et al., 2018). For web pages and graphical user interfaces, there are previous works on using natural language to perform computations on web tables (Pasupat and Liang, 2015; Zhong et al., 2017) and submit web forms (Shi et al., 2017). Our task is similar to previous works on interpreting instructions on user interfaces (Branavan et al., 2009, 2010; Liu et al., 2018). While their works focuses on learning from distant supervision, we consider shallower interactions but on a much broader domain. Previous work also explores the reverse problem of generating natural language description of objects (Vinyals et al., 2014; Karpathy and FeiFei, 2015; Zarriaiß and Schlangen, 2017). We hope that our dataset could also be useful for exploring the reverse task of describing"
D18-1540,P17-1023,0,0.0297244,"rfaces, there are previous works on using natural language to perform computations on web tables (Pasupat and Liang, 2015; Zhong et al., 2017) and submit web forms (Shi et al., 2017). Our task is similar to previous works on interpreting instructions on user interfaces (Branavan et al., 2009, 2010; Liu et al., 2018). While their works focuses on learning from distant supervision, we consider shallower interactions but on a much broader domain. Previous work also explores the reverse problem of generating natural language description of objects (Vinyals et al., 2014; Karpathy and FeiFei, 2015; Zarriaiß and Schlangen, 2017). We hope that our dataset could also be useful for exploring the reverse task of describing actions on web pages. Reference games. In a reference game, the system has to select the correct object referenced by the given utterance (Frank and Goodman, 2012). Previous work on reference games focuses on a small number of objects with similar properties, and applies pragmatics to handle ambiguous utterance (Golland et al., 2010; Smith et al., 2013; Çelikyilmaz et al., 2014; Andreas and Klein, 2016; Yu et al., 2017). Our task can be viewed as a reference game with several challenges: higher number"
D18-1540,P15-1150,0,0.0258356,"l of the DOM hierarchy. 3.2 <a class=""dd-head"" id=""tip-link"" href=""submit_story/"">Tip Us</a> Text content: String attributes: Visual features: Figure 2: Example of properties used to compute the embedding g(e) of the element e. • String attributes. We tokenize other string attributes (tag, id, class) at punctuation marks and camel-case boundaries. Then we embed them with separate lookup tables and average the resulting vectors. Embedding-based model A common method for matching two pieces of text is to embed them separately and then compute a score from the two embeddings (Kiros et al., 2015; Tai et al., 2015). For a command c and elements e1 , . . . , ek , we define the following conditional distribution over the elements: • Visual features. We form a vector consisting of the coordinates of the element’s center (as fractions of the page width and height) and visibility (as a boolean). p (ei |c) ∝ exp [s(f (c), g(ei ))] where s is a scoring function, f (c) is the embedding of c, and g(ei ) is the embedding of ei , described below. The model is trained to maximize the log-likelihood of the correct element in the training data. Command embedding. To compute f (c), we embed each token of c into a fixe"
D18-1540,D07-1071,0,0.0169591,"the web page). In these cases, the annotation usually gives the most prominent element among the possible candidates. To provide a natural interface for users, the model should arguably learn to predict such prominent elements instead of more obscure ones. 5 Related work and discussion Mapping natural language to actions. Previous work on semantic parsing learns to perform actions described by natural language utterances in various environments. Examples of such actions include API calls (Young et al., 2013; Su et al., 2017; Bordes and Weston, 2017), database queries (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2007; Berant et al., 2013; Yih et al., 2015), navigation (Artzi and Zettlemoyer, 2013; Janner et al., 2018), and object manipulation (Tellex et al., 2011; Andreas and Klein, 2015; Guu et al., 2017; Fried et al., 2018). For web pages and graphical user interfaces, there are previous works on using natural language to perform computations on web tables (Pasupat and Liang, 2015; Zhong et al., 2017) and submit web forms (Shi et al., 2017). Our task is similar to previous works on interpreting instructions on user interfaces (Branavan et al., 2009, 2010; Liu et al., 2018). While their works focuses on"
D18-1540,P15-1128,0,0.0575354,"ly gives the most prominent element among the possible candidates. To provide a natural interface for users, the model should arguably learn to predict such prominent elements instead of more obscure ones. 5 Related work and discussion Mapping natural language to actions. Previous work on semantic parsing learns to perform actions described by natural language utterances in various environments. Examples of such actions include API calls (Young et al., 2013; Su et al., 2017; Bordes and Weston, 2017), database queries (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2007; Berant et al., 2013; Yih et al., 2015), navigation (Artzi and Zettlemoyer, 2013; Janner et al., 2018), and object manipulation (Tellex et al., 2011; Andreas and Klein, 2015; Guu et al., 2017; Fried et al., 2018). For web pages and graphical user interfaces, there are previous works on using natural language to perform computations on web tables (Pasupat and Liang, 2015; Zhong et al., 2017) and submit web forms (Shi et al., 2017). Our task is similar to previous works on interpreting instructions on user interfaces (Branavan et al., 2009, 2010; Liu et al., 2018). While their works focuses on learning from distant supervision, we co"
D18-1540,Q16-1019,0,0.0839996,"Missing"
D19-1275,P17-1080,0,0.562068,"large-scale unsupervised representations such as BERT and ELMo improve downstream performance on a wide range of natural language tasks (Devlin et al., 2019; Peters et al., 2018a; Radford et al., 2019), what these models learn about language remains an open scientific question. An emerging body of work investigates this question through probes, supervised models trained to predict a property (like parts-of-speech) from a constrained view of the representation. Probes trained on various representations have obtained high accuracy on tasks requiring part-of-speech and morphological information (Belinkov et al., 2017), syntactic and semantic information (Peters et al., 2018b; Tenney et al., 2019), among other properties (Conneau et al., 2018), providing evidence that deep representations trained on large datasets are predictive of a broad range of linguistic properties. But when a probe achieves high accuracy on a linguistic task using a representation, can we conclude that the representation encodes linguistic structure, or has the probe just learned the task? Probing papers tend to acknowledge this uncertainty, putting accuracies in context using random representation baselines (Zhang and Bowman, 2018) a"
D19-1275,Q19-1004,0,0.129642,"each linguistic task requires fewer samples than our control task. However, for dependency edge prediction, this leads to significantly reduced linguistic task accuracy. Finally, we find that the right weight decay constant can also lead to highaccuracy, high-selectivity probes, especially for dependency edge prediction. As shown, however, it is unclear what hyperparameters to use (e.g., weight decay 0.1) to achieve both high accuracy and high selectivity; that is, finding selective MLP probes is non-trivial. Applying dropout, the most popular probing regularization method (Adi et al., 2017; Belinkov and Glass, 2019; Sahin ¸ et al., 2019; Kim et al., 2019; Elloumi et al., 2018; Belinkov and Glass, 2017; Belinkov et al., 2018) does not consistently lead to high-accuracy, high-selectivity MLP probes across a broad range of dropout probabilities (p = 0.2 to p = 0.8) on part-of-speech tagging. For dependency edge prediction, dropout of p = 0.6 improves the selectivity of MLP-2 but not MLP-1, and considerably increases the already relatively large selectivity of the bilinear probe. Early stopping in the ranges tested also has little impact on part-ofspeech tagging, selectivity, but does improve selectivity of"
D19-1275,I17-1001,0,0.102174,"Missing"
D19-1275,D18-1313,0,0.0553329,"ects the properties of the representation. Thus, a good probe (one that provides insights into the linguistic properties of a representation) should be what we call selective, achieving high linguistic task accuracy and low control task accuracy (see Figure 2). We show that selectivity can be a guide in designing probes and interpreting probing results, complementary to random representation baselines; as of now, there is little consensus on how to design probes. Early probing papers used linear functions (Shi et al., 2016; Ettinger et al., 2016; Alain and Bengio, 2016), which are still used (Bisazza and Tump, 2018; Liu et al., 2019), but multi-layer perceptron (MLP) probes are at least as popular (Belinkov et al., 2017; Conneau et al., 2018; Adi et al., 2017; Tenney et al., 2019; Ettinger et al., 2018). Arguments have been made for “simple” probes, e.g., that we want to find easily accessible information in a representation (Liu et al., 2019; Alain and Bengio, 2016). As a counterpoint though, “complex” MLP probes have also been suggested since useful properties might be encoded non-linearly (Conneau et al., 2018), and they tend to report similar trends to simpler probes anyway (Belinkov et al., 2017; Q"
D19-1275,P18-1198,0,0.490471,"e tasks (Devlin et al., 2019; Peters et al., 2018a; Radford et al., 2019), what these models learn about language remains an open scientific question. An emerging body of work investigates this question through probes, supervised models trained to predict a property (like parts-of-speech) from a constrained view of the representation. Probes trained on various representations have obtained high accuracy on tasks requiring part-of-speech and morphological information (Belinkov et al., 2017), syntactic and semantic information (Peters et al., 2018b; Tenney et al., 2019), among other properties (Conneau et al., 2018), providing evidence that deep representations trained on large datasets are predictive of a broad range of linguistic properties. But when a probe achieves high accuracy on a linguistic task using a representation, can we conclude that the representation encodes linguistic structure, or has the probe just learned the task? Probing papers tend to acknowledge this uncertainty, putting accuracies in context using random representation baselines (Zhang and Bowman, 2018) and careful task design (Hupkes et al., 2018). Even so, as long as a representation is a lossless encoding, a sufficiently expre"
D19-1275,N19-1423,0,0.114733,"ntence 2 The dog ran Part-of-speech DT NN VBD Control task 10 15 10 after IN 42 ! . 42 Figure 1: Our control tasks define random behavior (like a random output, top) for each word type in the vocabulary. Each word token is assigned its type’s output, regardless of context (middle, bottom.) Control tasks have the same input and output space as a linguistic task (e.g., parts-of-speech) but can only be learned if the probe memorizes the mapping. Introduction As large-scale unsupervised representations such as BERT and ELMo improve downstream performance on a wide range of natural language tasks (Devlin et al., 2019; Peters et al., 2018a; Radford et al., 2019), what these models learn about language remains an open scientific question. An emerging body of work investigates this question through probes, supervised models trained to predict a property (like parts-of-speech) from a constrained view of the representation. Probes trained on various representations have obtained high accuracy on tasks requiring part-of-speech and morphological information (Belinkov et al., 2017), syntactic and semantic information (Peters et al., 2018b; Tenney et al., 2019), among other properties (Conneau et al., 2018), provi"
D19-1275,W18-5402,0,0.0149802,". However, for dependency edge prediction, this leads to significantly reduced linguistic task accuracy. Finally, we find that the right weight decay constant can also lead to highaccuracy, high-selectivity probes, especially for dependency edge prediction. As shown, however, it is unclear what hyperparameters to use (e.g., weight decay 0.1) to achieve both high accuracy and high selectivity; that is, finding selective MLP probes is non-trivial. Applying dropout, the most popular probing regularization method (Adi et al., 2017; Belinkov and Glass, 2019; Sahin ¸ et al., 2019; Kim et al., 2019; Elloumi et al., 2018; Belinkov and Glass, 2017; Belinkov et al., 2018) does not consistently lead to high-accuracy, high-selectivity MLP probes across a broad range of dropout probabilities (p = 0.2 to p = 0.8) on part-of-speech tagging. For dependency edge prediction, dropout of p = 0.6 improves the selectivity of MLP-2 but not MLP-1, and considerably increases the already relatively large selectivity of the bilinear probe. Early stopping in the ranges tested also has little impact on part-ofspeech tagging, selectivity, but does improve selectivity of MLP dependency edge prediction probes. From our study, we pic"
D19-1275,C18-1152,0,0.0286911,"inguistic task accuracy and low control task accuracy (see Figure 2). We show that selectivity can be a guide in designing probes and interpreting probing results, complementary to random representation baselines; as of now, there is little consensus on how to design probes. Early probing papers used linear functions (Shi et al., 2016; Ettinger et al., 2016; Alain and Bengio, 2016), which are still used (Bisazza and Tump, 2018; Liu et al., 2019), but multi-layer perceptron (MLP) probes are at least as popular (Belinkov et al., 2017; Conneau et al., 2018; Adi et al., 2017; Tenney et al., 2019; Ettinger et al., 2018). Arguments have been made for “simple” probes, e.g., that we want to find easily accessible information in a representation (Liu et al., 2019; Alain and Bengio, 2016). As a counterpoint though, “complex” MLP probes have also been suggested since useful properties might be encoded non-linearly (Conneau et al., 2018), and they tend to report similar trends to simpler probes anyway (Belinkov et al., 2017; Qian et al., 2016). We define control tasks corresponding to English part-of-speech tagging and dependency 2. Linear and bilinear probes achieve relatively high selectivity across a range of hy"
D19-1275,W16-2524,0,0.263613,"sentation, the less its accuracy on a linguistic task necessarily reflects the properties of the representation. Thus, a good probe (one that provides insights into the linguistic properties of a representation) should be what we call selective, achieving high linguistic task accuracy and low control task accuracy (see Figure 2). We show that selectivity can be a guide in designing probes and interpreting probing results, complementary to random representation baselines; as of now, there is little consensus on how to design probes. Early probing papers used linear functions (Shi et al., 2016; Ettinger et al., 2016; Alain and Bengio, 2016), which are still used (Bisazza and Tump, 2018; Liu et al., 2019), but multi-layer perceptron (MLP) probes are at least as popular (Belinkov et al., 2017; Conneau et al., 2018; Adi et al., 2017; Tenney et al., 2019; Ettinger et al., 2018). Arguments have been made for “simple” probes, e.g., that we want to find easily accessible information in a representation (Liu et al., 2019; Alain and Bengio, 2016). As a counterpoint though, “complex” MLP probes have also been suggested since useful properties might be encoded non-linearly (Conneau et al., 2018), and they tend to r"
D19-1275,W18-5426,0,0.0838753,"Adi et al., 2017; Conneau et al., 2018), word properties like verb tense or part-of-speech using word vectors (Shi et al., 2016; Belinkov et al., 2017; Liu et al., 2019), or word-pair properties like syntactic relationships using pairs of vectors (Tenney et al., 2019; Hewitt and Manning, 2019). Probes have been used to make relative claims between models or components (Adi et al., 2017; Liu et al., 2019; Belinkov et al., 2017) or absolute claims about models above baselines. Probes have also been used to test hypotheses about the mechanisms by which models perform tasks (Hupkes et al., 2018; Giulianelli et al., 2018). Previous work has made extensive use of control representations like non-contextual word embeddings or models with random weights (Belinkov et al., 2017; Tenney et al., 2019; Saphra and Lopez, 2019; Hewitt and Manning, 2019); our control tasks provide a complementary perspective, measuring a probe’s ability to decode a random function from the representation of interest. The most related work to ours is that of Zhang and Bowman (2018), who presented experiments for understanding the roles probe training sample size and memorization have on linguistic 2740 task accuracy. They observed that un"
D19-1275,D15-1002,0,0.0183179,"emergent properties of the representation. 5 Related Work Early work in probing, (also known as diagnostic classification (Hupkes et al., 2018),) extracted properties like parts-of-speech, gender, tense, and number from distributional word vector spaces like word2vec and GloVe (Mikolov et al., 2013; Pennington et al., 2014) using linear classifiers (Köhn, probe accuracies and selectivities across three representations. ELMo1 and ELMo2 are the two contextual layers of ELMo, while Proj0 refers to an untrained BiLSTM contextualization of ELMo’s non-contextual character CNN representations. 2015; Gupta et al., 2015). Soon after, the investigation of intermediate layers of deep models using linear probes was introduced independently by Ettinger et al. (2016) and Shi et al. (2016) in NLP and Alain and Bengio (2016) in computer vision. Since then, probing methods have varied as to whether they investigate whole-sentence properties like sentence length and word content using a sentence vector (Shi et al., 2016; Adi et al., 2017; Conneau et al., 2018), word properties like verb tense or part-of-speech using word vectors (Shi et al., 2016; Belinkov et al., 2017; Liu et al., 2019), or word-pair properties like"
D19-1275,N19-1419,1,0.561721,"er to the word representations than ELMo2, it may be easier to identify word identities from it, meaning the probe may utilize word identities more readily, as opposed to picking up on a representation of part-of-speech. 4.1 Experiments We run experiments on the first and second contextual layers of ELMo, denoted ELMo1 and ELMo2. 2739 We also examine the representations of an untrained BiLSTM run on the non-contextual character CNN word embeddings of ELMo, shown to be a strong baseline contextualization method, but without any linguistic knowledge learned from context (Zhang and Bowman, 2018; Hewitt and Manning, 2019). We denote this model Proj0. We train linear and MLP-1 probes for part-ofspeech tagging, and bilinear and MLP-1 probes for dependency edge prediction, all with default hyperparameters (§ 3.2). We examine both the linguistic task accuracy and selectivity achieved by each probe on each representation. Part-of-speech Tagging Linear Model Proj0 ELMo1 ELMo2 MLP-1 Accuracy Selectivity 96.3 97.2 96.6 20.6 26.0 31.4 Accuracy Selectivity 97.1 97.3 97.0 1.6 4.5 8.8 Dependency Edge Prediction Bilinear Model Proj0 ELMo1 ELMo2 MLP-1 Accuracy Selectivity 79.9 89.7 84.5 -4.3 6.7 6.2 Accuracy Selectivity 86."
D19-1275,S19-1026,0,0.0608317,"Missing"
D19-1275,N19-1112,0,0.196033,"he representation. Thus, a good probe (one that provides insights into the linguistic properties of a representation) should be what we call selective, achieving high linguistic task accuracy and low control task accuracy (see Figure 2). We show that selectivity can be a guide in designing probes and interpreting probing results, complementary to random representation baselines; as of now, there is little consensus on how to design probes. Early probing papers used linear functions (Shi et al., 2016; Ettinger et al., 2016; Alain and Bengio, 2016), which are still used (Bisazza and Tump, 2018; Liu et al., 2019), but multi-layer perceptron (MLP) probes are at least as popular (Belinkov et al., 2017; Conneau et al., 2018; Adi et al., 2017; Tenney et al., 2019; Ettinger et al., 2018). Arguments have been made for “simple” probes, e.g., that we want to find easily accessible information in a representation (Liu et al., 2019; Alain and Bengio, 2016). As a counterpoint though, “complex” MLP probes have also been suggested since useful properties might be encoded non-linearly (Conneau et al., 2018), and they tend to report similar trends to simpler probes anyway (Belinkov et al., 2017; Qian et al., 2016)."
D19-1275,J93-2004,0,0.070014,"unnormalized by batch size. Early stopping. All of our probing models are trained with Adam (Kingma and Ba, 2014). By default, we anneal the learning rate by a factor of 0.5 each time an epoch does not lead to a new minimum loss on the development set, and stop training when 4 such epochs occur in a row. However, in early stopping, we explicitly halt training at a fixed number of gradient steps. From the default of 100000 (approximately 40 epochs), we let this maximum take on the values {50000, 25000, 12500, 6000, 3000, 1500}. 3.3 Representation Dataset We use the Penn Treebank (PTB) dataset (Marcus et al., 1993) with the traditional parsing training/development/testing splits5 without preprocess5 As given by the code of Qi and Manning (2017) at https://github.com/qipeng/arc-swift. Results Selectivity of default hyperparameters. Our results with linear, bilinear, and MLP probes with “default” hyperparameters, as specified in § 3.2, are found in Table 1 (top). We find that linear probes achieve similar part-of-speech accuracies to MLPs (97.2 compared to 97.3) with substantially higher selectivity (26.0 vs 4.50). In dependency edge prediction, we find a definite gap between bilinear probe accuracy (89.0"
D19-1275,de-marneffe-etal-2006-generating,0,0.0213188,"Missing"
D19-1275,D14-1162,0,0.0835742,"Missing"
D19-1275,N18-1202,0,0.739803,"Part-of-speech DT NN VBD Control task 10 15 10 after IN 42 ! . 42 Figure 1: Our control tasks define random behavior (like a random output, top) for each word type in the vocabulary. Each word token is assigned its type’s output, regardless of context (middle, bottom.) Control tasks have the same input and output space as a linguistic task (e.g., parts-of-speech) but can only be learned if the probe memorizes the mapping. Introduction As large-scale unsupervised representations such as BERT and ELMo improve downstream performance on a wide range of natural language tasks (Devlin et al., 2019; Peters et al., 2018a; Radford et al., 2019), what these models learn about language remains an open scientific question. An emerging body of work investigates this question through probes, supervised models trained to predict a property (like parts-of-speech) from a constrained view of the representation. Probes trained on various representations have obtained high accuracy on tasks requiring part-of-speech and morphological information (Belinkov et al., 2017), syntactic and semantic information (Peters et al., 2018b; Tenney et al., 2019), among other properties (Conneau et al., 2018), providing evidence that de"
D19-1275,N19-1329,0,0.17074,"yntactic relationships using pairs of vectors (Tenney et al., 2019; Hewitt and Manning, 2019). Probes have been used to make relative claims between models or components (Adi et al., 2017; Liu et al., 2019; Belinkov et al., 2017) or absolute claims about models above baselines. Probes have also been used to test hypotheses about the mechanisms by which models perform tasks (Hupkes et al., 2018; Giulianelli et al., 2018). Previous work has made extensive use of control representations like non-contextual word embeddings or models with random weights (Belinkov et al., 2017; Tenney et al., 2019; Saphra and Lopez, 2019; Hewitt and Manning, 2019); our control tasks provide a complementary perspective, measuring a probe’s ability to decode a random function from the representation of interest. The most related work to ours is that of Zhang and Bowman (2018), who presented experiments for understanding the roles probe training sample size and memorization have on linguistic 2740 task accuracy. They observed that untrained BiLSTM contextualizers achieved almost the same part-of-speech tagging accuracies as trained contextualizers, and found that by reducing the probe training set, the trained models could be sh"
D19-1275,D16-1159,0,0.228754,"perties of a representation, the less its accuracy on a linguistic task necessarily reflects the properties of the representation. Thus, a good probe (one that provides insights into the linguistic properties of a representation) should be what we call selective, achieving high linguistic task accuracy and low control task accuracy (see Figure 2). We show that selectivity can be a guide in designing probes and interpreting probing results, complementary to random representation baselines; as of now, there is little consensus on how to design probes. Early probing papers used linear functions (Shi et al., 2016; Ettinger et al., 2016; Alain and Bengio, 2016), which are still used (Bisazza and Tump, 2018; Liu et al., 2019), but multi-layer perceptron (MLP) probes are at least as popular (Belinkov et al., 2017; Conneau et al., 2018; Adi et al., 2017; Tenney et al., 2019; Ettinger et al., 2018). Arguments have been made for “simple” probes, e.g., that we want to find easily accessible information in a representation (Liu et al., 2019; Alain and Bengio, 2016). As a counterpoint though, “complex” MLP probes have also been suggested since useful properties might be encoded non-linearly (Conneau et al., 20"
D19-1275,W18-5448,0,0.28532,"n (Belinkov et al., 2017), syntactic and semantic information (Peters et al., 2018b; Tenney et al., 2019), among other properties (Conneau et al., 2018), providing evidence that deep representations trained on large datasets are predictive of a broad range of linguistic properties. But when a probe achieves high accuracy on a linguistic task using a representation, can we conclude that the representation encodes linguistic structure, or has the probe just learned the task? Probing papers tend to acknowledge this uncertainty, putting accuracies in context using random representation baselines (Zhang and Bowman, 2018) and careful task design (Hupkes et al., 2018). Even so, as long as a representation is a lossless encoding, a sufficiently expressive probe with enough training data can learn any task on top of it. In this paper, we propose control tasks, which associate word types with random outputs, to give intuition for the expressivity of probe families and 2733 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 2733–2743, c Hong Kong, China, November 3–7, 2019. 2019 Association for Compu"
D19-1275,D18-1179,0,0.478763,"Part-of-speech DT NN VBD Control task 10 15 10 after IN 42 ! . 42 Figure 1: Our control tasks define random behavior (like a random output, top) for each word type in the vocabulary. Each word token is assigned its type’s output, regardless of context (middle, bottom.) Control tasks have the same input and output space as a linguistic task (e.g., parts-of-speech) but can only be learned if the probe memorizes the mapping. Introduction As large-scale unsupervised representations such as BERT and ELMo improve downstream performance on a wide range of natural language tasks (Devlin et al., 2019; Peters et al., 2018a; Radford et al., 2019), what these models learn about language remains an open scientific question. An emerging body of work investigates this question through probes, supervised models trained to predict a property (like parts-of-speech) from a constrained view of the representation. Probes trained on various representations have obtained high accuracy on tasks requiring part-of-speech and morphological information (Belinkov et al., 2017), syntactic and semantic information (Peters et al., 2018b; Tenney et al., 2019), among other properties (Conneau et al., 2018), providing evidence that de"
D19-1275,P17-2018,0,0.0168057,"nneal the learning rate by a factor of 0.5 each time an epoch does not lead to a new minimum loss on the development set, and stop training when 4 such epochs occur in a row. However, in early stopping, we explicitly halt training at a fixed number of gradient steps. From the default of 100000 (approximately 40 epochs), we let this maximum take on the values {50000, 25000, 12500, 6000, 3000, 1500}. 3.3 Representation Dataset We use the Penn Treebank (PTB) dataset (Marcus et al., 1993) with the traditional parsing training/development/testing splits5 without preprocess5 As given by the code of Qi and Manning (2017) at https://github.com/qipeng/arc-swift. Results Selectivity of default hyperparameters. Our results with linear, bilinear, and MLP probes with “default” hyperparameters, as specified in § 3.2, are found in Table 1 (top). We find that linear probes achieve similar part-of-speech accuracies to MLPs (97.2 compared to 97.3) with substantially higher selectivity (26.0 vs 4.50). In dependency edge prediction, we find a definite gap between bilinear probe accuracy (89.0) and MLP-1 accuracy (92.3). However, the bilinear probe achieves 16.7 selectivity, compared to −0.7 by MLP-1 and 1.3 by MLP-2. Thus"
D19-1275,P16-1140,0,0.0662839,"Missing"
D19-1432,P07-1033,0,0.12113,"Missing"
D19-1432,N19-1423,0,0.0798266,"Missing"
D19-1432,P12-2023,0,0.027851,"istinct and learnable. More recent work consider errors in estimating the target domain (Hoffman et al., 2018) and derive learning bounds with respect to such errors. While these approaches make use of cluster and topic structures as prior, they still require some knowledge of the target distribution and train a model tailored to the target distribution. Instead, we assume no knowledge on the target distribution and train a single model by considering the worst case. In conditional settings such as machine translation, prior works connect topic modeling and domain adaptation (Hu et al., 2014; Eidelman et al., 2012). However, unlike our work, these approaches use topics at test time by inferring the domain from the input variable x. In language modeling, we have no inputs and thus must find models robust to unknown domain shifts at test time. In addition, it can be difficult to infer the test distribution as the distribution can rapidly change across users and time. Distributional Robustness: Our approach is based upon existing work in the distributionally robust optimization (DRO) literature. Optimizing on a ball of distributions around the empirical distribution has been considered in prior work (Ben-T"
D19-1432,D15-1162,0,0.0124812,"worst-case set or not. If it is, we update the model and otherwise we ignore it. 5 Experiments We demonstrate that topic CVaR improves maximum likelihood language models when ptrain 6= x test px . Section 5.1 outlines the experimental setup while Section 5.2 shows the robustness improvements and analysis of topic CVaR. 5.1 Evaluation Details Datasets. We use the following three corpora: the Yelp review corpus (Y ELP, (2017)), One Billion Word benchmark corpus (O NE BW ORD), and the TripAdvisor Annotated Dataset (T RIPA DV, Marcheggiani et al. (2014)). We preprocess the corpora using S PAC Y (Honnibal and Johnson (2015)) by removing sentences with fewer than 10 characters, segmenting sentences, tagging named-entities, and replacing each entity with its corresponding OntoNotes tag. 3 pˆtrain z ˆ (t) = [40, 30, 60], and For example with α = 0.2, L (t) = [0.2, 0.8, 0.1], then pz = [0.5, 0, 0.5]. Vocabulary. Our experiments will evaluate models using perplexity, which depends on the choice of vocabulary. To make perplexity comparable for models trained on different datasets, we use a single, fixed vocabulary formed by combining the most frequently occurring 10, 000 words in each corpus. All words in the mixtures"
D19-1432,P14-1110,0,0.0653192,"Missing"
D19-1432,P16-1029,0,0.0669947,"Missing"
D19-1432,K16-1028,0,0.0344903,"Missing"
D19-1432,N18-1202,0,0.114586,"Missing"
D19-1432,W17-4712,0,0.0228467,"o on staff was absolutely incredible. Table 1. Examples from the Y ELP corpus for which MLE outperforms topic CVaR (left column) and vice versa. Brackets indicate O NTO N OTES named-entity tags. The examples preferred by topic CVaR are stereotypical Yelp sentences, while those preferred by MLE refer to locations and accidents. 6 Related Work Domain Adaptation: In the case of known source (train) and target (test) domains, there exist a variety of techniques to learn robust models (Shimodaira, 2000; Qui˜nonero-Candela et al., 2009; Daume III, 2007; Ben-David et al., 2010; Blitzer et al., 2011; Pryzant et al., 2017) or domaininvariant features (Ganin and Lempitsky, 2015; Tzeng et al., 2014). However, such methods require accurate domain membership annotations. In the absence of domain membership annotations, prior multi-source domain adaptation (Mansour et al., 2009) approaches propose the use of clustering to identify candidate domains. For instance, Hoffman et al. (2012) and Xiong et al. (2014) discover latent domains in classification by clustering data using class labels. Gong et al. (2013) extend this work by identifying subsets which are distinct and learnable. More recent work consider errors in e"
D19-1432,N15-1020,0,0.0845783,"Missing"
D19-1432,D13-1140,0,0.0900836,"Missing"
J13-2005,P11-1060,1,0.174971,"Missing"
J13-2005,D11-1039,0,0.223903,"evised submission received: 19 February 2012; accepted for publication: 18 April 2012. doi:10.1162/COLI a 00127 No rights reserved. This work was authored as part of the Contributor’s ofﬁcial duties as an Employee of the United States Government and is therefore a work of the United States Government. In accordance with 17 U.S.C. 105, no copyright protection is available for such works under U.S. law. Computational Linguistics Volume 39, Number 2 required for various semantics-related tasks (Zettlemoyer and Collins 2005; Branavan et al. 2009; Liang, Jordan, and Klein 2009; Clarke et al. 2010; Artzi and Zettlemoyer 2011; Goldwasser et al. 2011). In this article, we develop new techniques to learn accurate semantic parsers from even weaker supervision. We demonstrate our techniques on the concrete task of building a system to answer questions given a structured database of facts; see Figure 1 for an example in the domain of U.S. geography. This problem of building natural language interfaces to databases (NLIDBs) has a long history in NLP, starting from the early days of artiﬁcial intelligence with systems such as L UNAR (Woods, Kaplan, and Webber 1972), C HAT-80 (Warren and Pereira 1982), and many others (se"
J13-2005,P02-1041,0,0.0305829,"dependencies such as those arising from anaphora. For example, in the phrase a state with a river that traverses its capital, its binds to state, but this dependence cannot be captured in a tree structure. A solution is to simply add an edge between the its node and the state node that forces the two nodes to have the same value. The result is still a well-deﬁned CSP, though not a treestructured one. The situation would become trickier if we were to integrate the other relations (aggregate, mark, and execute). We might be able to incorporate some ideas from Hybrid Logic Dependency Semantics (Baldridge and Kruijff 2002; White 2006), given that hybrid logic extends the tree structures of modal logic with nominals, thereby allowing a node to freely reference other nodes. In this article, however, we will stick to trees and leave the full exploration of non-trees for future work. 2.4.2 Computation of Join Relations. So far, we have given a declarative deﬁnition of the denotation zw of a DCS tree z with only join relations. Now we will show how to compute zw efﬁciently. Recall that the denotation is the set of feasible values for the root node. In general, ﬁnding the solution to a CSP is NP-hard, but for tr"
J13-2005,C04-1180,0,0.0384063,"Missing"
J13-2005,P09-1010,0,0.0460809,"E-mail: klein@cs.berkeley.edu. Submission received: 12 September 2011; revised submission received: 19 February 2012; accepted for publication: 18 April 2012. doi:10.1162/COLI a 00127 No rights reserved. This work was authored as part of the Contributor’s ofﬁcial duties as an Employee of the United States Government and is therefore a work of the United States Government. In accordance with 17 U.S.C. 105, no copyright protection is available for such works under U.S. law. Computational Linguistics Volume 39, Number 2 required for various semantics-related tasks (Zettlemoyer and Collins 2005; Branavan et al. 2009; Liang, Jordan, and Klein 2009; Clarke et al. 2010; Artzi and Zettlemoyer 2011; Goldwasser et al. 2011). In this article, we develop new techniques to learn accurate semantic parsers from even weaker supervision. We demonstrate our techniques on the concrete task of building a system to answer questions given a structured database of facts; see Figure 1 for an example in the domain of U.S. geography. This problem of building natural language interfaces to databases (NLIDBs) has a long history in NLP, starting from the early days of artiﬁcial intelligence with systems such as L UNAR (Woods, Ka"
J13-2005,P11-1028,0,0.0638172,"Missing"
J13-2005,P10-1129,0,0.0180014,"Missing"
J13-2005,W10-2903,0,0.738828,"12 September 2011; revised submission received: 19 February 2012; accepted for publication: 18 April 2012. doi:10.1162/COLI a 00127 No rights reserved. This work was authored as part of the Contributor’s ofﬁcial duties as an Employee of the United States Government and is therefore a work of the United States Government. In accordance with 17 U.S.C. 105, no copyright protection is available for such works under U.S. law. Computational Linguistics Volume 39, Number 2 required for various semantics-related tasks (Zettlemoyer and Collins 2005; Branavan et al. 2009; Liang, Jordan, and Klein 2009; Clarke et al. 2010; Artzi and Zettlemoyer 2011; Goldwasser et al. 2011). In this article, we develop new techniques to learn accurate semantic parsers from even weaker supervision. We demonstrate our techniques on the concrete task of building a system to answer questions given a structured database of facts; see Figure 1 for an example in the domain of U.S. geography. This problem of building natural language interfaces to databases (NLIDBs) has a long history in NLP, starting from the early days of artiﬁcial intelligence with systems such as L UNAR (Woods, Kaplan, and Webber 1972), C HAT-80 (Warren and Pereir"
J13-2005,D09-1100,0,0.0262575,"Missing"
J13-2005,W05-0602,0,0.0145139,"ances. In response, against the backdrop of a statistical revolution in NLP during the 1990s, researchers began to build systems that could learn from examples, with the hope of overcoming the limitations of rule-based methods. One of the earliest statistical efforts was the C HILL system (Zelle and Mooney 1996), which learned a shift-reduce semantic parser. Since then, there has been a healthy line of work yielding increasingly more accurate semantic parsers by using new semantic representations and machine learning techniques (Miller et al. 1996; Zelle and Mooney 1996; Tang and Mooney 2001; Ge and Mooney 2005; Kate, Wong, and Mooney 2005; Zettlemoyer and Collins 2005; Kate and Mooney 2006; Wong and Mooney 2006; Kate and Mooney 2007; Wong and Mooney 2007; Zettlemoyer and Collins 2007; Kwiatkowski et al. 2010, 2011). Although statistical methods provided advantages such as robustness and portability, however, their application in semantic parsing achieved only limited success. One of the main obstacles was that these methods depended crucially on having examples of utterances paired with logical forms, and this requires substantial human effort to obtain. Furthermore, the annotators must be proﬁcien"
J13-2005,P11-1149,0,0.0998202,"19 February 2012; accepted for publication: 18 April 2012. doi:10.1162/COLI a 00127 No rights reserved. This work was authored as part of the Contributor’s ofﬁcial duties as an Employee of the United States Government and is therefore a work of the United States Government. In accordance with 17 U.S.C. 105, no copyright protection is available for such works under U.S. law. Computational Linguistics Volume 39, Number 2 required for various semantics-related tasks (Zettlemoyer and Collins 2005; Branavan et al. 2009; Liang, Jordan, and Klein 2009; Clarke et al. 2010; Artzi and Zettlemoyer 2011; Goldwasser et al. 2011). In this article, we develop new techniques to learn accurate semantic parsers from even weaker supervision. We demonstrate our techniques on the concrete task of building a system to answer questions given a structured database of facts; see Figure 1 for an example in the domain of U.S. geography. This problem of building natural language interfaces to databases (NLIDBs) has a long history in NLP, starting from the early days of artiﬁcial intelligence with systems such as L UNAR (Woods, Kaplan, and Webber 1972), C HAT-80 (Warren and Pereira 1982), and many others (see Androutsopoulos, Ritchi"
J13-2005,P06-1063,0,0.0215908,"Missing"
J13-2005,P06-1115,0,0.712544,"ing the 1990s, researchers began to build systems that could learn from examples, with the hope of overcoming the limitations of rule-based methods. One of the earliest statistical efforts was the C HILL system (Zelle and Mooney 1996), which learned a shift-reduce semantic parser. Since then, there has been a healthy line of work yielding increasingly more accurate semantic parsers by using new semantic representations and machine learning techniques (Miller et al. 1996; Zelle and Mooney 1996; Tang and Mooney 2001; Ge and Mooney 2005; Kate, Wong, and Mooney 2005; Zettlemoyer and Collins 2005; Kate and Mooney 2006; Wong and Mooney 2006; Kate and Mooney 2007; Wong and Mooney 2007; Zettlemoyer and Collins 2007; Kwiatkowski et al. 2010, 2011). Although statistical methods provided advantages such as robustness and portability, however, their application in semantic parsing achieved only limited success. One of the main obstacles was that these methods depended crucially on having examples of utterances paired with logical forms, and this requires substantial human effort to obtain. Furthermore, the annotators must be proﬁcient in some formal language, which drastically reduces the size of the annotator po"
J13-2005,D10-1119,0,0.113656,"Missing"
J13-2005,D11-1140,0,0.662189,"Missing"
J13-2005,P09-1011,1,0.879128,"Missing"
J13-2005,N09-1069,1,0.74662,"Missing"
J13-2005,D08-1082,0,0.0334655,"Missing"
J13-2005,J93-2004,0,0.0450344,"Missing"
J13-2005,P96-1008,0,0.137423,"cult to scale up, both to other domains and to more complex utterances. In response, against the backdrop of a statistical revolution in NLP during the 1990s, researchers began to build systems that could learn from examples, with the hope of overcoming the limitations of rule-based methods. One of the earliest statistical efforts was the C HILL system (Zelle and Mooney 1996), which learned a shift-reduce semantic parser. Since then, there has been a healthy line of work yielding increasingly more accurate semantic parsers by using new semantic representations and machine learning techniques (Miller et al. 1996; Zelle and Mooney 1996; Tang and Mooney 2001; Ge and Mooney 2005; Kate, Wong, and Mooney 2005; Zettlemoyer and Collins 2005; Kate and Mooney 2006; Wong and Mooney 2006; Kate and Mooney 2007; Wong and Mooney 2007; Zettlemoyer and Collins 2007; Kwiatkowski et al. 2010, 2011). Although statistical methods provided advantages such as robustness and portability, however, their application in semantic parsing achieved only limited success. One of the main obstacles was that these methods depended crucially on having examples of utterances paired with logical forms, and this requires substantial hum"
J13-2005,P06-1055,1,0.165372,"uld trigger only city because city is a prototype word, but town would trigger all the NN predicates (city, state, country, etc.) because it is not a prototype word. Prototype triggers require only a modest amount of domain-speciﬁc supervision (see the right side of Figure 22 for the entire list for G EO and J OBS). In fact, as we’ll see in Section 4.2, prototype triggers are not absolutely required to obtain good accuracies, but they give an extra boost and also improve computational efﬁciency by reducing the set of candidate DCS trees. 13 To perform POS tagging, we used the Berkeley Parser (Petrov et al. 2006), trained on the WSJ Treebank (Marcus, Marcinkiewicz, and Santorini 1993) and the Question Treebank (Judge, Cahill, and v. Genabith 2006)—thanks to Slav Petrov for providing the trained parser. 431 Computational Linguistics Volume 39, Number 2 Figure 22 Lexical triggers used in our experiments. Finally, to determine triggering, we stem all words using the Porter stemmer (Porter 1980), so that mountains triggers the same predicates as mountain. We also decompose superlatives into two words (e.g., largest is mapped to most large), allowing us to construct the logical form more compositionally. 4"
J13-2005,P03-1067,0,0.0845904,"Missing"
J13-2005,P10-1083,0,0.0353523,"Missing"
J13-2005,J82-3002,0,0.456079,"larke et al. 2010; Artzi and Zettlemoyer 2011; Goldwasser et al. 2011). In this article, we develop new techniques to learn accurate semantic parsers from even weaker supervision. We demonstrate our techniques on the concrete task of building a system to answer questions given a structured database of facts; see Figure 1 for an example in the domain of U.S. geography. This problem of building natural language interfaces to databases (NLIDBs) has a long history in NLP, starting from the early days of artiﬁcial intelligence with systems such as L UNAR (Woods, Kaplan, and Webber 1972), C HAT-80 (Warren and Pereira 1982), and many others (see Androutsopoulos, Ritchie, and Thanisch [1995] for an overview). We believe NLIDBs provide an appropriate starting point for semantic parsing because they lead directly to practical systems, and they allow us to temporarily sidestep intractable philosophical questions on how to represent meaning in general. Early NLIDBs were quite successful in their respective limited domains, but because these systems were constructed from manually built rules, they became difﬁcult to scale up, both to other domains and to more complex utterances. In response, against the backdrop of a"
J13-2005,N06-1056,0,0.608415,"hers began to build systems that could learn from examples, with the hope of overcoming the limitations of rule-based methods. One of the earliest statistical efforts was the C HILL system (Zelle and Mooney 1996), which learned a shift-reduce semantic parser. Since then, there has been a healthy line of work yielding increasingly more accurate semantic parsers by using new semantic representations and machine learning techniques (Miller et al. 1996; Zelle and Mooney 1996; Tang and Mooney 2001; Ge and Mooney 2005; Kate, Wong, and Mooney 2005; Zettlemoyer and Collins 2005; Kate and Mooney 2006; Wong and Mooney 2006; Kate and Mooney 2007; Wong and Mooney 2007; Zettlemoyer and Collins 2007; Kwiatkowski et al. 2010, 2011). Although statistical methods provided advantages such as robustness and portability, however, their application in semantic parsing achieved only limited success. One of the main obstacles was that these methods depended crucially on having examples of utterances paired with logical forms, and this requires substantial human effort to obtain. Furthermore, the annotators must be proﬁcient in some formal language, which drastically reduces the size of the annotator pool, dampening any hope"
J13-2005,P07-1121,0,0.654479,"from examples, with the hope of overcoming the limitations of rule-based methods. One of the earliest statistical efforts was the C HILL system (Zelle and Mooney 1996), which learned a shift-reduce semantic parser. Since then, there has been a healthy line of work yielding increasingly more accurate semantic parsers by using new semantic representations and machine learning techniques (Miller et al. 1996; Zelle and Mooney 1996; Tang and Mooney 2001; Ge and Mooney 2005; Kate, Wong, and Mooney 2005; Zettlemoyer and Collins 2005; Kate and Mooney 2006; Wong and Mooney 2006; Kate and Mooney 2007; Wong and Mooney 2007; Zettlemoyer and Collins 2007; Kwiatkowski et al. 2010, 2011). Although statistical methods provided advantages such as robustness and portability, however, their application in semantic parsing achieved only limited success. One of the main obstacles was that these methods depended crucially on having examples of utterances paired with logical forms, and this requires substantial human effort to obtain. Furthermore, the annotators must be proﬁcient in some formal language, which drastically reduces the size of the annotator pool, dampening any hope of acquiring enough data to fulﬁll the visi"
J13-2005,D07-1071,0,0.712536,"he hope of overcoming the limitations of rule-based methods. One of the earliest statistical efforts was the C HILL system (Zelle and Mooney 1996), which learned a shift-reduce semantic parser. Since then, there has been a healthy line of work yielding increasingly more accurate semantic parsers by using new semantic representations and machine learning techniques (Miller et al. 1996; Zelle and Mooney 1996; Tang and Mooney 2001; Ge and Mooney 2005; Kate, Wong, and Mooney 2005; Zettlemoyer and Collins 2005; Kate and Mooney 2006; Wong and Mooney 2006; Kate and Mooney 2007; Wong and Mooney 2007; Zettlemoyer and Collins 2007; Kwiatkowski et al. 2010, 2011). Although statistical methods provided advantages such as robustness and portability, however, their application in semantic parsing achieved only limited success. One of the main obstacles was that these methods depended crucially on having examples of utterances paired with logical forms, and this requires substantial human effort to obtain. Furthermore, the annotators must be proﬁcient in some formal language, which drastically reduces the size of the annotator pool, dampening any hope of acquiring enough data to fulﬁll the vision of learning highly accurate"
J13-2005,D13-1160,1,\N,Missing
N06-1014,P04-1061,1,0.146867,"= argmax θ + P x,z P |x; θ1 )p2 (z |x; θ2 ), q(z; x) log p1 (x, z; θ1 ) q(z; x) log p2 (x, z; θ2 ), x,z where Zx is a normalization constant. The M-step decouples neatly into two independent optimization problems, which lead to single model updates using the expected counts from q(z; x). To compute Zx in the E-step, we must sum the product of two model posteriors over the set of possible zs with nonzero probability under both models. In general, if both posterior distributions over the latent variables z decompose in the same tractable manner, as in the context-free grammar induction work of Klein and Manning (2004), the summation could be carried out efficiently, for example using dynamic programming. In our case, we would have to sum over the set of alignments where each word in English is aligned to at most one word in French and each word in French is aligned to at most one word in English. Unfortunately, for even very simple models such as IBM 1 or 2, computing the normalization constant over this set of alignments is a #P -complete problem, by a reduction from counting matchings in a bipartite graph (Valiant, 1979). We could perhaps attempt to compute q using a variety of approximate probabilistic"
N06-1014,N03-1017,0,0.0774831,"but the words between them are not good translations of each other. If the intervening English words were null-aligned, we would have to pay a big distortion penalty for jumping 4 positions. On the other hand, if the edge (i+2, j+2) were included, that penalty would be mitigated. The translation cost for forcing that edge is smaller than the distortion cost. 4.2 BLEU evaluation To see whether our improvement in AER also improves BLEU score, we aligned 100K EnglishFrench sentences from the Europarl corpus and tested on 3000 sentences of length 5–15. Using GIZA++ model 4 alignments and Pharaoh (Koehn et al., 2003), we achieved a BLEU score of 0.3035. By using alignments from our jointly trained HMMs instead, we get a BLEU score of 0.3051. While this improvement is very modest, we are currently investigating alternative ways of interfacing with phrase table construction to make a larger impact on translation quality. 5 Related Work Our approach is similar in spirit to co-training, where two classifiers, complementary by the virtue of having different views of the data, are trained jointly to encourage agreement (Blum and Mitchell, 1998; Collins and Singer, 1999). One key difference in our work is that w"
N06-1014,C04-1032,0,0.0868641,"Missing"
N06-1014,P04-1066,0,0.438888,"er ` a la r´ eunion et en avons inform´ e le cojo en cons´ equence . E→F: 89.9/93.6/8.7 F→E: 92.2/93.5/7.3 Intersection: 96.5/91.4/5.7 Figure 1: An example of the Viterbi output of a pair of independently trained HMMs (top) and a pair of jointly trained HMMs (bottom), both trained on 1.1 million sentences. Rounded boxes denote possible alignments, square boxes are sure alignments, and solid boxes are model predictions. For each model, the overall Precision/Recall/AER on the development set is given. See Section 4 for details. this example, COJO is a rare word that becomes a garbage collector (Moore, 2004) for the models in both directions. Intersection eliminates the spurious alignments, but at the expense of recall. Intersection after training produces alignments that both models agree on. The joint training procedure we describe below builds on this idea by encouraging the models to agree during training. Consider the output of the jointly trained HMMs in Figure 1 (bottom). The garbage-collecting rare word is 106 no longer a problem. Not only are the individual E→F and F→E jointly-trained models better than their independently-trained counterparts, the jointlytrained intersected model also p"
N06-1014,H05-1011,0,0.0429226,"Missing"
N06-1014,C96-2141,0,0.989159,"Missing"
N06-1014,J03-1002,0,0.151201,"R on the standard English-French Hansards task. To our knowledge, this is the lowest published unsupervised AER result, and it is competitive with supervised approaches. Furthermore, our approach is very practical: it is no harder to implement than a standard HMM model, and joint training is no slower than the standard training of two HMM models. Finally, we show that word alignments from our system can be used in a phrasebased translation system to modestly improve BLEU score. 2 Alignment models: IBM 1, 2 and HMM We briefly review the sequence-based word alignment models (Brown et al., 1994; Och and Ney, 2003) and describe some of the choices in our implementation. All three modelsP are generative models of the form p(f |e) = a p(a, f |e), where e = (e1 , . . . , eI ) is the English sentence, f = (f1 , . . . , fJ ) is the French sentence, and a = (a1 , . . . , aJ ) is the (asymmetric) alignment which specifies the position of an English word aligned to each French word. All three models factor in the following way: p(a, f |e) = J Y pd (aj |aj− , j)pt (fj |eaj ), (1) j=1 where j− is the position of the last non-null-aligned French word before position j.2 The translation parameters pt (fj |eaj ) are"
N06-1014,H05-1010,1,0.627741,"Missing"
N06-1014,W99-0613,0,0.0563426,"5. Using GIZA++ model 4 alignments and Pharaoh (Koehn et al., 2003), we achieved a BLEU score of 0.3035. By using alignments from our jointly trained HMMs instead, we get a BLEU score of 0.3051. While this improvement is very modest, we are currently investigating alternative ways of interfacing with phrase table construction to make a larger impact on translation quality. 5 Related Work Our approach is similar in spirit to co-training, where two classifiers, complementary by the virtue of having different views of the data, are trained jointly to encourage agreement (Blum and Mitchell, 1998; Collins and Singer, 1999). One key difference in our work is that we rely exclusively on data likelihood to guide the two models in an unsupervised manner, rather than relying on an initial handful of labeled examples. The idea of exploiting agreement between two latent variable models is not new; there has been substantial previous work on leveraging the strengths of two complementary models. Klein and Manning (2004) combine two complementary models for grammar induction, one that models constituency and one that models dependency, in a manner broadly similar to the current work. Aside from investigating a different"
N06-1014,H05-1012,0,0.0809116,"Missing"
N06-1014,J93-2003,0,\N,Missing
N09-1069,W02-1001,0,0.0509712,"y small amount of noise to initialize the parameters, the variance due to initialization is systematically smaller than the variance due to permutation. sEM` is less sensitive to initialization than EM, but additional variance is created by randomly permuting the examples. Overall, the accuracy of sEM` is more variable than that of EM, but not by a large amount. 5 Discussion and related work As datasets increase in size, the demand for online algorithms has grown in recent years. One sees this clear trend in the supervised NLP literature— examples include the Perceptron algorithm for tagging (Collins, 2002), MIRA for dependency parsing (McDonald et al., 2005), exponentiated gradient algorithms (Collins et al., 2008), stochastic gradient for constituency parsing (Finkel et al., 2008), just to name a few. Empirically, online methods are of618 ten faster by an order of magnitude (Collins et al., 2008), and it has been argued on theoretical grounds that the fast, approximate nature of online methods is a good fit given that we are interested in test performance, not the training objective (Bottou and Bousquet, 2008; Shalev-Shwartz and Srebro, 2008). However, in the unsupervised NLP literature, onlin"
N09-1069,P08-1109,0,0.0237836,"ve to initialization than EM, but additional variance is created by randomly permuting the examples. Overall, the accuracy of sEM` is more variable than that of EM, but not by a large amount. 5 Discussion and related work As datasets increase in size, the demand for online algorithms has grown in recent years. One sees this clear trend in the supervised NLP literature— examples include the Perceptron algorithm for tagging (Collins, 2002), MIRA for dependency parsing (McDonald et al., 2005), exponentiated gradient algorithms (Collins et al., 2008), stochastic gradient for constituency parsing (Finkel et al., 2008), just to name a few. Empirically, online methods are of618 ten faster by an order of magnitude (Collins et al., 2008), and it has been argued on theoretical grounds that the fast, approximate nature of online methods is a good fit given that we are interested in test performance, not the training objective (Bottou and Bousquet, 2008; Shalev-Shwartz and Srebro, 2008). However, in the unsupervised NLP literature, online methods are rarely seen,5 and when they are, incremental EM is the dominant variant (Gildea and Hofmann, 1999; Kuo et al., 2008). Indeed, as we have shown, applying online EM do"
N09-1069,P07-1094,0,0.0085397,"bro, 2008). However, in the unsupervised NLP literature, online methods are rarely seen,5 and when they are, incremental EM is the dominant variant (Gildea and Hofmann, 1999; Kuo et al., 2008). Indeed, as we have shown, applying online EM does require some care, and some variants (including incremental EM) can fail catastrophically in face of local optima. Stepwise EM provides finer control via its optimization parameters and has proven quite successful. One family of methods that resembles incremental EM includes collapsed samplers for Bayesian models—for example, Goldwater et al. (2006) and Goldwater and Griffiths (2007). These samplers keep track of a sample of the latent variables for each example, akin to the sufficient statistics that we store in incremental EM. In contrast, stepwise EM does not require this storage and operates more in the spirit of a truly online algorithm. Besides speed, online algorithms are of interest for two additional reasons. First, in some applications, we receive examples sequentially and would like to estimate a model in real-time, e.g., in the clustering of news articles. Second, since humans learn sequentially, studying online EM might suggest new connections to cognitive me"
N09-1069,P06-1085,0,0.0125539,"2008; Shalev-Shwartz and Srebro, 2008). However, in the unsupervised NLP literature, online methods are rarely seen,5 and when they are, incremental EM is the dominant variant (Gildea and Hofmann, 1999; Kuo et al., 2008). Indeed, as we have shown, applying online EM does require some care, and some variants (including incremental EM) can fail catastrophically in face of local optima. Stepwise EM provides finer control via its optimization parameters and has proven quite successful. One family of methods that resembles incremental EM includes collapsed samplers for Bayesian models—for example, Goldwater et al. (2006) and Goldwater and Griffiths (2007). These samplers keep track of a sample of the latent variables for each example, akin to the sufficient statistics that we store in incremental EM. In contrast, stepwise EM does not require this storage and operates more in the spirit of a truly online algorithm. Besides speed, online algorithms are of interest for two additional reasons. First, in some applications, we receive examples sequentially and would like to estimate a model in real-time, e.g., in the clustering of news articles. Second, since humans learn sequentially, studying online EM might sugg"
N09-1069,D07-1031,0,0.0115303,"-of-speech tagging, document classification, word segmentation, and word alignment. 1 Introduction In unsupervised NLP tasks such as tagging, parsing, and alignment, one wishes to induce latent linguistic structures from raw text. Probabilistic modeling has emerged as a dominant paradigm for these problems, and the EM algorithm has been a driving force for learning models in a simple and intuitive manner. However, on some tasks, EM can converge slowly. For instance, on unsupervised part-ofspeech tagging, EM requires over 100 iterations to reach its peak performance on the Wall-Street Journal (Johnson, 2007). The slowness of EM is mainly due to its batch nature: Parameters are updated only once after each pass through the data. When parameter estimates are still rough or if there is high redundancy in the data, computing statistics on the entire dataset just to make one update can be wasteful. In this paper, we investigate two flavors of online EM—incremental EM (Neal and Hinton, 1998) and stepwise EM (Sato and Ishii, 2000; Capp´e and Moulines, 2009), both of which involve updating parameters after each example or after a mini-batch 611 (subset) of examples. Online algorithms have the potential t"
N09-1069,P08-1046,0,0.0325478,"Missing"
N09-1069,I08-4003,0,0.0150271,"stochastic gradient for constituency parsing (Finkel et al., 2008), just to name a few. Empirically, online methods are of618 ten faster by an order of magnitude (Collins et al., 2008), and it has been argued on theoretical grounds that the fast, approximate nature of online methods is a good fit given that we are interested in test performance, not the training objective (Bottou and Bousquet, 2008; Shalev-Shwartz and Srebro, 2008). However, in the unsupervised NLP literature, online methods are rarely seen,5 and when they are, incremental EM is the dominant variant (Gildea and Hofmann, 1999; Kuo et al., 2008). Indeed, as we have shown, applying online EM does require some care, and some variants (including incremental EM) can fail catastrophically in face of local optima. Stepwise EM provides finer control via its optimization parameters and has proven quite successful. One family of methods that resembles incremental EM includes collapsed samplers for Bayesian models—for example, Goldwater et al. (2006) and Goldwater and Griffiths (2007). These samplers keep track of a sample of the latent variables for each example, akin to the sufficient statistics that we store in incremental EM. In contrast,"
N09-1069,P08-1100,1,0.875983,"aking updates more frequently. However, these updates can be seen as noisy approximations to the full batch update, and this noise can in fact impede learning. This tradeoff between speed and stability is familiar to online algorithms for convex supervised learning problems—e.g., Perceptron, MIRA, stochastic gradient, etc. Unsupervised learning raises two additional issues: (1) Since the EM objective is nonconvex, we often get convergence to different local optima of varying quality; and (2) we evaluate on accuracy metrics which are at best loosely correlated with the EM likelihood objective (Liang and Klein, 2008). We will see that these issues can lead to surprising results. In Section 4, we present a thorough investigation of online EM, mostly focusing on stepwise EM since it dominates incremental EM. For stepwise EM, we find that choosing a good stepsize and mini-batch size is important but can fortunately be done adequately without supervision. With a proper choice, stepwise EM reaches the same performance as batch EM, but much more quickly. Moreover, it can even surpass the performance of batch EM. Our results are particularly striking on part-of-speech tagging: Batch EM crawls to an accuracy of 5"
N09-1069,P05-1012,0,0.0313674,"ameters, the variance due to initialization is systematically smaller than the variance due to permutation. sEM` is less sensitive to initialization than EM, but additional variance is created by randomly permuting the examples. Overall, the accuracy of sEM` is more variable than that of EM, but not by a large amount. 5 Discussion and related work As datasets increase in size, the demand for online algorithms has grown in recent years. One sees this clear trend in the supervised NLP literature— examples include the Perceptron algorithm for tagging (Collins, 2002), MIRA for dependency parsing (McDonald et al., 2005), exponentiated gradient algorithms (Collins et al., 2008), stochastic gradient for constituency parsing (Finkel et al., 2008), just to name a few. Empirically, online methods are of618 ten faster by an order of magnitude (Collins et al., 2008), and it has been argued on theoretical grounds that the fast, approximate nature of online methods is a good fit given that we are interested in test performance, not the training objective (Bottou and Bousquet, 2008; Shalev-Shwartz and Srebro, 2008). However, in the unsupervised NLP literature, online methods are rarely seen,5 and when they are, increm"
N09-1069,J03-1002,0,0.0153304,"Missing"
N09-1069,P07-1049,0,0.0100166,"Missing"
N09-1069,J01-3002,0,0.118831,"Missing"
N10-1082,J92-4003,0,0.11493,"r T OKEN, we consider three other alternative samplers. First, annealing (T OKENanneal ) is a commonly-used technique to improve mixing, where (3) is raised to some inverse temperature.7 In Figure 5(a)–(c), we see that unlike T YPE, T OKENanneal does not improve over T OKEN uniformly: it hurts for the HMM, improves slightly for the USM, and makes no difference for the PTSG. Although annealing does increase mobility of the sampler, this mobility is undirected, whereas type-based sampling increases mobility in purely model-driven directions. Unlike past work that operated on types (Wolff, 1988; Brown et al., 1992; Stolcke and Omohundro, 1994), type-based sampling makes stochastic choices, and moreover, these choices are reversible. Is this stochasticity important? To answer this, we consider a variant of T YPE, T YPEgreedy : instead of sampling from (7), T YPEgreedy considers a type block S and sets bs to 0 for all s ∈ S if p(bS = (0, . . . , 0) |· · · ) > p(bS = (1, . . . , 1) |· · · ); else it sets bs to 1 for all s ∈ S. From Figure 5(a)–(c), we see that greediness is disastrous for the HMM, hurts a little for USM, and makes no difference on the PTSG. These results show that stochasticity can indeed"
N10-1082,N09-1062,0,0.287322,"not deal with the strong type-based coupling (e.g., all instances of a word should be tagged similarly). The type-based sampler we will present is designed exactly to tackle this coupling, which we argue is stronger and more important to deal with in unsupervised learning. Figure 1 depicts the updates made by each of the three samplers. We tested our sampler on three models: a Bayesian HMM for part-of-speech induction (Goldwater and Griffiths, 2007), a nonparametric Bayesian model for word segmentation (Goldwater et al., 2006), and a nonparametric Bayesian model of tree substitution grammars (Cohn et al., 2009; Post and Gildea, 2009). Empirically, we find that typebased sampling improves performance and is less sensitive to initialization (Section 5). The ascending factorial function arises from marginalizing Dirichlet distributions and is responsible the rich-gets-richer phenomenon: the larger n is, more we gain by increasing it.  n possibilithe particular z uniformly out of the m ties. Figure 2(b) shows the effectiveness of this typebased sampler. This simple example exposes the fundamental challenge of multimodality in unsupervised learning. Both m = 0 and m = n are modes due to the rich-gets-r"
N10-1082,N09-1026,1,0.834248,"g step on the trees involving Markovization, binarization, and collapsing of unary chains; words occurring once are replaced with one of 50 “unknown word” tokens, using base distributions {µr } that penalize the size of trees, and sampling the hyperparameters (see Cohn et al. (2009) for details). 6 To evaluate, we created a grammar where the rule probabilities are the mean values under the PTSG distribution: this involves taking a weighted combination (based on the concentration parameters) of the rule counts from the PTSG samples and the PCFG-derived base distribution. We used the decoder of DeNero et al. (2009) to parse. 579 improve the likelihood but actually hurt parsing accuracy, suggesting that the PTSG model is overfitting. To better understand the gains from T YPE over T OKEN, we consider three other alternative samplers. First, annealing (T OKENanneal ) is a commonly-used technique to improve mixing, where (3) is raised to some inverse temperature.7 In Figure 5(a)–(c), we see that unlike T YPE, T OKENanneal does not improve over T OKEN uniformly: it hurts for the HMM, improves slightly for the USM, and makes no difference for the PTSG. Although annealing does increase mobility of the sampler,"
N10-1082,D08-1036,0,0.10051,"ng large blocks is computationally expensive. Past work for clustering models maintained tractability by using Metropolis-Hastings proposals (Dahl, 2003) or introducing auxiliary variables (Swendsen and Wang, 1987; Liang et al., 2007). In contrast, our type-based sampler simply identifies tractable 580 blocks based on exchangeability. Other methods for learning latent-variable models include EM, variational approximations, and uncollapsed samplers. All of these methods maintain distributions over (or settings of) the latent variables of the model and update the representation iteratively (see Gao and Johnson (2008) for an overview in the context of POS induction). However, these methods are at the core all token-based, since they only update variables in a single example at a time.8 Blocking variables by type—the key idea of this paper—is a fundamental departure from tokenbased methods. Though type-based changes have also been proposed (Brown et al., 1992; Stolcke and Omohundro, 1994), these methods operated greedily, and in Section 5.1, we saw that being greedy led to more brittle results. By working in a sampling framework, we were able bring type-based changes to fruition. 8 While EM technically upda"
N10-1082,P07-1094,0,0.108526,". To combat the problems associated with tokenbased algorithms, we propose a new sampling algorithm that operates on types. Our sampler would, for example, be able to change all occurrences of ((subject verb) object) to (subject (verb object)) in one step. These type-based operations are reminiscent of the type-based grammar operations of early chunkmerge systems (Wolff, 1988; Stolcke and Omohundro, 1994), but we work within a sampling framework for increased robustness. In NLP, perhaps the the most simple and popular sampler is the token-based Gibbs sampler,1 used in Goldwater et al. (2006), Goldwater and Griffiths (2007), and many others. By sampling only one 1 In NLP, this is sometimes referred to as simply the collapsed Gibbs sampler. 573 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 573–581, c Los Angeles, California, June 2010. 2010 Association for Computational Linguistics 2 Basic Idea via a Motivating Example The key technical problem we solve in this paper is finding a block of variables which are both highly coupled and yet tractable to sample jointly. This section illustrates the main idea behind type-based sampling on a small word segmentatio"
N10-1082,P06-1085,0,0.249744,"resulting in slow mixing. To combat the problems associated with tokenbased algorithms, we propose a new sampling algorithm that operates on types. Our sampler would, for example, be able to change all occurrences of ((subject verb) object) to (subject (verb object)) in one step. These type-based operations are reminiscent of the type-based grammar operations of early chunkmerge systems (Wolff, 1988; Stolcke and Omohundro, 1994), but we work within a sampling framework for increased robustness. In NLP, perhaps the the most simple and popular sampler is the token-based Gibbs sampler,1 used in Goldwater et al. (2006), Goldwater and Griffiths (2007), and many others. By sampling only one 1 In NLP, this is sometimes referred to as simply the collapsed Gibbs sampler. 573 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 573–581, c Los Angeles, California, June 2010. 2010 Association for Computational Linguistics 2 Basic Idea via a Motivating Example The key technical problem we solve in this paper is finding a block of variables which are both highly coupled and yet tractable to sample jointly. This section illustrates the main idea behind type-based samp"
N10-1082,N06-1041,1,0.561614,"ing on marginal like4 A site could be sampled more than once if it belonged to more than one type block during the iteration (recall that types depend on z and thus could change during sampling). lihood (3) and accuracy for our three models: • HMM: We learned a K = 45 state HMM on the Wall Street Journal (WSJ) portion of the Penn Treebank (49208 sentences, 45 tags) for part-ofspeech induction. We fixed αr to 0.1 and µr to uniform for all r. For accuracy, we used the standard metric based on greedy mapping, where each state is mapped to the POS tag that maximizes the number of correct matches (Haghighi and Klein, 2006). We did not use a tagging dictionary. • USM: We learned a USM model on the Bernstein-Ratner corpus from the CHILDES database used in Goldwater et al. (2006) (9790 sentences) for word segmentation. We fixed α0 to 0.1. The base distribution µ0 penalizes the length of words (see Goldwater et al. (2009) for details). For accuracy, we used word token F1 . • PTSG: We learned a PTSG model on sections 2– 21 of the WSJ treebank.5 For accuracy, we used EVALB parsing F1 on section 22.6 Note this is a supervised task with latent-variables, whereas the other two are purely unsupervised. 5.1 Basic Comparis"
N10-1082,P06-1055,1,0.111558,"Missing"
N10-1082,P09-2012,0,0.169842,"trong type-based coupling (e.g., all instances of a word should be tagged similarly). The type-based sampler we will present is designed exactly to tackle this coupling, which we argue is stronger and more important to deal with in unsupervised learning. Figure 1 depicts the updates made by each of the three samplers. We tested our sampler on three models: a Bayesian HMM for part-of-speech induction (Goldwater and Griffiths, 2007), a nonparametric Bayesian model for word segmentation (Goldwater et al., 2006), and a nonparametric Bayesian model of tree substitution grammars (Cohn et al., 2009; Post and Gildea, 2009). Empirically, we find that typebased sampling improves performance and is less sensitive to initialization (Section 5). The ascending factorial function arises from marginalizing Dirichlet distributions and is responsible the rich-gets-richer phenomenon: the larger n is, more we gain by increasing it.  n possibilithe particular z uniformly out of the m ties. Figure 2(b) shows the effectiveness of this typebased sampler. This simple example exposes the fundamental challenge of multimodality in unsupervised learning. Both m = 0 and m = n are modes due to the rich-gets-richer property which ari"
N18-1169,Q18-1031,1,0.0566574,"encodes the sequence of attribute markers a(xtgt , v tgt ) with another RNN. The RNN decoder uses the concatenation of this vector and the content embedding to generate y. D ELETE A ND R ETRIEVE combines the advantages of T EMPLATE BASED and D ELE TE O NLY . Unlike T EMPLATE BASED , D ELETE A ND R ETRIEVE can pick a better place to insert the given attribute markers, and can add or remove function words to ensure grammaticality. Compared to D ELETE O NLY, D ELETE A ND R ETRIEVE has a stronger inductive bias towards using target attribute markers that are likely to fit in the current context. Guu et al. (2018) showed that retrieval strategies like ours can help neural generative models. Finally, D ELETE A ND R ETRIEVE gives us finer control over the output; for example, we can control the degree of sentiment by deciding whether to add “good” or “fantastic” based on the retrieved sentence xtgt . 2 Markers are replaced from left to right, in order. If there are not enough markers in xtgt , we use an empty string. 3.4 Training We now describe how to train D ELETE A N D R ETRIEVE and D ELETE O NLY . Recall that at training time, we do not have access to ground truth outputs that express the target attr"
N18-1169,P13-1162,0,0.00862492,"eatures (Globerson and Roweis, 2006), similarly to how adversarial methods are trained to fool an attribute classifier. One difference is that our classifier is fixed, not jointly trained with the model. To conclude, we have described a simple method for text attribute transfer that outperforms previous models based on adversarial training. The main leverage comes from the inductive bias that attributes are usually manifested in localized discriminative phrases. While many prior works on linguistic style analysis confirm our observation that attributes often manifest in idiosyncratic phrases (Recasens et al., 2013; Schwartz et al., 2017; Newman et al., 2003), we recognize the fact that in some problems (e.g., Pavlick and Tetreault (2017)), content and attribute cannot be so cleanly separated along phrase boundaries. Looking forward, a fruitful direction is to develop a notion of attributes more general than n-grams, but with more inductive bias than arbitrary latent vectors. Reproducibility. All code, data, and experiments for this paper are available on the CodaLab platform at https://worksheets. codalab.org/worksheets/ 0xe3eb416773ed4883bb737662b31b4948/. Acknowledgements. This work is supported by t"
N18-1169,K17-1004,0,0.0232906,"Roweis, 2006), similarly to how adversarial methods are trained to fool an attribute classifier. One difference is that our classifier is fixed, not jointly trained with the model. To conclude, we have described a simple method for text attribute transfer that outperforms previous models based on adversarial training. The main leverage comes from the inductive bias that attributes are usually manifested in localized discriminative phrases. While many prior works on linguistic style analysis confirm our observation that attributes often manifest in idiosyncratic phrases (Recasens et al., 2013; Schwartz et al., 2017; Newman et al., 2003), we recognize the fact that in some problems (e.g., Pavlick and Tetreault (2017)), content and attribute cannot be so cleanly separated along phrase boundaries. Looking forward, a fruitful direction is to develop a notion of attributes more general than n-grams, but with more inductive bias than arbitrary latent vectors. Reproducibility. All code, data, and experiments for this paper are available on the CodaLab platform at https://worksheets. codalab.org/worksheets/ 0xe3eb416773ed4883bb737662b31b4948/. Acknowledgements. This work is supported by the DARPA Communicating"
N18-1169,P12-2018,0,0.0381889,"sfered to the target domain from the source domain. More recently, unsupervised machine translation models (Artetxe et al., 2017; Lample et al., 2017) used a cycle loss similar to Jun-Yan et al. (2017) to ensure that the content is preserved during the transformation. These methods often rely on bilinguial word vectors to provide word-for-word translations, which are then finetune by back-translation. Thus they can be used to further improve our results. Our method of detecting attribute markers is reminiscent of Naive Bayes, which is a strong baseline for tasks like sentiment classification (Wang and Manning, 2012). Deleting these attribute markers can be viewed as attacking a Naive Bayes classifier by deleting the most informative features (Globerson and Roweis, 2006), similarly to how adversarial methods are trained to fool an attribute classifier. One difference is that our classifier is fixed, not jointly trained with the model. To conclude, we have described a simple method for text attribute transfer that outperforms previous models based on adversarial training. The main leverage comes from the inductive bias that attributes are usually manifested in localized discriminative phrases. While many p"
N18-1169,Q16-1005,0,\N,Missing
N19-1169,K16-1002,0,0.0584192,"all) in the context of generative image models. However, they rely on assuming that pref and pmodel can be estimated accurately using the Fr´echet Inception Distance (FID) (Heusel et al., 2017). HUSE avoids such assumptions and instead directly leverages human judgments, resulting in a simple and reliable metric more suitable for use as a gold-standard. Estimating optimal classification error. Evaluating a model by estimating its optimal classification error has been considered by several earlier works (Olsson et al., 2018; Kannan and Vinyals, 2016; Li et al., 2017; Bruni and Fernandez, 2017; Bowman et al., 2016). However, these methods have focused on classifying sentences directly, which is quite challenging to do reliably. Existing adversarial evaluation methods do not yet reliably outperform human classification (Kannan and Vinyals, 2016; Bruni and Fernandez, 2017). We propose the use of both human evaluation and model probabilities as part of the adversarial evaluation framework, and demonstrate that the resulting classifier reliably outperforms humans and captures both the sample quality and diversity of a model. Distributional divergence estimation. Our proposed evaluation metric is closely rel"
N19-1169,W17-5534,0,0.21365,"they call precision and recall) in the context of generative image models. However, they rely on assuming that pref and pmodel can be estimated accurately using the Fr´echet Inception Distance (FID) (Heusel et al., 2017). HUSE avoids such assumptions and instead directly leverages human judgments, resulting in a simple and reliable metric more suitable for use as a gold-standard. Estimating optimal classification error. Evaluating a model by estimating its optimal classification error has been considered by several earlier works (Olsson et al., 2018; Kannan and Vinyals, 2016; Li et al., 2017; Bruni and Fernandez, 2017; Bowman et al., 2016). However, these methods have focused on classifying sentences directly, which is quite challenging to do reliably. Existing adversarial evaluation methods do not yet reliably outperform human classification (Kannan and Vinyals, 2016; Bruni and Fernandez, 2017). We propose the use of both human evaluation and model probabilities as part of the adversarial evaluation framework, and demonstrate that the resulting classifier reliably outperforms humans and captures both the sample quality and diversity of a model. Distributional divergence estimation. Our proposed evaluation"
N19-1169,P18-1060,1,0.950197,"ce the classification error of distinguishing reference and generated text based on human judgment scores and model probabilities. HUSE identifies samples with defects in quality (Sharon has stroke . . .) and diversity (Cleared coach facing . . .). Introduction Generating text is a core part of many NLP tasks such as image captioning (Lin et al., 2014), opendomain dialogue (Sordoni et al., 2015), story generation (Roemmele, 2016), and summarization (Nallapati et al., 2016). However, proper evaluation of natural language generation has proven difficult (Liu et al., 2016; Novikova et al., 2017; Chaganty et al., 2018). A good evaluation metric should not only capture the quality of generation, but also the diversity of generation, which is especially crucial for creative, open-ended tasks like dialogue or story generation. Human evaluation, which is often viewed as the gold standard evaluation, captures quality but fails to capture diversity. As an example, for language modeling, a model that directly plagiarizes sentences from the training set would pass the human quality bar but would have zero generalization ability and thus have inadequate diversity. On the other hand, statistical evaluation—i.e., perp"
N19-1169,P15-2073,0,0.0278645,"s of diversity. The importance of diverse responses has previously been acknowledged for summarization (Nenkova et al., 2007) and information retrieval (Clarke et al., 2008). Our work differs in considering a single evaluation measure that captures quality and diversity applicable to any generation task. Automated metrics based on n-gram overlap such as BLEU, METEOR, ROUGE (Papineni et al., 2002; Lavie and Denkowski, 2009; Lin and Rey, 2004) work well for machine translation but do not generalize well to domains with a diverse spectrum of correct responses. While variants (Sun and Zhou, 2012; Galley et al., 2015; Shima 1696 and Mitamura, 2011) have adapted such metrics to high entropy generative environments, they are still significantly inferior to the human judgments they attempt to mimic. Caccia et al. (2018) recently examined the diversity and quality tradeoffs for different language model architectures on synthetic datasets. However, as their approach relies on measuring loglikelihoods under both the model and reference distributions, it cannot be applied to real data where pref is unavailable. Our main conceptual contribution overcomes this by showing that HJ is an acceptable proxy for pref . S"
N19-1169,D18-1443,0,0.018002,"We use HUSE to evaluate three different types of single-sentence natural language generation tasks: (i) unconditional and high entropy (language modeling); (ii) conditional and high entropy (story generation, chit-chat dialogue); and (iii) conditional and low entropy (summarization). We show that HUSE provides a direct and interpretable measure of diversity on high-entropy tasks, while also serving as a useful model diagnostic on lowentropy ones. The four tasks along with the datasets and models are as follows: • Summarization: Giganews story to headline dataset and the pre-trained model from Gehrmann et al. (2018). The dataset consists of 3.8 million news story-headline pairs. Examples from this dataset are shown in Table 2. • Story generation: Last sentence generation for ROC stories (Mostafazadeh et al., 2016) consisting of 96,198 examples of partially written four-sentence stories as input, and a single sentence which completes the story as the target. We use a standard OpenNMT model with global attention (Klein et al., 2017). • Chit-chat dialogue: Two-turn chit-chat dialogue dataset consisting of 37.3 million comment-response pairs from Reddit (Appendix A.4). Comments are generally short (5–15 toke"
N19-1169,P17-4012,0,0.0443734,"Missing"
N19-1169,N16-1014,0,0.0717064,"hile perplexity and n-gram counts can in principle evaluate diversity, their practical implementations suffer from serious drawbacks. When human evaluation and perplexity are both evaluated, they are almost always done on separate models—human evaluations are done on beamsearched output, while perplexity is computed on the softmax outputs. This makes it appear as if the models can simultaneously generate high quality outputs while also being diverse, when in fact they can only be one at a time based on whether they sample or run beam search. On the other hand, n-gram diversity was proposed by Li et al. (2016) to identify models with the generic utterance problem where models repeat phrases such as ‘I don’t know’. Unfortunately, n-gram diversity is computed across contexts by counting the number of unique n-grams generated, and so does not measure a model’s ability to generate multiple valid utterances at any single context. In particular, a model which only outputs a single memorized utterance per context (e.g., via memorization or retrieval) can still have high n-gram diversity as long as the memorized sentences differ across contexts. Finally, all existing diversity measures are computed separat"
N19-1169,N18-1169,1,0.818423,"distinguishable from reference) to 1.0 (indistinguishable from reference) where the implied classification error is HUSE/2. HUSE-D may exceed 1.0 with small sample sizes when HUSE-Q &gt; HUSE. under diverse. As a non-neural baseline, we also consider retrieval based models based on Apache solr on a few tasks. For this approach, we retrieve the single most relevant response from the training set using the BM25 similarity metric on inputs. Such models are known to perform well in tasks with complex outputs such as program generation (Hayati et al., 2018; Hashimoto et al., 2018) and style transfer (Li et al., 2018). For cost reasons, we did not measure certain combinations of task and generation mechanisms. We did not measure retrieval for chit-chat dialogue, as we observed its outputs were lower quality than a low-temperature neural model. We also did not anneal language models, as the generation quality from the language model was already high, and our goal was to show that they achieved high HUSE. Our set of measurements, while not comprehensive, generally covers the available qualitydiversity tradeoffs for conditional tasks. Finally, we collect human judgments HJ(x, y) as per Section 4.1 where we qu"
N19-1169,D17-1230,0,0.0363427,"d quality (which they call precision and recall) in the context of generative image models. However, they rely on assuming that pref and pmodel can be estimated accurately using the Fr´echet Inception Distance (FID) (Heusel et al., 2017). HUSE avoids such assumptions and instead directly leverages human judgments, resulting in a simple and reliable metric more suitable for use as a gold-standard. Estimating optimal classification error. Evaluating a model by estimating its optimal classification error has been considered by several earlier works (Olsson et al., 2018; Kannan and Vinyals, 2016; Li et al., 2017; Bruni and Fernandez, 2017; Bowman et al., 2016). However, these methods have focused on classifying sentences directly, which is quite challenging to do reliably. Existing adversarial evaluation methods do not yet reliably outperform human classification (Kannan and Vinyals, 2016; Bruni and Fernandez, 2017). We propose the use of both human evaluation and model probabilities as part of the adversarial evaluation framework, and demonstrate that the resulting classifier reliably outperforms humans and captures both the sample quality and diversity of a model. Distributional divergence estimati"
N19-1169,D16-1230,0,0.351717,"robability (pmodel) Figure 1: HUSE is twice the classification error of distinguishing reference and generated text based on human judgment scores and model probabilities. HUSE identifies samples with defects in quality (Sharon has stroke . . .) and diversity (Cleared coach facing . . .). Introduction Generating text is a core part of many NLP tasks such as image captioning (Lin et al., 2014), opendomain dialogue (Sordoni et al., 2015), story generation (Roemmele, 2016), and summarization (Nallapati et al., 2016). However, proper evaluation of natural language generation has proven difficult (Liu et al., 2016; Novikova et al., 2017; Chaganty et al., 2018). A good evaluation metric should not only capture the quality of generation, but also the diversity of generation, which is especially crucial for creative, open-ended tasks like dialogue or story generation. Human evaluation, which is often viewed as the gold standard evaluation, captures quality but fails to capture diversity. As an example, for language modeling, a model that directly plagiarizes sentences from the training set would pass the human quality bar but would have zero generalization ability and thus have inadequate diversity. On th"
N19-1169,P17-1103,0,0.581413,"rence distribution or the model (Section 2). If a model generates gibberish (low quality), the optimal discriminator can classify these accurately as coming from the model. If the reference distribution contains sentences the model cannot generate (low diversity), the optimal discriminator can classify these accurately as coming from the reference. Unfortunately, the optimal discriminator is unavailable. Human discriminators cannot capture diversity effectively, and learned discriminators— e.g., from a Generative Adversarial Network (Goodfellow et al., 2014) or one trained on human judgments (Lowe et al., 2017)—are too unreliable to use for rigorous evaluation. Our key result (Section 3) is based on the observation that the optimal classifier depends only on two numbers: the probability of a sentence under the model and the probability under the reference distribution. The former can be computed directly from the model, and we show that the latter can be well-approximated by human judgment scores. The resulting two-dimensional space is illustrated in Figure 1. We apply a simple k-nearest neighbor classifier in this space and define Human Unified with Statistical Evaluation (HUSE) as twice the leave-"
N19-1169,D18-1111,0,0.0249699,"SE-D). The scale for HUSE and HUSE-Q ranges from 0.0 (completely distinguishable from reference) to 1.0 (indistinguishable from reference) where the implied classification error is HUSE/2. HUSE-D may exceed 1.0 with small sample sizes when HUSE-Q &gt; HUSE. under diverse. As a non-neural baseline, we also consider retrieval based models based on Apache solr on a few tasks. For this approach, we retrieve the single most relevant response from the training set using the BM25 similarity metric on inputs. Such models are known to perform well in tasks with complex outputs such as program generation (Hayati et al., 2018; Hashimoto et al., 2018) and style transfer (Li et al., 2018). For cost reasons, we did not measure certain combinations of task and generation mechanisms. We did not measure retrieval for chit-chat dialogue, as we observed its outputs were lower quality than a low-temperature neural model. We also did not anneal language models, as the generation quality from the language model was already high, and our goal was to show that they achieved high HUSE. Our set of measurements, while not comprehensive, generally covers the available qualitydiversity tradeoffs for conditional tasks. Finally, we c"
N19-1169,N16-1098,0,0.0199692,"eneration, chit-chat dialogue); and (iii) conditional and low entropy (summarization). We show that HUSE provides a direct and interpretable measure of diversity on high-entropy tasks, while also serving as a useful model diagnostic on lowentropy ones. The four tasks along with the datasets and models are as follows: • Summarization: Giganews story to headline dataset and the pre-trained model from Gehrmann et al. (2018). The dataset consists of 3.8 million news story-headline pairs. Examples from this dataset are shown in Table 2. • Story generation: Last sentence generation for ROC stories (Mostafazadeh et al., 2016) consisting of 96,198 examples of partially written four-sentence stories as input, and a single sentence which completes the story as the target. We use a standard OpenNMT model with global attention (Klein et al., 2017). • Chit-chat dialogue: Two-turn chit-chat dialogue dataset consisting of 37.3 million comment-response pairs from Reddit (Appendix A.4). Comments are generally short (5–15 tokens) and cover a single topic (e.g. given “wow how did i not notice that”, the response is “you were focusing on other things its understandable”). We train a convolutional model using fairseq (Gehring e"
N19-1169,K16-1028,0,0.0151775,"Australian open Model Generations Agassi bows out of Australian open Sharon has stroke for stroke Model Probability (pmodel) Figure 1: HUSE is twice the classification error of distinguishing reference and generated text based on human judgment scores and model probabilities. HUSE identifies samples with defects in quality (Sharon has stroke . . .) and diversity (Cleared coach facing . . .). Introduction Generating text is a core part of many NLP tasks such as image captioning (Lin et al., 2014), opendomain dialogue (Sordoni et al., 2015), story generation (Roemmele, 2016), and summarization (Nallapati et al., 2016). However, proper evaluation of natural language generation has proven difficult (Liu et al., 2016; Novikova et al., 2017; Chaganty et al., 2018). A good evaluation metric should not only capture the quality of generation, but also the diversity of generation, which is especially crucial for creative, open-ended tasks like dialogue or story generation. Human evaluation, which is often viewed as the gold standard evaluation, captures quality but fails to capture diversity. As an example, for language modeling, a model that directly plagiarizes sentences from the training set would pass the huma"
N19-1169,D17-1238,0,0.100462,"Missing"
N19-1169,P02-1040,0,0.106398,"the training set would pass the human quality bar but would have zero generalization ability and thus have inadequate diversity. On the other hand, statistical evaluation—i.e., perplexity on a reference test set—captures diversity, as it ensures a model must assign reasonable probability to novel sentences, but perplexity provides an inadequate measure of quality (Theis et al., 2015). For example, modifying a perfect model by removing its ability to generate even a single test sentence results in infinite perplexity even though the model is still near-perfect. Automatic metrics such as BLEU (Papineni et al., 2002) and ROUGE (Lin and Rey, 2004) capture quality better than perplexity but still correlate poorly with human evaluation and fail to capture diversity (Novikova et al., 2017; Chaganty et al., 2018). Existing approaches to combining statistical and human evaluation have been ad-hoc, leading to misleading performance measures. A common approach is to measure diversity through the perplexity of a probabilistic model and quality through human evaluation on beam-searched out1689 Proceedings of NAACL-HLT 2019, pages 1689–1701 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Comput"
N19-1169,W11-2405,0,0.0801657,"Missing"
N19-1169,N15-1020,0,0.0299004,"coach facing another grilling from British swim bosses Agassi withdraws from Australian open Model Generations Agassi bows out of Australian open Sharon has stroke for stroke Model Probability (pmodel) Figure 1: HUSE is twice the classification error of distinguishing reference and generated text based on human judgment scores and model probabilities. HUSE identifies samples with defects in quality (Sharon has stroke . . .) and diversity (Cleared coach facing . . .). Introduction Generating text is a core part of many NLP tasks such as image captioning (Lin et al., 2014), opendomain dialogue (Sordoni et al., 2015), story generation (Roemmele, 2016), and summarization (Nallapati et al., 2016). However, proper evaluation of natural language generation has proven difficult (Liu et al., 2016; Novikova et al., 2017; Chaganty et al., 2018). A good evaluation metric should not only capture the quality of generation, but also the diversity of generation, which is especially crucial for creative, open-ended tasks like dialogue or story generation. Human evaluation, which is often viewed as the gold standard evaluation, captures quality but fails to capture diversity. As an example, for language modeling, a mode"
N19-1169,P12-2008,0,0.03248,". Related evaluations of diversity. The importance of diverse responses has previously been acknowledged for summarization (Nenkova et al., 2007) and information retrieval (Clarke et al., 2008). Our work differs in considering a single evaluation measure that captures quality and diversity applicable to any generation task. Automated metrics based on n-gram overlap such as BLEU, METEOR, ROUGE (Papineni et al., 2002; Lavie and Denkowski, 2009; Lin and Rey, 2004) work well for machine translation but do not generalize well to domains with a diverse spectrum of correct responses. While variants (Sun and Zhou, 2012; Galley et al., 2015; Shima 1696 and Mitamura, 2011) have adapted such metrics to high entropy generative environments, they are still significantly inferior to the human judgments they attempt to mimic. Caccia et al. (2018) recently examined the diversity and quality tradeoffs for different language model architectures on synthetic datasets. However, as their approach relies on measuring loglikelihoods under both the model and reference distributions, it cannot be applied to real data where pref is unavailable. Our main conceptual contribution overcomes this by showing that HJ is an acceptab"
N19-1172,S17-2011,0,0.145133,"egotiator” and getting “negotiator am just a woman trying to peace her life back together.”. Therefore, we use a sequence-tosequence model to smooth the edited sentence (R ETRIEVE +S WAP +T OPIC +S MOOTHER). We smooth the sentence by deleting words around the topic word and train a model to fill in the blank. The smoother is trained in a similar fashion to denoising autoencoders: we delete immediate neighbors of a word in a sentence, and ask the model to reconstruct the sentence by predicting missing neighbors. A training example is shown below: We use the pun dataset from 2017 SemEval task7 (Doogan et al., 2017). The dataset contains 1099 human-written puns annotated with pun words and alternative words, from which we take 219 for development. We use BookCorpus (Zhu et al., 2015) as the generic corpus for retrieval and training various components of our system. 4.1 4.2 Datasets Analysis of the Surprisal Principle We evaluate the surprisal principle by analyzing how well the local-global surprisal score (Equation (4)) predicts funniness rated by humans. We first give a brief overview of previous computational accounts of humor, and then analyze the correlation between each metric and human ratings. Pr"
N19-1172,P18-1082,0,0.0328441,"ion baseline. 1 Pun word: dyed. Alternative word: died. Figure 1: An illustration of a homophonic pun. The pun word appears in the sentence, while the alternative word, which has the same pronunciation but different meaning, is implicated. The local context refers to the immediate words around the pun word, whereas the global context refers to the whole sentence. Introduction Generating creative content is a key requirement in many natural language generation tasks such as poetry generation (Manurung et al., 2000; Ghazvininejad et al., 2016), story generation (Meehan, 1977; Peng et al., 2018; Fan et al., 2018; Yao et al., 2019), and social chatbots (Weizenbaum, 1966; Hao et al., 2018). In this paper, we explore creative generation with a focus on puns. We follow the definition of puns in Aarons (2017); Miller et al. (2017): “A pun is a form of wordplay in which one sign (e.g., a word or a phrase) suggests two or more meanings by exploiting polysemy, homonymy, or phonological ∗ Equal contribution. similarity to another sign, for an intended humorous or rhetorical effect.” We focus on a typical class of puns where the ambiguity comes from two (near) homophones. Consider the example in Figure 1: “Yes"
N19-1172,D16-1126,0,0.0306005,"pproach to improve both grammaticality and diversity of the generated text. Shen et al. (2017); Fu et al. (2018) explored adversarial training to manipulate the style of a sentence. Our neural smoother is also closely related to Li et al. (2018)’s delete-retrieve-edit approach to text style transfer. Creative generation is more challenging as it requires both formality (e.g., grammaticality, rhythm, and rhyme) and novelty. Therefore, many works (including us) impose strong constraints on the generative process, such as Petrovic and Matthews (2013); Valitutti et al. (2013) for joke generation, Ghazvininejad et al. (2016) for poetry generation, and Yao et al. (2019) for storytelling. 6 Conclusion In this paper, we tackled pun generation by developing and exploring a local-global surprisal principle. We show that a simple instantiation based on only a language model trained on non-humorous text is effective at detecting puns (though is not fine-grained enough to detect the degree of funniness within puns). To generate puns, we operationalize the surprisal principle with a retrieve-and-edit framework to create contrast in the amount of surprise in local and global contexts. While we improve beyond current techni"
N19-1172,Q18-1031,1,0.886836,"Missing"
N19-1172,N18-1169,1,0.849493,"between words, and word types. Our approach also relies on specific pun structures; we have proposed and operationalized a local-global surprisal principle for pun generation. 5.3 Creative text generation Our work is also built upon generic text generation techniques, in particular recent neural generation models. Hashimoto et al. (2018) developed a retrieve-and-edit approach to improve both grammaticality and diversity of the generated text. Shen et al. (2017); Fu et al. (2018) explored adversarial training to manipulate the style of a sentence. Our neural smoother is also closely related to Li et al. (2018)’s delete-retrieve-edit approach to text style transfer. Creative generation is more challenging as it requires both formality (e.g., grammaticality, rhythm, and rhyme) and novelty. Therefore, many works (including us) impose strong constraints on the generative process, such as Petrovic and Matthews (2013); Valitutti et al. (2013) for joke generation, Ghazvininejad et al. (2016) for poetry generation, and Yao et al. (2019) for storytelling. 6 Conclusion In this paper, we tackled pun generation by developing and exploring a local-global surprisal principle. We show that a simple instantiation"
N19-1172,S17-2005,0,0.170731,"Missing"
N19-1172,P12-1101,0,0.0733225,"s to connect the topic word able (indicating ambiguity), the pun meaning is with the seed sentence in a grammatical way, e.g., unexpected based on the local context. Second, “the negotiator is just a woman trying to peace the local-global surprisal contrast requires the pun her life back together.” (the part rewritten by the word to be well supported in the global context. smoother is underlined). Given the anomalous nature of puns, we also consider a metric for unusualness based 4 Experiments on normalized log-probabilities under a language We first evaluate how well our surprisal prinmodel (Pauls and Klein, 2012): ciple predicts the funniness of sentences per! n Y 1 def ceived by humans (Section 4.2), and then comUnusualness = − log p(x1 , . . . , xn )/ p(xi ) . n pare our pun generation system and its variai=1 2 (6) Path similarity is a score between 0 and 1 that is inthe man slowly walked towards the woods . versely proportional to the shortest distance between two word senses in WordNet. 3 Pronouns are mapped to the synset person.n.01. Implementation details. Both ambiguity and distinctiveness are based on a generative model 1737 Type Pun Swap-pun Non-pun S EM E VAL Example K AO Count Funniness Cou"
N19-1172,W18-1505,1,0.835385,"of a neural generation baseline. 1 Pun word: dyed. Alternative word: died. Figure 1: An illustration of a homophonic pun. The pun word appears in the sentence, while the alternative word, which has the same pronunciation but different meaning, is implicated. The local context refers to the immediate words around the pun word, whereas the global context refers to the whole sentence. Introduction Generating creative content is a key requirement in many natural language generation tasks such as poetry generation (Manurung et al., 2000; Ghazvininejad et al., 2016), story generation (Meehan, 1977; Peng et al., 2018; Fan et al., 2018; Yao et al., 2019), and social chatbots (Weizenbaum, 1966; Hao et al., 2018). In this paper, we explore creative generation with a focus on puns. We follow the definition of puns in Aarons (2017); Miller et al. (2017): “A pun is a form of wordplay in which one sign (e.g., a word or a phrase) suggests two or more meanings by exploiting polysemy, homonymy, or phonological ∗ Equal contribution. similarity to another sign, for an intended humorous or rhetorical effect.” We focus on a typical class of puns where the ambiguity comes from two (near) homophones. Consider the example"
N19-1172,P13-2041,0,0.487445,"urprisal principle is also related to studies in psycholinguistics on the relation between surprisal and human comprehension (Levy, 2013; Levy and Gibson, 2013). Our study suggests it could be a fruitful direction to formally study the relationship between human perception of surprisal and humor. 5.2 Humor generation Early approaches to joke generation (Binsted, 1996; Ritchie, 2005) largely rely on templates for specific types of puns. For example, JAPE (Binsted, 1996) generates noun phrase puns as question-answer pairs, e.g., “What do you call a [murderer] with [fiber]? A [cereal] [killer].” Petrovic and Matthews (2013) fill in a joke template based on word similarity and uncommonness. Similar to our editing approach, Valitutti et al. (2013) substitutes a word with a taboo word based on form similarity and local coherence to generate adult jokes. Recently, Yu et al. (2018) generates puns from a generic neural language model by simultaneously conditioning on two meanings. Most of these approaches leverage some assumptions of joke structures, e.g., incongruity, relations between words, and word types. Our approach also relies on specific pun structures; we have proposed and operationalized a local-global surpr"
N19-1172,W05-1614,0,0.294428,"for puns focuses on ambiguity between two concepts and the heterogeneity nature of the ambiguity. Our surprisal principle further formalizes unexpectedness (local surprisal) and incongruity resolution (global association). The surprisal principle is also related to studies in psycholinguistics on the relation between surprisal and human comprehension (Levy, 2013; Levy and Gibson, 2013). Our study suggests it could be a fruitful direction to formally study the relationship between human perception of surprisal and humor. 5.2 Humor generation Early approaches to joke generation (Binsted, 1996; Ritchie, 2005) largely rely on templates for specific types of puns. For example, JAPE (Binsted, 1996) generates noun phrase puns as question-answer pairs, e.g., “What do you call a [murderer] with [fiber]? A [cereal] [killer].” Petrovic and Matthews (2013) fill in a joke template based on word similarity and uncommonness. Similar to our editing approach, Valitutti et al. (2013) substitutes a word with a taboo word based on form similarity and local coherence to generate adult jokes. Recently, Yu et al. (2018) generates puns from a generic neural language model by simultaneously conditioning on two meanings"
N19-1172,N18-5020,0,0.0189354,"ation of a homophonic pun. The pun word appears in the sentence, while the alternative word, which has the same pronunciation but different meaning, is implicated. The local context refers to the immediate words around the pun word, whereas the global context refers to the whole sentence. Introduction Generating creative content is a key requirement in many natural language generation tasks such as poetry generation (Manurung et al., 2000; Ghazvininejad et al., 2016), story generation (Meehan, 1977; Peng et al., 2018; Fan et al., 2018; Yao et al., 2019), and social chatbots (Weizenbaum, 1966; Hao et al., 2018). In this paper, we explore creative generation with a focus on puns. We follow the definition of puns in Aarons (2017); Miller et al. (2017): “A pun is a form of wordplay in which one sign (e.g., a word or a phrase) suggests two or more meanings by exploiting polysemy, homonymy, or phonological ∗ Equal contribution. similarity to another sign, for an intended humorous or rhetorical effect.” We focus on a typical class of puns where the ambiguity comes from two (near) homophones. Consider the example in Figure 1: “Yesterday I accidentally swallowed some food coloring. The doctor says I’m OK, b"
N19-1172,P13-2044,0,0.708842,"y, 2013; Levy and Gibson, 2013). Our study suggests it could be a fruitful direction to formally study the relationship between human perception of surprisal and humor. 5.2 Humor generation Early approaches to joke generation (Binsted, 1996; Ritchie, 2005) largely rely on templates for specific types of puns. For example, JAPE (Binsted, 1996) generates noun phrase puns as question-answer pairs, e.g., “What do you call a [murderer] with [fiber]? A [cereal] [killer].” Petrovic and Matthews (2013) fill in a joke template based on word similarity and uncommonness. Similar to our editing approach, Valitutti et al. (2013) substitutes a word with a taboo word based on form similarity and local coherence to generate adult jokes. Recently, Yu et al. (2018) generates puns from a generic neural language model by simultaneously conditioning on two meanings. Most of these approaches leverage some assumptions of joke structures, e.g., incongruity, relations between words, and word types. Our approach also relies on specific pun structures; we have proposed and operationalized a local-global surprisal principle for pun generation. 5.3 Creative text generation Our work is also built upon generic text generation techniqu"
N19-1172,P18-1153,0,0.23567,"ndicates one interpretation: the person is colored inside by food coloring. On the other hand, an alternative word (“died”) is implied by the context for another interpretation: the person is sad due to the accident. Current approaches to text generation require lots of training data, but there is no large corpus of puns. Even such a corpus existed, learning the distribution of existing data and sampling from it is unlikely to lead to truly novel, creative sentences. Creative composition requires deviating from the norm, whereas standard generation approaches seek to mimic the norm. Recently, Yu et al. (2018) proposed an unsupervised approach that generates puns from a neural language model by jointly decoding conditioned on both the pun and the alternative words, thus injecting ambiguity to the output sentence. However, Kao et al. (2015) showed that ambiguity alone is insufficient to bring humor; the two mean1734 Proceedings of NAACL-HLT 2019, pages 1734–1744 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics ings must also be supported by distinct sets of words in the sentence. Inspired by Kao et al. (2015), we propose a general principle for puns whi"
P06-1096,J92-4003,0,0.0353914,"ion in isolation. This apparent degradation causes no problems, because when des should actually be translated to of, these words are usually embedded in larger phrases, in which case the isolated translation probability plays no role. Another example of a related phenomenon is the following: Input B B+L ... ... ... pour cela que for that for this reason j ’ ai i have i vot´e favorablement voted in favour voted in favour . . . Counterintuitively, the phrase pair (j ’ ai, I have) ends up with a very negative weight. The reason behind this is that in French, 7 We also tried using word clusters (Brown et al., 1992) instead of POS but found that POS was more helpful. 766 Features B LANKET B LANKET +L EX B LANKET +L EX +POS , ce mˆ eme zero growth rate , that same croissance z´ ero secure refuge abri sˆ ur (a) (b) (c) +C ONST 32.2 32.5 32.5 Table 4: D EV BLEU score increase resulting from adding constellation features. Figure 3: Three constellation features with example phrase pairs. Constellations (a) and (b) have large positive weights and (c) has a large negative weight. good phrase pairs, we introduce an alignment constellation feature to indicate the presence of a particular alignment constellation.9"
P06-1096,P05-1033,0,0.200868,") the segmentation of the input sentence into phrases, (2) the segmentation of the output sentence into the same number of phrases, and (3) a bijection between the input and output phrases. The feature vector Φ(x, y, h) contains four components: the log probability of the output sentence y under a language model, the score of translating x into y based on a phrase table, a distortion score, and a length penalty.1 In Section 6, we vastly increase the number of features to take advantage of the full power of discriminative training. Another example of this framework is the hierarchical model of Chiang (2005). In this model the correspondence h is a synchronous parse tree 2.2 Perceptron-based training To tune the parameters w of the model, we use the averaged perceptron algorithm (Collins, 2002) because of its efficiency and past success on various NLP tasks (Collins and Roark, 2004; Roark et al., 2004). In principle, w could have been tuned by maximizing conditional probability or maximizing margin. However, these two options require either marginalization or numerical optimization, neither of which is tractable over the space of output sentences y and correspondences h. In contrast, the perceptr"
P06-1096,P04-1015,0,0.026227,"the output sentence y under a language model, the score of translating x into y based on a phrase table, a distortion score, and a length penalty.1 In Section 6, we vastly increase the number of features to take advantage of the full power of discriminative training. Another example of this framework is the hierarchical model of Chiang (2005). In this model the correspondence h is a synchronous parse tree 2.2 Perceptron-based training To tune the parameters w of the model, we use the averaged perceptron algorithm (Collins, 2002) because of its efficiency and past success on various NLP tasks (Collins and Roark, 2004; Roark et al., 2004). In principle, w could have been tuned by maximizing conditional probability or maximizing margin. However, these two options require either marginalization or numerical optimization, neither of which is tractable over the space of output sentences y and correspondences h. In contrast, the perceptron algorithm requires only a decoder that computes f (x; w). Recall the traditional perceptron update rule on an example (xi , yi ) is w ← w + Φ(xi , yt ) − Φ(xi , yp ), (2) where yt = yi is the target output and yp = f (xi ; w) = argmaxy w · Φ(xi , y) is the prediction using th"
P06-1096,W02-1001,0,0.752498,"The feature vector Φ(x, y, h) contains four components: the log probability of the output sentence y under a language model, the score of translating x into y based on a phrase table, a distortion score, and a length penalty.1 In Section 6, we vastly increase the number of features to take advantage of the full power of discriminative training. Another example of this framework is the hierarchical model of Chiang (2005). In this model the correspondence h is a synchronous parse tree 2.2 Perceptron-based training To tune the parameters w of the model, we use the averaged perceptron algorithm (Collins, 2002) because of its efficiency and past success on various NLP tasks (Collins and Roark, 2004; Roark et al., 2004). In principle, w could have been tuned by maximizing conditional probability or maximizing margin. However, these two options require either marginalization or numerical optimization, neither of which is tractable over the space of output sentences y and correspondences h. In contrast, the perceptron algorithm requires only a decoder that computes f (x; w). Recall the traditional perceptron update rule on an example (xi , yi ) is w ← w + Φ(xi , yt ) − Φ(xi , yp ), (2) where yt = yi is"
P06-1096,N03-1017,0,0.514782,"as the foundation for most of the work in statistical machine translation (Brown et al., 1994). At the same time, discriminative methods have provided substantial improvements over generative models on a wide range of NLP tasks. They allow one to easily encode domain knowledge in the form of features. Moreover, parameters are tuned to directly minimize error rather than to maximize joint likelihood, which may not correspond well to the task objective. In this paper, we present an end-to-end discriminative approach to machine translation. The proposed system is phrase-based, as in Koehn et al. (2003), but uses an online perceptron training scheme to learn model parameters. Unlike minimum error rate training (Och, 2003), our system is able to exploit large numbers of specific features in the same manner as static reranking systems (Shen et al., 2004; Och et al., 2004). However, unlike static rerankers, our system does not rely on a baseline translation system. Instead, it updates based on its own n-best lists. As parameter 761 Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 761–768, c Sydney, July 2006. 2006 Associatio"
P06-1096,H05-1064,0,0.00570461,"system as we developed it. Note that the D EV set was not used to tune any parameters; tuning was done exclusively on T RAIN. At the end we ran our models once on T EST to get final numbers.2      Figure 1: Given the current prediction (a), there are two possible updates, local (b) and bold (c). Although the bold update (c) reaches the reference translation, a bad correspondence is used. The local update (b) does not reach the reference, but is more reasonable than (c). Discriminative training with hidden variables has been handled in this probabilistic framework (Quattoni et al., 2004; Koo and Collins, 2005), but we choose Equation 3 for efficiency. It turns out that using the Viterbi approximation (which we call bold updating) is not always the best strategy. To appreciate the difficulty, consider the example in Figure 1. Suppose we make the prediction (a) with the current set of parameters. There are often several acceptable output translations y, for example, (b) and (c). Since (c)’s output matches the reference translation, should we update towards (c)? In this case, the answer is negative. The problem with (c) is that the correspondence h contains an incorrect alignment (’, a). However, sinc"
P06-1096,P02-1038,0,0.825922,"Missing"
P06-1096,N04-1021,0,0.0488936,"domain knowledge in the form of features. Moreover, parameters are tuned to directly minimize error rather than to maximize joint likelihood, which may not correspond well to the task objective. In this paper, we present an end-to-end discriminative approach to machine translation. The proposed system is phrase-based, as in Koehn et al. (2003), but uses an online perceptron training scheme to learn model parameters. Unlike minimum error rate training (Och, 2003), our system is able to exploit large numbers of specific features in the same manner as static reranking systems (Shen et al., 2004; Och et al., 2004). However, unlike static rerankers, our system does not rely on a baseline translation system. Instead, it updates based on its own n-best lists. As parameter 761 Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 761–768, c Sydney, July 2006. 2006 Association for Computational Linguistics over input and output sentences, and features include the scores of various productions used in the tree. Given features Φ and a corresponding set of parameters w, a standard classification rule f is to return the highest scoring output sen"
P06-1096,P03-1021,0,0.422148,"criminative methods have provided substantial improvements over generative models on a wide range of NLP tasks. They allow one to easily encode domain knowledge in the form of features. Moreover, parameters are tuned to directly minimize error rather than to maximize joint likelihood, which may not correspond well to the task objective. In this paper, we present an end-to-end discriminative approach to machine translation. The proposed system is phrase-based, as in Koehn et al. (2003), but uses an online perceptron training scheme to learn model parameters. Unlike minimum error rate training (Och, 2003), our system is able to exploit large numbers of specific features in the same manner as static reranking systems (Shen et al., 2004; Och et al., 2004). However, unlike static rerankers, our system does not rely on a baseline translation system. Instead, it updates based on its own n-best lists. As parameter 761 Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 761–768, c Sydney, July 2006. 2006 Association for Computational Linguistics over input and output sentences, and features include the scores of various productions u"
P06-1096,W05-0908,0,0.0723205,"Missing"
P06-1096,P04-1007,0,0.353314,"er a language model, the score of translating x into y based on a phrase table, a distortion score, and a length penalty.1 In Section 6, we vastly increase the number of features to take advantage of the full power of discriminative training. Another example of this framework is the hierarchical model of Chiang (2005). In this model the correspondence h is a synchronous parse tree 2.2 Perceptron-based training To tune the parameters w of the model, we use the averaged perceptron algorithm (Collins, 2002) because of its efficiency and past success on various NLP tasks (Collins and Roark, 2004; Roark et al., 2004). In principle, w could have been tuned by maximizing conditional probability or maximizing margin. However, these two options require either marginalization or numerical optimization, neither of which is tractable over the space of output sentences y and correspondences h. In contrast, the perceptron algorithm requires only a decoder that computes f (x; w). Recall the traditional perceptron update rule on an example (xi , yi ) is w ← w + Φ(xi , yt ) − Φ(xi , yp ), (2) where yt = yi is the target output and yp = f (xi ; w) = argmaxy w · Φ(xi , y) is the prediction using the current parameters"
P06-1096,N04-1023,0,0.415602,"e to easily encode domain knowledge in the form of features. Moreover, parameters are tuned to directly minimize error rather than to maximize joint likelihood, which may not correspond well to the task objective. In this paper, we present an end-to-end discriminative approach to machine translation. The proposed system is phrase-based, as in Koehn et al. (2003), but uses an online perceptron training scheme to learn model parameters. Unlike minimum error rate training (Och, 2003), our system is able to exploit large numbers of specific features in the same manner as static reranking systems (Shen et al., 2004; Och et al., 2004). However, unlike static rerankers, our system does not rely on a baseline translation system. Instead, it updates based on its own n-best lists. As parameter 761 Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 761–768, c Sydney, July 2006. 2006 Association for Computational Linguistics over input and output sentences, and features include the scores of various productions used in the tree. Given features Φ and a corresponding set of parameters w, a standard classification rule f is to return the highest"
P06-1096,P05-1069,0,0.0247617,"onal Linguistics over input and output sentences, and features include the scores of various productions used in the tree. Given features Φ and a corresponding set of parameters w, a standard classification rule f is to return the highest scoring output sentence y, maximizing over correspondences h: feature provides a potential way to initially extract phrases more aggressively and then later downweight undesirable patterns, essentially learning a weighted extraction heuristic. Finally, we use POS features to parameterize a distortion model in a limited distortion decoder (Zens and Ney, 2004; Tillmann and Zhang, 2005). We show that overall, BLEU score increases from 28.4 to 29.6 on French-English. f (x; w) = argmax w · Φ(x, y, h). 2 Approach 2.1 (1) y,h In the phrase-based model, computing the argmax exactly is intractable, so we approximate f with beam decoding. Translation as structured classification Machine translation can be seen as a structured classification task, in which the goal is to learn a mapping from an input (French) sentence x to an output (English) sentence y. Given this setup, discriminative methods allow us to define a broad class of features Φ that operate on (x, y). For example, some"
P06-1096,J93-2003,0,\N,Missing
P06-1096,N04-1033,0,\N,Missing
P08-1088,N04-4038,0,0.00685771,"ogously. The increased accuracy may not be an accident: whether two words are translations is perhaps better characterized directly by how close their latent concepts are, whereas log-probability is more sensitive to perturbations in the source and target spaces. 774 Lexicon Note that the although the corpora here are derived from a parallel corpus, there are no parallel sentences. 8 LDC catalog # 2002E18. 9 LDC catalog # 2004E13. 10 These corpora contain no parallel sentences. 11 We use the Tree Tagger (Schmid, 1994) for all POS tagging except for Arabic, where we use the tagger described in Diab et al. (2004). Setting p0.1 p0.25 p0.33 p0.50 Best-F1 E DIT D IST O RTHO C ONTEXT MCCA 58.6 76.0 91.1 87.2 62.6 81.3 81.3 89.7 61.1 80.1 80.2 89.0 —52.3 65.3 89.7 47.4 55.0 58.0 72.0 1 EN-ES-P EN-ES-W 0.95 Precision 0.9 0.85 0.8 0.75 Table 1: Performance of E DIT D IST and our model with various features sets on EN-ES-W. See section 5. 0.7 0.65 0.6 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 Recall Figure 3: Example precision/recall curve of our system on EN-ES-P and EN-ES-W settings. See section 6.1. all languages pairs except English-Arabic, we extract evaluation lexicons from the Wiktionary online dictionary. As"
P08-1088,W95-0114,0,0.842841,"s. We present the highest confidence system predictions, where the only editing done is to ignore predictions which consist of identical source and target words. D and EN-FR-D, presumably due in part to the lack of orthographic features. However, MCCA still achieved surprising precision at lower recall levels. For instance, at p0.1 , MCCA yielded 60.1 and 70.0 on Chinese and Arabic, respectively. Figure 3 shows the highest-confidence outputs in several languages. 6.4 Comparison To Previous Work There has been previous work in extracting translation pairs from non-parallel corpora (Rapp, 1995; Fung, 1995; Koehn and Knight, 2002), but generally not in as extreme a setting as the one considered here. Due to unavailability of data and specificity in experimental conditions and evaluations, it is not possible to perform exact comparisons. How(a) Example Non-Cognate Pairs health traceability youth report advantages salud rastreabilidad juventud informe ventajas (b) Interesting Incorrect Pairs liberal Kirkhope action Albanians a.m. Netherlands partido Gorsel reacci´ on Bosnia horas Breta˜ na Table 4: System analysis on EN-ES-W: (a) non-cognate pairs proposed by our system, (b) hand-selected represe"
P08-1088,P06-1121,0,0.0217612,"Missing"
P08-1088,W02-0902,0,0.871975,"ihood weights with a simple proxy, the distances between the words’ mean latent concepts: wi,j = A − ||zi∗ − zj∗ ||2 , • EN-ES-P: 1st 100k sentences of text from the parallel English and Spanish Europarl corpus (Koehn, 2005). • EN-CH-D: English: 1st 50k sentences of Xinhua parallel news corpora;8 Chinese: 2nd 50k sentences. • EN-AR-D: English: 1st 50k sentences of 1994 proceedings of UN parallel corpora;9 Arabic: 2nd 50k sentences. • EN-ES-G: English: 100k sentences of English Gigaword; Spanish: 100k sentences of Spanish Gigaword.10 Each experiment requires a lexicon for evaluation. Following Koehn and Knight (2002), we consider lexicons over only noun word types, although this is not a fundamental limitation of our model. We consider a word type to be a noun if its most common tag is a noun in our monolingual corpus.11 For 7 (5) where A is a thresholding constant, zi∗ = E(zi,j |fS (si )) = P 1/2 US> fS (si ), and zj∗ is defined analogously. The increased accuracy may not be an accident: whether two words are translations is perhaps better characterized directly by how close their latent concepts are, whereas log-probability is more sensitive to perturbations in the source and target spaces. 774 Lexicon"
P08-1088,koen-2004-pharaoh,0,0.0112554,"Missing"
P08-1088,2005.mtsummit-papers.11,0,0.0928857,"plicit use is ever made of document or sentence-level alignments. In particular, our method is robust to permutations of the sentences in the corpora. 4 4.2 Experimental Setup In section 5, we present developmental experiments in English-Spanish lexicon induction; experiments 6 Empirically, we obtained much better efficiency and even increased accuracy by replacing these marginal likelihood weights with a simple proxy, the distances between the words’ mean latent concepts: wi,j = A − ||zi∗ − zj∗ ||2 , • EN-ES-P: 1st 100k sentences of text from the parallel English and Spanish Europarl corpus (Koehn, 2005). • EN-CH-D: English: 1st 50k sentences of Xinhua parallel news corpora;8 Chinese: 2nd 50k sentences. • EN-AR-D: English: 1st 50k sentences of 1994 proceedings of UN parallel corpora;9 Arabic: 2nd 50k sentences. • EN-ES-G: English: 100k sentences of English Gigaword; Spanish: 100k sentences of Spanish Gigaword.10 Each experiment requires a lexicon for evaluation. Following Koehn and Knight (2002), we consider lexicons over only noun word types, although this is not a fundamental limitation of our model. We consider a word type to be a noun if its most common tag is a noun in our monolingual co"
P08-1088,P95-1050,0,0.779727,"inese systems. We present the highest confidence system predictions, where the only editing done is to ignore predictions which consist of identical source and target words. D and EN-FR-D, presumably due in part to the lack of orthographic features. However, MCCA still achieved surprising precision at lower recall levels. For instance, at p0.1 , MCCA yielded 60.1 and 70.0 on Chinese and Arabic, respectively. Figure 3 shows the highest-confidence outputs in several languages. 6.4 Comparison To Previous Work There has been previous work in extracting translation pairs from non-parallel corpora (Rapp, 1995; Fung, 1995; Koehn and Knight, 2002), but generally not in as extreme a setting as the one considered here. Due to unavailability of data and specificity in experimental conditions and evaluations, it is not possible to perform exact comparisons. How(a) Example Non-Cognate Pairs health traceability youth report advantages salud rastreabilidad juventud informe ventajas (b) Interesting Incorrect Pairs liberal Kirkhope action Albanians a.m. Netherlands partido Gorsel reacci´ on Bosnia horas Breta˜ na Table 4: System analysis on EN-ES-W: (a) non-cognate pairs proposed by our system, (b) hand-sele"
P08-1088,P06-1072,0,0.0065989,"rd types. If too few word types are matched, learning will not progress quickly; if too many are matched, the model will be swamped with noise. We found that it was helpful to explicitly control the number of edges. Thus, we adopt a bootstrapping-style approach that only permits high confidence edges at first, and then slowly permits more over time. In particular, we compute the optimal full matching, but only retain the highest weighted edges. As we run EM, we gradually increase the number of edges to retain. In our context, bootstrapping has a similar motivation to the annealing approach of Smith and Eisner (2006), which also tries to alter the space of hidden outputs in the E-step over time to facilitate learning in the M-step, though of course the use of bootstrapping in general is quite widespread (Yarowsky, 1995). • EN-ES(FR)-D: English: 1st 50k sentences of Europarl; Spanish (French): 2nd 50k sentences of Europarl.7 Note that even when corpora are derived from parallel sources, no explicit use is ever made of document or sentence-level alignments. In particular, our method is robust to permutations of the sentences in the corpora. 4 4.2 Experimental Setup In section 5, we present developmental exp"
P08-1088,P95-1026,0,0.225036,"Thus, we adopt a bootstrapping-style approach that only permits high confidence edges at first, and then slowly permits more over time. In particular, we compute the optimal full matching, but only retain the highest weighted edges. As we run EM, we gradually increase the number of edges to retain. In our context, bootstrapping has a similar motivation to the annealing approach of Smith and Eisner (2006), which also tries to alter the space of hidden outputs in the E-step over time to facilitate learning in the M-step, though of course the use of bootstrapping in general is quite widespread (Yarowsky, 1995). • EN-ES(FR)-D: English: 1st 50k sentences of Europarl; Spanish (French): 2nd 50k sentences of Europarl.7 Note that even when corpora are derived from parallel sources, no explicit use is ever made of document or sentence-level alignments. In particular, our method is robust to permutations of the sentences in the corpora. 4 4.2 Experimental Setup In section 5, we present developmental experiments in English-Spanish lexicon induction; experiments 6 Empirically, we obtained much better efficiency and even increased accuracy by replacing these marginal likelihood weights with a simple proxy, th"
P08-1100,W01-0713,0,0.0668972,"ding language processing systems such as parsers from limited resources. Early work on inducing grammars via EM encountered two serious obstacles: the inappropriateness of the likelihood objective and the tendency of EM to get stuck in local optima. Without additional constraints on bracketing (Pereira and Shabes, 1992) or on allowable rewrite rules (Carroll and Charniak, 1992), unsupervised grammar learning was ineffective. Since then, there has been a large body of work addressing the flaws of the EM-based approach. Syntactic models empirically more learnable than PCFGs have been developed (Clark, 2001; Klein and Manning, 2004). Smith and Eisner (2005) proposed a new objective function; Smith and Eisner (2006) introduced a new training procedure. Bayesian approaches can also improve performance (Goldwater and Griffiths, 2007; Johnson, 2007; Kurihara and Sato, 2006). Though these methods have improved induction accuracy, at the core they all still involve optimizing non-convex objective functions related to the likelihood of some model, and thus are not completely immune to the difficulties associated with early approaches. It is therefore important to better understand the behavior of unsup"
P08-1100,P07-1094,0,0.0318791,"ency of EM to get stuck in local optima. Without additional constraints on bracketing (Pereira and Shabes, 1992) or on allowable rewrite rules (Carroll and Charniak, 1992), unsupervised grammar learning was ineffective. Since then, there has been a large body of work addressing the flaws of the EM-based approach. Syntactic models empirically more learnable than PCFGs have been developed (Clark, 2001; Klein and Manning, 2004). Smith and Eisner (2005) proposed a new objective function; Smith and Eisner (2006) introduced a new training procedure. Bayesian approaches can also improve performance (Goldwater and Griffiths, 2007; Johnson, 2007; Kurihara and Sato, 2006). Though these methods have improved induction accuracy, at the core they all still involve optimizing non-convex objective functions related to the likelihood of some model, and thus are not completely immune to the difficulties associated with early approaches. It is therefore important to better understand the behavior of unsupervised induction systems in general. In this paper, we take a step back and present a more statistical view of unsupervised learning in the context of grammar induction. We identify four types of error that a system can make:"
P08-1100,P05-1046,1,0.824376,"(x) 6= pθ0 (x) for every θ, θ0 ∈ S where θ 6= θ0 .7 In general, identifiability error is incurred when the set of maximizers of E log pθ (x) is non-identifiable.8 Label symmetry is perhaps the most familiar example of non-identifiability and is intrinsic to models with hidden labels (HMM and PCFG, but not DMV). We can permute the hidden labels without changing the objective function or even the nature of the solution, so there is no reason to prefer one permutation over another. While seemingly benign, this symmetry actually presents a serious challenge in measuring discrepancy (Section 5.1). Grenager et al. (2005) augments an HMM to allow emission from a generic stopword distribution at any position with probability q. Their model would definitely not be identifiable if q were a free parameter, since we can set q to 0 and just mix in the stopword distribution with each of the other emission distributions to obtain a different parameter setting yielding the same overall distribution. This is a case where our notion of desired structure is absent in the likelihood, and a prior over parameters could help break ties. Permutation-invariant distance KL-divergence is a natural measure of discrepancy between t"
P08-1100,D07-1031,0,0.063917,"l optima. Without additional constraints on bracketing (Pereira and Shabes, 1992) or on allowable rewrite rules (Carroll and Charniak, 1992), unsupervised grammar learning was ineffective. Since then, there has been a large body of work addressing the flaws of the EM-based approach. Syntactic models empirically more learnable than PCFGs have been developed (Clark, 2001; Klein and Manning, 2004). Smith and Eisner (2005) proposed a new objective function; Smith and Eisner (2006) introduced a new training procedure. Bayesian approaches can also improve performance (Goldwater and Griffiths, 2007; Johnson, 2007; Kurihara and Sato, 2006). Though these methods have improved induction accuracy, at the core they all still involve optimizing non-convex objective functions related to the likelihood of some model, and thus are not completely immune to the difficulties associated with early approaches. It is therefore important to better understand the behavior of unsupervised induction systems in general. In this paper, we take a step back and present a more statistical view of unsupervised learning in the context of grammar induction. We identify four types of error that a system can make: approximation,"
P08-1100,P04-1061,1,0.965617,"processing systems such as parsers from limited resources. Early work on inducing grammars via EM encountered two serious obstacles: the inappropriateness of the likelihood objective and the tendency of EM to get stuck in local optima. Without additional constraints on bracketing (Pereira and Shabes, 1992) or on allowable rewrite rules (Carroll and Charniak, 1992), unsupervised grammar learning was ineffective. Since then, there has been a large body of work addressing the flaws of the EM-based approach. Syntactic models empirically more learnable than PCFGs have been developed (Clark, 2001; Klein and Manning, 2004). Smith and Eisner (2005) proposed a new objective function; Smith and Eisner (2006) introduced a new training procedure. Bayesian approaches can also improve performance (Goldwater and Griffiths, 2007; Johnson, 2007; Kurihara and Sato, 2006). Though these methods have improved induction accuracy, at the core they all still involve optimizing non-convex objective functions related to the likelihood of some model, and thus are not completely immune to the difficulties associated with early approaches. It is therefore important to better understand the behavior of unsupervised induction systems"
P08-1100,J94-2001,0,0.404717,"enerative training) or even conditional likelihood E log pθ (y |x) (discriminative training). In the remaining sections, we try to study each of the four errors in isolation. In practice, since it is difficult to work with some of the parameter settings that participate in the error decomposition, we use computationally feasible surrogates so that the error under study remains the dominant effect. 4 Approximation error We start by analyzing approximation error, the discrepancy between p∗ and pθ1∗ (the model found by optimizing likelihood), a point which has been dis881 cussed by many authors (Merialdo, 1994; Smith and Eisner, 2005; Haghighi and Klein, 2006).2 To confront the question of specifically how the likelihood diverges from prediction accuracy, we perform the following experiment: we initialize EM with the supervised estimate3 θˆgen = ˆ log pθ (x, y), which acts as a surrogate argmaxθ E ∗ for p . As we run EM, the likelihood increases but the accuracy decreases (Figure 2 shows this trend for the PCFG; the HMM and DMV models behave similarly). We believe that the initial iterations of EM contain valuable information about the incorrect biases of these models. However, EM is changing hundr"
P08-1100,P92-1017,0,0.323103,"iated by increasing the number of training examples. We conduct our analyses on three models: the HMM, the PCFG, and a simple dependency model. 1 Introduction The unsupervised induction of linguistic structure from raw text is an important problem both for understanding language acquisition and for building language processing systems such as parsers from limited resources. Early work on inducing grammars via EM encountered two serious obstacles: the inappropriateness of the likelihood objective and the tendency of EM to get stuck in local optima. Without additional constraints on bracketing (Pereira and Shabes, 1992) or on allowable rewrite rules (Carroll and Charniak, 1992), unsupervised grammar learning was ineffective. Since then, there has been a large body of work addressing the flaws of the EM-based approach. Syntactic models empirically more learnable than PCFGs have been developed (Clark, 2001; Klein and Manning, 2004). Smith and Eisner (2005) proposed a new objective function; Smith and Eisner (2006) introduced a new training procedure. Bayesian approaches can also improve performance (Goldwater and Griffiths, 2007; Johnson, 2007; Kurihara and Sato, 2006). Though these methods have improved induc"
P08-1100,P05-1044,0,0.344424,"s parsers from limited resources. Early work on inducing grammars via EM encountered two serious obstacles: the inappropriateness of the likelihood objective and the tendency of EM to get stuck in local optima. Without additional constraints on bracketing (Pereira and Shabes, 1992) or on allowable rewrite rules (Carroll and Charniak, 1992), unsupervised grammar learning was ineffective. Since then, there has been a large body of work addressing the flaws of the EM-based approach. Syntactic models empirically more learnable than PCFGs have been developed (Clark, 2001; Klein and Manning, 2004). Smith and Eisner (2005) proposed a new objective function; Smith and Eisner (2006) introduced a new training procedure. Bayesian approaches can also improve performance (Goldwater and Griffiths, 2007; Johnson, 2007; Kurihara and Sato, 2006). Though these methods have improved induction accuracy, at the core they all still involve optimizing non-convex objective functions related to the likelihood of some model, and thus are not completely immune to the difficulties associated with early approaches. It is therefore important to better understand the behavior of unsupervised induction systems in general. In this paper"
P08-1100,P06-1072,0,0.0348674,"ammars via EM encountered two serious obstacles: the inappropriateness of the likelihood objective and the tendency of EM to get stuck in local optima. Without additional constraints on bracketing (Pereira and Shabes, 1992) or on allowable rewrite rules (Carroll and Charniak, 1992), unsupervised grammar learning was ineffective. Since then, there has been a large body of work addressing the flaws of the EM-based approach. Syntactic models empirically more learnable than PCFGs have been developed (Clark, 2001; Klein and Manning, 2004). Smith and Eisner (2005) proposed a new objective function; Smith and Eisner (2006) introduced a new training procedure. Bayesian approaches can also improve performance (Goldwater and Griffiths, 2007; Johnson, 2007; Kurihara and Sato, 2006). Though these methods have improved induction accuracy, at the core they all still involve optimizing non-convex objective functions related to the likelihood of some model, and thus are not completely immune to the difficulties associated with early approaches. It is therefore important to better understand the behavior of unsupervised induction systems in general. In this paper, we take a step back and present a more statistical view o"
P09-1011,D07-1071,0,0.0893892,"esent a generative model that simultaneously segments the text into utterances and maps each utterance to a meaning representation grounded in the world state. We show that our model generalizes across three domains of increasing difficulty—Robocup sportscasting, weather forecasts (a new domain), and NFL recaps. 1 Dan Klein UC Berkeley klein@cs.berkeley.edu Introduction Recent work in learning semantics has focused on mapping sentences to meaning representations (e.g., some logical form) given aligned sentence/meaning pairs as training data (Ge and Mooney, 2005; Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007; Lu et al., 2008). However, this degree of supervision is unrealistic for modeling human language acquisition and can be costly to obtain for building large-scale, broadcoverage language understanding systems. A more flexible direction is grounded language acquisition: learning the meaning of sentences in the context of an observed world state. The grounded approach has gained interest in various disciplines (Siskind, 1996; Yu and Ballard, 2004; Feldman and Narayanan, 2004; Gorniak and Roy, 2007). Some recent work in the NLP community has also moved in this direction by relaxing the amount of"
P09-1011,H05-1042,0,0.108796,"occur in multiple records, so there is still uncertainty about which record is referenced by a given sentence. NFL Recaps In this domain, each scenario represents a single NFL football game (see Figure 1(c) for an example). The world state (the things that happened during the game) is represented by database tables, e.g., scoring summary, team comparison, drive chart, play-by-play, etc. Each record is a database entry, for instance, the receiving statistics for a certain player. The text is the recap of the game— an article summarizing the game highlights. The dataset we used was collected by Barzilay and Lapata (2005). The data includes 466 games during the 2003–2004 NFL season. 78 of these games were annotated by Snyder and Barzilay (2007), who aligned each sentence to a set of records. This domain is by far the most complicated of the three. Many records corresponding to inconsequential game statistics are not mentioned. Conversely, the text contains many general remarks (e.g., it was just that type of game) which are not present in any of the records. Furthermore, the complexity of the language used in the recap is far greater than what we can represent us3 Generative Model To learn the correspondence b"
P09-1011,J93-2003,0,0.0224271,". To quantify the benefits of incorporating these two aspects, we compare our full model with two simpler variants. Table 2: Highest probability words for the categorical field skyCover.mode in the weather domain. It is interesting to note that skyCover=75-100 is so highly correlated with rain that the model learns to connect an overcast sky in the world to the indication of rain in the text. • Model 1 (no model of segmentation or coherence): Each record is chosen independently; each record generates one field, and each field generates one word. This model is similar in spirit to IBM model 1 (Brown et al., 1993). separate multinomial distribution over words from which w is drawn. An example of a categorical field is skyCover.mode in the weather domain, which has four values: 0-25, 25-50, 50-75, and 75-100. Table 2 shows the top words for each of these field values learned by our model. • Model 2 (models segmentation but not coherence): Records and fields are still generated independently, but each field can now generate multiple words. 4 • Model 3 (our full model of segmentation and coherence): Records and fields are generated according to the Markov chains described in Section 3. Learning and Infere"
P09-1011,D08-1033,1,0.619859,"Missing"
P09-1011,D08-1035,0,0.00830371,"consequential game statistics are not mentioned. Conversely, the text contains many general remarks (e.g., it was just that type of game) which are not present in any of the records. Furthermore, the complexity of the language used in the recap is far greater than what we can represent us3 Generative Model To learn the correspondence between a text w and a world state s, we propose a generative model p(w |s) with latent variables specifying this correspondence. Our model combines segmentation with alignment. The segmentation aspect of our model is similar to that of Grenager et al. (2005) and Eisenstein and Barzilay (2008), but in those two models, the segments are clustered into topics rather than grounded to a world state. The alignment aspect of our model is similar to the HMM model for word alignment (Ney and Vogel, 1996). DeNero et al. (2008) perform joint segmentation and word alignment for machine translation, but the nature of that task is different from ours. The model is defined by a generative process, 93 def which proceeds in three stages (Figure 2 shows the corresponding graphical model): where s(t) = {r ∈ s : r.t = t} and r0 .t is a dedicated START record type.2 We also model the transition of the"
P09-1011,W05-0602,0,0.0650037,"h degree of ambiguity present in this setting, we present a generative model that simultaneously segments the text into utterances and maps each utterance to a meaning representation grounded in the world state. We show that our model generalizes across three domains of increasing difficulty—Robocup sportscasting, weather forecasts (a new domain), and NFL recaps. 1 Dan Klein UC Berkeley klein@cs.berkeley.edu Introduction Recent work in learning semantics has focused on mapping sentences to meaning representations (e.g., some logical form) given aligned sentence/meaning pairs as training data (Ge and Mooney, 2005; Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007; Lu et al., 2008). However, this degree of supervision is unrealistic for modeling human language acquisition and can be costly to obtain for building large-scale, broadcoverage language understanding systems. A more flexible direction is grounded language acquisition: learning the meaning of sentences in the context of an observed world state. The grounded approach has gained interest in various disciplines (Siskind, 1996; Yu and Ballard, 2004; Feldman and Narayanan, 2004; Gorniak and Roy, 2007). Some recent work in the NLP commun"
P09-1011,P05-1046,1,0.205742,"records corresponding to inconsequential game statistics are not mentioned. Conversely, the text contains many general remarks (e.g., it was just that type of game) which are not present in any of the records. Furthermore, the complexity of the language used in the recap is far greater than what we can represent us3 Generative Model To learn the correspondence between a text w and a world state s, we propose a generative model p(w |s) with latent variables specifying this correspondence. Our model combines segmentation with alignment. The segmentation aspect of our model is similar to that of Grenager et al. (2005) and Eisenstein and Barzilay (2008), but in those two models, the segments are clustered into topics rather than grounded to a world state. The alignment aspect of our model is similar to the HMM model for word alignment (Ney and Vogel, 1996). DeNero et al. (2008) perform joint segmentation and word alignment for machine translation, but the nature of that task is different from ours. The model is defined by a generative process, 93 def which proceeds in three stages (Figure 2 shows the corresponding graphical model): where s(t) = {r ∈ s : r.t = t} and r0 .t is a dedicated START record type.2"
P09-1011,N06-1014,1,0.612603,"e predictions made by each of our three models for a particular example. Model 1 easily mistakes pink10 for the recipient of a pass record because decisions are made independently for each word. Model 2 chooses the correct record, but having no model of the field structure inside a record, it proposes an incorrect field segmentation (although our evaluation is insensitive to this). Equipped with the ability to prefer a coherent field sequence, Model 3 fixes these errors. Many of the remaining errors are due to the garbage collection phenomenon familiar from word alignment models (Moore, 2004; Liang et al., 2006). For example, the ballstopped record occurs frequently but is never mentioned in the text. At the same time, there is a correlation between ballstopped and utterances such as pink2 holds onto the ball, which are not aligned to any record in the annotation. As a result, our model incorrectly chooses to align the two. tialize the parameters of the latter.6 We also prohibited utterances in Models 2 and 3 from crossing punctuation during inference. Table 5 shows that performance improves substantially in the more sophisticated models, the gains being greater than in the Robocup domain. Figure 5 s"
P09-1011,D08-1082,0,0.166727,"imultaneously segments the text into utterances and maps each utterance to a meaning representation grounded in the world state. We show that our model generalizes across three domains of increasing difficulty—Robocup sportscasting, weather forecasts (a new domain), and NFL recaps. 1 Dan Klein UC Berkeley klein@cs.berkeley.edu Introduction Recent work in learning semantics has focused on mapping sentences to meaning representations (e.g., some logical form) given aligned sentence/meaning pairs as training data (Ge and Mooney, 2005; Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007; Lu et al., 2008). However, this degree of supervision is unrealistic for modeling human language acquisition and can be costly to obtain for building large-scale, broadcoverage language understanding systems. A more flexible direction is grounded language acquisition: learning the meaning of sentences in the context of an observed world state. The grounded approach has gained interest in various disciplines (Siskind, 1996; Yu and Ballard, 2004; Feldman and Narayanan, 2004; Gorniak and Roy, 2007). Some recent work in the NLP community has also moved in this direction by relaxing the amount of supervision to th"
P09-1011,P04-1066,0,0.00918886,"er look at the predictions made by each of our three models for a particular example. Model 1 easily mistakes pink10 for the recipient of a pass record because decisions are made independently for each word. Model 2 chooses the correct record, but having no model of the field structure inside a record, it proposes an incorrect field segmentation (although our evaluation is insensitive to this). Equipped with the ability to prefer a coherent field sequence, Model 3 fixes these errors. Many of the remaining errors are due to the garbage collection phenomenon familiar from word alignment models (Moore, 2004; Liang et al., 2006). For example, the ballstopped record occurs frequently but is never mentioned in the text. At the same time, there is a correlation between ballstopped and utterances such as pink2 holds onto the ball, which are not aligned to any record in the annotation. As a result, our model incorrectly chooses to align the two. tialize the parameters of the latter.6 We also prohibited utterances in Models 2 and 3 from crossing punctuation during inference. Table 5 shows that performance improves substantially in the more sophisticated models, the gains being greater than in the Roboc"
P09-1011,C96-2141,0,0.0633055,"language used in the recap is far greater than what we can represent us3 Generative Model To learn the correspondence between a text w and a world state s, we propose a generative model p(w |s) with latent variables specifying this correspondence. Our model combines segmentation with alignment. The segmentation aspect of our model is similar to that of Grenager et al. (2005) and Eisenstein and Barzilay (2008), but in those two models, the segments are clustered into topics rather than grounded to a world state. The alignment aspect of our model is similar to the HMM model for word alignment (Ney and Vogel, 1996). DeNero et al. (2008) perform joint segmentation and word alignment for machine translation, but the nature of that task is different from ours. The model is defined by a generative process, 93 def which proceeds in three stages (Figure 2 shows the corresponding graphical model): where s(t) = {r ∈ s : r.t = t} and r0 .t is a dedicated START record type.2 We also model the transition of the final record type to a designated STOP record type in order to capture regularities about the types of records which are described last. More sophisticated models of coherence could also be employed here (B"
P09-1011,J08-1001,0,\N,Missing
P11-1060,P09-1010,0,0.521693,"ng problem. Piantadosi et al. (2008) induces first-order formulae using CCG in a small domain assuming observed lexical semantics. The closest work to ours is Clarke et al. (2010), which we discussed earlier. The integration of natural language with denotations computed against a world (grounding) is becoming increasingly popular. Feedback from the world has been used to guide both syntactic parsing (Schuler, 2003) and semantic parsing (Popescu et al., 2003; Clarke et al., 2010). Past work has also focused on aligning text to a world (Liang et al., 2009), using text in reinforcement learning (Branavan et al., 2009; Branavan et al., 2010), and many others. Our work pushes the grounded language agenda towards deeper representations of language—think grounded compositional semantics. 6 Conclusion We built a system that interprets natural language utterances much more accurately than existing systems, despite using no annotated logical forms. Our system is based on a new semantic representation, DCS, which offers a simple and expressive alternative to lambda calculus. Free from the burden of annotating logical forms, we hope to use our techniques in developing even more accurate and broader-coverage langua"
P11-1060,P10-1129,0,0.285074,"et al. (2008) induces first-order formulae using CCG in a small domain assuming observed lexical semantics. The closest work to ours is Clarke et al. (2010), which we discussed earlier. The integration of natural language with denotations computed against a world (grounding) is becoming increasingly popular. Feedback from the world has been used to guide both syntactic parsing (Schuler, 2003) and semantic parsing (Popescu et al., 2003; Clarke et al., 2010). Past work has also focused on aligning text to a world (Liang et al., 2009), using text in reinforcement learning (Branavan et al., 2009; Branavan et al., 2010), and many others. Our work pushes the grounded language agenda towards deeper representations of language—think grounded compositional semantics. 6 Conclusion We built a system that interprets natural language utterances much more accurately than existing systems, despite using no annotated logical forms. Our system is based on a new semantic representation, DCS, which offers a simple and expressive alternative to lambda calculus. Free from the burden of annotating logical forms, we hope to use our techniques in developing even more accurate and broader-coverage language understanding systems"
P11-1060,W10-2903,0,0.815625,"e types of complex questions compositionally involves first mapping the questions into logical forms (semantic parsing). Supervised semantic parsers (Zelle and Mooney, 1996; Tang and Mooney, 2001; Ge and Mooney, 2005; Zettlemoyer and Collins, 2005; Kate and Mooney, 2007; Zettlemoyer and Collins, 2007; Wong and Mooney, 2007; Kwiatkowski et al., 2010) rely on manual annotation of logical forms, which is expensive. On the other hand, existing unsupervised semantic parsers (Poon and Domingos, 2009) do not handle deeper linguistic phenomena such as quantification, negation, and superlatives. As in Clarke et al. (2010), we obviate the need for annotated logical forms by considering the endto-end problem of mapping questions to answers. However, we still model the logical form (now as a latent variable) to capture the complexities of language. Figure 1 shows our probabilistic model: 590 We want to induce latent logical forms z (and parameters θ) given only question-answer pairs (x, y), which is much cheaper to obtain than (x, z) pairs. The core problem that arises in this setting is program induction: finding a logical form z (over an exponentially large space of possibilities) that produces the target answe"
P11-1060,D09-1100,0,0.0135121,"s in programs (DCS trees) which are much simpler than the logically-equivalent lambda calculus formulae. The idea of using CSPs to represent semantics is inspired by Discourse Representation Theory (DRT) (Kamp and Reyle, 1993; Kamp et al., 2005), where variables are discourse referents. The restriction to trees is similar to economical DRT (Bos, 2009). The other major focus of this work is program induction—inferring logical forms from their denotations. There has been a fair amount of past work on this topic: Liang et al. (2010) induces combinatory logic programs in a non-linguistic setting. Eisenstein et al. (2009) induces conjunctive formulae and uses them as features in another learning problem. Piantadosi et al. (2008) induces first-order formulae using CCG in a small domain assuming observed lexical semantics. The closest work to ours is Clarke et al. (2010), which we discussed earlier. The integration of natural language with denotations computed against a world (grounding) is becoming increasingly popular. Feedback from the world has been used to guide both syntactic parsing (Schuler, 2003) and semantic parsing (Popescu et al., 2003; Clarke et al., 2010). Past work has also focused on aligning tex"
P11-1060,W05-0602,0,0.0374606,"1 z ∼ pθ (z |x) area c argmax y = JzKw Figure 1: Our probabilistic model: a question x is mapped to a latent logical form z, which is then evaluated with respect to a world w (database of facts), producing an answer y. We represent logical forms z as labeled trees, induced automatically from (x, y) pairs. Introduction What is the total population of the ten largest capitals in the US? Answering these types of complex questions compositionally involves first mapping the questions into logical forms (semantic parsing). Supervised semantic parsers (Zelle and Mooney, 1996; Tang and Mooney, 2001; Ge and Mooney, 2005; Zettlemoyer and Collins, 2005; Kate and Mooney, 2007; Zettlemoyer and Collins, 2007; Wong and Mooney, 2007; Kwiatkowski et al., 2010) rely on manual annotation of logical forms, which is expensive. On the other hand, existing unsupervised semantic parsers (Poon and Domingos, 2009) do not handle deeper linguistic phenomena such as quantification, negation, and superlatives. As in Clarke et al. (2010), we obviate the need for annotated logical forms by considering the endto-end problem of mapping questions to answers. However, we still model the logical form (now as a latent variable) to captu"
P11-1060,D10-1119,0,0.86269,"ch is then evaluated with respect to a world w (database of facts), producing an answer y. We represent logical forms z as labeled trees, induced automatically from (x, y) pairs. Introduction What is the total population of the ten largest capitals in the US? Answering these types of complex questions compositionally involves first mapping the questions into logical forms (semantic parsing). Supervised semantic parsers (Zelle and Mooney, 1996; Tang and Mooney, 2001; Ge and Mooney, 2005; Zettlemoyer and Collins, 2005; Kate and Mooney, 2007; Zettlemoyer and Collins, 2007; Wong and Mooney, 2007; Kwiatkowski et al., 2010) rely on manual annotation of logical forms, which is expensive. On the other hand, existing unsupervised semantic parsers (Poon and Domingos, 2009) do not handle deeper linguistic phenomena such as quantification, negation, and superlatives. As in Clarke et al. (2010), we obviate the need for annotated logical forms by considering the endto-end problem of mapping questions to answers. However, we still model the logical form (now as a latent variable) to capture the complexities of language. Figure 1 shows our probabilistic model: 590 We want to induce latent logical forms z (and parameters θ"
P11-1060,P09-1011,1,0.536083,"nctive formulae and uses them as features in another learning problem. Piantadosi et al. (2008) induces first-order formulae using CCG in a small domain assuming observed lexical semantics. The closest work to ours is Clarke et al. (2010), which we discussed earlier. The integration of natural language with denotations computed against a world (grounding) is becoming increasingly popular. Feedback from the world has been used to guide both syntactic parsing (Schuler, 2003) and semantic parsing (Popescu et al., 2003; Clarke et al., 2010). Past work has also focused on aligning text to a world (Liang et al., 2009), using text in reinforcement learning (Branavan et al., 2009; Branavan et al., 2010), and many others. Our work pushes the grounded language agenda towards deeper representations of language—think grounded compositional semantics. 6 Conclusion We built a system that interprets natural language utterances much more accurately than existing systems, despite using no annotated logical forms. Our system is based on a new semantic representation, DCS, which offers a simple and expressive alternative to lambda calculus. Free from the burden of annotating logical forms, we hope to use our techniques"
P11-1060,P06-1055,1,0.413012,"Missing"
P11-1060,D09-1001,0,0.303397,"omatically from (x, y) pairs. Introduction What is the total population of the ten largest capitals in the US? Answering these types of complex questions compositionally involves first mapping the questions into logical forms (semantic parsing). Supervised semantic parsers (Zelle and Mooney, 1996; Tang and Mooney, 2001; Ge and Mooney, 2005; Zettlemoyer and Collins, 2005; Kate and Mooney, 2007; Zettlemoyer and Collins, 2007; Wong and Mooney, 2007; Kwiatkowski et al., 2010) rely on manual annotation of logical forms, which is expensive. On the other hand, existing unsupervised semantic parsers (Poon and Domingos, 2009) do not handle deeper linguistic phenomena such as quantification, negation, and superlatives. As in Clarke et al. (2010), we obviate the need for annotated logical forms by considering the endto-end problem of mapping questions to answers. However, we still model the logical form (now as a latent variable) to capture the complexities of language. Figure 1 shows our probabilistic model: 590 We want to induce latent logical forms z (and parameters θ) given only question-answer pairs (x, y), which is much cheaper to obtain than (x, z) pairs. The core problem that arises in this setting is progra"
P11-1060,P03-1067,0,0.0381539,"on this topic: Liang et al. (2010) induces combinatory logic programs in a non-linguistic setting. Eisenstein et al. (2009) induces conjunctive formulae and uses them as features in another learning problem. Piantadosi et al. (2008) induces first-order formulae using CCG in a small domain assuming observed lexical semantics. The closest work to ours is Clarke et al. (2010), which we discussed earlier. The integration of natural language with denotations computed against a world (grounding) is becoming increasingly popular. Feedback from the world has been used to guide both syntactic parsing (Schuler, 2003) and semantic parsing (Popescu et al., 2003; Clarke et al., 2010). Past work has also focused on aligning text to a world (Liang et al., 2009), using text in reinforcement learning (Branavan et al., 2009; Branavan et al., 2010), and many others. Our work pushes the grounded language agenda towards deeper representations of language—think grounded compositional semantics. 6 Conclusion We built a system that interprets natural language utterances much more accurately than existing systems, despite using no annotated logical forms. Our system is based on a new semantic representation, DCS, which"
P11-1060,P07-1121,0,0.913929,"ent logical form z, which is then evaluated with respect to a world w (database of facts), producing an answer y. We represent logical forms z as labeled trees, induced automatically from (x, y) pairs. Introduction What is the total population of the ten largest capitals in the US? Answering these types of complex questions compositionally involves first mapping the questions into logical forms (semantic parsing). Supervised semantic parsers (Zelle and Mooney, 1996; Tang and Mooney, 2001; Ge and Mooney, 2005; Zettlemoyer and Collins, 2005; Kate and Mooney, 2007; Zettlemoyer and Collins, 2007; Wong and Mooney, 2007; Kwiatkowski et al., 2010) rely on manual annotation of logical forms, which is expensive. On the other hand, existing unsupervised semantic parsers (Poon and Domingos, 2009) do not handle deeper linguistic phenomena such as quantification, negation, and superlatives. As in Clarke et al. (2010), we obviate the need for annotated logical forms by considering the endto-end problem of mapping questions to answers. However, we still model the logical form (now as a latent variable) to capture the complexities of language. Figure 1 shows our probabilistic model: 590 We want to induce latent logica"
P11-1060,D07-1071,0,0.91835,"a question x is mapped to a latent logical form z, which is then evaluated with respect to a world w (database of facts), producing an answer y. We represent logical forms z as labeled trees, induced automatically from (x, y) pairs. Introduction What is the total population of the ten largest capitals in the US? Answering these types of complex questions compositionally involves first mapping the questions into logical forms (semantic parsing). Supervised semantic parsers (Zelle and Mooney, 1996; Tang and Mooney, 2001; Ge and Mooney, 2005; Zettlemoyer and Collins, 2005; Kate and Mooney, 2007; Zettlemoyer and Collins, 2007; Wong and Mooney, 2007; Kwiatkowski et al., 2010) rely on manual annotation of logical forms, which is expensive. On the other hand, existing unsupervised semantic parsers (Poon and Domingos, 2009) do not handle deeper linguistic phenomena such as quantification, negation, and superlatives. As in Clarke et al. (2010), we obviate the need for annotated logical forms by considering the endto-end problem of mapping questions to answers. However, we still model the logical form (now as a latent variable) to capture the complexities of language. Figure 1 shows our probabilistic model: 590 We want"
P11-1060,W11-0103,0,\N,Missing
P11-1060,J93-2004,0,\N,Missing
P11-1060,C04-1180,0,\N,Missing
P11-1060,J03-4003,0,\N,Missing
P11-1060,D11-1039,0,\N,Missing
P11-1060,N06-1056,0,\N,Missing
P11-1060,P10-1083,0,\N,Missing
P11-1060,P02-1041,0,\N,Missing
P11-1060,D08-1082,0,\N,Missing
P11-1060,P06-1063,0,\N,Missing
P11-1060,P06-1115,0,\N,Missing
P11-1060,P96-1008,0,\N,Missing
P11-1060,J82-3002,0,\N,Missing
P11-1060,P11-1149,0,\N,Missing
P11-1060,D11-1140,0,\N,Missing
P11-1060,P11-1028,0,\N,Missing
P11-1060,N09-1069,1,\N,Missing
P14-1037,D13-1161,0,0.0181826,"Missing"
P14-1037,D13-1160,1,0.737763,"where the compatibility function c maps each y to c(y) ∈ {0, 1} denoting the (approximate) correctness of the list y. In this paper, an entity list y is compatible (c(y) = 1) when the first, second, and last elements of y match the annotation; otherwise, it is incompatible (c(y) = 0). 2.1 New partial queries list of X list of X list of X IN list of IN Y list of X list of X list of Y list of Y list of w list of w list of X Dataset To experiment with a diverse set of queries and web pages, we created a new dataset, O PEN W EB, using web pages from Google search results.1 We use the method from Berant et al. (2013) to generate search queries by performing a breadth-first search over the query space. Specifically, we use the Google Suggest API, which takes a partial query (e.g., “list of movies”) and outputs several complete queries (e.g., “list of horror movies”). We start with seed partial queries “list of • ” where • is one or two initial letters. In each step, we call the Google Suggest API on the partial queries to obtain complete queries, 3 Approach Figure 3 shows the framework of our system. Given a query x and a web page w, the system generates a set Z(w) of extraction predicates z which can extr"
P14-1037,P13-1042,0,0.102692,"Missing"
P14-1037,P09-1113,0,0.135655,"Missing"
P14-1037,N13-1008,0,0.0446686,"Missing"
P14-1037,D12-1042,0,0.0481199,"Missing"
P14-1037,D09-1156,0,0.0991468,"problem where the input is the query and the web page, and the output is a list of entities, mediated by a latent extraction predicate. To generalize across different inputs, we rely on two types of features: structural features, which look at the layout and placement of the entities being extracted; and denotation feaIntroduction We consider the task of extracting entities of a given category (e.g., hiking trails) from web pages. Previous approaches either (i) assume that the same entities appear on multiple web pages, or (ii) require information such as seed examples (Etzioni et al., 2005; Wang and Cohen, 2009; Dalvi et al., 2012). These approaches work well for common categories but encounter data sparsity problems for more specific categories, such as the products of a small company or the dishes at a local restaurant. In this context, we may have only a single web page that contains the information we need and no seed examples. In this paper, we propose a novel task, zeroshot entity extraction, where the specification of the desired entities is provided as a natural language query. Given a query (e.g., hiking 391 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguis"
P14-1037,C92-2082,0,0.117129,"Missing"
P14-1037,P11-1055,0,0.0630781,"Missing"
P14-1133,D13-1160,1,0.864467,"candidate logical form (in red), we generate canonical utterances (in purple). The model is trained to paraphrase the input utterance (in green) into the canonical utterances associated with the correct denotation (in blue). Introduction We consider the semantic parsing problem of mapping natural language utterances into logical forms to be executed on a knowledge base (KB) (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Kwiatkowski et al., 2010). Scaling semantic parsers to large knowledge bases has attracted substantial attention recently (Cai and Yates, 2013; Berant et al., 2013; Kwiatkowski et al., 2013), since it drives applications such as question answering (QA) and information extraction (IE). Semantic parsers need to somehow associate natural language phrases with logical predicates, e.g., they must learn that the constructions “What does X do for a living?”, “What is X’s profession?”, and “Who is X?”, should all map to the logical predicate Profession. To learn these mappings, traditional semantic parsers use data which pairs natural language with the KB. However, this leaves untapped a vast amount of text not related to the KB. For instance, the utterances “W"
P14-1133,P13-1042,0,0.48901,"raphrasing: For each candidate logical form (in red), we generate canonical utterances (in purple). The model is trained to paraphrase the input utterance (in green) into the canonical utterances associated with the correct denotation (in blue). Introduction We consider the semantic parsing problem of mapping natural language utterances into logical forms to be executed on a knowledge base (KB) (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Kwiatkowski et al., 2010). Scaling semantic parsers to large knowledge bases has attracted substantial attention recently (Cai and Yates, 2013; Berant et al., 2013; Kwiatkowski et al., 2013), since it drives applications such as question answering (QA) and information extraction (IE). Semantic parsers need to somehow associate natural language phrases with logical predicates, e.g., they must learn that the constructions “What does X do for a living?”, “What is X’s profession?”, and “Who is X?”, should all map to the logical predicate Profession. To learn these mappings, traditional semantic parsers use data which pairs natural language with the KB. However, this leaves untapped a vast amount of text not related to the KB. For instan"
P14-1133,N10-1066,0,0.0293492,"a vector space model, which learns to score the similarity between vector representations of natural language utterances (Section 5). Model We define a discriminative log-linear model that places a probability distribution over pairs of logical forms and canonical utterances (c, z), given an utterance x: input utterance, and are briefly described in this section. Many existing paraphrase models introduce latent variables to describe the derivation of c from x, e.g., with transformations (Heilman and Smith, 2010; Stern and Dagan, 2011) or alignments (Haghighi et al., 2005; Das and Smith, 2009; Chang et al., 2010). However, we opt for a simpler paraphrase model without latent variables in the interest of efficiency. Logical form features The parameters θlf correspond to the following features adopted from Berant et al. (2013). For a logical form z, we extract the size of its denotation JzKK . We also add all binary predicates in z as features. Moreover, we extract a popularity feature for predicates based on the number of instances they have in K. For Freebase entities, we extract a popularity feature based on the entity frequency in an entity linked subset of Reverb (Lin et al., 2012). Lastly, Freebas"
P14-1133,P09-1053,0,0.447958,"orpus, combined with a vector space model, which learns to score the similarity between vector representations of natural language utterances (Section 5). Model We define a discriminative log-linear model that places a probability distribution over pairs of logical forms and canonical utterances (c, z), given an utterance x: input utterance, and are briefly described in this section. Many existing paraphrase models introduce latent variables to describe the derivation of c from x, e.g., with transformations (Heilman and Smith, 2010; Stern and Dagan, 2011) or alignments (Haghighi et al., 2005; Das and Smith, 2009; Chang et al., 2010). However, we opt for a simpler paraphrase model without latent variables in the interest of efficiency. Logical form features The parameters θlf correspond to the following features adopted from Berant et al. (2013). For a logical form z, we extract the size of its denotation JzKK . We also add all binary predicates in z as features. Moreover, we extract a popularity feature for predicates based on the number of instances they have in K. For Freebase entities, we extract a popularity feature based on the entity frequency in an entity linked subset of Reverb (Lin et al., 2"
P14-1133,C04-1051,0,0.439373,"problem is reduced to scoring pairs (c, z) based on a paraphrase model. The NLP paraphrase literature is vast and ranges from simple methods employing surface features (Wan et al., 2006), through vector space models (Socher et al., 2011), to latent variable models (Das and Smith, 2009; Wang and Manning, 2010; Stern and Dagan, 2011). In this paper, we focus on two paraphrase models that emphasize simplicity and efficiency. This is important since for each question-answer pair, we consider thousands of canonical utterances as potential paraphrases. In contrast, traditional paraphrase detection (Dolan et al., 2004) and Recognizing Textual Entailment (RTE) tasks (Dagan et al., 2013) consider examples consisting of only a single pair of candidate paraphrases. Our paraphrase model decomposes into an association model and a vector space model: φpr (x, c)> θpr = φas (x, c)> θas + φvs (x, c)> θvs . x : What Association model The goal of the association model is to determine whether x and c contain phrases that are likely to be paraphrases. Given an utterance x = hx0 , x1 , .., xn−1 i, we denote by xi:j the span from token i to token j. For each pair of utterances (x, c), we go through all spans of x and c and"
P14-1133,D11-1142,0,0.0345079,"“Who is X?”, should all map to the logical predicate Profession. To learn these mappings, traditional semantic parsers use data which pairs natural language with the KB. However, this leaves untapped a vast amount of text not related to the KB. For instance, the utterances “Where is ACL in 2014?” and “What is the location of ACL 2014?” cannot be used in traditional semantic parsing methods, since the KB does not contain an entity ACL2014, but this pair clearly contains valuable linguistic information. As another reference point, out of 500,000 relations extracted by the ReVerb Open IE system (Fader et al., 2011), only about 10,000 can be aligned to Freebase (Berant et al., 2013). In this paper, we present a novel approach for semantic parsing based on paraphrasing that can exploit large amounts of text not covered by the KB (Figure 1). Our approach targets factoid questions with a modest amount of compositionality. Given an input utterance, we first use a simple deterministic procedure to construct a manageable set of candidate logical forms (ideally, we would generate canonical utterances for all possible logical forms, but this is intractable). Next, we heuris1415 Proceedings of the 52nd Annual Mee"
P14-1133,P13-1158,0,0.694577,"s, and a vector space model, which represents each utterance as a vector and learns a similarity score between them. The entire system is trained jointly from question-answer pairs only. Our work relates to recent lines of research in semantic parsing and question answering. Kwiatkowski et al. (2013) first maps utterances to a domain-independent intermediate logical form, and then performs ontology matching to produce the final logical form. In some sense, we approach the problem from the opposite end, using an intermediate utterance, which allows us to employ paraphrasing methods (Figure 2). Fader et al. (2013) presented a QA system that maps questions onto simple queries against Open IE extractions, by learning paraphrases from a large monolingual parallel corpus, and performing a single paraphrasing step. We adopt the idea of using paraphrasing for QA, but suggest a more general paraphrase model and work against a formal KB (Freebase). We apply our semantic parser on two datasets: W EB Q UESTIONS (Berant et al., 2013), which contains 5,810 question-answer pairs with common questions asked by web users; and 2 Setup Our task is as follows: Given (i) a knowledge base K, and (ii) a training set of que"
P14-1133,P05-1045,0,0.00789813,"Missing"
P14-1133,H05-1049,0,0.0119153,"monolingual parallel corpus, combined with a vector space model, which learns to score the similarity between vector representations of natural language utterances (Section 5). Model We define a discriminative log-linear model that places a probability distribution over pairs of logical forms and canonical utterances (c, z), given an utterance x: input utterance, and are briefly described in this section. Many existing paraphrase models introduce latent variables to describe the derivation of c from x, e.g., with transformations (Heilman and Smith, 2010; Stern and Dagan, 2011) or alignments (Haghighi et al., 2005; Das and Smith, 2009; Chang et al., 2010). However, we opt for a simpler paraphrase model without latent variables in the interest of efficiency. Logical form features The parameters θlf correspond to the following features adopted from Berant et al. (2013). For a logical form z, we extract the size of its denotation JzKK . We also add all binary predicates in z as features. Moreover, we extract a popularity feature for predicates based on the number of instances they have in K. For Freebase entities, we extract a popularity feature based on the entity frequency in an entity linked subset of"
P14-1133,P06-1114,0,0.0147557,"was the gallipoli campaign waged?” is Galipoli and not GalipoliCampaign. Last, PARA S EMPRE does not handle temporal information, which causes errors in questions like “Where did Harriet Tubman live after the civil war?” 7 Discussion In this work, we approach the problem of semantic parsing from a paraphrasing viewpoint. A fundamental motivation and long standing goal of the paraphrasing and RTE communities has been to cast various semantic applications as paraphrasing/textual entailment (Dagan et al., 2013). While it has been shown that paraphrasing methods are useful for question answering (Harabagiu and Hickl, 2006) and relation extraction (Romano et al., 2006), this is, to the best of our knowledge, the first paper to perform semantic parsing through paraphrasing. Our paraphrase model emphasizes simplicity and efficiency, but the framework is agnostic to the internals of the paraphrase method. On the semantic parsing side, our work is most related to Kwiatkowski et al. (2013). The main challenge in semantic parsing is coping with the mismatch between language and the KB. In both Kwiatkowski et al. (2013) and this work, an intermediate representation is employed to handle the mismatch, but while they use"
P14-1133,N10-1145,0,0.139126,"ssociate natural language phrases that co-occur frequently in a monolingual parallel corpus, combined with a vector space model, which learns to score the similarity between vector representations of natural language utterances (Section 5). Model We define a discriminative log-linear model that places a probability distribution over pairs of logical forms and canonical utterances (c, z), given an utterance x: input utterance, and are briefly described in this section. Many existing paraphrase models introduce latent variables to describe the derivation of c from x, e.g., with transformations (Heilman and Smith, 2010; Stern and Dagan, 2011) or alignments (Haghighi et al., 2005; Das and Smith, 2009; Chang et al., 2010). However, we opt for a simpler paraphrase model without latent variables in the interest of efficiency. Logical form features The parameters θlf correspond to the following features adopted from Berant et al. (2013). For a logical form z, we extract the size of its denotation JzKK . We also add all binary predicates in z as features. Moreover, we extract a popularity feature for predicates based on the number of instances they have in K. For Freebase entities, we extract a popularity feature"
P14-1133,P12-1092,0,0.0624549,"ctor space model The association model relies on having a good set of candidate associations, but mining associations suffers from coverage issues. We now introduce a vector space (VS) model, which assigns a vector representation for each utterance, and learns a scoring function that ranks paraphrase candidates. We start by constructing vector representations of words. We run the WORD 2 VEC tool (Mikolov et al., 2013) on lower-cased Wikipedia text (1.59 billion tokens), using the CBOW model with a window of 5 and hierarchical softmax. We also experiment with publicly released word embeddings (Huang et al., 2012), which were trained using both local and global context. Both result in kdimensional vectors (k = 50). Next, we construct a vector vx ∈ Rk for each utterance x by simply averaging the vectors of all content words (nouns, verbs, and adjectives) in x. We can now estimate a paraphrase score for two utterances x and c via a weighted combination of the components of the vector representations: vx> W vc = k X wij vx,i vc,j i,j=1 where W ∈ Rk×k is a parameter matrix. In terms of our earlier notation, we have θvs = vec(W ) and φvs (x, c) = vec(vx vc> ), where vec(·) unrolls a matrix into a vector. In"
P14-1133,P03-1054,0,0.0553387,"Missing"
P14-1133,D10-1119,0,0.111259,"y? Type.PoliticalParty u Founder.HenryClay ... Type.Event u Involved.HenryClay Whig Party Figure 1: Semantic parsing via paraphrasing: For each candidate logical form (in red), we generate canonical utterances (in purple). The model is trained to paraphrase the input utterance (in green) into the canonical utterances associated with the correct denotation (in blue). Introduction We consider the semantic parsing problem of mapping natural language utterances into logical forms to be executed on a knowledge base (KB) (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Kwiatkowski et al., 2010). Scaling semantic parsers to large knowledge bases has attracted substantial attention recently (Cai and Yates, 2013; Berant et al., 2013; Kwiatkowski et al., 2013), since it drives applications such as question answering (QA) and information extraction (IE). Semantic parsers need to somehow associate natural language phrases with logical predicates, e.g., they must learn that the constructions “What does X do for a living?”, “What is X’s profession?”, and “Who is X?”, should all map to the logical predicate Profession. To learn these mappings, traditional semantic parsers use data which pair"
P14-1133,D13-1161,0,0.286014,"m (in red), we generate canonical utterances (in purple). The model is trained to paraphrase the input utterance (in green) into the canonical utterances associated with the correct denotation (in blue). Introduction We consider the semantic parsing problem of mapping natural language utterances into logical forms to be executed on a knowledge base (KB) (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Kwiatkowski et al., 2010). Scaling semantic parsers to large knowledge bases has attracted substantial attention recently (Cai and Yates, 2013; Berant et al., 2013; Kwiatkowski et al., 2013), since it drives applications such as question answering (QA) and information extraction (IE). Semantic parsers need to somehow associate natural language phrases with logical predicates, e.g., they must learn that the constructions “What does X do for a living?”, “What is X’s profession?”, and “Who is X?”, should all map to the logical predicate Profession. To learn these mappings, traditional semantic parsers use data which pairs natural language with the KB. However, this leaves untapped a vast amount of text not related to the KB. For instance, the utterances “Where is ACL in 2014?” and “"
P14-1133,J13-2005,1,0.499567,"ry predicate; e.g., Founded.Microsoft denotes the entities that are Microsoft founders. In PlaceOfBirth.Seattle u Founded.Microsoft, an intersection operator allows us to denote the set of Seattle-born Microsoft founders. A reverse operator reverses the order of arguments: R[PlaceOfBirth].BillGates denotes Bill Gates’s birthplace (in contrast to PlaceOfBirth.Seattle). Lastly, count(Founded.Microsoft) denotes set cardinality, in this case, the number of Microsoft founders. The denotation of a logical form z with respect to a KB K is given by JzKK . For a formal description of simple λ-DCS, see Liang (2013) and Berant et al. (2013). 3 Model overview We now present the general framework for semantic parsing via paraphrasing, including the model and the learning algorithm. In Sections 4 and 5, we provide the details of our implementation. Canonical utterance construction Given an utterance x and the KB, we construct a set of candi1416 date logical forms Zx , and then for each z ∈ Zx generate a small set of canonical natural language utterances Cz . Our goal at this point is only to generate a manageable set of logical forms containing the correct one, and then generate an appropriate canonical utt"
P14-1133,W12-3016,0,0.00963061,"d Smith, 2009; Chang et al., 2010). However, we opt for a simpler paraphrase model without latent variables in the interest of efficiency. Logical form features The parameters θlf correspond to the following features adopted from Berant et al. (2013). For a logical form z, we extract the size of its denotation JzKK . We also add all binary predicates in z as features. Moreover, we extract a popularity feature for predicates based on the number of instances they have in K. For Freebase entities, we extract a popularity feature based on the entity frequency in an entity linked subset of Reverb (Lin et al., 2012). Lastly, Freebase formulas have types (see Section 4), and we conjoin the type of z with the first word of x, to capture the correlation between a word (e.g., “where”) with the Freebase type (e.g., Location). Learning As our training data consists of question-answer pairs (xi , yi ), we maximize the log-likelihood of the correct answer. The probability of an answer y is obtained by marginalizing over canonical utterances c and logical forms z whose denotation is y. Formally, our objective function O(θ) is as follows: exp{φ(x, c, z)> θ} pθ (c, z |x) = P , 0 0 > z 0 ∈Zx ,c0 ∈Cz exp{φ(x, c , z )"
P14-1133,J04-4002,0,0.0223798,"mma(xi:j ) denote the POS tag and lemma sequence of xi:j . corpora, containing 18 million pairs of question paraphrases from wikianswers.com, which were tagged as having the same meaning by users. PARALEX is suitable for our needs since it focuses on question paraphrases. For example, the phrase “do for a living” occurs mostly in questions, and we can extract associations for this phrase from PARALEX. Paraphrase pairs in PAR ALEX are word-aligned using standard machine translation methods. We use the word alignments to construct a phrase table by applying the consistent phrase pair heuristic (Och and Ney, 2004) to all 5-grams. This results in a phrase table with approximately 1.3 million phrase pairs. We let A denote this set of mined candidate associations. For a pair (x, c), we also consider as candidate associations the set B (represented implicitly), which contains token pairs (xi , ci0 ) such that xi and ci0 share the same lemma, the same POS tag, or are linked through a derivation link on WordNet (Fellbaum, 1998). This allows us to learn paraphrases for words that appear in our datasets but are not covered by the phrase table, and to handle nominalizations for phrase pairs such as “Who designe"
P14-1133,E06-1052,0,0.00946005,"ot GalipoliCampaign. Last, PARA S EMPRE does not handle temporal information, which causes errors in questions like “Where did Harriet Tubman live after the civil war?” 7 Discussion In this work, we approach the problem of semantic parsing from a paraphrasing viewpoint. A fundamental motivation and long standing goal of the paraphrasing and RTE communities has been to cast various semantic applications as paraphrasing/textual entailment (Dagan et al., 2013). While it has been shown that paraphrasing methods are useful for question answering (Harabagiu and Hickl, 2006) and relation extraction (Romano et al., 2006), this is, to the best of our knowledge, the first paper to perform semantic parsing through paraphrasing. Our paraphrase model emphasizes simplicity and efficiency, but the framework is agnostic to the internals of the paraphrase method. On the semantic parsing side, our work is most related to Kwiatkowski et al. (2013). The main challenge in semantic parsing is coping with the mismatch between language and the KB. In both Kwiatkowski et al. (2013) and this work, an intermediate representation is employed to handle the mismatch, but while they use a logical representation, we opt for a text-b"
P14-1133,R11-1063,0,0.0374621,"phrases that co-occur frequently in a monolingual parallel corpus, combined with a vector space model, which learns to score the similarity between vector representations of natural language utterances (Section 5). Model We define a discriminative log-linear model that places a probability distribution over pairs of logical forms and canonical utterances (c, z), given an utterance x: input utterance, and are briefly described in this section. Many existing paraphrase models introduce latent variables to describe the derivation of c from x, e.g., with transformations (Heilman and Smith, 2010; Stern and Dagan, 2011) or alignments (Haghighi et al., 2005; Das and Smith, 2009; Chang et al., 2010). However, we opt for a simpler paraphrase model without latent variables in the interest of efficiency. Logical form features The parameters θlf correspond to the following features adopted from Berant et al. (2013). For a logical form z, we extract the size of its denotation JzKK . We also add all binary predicates in z as features. Moreover, we extract a popularity feature for predicates based on the number of instances they have in K. For Freebase entities, we extract a popularity feature based on the entity fre"
P14-1133,N03-1033,0,0.0202947,"Missing"
P14-1133,W09-0607,0,0.0323385,"rasing method that maps a test question to a question for which the answer is already known in a single step. We propose a general paraphrasing framework and instantiate it with two paraphrase models. Lastly, Fader et al. handle queries with only one property and entity whereas we generalize to more types of logical forms. Since our generated questions are passed to a paraphrase model, we took a very simple approach, mostly ensuring that we preserved the semantics of the utterance without striving for the most fluent realization. Research on generation (Dale et al., 2003; Reiter et al., 2005; Turner et al., 2009; Piwek and Boyer, 2012) typically focuses on generating natural utterances for human consumption, where fluency is important. In conclusion, the main contribution of this paper is a novel approach for semantic parsing based on a simple generation procedure and a paraphrase model. We achieve state-of-the-art results on two recently released datasets. We believe that our approach opens a window of opportunity for learning semantic parsers from raw text not necessarily related to the target KB. With more sophisticated generation and paraphrase, we hope to tackle compositionally richer utterances"
P14-1133,U06-1019,0,0.250324,"urts accuracy by only a modest amount. 5 Paraphrasing type c : What is of the music musical did Richard genres of Wagner Richard play Wagner Figure 3: Token associations extracted for a paraphrase pair. Blue and dashed (red and solid) indicate positive (negative) score. Line width is proportional to the absolute value of the score. 5.1 Once the candidate set of logical forms paired with canonical utterances is constructed, our problem is reduced to scoring pairs (c, z) based on a paraphrase model. The NLP paraphrase literature is vast and ranges from simple methods employing surface features (Wan et al., 2006), through vector space models (Socher et al., 2011), to latent variable models (Das and Smith, 2009; Wang and Manning, 2010; Stern and Dagan, 2011). In this paper, we focus on two paraphrase models that emphasize simplicity and efficiency. This is important since for each question-answer pair, we consider thousands of canonical utterances as potential paraphrases. In contrast, traditional paraphrase detection (Dolan et al., 2004) and Recognizing Textual Entailment (RTE) tasks (Dagan et al., 2013) consider examples consisting of only a single pair of candidate paraphrases. Our paraphrase model"
P14-1133,C10-1131,0,0.0606614,"er Richard play Wagner Figure 3: Token associations extracted for a paraphrase pair. Blue and dashed (red and solid) indicate positive (negative) score. Line width is proportional to the absolute value of the score. 5.1 Once the candidate set of logical forms paired with canonical utterances is constructed, our problem is reduced to scoring pairs (c, z) based on a paraphrase model. The NLP paraphrase literature is vast and ranges from simple methods employing surface features (Wan et al., 2006), through vector space models (Socher et al., 2011), to latent variable models (Das and Smith, 2009; Wang and Manning, 2010; Stern and Dagan, 2011). In this paper, we focus on two paraphrase models that emphasize simplicity and efficiency. This is important since for each question-answer pair, we consider thousands of canonical utterances as potential paraphrases. In contrast, traditional paraphrase detection (Dolan et al., 2004) and Recognizing Textual Entailment (RTE) tasks (Dagan et al., 2013) consider examples consisting of only a single pair of candidate paraphrases. Our paraphrase model decomposes into an association model and a vector space model: φpr (x, c)> θpr = φas (x, c)> θas + φvs (x, c)> θvs . x : Wh"
P14-1133,P07-1121,0,0.760219,"ed the people Henry Clay? Type.PoliticalParty u Founder.HenryClay ... Type.Event u Involved.HenryClay Whig Party Figure 1: Semantic parsing via paraphrasing: For each candidate logical form (in red), we generate canonical utterances (in purple). The model is trained to paraphrase the input utterance (in green) into the canonical utterances associated with the correct denotation (in blue). Introduction We consider the semantic parsing problem of mapping natural language utterances into logical forms to be executed on a knowledge base (KB) (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Kwiatkowski et al., 2010). Scaling semantic parsers to large knowledge bases has attracted substantial attention recently (Cai and Yates, 2013; Berant et al., 2013; Kwiatkowski et al., 2013), since it drives applications such as question answering (QA) and information extraction (IE). Semantic parsers need to somehow associate natural language phrases with logical predicates, e.g., they must learn that the constructions “What does X do for a living?”, “What is X’s profession?”, and “Who is X?”, should all map to the logical predicate Profession. To learn these mappings, traditional semantic"
P15-1096,Q13-1005,0,0.52831,"d approach that leverages the environment to induce new lexical entries at test time, even for new verbs. Our semantic parsing model jointly reasons about the text, logical forms, and environment over multi-stage instruction sequences. We introduce a new dataset and show that our approach is able to successfully ground new verbs such as distribute, mix, arrange to complex logical forms, each containing up to four predicates. 1 Ashutosh Saxena∗ asaxena@cs.cornell.edu Introduction The task of mapping natural language instructions to actions for a robot has been gaining momentum in recent years (Artzi and Zettlemoyer, 2013; Tellex et al., 2011; Misra et al., 2014; Bollini et al., 2011; Guadarrama et al., 2013; Matuszek et al., 2012b; Fasola and Mataric, 2013). We are particularly interested in instructions containing verbs such as “microwave” denoting high-level concepts, which correspond to more than 10 lowlevel symbolic actions such as grasp. In this setting, it is common to find new verbs requiring new concepts at test time. For example, in Figure 1, suppose that we have never seen the verb “fill”. Can we impute the correct interpretation, and moreover seize the opportunity to learn what “fill” means in a wa"
P15-1096,P14-1133,1,0.211099,"(microwave1 ,is-on) Unseen verb “ fill ” is grounded at test time using environment. Figure 1: A lexicon learned on the training data cannot possibly cover all the verb-concept mappings needed at test time. Our algorithm learns the meaning of new verbs (e.g., “fill”) using the environment context. Previous work in semantic parsing handles lexical coverage in one of two ways. Kwiatkowski et al. (2010) induces a highly constrained CCG lexicon capable of mapping words to complex logical forms, but it would have to skip new words (which in Figure 1 would lead to microwaving an empty cup). Others (Berant and Liang, 2014) take a freer approach by performing a search over logical forms, which can handle new words, but the logical forms there are much simpler than the ones we consider. In this paper, we present an hybrid approach that uses a lexicon to represent complex concepts but also strongly leverages the environment to guide the search space. The environment can provide helpful cues in several ways: • Only a few environments are likely for a given scenario—e.g., the text is unlikely to ask the robot to microwave an empty cup or put books on the floor. • The logical form of one segment of text constrains th"
P15-1096,D13-1160,1,0.0990104,"actoring a lexical entry into a template plus a lexeme, but the rigidity of the template remains. This is satisfactory when words map to one (or two) predicates, which is the case in most existing semantic parsing tasks. For example, in Artzi and Zettlemoyer (2013), verbs are associated with single predicates (“move” to move, “walk” to walk, etc.) In our setting, verbs contain multi-predicate postconditions, for which these techniques would not be suitable. As annotated logical forms for training semantic parsers are expensive to obtain, several works (Clarke et al., 2010; Liang et al., 2011; Berant et al., 2013; Kwiatkowski et al., 2013) have developed methods to learn from weaker supervision, and as in our work, use the execution of the logical forms to guide the search. Our supervision is even weaker in that we are able to learn at test time grasping(robot, icecream2 )∧grasping(robot, syrup1 ) While this postcondition uses all the objects described in the text, the environment-based features suggest it makes little sense for the task to end with the robot eternally grasping objects. For the second example, alternate postconditions considered included: 1. on(pillow1 , pillow2 ) ∧ on(pillow3 , pillo"
P15-1096,P09-1010,0,0.23517,"Missing"
P15-1096,P10-1129,0,0.0843585,"Missing"
P15-1096,P12-1014,0,0.0456851,"efire1) ∧ state(kettle, water) 109 “change the channel to a movie” state(tv1 , channel4) ∧ on(book1 , loveseat1 ) 98 ment. The VEIL baseline used actions for representation and does not generalize as well as the postconditions in our logical forms. It is also instructive to examine the alternate postconditions that the search procedure considers. For the first example in Table 2, the following postcondition was considered by not selected: ever, they can only handle simple actions, whereas our planner and simulator allows us to work with postconditions, and thus tackle high-level instructions. Branavan et al. (2012) extract precondition relations from text, learn to map text to subgoals (postconditions) for a planner. However, their postconditions are atomic, whereas ours are complex conjunctions. Other works (Chen and Mooney, 2011; Kim and Mooney, 2012; Kollar et al., 2010; Fasola and Mataric, 2013) have focused only on navigational verbs and spatial relations, but do not handle highlevel verbs. Artzi and Zettlemoyer (2013) also fall into the above category and offer a more compositional treatment. They focus on how words compose; we focus on unraveling single words. The broader problem of grounded lang"
P15-1096,W10-2903,0,0.106433,"t al. (2011) enhanced generalization by factoring a lexical entry into a template plus a lexeme, but the rigidity of the template remains. This is satisfactory when words map to one (or two) predicates, which is the case in most existing semantic parsing tasks. For example, in Artzi and Zettlemoyer (2013), verbs are associated with single predicates (“move” to move, “walk” to walk, etc.) In our setting, verbs contain multi-predicate postconditions, for which these techniques would not be suitable. As annotated logical forms for training semantic parsers are expensive to obtain, several works (Clarke et al., 2010; Liang et al., 2011; Berant et al., 2013; Kwiatkowski et al., 2013) have developed methods to learn from weaker supervision, and as in our work, use the execution of the logical forms to guide the search. Our supervision is even weaker in that we are able to learn at test time grasping(robot, icecream2 )∧grasping(robot, syrup1 ) While this postcondition uses all the objects described in the text, the environment-based features suggest it makes little sense for the task to end with the robot eternally grasping objects. For the second example, alternate postconditions considered included: 1. on"
P15-1096,W05-0614,0,0.0390988,"and Mooney, 2011; Kim and Mooney, 2012; Kollar et al., 2010; Fasola and Mataric, 2013) have focused only on navigational verbs and spatial relations, but do not handle highlevel verbs. Artzi and Zettlemoyer (2013) also fall into the above category and offer a more compositional treatment. They focus on how words compose; we focus on unraveling single words. The broader problem of grounded language acquisition, involving connecting words to aspects of a situated context has been heavily studied (Duvallet et al., 2014; Yu and Siskind, 2013; Chu et al., 2013; Chen and Mooney, 2008; Mooney, 2008; Fleischman and Roy, 2005; Liang et al., 2009). Semantic Parsing. In semantic parsing, much work has leveraged CCG (Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007; Kwiatkowski et al., 2010). One challenge behind lexically-heavy approaches is ensuring adequate lexical coverage. Kwiatkowski et al. (2011) enhanced generalization by factoring a lexical entry into a template plus a lexeme, but the rigidity of the template remains. This is satisfactory when words map to one (or two) predicates, which is the case in most existing semantic parsing tasks. For example, in Artzi and Zettlemoyer (2013), verbs are as"
P15-1096,D12-1040,0,0.489648,"rms. It is also instructive to examine the alternate postconditions that the search procedure considers. For the first example in Table 2, the following postcondition was considered by not selected: ever, they can only handle simple actions, whereas our planner and simulator allows us to work with postconditions, and thus tackle high-level instructions. Branavan et al. (2012) extract precondition relations from text, learn to map text to subgoals (postconditions) for a planner. However, their postconditions are atomic, whereas ours are complex conjunctions. Other works (Chen and Mooney, 2011; Kim and Mooney, 2012; Kollar et al., 2010; Fasola and Mataric, 2013) have focused only on navigational verbs and spatial relations, but do not handle highlevel verbs. Artzi and Zettlemoyer (2013) also fall into the above category and offer a more compositional treatment. They focus on how words compose; we focus on unraveling single words. The broader problem of grounded language acquisition, involving connecting words to aspects of a situated context has been heavily studied (Duvallet et al., 2014; Yu and Siskind, 2013; Chu et al., 2013; Chen and Mooney, 2008; Mooney, 2008; Fleischman and Roy, 2005; Liang et al."
P15-1096,P03-1054,0,0.125495,"ext “if any of the pots has food” There are two types of conditional nodes: branching and temporal. A branching conditional node represents an “if ” statement and has two successor nodes corresponding to whether the condition evaluates to true or false in the current environment. A temporal conditional node represents an “until” statement and waits until the condition is false in the environment. 3.2 Formal Overview Shallow Parsing. We deterministically convert the text x into its control flow graph G using a set of manual rules applied on its constituency parse tree from the Stanford parser (Klein and Manning, 2003). Conditionals in our dataset are simple and can be converted into postconditions directly using a few rules, unlike the action verbs (e.g., “fill”), which is the focus of this paper. The details of our shallow parsing procedure is described in the appendix. Given an environment e1 , G is reduced to a single sequence of frame nodes c1 , . . . , ck , by evaluating all the branch conditionals on e1 . Semantic Parsing Model. For each frame node ci and given the current environment ei , the semantic parsing model (Section 5) places a distribution over logical forms zi . This logical form zi repres"
P15-1096,D10-1119,0,0.157138,"” means in a way that generalizes to future instructions? Text: “get the cup, fill it with water and then microwave the cup” grasping cup3 ∧ near(robot1 ,cup3 ) state cup3 ,water ∧ on(cup3 ,sink) in cup3 ,microwave ∧ state(microwave1 ,is-on) Unseen verb “ fill ” is grounded at test time using environment. Figure 1: A lexicon learned on the training data cannot possibly cover all the verb-concept mappings needed at test time. Our algorithm learns the meaning of new verbs (e.g., “fill”) using the environment context. Previous work in semantic parsing handles lexical coverage in one of two ways. Kwiatkowski et al. (2010) induces a highly constrained CCG lexicon capable of mapping words to complex logical forms, but it would have to skip new words (which in Figure 1 would lead to microwaving an empty cup). Others (Berant and Liang, 2014) take a freer approach by performing a search over logical forms, which can handle new words, but the logical forms there are much simpler than the ones we consider. In this paper, we present an hybrid approach that uses a lexicon to represent complex concepts but also strongly leverages the environment to guide the search space. The environment can provide helpful cues in seve"
P15-1096,D11-1140,0,0.0115763,"hey focus on how words compose; we focus on unraveling single words. The broader problem of grounded language acquisition, involving connecting words to aspects of a situated context has been heavily studied (Duvallet et al., 2014; Yu and Siskind, 2013; Chu et al., 2013; Chen and Mooney, 2008; Mooney, 2008; Fleischman and Roy, 2005; Liang et al., 2009). Semantic Parsing. In semantic parsing, much work has leveraged CCG (Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007; Kwiatkowski et al., 2010). One challenge behind lexically-heavy approaches is ensuring adequate lexical coverage. Kwiatkowski et al. (2011) enhanced generalization by factoring a lexical entry into a template plus a lexeme, but the rigidity of the template remains. This is satisfactory when words map to one (or two) predicates, which is the case in most existing semantic parsing tasks. For example, in Artzi and Zettlemoyer (2013), verbs are associated with single predicates (“move” to move, “walk” to walk, etc.) In our setting, verbs contain multi-predicate postconditions, for which these techniques would not be suitable. As annotated logical forms for training semantic parsers are expensive to obtain, several works (Clarke et al"
P15-1096,D13-1161,0,0.0170647,"try into a template plus a lexeme, but the rigidity of the template remains. This is satisfactory when words map to one (or two) predicates, which is the case in most existing semantic parsing tasks. For example, in Artzi and Zettlemoyer (2013), verbs are associated with single predicates (“move” to move, “walk” to walk, etc.) In our setting, verbs contain multi-predicate postconditions, for which these techniques would not be suitable. As annotated logical forms for training semantic parsers are expensive to obtain, several works (Clarke et al., 2010; Liang et al., 2011; Berant et al., 2013; Kwiatkowski et al., 2013) have developed methods to learn from weaker supervision, and as in our work, use the execution of the logical forms to guide the search. Our supervision is even weaker in that we are able to learn at test time grasping(robot, icecream2 )∧grasping(robot, syrup1 ) While this postcondition uses all the objects described in the text, the environment-based features suggest it makes little sense for the task to end with the robot eternally grasping objects. For the second example, alternate postconditions considered included: 1. on(pillow1 , pillow2 ) ∧ on(pillow3 , pillow4 ) 2. ∧4j=1 on(pillowj ,"
P15-1096,P09-1011,1,0.808597,"Mooney, 2012; Kollar et al., 2010; Fasola and Mataric, 2013) have focused only on navigational verbs and spatial relations, but do not handle highlevel verbs. Artzi and Zettlemoyer (2013) also fall into the above category and offer a more compositional treatment. They focus on how words compose; we focus on unraveling single words. The broader problem of grounded language acquisition, involving connecting words to aspects of a situated context has been heavily studied (Duvallet et al., 2014; Yu and Siskind, 2013; Chu et al., 2013; Chen and Mooney, 2008; Mooney, 2008; Fleischman and Roy, 2005; Liang et al., 2009). Semantic Parsing. In semantic parsing, much work has leveraged CCG (Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007; Kwiatkowski et al., 2010). One challenge behind lexically-heavy approaches is ensuring adequate lexical coverage. Kwiatkowski et al. (2011) enhanced generalization by factoring a lexical entry into a template plus a lexeme, but the rigidity of the template remains. This is satisfactory when words map to one (or two) predicates, which is the case in most existing semantic parsing tasks. For example, in Artzi and Zettlemoyer (2013), verbs are associated with single"
P15-1096,P11-1060,1,0.682641,"generalization by factoring a lexical entry into a template plus a lexeme, but the rigidity of the template remains. This is satisfactory when words map to one (or two) predicates, which is the case in most existing semantic parsing tasks. For example, in Artzi and Zettlemoyer (2013), verbs are associated with single predicates (“move” to move, “walk” to walk, etc.) In our setting, verbs contain multi-predicate postconditions, for which these techniques would not be suitable. As annotated logical forms for training semantic parsers are expensive to obtain, several works (Clarke et al., 2010; Liang et al., 2011; Berant et al., 2013; Kwiatkowski et al., 2013) have developed methods to learn from weaker supervision, and as in our work, use the execution of the logical forms to guide the search. Our supervision is even weaker in that we are able to learn at test time grasping(robot, icecream2 )∧grasping(robot, syrup1 ) While this postcondition uses all the objects described in the text, the environment-based features suggest it makes little sense for the task to end with the robot eternally grasping objects. For the second example, alternate postconditions considered included: 1. on(pillow1 , pillow2 )"
P15-1096,D07-1071,0,0.218243,"orm zi produced by each postcondition. We then take the conjunction of every pair of postconditions corresponding to the 200 highest-scoring logical forms. This gives us new set of postconditions on which we repeat the generalization-scoring-conjunction cycle. We keep doing this while the scores of the new logical forms is increasing or while there are logical forms remaining. If a logical form z = ([ν ⇒ (λ~v .S, ∅)], ξ) is used by the predicted action sequence, we add the lexical entry [ν ⇒ (λ~v .S, ξ)] to the lexicon Λ. This is different to other lexicon induction procedures such as GENLEX (Zettlemoyer and Collins, 2007) which are done at training time only and require more supervision. Moreover, GENLEX does not use the environment context in creating new lexical entries and thus is not appropriate at test time, since it would vastly overgenerate lexical entries compared to our approach. For us, the environment thus provides implicit supervision for lexicon induction. 8 Inference and Parameter Estimation Inference. Given a text x (which is converted to c1:k via Section 3.2) and an initial environment e1 , we wish to predict an action sequence a based on pθ (a1:k |c1:k , e1 ), which marginalizes over all logic"
P15-1096,J03-1002,0,0.0050596,"correlation ρ(ω, o) using the following approach: • If ω is a pronoun, ρ(ω, o) is the ratio of the position of the last reference of o to the length of the action sequence computed so far, thus preferring recent objects. • Otherwise, we compute the correlation using various sources: the object’s category; the object’s state for handling metonymy (e.g., the description “coffee” correlates well with the object mug1 if mug1 contains coffee— state(mug1 , has-coffee) is true), WordNet (Fellbaum, 1998) for dealing synonymy and hyponymy; and word alignments between the objects and text from Giza++ (Och and Ney, 2003) to learn domain-specific references (e.g., “Guinness book” refers to book1 , not book2 ). More details can be found in the supplemental material. 6 Lexicon Induction from Training Data In order to map text to logical forms, we first induce an initial anchored lexicon Λ from the training data {(x(m) , e(m) , a(m) , π (m) )}M m=1 . At test time, we add new lexical entries (Section 7) to Λ. Recall that shallow parsing x(m) yields a list of frame nodes c1:k . For each frame node ci and its aligned action sequence ai , we take the conjunction of all the atoms (and their negations) which are false"
P15-1096,P10-1083,0,0.117467,"Missing"
P15-1096,P13-1006,0,0.0465375,"ditions are atomic, whereas ours are complex conjunctions. Other works (Chen and Mooney, 2011; Kim and Mooney, 2012; Kollar et al., 2010; Fasola and Mataric, 2013) have focused only on navigational verbs and spatial relations, but do not handle highlevel verbs. Artzi and Zettlemoyer (2013) also fall into the above category and offer a more compositional treatment. They focus on how words compose; we focus on unraveling single words. The broader problem of grounded language acquisition, involving connecting words to aspects of a situated context has been heavily studied (Duvallet et al., 2014; Yu and Siskind, 2013; Chu et al., 2013; Chen and Mooney, 2008; Mooney, 2008; Fleischman and Roy, 2005; Liang et al., 2009). Semantic Parsing. In semantic parsing, much work has leveraged CCG (Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007; Kwiatkowski et al., 2010). One challenge behind lexically-heavy approaches is ensuring adequate lexical coverage. Kwiatkowski et al. (2011) enhanced generalization by factoring a lexical entry into a template plus a lexeme, but the rigidity of the template remains. This is satisfactory when words map to one (or two) predicates, which is the case in most existing s"
P15-1129,D11-1039,0,0.00781714,"compositionality. Moreover, specialized domains with specialized jargon will always exist, e.g., in regular expressions (Kushman and Barzilay, 2013) or grounding to perception (Matuszek et al., 2012; Tellex et al., 2011; Krishnamurthy and Kollar, 2013). Therefore, we believe building a targeted domain-specific semantic parser for a new website or device is a very practical goal. Recent work has made significant strides in reducing supervision from logical forms (Zettlemoyer and Collins, 2005; Wong and Mooney, 2007) to denotations (Clarke et al., 2010; Liang et al., 2011) and to weaker forms (Artzi and Zettlemoyer, 2011; Reddy et al., 2014). All of these works presuppose having input utterances, which do not exist in a new domain. Our methodology overcomes this hurdle by exploiting a very lightweight form of annotation: paraphrasing. Paraphrasing has been applied to singleproperty question answering (Fader et al., 2013) and semantic parsing (Berant and Liang, 2014). We not only use paraphrasing in the semantic parser, but also for data collection. Table 2 might evoke rule-based systems (Woods et al., 1972; Warren and Pereira, 1982) or controlled natural languages (Schwitter, 2010). However, there is an impor"
P15-1129,Q13-1005,0,0.079901,"person that is author of the most number of article argmax(type.person, R(λx.count(type.article u author.x))) ... (3) via crowdsourcing (∼5 hours) Paraphrases what is the newest published article? who has published the most articles? ... (4) by training a paraphrasing model Semantic parser Introduction By mapping natural language utterances to executable logical forms, semantic parsers have been useful for a variety of applications requiring precise language understanding (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Liang et al., 2011; Berant et al., 2013; Kwiatkowski et al., 2013; Artzi and Zettlemoyer, 2013; Kushman and Barzilay, 2013). Previous work has focused on how to train a semantic parser given input utterances, but suppose we wanted to build a semantic parser for a new domain—for example, a natural language interface into a publications database. Since no such interface exists, we do not even have a naturally occurring source of input utterances that we can annotate. So where do we start? In this paper, we advocate a functionalitydriven process for rapidly building a semantic ∗ Both authors equally contributed to the paper. Figure 1: Functionality-driven process for building semantic par"
P15-1129,P14-1133,1,0.955127,"al., 2013) contains no questions with numeric answers, so any semantic parser trained on that dataset would lack that functionality. These biases are not codified, which results in an idiosyncratic and mysterious user experience, a major drawback of natural language interfaces (Rangel et al., 2014). In contrast, our compact grammar precisely specifies the logical functionality. We enforce completeness by generating canonical utterances that exercise every grammar rule. In terms of supervision, state-of-the-art semantic parsers are trained from question-answer pairs (Kwiatkowski et al., 2013; Berant and Liang, 2014). Although this is a marked improvement in cost and scalability compared to annotated logical forms, it still requires non-trivial effort: the annotator must (i) understand the question and (ii) figure out the answer, which becomes even harder with compositional utterances. In contrast, our main source of supervision is paraphrases, which only requires (i), not (ii). Such data is thus cheaper and faster to obtain. Linguistic reflections. The centerpiece of our framework is a domain-general grammar that connects logical forms with canonical utterances. This connection warrants further scrutiny,"
P15-1129,D13-1160,1,0.752966,"ion date argmax(type.article, publicationDate) person that is author of the most number of article argmax(type.person, R(λx.count(type.article u author.x))) ... (3) via crowdsourcing (∼5 hours) Paraphrases what is the newest published article? who has published the most articles? ... (4) by training a paraphrasing model Semantic parser Introduction By mapping natural language utterances to executable logical forms, semantic parsers have been useful for a variety of applications requiring precise language understanding (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Liang et al., 2011; Berant et al., 2013; Kwiatkowski et al., 2013; Artzi and Zettlemoyer, 2013; Kushman and Barzilay, 2013). Previous work has focused on how to train a semantic parser given input utterances, but suppose we wanted to build a semantic parser for a new domain—for example, a natural language interface into a publications database. Since no such interface exists, we do not even have a naturally occurring source of input utterances that we can annotate. So where do we start? In this paper, we advocate a functionalitydriven process for rapidly building a semantic ∗ Both authors equally contributed to the paper. Figure 1:"
P15-1129,P13-1042,0,0.085091,"Missing"
P15-1129,P11-1060,1,0.833794,"the largest publication date argmax(type.article, publicationDate) person that is author of the most number of article argmax(type.person, R(λx.count(type.article u author.x))) ... (3) via crowdsourcing (∼5 hours) Paraphrases what is the newest published article? who has published the most articles? ... (4) by training a paraphrasing model Semantic parser Introduction By mapping natural language utterances to executable logical forms, semantic parsers have been useful for a variety of applications requiring precise language understanding (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Liang et al., 2011; Berant et al., 2013; Kwiatkowski et al., 2013; Artzi and Zettlemoyer, 2013; Kushman and Barzilay, 2013). Previous work has focused on how to train a semantic parser given input utterances, but suppose we wanted to build a semantic parser for a new domain—for example, a natural language interface into a publications database. Since no such interface exists, we do not even have a naturally occurring source of input utterances that we can annotate. So where do we start? In this paper, we advocate a functionalitydriven process for rapidly building a semantic ∗ Both authors equally contributed to"
P15-1129,J04-4002,0,0.041243,"bigrams in x and c, if they share a lemma or are aligned in the PPDB resource (Ganitkevitch et al., 2013). We count the number of exact matches, PPDB matches, and unmatched words. To obtain lexical features, we run the Berkeley Aligner (Liang et al., 2006) on the training set and compute conditional probabilities of aligning one word type to another. Based on these probabilities we compute a maximum weight alignment A between words in x and c. We define features over A (see Table 4). We also use the word alignments to construct a phrase table by applying the consistent phrase pair heuristic (Och and Ney, 2004). We define an indicator feature for every phrase pair of x and c that appear in the phrase table. Examples from the P UBLICATIONS domain include fewest– least number and by–whose author is. Note that we do not build a hard lexicon but only use A and the phrase table to define features, allowing the model to learn useful paraphrases during training. Finally, we define standard features on logical forms and denotations (Berant et al., 2013). 7 Experimental Evaluation We evaluated our functionality-driven process on the seven domains described in Section 5 and one new domain we describe in Secti"
P15-1129,P15-1142,1,0.816375,"ch search state, which includes the syntactic category s (e.g., NP) and the depth of the logical form, we generate at most K = 20 candidates by applying the rules in Table 2. In practice, the lexical rules T(x) are applied first, and composition is performed, but not constrained to the utterance. For example, the utterance “article” would generate the logical form count(type.article). Instead, soft paraphrasing features are used to guide the search. This rather unorthodox approach to semantic parsing can be seen as a generalization of Berant and Liang (2014) and is explained in more detail in Pasupat and Liang (2015). Training. We train our model by maximizing the regularized log-likelihood O(θ) = 1337 Domain C ALENDAR # pred. 22 # ex. 837 Phenomena temporal language B LOCKS 19 1995 spatial language H OUSING 24 941 measurement units R ESTAURANTS 32 1657 long unary relations P UBLICATIONS 15 801 S OCIAL 45 4419 multi-arity relations BASKETBALL 24 1952 parentheticals sublexical compositionality Example x: “Show me meetings after the weekly standup day” c: “meeting whose date is at least date of weekly standup” z: type.meeting u date. > R(date).weeklyStandup x: “Select the brick that is to the furthest left."
P15-1129,W10-2903,0,0.0167669,"n answering datasets (Free917 and WebQuestions) are limited in compositionality. Moreover, specialized domains with specialized jargon will always exist, e.g., in regular expressions (Kushman and Barzilay, 2013) or grounding to perception (Matuszek et al., 2012; Tellex et al., 2011; Krishnamurthy and Kollar, 2013). Therefore, we believe building a targeted domain-specific semantic parser for a new website or device is a very practical goal. Recent work has made significant strides in reducing supervision from logical forms (Zettlemoyer and Collins, 2005; Wong and Mooney, 2007) to denotations (Clarke et al., 2010; Liang et al., 2011) and to weaker forms (Artzi and Zettlemoyer, 2011; Reddy et al., 2014). All of these works presuppose having input utterances, which do not exist in a new domain. Our methodology overcomes this hurdle by exploiting a very lightweight form of annotation: paraphrasing. Paraphrasing has been applied to singleproperty question answering (Fader et al., 2013) and semantic parsing (Berant and Liang, 2014). We not only use paraphrasing in the semantic parser, but also for data collection. Table 2 might evoke rule-based systems (Woods et al., 1972; Warren and Pereira, 1982) or cont"
P15-1129,Q14-1030,0,0.145706,"specialized domains with specialized jargon will always exist, e.g., in regular expressions (Kushman and Barzilay, 2013) or grounding to perception (Matuszek et al., 2012; Tellex et al., 2011; Krishnamurthy and Kollar, 2013). Therefore, we believe building a targeted domain-specific semantic parser for a new website or device is a very practical goal. Recent work has made significant strides in reducing supervision from logical forms (Zettlemoyer and Collins, 2005; Wong and Mooney, 2007) to denotations (Clarke et al., 2010; Liang et al., 2011) and to weaker forms (Artzi and Zettlemoyer, 2011; Reddy et al., 2014). All of these works presuppose having input utterances, which do not exist in a new domain. Our methodology overcomes this hurdle by exploiting a very lightweight form of annotation: paraphrasing. Paraphrasing has been applied to singleproperty question answering (Fader et al., 2013) and semantic parsing (Berant and Liang, 2014). We not only use paraphrasing in the semantic parser, but also for data collection. Table 2 might evoke rule-based systems (Woods et al., 1972; Warren and Pereira, 1982) or controlled natural languages (Schwitter, 2010). However, there is an important distinction: the"
P15-1129,P13-1158,0,0.0467733,"emantic parser for a new website or device is a very practical goal. Recent work has made significant strides in reducing supervision from logical forms (Zettlemoyer and Collins, 2005; Wong and Mooney, 2007) to denotations (Clarke et al., 2010; Liang et al., 2011) and to weaker forms (Artzi and Zettlemoyer, 2011; Reddy et al., 2014). All of these works presuppose having input utterances, which do not exist in a new domain. Our methodology overcomes this hurdle by exploiting a very lightweight form of annotation: paraphrasing. Paraphrasing has been applied to singleproperty question answering (Fader et al., 2013) and semantic parsing (Berant and Liang, 2014). We not only use paraphrasing in the semantic parser, but also for data collection. Table 2 might evoke rule-based systems (Woods et al., 1972; Warren and Pereira, 1982) or controlled natural languages (Schwitter, 2010). However, there is an important distinction: the grammar need only connect a logical form to one canonical utterance; it is not used directly for parsing. This relaxation allows the grammar to be much simpler. Our philosophy is to use the simple domain-general grammar to carry the torch just to the point of being understandable by"
P15-1129,N13-1092,0,0.0451735,"j) ∈ A. (xi:i , cj−1:j ) ∀(i, j), (i + 1, j + 1) ∈ A. (xi:i+1 , cj:j+1 ) all unaligned words in x and c (xi:j , ci0 :j 0 ) if in phrase table Table 4: Features for the paraphrasing model. pos(xi:i ) is the POS tag; type(JzKw ) is a coarse semantic type for the denotation (an entity or a number). A is a maximum weight alignment between x and c. P (x,c,z)∈D log pθ (z, c |x, w) − λkθk1 . To optimize, we use AdaGrad (Duchi et al., 2010). Features Table 4 describes the features. Our basic features mainly match words and bigrams in x and c, if they share a lemma or are aligned in the PPDB resource (Ganitkevitch et al., 2013). We count the number of exact matches, PPDB matches, and unmatched words. To obtain lexical features, we run the Berkeley Aligner (Liang et al., 2006) on the training set and compute conditional probabilities of aligning one word type to another. Based on these probabilities we compute a maximum weight alignment A between words in x and c. We define features over A (see Table 4). We also use the word alignments to construct a phrase table by applying the consistent phrase pair heuristic (Och and Ney, 2004). We define an indicator feature for every phrase pair of x and c that appear in the phr"
P15-1129,Q13-1016,0,0.00746168,"onsiderable room for improvement in the paraphrasing model. 8 Related work and discussion Much of current excitement around semantic parsing emphasizes large knowledge bases such as Freebase (Cai and Yates, 2013; Kwiatkowski et al., 2013; Berant et al., 2013). However, despite the apparent scale, the actual question answering datasets (Free917 and WebQuestions) are limited in compositionality. Moreover, specialized domains with specialized jargon will always exist, e.g., in regular expressions (Kushman and Barzilay, 2013) or grounding to perception (Matuszek et al., 2012; Tellex et al., 2011; Krishnamurthy and Kollar, 2013). Therefore, we believe building a targeted domain-specific semantic parser for a new website or device is a very practical goal. Recent work has made significant strides in reducing supervision from logical forms (Zettlemoyer and Collins, 2005; Wong and Mooney, 2007) to denotations (Clarke et al., 2010; Liang et al., 2011) and to weaker forms (Artzi and Zettlemoyer, 2011; Reddy et al., 2014). All of these works presuppose having input utterances, which do not exist in a new domain. Our methodology overcomes this hurdle by exploiting a very lightweight form of annotation: paraphrasing. Paraphr"
P15-1129,C10-2128,0,0.00740735,"to weaker forms (Artzi and Zettlemoyer, 2011; Reddy et al., 2014). All of these works presuppose having input utterances, which do not exist in a new domain. Our methodology overcomes this hurdle by exploiting a very lightweight form of annotation: paraphrasing. Paraphrasing has been applied to singleproperty question answering (Fader et al., 2013) and semantic parsing (Berant and Liang, 2014). We not only use paraphrasing in the semantic parser, but also for data collection. Table 2 might evoke rule-based systems (Woods et al., 1972; Warren and Pereira, 1982) or controlled natural languages (Schwitter, 2010). However, there is an important distinction: the grammar need only connect a logical form to one canonical utterance; it is not used directly for parsing. This relaxation allows the grammar to be much simpler. Our philosophy is to use the simple domain-general grammar to carry the torch just to the point of being understandable by a human, and let the human perform the remaining correction to produce a natural utterance. In summary, our contributions are two-fold: a new functionality-driven process and an exploration of some of its linguistic implications. We believe that our methodology is a"
P15-1129,J82-3002,0,0.153592,"denotations (Clarke et al., 2010; Liang et al., 2011) and to weaker forms (Artzi and Zettlemoyer, 2011; Reddy et al., 2014). All of these works presuppose having input utterances, which do not exist in a new domain. Our methodology overcomes this hurdle by exploiting a very lightweight form of annotation: paraphrasing. Paraphrasing has been applied to singleproperty question answering (Fader et al., 2013) and semantic parsing (Berant and Liang, 2014). We not only use paraphrasing in the semantic parser, but also for data collection. Table 2 might evoke rule-based systems (Woods et al., 1972; Warren and Pereira, 1982) or controlled natural languages (Schwitter, 2010). However, there is an important distinction: the grammar need only connect a logical form to one canonical utterance; it is not used directly for parsing. This relaxation allows the grammar to be much simpler. Our philosophy is to use the simple domain-general grammar to carry the torch just to the point of being understandable by a human, and let the human perform the remaining correction to produce a natural utterance. In summary, our contributions are two-fold: a new functionality-driven process and an exploration of some of its linguistic"
P15-1129,N13-1103,0,0.217487,"most number of article argmax(type.person, R(λx.count(type.article u author.x))) ... (3) via crowdsourcing (∼5 hours) Paraphrases what is the newest published article? who has published the most articles? ... (4) by training a paraphrasing model Semantic parser Introduction By mapping natural language utterances to executable logical forms, semantic parsers have been useful for a variety of applications requiring precise language understanding (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Liang et al., 2011; Berant et al., 2013; Kwiatkowski et al., 2013; Artzi and Zettlemoyer, 2013; Kushman and Barzilay, 2013). Previous work has focused on how to train a semantic parser given input utterances, but suppose we wanted to build a semantic parser for a new domain—for example, a natural language interface into a publications database. Since no such interface exists, we do not even have a naturally occurring source of input utterances that we can annotate. So where do we start? In this paper, we advocate a functionalitydriven process for rapidly building a semantic ∗ Both authors equally contributed to the paper. Figure 1: Functionality-driven process for building semantic parsers. The two red boxes are t"
P15-1129,P07-1121,0,0.0103617,"the apparent scale, the actual question answering datasets (Free917 and WebQuestions) are limited in compositionality. Moreover, specialized domains with specialized jargon will always exist, e.g., in regular expressions (Kushman and Barzilay, 2013) or grounding to perception (Matuszek et al., 2012; Tellex et al., 2011; Krishnamurthy and Kollar, 2013). Therefore, we believe building a targeted domain-specific semantic parser for a new website or device is a very practical goal. Recent work has made significant strides in reducing supervision from logical forms (Zettlemoyer and Collins, 2005; Wong and Mooney, 2007) to denotations (Clarke et al., 2010; Liang et al., 2011) and to weaker forms (Artzi and Zettlemoyer, 2011; Reddy et al., 2014). All of these works presuppose having input utterances, which do not exist in a new domain. Our methodology overcomes this hurdle by exploiting a very lightweight form of annotation: paraphrasing. Paraphrasing has been applied to singleproperty question answering (Fader et al., 2013) and semantic parsing (Berant and Liang, 2014). We not only use paraphrasing in the semantic parser, but also for data collection. Table 2 might evoke rule-based systems (Woods et al., 197"
P15-1129,D13-1161,0,0.154588,"article, publicationDate) person that is author of the most number of article argmax(type.person, R(λx.count(type.article u author.x))) ... (3) via crowdsourcing (∼5 hours) Paraphrases what is the newest published article? who has published the most articles? ... (4) by training a paraphrasing model Semantic parser Introduction By mapping natural language utterances to executable logical forms, semantic parsers have been useful for a variety of applications requiring precise language understanding (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Liang et al., 2011; Berant et al., 2013; Kwiatkowski et al., 2013; Artzi and Zettlemoyer, 2013; Kushman and Barzilay, 2013). Previous work has focused on how to train a semantic parser given input utterances, but suppose we wanted to build a semantic parser for a new domain—for example, a natural language interface into a publications database. Since no such interface exists, we do not even have a naturally occurring source of input utterances that we can annotate. So where do we start? In this paper, we advocate a functionalitydriven process for rapidly building a semantic ∗ Both authors equally contributed to the paper. Figure 1: Functionality-driven proc"
P15-1129,N06-1014,1,0.127494,"s for the paraphrasing model. pos(xi:i ) is the POS tag; type(JzKw ) is a coarse semantic type for the denotation (an entity or a number). A is a maximum weight alignment between x and c. P (x,c,z)∈D log pθ (z, c |x, w) − λkθk1 . To optimize, we use AdaGrad (Duchi et al., 2010). Features Table 4 describes the features. Our basic features mainly match words and bigrams in x and c, if they share a lemma or are aligned in the PPDB resource (Ganitkevitch et al., 2013). We count the number of exact matches, PPDB matches, and unmatched words. To obtain lexical features, we run the Berkeley Aligner (Liang et al., 2006) on the training set and compute conditional probabilities of aligning one word type to another. Based on these probabilities we compute a maximum weight alignment A between words in x and c. We define features over A (see Table 4). We also use the word alignments to construct a phrase table by applying the consistent phrase pair heuristic (Och and Ney, 2004). We define an indicator feature for every phrase pair of x and c that appear in the phrase table. Examples from the P UBLICATIONS domain include fewest– least number and by–whose author is. Note that we do not build a hard lexicon but onl"
P15-1129,J13-2005,1,\N,Missing
P15-1142,P14-1133,1,0.850186,"lorida?”), our dataset contains fairly compositional questions on average. To parse a compositional utterance, many works rely on a lexicon that translates phrases to entities, relations, and logical operations. A lexicon can be automatically generated (Unger and Cimiano, 2011; Unger et al., 2012), learned from data (Zettlemoyer and Collins, 2007; Kwiatkowski et al., 2011), or extracted from external sources (Cai and Yates, 2013; Berant et al., 2013), but requires some techniques to generalize to unseen data. Our work takes a different approach similar to the logical form growing algorithm in Berant and Liang (2014) by not anchoring relations and operations to the utterance. Knowledge domain. Recent works on semantic parsing for question answering operate on more open and diverse data domains. In particular, large-scale knowledge bases have gained popularity in the semantic parsing community (Cai and Yates, 2013; Berant et al., 2013; Fader et al., 2014). The increasing number of relations and entities motivates new resources and techniques for improving the accuracy, including the use of ontology matching models (Kwiatkowski et al., 2013), paraphrase models (Fader et al., 2013; Berant and Liang, 2014), a"
P15-1142,D13-1160,1,0.699109,", yi )}. Introduction In semantic parsing for question answering, natural language questions are converted into logical forms, which can be executed on a knowledge source to obtain answer denotations. Early semantic parsing systems were trained to answer highly compositional questions, but the knowledge sources were limited to small closed-domain databases (Zelle and Mooney, 1996; Wong and Mooney, 2007; Zettlemoyer and Collins, 2007; Kwiatkowski et al., 2011). More recent work sacrifices compositionality in favor of using more open-ended knowledge bases such as Freebase (Cai and Yates, 2013; Berant et al., 2013; Fader et al., 2014; Reddy et al., 2014). However, even these broader knowledge sources still define a rigid schema over entities and relation types, thus restricting the scope of answerable questions. To simultaneously increase both the breadth of the knowledge source and the depth of logical compositionality, we propose a new task (with an associated dataset): answering a question using an HTML table as the knowledge source. Figure 1 shows several question-answer pairs and an accompanying table, which are typical of those in our dataset. Note that the questions are logically quite complex,"
P15-1142,P13-1042,0,0.197698,"wer triples {(xi , ti , yi )}. Introduction In semantic parsing for question answering, natural language questions are converted into logical forms, which can be executed on a knowledge source to obtain answer denotations. Early semantic parsing systems were trained to answer highly compositional questions, but the knowledge sources were limited to small closed-domain databases (Zelle and Mooney, 1996; Wong and Mooney, 2007; Zettlemoyer and Collins, 2007; Kwiatkowski et al., 2011). More recent work sacrifices compositionality in favor of using more open-ended knowledge bases such as Freebase (Cai and Yates, 2013; Berant et al., 2013; Fader et al., 2014; Reddy et al., 2014). However, even these broader knowledge sources still define a rigid schema over entities and relation types, thus restricting the scope of answerable questions. To simultaneously increase both the breadth of the knowledge source and the depth of logical compositionality, we propose a new task (with an associated dataset): answering a question using an HTML table as the knowledge source. Figure 1 shows several question-answer pairs and an accompanying table, which are typical of those in our dataset. Note that the questions are logi"
P15-1142,P13-1158,0,0.0183449,"m growing algorithm in Berant and Liang (2014) by not anchoring relations and operations to the utterance. Knowledge domain. Recent works on semantic parsing for question answering operate on more open and diverse data domains. In particular, large-scale knowledge bases have gained popularity in the semantic parsing community (Cai and Yates, 2013; Berant et al., 2013; Fader et al., 2014). The increasing number of relations and entities motivates new resources and techniques for improving the accuracy, including the use of ontology matching models (Kwiatkowski et al., 2013), paraphrase models (Fader et al., 2013; Berant and Liang, 2014), and unlabeled sentences (Krishnamurthy and Kollar, 2013; Reddy et al., 2014). Our work leverages open-ended data from the Web through semi-structured tables. There have been several studies on analyzing or inferring the table schemas (Cafarella et al., 2008; Venetis et al., 2011; Syed et al., 2010; Limaye et al., 2010) and answering search queries by joining tables on similar columns (Cafarella et al., 2008; Gonzalez et al., 2010; Pimplikar and Sarawagi, 2012). While the latter is similar to question answering, the queries tend to be keyword lists instead of natural"
P15-1142,D13-1161,0,0.0538519,"different approach similar to the logical form growing algorithm in Berant and Liang (2014) by not anchoring relations and operations to the utterance. Knowledge domain. Recent works on semantic parsing for question answering operate on more open and diverse data domains. In particular, large-scale knowledge bases have gained popularity in the semantic parsing community (Cai and Yates, 2013; Berant et al., 2013; Fader et al., 2014). The increasing number of relations and entities motivates new resources and techniques for improving the accuracy, including the use of ontology matching models (Kwiatkowski et al., 2013), paraphrase models (Fader et al., 2013; Berant and Liang, 2014), and unlabeled sentences (Krishnamurthy and Kollar, 2013; Reddy et al., 2014). Our work leverages open-ended data from the Web through semi-structured tables. There have been several studies on analyzing or inferring the table schemas (Cafarella et al., 2008; Venetis et al., 2011; Syed et al., 2010; Limaye et al., 2010) and answering search queries by joining tables on similar columns (Cafarella et al., 2008; Gonzalez et al., 2010; Pimplikar and Sarawagi, 2012). While the latter is similar to question answering, the queries tend"
P15-1142,D12-1048,0,0.00731552,"3; Reddy et al., 2014). Our work leverages open-ended data from the Web through semi-structured tables. There have been several studies on analyzing or inferring the table schemas (Cafarella et al., 2008; Venetis et al., 2011; Syed et al., 2010; Limaye et al., 2010) and answering search queries by joining tables on similar columns (Cafarella et al., 2008; Gonzalez et al., 2010; Pimplikar and Sarawagi, 2012). While the latter is similar to question answering, the queries tend to be keyword lists instead of natural language sentences. In parallel, open information extraction (Wu and Weld, 2010; Masaum et al., 2012) and knowledge base population (Ji and Grishman, 2011) extract information from web pages and compile them into structured data. The resulting knowledge base is systematically organized, but as a trade-off, some knowledge is inevitably lost during extraction and the information is forced to conform to a specific schema. To avoid these issues, we choose to work on HTML tables directly. In future work, we wish to draw information from other semi-structured formats such as colon-delimited pairs (Wong et al., 2009), bulleted lists (Gupta and Sarawagi, 2009), and top-k lists (Zhang et al., 2013). P"
P15-1142,P14-1037,1,0.402266,"Missing"
P15-1142,H90-1020,0,0.114874,"Missing"
P15-1142,Q14-1030,0,0.175572,"g for question answering, natural language questions are converted into logical forms, which can be executed on a knowledge source to obtain answer denotations. Early semantic parsing systems were trained to answer highly compositional questions, but the knowledge sources were limited to small closed-domain databases (Zelle and Mooney, 1996; Wong and Mooney, 2007; Zettlemoyer and Collins, 2007; Kwiatkowski et al., 2011). More recent work sacrifices compositionality in favor of using more open-ended knowledge bases such as Freebase (Cai and Yates, 2013; Berant et al., 2013; Fader et al., 2014; Reddy et al., 2014). However, even these broader knowledge sources still define a rigid schema over entities and relation types, thus restricting the scope of answerable questions. To simultaneously increase both the breadth of the knowledge source and the depth of logical compositionality, we propose a new task (with an associated dataset): answering a question using an HTML table as the knowledge source. Figure 1 shows several question-answer pairs and an accompanying table, which are typical of those in our dataset. Note that the questions are logically quite complex, involving a variety of operations such as"
P15-1142,P11-1115,0,0.011773,"d data from the Web through semi-structured tables. There have been several studies on analyzing or inferring the table schemas (Cafarella et al., 2008; Venetis et al., 2011; Syed et al., 2010; Limaye et al., 2010) and answering search queries by joining tables on similar columns (Cafarella et al., 2008; Gonzalez et al., 2010; Pimplikar and Sarawagi, 2012). While the latter is similar to question answering, the queries tend to be keyword lists instead of natural language sentences. In parallel, open information extraction (Wu and Weld, 2010; Masaum et al., 2012) and knowledge base population (Ji and Grishman, 2011) extract information from web pages and compile them into structured data. The resulting knowledge base is systematically organized, but as a trade-off, some knowledge is inevitably lost during extraction and the information is forced to conform to a specific schema. To avoid these issues, we choose to work on HTML tables directly. In future work, we wish to draw information from other semi-structured formats such as colon-delimited pairs (Wong et al., 2009), bulleted lists (Gupta and Sarawagi, 2009), and top-k lists (Zhang et al., 2013). Pasupat and Liang (2014) used a framework similar to ou"
P15-1142,Q13-1016,0,0.0376005,"ons and operations to the utterance. Knowledge domain. Recent works on semantic parsing for question answering operate on more open and diverse data domains. In particular, large-scale knowledge bases have gained popularity in the semantic parsing community (Cai and Yates, 2013; Berant et al., 2013; Fader et al., 2014). The increasing number of relations and entities motivates new resources and techniques for improving the accuracy, including the use of ontology matching models (Kwiatkowski et al., 2013), paraphrase models (Fader et al., 2013; Berant and Liang, 2014), and unlabeled sentences (Krishnamurthy and Kollar, 2013; Reddy et al., 2014). Our work leverages open-ended data from the Web through semi-structured tables. There have been several studies on analyzing or inferring the table schemas (Cafarella et al., 2008; Venetis et al., 2011; Syed et al., 2010; Limaye et al., 2010) and answering search queries by joining tables on similar columns (Cafarella et al., 2008; Gonzalez et al., 2010; Pimplikar and Sarawagi, 2012). While the latter is similar to question answering, the queries tend to be keyword lists instead of natural language sentences. In parallel, open information extraction (Wu and Weld, 2010; M"
P15-1142,D11-1140,0,0.108264,"{10} Figure 1: Our task is to answer a highly compositional question from an HTML table. We learn a semantic parser from question-table-answer triples {(xi , ti , yi )}. Introduction In semantic parsing for question answering, natural language questions are converted into logical forms, which can be executed on a knowledge source to obtain answer denotations. Early semantic parsing systems were trained to answer highly compositional questions, but the knowledge sources were limited to small closed-domain databases (Zelle and Mooney, 1996; Wong and Mooney, 2007; Zettlemoyer and Collins, 2007; Kwiatkowski et al., 2011). More recent work sacrifices compositionality in favor of using more open-ended knowledge bases such as Freebase (Cai and Yates, 2013; Berant et al., 2013; Fader et al., 2014; Reddy et al., 2014). However, even these broader knowledge sources still define a rigid schema over entities and relation types, thus restricting the scope of answerable questions. To simultaneously increase both the breadth of the knowledge source and the depth of logical compositionality, we propose a new task (with an associated dataset): answering a question using an HTML table as the knowledge source. Figure 1 show"
P15-1142,P07-1121,0,0.138573,"pants were there in 1900 than in the first year?” y5 : {10} Figure 1: Our task is to answer a highly compositional question from an HTML table. We learn a semantic parser from question-table-answer triples {(xi , ti , yi )}. Introduction In semantic parsing for question answering, natural language questions are converted into logical forms, which can be executed on a knowledge source to obtain answer denotations. Early semantic parsing systems were trained to answer highly compositional questions, but the knowledge sources were limited to small closed-domain databases (Zelle and Mooney, 1996; Wong and Mooney, 2007; Zettlemoyer and Collins, 2007; Kwiatkowski et al., 2011). More recent work sacrifices compositionality in favor of using more open-ended knowledge bases such as Freebase (Cai and Yates, 2013; Berant et al., 2013; Fader et al., 2014; Reddy et al., 2014). However, even these broader knowledge sources still define a rigid schema over entities and relation types, thus restricting the scope of answerable questions. To simultaneously increase both the breadth of the knowledge source and the depth of logical compositionality, we propose a new task (with an associated dataset): answering a question"
P15-1142,P10-1013,0,0.0129817,"thy and Kollar, 2013; Reddy et al., 2014). Our work leverages open-ended data from the Web through semi-structured tables. There have been several studies on analyzing or inferring the table schemas (Cafarella et al., 2008; Venetis et al., 2011; Syed et al., 2010; Limaye et al., 2010) and answering search queries by joining tables on similar columns (Cafarella et al., 2008; Gonzalez et al., 2010; Pimplikar and Sarawagi, 2012). While the latter is similar to question answering, the queries tend to be keyword lists instead of natural language sentences. In parallel, open information extraction (Wu and Weld, 2010; Masaum et al., 2012) and knowledge base population (Ji and Grishman, 2011) extract information from web pages and compile them into structured data. The resulting knowledge base is systematically organized, but as a trade-off, some knowledge is inevitably lost during extraction and the information is forced to conform to a specific schema. To avoid these issues, we choose to work on HTML tables directly. In future work, we wish to draw information from other semi-structured formats such as colon-delimited pairs (Wong et al., 2009), bulleted lists (Gupta and Sarawagi, 2009), and top-k lists ("
P15-1142,D07-1071,0,0.56226,"0 than in the first year?” y5 : {10} Figure 1: Our task is to answer a highly compositional question from an HTML table. We learn a semantic parser from question-table-answer triples {(xi , ti , yi )}. Introduction In semantic parsing for question answering, natural language questions are converted into logical forms, which can be executed on a knowledge source to obtain answer denotations. Early semantic parsing systems were trained to answer highly compositional questions, but the knowledge sources were limited to small closed-domain databases (Zelle and Mooney, 1996; Wong and Mooney, 2007; Zettlemoyer and Collins, 2007; Kwiatkowski et al., 2011). More recent work sacrifices compositionality in favor of using more open-ended knowledge bases such as Freebase (Cai and Yates, 2013; Berant et al., 2013; Fader et al., 2014; Reddy et al., 2014). However, even these broader knowledge sources still define a rigid schema over entities and relation types, thus restricting the scope of answerable questions. To simultaneously increase both the breadth of the knowledge source and the depth of logical compositionality, we propose a new task (with an associated dataset): answering a question using an HTML table as the know"
P16-1002,Q13-1005,0,0.796702,"re engineering. However, this flexibility also puts RNNs at a disadvantage compared to standard semantic parsers, which can generalize naturally by leveraging their built-in awareness of logical compositionality. In this paper, we introduce data recombination, a generic framework for declaratively injectSemantic parsing—the precise translation of natural language utterances into logical forms—has many applications, including question answering (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007; Liang et al., 2011; Berant et al., 2013), instruction following (Artzi and Zettlemoyer, 2013b), and regular expression generation (Kushman and Barzilay, 2013). Modern semantic parsers (Artzi and Zettlemoyer, 2013a; Berant et al., 2013) are complex pieces of software, requiring handcrafted features, lexicons, and grammars. Meanwhile, recurrent neural networks (RNNs) 12 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 12–22, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics (2015b) showed that an RNN can reliably predict tree-structured outputs in a linear fashion. We evaluate our system on three existing"
P16-1002,N13-1103,0,0.0627029,"dvantage compared to standard semantic parsers, which can generalize naturally by leveraging their built-in awareness of logical compositionality. In this paper, we introduce data recombination, a generic framework for declaratively injectSemantic parsing—the precise translation of natural language utterances into logical forms—has many applications, including question answering (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007; Liang et al., 2011; Berant et al., 2013), instruction following (Artzi and Zettlemoyer, 2013b), and regular expression generation (Kushman and Barzilay, 2013). Modern semantic parsers (Artzi and Zettlemoyer, 2013a; Berant et al., 2013) are complex pieces of software, requiring handcrafted features, lexicons, and grammars. Meanwhile, recurrent neural networks (RNNs) 12 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 12–22, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics (2015b) showed that an RNN can reliably predict tree-structured outputs in a linear fashion. We evaluate our system on three existing semantic parsing datasets. Figure 2 shows sample input-output pair"
P16-1002,D13-1160,1,0.9515,"at a wide variety of tasks with minimal feature engineering. However, this flexibility also puts RNNs at a disadvantage compared to standard semantic parsers, which can generalize naturally by leveraging their built-in awareness of logical compositionality. In this paper, we introduce data recombination, a generic framework for declaratively injectSemantic parsing—the precise translation of natural language utterances into logical forms—has many applications, including question answering (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007; Liang et al., 2011; Berant et al., 2013), instruction following (Artzi and Zettlemoyer, 2013b), and regular expression generation (Kushman and Barzilay, 2013). Modern semantic parsers (Artzi and Zettlemoyer, 2013a; Berant et al., 2013) are complex pieces of software, requiring handcrafted features, lexicons, and grammars. Meanwhile, recurrent neural networks (RNNs) 12 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 12–22, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics (2015b) showed that an RNN can reliably predict tree-structured outputs in a linea"
P16-1002,D10-1119,0,0.0521406,"Missing"
P16-1002,D11-1140,0,0.18181,"Missing"
P16-1002,W10-2903,0,0.0364549,"use a seed lexicon for predicates. 2 • Overnight (OVERNIGHT) contains logical forms paired with natural language paraphrases across eight varied subdomains. Wang et al. (2015) constructed the dataset by generating all possible logical forms up to some depth threshold, then getting multiple natural language paraphrases for each logical form from workers on Amazon Mechanical Turk. We evaluate on the same train/test splits as Wang et al. (2015). In this paper, we only explore learning from logical forms. In the last few years, there has an emergence of semantic parsers learned from denotations (Clarke et al., 2010; Liang et al., 2011; Berant et al., 2013; Artzi and Zettlemoyer, 2013b). While our system cannot directly learn from denotations, it could be used to rerank candidate derivations generated by one of these other systems. 3 Sequence-to-sequence RNN Model Our sequence-to-sequence RNN model is based on existing attention-based neural machine translation models (Bahdanau et al., 2014; Luong et al., 2015a), but also includes a novel attention-based copying mechanism. Similar copying mechanisms have been explored in parallel by Gu et al. (2016) and Gulcehre et al. (2016). Problem statement We cast s"
P16-1002,P16-1004,0,0.42686,"es. These results underscore the importance training on longer, harder examples. 6 Discussion In this paper, we have presented a novel framework we term data recombination, in which we generate new training examples from a highprecision generative model induced from the original training dataset. We have demonstrated its effectiveness in improving the accuracy of a sequence-to-sequence RNN model on three semantic parsing datasets, using a synchronous context-free grammar as our generative model. There has been growing interest in applying neural networks to semantic parsing and related tasks. Dong and Lapata (2016) concurrently developed an attention-based RNN model for semantic parsing, although they did not use data recombination. Grefenstette et al. (2014) proposed a non-recurrent neural model for semantic parsing, though they did not run experiments. Mei et al. (2016) use an RNN model to perform a related task of instruction following. Our proposed attention-based copying mechanism bears a strong resemblance to two models that were developed independently by other groups. Gu et al. (2016) apply a very similar copying mechanism to text summarization and singleturn dialogue generation. Gulcehre et al."
P16-1002,P11-1060,1,0.844174,"otential to succeed at a wide variety of tasks with minimal feature engineering. However, this flexibility also puts RNNs at a disadvantage compared to standard semantic parsers, which can generalize naturally by leveraging their built-in awareness of logical compositionality. In this paper, we introduce data recombination, a generic framework for declaratively injectSemantic parsing—the precise translation of natural language utterances into logical forms—has many applications, including question answering (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007; Liang et al., 2011; Berant et al., 2013), instruction following (Artzi and Zettlemoyer, 2013b), and regular expression generation (Kushman and Barzilay, 2013). Modern semantic parsers (Artzi and Zettlemoyer, 2013a; Berant et al., 2013) are complex pieces of software, requiring handcrafted features, lexicons, and grammars. Meanwhile, recurrent neural networks (RNNs) 12 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 12–22, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics (2015b) showed that an RNN can reliably predict tree-structu"
P16-1002,P15-1033,0,0.00922087,"in [states border [utah]] ? what states border [states border [maine]] ? what states border [states border [utah]] ? Train Model Sequence-to-sequence RNN Figure 1: An overview of our system. Given a dataset, we induce a high-precision synchronous context-free grammar. We then sample from this grammar to generate new “recombinant” examples, which we use to train a sequence-to-sequence RNN. Introduction have made swift inroads into many structured prediction tasks in NLP, including machine translation (Sutskever et al., 2014; Bahdanau et al., 2014) and syntactic parsing (Vinyals et al., 2015b; Dyer et al., 2015). Because RNNs make very few domain-specific assumptions, they have the potential to succeed at a wide variety of tasks with minimal feature engineering. However, this flexibility also puts RNNs at a disadvantage compared to standard semantic parsers, which can generalize naturally by leveraging their built-in awareness of logical compositionality. In this paper, we introduce data recombination, a generic framework for declaratively injectSemantic parsing—the precise translation of natural language utterances into logical forms—has many applications, including question answering (Zelle and Moo"
P16-1002,W14-2405,0,0.0350019,"Missing"
P16-1002,D15-1166,0,0.138511,"Missing"
P16-1002,P16-1154,0,0.109939,"rgence of semantic parsers learned from denotations (Clarke et al., 2010; Liang et al., 2011; Berant et al., 2013; Artzi and Zettlemoyer, 2013b). While our system cannot directly learn from denotations, it could be used to rerank candidate derivations generated by one of these other systems. 3 Sequence-to-sequence RNN Model Our sequence-to-sequence RNN model is based on existing attention-based neural machine translation models (Bahdanau et al., 2014; Luong et al., 2015a), but also includes a novel attention-based copying mechanism. Similar copying mechanisms have been explored in parallel by Gu et al. (2016) and Gulcehre et al. (2016). Problem statement We cast semantic parsing as a sequence-tosequence task. The input utterance x is a sequence of words x1 , . . . , xm ∈ V (in) , the input vocabulary; similarly, the output logical form y is a sequence of tokens y1 , . . . , yn ∈ V (out) , the output vocabulary. A linear sequence of tokens might appear to lose the hierarchical structure of a logical form, but there is precedent for this choice: Vinyals et al. 3.1 Basic Model Encoder. The encoder converts the input sequence x1 , . . . , xm into a sequence of context13 sensitive embeddings b1 , . . ."
P16-1002,P15-1002,0,0.172728,"ame train/test splits as Wang et al. (2015). In this paper, we only explore learning from logical forms. In the last few years, there has an emergence of semantic parsers learned from denotations (Clarke et al., 2010; Liang et al., 2011; Berant et al., 2013; Artzi and Zettlemoyer, 2013b). While our system cannot directly learn from denotations, it could be used to rerank candidate derivations generated by one of these other systems. 3 Sequence-to-sequence RNN Model Our sequence-to-sequence RNN model is based on existing attention-based neural machine translation models (Bahdanau et al., 2014; Luong et al., 2015a), but also includes a novel attention-based copying mechanism. Similar copying mechanisms have been explored in parallel by Gu et al. (2016) and Gulcehre et al. (2016). Problem statement We cast semantic parsing as a sequence-tosequence task. The input utterance x is a sequence of words x1 , . . . , xm ∈ V (in) , the input vocabulary; similarly, the output logical form y is a sequence of tokens y1 , . . . , yn ∈ V (out) , the output vocabulary. A linear sequence of tokens might appear to lose the hierarchical structure of a logical form, but there is precedent for this choice: Vinyals et al."
P16-1002,P16-1014,0,0.193432,"rsers learned from denotations (Clarke et al., 2010; Liang et al., 2011; Berant et al., 2013; Artzi and Zettlemoyer, 2013b). While our system cannot directly learn from denotations, it could be used to rerank candidate derivations generated by one of these other systems. 3 Sequence-to-sequence RNN Model Our sequence-to-sequence RNN model is based on existing attention-based neural machine translation models (Bahdanau et al., 2014; Luong et al., 2015a), but also includes a novel attention-based copying mechanism. Similar copying mechanisms have been explored in parallel by Gu et al. (2016) and Gulcehre et al. (2016). Problem statement We cast semantic parsing as a sequence-tosequence task. The input utterance x is a sequence of words x1 , . . . , xm ∈ V (in) , the input vocabulary; similarly, the output logical form y is a sequence of tokens y1 , . . . , yn ∈ V (out) , the output vocabulary. A linear sequence of tokens might appear to lose the hierarchical structure of a logical form, but there is precedent for this choice: Vinyals et al. 3.1 Basic Model Encoder. The encoder converts the input sequence x1 , . . . , xm into a sequence of context13 sensitive embeddings b1 , . . . , bm using a bidirectional"
P16-1002,D15-1038,1,0.746603,"tionally cheaper second model. Vinyals et al. (2015b) generate a large dataset of constituency parse trees by taking sentences that multiple existing systems parse in the same way, and train a neural model on this dataset. Some of our induced grammars generate examples that are not in the test distribution, but nonetheless aid in generalization. Related work has also explored the idea of training on altered or out-of-domain data, often interpreting it as a form of regularization. Dropout training has been shown to be a form of adaptive regularization (Hinton et al., 2012; Wager et al., 2013). Guu et al. (2015) showed that encouraging a knowledge base completion model to handle longer path queries acts as a form of structural regularization. Language is a blend of crisp regularities and soft relationships. Our work takes RNNs, which excel at modeling soft phenomena, and uses a highly structured tool—synchronous context free grammars—to infuse them with an understanding of crisp structure. We believe this paradigm for simultaneously modeling the soft and hard aspects of language should have broader applicability beyond semantic parsing. expected, independent examples are more helpful than the recombi"
P16-1002,D10-1069,0,0.0235759,"es. 19 recombination strategies, these techniques only change inputs x, while keeping the labels y fixed. Additionally, these paraphrasing-based transformations can be described in terms of grammar induction, so they can be incorporated into our framework. In data recombination, data generated by a highprecision generative model is used to train a second, domain-general model. Generative oversampling (Liu et al., 2007) learns a generative model in a multiclass classification setting, then uses it to generate additional examples from rare classes in order to combat label imbalance. Uptraining (Petrov et al., 2010) uses data labeled by an accurate but slow model to train a computationally cheaper second model. Vinyals et al. (2015b) generate a large dataset of constituency parse trees by taking sentences that multiple existing systems parse in the same way, and train a neural model on this dataset. Some of our induced grammars generate examples that are not in the test distribution, but nonetheless aid in generalization. Related work has also explored the idea of training on altered or out-of-domain data, often interpreting it as a form of regularization. Dropout training has been shown to be a form of"
P16-1002,P13-1092,0,0.0448192,"Missing"
P16-1002,D15-1306,0,0.0135621,"Missing"
P16-1002,P15-1129,1,0.865301,"ee grammar (SCFG), creating new examples such as those shown in Figure 1; our domain-general model is a sequence-to-sequence RNN with a novel attention-based copying mechanism. Data recombination boosts the accuracy of our RNN model on three semantic parsing datasets. On the G EO dataset, data recombination improves test accuracy by 4.3 percentage points over our baseline RNN, leading to new state-of-the-art results for models that do not use a seed lexicon for predicates. 2 • Overnight (OVERNIGHT) contains logical forms paired with natural language paraphrases across eight varied subdomains. Wang et al. (2015) constructed the dataset by generating all possible logical forms up to some depth threshold, then getting multiple natural language paraphrases for each logical form from workers on Amazon Mechanical Turk. We evaluate on the same train/test splits as Wang et al. (2015). In this paper, we only explore learning from logical forms. In the last few years, there has an emergence of semantic parsers learned from denotations (Clarke et al., 2010; Liang et al., 2011; Berant et al., 2013; Artzi and Zettlemoyer, 2013b). While our system cannot directly learn from denotations, it could be used to rerank"
P16-1002,N06-1056,0,0.0207917,"nal symbols in α must be aligned to the same non-terminal symbol in β, and vice versa. Therefore, an SCFG defines a set of joint derivations of aligned pairs of strings. In our case, we use an SCFG to represent joint deriva15 4.3.2 tions of utterances x and logical forms y (which for us is just a sequence of tokens). After we induce an SCFG G from D, the corresponding generative model p˜(x, y) is the distribution over pairs (x, y) defined by sampling from G, where we choose production rules to apply uniformly at random. It is instructive to compare our SCFG-based data recombination with WASP (Wong and Mooney, 2006; Wong and Mooney, 2007), which uses an SCFG as the actual semantic parsing model. The grammar induced by WASP must have good coverage in order to generalize to new inputs at test time. WASP also requires the implementation of an efficient algorithm for computing the conditional probability p(y |x). In contrast, our SCFG is only used to convey prior knowledge about conditional independence structure, so it only needs to have high precision; our RNN model is responsible for boosting recall over the entire input space. We also only need to forward sample from the SCFG, which is considerably easi"
P16-1002,P07-1121,0,0.0289384,"e aligned to the same non-terminal symbol in β, and vice versa. Therefore, an SCFG defines a set of joint derivations of aligned pairs of strings. In our case, we use an SCFG to represent joint deriva15 4.3.2 tions of utterances x and logical forms y (which for us is just a sequence of tokens). After we induce an SCFG G from D, the corresponding generative model p˜(x, y) is the distribution over pairs (x, y) defined by sampling from G, where we choose production rules to apply uniformly at random. It is instructive to compare our SCFG-based data recombination with WASP (Wong and Mooney, 2006; Wong and Mooney, 2007), which uses an SCFG as the actual semantic parsing model. The grammar induced by WASP must have good coverage in order to generalize to new inputs at test time. WASP also requires the implementation of an efficient algorithm for computing the conditional probability p(y |x). In contrast, our SCFG is only used to convey prior knowledge about conditional independence structure, so it only needs to have high precision; our RNN model is responsible for boosting recall over the entire input space. We also only need to forward sample from the SCFG, which is considerably easier to implement than con"
P16-1002,D07-1071,0,0.426522,"ic assumptions, they have the potential to succeed at a wide variety of tasks with minimal feature engineering. However, this flexibility also puts RNNs at a disadvantage compared to standard semantic parsers, which can generalize naturally by leveraging their built-in awareness of logical compositionality. In this paper, we introduce data recombination, a generic framework for declaratively injectSemantic parsing—the precise translation of natural language utterances into logical forms—has many applications, including question answering (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Zettlemoyer and Collins, 2007; Liang et al., 2011; Berant et al., 2013), instruction following (Artzi and Zettlemoyer, 2013b), and regular expression generation (Kushman and Barzilay, 2013). Modern semantic parsers (Artzi and Zettlemoyer, 2013a; Berant et al., 2013) are complex pieces of software, requiring handcrafted features, lexicons, and grammars. Meanwhile, recurrent neural networks (RNNs) 12 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 12–22, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics (2015b) showed that an RNN can reliably"
P16-1002,N15-1162,0,0.237892,"Missing"
P16-1003,P14-1133,1,0.81979,"Missing"
P16-1003,P15-1142,1,0.502827,", and z3 are different ways to represent the semantics of x, while spurious logical forms z4 and z5 get the right answer y for the wrong reasons. Worlds. We use the term world to refer to a collection of entities and relations between entities. One way to represent a world w is as a directed graph with nodes for entities and directed edges for relations. (For example, a world about geography would contain a node Europe with an edge Contains to another node Germany.) In this paper, we use data tables from the Web as knowledge sources, such as the one in Figure 1. We follow the construction in Pasupat and Liang (2015) for converting a table into a directed graph (see Figure 2). Rows and cells become nodes (e.g., r0 = first row and Finland) while columns become labeled directed edges between them (e.g., Venue maps r1 to Finland). The graph is augmented with additional edges Next (from each fictitious worlds to test the denotations of the logical forms in Z. We use crowdsourcing to annotate the correct denotations on a subset of the generated worlds. To reduce the amount of annotation needed, we choose the subset that maximizes the expected information gain. The pruned set of logical forms would provide a st"
P16-1003,D13-1160,1,0.906297,"Missing"
P16-1003,W10-2903,0,0.0249146,"Missing"
P16-1003,W16-0105,0,0.017707,"Missing"
P16-1003,D15-1038,1,0.872201,"Missing"
P16-1003,D07-1071,0,0.126258,"Missing"
P16-1003,D10-1119,0,0.0488932,"Missing"
P16-1003,D13-1161,0,0.0627568,"Missing"
P16-1003,P11-1060,1,0.554925,"Missing"
P16-1055,D10-1049,1,0.652525,"ses problems for both formula selection and description generation. On the formula selection side, we must compose facts that make sense. For semantic compatibility between the mention and description, we have relied on simple word vectors (Mikolov et al., 2013), but more sophisticated forms of semantic relations on larger units of text might yield better results (Bowman et al., 2015). On the description generation side, there is a long line of work in generating natural language descriptions of structured data or logical forms Wong and Mooney (2007); Chen and Mooney (2008); Lu and Ng (2012); Angeli et al. (2010). We lean on the recent developments of neural sequence-to-sequence models (Sutskever et al., 2014; Bahdanau et al., 2014; Luong et al., 2015). Our problem bears some similarity to the semantic parsing work of Wang et al. (2015), who connect generated canonical utterances (representing logical forms) to real utterances. If we return to our initial goal of helping people understand numbers, there are two important directions to explore. First, we have used a small knowledge base, which limits the coverage of perspectives we can generate. Using Freebase (BolReproducibility All code, data, and ex"
P16-1055,D15-1166,0,0.0285293,"Missing"
P16-1055,D12-1091,0,0.0183424,"position, we have a hidden state vector (sj ) which is used to produce an “attention” distribution (αj = (αji )) over input tokens: αji = Attend(sj , bi ). This distribution is used to generate the output token and update the hidden state vector. To generate the token, we eiEvaluation. We train a logistic regression classifier using the features described in Table 3 using the perspective ratings collected in Section 3. Recall that the formula for each perspective in the dataset is assigned a positive (“useful”) label if 6 Significance results are computed by the bootstrap test as described in Berg-Kirkpatrick et al. (2012) using the output of classifiers trained on the entire training set. 583 Feature set Train R F1 P Proximity Similarity Familiarity∗ Compatibility+ F + C† F + C + P† F + C + P + S† 56.4 65.1 70.5 66.9 73.8 73.8 73.8 48.7 34.9 63.5 74.4 70.3 70.3 70.3 52.2 45.4 66.8 70.4 72.1 72.1 72.0 P Dev R F1 56.3 65.1 69.6 65.4 71.5 71.5 71.4 48.8 34.9 62.9 73.1 68.9 68.9 68.6 52.3 45.4 66.1 69.0 70.1 70.1 69.9 System Train BLEU Test BLEU Baseline RNN∗ 65.00 81.50 57.32 69.79 (b) the description generation system. ∗ significant BLEU score versus the baseline with p < 0.01. (a) the formula construction syste"
P16-1055,D15-1075,0,0.0157621,"We have proposed a new task of perspective generation. Compositionality is the key ingredient of our approach, which allows us synthesize information across multiple sources of information. At the same time, compositionality also poses problems for both formula selection and description generation. On the formula selection side, we must compose facts that make sense. For semantic compatibility between the mention and description, we have relied on simple word vectors (Mikolov et al., 2013), but more sophisticated forms of semantic relations on larger units of text might yield better results (Bowman et al., 2015). On the description generation side, there is a long line of work in generating natural language descriptions of structured data or logical forms Wong and Mooney (2007); Chen and Mooney (2008); Lu and Ng (2012); Angeli et al. (2010). We lean on the recent developments of neural sequence-to-sequence models (Sutskever et al., 2014; Bahdanau et al., 2014; Luong et al., 2015). Our problem bears some similarity to the semantic parsing work of Wang et al. (2015), who connect generated canonical utterances (representing logical forms) to real utterances. If we return to our initial goal of helping p"
P16-1055,P15-1129,1,0.725813,"d vectors (Mikolov et al., 2013), but more sophisticated forms of semantic relations on larger units of text might yield better results (Bowman et al., 2015). On the description generation side, there is a long line of work in generating natural language descriptions of structured data or logical forms Wong and Mooney (2007); Chen and Mooney (2008); Lu and Ng (2012); Angeli et al. (2010). We lean on the recent developments of neural sequence-to-sequence models (Sutskever et al., 2014; Bahdanau et al., 2014; Luong et al., 2015). Our problem bears some similarity to the semantic parsing work of Wang et al. (2015), who connect generated canonical utterances (representing logical forms) to real utterances. If we return to our initial goal of helping people understand numbers, there are two important directions to explore. First, we have used a small knowledge base, which limits the coverage of perspectives we can generate. Using Freebase (BolReproducibility All code, data, and experiments for this paper are available on the CodaLab platform at https: //worksheets.codalab.org/worksheets/ 0x243284b4d81d4590b46030cdd3b72633/. Acknowledgments We would like to thank Glen Chiacchieri for providing us informat"
P16-1055,N07-1022,0,0.0111684,"sources of information. At the same time, compositionality also poses problems for both formula selection and description generation. On the formula selection side, we must compose facts that make sense. For semantic compatibility between the mention and description, we have relied on simple word vectors (Mikolov et al., 2013), but more sophisticated forms of semantic relations on larger units of text might yield better results (Bowman et al., 2015). On the description generation side, there is a long line of work in generating natural language descriptions of structured data or logical forms Wong and Mooney (2007); Chen and Mooney (2008); Lu and Ng (2012); Angeli et al. (2010). We lean on the recent developments of neural sequence-to-sequence models (Sutskever et al., 2014; Bahdanau et al., 2014; Luong et al., 2015). Our problem bears some similarity to the semantic parsing work of Wang et al. (2015), who connect generated canonical utterances (representing logical forms) to real utterances. If we return to our initial goal of helping people understand numbers, there are two important directions to explore. First, we have used a small knowledge base, which limits the coverage of perspectives we can gen"
P16-1055,D11-1142,0,0.0143117,"a range of 8,000 km. the 2.7 million square feet that Mission Bay’s largest developer is entitled to build Las Vegas Sands claims the 10.5 million square feet is the largest building in Asia. the amount of oil produced by the US during a lifetime the distance from San Francisco to Beijing twice the area of forest logged in a minute one half of an area of an average farm Table 8: Examples of perspectives generated by our system that frame the mentioned quantity to be larger or smaller (top to bottom) than initially the authors thought. lacker et al., 2008) or even open information extraction (Fader et al., 2011) would dramatically increase the number of facts and therefore the scope of possible perspectives. Second, while we have focused mostly on basic compatibility, it would be interesting to explore more deeply how the juxtaposition of facts affects framing. Table 8 presents several examples generated by our system that frame the mentioned quantities to be larger or smaller than the authors originally thought. We think perspective generation is an exciting setting to study aspects of numeric framing (Teigen, 2015). formula exists within the knowledge base (within an order of magnitude of the menti"
P16-1055,P16-1002,1,0.686963,"computed using word2vec (Mikolov et al., 2013). Baseline. As a simple approach to generate perspectives, we just combine tuples in the formula with the neutral prepositions of and for, e.g. “1/5th of the cost of an employee for the population of Texas for the time taken for lunch.” Sequence-to-sequence RNN. We use formulaperspective pairs from the dataset to create a sequence-to-sequence task: the input is composed using the formula’s multiplier and descriptions of its tuples connected with the symbol ‘*’; the output is the perspective (Figure 7). Our system is based on the model described in Jia and Liang (2016). Given a sequence of input tokens (x = (xi )), the model computes a contextdependent vector (b = (bi )) for each token using a bidirectional RNN with LSTM units. We then generate the output sequence (yj ) left to right as follows. At each output position, we have a hidden state vector (sj ) which is used to produce an “attention” distribution (αj = (αji )) over input tokens: αji = Attend(sj , bi ). This distribution is used to generate the output token and update the hidden state vector. To generate the token, we eiEvaluation. We train a logistic regression classifier using the features descr"
P16-1055,D11-1149,0,\N,Missing
P16-1090,P11-1060,1,0.923787,"P is: Extensions Learning from denotations ∀i, P xi M = πi Ti ∀i, j πij = 1 π, M ≥ 0 Given a new input x, we return the same output if xM is same for all consistent solutions (M, π). Note that we can effectively “marginalize out” π. We can also relax this ILP into an linear program following Section 3.2. Up until now, we have assumed that we have input-output pairs. For semantic parsing, this means annotating sentences with logical forms (e.g., area of Ohio to area(OH)) which is very expensive. This has motivated previous work to learn from question-answer pairs (e.g., area of Ohio to 44825) (Liang et al., 2011). This provides weaker supervision: For example, 44825 is the area of Ohio (in squared miles), but it is also the zip code of Chatfield. So, the true output could be either area(OH) or zipcode(Chatfield). In this section, we show how to handle this form of weak supervision by asking for unanimity over additional selection variables. Formally, we have D = {(x1 , Y1 ), . . . , (xn , Yn )} as a set of training examples, here each Yi consists of ki candidate outputs for xi . In this case, the unknowns are the mapping M as before along with a selection vector πi , which specifies which of the ki ou"
P16-1090,D11-1039,0,0.0141697,"we are interested in β’s such that α> T = β > T , or in other words, α − β is in the null space of T > . Let B be a basis for the null space of T > . We can then write α − β = Bv for some v. Therefore, the set of paraphrases of x ∈ FLS are: def Paraphrases(x) = {(α − Bv)> S : v ∈ Rn }. (9) 6 Discussion and related work Our work is motivated by the semantic parsing task (though it can be applied to any set-to-set prediction task). Over the last decade, there has been much work on semantic parsing, mostly focusing on learning from weaker supervision (Liang et al., 2011; Goldwasser et al., 2011; Artzi and Zettlemoyer, 2011; Artzi and Zettlemoyer, 2013), scaling up beyond small databases (Cai and Yates, 2013; Berant et al., 2013; Pasupat and Liang, 2015), and applying semantic parsing to other tasks (Matuszek et al., 2012; Kushman and Barzilay, 2013; Artzi and Zettlemoyer, 2013). HowReproducibility. All code, data, and experiments for this paper are available on the CodaLab platform at https: //worksheets.codalab.org/worksheets/ 0x593676a278fc4e5abe2d8bac1e3df486/. Acknowledgments. We would like to thank the anonymous reviewers for their helpful comments. We are also grateful for a Future Of Life Research Award"
P16-1090,Q13-1005,0,0.015312,"that α> T = β > T , or in other words, α − β is in the null space of T > . Let B be a basis for the null space of T > . We can then write α − β = Bv for some v. Therefore, the set of paraphrases of x ∈ FLS are: def Paraphrases(x) = {(α − Bv)> S : v ∈ Rn }. (9) 6 Discussion and related work Our work is motivated by the semantic parsing task (though it can be applied to any set-to-set prediction task). Over the last decade, there has been much work on semantic parsing, mostly focusing on learning from weaker supervision (Liang et al., 2011; Goldwasser et al., 2011; Artzi and Zettlemoyer, 2011; Artzi and Zettlemoyer, 2013), scaling up beyond small databases (Cai and Yates, 2013; Berant et al., 2013; Pasupat and Liang, 2015), and applying semantic parsing to other tasks (Matuszek et al., 2012; Kushman and Barzilay, 2013; Artzi and Zettlemoyer, 2013). HowReproducibility. All code, data, and experiments for this paper are available on the CodaLab platform at https: //worksheets.codalab.org/worksheets/ 0x593676a278fc4e5abe2d8bac1e3df486/. Acknowledgments. We would like to thank the anonymous reviewers for their helpful comments. We are also grateful for a Future Of Life Research Award and NSF grant CCF-1138967, whi"
P16-1090,D13-1160,1,0.79154,"Missing"
P16-1090,P13-1042,0,0.0323455,"Missing"
P16-1090,P15-1142,1,0.864095,"Missing"
P16-1090,P11-1149,0,0.0197694,"> T , respectively. Thus we are interested in β’s such that α> T = β > T , or in other words, α − β is in the null space of T > . Let B be a basis for the null space of T > . We can then write α − β = Bv for some v. Therefore, the set of paraphrases of x ∈ FLS are: def Paraphrases(x) = {(α − Bv)> S : v ∈ Rn }. (9) 6 Discussion and related work Our work is motivated by the semantic parsing task (though it can be applied to any set-to-set prediction task). Over the last decade, there has been much work on semantic parsing, mostly focusing on learning from weaker supervision (Liang et al., 2011; Goldwasser et al., 2011; Artzi and Zettlemoyer, 2011; Artzi and Zettlemoyer, 2013), scaling up beyond small databases (Cai and Yates, 2013; Berant et al., 2013; Pasupat and Liang, 2015), and applying semantic parsing to other tasks (Matuszek et al., 2012; Kushman and Barzilay, 2013; Artzi and Zettlemoyer, 2013). HowReproducibility. All code, data, and experiments for this paper are available on the CodaLab platform at https: //worksheets.codalab.org/worksheets/ 0x593676a278fc4e5abe2d8bac1e3df486/. Acknowledgments. We would like to thank the anonymous reviewers for their helpful comments. We are also grateful for a F"
P16-1090,N13-1103,0,0.0306918,"Missing"
P16-1090,D10-1119,0,0.0717339,"Missing"
P16-1090,P07-1121,0,0.0957505,"Missing"
P16-1138,Q13-1005,0,0.795811,"f the world) has some denotation (the effect of the command)—see Figure 1 for an example. How can we build a system to learn from examples like these with no initial knowledge about what any of the words mean? We start with the classic paradigm of training semantic parsers that map utterances to logical forms, which are executed to produce the denotation (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Zettlemoyer and Collins, 2009; Kwiatkowski et al., 2010). More recent work learns directly from denotations (Clarke et al., 2010; Liang, 2013; Berant et al., 2013; Artzi and Zettlemoyer, 2013), but in this setting, a constant struggle is to contain the exponential explosion of possible logical forms. With no initial lexicon and longer contextdependent texts, our situation is exacerbated. • Model A: our full model that derives logical forms (e.g., in Figure 1, the last utterance maps to mix(args[1][1])) compositionally from the text so that spans of the utterance (e.g., “it”) align to parts of the logical form (e.g., args[1][1], which retrieves an argument from a previous logical form). This is based on standard semantic parsing (e.g., Zettlemoyer and Collins (2005)). • Model B: col"
P16-1138,D13-1160,1,0.861977,"some context (state of the world) has some denotation (the effect of the command)—see Figure 1 for an example. How can we build a system to learn from examples like these with no initial knowledge about what any of the words mean? We start with the classic paradigm of training semantic parsers that map utterances to logical forms, which are executed to produce the denotation (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Zettlemoyer and Collins, 2009; Kwiatkowski et al., 2010). More recent work learns directly from denotations (Clarke et al., 2010; Liang, 2013; Berant et al., 2013; Artzi and Zettlemoyer, 2013), but in this setting, a constant struggle is to contain the exponential explosion of possible logical forms. With no initial lexicon and longer contextdependent texts, our situation is exacerbated. • Model A: our full model that derives logical forms (e.g., in Figure 1, the last utterance maps to mix(args[1][1])) compositionally from the text so that spans of the utterance (e.g., “it”) align to parts of the logical form (e.g., args[1][1], which retrieves an argument from a previous logical form). This is based on standard semantic parsing (e.g., Zettlemoyer and C"
P16-1138,D14-1067,0,0.10375,"ligned to predicates, which leads to multiple derivations with the same logical form. Model B discards these alignments, and Model C collapses the arguments of the logical forms to denotations. Figure 3: S CENE dataset: Each person has a shirt of some color and a hat of some color. They enter, leave, move around on a stage, and trade hats. Context: Text: Delete the second figure. Bring it back as the first figure. Denotation: to flat logical forms (e.g., mix(beaker2)), where the arguments of the top-level predicate are objects in the world. This model is in the spirit of Yao et al. (2014) and Bordes et al. (2014), who directly predicted concrete paths in a knowledge graph for question answering. Model A excels at credit assignment: the latent derivation explains how parts of the logical form are triggered by parts of the utterance. The price is an unmanageably large search space, given that we do not have a seed lexicon. At the other end, Model C only considers a small set of logical forms, but the mapping from text to the correct logical form is more complex and harder to model. We collected three new context-dependent semantic parsing datasets using Amazon Mechanical Turk: A LCHEMY (Figure 1), S CEN"
P16-1138,P13-1042,0,0.016703,"l forms; Artzi and Zettlemoyer (2013) developed a model that handles references to the current world state. Our system considers both types of context, handling linguistic phenomena such as ellipsis and anaphora that reference both previous world states and logical forms. Logical form generation. Traditional semantic parsers generate logical forms by aligning each part of the logical form to the utterance (Zelle and Mooney, 1996; Wong and Mooney, 2007; Zettlemoyer and Collins, 2007; Kwiatkowski et al., 2011). In general, such systems rely on a lexicon, which can be hand-engineered, extracted (Cai and Yates, 2013; Berant et al., 2013), or automatically learned from annotated logical forms (Kwiatkowski et al., 2010; Chen, 2012). Recent work on learning from denotations has moved away from anchored logical forms. Pasupat and Liang (2014) and Wang et al. (2015) proposed generating logical forms without alignments, similar to our Model B. Yao et al. (2014) and Bordes et al. (2014) have explored predicting paths in a knowledge graph directly, which is similar to the flat logical forms of Model C. Relaxation and bootstrapping. The idea of first training a simpler model in order to work up to a more complex"
P16-1138,P12-1045,0,0.0123955,"ders both types of context, handling linguistic phenomena such as ellipsis and anaphora that reference both previous world states and logical forms. Logical form generation. Traditional semantic parsers generate logical forms by aligning each part of the logical form to the utterance (Zelle and Mooney, 1996; Wong and Mooney, 2007; Zettlemoyer and Collins, 2007; Kwiatkowski et al., 2011). In general, such systems rely on a lexicon, which can be hand-engineered, extracted (Cai and Yates, 2013; Berant et al., 2013), or automatically learned from annotated logical forms (Kwiatkowski et al., 2010; Chen, 2012). Recent work on learning from denotations has moved away from anchored logical forms. Pasupat and Liang (2014) and Wang et al. (2015) proposed generating logical forms without alignments, similar to our Model B. Yao et al. (2014) and Bordes et al. (2014) have explored predicting paths in a knowledge graph directly, which is similar to the flat logical forms of Model C. Relaxation and bootstrapping. The idea of first training a simpler model in order to work up to a more complex one has been explored other contexts. In the unsupervised learning of generative models, bootstrapping can help esca"
P16-1138,W10-2903,0,0.0269264,"at a piece of text (a command) in some context (state of the world) has some denotation (the effect of the command)—see Figure 1 for an example. How can we build a system to learn from examples like these with no initial knowledge about what any of the words mean? We start with the classic paradigm of training semantic parsers that map utterances to logical forms, which are executed to produce the denotation (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Zettlemoyer and Collins, 2009; Kwiatkowski et al., 2010). More recent work learns directly from denotations (Clarke et al., 2010; Liang, 2013; Berant et al., 2013; Artzi and Zettlemoyer, 2013), but in this setting, a constant struggle is to contain the exponential explosion of possible logical forms. With no initial lexicon and longer contextdependent texts, our situation is exacerbated. • Model A: our full model that derives logical forms (e.g., in Figure 1, the last utterance maps to mix(args[1][1])) compositionally from the text so that spans of the utterance (e.g., “it”) align to parts of the logical form (e.g., args[1][1], which retrieves an argument from a previous logical form). This is based on standard semanti"
P16-1138,Q14-1042,0,0.0432848,"Each example has L = 5 utterances, each denoting some transformation of the world state. Our datasets are unique in that they are grounded to a world state and have rich linguistic context-dependence. In the context-dependent ATIS dataset (Dahl et al., 1994) used by Zettlemoyer and Collins (2009), logical forms of utterances depend on previous logical forms, though there is no world state and the linguistic phenomena is limited to nominal references. In the map navigation dataset (Chen and Mooney, 2011), used by Artzi and Zettlemoyer (2013), utterances only reference the current world state. Vlachos and Clark (2014) released a corpus of annotated dialogues, which has interesting linguistic contextdependence, but there is no world state. Data collection. Our strategy was to automatically generate sequences of world states and ask Amazon Mechanical Turk (AMT) workers to describe the successive transformations. Specifically, we started with a random world state w0 . For each i = 1, . . . , L, we sample a valid action and argument (e.g., pour(beaker1, beaker2)). To encourage context-dependent descriptions, we upweight recently used actions and arguments (e.g., the next action is more like to be drain(beaker2"
P16-1138,H94-1010,0,0.850191,"f properties and actions. In A LCHEMY, properties are color, and amount; actions are pour, drain, and mix. In S CENE, properties are hat-color and shirt-color; actions are enter, leave, move, and trade-hats. In TANGRAMS, there is one property (shape), and actions are add, remove, and swap. In addition, we include the position property (pos) in each dataset. Each example has L = 5 utterances, each denoting some transformation of the world state. Our datasets are unique in that they are grounded to a world state and have rich linguistic context-dependence. In the context-dependent ATIS dataset (Dahl et al., 1994) used by Zettlemoyer and Collins (2009), logical forms of utterances depend on previous logical forms, though there is no world state and the linguistic phenomena is limited to nominal references. In the map navigation dataset (Chen and Mooney, 2011), used by Artzi and Zettlemoyer (2013), utterances only reference the current world state. Vlachos and Clark (2014) released a corpus of annotated dialogues, which has interesting linguistic contextdependence, but there is no world state. Data collection. Our strategy was to automatically generate sequences of world states and ask Amazon Mechanical"
P16-1138,P15-1129,1,0.645636,"l that derives logical forms (e.g., in Figure 1, the last utterance maps to mix(args[1][1])) compositionally from the text so that spans of the utterance (e.g., “it”) align to parts of the logical form (e.g., args[1][1], which retrieves an argument from a previous logical form). This is based on standard semantic parsing (e.g., Zettlemoyer and Collins (2005)). • Model B: collapse all derivations with the same logical form; we map utterances to full logical forms, but without an alignment between the utterance and logical forms. This “floating” approach was used in Pasupat and Liang (2015) and Wang et al. (2015). • Model C: further collapse all logical forms whose top-level arguments have the same denotation. In other words, we map utterances 1456 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 1456–1465, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics Model A mix(args[1][1]) mix(args[1][1]) mix args[1][1] args[1][1] mix Mix it Mix Context: mix(pos(2)) it mix(pos(2)) mix pos(2) pos(2) mix Mix it Mix Text: A man in a red shirt and orange hat leaves to the right, leaving behind a man in a blue shirt in the middle. He ta"
P16-1138,D15-1038,1,0.882948,"Missing"
P16-1138,D10-1119,0,0.0611876,"der the following sequence of models: Introduction Suppose we are only told that a piece of text (a command) in some context (state of the world) has some denotation (the effect of the command)—see Figure 1 for an example. How can we build a system to learn from examples like these with no initial knowledge about what any of the words mean? We start with the classic paradigm of training semantic parsers that map utterances to logical forms, which are executed to produce the denotation (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Zettlemoyer and Collins, 2009; Kwiatkowski et al., 2010). More recent work learns directly from denotations (Clarke et al., 2010; Liang, 2013; Berant et al., 2013; Artzi and Zettlemoyer, 2013), but in this setting, a constant struggle is to contain the exponential explosion of possible logical forms. With no initial lexicon and longer contextdependent texts, our situation is exacerbated. • Model A: our full model that derives logical forms (e.g., in Figure 1, the last utterance maps to mix(args[1][1])) compositionally from the text so that spans of the utterance (e.g., “it”) align to parts of the logical form (e.g., args[1][1], which retrieves an a"
P16-1138,P07-1121,0,0.0645189,"form derivations. As illustrated in Figure 2, we consider the following sequence of models: Introduction Suppose we are only told that a piece of text (a command) in some context (state of the world) has some denotation (the effect of the command)—see Figure 1 for an example. How can we build a system to learn from examples like these with no initial knowledge about what any of the words mean? We start with the classic paradigm of training semantic parsers that map utterances to logical forms, which are executed to produce the denotation (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Zettlemoyer and Collins, 2009; Kwiatkowski et al., 2010). More recent work learns directly from denotations (Clarke et al., 2010; Liang, 2013; Berant et al., 2013; Artzi and Zettlemoyer, 2013), but in this setting, a constant struggle is to contain the exponential explosion of possible logical forms. With no initial lexicon and longer contextdependent texts, our situation is exacerbated. • Model A: our full model that derives logical forms (e.g., in Figure 1, the last utterance maps to mix(args[1][1])) compositionally from the text so that spans of the utterance (e.g., “it”) align to parts o"
P16-1138,W14-2416,0,0.0514565,"ons) where words are aligned to predicates, which leads to multiple derivations with the same logical form. Model B discards these alignments, and Model C collapses the arguments of the logical forms to denotations. Figure 3: S CENE dataset: Each person has a shirt of some color and a hat of some color. They enter, leave, move around on a stage, and trade hats. Context: Text: Delete the second figure. Bring it back as the first figure. Denotation: to flat logical forms (e.g., mix(beaker2)), where the arguments of the top-level predicate are objects in the world. This model is in the spirit of Yao et al. (2014) and Bordes et al. (2014), who directly predicted concrete paths in a knowledge graph for question answering. Model A excels at credit assignment: the latent derivation explains how parts of the logical form are triggered by parts of the utterance. The price is an unmanageably large search space, given that we do not have a seed lexicon. At the other end, Model C only considers a small set of logical forms, but the mapping from text to the correct logical form is more complex and harder to model. We collected three new context-dependent semantic parsing datasets using Amazon Mechanical Turk: A"
P16-1138,D11-1140,0,0.0188856,"world state context. Zettlemoyer and Collins (2009) developed a model that handles references to previous logical forms; Artzi and Zettlemoyer (2013) developed a model that handles references to the current world state. Our system considers both types of context, handling linguistic phenomena such as ellipsis and anaphora that reference both previous world states and logical forms. Logical form generation. Traditional semantic parsers generate logical forms by aligning each part of the logical form to the utterance (Zelle and Mooney, 1996; Wong and Mooney, 2007; Zettlemoyer and Collins, 2007; Kwiatkowski et al., 2011). In general, such systems rely on a lexicon, which can be hand-engineered, extracted (Cai and Yates, 2013; Berant et al., 2013), or automatically learned from annotated logical forms (Kwiatkowski et al., 2010; Chen, 2012). Recent work on learning from denotations has moved away from anchored logical forms. Pasupat and Liang (2014) and Wang et al. (2015) proposed generating logical forms without alignments, similar to our Model B. Yao et al. (2014) and Bordes et al. (2014) have explored predicting paths in a knowledge graph directly, which is similar to the flat logical forms of Model C. Relax"
P16-1138,P09-1011,1,0.791281,"logical forms. Pasupat and Liang (2014) and Wang et al. (2015) proposed generating logical forms without alignments, similar to our Model B. Yao et al. (2014) and Bordes et al. (2014) have explored predicting paths in a knowledge graph directly, which is similar to the flat logical forms of Model C. Relaxation and bootstrapping. The idea of first training a simpler model in order to work up to a more complex one has been explored other contexts. In the unsupervised learning of generative models, bootstrapping can help escape local optima and provide helpful regularization (Och and Ney, 2003; Liang et al., 2009). When it is difficult to even find one logical form that reaches the denotation, one can use the relaxation technique of Steinhardt and Liang (2015). Recall that projecting from Model A to C creates a more computationally tractable model at the cost of expressivity. However, this is because Model C used a linear model. One might imagine that a non-linear model would be able to recuperate some of the loss of expressivity. Indeed, Neelakantan et al. (2016) use recurrent neural networks attempt to perform logical operations. One could go one step further and bypass logical forms altogether, perf"
P16-1138,D07-1071,0,0.0999254,"n either linguistic context or world state context. Zettlemoyer and Collins (2009) developed a model that handles references to previous logical forms; Artzi and Zettlemoyer (2013) developed a model that handles references to the current world state. Our system considers both types of context, handling linguistic phenomena such as ellipsis and anaphora that reference both previous world states and logical forms. Logical form generation. Traditional semantic parsers generate logical forms by aligning each part of the logical form to the utterance (Zelle and Mooney, 1996; Wong and Mooney, 2007; Zettlemoyer and Collins, 2007; Kwiatkowski et al., 2011). In general, such systems rely on a lexicon, which can be hand-engineered, extracted (Cai and Yates, 2013; Berant et al., 2013), or automatically learned from annotated logical forms (Kwiatkowski et al., 2010; Chen, 2012). Recent work on learning from denotations has moved away from anchored logical forms. Pasupat and Liang (2014) and Wang et al. (2015) proposed generating logical forms without alignments, similar to our Model B. Yao et al. (2014) and Bordes et al. (2014) have explored predicting paths in a knowledge graph directly, which is similar to the flat logi"
P16-1138,P09-1110,0,0.240432,"lustrated in Figure 2, we consider the following sequence of models: Introduction Suppose we are only told that a piece of text (a command) in some context (state of the world) has some denotation (the effect of the command)—see Figure 1 for an example. How can we build a system to learn from examples like these with no initial knowledge about what any of the words mean? We start with the classic paradigm of training semantic parsers that map utterances to logical forms, which are executed to produce the denotation (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Zettlemoyer and Collins, 2009; Kwiatkowski et al., 2010). More recent work learns directly from denotations (Clarke et al., 2010; Liang, 2013; Berant et al., 2013; Artzi and Zettlemoyer, 2013), but in this setting, a constant struggle is to contain the exponential explosion of possible logical forms. With no initial lexicon and longer contextdependent texts, our situation is exacerbated. • Model A: our full model that derives logical forms (e.g., in Figure 1, the last utterance maps to mix(args[1][1])) compositionally from the text so that spans of the utterance (e.g., “it”) align to parts of the logical form (e.g., args["
P16-1138,J03-1002,0,0.0352453,"away from anchored logical forms. Pasupat and Liang (2014) and Wang et al. (2015) proposed generating logical forms without alignments, similar to our Model B. Yao et al. (2014) and Bordes et al. (2014) have explored predicting paths in a knowledge graph directly, which is similar to the flat logical forms of Model C. Relaxation and bootstrapping. The idea of first training a simpler model in order to work up to a more complex one has been explored other contexts. In the unsupervised learning of generative models, bootstrapping can help escape local optima and provide helpful regularization (Och and Ney, 2003; Liang et al., 2009). When it is difficult to even find one logical form that reaches the denotation, one can use the relaxation technique of Steinhardt and Liang (2015). Recall that projecting from Model A to C creates a more computationally tractable model at the cost of expressivity. However, this is because Model C used a linear model. One might imagine that a non-linear model would be able to recuperate some of the loss of expressivity. Indeed, Neelakantan et al. (2016) use recurrent neural networks attempt to perform logical operations. One could go one step further and bypass logical f"
P16-1138,P14-1037,1,0.426527,"ence both previous world states and logical forms. Logical form generation. Traditional semantic parsers generate logical forms by aligning each part of the logical form to the utterance (Zelle and Mooney, 1996; Wong and Mooney, 2007; Zettlemoyer and Collins, 2007; Kwiatkowski et al., 2011). In general, such systems rely on a lexicon, which can be hand-engineered, extracted (Cai and Yates, 2013; Berant et al., 2013), or automatically learned from annotated logical forms (Kwiatkowski et al., 2010; Chen, 2012). Recent work on learning from denotations has moved away from anchored logical forms. Pasupat and Liang (2014) and Wang et al. (2015) proposed generating logical forms without alignments, similar to our Model B. Yao et al. (2014) and Bordes et al. (2014) have explored predicting paths in a knowledge graph directly, which is similar to the flat logical forms of Model C. Relaxation and bootstrapping. The idea of first training a simpler model in order to work up to a more complex one has been explored other contexts. In the unsupervised learning of generative models, bootstrapping can help escape local optima and provide helpful regularization (Och and Ney, 2003; Liang et al., 2009). When it is difficul"
P16-1138,P15-1142,1,0.502904,"ted. • Model A: our full model that derives logical forms (e.g., in Figure 1, the last utterance maps to mix(args[1][1])) compositionally from the text so that spans of the utterance (e.g., “it”) align to parts of the logical form (e.g., args[1][1], which retrieves an argument from a previous logical form). This is based on standard semantic parsing (e.g., Zettlemoyer and Collins (2005)). • Model B: collapse all derivations with the same logical form; we map utterances to full logical forms, but without an alignment between the utterance and logical forms. This “floating” approach was used in Pasupat and Liang (2015) and Wang et al. (2015). • Model C: further collapse all logical forms whose top-level arguments have the same denotation. In other words, we map utterances 1456 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 1456–1465, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics Model A mix(args[1][1]) mix(args[1][1]) mix args[1][1] args[1][1] mix Mix it Mix Context: mix(pos(2)) it mix(pos(2)) mix pos(2) pos(2) mix Mix it Mix Text: A man in a red shirt and orange hat leaves to the right, leaving behind a man in a blue shi"
P16-1138,J13-2005,1,\N,Missing
P16-1224,D13-1160,1,0.86034,"= 1, 2, 3}. The set of all features is just the cross product of utterance features and logical form features. For example, if x = ‘enlever tout’ and z = remove(all()), then features include: (‘enlever’, all) (‘enlever’, remove) (‘enlever’, (remove, 1, all)) (‘tout’, (remove, 1, all)) (‘tout’, all) (‘tout’, remove) Note that we do not model an explicit alignment or derivation compositionally connecting the utterance and the logical form, in contrast to most traditional work in semantic parsing (Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Liang et al., 2011; Kwiatkowski et al., 2010; Berant et al., 2013), instead following a looser model of semantics similar to (Pasupat and Liang, 2015). Modeling explicit alignments or derivations is only computationally feasible when we are learning from annotated logical forms or have a seed lexicon, since the number of derivations is much larger than the number of logical forms. In the ILLG setting, neither are available. Generation/parsing. We generate logical forms from smallest to largest using beam search. Specifically, for each size n = 1, . . . , 8, we construct a set of logical forms of size n (with exactly n predicates) by combining logical forms o"
P16-1224,P09-1010,0,0.116088,"use precise and consistent languages. Interestingly, our pragmatics model did not help and can even hurt the less successful players who are less precise and consistent. This is expected behavior: the pragmatics model assumes that the human is cooperative and behaving rationally. For the bottom half of the players, this assumption is not true, in which case the pragmatics model is not useful. 6 Related Work and Discussion Our work connects with a broad body of work on grounded language, in which language is used in some environment as a means towards some goal. Examples include playing games (Branavan et al., 2009, 2010; Reckman et al., 2010) interacting with robotics (Tellex et al., 2011, 2014), and following instructions (Vogel and Jurafsky, 2010; Chen and Mooney, 2011; Artzi and Zettlemoyer, 2013) Semantic parsing utterances to logical forms, which we leverage, plays an important role in these settings (Kollar et al., 2010; Matuszek et al., 2012; Artzi and Zettlemoyer, 2013). What makes this work unique is our new interactive learning of language games (ILLG) setting, in which a model has to learn a language from scratch through interaction. While online gradient descent is frequently used, for exam"
P16-1224,P10-1129,0,0.0563282,"Missing"
P16-1224,D10-1119,0,0.0429812,"{(h, i, ψ(h.i, d − 1)) |i = 1, 2, 3}. The set of all features is just the cross product of utterance features and logical form features. For example, if x = ‘enlever tout’ and z = remove(all()), then features include: (‘enlever’, all) (‘enlever’, remove) (‘enlever’, (remove, 1, all)) (‘tout’, (remove, 1, all)) (‘tout’, all) (‘tout’, remove) Note that we do not model an explicit alignment or derivation compositionally connecting the utterance and the logical form, in contrast to most traditional work in semantic parsing (Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Liang et al., 2011; Kwiatkowski et al., 2010; Berant et al., 2013), instead following a looser model of semantics similar to (Pasupat and Liang, 2015). Modeling explicit alignments or derivations is only computationally feasible when we are learning from annotated logical forms or have a seed lexicon, since the number of derivations is much larger than the number of logical forms. In the ILLG setting, neither are available. Generation/parsing. We generate logical forms from smallest to largest using beam search. Specifically, for each size n = 1, . . . , 8, we construct a set of logical forms of size n (with exactly n predicates) by com"
P16-1224,P12-1045,0,0.0165039,"(Tellex et al., 2011, 2014), and following instructions (Vogel and Jurafsky, 2010; Chen and Mooney, 2011; Artzi and Zettlemoyer, 2013) Semantic parsing utterances to logical forms, which we leverage, plays an important role in these settings (Kollar et al., 2010; Matuszek et al., 2012; Artzi and Zettlemoyer, 2013). What makes this work unique is our new interactive learning of language games (ILLG) setting, in which a model has to learn a language from scratch through interaction. While online gradient descent is frequently used, for example in semantic parsing (Zettlemoyer and Collins, 2007; Chen, 2012), we using it in a truly online setting, taking one pass over the data and measuring online accuracy (Cesa-Bianchi and Lugosi, 2006). To speed up learning, we leverage computational models of pragmatics (Jäger, 2008; Golland et al., 2010; Frank and Goodman, 2012; Smith et al., 2013; Vogel et al., 2013). The main difference is these previous works use pragmatics with a trained base model, whereas we learn the model online. Monroe and Potts (2015) uses learning to improve the pragmatics model. In contrast, we use pragmatics to speed up the learning process by capturing phenomena like mutual excl"
P16-1224,D10-1040,1,0.962541,"fic language. While we would not expect the computer to magically guess ‘remove cyan’ 7→ remove(with(cyan)), it should at least push down the probability of zrm-red because zrm-red intuitively is already well-explained by another utterance ‘remove red’. This phenomenon, mutual exclusivity, was studied by Markman and Wachtel (1988). They found that children, during their language acquisition process, reject a second label for an object and treat it instead as a label for a novel object. The pragmatic computer. To model mutual exclusivity formally, we turn to probabilistic models of pragmatics (Golland et al., 2010; Frank and Goodman, 2012; Smith et al., 2013; Goodman and Lassiter, 2015), which operationalize the ideas of Grice (1975). The central idea in these models is to treat language as a cooperative game between a speaker (human) and a listener (computer) as we are doing, but where the listener has an explicit model of the speaker’s strategy, which in turn models the listener. Formally, let S(x |z) be the speaker’s strategy and L(z |x) be the listener’s 2371 zrm-red zrm-cyan z3 , z4 , . . . pθ (z |x) ‘remove red’ 0.8 0.1 0.1 ‘remove cyan’ 0.6 0.2 0.2 S(x |z) ‘remove red’ 0.57 0.33 0.33 ‘remove cya"
P16-1224,P11-1060,1,0.851422,"0) = {h}, ψ(h, d) = {(h, i, ψ(h.i, d − 1)) |i = 1, 2, 3}. The set of all features is just the cross product of utterance features and logical form features. For example, if x = ‘enlever tout’ and z = remove(all()), then features include: (‘enlever’, all) (‘enlever’, remove) (‘enlever’, (remove, 1, all)) (‘tout’, (remove, 1, all)) (‘tout’, all) (‘tout’, remove) Note that we do not model an explicit alignment or derivation compositionally connecting the utterance and the logical form, in contrast to most traditional work in semantic parsing (Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Liang et al., 2011; Kwiatkowski et al., 2010; Berant et al., 2013), instead following a looser model of semantics similar to (Pasupat and Liang, 2015). Modeling explicit alignments or derivations is only computationally feasible when we are learning from annotated logical forms or have a seed lexicon, since the number of derivations is much larger than the number of logical forms. In the ILLG setting, neither are available. Generation/parsing. We generate logical forms from smallest to largest using beam search. Specifically, for each size n = 1, . . . , 8, we construct a set of logical forms of size n (with ex"
P16-1224,P15-1142,1,0.850238,"res and logical form features. For example, if x = ‘enlever tout’ and z = remove(all()), then features include: (‘enlever’, all) (‘enlever’, remove) (‘enlever’, (remove, 1, all)) (‘tout’, (remove, 1, all)) (‘tout’, all) (‘tout’, remove) Note that we do not model an explicit alignment or derivation compositionally connecting the utterance and the logical form, in contrast to most traditional work in semantic parsing (Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Liang et al., 2011; Kwiatkowski et al., 2010; Berant et al., 2013), instead following a looser model of semantics similar to (Pasupat and Liang, 2015). Modeling explicit alignments or derivations is only computationally feasible when we are learning from annotated logical forms or have a seed lexicon, since the number of derivations is much larger than the number of logical forms. In the ILLG setting, neither are available. Generation/parsing. We generate logical forms from smallest to largest using beam search. Specifically, for each size n = 1, . . . , 8, we construct a set of logical forms of size n (with exactly n predicates) by combining logical forms of smaller sizes according to the grammar rules in Table 1. For each n, we keep the 1"
P16-1224,N13-1127,0,0.021808,"d Zettlemoyer, 2013). What makes this work unique is our new interactive learning of language games (ILLG) setting, in which a model has to learn a language from scratch through interaction. While online gradient descent is frequently used, for example in semantic parsing (Zettlemoyer and Collins, 2007; Chen, 2012), we using it in a truly online setting, taking one pass over the data and measuring online accuracy (Cesa-Bianchi and Lugosi, 2006). To speed up learning, we leverage computational models of pragmatics (Jäger, 2008; Golland et al., 2010; Frank and Goodman, 2012; Smith et al., 2013; Vogel et al., 2013). The main difference is these previous works use pragmatics with a trained base model, whereas we learn the model online. Monroe and Potts (2015) uses learning to improve the pragmatics model. In contrast, we use pragmatics to speed up the learning process by capturing phenomena like mutual exclusivity (Markman and Wachtel, 1988). We also differ from prior work in several details. First, we model pragmatics in the online learning setting where we use an online update for the pragmatics model. Second, unlikely the reference games where pragmatic effects plays an important role by design, SHRDL"
P16-1224,P10-1083,0,0.0439218,"ho are less precise and consistent. This is expected behavior: the pragmatics model assumes that the human is cooperative and behaving rationally. For the bottom half of the players, this assumption is not true, in which case the pragmatics model is not useful. 6 Related Work and Discussion Our work connects with a broad body of work on grounded language, in which language is used in some environment as a means towards some goal. Examples include playing games (Branavan et al., 2009, 2010; Reckman et al., 2010) interacting with robotics (Tellex et al., 2011, 2014), and following instructions (Vogel and Jurafsky, 2010; Chen and Mooney, 2011; Artzi and Zettlemoyer, 2013) Semantic parsing utterances to logical forms, which we leverage, plays an important role in these settings (Kollar et al., 2010; Matuszek et al., 2012; Artzi and Zettlemoyer, 2013). What makes this work unique is our new interactive learning of language games (ILLG) setting, in which a model has to learn a language from scratch through interaction. While online gradient descent is frequently used, for example in semantic parsing (Zettlemoyer and Collins, 2007; Chen, 2012), we using it in a truly online setting, taking one pass over the data"
P16-1224,H89-1033,0,0.13034,"ge consisting of four words— ‘block’, ‘pillar’, ‘slab’, ‘beam’—to successfully communicate what block to pass from A to B. This is only one such language; many others would also work for accomplishing the cooperative goal. This paper operationalizes and explores the idea of language games in a learning setting, which we call interactive learning through language games (ILLG). In the ILLG setting, the two parties do not initially speak a common language, but nonetheless need to collaboratively accomplish a goal. Specifically, we created a game called SHRDLURN,1 in homage to the seminal work of Winograd (1972). As shown in Figure 1, the objective is to transform a start state into a goal state, but the only action the human can take is entering an utterance. The computer parses the utterance and produces a ranked list of possible interpretations according to its current model. The human scrolls through the list and chooses the intended one, simultaneously advancing the state of the blocks and providing feedback to the computer. Both the human and the computer wish to reach the goal state 1 Demo: http://shrdlurn.sidaw.xyz 2368 Proceedings of the 54th Annual Meeting of the Association for Computation"
P16-1224,P07-1121,0,0.0635806,"ively as follows: ψ(h, 0) = {h}, ψ(h, d) = {(h, i, ψ(h.i, d − 1)) |i = 1, 2, 3}. The set of all features is just the cross product of utterance features and logical form features. For example, if x = ‘enlever tout’ and z = remove(all()), then features include: (‘enlever’, all) (‘enlever’, remove) (‘enlever’, (remove, 1, all)) (‘tout’, (remove, 1, all)) (‘tout’, all) (‘tout’, remove) Note that we do not model an explicit alignment or derivation compositionally connecting the utterance and the logical form, in contrast to most traditional work in semantic parsing (Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Liang et al., 2011; Kwiatkowski et al., 2010; Berant et al., 2013), instead following a looser model of semantics similar to (Pasupat and Liang, 2015). Modeling explicit alignments or derivations is only computationally feasible when we are learning from annotated logical forms or have a seed lexicon, since the number of derivations is much larger than the number of logical forms. In the ILLG setting, neither are available. Generation/parsing. We generate logical forms from smallest to largest using beam search. Specifically, for each size n = 1, . . . , 8, we construct a set of logical form"
P16-1224,D07-1071,0,0.141565,"010) interacting with robotics (Tellex et al., 2011, 2014), and following instructions (Vogel and Jurafsky, 2010; Chen and Mooney, 2011; Artzi and Zettlemoyer, 2013) Semantic parsing utterances to logical forms, which we leverage, plays an important role in these settings (Kollar et al., 2010; Matuszek et al., 2012; Artzi and Zettlemoyer, 2013). What makes this work unique is our new interactive learning of language games (ILLG) setting, in which a model has to learn a language from scratch through interaction. While online gradient descent is frequently used, for example in semantic parsing (Zettlemoyer and Collins, 2007; Chen, 2012), we using it in a truly online setting, taking one pass over the data and measuring online accuracy (Cesa-Bianchi and Lugosi, 2006). To speed up learning, we leverage computational models of pragmatics (Jäger, 2008; Golland et al., 2010; Frank and Goodman, 2012; Smith et al., 2013; Vogel et al., 2013). The main difference is these previous works use pragmatics with a trained base model, whereas we learn the model online. Monroe and Potts (2015) uses learning to improve the pragmatics model. In contrast, we use pragmatics to speed up the learning process by capturing phenomena lik"
P16-1224,Q13-1005,0,\N,Missing
P17-1086,D10-1119,0,0.215456,"vation. ID features fire on specific rules (by ID). Type features track whether a rule is part of the core language or induced, whether it has been 5 Grammar induction Recall that the main form of supervision is via user definitions, which allows creation of user-defined concepts. In this section, we show how to turn 933 these definitions into new grammar rules that can be used by the system to parse new utterances. Previous systems of grammar induction for semantic parsing were given utterance-program pairs (x, z). Both the GENLEX (Zettlemoyer and Collins, 2005) and higher-order unification (Kwiatkowski et al., 2010) algorithms overgenerate rules that liberally associate parts of x with parts of z. Though some rules are immediately pruned, many spurious rules are undoubtedly still kept. In the interactive setting, we must keep the number of candidates small to avoid a bad user experience, which means a higher precision bar for new rules. Fortunately, the structure of definitions makes the grammar induction task easier. Rather than being given an utterance-program (x, z) pair, we are given a definition, which consists of an utterance x (head) along with the body X = [x1 , . . . , xn ], which is a sequence"
P17-1086,D11-1039,0,0.0416967,"t al., 2017) and robots (Tellex et al., 2011), people need computers to perform well-specified but complex actions. To accomplish this, one route is to use a programming language, but this is inaccessible to most and can be tedious even for experts because the syntax is uncompromising and all statements have to be precise. Another route is to convert natural language into a formal lanFigure 1: Some examples of users building structures using a naturalized language in Voxelurn: http://www.voxelurn.com guage, which has been the subject of work in semantic parsing (Zettlemoyer and Collins, 2005; Artzi and Zettlemoyer, 2011, 2013; Pasupat and Liang, 2015). However, the capability of semantic parsers is still quite primitive compared to the power one wields with a programming language. This gap is increasingly limiting the potential of 929 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 929–938 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-1086 can be used by another user. Thus a community of users evolves the language to becomes more efficient over time, in a distributed way, through int"
P17-1086,Q13-1005,0,0.139804,"Missing"
P17-1086,P16-1224,1,0.924518,"is an unnatural concept for non-programmers. Therefore when the choice is not explicit, the parser generates all three possible scoping interpretations, and the model learns which is intended based on the user, the rule, and potentially the context. 3 def: add palm tree def: brown trunk height 3 def: add brown top 3 times repeat 3 [add brown top] def: go to top of tree select very top of has color brown def: add leaves here def: select all sides select left or right or front or back add green Learning interactively from definitions The goal of the user is to build a structure in Voxelurn. In Wang et al. (2016), the user provided interactive supervision to the system by selecting from a list of candidates. This is practical when there are less than tens of candidates, but is completely infeasible for a complex action space such as Voxelurn. Roughly, 10 possible colors over the 3 × 3 × 4 box containing the palm tree in Figure 2 yields 1036 distinct denotations, and many more programs. Obtaining the structures in Figure 1 by selecting candidates alone would be infeasible. This work thus uses definitions in addition to selecting candidates as the supervision signal. Each definition consists of a head u"
P17-1086,D13-1160,1,0.884347,"ward – add green monster – go down 8 – go right and front – add brown floor – add girl – go back and down – add door – add black column 30 – go up 9 – finish door – (some steps for moving are omitted) Deer: initial – bird’s eye view – deer head; up; left 2; back 2; { left antler }; right 2; {right antler} – down 4; front 2; left 3; deer body; down 6; {deer leg front}; back 7; {deer leg back}; left 4; {deer leg back}; front 7; {deer leg front} – (some steps omitted) Introduction In tasks such as analyzing and plotting data (Gulwani and Marron, 2014), querying databases (Zelle and Mooney, 1996; Berant et al., 2013), manipulating text (Kushman and Barzilay, 2013), or controlling the Internet of Things (Campagna et al., 2017) and robots (Tellex et al., 2011), people need computers to perform well-specified but complex actions. To accomplish this, one route is to use a programming language, but this is inaccessible to most and can be tedious even for experts because the syntax is uncompromising and all statements have to be precise. Another route is to convert natural language into a formal lanFigure 1: Some examples of users building structures using a naturalized language in Voxelurn: http://www.voxelurn"
P17-1086,D07-1071,0,0.222975,"Missing"
P17-1086,N13-1103,0,0.0160373,"right and front – add brown floor – add girl – go back and down – add door – add black column 30 – go up 9 – finish door – (some steps for moving are omitted) Deer: initial – bird’s eye view – deer head; up; left 2; back 2; { left antler }; right 2; {right antler} – down 4; front 2; left 3; deer body; down 6; {deer leg front}; back 7; {deer leg back}; left 4; {deer leg back}; front 7; {deer leg front} – (some steps omitted) Introduction In tasks such as analyzing and plotting data (Gulwani and Marron, 2014), querying databases (Zelle and Mooney, 1996; Berant et al., 2013), manipulating text (Kushman and Barzilay, 2013), or controlling the Internet of Things (Campagna et al., 2017) and robots (Tellex et al., 2011), people need computers to perform well-specified but complex actions. To accomplish this, one route is to use a programming language, but this is inaccessible to most and can be tedious even for experts because the syntax is uncompromising and all statements have to be precise. Another route is to convert natural language into a formal lanFigure 1: Some examples of users building structures using a naturalized language in Voxelurn: http://www.voxelurn.com guage, which has been the subject of work i"
P17-1086,P15-1142,1,\N,Missing
P17-1086,J13-2005,1,\N,Missing
P17-1097,D11-1039,0,0.097173,"Missing"
P17-1097,Q13-1005,0,0.192449,"o prevent the model from being drawn to spurious programs. Introduction We are interested in learning a semantic parser that maps natural language utterances into executable programs (e.g., logical forms). For example, in Figure 1, a program corresponding to the utterance transforms an initial world state into a new world state. We would like to learn from indirect supervision, where each training example is only labeled with the correct output (e.g. a target world state), but not the program that produced that output (Clarke et al., 2010; Liang et al., 2011; Krishnamurthy and Mitchell, 2012; Artzi and Zettlemoyer, 2013; Liang et al., 2017). The process of constructing a program can be formulated as a sequential decision-making process, where feedback is only received at the end of the sequence when the completed program is executed. In the natural language processing literature, there are two common approaches for handling this situation: 1) reinforcement learning (RL), particularly the REINFORCE algorithm (Williams, 1992; Sutton et al., 1999), which maximizes the expected reward of a sequence of actions; and 2) maximum marginal likelihood (MML), which treats the sequence of actions as a latent variable, an"
P17-1097,P09-1010,0,0.258186,"Missing"
P17-1097,D16-1245,0,0.0125053,"Missing"
P17-1097,W10-2903,0,0.121723,"s. 0.1 0.1 0.1 yellow move hasHat blue hasShirt leftOf We develop methods to prevent the model from being drawn to spurious programs. Introduction We are interested in learning a semantic parser that maps natural language utterances into executable programs (e.g., logical forms). For example, in Figure 1, a program corresponding to the utterance transforms an initial world state into a new world state. We would like to learn from indirect supervision, where each training example is only labeled with the correct output (e.g. a target world state), but not the program that produced that output (Clarke et al., 2010; Liang et al., 2011; Krishnamurthy and Mitchell, 2012; Artzi and Zettlemoyer, 2013; Liang et al., 2017). The process of constructing a program can be formulated as a sequential decision-making process, where feedback is only received at the end of the sequence when the completed program is executed. In the natural language processing literature, there are two common approaches for handling this situation: 1) reinforcement learning (RL), particularly the REINFORCE algorithm (Williams, 1992; Sutton et al., 1999), which maximizes the expected reward of a sequence of actions; and 2) maximum margi"
P17-1097,P16-1004,0,0.0300933,"003); Smith and Eisner (2006); Shen et al. (2015). However, a key difference is that these methods smooth the objective function whereas we smooth an expectation in the gradient. To underscore the difference, we note that fixing β = 0 in our method (total smoothing) is quite effective, whereas total smoothing in the simulated annealing methods would correspond to a completely flat objective function, and an uninformative gradient of zero everywhere. Neural semantic parsing. There has been recent interest in using recurrent neural networks for semantic parsing, both for modeling logical forms (Dong and Lapata, 2016; Jia and Liang, 2016; Liang et al., 2017) and for end-to-end execution (Yin et al., 2015; Neelakantan et al., 2016). We develop a neural model for the context-dependent setting, which is made possible by a new stackbased language similar to Riedel et al. (2016). Acknowledgments. This work was supported by the NSF Graduate Research Fellowship under No. DGE-114747 and the NSF CAREER Award under No. IIS-1552635. Reproducibility. Our code is made available at https://github.com/kelvinguu/lang2program. Reproducible experiments are available at https://worksheets.codalab.org/worksheets/ 0x88c914ee1"
P17-1097,P16-1002,1,0.711414,"(2006); Shen et al. (2015). However, a key difference is that these methods smooth the objective function whereas we smooth an expectation in the gradient. To underscore the difference, we note that fixing β = 0 in our method (total smoothing) is quite effective, whereas total smoothing in the simulated annealing methods would correspond to a completely flat objective function, and an uninformative gradient of zero everywhere. Neural semantic parsing. There has been recent interest in using recurrent neural networks for semantic parsing, both for modeling logical forms (Dong and Lapata, 2016; Jia and Liang, 2016; Liang et al., 2017) and for end-to-end execution (Yin et al., 2015; Neelakantan et al., 2016). We develop a neural model for the context-dependent setting, which is made possible by a new stackbased language similar to Riedel et al. (2016). Acknowledgments. This work was supported by the NSF Graduate Research Fellowship under No. DGE-114747 and the NSF CAREER Award under No. IIS-1552635. Reproducibility. Our code is made available at https://github.com/kelvinguu/lang2program. Reproducible experiments are available at https://worksheets.codalab.org/worksheets/ 0x88c914ee1d4b4a4587a07f36f090f3"
P17-1097,D12-1069,0,0.148682,"sShirt leftOf We develop methods to prevent the model from being drawn to spurious programs. Introduction We are interested in learning a semantic parser that maps natural language utterances into executable programs (e.g., logical forms). For example, in Figure 1, a program corresponding to the utterance transforms an initial world state into a new world state. We would like to learn from indirect supervision, where each training example is only labeled with the correct output (e.g. a target world state), but not the program that produced that output (Clarke et al., 2010; Liang et al., 2011; Krishnamurthy and Mitchell, 2012; Artzi and Zettlemoyer, 2013; Liang et al., 2017). The process of constructing a program can be formulated as a sequential decision-making process, where feedback is only received at the end of the sequence when the completed program is executed. In the natural language processing literature, there are two common approaches for handling this situation: 1) reinforcement learning (RL), particularly the REINFORCE algorithm (Williams, 1992; Sutton et al., 1999), which maximizes the expected reward of a sequence of actions; and 2) maximum marginal likelihood (MML), which treats the sequence of act"
P17-1097,D16-1127,0,0.00790466,"Missing"
P17-1097,P17-1003,0,0.410462,"g drawn to spurious programs. Introduction We are interested in learning a semantic parser that maps natural language utterances into executable programs (e.g., logical forms). For example, in Figure 1, a program corresponding to the utterance transforms an initial world state into a new world state. We would like to learn from indirect supervision, where each training example is only labeled with the correct output (e.g. a target world state), but not the program that produced that output (Clarke et al., 2010; Liang et al., 2011; Krishnamurthy and Mitchell, 2012; Artzi and Zettlemoyer, 2013; Liang et al., 2017). The process of constructing a program can be formulated as a sequential decision-making process, where feedback is only received at the end of the sequence when the completed program is executed. In the natural language processing literature, there are two common approaches for handling this situation: 1) reinforcement learning (RL), particularly the REINFORCE algorithm (Williams, 1992; Sutton et al., 1999), which maximizes the expected reward of a sequence of actions; and 2) maximum marginal likelihood (MML), which treats the sequence of actions as a latent variable, and then maximizes the"
P17-1097,P11-1060,1,0.915199,"move hasHat blue hasShirt leftOf We develop methods to prevent the model from being drawn to spurious programs. Introduction We are interested in learning a semantic parser that maps natural language utterances into executable programs (e.g., logical forms). For example, in Figure 1, a program corresponding to the utterance transforms an initial world state into a new world state. We would like to learn from indirect supervision, where each training example is only labeled with the correct output (e.g. a target world state), but not the program that produced that output (Clarke et al., 2010; Liang et al., 2011; Krishnamurthy and Mitchell, 2012; Artzi and Zettlemoyer, 2013; Liang et al., 2017). The process of constructing a program can be formulated as a sequential decision-making process, where feedback is only received at the end of the sequence when the completed program is executed. In the natural language processing literature, there are two common approaches for handling this situation: 1) reinforcement learning (RL), particularly the REINFORCE algorithm (Williams, 1992; Sutton et al., 1999), which maximizes the expected reward of a sequence of actions; and 2) maximum marginal likelihood (MML)"
P17-1097,P16-1138,1,0.178305,"ANDO M ER combines desirable qualities from both REINFORCE and BS-MML. 6 Experiments Evaluation. We evaluate our proposed methods on all three domains of the SCONE dataset. Accuracy is defined as the percentage of test examples where the model produces the correct final world state wM . All test examples have M = 5 (5utts), but we also report accuracy after processing the first 3 utterances (3utts). To control for the effects of randomness, we train 5 instances of each model with different random seeds. We report the median accuracy of the instances unless otherwise noted. Training. Following Long et al. (2016), we decompose each training example into smaller examples. Given an example with 5 utterances, u = [u1 , . . . , u5 ], we consider all length-1 and length-2 substrings of u: [u1 ], [u2 ], . . . , [u3 , u4 ], [u4 , u5 ] (9 total). We form a new training example from each 0 ) where u0 = [u , u ], substring, e.g., (u0 , w00 , wM 4 5 0 0 w0 = w3 and wM = w5 . All models are implemented in TensorFlow (Abadi et al., 2015). Model parameters are randomly initialized (Glorot and Bengio, 2010), with no pre-training. We use the Adam optimizer (Kingma and Ba, 2014) (which is applied to the gradient in (6"
P17-1097,D15-1001,0,0.0331872,"Missing"
P17-1097,P03-1021,0,0.0499675,"would amount to labeling each example with the correct program, which is not known. Hence, these methods cannot be directly applied. Without access to correct output sequences, we cannot directly maximize likelihood, and instead resort to maximizing the marginal likelihood (MML). Rather than proposing MML as a form of pre-training, we argue that MML is a superior substitute for the standard RL objective, and that the β-meritocratic update is even better. Simulated annealing. Our β-meritocratic update employs exponential smoothing, which bears resemblance to the simulated annealing strategy of Och (2003); Smith and Eisner (2006); Shen et al. (2015). However, a key difference is that these methods smooth the objective function whereas we smooth an expectation in the gradient. To underscore the difference, we note that fixing β = 0 in our method (total smoothing) is quite effective, whereas total smoothing in the simulated annealing methods would correspond to a completely flat objective function, and an uninformative gradient of zero everywhere. Neural semantic parsing. There has been recent interest in using recurrent neural networks for semantic parsing, both for modeling logical forms (Dong"
P17-1097,P15-1142,1,0.175137,"Missing"
P17-1097,P16-1003,1,0.165804,"learning (RL), particularly the REINFORCE algorithm (Williams, 1992; Sutton et al., 1999), which maximizes the expected reward of a sequence of actions; and 2) maximum marginal likelihood (MML), which treats the sequence of actions as a latent variable, and then maximizes the marginal likelihood of observing the correct program output (Dempster et al., 1977). While the two approaches have enjoyed success on many tasks, we found them to work poorly out of the box for our task. This is because in addition to the sparsity of correct programs, our task also requires weeding out spurious programs (Pasupat and Liang, 2016): incorrect interpretations 1051 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 1051–1062 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-1097 of the utterances that accidentally produce the correct output, as illustrated in Figure 1. We show that MML and RL optimize closely related objectives. Furthermore, both MML and RL methods have a mechanism for exploring program space in search of programs that generate the correct output. We explain why this exploration tends to"
P17-1097,D14-1162,0,0.0894244,"p(zt |x, z1:t−1 ) over the possible values of zt ∈ Z. The next token zt is sampled from this distribution. If an action token (e.g., move) is generated, the model increments the utterance pointer m. The process terminates when all M utterances are processed. The final probability of generating a Q particular program z = (z1 , . . . , zT ) is p(z |x) = Tt=1 p(zt |x, z1:t−1 ). Encoder. The utterance um under the pointer is encoded using a bidirectional LSTM: F hF i = LSTM(hi−1 , Φu (um,i )) B hB i = LSTM(hi+1 , Φu (um,i )) B hi = [hF i ; hi ], where Φu (um,i ) is the fixed GloVe word embedding (Pennington et al., 2014) of the ith word in um . The final utterance embedding is the concateB nation em = [hF |um |; h1 ]. Decoder. Unlike Bahdanau et al. (2015), which used a recurrent network for the decoder, we opt for a feed-forward network for simplicity. We use em and an embedding f (z1:t−1 ) of the previous execution history (described later) as inputs to qt = ReLU(Wq [em ; f (z1:t−1 )]) αi ∝ exp(qt> Wa hi ) X ct = αi hi . (i = 1, . . . , |um |) i Finally, after concatenating qt with ct , the distribution over the set Z of possible program tokens is computed via a softmax: p(zt |x, z1:t−1 ) ∝ exp(Φz (zt )> Ws"
P17-1097,Q14-1030,0,0.0206015,"Missing"
P17-1097,P06-2101,0,0.103349,"to labeling each example with the correct program, which is not known. Hence, these methods cannot be directly applied. Without access to correct output sequences, we cannot directly maximize likelihood, and instead resort to maximizing the marginal likelihood (MML). Rather than proposing MML as a form of pre-training, we argue that MML is a superior substitute for the standard RL objective, and that the β-meritocratic update is even better. Simulated annealing. Our β-meritocratic update employs exponential smoothing, which bears resemblance to the simulated annealing strategy of Och (2003); Smith and Eisner (2006); Shen et al. (2015). However, a key difference is that these methods smooth the objective function whereas we smooth an expectation in the gradient. To underscore the difference, we note that fixing β = 0 in our method (total smoothing) is quite effective, whereas total smoothing in the simulated annealing methods would correspond to a completely flat objective function, and an uninformative gradient of zero everywhere. Neural semantic parsing. There has been recent interest in using recurrent neural networks for semantic parsing, both for modeling logical forms (Dong and Lapata, 2016; Jia an"
P17-1162,P17-1045,0,0.0339131,"umbia? A: Hello? A: I have Jessica a friend of mine A: and Josh, both went to columbia B: or anyone working at apple? B: SELECT (Jessica, Columbia, Computer Science, Google) A: SELECT (Jessica, Columbia, Computer Science, Google) Figure 1: An example dialogue from the MutualFriends task in which two agents, A and B, each given a private list of a friends, try to identify their mutual friend. Our objective is to build an agent that can perform the task with a human. Crosstalk (Section 2.3) is italicized. Introduction Current task-oriented dialogue systems (Young et al., 2013; Wen et al., 2017; Dhingra et al., 2017) require a pre-defined dialogue state (e.g., slots such as food type and price range for a restaurant searching task) and a fixed set of dialogue acts (e.g., request, inform). However, human conversation often requires richer dialogue states and more nuanced, pragmatic dialogue acts. Recent opendomain chat systems (Shang et al., 2015; Serban et al., 2015b; Sordoni et al., 2015; Li et al., 2016a; Lowe et al., 2017; Mei et al., 2017) learn a mapping directly from previous utterances to the next utterance. While these models capture open-ended aspects of dialogue, the lack of structured dialogue"
P17-1162,P05-2014,0,0.0215319,"Missing"
P17-1162,P16-1002,1,0.647232,"(ht,j (5) 1 , [At (xt,j ), ct,j ]), where ct,j is a weighted sum ofPnode embeddings in the current turn: ct,j = v2Gt ↵t,j,v Vt (v), where ↵t,j,v are the attention weights over the nodes. Intuitively, high weight should be given to relevant entity nodes as shown in Figure 3,. We compute the weights through standard attention mechanism (Bahdanau et al., 2015): 1770 ↵t,j = softmax(st,j ), st,j,v = wattn · tanh W attn [ht,j 1 , Vt (v)] , where vector wattn and W attn are parameters. Finally, we define a distribution over both words in the vocabulary and nodes in Gt using the copying mechanism of Jia and Liang (2016): what to talk about and which item to select. It has a pattern-matching semantic parser, a rulebased policy, and a templated generator. See Appendix G for details. p(xt,j+1 = y |Gt , xt,j ) / exp W vocab ht,j + b , 4.2 p(xt,j+1 = r(v) |Gt , xt,j ) / exp (st,j,v ) , where y is a word in the vocabulary, W vocab and b are parameters, and r(v) is the realization of the entity represented by node v, e.g., google is realized to “Google” during copying.10 4 Experiments We compare our model with a rule-based system and a baseline neural model. Both automatic and human evaluations are conducted to t"
P17-1162,E17-2077,0,0.027086,"ban et al., 2015a). Most work focuses on information-querying tasks, using Wizard-ofOz data collection (Williams et al., 2016; Asri et al., 2016) or simulators (Bordes and Weston, 2017; Li et al., 2016d), In contrast, collaborative dialogues are easy to collect as natural human conversations, and are also challenging enough given the large number of scenarios and diverse conversation phenomena. There are some interesting strategic dialogue datasets—settlers of Catan (Afantenos et al., 2012) (2K turns) and the cards corpus (Potts, 2012) (1.3K dialogues), as well as work on dialogue strategies (Keizer et al., 2017; Vogel et al., 2013), though no full dialogue system has been built for these datasets. Most task-oriented dialogue systems follow the POMDP-based approach (Williams and Young, 2007; Young et al., 2013). Despite their success (Wen et al., 2017; Dhingra et al., 2017; Su et al., 2016), the requirement for handcrafted slots limits their scalability to new domains and burdens data collection with extra state labeling. To go past this limit, Bordes and Weston (2017) proposed a Memory-Networks-based approach without domain-specific features. However, the memory is unstructured and interfacing with"
P17-1162,D16-1032,0,0.00699203,"in each KB. Bots’ utterances are in bold and selected items are represented by item IDs. Only the first half of the humanRule chat is shown due to limited space. Multiple utterances of one agent rae separated by ||. chitecture is most similar to EntNet (Henaff et al., 2017), where memories are also updated by input sentences recurrently. The main difference is that our model allows information to be propagated between structured entities, which is shown to be crucial in our setting (Section 4.3). Our work is also related to language generation conditioned on knowledge bases (Mei et al., 2016; Kiddon et al., 2016). One challenge here is to avoid generating false or contradicting statements, which is currently a weakness of neural models. Our model is mostly accurate when generating facts and answering existence questions about a single entity, but will need a more advanced attention mechanism for generating utterances involving multiple entities, e.g., attending to items or attributes first, then selecting entities; generating high-level concepts before composing them to natural tokens (Serban et al., 2017a). In conclusion, we believe the symmetric collaborative dialogue setting and our dataset provide"
P17-1162,P16-1094,0,0.530198,"ective is to build an agent that can perform the task with a human. Crosstalk (Section 2.3) is italicized. Introduction Current task-oriented dialogue systems (Young et al., 2013; Wen et al., 2017; Dhingra et al., 2017) require a pre-defined dialogue state (e.g., slots such as food type and price range for a restaurant searching task) and a fixed set of dialogue acts (e.g., request, inform). However, human conversation often requires richer dialogue states and more nuanced, pragmatic dialogue acts. Recent opendomain chat systems (Shang et al., 2015; Serban et al., 2015b; Sordoni et al., 2015; Li et al., 2016a; Lowe et al., 2017; Mei et al., 2017) learn a mapping directly from previous utterances to the next utterance. While these models capture open-ended aspects of dialogue, the lack of structured dialogue state prevents them from being directly applied to settings that require interfacing with structured knowledge. In order to bridge the gap between the two types of systems, we focus on a symmetric collaborative dialogue setting, which is task-oriented but encourages open-ended dialogue acts. In our setting, two agents, each with a private list of items with attributes, must communicate to iden"
P17-1162,N16-1014,0,0.421835,"ective is to build an agent that can perform the task with a human. Crosstalk (Section 2.3) is italicized. Introduction Current task-oriented dialogue systems (Young et al., 2013; Wen et al., 2017; Dhingra et al., 2017) require a pre-defined dialogue state (e.g., slots such as food type and price range for a restaurant searching task) and a fixed set of dialogue acts (e.g., request, inform). However, human conversation often requires richer dialogue states and more nuanced, pragmatic dialogue acts. Recent opendomain chat systems (Shang et al., 2015; Serban et al., 2015b; Sordoni et al., 2015; Li et al., 2016a; Lowe et al., 2017; Mei et al., 2017) learn a mapping directly from previous utterances to the next utterance. While these models capture open-ended aspects of dialogue, the lack of structured dialogue state prevents them from being directly applied to settings that require interfacing with structured knowledge. In order to bridge the gap between the two types of systems, we focus on a symmetric collaborative dialogue setting, which is task-oriented but encourages open-ended dialogue acts. In our setting, two agents, each with a private list of items with attributes, must communicate to iden"
P17-1162,D16-1127,0,0.542094,"ective is to build an agent that can perform the task with a human. Crosstalk (Section 2.3) is italicized. Introduction Current task-oriented dialogue systems (Young et al., 2013; Wen et al., 2017; Dhingra et al., 2017) require a pre-defined dialogue state (e.g., slots such as food type and price range for a restaurant searching task) and a fixed set of dialogue acts (e.g., request, inform). However, human conversation often requires richer dialogue states and more nuanced, pragmatic dialogue acts. Recent opendomain chat systems (Shang et al., 2015; Serban et al., 2015b; Sordoni et al., 2015; Li et al., 2016a; Lowe et al., 2017; Mei et al., 2017) learn a mapping directly from previous utterances to the next utterance. While these models capture open-ended aspects of dialogue, the lack of structured dialogue state prevents them from being directly applied to settings that require interfacing with structured knowledge. In order to bridge the gap between the two types of systems, we focus on a symmetric collaborative dialogue setting, which is task-oriented but encourages open-ended dialogue acts. In our setting, two agents, each with a private list of items with attributes, must communicate to iden"
P17-1162,D16-1230,0,0.0109471,"’s use of knowledge graphs captures the grounding capability of classic task-oriented systems and the graph embedding provides the representational flexibility of neural models. The naturalness of communication in the symmetric collaborative setting enables large-scale data collection: We were able to crowdsource around 11K human-human dialogues on Amazon Mechanical Turk (AMT) in less than 15 hours.1 We show that the new dataset calls for more flexible representations beyond fully-structured states (Section 2.2). In addition to conducting the third-party human evaluation adopted by most work (Liu et al., 2016; Li et al., 2016b,c), we also conduct partner evaluation (Wen et al., 2017) where AMT workers rate their conversational partners (other workers or our models) based on fluency, correctness, cooperation, and human-likeness. We compare DynoNet with baseline neural models and a strong rulebased system. The results show that DynoNet can perform the task with humans efficiently and naturally; it also captures some strategic aspects of human-human dialogues. The contributions of this work are: (i) a new symmetric collaborative dialogue setting and a large dialogue corpus that pushes the boundaries"
P17-1162,N16-1086,0,0.00564192,"ghlighted in blue in each KB. Bots’ utterances are in bold and selected items are represented by item IDs. Only the first half of the humanRule chat is shown due to limited space. Multiple utterances of one agent rae separated by ||. chitecture is most similar to EntNet (Henaff et al., 2017), where memories are also updated by input sentences recurrently. The main difference is that our model allows information to be propagated between structured entities, which is shown to be crucial in our setting (Section 4.3). Our work is also related to language generation conditioned on knowledge bases (Mei et al., 2016; Kiddon et al., 2016). One challenge here is to avoid generating false or contradicting statements, which is currently a weakness of neural models. Our model is mostly accurate when generating facts and answering existence questions about a single entity, but will need a more advanced attention mechanism for generating utterances involving multiple entities, e.g., attending to items or attributes first, then selecting entities; generating high-level concepts before composing them to natural tokens (Serban et al., 2017a). In conclusion, we believe the symmetric collaborative dialogue setting a"
P17-1162,P15-1152,0,0.0185644,"list of a friends, try to identify their mutual friend. Our objective is to build an agent that can perform the task with a human. Crosstalk (Section 2.3) is italicized. Introduction Current task-oriented dialogue systems (Young et al., 2013; Wen et al., 2017; Dhingra et al., 2017) require a pre-defined dialogue state (e.g., slots such as food type and price range for a restaurant searching task) and a fixed set of dialogue acts (e.g., request, inform). However, human conversation often requires richer dialogue states and more nuanced, pragmatic dialogue acts. Recent opendomain chat systems (Shang et al., 2015; Serban et al., 2015b; Sordoni et al., 2015; Li et al., 2016a; Lowe et al., 2017; Mei et al., 2017) learn a mapping directly from previous utterances to the next utterance. While these models capture open-ended aspects of dialogue, the lack of structured dialogue state prevents them from being directly applied to settings that require interfacing with structured knowledge. In order to bridge the gap between the two types of systems, we focus on a symmetric collaborative dialogue setting, which is task-oriented but encourages open-ended dialogue acts. In our setting, two agents, each with a pr"
P17-1162,N15-1020,0,0.0335066,"mutual friend. Our objective is to build an agent that can perform the task with a human. Crosstalk (Section 2.3) is italicized. Introduction Current task-oriented dialogue systems (Young et al., 2013; Wen et al., 2017; Dhingra et al., 2017) require a pre-defined dialogue state (e.g., slots such as food type and price range for a restaurant searching task) and a fixed set of dialogue acts (e.g., request, inform). However, human conversation often requires richer dialogue states and more nuanced, pragmatic dialogue acts. Recent opendomain chat systems (Shang et al., 2015; Serban et al., 2015b; Sordoni et al., 2015; Li et al., 2016a; Lowe et al., 2017; Mei et al., 2017) learn a mapping directly from previous utterances to the next utterance. While these models capture open-ended aspects of dialogue, the lack of structured dialogue state prevents them from being directly applied to settings that require interfacing with structured knowledge. In order to bridge the gap between the two types of systems, we focus on a symmetric collaborative dialogue setting, which is task-oriented but encourages open-ended dialogue acts. In our setting, two agents, each with a private list of items with attributes, must co"
P17-1162,N13-1127,0,0.0699858,"Missing"
P17-1162,E17-1042,0,0.334523,"ne who went to columbia? A: Hello? A: I have Jessica a friend of mine A: and Josh, both went to columbia B: or anyone working at apple? B: SELECT (Jessica, Columbia, Computer Science, Google) A: SELECT (Jessica, Columbia, Computer Science, Google) Figure 1: An example dialogue from the MutualFriends task in which two agents, A and B, each given a private list of a friends, try to identify their mutual friend. Our objective is to build an agent that can perform the task with a human. Crosstalk (Section 2.3) is italicized. Introduction Current task-oriented dialogue systems (Young et al., 2013; Wen et al., 2017; Dhingra et al., 2017) require a pre-defined dialogue state (e.g., slots such as food type and price range for a restaurant searching task) and a fixed set of dialogue acts (e.g., request, inform). However, human conversation often requires richer dialogue states and more nuanced, pragmatic dialogue acts. Recent opendomain chat systems (Shang et al., 2015; Serban et al., 2015b; Sordoni et al., 2015; Li et al., 2016a; Lowe et al., 2017; Mei et al., 2017) learn a mapping directly from previous utterances to the next utterance. While these models capture open-ended aspects of dialogue, the lack"
P17-1162,P17-1062,0,0.0315295,"iented dialogue systems follow the POMDP-based approach (Williams and Young, 2007; Young et al., 2013). Despite their success (Wen et al., 2017; Dhingra et al., 2017; Su et al., 2016), the requirement for handcrafted slots limits their scalability to new domains and burdens data collection with extra state labeling. To go past this limit, Bordes and Weston (2017) proposed a Memory-Networks-based approach without domain-specific features. However, the memory is unstructured and interfacing with KBs relies on API calls, whereas our model embeds both the dialogue history and the KB structurally. Williams et al. (2017) use an LSTM to automatically infer the dialogue state, but as they focus on dialogue control rather than the full problem, the response is modeled as a templated action, which restricts the generation of richer utterances. Our network ar1773 Friends of A Friends of B ID Name Company Time Location ID Name Company Time Location 1 2 3 4 5 6 7 8 9 TRT Holdings Dollar General TRT Holdings SFN Group Dollar General Weis Markets TRT Holdings TRT Holdings L&L Hawaiian Barbecue afternoon afternoon afternoon afternoon afternoon afternoon morning afternoon afternoon indoor indoor outdoor indoor indoor in"
P18-1060,D16-1230,0,0.0889065,"room for improvement on this task. 5 Recall that our primary quantity of interest is data efficiency, the ratio of the number of human judgments required to estimate the overall human evaluation score for the control variates estimator versus the sample mean. We’ll briefly review the automatic metrics used in our evaluation before analyzing the results. Automatic metrics. We consider the following frequently used automatic word-overlap based metrics in our work: BLEU (Papineni et al., 2002), ROUGE (Lin and Rey, 2004) and METEOR (Lavie and Denkowski, 2009). Following Novikova et al. (2017) and Liu et al. (2016b), we also compared a vector-based sentence-similarity using sent2vec (Pagliardini et al., 2017) to compare sentences (VecSim). Figure 5 shows how each of these metrics is correlated with human judgment for the systems being evaluated. Unsurprisingly, the correlation varies considerably across systems, with token-based metrics correlating more strongly for systems that are more extractive in nature (fastqa and fastqa ext). Results.5 In Section 3 we proved that the control variates estimator is not only unbiased but also has the least variance among other unbiased estimators. Figure 6 plots th"
P18-1060,D17-1109,1,0.848875,"lly correct but poorly scored according to ROUGE-L. On the CNN/Daily Mail task, a significant number of examples which are scored highly by VecSim are poorly rated by humans, and likewise many examples scored poorly by VecSim are highly rated by humans. 645 def had hardly improved on human evaluation. Hillclimbing on ROUGE can also lead to a system that does worse on human scores, e.g. in machine translation (Wu et al., 2016). Conversely, genuine quality improvements might not be reflected in improvements in ROUGE. This bias also appears in pool-based evaluation for knowledge base population (Chaganty et al., 2017). Thus the problems with automatic metrics clearly motivate the need for human evaluation, but can we still use the automatic metrics somehow to save costs? 3 We can define σf2 = Var(f (z)) as the variance def of the human metric and σa2 = Ez [Var(Y (z))] as the variance of human judgment averaged over Z. By the law of total variance, the variance of our estimator is 1 Var(ˆ µmean ) = (σf2 + σa2 ). (2) n 3.2 Now let us see how an automatic metric g can reduce variance. If there is no annotator variance (σa2 = 0) so that Y (z) = f (z), we should expect the variance of f (z) − g(z) to be lower t"
P18-1060,D17-1234,0,0.0219394,"to improve the automatic metric (which is potentially as difficult as solving the task) and brainstorming alternative ways of soliciting evaluation (which has been less explored). Alternatively, we could give up on measuring absolute scores, and seek instead to find techniques stably rank methods and thus improve them. As the NLP community tackles increasingly difficult tasks, human evaluation will only become more important. We hope our work provides some clarity on to how to make it more cost effective. systems and does not apply to evaluating natural language generation. In a similar vein, Chang et al. (2017) dynamically collect human feedback to learn better dialog policies. 7 Discussion Prior work has shown that existing automatic metrics have poor instance-level correlation with mean human judgment and that they score many good quality responses poorly. As a result, the evaluation is systematically biased against genuine system improvements that would lead to higher human evaluation scores but not improve automatic metrics. In this paper, we have explored using an automatic metric to decrease the cost of human evaluation without introducing bias. In practice, we find that with current automatic"
P18-1060,P17-1103,0,0.198536,"rove that our estimator is optimal: there is no unbiased estimator with lower cost. Our theory further highlights the two fundamental bottlenecks—the automatic metric and the prompt shown to human evaluators— both of which need to be improved to obtain greater cost savings. 1 Introduction In recent years, there has been an increasing interest in tasks that require generating natural language, including abstractive summarization (Nallapati et al., 2016), open-response question answering (Nguyen et al., 2016; Koˇcisky et al., 2017), image captioning (Lin et al., 2014), and open-domain dialogue (Lowe et al., 2017b). Unfortunately, the evaluation of these systems remains a thorny issue because of the diversity of possible correct responses. As the gold standard of performing human evaluation is often too expensive, there has been a large effort develop∗ Authors contributed equally. 643 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 643–653 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics 0.8 ROUGE-L 0.4 ROUGE-L 1.0 fastqa fastqa ext snet snet.ens 0.3 0.6 0.4 0.2 0.2 0.0 0.5 0.6 0.7 Human judgement 0."
P18-1060,C08-1019,0,0.0759242,"is only ρ = 0.31. A closer look at the instance-level correlation reveals that while ROUGE is able to correctly assign low scores to bad examples (lower left), it is bad at judging good examples and often assigns them low ROUGE scores (lower right)— see Table 1 for examples. This observation agrees with a finding reported in Novikova et al. (2017) that automatic metrics correlate better with human judgments on bad examples than average or good examples. Thus, as Figure 1(a) shows, we can improve low-scoring ROUGE examples without improving their human judgment (M) and vice versa (.). Indeed, Conroy and Dang (2008) report that summarization systems were optimized for ROUGE during the DUC challenge (Dang, 2006) until they were indistinguishable from the ROUGE scores of human-generated summaries, but the systems Bias in automatic evaluation It is well understood that current automatic metrics tend to correlate poorly with human judgment at the instance-level. For example, Novikova et al. (2017) report correlations less than 0.3 for a large suite of word-based and grammar-based evaluation methods on a generation task. Similarly, Liu et al. (2016b) find correlations less than 0.35 for automatic metrics on a"
P18-1060,P14-5010,0,0.00386074,"system’s summary to improve its quality, similar to the post-editing step in MT evaluations (Snover et al., 2006). Obtaining judgments costs about $0.15 per summary and this cost rises to about $0.40 per summary for post-editing. We collected judgments on the summaries generated by the seq2seq and pointer models of See et al. (2017), the ml and ml+rl models of Paulus et al. (2018), and the reference summaries.3 Before presenting the summaries to human annotators, we performed some minimal post-processing: we true-cased and de-tokenized the output of seq2seq and pointer using Stanford CoreNLP (Manning et al., 2014) and replaced “unknown” tokens in each system with a special symbol (). Evaluating answer correctness. Next, we look at evaluating the correctness of system outputs in question answering using the MS MARCO question answering dataset (Nguyen et al., 2016). Here, each system is provided with a question and up to 10 paragraphs of context. The system generates open-response answers that do not need to be tied to a span in any paragraph. We first ask annotators to judge if the output is even plausible for the question, and if yes, ask them identify if it is correct according to each context paragr"
P18-1060,W14-3348,0,0.149089,"Missing"
P18-1060,K16-1028,0,0.0697435,"reduction in number of human judgments needed to obtain the same accuracy versus naive human evaluation—and show that it depends solely on two factors: (a) the annotator variance (which is a function of the human evaluation prompt) and (b) the correlation between human judgments and the automatic metric. This factorization allows us to calculate typical and best-case data efficiencies and accordingly refine the evaluation prompt or automatic metric. Finally, we evaluate our estimator on stateof-the-art systems from two tasks, summarization on the CNN/Daily Mail dataset (Hermann et al., 2015; Nallapati et al., 2016) and openresponse question answering on the MS MARCOv1.0 dataset (Nguyen et al., 2016). To study our estimators offline, we preemptively collected 10,000 human judgments which cover several For evaluating generation systems, automatic metrics such as BLEU cost nothing to run but have been shown to correlate poorly with human judgment, leading to systematic bias against certain model improvements. On the other hand, averaging human judgments, the unbiased gold standard, is often too expensive. In this paper, we use control variates to combine automatic metrics with human evaluation to obtain an"
P18-1060,D17-1238,0,0.160331,"etter with human judgments on bad examples than average or good examples. Thus, as Figure 1(a) shows, we can improve low-scoring ROUGE examples without improving their human judgment (M) and vice versa (.). Indeed, Conroy and Dang (2008) report that summarization systems were optimized for ROUGE during the DUC challenge (Dang, 2006) until they were indistinguishable from the ROUGE scores of human-generated summaries, but the systems Bias in automatic evaluation It is well understood that current automatic metrics tend to correlate poorly with human judgment at the instance-level. For example, Novikova et al. (2017) report correlations less than 0.3 for a large suite of word-based and grammar-based evaluation methods on a generation task. Similarly, Liu et al. (2016b) find correlations less than 0.35 for automatic metrics on a dialog generation task in one domain, but find correlations with the same metric dropped significantly to less than 0.16 when used in another domain. Still, somewhat surprisingly, several automatic metrics 1 An anonymized version of this data and the annotation interfaces used can be found at https://bit.ly/ price-of-debiasing. 644 Question and reference answer System answer (Syste"
P18-1060,P02-1040,0,0.107902,"Missing"
P18-1060,Q14-1025,0,0.0227276,"rompts, upper bounding the data efficiency on these tasks. A notable exception is the Edit prompt wherein systems are compared on the number of post-edits required to improve their quality. 3.4 Discussion of assumptions We will soon see that empirical instantiations of γ and ρ lead to rather underwhelming data efficiencies in practice. In light of our optimality result, does this mean there is no hope for gains? Let us probe our assumptions. We assumed that the human judgments are uncorrelated across different system outputs; it is possible that a more accurate model of human annotators (e.g. Passonneau and Carpenter (2014)) could offer improvements. Perhaps with additional information about g(z) such as calibrated confidence estimates, we would be able to sample more adaptively. Of course the most direct routes to improvement involve increasing the correlation of g with human judgments and reducing annotator variance, which we will discuss more later. 4 Evaluating language quality in automatic summarization. In automatic summarization, systems must generate a short (on average two or three sentence) summary of an article: for our study, we chose articles from the CNN/Daily Mail (CDM) dataset (Hermann et al., 20"
P18-1060,N16-1104,0,0.0208109,"room for improvement on this task. 5 Recall that our primary quantity of interest is data efficiency, the ratio of the number of human judgments required to estimate the overall human evaluation score for the control variates estimator versus the sample mean. We’ll briefly review the automatic metrics used in our evaluation before analyzing the results. Automatic metrics. We consider the following frequently used automatic word-overlap based metrics in our work: BLEU (Papineni et al., 2002), ROUGE (Lin and Rey, 2004) and METEOR (Lavie and Denkowski, 2009). Following Novikova et al. (2017) and Liu et al. (2016b), we also compared a vector-based sentence-similarity using sent2vec (Pagliardini et al., 2017) to compare sentences (VecSim). Figure 5 shows how each of these metrics is correlated with human judgment for the systems being evaluated. Unsurprisingly, the correlation varies considerably across systems, with token-based metrics correlating more strongly for systems that are more extractive in nature (fastqa and fastqa ext). Results.5 In Section 3 we proved that the control variates estimator is not only unbiased but also has the least variance among other unbiased estimators. Figure 6 plots th"
P18-1060,P17-1099,0,0.0701829,"r each summary, we collected human judgments on a scale from 1–3 (Figure 4a) for fluency, (lack of) redundancy, and overall quality of the summary using guidelines from the DUC summarization challenge (Dang, 2006). As an alternate human metric, we also asked workers to postedit the system’s summary to improve its quality, similar to the post-editing step in MT evaluations (Snover et al., 2006). Obtaining judgments costs about $0.15 per summary and this cost rises to about $0.40 per summary for post-editing. We collected judgments on the summaries generated by the seq2seq and pointer models of See et al. (2017), the ml and ml+rl models of Paulus et al. (2018), and the reference summaries.3 Before presenting the summaries to human annotators, we performed some minimal post-processing: we true-cased and de-tokenized the output of seq2seq and pointer using Stanford CoreNLP (Manning et al., 2014) and replaced “unknown” tokens in each system with a special symbol (). Evaluating answer correctness. Next, we look at evaluating the correctness of system outputs in question answering using the MS MARCO question answering dataset (Nguyen et al., 2016). Here, each system is provided with a question and up to"
P18-1060,2006.amta-papers.25,0,0.116208,"ermann et al., 2015; Nallapati et al., 2016) which come paired with reference summaries in the form of story highlights. We focus on the language quality of summaries and leave evaluating content selection to future work. For each summary, we collected human judgments on a scale from 1–3 (Figure 4a) for fluency, (lack of) redundancy, and overall quality of the summary using guidelines from the DUC summarization challenge (Dang, 2006). As an alternate human metric, we also asked workers to postedit the system’s summary to improve its quality, similar to the post-editing step in MT evaluations (Snover et al., 2006). Obtaining judgments costs about $0.15 per summary and this cost rises to about $0.40 per summary for post-editing. We collected judgments on the summaries generated by the seq2seq and pointer models of See et al. (2017), the ml and ml+rl models of Paulus et al. (2018), and the reference summaries.3 Before presenting the summaries to human annotators, we performed some minimal post-processing: we true-cased and de-tokenized the output of seq2seq and pointer using Stanford CoreNLP (Manning et al., 2014) and replaced “unknown” tokens in each system with a special symbol (). Evaluating answer c"
P18-1060,K17-1028,0,0.0319105,"Missing"
P18-1175,N09-3010,0,0.145619,"is to have a human annotator view each example, assess its relevance, and provide a label (e.g., positive or negative for binary classification). However, this only provides one bit of information per example. This invites the question: how can we get more information per example, given that the annotator has already spent the effort reading and understanding an example? Previous works have relied on identifying relevant parts of the input such as labeling features (Druck et al., 2009; Raghavan et al., 2005; Liang et al., 2009), highlighting rationale phrases in text (Zaidan and Eisner, 2008; Arora and Nyberg, 2009), or marking relevant regions in images (Ahn et al., 2006). But there are certain types of information which cannot be easily reduced to annotating a portion of the input, such as the absence of a certain word, or the presence of at least two words. In this work, we tap into the power of natural language and allow annotators to provide supervision to a classifier via natural language explanations. Specifically, we propose a framework in which annotators provide a natural language explanation for each label they assign to an example (see Figure 1). These explanations are parsed into logical for"
P18-1175,W10-2903,0,0.0749343,"d Roth (2011) convert natural language into concepts (e.g., the rules of a card game). Ling and Fidler (2017) use natural language explanations to assist in supervising an image captioning model. Weston (2016); Li et al. (2016) learn from natural language feedback in a dialogue. Wang et al. (2017) convert natural language definitions to rules in a semantic parser to build up progressively higher-level concepts. We lean on the formalism of semantic parsing (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Liang, 2016). One notable trend is to learn semantic parsers from weak supervision (Clarke et al., 2010; Liang et al., 2011), whereas our goal is to obtain weak supervision signal from semantic parsers. The broader topic of weak supervision has received much attention; we mention some works most related to relation extraction. In distant supervision (Craven et al., 1999; Mintz et al., 2009) and multi-instance learning (Riedel et al., 2010; Hoffmann et al., 2011), an existing knowledge base is used to (probabilistically) impute a training set. Various extensions have focused on aggregating a variety of supervision sources by learning generative models from noisy labels (Alfonseca et al., 2012; T"
P18-1175,D09-1009,0,0.0418041,"data into a large labeled dataset for training a classifier. Introduction The standard protocol for obtaining a labeled dataset is to have a human annotator view each example, assess its relevance, and provide a label (e.g., positive or negative for binary classification). However, this only provides one bit of information per example. This invites the question: how can we get more information per example, given that the annotator has already spent the effort reading and understanding an example? Previous works have relied on identifying relevant parts of the input such as labeling features (Druck et al., 2009; Raghavan et al., 2005; Liang et al., 2009), highlighting rationale phrases in text (Zaidan and Eisner, 2008; Arora and Nyberg, 2009), or marking relevant regions in images (Ahn et al., 2006). But there are certain types of information which cannot be easily reduced to annotating a portion of the input, such as the absence of a certain word, or the presence of at least two words. In this work, we tap into the power of natural language and allow annotators to provide supervision to a classifier via natural language explanations. Specifically, we propose a framework in which annotators provide"
P18-1175,D16-1011,0,0.126027,"Missing"
P18-1175,P11-1060,1,0.883209,"tions (LFs). Many incorrect LFs are filtered out automatically by the filter bank. The remaining functions provide heuristic labels over the unlabeled dataset, which are aggregated into one noisy label per example, yielding a large, noisily-labeled training set for a classifier. then executed on many unlabeled examples, resulting in a large, weakly-supervised training set that is then used to train a classifier. Semantic parsing of natural language into logical forms is recognized as a challenging problem and has been studied extensively (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Liang et al., 2011; Liang, 2016). One of our major findings is that in our setting, even a simple rule-based semantic parser suffices for three reasons: First, we find that the majority of incorrect LFs can be automatically filtered out either semantically (e.g., is it consistent with the associated example?) or pragmatically (e.g., does it avoid assigning the same label to the entire training set?). Second, LFs near the gold LF in the space of logical forms are often just as accurate (and sometimes even more accurate). Third, techniques for combining weak supervision sources are built to tolerate some noise (A"
P18-1175,D13-1003,0,0.021777,"whereas our goal is to obtain weak supervision signal from semantic parsers. The broader topic of weak supervision has received much attention; we mention some works most related to relation extraction. In distant supervision (Craven et al., 1999; Mintz et al., 2009) and multi-instance learning (Riedel et al., 2010; Hoffmann et al., 2011), an existing knowledge base is used to (probabilistically) impute a training set. Various extensions have focused on aggregating a variety of supervision sources by learning generative models from noisy labels (Alfonseca et al., 2012; Takamatsu et al., 2012; Roth and Klakow, 2013; Ratner et al., 2016; Varma et al., 2017). 1891 Finally, while we have used natural language explanations as input to train models, they can also be output to interpret models (Krening et al., 2017; Lei et al., 2016). More generally, from a machine learning perspective, labels are the primary asset, but they are a low bandwidth signal between annotators and the learning algorithm. Natural language opens up a much higher-bandwidth communication channel. We have shown promising results in relation extraction (where one explanation can be “worth” 100 labels), and it would be interesting to exten"
P18-1175,D17-1161,0,0.165664,"bel to the entire training set?). Second, LFs near the gold LF in the space of logical forms are often just as accurate (and sometimes even more accurate). Third, techniques for combining weak supervision sources are built to tolerate some noise (Alfonseca et al., 2012; Takamatsu et al., 2012; Ratner et al., 2018). The significance of this is that we can deploy the same semantic parser across tasks without task-specific training. We show how we can tackle a real-world biomedical application with the same semantic parser used to extract instances of spouses. Our work is most similar to that of Srivastava et al. (2017), who also use natural language explanations to train a classifier, but with two important differences. First, they jointly train a task-specific semantic parser and classifier, whereas we use a simple rule-based parser. In Section 4, we find that in our weak supervision framework, the rule-based semantic parser and the perfect parser yield nearly identical downstream performance. Second, while they use the logical forms of explanations to produce features that are fed directly to a classifier, we use them as functions for labeling a much larger training set. In Section 4, we show that using f"
P18-1175,P12-1076,0,0.256716,"our major findings is that in our setting, even a simple rule-based semantic parser suffices for three reasons: First, we find that the majority of incorrect LFs can be automatically filtered out either semantically (e.g., is it consistent with the associated example?) or pragmatically (e.g., does it avoid assigning the same label to the entire training set?). Second, LFs near the gold LF in the space of logical forms are often just as accurate (and sometimes even more accurate). Third, techniques for combining weak supervision sources are built to tolerate some noise (Alfonseca et al., 2012; Takamatsu et al., 2012; Ratner et al., 2018). The significance of this is that we can deploy the same semantic parser across tasks without task-specific training. We show how we can tackle a real-world biomedical application with the same semantic parser used to extract instances of spouses. Our work is most similar to that of Srivastava et al. (2017), who also use natural language explanations to train a classifier, but with two important differences. First, they jointly train a task-specific semantic parser and classifier, whereas we use a simple rule-based parser. In Section 4, we find that in our weak supervisi"
P18-1175,P17-1086,1,0.837133,"g natural language explanations/instructions and learning from weak supervision. The closest body of work is on “learning from natural language.” As mentioned earlier, Srivastava et al. (2017) convert natural language explanations into classifier features (whereas we convert them into labeling functions). Goldwasser and Roth (2011) convert natural language into concepts (e.g., the rules of a card game). Ling and Fidler (2017) use natural language explanations to assist in supervising an image captioning model. Weston (2016); Li et al. (2016) learn from natural language feedback in a dialogue. Wang et al. (2017) convert natural language definitions to rules in a semantic parser to build up progressively higher-level concepts. We lean on the formalism of semantic parsing (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Liang, 2016). One notable trend is to learn semantic parsers from weak supervision (Clarke et al., 2010; Liang et al., 2011), whereas our goal is to obtain weak supervision signal from semantic parsers. The broader topic of weak supervision has received much attention; we mention some works most related to relation extraction. In distant supervision (Craven et al., 1999; Mintz et"
P18-1175,D08-1004,0,0.564982,"aining a labeled dataset is to have a human annotator view each example, assess its relevance, and provide a label (e.g., positive or negative for binary classification). However, this only provides one bit of information per example. This invites the question: how can we get more information per example, given that the annotator has already spent the effort reading and understanding an example? Previous works have relied on identifying relevant parts of the input such as labeling features (Druck et al., 2009; Raghavan et al., 2005; Liang et al., 2009), highlighting rationale phrases in text (Zaidan and Eisner, 2008; Arora and Nyberg, 2009), or marking relevant regions in images (Ahn et al., 2006). But there are certain types of information which cannot be easily reduced to annotating a portion of the input, such as the absence of a certain word, or the presence of at least two words. In this work, we tap into the power of natural language and allow annotators to provide supervision to a classifier via natural language explanations. Specifically, we propose a framework in which annotators provide a natural language explanation for each label they assign to an example (see Figure 1). These explanations ar"
P18-1175,P09-1113,0,\N,Missing
P18-1175,P12-2011,0,\N,Missing
P18-1175,P11-1055,0,\N,Missing
P18-2124,D15-1075,0,0.0597556,"System BNA DocQA DocQA + ELMo SQuAD + T F I DF EM F1 72.7 76.6 75.6 79.2 79.4 83.0 SQuAD + RULE BASED EM F1 80.1 84.8 80.8 84.8 85.7 89.6 SQ UADRU N dev EM F1 59.8 62.6 61.9 64.8 65.1 67.6 Table 4: Exact Match (EM) and F1 scores on the SQ UADRU N development set, compared with SQuAD with two types of automatically generated negative examples. SQ UADRU N is more challenging for current models. 5.3 Automatically generated negatives textual entailment (RTE) requires systems to decide whether a hypothesis is entailed by, contradicted by, or neutral with respect to a premise (Marelli et al., 2014; Bowman et al., 2015). Relation extraction systems must understand when a possible relationship between two entities is not entailed by the text (Zhang et al., 2017). Jia and Liang (2017) created adversarial examples that fool pre-trained SQuAD models at test time. However, models that train on similar examples are not easily fooled by their method. In contrast, the adversarial examples in SQ UADRU N are difficult even for models trained on examples from the same distribution. In conclusion, we have presented SQ UADRU N, a challenging, diverse, and large-scale dataset that forces models to understand when a questi"
P18-2124,N18-1202,0,0.0831354,"and selected the final answer by majority vote, breaking ties in favor of answering questions and preferring shorter answers to longer ones. On average, we collected 4.8 answers per question. We note that for the original SQuAD, Rajpurkar et al. (2016) evaluated a single human’s performance; therefore, they likely underestimate human accuracy. 4.3 Models We evaluated three existing model architectures: the BiDAF-No-Answer (BNA) model proposed by Levy et al. (2017), and two versions of the DocumentQA No-Answer (DocQA) model from Clark and Gardner (2017), namely versions with and without ELMo (Peters et al., 2018). These models all learn to predict the probability that a question is unanswerable, in addition to a distribution over answer choices. At test time, models abstain whenever their predicted probability that a question is unanswerable exceeds some threshold. We tune this threshold separately for each model on the development set. When evaluating on the test set, we use the threshold that maximizes F1 score on the development set. We find this strategy does slightly better than simply taking the argmax prediction, possibly due to the different proportions of negative examples at training and tes"
P18-2124,D16-1264,1,0.859912,"estion 1: “Which laws faced significant opposition?” Plausible Answer: later laws Question 2: “What was the name of the 1937 treaty?” Plausible Answer: Bald Eagle Protection Act Figure 1: Two unanswerable questions written by crowdworkers, along with plausible (but incorrect) answers. Relevant keywords are shown in blue. et al., 2017; Wang et al., 2017; Clark and Gardner, 2017; Huang et al., 2018). Recent work has even produced systems that surpass human-level exact match accuracy on the Stanford Question Answering Dataset (SQuAD), one of the most widely-used reading comprehension benchmarks (Rajpurkar et al., 2016). Nonetheless, these systems are still far from true language understanding. Recent analysis shows that models can do well at SQuAD by learning context and type-matching heuristics (Weissenborn et al., 2017), and that success on SQuAD does not ensure robustness to distracting sentences (Jia and Liang, 2017). One root cause of these problems is SQuAD’s focus on questions for which a correct answer is guaranteed to exist in the context document. Therefore, models only need to select the span that seems most related to the question, instead of checking that the answer is actually entailed by the"
P18-2124,D13-1020,0,0.188219,"wrote 25 or fewer questions on that article; this filter helped remove noise from workers who had trouble understanding the task, and therefore quit before completing the whole article. We applied this filter to both our new data and the existing answerable questions in SQuAD. To generate train, development, and test splits, we used the same partition of articles as SQuAD, and combined the existing SQuAD data with our new data for each split. For the SQ UADRU N development and test sets, we removed articles for which we did not colMultiple choice datasets Finally, some datasets, like MCTest (Richardson et al., 2013) and RACE (Lai et al., 2017), pose multiple choice questions, which can have a “none of the above” option. In practice, multiple choice options are often unavailable, making these datasets less suited for training user-facing systems. Multiple choice questions also tend to be quite different from extractive ones, with more emphasis on fill-in-the-blank, interpretation, and summarization (Lai et al., 2017). 4 Dataset creation The SQ UADRU N dataset We now describe our new dataset, which we constructed to satisfy both the relevance and plausible answer desiderata from Section 2. 786 Train Total"
P18-2124,P16-1145,0,0.0631935,"cause of these problems is SQuAD’s focus on questions for which a correct answer is guaranteed to exist in the context document. Therefore, models only need to select the span that seems most related to the question, instead of checking that the answer is actually entailed by the text. In this work, we construct SQ UADRU N,1 a new dataset that combines the existing questions in SQuAD with 53,775 new, unanswerable quesIntroduction Machine reading comprehension has become a central task in natural language understanding, fueled by the creation of many large-scale datasets (Hermann et al., 2015; Hewlett et al., 2016; Rajpurkar et al., 2016; Nguyen et al., 2016; Trischler et al., 2017; Joshi et al., 2017). In turn, these datasets have spurred a diverse array of model architecture improvements (Seo et al., 2016; Hu ∗ 1 The first two authors contributed equally to this paper. SQuAD with adveRsarial Unanswerable questions 784 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 784–789 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics tions about the same paragraphs. Crowdworkers crafted these questions so that"
P18-2124,W17-2623,0,0.135271,"orrect answer is guaranteed to exist in the context document. Therefore, models only need to select the span that seems most related to the question, instead of checking that the answer is actually entailed by the text. In this work, we construct SQ UADRU N,1 a new dataset that combines the existing questions in SQuAD with 53,775 new, unanswerable quesIntroduction Machine reading comprehension has become a central task in natural language understanding, fueled by the creation of many large-scale datasets (Hermann et al., 2015; Hewlett et al., 2016; Rajpurkar et al., 2016; Nguyen et al., 2016; Trischler et al., 2017; Joshi et al., 2017). In turn, these datasets have spurred a diverse array of model architecture improvements (Seo et al., 2016; Hu ∗ 1 The first two authors contributed equally to this paper. SQuAD with adveRsarial Unanswerable questions 784 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 784–789 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics tions about the same paragraphs. Crowdworkers crafted these questions so that (1) they are relevant to the paragraph, and (2) the paragraph contain"
P18-2124,D07-1003,0,0.0258821,"Missing"
P18-2124,P17-1018,0,0.0969758,"Missing"
P18-2124,D17-1215,1,0.884152,"wn in blue. et al., 2017; Wang et al., 2017; Clark and Gardner, 2017; Huang et al., 2018). Recent work has even produced systems that surpass human-level exact match accuracy on the Stanford Question Answering Dataset (SQuAD), one of the most widely-used reading comprehension benchmarks (Rajpurkar et al., 2016). Nonetheless, these systems are still far from true language understanding. Recent analysis shows that models can do well at SQuAD by learning context and type-matching heuristics (Weissenborn et al., 2017), and that success on SQuAD does not ensure robustness to distracting sentences (Jia and Liang, 2017). One root cause of these problems is SQuAD’s focus on questions for which a correct answer is guaranteed to exist in the context document. Therefore, models only need to select the span that seems most related to the question, instead of checking that the answer is actually entailed by the text. In this work, we construct SQ UADRU N,1 a new dataset that combines the existing questions in SQuAD with 53,775 new, unanswerable quesIntroduction Machine reading comprehension has become a central task in natural language understanding, fueled by the creation of many large-scale datasets (Hermann et"
P18-2124,K17-1028,0,0.177597,"questions written by crowdworkers, along with plausible (but incorrect) answers. Relevant keywords are shown in blue. et al., 2017; Wang et al., 2017; Clark and Gardner, 2017; Huang et al., 2018). Recent work has even produced systems that surpass human-level exact match accuracy on the Stanford Question Answering Dataset (SQuAD), one of the most widely-used reading comprehension benchmarks (Rajpurkar et al., 2016). Nonetheless, these systems are still far from true language understanding. Recent analysis shows that models can do well at SQuAD by learning context and type-matching heuristics (Weissenborn et al., 2017), and that success on SQuAD does not ensure robustness to distracting sentences (Jia and Liang, 2017). One root cause of these problems is SQuAD’s focus on questions for which a correct answer is guaranteed to exist in the context document. Therefore, models only need to select the span that seems most related to the question, instead of checking that the answer is actually entailed by the text. In this work, we construct SQ UADRU N,1 a new dataset that combines the existing questions in SQuAD with 53,775 new, unanswerable quesIntroduction Machine reading comprehension has become a central tas"
P18-2124,P17-1147,0,0.128517,"eed to exist in the context document. Therefore, models only need to select the span that seems most related to the question, instead of checking that the answer is actually entailed by the text. In this work, we construct SQ UADRU N,1 a new dataset that combines the existing questions in SQuAD with 53,775 new, unanswerable quesIntroduction Machine reading comprehension has become a central task in natural language understanding, fueled by the creation of many large-scale datasets (Hermann et al., 2015; Hewlett et al., 2016; Rajpurkar et al., 2016; Nguyen et al., 2016; Trischler et al., 2017; Joshi et al., 2017). In turn, these datasets have spurred a diverse array of model architecture improvements (Seo et al., 2016; Hu ∗ 1 The first two authors contributed equally to this paper. SQuAD with adveRsarial Unanswerable questions 784 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 784–789 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics tions about the same paragraphs. Crowdworkers crafted these questions so that (1) they are relevant to the paragraph, and (2) the paragraph contains a plausible answer—"
P18-2124,D15-1237,0,0.114455,"Missing"
P18-2124,D17-1082,0,0.156647,"Missing"
P18-2124,P13-1171,0,0.0229205,"are not very diverse: they only replace entities and numbers with similar words, and replace nouns and adjectives with WordNet antonyms. We refer to these unanswerable questions as RULE BASED questions. Desiderata We first outline our goals for SQ UADRU N. Besides the generic goals of large size, diversity, and low noise, we posit two desiderata specific to unanswerable questions: Relevance. The unanswerable questions should appear relevant to the topic of the context paragraph. Otherwise, simple heuristics (e.g., based on word overlap) could distinguish answerable and unanswerable questions (Yih et al., 2013). Existence of plausible answers. There should be some span in the context whose type matches the type of answer the question asks for. For example, if the question asks, “What company was founded in 1992?”, then some company should be mentioned in the context. Otherwise, typematching heuristics could distinguish answerable and unanswerable questions (Weissenborn et al., 2017). 3 Extractive datasets Existing datasets Next, we survey existing reading comprehension datasets with these criteria in mind. We use the 3.2 2 As with previous versions of SQuAD, we release SQ UADRU N under the CC BY-SA"
P18-2124,K17-1034,0,0.0365374,"d to spend one minute per question, and were paid $10.50 per hour. To reduce crowdworker noise, we collected multiple human answers for each question and selected the final answer by majority vote, breaking ties in favor of answering questions and preferring shorter answers to longer ones. On average, we collected 4.8 answers per question. We note that for the original SQuAD, Rajpurkar et al. (2016) evaluated a single human’s performance; therefore, they likely underestimate human accuracy. 4.3 Models We evaluated three existing model architectures: the BiDAF-No-Answer (BNA) model proposed by Levy et al. (2017), and two versions of the DocumentQA No-Answer (DocQA) model from Clark and Gardner (2017), namely versions with and without ELMo (Peters et al., 2018). These models all learn to predict the probability that a question is unanswerable, in addition to a distribution over answer choices. At test time, models abstain whenever their predicted probability that a question is unanswerable exceeds some threshold. We tune this threshold separately for each model on the development set. When evaluating on the test set, we use the threshold that maximizes F1 score on the development set. We find this str"
P18-2124,D17-1004,0,0.0171196,"U N dev EM F1 59.8 62.6 61.9 64.8 65.1 67.6 Table 4: Exact Match (EM) and F1 scores on the SQ UADRU N development set, compared with SQuAD with two types of automatically generated negative examples. SQ UADRU N is more challenging for current models. 5.3 Automatically generated negatives textual entailment (RTE) requires systems to decide whether a hypothesis is entailed by, contradicted by, or neutral with respect to a premise (Marelli et al., 2014; Bowman et al., 2015). Relation extraction systems must understand when a possible relationship between two entities is not entailed by the text (Zhang et al., 2017). Jia and Liang (2017) created adversarial examples that fool pre-trained SQuAD models at test time. However, models that train on similar examples are not easily fooled by their method. In contrast, the adversarial examples in SQ UADRU N are difficult even for models trained on examples from the same distribution. In conclusion, we have presented SQ UADRU N, a challenging, diverse, and large-scale dataset that forces models to understand when a question cannot be answered given the context. We are optimistic that SQ UADRU N will encourage the development of new reading comprehension models th"
P18-2124,marelli-etal-2014-sick,0,0.0247178,"or model improvement. System BNA DocQA DocQA + ELMo SQuAD + T F I DF EM F1 72.7 76.6 75.6 79.2 79.4 83.0 SQuAD + RULE BASED EM F1 80.1 84.8 80.8 84.8 85.7 89.6 SQ UADRU N dev EM F1 59.8 62.6 61.9 64.8 65.1 67.6 Table 4: Exact Match (EM) and F1 scores on the SQ UADRU N development set, compared with SQuAD with two types of automatically generated negative examples. SQ UADRU N is more challenging for current models. 5.3 Automatically generated negatives textual entailment (RTE) requires systems to decide whether a hypothesis is entailed by, contradicted by, or neutral with respect to a premise (Marelli et al., 2014; Bowman et al., 2015). Relation extraction systems must understand when a possible relationship between two entities is not entailed by the text (Zhang et al., 2017). Jia and Liang (2017) created adversarial examples that fool pre-trained SQuAD models at test time. However, models that train on similar examples are not easily fooled by their method. In contrast, the adversarial examples in SQ UADRU N are difficult even for models trained on examples from the same distribution. In conclusion, we have presented SQ UADRU N, a challenging, diverse, and large-scale dataset that forces models to un"
Q15-1039,P11-1158,0,0.0165153,"e.g., (S ET, 3:5)) is capped at K. Typical bottom-up parsing is employed, where we build all parses for spans of length n before n + 1, etc. This fixed-order parsing strategy constructs many unnecessary parses though. For example, it would create K parses for the category E NTITY and the span over “lincoln”, generating the logical form USSLincoln, although it is unlikely that this entity would be in the final logical form. To overcome the problems with fixed-order parsing, we turn to agenda-based parsing (Kay, 1986; Caraballo and Charniak, 1998; Klein and Manning, 2003; Pauls and Klein, 2009; Auli and Lopez, 2011). In agenda-based parsing, an agenda (priority queue) holds partial parses that can be constructed next. At each step, the parse with the highest priority is popped from the agenda and put into the chart. This gives the parser full control over the sequence of parses constructed. But importantly, agenda-based parsing requires a good scoring function that can rank not just full parses but also partial parses on the agenda. How do we obtain such a scoring function? To this end, we borrow ideas from imitation learning for structured prediction (Daume et al., 2009; Ross et al., 2011; Goldberg and"
Q15-1039,P14-1091,0,0.674948,"Missing"
Q15-1039,P14-1133,1,0.716721,"the task of mapping natural language to semantic representations (e.g., logical forms), has emerged in recent years as a promising paradigm for developing question answering systems (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Kwiatkowski et al., 2010; Liang et al., 2011) and other natural language interfaces (Zettlemoyer and Collins, 2007; Tellex et al., 2011; Matuszek et al., 2012). Recently, there have been two major trends: The first is to scale semantic parsing to large knowledge bases (KB) such as Freebase (Cai and Yates, 2013; Kwiatkowski et al., 2013; Berant and Liang, 2014). The second is to learn semantic parsers without relying on annotated logical forms, but instead on their denotations (answers) (Clarke et al., 2010; Liang et al., 2011); this lessens the annotation burden and has been instrumental in fueling the first trend (Berant et al., 2013). In this paper, we are interested in training semantic parsers from denotations on large KBs. The challenge in this setting is that the vocabulary of the target logical language often contains thousands of logical predicates, and there is a mismatch between the structure of the natural language and the logical langua"
Q15-1039,D13-1160,1,0.830462,"0; Liang et al., 2011) and other natural language interfaces (Zettlemoyer and Collins, 2007; Tellex et al., 2011; Matuszek et al., 2012). Recently, there have been two major trends: The first is to scale semantic parsing to large knowledge bases (KB) such as Freebase (Cai and Yates, 2013; Kwiatkowski et al., 2013; Berant and Liang, 2014). The second is to learn semantic parsers without relying on annotated logical forms, but instead on their denotations (answers) (Clarke et al., 2010; Liang et al., 2011); this lessens the annotation burden and has been instrumental in fueling the first trend (Berant et al., 2013). In this paper, we are interested in training semantic parsers from denotations on large KBs. The challenge in this setting is that the vocabulary of the target logical language often contains thousands of logical predicates, and there is a mismatch between the structure of the natural language and the logical language. As a result, the space of possible semantic parses for even a short utterance grows quickly. For example, consider the utterance “what city was abraham lincoln born in”. Figure 1 illustrates the number of possible semantic parses that can be constructed over some of the uttera"
Q15-1039,P11-1045,0,0.120641,"as F IXED O RDER with beam size 10–20. For the chosen beam size (K = 200), AGENDA IL is 9x faster than F IXE D O RDER . For K = 1, performance is poor for AGENDA IL and zero for F IXED O RDER. This highlights the inherent difficulty of mapping to logical forms compared to more shallow tasks, as maintaining just a single best derivation for each parsing state is not sufficient. A common variant on beam parsing is to replace the fixed beam size K with a threshold α, and prune any derivation whose probability is at least α times smaller than the best derivation in that state (Zhang et al., 2010; Bodenstab et al., 2011). We implemented this baseline and compared it to AGENDA IL 30 FixedOrder AgendaIL 25 3.0 30 20 2.5 2.0 1.5 1.0 10 0 1 3.5 time (sec) accuracy 40 4.0 FixedOrder AgendaIL #derivations (thousands) 50 0.0 1 20 15 10 5 0.5 10 20 50 100 200 400 beam size FixedOrder AgendaIL (scored) AgendaIL (popped) 10 20 50 100 200 400 beam size 0 1 10 20 50 100 200 400 beam size Figure 8: Comparing AGENDA IL and F IXED O RDER for various beam sizes (left: accuracy, middle: parsing time at test time in seconds, right: number of thousands of derivations scored and popped). The x-axis is on a logarithmic scale. and"
Q15-1039,D14-1067,0,0.277654,"Missing"
Q15-1039,P13-1042,0,0.223438,"this utterance. Introduction Semantic parsing, the task of mapping natural language to semantic representations (e.g., logical forms), has emerged in recent years as a promising paradigm for developing question answering systems (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Kwiatkowski et al., 2010; Liang et al., 2011) and other natural language interfaces (Zettlemoyer and Collins, 2007; Tellex et al., 2011; Matuszek et al., 2012). Recently, there have been two major trends: The first is to scale semantic parsing to large knowledge bases (KB) such as Freebase (Cai and Yates, 2013; Kwiatkowski et al., 2013; Berant and Liang, 2014). The second is to learn semantic parsers without relying on annotated logical forms, but instead on their denotations (answers) (Clarke et al., 2010; Liang et al., 2011); this lessens the annotation burden and has been instrumental in fueling the first trend (Berant et al., 2013). In this paper, we are interested in training semantic parsers from denotations on large KBs. The challenge in this setting is that the vocabulary of the target logical language often contains thousands of logical predicates, and there is a mismatch between the struc"
Q15-1039,J98-2004,0,0.501273,"d beam search, where the number of parses (see Figure 2) for each chart cell (e.g., (S ET, 3:5)) is capped at K. Typical bottom-up parsing is employed, where we build all parses for spans of length n before n + 1, etc. This fixed-order parsing strategy constructs many unnecessary parses though. For example, it would create K parses for the category E NTITY and the span over “lincoln”, generating the logical form USSLincoln, although it is unlikely that this entity would be in the final logical form. To overcome the problems with fixed-order parsing, we turn to agenda-based parsing (Kay, 1986; Caraballo and Charniak, 1998; Klein and Manning, 2003; Pauls and Klein, 2009; Auli and Lopez, 2011). In agenda-based parsing, an agenda (priority queue) holds partial parses that can be constructed next. At each step, the parse with the highest priority is popped from the agenda and put into the chart. This gives the parser full control over the sequence of parses constructed. But importantly, agenda-based parsing requires a good scoring function that can rank not just full parses but also partial parses on the agenda. How do we obtain such a scoring function? To this end, we borrow ideas from imitation learning for stru"
Q15-1039,W10-2903,0,0.133798,"g question answering systems (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Kwiatkowski et al., 2010; Liang et al., 2011) and other natural language interfaces (Zettlemoyer and Collins, 2007; Tellex et al., 2011; Matuszek et al., 2012). Recently, there have been two major trends: The first is to scale semantic parsing to large knowledge bases (KB) such as Freebase (Cai and Yates, 2013; Kwiatkowski et al., 2013; Berant and Liang, 2014). The second is to learn semantic parsers without relying on annotated logical forms, but instead on their denotations (answers) (Clarke et al., 2010; Liang et al., 2011); this lessens the annotation burden and has been instrumental in fueling the first trend (Berant et al., 2013). In this paper, we are interested in training semantic parsers from denotations on large KBs. The challenge in this setting is that the vocabulary of the target logical language often contains thousands of logical predicates, and there is a mismatch between the structure of the natural language and the logical language. As a result, the space of possible semantic parses for even a short utterance grows quickly. For example, consider the utterance “what city was a"
Q15-1039,D13-1197,0,0.0335917,"Missing"
Q15-1039,Q13-1033,0,0.0703911,"Lopez, 2011). In agenda-based parsing, an agenda (priority queue) holds partial parses that can be constructed next. At each step, the parse with the highest priority is popped from the agenda and put into the chart. This gives the parser full control over the sequence of parses constructed. But importantly, agenda-based parsing requires a good scoring function that can rank not just full parses but also partial parses on the agenda. How do we obtain such a scoring function? To this end, we borrow ideas from imitation learning for structured prediction (Daume et al., 2009; Ross et al., 2011; Goldberg and Nivre, 2013; Chang et al., 2015). Specifically, we cast agenda-based semantic parsing as a Markov decision process, where the goal is to learn a policy, that given a state (i.e., the current chart and agenda), chooses the best next action (i.e., the parse to pop from the agenda). The supervision signal is used to generate a sequence of 546 oracle actions, from which the model is trained. Our work bears a strong resemblance to Jiang et al. (2012), who applied imitation learning to agendabased parsing, but in the context of syntactic parsing. However, two new challenges arise in semantic parsing. First, sy"
Q15-1039,W05-1506,0,0.0403074,"2012), who had just a single item in each chart cell. 7 Lazy Agenda As we saw in Section 3, a single semantic function (e.g., L EX, B RIDGE) can create hundreds of derivations. Scoring all these derivations when adding them to the agenda is wasteful, because most have low probability. In this section, we assume semantic functions return a derivation stream, i.e., an iterator that lazily computes derivations on demand. Our lazy agenda G will hold derivation streams rather than derivations, and the actual agenda Q will be defined only implicitly. The intuition is similar to lazy K-best parsing (Huang and Chiang, 2005), but is applied to agenda-based semantic parsing. Our main assumption is that every derivation stream g = [d1 , d2 , . . . ], is sorted by decreasing score: s(d1 ) ≥ s(d2 ) ≥ · · · (in practice, this is only approximated as we explain at the end of this section). We define the score of a derivation stream as s(g) = s(d1 ). At test time the only change to Algorithm 1 is in line 4, where instead of popping the G s(g[1]) |g| U [d1 ] 7 1 0.88 5 100 11.92 [d2 , d3 , d4 , . . . ] G s(g[1]) |g| U [d1 ] 7 1 0.88 [d2 ] 5 1 0.12 1 1 0.006 [d3 ] [d4 , . . . ] −2 98 0.004 Figure 7: Unrolling a derivation"
Q15-1039,P07-1019,0,0.00809241,"in imitation learning (Abbeel and Ng, 2004; Daume et al., 2009; Ross et al., 2011; Goldberg and Nivre, 2013) has shown that interpolating with the model (corresponding to smaller β) can improve generalization. We were unable to improve accuracy by annealing β from 1000 to 0, so understanding this dynamic remains an open question. Parsing. In this paper, we avoided computing K derivations in each chart cell using an agenda and learning a scoring function for choosing agenda items. A complementary and purely algorithmic solution is lazy K-best parsing (Huang and Chiang, 2005), or cube growing (Huang and Chiang, 2007), which do not involve learning or an agenda. Similar to our work, cube growing approximates the best derivations in each chart cell in the case where features do not decompose Work in the past attempted to speed up inference using a simple model that is trained separately and used to prune the hypotheses considered by the main parsing model (Bodenstab et al., 2011; FitzGerald et al., 2013). We on the other hand speed up inference by training a single model that learns to follow good parsing actions. Work in agenda-based syntactic parsing (Klein and Manning, 2003; Pauls and Klein, 2009) focuse"
Q15-1039,P08-1067,0,0.0130397,"ore), and a completion estimate (outside score). Good estimates for the outside score result in a decrease in the number of derivations. Currently actions depend on the inside score, but we could add features based on chart derivations to provide “outside” information. Adding such features would present computational challenges as scores on the agenda would have to be updated as the agenda and chart are modified. Semantic parsing has been gaining momentum in recent years, but still there has been relatively little work on developing faster algorithms, especially compared to syntactic parsing (Huang, 2008; Kummerfeld et al., 2010; Rush and Petrov, 2012; Lewis and Steedman, 2014). While we have obtained significant speedups, we hope to encourage new ideas that exploit the structure of semantic parsing to yield better algorithms. Reproducibility. All code,8 data, and experiments for this paper are available on the CodaLab platform at https://www.codalab.org/worksheets/ 0x8fdfad310dd84b7baf683b520b4b64d5/. Acknowledgments We thank the anonymous reviewers and the action editor, Jason Eisner, for their thorough reviews and constructive feedback. We also gratefully acknowledge the support of the DAR"
Q15-1039,N03-1016,0,0.377514,"r of parses (see Figure 2) for each chart cell (e.g., (S ET, 3:5)) is capped at K. Typical bottom-up parsing is employed, where we build all parses for spans of length n before n + 1, etc. This fixed-order parsing strategy constructs many unnecessary parses though. For example, it would create K parses for the category E NTITY and the span over “lincoln”, generating the logical form USSLincoln, although it is unlikely that this entity would be in the final logical form. To overcome the problems with fixed-order parsing, we turn to agenda-based parsing (Kay, 1986; Caraballo and Charniak, 1998; Klein and Manning, 2003; Pauls and Klein, 2009; Auli and Lopez, 2011). In agenda-based parsing, an agenda (priority queue) holds partial parses that can be constructed next. At each step, the parse with the highest priority is popped from the agenda and put into the chart. This gives the parser full control over the sequence of parses constructed. But importantly, agenda-based parsing requires a good scoring function that can rank not just full parses but also partial parses on the agenda. How do we obtain such a scoring function? To this end, we borrow ideas from imitation learning for structured prediction (Daume"
Q15-1039,D12-1069,0,0.709011,"on batch: 10/2015; Published 11/2015. c 2015 Association for Computational Linguistics. Distributed under a CC-BY 4.0 license. Root:Type.City u PlaceOfBirthOf.AbeLincoln Intersect what Set:Type.City was Set:PlaceOfBirthOf.AbeLincoln Join Lex city Entity:AbeLincoln Lex abraham lincoln Binary:PlaceOfBirthOf Lex born in Figure 2: An example semantic parse, or derivation, for the utterance “what city was abraham lincoln born in”. Each node in the tree has a category (e.g., E NTITY) and a logical form (e.g., AbeLincoln). hard search problem. To manage this combinatorial explosion, past approaches (Krishnamurthy and Mitchell, 2012; Kwiatkowski et al., 2013; Berant et al., 2013) used beam search, where the number of parses (see Figure 2) for each chart cell (e.g., (S ET, 3:5)) is capped at K. Typical bottom-up parsing is employed, where we build all parses for spans of length n before n + 1, etc. This fixed-order parsing strategy constructs many unnecessary parses though. For example, it would create K parses for the category E NTITY and the span over “lincoln”, generating the logical form USSLincoln, although it is unlikely that this entity would be in the final logical form. To overcome the problems with fixed-order p"
Q15-1039,P10-1036,0,0.026314,"ompletion estimate (outside score). Good estimates for the outside score result in a decrease in the number of derivations. Currently actions depend on the inside score, but we could add features based on chart derivations to provide “outside” information. Adding such features would present computational challenges as scores on the agenda would have to be updated as the agenda and chart are modified. Semantic parsing has been gaining momentum in recent years, but still there has been relatively little work on developing faster algorithms, especially compared to syntactic parsing (Huang, 2008; Kummerfeld et al., 2010; Rush and Petrov, 2012; Lewis and Steedman, 2014). While we have obtained significant speedups, we hope to encourage new ideas that exploit the structure of semantic parsing to yield better algorithms. Reproducibility. All code,8 data, and experiments for this paper are available on the CodaLab platform at https://www.codalab.org/worksheets/ 0x8fdfad310dd84b7baf683b520b4b64d5/. Acknowledgments We thank the anonymous reviewers and the action editor, Jason Eisner, for their thorough reviews and constructive feedback. We also gratefully acknowledge the support of the DARPA Communicating with Com"
Q15-1039,D10-1119,0,0.0394942,"art for the utterance “what city was abraham lincoln born in”. Numbers in chart cells indicate the number of possible semantic parses constructed over that span, and arrows point to some of the logical forms that were constructed. There are more than one million possible semantic parses for this utterance. Introduction Semantic parsing, the task of mapping natural language to semantic representations (e.g., logical forms), has emerged in recent years as a promising paradigm for developing question answering systems (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Kwiatkowski et al., 2010; Liang et al., 2011) and other natural language interfaces (Zettlemoyer and Collins, 2007; Tellex et al., 2011; Matuszek et al., 2012). Recently, there have been two major trends: The first is to scale semantic parsing to large knowledge bases (KB) such as Freebase (Cai and Yates, 2013; Kwiatkowski et al., 2013; Berant and Liang, 2014). The second is to learn semantic parsers without relying on annotated logical forms, but instead on their denotations (answers) (Clarke et al., 2010; Liang et al., 2011); this lessens the annotation burden and has been instrumental in fueling the first trend (B"
Q15-1039,D13-1161,0,0.183508,"duction Semantic parsing, the task of mapping natural language to semantic representations (e.g., logical forms), has emerged in recent years as a promising paradigm for developing question answering systems (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Kwiatkowski et al., 2010; Liang et al., 2011) and other natural language interfaces (Zettlemoyer and Collins, 2007; Tellex et al., 2011; Matuszek et al., 2012). Recently, there have been two major trends: The first is to scale semantic parsing to large knowledge bases (KB) such as Freebase (Cai and Yates, 2013; Kwiatkowski et al., 2013; Berant and Liang, 2014). The second is to learn semantic parsers without relying on annotated logical forms, but instead on their denotations (answers) (Clarke et al., 2010; Liang et al., 2011); this lessens the annotation burden and has been instrumental in fueling the first trend (Berant et al., 2013). In this paper, we are interested in training semantic parsers from denotations on large KBs. The challenge in this setting is that the vocabulary of the target logical language often contains thousands of logical predicates, and there is a mismatch between the structure of the natural langua"
Q15-1039,D14-1107,0,0.0332052,"es for the outside score result in a decrease in the number of derivations. Currently actions depend on the inside score, but we could add features based on chart derivations to provide “outside” information. Adding such features would present computational challenges as scores on the agenda would have to be updated as the agenda and chart are modified. Semantic parsing has been gaining momentum in recent years, but still there has been relatively little work on developing faster algorithms, especially compared to syntactic parsing (Huang, 2008; Kummerfeld et al., 2010; Rush and Petrov, 2012; Lewis and Steedman, 2014). While we have obtained significant speedups, we hope to encourage new ideas that exploit the structure of semantic parsing to yield better algorithms. Reproducibility. All code,8 data, and experiments for this paper are available on the CodaLab platform at https://www.codalab.org/worksheets/ 0x8fdfad310dd84b7baf683b520b4b64d5/. Acknowledgments We thank the anonymous reviewers and the action editor, Jason Eisner, for their thorough reviews and constructive feedback. We also gratefully acknowledge the support of the DARPA Communicating with Computers (CwC) program under ARO prime contract no."
Q15-1039,P11-1060,1,0.904504,"t city was abraham lincoln born in”. Numbers in chart cells indicate the number of possible semantic parses constructed over that span, and arrows point to some of the logical forms that were constructed. There are more than one million possible semantic parses for this utterance. Introduction Semantic parsing, the task of mapping natural language to semantic representations (e.g., logical forms), has emerged in recent years as a promising paradigm for developing question answering systems (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Kwiatkowski et al., 2010; Liang et al., 2011) and other natural language interfaces (Zettlemoyer and Collins, 2007; Tellex et al., 2011; Matuszek et al., 2012). Recently, there have been two major trends: The first is to scale semantic parsing to large knowledge bases (KB) such as Freebase (Cai and Yates, 2013; Kwiatkowski et al., 2013; Berant and Liang, 2014). The second is to learn semantic parsers without relying on annotated logical forms, but instead on their denotations (answers) (Clarke et al., 2010; Liang et al., 2011); this lessens the annotation burden and has been instrumental in fueling the first trend (Berant et al., 2013)."
Q15-1039,P14-5010,0,0.00359529,"Missing"
Q15-1039,P09-1108,0,0.271725,") for each chart cell (e.g., (S ET, 3:5)) is capped at K. Typical bottom-up parsing is employed, where we build all parses for spans of length n before n + 1, etc. This fixed-order parsing strategy constructs many unnecessary parses though. For example, it would create K parses for the category E NTITY and the span over “lincoln”, generating the logical form USSLincoln, although it is unlikely that this entity would be in the final logical form. To overcome the problems with fixed-order parsing, we turn to agenda-based parsing (Kay, 1986; Caraballo and Charniak, 1998; Klein and Manning, 2003; Pauls and Klein, 2009; Auli and Lopez, 2011). In agenda-based parsing, an agenda (priority queue) holds partial parses that can be constructed next. At each step, the parse with the highest priority is popped from the agenda and put into the chart. This gives the parser full control over the sequence of parses constructed. But importantly, agenda-based parsing requires a good scoring function that can rank not just full parses but also partial parses on the agenda. How do we obtain such a scoring function? To this end, we borrow ideas from imitation learning for structured prediction (Daume et al., 2009; Ross et a"
Q15-1039,N12-1054,0,0.0505466,"de score). Good estimates for the outside score result in a decrease in the number of derivations. Currently actions depend on the inside score, but we could add features based on chart derivations to provide “outside” information. Adding such features would present computational challenges as scores on the agenda would have to be updated as the agenda and chart are modified. Semantic parsing has been gaining momentum in recent years, but still there has been relatively little work on developing faster algorithms, especially compared to syntactic parsing (Huang, 2008; Kummerfeld et al., 2010; Rush and Petrov, 2012; Lewis and Steedman, 2014). While we have obtained significant speedups, we hope to encourage new ideas that exploit the structure of semantic parsing to yield better algorithms. Reproducibility. All code,8 data, and experiments for this paper are available on the CodaLab platform at https://www.codalab.org/worksheets/ 0x8fdfad310dd84b7baf683b520b4b64d5/. Acknowledgments We thank the anonymous reviewers and the action editor, Jason Eisner, for their thorough reviews and constructive feedback. We also gratefully acknowledge the support of the DARPA Communicating with Computers (CwC) program un"
Q15-1039,P07-1121,0,0.086672,"Figure 1: A parsing chart for the utterance “what city was abraham lincoln born in”. Numbers in chart cells indicate the number of possible semantic parses constructed over that span, and arrows point to some of the logical forms that were constructed. There are more than one million possible semantic parses for this utterance. Introduction Semantic parsing, the task of mapping natural language to semantic representations (e.g., logical forms), has emerged in recent years as a promising paradigm for developing question answering systems (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Kwiatkowski et al., 2010; Liang et al., 2011) and other natural language interfaces (Zettlemoyer and Collins, 2007; Tellex et al., 2011; Matuszek et al., 2012). Recently, there have been two major trends: The first is to scale semantic parsing to large knowledge bases (KB) such as Freebase (Cai and Yates, 2013; Kwiatkowski et al., 2013; Berant and Liang, 2014). The second is to learn semantic parsers without relying on annotated logical forms, but instead on their denotations (answers) (Clarke et al., 2010; Liang et al., 2011); this lessens the annotation burden and has been instrumental in"
Q15-1039,D14-1071,0,0.479256,"Missing"
Q15-1039,P14-1090,0,0.673614,"Missing"
Q15-1039,P15-1128,0,0.736716,"AGENDA F IXED +AGENDA α = 1000 α = 100 α = 10 p+w θ p+c θ pθ -B INARYA ND L EMMA Table 1: Test set results for the standard fixed-order parser (F IXED O RDER) and our new agenda-based parser (AGENDA IL), which substantially reduces parsing time and the number of parsing actions at no cost to accuracy. System YV14 BCFL13 BDZZ14 BWC14 BL14 YDZR14 BWC14+ BL14 WYWH14 WYWH14 YCHG15 F IXED O RDER AGENDA IL Authors Yao and Van-Durme (2014) Berant et al. (2013) Bao et al. (2014) Bordes et al. (2014) Berant and Liang (2014) Yang et al. (2014) Bordes et al. (2014) Wang et al. (2014) Wang et al. (2014) Yih et al. (2015) this work this work Acc. 35.4 35.7 37.5 39.2 39.9 41.3 41.8 45.3 45.3 52.5 49.6 49.7 Dev. 48.0 49.1 45.9 47.1 47.8 35.6 27.0 43.3 36.8 1.2 40.5 |Act.| 1,421 18,259 6,211 6,281 11,279 3,858 1,604 1,706 3,758 12,302 1,561 |Feat.| 1,912 18,259 6,320 6,615 11,279 3,858 1,604 2,121 4,278 15,524 2,110 Time 214 1,972 419 775 1,216 174 78 238 358 1,497 167 Table 3: Development set results for variants of AGENDA IL. ates a larger space of derivations. 9.2 Analysis To gain insight into our system components, we perform extensive experiments on the development set. Table 2: Results on the W EB Q UESTION"
Q15-1039,D07-1071,0,0.172777,"indicate the number of possible semantic parses constructed over that span, and arrows point to some of the logical forms that were constructed. There are more than one million possible semantic parses for this utterance. Introduction Semantic parsing, the task of mapping natural language to semantic representations (e.g., logical forms), has emerged in recent years as a promising paradigm for developing question answering systems (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Kwiatkowski et al., 2010; Liang et al., 2011) and other natural language interfaces (Zettlemoyer and Collins, 2007; Tellex et al., 2011; Matuszek et al., 2012). Recently, there have been two major trends: The first is to scale semantic parsing to large knowledge bases (KB) such as Freebase (Cai and Yates, 2013; Kwiatkowski et al., 2013; Berant and Liang, 2014). The second is to learn semantic parsers without relying on annotated logical forms, but instead on their denotations (answers) (Clarke et al., 2010; Liang et al., 2011); this lessens the annotation burden and has been instrumental in fueling the first trend (Berant et al., 2013). In this paper, we are interested in training semantic parsers from de"
Q15-1039,C10-2168,0,0.0159143,"oughly as efficient as F IXED O RDER with beam size 10–20. For the chosen beam size (K = 200), AGENDA IL is 9x faster than F IXE D O RDER . For K = 1, performance is poor for AGENDA IL and zero for F IXED O RDER. This highlights the inherent difficulty of mapping to logical forms compared to more shallow tasks, as maintaining just a single best derivation for each parsing state is not sufficient. A common variant on beam parsing is to replace the fixed beam size K with a threshold α, and prune any derivation whose probability is at least α times smaller than the best derivation in that state (Zhang et al., 2010; Bodenstab et al., 2011). We implemented this baseline and compared it to AGENDA IL 30 FixedOrder AgendaIL 25 3.0 30 20 2.5 2.0 1.5 1.0 10 0 1 3.5 time (sec) accuracy 40 4.0 FixedOrder AgendaIL #derivations (thousands) 50 0.0 1 20 15 10 5 0.5 10 20 50 100 200 400 beam size FixedOrder AgendaIL (scored) AgendaIL (popped) 10 20 50 100 200 400 beam size 0 1 10 20 50 100 200 400 beam size Figure 8: Comparing AGENDA IL and F IXED O RDER for various beam sizes (left: accuracy, middle: parsing time at test time in seconds, right: number of thousands of derivations scored and popped). The x-axis is on"
Q15-1039,J13-2005,1,\N,Missing
Q18-1031,S14-2010,0,0.012801,"Missing"
Q18-1031,K16-1002,0,0.420783,"(κ) − + log(κ)(d/2 − 1) − (d − 2) log(2)/2, d−2 2κ (6) where In (κ) is the modified Bessel function of the first kind, Γ is the gamma function, and d is the dimensionality of f . We can see that this too is constant with respect to Θq via the following intuition: both the KL divergence and the prior do not change under rotations, and thus we can see KL(vMF(µ, κ)kvMF(µ, 0))) = KL(vMF(e1 , κ)kvMF(e1 , 0))) by rotating µ to the first canonical basis vector. Hence ∇Θq LKL = 0. Comparison with existing VAE encoders. Our design of q differs from the typical choice of a standard normal distribution (Bowman et al., 2016; Kingma and Welling, 2014) for two reasons: First, by construction, edit vectors are sums of word vectors and since cosine distances are traditionally used to measure distances between word vectors, it would be natural to encode distances between edit vectors by the cosine distance. The vonMises Fisher distribution captures this idea, as the log likelihood decays with cosine similarity. Second, our design of q allows us to explicitly control the tradeoff between the two terms in our objective, Lgen and LKL . Note from equations 5 and 6 that LKL is purely a function of the hyperparameters  an"
Q18-1031,Q16-1020,1,0.661856,"ce. However, the SVAE’s latent vector is meant to represent the entire sentence, whereas the neural editor’s latent vector represents an edit. Our results from Section 4.3 suggest that local variation over edits is easier to model than global variation over sentences. Our use of lexical similarity neighborhoods is comparable to context windows in word vector training (Mikolov et al., 2013a). More generally, results in manifold learning demonstrate that a weak metric such as lexical similarity can be used to extract semantic similarity through distributional statistics (Tenenbaum et al., 2000; Hashimoto et al., 2016). From a generative modeling perspective, editing randomly sampled training sentences closely resembles nonparametric kernel density estimation (Parzen, 1962) where one samples points from a training set, and adds noise to smooth the density. Our edit model is the text equivalent of Gaussian noise, and our training mechanism is a type of learned smoothing kernel. Prototype-then-edit is a semi-parametric approach that remembers the entire training set and uses a neural editor to generalize meaningfully beyond the training set. The training set provides a strong inductive bias — that the corpus"
Q18-1031,D15-1006,0,0.0300655,"Missing"
Q18-1031,P13-2121,0,0.0121957,"both corpora, we used the named-entity recognizer (NER) in spaCy2 to replace named entities with their NER categories. We replaced tokens outside the top 10,000 most frequent tokens with an “out-of-vocabulary” token. 4.2 Generative modeling We compare N EURAL E DITOR as a language model against the following baseline language models: 1. NLM: a standard left-to-right neural language model generating from scratch. For fair com2 honnibal.github.io/spaCy 442 parison, we use the exact same architecture as the decoder of N EURAL E DITOR. 2. KN5: a standard 5-gram Kneser-Ney language model in KenLM (Heafield et al., 2013). 3. M EMORIZATION: generates by sampling a sentence from the training set. Perplexity. We start by evaluating N EURAL E DI TOR ’s value as a language model, measured in terms of perplexity. We use the likelihood lower bound in Equation 3, where we sum over training set instances within Jaccard distance &lt; 0.5, and for the VAE term in N EURAL E DITOR, we use the onesample approximation to the lower bound used in Kingma (2014) and Bowman (2016). To evaluate N EURAL E DITOR’s perplexity, we use linear smoothing with NLM to account for rare sentences not within our Jaccard distance threshold. This"
Q18-1031,D13-1176,0,0.0205551,"n the context of a Yelp review. Examples in Table 7 show the model is accurate and captures lexical analogies requiring word reorderings. 5 Related work and discussion Our work connects with a broad literature on attention-based neural models, retrieval-augmented text generation, semantically meaningful representations, and nonparametric statistics. Based upon recurrent neural networks and sequence-to-sequence architectures (Sutskever et al., 2014), neural language models (Bengio et al., 2003) have been widely used due to their flexibility and performance across a wide range of NLP tasks 447 (Kalchbrenner and Blunsom, 2013; Hahn and Mani, 2000; Ritter et al., 2011). Our work is motivated by an emerging consensus that attention-based mechanisms (Bahdanau et al., 2015) can substantially improve performance on various sequence to sequence tasks by capturing more information from the input sequence (Vaswani et al., 2017). Our work extends the applicability of attention mechanisms beyond sequence-to-sequence models by allowing models to attend to randomly sampled sentences. There is a growing literature on applying retrieval mechanisms to augment text generation models. For example, in the image captioning literatur"
Q18-1031,P13-2138,0,0.0219775,"Missing"
Q18-1031,N16-1014,0,0.0603331,"e-then-edit model generates a sentence by sampling a random example from the training set and then editing it using a randomly sampled edit vector. Introduction The ability to generate sentences is core to many NLP tasks, including machine translation, summarization, speech recognition, and dialogue. Most neural models for these tasks are based on recurrent neural language models (NLMs), which generate sentences from scratch, often in a left-to-right manner (Bengio et al., 2003). It is often observed that such NLMs suffer from the problem of favoring generic utterances such as “I don’t know” (Li et al., 2016). At the same time, naive strategies to increase diversity have been shown to compromise grammaticality (Shao et al., 2017), suggesting that current NLMs may lack the inductive bias to faithfully represent the full diversity of complex utterances. Indeed, it is difficult even for humans to write complex text from scratch in a single pass; we often create an initial draft and incrementally revise it (Hayes and Flower, 1986). Inspired by this process, we propose a new unconditional generative model of text which we call the prototype-then-edit model, illustrated in Figure 1. It first samples a r"
Q18-1031,W14-1602,0,0.216568,"Missing"
Q18-1031,N13-1090,0,0.651575,"w sentence y1 , we would like to find a y2 such that the same relation r holds between y1 and y2 . Our approach is to estimate the edit vector between x1 and x2 as zˆ = f (x1 , x2 ) — the mode of the inverse neural editor q. We then apply this edit vector to y1 using the neural editor to yield yˆ2 = argmaxx pedit (x |y1 , zˆ). Since it is difficult to output yˆ2 exactly matching y2 , we take the top k candidate outputs of pedit (using beam search) and evaluate whether the gold y2 appears among the top k elements. We generate the semantic relations r using prior evaluations for word analogies (Mikolov et al., 2013a; Mikolov et al., 2013b). We leverage these to generate a new dataset of sentence analogies, using a simple strategy: given an analogous word pair (w1 , w2 ), we mine the Yelp corpus for sentence pairs Google Microsoft Method gram4-superlative gram3-comparative family JJR_JJS VB_VBD VBD_VBZ NN_NNS VB_VBZ JJ_JJR 0.45 0.85 1.0 0.75 0.63 0.82 0.82 0.61 0.77 GloVE 0.75 0.60 0.10 0.75 0.32 0.10 0.29 0.01 0.09 0.79 0.45 0.10 0.57 0.16 0.08 0.60 0.23 0.14 0.58 0.17 0.15 0.41 0.01 0.05 0.24 0.06 0.03 Edit vector (top 10) Edit vector (top 1) Sampling (top 10) Table 6: Edit vectors capture one-word sen"
Q18-1031,D14-1162,0,0.112979,"Missing"
Q18-1031,D11-1054,0,0.0122992,"how the model is accurate and captures lexical analogies requiring word reorderings. 5 Related work and discussion Our work connects with a broad literature on attention-based neural models, retrieval-augmented text generation, semantically meaningful representations, and nonparametric statistics. Based upon recurrent neural networks and sequence-to-sequence architectures (Sutskever et al., 2014), neural language models (Bengio et al., 2003) have been widely used due to their flexibility and performance across a wide range of NLP tasks 447 (Kalchbrenner and Blunsom, 2013; Hahn and Mani, 2000; Ritter et al., 2011). Our work is motivated by an emerging consensus that attention-based mechanisms (Bahdanau et al., 2015) can substantially improve performance on various sequence to sequence tasks by capturing more information from the input sequence (Vaswani et al., 2017). Our work extends the applicability of attention mechanisms beyond sequence-to-sequence models by allowing models to attend to randomly sampled sentences. There is a growing literature on applying retrieval mechanisms to augment text generation models. For example, in the image captioning literature, Hodosh (2013), Kuznetsova (2013) and Mas"
Q18-1031,D17-1235,0,0.00731872,"domly sampled edit vector. Introduction The ability to generate sentences is core to many NLP tasks, including machine translation, summarization, speech recognition, and dialogue. Most neural models for these tasks are based on recurrent neural language models (NLMs), which generate sentences from scratch, often in a left-to-right manner (Bengio et al., 2003). It is often observed that such NLMs suffer from the problem of favoring generic utterances such as “I don’t know” (Li et al., 2016). At the same time, naive strategies to increase diversity have been shown to compromise grammaticality (Shao et al., 2017), suggesting that current NLMs may lack the inductive bias to faithfully represent the full diversity of complex utterances. Indeed, it is difficult even for humans to write complex text from scratch in a single pass; we often create an initial draft and incrementally revise it (Hayes and Flower, 1986). Inspired by this process, we propose a new unconditional generative model of text which we call the prototype-then-edit model, illustrated in Figure 1. It first samples a random prototype sentence from the training corpus, and then invokes a neural editor, which draws a random “edit vector” and"
Q18-1037,N16-1181,0,0.0333336,"Missing"
Q18-1037,D10-1040,1,0.781111,"Missing"
Q18-1037,Q13-1016,0,0.0183958,"tate of the world based on what others say (inference), all while taking into account that others are strategizing agents too (pragmatics). All three aspects have been studied in both the linguistics and AI communities. For planning, Markov Decision Processes and their extensions can be used to compute utility-maximizing actions via forward-looking recurrences (e.g., Vogel et al. (2013a)). For inference, model-theoretic semantics (Montague, 1973) provides a mechanism for utterances to constrain possible worlds, and this has been implemented recently in semantic parsing (Matuszek et al., 2012; Krishnamurthy and Kollar, 2013). Finally, for pragmatics, the cooperative principle of Grice (1975) can be realized by models in which a speaker simulates a listener—e.g., Franke (2009) and Frank and Goodman (2012). Find B2 Find B2 C? ?2 B? ?3 B? ?2 Pletter view Pdigit view Pletter : square Pdigit : circle Pletter : click (1,3) Figure 1: A game of InfoJigsaw played by two human players. One of the players (Pletter ) only sees the letters, while the other one (Pdigit ) only sees the digits. Their goal is to identify the goal object, B2, by exchanging a few words. The clouds show the hypothesized role of planning, inference,"
Q18-1037,N13-1127,0,0.118492,"Inference: The square’s letter must be B. Pragmatics: The square’s digit cannot be 2. Introduction Human communication is extraordinarily rich. People routinely choose what to say based on their goals (planning), figure out the state of the world based on what others say (inference), all while taking into account that others are strategizing agents too (pragmatics). All three aspects have been studied in both the linguistics and AI communities. For planning, Markov Decision Processes and their extensions can be used to compute utility-maximizing actions via forward-looking recurrences (e.g., Vogel et al. (2013a)). For inference, model-theoretic semantics (Montague, 1973) provides a mechanism for utterances to constrain possible worlds, and this has been implemented recently in semantic parsing (Matuszek et al., 2012; Krishnamurthy and Kollar, 2013). Finally, for pragmatics, the cooperative principle of Grice (1975) can be realized by models in which a speaker simulates a listener—e.g., Franke (2009) and Frank and Goodman (2012). Find B2 Find B2 C? ?2 B? ?3 B? ?2 Pletter view Pdigit view Pletter : square Pdigit : circle Pletter : click (1,3) Figure 1: A game of InfoJigsaw played by two human players"
Q18-1037,P13-2014,0,0.0772187,"Inference: The square’s letter must be B. Pragmatics: The square’s digit cannot be 2. Introduction Human communication is extraordinarily rich. People routinely choose what to say based on their goals (planning), figure out the state of the world based on what others say (inference), all while taking into account that others are strategizing agents too (pragmatics). All three aspects have been studied in both the linguistics and AI communities. For planning, Markov Decision Processes and their extensions can be used to compute utility-maximizing actions via forward-looking recurrences (e.g., Vogel et al. (2013a)). For inference, model-theoretic semantics (Montague, 1973) provides a mechanism for utterances to constrain possible worlds, and this has been implemented recently in semantic parsing (Matuszek et al., 2012; Krishnamurthy and Kollar, 2013). Finally, for pragmatics, the cooperative principle of Grice (1975) can be realized by models in which a speaker simulates a listener—e.g., Franke (2009) and Frank and Goodman (2012). Find B2 Find B2 C? ?2 B? ?3 B? ?2 Pletter view Pdigit view Pletter : square Pdigit : circle Pletter : click (1,3) Figure 1: A game of InfoJigsaw played by two human players"
Q18-1037,E17-1042,0,0.0683341,"Missing"
