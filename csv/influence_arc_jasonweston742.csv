2020.acl-main.183,N19-1423,0,0.0266703,"k resource, in an effort to improve the model’s ability to deal with blended data. We take the best model previously trained, and tune it in this fashion. Harnessing those trained models could potentially allow a conversational agent to jointly exhibit all skills, with minimal additional training. Instead, one trains a top-level ‘dialogue manager’ which is a classifier with the dialogue context as input, that predicts which skill to use on each turn, and then outputs the utterance produced by the corresponding trained model. Specifically, we train a three-class classifier on top of BERT-base (Devlin et al., 2019) that assigns an utterance to the dataset it came from. We remove duplicate utterances present in more than one of the datasets prior to training and upsample with replacement to create equal representation in the classifier’s training set. We also remove context from the utterances including topics from Wizard of Wikipedia and personas from ConvAI2 before training this classifier and when performing evaluation to prevent the classifier from relying on these (cf. the bias mitigation mentioned above). 4 Experiments In Section 4.1, we introduce the automated metrics and human evaluations that we"
2020.acl-main.183,D19-1461,1,0.944147,"mlessly blend listening with empathy, providing knowledgeable responses, and talking about various topics from everyday life to their favorite hobbies or latest challenges. 1 ”Skills” in the conversational AI literature is sometimes taken to mean a very defined specific set of abilities such as telling the weather (e.g., Zhou et al. (2020)). Our use in this paper is much more general and refers to any desirable capability. Recent research has made solid strides towards gauging and improving performance of opendomain conversational agents along specific axes such as how knowledgeable they are (Dinan et al., 2019b; Moghe et al., 2018; Qin et al., 2019), how well they can display empathy (Rashkin et al., 2019; Lin et al., 2019) or talk about their personal background (Zhang et al., 2018; Li et al., 2017). However it remains unclear whether models optimized for performance along one of these axes can retain the learned skill while blending it with other desirable skills, or how to best conduct simultaneous training of multiple skills. In this work, we compare several ways to combine tasks designed to evaluate and improve a single conversational skill, ranging from multi-task training over several datase"
2020.acl-main.183,D19-1107,0,0.03562,"Missing"
2020.acl-main.183,I17-1099,0,0.109815,"rsational AI literature is sometimes taken to mean a very defined specific set of abilities such as telling the weather (e.g., Zhou et al. (2020)). Our use in this paper is much more general and refers to any desirable capability. Recent research has made solid strides towards gauging and improving performance of opendomain conversational agents along specific axes such as how knowledgeable they are (Dinan et al., 2019b; Moghe et al., 2018; Qin et al., 2019), how well they can display empathy (Rashkin et al., 2019; Lin et al., 2019) or talk about their personal background (Zhang et al., 2018; Li et al., 2017). However it remains unclear whether models optimized for performance along one of these axes can retain the learned skill while blending it with other desirable skills, or how to best conduct simultaneous training of multiple skills. In this work, we compare several ways to combine tasks designed to evaluate and improve a single conversational skill, ranging from multi-task training over several datasets to training a top-level classifier to play the role of a dialogue manager and query the most appropriate single-skill pretrained model for a response. In order to evaluate those methods, we p"
2020.acl-main.183,D19-1012,0,0.0315418,"ay life to their favorite hobbies or latest challenges. 1 ”Skills” in the conversational AI literature is sometimes taken to mean a very defined specific set of abilities such as telling the weather (e.g., Zhou et al. (2020)). Our use in this paper is much more general and refers to any desirable capability. Recent research has made solid strides towards gauging and improving performance of opendomain conversational agents along specific axes such as how knowledgeable they are (Dinan et al., 2019b; Moghe et al., 2018; Qin et al., 2019), how well they can display empathy (Rashkin et al., 2019; Lin et al., 2019) or talk about their personal background (Zhang et al., 2018; Li et al., 2017). However it remains unclear whether models optimized for performance along one of these axes can retain the learned skill while blending it with other desirable skills, or how to best conduct simultaneous training of multiple skills. In this work, we compare several ways to combine tasks designed to evaluate and improve a single conversational skill, ranging from multi-task training over several datasets to training a top-level classifier to play the role of a dialogue manager and query the most appropriate single-s"
2020.acl-main.183,D18-1298,0,0.044772,"Missing"
2020.acl-main.183,D18-1255,0,0.0197381,"ng with empathy, providing knowledgeable responses, and talking about various topics from everyday life to their favorite hobbies or latest challenges. 1 ”Skills” in the conversational AI literature is sometimes taken to mean a very defined specific set of abilities such as telling the weather (e.g., Zhou et al. (2020)). Our use in this paper is much more general and refers to any desirable capability. Recent research has made solid strides towards gauging and improving performance of opendomain conversational agents along specific axes such as how knowledgeable they are (Dinan et al., 2019b; Moghe et al., 2018; Qin et al., 2019), how well they can display empathy (Rashkin et al., 2019; Lin et al., 2019) or talk about their personal background (Zhang et al., 2018; Li et al., 2017). However it remains unclear whether models optimized for performance along one of these axes can retain the learned skill while blending it with other desirable skills, or how to best conduct simultaneous training of multiple skills. In this work, we compare several ways to combine tasks designed to evaluate and improve a single conversational skill, ranging from multi-task training over several datasets to training a top-"
2020.acl-main.183,P19-1539,0,0.0248343,"viding knowledgeable responses, and talking about various topics from everyday life to their favorite hobbies or latest challenges. 1 ”Skills” in the conversational AI literature is sometimes taken to mean a very defined specific set of abilities such as telling the weather (e.g., Zhou et al. (2020)). Our use in this paper is much more general and refers to any desirable capability. Recent research has made solid strides towards gauging and improving performance of opendomain conversational agents along specific axes such as how knowledgeable they are (Dinan et al., 2019b; Moghe et al., 2018; Qin et al., 2019), how well they can display empathy (Rashkin et al., 2019; Lin et al., 2019) or talk about their personal background (Zhang et al., 2018; Li et al., 2017). However it remains unclear whether models optimized for performance along one of these axes can retain the learned skill while blending it with other desirable skills, or how to best conduct simultaneous training of multiple skills. In this work, we compare several ways to combine tasks designed to evaluate and improve a single conversational skill, ranging from multi-task training over several datasets to training a top-level classifier to"
2020.acl-main.183,P19-1534,1,0.937497,"ous topics from everyday life to their favorite hobbies or latest challenges. 1 ”Skills” in the conversational AI literature is sometimes taken to mean a very defined specific set of abilities such as telling the weather (e.g., Zhou et al. (2020)). Our use in this paper is much more general and refers to any desirable capability. Recent research has made solid strides towards gauging and improving performance of opendomain conversational agents along specific axes such as how knowledgeable they are (Dinan et al., 2019b; Moghe et al., 2018; Qin et al., 2019), how well they can display empathy (Rashkin et al., 2019; Lin et al., 2019) or talk about their personal background (Zhang et al., 2018; Li et al., 2017). However it remains unclear whether models optimized for performance along one of these axes can retain the learned skill while blending it with other desirable skills, or how to best conduct simultaneous training of multiple skills. In this work, we compare several ways to combine tasks designed to evaluate and improve a single conversational skill, ranging from multi-task training over several datasets to training a top-level classifier to play the role of a dialogue manager and query the most a"
2020.acl-main.183,P18-1205,1,0.927134,"Skills” in the conversational AI literature is sometimes taken to mean a very defined specific set of abilities such as telling the weather (e.g., Zhou et al. (2020)). Our use in this paper is much more general and refers to any desirable capability. Recent research has made solid strides towards gauging and improving performance of opendomain conversational agents along specific axes such as how knowledgeable they are (Dinan et al., 2019b; Moghe et al., 2018; Qin et al., 2019), how well they can display empathy (Rashkin et al., 2019; Lin et al., 2019) or talk about their personal background (Zhang et al., 2018; Li et al., 2017). However it remains unclear whether models optimized for performance along one of these axes can retain the learned skill while blending it with other desirable skills, or how to best conduct simultaneous training of multiple skills. In this work, we compare several ways to combine tasks designed to evaluate and improve a single conversational skill, ranging from multi-task training over several datasets to training a top-level classifier to play the role of a dialogue manager and query the most appropriate single-skill pretrained model for a response. In order to evaluate t"
2020.acl-main.183,2020.cl-1.2,0,0.0612468,"Missing"
2020.acl-main.183,D17-2014,1,0.860062,"pout fractions, learning-rate schedule, the number of polyencoder codes used to represent the context, the output scaling factor, and the output reduction type (max across outputs vs. mean across outputs vs. first output only). Hyperparameters that were held constant included a training batch size of 512 and learning with Adamax; 12 encoder layers and an embedding size of 768; and label and text truncation lengths of 72 and 360. Note this model discards all casing information. Models were trained until validation-set hits@1 failed to improve for 10 epochs. All training is conducted in ParlAI (Miller et al., 2017). Model selection during fine-tuning is performed by choosing the model that scores highest on hits@1 on the validation set. This architecture is then leveraged in different ways to combine different skills in a single agent. Fine-tuning on the BlendedSkillTalk Dataset The simplest setting is to directly fine-tune the base architecture on a dataset that exhibits the blended skills we are looking for. In this setting, we simply fine-tune the poly-encoder pre-trained on pushshift.io Reddit on the BlendedSkillTalk dataset, following the procedure in Humeau et al. (2019). This setting is referred"
2020.acl-main.219,I17-1047,0,0.0896997,"Missing"
2020.acl-main.219,W18-3022,0,0.0315537,"tations. An overview of the retrieval archictecture is shown in Figure 2. For the generative model, the three encoders are used as input, and a further decoder Transformer is used for outputting a token sequence; beam search is applied. Image Encoder We build our models on top of pretrained image features, and compare the performance of two types of image encoders. The first is a residual network with 152 layers described in He et al. (2016) trained on ImageNet (Russakovsky et al., 2015) to classify images among 1000 classes, which we refer to in the rest of the pa2416 logue tasks previously (Yang et al., 2018; Mazare et al., 2018). We use a Transformer with 4 layers, 300 hidden units, and 6 attention heads. The outputs are pooled (mean) to give a final vectorial encoding. We pretrain the entire encoder following the setup described in Mazare et al. (2018): we train two encoders on a next-utterance retrieval task on a Reddit dataset of dialogues containing 1.7 billion pairs of utterances, where one encodes the context and another the candidates for the next utterance; their dot product indicates the degree of match, and they are trained with negative log-likelihood and k-negative sampling. We then"
2020.acl-main.219,D18-1012,0,0.0204209,"onent, but concentrates on image-grounded dialogue, rather than image captioning. Visual question answering (Antol et al., 2015) and visual dialogue (Das et al., 2017) are another set of tasks which employ vision and language. They require the machine to answer factual questions about the contents of the image, either in single turn or dialogue form. They do not attempt to model natural conversation, but rather assess whether the machine can perform basic perception over the image via a series of questions. There are some works which directly address dialogue grounded with vision. The work of Pasunuru and Bansal (2018) assesses the ability to execute dialogue given video of computer soccer games. The work of Huber et al. (2018) investigates the use of sentiment-based visual features and facial expressions for emotional image-based dialogue. Perhaps the most related work to ours is Mostafazadeh et al. (2017). Their work considers (visual context, textual context, question, response) tuples, and builds validation and test sets based on 4k eventful images called Image Grounded Conversations (IGC). No training data is provided, but instead the authors use Twitter for that in their experiments. In contrast, we p"
2020.acl-main.219,Q14-1006,0,0.0591166,"are text-based only, many of the techniques developed can likely be transferred for use in multimodal systems, for example using state-of-the-art Transformer representations for text (Mazare et al., 2018) as a sub-component. In the area of language and vision, one of the most widely studied areas is image captioning, whereby a single utterance is output given an input image. This typically involves producing a factual, descriptive sentence describing the image, in contrast to producing a conversational utterance as in dialogue. Popular datasets include COCO (Chen et al., 2015) and Flickr30k (Young et al., 2014). Again, a variety of sequence-to-sequence (Vinyals et al., 2015; Xu et al., 2015; Anderson et al., 2018) and retrieval models (Gu et al., 2018; Faghri et al., 2018; Nam et al., 2016) have been applied. These tasks measure the ability of models to understand the content of an image, but not to carry out an engaging conversation grounded in perception. Some works have extended image captioning from being purely factual towards more engaging captions by incorporating style while still being single turn, e.g. (Mathews et al., 2018, 2016; Gan et al., 2017; Guo et al., 2019; Shuster et al., 2019)."
2020.acl-main.219,W16-3608,0,0.0310834,"ounded Conversations (IGC). No training data is provided, but instead the authors use Twitter for that in their experiments. In contrast, we provide training, validation and testing sets over 202k images for our task (that do not overlap with IGC), and consider a general set of images and dialogues, not just events and questions plus responses. In our experiments we also show strong transfer ability of our models to the IGC task. While there are many ways to measure dialogue quality, human engagement is a popular metric. Engagement itself can be measured in many ways (Bohus and Horvitz, 2009; Yu et al., 2016) but here we adopt the common approach of simply asking humans which speaker they find more engaging, following other works (Li et al., 2019; Dinan et al., 2020). 3 Image-Chat The I MAGE -C HAT dataset is a large collection of (image, style trait for speaker A, style trait for speaker B, dialogue between A & B) tuples that we collected using crowd-workers, Each dialogue consists of consecutive turns by speaker A and B. No particular constraints are placed on the kinds of utterance, only that we ask the speakers to both use the provided style trait, and to respond to the given image and dialogu"
2020.acl-main.219,P18-1205,1,0.94676,"sider transfer to the existing Image Grounded Conversations (IGC) task of Mostafazadeh et al. (2017), where we obtain stateof-the-art results. 2 Related Work The majority of work in dialogue is not grounded in perception, e.g. much recent work explores sequence-to-sequence models or retrieval models for goal-directed (Henderson et al., 2014) or chit1 http://parl.ai/projects/image_chat 2414 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 2414–2429 c July 5 - 10, 2020. 2020 Association for Computational Linguistics chat tasks (Vinyals and Le, 2015; Zhang et al., 2018). While these tasks are text-based only, many of the techniques developed can likely be transferred for use in multimodal systems, for example using state-of-the-art Transformer representations for text (Mazare et al., 2018) as a sub-component. In the area of language and vision, one of the most widely studied areas is image captioning, whereby a single utterance is output given an input image. This typically involves producing a factual, descriptive sentence describing the image, in contrast to producing a conversational utterance as in dialogue. Popular datasets include COCO (Chen et al., 20"
2020.acl-main.219,W09-3933,0,\N,Missing
2020.acl-main.219,W14-4337,0,\N,Missing
2020.acl-main.219,D18-1298,1,\N,Missing
2020.acl-main.222,D18-1241,0,0.022471,"19a; Shuster et al., 2018). As well as citing the relevant papers’ results where possible in the experiments section, we also train a BERTbased (Devlin et al., 2019) generative model as an additional baseline. 3.2 Related Tasks and Collections In the interests of feasibility, there are tasks we did not include in dodecaDialogue. For example, there are additional knowledge tasks (Qin et al., 2019; Moghe et al., 2018; Gopalakrishnan et al., 2019) and image-based datasets (Das et al., 2017) one could use. There are also a large number of QA tasks we did not include, e.g. Rajpurkar et al. (2016); Choi et al. (2018). In general, our choices were made based on tasks that after training might produce an engaging dialogue agent that humans naturally would want to talk to – which means either natural datasets or crowdsourced datasets where crowdworkers were encouraged to engage one another. As computational resources and ambitions scale, it would be interesting to add more tasks as well, while retaining the twelve we have chosen here in order to continue to evaluate their success, whilst extending the scope of the entire system. All the subtasks in the collection we use here already exist. Other research pro"
2020.acl-main.222,W11-0609,0,0.0401907,"trieving over documents from the internet. Empathetic Dialogues Rashkin et al. (2019) constructed a dataset of crowdworker conversations grounded in an emotional situation. In each dia2454 logue, one speaker describes a personal situation and the other plays a “listener” role, displaying empathy during the discussion. The dataset contains descriptions of the situations being discussed with an attached emotion label, but these are not used here. Trained models are measured playing the part of the empathetic listener, an important feature of an agent to which humans wish to speak. Cornell Movie Danescu-Niculescu-Mizil and Lee (2011) constructed a corpus containing a collection of fictional conversations from movie scripts, thus covering a large diversity of topics and emotional states. LIGHT LIGHT (Urbanek et al., 2019) involves situated interactions between characters in a text adventure game. Similar to ConvAI2, personas for each character are given, with the training set including conversations between crowdworkers playing those roles. Different from ConvAI2, included are emotes and actions grounded within the game world (e.g. picking up and giving objects). As such, it measures the ability of a conversational agent t"
2020.acl-main.222,N19-1423,0,0.0489264,"Missing"
2020.acl-main.222,P19-1346,1,0.830793,"nversations from movie scripts, thus covering a large diversity of topics and emotional states. LIGHT LIGHT (Urbanek et al., 2019) involves situated interactions between characters in a text adventure game. Similar to ConvAI2, personas for each character are given, with the training set including conversations between crowdworkers playing those roles. Different from ConvAI2, included are emotes and actions grounded within the game world (e.g. picking up and giving objects). As such, it measures the ability of a conversational agent to ground its discussion on a dynamic environment. ELI5 ELI5 (Fan et al., 2019) involves long-form question answering grounded on multiple retrieved documents in order to answer common questions which people ask on the popular ELI5 subreddit. As such, the answers are in a conversational form applicable to a dialogue agent. Ubuntu Lowe et al. (2015) built a dataset that involves in-depth discussions in solving Ubuntu problems. This studies the ability of an agent on a very focused single topic, and is also a standard benchmark in the field. Twitter We use a variant of Twitter discussions (text-only), which have been used in many existing studies, e.g. Sordoni et al. (2015"
2020.acl-main.222,W15-4640,0,0.0339486,"the training set including conversations between crowdworkers playing those roles. Different from ConvAI2, included are emotes and actions grounded within the game world (e.g. picking up and giving objects). As such, it measures the ability of a conversational agent to ground its discussion on a dynamic environment. ELI5 ELI5 (Fan et al., 2019) involves long-form question answering grounded on multiple retrieved documents in order to answer common questions which people ask on the popular ELI5 subreddit. As such, the answers are in a conversational form applicable to a dialogue agent. Ubuntu Lowe et al. (2015) built a dataset that involves in-depth discussions in solving Ubuntu problems. This studies the ability of an agent on a very focused single topic, and is also a standard benchmark in the field. Twitter We use a variant of Twitter discussions (text-only), which have been used in many existing studies, e.g. Sordoni et al. (2015); See et al. (2019). This data naturally involves everyday discussions about topics that people care about. The public forum makes them different from the more personal discussions of some of the other tasks. This is the second largest dataset in the collection, and we"
2020.acl-main.222,D18-1298,0,0.0653183,"Missing"
2020.acl-main.222,D17-2014,1,0.956988,"BERT baseline is roughly equivalent to the model of Wolf et al. (2019b), but does not have a classification loss term. The implementation relies on HuggingFace Transformers (Wolf et al., 2019a). We thus finetune this model for each of our tasks, except Image Chat and IGC which require images as input. Image+Seq2Seq. We use a modification of a transformer Seq2Seq architecture (Vaswani et al., 2017), additionally adding image features to the encoder. Our model is a 8 layer encoder, 8 layer decoder with 512 dimensional embeddings and 16 attention heads, and is based on the ParlAI implementation (Miller et al., 2017). We use BPE following Humeau et al. (2019) via lower-cased Wikipedia, Toronto Books, and Open Subtitles with 30k merges, giving 54,940 terms. Reported perplexities are computed with this dictionary. For image features, we use the pre-trained image features from the ResNeXt-IG-3.5B model, a ResNeXt 32 x 48d architecture (Xie et al., 2017) trained on 3.5 billion Instagram images following the procedure 2456 BE RT -b as ed Si ng le Ta sk (fr Si om ng le sc Ta ra tc sk h) ( fa Tw s t itt Te er xt + in Si it) ng Re l e dd Ta it sk O nl y Re dd it + Si ng M le T Ta A sk ll Ta sk s+ A ll FT Ta Si sk"
2020.acl-main.222,D18-1255,0,0.031384,"sking ability (Radford et al., 2019; Keskar et al., 2019), and transformer-based approaches have been adapted to language and vision tasks as well (Lu et al., 2019; Tan and Bansal, 2019; Li et al., 2019a; Shuster et al., 2018). As well as citing the relevant papers’ results where possible in the experiments section, we also train a BERTbased (Devlin et al., 2019) generative model as an additional baseline. 3.2 Related Tasks and Collections In the interests of feasibility, there are tasks we did not include in dodecaDialogue. For example, there are additional knowledge tasks (Qin et al., 2019; Moghe et al., 2018; Gopalakrishnan et al., 2019) and image-based datasets (Das et al., 2017) one could use. There are also a large number of QA tasks we did not include, e.g. Rajpurkar et al. (2016); Choi et al. (2018). In general, our choices were made based on tasks that after training might produce an engaging dialogue agent that humans naturally would want to talk to – which means either natural datasets or crowdsourced datasets where crowdworkers were encouraged to engage one another. As computational resources and ambitions scale, it would be interesting to add more tasks as well, while retaining the twel"
2020.acl-main.222,D19-3025,0,0.0472735,"Missing"
2020.acl-main.222,I17-1047,0,0.11394,"is the largest dataset in the collection – much larger than the others. The subreddits cover a vast range of topics, and hence this is a strong candidate for helping improve performance on other tasks via pre-training and multi-tasking. Note this dataset does not overlap with ELI5. Image Chat Shuster et al. (2018) collected a crowdsourced dataset of human-human conversations about an image with a given personality, where the goal is to engage the other speaker. As such, it covers natural conversational responses, including displays of emotion and humor. Image Grounded Conversations (IGC) IGC (Mostafazadeh et al., 2017) similarly involves two speakers discussing an image, here focusing on questions and responses. It only includes a validation and test set, and so we converted most of the validation set to form a small training set. 2.1 Evaluation Metrics For all tasks, we use the following metrics: perplexity (PPL), BLEU, ROUGE-1,-2 and -L and F1, and also pick the metric most used in the literature as that subtask’s ‘Score’ to compare to existing work. Multi-tasking As we are interested in building a single conversational agent, we measure the ability of multi-tasked models that can perform all twelve tasks"
2020.acl-main.222,P19-1539,0,0.0390604,"rowess in multi-tasking ability (Radford et al., 2019; Keskar et al., 2019), and transformer-based approaches have been adapted to language and vision tasks as well (Lu et al., 2019; Tan and Bansal, 2019; Li et al., 2019a; Shuster et al., 2018). As well as citing the relevant papers’ results where possible in the experiments section, we also train a BERTbased (Devlin et al., 2019) generative model as an additional baseline. 3.2 Related Tasks and Collections In the interests of feasibility, there are tasks we did not include in dodecaDialogue. For example, there are additional knowledge tasks (Qin et al., 2019; Moghe et al., 2018; Gopalakrishnan et al., 2019) and image-based datasets (Das et al., 2017) one could use. There are also a large number of QA tasks we did not include, e.g. Rajpurkar et al. (2016); Choi et al. (2018). In general, our choices were made based on tasks that after training might produce an engaging dialogue agent that humans naturally would want to talk to – which means either natural datasets or crowdsourced datasets where crowdworkers were encouraged to engage one another. As computational resources and ambitions scale, it would be interesting to add more tasks as well, whil"
2020.acl-main.222,D16-1264,0,0.081504,"nsal, 2019; Li et al., 2019a; Shuster et al., 2018). As well as citing the relevant papers’ results where possible in the experiments section, we also train a BERTbased (Devlin et al., 2019) generative model as an additional baseline. 3.2 Related Tasks and Collections In the interests of feasibility, there are tasks we did not include in dodecaDialogue. For example, there are additional knowledge tasks (Qin et al., 2019; Moghe et al., 2018; Gopalakrishnan et al., 2019) and image-based datasets (Das et al., 2017) one could use. There are also a large number of QA tasks we did not include, e.g. Rajpurkar et al. (2016); Choi et al. (2018). In general, our choices were made based on tasks that after training might produce an engaging dialogue agent that humans naturally would want to talk to – which means either natural datasets or crowdsourced datasets where crowdworkers were encouraged to engage one another. As computational resources and ambitions scale, it would be interesting to add more tasks as well, while retaining the twelve we have chosen here in order to continue to evaluate their success, whilst extending the scope of the entire system. All the subtasks in the collection we use here already exist"
2020.acl-main.222,P19-1534,1,0.831197,"ipedia from which the dialogues were grounded during the human-human crowdsourced conversations. The topics were also crowdsourced and range from e-books to toga parties to showers. A model can thus learn to also perform similar retrieval and grounding at test time to potentially discuss any topic if it can generalize. We use the gold knowledge version of the task. We see this skill as a core component of an agent being able to not just chitchat, but actually engage a user in discussing real information about the world, e.g. by retrieving over documents from the internet. Empathetic Dialogues Rashkin et al. (2019) constructed a dataset of crowdworker conversations grounded in an emotional situation. In each dia2454 logue, one speaker describes a personal situation and the other plays a “listener” role, displaying empathy during the discussion. The dataset contains descriptions of the situations being discussed with an attached emotion label, but these are not used here. Trained models are measured playing the part of the empathetic listener, an important feature of an agent to which humans wish to speak. Cornell Movie Danescu-Niculescu-Mizil and Lee (2011) constructed a corpus containing a collection o"
2020.acl-main.222,N19-1170,1,0.819863,"volves long-form question answering grounded on multiple retrieved documents in order to answer common questions which people ask on the popular ELI5 subreddit. As such, the answers are in a conversational form applicable to a dialogue agent. Ubuntu Lowe et al. (2015) built a dataset that involves in-depth discussions in solving Ubuntu problems. This studies the ability of an agent on a very focused single topic, and is also a standard benchmark in the field. Twitter We use a variant of Twitter discussions (text-only), which have been used in many existing studies, e.g. Sordoni et al. (2015); See et al. (2019). This data naturally involves everyday discussions about topics that people care about. The public forum makes them different from the more personal discussions of some of the other tasks. This is the second largest dataset in the collection, and we thus measure in experiments its ability to help performance on other tasks. pushshift.io Reddit We use a variant of Reddit discussions (text-only), which has also been used in several existing studies, see e.g. Yang et al. (2018); Mazar´e et al. (2018); Keskar et al. (2019). Following Humeau et al. (2019), we use a previously existing Reddit datas"
2020.acl-main.222,N15-1020,0,0.0597815,"Missing"
2020.acl-main.222,P19-1485,0,0.0189597,"to engage one another. As computational resources and ambitions scale, it would be interesting to add more tasks as well, while retaining the twelve we have chosen here in order to continue to evaluate their success, whilst extending the scope of the entire system. All the subtasks in the collection we use here already exist. Other research projects have also built such collection-based tasks before as well. In particular, the NLP decathlon (McCann et al., 2018), from which the name of this paper is inspired, collects together a diverse set of NLP tasks – from sentiment detection to parsing. Talmor and Berant (2019) collect a set of 10 QA datasets and build M ULTI QA. Recently, (Raffel et al., 2019) also similarly multi-tasked a large set of NLP tasks, on an even bigger scale. Our work differs from these in that it is focused on dialogue tasks which naturally group together to form a conversational agent. 4 Models BERT baseline. We implement a generative baseline using BERT via adapting the model using a standard auto-regressive loss. We concatenate both the context and current generation and provide these as input to the model, using BERT’s sentence embeddings to distinguish the roles in the network. Al"
2020.acl-main.428,D15-1075,0,0.0399269,"anguage they produce. That is, they can produce logically or factually inaccurate, or contradicting statements (Welleck et al., 2019b; Zhang et al., 2018; Hayashi et al., 2019; Petroni et al., 2019). Here, we show how the unlikelihood objective can be used to train such models to assign low probability to inconsistent and contradictory utterances. To do so, we assume the existence of training data of both positive and negative examples of coherent behavior. There is a raft of recent largescale, high quality data that can be massaged into this form, from natural language inference (NLI) tasks (Bowman et al., 2015; Williams et al., 2018; Welleck et al., 2019b) to commonsense reasoning tasks (Zellers et al., 2019; Qin et al., 2019). Two collections of data can be derived from the labels of such a supervised task: D+ = {(x(i) , y(i)+ )}, D− = {(x(i) , y(i)− )}, where D+ is coherent behavior, e.g. neutral or entailing data in NLI, and D− is incoherent behavior, e.g. contradictions. In general, many forms of this type of data can be collected, not just NLI, and it is also not necessary for the contexts x(i) to overlap as we have written here. Standard likelihood training can then be performed on coherent d"
2020.acl-main.428,W18-2706,0,0.0192674,"logue models. Outside of that work, the use of negative training in dialogue retrieval, rather than generation, has been previously extensively studied, see e.g. (Humeau et al., 2019; Nugmanova 4717 et al., 2019). In the area of generative dialogue, a number of works have focused on improving the standard likelihood training approach. Closer to our work is that of He and Glass (2019) which developed the approach of negative training to prevent generic and malicious responses in dialogue models. In terms of improving repetition and specificity, a recent alternative approach is that of control (Fan et al., 2018; Ficler and Goldberg, 2017; Ghazvininejad et al., 2017; See et al., 2019). Nucleus sampling (Holtzman et al., 2019) can help to remove generic or repetitive utterances at the expense of accuracy, but was shown to be inferior to beam blocking, which in turn was shown to be inferior to unlikelihood in Welleck et al. (2019a). In terms of dialogue coherence, Welleck et al. (2019b) showed that retrieval, but not generative models, could be improved with NLI as a rescorer, while Yang et al. (2018) multi-tasked with NLI. The work of Gabriel et al. (2019) has also studied improving narrative flow wit"
2020.acl-main.428,P19-1346,1,0.925994,"ith minimal changes in perplexity and F1. Repetition Model PPL F1 Context Label Human MLE Baseline 8.3 .368 .160 .441 .001 .014 UL (Context only) UL (Label only) UL (Context + Label) 8.8 .346 8.3 .371 8.5 .358 .229 .426 .313 .037 .001 .009 Table 2: Evaluation on the Wizard of Wikipedia test set, comparing standard likelihood (MLE) with context and label repetition unlikelihood loss training. The repetition types can be decreased depending on the type of unlikelihood loss used, while minimally impacting F1. knowledge-grounded dialogue (Dinan et al., 2019) and ELI5 long-form question answering (Fan et al., 2019) datasets to evaluate the effect of using unlikelihood to reduce copying and repetition in model generated utterances. On each dataset, we fine-tune the pre-trained pushshift.io Reddit model, then evaluate by generating nextutterances for dialogue contexts from the test set (or validation in ConvAI2, as the test set is hidden). We use greedy decoding in our main experiments for simplicity and scalability, but we also obtained similar results with beam search, shown in Appendix A. To measure label repetition in a sequence y, we use the portion of duplicate n-grams: 1.0 − |unique n-grams(y)| , |"
2020.acl-main.428,W17-4912,0,0.0218234,"ide of that work, the use of negative training in dialogue retrieval, rather than generation, has been previously extensively studied, see e.g. (Humeau et al., 2019; Nugmanova 4717 et al., 2019). In the area of generative dialogue, a number of works have focused on improving the standard likelihood training approach. Closer to our work is that of He and Glass (2019) which developed the approach of negative training to prevent generic and malicious responses in dialogue models. In terms of improving repetition and specificity, a recent alternative approach is that of control (Fan et al., 2018; Ficler and Goldberg, 2017; Ghazvininejad et al., 2017; See et al., 2019). Nucleus sampling (Holtzman et al., 2019) can help to remove generic or repetitive utterances at the expense of accuracy, but was shown to be inferior to beam blocking, which in turn was shown to be inferior to unlikelihood in Welleck et al. (2019a). In terms of dialogue coherence, Welleck et al. (2019b) showed that retrieval, but not generative models, could be improved with NLI as a rescorer, while Yang et al. (2018) multi-tasked with NLI. The work of Gabriel et al. (2019) has also studied improving narrative flow with a discriminative rescorer"
2020.acl-main.428,P17-4008,0,0.0276722,"f negative training in dialogue retrieval, rather than generation, has been previously extensively studied, see e.g. (Humeau et al., 2019; Nugmanova 4717 et al., 2019). In the area of generative dialogue, a number of works have focused on improving the standard likelihood training approach. Closer to our work is that of He and Glass (2019) which developed the approach of negative training to prevent generic and malicious responses in dialogue models. In terms of improving repetition and specificity, a recent alternative approach is that of control (Fan et al., 2018; Ficler and Goldberg, 2017; Ghazvininejad et al., 2017; See et al., 2019). Nucleus sampling (Holtzman et al., 2019) can help to remove generic or repetitive utterances at the expense of accuracy, but was shown to be inferior to beam blocking, which in turn was shown to be inferior to unlikelihood in Welleck et al. (2019a). In terms of dialogue coherence, Welleck et al. (2019b) showed that retrieval, but not generative models, could be improved with NLI as a rescorer, while Yang et al. (2018) multi-tasked with NLI. The work of Gabriel et al. (2019) has also studied improving narrative flow with a discriminative rescorer, but in that case for gener"
2020.acl-main.428,D17-2014,1,0.852974,"rticular tasks with the objectives outlined in Section 2 and specified in each experiment below. Following previous work (Humeau et al., 2019), we pre-train our model on dialogue data, using a previously existing Reddit dataset extracted and obtained by a third party and made available on pushshift.io, training to generate a comment conditioned on the full thread leading up to the comment, spanning ∼ 2200M training examples. Our Transformer model consists of an 8 layer encoder, 8 layer decoder with 512-dimensional embeddings and 16 attention heads, and is based on the ParlAI implementation of Miller et al. (2017). The model was trained with a batch size of 3072 sequences for approximately 3M updates using a learning rate of 5e-4, and an inverse square root scheduler. This pre-training took approximately two weeks using 64 NVIDIA V100s. 4.1 Repetition and Copying We use the ConvAI2 persona-based dialogue (Zhang et al., 2018), Wizard of Wikipedia Repetition Model PPL F1 Context Label Human MLE Baseline 11.4 .199 .0223 .0004 .1131 .0210 UL (Context only) 11.8 .194 UL (Label only) 11.4 .203 UL (Context & Label) 11.9 .193 .0330 .0069 .0984 .0005 .0352 .0023 Table 1: Evaluation on the ConvAI2 task valid set"
2020.acl-main.428,D19-1250,0,0.0682843,"Missing"
2020.acl-main.428,D19-1509,0,0.0496421,"Missing"
2020.acl-main.428,N19-1170,1,0.854072,"gue retrieval, rather than generation, has been previously extensively studied, see e.g. (Humeau et al., 2019; Nugmanova 4717 et al., 2019). In the area of generative dialogue, a number of works have focused on improving the standard likelihood training approach. Closer to our work is that of He and Glass (2019) which developed the approach of negative training to prevent generic and malicious responses in dialogue models. In terms of improving repetition and specificity, a recent alternative approach is that of control (Fan et al., 2018; Ficler and Goldberg, 2017; Ghazvininejad et al., 2017; See et al., 2019). Nucleus sampling (Holtzman et al., 2019) can help to remove generic or repetitive utterances at the expense of accuracy, but was shown to be inferior to beam blocking, which in turn was shown to be inferior to unlikelihood in Welleck et al. (2019a). In terms of dialogue coherence, Welleck et al. (2019b) showed that retrieval, but not generative models, could be improved with NLI as a rescorer, while Yang et al. (2018) multi-tasked with NLI. The work of Gabriel et al. (2019) has also studied improving narrative flow with a discriminative rescorer, but in that case for generated language. In o"
2020.acl-main.428,P19-1363,1,0.689817,"Facebook AI Research 2 New York University {margaretli,roller,ylan,kyunghyuncho,jase}@fb.com wellecks@nyu.edu,kulikov@cs.nyu.edu Abstract Generative dialogue models currently suffer from a number of problems which standard maximum likelihood training does not address. They tend to produce generations that (i) rely too much on copying from the context, (ii) contain repetitions within utterances, (iii) overuse frequent words, and (iv) at a deeper level, contain logical flaws. In this work we show how all of these problems can be addressed by extending the recently introduced unlikelihood loss (Welleck et al., 2019a) to these cases. We show that appropriate loss functions which regularize generated outputs to match human distributions are effective for the first three issues. For the last important general issue, we show applying unlikelihood to collected data of what a model should not do is effective for improving logical consistency, potentially paving the way to generative models with greater reasoning ability. We demonstrate the efficacy of our approach across several dialogue tasks. 1 Figure 1: GPT-2 345M model completions can show lack of coherence, e.g. direct contradictions. Introduction Open-e"
2020.acl-main.428,N18-1101,0,0.0267128,"That is, they can produce logically or factually inaccurate, or contradicting statements (Welleck et al., 2019b; Zhang et al., 2018; Hayashi et al., 2019; Petroni et al., 2019). Here, we show how the unlikelihood objective can be used to train such models to assign low probability to inconsistent and contradictory utterances. To do so, we assume the existence of training data of both positive and negative examples of coherent behavior. There is a raft of recent largescale, high quality data that can be massaged into this form, from natural language inference (NLI) tasks (Bowman et al., 2015; Williams et al., 2018; Welleck et al., 2019b) to commonsense reasoning tasks (Zellers et al., 2019; Qin et al., 2019). Two collections of data can be derived from the labels of such a supervised task: D+ = {(x(i) , y(i)+ )}, D− = {(x(i) , y(i)− )}, where D+ is coherent behavior, e.g. neutral or entailing data in NLI, and D− is incoherent behavior, e.g. contradictions. In general, many forms of this type of data can be collected, not just NLI, and it is also not necessary for the contexts x(i) to overlap as we have written here. Standard likelihood training can then be performed on coherent data D+ , while the unli"
2020.acl-main.428,W18-3022,0,0.0230585,"els. In terms of improving repetition and specificity, a recent alternative approach is that of control (Fan et al., 2018; Ficler and Goldberg, 2017; Ghazvininejad et al., 2017; See et al., 2019). Nucleus sampling (Holtzman et al., 2019) can help to remove generic or repetitive utterances at the expense of accuracy, but was shown to be inferior to beam blocking, which in turn was shown to be inferior to unlikelihood in Welleck et al. (2019a). In terms of dialogue coherence, Welleck et al. (2019b) showed that retrieval, but not generative models, could be improved with NLI as a rescorer, while Yang et al. (2018) multi-tasked with NLI. The work of Gabriel et al. (2019) has also studied improving narrative flow with a discriminative rescorer, but in that case for generated language. In our work, the improvements are tightly integrated into the training of the model itself. 4 Experiments In all of our experiments we employ a large pre-trained seq2seq Transformer (Vaswani et al., 2017) as our base model, which we then fine-tune for particular tasks with the objectives outlined in Section 2 and specified in each experiment below. Following previous work (Humeau et al., 2019), we pre-train our model on dia"
2020.acl-main.428,P19-1472,0,0.026468,"statements (Welleck et al., 2019b; Zhang et al., 2018; Hayashi et al., 2019; Petroni et al., 2019). Here, we show how the unlikelihood objective can be used to train such models to assign low probability to inconsistent and contradictory utterances. To do so, we assume the existence of training data of both positive and negative examples of coherent behavior. There is a raft of recent largescale, high quality data that can be massaged into this form, from natural language inference (NLI) tasks (Bowman et al., 2015; Williams et al., 2018; Welleck et al., 2019b) to commonsense reasoning tasks (Zellers et al., 2019; Qin et al., 2019). Two collections of data can be derived from the labels of such a supervised task: D+ = {(x(i) , y(i)+ )}, D− = {(x(i) , y(i)− )}, where D+ is coherent behavior, e.g. neutral or entailing data in NLI, and D− is incoherent behavior, e.g. contradictions. In general, many forms of this type of data can be collected, not just NLI, and it is also not necessary for the contexts x(i) to overlap as we have written here. Standard likelihood training can then be performed on coherent data D+ , while the unlikelihood objective is applied to D− as we wish to push down the probability o"
2020.acl-main.428,P18-1205,1,0.914215,"loss, each step’s candidate is thus the current token, Ctidentity = {yt }, and each token’s unlikelihood loss is scaled according to the mismatch between the approximated model and human distributions,  β(yc ) = pmodel(yc ) log pmodel (yc ) p∗ (yc )  . (i) identity LUL (pθ , C1:|y| , x(i) , y) where y is a model-generated sequence. Neural generation models appear fluent, especially when pre-trained on large datasets, but are still poor at understanding the language they produce. That is, they can produce logically or factually inaccurate, or contradicting statements (Welleck et al., 2019b; Zhang et al., 2018; Hayashi et al., 2019; Petroni et al., 2019). Here, we show how the unlikelihood objective can be used to train such models to assign low probability to inconsistent and contradictory utterances. To do so, we assume the existence of training data of both positive and negative examples of coherent behavior. There is a raft of recent largescale, high quality data that can be massaged into this form, from natural language inference (NLI) tasks (Bowman et al., 2015; Williams et al., 2018; Welleck et al., 2019b) to commonsense reasoning tasks (Zellers et al., 2019; Qin et al., 2019). Two collectio"
2020.acl-main.441,D15-1075,0,0.487871,"chmarks, while posing a more difficult challenge with its new test set. Our analysis sheds light on the shortcomings of current state-of-theart models, and shows that non-expert annotators are successful at finding their weaknesses. The data collection method can be applied in a never-ending learning scenario, becoming a moving target for NLU, rather than a static benchmark that will quickly saturate. 1 Introduction Progress in AI has been driven by, among other things, the development of challenging large-scale benchmarks like ImageNet (Russakovsky et al., 2015) in computer vision, and SNLI (Bowman et al., 2015), SQuAD (Rajpurkar et al., 2016), and others in natural language processing (NLP). Recently, for natural language understanding (NLU) in particular, the focus has shifted to combined benchmarks like SentEval (Conneau and Kiela, 2018) and GLUE (Wang et al., 2018), which track model performance on multiple tasks and provide a unified platform for analysis. With the rapid pace of advancement in AI, however, NLU benchmarks struggle to keep up with model improvement. Whereas it took around 15 years to achieve “near-human performance” on MNIST (LeCun et al., 1998; Cires¸an et al., 2012; Wan et al.,"
2020.acl-main.441,P17-1171,1,0.856888,"Missing"
2020.acl-main.441,W19-2008,0,0.0318535,"Missing"
2020.acl-main.441,L18-1269,1,0.839755,"es. The data collection method can be applied in a never-ending learning scenario, becoming a moving target for NLU, rather than a static benchmark that will quickly saturate. 1 Introduction Progress in AI has been driven by, among other things, the development of challenging large-scale benchmarks like ImageNet (Russakovsky et al., 2015) in computer vision, and SNLI (Bowman et al., 2015), SQuAD (Rajpurkar et al., 2016), and others in natural language processing (NLP). Recently, for natural language understanding (NLU) in particular, the focus has shifted to combined benchmarks like SentEval (Conneau and Kiela, 2018) and GLUE (Wang et al., 2018), which track model performance on multiple tasks and provide a unified platform for analysis. With the rapid pace of advancement in AI, however, NLU benchmarks struggle to keep up with model improvement. Whereas it took around 15 years to achieve “near-human performance” on MNIST (LeCun et al., 1998; Cires¸an et al., 2012; Wan et al., 2013) and approximately 7 years to surpass humans on ImageNet (Deng et al., 2009; Russakovsky et al., 2015; He et al., 2016), the GLUE benchmark did not last as long as we would have hoped after the advent of BERT (Devlin et al., 201"
2020.acl-main.441,D17-1070,1,0.886955,"Missing"
2020.acl-main.441,D19-1461,1,0.933183,"fy examples and make splits Verifier Agree Step 4: Retrain model for next round Figure 1: Adversarial NLI data collection via human-and-model-in-the-loop enabled training (HAMLET). The four steps make up one round of data collection. In step 3, model-correct examples are included in the training set; development and test sets are constructed solely from model-wrong verified-correct examples. forts that gamify collaborative training of machine learning agents over multiple rounds (Yang et al., 2017) and pit “builders” against “breakers” to learn better models (Ettinger et al., 2017). Recently, Dinan et al. (2019) showed that such an approach can be used to make dialogue safety classifiers more robust. Here, we focus on natural language inference (NLI), arguably the most canonical task in NLU. We collected three rounds of data, and call our new dataset Adversarial NLI (ANLI). Our contributions are as follows: 1) We introduce a novel human-and-model-in-the-loop dataset, consisting of three rounds that progressively increase in difficulty and complexity, that includes annotator-provided explanations. 2) We show that training models on this new dataset leads to state-of-the-art performance on a variety of"
2020.acl-main.441,W17-5401,0,0.115146,"Missing"
2020.acl-main.441,N18-1074,0,0.0577718,"Missing"
2020.emnlp-main.23,W19-3804,0,0.0370541,"nally (iv) we illustrate our classifiers’ utility for several downstream applications. All datasets, annotations, and classifiers will be released publicly to facilitate further research into the important problem of gender bias in text. 2 Related Work Gender affects myriad aspects of NLP, including corpora, tasks, algorithms, and systems (Chang et al., 2019; Costa-juss`a, 2019; Sun et al., 2019). For example, statistical gender biases are rampant in word embeddings (Jurgens et al., 2012; Bolukbasi et al., 2016; Caliskan et al., 2017; Garg et al., 2018; Zhao et al., 2018b; Basta et al., 2019; Chaloner and Maldonado, 2019; Du et al., 2019; For dialogue, gender biases in training corpora have been found to be amplified in machine learning models (Lee et al., 2019; Dinan et al., 2020; Liu et al., 2019). While many of the works cited above proposed methods of mitigating the unwanted effects of gender on text, Hall Maudslay et al. (2019), Liu et al. (2019), Zmigrod et al. (2019), and Dinan et al. (2020) in particular relied on counterfactual data to alter the training distribution to offset gender-based statistical imbalances (see §4.2 for more discussion of training set imbalances). Also relevant is Kang et al. ("
2020.emnlp-main.23,D19-2004,0,0.0314687,"aset for performing gender identification that contains utterances re-written from the perspective of a specific gender along all three dimensions, (iii) we build a suite of classifiers capable of labeling gender in both a single and multitask set up, and finally (iv) we illustrate our classifiers’ utility for several downstream applications. All datasets, annotations, and classifiers will be released publicly to facilitate further research into the important problem of gender bias in text. 2 Related Work Gender affects myriad aspects of NLP, including corpora, tasks, algorithms, and systems (Chang et al., 2019; Costa-juss`a, 2019; Sun et al., 2019). For example, statistical gender biases are rampant in word embeddings (Jurgens et al., 2012; Bolukbasi et al., 2016; Caliskan et al., 2017; Garg et al., 2018; Zhao et al., 2018b; Basta et al., 2019; Chaloner and Maldonado, 2019; Du et al., 2019; For dialogue, gender biases in training corpora have been found to be amplified in machine learning models (Lee et al., 2019; Dinan et al., 2020; Liu et al., 2019). While many of the works cited above proposed methods of mitigating the unwanted effects of gender on text, Hall Maudslay et al. (2019), Liu et al. ("
2020.emnlp-main.23,W19-3823,0,0.0318843,"structure. Decomposing gender into separate dimensions also allows for better identification of gender bias, which subsequently enables us to train a suite of classifiers for detecting different kinds of gender 314 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 314–331, c November 16–20, 2020. 2020 Association for Computational Linguistics M F N akin vain descriptive bench sicilian feminist lesbian uneven transgender feminine optional tropical volcanic glacial abundant Ethayarajh et al., 2019; Gonen and Goldberg, 2019; Kaneko and Bollegala, 2019; Kurita et al., 2019; Zhao et al., 2019; Wang et al., 2020)—including multilingual ones (Escud´e Font and Costa-juss`a, 2019; Gonen et al., 2019; Zhou et al., 2019)—and affect a wide range of downstream tasks including coreference resolution (Zhao et al., 2018a; Cao and Daum´e III, 2020; Emami et al., 2019), part-ofspeech and dependency parsing (Garimella et al., 2019), language modeling (Qian et al., 2019; Nangia et al., 2020), appropriate turn-taking classification (Lepp, 2019), relation extraction (Gaut et al., 2020), identification of offensive content (Sharifirad and Matwin, 2019; Sharifirad et al., 2019), a"
2020.emnlp-main.23,N19-1063,0,0.0906205,"oles (i.e., topics, addressees, and creators of text). 315 Finally, when investigating gender biases, one cannot ignore the intersectionality of gender identities, i.e., when gender non-additively interacts with other identity characteristics. Negative gender stereotyping is known to be alternatively weakened or reinforced by the presence of social attributes like dialect (Tatman, 2017), class (DegaetanoOrtlieb, 2018) and race (Davis, 1981; Crenshaw, 1989). These differences have been found to affect gender classification in images (Buolamwini and Gebru, 2018), and also in sentences encoders (May et al., 2019). We acknowledge that these are crucial considerations, and intend to incorporate them in future work. For a thorough survey and a critical discussion of best practices for researching social “biases” in NLP, including and beyond gender, see Blodgett et al. (2020). 3 Dimensions of Gender Bias Gender permeates language differently depending on the conversational role played by the people using that language (see Figure 1). We decompose gender bias along multiple dimensions: bias when speaking ABOUT someone, bias when speaking TO someone, and bias from speaking AS someone. This framework enables"
2020.emnlp-main.23,D17-2014,1,0.940799,"ose gender bias over sentences into semantic and/or pragmatic dimensions (about/to/as), including gender information that (i) falls outside the malefemale binary, (ii) can be contextually determined, and (iii) is statistically as opposed to explicitly gendered. In the subsequent sections, we provide details regarding the annotation of data, and details for training these classifiers. 4.1 sion of inferrable information about one or more of our dimensions, diversity in textual domain, and high quality, open-source data. The datasets are: Wikipedia, Funpedia (a less formal version of Wikipedia) (Miller et al., 2017), Wizard of Wikipedia (knowledge-based conversation) (Dinan et al., 2019c), Yelp Reviews2 , ConvAI2 (chit-chat dialogue) (Dinan et al., 2019b), ImageChat (chit-chat dialogue about an image) (Shuster et al., 2018), OpenSubtitles (dialogue from movies) (Lison and Tiedemann, 2016), and LIGHT (chit-chat fantasy dialogue) (Urbanek et al., 2019). Table 2 presents dataset statistics. Some of the datasets contain gender annotations provided by existing work. For example, classifiers trained for style transfer algorithms have previously annotated the gender of Yelp reviewers (Subramanian et al., 2018)."
2020.emnlp-main.23,L16-1147,0,0.0399975,"sequent sections, we provide details regarding the annotation of data, and details for training these classifiers. 4.1 sion of inferrable information about one or more of our dimensions, diversity in textual domain, and high quality, open-source data. The datasets are: Wikipedia, Funpedia (a less formal version of Wikipedia) (Miller et al., 2017), Wizard of Wikipedia (knowledge-based conversation) (Dinan et al., 2019c), Yelp Reviews2 , ConvAI2 (chit-chat dialogue) (Dinan et al., 2019b), ImageChat (chit-chat dialogue about an image) (Shuster et al., 2018), OpenSubtitles (dialogue from movies) (Lison and Tiedemann, 2016), and LIGHT (chit-chat fantasy dialogue) (Urbanek et al., 2019). Table 2 presents dataset statistics. Some of the datasets contain gender annotations provided by existing work. For example, classifiers trained for style transfer algorithms have previously annotated the gender of Yelp reviewers (Subramanian et al., 2018). In other datasets, we infer the gender labels. For example, in datasets where users are first assigned a persona to represent before chatting, often the gender of the persona is predetermined. In some cases gender annotations are not provided. In these cases, we sometimes impu"
2020.emnlp-main.23,W17-1609,0,0.091099,"Missing"
2020.emnlp-main.23,2020.acl-main.486,0,0.105491,"9; Dinan et al., 2020; Liu et al., 2019). While many of the works cited above proposed methods of mitigating the unwanted effects of gender on text, Hall Maudslay et al. (2019), Liu et al. (2019), Zmigrod et al. (2019), and Dinan et al. (2020) in particular relied on counterfactual data to alter the training distribution to offset gender-based statistical imbalances (see §4.2 for more discussion of training set imbalances). Also relevant is Kang et al. (2019, PASTEL), which introduced a parallel style corpus and showed gains on style-transfer across binary genders. Most relevant to this work, Sap et al. (2020) proposed a framework for modeling pragmatic aspects of many social biases in text. Our work and theirs focus on complementary aspects of a larger goal—namely, making NLP safe and inclusive for everyone—but the two approaches differ in several ways. We treat statistical gender bias in human or model generated text specifically, and in detail. Sap et al. (2020) proposed a different but compatible perspective, and aimed to situate gender bias within the broader landscape of negative stereotypes in social media text, an approach that can make parallels apparent across different kinds of harmful t"
2020.emnlp-main.23,N19-1170,1,0.820569,"The gender classifiers along the TO , AS and ABOUT dimensions are trained on a variety of different existing datasets across multiple domains. We analyze which datasets are the most difficult to classify correctly 319 6.1 Controllable Generation By learning to associate control variables with textual properties, generative models can be controlled at inference time to adjust the generated text based on the desired properties of the user. This has been applied to a variety of different cases, including generating text of different lengths (Fan et al., 2018a), generating questions in chit-chat (See et al., 2019), and reducing bias (Dinan et al., 2020). Previous work in gender bias used word lists to control bias, but found that word lists were limGeneration Statistics Control Token # Gend. words Pct. masc. TO:feminine AS:feminine ABOUT:feminine Word list, feminine 246 227 1151 1158 48.0 51.0 19.72 18.22 TO:masculine AS:masculine ABOUT:masculine Word list, masculine 372 402 800 1459 75.0 71.6 91.62 94.8 Table 6: Word statistics measured on text generated from 1000 different seed utterances from ConvAI2 for each control token. We measure the number of gendered words (from a word list) that appear in th"
2020.emnlp-main.23,D19-1062,1,0.8993,"Missing"
2020.emnlp-main.23,W19-3609,0,0.0461279,", 2019; Kurita et al., 2019; Zhao et al., 2019; Wang et al., 2020)—including multilingual ones (Escud´e Font and Costa-juss`a, 2019; Gonen et al., 2019; Zhou et al., 2019)—and affect a wide range of downstream tasks including coreference resolution (Zhao et al., 2018a; Cao and Daum´e III, 2020; Emami et al., 2019), part-ofspeech and dependency parsing (Garimella et al., 2019), language modeling (Qian et al., 2019; Nangia et al., 2020), appropriate turn-taking classification (Lepp, 2019), relation extraction (Gaut et al., 2020), identification of offensive content (Sharifirad and Matwin, 2019; Sharifirad et al., 2019), and machine translation (Stanovsky et al., 2019; Hovy et al., 2020). Table 1: Bias in Wikipedia. We compare the most over-represented adjectives in Wikipedia biographies of men and women to those in gender-neutral pages. We use a part-of-speech tagger (Honnibal and Montani, 2017), and computed P (word |gender)/P (word) for words that appear more than 500 times. bias in text. We train several classifiers on publicly available data that we annotate with gender information along our dimensions. We also collect a new crowdsourced dataset (MDG ENDER) for better fine-grained evaluation of gender c"
2020.emnlp-main.23,W19-3616,0,0.0922372,"Missing"
2020.emnlp-main.23,P19-1164,0,0.0489935,"g et al., 2020)—including multilingual ones (Escud´e Font and Costa-juss`a, 2019; Gonen et al., 2019; Zhou et al., 2019)—and affect a wide range of downstream tasks including coreference resolution (Zhao et al., 2018a; Cao and Daum´e III, 2020; Emami et al., 2019), part-ofspeech and dependency parsing (Garimella et al., 2019), language modeling (Qian et al., 2019; Nangia et al., 2020), appropriate turn-taking classification (Lepp, 2019), relation extraction (Gaut et al., 2020), identification of offensive content (Sharifirad and Matwin, 2019; Sharifirad et al., 2019), and machine translation (Stanovsky et al., 2019; Hovy et al., 2020). Table 1: Bias in Wikipedia. We compare the most over-represented adjectives in Wikipedia biographies of men and women to those in gender-neutral pages. We use a part-of-speech tagger (Honnibal and Montani, 2017), and computed P (word |gender)/P (word) for words that appear more than 500 times. bias in text. We train several classifiers on publicly available data that we annotate with gender information along our dimensions. We also collect a new crowdsourced dataset (MDG ENDER) for better fine-grained evaluation of gender classifier performance. The classifiers we train h"
2020.emnlp-main.23,2020.acl-main.484,0,0.0188642,"ate dimensions also allows for better identification of gender bias, which subsequently enables us to train a suite of classifiers for detecting different kinds of gender 314 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 314–331, c November 16–20, 2020. 2020 Association for Computational Linguistics M F N akin vain descriptive bench sicilian feminist lesbian uneven transgender feminine optional tropical volcanic glacial abundant Ethayarajh et al., 2019; Gonen and Goldberg, 2019; Kaneko and Bollegala, 2019; Kurita et al., 2019; Zhao et al., 2019; Wang et al., 2020)—including multilingual ones (Escud´e Font and Costa-juss`a, 2019; Gonen et al., 2019; Zhou et al., 2019)—and affect a wide range of downstream tasks including coreference resolution (Zhao et al., 2018a; Cao and Daum´e III, 2020; Emami et al., 2019), part-ofspeech and dependency parsing (Garimella et al., 2019), language modeling (Qian et al., 2019; Nangia et al., 2020), appropriate turn-taking classification (Lepp, 2019), relation extraction (Gaut et al., 2020), identification of offensive content (Sharifirad and Matwin, 2019; Sharifirad et al., 2019), and machine translation (Stanovsky et al"
2020.emnlp-main.23,P19-1159,0,0.018543,"n that contains utterances re-written from the perspective of a specific gender along all three dimensions, (iii) we build a suite of classifiers capable of labeling gender in both a single and multitask set up, and finally (iv) we illustrate our classifiers’ utility for several downstream applications. All datasets, annotations, and classifiers will be released publicly to facilitate further research into the important problem of gender bias in text. 2 Related Work Gender affects myriad aspects of NLP, including corpora, tasks, algorithms, and systems (Chang et al., 2019; Costa-juss`a, 2019; Sun et al., 2019). For example, statistical gender biases are rampant in word embeddings (Jurgens et al., 2012; Bolukbasi et al., 2016; Caliskan et al., 2017; Garg et al., 2018; Zhao et al., 2018b; Basta et al., 2019; Chaloner and Maldonado, 2019; Du et al., 2019; For dialogue, gender biases in training corpora have been found to be amplified in machine learning models (Lee et al., 2019; Dinan et al., 2020; Liu et al., 2019). While many of the works cited above proposed methods of mitigating the unwanted effects of gender on text, Hall Maudslay et al. (2019), Liu et al. (2019), Zmigrod et al. (2019), and Dinan"
2020.emnlp-main.23,W17-1606,0,0.0253012,"rent pragmatic dimensions than we do: they targeted negatively stereotyped commonsense implications in arguably innocuous statements, whereas we investigate pragmatic dimensions that straightforwardly map to conversational roles (i.e., topics, addressees, and creators of text). 315 Finally, when investigating gender biases, one cannot ignore the intersectionality of gender identities, i.e., when gender non-additively interacts with other identity characteristics. Negative gender stereotyping is known to be alternatively weakened or reinforced by the presence of social attributes like dialect (Tatman, 2017), class (DegaetanoOrtlieb, 2018) and race (Davis, 1981; Crenshaw, 1989). These differences have been found to affect gender classification in images (Buolamwini and Gebru, 2018), and also in sentences encoders (May et al., 2019). We acknowledge that these are crucial considerations, and intend to incorporate them in future work. For a thorough survey and a critical discussion of best practices for researching social “biases” in NLP, including and beyond gender, see Blodgett et al. (2020). 3 Dimensions of Gender Bias Gender permeates language differently depending on the conversational role pla"
2020.emnlp-main.23,N19-1064,0,0.0745267,"Missing"
2020.emnlp-main.23,N18-2003,0,0.449802,"ference on Empirical Methods in Natural Language Processing, pages 314–331, c November 16–20, 2020. 2020 Association for Computational Linguistics M F N akin vain descriptive bench sicilian feminist lesbian uneven transgender feminine optional tropical volcanic glacial abundant Ethayarajh et al., 2019; Gonen and Goldberg, 2019; Kaneko and Bollegala, 2019; Kurita et al., 2019; Zhao et al., 2019; Wang et al., 2020)—including multilingual ones (Escud´e Font and Costa-juss`a, 2019; Gonen et al., 2019; Zhou et al., 2019)—and affect a wide range of downstream tasks including coreference resolution (Zhao et al., 2018a; Cao and Daum´e III, 2020; Emami et al., 2019), part-ofspeech and dependency parsing (Garimella et al., 2019), language modeling (Qian et al., 2019; Nangia et al., 2020), appropriate turn-taking classification (Lepp, 2019), relation extraction (Gaut et al., 2020), identification of offensive content (Sharifirad and Matwin, 2019; Sharifirad et al., 2019), and machine translation (Stanovsky et al., 2019; Hovy et al., 2020). Table 1: Bias in Wikipedia. We compare the most over-represented adjectives in Wikipedia biographies of men and women to those in gender-neutral pages. We use a part-of-spe"
2020.emnlp-main.23,D18-1521,0,0.466172,"ference on Empirical Methods in Natural Language Processing, pages 314–331, c November 16–20, 2020. 2020 Association for Computational Linguistics M F N akin vain descriptive bench sicilian feminist lesbian uneven transgender feminine optional tropical volcanic glacial abundant Ethayarajh et al., 2019; Gonen and Goldberg, 2019; Kaneko and Bollegala, 2019; Kurita et al., 2019; Zhao et al., 2019; Wang et al., 2020)—including multilingual ones (Escud´e Font and Costa-juss`a, 2019; Gonen et al., 2019; Zhou et al., 2019)—and affect a wide range of downstream tasks including coreference resolution (Zhao et al., 2018a; Cao and Daum´e III, 2020; Emami et al., 2019), part-ofspeech and dependency parsing (Garimella et al., 2019), language modeling (Qian et al., 2019; Nangia et al., 2020), appropriate turn-taking classification (Lepp, 2019), relation extraction (Gaut et al., 2020), identification of offensive content (Sharifirad and Matwin, 2019; Sharifirad et al., 2019), and machine translation (Stanovsky et al., 2019; Hovy et al., 2020). Table 1: Bias in Wikipedia. We compare the most over-represented adjectives in Wikipedia biographies of men and women to those in gender-neutral pages. We use a part-of-spe"
2020.emnlp-main.23,D19-1531,0,0.0211547,"in a suite of classifiers for detecting different kinds of gender 314 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 314–331, c November 16–20, 2020. 2020 Association for Computational Linguistics M F N akin vain descriptive bench sicilian feminist lesbian uneven transgender feminine optional tropical volcanic glacial abundant Ethayarajh et al., 2019; Gonen and Goldberg, 2019; Kaneko and Bollegala, 2019; Kurita et al., 2019; Zhao et al., 2019; Wang et al., 2020)—including multilingual ones (Escud´e Font and Costa-juss`a, 2019; Gonen et al., 2019; Zhou et al., 2019)—and affect a wide range of downstream tasks including coreference resolution (Zhao et al., 2018a; Cao and Daum´e III, 2020; Emami et al., 2019), part-ofspeech and dependency parsing (Garimella et al., 2019), language modeling (Qian et al., 2019; Nangia et al., 2020), appropriate turn-taking classification (Lepp, 2019), relation extraction (Gaut et al., 2020), identification of offensive content (Sharifirad and Matwin, 2019; Sharifirad et al., 2019), and machine translation (Stanovsky et al., 2019; Hovy et al., 2020). Table 1: Bias in Wikipedia. We compare the most over-represented adjectives"
2020.emnlp-main.23,P19-1161,0,0.0367524,"-juss`a, 2019; Sun et al., 2019). For example, statistical gender biases are rampant in word embeddings (Jurgens et al., 2012; Bolukbasi et al., 2016; Caliskan et al., 2017; Garg et al., 2018; Zhao et al., 2018b; Basta et al., 2019; Chaloner and Maldonado, 2019; Du et al., 2019; For dialogue, gender biases in training corpora have been found to be amplified in machine learning models (Lee et al., 2019; Dinan et al., 2020; Liu et al., 2019). While many of the works cited above proposed methods of mitigating the unwanted effects of gender on text, Hall Maudslay et al. (2019), Liu et al. (2019), Zmigrod et al. (2019), and Dinan et al. (2020) in particular relied on counterfactual data to alter the training distribution to offset gender-based statistical imbalances (see §4.2 for more discussion of training set imbalances). Also relevant is Kang et al. (2019, PASTEL), which introduced a parallel style corpus and showed gains on style-transfer across binary genders. Most relevant to this work, Sap et al. (2020) proposed a framework for modeling pragmatic aspects of many social biases in text. Our work and theirs focus on complementary aspects of a larger goal—namely, making NLP safe and inclusive for everyon"
2020.emnlp-main.23,E99-1021,0,\N,Missing
2020.emnlp-main.23,S12-1047,0,\N,Missing
2020.emnlp-main.23,P10-2008,0,\N,Missing
2020.emnlp-main.23,C08-1065,0,\N,Missing
2020.emnlp-main.23,W11-0310,0,\N,Missing
2020.emnlp-main.23,Q14-1029,0,\N,Missing
2020.emnlp-main.23,P16-2096,0,\N,Missing
2020.emnlp-main.23,W17-1602,0,\N,Missing
2020.emnlp-main.23,E17-1107,0,\N,Missing
2020.emnlp-main.23,P18-1082,1,\N,Missing
2020.emnlp-main.23,P19-1160,0,\N,Missing
2020.emnlp-main.23,P19-1163,0,\N,Missing
2020.emnlp-main.23,P19-2007,0,\N,Missing
2020.emnlp-main.23,P19-2031,0,\N,Missing
2020.emnlp-main.23,N19-1423,0,\N,Missing
2020.emnlp-main.23,D19-1461,1,\N,Missing
2020.emnlp-main.23,D19-1635,0,\N,Missing
2020.emnlp-main.23,D19-1179,0,\N,Missing
2020.emnlp-main.23,D19-1530,0,\N,Missing
2020.emnlp-main.23,W19-3622,0,\N,Missing
2020.emnlp-main.23,K19-1043,0,\N,Missing
2020.emnlp-main.23,2020.acl-main.647,0,\N,Missing
2020.emnlp-main.656,D19-2004,0,0.0588153,"Missing"
2021.acl-long.134,D15-1075,0,0.308314,"stand what they are saying at all (Marcus, 2018). From a listener’s perspective, such inconsistent bots fail to gain user trust and their long-term communication confidence. From a speaker’s perspective, it violates the maxim of quality in Grice’s cooperative principles (Grice, 1975) —”Do not say what you believe to be false.” Hence, efforts on reducing contradicting or inconsistent conversations by open-domain chatbots are imperative. Prior works (Welleck et al., 2019) characterized the modeling of persona-related consistency as a natural language inference (NLI) problem (Dagan et al., 2005; Bowman et al., 2015), and constructed a dialog NLI dataset based on Persona-Chat (Zhang et al., 2018), but so far state-of-the-art chatbots (Roller et al., 2020) have not been able to make use of NLI techniques in improving dialogue consistency. Overall, the challenge remains that we are still unable to answer the simple yet important question—“how good are we at modeling consistency (including persona, logic, causality, etc.) in a general conversation?”. The inability to measure this obscures to what degree building new modules or techniques can in turn help prevent contradicting responses during generation. See"
2021.acl-long.134,N19-1423,0,0.164167,"se aimed to cover the dialogue domain; (ii) Transformer models that explicitly hinge on utterance structures for dialogue contradiction detection are more robust and generalize well on both analysis and outof-distribution dialogues than standard (unstructured) Transformers. We also show that our best contradiction detection model correlates well with human judgments and further provide evidence for its usage in both automatically evaluating and improving the consistency of state-of-the-art generative chatbots. 1 Introduction Recent progress on neural approaches to natural language processing (Devlin et al., 2019; Brown et al., 2020), and the availability of large amounts of conversational data (Lowe et al., 2015; Smith et al., 2020) have triggered a resurgent interest on building intelligent open-domain chatbots. Newly developed end-to-end neural bots (Zhang et al., 2020; Adiwardana et al., 2020; Roller et al., 2020) are claimed to be superior to their predecessors (Worsnick, 2018; Zhou et al., 2020) using various human evaluation techniques (See et al., 2019; Li et al., 2019; Adiwardana et al., 2020) that aim to give a more accurate measure of what makes a good conversation. While the success is ind"
2021.acl-long.134,W19-4103,0,0.0269653,"roving the consistency of state-of-the-art generative chatbots. 2 Related Work Several prior works on improving dialogue consistency have explored using direct modeling of the dialogue context in generation algorithms. The modeling can be implicit where the dialogue consistency-related information like style (Wang et al., 2017), topics, or personal facts are maintained in distributed embeddings (Li et al., 2016; Zhang et al., 2019a), neural long-term memories (Bang et al., 2015), hierarchical neural architecture (Serban et al., 2016), latent variables (Serban et al., 2017), topical attention (Dziri et al., 2019a), or even self-learned feature vectors (Zhang et al., 2019b). Some works have grounded generation models on explicit user input (Qian et al., 2018), or designated personas (Zhang et al., 2018). Although, improvements on automatic generation metrics were often shown on guided response generation based on the consistency modeling, the issue of contradiction has never been resolved, nor have generally applicable methods to gauge the consistency improvements been developed. Further, simply scaling models has not made the problem go away, as is evident in the largest chatbots trained such as Blen"
2021.acl-long.134,W19-3646,0,0.160375,"roving the consistency of state-of-the-art generative chatbots. 2 Related Work Several prior works on improving dialogue consistency have explored using direct modeling of the dialogue context in generation algorithms. The modeling can be implicit where the dialogue consistency-related information like style (Wang et al., 2017), topics, or personal facts are maintained in distributed embeddings (Li et al., 2016; Zhang et al., 2019a), neural long-term memories (Bang et al., 2015), hierarchical neural architecture (Serban et al., 2016), latent variables (Serban et al., 2017), topical attention (Dziri et al., 2019a), or even self-learned feature vectors (Zhang et al., 2019b). Some works have grounded generation models on explicit user input (Qian et al., 2018), or designated personas (Zhang et al., 2018). Although, improvements on automatic generation metrics were often shown on guided response generation based on the consistency modeling, the issue of contradiction has never been resolved, nor have generally applicable methods to gauge the consistency improvements been developed. Further, simply scaling models has not made the problem go away, as is evident in the largest chatbots trained such as Blen"
2021.acl-long.134,P18-1082,0,0.0525208,"Missing"
2021.acl-long.134,2020.acl-main.428,1,0.895103,"iction has never been resolved, nor have generally applicable methods to gauge the consistency improvements been developed. Further, simply scaling models has not made the problem go away, as is evident in the largest chatbots trained such as BlenderBot with up to 9.4B parameter Transformers (Roller et al., 2020). More similar to our work is utilizing NLI models in dialogue consistency. Dziri et al. (2019b) attempted to use entailment models trained on syn1700 thetic datasets for dialogue topic coherence evaluation. Particularly, Welleck et al. (2019) constructed the dialogue NLI dataset and (Li et al., 2020) utilized it to try to reduce inconsistency in generative models via unlikelihood training in a preliminary study that reports perplexity results, but did not measure actual generations or contradiction rates. We note that the dialogue NLI dataset is only semi-automatically generated, with limited coverage of only Persona-chat data (Zhang et al., 2018), whereas our DECODE is human-written and across multiple domains. Our task also involves logical and context-related reasoning beyond personal facts. We show that transfer of DECODE is subsequently more robust than dialogue NLI on both human-hum"
2021.acl-long.134,2021.ccl-1.108,0,0.0674875,"Missing"
2021.acl-long.134,D17-2014,1,0.889652,"Missing"
2021.acl-long.134,2020.acl-main.441,1,0.83802,"ANLI-R3, DECODE. “All DNLI” denotes all the datasets with DNLI removed. BART (Lewis et al., 2020). They represent the start-of-the-art language representation models and have yielded successes in many NLU tasks. The input format of fθ follows how these models handle sequence-pairs (C and u) for classification tasks with padding, separator and other special tokens such as position embeddings and segment features inserted at designated locations accordingly. We fine-tune fθ on different combinations of NLI training data including SNLI (Bowman et al., 2015), MNLI (Williams et al., 2018), ANLIR3 (Nie et al., 2020a)4 , DNLI (Welleck et al., 2019), as well as our DECODE Main training set. We convert the 3-way labels of the examples in existing NLI datasets to 2-way, as described before, and θ is optimized using cross-entropy loss. When training fθU B in the utterance-based approach using the DECODE training set, the input sequences 4 ANLI data is collected in three rounds resulting in three subsets (R1, R2, R3). We only used training data in R3 since it contains some dialogue-related examples. 1703 5 5.1 Results and Analysis Performance on Constructed Dataset Our main results comparing various detectors"
2021.acl-long.134,2020.emnlp-main.734,1,0.806327,"Missing"
2021.eacl-main.24,2020.emnlp-main.703,0,0.0535983,"Missing"
2021.eacl-main.24,P18-1082,0,0.0259682,"focuses on empathy, and Wizard of Wikipedia (Dinan et al., 2019c) focuses on knowledge. Finally, Blended Skill Talk (Smith et al., 2020) provides a dataset that focuses on blending these skills. We refer to the “BST tasks” as training on all four tasks together. In addition to skilled-focus datasets, we apply a classifier similar to the one trained in (Dinan et al., 2019b) at test time to detect toxic language before it is shown, and gauge how often such classifiers flag model responses. 4 Decoding We compare several well-known approaches: beam search for different beam sizes, top-k sampling (Fan et al., 2018), sample-and-rank (Adiwardana et al., 2020). We also experiment with minimumlength constraints that forbid end-token generation below a minimum length, and a predictive-length approach that predicts one of four utterance lengths Training Data Evaluation Methods While we employ and report automatic metrics, our main evaluation involves the ACUTE-Eval procedure (Li et al., 2019b), whereby evaluators are asked to make pairwise evaluations of complete 302 4 Note that the 90M model discussed later in the paper uses a variant of the corpus with less filtering. See Shuster et al. (2019) for details."
2021.eacl-main.24,P19-1358,1,0.898912,"Missing"
2021.eacl-main.24,P18-1152,0,0.0284111,"man-human logs), we are nowhere near as close to solving the problem of open-domain conversation as this evaluation would indicate. Here, we highlight problems with our models, and elucidate why our evaluation does not capture them. Selected example failures from crowdworker logs are given as conversation snippets in Figure 6, and further failures constructed by the paper authors are shown in the Appendix (H.5). Vocabulary Usage Generative models employing beam search decoding tend to generate common words too frequently, and rare words too infrequently, as compared to the human distribution (Holtzman et al., 2018; Welleck et al., 2020; Li et al., 2019a). In dialogue, humans can interpret 305 Model Model Human Meena BST Gen (2.7B) std beam. BST Gen (2.7B) Human 10.4 9.5 21.3 18.0 8.2 11.3 16.3 18.0 Figure 4: Response length statistics for various models. We note the best performing methods have longer response lengths, and humans interacting with them have longer response lengths in kind. n-gram Do you have you have any a lot of What do you you like to MLE UL Human 110 82 74 57 54 60 46 46 20 43 6 2 14 6 1 Figure 5: Counts of 5 most common 3-grams from the BST Generative 2.7B model (MLE) from 100 conve"
2021.eacl-main.24,N19-4009,1,0.895875,"Missing"
2021.eacl-main.24,P19-1534,1,0.88315,"Missing"
2021.eacl-main.24,W18-5713,1,0.943117,"al. (2020), with 2 encoder layers, 24 decoder layers, 2560 dimensional embeddings, and 32 attention heads. 3 https://github.com/huggingface/ tokenizers https://www.pandorabots.com/mitsuku/ https://www.cleverbot.com/ 301 Retrieve and Refine Current generative models are known to have issues with producing dull and repetitive responses which are improved, but not resolved, by simply scaling (Holtzman et al., 2019; Welleck et al., 2020; Li et al., 2019a). One approach to try to alleviate these problems is to combine a retrieval step before generation, referred to as a retrieve and refine model (Weston et al., 2018). We consider two variants for the retrieval step: dialogue retrieval and knowledge retrieval. Dialogue retrieval uses a retrieval-based dialogue model (see above) first to produce a response, which is then appended to the input sequence of the generator, along with a special separator token, and then generate from that expanded context with the generative architecture above. Knowledge retrieval first retrieves from a large knowledge base and conditions the generation on the retrieved knowledge, as done in (Dinan et al., 2019c). We hence refer to this as Wiz Generative model. We use the same k"
2021.eacl-main.24,W18-3022,0,0.0555999,"Missing"
2021.eacl-main.24,P18-1205,1,0.733419,"Missing"
2021.emnlp-main.398,D18-1298,0,0.0199685,"e-Trained Representations Our multi-modal model is constructed from modThe training procedure and initial pre-trained els pre-trained in other, related domains; specifmodel weights will be made publicly available to ically, we seek to fuse the resulting weights of allow for fully reproducible results. large-scale, uni-modal pre-training to achieve good 4864 performance on downstream, multi-modal tasks. Adapting pre-trained representations to later downstream tasks has been shown to be successful in NLP (Peters et al., 2019; Devlin et al., 2019) and dialogue in particular (Roller et al., 2020; Mazaré et al., 2018), while large-scale multi-modal pretraining has been shown to be effective in other downstream multi-modal tasks (Li et al., 2020; Chen et al., 2020; Singh et al., 2020b). Our work does not contain multi-modal pre-training in itself, but rather we explore “domain-adaptive pretraining” (Gururangan et al., 2020) or “intermediate task transfer"" (Pruksachatkun et al., 2020), in which pre-trained representations are “adapted"" to a certain domain via an intermediate training step, before training/evaluating on the requisite downstream tasks. We also employ multi-task training, to both help generaliz"
2021.emnlp-main.398,R19-1125,0,0.0153428,"show that our model is not only more preferred by humans but can also discuss and reference visual context throughout a conversation. See Figure 1 for one sample cherrypicked conversation with our model, with random and lemon-picked conversations in Figures 2 and 3. One important avenue we explore with our best models is safety - that is, ensuring that our models are not offensive to their conversational partners. Dialogue safety is indeed a well-studied, but still unsolved, research area (Dinan et al., 2019b; Liu et al., 2019; Dinan et al., 2019a; Blodgett et al., 2020; Khatri et al., 2018; Schäfer and Burtenshaw, 2019; Zhang et al., 2018a), yet we note that safety in the context of image-dialogue is relatively less explored. In this work we examine gender bias and toxicity of text generations in the context of various styles from the Image-Chat dataset (Shuster et al., 2020). Notably, after tuning the model to reduce toxicity and gender bias, we find that human preference for this model does not diminish. 2 2.1 Related Work Multi-Modal Models and Tasks Rich Representations Modeling multi-modal inputs, i.e. in visual + textual contexts, is a wellresearched area. Much of the existing literature explores simi"
2021.emnlp-main.398,I17-1047,0,0.0288686,"the one-turn text generation expected for captioning an image. Other recent architectures have explored text generation (Wang et al., 2020; Park et al., 2020) in the context of the Visual Dialog (Das et al., 2017b) task; however, this task is primarily used to measure the ability to answer questions about an image in the flow of a natural conversation, which differs somewhat from the open-domain dialogue task. Further still, there have been recent forays into open-domain natural dialogue in the context of images, e.g. in the Image-Chat (Shuster et al., 2020) and Image-grounded Conversations (Mostafazadeh et al., 2017) tasks. Again, retrieval-based (Shuster et al., 2020; Ju et al., 2019) and sequence-tosequence (Shuster et al., 2019b, 2020) models have been used to conduct dialogue in this regime. 2.2 Multi-Task Training / Using Pre-Trained Representations Our multi-modal model is constructed from modThe training procedure and initial pre-trained els pre-trained in other, related domains; specifmodel weights will be made publicly available to ically, we seek to fuse the resulting weights of allow for fully reproducible results. large-scale, uni-modal pre-training to achieve good 4864 performance on downstre"
2021.emnlp-main.398,W19-4302,0,0.0566296,"Missing"
2021.emnlp-main.398,2020.acl-main.467,0,0.0274336,"rmance on downstream, multi-modal tasks. Adapting pre-trained representations to later downstream tasks has been shown to be successful in NLP (Peters et al., 2019; Devlin et al., 2019) and dialogue in particular (Roller et al., 2020; Mazaré et al., 2018), while large-scale multi-modal pretraining has been shown to be effective in other downstream multi-modal tasks (Li et al., 2020; Chen et al., 2020; Singh et al., 2020b). Our work does not contain multi-modal pre-training in itself, but rather we explore “domain-adaptive pretraining” (Gururangan et al., 2020) or “intermediate task transfer"" (Pruksachatkun et al., 2020), in which pre-trained representations are “adapted"" to a certain domain via an intermediate training step, before training/evaluating on the requisite downstream tasks. We also employ multi-task training, to both help generalize the applicability of the model and improve its performance on downstream tasks/evaluations; this has been shown recently to help in both image-based (Singh et al., 2020b; Ju et al., 2019; Lu et al., 2020) and text-based (Shuster et al., 2019b; Roller et al., 2020) tasks. 2.3 Comparison to Existing Models specifically use the 2AMMC model from Ju et al. (2019) because t"
2021.emnlp-main.398,P19-1534,1,0.850647,"are in Table 12 (each in the appendix). Results are summarized in Table 1, and we note 1 some interesting conclusions here, with further deUnlike in those works, the output of the encoder is then passed to a decoder, as in the late fusion case. tails in Appendix E. First, overloading the Trans4866 The goal of our resulting model is to perform well in a multi-modal dialogue setting; thus, we fine-tune the model on both dialogue and imagedialogue datasets. For dialogue-based datasets, we consider the same four as in Roller et al. (2020): ConvAI2 (Dinan et al., 2020b), EmpatheticDialogues (ED) (Rashkin et al., 2019), WizImage Features None ResNeXt WSL Faster R-CNN Training Data None BST+ BST+ + IC Image Fusion ConvAI2 ED WoW BST BST+ BST+ BST+ BST+ BST+ BST+ BST+ BST+ 12.41 10.08 10.03 IC 1st Turn 32.36 38.94 16.03 None 12.31 8.74 8.72 10.21 8.32 8.24 13.00 8.78 8.81 + IC + IC + IC + COCO + IC + COCO Late Early Late Early 8.71 8.80 8.79 8.91 8.25 8.32 8.36 8.38 8.87 8.79 9.00 8.99 10.09 10.17 10.21 10.29 + IC + IC + IC + COCO + Reddit + IC + COCO + Reddit Late Early Late Early 8.70 8.81 8.75 8.78 8,24 8.33 8.31 8.31 8.92 8.81 8.93 8.85 10.07 10.15 10.14 10.15 IC 21.48 23.13 13.21 Text Avg. 11.98 8.98 8.9"
2021.emnlp-main.398,2020.acl-main.219,1,0.865244,"ferent ing to humans (Hu et al., 2014). Recent efforts conversational skills over the course of rich dia- have gone beyond classical, fact-based tasks such logue. Much recent work has explored building and as image captioning or visual question answering training dialogue agents that can blend such skills (Antol et al., 2015; Das et al., 2017a) to produce throughout natural conversation, with the ultimate models that can respond and communicate about goal of providing an interesting and engrossing ex- images in the flow of natural conversation (Shuster perience for humans (Smith et al., 2020; Shuster et al., 2020, 2019b). et al., 2019b). Coupled with the advancement of In this work, we explore the extension of large∗ scale conversational agents to image-based diaJoint First Authors. 4863 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 4863–4883 c November 7–11, 2021. 2021 Association for Computational Linguistics logue. We combine representations from imagebased models that have been trained on object detection tasks (Lu et al., 2020, 2019) with representations from Transformers with billions of parameters pre-trained on massive (text-only) dialogue datase"
2021.emnlp-main.398,2020.emnlp-main.269,0,0.0198876,"have been designed to measure the ability of a model to produce text in the context of images. Specifically, COCO Captions (Chen et al., 2015) and Flickr30k (Young et al., 2014) require a model to produce a caption for a given image. A variety of sequence-to-sequence (Vinyals et al., 2015; Xu et al., 2015; Anderson et al., 2018) and retrievalbased (Gu et al., 2018; Faghri et al., 2018; Nam et al., 2016) models have been applied to these tasks, however they do not go beyond the one-turn text generation expected for captioning an image. Other recent architectures have explored text generation (Wang et al., 2020; Park et al., 2020) in the context of the Visual Dialog (Das et al., 2017b) task; however, this task is primarily used to measure the ability to answer questions about an image in the flow of a natural conversation, which differs somewhat from the open-domain dialogue task. Further still, there have been recent forays into open-domain natural dialogue in the context of images, e.g. in the Image-Chat (Shuster et al., 2020) and Image-grounded Conversations (Mostafazadeh et al., 2017) tasks. Again, retrieval-based (Shuster et al., 2020; Ju et al., 2019) and sequence-tosequence (Shuster et al., 2"
2021.emnlp-main.398,Q14-1006,0,0.00776153,". Others have explored modifications to the standard self-attention scheme in Transformers by incorporating additional co-attention (Lu et al., 2019; Tan and Bansal, 2019) or cross-attention (Stefanini et al., 2020) layers. These models have primarily been used for generating rich joint representations of images and text for use in downstream tasks, and primarily focus on the encoding aspect. Visual Dialogue/Caption Generation Many tasks have been designed to measure the ability of a model to produce text in the context of images. Specifically, COCO Captions (Chen et al., 2015) and Flickr30k (Young et al., 2014) require a model to produce a caption for a given image. A variety of sequence-to-sequence (Vinyals et al., 2015; Xu et al., 2015; Anderson et al., 2018) and retrievalbased (Gu et al., 2018; Faghri et al., 2018; Nam et al., 2016) models have been applied to these tasks, however they do not go beyond the one-turn text generation expected for captioning an image. Other recent architectures have explored text generation (Wang et al., 2020; Park et al., 2020) in the context of the Visual Dialog (Das et al., 2017b) task; however, this task is primarily used to measure the ability to answer question"
2021.emnlp-main.398,P18-1125,0,0.0285678,"ly more preferred by humans but can also discuss and reference visual context throughout a conversation. See Figure 1 for one sample cherrypicked conversation with our model, with random and lemon-picked conversations in Figures 2 and 3. One important avenue we explore with our best models is safety - that is, ensuring that our models are not offensive to their conversational partners. Dialogue safety is indeed a well-studied, but still unsolved, research area (Dinan et al., 2019b; Liu et al., 2019; Dinan et al., 2019a; Blodgett et al., 2020; Khatri et al., 2018; Schäfer and Burtenshaw, 2019; Zhang et al., 2018a), yet we note that safety in the context of image-dialogue is relatively less explored. In this work we examine gender bias and toxicity of text generations in the context of various styles from the Image-Chat dataset (Shuster et al., 2020). Notably, after tuning the model to reduce toxicity and gender bias, we find that human preference for this model does not diminish. 2 2.1 Related Work Multi-Modal Models and Tasks Rich Representations Modeling multi-modal inputs, i.e. in visual + textual contexts, is a wellresearched area. Much of the existing literature explores similar architectures to"
2021.emnlp-main.398,P18-1205,1,0.771983,"ly more preferred by humans but can also discuss and reference visual context throughout a conversation. See Figure 1 for one sample cherrypicked conversation with our model, with random and lemon-picked conversations in Figures 2 and 3. One important avenue we explore with our best models is safety - that is, ensuring that our models are not offensive to their conversational partners. Dialogue safety is indeed a well-studied, but still unsolved, research area (Dinan et al., 2019b; Liu et al., 2019; Dinan et al., 2019a; Blodgett et al., 2020; Khatri et al., 2018; Schäfer and Burtenshaw, 2019; Zhang et al., 2018a), yet we note that safety in the context of image-dialogue is relatively less explored. In this work we examine gender bias and toxicity of text generations in the context of various styles from the Image-Chat dataset (Shuster et al., 2020). Notably, after tuning the model to reduce toxicity and gender bias, we find that human preference for this model does not diminish. 2 2.1 Related Work Multi-Modal Models and Tasks Rich Representations Modeling multi-modal inputs, i.e. in visual + textual contexts, is a wellresearched area. Much of the existing literature explores similar architectures to"
2021.emnlp-main.398,2020.acl-demos.30,0,0.550138,"g existing models in multi-modal dialogue while simultaneously performing as well as its predecessor (text-only) BlenderBot (Roller et al., 2020) in text-based conversation. We additionally investigate and incorporate safety components in our final model, and show that such efforts do not diminish model performance with respect to human preference. Figure 1: Cherry picked conversation between a paper author (right) and our MMB DegenPos model (left). More sample conversations are in the appendix. large-scale model training schemes, such models are becoming increasingly human-like and engaging (Zhang et al., 2020; Adiwardana et al., 2020; Roller et al., 2020). 1 Introduction In order to better approach human-like ability, An important goal of artificial intelligence is the however, it is necessary that agents can converse construction of open-domain conversational agents with both textual and visual context, similarly to that can engage humans in discourse. Indeed, the how humans interact in the real world; indeed, comfuture of human interaction with AI is predicated munication grounded in images is naturally engagon models that can exhibit a number of different ing to humans (Hu et al., 2014). Recent"
2021.findings-acl.54,N19-1170,1,0.850422,"from the given training batch. Architecture and Training Choices We employ the 90M and 622M parameter models from (Roller et al., 2020) that have been pre-trained on 1.5B training examples from pushshift.io Reddit, which we then fine-tune. We also consider two other enhancements, chosen to mitigate problems that we observed with the models: (i) negative context training, whereby negatives are also selected from the immediate dialogue history as well as the batch which can help reduce a model’s tendency to repeat itself (Holtzman et al., 2019; Welleck et al., 2020); and (ii) decoding control (See et al., 2019) whereby at decoding time responses are rescaled before scoring based on their specificity (normalized inverse document frequency). The latter can control the genericness of the responses, which is known to affect human judgments. Generative Models In addition to the deployed models, we also train and evaluate generative models offline, where safety concerns are less important as the models are not user-facing. We employ an encoder-decoder Transformer architecture using the state of the art pre-trained 2.7 billion parameter BlenderBot model (Roller et al., 2020), which we fine-tune on our task"
2021.findings-acl.54,D19-1062,1,0.942621,"-Domain Dialogue Dialogue in the opendomain (chitchat) setting, which involves chat about any topic, rather than a specific goal-directed topic, is commonly studied in the train/valid/test static dataset paradigm utilizing supervised learning. A number of crowdsourced or scraped datasets have been developed to that end, including Daily Dialogue (Li et al., 2017), PersonaChat (Zhang et al., 2018), Empathetic Dialogues (Rashkin et al., 2019) and Wizard of Wikipedia (Dinan et al., 2019b); see Huang et al. (2020) for a review. LIGHT In this work we specifically focus on dialogue setting of LIGHT (Urbanek et al., 2019). LIGHT focuses on situated characters playing character roles that can chat about any topic, within the context of a medieval fantasy world. This setting is known to be engaging for human role-players, and also alleviates some safety concerns in that the roleplaying means they should not divulge personally identifying information. The authors crowdsourced a dialogue dataset consisting of 8.5k episodes and 111k utterances, which they publicly released. We refer to this as LIGHT MTurk data, or LIGHT data for short, in the rest of this paper. In this work we utilize this data to build a deployed"
2021.findings-acl.54,P18-1205,1,0.926808,"ing within games. The training code and parameters of the models deployed, and the data collected in this work will be made publicly available for reproducibility and further research by the community. 2 Related Work Open-Domain Dialogue Dialogue in the opendomain (chitchat) setting, which involves chat about any topic, rather than a specific goal-directed topic, is commonly studied in the train/valid/test static dataset paradigm utilizing supervised learning. A number of crowdsourced or scraped datasets have been developed to that end, including Daily Dialogue (Li et al., 2017), PersonaChat (Zhang et al., 2018), Empathetic Dialogues (Rashkin et al., 2019) and Wizard of Wikipedia (Dinan et al., 2019b); see Huang et al. (2020) for a review. LIGHT In this work we specifically focus on dialogue setting of LIGHT (Urbanek et al., 2019). LIGHT focuses on situated characters playing character roles that can chat about any topic, within the context of a medieval fantasy world. This setting is known to be engaging for human role-players, and also alleviates some safety concerns in that the roleplaying means they should not divulge personally identifying information. The authors crowdsourced a dialogue dataset"
2021.findings-emnlp.320,P17-1171,1,0.888066,"Missing"
2021.findings-emnlp.320,2021.tacl-1.6,0,0.34195,"ecent methods have focused on: determining which elements of a given piece of knowledge are informative to the dialogue, which is commonly referred to as “knowledge selection” (Zhao et al., 2020b; Kim et al., 2020; Bruyn et al., 2020); learning how to attend to the relevant knowledge (Ma et al., 2020; Cai et al., 2020; Zhao et al., 2020a); or examining how much knowledge is present in large language models (Zhao et al., 2020c). Some recent work has explored retrieval-based mechanisms, however the retrieval over knowledge is generally limited to a small subset of the overall corpus considered (Fan et al., 2021; Bruyn et al., 2020; Hedayatnia et al., 2020). Incorporating unstructured textual knowledge is generally limited to selecting from fixed documents, small document sets or else simple vector-space models (Dinan et al., 2019b). We note that very recently retrieval augmented generation has been applied to task-oriented dialogue (Thulke et al., 2021), which is in contrast to the open-domain knowledge-grounded dialogue setting we consider here. Other work that includes a retrieval-augmentation step includes the area of language modeling, where it is used for pretraining (Guu et al., 2020), and as"
2021.findings-emnlp.320,P18-1082,0,0.0706773,"Missing"
2021.findings-emnlp.320,P17-1147,0,0.0765016,"Missing"
2021.findings-emnlp.320,2020.emnlp-main.550,0,0.0233327,"ion. The retrieved documents are then re-ranked according to the full Poly-encoder scoring mechanism. Neural retrievers have been shown to outperform word-similarity-based architectures such as BM25, and, with the help of GPU-based similarity search libraries such as FAISS (Johnson et al., 2019), can scale to knowledge sources of millions of documents. We first discuss these new architectures. Lewis et al. (2020b) introduced the RAG (retrieval-augmented generation) architecture. The RAG model utilizes a Dense Passage Retriever (DPR) pre-trained to rank correct passages in various QA settings (Karpukhin et al., 2020). A large FAISS index stores d(zj ), with q(xi ) as the query for relevant documents. RAG-Sequence considers documents independently, generating an output sequence for each concatenated context separately and marginalizing over the output generations. RAG-Token marginalizes the output distribution over all documents, allowing the generator to attend over a different document for each token. Though d(zj ) remains fixed during training, token losses are propagated to the retriever itself, and the context representations q(xi ) are updated in order to better fit the retriever for the task. Izacar"
2021.findings-emnlp.320,2021.acl-short.47,0,0.0193211,"ge is generally limited to selecting from fixed documents, small document sets or else simple vector-space models (Dinan et al., 2019b). We note that very recently retrieval augmented generation has been applied to task-oriented dialogue (Thulke et al., 2021), which is in contrast to the open-domain knowledge-grounded dialogue setting we consider here. Other work that includes a retrieval-augmentation step includes the area of language modeling, where it is used for pretraining (Guu et al., 2020), and as a memory (Yogatama et al., 2021), especially using k-nearest neighbor-based cache models (Khandelwal et al., 2021, 2020; Grave et al., 2017; Merity et al., 2017). Hallucination in text-generation models is a topic that has received attention recently, particularly in the settings of summarization (Maynez et al., 2020), machine translation (Zhou et al., 2021), and news generation (Zellers et al., 2019). For dialogue, it has been observed in state-of-the-art models (Roller et al., 2021) and studied in depth (Mielke et al., 2020), but so far without resolution. Open-domain question answering (QA) has 3 Model Architectures long considered retrieval as an intermediate step (Voorhees and Tice, 2000). It has be"
2021.findings-emnlp.320,Q19-1026,0,0.0293049,"Missing"
2021.findings-emnlp.320,2020.acl-main.703,0,0.654437,"the text is the GPT3 generation using default parameters. Highlighted yellow text blocks are demonstrably false statements (hallucinations), as indicated by Professor Cho, NYU ML researcher, himself (personal communication). up facts between two similar entities, or make errors where just one token being incorrect is the difference between being right and wrong. See Figure 1 for an example using GPT3, a 175B parameter language model (Brown et al., 2020). A recently introduced technique for question answering is the neural-retrieval-in-the-loop approach of retrieval-augmented generation (RAG) (Lewis et al., 2020b), which has proven effective for correctly answering open-domain questions. The tech1 Introduction nique employs an encoder-decoder to encode the Large language models trained on large corpora question and decode (generate) the answer, where have made great inroads in the fluency and con- the encoding is augmented with documents or pasversational ability of dialogue agents (Adiwardana sages retrieved from a large unstructured document et al., 2020; Roller et al., 2021), yielding low per- set using a learnt matching function; the entire neuplexity models that have corresponding high to- ral n"
2021.findings-emnlp.320,D16-1230,0,0.0937326,"Missing"
2021.findings-emnlp.320,2020.findings-emnlp.122,0,0.0338973,"its occurrence (Dinan et al., 2019b; Ghazvininejad et al., 2018; Gopalakrishnan et al., 2019; Galetzka et al., 2020). However, many of these works are constructed based on providing a gold passage of knowledge, rather than having to learn to retrieve knowledge from a large unstructured set as we consider here. Recent methods have focused on: determining which elements of a given piece of knowledge are informative to the dialogue, which is commonly referred to as “knowledge selection” (Zhao et al., 2020b; Kim et al., 2020; Bruyn et al., 2020); learning how to attend to the relevant knowledge (Ma et al., 2020; Cai et al., 2020; Zhao et al., 2020a); or examining how much knowledge is present in large language models (Zhao et al., 2020c). Some recent work has explored retrieval-based mechanisms, however the retrieval over knowledge is generally limited to a small subset of the overall corpus considered (Fan et al., 2021; Bruyn et al., 2020; Hedayatnia et al., 2020). Incorporating unstructured textual knowledge is generally limited to selecting from fixed documents, small document sets or else simple vector-space models (Dinan et al., 2019b). We note that very recently retrieval augmented generation"
2021.findings-emnlp.320,2020.acl-main.173,0,0.0342436,"ed to task-oriented dialogue (Thulke et al., 2021), which is in contrast to the open-domain knowledge-grounded dialogue setting we consider here. Other work that includes a retrieval-augmentation step includes the area of language modeling, where it is used for pretraining (Guu et al., 2020), and as a memory (Yogatama et al., 2021), especially using k-nearest neighbor-based cache models (Khandelwal et al., 2021, 2020; Grave et al., 2017; Merity et al., 2017). Hallucination in text-generation models is a topic that has received attention recently, particularly in the settings of summarization (Maynez et al., 2020), machine translation (Zhou et al., 2021), and news generation (Zellers et al., 2019). For dialogue, it has been observed in state-of-the-art models (Roller et al., 2021) and studied in depth (Mielke et al., 2020), but so far without resolution. Open-domain question answering (QA) has 3 Model Architectures long considered retrieval as an intermediate step (Voorhees and Tice, 2000). It has become a We extend neural-retriever-in-the-loop generativemore intensively studied topic recently, first using based architectures, which have performed well in simple vector-space based retrievers (Chen et a"
2021.findings-emnlp.320,2020.acl-main.64,0,0.0158586,"logue context and retrieving knowledge from the entire of Wikipedia. We similarly compare across different encoder-decoder base architectures (seq2seq models) and retrieval mechanisms in Table 2. Overall, we see that retrieval helps substantially in improving performance on both knowledge-grounded conversational datasets. 4.2 Eliminating Hallucination fields such as machine translation and QA, standard automated metrics such as F1, BLEU, and ROUGE have been shown to be not totally correlated with how well neural conversational models perform in the wild (Liu et al., 2016; Dinan et al., 2019a; Mehri and Eskenazi, 2020). We thus introduce an additional metric, Knowledge F1. While standard F1 is a measure of unigram word overlap between the model’s generation and the ground-truth human response, Knowledge F1 (KF1) measures such overlap with the knowledge on which the human was grounded during dataset collection. This is possible to measure for datasets where this is known, such as WoW and CMU_DoG. KF1 attempts to capture whether a model is speaking knowledgeably by using relevant knowledge as judged by humans, whereas standard F1 captures conversational ability, including token overlap that is unrelated to kn"
2021.findings-emnlp.320,D17-2014,1,0.877898,"Missing"
2021.findings-emnlp.320,2021.findings-acl.120,0,0.0312853,"Missing"
2021.findings-emnlp.320,D18-1076,0,0.127772,"r, knowledge-grounded dialogue offers a more challenging (or at the very least, materially different) retrieval task than question answering. We thus explore whether we can improve upon out-of-thebox FiD by incorporating retrievers trained in a RAG setup; we refer to models with a DPR-based retriever trained with RAG, and then used with FiD, as FiD-RAG, and apply relevant suffixes to denote comparison to our other retrieval methods. 4 Experiments Datasets: We conduct experiments on two datasets: Wizard of Wikipedia (WoW) (Dinan et al., 2019b) and CMU Document Grounded Conversations (CMU_DoG) (Zhou et al., 2018) which are both sets of knowledge-grounded dialogues collected through human-human crowdworker chats in English, where one of the crowdworkers had access to external knowledge from Wikipedia; WoW discusses various topics, and CMU_DoG discusses movies. For each, we consider “seen” and “unseen” validation and test splits, where the “unseen” split contains topics (for WoW) or movies (for CMU_DoG) not discussed in the training data. WoW provides these splits, and we constructed our own for CMU_DoG. We employ the standard KiLT Wikipedia dump (Petroni et al., 2021) as our knowledge source for retrie"
2021.findings-emnlp.320,P19-1417,0,0.047496,"Missing"
2021.naacl-main.235,D19-1461,1,0.406496,"ngs suggest it is insufficient to merely exclude toxic data from training, as the model would not know how to answer hostile out-of-domain inputs, and positive biases where models tend to agree rather than contradict (Roller et al., 2020) would lead to undesirable outcomes. As shown in Gehman et al. (2020), training on sanitized data can decrease the amount of unprompted toxic content, yet still leave models vulnerable to generating toxic content based on specific prompts. The moving target of toxic content requires dynamic methods that repeatedly update benchmarks to improve current systems (Dinan et al., 2019a; Nie et al., 2019)3 . The iterative procedure in Dinan et al. (2019a) strictly focuses on detection of toxicity in human-generated utterances through several rounds of humans attempting to “break” a toxicity classifier, without addressing generation. Our BAD approach is similar in spirit, but centers on generations of a bot in a human-bot conversation, closer to the context of deployed conversational models. Focusing on generation requires deciding how to address “bad content.” Previous works have compared response strategies, including avoidance, joking or polite deflection, non-committal a"
2021.naacl-main.64,D19-5301,1,0.516736,"al., 2017), where two agents are trying to convince each other to perform certain actions, are related to the tasks in LIGHT-Quests. These works all lack environment grounding and the notion of diverse agent motivations. Commonsense reasoning in language. Works such as (Bosselut et al., 2019; Guan et al., 2020) focus on pre-training transformer-based language learning systems with large-scale commonsense knowledge graphs such as ATOMIC (Sap et al., 2019) and ConceptNet (Speer and Havasi, 2012) for use in knowledge graph completion and story ending generation respectively. (Fulda et al., 2017; Ammanabrolu and Riedl, 2019; Ammanabrolu et al., 2020; Murugesan et al., 2020) look at commonsense reasoning in interactive environments, with the former focusing on affordance extraction using word embeddings and the latter three on transferring text-game playing skills via pretraining using question-answering and large-scale knowledge graphs. and speech sequences of average length 12.92. 3 LIGHT-Quests and ATOMIC-LIGHT This section first provides a brief overview of the LIGHT game environment, followed by descriptions of the LIGHT-Quests and ATOMIC-LIGHT datasets used in this paper. Background. The LIGHT game environm"
2021.naacl-main.64,N16-1089,0,0.0727251,"Missing"
2021.naacl-main.64,P19-1470,0,0.0216079,"faces (Henderson et al., 2014; 808 El Asri et al., 2017). RL has been studied for such tasks, usually to improve dialogue state management (Singh et al., 2000; Pietquin et al., 2011; Fatemi et al., 2016) and to improve response quality (Li et al., 2016). In particular, the negotiation tasks of (Yarats and Lewis, 2017; Lewis et al., 2017), where two agents are trying to convince each other to perform certain actions, are related to the tasks in LIGHT-Quests. These works all lack environment grounding and the notion of diverse agent motivations. Commonsense reasoning in language. Works such as (Bosselut et al., 2019; Guan et al., 2020) focus on pre-training transformer-based language learning systems with large-scale commonsense knowledge graphs such as ATOMIC (Sap et al., 2019) and ConceptNet (Speer and Havasi, 2012) for use in knowledge graph completion and story ending generation respectively. (Fulda et al., 2017; Ammanabrolu and Riedl, 2019; Ammanabrolu et al., 2020; Murugesan et al., 2020) look at commonsense reasoning in interactive environments, with the former focusing on affordance extraction using word embeddings and the latter three on transferring text-game playing skills via pretraining usin"
2021.naacl-main.64,2020.emnlp-main.656,1,0.828507,"Missing"
2021.naacl-main.64,W17-5526,0,0.0684239,"Missing"
2021.naacl-main.64,D19-1001,0,0.0414854,"Missing"
2021.naacl-main.64,D19-1447,0,0.0232567,"are trained with RL. Solid lines indicate gradient flow. that is pre-trained on the Reddit dialogue corpus, then on LIGHT and the human demonstrations of LIGHT-Quests. Following the format seen in Figure 3, the partner agent does not have a motivation itself but is trained to react to agents with motivations. Following (Prabhumoye et al., 2020), we keep the partner model fixed during the episodes where the LIGHT agent trains to ensure that it retains natural English semantics—avoiding the problem of language drift by learning an emergent language with that must agree with the partner’s usage (Lee et al., 2019). Action Rewards via the Game Engine. All actions, either those of the agent-in-training or the partner agent, are processed by the engine, checking for goal state completion—hence known as act goals. For example, if the LIGHT agent had the motivation to acquire a sword, the goal could be completed via a: 1. self act completion: where the agent acquires a sword itself by picking it up, stealing it, convincing the partner to drop theirs so you can pick it up, etc. 2. partner act completion: where the agent uses speech to convince their partner to achieve the goal for them (e.g., by persuading t"
2021.naacl-main.64,D17-1259,0,0.0177625,"oal-oriented dialogue. This form of dialogue commonsense reasoning in language, and general has traditionally been closely related to specific language-informed RL. tasks useful in the context of personal assistants Text-based game-playing. (Côté et al., 2018) with dialogue interfaces (Henderson et al., 2014; 808 El Asri et al., 2017). RL has been studied for such tasks, usually to improve dialogue state management (Singh et al., 2000; Pietquin et al., 2011; Fatemi et al., 2016) and to improve response quality (Li et al., 2016). In particular, the negotiation tasks of (Yarats and Lewis, 2017; Lewis et al., 2017), where two agents are trying to convince each other to perform certain actions, are related to the tasks in LIGHT-Quests. These works all lack environment grounding and the notion of diverse agent motivations. Commonsense reasoning in language. Works such as (Bosselut et al., 2019; Guan et al., 2020) focus on pre-training transformer-based language learning systems with large-scale commonsense knowledge graphs such as ATOMIC (Sap et al., 2019) and ConceptNet (Speer and Havasi, 2012) for use in knowledge graph completion and story ending generation respectively. (Fulda et al., 2017; Ammanabrol"
2021.naacl-main.64,D16-1127,0,0.0430964,"our major areas of related work: text-based game-playing, goal-oriented dialogue, Goal-oriented dialogue. This form of dialogue commonsense reasoning in language, and general has traditionally been closely related to specific language-informed RL. tasks useful in the context of personal assistants Text-based game-playing. (Côté et al., 2018) with dialogue interfaces (Henderson et al., 2014; 808 El Asri et al., 2017). RL has been studied for such tasks, usually to improve dialogue state management (Singh et al., 2000; Pietquin et al., 2011; Fatemi et al., 2016) and to improve response quality (Li et al., 2016). In particular, the negotiation tasks of (Yarats and Lewis, 2017; Lewis et al., 2017), where two agents are trying to convince each other to perform certain actions, are related to the tasks in LIGHT-Quests. These works all lack environment grounding and the notion of diverse agent motivations. Commonsense reasoning in language. Works such as (Bosselut et al., 2019; Guan et al., 2020) focus on pre-training transformer-based language learning systems with large-scale commonsense knowledge graphs such as ATOMIC (Sap et al., 2019) and ConceptNet (Speer and Havasi, 2012) for use in knowledge grap"
2021.naacl-main.64,D18-1298,0,0.0187088,"to act consistently in the world. For example, given a clause such as “The knight wishes to slay the dragon, as a result the knight needs to acquire a sword,” the task would be to predict the underlined text—a form of knowledge graph completion (Wang et al., 2017). Reddit We use a previously existing Reddit dataset extracted and obtained by a third party and made available on pushshift.io (Baumgartner et al., 2020) seen in (Roller et al., 2020). This dataset has been used in several existing dialogue-based studies and has been shown to result in more natural conversations (Yang et al., 2018; Mazaré et al., 2018). LIGHT-Original The original LIGHT dataset (Urbanek et al., 2019) is organized similarly to the human demonstrations found in LIGHT-Quests, i.e. an interspersed sequence of dialogue and actions collected from humans role-playing a character. The task itself is to predict the next action or utterance given the prior dialogue history as well as the current setting and persona for a character. They are collected in a chit-chat fashion, with no notion of objectives, and so provide priors on how to generally act consistently and speak in a fantasy world, but not directly how to complete quests. LI"
2021.naacl-main.64,D15-1001,0,0.216972,"and May, 2019; Adolphs and Hofmann, 2019; Adhikari et al., 2020) build agents that operate in this environment—focusing on aspects such as efficient exploration and zeroshot generalization to new, procedurally generated environments. Similarly, (Hausknecht et al., 2020) introduce Jericho, a framework and series of baseline agents for interacting with human-made textgames such as Zork (Anderson et al., 1979). This resulted in agents developed by works such as (Zahavy et al., 2018; Ammanabrolu and Hausknecht, 2020), aiming to learn to execute contextually relevant actions. Other works such as (Narasimhan et al., 2015; He et al., 2016) explore how to best factorize such text-game action spaces. None of these works consider agents with motivations and personas nor require any dialogue. We focus on four major areas of related work: text-based game-playing, goal-oriented dialogue, Goal-oriented dialogue. This form of dialogue commonsense reasoning in language, and general has traditionally been closely related to specific language-informed RL. tasks useful in the context of personal assistants Text-based game-playing. (Côté et al., 2018) with dialogue interfaces (Henderson et al., 2014; 808 El Asri et al., 20"
2021.naacl-main.64,D19-1062,1,0.750496,"ch as “The knight wishes to slay the dragon, as a result the knight needs to acquire a sword,” the task would be to predict the underlined text—a form of knowledge graph completion (Wang et al., 2017). Reddit We use a previously existing Reddit dataset extracted and obtained by a third party and made available on pushshift.io (Baumgartner et al., 2020) seen in (Roller et al., 2020). This dataset has been used in several existing dialogue-based studies and has been shown to result in more natural conversations (Yang et al., 2018; Mazaré et al., 2018). LIGHT-Original The original LIGHT dataset (Urbanek et al., 2019) is organized similarly to the human demonstrations found in LIGHT-Quests, i.e. an interspersed sequence of dialogue and actions collected from humans role-playing a character. The task itself is to predict the next action or utterance given the prior dialogue history as well as the current setting and persona for a character. They are collected in a chit-chat fashion, with no notion of objectives, and so provide priors on how to generally act consistently and speak in a fantasy world, but not directly how to complete quests. LIGHT-Quests Pre-training with this newly introduced dataset consist"
2021.naacl-main.64,W18-3022,0,0.0288532,"for an agent on how to act consistently in the world. For example, given a clause such as “The knight wishes to slay the dragon, as a result the knight needs to acquire a sword,” the task would be to predict the underlined text—a form of knowledge graph completion (Wang et al., 2017). Reddit We use a previously existing Reddit dataset extracted and obtained by a third party and made available on pushshift.io (Baumgartner et al., 2020) seen in (Roller et al., 2020). This dataset has been used in several existing dialogue-based studies and has been shown to result in more natural conversations (Yang et al., 2018; Mazaré et al., 2018). LIGHT-Original The original LIGHT dataset (Urbanek et al., 2019) is organized similarly to the human demonstrations found in LIGHT-Quests, i.e. an interspersed sequence of dialogue and actions collected from humans role-playing a character. The task itself is to predict the next action or utterance given the prior dialogue history as well as the current setting and persona for a character. They are collected in a chit-chat fashion, with no notion of objectives, and so provide priors on how to generally act consistently and speak in a fantasy world, but not directly how"
2021.naacl-main.64,2020.acl-main.704,0,0.014192,"or the partner agent, are processed by the engine, checking for goal state completion—hence known as act goals. For example, if the LIGHT agent had the motivation to acquire a sword, the goal could be completed via a: 1. self act completion: where the agent acquires a sword itself by picking it up, stealing it, convincing the partner to drop theirs so you can pick it up, etc. 2. partner act completion: where the agent uses speech to convince their partner to achieve the goal for them (e.g., by persuading the partner to give them the sword). automatic evaluation of natural language generation (Sellam et al., 2020), we utilize a learned model–the Dungeon Master (DM)—to score the agent’s ability to speak. The DM used here is a poly-encoder model trained on collected human quest demonstrations as well as the original conversations in LIGHT. It is conditioned on quests and motivations and thus able to provide a (noisy) indication of how natural the agent’s dialogue utterances are given its immediate context, similarly to the function of the DM during the data collection process. Given the dialogue portion of a human quest demonstration of length n, the DM returns a 1 reward ru of 2n if an utterance was in"
D13-1136,P05-1045,0,0.111865,"Missing"
D13-1136,P11-1055,0,0.686939,"al., 1999), which matched the Yeast Protein Database with PubMed abstracts. It was also used to train open extractors based on Wikipedia infoboxes and corresponding sentences (Wu and Weld, 2007; Wu and Weld, 2010). Largescale open IE projects (e.g. (Banko et al., 2007)) also rely on weak supervision, since they learn models from a seed KB in order to extend it. Weak supervision is also a popular option for RE: Mintz et al. (2009) used Freebase to train weakly supervised relational extractors on Wikipedia, an approach generalized by the multi-instance learning frameworks (Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012). All these works only use textual information to perform extraction. Lao et al. (2012) proposed the first work aiming to perform RE employing both KB data and text, using a rule-based random walk method. Recently, Riedel et al. (2013) proposed another joint approach based on collaborative filtering for learning entity embeddings. This approach connects text with Freebase by learning shared embeddings of entities through weak supervision, in contrast to our method where no joint learning is performed. We do not compare to these two approaches since they use two differen"
D13-1136,D12-1093,0,0.0200963,"ors based on Wikipedia infoboxes and corresponding sentences (Wu and Weld, 2007; Wu and Weld, 2010). Largescale open IE projects (e.g. (Banko et al., 2007)) also rely on weak supervision, since they learn models from a seed KB in order to extend it. Weak supervision is also a popular option for RE: Mintz et al. (2009) used Freebase to train weakly supervised relational extractors on Wikipedia, an approach generalized by the multi-instance learning frameworks (Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012). All these works only use textual information to perform extraction. Lao et al. (2012) proposed the first work aiming to perform RE employing both KB data and text, using a rule-based random walk method. Recently, Riedel et al. (2013) proposed another joint approach based on collaborative filtering for learning entity embeddings. This approach connects text with Freebase by learning shared embeddings of entities through weak supervision, in contrast to our method where no joint learning is performed. We do not compare to these two approaches since they use two different evaluation protocols that greatly differ from those used in all aforementioned previous works. Nevertheless,"
D13-1136,P09-1011,0,0.0122155,"features. This paper is organized as follows: Section 2 presents related work, Section 3 introduces our model and its main influences, and experimental results are displayed in Section 4. 2 Previous Work Learning under weak supervision is common in natural language processing, especially for tasks where the annotation costs are significant such as in se1366 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1366–1371, c Seattle, Washington, USA, 18-21 October 2013. 2013 Association for Computational Linguistics mantic parsing (Kate and Mooney, 2007; Liang et al., 2009; Bordes et al., 2010; Matuszek et al., 2012). This is also naturally used in IE, since it allows to train large-scale systems without requiring to label numerous texts. The idea was introduced by (Craven et al., 1999), which matched the Yeast Protein Database with PubMed abstracts. It was also used to train open extractors based on Wikipedia infoboxes and corresponding sentences (Wu and Weld, 2007; Wu and Weld, 2010). Largescale open IE projects (e.g. (Banko et al., 2007)) also rely on weak supervision, since they learn models from a seed KB in order to extend it. Weak supervision is also a p"
D13-1136,P09-1113,0,0.896141,"Matuszek et al., 2012). This is also naturally used in IE, since it allows to train large-scale systems without requiring to label numerous texts. The idea was introduced by (Craven et al., 1999), which matched the Yeast Protein Database with PubMed abstracts. It was also used to train open extractors based on Wikipedia infoboxes and corresponding sentences (Wu and Weld, 2007; Wu and Weld, 2010). Largescale open IE projects (e.g. (Banko et al., 2007)) also rely on weak supervision, since they learn models from a seed KB in order to extend it. Weak supervision is also a popular option for RE: Mintz et al. (2009) used Freebase to train weakly supervised relational extractors on Wikipedia, an approach generalized by the multi-instance learning frameworks (Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012). All these works only use textual information to perform extraction. Lao et al. (2012) proposed the first work aiming to perform RE employing both KB data and text, using a rule-based random walk method. Recently, Riedel et al. (2013) proposed another joint approach based on collaborative filtering for learning entity embeddings. This approach connects text with Freebase by learning sh"
D13-1136,N13-1008,0,0.159111,"al., 2007)) also rely on weak supervision, since they learn models from a seed KB in order to extend it. Weak supervision is also a popular option for RE: Mintz et al. (2009) used Freebase to train weakly supervised relational extractors on Wikipedia, an approach generalized by the multi-instance learning frameworks (Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012). All these works only use textual information to perform extraction. Lao et al. (2012) proposed the first work aiming to perform RE employing both KB data and text, using a rule-based random walk method. Recently, Riedel et al. (2013) proposed another joint approach based on collaborative filtering for learning entity embeddings. This approach connects text with Freebase by learning shared embeddings of entities through weak supervision, in contrast to our method where no joint learning is performed. We do not compare to these two approaches since they use two different evaluation protocols that greatly differ from those used in all aforementioned previous works. Nevertheless, our method is easier to integrate into existing systems than those, since KB data is used via the addition of a scoring term, which is trained separ"
D13-1136,D12-1042,0,0.762467,"hed the Yeast Protein Database with PubMed abstracts. It was also used to train open extractors based on Wikipedia infoboxes and corresponding sentences (Wu and Weld, 2007; Wu and Weld, 2010). Largescale open IE projects (e.g. (Banko et al., 2007)) also rely on weak supervision, since they learn models from a seed KB in order to extend it. Weak supervision is also a popular option for RE: Mintz et al. (2009) used Freebase to train weakly supervised relational extractors on Wikipedia, an approach generalized by the multi-instance learning frameworks (Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012). All these works only use textual information to perform extraction. Lao et al. (2012) proposed the first work aiming to perform RE employing both KB data and text, using a rule-based random walk method. Recently, Riedel et al. (2013) proposed another joint approach based on collaborative filtering for learning entity embeddings. This approach connects text with Freebase by learning shared embeddings of entities through weak supervision, in contrast to our method where no joint learning is performed. We do not compare to these two approaches since they use two different evaluation protocols t"
D13-1136,P10-1013,0,0.0714568,"atural Language Processing, pages 1366–1371, c Seattle, Washington, USA, 18-21 October 2013. 2013 Association for Computational Linguistics mantic parsing (Kate and Mooney, 2007; Liang et al., 2009; Bordes et al., 2010; Matuszek et al., 2012). This is also naturally used in IE, since it allows to train large-scale systems without requiring to label numerous texts. The idea was introduced by (Craven et al., 1999), which matched the Yeast Protein Database with PubMed abstracts. It was also used to train open extractors based on Wikipedia infoboxes and corresponding sentences (Wu and Weld, 2007; Wu and Weld, 2010). Largescale open IE projects (e.g. (Banko et al., 2007)) also rely on weak supervision, since they learn models from a seed KB in order to extend it. Weak supervision is also a popular option for RE: Mintz et al. (2009) used Freebase to train weakly supervised relational extractors on Wikipedia, an approach generalized by the multi-instance learning frameworks (Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012). All these works only use textual information to perform extraction. Lao et al. (2012) proposed the first work aiming to perform RE employing both KB data and text, usi"
D14-1067,P14-1133,0,0.192768,"g a structure among answers. We suppose that all potential answers are entities in the KB and that questions are sequences of words that include one identified KB entity. When this entity is not given, plain string matching is used to perform entity resolution. Smarter methods could be used but this is not our focus. We use W EB Q UESTIONS (Berant et al., 2013) as our evaluation bemchmark. Since it contains few training samples, it is impossible to learn on it alone, and this section describes the various data sources that were used for training. These are similar to those used in (Berant and Liang, 2014). ClueWeb Extractions F REEBASE data allows to train our model on 14M questions but these have a fixed lexicon and vocabulary, which is not realistic. Following (Berant et al., 2013), we also created questions using C LUE W EB extractions provided by (Lin et al., 2012). Using string matching, we ended up with 2M extractions structured as (subject, “text string”, object) with both subject and object linked to F REEBASE. We also converted these triples into questions by using simple patterns and F REEBASE types. An example of generated question is “Where barack obama was allegedly bear in?” (haw"
D14-1067,D13-1160,0,0.793606,"question into a valid query and then use fine-grained detection heuristics to identify the exact answer (Kolomiyets and Moens, 2011; Unger et al., 2012; 615 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 615–620, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics question in the graph); and (2) a richer representation of the answers which encodes the questionanswer path and surrounding subgraph of the KB. Our approach is competitive with the current stateof-the-art on the recent benchmark W EB Q UES TIONS (Berant et al., 2013) without using any lexicon, rules or additional system for part-of-speech tagging, syntactic or dependency parsing during training as most other systems do. 2 where two entities subject and object (identified by mids) are connected by the relation type type1.type2.predicate. We used a subset, created by only keeping triples where one of the entities was appearing in either the W EB Q UES TIONS training/validation set or in C LUE W EB extractions. We also removed all entities appearing less than 5 times and finally obtained a F REEBASE set containing 14M triples made of 2.2M entities and 7k rel"
D14-1067,P14-1090,0,0.711567,"Missing"
D14-1067,D14-1067,1,0.115731,"bypassing most of the annotation costs. Yet, even if both kinds of system have shown the ability to handle large-scale KBs, they still require experts to hand-craft lexicons, grammars, and KB schema to be effective. This non-negligible human intervention might not be generic enough to conveniently scale up to new databases with other schema, broader vocabularies or languages other than English. In contrast, (Fader et al., 2013) proposed a framework for open QA requiring almost no human annotation. Despite being an interesting approach, this method is outperformed by other competing methods. (Bordes et al., 2014b) introduced an embedding model, which learns lowdimensional vector representations of words and symbols (such as KBs constituents) and can be trained with even less supervision than the system of (Fader et al., 2013) while being able to achieve better prediction performance. However, this approach is only compared with (Fader et al., 2013) which operates in a simplified setting and has not been applied in more realistic conditions nor evaluated against the best performing methods. In this paper, we improve the model of (Bordes et al., 2014b) by providing the ability to answer more complicate"
D14-1067,P13-1158,0,0.0804658,"Berant and Liang, 2014; Fader et al., 2014) have shown that such systems can be efficiently trained under indirect and imperfect supervision and hence scale to large-scale regimes, while bypassing most of the annotation costs. Yet, even if both kinds of system have shown the ability to handle large-scale KBs, they still require experts to hand-craft lexicons, grammars, and KB schema to be effective. This non-negligible human intervention might not be generic enough to conveniently scale up to new databases with other schema, broader vocabularies or languages other than English. In contrast, (Fader et al., 2013) proposed a framework for open QA requiring almost no human annotation. Despite being an interesting approach, this method is outperformed by other competing methods. (Bordes et al., 2014b) introduced an embedding model, which learns lowdimensional vector representations of words and symbols (such as KBs constituents) and can be trained with even less supervision than the system of (Fader et al., 2013) while being able to achieve better prediction performance. However, this approach is only compared with (Fader et al., 2013) which operates in a simplified setting and has not been applied in mo"
D14-1067,D13-1161,0,0.0178161,"Missing"
D14-1067,W12-3016,0,0.00729787,"arter methods could be used but this is not our focus. We use W EB Q UESTIONS (Berant et al., 2013) as our evaluation bemchmark. Since it contains few training samples, it is impossible to learn on it alone, and this section describes the various data sources that were used for training. These are similar to those used in (Berant and Liang, 2014). ClueWeb Extractions F REEBASE data allows to train our model on 14M questions but these have a fixed lexicon and vocabulary, which is not realistic. Following (Berant et al., 2013), we also created questions using C LUE W EB extractions provided by (Lin et al., 2012). Using string matching, we ended up with 2M extractions structured as (subject, “text string”, object) with both subject and object linked to F REEBASE. We also converted these triples into questions by using simple patterns and F REEBASE types. An example of generated question is “Where barack obama was allegedly bear in?” (hawaii). WebQuestions This dataset is built using F REE BASE as the KB and contains 5,810 questionanswer pairs. It was created by crawling questions through the Google Suggest API, and then obtaining answers using Amazon Mechanical Turk. We used the original split (3,778"
D14-1194,C10-2028,0,0.0345048,"learning, and that our convolu1822 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1822–1827, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics convolution layer w1 w2 max tanh pooling layer linear layer hashtag lookup table word lookup table (l + K wl tanh layer 1) ⇥ d l⇥H l⇥H H N ⇥d H d t d f (w, t) Figure 1: #TAG S PACE convolutional network f (w, t) for scoring a (document, hashtag) pair. tional architecture performs better than W SABIE trained on the same hashtag task. 2 Prior Work Some previous work (Davidov et al., 2010; Godin et al., 2013; She and Chen, 2014) has addressed hashtag prediction. Most such work applies to much smaller sets of hashtags than the 100,000 we consider, with the notable exception of Ding et al. (2012), which uses an unsupervised method. As mentioned in Section 1, many approaches learn unsupervised word embeddings. In our experiments we use word2vec (Mikolov et al., 2013) as a representative scalable model for unsupervised embeddings. W SABIE (Weston et al., 2011) is a supervised embedding approach that has shown promise in NLP tasks (Weston et al., 2013; Hermann et al., 2014). W SABI"
D14-1194,C12-2027,0,0.25452,"ational Linguistics convolution layer w1 w2 max tanh pooling layer linear layer hashtag lookup table word lookup table (l + K wl tanh layer 1) ⇥ d l⇥H l⇥H H N ⇥d H d t d f (w, t) Figure 1: #TAG S PACE convolutional network f (w, t) for scoring a (document, hashtag) pair. tional architecture performs better than W SABIE trained on the same hashtag task. 2 Prior Work Some previous work (Davidov et al., 2010; Godin et al., 2013; She and Chen, 2014) has addressed hashtag prediction. Most such work applies to much smaller sets of hashtags than the 100,000 we consider, with the notable exception of Ding et al. (2012), which uses an unsupervised method. As mentioned in Section 1, many approaches learn unsupervised word embeddings. In our experiments we use word2vec (Mikolov et al., 2013) as a representative scalable model for unsupervised embeddings. W SABIE (Weston et al., 2011) is a supervised embedding approach that has shown promise in NLP tasks (Weston et al., 2013; Hermann et al., 2014). W SABIE is shallow, linear, and ignores word order information, and so may have less modeling power than our approach. Convolutional neural networks (CNNs), in which shared weights are applied across the input, are p"
D14-1194,D11-1014,0,0.04745,". In particular, the search for a negative candidate tag means that more energy is spent on improving the ranking performance of positive labels already near the top of the ranked list, compared to only randomly sampling of negatives, which would optimize the average rank instead. Minimizing our loss is achieved with parallel stochastic gradient descent using the hogwild algorithm (Niu et al., 2011). The lookup-table layers are initialized with the embeddings learned by W SABIE to expedite convergence. This kind of ‘pre-training’ is a standard trick in the neural network literature, see e.g. (Socher et al., 2011). The ranking loss makes our model scalable to 100,000 (or more) hashtags. At each training example only a subset of tags have to be computed, so it is far more efficient than a standard classification loss that considers them all. 4 Experiments 4.1 Data Our experiments use two large corpora of posts containing hashtags from a popular social network.1 The first corpus, which we call people, consists of 201 million posts from individual user accounts, comprising 5.5 billion words. The second corpus, which we call pages, consists of 35.3 million page posts, comprising 1.6 1 Both corpora were de-"
D14-1194,D13-1170,0,0.00317668,"in NLP tasks (Weston et al., 2013; Hermann et al., 2014). W SABIE is shallow, linear, and ignores word order information, and so may have less modeling power than our approach. Convolutional neural networks (CNNs), in which shared weights are applied across the input, are popular in the vision domain and have recently been applied to semantic role labeling (Collobert et al., 2011) and parsing (Collobert, 2011). Neural networks in general have also been applied to part-of-speech tagging, chunking, named entity recognition (Collobert et al., 2011; Turian et al., 2010), and sentiment detection (Socher et al., 2013). All these tasks involve predicting a limited (2-30) number of labels. In this work, we make use of CNNs, but apply them to the task of ranking a very large set of tags. We thus propose a model and training scheme that can scale to this class of problem. 3 Convolutional Embedding Model Our model #TAG S PACE (see Figure 1), like other word embedding models, starts by assigning a ddimensional vector to each of the l words of an input document w1 , . . . , wl , resulting in a matrix of size l × d. This is achieved using a matrix of N × d parameters, termed the lookup-table layer (Collobert et al"
D14-1194,P10-1040,0,0.0399268,"vised embedding approach that has shown promise in NLP tasks (Weston et al., 2013; Hermann et al., 2014). W SABIE is shallow, linear, and ignores word order information, and so may have less modeling power than our approach. Convolutional neural networks (CNNs), in which shared weights are applied across the input, are popular in the vision domain and have recently been applied to semantic role labeling (Collobert et al., 2011) and parsing (Collobert, 2011). Neural networks in general have also been applied to part-of-speech tagging, chunking, named entity recognition (Collobert et al., 2011; Turian et al., 2010), and sentiment detection (Socher et al., 2013). All these tasks involve predicting a limited (2-30) number of labels. In this work, we make use of CNNs, but apply them to the task of ranking a very large set of tags. We thus propose a model and training scheme that can scale to this class of problem. 3 Convolutional Embedding Model Our model #TAG S PACE (see Figure 1), like other word embedding models, starts by assigning a ddimensional vector to each of the l words of an input document w1 , . . . , wl , resulting in a matrix of size l × d. This is achieved using a matrix of N × d parameters,"
D14-1194,D13-1136,1,0.624842,"rior Work Some previous work (Davidov et al., 2010; Godin et al., 2013; She and Chen, 2014) has addressed hashtag prediction. Most such work applies to much smaller sets of hashtags than the 100,000 we consider, with the notable exception of Ding et al. (2012), which uses an unsupervised method. As mentioned in Section 1, many approaches learn unsupervised word embeddings. In our experiments we use word2vec (Mikolov et al., 2013) as a representative scalable model for unsupervised embeddings. W SABIE (Weston et al., 2011) is a supervised embedding approach that has shown promise in NLP tasks (Weston et al., 2013; Hermann et al., 2014). W SABIE is shallow, linear, and ignores word order information, and so may have less modeling power than our approach. Convolutional neural networks (CNNs), in which shared weights are applied across the input, are popular in the vision domain and have recently been applied to semantic role labeling (Collobert et al., 2011) and parsing (Collobert, 2011). Neural networks in general have also been applied to part-of-speech tagging, chunking, named entity recognition (Collobert et al., 2011; Turian et al., 2010), and sentiment detection (Socher et al., 2013). All these ta"
D14-1194,P14-1136,1,0.71851,"us work (Davidov et al., 2010; Godin et al., 2013; She and Chen, 2014) has addressed hashtag prediction. Most such work applies to much smaller sets of hashtags than the 100,000 we consider, with the notable exception of Ding et al. (2012), which uses an unsupervised method. As mentioned in Section 1, many approaches learn unsupervised word embeddings. In our experiments we use word2vec (Mikolov et al., 2013) as a representative scalable model for unsupervised embeddings. W SABIE (Weston et al., 2011) is a supervised embedding approach that has shown promise in NLP tasks (Weston et al., 2013; Hermann et al., 2014). W SABIE is shallow, linear, and ignores word order information, and so may have less modeling power than our approach. Convolutional neural networks (CNNs), in which shared weights are applied across the input, are popular in the vision domain and have recently been applied to semantic role labeling (Collobert et al., 2011) and parsing (Collobert, 2011). Neural networks in general have also been applied to part-of-speech tagging, chunking, named entity recognition (Collobert et al., 2011; Turian et al., 2010), and sentiment detection (Socher et al., 2013). All these tasks involve predicting"
D15-1044,D13-1176,0,0.429435,"eural model, we fix θ and tune the α parameters. We follow the statistical machine translation setup and use minimumerror rate training (MERT) to tune for the summarization metric on tuning data (Och, 2003). This tuning step is also identical to the one used for the phrase-based machine translation baseline. 6 Neural MT This work is closely related to recent work on neural network language models (NNLM) and to work on neural machine translation. The core of our model is a NNLM based on that of Bengio et al. (2003). Recently, there have been several papers about models for machine translation (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014). Of these our model is most closely related to the attention-based model of Bahdanau et al. (2014), which explicitly finds a soft alignment between the current position and the input source. Most of these models utilize recurrent neural networks (RNNs) for generation as opposed to feedforward models. We hope to incorporate an RNNLM in future work. Related Work Abstractive sentence summarization has been traditionally connected to the task of headline generation. Our work is similar to early work of Banko et al. (2000) who developed a statistical mach"
D15-1044,P00-1041,0,0.8743,"beddings, and h is a hidden layer of size H. The black-box function enc is a contextual encoder term that returns a vector of size H representing the input and current context; we consider several possible variants, described subsequently. Figure 3a gives a schematic representation of the decoder architecture. Model The distribution of interest, p(yi+1 |x, yc ; θ), is a conditional language model based on the input sentence x. Past work on summarization and compression has used a noisy-channel approach to split and independently estimate a language model and a conditional summarization model (Banko et al., 2000; Knight and Marcu, 2002; Daum´e III and Marcu, 2002), i.e., 3.2 Encoders Note that without the encoder term this represents a standard language model. By incorporating in enc and training the two elements jointly we crucially can incorporate the input text into generation. We discuss next several possible instantiations of the encoder. arg max log p(y|x) = arg max log p(y)p(x|y) y where p(y) and p(x|y) are estimated separately. Here we instead follow work in neural machine translation and directly parameterize the original distribution as a neural network. The network contains both a neural p"
D15-1044,P07-2045,0,0.0480613,"e original sentence along with a language model trained on the headline data to produce a compressed output. The syntax and language model are combined with a set of linguistic constraints and decoding is performed with an ILP solver. To control for memorizing titles from training, we implement an information retrieval baseline, IR. This baseline indexes the training set, and gives the title for the article with highest BM-25 match to the input (see Manning et al. (2008)). Finally, we use a phrase-based statistical machine translation system trained on Gigaword to produce summaries, M OSES + (Koehn et al., 2007). To improve the baseline for this task, we augment the phrase table with “deletion” rules mapping each article word to , include an additional deletion feature for these rules, and allow for an infinite distortion limit. We also explicitly tune the model using MERT to target the 75byte capped ROUGE score as opposed to standard In addition to the standard DUC-2014 evaluation, we also report evaluation on single reference headline-generation using a randomly heldout subset of Gigaword. This evaluation is closer to the task the model is trained for, and it allows us to use a bigger evaluation s"
D15-1044,W04-1013,0,0.302425,"es and Associated Press Wire services each paired with 4 different human-generated reference summaries (not actually headlines), capped at 75 bytes. This data set is evaluation-only, although the similarly sized DUC-2003 data set was made available for the task. The expectation is for a summary of roughly 14 words, based on the text of a complete article (although we only make use of the first sentence). The full data set is available by request at http://duc.nist.gov/data.html. For this shared task, systems were entered and evaluated using several variants of the recalloriented ROUGE metric (Lin, 2004). To make recall-only evaluation unbiased to length, output of all systems is cut-off after 75-characters and no bonus is given for shorter summaries. 384 headline and the input; although only 2.6 in the first 75-characters of the input. Unlike BLEU which interpolates various n-gram matches, there are several versions of ROUGE for different match lengths. The DUC evaluation uses ROUGE-1 (unigrams), ROUGE-2 (bigrams), and ROUGE-L (longest-common substring), all of which we report. 7.2 Baselines Due to the variety of approaches to the sentence summarization problem, we report a broad set of head"
D15-1044,C08-1018,0,0.43679,"not appear as part of the original. We focus on the task of sentence-level summarization. While much work on this task has looked at deletion-based sentence compression techniques (Knight and Marcu (2002), among many others), studies of human summarizers show that it is common to apply various other operations while condensing, such as paraphrasing, generalization, and reordering (Jing, 2002). Past work has modeled this abstractive summarization problem either using linguistically-inspired constraints (Dorr et al., 2003; Zajic et al., 2004) or with syntactic transformations of the input text (Cohn and Lapata, 2008; Woodsend et al., 2010). These approaches are described in more detail in Section 6. We instead explore a fully data-driven approach for generating abstractive summaries. Inspired by the recent success of neural machine translation, we combine a neural language model with a contextual input encoder. Our encoder is modeled off of the attention-based encoder of Bahdanau et al. (2014) in that it learns a latent soft alignment over the input text to help inform the summary (as shown in Figure 1). Crucially both the encoder and the generation model are trained jointly on the sentence summarization"
D15-1044,P14-5010,0,0.0191357,"n addition to the standard DUC-2014 evaluation, we also report evaluation on single reference headline-generation using a randomly heldout subset of Gigaword. This evaluation is closer to the task the model is trained for, and it allows us to use a bigger evaluation set, which we will include in our code release. For this evaluation, we tune systems to generate output of the average title length. For training data for both tasks, we utilize the annotated Gigaword data set (Graff et al., 2003; Napoles et al., 2012), which consists of standard Gigaword, preprocessed with Stanford CoreNLP tools (Manning et al., 2014). Our model only uses annotations for tokenization and sentence separation, although several of the baselines use parsing and tagging as well. Gigaword contains around 9.5 million news articles sourced from various domestic and international news services over the last two decades. For our training set, we pair the headline of each article with its first sentence to create an inputsummary pair. While the model could in theory be trained on any pair, Gigaword contains many spurious headline-article pairs. We therefore prune training based on the following heuristic filters: (1) Are there no non"
D15-1044,P02-1057,0,0.0280446,"Missing"
D15-1044,W03-0501,0,0.894234,"st, abstractive summarization attempts to produce a bottom-up summary, aspects of which may not appear as part of the original. We focus on the task of sentence-level summarization. While much work on this task has looked at deletion-based sentence compression techniques (Knight and Marcu (2002), among many others), studies of human summarizers show that it is common to apply various other operations while condensing, such as paraphrasing, generalization, and reordering (Jing, 2002). Past work has modeled this abstractive summarization problem either using linguistically-inspired constraints (Dorr et al., 2003; Zajic et al., 2004) or with syntactic transformations of the input text (Cohn and Lapata, 2008; Woodsend et al., 2010). These approaches are described in more detail in Section 6. We instead explore a fully data-driven approach for generating abstractive summaries. Inspired by the recent success of neural machine translation, we combine a neural language model with a contextual input encoder. Our encoder is modeled off of the attention-based encoder of Bahdanau et al. (2014) in that it learns a latent soft alignment over the input text to help inform the summary (as shown in Figure 1). Cruci"
D15-1044,W12-3018,0,0.146461,"Missing"
D15-1044,D13-1155,0,0.197713,"generation and allows it to fit with a wider range of training data. In this work we focus on factored scoring functions, s, that take into account a fixed window of previous words: s(x, y) ≈ N −1 X g(yi+1 , x, yc ), (4) i=0 2 For the DUC-2004 evaluation, it is actually the number of bytes of the output that is capped. More detail is given in Section 7. 3 Unfortunately the literature is inconsistent on the formal definition of this distinction. Some systems self-described as abstractive would be extractive under our definition. 1 In contrast to a large-scale sentence compression systems like Filippova and Altun (2013) which require monotonic aligned compressions. 380 where we define yc , y[i−C+1,...,i] for a window of size C. In particular consider the conditional logprobability of a summary given the input, s(x, y) = log p(y|x; θ). We can write this as: log p(y|x; θ) ≈ N −1 X log p(yi+1 |x, yc ; θ), x P U y ˜c x ˜ y ˜c0 E yc F x G yc (b) (a) Figure 3: (a) A network diagram for the NNLM decoder with additional encoder element. (b) A network diagram for the attention-based encoder enc3 . The parameters are θ = (E, U, V, W) where E ∈ RD×V is a word embedding matrix, U ∈ R(CD)×H , V ∈ RV ×H , W ∈ RV ×H are we"
D15-1044,P03-1021,0,0.0445649,"ors of unigram, bigram, and trigram match with the input as well as reordering of input words. Note that setting α = h1, 0, . . . , 0i gives a model identical to standard A BS. 383 pression, the sentences are transformed by a series of heuristics such that the words are in monotonic alignment. Our system does not require this alignment step but instead uses the text directly. After training the main neural model, we fix θ and tune the α parameters. We follow the statistical machine translation setup and use minimumerror rate training (MERT) to tune for the summarization metric on tuning data (Och, 2003). This tuning step is also identical to the one used for the phrase-based machine translation baseline. 6 Neural MT This work is closely related to recent work on neural network language models (NNLM) and to work on neural machine translation. The core of our model is a NNLM based on that of Bengio et al. (2003). Recently, there have been several papers about models for machine translation (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014). Of these our model is most closely related to the attention-based model of Bahdanau et al. (2014), which explicitly finds a soft al"
D15-1044,J02-4006,0,0.236151,"ilize extractive approaches that crop out and stitch together portions of the text to produce a condensed version. In contrast, abstractive summarization attempts to produce a bottom-up summary, aspects of which may not appear as part of the original. We focus on the task of sentence-level summarization. While much work on this task has looked at deletion-based sentence compression techniques (Knight and Marcu (2002), among many others), studies of human summarizers show that it is common to apply various other operations while condensing, such as paraphrasing, generalization, and reordering (Jing, 2002). Past work has modeled this abstractive summarization problem either using linguistically-inspired constraints (Dorr et al., 2003; Zajic et al., 2004) or with syntactic transformations of the input text (Cohn and Lapata, 2008; Woodsend et al., 2010). These approaches are described in more detail in Section 6. We instead explore a fully data-driven approach for generating abstractive summaries. Inspired by the recent success of neural machine translation, we combine a neural language model with a contextual input encoder. Our encoder is modeled off of the attention-based encoder of Bahdanau et"
D15-1044,D10-1050,0,0.771207,"he original. We focus on the task of sentence-level summarization. While much work on this task has looked at deletion-based sentence compression techniques (Knight and Marcu (2002), among many others), studies of human summarizers show that it is common to apply various other operations while condensing, such as paraphrasing, generalization, and reordering (Jing, 2002). Past work has modeled this abstractive summarization problem either using linguistically-inspired constraints (Dorr et al., 2003; Zajic et al., 2004) or with syntactic transformations of the input text (Cohn and Lapata, 2008; Woodsend et al., 2010). These approaches are described in more detail in Section 6. We instead explore a fully data-driven approach for generating abstractive summaries. Inspired by the recent success of neural machine translation, we combine a neural language model with a contextual input encoder. Our encoder is modeled off of the attention-based encoder of Bahdanau et al. (2014) in that it learns a latent soft alignment over the input text to help inform the summary (as shown in Figure 1). Crucially both the encoder and the generation model are trained jointly on the sentence summarization task. The model is desc"
D15-1044,P12-1107,0,0.00783747,"Missing"
D16-1147,D13-1160,0,0.347765,"ree settings. It also achieves state-of-the-art results on the existing W IKI QA benchmark. 1 Introduction Question answering (QA) has been a long standing research problem in natural language processing, with the first systems attempting to answer questions by directly reading documents (Voorhees and Tice, 2000). The development of large-scale Knowledge Bases (KBs) such as Freebase (Bollacker et al., 2008) helped organize information into structured forms, prompting recent progress to focus on answering questions by converting them into logical forms that can be used to query such databases (Berant et al., 2013; Kwiatkowski et al., 2013; Fader et al., 2014). Unfortunately, KBs have intrinsic limitations such as their inevitable incompleteness and fixed schemas that cannot support all varieties of answers. Since information extraction (IE) (Craven et al., 2000), intended to fill in missing information in KBs, is neither accurate nor reliable enough, collections of raw textual resources and documents such as Wikipedia will always contain more information. As a result, even if KBs can be satisfactory for closed-domain problems, they are unlikely to scale up to answer general questions on any topic. Sta"
D16-1147,D14-1067,1,0.724499,"orded differently) cannot appear in both train and test sets. Note that this is much larger than most existing datasets; for example, the W IK I QA dataset (Yang et al., 2015) for which we also conduct experiments in Sec. 5.2 has only ∼1000 training pairs. 5 Experiments This section describes our experiments on W IKI M OVIES and W IKI QA. 5.1 WikiMovies We conducted experiments on the W IKI M OVIES dataset described in Sec. 4. Our main goal is to compare the performance of KB, IE and Wikipedia (Doc) sources when trying varying learning methods. We compare four approaches: (i) the QA system of Bordes et al. (2014) that performs well on existing datasets WebQuestions (Berant et al., 2013) and SimpleQuestions (Bordes et al., 2015) that use KBs only; (ii) supervised embeddings that do not make use of a KB at all but learn question-to-answer embeddings directly and hence act as a sanity check (Dodge et al., 2016); (iii) Memory Networks; and (iv) Key-Value Memory Networks. Performance is reported using the accuracy of the top hit (single answer) over all possible answers (all entities), i.e. the hits@1 metric measured in percent. In all cases hyperparameters are optimized on the development set, including t"
D16-1147,D13-1161,0,0.123595,"achieves state-of-the-art results on the existing W IKI QA benchmark. 1 Introduction Question answering (QA) has been a long standing research problem in natural language processing, with the first systems attempting to answer questions by directly reading documents (Voorhees and Tice, 2000). The development of large-scale Knowledge Bases (KBs) such as Freebase (Bollacker et al., 2008) helped organize information into structured forms, prompting recent progress to focus on answering questions by converting them into logical forms that can be used to query such databases (Berant et al., 2013; Kwiatkowski et al., 2013; Fader et al., 2014). Unfortunately, KBs have intrinsic limitations such as their inevitable incompleteness and fixed schemas that cannot support all varieties of answers. Since information extraction (IE) (Craven et al., 2000), intended to fill in missing information in KBs, is neither accurate nor reliable enough, collections of raw textual resources and documents such as Wikipedia will always contain more information. As a result, even if KBs can be satisfactory for closed-domain problems, they are unlikely to scale up to answer general questions on any topic. Starting from this observatio"
D16-1147,P14-5010,0,0.00227809,"rces. IE As an alternative to directly reading documents, we explore leveraging information extraction techniques to transform documents into a KB format. An IE-KB representation has attractive properties such as more precise and compact expressions of facts and logical key-value pairings based on subjectverb-object groupings. This can come at the cost of lower recall due to malformed or completely missing triplets. For IE we use standard open-source software followed by some task-specific engineering to improve the results. We first employ coreference resolution via the Stanford NLP Toolkit (Manning et al., 2014) to reduce ambiguity by replacing pronominal (“he”, “it”) and nominal (“the film”) references with their representative entities. Next we use the SENNA semantic role labeling tool (Collobert et al., 2011) to uncover the grammatical structure of each sentence and pair verbs with their arguments. Each triplet is cleaned of words that are not recognized entities, and lemmatization is done to collapse different inflections of important task-specific verbs to one form (e.g. stars, starring, star → starred). Finally, we append the movie title to each triple similar to the “Window + Title” representa"
D16-1147,D07-1003,0,0.0628274,"Missing"
D16-1147,C16-1127,0,0.0911774,"answering from a KB against answering from pure text. Using similar resources as the dialog dataset of Dodge et al. (2016), our new benchmark W IKI M OVIES addresses both deficiencies by providing a substantial corpus of questionanswer pairs that can be answered by either using a KB or a corresponding set of documents. Even though standard pipeline QA systems like AskMR (Banko et al., 2002) have been recently revisited (Tsai et al., 2015), the best published results on T REC QA and W IKI QA have been obtained by either convolutional neural networks (Santos et al., 2016; Yin and Schütze, 2015; Wang et al., 2016) or recurrent neural networks (Miao et al., 2015)— both usually with attention mechanisms inspired by (Bahdanau et al., 2015). In this work, we introduce KV-MemNNs, a Memory Network model that operates a symbolic memory structured as (key, value) pairs. Such structured memory is not employed in any existing attention-based neural network architecture for QA. As we will show, it gives the model greater flexibility for encoding knowledge sources and helps shrink the gap between directly reading documents and answering from a KB. 3 Key-Value Memory Networks The Key-Value Memory Network model is b"
D16-1147,D15-1237,0,0.732032,"ding values are subsequently returned. This structure allows the model to encode prior knowledge for the considered task and to leverage possibly complex transforms between keys and values, while still being trained using standard backpropagation via stochastic gradient descent. Our experiments on W IKI M OVIES indicate that, thanks to its key-value memory, the KV-MemNN consistently outperforms the original Memory Network, and reduces the gap between answering from a human-annotated KB, from an automatically extracted KB or from directly reading Wikipedia. We confirm our findings on W IKI QA (Yang et al., 2015), another Wikipedia-based QA benchmark where no KB is available, where we demonstrate that KV-MemNN can reach state-of-the-art results— surpassing the most recent attention-based neural network models. 2 Related Work Early QA systems were based on information retrieval and were designed to return snippets of text containing an answer (Voorhees and Tice, 2000; Banko et al., 2002), with limitations in terms of question complexity and response coverage. The creation of large-scale KBs (Auer et al., 2007; Bollacker et al., 2008) have led to the development of a new class of QA methods based on sem"
D16-1147,P15-1128,0,0.259538,"ractitioner to encode prior knowledge about their task; and (ii) more effective power in the model via nontrivial transforms between key and value. The key should be designed with features to help match it to the question, while the value should be designed with features to help match it to the response (answer). An important property of the model is that the entire model can be trained with key-value transforms while still using standard backpropagation via stochastic gradient descent. 3.1 Model Description Our model is based on the end-to-end Memory Network architecture of Sukhbaatar et al. (2015). A high-level view of both models is as follows: one defines a memory, which is a possibly very large array of slots which can encode both long-term and short-term context. At test time one is given a query (e.g. the question in QA tasks), which is used to iteratively address and read from the memory (these iterations are also referred to as “hops”) looking for relevant information to answer the question. At each step, the collected information from the memory is cumulatively added to the original query to build context for the next round. At the last iteration, the final 1402 retrieved conte"
D17-2014,W15-4640,0,0.0274898,"(e.g. learning models). • examples: contains examples of different mains (display data, training and evaluation). 81 Teacher: { ’text’: ’Sam went to the kitchen.
 Pat gave Sam the milk.
Where is the milk?’,\ ’labels’: [’kitchen’], ’label_candidates’: [’hallway’, ’kitchen’, ’bathroom’], ’episode_done’: False } 6.4 Tasks Over 20 tasks are supported in the first release, including popular datasets such as SQuAD (Rajpurkar et al., 2016), bAbI tasks (Weston et al., 2015), QACNN and QADailyMail (Hermann et al., 2015), CBT (Hill et al., 2015), bAbI Dialog tasks (Bordes and Weston, 2016), Ubuntu (Lowe et al., 2015) and VQA (Antol et al., 2015). All the datasets in the first release are shown in Fig. 14 . The tasks are separated into five categories: • Question answering (QA): one of the simplest forms of dialog, with only 1 turn per speaker. Any intelligent dialog agent should be capable of answering questions, and there are many kinds of questions (and hence datasets) that one can build, providing a set of very important tests. Question answering is particularly useful in that the evaluation is simpler than other forms of dialog if the dataset is labeled with QA pairs and the questions are mostly unamb"
D17-2014,D16-1264,0,0.176575,"on, learning compositionality and other AI subgoals also have clear roles in dialog. However, to pursue these research goals, we require software tools that unify the different dialog sub-tasks Figure 2: MTurk Live Chat for collecting QA datasets in ParlAI. and the agents that can learn from them. Working on individual datasets can lead to siloed research, where the overfitting to specific qualities of a dataset do not generalize to solving other tasks. For example, methods that do not generalize beyond WebQuestions (Berant et al., 2013) because they specialize on knowledge bases only, SQuAD (Rajpurkar et al., 2016) because they predict start and end context indices (see Sec. 7), or bAbI (Weston et al., 2015) because they use supporting facts or make use of its simulated nature. In this paper we present a software platform, ParlAI (pronounced “par-lay”), that provides researchers a unified framework for training and testing dialog models, especially multitask training or evaluation over 79 Proceedings of the 2017 EMNLP System Demonstrations, pages 79–84 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics many tasks at once, as well as seamless integration with Amaz"
D17-2014,D11-1054,0,0.0180517,"nt to the bathroom
Where is the milk?’, ’labels’: [’hallway’], ’label_candidates’: [’hallway’, ’kitchen’, ’bathroom’], ’done’: True } Student: { ’text’: ’hallway’ } ... Figure 6: A typical exchange from a ParlAI training set involves messages passed using the observation/action dict (the test set would not include labels). Shown here is the bAbI dataset. • remote agent: basic class for any agent connecting over ZeroMQ. • seq2seq: basic GRU sequence to sequence model (Sutskever et al., 2014) • ir baseline: information retrieval (IR) baseline that scores responses with TFIDF-weighted matching (Ritter et al., 2011). • repeat label: basic class for merely repeating all data sent to it (e.g. for debugging). 6.3 Examples This directory contains examples of different mains:. • display data: display data from a particular task provided on the command-line. • display model: show the predictions of a provided model. • eval model: compute evaluation metrics for a given model on a given task. • train model: execute a standard training procedure with a given task and model, including logging and possibly alternating between training and validation. For example, one can display 10 random examples from the bAbI tas"
D17-2014,E17-1001,0,0.0142966,"Missing"
D17-2014,D13-1160,0,0.0126843,"Memory, logical and commonsense reasoning, planning, learning from interaction, learning compositionality and other AI subgoals also have clear roles in dialog. However, to pursue these research goals, we require software tools that unify the different dialog sub-tasks Figure 2: MTurk Live Chat for collecting QA datasets in ParlAI. and the agents that can learn from them. Working on individual datasets can lead to siloed research, where the overfitting to specific qualities of a dataset do not generalize to solving other tasks. For example, methods that do not generalize beyond WebQuestions (Berant et al., 2013) because they specialize on knowledge bases only, SQuAD (Rajpurkar et al., 2016) because they predict start and end context indices (see Sec. 7), or bAbI (Weston et al., 2015) because they use supporting facts or make use of its simulated nature. In this paper we present a software platform, ParlAI (pronounced “par-lay”), that provides researchers a unified framework for training and testing dialog models, especially multitask training or evaluation over 79 Proceedings of the 2017 EMNLP System Demonstrations, pages 79–84 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computa"
D17-2014,P17-1171,1,0.116929,"ta, as well as actions other than speech acts. Each of these fields are technically optional, depending on the dataset, though the text field will most likely be used in nearly all exchanges. A typical exchange from a ParlAI training set is shown in Fig. 6. Figure 5: The observation/action dict is the central message passing object in ParlAI: agents send this message to speak, and receive a message of this form to observe other speakers and the environment. 6 The agents directory contains machine learning agents. Currently available within this directory: • drqa: an attentive LSTM model DrQA (Chen et al., 2017) implemented in PyTorch that has competitive results on SQuAD (Rajpurkar et al., 2016) amongst other datasets. • memnn: code for an end-to-end memory network (Sukhbaatar et al., 2015) in Lua Torch. • tasks: contains code for the different tasks available from within ParlAI. • mturk: contains code for setting up Mechanical Turk and sample MTurk tasks. 6.1 Core The core library contains the following files: • agents.py: defines the Agent base class for all agents, which implements the observe() and act() methods, the Teacher class which also reports metrics, and MultiTaskTeacher for multitask tr"
D19-1062,W05-0614,0,0.0771019,"orld (Cˆot´e et al., 2018), but these do not have human dialogue within the game. Similar single player text adventure games have also been used to study referring expressions (Gabsdil et al., 2001, 2002) and parsing (Koller et al., 2004). Yang et al. (2017) and Bordes et al. (2010) also proposed small world setups for instruction following or labeling, but these are much more restricted than the large multi-player text adventure game environment with rich dialogue that we propose here. 3 LIGHT Environment and Task Setup Other examples are instruction-following in the Neverwinter Nights game (Fleischman and Roy, 2005), studies of emotional response in adventure games (Fraser et al., 2018), dialogue about soccer videogames (Pasunuru and Bansal, 2018), placing blocks appropriately given a final plan (Wang et al., 2016) and a more open ended building task using a grid of voxels (Wang et al., 2017). In the latter two cases the communication is one-sided with only the human issuing instructions, rather than dialogue, with the agent only able to act. LIGHT is a large-scale, configurable text adventure environment for research on learning grounded language and actions. It features both humans and models as agents"
D19-1062,D18-1298,1,0.880452,"Missing"
D19-1062,C02-1113,0,0.259734,"Missing"
D19-1062,D17-2014,1,0.823239,"ly tedious and easy to fake. In order to mitigate these problems, during the evaluation we provide annotated examples on the training in addition to examples on the test set. We only keep the annotations of evaluators who had high accuracy on the training examples to filter low-accuracy evaluators. The training accuracy bar was selected due to the difficulty of the separate tasks as evaluated by our own success rates. Our methods for human evaluation are described in more detail in Appendix F along with how many turns were evaluated. Implementation We implement models using PyTorch in ParlAI (Miller et al., 2017). Ranking Transformer models are pretrained on Reddit data (Mazar´e et al., 2018) and fine-tuned. We use the BERT (Devlin et al., 2018) implementation provided by Hugging Face2 with pre-trained weights, then adapted to our Bi-Ranker and Cross-Ranker setups. Generative models are pretrained on the Toronto Books Corpus and fine-tuned except for emote prediction which does not leverage pretraining. We apply byte-pair encoding (Sennrich et al., 2016) to reduce the vocabulary size for generative models. We decode using beam search with beam size 5. 2 Evaluation 5 Results The ranking models are comp"
D19-1062,W14-4337,0,0.0232878,"hout explicit understanding of the world that the language describes. This work is built on the hypothesis that dialogue agents embodied in a rich and cohesive (but tractable) world can more easily be trained to use language effectively than those only exposed to standard large-scale text-only corpora. To that end, we introduce the LIGHT1 research platform. LIGHT is a multi-player fantasy text adventure world designed for studying situated dialogue, and allows interactions between humans, 1 2 Related Work Most recent work in dialogue exploring generative or retrieval models for goal-directed (Henderson et al., 2014; Bordes et al., 2017) or chitchat tasks (Vinyals and Le, 2015; Sordoni et al., 2015; Zhang et al., 2018) is not situated, or even Learning in Interactive Games with Humans and Text. 673 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 673–683, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics Mostafazadeh et al., 2017). While grounded, the agent has no ability to act in these tasks. Talk the Walk (de Vries et al., 2018) introduces a naviga"
D19-1062,I17-1047,0,0.0243704,"ions between humans, 1 2 Related Work Most recent work in dialogue exploring generative or retrieval models for goal-directed (Henderson et al., 2014; Bordes et al., 2017) or chitchat tasks (Vinyals and Le, 2015; Sordoni et al., 2015; Zhang et al., 2018) is not situated, or even Learning in Interactive Games with Humans and Text. 673 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 673–683, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics Mostafazadeh et al., 2017). While grounded, the agent has no ability to act in these tasks. Talk the Walk (de Vries et al., 2018) introduces a navigation game that involves action, perception and two-way dialogue, but is limited to small grids. In summary, compared to many setups, our framework allows learning from both actions and (two-way) dialogue, while many existing simulations typically address one or the other but not both. In addition, being based on a gaming setup, our hope is that LIGHT can be fun for humans to interact with, enabling future engagement with our models. All utterances in LIGHT are produced by"
D19-1062,D15-1001,0,0.0815297,"from the dialogue history as input, and output a new utterance. While some goal-directed setups may use external knowledge bases (e.g. flight data for airline booking), dialogues tend to implicitly refer to an external world during the conversations without explicit grounding to objects or actions. Several position papers have proposed virtual embodiment as a strategy for language research (Brooks, 1991; Kiela et al., 2016; Gauthier and Mordatch, 2016; Mikolov et al., 2016; Lake et al., 2017). Single-player text adventure game frameworks for training reinforcement learning agents exist, i.e., Narasimhan et al. (2015) and TextWorld (Cˆot´e et al., 2018), but these do not have human dialogue within the game. Similar single player text adventure games have also been used to study referring expressions (Gabsdil et al., 2001, 2002) and parsing (Koller et al., 2004). Yang et al. (2017) and Bordes et al. (2010) also proposed small world setups for instruction following or labeling, but these are much more restricted than the large multi-player text adventure game environment with rich dialogue that we propose here. 3 LIGHT Environment and Task Setup Other examples are instruction-following in the Neverwinter Nig"
D19-1062,D14-1162,0,0.085006,"Missing"
D19-1062,P16-1162,0,0.0102372,"uation are described in more detail in Appendix F along with how many turns were evaluated. Implementation We implement models using PyTorch in ParlAI (Miller et al., 2017). Ranking Transformer models are pretrained on Reddit data (Mazar´e et al., 2018) and fine-tuned. We use the BERT (Devlin et al., 2018) implementation provided by Hugging Face2 with pre-trained weights, then adapted to our Bi-Ranker and Cross-Ranker setups. Generative models are pretrained on the Toronto Books Corpus and fine-tuned except for emote prediction which does not leverage pretraining. We apply byte-pair encoding (Sennrich et al., 2016) to reduce the vocabulary size for generative models. We decode using beam search with beam size 5. 2 Evaluation 5 Results The ranking models are compared in Table 4 on the seen and unseen test sets, and ablations are shown for both the BERT-based Bi-Ranker and https://github.com/huggingface/pytorch-pretrained-BERT 678 Dialogue R@1/20 Test Seen Action Acc Emote Acc Dialogue R@1/20 5.0 23.7 53.8 70.9 76.5 74.9 12.2 20.6 17.8 24.5 42.5 50.7 4.5 7.5 13.2 11.6 17.3 25.0 25.8 5.0 21.8 27.9 66.0 70.5 69.7 12.1 20.5 16.4 21.1 38.6 51.8 4.5 8.46 9.92 9.8 16.6 25.7 28.6 *87.5±2.4 *62.0±3.1 *27.0±2.5 *9"
D19-1062,N15-1020,0,0.0822381,"Missing"
D19-1062,P18-1205,1,0.927894,"hat dialogue agents embodied in a rich and cohesive (but tractable) world can more easily be trained to use language effectively than those only exposed to standard large-scale text-only corpora. To that end, we introduce the LIGHT1 research platform. LIGHT is a multi-player fantasy text adventure world designed for studying situated dialogue, and allows interactions between humans, 1 2 Related Work Most recent work in dialogue exploring generative or retrieval models for goal-directed (Henderson et al., 2014; Bordes et al., 2017) or chitchat tasks (Vinyals and Le, 2015; Sordoni et al., 2015; Zhang et al., 2018) is not situated, or even Learning in Interactive Games with Humans and Text. 673 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 673–683, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics Mostafazadeh et al., 2017). While grounded, the agent has no ability to act in these tasks. Talk the Walk (de Vries et al., 2018) introduces a navigation game that involves action, perception and two-way dialogue, but is limited to small grids. In summar"
D19-1062,P17-1086,0,0.0172141,"proposed small world setups for instruction following or labeling, but these are much more restricted than the large multi-player text adventure game environment with rich dialogue that we propose here. 3 LIGHT Environment and Task Setup Other examples are instruction-following in the Neverwinter Nights game (Fleischman and Roy, 2005), studies of emotional response in adventure games (Fraser et al., 2018), dialogue about soccer videogames (Pasunuru and Bansal, 2018), placing blocks appropriately given a final plan (Wang et al., 2016) and a more open ended building task using a grid of voxels (Wang et al., 2017). In the latter two cases the communication is one-sided with only the human issuing instructions, rather than dialogue, with the agent only able to act. LIGHT is a large-scale, configurable text adventure environment for research on learning grounded language and actions. It features both humans and models as agents situated (symbolically) within a multi-player fantasy MUD (multiuser dungeon)-like (Dieterle, 2009) environment. The environment is moderated by a simple game engine which passes dialogue and emote turns between characters and allows actions to cause transitions of the world state"
D19-1062,P16-1224,0,0.042343,"arsing (Koller et al., 2004). Yang et al. (2017) and Bordes et al. (2010) also proposed small world setups for instruction following or labeling, but these are much more restricted than the large multi-player text adventure game environment with rich dialogue that we propose here. 3 LIGHT Environment and Task Setup Other examples are instruction-following in the Neverwinter Nights game (Fleischman and Roy, 2005), studies of emotional response in adventure games (Fraser et al., 2018), dialogue about soccer videogames (Pasunuru and Bansal, 2018), placing blocks appropriately given a final plan (Wang et al., 2016) and a more open ended building task using a grid of voxels (Wang et al., 2017). In the latter two cases the communication is one-sided with only the human issuing instructions, rather than dialogue, with the agent only able to act. LIGHT is a large-scale, configurable text adventure environment for research on learning grounded language and actions. It features both humans and models as agents situated (symbolically) within a multi-player fantasy MUD (multiuser dungeon)-like (Dieterle, 2009) environment. The environment is moderated by a simple game engine which passes dialogue and emote turn"
D19-1062,N19-1423,0,\N,Missing
D19-1203,P17-1171,1,0.897135,"Missing"
D19-1203,N19-1423,0,0.0103631,"he percentage of times the correct movie is among the top k recommendations (hit@k). In order to see the usefulness of dialogue for recommendation, precision is measured per each expert turn of the dialogue (Turn@k) regardless of the decision to speak or recommend, and at the end of the dialogue (Chat@k). Models. We compare our models with Information Retrieval (IR) based models and recommendation-only models. The IR models retrieve the most relevant utterances from the set of candidate responses of the training data and rank them by comparing cosine similarities using TFIDF features or BERT (Devlin et al., 2019) encoder features. Note that IR models make no recommendation. The recommendation-only models 13 https://spacy.io/ 1956 https://github.com/facebookresearch/ParlAI Decision F1 BLEU Turn@1 Turn@3 Chat@1 Chat@3 Acc Baseline Recommendation TFIDF-Ranker BERT-Ranker Random Recc. BERT Recc. 32.5 38.3 3.6 16.5 27.8 23.9 0.1 0.2 21.3 25.5 59.2 66.3 23.1 26.4 62.2 68.3 - Ours Generation Generate +predict +Decide +Plan 39.5 40.2 41.0 40.9 26.0 26.4 27.4 26.8 76.4 77.8 76.3 96.9 97.1 95.7 75.7 78.2 77.5 97.0 97.7 97.6 67.6 53.6 Table 2: Evaluation on supervised models. We incrementally add different aspec"
D19-1203,P17-1162,1,0.815206,"re-trained models (bot-play), in order to achieve the dialogue goal. Our experiments show that models finetuned with bot-play learn improved dialogue strategies, reach the dialogue goal more often when paired with a human, and are rated as more consistent by humans compared to models trained without bot-play. The dataset and code are publicly available through the ParlAI framework1 . 1 Introduction Traditional recommendation systems factorize users’ historical data (i.e., ratings on movies) to extract common preference patterns (Koren et al., 1 https://github.com/facebookresearch/ParlAI 2009; He et al., 2017b). However, besides making it difficult to accommodate new users because of the cold-start problem, relying on aggregated history makes these systems static, and prevents users from making specific requests, or exploring a temporary interest. For example, a user who usually likes horror movies, but is in the mood for a fantasy movie, has no way to indicate their preference to the system, and would likely get a recommendation that is not useful. Further, they cannot iterate upon initial recommendations with clarifications or modified requests, all of which are best specified in natural languag"
D19-1203,D18-1256,1,0.860226,"Missing"
D19-1203,E17-2068,0,0.0573867,"Missing"
D19-1203,D17-1259,0,0.024584,"for j ∈ 1..K, Ldecide = p MLP (dt+1 |ht , c1 , · · · , cK ), (2) (3) (4) (5) with pgen the output distribution of an attentive seq2seq generative model (Bahdanau et al., 2015), p a softmax distribution over dot products ht · mk that capture how aligned the dialogue history ht is with the description mk of the k-th movie, and p MLP the output distribution of a multi-layer perceptron predictor that takes c1 , · · · , cK as inputs8 . 3.2 Bot-Play Motivated by the recent success of self-play in strategic games (Silver et al., 2017; Vinyals et al., 2019; OpenAI, 2018) and in negotiation dialogues (Lewis et al., 2017), we show in this section how we construct a reward function to perform botplay between two bots in our setting, with the aim of developing a better expert dialogue agent for recommendation. Plan optimizes long-term policies of the various aspects over multiple turns of the dialogue game by maximizing game-specific rewards. We 8 We experimented with various other encoding functions, detailed in the Appendix. first pre-train expert and seeker models individually: the expert model Mexpert (θ) = minθ L sup is pre-trained by minimizing the supervised loss in Eq 1, and the seeker model M seeker (φ)"
D19-1203,D17-2014,1,0.860206,"the seeker set and the correct movie. We filter out movie sets that are too difficult or 6 We also tried a classical matrix-factorization based recommendation model, which shows comparable performance to the embedding model. Figure 3: Movie set selection: watched movies for seeker (grey) and correct (light blue) / incorrect (light red) movies for expert. easy for the recommendation task (see Appendix), and choose 10,000 pairs of seeker-expert movie sets at random. 2.3 Data Collection For each dialogue game, a movie set is randomly chosen without duplication. We collect dialogues using ParlAI (Miller et al., 2017) to interface with Amazon Mechanical Turk. More details about data collection are included in the Appendix. Table 1 shows detailed statistics of our dataset regarding the movie sets, the annotated dialogues, actions made by expert and seeker, dialogue 1953 Dialogue statistics Number of dialogues Number of utterances Number of unique utterances Avg length of a dialogue Avg duration (minutes) of a dialogue 9,125 170,904 85,208 23.0 5.2 Expert’s utterance statistics Avg utterance length Unique tokens Unique utterances 8.40 11,757 40,550 Seeker’s utterance statistics Avg utterance length Unique to"
D19-1203,D16-1147,1,0.819616,"money is given if the expert recommends the correct movie, or if the seeker accepts the correct movie or rejects an incorrect one. 2.2 Picking Expert and Seeker movie sets This section describes how movie sets are selected for experts and seekers. Pool of movies To reflect movie preferences of real users, our dataset uses the MovieLens dataset4 , comprising 27M ratings applied to 58K movies by 280K real users. We obtain descriptive text for each movie from Wikipedia5 (i.e., the first paragraph). We also extract entity-level features (e.g., directors, actors, year) using the MovieWiki dataset (Miller et al., 2016) (See Figure 1). We filter out less frequent movies and user profiles (see Appendix), resulting in a set of 5,330 movies and 65,181 user profiles with their ratings. Movie similarity metric In order to simulate a natural setting, the movies in the seeker’s set 3 Our model doesn’t utilize this or the engagingness scores for learning, but these are potential future directions. 4 https://grouplens.org/datasets/movielens/ 5 https://dumps.wikimedia.org/ 1952 Seeker Rushmore 1998 Comedy, Drama Reservoir Dogs 1992 Crime, Mystery, Thriller Election 1999 Comedy Big Fish 2003 Drama, Fantasy, Romance Van"
D19-1203,P02-1040,0,0.103658,"text (Joulin et al., 2017) embeddings, [1, 2] layers of [256, 512]-dimensional Uni/Bi-directional LSTMs (Hochreiter and Schmidhuber, 1997) with 0.1 dropout ratio, and soft attention (Bahdanau et al., 2015). At decoding, we use beam search with a beam of size 3, and choose the maximum likelihood output. For each turn, the initial 12 Evaluation of Supervised Models Metrics. We first evaluate our supervised models on the three supervised tasks: dialogue generation, movie recommendation, and per-turn decision to speak or recommend. The dialogue generation is evaluated using the F1 score and BLEU (Papineni et al., 2002) comparing the predicted and ground-truth utterances. The F1 score is computed at token-level. The recommendation model is evaluated by calculating the percentage of times the correct movie is among the top k recommendations (hit@k). In order to see the usefulness of dialogue for recommendation, precision is measured per each expert turn of the dialogue (Turn@k) regardless of the decision to speak or recommend, and at the end of the dialogue (Chat@k). Models. We compare our models with Information Retrieval (IR) based models and recommendation-only models. The IR models retrieve the most relev"
D19-1203,D14-1162,0,0.0831648,"Missing"
D19-1203,P13-2089,0,0.309937,"ommodate new users because of the cold-start problem, relying on aggregated history makes these systems static, and prevents users from making specific requests, or exploring a temporary interest. For example, a user who usually likes horror movies, but is in the mood for a fantasy movie, has no way to indicate their preference to the system, and would likely get a recommendation that is not useful. Further, they cannot iterate upon initial recommendations with clarifications or modified requests, all of which are best specified in natural language. Recommending through dialogue interactions (Reschke et al., 2013; W¨arnestål, 2005) offers a promising solution to these problems, and recent work by Li et al. (2018) explores this approach in detail. However, the dataset introduced in that work does not capture higher-level strategic behaviors that can impact the quality of the recommendation made (for example, it may be better to elicit user preferences first, before making a recommendation). This makes it difficult for models trained on this data to learn optimal recommendation strategies. Additionally, the recommendations are not grounded in real observed movie preferences, which may make trained model"
D19-1203,P16-1162,0,0.0510524,"Missing"
D19-1203,P18-1205,1,0.866889,"ulated) recommendation and decision metrics (Turn@1/Accuracy) at the bottom14 . The accumulated recommendation and decision performance sharply rises at the end of the dialogue and variance decreases. The generation performance increases, because longer dialogue contexts helps predict the correct utterances. 14 For better understanding of the effect of recommendation and decision, we show accumulated values, and per-turn values for generation. 1957 sure automatic metrics as well as dialogue quality scores provided by the player: fluency, consistency, and engagingness (scored between 1 and 5) (Zhang et al., 2018). We use the full test set (i.e., 911 movie sets) for bot-bot games and use 20 random samples from the test set for {bot,human}human games. Models. We compare our best supervised model with several variants of our fine-tuned bot-play models. We consider bot-play of an expert model with different seeker models such as BERTRanker based seeker and Seq-to-Seq based seeker. Each bot-play model is trained on the same train set that is used for training the original supervised model. The seeker model uses retrieval based on BERT pretrained representations of dialogue context (BERT-R) 15 . (a) Rank of"
D19-1244,N19-1264,0,0.0485276,"Missing"
D19-1244,P17-1020,0,0.0218616,"scussed in §4.1 on (dis)similarities between patterns learned by humans and neural networks. Evidence Extraction Various papers have explored the related problem of extracting evidence or summaries to aid downstream QA. Wang et al. (2018a) concurrently introduced a neural model that extracts evidence specifically for the correct answer, as an intermediate step in a QA pipeline. Prior work uses similar methods to explain what a specific model has learned (Lei et al., 2016; Li et al., 2016; Yu et al., 2019). Others extract evidence to improve downstream QA efficiency over large amounts of text (Choi et al., 2017; Kratzwald and Feuerriegel, 2019; Wang et al., 2018b). More broadly, extracting evidence can facilitate fact verification (Thorne et al., 2018) and debate.2 2 IBM Project Debater: www.research.ibm.com/ artificial-intelligence/project-debater 2409 Generic Summarization In contrast, various papers focus primarily on summarization rather than QA, using downstream QA accuracy only as a reward to optimize generic (question-agnostic) summarization models (Arumae and Liu, 2018, 2019; Eyal et al., 2019). Debate Evidence extraction can be viewed as a form of debate, in which multiple agents support di"
D19-1244,N19-1423,0,0.0404619,"rity between two using Adam (Kingma and Ba, 2015) on one loss texts (Perone et al., 2018). Using this funcfrom Table 1. For t &gt; 1, we find it effective to tion, we define a model class that selects the ansimply predict the judge model at t = 1 and use swer most similar to the input passage context: this distribution for all time steps during inference. L(i) = fastText(S, A(i)). This trick speeds up training by enabling us to precompute prediction targets using the judge model, BERT L(i) is computed using the multipleinstead of querying it constantly during training. choice adaptation of BERT (Devlin et al., 2019; Radford et al., 2018; Si, 2019), a pre-trained We use BERTBASE for all learned agents. transformer network (Vaswani et al., 2017). We Learned agents predict the BERTBASE judge, as fine-tune all BERT parameters during trainit is more efficient to compute than BERTLARGE . ing. This model predicts L(i) using a trainEach agent AGENT(i) is assigned the answer A(i) able vector v and BERT’s first token embedding: that it should support. We train one learned agent L(i) = v &gt; · BERT([S; Q; A(i)]). to find evidence for an arbitrary answer i. We We experiment with both the BERTBASE model condition AGEN"
D19-1244,N19-1395,0,0.043278,"Missing"
D19-1244,W18-2501,0,0.0315432,"Missing"
D19-1244,D18-1316,0,0.0260321,"Missing"
D19-1244,N18-2017,0,0.0434092,"Missing"
D19-1244,P18-3015,0,0.0467999,"Missing"
D19-1244,D17-1215,0,0.052598,"Missing"
D19-1244,E17-2068,0,0.093309,"Missing"
D19-1244,D16-1011,0,0.0337657,"duction There is great value in understanding the fundamental nature of a question (Chalmers, 2015). Distilling the core of an issue, however, is timeconsuming. Finding the correct answer to a given question may require reading large volumes of text or understanding complex arguments. Here, we examine if we can automatically discover the underlying properties of problems such as question answering by examining how machine learning models learn to solve that task. We examine this question in the context of passage-based question-answering (QA). Inspired by work in interpreting neural networks (Lei et al., 2016), we have agents find a subset of the passage (i.e., supporting evidence) that maximizes a QA model’s probability of a particular answer. Each agent (one agent per answer) finds the sentences that a QA model regards as strong evidence for its answer, using either exhaustive search or learned prediction. Figure 1 shows an example. Figure 1: Evidence agents quote sentences from the passage to convince a question-answering judge model of an answer. To examine to what extent evidence is general and independent of the model, we evaluate if humans and other models find selected evidence to be valid"
D19-1244,D17-2014,1,0.848518,"acy. However, each judge model’s accuracy is useful to know for analysis purposes. Table 2 shows model accuracies, which cover a broad range. BERT models significantly outperform word-based baselines (TFIDF and fastText), and BERTLARGE achieves the best overall accuracy. No model achieves the estimated human ceiling for either RACE (Lai et al., 2017) or DREAM (Sun et al., 2019). Our code is available at https://github. com/ethanjperez/convince. We build off AllenNLP (Gardner et al., 2018) using PyTorch (Paszke et al., 2017). For all human evaluations, we use Amazon Mechanical Turk via ParlAI (Miller et al., 2017). Appendix B describes preprocessing and training details. 4 4.1 Agents Select General Evidence Human Evaluation of Evidence Would evidence that convinces a model also be valid evidence to humans? On one hand, there is ample work suggesting that neural networks can learn similar patterns as humans do. Convolutional networks trained on ImageNet share similarities with the human visual cortex (Cadieu et al., 2014). In machine translation, attention learns to align foreign words with their native counterparts (Bahdanau et al., 2015). On the other hand, neural networks often do not behave as human"
D19-1244,P18-1079,0,0.0321551,"Missing"
D19-1244,Q19-1014,0,0.057546,"convince the judge model when supporting the correct answer (one answer per question). 3.2 Training and Evaluating Models Our setup is not directly comparable to standard QA setups, as we aim to evaluate evidence rather than raw QA accuracy. However, each judge model’s accuracy is useful to know for analysis purposes. Table 2 shows model accuracies, which cover a broad range. BERT models significantly outperform word-based baselines (TFIDF and fastText), and BERTLARGE achieves the best overall accuracy. No model achieves the estimated human ceiling for either RACE (Lai et al., 2017) or DREAM (Sun et al., 2019). Our code is available at https://github. com/ethanjperez/convince. We build off AllenNLP (Gardner et al., 2018) using PyTorch (Paszke et al., 2017). For all human evaluations, we use Amazon Mechanical Turk via ParlAI (Miller et al., 2017). Appendix B describes preprocessing and training details. 4 4.1 Agents Select General Evidence Human Evaluation of Evidence Would evidence that convinces a model also be valid evidence to humans? On one hand, there is ample work suggesting that neural networks can learn similar patterns as humans do. Convolutional networks trained on ImageNet share similari"
D19-1244,N18-1074,0,0.0201572,"the related problem of extracting evidence or summaries to aid downstream QA. Wang et al. (2018a) concurrently introduced a neural model that extracts evidence specifically for the correct answer, as an intermediate step in a QA pipeline. Prior work uses similar methods to explain what a specific model has learned (Lei et al., 2016; Li et al., 2016; Yu et al., 2019). Others extract evidence to improve downstream QA efficiency over large amounts of text (Choi et al., 2017; Kratzwald and Feuerriegel, 2019; Wang et al., 2018b). More broadly, extracting evidence can facilitate fact verification (Thorne et al., 2018) and debate.2 2 IBM Project Debater: www.research.ibm.com/ artificial-intelligence/project-debater 2409 Generic Summarization In contrast, various papers focus primarily on summarization rather than QA, using downstream QA accuracy only as a reward to optimize generic (question-agnostic) summarization models (Arumae and Liu, 2018, 2019; Eyal et al., 2019). Debate Evidence extraction can be viewed as a form of debate, in which multiple agents support different stances (Irving et al., 2018; Irving and Askell, 2019). Chen et al. (2018) show that evidence-based debate improves the accuracy of crow"
D19-1244,D16-1264,0,0.115473,"Missing"
D19-1461,P18-2006,0,0.0254857,"rators (Brock et al., 2018) as well as image classifiers to be robust to adversarial examples (Liu et al., 2019). These methods find the break1 https://parl.ai/projects/dialogue_ safety/ ing examples algorithmically, rather than by using humans breakers as we do. Applying the same approaches to NLP tends to be more challenging because, unlike for images, even small changes to a sentence can cause a large change in the meaning of that sentence, which a human can detect but a lower quality model cannot. Nevertheless algorithmic approaches have been attempted, for example in text classification (Ebrahimi et al., 2018), machine translation (Belinkov and Bisk, 2018), dialogue generation tasks (Li et al., 2017) and reading comprehension (Jia and Liang, 2017). The latter was particularly effective at proposing a more difficult version of the popular SQuAD dataset. As mentioned in the introduction, our approach takes inspiration from “Build it Break it” approaches which have been successfully tried in other domains (Ruef et al., 2016; Ettinger et al., 2017). Those approaches advocate finding faults in systems by having humans look for insecurities (in software) or prediction failures (in models), but do not adv"
D19-1461,D17-1215,0,0.0232744,"1 https://parl.ai/projects/dialogue_ safety/ ing examples algorithmically, rather than by using humans breakers as we do. Applying the same approaches to NLP tends to be more challenging because, unlike for images, even small changes to a sentence can cause a large change in the meaning of that sentence, which a human can detect but a lower quality model cannot. Nevertheless algorithmic approaches have been attempted, for example in text classification (Ebrahimi et al., 2018), machine translation (Belinkov and Bisk, 2018), dialogue generation tasks (Li et al., 2017) and reading comprehension (Jia and Liang, 2017). The latter was particularly effective at proposing a more difficult version of the popular SQuAD dataset. As mentioned in the introduction, our approach takes inspiration from “Build it Break it” approaches which have been successfully tried in other domains (Ruef et al., 2016; Ettinger et al., 2017). Those approaches advocate finding faults in systems by having humans look for insecurities (in software) or prediction failures (in models), but do not advocate an automated approach as we do here. Our work is also closely connected to the “Mechanical Turker Descent” algorithm detailed in (Yang"
D19-1461,E17-2068,0,0.0540289,"Missing"
D19-1461,W18-4401,0,0.0622177,"Missing"
D19-1461,D17-1230,0,0.0395584,"et al., 2019). These methods find the break1 https://parl.ai/projects/dialogue_ safety/ ing examples algorithmically, rather than by using humans breakers as we do. Applying the same approaches to NLP tends to be more challenging because, unlike for images, even small changes to a sentence can cause a large change in the meaning of that sentence, which a human can detect but a lower quality model cannot. Nevertheless algorithmic approaches have been attempted, for example in text classification (Ebrahimi et al., 2018), machine translation (Belinkov and Bisk, 2018), dialogue generation tasks (Li et al., 2017) and reading comprehension (Jia and Liang, 2017). The latter was particularly effective at proposing a more difficult version of the popular SQuAD dataset. As mentioned in the introduction, our approach takes inspiration from “Build it Break it” approaches which have been successfully tried in other domains (Ruef et al., 2016; Ettinger et al., 2017). Those approaches advocate finding faults in systems by having humans look for insecurities (in software) or prediction failures (in models), but do not advocate an automated approach as we do here. Our work is also closely connected to the “Mechan"
D19-1461,S19-2010,0,0.0199373,"e context of a dialogue rather than a sentence without context provides more sophisticated attacks. We show that model architectures that use the dialogue context efficiently perform much bet4537 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 4537–4546, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics ter than systems that do not, where the latter has been the main focus of existing research (Wulczyn et al., 2017; Davidson et al., 2017; Zampieri et al., 2019). Code for our entire build it, break it, fix it algorithm is made open source, complete with model training code and crowdsourcing interface for humans. Our data and trained models will also be made available for the community via ParlAI1 . 2 Related Work The task of detecting offensive language has been studied across a variety of content classes. Perhaps the most commonly studied class is hate speech, but work has also covered bullying, aggression, and toxic comments (Zampieri et al., 2019). To this end, various datasets have been created to benchmark progress in the field. In hate speech d"
D19-1461,P18-1205,1,0.895829,"Missing"
D19-1461,D14-1162,0,0.0830087,"Missing"
D19-1461,W17-5401,0,\N,Missing
N19-1170,D18-1431,0,0.127131,"ributes, and to overall quality judgments. Controllable neural text generation Researchers have proposed several approaches to control aspects of RNN-based natural language generation such as sentiment, length, speaker style and tense (Fan et al., 2018; Ficler and Goldberg, 2017; Ghazvininejad et al., 2017; Hu et al., 2017; Kikuchi et al., 2016; Peng et al., 2018; Wang et al., 2017). In particular, several works use control to tackle the same common sequence-to-sequence problems we address here (particularly genericness and unrelated output), in the context of single-turn response generation (Baheti et al., 2018; Li et al., 2016a, 2017a; Shen et al., 2017; Xing et al., 2017; Zhang et al., 2018a; Zhou et al., 2017). By contrast, we focus on developing controls for, and human evaluation of, multi-turn interactive dialogue – this includes a new method (described in Section 5) to control attributes at the dialogue level rather than the utterance level. In this work, we require a control method that is both general-purpose (one technique to simultaneously control many attributes) and easily tunable (the control setting is adjustable after training). Given these constraints, we study two control methods: c"
N19-1170,W17-5526,0,0.0372956,"Missing"
N19-1170,W18-2706,0,0.190809,"e automatic metrics designed to capture various conversational aspects (engagement, coherence, domain coverage, conversational depth and topical diversity). Though these aspects have some similarity to the aspects studied here, we also focus on lower-level aspects (e.g. avoiding repetition, fluency), to understand how they correspond to both our controllable attributes, and to overall quality judgments. Controllable neural text generation Researchers have proposed several approaches to control aspects of RNN-based natural language generation such as sentiment, length, speaker style and tense (Fan et al., 2018; Ficler and Goldberg, 2017; Ghazvininejad et al., 2017; Hu et al., 2017; Kikuchi et al., 2016; Peng et al., 2018; Wang et al., 2017). In particular, several works use control to tackle the same common sequence-to-sequence problems we address here (particularly genericness and unrelated output), in the context of single-turn response generation (Baheti et al., 2018; Li et al., 2016a, 2017a; Shen et al., 2017; Xing et al., 2017; Zhang et al., 2018a; Zhou et al., 2017). By contrast, we focus on developing controls for, and human evaluation of, multi-turn interactive dialogue – this includes a ne"
N19-1170,W17-4912,0,0.0572139,"s designed to capture various conversational aspects (engagement, coherence, domain coverage, conversational depth and topical diversity). Though these aspects have some similarity to the aspects studied here, we also focus on lower-level aspects (e.g. avoiding repetition, fluency), to understand how they correspond to both our controllable attributes, and to overall quality judgments. Controllable neural text generation Researchers have proposed several approaches to control aspects of RNN-based natural language generation such as sentiment, length, speaker style and tense (Fan et al., 2018; Ficler and Goldberg, 2017; Ghazvininejad et al., 2017; Hu et al., 2017; Kikuchi et al., 2016; Peng et al., 2018; Wang et al., 2017). In particular, several works use control to tackle the same common sequence-to-sequence problems we address here (particularly genericness and unrelated output), in the context of single-turn response generation (Baheti et al., 2018; Li et al., 2016a, 2017a; Shen et al., 2017; Xing et al., 2017; Zhang et al., 2018a; Zhou et al., 2017). By contrast, we focus on developing controls for, and human evaluation of, multi-turn interactive dialogue – this includes a new method (described in Sect"
N19-1170,P17-4008,0,0.358287,"us conversational aspects (engagement, coherence, domain coverage, conversational depth and topical diversity). Though these aspects have some similarity to the aspects studied here, we also focus on lower-level aspects (e.g. avoiding repetition, fluency), to understand how they correspond to both our controllable attributes, and to overall quality judgments. Controllable neural text generation Researchers have proposed several approaches to control aspects of RNN-based natural language generation such as sentiment, length, speaker style and tense (Fan et al., 2018; Ficler and Goldberg, 2017; Ghazvininejad et al., 2017; Hu et al., 2017; Kikuchi et al., 2016; Peng et al., 2018; Wang et al., 2017). In particular, several works use control to tackle the same common sequence-to-sequence problems we address here (particularly genericness and unrelated output), in the context of single-turn response generation (Baheti et al., 2018; Li et al., 2016a, 2017a; Shen et al., 2017; Xing et al., 2017; Zhang et al., 2018a; Zhou et al., 2017). By contrast, we focus on developing controls for, and human evaluation of, multi-turn interactive dialogue – this includes a new method (described in Section 5) to control attributes"
N19-1170,W18-1505,0,0.147489,"conversational depth and topical diversity). Though these aspects have some similarity to the aspects studied here, we also focus on lower-level aspects (e.g. avoiding repetition, fluency), to understand how they correspond to both our controllable attributes, and to overall quality judgments. Controllable neural text generation Researchers have proposed several approaches to control aspects of RNN-based natural language generation such as sentiment, length, speaker style and tense (Fan et al., 2018; Ficler and Goldberg, 2017; Ghazvininejad et al., 2017; Hu et al., 2017; Kikuchi et al., 2016; Peng et al., 2018; Wang et al., 2017). In particular, several works use control to tackle the same common sequence-to-sequence problems we address here (particularly genericness and unrelated output), in the context of single-turn response generation (Baheti et al., 2018; Li et al., 2016a, 2017a; Shen et al., 2017; Xing et al., 2017; Zhang et al., 2018a; Zhou et al., 2017). By contrast, we focus on developing controls for, and human evaluation of, multi-turn interactive dialogue – this includes a new method (described in Section 5) to control attributes at the dialogue level rather than the utterance level. In"
N19-1170,D14-1162,0,0.0839219,"gment via the question “How much did you enjoy talking to this user?” on a scale of 1–4. 4 Baseline model Our baseline model is a 2-layer LSTM sequenceto-sequence model with attention. On any dialogue turn, the input x to the encoder is the entire dialogue history (separated using unique speakeridentifying tokens), with the model’s own persona prepended. Conditioned on this input sequence x, the decoder generates a response y. Except when stated otherwise, all our models decode using beam search with beam size 20. We initialized the word embedding matrix with 300-dimensional GloVe embeddings (Pennington et al., 2014). Using the ParlAI framework (Miller et al., 2017), we pretrained the model on a dataset of 2.5 million Twitter message-response pairs,1 then fine-tuned it on PersonaChat. On the PersonaChat validation set, the baseline model has a perplexity of 26.83 and F1 of 17.02, which would have placed us 4th out of 26 models in the ConvAI2 competition (Dinan et al., 2019). We attempt to improve over this baseline using control. 5 Controllable text generation methods Suppose we have a sequence-to-sequence model which gives P (y|x) = Πt P (yt |x, y1 , . . . , yt−1 ), the conditional probability of a respo"
N19-1170,E17-1042,0,0.0439964,"Missing"
N19-1170,P18-1102,0,0.242313,"ibutes, we consider two simple but general algorithms: conditional training, in which the neural model is conditioned on additional control features, and weighted decoding, in which control features are added to the decoding scoring function at test time only. One major result of our findings is that existing work has ignored the importance of conversational flow, as standard models (i) repeat or contradict previous statements, (ii) fail to balance specificity with genericness, and (iii) fail to balance asking questions with other dialogue acts. Conducting experiments on the PersonaChat task (Zhang et al., 2018b), we obtain significantly higher engagingness scores than the baseline by optimizing control of repetition, specificity and question-asking over multiple turns. Using these findings, our best model matches the performance of the winning entry in the recent NeurIPS ConvAI2 competition (Dinan et al., 2019), which was trained on much 1702 Proceedings of NAACL-HLT 2019, pages 1702–1723 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics more data but had no control (see Section 8.1). Our code, pretrained models, and full chatlogs, are available at http"
N19-1170,P18-1205,1,0.917759,"ibutes, we consider two simple but general algorithms: conditional training, in which the neural model is conditioned on additional control features, and weighted decoding, in which control features are added to the decoding scoring function at test time only. One major result of our findings is that existing work has ignored the importance of conversational flow, as standard models (i) repeat or contradict previous statements, (ii) fail to balance specificity with genericness, and (iii) fail to balance asking questions with other dialogue acts. Conducting experiments on the PersonaChat task (Zhang et al., 2018b), we obtain significantly higher engagingness scores than the baseline by optimizing control of repetition, specificity and question-asking over multiple turns. Using these findings, our best model matches the performance of the winning entry in the recent NeurIPS ConvAI2 competition (Dinan et al., 2019), which was trained on much 1702 Proceedings of NAACL-HLT 2019, pages 1702–1723 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics more data but had no control (see Section 8.1). Our code, pretrained models, and full chatlogs, are available at http"
P07-1071,A00-2018,0,0.0204368,"elative to some verb. Shallow semantic parsing has immediate applications in tasks such as meta-data extraction (e.g. from web documents) and question and answer based systems (e.g. call center systems), amongst others. 3 Previous Work Several authors have already attempted to build machine learning approaches for the semantic rolelabeling problem. In (Gildea and Jurafsky, 2002) the authors presented a statistical approach to learning (for FrameNet), with some success. They proposed to take advantage of the syntactic tree structure that can be predicted by a parser, such as Charniak’s parser (Charniak, 2000). Their aim is, given a node in the parse tree, to assign a semantic role label to the words that are the children of that node. They extract several key types of features from the parse tree to be used in a statistical model for prediction. These same features also proved crucial to subsequent approaches, e.g. (Pradhan et al., 2004). These features include: • The parts of speech and syntactic labels of words and nodes in the tree. • The node’s position (left or right) in relation to the verb. • The syntactic path to the verb in the parse tree. • Whether a node in the parse tree is part of a n"
P07-1071,J02-3001,0,0.187568,"f the numbering system is labeled for each particular verb as so-called frames. Additionally, semantic roles can also be labeled with one of 13 ARGM adjunct labels, such as ARGM-LOC or ARGM-TMP for additional locational or temporal information relative to some verb. Shallow semantic parsing has immediate applications in tasks such as meta-data extraction (e.g. from web documents) and question and answer based systems (e.g. call center systems), amongst others. 3 Previous Work Several authors have already attempted to build machine learning approaches for the semantic rolelabeling problem. In (Gildea and Jurafsky, 2002) the authors presented a statistical approach to learning (for FrameNet), with some success. They proposed to take advantage of the syntactic tree structure that can be predicted by a parser, such as Charniak’s parser (Charniak, 2000). Their aim is, given a node in the parse tree, to assign a semantic role label to the words that are the children of that node. They extract several key types of features from the parse tree to be used in a statistical model for prediction. These same features also proved crucial to subsequent approaches, e.g. (Pradhan et al., 2004). These features include: • The"
P07-1071,P04-1013,0,0.00912017,"y have to be computationally cheap to deal with an enormous quantity of data, e.g. web-based systems process large numbers of documents, whilst interactive human-machine applications require almost instant response. Another issue is the cost of producing labeled training data required for statistical models, which is exacerbated when those models also depend on syntactic features which must them1 Even though some parsers effectively exhibit linear beselves be learnt. havior in sentence length (Ratnaparkhi, 1997), fast statistical To achieve the goal of semantic understanding, parsers such as (Henderson, 2004) still take around 1.5 seconds the current consensus is to divide and conquer the for sentences of length 35 in tests that we made. 560 Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 560–567, c Prague, Czech Republic, June 2007. 2007 Association for Computational Linguistics tactic constituent of a sentence, i.e. annotating the predicate argument structure in text (see for example Figure 1). Because of its nature, role labeling seems to require the syntactic analysis of a sentence before attributing semantic labels. Using this intuition, state-of-"
P07-1071,J93-2004,0,0.0333731,"tecture we employ – followed by experiments that evaluate our method. Finally, we conclude with a summary and discussion of future work. 2 Shallow Semantic Parsing FrameNet (Baker et al., 1998) and the Proposition Bank (Palmer et al., 2005), or PropBank for short, are the two main systems currently developed for semantic role-labeling annotation. We focus here on PropBank. PropBank encodes role labels by semantically tagging the syntactic structures of hand 561 annotated parses of sentences. The current version of the dataset gives semantic tags for the same sentences as in the Penn Treebank (Marcus et al., 1993), which are excerpts from the Wall Street Journal. The central idea is that each verb in a sentence is labeled with its propositional arguments, where the abstract numbered arguments are intended to fill typical roles. For example, ARG0 is typically the actor, and ARG1 is typically the thing acted upon. The precise usage of the numbering system is labeled for each particular verb as so-called frames. Additionally, semantic roles can also be labeled with one of 13 ARGM adjunct labels, such as ARGM-LOC or ARGM-TMP for additional locational or temporal information relative to some verb. Shallow s"
P07-1071,J05-1004,0,0.260265,"and Jurafsky, 2002). Our method instead learns a direct mapping from source sentence to semantic tags for a given predicate without the aid of a parser or a chunker. Our resulting system obtains accuracies comparable to the current state-of-the-art at a fraction of the computational cost. 1 Introduction Jason Weston NEC Laboratories America, Inc. 4 Independence Way Suite 200, Princeton, NJ 08540 jasonw@nec-labs.com [The company]ARG0 [bought]REL [sugar]ARG1 [on the world market]ARGM-LOC [to meet export commitments]ARGM-PNC Figure 1: Example of Semantic Role Labeling from the PropBank dataset (Palmer et al., 2005). ARG0 is typically an actor, REL an action, ARG1 an object, and ARGM describe various modifiers such as location (LOC) and purpose (PNC). problem. Researchers tackle several layers of processing tasks ranging from the syntactic, such as part-of-speech labeling and parsing, to the semantic: word-sense disambiguation, semantic role-labeling, named entity extraction, co-reference resolution and entailment. None of these tasks are end goals in themselves but can be seen as layers of feature extraction that can help in a language-based end application, such as the ones described above. Unfortunate"
P07-1071,N04-1030,0,0.0945864,"utational Linguistics, pages 560–567, c Prague, Czech Republic, June 2007. 2007 Association for Computational Linguistics tactic constituent of a sentence, i.e. annotating the predicate argument structure in text (see for example Figure 1). Because of its nature, role labeling seems to require the syntactic analysis of a sentence before attributing semantic labels. Using this intuition, state-of-the-art systems first build a parse tree, and syntactic constituents are then labeled by feeding hand-built features extracted from the parse tree to a machine learning system, e.g. the ASSERT system (Pradhan et al., 2004). This is rather slow, taking a few seconds per sentence at test time, partly because of the parse tree component, and partly because of the use of Support Vector Machines (Boser et al., 1992), which have linear complexity in testing time with respect to the number of training examples. This makes it hard to apply this method to interesting end user applications. Here, we propose a radically different approach that avoids the more complex task of building a full parse tree. From a machine learning point of view, a human does not need to be taught about parse trees to talk. It is possible, howe"
P07-1071,W05-0639,0,0.0231851,"o alleviate the burden of parse tree computation, several attempts have been made to remove the full 562 parse tree information from the semantic role labeling system, in fact the shared task of CONLL 2004 was devoted to this goal, but the results were not completely satisfactory. Previously, in (Gildea and Palmer, 2001), the authors tried to show that the parse tree is necessary for good generalization by showing that segments derived from a shallow syntactic parser or chunker do not perform as well for this goal. A further analysis of using chunkers, with improved results was also given in (Punyakanok et al., 2005), but still concluded the full parse tree is most useful. 4 Neural Network Architecture Ideally, we want an end-to-end fast learning system to output semantic roles for syntactic constituents without using a time consuming parse tree. Also, as explained before, we are interesting in exploring whether machine learning approaches can learn structure implicitly. Hence, even if there is a deep relationship between syntax and semantics, we prefer to avoid hand-engineered features that exploit this, and see if we can develop a model that can learn these features instead. We are thus not interested i"
P07-1071,P98-1013,0,0.0106384,"stem also provides semantic tags at a fraction of the computational cost of other methods, taking on average 0.02 seconds to label a sentence from the Penn Treebank, with almost no loss in accuracy. The rest of the article is as follows. First, we describe the problem of shallow semantic parsing in more detail, as well as existing solutions to this problem. We then detail our algorithmic approach – the neural network architecture we employ – followed by experiments that evaluate our method. Finally, we conclude with a summary and discussion of future work. 2 Shallow Semantic Parsing FrameNet (Baker et al., 1998) and the Proposition Bank (Palmer et al., 2005), or PropBank for short, are the two main systems currently developed for semantic role-labeling annotation. We focus here on PropBank. PropBank encodes role labels by semantically tagging the syntactic structures of hand 561 annotated parses of sentences. The current version of the dataset gives semantic tags for the same sentences as in the Penn Treebank (Marcus et al., 1993), which are excerpts from the Wall Street Journal. The central idea is that each verb in a sentence is labeled with its propositional arguments, where the abstract numbered"
P07-1071,W97-0301,0,0.032083,"r based systems, as well as machine translation, summarization and search. Such applications typically have to be computationally cheap to deal with an enormous quantity of data, e.g. web-based systems process large numbers of documents, whilst interactive human-machine applications require almost instant response. Another issue is the cost of producing labeled training data required for statistical models, which is exacerbated when those models also depend on syntactic features which must them1 Even though some parsers effectively exhibit linear beselves be learnt. havior in sentence length (Ratnaparkhi, 1997), fast statistical To achieve the goal of semantic understanding, parsers such as (Henderson, 2004) still take around 1.5 seconds the current consensus is to divide and conquer the for sentences of length 35 in tests that we made. 560 Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 560–567, c Prague, Czech Republic, June 2007. 2007 Association for Computational Linguistics tactic constituent of a sentence, i.e. annotating the predicate argument structure in text (see for example Figure 1). Because of its nature, role labeling seems to require the s"
P07-1071,C98-1013,0,\N,Missing
P07-1071,P02-1031,0,\N,Missing
P14-1136,P98-1013,0,0.728343,"frame identification, namely the disambiguation of a given predicate to a frame, and argument identification (or semantic role labeling), the analysis of words and phrases in the sentential context that satisfy the frame’s semantic roles (Das et al., 2010; Das et al., 2014).1 Here, we focus on the first subtask of frame identification for given predicates; we use our novel method (§3) in conjunction with a standard argument identification model (§4) to perform full frame-semantic parsing. We present experiments on two tasks. First, we show that for frame identification on the FrameNet corpus (Baker et al., 1998; Fillmore et al., 2003), we outperform the prior state of the art (Das et al., 2014). Moreover, for full frame-semantic parsing, with the presented frame identification technique followed by our argument identification method, we report the best results on this task to date. Second, we present results on PropBank-style semantic role labeling (Palmer et al., 2005; Meyers et al., 2004; M`arquez et al., 2008), that approach strong baselines, and are on par with prior state of the art (Punyakanok et al., 2008). 2 Overview Early work in frame-semantic analysis was pioneered by Gildea and Jurafsky"
P14-1136,S07-1018,0,0.244314,"and are on par with prior state of the art (Punyakanok et al., 2008). 2 Overview Early work in frame-semantic analysis was pioneered by Gildea and Jurafsky (2002). Subsequent work in this area focused on either the FrameNet or PropBank frameworks, and research on the latter has been more popular. Since the CoNLL 2004-2005 shared tasks (Carreras and M`arquez, 1 There are exceptions, wherein the task has been modeled using a pipeline of three classifiers that perform frame identification, a binary stage that classifies candidate arguments, and argument identification on the filtered candidates (Baker et al., 2007; Johansson and Nugues, 2007). COMMERCE_BUY buy.01 buy.V buy.V John bought a car . Buyer Goods John bought a car . A0 A1 COMMERCE_BUY sell.01 sell.V sell.V Mary sold Seller a car . Goods Mary sold A0 (a) a car . A1 (b) Figure 1: Example sentences with frame-semantic analyses. FrameNet annotation conventions are used in (a) while (b) denotes PropBank conventions. 2004; Carreras and M`arquez, 2005) on PropBank semantic role labeling (SRL), it has been treated as an important NLP problem. However, research has mostly focused on argument analysis, skipping the frame disambiguation step, and its in"
P14-1136,W04-2412,0,0.084247,"Missing"
P14-1136,W05-0620,0,0.04886,"Missing"
P14-1136,J08-2001,0,0.0329993,"Missing"
P14-1136,W04-2705,0,0.205383,"3) in conjunction with a standard argument identification model (§4) to perform full frame-semantic parsing. We present experiments on two tasks. First, we show that for frame identification on the FrameNet corpus (Baker et al., 1998; Fillmore et al., 2003), we outperform the prior state of the art (Das et al., 2014). Moreover, for full frame-semantic parsing, with the presented frame identification technique followed by our argument identification method, we report the best results on this task to date. Second, we present results on PropBank-style semantic role labeling (Palmer et al., 2005; Meyers et al., 2004; M`arquez et al., 2008), that approach strong baselines, and are on par with prior state of the art (Punyakanok et al., 2008). 2 Overview Early work in frame-semantic analysis was pioneered by Gildea and Jurafsky (2002). Subsequent work in this area focused on either the FrameNet or PropBank frameworks, and research on the latter has been more popular. Since the CoNLL 2004-2005 shared tasks (Carreras and M`arquez, 1 There are exceptions, wherein the task has been modeled using a pipeline of three classifiers that perform frame identification, a binary stage that classifies candidate arguments"
P14-1136,P08-1028,0,0.0174848,"tion method inspired by prior work, we achieve state-ofthe-art results on FrameNet-style framesemantic analysis. Additionally, we report strong results on PropBank-style semantic role labeling in comparison to prior work. 1 Introduction Distributed representations of words have proved useful for a number of tasks. By providing richer representations of meaning than what can be encompassed in a discrete representation, such approaches have successfully been applied to tasks such as sentiment analysis (Socher et al., 2011), topic classification (Klementiev et al., 2012) or word-word similarity (Mitchell and Lapata, 2008). We present a new technique for semantic frame identification that leverages distributed word representations. According to the theory of frame semantics (Fillmore, 1982), a semantic frame represents an event or scenario, and possesses frame elements (or semantic roles) that participate in the ∗ The majority of this research was carried out during an internship at Google. event. Most work on frame-semantic parsing has usually divided the task into two major subtasks: frame identification, namely the disambiguation of a given predicate to a frame, and argument identification (or semantic role"
P14-1136,J05-1004,0,0.453818,"e our novel method (§3) in conjunction with a standard argument identification model (§4) to perform full frame-semantic parsing. We present experiments on two tasks. First, we show that for frame identification on the FrameNet corpus (Baker et al., 1998; Fillmore et al., 2003), we outperform the prior state of the art (Das et al., 2014). Moreover, for full frame-semantic parsing, with the presented frame identification technique followed by our argument identification method, we report the best results on this task to date. Second, we present results on PropBank-style semantic role labeling (Palmer et al., 2005; Meyers et al., 2004; M`arquez et al., 2008), that approach strong baselines, and are on par with prior state of the art (Punyakanok et al., 2008). 2 Overview Early work in frame-semantic analysis was pioneered by Gildea and Jurafsky (2002). Subsequent work in this area focused on either the FrameNet or PropBank frameworks, and research on the latter has been more popular. Since the CoNLL 2004-2005 shared tasks (Carreras and M`arquez, 1 There are exceptions, wherein the task has been modeled using a pipeline of three classifiers that perform frame identification, a binary stage that classifie"
P14-1136,N10-1138,1,0.583937,"According to the theory of frame semantics (Fillmore, 1982), a semantic frame represents an event or scenario, and possesses frame elements (or semantic roles) that participate in the ∗ The majority of this research was carried out during an internship at Google. event. Most work on frame-semantic parsing has usually divided the task into two major subtasks: frame identification, namely the disambiguation of a given predicate to a frame, and argument identification (or semantic role labeling), the analysis of words and phrases in the sentential context that satisfy the frame’s semantic roles (Das et al., 2010; Das et al., 2014).1 Here, we focus on the first subtask of frame identification for given predicates; we use our novel method (§3) in conjunction with a standard argument identification model (§4) to perform full frame-semantic parsing. We present experiments on two tasks. First, we show that for frame identification on the FrameNet corpus (Baker et al., 1998; Fillmore et al., 2003), we outperform the prior state of the art (Das et al., 2014). Moreover, for full frame-semantic parsing, with the presented frame identification technique followed by our argument identification method, we report"
P14-1136,J08-2005,0,0.196193,"xperiments on two tasks. First, we show that for frame identification on the FrameNet corpus (Baker et al., 1998; Fillmore et al., 2003), we outperform the prior state of the art (Das et al., 2014). Moreover, for full frame-semantic parsing, with the presented frame identification technique followed by our argument identification method, we report the best results on this task to date. Second, we present results on PropBank-style semantic role labeling (Palmer et al., 2005; Meyers et al., 2004; M`arquez et al., 2008), that approach strong baselines, and are on par with prior state of the art (Punyakanok et al., 2008). 2 Overview Early work in frame-semantic analysis was pioneered by Gildea and Jurafsky (2002). Subsequent work in this area focused on either the FrameNet or PropBank frameworks, and research on the latter has been more popular. Since the CoNLL 2004-2005 shared tasks (Carreras and M`arquez, 1 There are exceptions, wherein the task has been modeled using a pipeline of three classifiers that perform frame identification, a binary stage that classifies candidate arguments, and argument identification on the filtered candidates (Baker et al., 2007; Johansson and Nugues, 2007). COMMERCE_BUY buy.01"
P14-1136,J14-1002,1,0.233868,"heory of frame semantics (Fillmore, 1982), a semantic frame represents an event or scenario, and possesses frame elements (or semantic roles) that participate in the ∗ The majority of this research was carried out during an internship at Google. event. Most work on frame-semantic parsing has usually divided the task into two major subtasks: frame identification, namely the disambiguation of a given predicate to a frame, and argument identification (or semantic role labeling), the analysis of words and phrases in the sentential context that satisfy the frame’s semantic roles (Das et al., 2010; Das et al., 2014).1 Here, we focus on the first subtask of frame identification for given predicates; we use our novel method (§3) in conjunction with a standard argument identification model (§4) to perform full frame-semantic parsing. We present experiments on two tasks. First, we show that for frame identification on the FrameNet corpus (Baker et al., 1998; Fillmore et al., 2003), we outperform the prior state of the art (Das et al., 2014). Moreover, for full frame-semantic parsing, with the presented frame identification technique followed by our argument identification method, we report the best results o"
P14-1136,D11-1014,0,0.012179,"ion. The latter is used for semantic frame identification; with a standard argument identification method inspired by prior work, we achieve state-ofthe-art results on FrameNet-style framesemantic analysis. Additionally, we report strong results on PropBank-style semantic role labeling in comparison to prior work. 1 Introduction Distributed representations of words have proved useful for a number of tasks. By providing richer representations of meaning than what can be encompassed in a discrete representation, such approaches have successfully been applied to tasks such as sentiment analysis (Socher et al., 2011), topic classification (Klementiev et al., 2012) or word-word similarity (Mitchell and Lapata, 2008). We present a new technique for semantic frame identification that leverages distributed word representations. According to the theory of frame semantics (Fillmore, 1982), a semantic frame represents an event or scenario, and possesses frame elements (or semantic roles) that participate in the ∗ The majority of this research was carried out during an internship at Google. event. Most work on frame-semantic parsing has usually divided the task into two major subtasks: frame identification, namel"
P14-1136,P10-1040,0,0.0243059,"in sentential context, we want to disambiguate its frame. (Although PropBank never formally uses the term lexical unit, we adopt its usage from the frame semantics literature.) 2.2 Distributed Frame Identification We present a model that takes word embeddings as input and learns to identify semantic frames. A word embedding is a distributed representation of meaning where each word is represented as a vector in Rn . Such representations allow a model to share meaning between similar words, and have been used to capture semantic, syntactic and morphological content (Collobert and Weston, 2008; Turian et al., 2010, inter alia). We use word embeddings to represent the syntactic context of a particular predicate instance as a vector. For example, consider the sentence “He runs the company.” The predicate runs has two syntactic dependents – a subject and direct object (but no prepositional phrases or clausal complements). We could represent the syntactic context of runs as a vector with blocks for all the possible dependents warranted by a syntactic parser; for example, we could assume that positions 0 . . . n in the vector correspond to the subject dependent, n+1 . . . 2n correspond to the clausal comple"
P14-1136,P08-1086,0,0.0271771,"fe and Manning, 2013) and uses an arc-eager transition system with beam size of 8; the parser and its features are described by Zhang and Nivre (2011). Before parsing the data, it is tagged with a POS tagger trained with a conditional random field (Lafferty et al., 2001) with the following emission features: word, the word cluster, word suffixes of length 1, 2 and 3, capitalization, whether it has a hyphen, digit and punctuation. Beyond the bias transition feature, we have two cluster features for the left and right words in the transition. We use Brown clusters learned using the algorithm of Uszkoreit and Brants (2008) on a large English newswire corpus for cluster features. We use the same word clusters for the argument identification features in Table 1. We learn the initial embedding representations for our frame identification model (§3) using a deep neural language model similar to the one proposed by Bengio et al. (2003). We use 3 hidden layers each with 1024 neurons and learn a 128dimensional embedding from a large corpus containing over 100 billion tokens. In order to speed up learning, we use an unnormalized output layer and a hinge-loss objective. The objective tries to ensure that the correct wor"
P14-1136,J02-3001,0,0.444782,"(Baker et al., 1998; Fillmore et al., 2003), we outperform the prior state of the art (Das et al., 2014). Moreover, for full frame-semantic parsing, with the presented frame identification technique followed by our argument identification method, we report the best results on this task to date. Second, we present results on PropBank-style semantic role labeling (Palmer et al., 2005; Meyers et al., 2004; M`arquez et al., 2008), that approach strong baselines, and are on par with prior state of the art (Punyakanok et al., 2008). 2 Overview Early work in frame-semantic analysis was pioneered by Gildea and Jurafsky (2002). Subsequent work in this area focused on either the FrameNet or PropBank frameworks, and research on the latter has been more popular. Since the CoNLL 2004-2005 shared tasks (Carreras and M`arquez, 1 There are exceptions, wherein the task has been modeled using a pipeline of three classifiers that perform frame identification, a binary stage that classifies candidate arguments, and argument identification on the filtered candidates (Baker et al., 2007; Johansson and Nugues, 2007). COMMERCE_BUY buy.01 buy.V buy.V John bought a car . Buyer Goods John bought a car . A0 A1 COMMERCE_BUY sell.01 se"
P14-1136,S07-1048,0,0.0165202,"prior state of the art (Punyakanok et al., 2008). 2 Overview Early work in frame-semantic analysis was pioneered by Gildea and Jurafsky (2002). Subsequent work in this area focused on either the FrameNet or PropBank frameworks, and research on the latter has been more popular. Since the CoNLL 2004-2005 shared tasks (Carreras and M`arquez, 1 There are exceptions, wherein the task has been modeled using a pipeline of three classifiers that perform frame identification, a binary stage that classifies candidate arguments, and argument identification on the filtered candidates (Baker et al., 2007; Johansson and Nugues, 2007). COMMERCE_BUY buy.01 buy.V buy.V John bought a car . Buyer Goods John bought a car . A0 A1 COMMERCE_BUY sell.01 sell.V sell.V Mary sold Seller a car . Goods Mary sold A0 (a) a car . A1 (b) Figure 1: Example sentences with frame-semantic analyses. FrameNet annotation conventions are used in (a) while (b) denotes PropBank conventions. 2004; Carreras and M`arquez, 2005) on PropBank semantic role labeling (SRL), it has been treated as an important NLP problem. However, research has mostly focused on argument analysis, skipping the frame disambiguation step, and its interaction with argument ident"
P14-1136,W04-3212,0,0.0777944,"search for the stochastic gradient learning rate in {0.0001, 0.001, 0.01}, the margin γ ∈ {0.001, 0.01, 0.1, 1} and the dimensionality of the final vector space m ∈ {256, 512}, to maximize the frame identification accuracy of ambiguous lexical units; by ambiguous, we imply lexical units that appear in the training data or the lexicon with more than one semantic frame. The underlined values are the chosen hyperparameters used to analyze the test data. Argument Candidates The candidate argument extraction method used for the FrameNet data, (as mentioned in §4) was adapted from the algorithm of Xue and Palmer (2004) applied to dependency trees. Since the original algorithm was designed for verbs, we added a few extra rules to handle non-verbal predicates: we added 1) the predicate itself as a candidate argument, 2) the span ranging from the sentence position to the right of the predicate to the rightmost index of the subtree headed by the predicate’s head; this helped capture cases like “a few months” (where few is the predicate and months is the argument), and 3) the span ranging from the leftmost index of the subtree headed by the predicate’s head to the position immediately before the predicate, for c"
P14-1136,C12-1089,0,0.00922816,"dentification; with a standard argument identification method inspired by prior work, we achieve state-ofthe-art results on FrameNet-style framesemantic analysis. Additionally, we report strong results on PropBank-style semantic role labeling in comparison to prior work. 1 Introduction Distributed representations of words have proved useful for a number of tasks. By providing richer representations of meaning than what can be encompassed in a discrete representation, such approaches have successfully been applied to tasks such as sentiment analysis (Socher et al., 2011), topic classification (Klementiev et al., 2012) or word-word similarity (Mitchell and Lapata, 2008). We present a new technique for semantic frame identification that leverages distributed word representations. According to the theory of frame semantics (Fillmore, 1982), a semantic frame represents an event or scenario, and possesses frame elements (or semantic roles) that participate in the ∗ The majority of this research was carried out during an internship at Google. event. Most work on frame-semantic parsing has usually divided the task into two major subtasks: frame identification, namely the disambiguation of a given predicate to a f"
P14-1136,P11-2033,0,0.00588532,"decouple the W SABIE training from the embedding input, and trains a log linear model using the embeddings. So the second baseline has the same input representation as W SABIE E MBEDDING but uses a log-linear model instead of W SABIE. We call this model L OG -L INEAR E MBED DING . 5.3 Common Experimental Setup We process our PropBank and FrameNet training, development and test corpora with a shift-reduce dependency parser that uses the Stanford conventions (de Marneffe and Manning, 2013) and uses an arc-eager transition system with beam size of 8; the parser and its features are described by Zhang and Nivre (2011). Before parsing the data, it is tagged with a POS tagger trained with a conditional random field (Lafferty et al., 2001) with the following emission features: word, the word cluster, word suffixes of length 1, 2 and 3, capitalization, whether it has a hyphen, digit and punctuation. Beyond the bias transition feature, we have two cluster features for the left and right words in the transition. We use Brown clusters learned using the algorithm of Uszkoreit and Brants (2008) on a large English newswire corpus for cluster features. We use the same word clusters for the argument identification fea"
P14-1136,C98-1013,0,\N,Missing
P15-1137,D13-1203,0,0.841381,"ge by pre-training on a pair of corresponding subtasks. Although we use only simple, unconjoined features, the model is able to learn useful representations, and we report the best overall score on the CoNLL 2012 English test set to date. 1 Introduction One of the major challenges associated with resolving coreference is that in typical documents the number of mentions (syntactic units capable of referring or being referred to) that are nonanaphoric – that is, that are not coreferent with any previous mention – far exceeds the number of mentions that are anaphoric (Kummerfeld and Klein, 2013; Durrett and Klein, 2013). This preponderance of non-anaphoric mentions makes coreference resolution challenging, partly because many basic coreference features, such as those looking at head, number, or gender match fail to distinguish between truly coreferent pairs and the large number of matching but nonetheless non-coreferent pairs. Indeed, several authors have noted that it is difficult to obtain good performance on the coreference task using simple features (Lee et al., 2011; Fernandes et al., 2012; Durrett and Klein, 2013; Kummerfeld and Klein, 2013; Bj¨orkelund and Kuhn, 2014) and, as a result, state-of-the-ar"
P15-1137,Q14-1037,0,0.108586,"Missing"
P15-1137,D08-1031,0,0.0330564,"1.5 points over the state-of-the-art coreference system. Moreover, unlike current state-ofthe-art systems, our model does only local inference, and is therefore significantly simpler. 1.1 Problem Setting We consider here the mention-ranking (or “mention-synchronous”) approach to coreference 1416 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 1416–1426, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics resolution (Denis and Baldridge, 2008; Bengtson and Roth, 2008; Rahman and Ng, 2009), which has been adopted by several recent coreference systems (Durrett and Klein, 2013; Chang et al., 2013). Such systems aim to identify whether a mention is coreferent with an antecedent mention, or whether it is instead non-anaphoric (the first mention in the document referring to a particular entity). This is accomplished by assigning a score to the mention’s potential antecedents as well as to the possibility that it is non-anaphoric, and then predicting the greatest scoring option. We furthermore assume the more realistic “system mention” setting, where it is not k"
P15-1137,W12-4503,0,0.0829107,"Missing"
P15-1137,P14-1005,0,0.290072,"Missing"
P15-1137,D13-1057,0,0.0221907,"inference, and is therefore significantly simpler. 1.1 Problem Setting We consider here the mention-ranking (or “mention-synchronous”) approach to coreference 1416 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 1416–1426, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics resolution (Denis and Baldridge, 2008; Bengtson and Roth, 2008; Rahman and Ng, 2009), which has been adopted by several recent coreference systems (Durrett and Klein, 2013; Chang et al., 2013). Such systems aim to identify whether a mention is coreferent with an antecedent mention, or whether it is instead non-anaphoric (the first mention in the document referring to a particular entity). This is accomplished by assigning a score to the mention’s potential antecedents as well as to the possibility that it is non-anaphoric, and then predicting the greatest scoring option. We furthermore assume the more realistic “system mention” setting, where it is not known a priori which mentions in a document participate in coreference clusters, and so (all) mentions must be automatically extrac"
P15-1137,D08-1069,0,0.0483972,"et al., 2012), and of over 1.5 points over the state-of-the-art coreference system. Moreover, unlike current state-ofthe-art systems, our model does only local inference, and is therefore significantly simpler. 1.1 Problem Setting We consider here the mention-ranking (or “mention-synchronous”) approach to coreference 1416 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 1416–1426, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics resolution (Denis and Baldridge, 2008; Bengtson and Roth, 2008; Rahman and Ng, 2009), which has been adopted by several recent coreference systems (Durrett and Klein, 2013; Chang et al., 2013). Such systems aim to identify whether a mention is coreferent with an antecedent mention, or whether it is instead non-anaphoric (the first mention in the document referring to a particular entity). This is accomplished by assigning a score to the mention’s potential antecedents as well as to the possibility that it is non-anaphoric, and then predicting the greatest scoring option. We furthermore assume the more realistic “system mention” s"
P15-1137,N07-1030,0,0.0359765,"Missing"
P15-1137,N06-2015,0,0.277262,"Missing"
P15-1137,W04-3250,0,0.0267784,"Missing"
P15-1137,D13-1027,0,0.0563879,"nt ranking, which we encourage by pre-training on a pair of corresponding subtasks. Although we use only simple, unconjoined features, the model is able to learn useful representations, and we report the best overall score on the CoNLL 2012 English test set to date. 1 Introduction One of the major challenges associated with resolving coreference is that in typical documents the number of mentions (syntactic units capable of referring or being referred to) that are nonanaphoric – that is, that are not coreferent with any previous mention – far exceeds the number of mentions that are anaphoric (Kummerfeld and Klein, 2013; Durrett and Klein, 2013). This preponderance of non-anaphoric mentions makes coreference resolution challenging, partly because many basic coreference features, such as those looking at head, number, or gender match fail to distinguish between truly coreferent pairs and the large number of matching but nonetheless non-coreferent pairs. Indeed, several authors have noted that it is difficult to obtain good performance on the coreference task using simple features (Lee et al., 2011; Fernandes et al., 2012; Durrett and Klein, 2013; Kummerfeld and Klein, 2013; Bj¨orkelund and Kuhn, 2014) and, as"
P15-1137,P13-1049,0,0.0594376,"Missing"
P15-1137,J13-4004,0,0.205374,"Missing"
P15-1137,P14-2005,0,0.101775,"Missing"
P15-1137,H05-1004,0,0.414638,"Missing"
P15-1137,D14-1225,0,0.389411,"Missing"
P15-1137,C02-1139,0,0.844818,"Missing"
P15-1137,P04-1020,0,0.110193,"presentations for natural language tasks (Collobert et al., 2011), we explore neural network models which take only raw, unconjoined features as input, and attempt to learn intermediate representations automatically. In particular, the model we describe attempts to create independent feature representations useful for both detecting the anaphoricity of a mention (that is, whether or not a mention is anaphoric) and ranking the potential antecedents of an anaphoric mention. Adequately capturing anaphoricity information has long been thought to be an important aspect of the coreference task (see Ng (2004) and Section 7), since a strong non-anaphoric signal might, for instance, discourage the erroneous prediction of an antecedent for a non-anaphoric mention even in the presence of a misleading head match. We furthermore attempt to encourage the learning of the desired feature representations by pretraining the model’s weights on two corresponding subtasks, namely, anaphoricity detection and antecedent ranking of known anaphoric mentions. Overall our best model has an absolute gain of almost 2 points in CoNLL score over a similar but linear mention-ranking model on the CoNLL 2012 English test se"
P15-1137,W12-4501,0,0.684515,"d Section 7), since a strong non-anaphoric signal might, for instance, discourage the erroneous prediction of an antecedent for a non-anaphoric mention even in the presence of a misleading head match. We furthermore attempt to encourage the learning of the desired feature representations by pretraining the model’s weights on two corresponding subtasks, namely, anaphoricity detection and antecedent ranking of known anaphoric mentions. Overall our best model has an absolute gain of almost 2 points in CoNLL score over a similar but linear mention-ranking model on the CoNLL 2012 English test set (Pradhan et al., 2012), and of over 1.5 points over the state-of-the-art coreference system. Moreover, unlike current state-ofthe-art systems, our model does only local inference, and is therefore significantly simpler. 1.1 Problem Setting We consider here the mention-ranking (or “mention-synchronous”) approach to coreference 1416 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 1416–1426, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics resolution (Denis and Bald"
P15-1137,D09-1101,0,0.0965473,"e-of-the-art coreference system. Moreover, unlike current state-ofthe-art systems, our model does only local inference, and is therefore significantly simpler. 1.1 Problem Setting We consider here the mention-ranking (or “mention-synchronous”) approach to coreference 1416 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 1416–1426, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics resolution (Denis and Baldridge, 2008; Bengtson and Roth, 2008; Rahman and Ng, 2009), which has been adopted by several recent coreference systems (Durrett and Klein, 2013; Chang et al., 2013). Such systems aim to identify whether a mention is coreferent with an antecedent mention, or whether it is instead non-anaphoric (the first mention in the document referring to a particular entity). This is accomplished by assigning a score to the mention’s potential antecedents as well as to the possibility that it is non-anaphoric, and then predicting the greatest scoring option. We furthermore assume the more realistic “system mention” setting, where it is not known a priori which me"
P15-1137,N13-1071,0,0.160357,"Missing"
P15-1137,D11-1014,0,0.0258548,"Missing"
P15-1137,D12-1110,0,0.0274344,"Missing"
P15-1137,J01-4004,0,0.741286,"Missing"
P15-1137,C12-1154,0,0.0202981,"Missing"
P15-1137,M95-1005,0,0.672142,"Missing"
P15-1137,P12-1040,0,0.0193087,"Missing"
P15-1137,P14-2006,0,0.333577,"Missing"
P15-1137,N07-1011,0,\N,Missing
P15-1137,W12-4502,0,\N,Missing
P15-1137,N10-1061,0,\N,Missing
P15-1137,Q14-1043,0,\N,Missing
P15-1137,D08-1067,0,\N,Missing
P15-1137,W11-1902,0,\N,Missing
P17-1171,W02-1033,0,0.515364,"it as a resource for seeking answers to questions, they focus on validating answers returned by their QA system, and use Wikipedia categories for determining a set of patterns that should fit with the expected answer. In our work, we consider the comprehension of text only, and use Wikipedia text documents as the sole resource in order to emphasize the task of machine reading at scale, as described in the introduction. There are a number of highly developed full pipeline QA approaches using either the Web, as does QuASE (Sun et al., 2015), or Wikipedia as a resource, as do Microsoft’s AskMSR (Brill et al., 2002), IBM’s DeepQA (Ferrucci et al., 2010) and ˇ YodaQA (Baudiˇs, 2015; Baudiˇs and Sediv` y, 2015) — the latter of which is open source and hence reproducible for comparison purposes. AskMSR is a search-engine based QA system that relies on “data redundancy rather than sophisticated linguistic analyses of either questions or candidate answers”, i.e., it does not focus on machine comprehension, as we do. DeepQA is a very sophisticated system that relies on both unstructured information including text documents as well as structured data such as KBs, databases and ontologies to generate candidate a"
P17-1171,S13-1045,0,0.160372,"titive SQuAD benchmark (Rajpurkar et al., 2016). Finally, our full system is evaluated using multiple benchmarks. In particular, we show that performance is improved across all datasets through the use of multitask learning and distant supervision compared to single task training. 2 Related Work Open-domain QA was originally defined as finding answers in collections of unstructured documents, following the setting of the annual TREC competitions1 . With the development of KBs, many recent innovations have occurred in the context of QA from KBs with the creation of resources like WebQuestions (Berant et al., 2013) and SimpleQuestions (Bordes et al., 2015) based on the Freebase KB (Bollacker et al., 2008), or on automatically extracted KBs, e.g., OpenIE triples and NELL (Fader et al., 2014). However, KBs have inherent limitations (incompleteness, fixed schemas) that motivated researchers to return to the original setting of answering from raw text. A second motivation to cast a fresh look at this problem is that of machine comprehension of text, i.e., answering questions after reading a short text or story. That subfield has made considerable progress recently thanks to new deep learning architectures l"
P17-1171,P09-1113,0,0.0249694,"Missing"
P17-1171,D14-1162,0,0.114965,"match features, these features add soft alignments between similar but non-identical words (e.g., car and vehicle). ˜ m }), {p1 , . . . , pm } = RNN({˜ p1 , . . . , p where pi is expected to encode useful context information around token pi . Specifically, we choose to use a multi-layer bidirectional long short-term memory network (LSTM), and take pi as the concatenation of each layer’s hidden units in the end. ˜ i is comprised of the folThe feature vector p lowing parts: • Word embeddings: femb (pi ) = E(pi ). We use the 300-dimensional Glove word embeddings trained from 840B Web crawl data (Pennington et al., 2014). We keep most of the pre-trained word embeddings fixed and only fine-tune the 1000 most frequent question words because the representations of some key words such as what, how, which, many could be crucial for QA systems. • Exact match: fexact match (pi ) = I(pi ∈ q). We use three simple binary features, indicating whether pi can be exactly matched to one question word in q, either in its original, lowercase or lemma form. These simple features turn out to be extremely helpful, as we will show in Section 5. • Token features: ftoken (pi ) = (POS(pi ), NER(pi ), TF(pi )). Question encoding The"
P17-1171,D16-1264,0,0.728652,"tems like IBM’s DeepQA (Ferrucci et al., 2010) rely on multiple sources to answer: besides Wikipedia, it is also paired with KBs, dictionaries, and even news articles, books, etc. As a result, such systems heavily rely on information redundancy among the sources to answer correctly. Having a single knowledge source forces the model to be very precise while searching for an answer as the evidence might appear only once. This challenge thus encourages research in the ability of a machine to read, a key motivation for the machine comprehension subfield and the creation of datasets such as SQuAD (Rajpurkar et al., 2016), CNN/Daily Mail (Hermann et al., 2015) and CBT (Hill et al., 2016). However, those machine comprehension resources typically assume that a short piece of relevant text is already identified and given to the model, which is not realistic for building an opendomain QA system. In sharp contrast, methods that use KBs or information retrieval over documents have to employ search as an integral part of 1870 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 1870–1879 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguis"
P17-1171,D14-1070,0,0.0559824,"Missing"
P17-1171,P14-5010,0,0.00201295,"ndicate that our simple approach outperforms Wikipedia Search, especially with bigram hashing. We also compare doing retrieval with Okapi BM25 or by using cosine distance in the word embeddings space (by encoding questions and articles as bag-of-embeddings), both of which we find performed worse. 5.2 Reader Evaluation on SQuAD Next we evaluate our Document Reader component on the standard SQuAD evaluation (Rajpurkar et al., 2016). Implementation details We use 3-layer bidirectional LSTMs with h = 128 hidden units for both paragraph and question encoding. We apply the Stanford CoreNLP toolkit (Manning et al., 2014) for tokenization and also generating lemma, partof-speech, and named entity tags. Lastly, all the training examples are sorted by the length of paragraph and divided into minibatches of 32 examples each. We use Adamax for optimization as described in (Kingma and Ba, 5 We use the Wikipedia Search API https://www. mediawiki.org/wiki/API:Search. 2014). Dropout with p = 0.3 is applied to word embeddings and all the hidden units of LSTMs. Result and analysis Table 4 presents our evaluation results on both development and test sets. SQuAD has been a very competitive machine comprehension benchmark"
P17-1171,D16-1147,1,0.273339,"iˇs and Sediv` y (2015). We use the large version, which contains a total of 2,180 questions extracted from the datasets from TREC 1999, 2000, 2001 and 2002.4 WebQuestions Introduced in (Berant et al., 2013), this dataset is built to answer questions from the Freebase KB. It was created by crawling questions through the Google Suggest API, and then obtaining answers using Amazon Mechanical Turk. We convert each answer to text by using entity names so that the dataset does not reference Freebase IDs and is purely made of plain text question-answer pairs. WikiMovies This dataset, introduced in (Miller et al., 2016), contains 96k question-answer pairs in the domain of movies. Originally created from the OMDb and MovieLens databases, the examples are built such that they can also be answered by using a subset of Wikipedia as the knowledge source (the title and the first section of articles from the movie domain). 4 This dataset is available at https://github.com/ brmson/dataset-factoid-curated. 1874 Dataset SQuAD Example Q: How many provinces did the Ottoman empire contain in the 17th century? A: 32 CuratedTREC Q: What U.S. state’s motto is “Live free or Die”? A: New Hampshire WebQuestions Q: What part of"
P17-1171,buscaldi-rosso-2006-mining,0,\N,Missing
P17-1171,D13-1160,0,\N,Missing
P17-1171,P16-1223,1,\N,Missing
P17-1171,P16-1145,0,\N,Missing
P18-1205,P17-1171,1,0.794153,"sy nor too difficult for the current technology (Voorhees et al., 1999). One issue with conditioning on textual personas is that there is a danger that humans will, even if asked not to, unwittingly repeat profile information either verbatim or with significant word overlap. This may make any subsequent machine learning tasks less challenging, and the solutions will not generalize to more difficult tasks. This has been a problem in some recent datasets: for example, the dataset curation technique used for the well-known SQuAD dataset suffers from this word overlap problem to a certain extent (Chen et al., 2017). To alleviate this problem, we presented the original personas we collected to a new set of crowdworkers and asked them to rewrite the sentences so that a new sentence is about “a related characteristic that the same person may have”, hence the revisions could be rephrases, generalizations or specializations. For example “I like basketball” can be revised as “I am a big fan of Michael Jordan” not because they mean the same thing but because the same persona could contain both. In the revision task, workers are instructed not to trivially rephrase the sentence by copying the original words. Ho"
P18-1205,P16-1094,0,0.264567,"Missing"
P18-1205,D16-1127,0,0.802794,"1 Introduction Despite much recent success in natural language processing and dialogue research, communication between a human and a machine is still in its infancy. It is only recently that neural models have had sufficient capacity and access to sufficiently large datasets that they appear to generate meaningful responses in a chit-chat setting. Still, conversing with such generic chit-chat models for even a short amount of time quickly exposes their weaknesses (Serban et al., 2016; Vinyals and Le, 2015). Common issues with chit-chat models include: (i) the lack of a consistent personality (Li et al., 2016a) as they are typically trained over many dialogs each with different speakers, (ii) the lack of an explicit long-term memory as they are typically trained to produce an utterance given only the recent dialogue history (Vinyals and Le, 2015); 1 Work done while at Facebook AI Research. and (iii) a tendency to produce non-specific answers like “I don’t know” (Li et al., 2015). Those three problems combine to produce an unsatisfying overall experience for a human to engage with. We believe some of those problems are due to there being no good publicly available dataset for general chit-chat. Bec"
P18-1205,D16-1230,0,0.231377,"which are in Tables 5 and 6 in the Appendix. Using “Their persona” has less impact on this dataset. We believe this is because most speakers tend to focus on themselves when it comes to their interests. It would be interesting how often this is the case in other datasets. Certainly this is skewed by the particular instructions one could give to the crowdworkers. For example if we gave the instructions “try not to talk about yourself, but about the other’s interests’ likely these metrics would change. 2210 5.2 Human Evaluation As automated metrics are notoriously poor for evaluating dialogue (Liu et al., 2016) we also perform human evaluation using crowdsourced workers. The procedure is as follows. We perform almost exactly the same setup as in the dataset collection process itself as in Section 3.3. In that setup, we paired two Turkers and assigned them each a random (original) persona from the collected pool, and asked them to chat. Here, from the Turker’s point of view everything looks the same except instead of being paired with a Turker they are paired with one of our models instead (they do not know this). In this setting, for both the Turker and the model, the personas come from the test set"
P18-1205,W15-4640,0,0.0999014,"ning agent. Our goal is to enable interesting directions for future research, where chatbots can for instance have personalities, or imputed personas could be used to make dialogue more engaging to the user. We consider this in four possible scenarios: conditioning on no persona, your own persona, their persona, or both. These scenarios can be tried using either the original personas, or the revised ones. We then evaluate the task using three metrics: (i) the log likelihood of the correct sequence, measured via perplexity, (ii) F1 score, and (iii) next utterance classification loss, following Lowe et al. (2015). The latter consists of choosing N random distractor responses from other dialogues (in our setting, N =19) and the model selecting the best response among them, resulting in a score of one if the model chooses the correct response, and zero otherwise (called hits@1 in the experiments). 4 Models We consider two classes of model for next utterance prediction: ranking models and generative models. Ranking models produce a next utterance by considering any utterance in the training set as a possible candidate reply. Generative models generate novel sentences by conditioning on the dialogue histo"
P18-1205,D16-1147,1,0.752999,"rofile, the two models are identical. When the profile is available attention is performed by computing the similarity of the input q with the profile sentences pi , computing the softmax, and taking the weighted sum: X q+ = q + si pi , si = Softmax(sim(q, pi )) P where Softmax(zi ) = ezi / j ezj . One can then rank the candidates c0 using sim(q + , c0 ). One can also perform multiple “hops” of attention over the profile rather than one, as shown here, although that did not bring significant gains in our parameter sweeps. 4.3 Key-Value Profile Memory Network The key-value (KV) memory network (Miller et al., 2016) was proposed as an improvement to the memory network by performing attention over keys and outputting the values (instead of the same keys as in the original), which can outperform memory networks dependent on the task and definition of the key-value pairs. Here, we apply this model to dialogue, and consider the keys as dialog histories (from the training set), and the values as the next dialogue utterances, i.e., the replies from the speaking partner. This allows the model 2208 to have a memory of past dialogues that it can directly use to help influence its prediction for the current conver"
P18-1205,D14-1162,0,0.0822456,"ng very slow. In our experiments we simply trained the profile memory network and used the same weights from that model and applied this architecture at test time instead. Training the model directly would presumably give better results, however this heuristic already proved beneficial compared to the original network. Zipf’s law4 . Let F be the set of encoded memories. The decoder now attends over the encoded profile entries, i.e., we compute the mask at , context ct and next input x ˆt as: 4.4 5.1 Seq2Seq The input sequence x is encoded by applying het = LST Menc (xt |het−1 ). We use GloVe (Pennington et al., 2014) for our word embeddings. The final hidden state, het , is fed into the decoder LST Mdec as the initial state hd0 . For each time step t, the decoder then produces the probability of a word j occurring in that place via the softmax, i.e., exp(wj hdt ) p(yt,j = 1 |yt−1 , . . . , y1 ) = PK . d j 0 =1 exp(wj 0 ht ) The model is trained via negative log likelihood. The basic model can be extended to include persona information, in which case we simply prepend it to the input sequence x, i.e., x = ∀p ∈ P ||x, where ||denotes concatenation. For the OpenSubtitles and Twitter datasets trained in Secti"
P18-1205,D16-1264,0,0.0418531,"Missing"
P18-1205,N15-1020,0,0.171556,"Missing"
P19-1346,Q17-1010,0,0.013519,"raw upon when generating an answer. Wikipedia has been found effective for factoid-oriented questions (Joshi et al., 2017; Chen et al., 2017). However, early experiments in our setting showed it to be insufficient to cover the wide range of topics present in ELI5 and to address the open-ended nature of the questions. Instead, we use web data provided by Common Crawl.4 Specifically, we consider each of the individual pages in the July 2018 archive (roughly one per URL) as a single document. The data is tokenized with Spacy5 and we select English documents with FastText language identification (Bojanowski et al., 2017). Finally, we index the data with Apache Lucene.6 Creating support documents. We query the index for the 272K questions and gather the 100 most relevant web sources for each question, excluding Reddit. Each web source is the extracted text of one page in Common Crawl. This leads to supporting text for each question of a few hundred thousand words. There is a good chance that the supporting text contains the necessary information to answer the question, but the sheer amount of data is far beyond the scope of what many modern models can handle. We therefore filter the 100 web sources by selectin"
P19-1346,P17-1171,1,0.901368,"Missing"
P19-1346,W18-2706,1,0.891356,"Missing"
P19-1346,P17-1147,0,0.164702,"al overlap methods (Weissenborn et al., 2017), ELI5 poses a significant challenge in siphoning out important information, as no single sentence or phrase contains the full answer. While there are some datasets that do require multi-sentence supporting knowl3558 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3558–3567 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics Figure 2: ELI5 questions by starting word, where box size represents frequency. Questions are open ended and diverse. edge such as TriviaQA (Joshi et al., 2017), their answers are still short. We benchmark the performance of several extractive, retrieval, and generative models. Evaluation of our task, and of multi-sentence text generation in general, is challenging. We draw upon several evaluation metrics that quantify performance on intermediary fill-in tasks that lead up to the full answer generation. The overall answer generation quality is measured with ROUGE (Lin, 2004) and various human evaluation studies. We develop a strong abstractive baseline by training a Seq2Seq model on multiple tasks over the same data: language modeling, masked word pr"
P19-1346,Q18-1023,0,0.103701,"et al., 2017), SearchQA (Dunn et al., 2017), and QuAC (Choi et al., 2018) constrain the answer to a word or short phrase from the input and evaluate using exact match or F1 with the ground truth span. HotpotQA (Yang et al., 2018) extends this approach by building questions which challenge models to conduct multi-hop reasoning across multiple paragraphs, but the answer is still a short span. Further, the answer must be straightforward, as it needs to be copied from the supporting evidence — precluding most “how” or “why” type questions. Abstractive QA Abstractive datasets include NarrativeQA (Kocisky et al., 2018), a dataset of movie and book summaries and CoQA (Reddy et al., 2018), a multi-domain dialogue dataset. Both collect responses with crowdworkers and find that written answers are mostly extractive and short. MS MARCO (Nguyen et al., 2016), a dataset of crowdsourced responses to Bing queries, has written answers around 1 sentence long with short input passages. TriviaQA (Joshi et al., 2017) contains longer multi-document web input, collected using Bing and Wikipedia. As the dataset is built from trivia, most questions can be answered with a short extractive span. Multi-document summarization Th"
P19-1346,W04-1013,0,0.0827838,"Missing"
P19-1346,W18-6301,1,0.842759,"Seq2Seq models. We train several models based on the Transformer architecture (Vaswani et al., 2017), both in its language model and sequence-to-sequence (Seq2Seq) con3562 Model PPL to generate 40K codes which are applied to all datasets. We model a vocabulary of 52,863 tokens for answer generation. We use the Transformer implementation of fairseq-py (Gehring et al., 2017) and train with the big architecture following the details in (Vaswani et al., 2017). Given our data length, we train with a large batch size by delaying gradient updates until a sufficient number of examples have been seen (Ott et al., 2018). ROUGE 2 L 2.3 10.2 2.3 12.5 Support Document Nearest Neighbor - 1 16.8 16.7 Extractive (TFIDF) Extractive (BidAF) Oracle support doc Oracle web sources - 20.6 23.5 27.4 54.8 2.9 3.1 2.8 8.6 17.0 17.5 19.9 40.3 LM Q + A LM Q + D + A Seq2Seq Q to A Seq2Seq Q + D to A Seq2Seq Multi-task 42.2 33.9 52.9 55.1 32.7 27.8 26.4 28.3 28.3 28.9 4.7 4.0 5.1 5.1 5.4 23.1 20.5 22.7 22.8 23.1 Table 3: Comparison of oracles, baselines, retrieval, extractive, and abstractive models on the full proposed answers. Model LM Q + A LM Q + D + A S2S Q to A S2S Q + D to A S2S Multi-task FILL-1 acc. N V A 31.0 29.6 20"
P19-1346,P18-2124,0,0.0638254,"Missing"
P19-1346,D16-1264,0,0.0741944,"s and generating paragraph-length explanations in response to complex, diverse questions (see illustrations in Figures 1 and 2). The first challenge of ELI5 is the length and diversity of answers that span multiple sentences: ⇤ Equal contribution ‡ Work done while at Facebook AI Research 1 Dataset, Pretrained Models, and Additional Information is available: https://facebookresearch. github.io/ELI5, https://github.com/ facebookresearch/ELI5 questions are complex and cannot be easily addressed by a short response (Nguyen et al., 2016) or by extracting a word or phrase from an evidence document (Rajpurkar et al., 2016). Answers also represent one of several valid ways of addressing the query. Many state-of-the-art question answering models perform well compared to human performance for extractive answer selection (Radford et al., 2018; Devlin et al., 2018). However, their success does not directly carry over to our setting. The second challenge is the length and diversity of the content from knowledge sources required to answer our questions. We leverage evidence queried from the web for each question. In contrast to previous datasets where the human written answer could be found with lexical overlap method"
P19-1346,P16-1162,0,0.0249473,"Missing"
P19-1346,W17-2623,0,0.064004,"Missing"
P19-1346,K17-1028,0,0.0124095,"nswers also represent one of several valid ways of addressing the query. Many state-of-the-art question answering models perform well compared to human performance for extractive answer selection (Radford et al., 2018; Devlin et al., 2018). However, their success does not directly carry over to our setting. The second challenge is the length and diversity of the content from knowledge sources required to answer our questions. We leverage evidence queried from the web for each question. In contrast to previous datasets where the human written answer could be found with lexical overlap methods (Weissenborn et al., 2017), ELI5 poses a significant challenge in siphoning out important information, as no single sentence or phrase contains the full answer. While there are some datasets that do require multi-sentence supporting knowl3558 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3558–3567 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics Figure 2: ELI5 questions by starting word, where box size represents frequency. Questions are open ended and diverse. edge such as TriviaQA (Joshi et al., 2017), their answers are still"
P19-1346,D18-1259,0,0.0321957,"t and generate long outputs to form a comprehensive answer, leaving this challenge for future research. 2 Related Work Various QA datasets have been proposed in roughly two categories: extractive answers and short abstractive answers (see Table 1). Extractive QA Extractive question answering datasets such as TREC (Voorhees, 2003), SQuAD (Rajpurkar et al., 2016, 2018), NewsQA (Trischler et al., 2017), SearchQA (Dunn et al., 2017), and QuAC (Choi et al., 2018) constrain the answer to a word or short phrase from the input and evaluate using exact match or F1 with the ground truth span. HotpotQA (Yang et al., 2018) extends this approach by building questions which challenge models to conduct multi-hop reasoning across multiple paragraphs, but the answer is still a short span. Further, the answer must be straightforward, as it needs to be copied from the supporting evidence — precluding most “how” or “why” type questions. Abstractive QA Abstractive datasets include NarrativeQA (Kocisky et al., 2018), a dataset of movie and book summaries and CoQA (Reddy et al., 2018), a multi-domain dialogue dataset. Both collect responses with crowdworkers and find that written answers are mostly extractive and short. M"
P19-1358,W13-4038,0,\N,Missing
P19-1358,P07-1073,0,\N,Missing
P19-1358,N15-1086,0,\N,Missing
P19-1358,Q17-1010,0,\N,Missing
P19-1358,D17-2014,1,\N,Missing
P19-1358,I17-1061,0,\N,Missing
P19-1358,P18-1243,0,\N,Missing
P19-1358,D18-1298,1,\N,Missing
P19-1358,N18-1187,0,\N,Missing
P19-1363,D15-1075,0,0.726082,"jarring, and because semantic plausibility is not enough to root them out, preventing them is challenging. One approach to increasing the consistency of a chit-chat dialogue model was proposed in (Zhang et al., 2018), where the dialogue agent was given a set of personal facts describing its character (a persona) and produces utterances that reflect the persona. The intended outcome is that the agent produces utterances consistent with its given persona. However, these models still face the consistency issue, as shown in Figure 1. Separately, the framework of Natural Language Inference (NLI) (Bowman et al., 2015; Dagan et al., 2006; Maccartney and Manning, 2009) involves learning a mapping between a sentence pair and an entailment category. It is hypothesized that the NLI task is a proxy for general goals in natural language processing, such as language understanding (Bowman et al., 2015; Williams et al., 2018). Thus, the NLI task has been used for learning general sentence representations (Conneau et al., 2017) and for evaluating NLP models (Poliak et al., 2018a; Wang et al., 2018), with the expectation that such models will be useful in downstream tasks. Despite this expectation, leveraging an NLI"
P19-1363,P17-1152,0,0.057996,"s can be categorized into sentence encoding based methods of the form fMLP (genc (s1 ), genc (s2 )), and attention-based methods of the form fMLP (gattn (s1 , s2 )) (Lan and Xu, 2018). We thus choose and train representative models of each type which have achieved competitive performance on existing NLI benchmark datasets. For the sentence encoding method, we use InferSent (Conneau et al., 2017), which encodes a sentence using a bidirectional LSTM followed by max-pooling over the output states. As the representative attention-based method we use the enhanced sequential inference model (ESIM, (Chen et al., 2017)), which computes an attention score for each word pair. We also report results from a model trained and evaluated using the hypothesis sentence only (InferSent Hyp. Only) (Gururangan et al., 2018; Poliak et al., 2018c), a model trained on the existing SNLI dataset (Bowman et al., 2015) but evaluated 5 In our experiments, the softmax output corresponding to the contradiction class from Dialogue NLI. 6 Future work could consider filtering previous-utterance contradictions (ui , uj ) as well. i,j 3735 Data Type Matching Triple (p, p) Example Pred. Actual i am a hopeless bookworm. when i have som"
P19-1363,D17-1070,0,0.0635909,"oduces utterances consistent with its given persona. However, these models still face the consistency issue, as shown in Figure 1. Separately, the framework of Natural Language Inference (NLI) (Bowman et al., 2015; Dagan et al., 2006; Maccartney and Manning, 2009) involves learning a mapping between a sentence pair and an entailment category. It is hypothesized that the NLI task is a proxy for general goals in natural language processing, such as language understanding (Bowman et al., 2015; Williams et al., 2018). Thus, the NLI task has been used for learning general sentence representations (Conneau et al., 2017) and for evaluating NLP models (Poliak et al., 2018a; Wang et al., 2018), with the expectation that such models will be useful in downstream tasks. Despite this expectation, leveraging an NLI model for a downstream task remains an underexplored research direction. An NLI model may improve downstream task performance if properly used, while downstream tasks may yield new datasets or identify issues with existing NLI models, thus expanding the NLI research domain. In this paper, we reduce the problem of consistency in dialogue to natural language inference. We first create a dataset, Dialogue NL"
P19-1363,N18-2017,0,0.0198173,"s choose and train representative models of each type which have achieved competitive performance on existing NLI benchmark datasets. For the sentence encoding method, we use InferSent (Conneau et al., 2017), which encodes a sentence using a bidirectional LSTM followed by max-pooling over the output states. As the representative attention-based method we use the enhanced sequential inference model (ESIM, (Chen et al., 2017)), which computes an attention score for each word pair. We also report results from a model trained and evaluated using the hypothesis sentence only (InferSent Hyp. Only) (Gururangan et al., 2018; Poliak et al., 2018c), a model trained on the existing SNLI dataset (Bowman et al., 2015) but evaluated 5 In our experiments, the softmax output corresponding to the contradiction class from Dialogue NLI. 6 Future work could consider filtering previous-utterance contradictions (ui , uj ) as well. i,j 3735 Data Type Matching Triple (p, p) Example Pred. Actual i am a hopeless bookworm. when i have some spare time i read. Neutral Entail Matching Triple (u, p) i am from italy. i love the early mornings. i like getting up bright and early. Neutral Entail Misc. Utterance i do not understand footba"
P19-1363,C18-1328,0,0.0155167,"g to sre-rank . Hyper-parameters λ and k control the NLI model’s influence in re-ranking. For example, if the top candidate has a contradiction score of 1.0, then with λ = 1, it will be moved to the k’th position in the ranking. λ = 0 corresponds to no re-ranking. 5 5.1 Experiments Experiment 1: NLI That is, if the candidate ui does not contradict any persona sentence pj according to the NLI Models Many recently proposed NLI models can be categorized into sentence encoding based methods of the form fMLP (genc (s1 ), genc (s2 )), and attention-based methods of the form fMLP (gattn (s1 , s2 )) (Lan and Xu, 2018). We thus choose and train representative models of each type which have achieved competitive performance on existing NLI benchmark datasets. For the sentence encoding method, we use InferSent (Conneau et al., 2017), which encodes a sentence using a bidirectional LSTM followed by max-pooling over the output states. As the representative attention-based method we use the enhanced sequential inference model (ESIM, (Chen et al., 2017)), which computes an attention score for each word pair. We also report results from a model trained and evaluated using the hypothesis sentence only (InferSent Hyp."
P19-1363,P16-1094,0,0.0582053,"Consistency is a long standing issue faced by dialogue models. In this paper, we frame the consistency of dialogue agents as natural language inference (NLI) and create a new natural language inference dataset called Dialogue NLI. We propose a method which demonstrates that a model trained on Dialogue NLI can be used to improve the consistency of a dialogue model, and evaluate the method with human evaluation and with automatic metrics on a suite of evaluation sets designed to measure a dialogue model’s consistency. 1 Introduction A long standing issue faced by dialogue models is consistency (Li et al., 2016; Vinyals et al., 2015; Zhang et al., 2018). An example from (Vinyals et al., 2015) shows a two-round dialogue in which their neural sequence model first responds to what is your job? with i’m a lawyer, then responds to what do you do? with i’m a doctor. Even when inconsistencies are relatively rare and semantically plausible, they are jarring, and because semantic plausibility is not enough to root them out, preventing them is challenging. One approach to increasing the consistency of a chit-chat dialogue model was proposed in (Zhang et al., 2018), where the dialogue agent was given a set of"
P19-1363,W09-3714,0,0.0232086,"ty is not enough to root them out, preventing them is challenging. One approach to increasing the consistency of a chit-chat dialogue model was proposed in (Zhang et al., 2018), where the dialogue agent was given a set of personal facts describing its character (a persona) and produces utterances that reflect the persona. The intended outcome is that the agent produces utterances consistent with its given persona. However, these models still face the consistency issue, as shown in Figure 1. Separately, the framework of Natural Language Inference (NLI) (Bowman et al., 2015; Dagan et al., 2006; Maccartney and Manning, 2009) involves learning a mapping between a sentence pair and an entailment category. It is hypothesized that the NLI task is a proxy for general goals in natural language processing, such as language understanding (Bowman et al., 2015; Williams et al., 2018). Thus, the NLI task has been used for learning general sentence representations (Conneau et al., 2017) and for evaluating NLP models (Poliak et al., 2018a; Wang et al., 2018), with the expectation that such models will be useful in downstream tasks. Despite this expectation, leveraging an NLI model for a downstream task remains an underexplore"
P19-1363,marelli-etal-2014-sick,0,0.0285929,"dsourcing three label annotations for each example in the test set. We keep each test example for which two or more annotators agreed with its dataset label. All sentences in Dialogue NLI were generated by humans during the crowdsourced dialogue collection process of the Persona-Chat dataset (Zhang et al., 2018). The resulting sentence pairs are thus drawn from a natural dialogue domain that differs from existing NLI datasets, which are either drawn from different domains such as image captions or created using synthetic templates (Bowman et al., 2015; Demszky et al., 2018; Khot et al., 2018; Marelli et al., 2014; Poliak et al., 2018b; Wang et al., 2018; Williams et al., 2018). 4 Consistent Dialogue Agents via Natural Language Inference We now present a method which demonstrates that natural language inference can be used to improve the consistency of dialogue agents. Candidate utterances are re-ranked based on whether the candidate is predicted to contradict a persona sentence. If the NLI model predicts that a candidate contradicts a persona sentence, the candidate’s score is penalized, with the penalty weighted by the NLI model’s confidence5 scaled by a constant. Specifically, assume a dialogue mode"
P19-1363,D18-1298,0,0.0851225,"Missing"
P19-1363,D17-2014,1,0.933196,"sona consistency in dialogue. The reported metrics are percentages computed over each validation set. Figure 3: Example from the Likes Evaluation Set, showing dialogue model candidates, NLI model predictions, and reranked candidates using the method proposed in Section 4. model and the Dialogue NLI model. For the dialogue model we train a key-value memory network (Zhang et al., 2018) on the Persona-Chat dataset, which uses persona sentences and the conversation prefix as context. This model achieved the best performance on Persona-Chat in (Zhang et al., 2018). We train the model using ParlAI (Miller et al., 2017) on the personachat:self original task, using the hyper-parameters given for the KVMemnnAgent in the ConvAI2 competition. For the NLI model we use the ESIM model trained on Dialogue NLI, based on the results of Experiment 5. To study the effect of re-ranking on persona consistency, we form evaluation sets which contain next-utterances which are likely to yield persona contradiction or entailment, as follows. Evaluation Sets Each example is formed by first finding a next-utterance ut+1 in the Persona-Chat validation set which has an associated triple (e1 , r, e2 ) of interest, e.g. (i, like mus"
P19-1363,D14-1162,0,0.0828782,"triple (e1 , r, e2 ), where r ∈ R, e1 ∈ E1 , and e2 ∈ E2 . Here E1 is the set of all annotated e1 from the drop-downs or the text-box, and E2 is similarly defined. Finally, utterances are associated with a triple as follows. Let p be a persona sentence with triple (e1 , r, e2 ). We start with all utterances, U , from agents that have p in their persona. An utterance u ∈ U is then associated with the triple (e1 , r, e2 ) and persona sentence p when e2 is a sub-string of u, or word similarity4 sim(u, p) ≥ τ is suitably large. 4 We use cosine similarity between the mean of TF-IDF weighted GloVe (Pennington et al., 2014) word vectors and set τ = 0.9. 3734 3.3 Statistics Table 2 summarizes the dataset and its underlying data types. The label, triple, and data type are supplied as annotations for each sentence pair. We additionally create a gold-standard test set (Test Gold) by crowdsourcing three label annotations for each example in the test set. We keep each test example for which two or more annotators agreed with its dataset label. All sentences in Dialogue NLI were generated by humans during the crowdsourced dialogue collection process of the Persona-Chat dataset (Zhang et al., 2018). The resulting senten"
P19-1363,N18-2082,0,0.0380422,"Missing"
P19-1363,W18-5441,0,0.0533604,"Missing"
P19-1363,W18-5446,0,0.0492296,"Missing"
P19-1363,N18-1101,0,0.248564,"(a persona) and produces utterances that reflect the persona. The intended outcome is that the agent produces utterances consistent with its given persona. However, these models still face the consistency issue, as shown in Figure 1. Separately, the framework of Natural Language Inference (NLI) (Bowman et al., 2015; Dagan et al., 2006; Maccartney and Manning, 2009) involves learning a mapping between a sentence pair and an entailment category. It is hypothesized that the NLI task is a proxy for general goals in natural language processing, such as language understanding (Bowman et al., 2015; Williams et al., 2018). Thus, the NLI task has been used for learning general sentence representations (Conneau et al., 2017) and for evaluating NLP models (Poliak et al., 2018a; Wang et al., 2018), with the expectation that such models will be useful in downstream tasks. Despite this expectation, leveraging an NLI model for a downstream task remains an underexplored research direction. An NLI model may improve downstream task performance if properly used, while downstream tasks may yield new datasets or identify issues with existing NLI models, thus expanding the NLI research domain. In this paper, we reduce the p"
P19-1363,P18-1205,1,0.903158,"ced by dialogue models. In this paper, we frame the consistency of dialogue agents as natural language inference (NLI) and create a new natural language inference dataset called Dialogue NLI. We propose a method which demonstrates that a model trained on Dialogue NLI can be used to improve the consistency of a dialogue model, and evaluate the method with human evaluation and with automatic metrics on a suite of evaluation sets designed to measure a dialogue model’s consistency. 1 Introduction A long standing issue faced by dialogue models is consistency (Li et al., 2016; Vinyals et al., 2015; Zhang et al., 2018). An example from (Vinyals et al., 2015) shows a two-round dialogue in which their neural sequence model first responds to what is your job? with i’m a lawyer, then responds to what do you do? with i’m a doctor. Even when inconsistencies are relatively rare and semantically plausible, they are jarring, and because semantic plausibility is not enough to root them out, preventing them is challenging. One approach to increasing the consistency of a chit-chat dialogue model was proposed in (Zhang et al., 2018), where the dialogue agent was given a set of personal facts describing its character (a"
W18-5713,C16-1172,0,0.0239867,"Missing"
W18-5713,P18-1123,0,0.0869386,"that produce an initial prediction and then refine it are growing in traction in NLP. They have been used in MT and summarization either for refinement of initial predictions (Junczys-Dowmunt and Grundkiewicz, 2017; Niehues et al., 2016; Novak et al., 2016; Xia et al., 2017; Grangier and Auli, 2017) or combining with retrieval (Gu et al., 2017; Cao et al., 2018), as well as for sentence correction or refinement without context (Guu et al., 2017; Schmaltz et al., 2017). There is little work in applying these methods to dialogue; one work we are aware of has been done concurrently with ours is Pandey et al. (2018). The usefulness of our approach is shown with detailed experiments on the ConvAI2 dataset1 which is a chit-chat task to get to know the other speaker’s profile, obtaining generations superior to both retrieval and sequence generation models in human evaluations. Sequence generation models for dialogue are known to have several problems: they tend to produce short, generic sentences that are uninformative and unengaging. Retrieval models on the other hand can surface interesting responses, but are restricted to the given retrieval set leading to erroneous replies that cannot be tuned to the sp"
W18-5713,D17-1298,0,0.0191774,"retrieval set. In this work we propose a Retrieve and Refine model to gain the advantages of both methods, and avoid both their disadvantages. Models that produce an initial prediction and then refine it are growing in traction in NLP. They have been used in MT and summarization either for refinement of initial predictions (Junczys-Dowmunt and Grundkiewicz, 2017; Niehues et al., 2016; Novak et al., 2016; Xia et al., 2017; Grangier and Auli, 2017) or combining with retrieval (Gu et al., 2017; Cao et al., 2018), as well as for sentence correction or refinement without context (Guu et al., 2017; Schmaltz et al., 2017). There is little work in applying these methods to dialogue; one work we are aware of has been done concurrently with ours is Pandey et al. (2018). The usefulness of our approach is shown with detailed experiments on the ConvAI2 dataset1 which is a chit-chat task to get to know the other speaker’s profile, obtaining generations superior to both retrieval and sequence generation models in human evaluations. Sequence generation models for dialogue are known to have several problems: they tend to produce short, generic sentences that are uninformative and unengaging. Retrieval models on the othe"
W18-5713,P18-1015,0,0.0825533,"m@fb.com Abstract tune to the specific context, as they can only produce a valid reply if it is in the retrieval set. In this work we propose a Retrieve and Refine model to gain the advantages of both methods, and avoid both their disadvantages. Models that produce an initial prediction and then refine it are growing in traction in NLP. They have been used in MT and summarization either for refinement of initial predictions (Junczys-Dowmunt and Grundkiewicz, 2017; Niehues et al., 2016; Novak et al., 2016; Xia et al., 2017; Grangier and Auli, 2017) or combining with retrieval (Gu et al., 2017; Cao et al., 2018), as well as for sentence correction or refinement without context (Guu et al., 2017; Schmaltz et al., 2017). There is little work in applying these methods to dialogue; one work we are aware of has been done concurrently with ours is Pandey et al. (2018). The usefulness of our approach is shown with detailed experiments on the ConvAI2 dataset1 which is a chit-chat task to get to know the other speaker’s profile, obtaining generations superior to both retrieval and sequence generation models in human evaluations. Sequence generation models for dialogue are known to have several problems: they"
W18-5713,P18-1205,1,0.899913,"., 2014) do not have this problem, but instead either produce engaging responses or else completely erroneous ones which they cannot 2 Retrieve and Refine The model we propose in this work is remarkably straight-forward: we take a standard generative model and concatenate the output of a retrieval model to its usual input, and then generate as usual, training the model under this setting. For the generator, we use a standard Seq2Seq model: a 2-layer LSTM with attention. For the retriever, we use the Key-Value Memory Network (Miller et al., 2016) already shown to perform well for this dataset (Zhang et al., 2018), which attends over the dialogue history, to learn input and candidate retrieval embeddings that match using cosine similarity. The top scoring utterance is provided Proceedings of the 2018 EMNLP Workshop SCAI: The 2nd International Workshop on Search-Oriented Conversational AI 978-1-948087-75-9 1 http://convai.io/ 87 Proceedings of the 2018 EMNLP Workshop SCAI: The 2nd Int’l Workshop on Search-Oriented Conversational AI, pages 87–92 c Brussels, Belgium, October 31, 2018. 2018 Association for Computational Linguistics ISBN 978-1-948087-75-9 RetNRef Retrieval Method None (Vanilla Seq2Seq) Rand"
W18-5713,I17-1013,0,0.0256397,"Retrieve and Refine: Improved Sequence Generation Models For Dialogue Jason Weston, Emily Dinan and Alexander H. Miller Facebook AI Research jase@fb.com, edinan@fb.com, ahm@fb.com Abstract tune to the specific context, as they can only produce a valid reply if it is in the retrieval set. In this work we propose a Retrieve and Refine model to gain the advantages of both methods, and avoid both their disadvantages. Models that produce an initial prediction and then refine it are growing in traction in NLP. They have been used in MT and summarization either for refinement of initial predictions (Junczys-Dowmunt and Grundkiewicz, 2017; Niehues et al., 2016; Novak et al., 2016; Xia et al., 2017; Grangier and Auli, 2017) or combining with retrieval (Gu et al., 2017; Cao et al., 2018), as well as for sentence correction or refinement without context (Guu et al., 2017; Schmaltz et al., 2017). There is little work in applying these methods to dialogue; one work we are aware of has been done concurrently with ours is Pandey et al. (2018). The usefulness of our approach is shown with detailed experiments on the ConvAI2 dataset1 which is a chit-chat task to get to know the other speaker’s profile, obtaining generations superior to"
W18-5713,D16-1230,0,0.155121,"Missing"
W18-5713,D16-1147,1,0.798016,"nces, but is uninformative and unengaging. Retrieval models (Ji et al., 2014) do not have this problem, but instead either produce engaging responses or else completely erroneous ones which they cannot 2 Retrieve and Refine The model we propose in this work is remarkably straight-forward: we take a standard generative model and concatenate the output of a retrieval model to its usual input, and then generate as usual, training the model under this setting. For the generator, we use a standard Seq2Seq model: a 2-layer LSTM with attention. For the retriever, we use the Key-Value Memory Network (Miller et al., 2016) already shown to perform well for this dataset (Zhang et al., 2018), which attends over the dialogue history, to learn input and candidate retrieval embeddings that match using cosine similarity. The top scoring utterance is provided Proceedings of the 2018 EMNLP Workshop SCAI: The 2nd International Workshop on Search-Oriented Conversational AI 978-1-948087-75-9 1 http://convai.io/ 87 Proceedings of the 2018 EMNLP Workshop SCAI: The 2nd Int’l Workshop on Search-Oriented Conversational AI, pages 87–92 c Brussels, Belgium, October 31, 2018. 2018 Association for Computational Linguistics ISBN 97"
W19-8609,P17-4012,0,0.0352946,"tice to use the score s(h) used during the search to select the final sequence, but it is an open question whether there are better selection strategies for choosing between these final candidate responses. Beam search Instead of maintaining a single hypothesis at a time, as in greedy search above, at time step t beam search maintains a set of K hypotheses Ht : Ht = {(y11 , . . . , yt1 ), . . . , (y1K , . . . , ytK )}. (3) Avoiding repeating n-grams Although this has not been reported in a formal publication in the context of neural dialogue modeling to our knowledge, Paulus et al. (2017) and Klein et al. (2017) implement so-called n-gram blocking. In n-gram blocking, a hypothesis in a beam Ht is discarded if there is an n-gram that appears more than once within it. Each hypothesis hiyi , i ∈ {1, . . . , K} from Ht is t expanded with all possible next tokens v from the vocabulary V to form candidate hypotheses. Each candidate is in the form of ˜ i = hi k(v) = (y i , . . . , y i , v), h v yt 1 t (4) and is assigned a score: ˜ i ) = s(hi i ) + log p(v|y i ). s(h v ≤t y t 3 (5) We now propose an improved search strategy. To address the locality issue in beam search, we propose an iterative beam search t"
W19-8609,D18-1035,1,0.837739,"aximum length T . It is thus necessary to resort to approximate search algorithms. Greedy search Greedy search has been the search algorithm of choice among the recent papers on neural dialogue modeling (Gu et al., 2018; Zhao et al., 2017; Xu et al., 2017; Weston et al., 2018; Zhang et al., 2018). It moves from left to right selecting one token at a time, simply choosing the most likely token at the current time step: s s¯ yˆt = arg max log p(yt = v|ˆ y&lt;t , Y&lt;l , Y&lt;l , U ). v∈V Greedy search has been found significantly suboptimal within the field of machine translation (see, e.g., Table 1 in Chen et al., 2018), where similar neural sequence models are frequently used. Final sequence selection We consider search strategies to produce a set of candidate responses for the model to choose from. While greedy search provides only a single possible sequence, beam search generates a candidate set of size |M|. It is usual practice to use the score s(h) used during the search to select the final sequence, but it is an open question whether there are better selection strategies for choosing between these final candidate responses. Beam search Instead of maintaining a single hypothesis at a time, as in greedy"
W19-8609,D17-1230,0,0.0326471,"hese two learning algorithms, variational lower-bound maximization and adversarial learning, have been combined into a single model by Shen et al. (2018), which has been followed by Gu et al. (2018). Despite abundant endeavors on modeling and learning, search has received only a little attention (Dinan et al., 2019). Most of the work on search has focused on training an additional neural network that provides a supplementary score to guide either greedy or beam search. Li et al. (2015) propose a maximum mutual information criterion for decoding using a reverse model. This has been extended by Li et al. (2017a), where an extra neural network is trained to predict an arbitrary reward given a partial hypothesis and used during decoding. Similarly, Zemlyanskiy and Sha (2018) train a neural network that predicts the other participant’s personality given a partial conversation and use its predictability as an auxiliary score for re-ranking a set of candidate responses. None of these approaches study how the choice of the underlying search algorithm, rather than its scoring function, affects the quality of the neural dialogue model. In this paper, we investigate the effects of varying search and selecti"
W19-8609,D16-1230,0,0.0376594,"se the subscript 0 to indicate that beam search has been done without any other constraint. Re-running beam search with an increased beam width K would result in the search space that overlaps significantly with S0 , and would not give us Beam search terminates when |∪tt0 =1 Mt |≥ where K 0 is the maximum number of candidate sequences to be returned, or when t ≥ Lmax , where Lmax is the maximum length allowed for each candidate sequence. When terminated, beam K 0, 78 a set of (often human generated) reference responses and compare a single generated response against them (Serban et al., 2015; Liu et al., 2016). There are several methods for this comparison: (1) measure the perplexity of reference responses using the neural dialogue model, (2) compute a string match-based metric of a generated response against reference responses, and (3) use human annotators to compare model generated responses against reference or other models’ responses. None of these approaches capture the effectiveness of a neural sequence model in conducting a full conversation, because the model responses are computed given a human-written context, i.e., it does not see its own responses in the dialogue history, but gold resp"
W19-8609,D15-1166,0,0.157793,"Missing"
W19-8609,D18-1298,0,0.0292964,"Missing"
W19-8609,D17-2014,1,0.832109,"rning maximizes the log-probabilities of all the conversations in the training set: 1 X L= log p(C), (2) |D| C∈D often done using stochastic gradient descent with backpropagation (Rumelhart et al., 1985). Neural dialogue modeling Since Vinyals and Le (2015), a neural autoregressive sequence model based on sequence-tosequence models Sutskever et al. (2014); Cho et al. (2014) have become one of the most widely studied approaches to dialogue modeling (see, e.g., Serban et al., 2016, 2017; Zhao et al., 2017; Xu et al., 2017; Li et al., 2016, 2017a,b; Zemlyanskiy and Sha, 2018; Zhang et al., 2018; Miller et al., 2017; Shen et al., 2018; Gu et al., 2018). In this approach, a neural sequence model is used to model 2.2 Inference (generation) In this paper, we generate a response to the current state of the conversation (but do not attempt to plan ahead to future exchanges), maximizing s s¯ p(Y |Y&lt;l , Y&lt;l , U) = T Y s s¯ log p(yt |y&lt;t , Y&lt;l , Y&lt;l , U ). t=1 Unfortunately, it is intractable to solve this problem due to the exponentially-growing space of all 77 search returns all the candidate sequences in M = ∪tt0 =1 Mt . One can increase the size of the subspace over which beam search searches for a response"
W19-8609,W18-5713,1,0.926484,"e problem that most of the hypotheses discovered in M are near each other in the response space (Li et al., 2016, 2015). For tasks such as dialogue modeling, which are much more open-ended than e.g. machine translation, this is particularly troublesome as many high quality responses may be missing in the beam. possible responses w.r.t. the maximum length T . It is thus necessary to resort to approximate search algorithms. Greedy search Greedy search has been the search algorithm of choice among the recent papers on neural dialogue modeling (Gu et al., 2018; Zhao et al., 2017; Xu et al., 2017; Weston et al., 2018; Zhang et al., 2018). It moves from left to right selecting one token at a time, simply choosing the most likely token at the current time step: s s¯ yˆt = arg max log p(yt = v|ˆ y&lt;t , Y&lt;l , Y&lt;l , U ). v∈V Greedy search has been found significantly suboptimal within the field of machine translation (see, e.g., Table 1 in Chen et al., 2018), where similar neural sequence models are frequently used. Final sequence selection We consider search strategies to produce a set of candidate responses for the model to choose from. While greedy search provides only a single possible sequence, beam search"
W19-8609,D14-1162,0,0.0807947,"Missing"
W19-8609,1983.tc-1.13,0,0.174556,"Missing"
W19-8609,D17-1065,0,0.111631,"og p(Yls |Y&lt;l , Y≤l , U ), s∈{a,b} l=1 (1) where s¯ = a if s = b and otherwise s¯ = b. Learning maximizes the log-probabilities of all the conversations in the training set: 1 X L= log p(C), (2) |D| C∈D often done using stochastic gradient descent with backpropagation (Rumelhart et al., 1985). Neural dialogue modeling Since Vinyals and Le (2015), a neural autoregressive sequence model based on sequence-tosequence models Sutskever et al. (2014); Cho et al. (2014) have become one of the most widely studied approaches to dialogue modeling (see, e.g., Serban et al., 2016, 2017; Zhao et al., 2017; Xu et al., 2017; Li et al., 2016, 2017a,b; Zemlyanskiy and Sha, 2018; Zhang et al., 2018; Miller et al., 2017; Shen et al., 2018; Gu et al., 2018). In this approach, a neural sequence model is used to model 2.2 Inference (generation) In this paper, we generate a response to the current state of the conversation (but do not attempt to plan ahead to future exchanges), maximizing s s¯ p(Y |Y&lt;l , Y&lt;l , U) = T Y s s¯ log p(yt |y&lt;t , Y&lt;l , Y&lt;l , U ). t=1 Unfortunately, it is intractable to solve this problem due to the exponentially-growing space of all 77 search returns all the candidate sequences in M = ∪tt0 =1"
W19-8609,W18-3022,0,0.0576322,"Missing"
W19-8609,K18-1053,0,0.120184,"which has been followed by Gu et al. (2018). Despite abundant endeavors on modeling and learning, search has received only a little attention (Dinan et al., 2019). Most of the work on search has focused on training an additional neural network that provides a supplementary score to guide either greedy or beam search. Li et al. (2015) propose a maximum mutual information criterion for decoding using a reverse model. This has been extended by Li et al. (2017a), where an extra neural network is trained to predict an arbitrary reward given a partial hypothesis and used during decoding. Similarly, Zemlyanskiy and Sha (2018) train a neural network that predicts the other participant’s personality given a partial conversation and use its predictability as an auxiliary score for re-ranking a set of candidate responses. None of these approaches study how the choice of the underlying search algorithm, rather than its scoring function, affects the quality of the neural dialogue model. In this paper, we investigate the effects of varying search and selection strategies on the quality of generated dialogue utterances. We start with an attention-based sequence-tosequence model (Bahdanau et al., 2014) trained on the recen"
W19-8609,P18-1205,1,0.531282,"cts the other participant’s personality given a partial conversation and use its predictability as an auxiliary score for re-ranking a set of candidate responses. None of these approaches study how the choice of the underlying search algorithm, rather than its scoring function, affects the quality of the neural dialogue model. In this paper, we investigate the effects of varying search and selection strategies on the quality of generated dialogue utterances. We start with an attention-based sequence-tosequence model (Bahdanau et al., 2014) trained on the recently-released PersonaChat dataset (Zhang et al., 2018). We evaluate three search algorithms: greedy search, beam search and iterative beam search, the last of which we design based on earWe investigate the impact of search strategies in neural dialogue modeling. We first compare two standard search algorithms, greedy and beam search, as well as our newly proposed iterative beam search which produces a more diverse set of candidate responses. We evaluate these strategies in realistic full conversations with humans and propose a modelbased Bayesian calibration to address annotator bias. These conversations are analyzed using two automatic metrics:"
W19-8609,P17-1061,0,0.0904272,"p(C) = L X X s s¯ log p(Yls |Y&lt;l , Y≤l , U ), s∈{a,b} l=1 (1) where s¯ = a if s = b and otherwise s¯ = b. Learning maximizes the log-probabilities of all the conversations in the training set: 1 X L= log p(C), (2) |D| C∈D often done using stochastic gradient descent with backpropagation (Rumelhart et al., 1985). Neural dialogue modeling Since Vinyals and Le (2015), a neural autoregressive sequence model based on sequence-tosequence models Sutskever et al. (2014); Cho et al. (2014) have become one of the most widely studied approaches to dialogue modeling (see, e.g., Serban et al., 2016, 2017; Zhao et al., 2017; Xu et al., 2017; Li et al., 2016, 2017a,b; Zemlyanskiy and Sha, 2018; Zhang et al., 2018; Miller et al., 2017; Shen et al., 2018; Gu et al., 2018). In this approach, a neural sequence model is used to model 2.2 Inference (generation) In this paper, we generate a response to the current state of the conversation (but do not attempt to plan ahead to future exchanges), maximizing s s¯ p(Y |Y&lt;l , Y&lt;l , U) = T Y s s¯ log p(yt |y&lt;t , Y&lt;l , Y&lt;l , U ). t=1 Unfortunately, it is intractable to solve this problem due to the exponentially-growing space of all 77 search returns all the candidate sequence"
