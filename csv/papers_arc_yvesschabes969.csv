P96-1010,Combining Trigram-Based and Feature-Based Methods for Context-Sensitive Spelling Correction,1996,11,107,2,0,55904,andrew golding,34th Annual Meeting of the Association for Computational Linguistics,1,"This paper addresses the problem of correcting spelling errors that result in valid, though unintended words (such as peace and piece, or quiet and quite) and also the problem of correcting particular word usage errors (such as amount and number, or among and between). Such corrections require contextual information and are not handled by conventional spelling programs such as Unix spell. First, we introduce a method called Trigrams that uses part-of-speech trigrams to encode the context. This method uses a small number of parameters compared to previous methods based on word trigrams. However, it is effectively unable to distinguish among words that have the same part of speech. For this case, an alternative feature-based method called Bayes performs better; but Bayes is less effective than Trigrams when the distinction among words depends on syntactic constraints. A hybrid method called Tribayes is then introduced that combines the best of the previous two methods. The improvement in performance of Tribayes over its components is verified experimentally. Tribayes is also compared with the grammar checker in Microsoft Word, and is found to have substantially higher performance."
J95-4002,"Tree Insertion Grammar: A Cubic-Time, Parsable Formalism that Lexicalizes Context-Free Grammar without Changing the Trees Produced",1995,41,145,1,1,55905,yves schabes,Computational Linguistics,0,"Tree insertion grammar (TIG) is a tree-based formalism that makes use of tree substitution and tree adjunction. TIG is related to tree adjoining grammar. However, the adjunction permitted in TIG is sufficiently restricted that TIGs only derive context-free languages and TIGs have the same cubic-time worst-case complexity bounds for recognition and parsing as context-free grammars. An efficient Earley-style parser for TIGs is presented.Any context-free grammar (CFG) can be converted into a lexicalized tree insertion grammar (LTIG) that generates the same trees. A constructive procedure is presented for converting a CFG into a left anchored (i.e., word initial) LTIG that preserves ambiguity and generates the same trees. The LTIG created can be represented compactly by taking advantage of sharing between the elementary trees in it. Methods of converting CFGs into left anchored CFGs, e.g., the methods of Greibach and Rosenkrantz, do not preserve the trees produced and result in very large output grammars.For the purpose of experimental evaluation, the LTIG lexicalization procedure was applied to eight different CFGs for subsets of English. The LTIGs created were smaller than the original CFGs. Using an implementation of the Earley-style TIG parser that was specialized for left anchored LTIGs, it was possible to parse more quickly with the LTIGs than with the original CFGs."
J95-2004,Deterministic Part-Of-Speech Tagging With Finite State Transducers,1995,23,138,2,0,56176,emmanuel roche,Computational Linguistics,0,"Stochastic approaches to natural language processing have often been preferred to rule-based approaches because of their robustness and their automatic training capabilities. This was the case for part-of-speech tagging until Brill showed how state-of-the-art part-of-speech tagging can be achieved with a rule-based tagger by inferring rules from a training corpus. However, current implementations of the rule-based tagger run more slowly than previous approaches. In this paper, we present a finite-state tagger, inspired by the rule-based tagger, that operates in optimal time in the sense that the time to assign tags to a sentence corresponds to the time required to follow a single path in a deterministic finite-state machine. This result is achieved by encoding the application of the rules found in the tagger as a nondeterministic finite-state transducer and then turning it into a deterministic transducer. The resulting deterministic transducer yields a part-of-speech tagger whose speed is dominated by the access time of mass storage devices. We then generalize the techniques to the class of transformation-based systems."
J95-1007,Book Reviews: The Functional Treatment of Parsing,1995,-1,-1,1,1,55905,yves schabes,Computational Linguistics,0,None
J94-1004,An Alternative Conception of Tree-Adjoining Derivation,1994,20,123,1,1,55905,yves schabes,Computational Linguistics,0,"The precise formulation of derivation for tree-adjoining grammars has important ramifications for a wide variety of uses of the formalism, from syntactic analysis to semantic interpretation and statistical language modeling. We argue that the definition of tree-adjoining derivation must be reformulated in order to manifest the proper linguistic dependencies in derivations. The particular proposal is both precisely characterizable through a definition of TAG derivations as equivalence classes of ordered derivation trees, and computationally operational, by virtue of a compilation to linear indexed grammars together with an efficient algorithm for recognition and parsing according to the compiled grammar."
P93-1017,Lexicalized Context-Fee Grammars,1993,10,31,1,1,55905,yves schabes,31st Annual Meeting of the Association for Computational Linguistics,1,"Lexicalized context-free grammar(LCFG) is an attractive compromise between the parsing efficiency of context-free grammar (CFG) and the elegance and lexical sensitivity of lexicalized tree adjoining grammar (LTAG). LCFG is a restricted form of LTAG that can only generate context-free languages and can be parsed in cubic time. However, LCFG supports much of the elegance of LTAG's analysis of English and shares with LTAG the ability to lexicalize CFGS without changing the trees generated."
E93-1040,Parsing the {W}all {S}treet {J}ournal with the Inside-Outside Algorithm,1993,9,40,1,1,55905,yves schabes,Sixth Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,"We report grammar inference experiments on partially parsed sentences taken from the Wall Street Journal corpus using the inside-outside algorithm for stochastic context-free grammars. The initial grammar for the inference process makes no assumption of the kinds of structures and their distributions. The inferred grammar is evaluated by its predicting power and by comparing the bracketing of held out sentences imposed by the inferred grammar with the partial bracketings of these sentences given in the corpus. Using part-of-speech tags as the only source of lexical information, high bracketing accuracy is achieved even with a small subset of the available training material (1045 sentences): 94.4% for test sentences shorter than 10 words and 90.2% for sentences shorter than 15 words."
1993.iwpt-1.20,Stochastic Lexicalized Context-Free Grammar,1993,-1,-1,1,1,55905,yves schabes,Proceedings of the Third International Workshop on Parsing Technologies,0,"Stochastic lexicalized context-free grammar (SLCFG) is an attractive compromise between the parsing efficiency of stochastic context-free grammar (SCFG) and the lexical sensitivity of stochastic lexicalized tree-adjoining grammar (SLTAG) . SLCFG is a restricted form of SLTAG that can only generate context-free languages and can be parsed in cubic time. However, SLCFG retains the lexical sensitivity of SLTAG and is therefore a much better basis for capturing distributional information about words than SCFG."
P92-1017,Inside-Outside Reestimation From Partially Bracketed Corpora,1992,11,67,2,0,28562,fernando pereira,30th Annual Meeting of the Association for Computational Linguistics,1,"The inside-outside algorithm for inferring the parameters of a stochastic context-free grammar is extended to take advantage of constituent information in a partially parsed corpus. Experiments on formal and natural language parsed corpora show that the new algorithm can achieve faster convergence and better modelling of hierarchical structure than the original one. In particular, over 90% of the constituents in the most likely analyses of a test set are compatible with test set constituents for a grammar trained on a corpus of 700 hand-parsed part-of-speech strings for ATIS sentences."
P92-1022,An Alternative Conception of Tree-Adjoining Derivation,1992,20,12,1,1,55905,yves schabes,30th Annual Meeting of the Association for Computational Linguistics,1,"The precise formulation of derivation for tree-adjoining grammars has important ramifications for a wide variety of uses of the formalism, from syntactic analysis to semantic interpretation and statistical language modeling. We argue that the definition of tree-adjoining derivation must be reformulated in order to manifest the proper linguistic dependencies in derivations. The particular proposal is both precisely characterizable, through a compilation to linear indexed grammars, and computationally operational, by virtue of an efficient algorithm for recognition and parsing."
H92-1024,Inside-Outside Reestimation From Partially Bracketed Corpora,1992,11,67,2,0,28562,fernando pereira,"Speech and Natural Language: Proceedings of a Workshop Held at Harriman, New York, {F}ebruary 23-26, 1992",0,"The inside-outside algorithm for inferring the parameters of a stochastic context-free grammar is extended to take advantage of constituent information in a partially parsed corpus. Experiments on formal and natural language parsed corpora show that the new algorithm can achieve faster convergence and better modelling of hierarchical structure than the original one. In particular, over 90% of the constituents in the most likely analyses of a test set are compatible with test set constituents for a grammar trained on a corpus of 700 hand-parsed part-of-speech strings for ATIS sentences."
H92-1027,Stochastic {T}ree-{A}djoining {G}rammars,1992,16,7,1,1,55905,yves schabes,"Speech and Natural Language: Proceedings of a Workshop Held at Harriman, New York, {F}ebruary 23-26, 1992",0,"The notion of stochastic lexicalized tree-adjoining grammar (SLTAG) is defined and basic algorithms for SLTAG are designed. The parameters of a SLTAG correspond to the probability of combining two structures each one associated with a word. The characteristics of SLTAG are unique and novel since it is lexically sensitive (as N-gram models or Hidden Markov Models) and yet hierarchical (as stochastic context-free grammars). An algorithm for computing the probability of a sentence generated by a SLTAG is presented. Then, an iterative algorithm for estimating the parameters of a SLTAG given a training corpus is introduced."
C92-3145,A Freely Available Wide Coverage Morphological Analyzer for {E}nglish,1992,10,88,2,0,57103,daniel karp,{COLING} 1992 Volume 3: The 14th {I}nternational {C}onference on {C}omputational {L}inguistics,0,"This paper presents a morphological lexicon for English that handle more than 317000 inflected forms derived from over 90000 stems. The lexicon is available in two formats. The first can be used by an implementation of a two-level processor for morphological analysis (Karttunen and Wittenburg, 1983; Antworth, 1990). The second, derived from the first one for efficiency reasons, consists of a disk-based database using a UNIX hash table facility (Seltzer and Yigit, 1991). We also built an X Window tool to facilitate the maintenance and browsing of the lexicon. The package is ready to be integrated into an natural language application such as a parser through hooks written in Lisp and C.To our knowledge, this package is the only available free English morphological analyzer with very wide coverage."
C92-2066,Stochastic Lexicalized Tree-adjoining Grammars,1992,16,129,1,1,55905,yves schabes,{COLING} 1992 Volume 2: The 14th {I}nternational {C}onference on {C}omputational {L}inguistics,0,"The notion of stochastic lexicalized tree-adjoining grammar (SLTAG) is formally defined. The parameters of a SLTAG correspond to the probability of combining two structures each one associated with a word. The characteristics of SLTAG are unique and novel since it is lexieally sensitive (as N-gram models or Hidden Markov Models) and yet hierarchical (as stochastic context-free grammars).Then, two basic algorithms for SLTAG arc introduced: an algorithm for computing the probability of a sentence generated by a SLTAG and an inside-outside-like iterative algorithm for estimating the parameters of a SLTAG given a training corpus.Finally, we should how SLTAG enables to define a lexicalized version of stochastic context-free grammars and we report preliminary experiments showing some of the advantages of SLTAG over stochastic context-free grammars."
C92-1034,Structure Sharing in {L}exicalized {T}ree-{A}djoining {G}rammars,1992,15,38,2,0,12137,vijayshanker,{COLING} 1992 Volume 1: The 14th {I}nternational {C}onference on {C}omputational {L}inguistics,0,"We present a scheme for efficiently representing a lexicalized tree-adjoining grammar (LTAG). The proposed representational scheme allows for structure-sharing between lexical entries and the trees associated with the lexical items. A compact organization is achieved by organizing the lexicon in a hierarchical fashion and using inheritance as well as by using lexical and syntactic rules.While different organizations (Flickinger, 1987; Pollard and Sag, 1987; Shieber, 1986) of the lexicon have been proposed, in the scheme we propose, the inheritance hierarchy not only provides structure-sharing of lexical information but also of the associated elementary trees of extended domain of locality. Furthermore, the lexical and syntactic rules can be used to derive new elementary trees from the default structures specified in the hierarchical lexicon.In the envisaged scheme, the use of a hierarchical lexicon and of lexical and syntactic rules for lexicalized tree-adjoining grammars will capture important linguistic generalizations and also allows for a space efficient representation of the grammar. This will allow for easy maintenance and facilitate updates to the grammar."
A92-1030,{XTAG} - A Graphical Workbench for Developing {T}ree-{A}djoining {G}rammars,1992,23,27,2,0,5615,patrick paroubek,Third Conference on Applied Natural Language Processing,0,"We describe a workbench (XTAG) for the development of tree-adjoining grammars and their parsers, and discuss some issues that arise in the design of the graphical interface.Contrary to string rewriting grammars generating trees, the elementary objects manipulated by a tree-adjoining grammar are extended trees (i.e. trees of depth one or more) which capture syntactic information of lexical items. The unique characteristics of tree-adjoining grammars, its elementary objects found in the lexicon (extended trees) and the derivational history of derived trees (also a tree), require a specially crafted interface in which the perspective has shifted from a string-based to a tree-based system. XTAG provides such a graphical interface in which the elementary objects are trees (or tree sets) and not symbols (or strings).The kernel of XTAG is a predictive left to right parser for unification-based tree-adjoining grammar [Schabes, 1991]. XTAG includes a graphical editor for trees, a graphical tree printer, utilities for manipulating and displaying feature structures for unification-based tree-adjoining grammar, facilities for keeping track of the derivational history of TAG trees combined with adjoining and substitution, a parser for unification based tree-adjoining grammars, utilities for defining grammars and lexicons for tree-adjoining grammars, a morphological recognizer for English (75 000 stems deriving 280 000 inflected forms) and a tree-adjoining grammar for English that covers a large range of linguistic phenomena.Considerations of portability, efficiency, homogeneity and ease of maintenance, lead us to the use of Common Lisp without its object language addition and to the use of the X Window interface to Common Lisp (CLX) for the implementation of XTAG.XTAG without the large morphological and syntactic lexicons is public domain software. The large morphological and syntactic lexicons can be obtained through an agreement with ACL's Data Collection Initiative.XTAG runs under Common Lisp and X Window (CLX)."
P91-1014,Polynomial Time and Space Shift-Reduce Parsing of Arbitrary Context-free Grammars.,1991,12,28,1,1,55905,yves schabes,29th Annual Meeting of the Association for Computational Linguistics,1,"We introduce an algorithm for designing a predictive left to right shift-reduce non-determinisic push-down machine corresponding to an arbitrary unrestricted context-free grammar and an algorithm for efficiently driving this machine in pseudo-parallel. The performance of the resulting parser is formally proven to be superior to Earley's parser (1970).The technique employed consists in constructing before run-time a parsing table that encodes a non-deterministic machine in the which the predictive behavior has been compiled out. At run time, the machine is driven in pseudo-parallel with the help of a chart.The recognizer behaves in the worst case in O(|G|2n3)-time and O(|G|n2)-space. However in practice it is always superior to Earley's parser since the prediction steps have been compiled before run-time.Finally, we explain how other more efficient variants of the basic parser can be obtained by determinizing portions of the basic non-deterministic push-down machine while still using the same pseudo-parallel driver."
H91-1035,Fixed and Flexible Phrase Structure: Coordination in {T}ree {A}djoining {G}rammars,1991,5,8,2,0,33217,aravind joshi,"Speech and Natural Language: Proceedings of a Workshop Held at Pacific Grove, California, {F}ebruary 19-22, 1991",0,"Phrase-structure grammars assign a unique phrase structure (constituency) to an unambiguous sentence. Thus, for example, John likes apples will be bracketed as follows (ignoring the phrase labels and ignoring some brackets not essential for our present purpose):(1) (John (likes apples))"
1991.iwpt-1.4,The Valid Prefix Property and Left to Right Parsing of {T}ree-{A}djoining {G}rammar,1991,-1,-1,1,1,55905,yves schabes,Proceedings of the Second International Workshop on Parsing Technologies,0,"The valid prefix property (VPP), the capability of a left to right parser to detect errors as soon as possible, often goes unnoticed in parsing CFGs. Earley{'}s parser for CFGs (Earley, 1968; Earley, 1970) maintains the valid prefix property and obtains an $O(n^3)$-time worst case complexity, as good as parsers that do not maintain such as the CKY parser (Younger, 1967; Kasami, 1965). Contrary to CFGs, maintaining the valid prefix property for TAGs is costly. In 1988, Schabes and Joshi proposed an Earley-type parser for TAGs. It maintains the valid prefix property at the expense of its worst case complexity ($O(n^9)$-time). To our knowledge, it is the only known polynomial time parser for TAGs that maintains the valid prefix property. In this paper, we explain why the valid prefix property is expensive to maintain for TAGs and we introduce a predictive left to right parser for TAGs that does not maintain the valid prefix property but that achieves an $O(n^6)$-time worst case behavior, $O(n^4)$-time for unambiguous grammars and linear time for a large class of grammars."
W90-0208,The valid prefix property and parsing {T}ree {A}djoining {G}rammars,1990,0,0,1,1,55905,yves schabes,Proceedings of the First International Workshop on Tree Adjoining Grammar and Related Frameworks ({TAG}+1),0,None
W90-0102,Generation and Synchronous {T}ree-{A}djoining {G}rammars,1990,11,258,2,0,12905,stuart shieber,Proceedings of the Fifth International Workshop on Natural Language Generation,0,"The unique properties of tree-adjoining grammars (TAG) present a challenge for the application of TAGs beyond the limited confines of syntax, for instance, to the task of semantic interpretation or automatic translation of natural language. We present a variant of TAGs, called synchronous TAGs, which characterize correspondences between languages. The formalism's intended usage is to relate expressions of natural languages to their associated semantics represented in a logical form language, or to their translates in another natural language; in summary, we intend it to allow TAGs to be used beyond their role in syntax proper. We discuss the application of synchronous TAGs to concrete examples, mentioning primarily in passing some computational issues that arise in its interpretation."
P90-1035,Deterministic Left to Right Parsing of Tree Adjoining Languages,1990,10,27,1,1,55905,yves schabes,28th Annual Meeting of the Association for Computational Linguistics,1,"We define a set of deterministic bottom-up left to right parsers which analyze a subset of Tree Adjoining Languages. The LR parsing strategy for Context Free Grammars is extended to Tree Adjoining Grammars (TAGs). We use a machine, called Bottom-up Embedded Push Down Automation (BEPDA), that recognizes in a bottom-up fashion the set of Tree Adjoining Languages (and exactly this set). Each parser consists of a finite state control that drives the moves of a Bottom-up Embedded Pushdown Automaton. The parsers handle deterministically some context-sensitive Tree Adjoining Languages. In this paper, we informally describe the BEPDA then given a parsing table, we explain the LR parsing algorithm. We then show how to construct an LR(0) parsing table (no lookahead). An example of a context-sensitive language recognized deterministically is given. Then, we explain informally the construction of SLR(1) parsing tables for BEPDA. We conclude with a discussion of our parsing method and current work."
H90-1010,Two Recent Developments in {T}ree {A}djoining {G}rammars: Semantics and Efficient Processing,1990,14,0,1,1,55905,yves schabes,"Speech and Natural Language: Proceedings of a Workshop Held at Hidden Valley, {P}ennsylvania, June 24-27,1990",0,"During the past year there have been two very significant developments in the area of Tree Adjoining Grammars (TAGs).The first development is a variant of TAGs, called synchronous TAGs, which allows TAG to be used beyond the confines of syntax by characterizing correspondences between languages. The formalism's intended usage is to relate expressions of natural languages to their associated semantics represented by a logical form language in TAG, or to their translates in another natural language. The formalism is incremental and inherently nondirectional. We will show by detailed examples the working of synchronous TAGs and some of its applications, for example in generation and in machine translation.The second development is the design of LR-style parsers for TAGs. LR parsing strategies evolved out of the original work of Knuth. Even though they are not powerful enough for NLP, they have found use in natural language processing (NLP) by solving by pseudo-parallelism conflicts between multiple choices. This gives rise to a class of powerful yet efficient parsers for natural language. In order to extend the LR techniques to TAGs it is necessary to find bottom-up automaton that is exactly equivalent to TAGs. This is precisely what has been achieved by the discovery of the Bottom-up Embedded Push Down Automaton (BEPDA). Using BEPDA, deterministic left to right parsers for the Tree Adjoining Languages have been developed."
C90-3001,Using Lexicalized Tags for Machine Translation,1990,13,73,2,1,23421,anne abeille,{COLING} 1990 Volume 3: Papers presented to the 13th International Conference on Computational Linguistics,0,"Lexicalized Tree Adjoining Grammar (LTAG) is an attractive formalism for linguistic description mainly because of its extended domain of locality and its factoring recursion out from the domain of local dependencies (Joshi, 1985, Kroch and Joshi, 1985, Abeille, 1988). LTAG's extended domain of locality enables one to localize syntactic dependencies (such as filler-gap), as well as semantic dependencies (such as predicate-arguments). The aim of this paper is to show that these properties combined with the lexicalized property of LTAG are especially attractive for machine translation.The transfer between two languages, such as French and English, can be done by putting directly into correspondence large elementary units without going through some interlingual representation and without major changes to the source and target grammars. The underlying formalism for the transfer is synchronous Tree Adjoining Grammars (Shieber and Schabes [1990]). Transfer rules are stated as correspondences between nodes of trees of large domain of locality which are associated with words. We can thus define lexical transfer rules that avoid the defects of a mere word-to-word approach but still benefit from the simplicity and elegance of a lexical approach.We rely on the French and English LTAG grammars (Abeille [1988], Abeille [1990 (b)], Abeille et al. [1990], Abeille and Schabes [1989, 1990]) that have been designed over the past two years jointly at University of Pennsylvania and University of Paris 7-Jussieu."
C90-3045,Synchronous {T}ree-{A}djoining {G}rammars,1990,11,258,2,0,12905,stuart shieber,{COLING} 1990 Volume 3: Papers presented to the 13th International Conference on Computational Linguistics,0,"The unique properties of tree-adjoining grammars (TAG) present a challenge for the application of TAGs beyond the limited confines of syntax, for instance, to the task of semantic interpretation or automatic translation of natural language. We present a variant of TAGs, called synchronous TAGs, which characterize correspondences between languages. The formalism's intended usage is to relate expressions of natural languages to their associated semantics represented in a logical form language, or to their translates in another natural language; in summary, we intend it to allow TAGs to be used beyond their role in syntax proper. We discuss the application of synchronous TAGs to concrete examples, mentioning primarily in passing some computational issues that arise in its interpretation."
W89-0235,The Relevance of Lexicalization to Parsing,1989,0,9,1,1,55905,yves schabes,Proceedings of the First International Workshop on Parsing Technologies,0,"In this paper, we investigate the processing of the so-called {`}lexicalized{'} grammar. In {`}lexicalized{'} grammars (Schabes, Abeille and Joshi, 1988), each elementary structure is systema tically associated with a lexical {`}head{'}. These structures specify extended domains of locality (as compared to CFGs) over which constraints can be stated. The {`}grammar{'} consists of a lexicon where each lexical item is associated with a finite number of structures for which that item is the {`}head{'} . There are no separate grammar rules. There are, of course, {`}rules{'} which tell us how these structures are combined. A general two-pass parsing strategy for {`}lexicalized{'} grammars follows naturally. In the first stage, the parser selects a set of elementary structures associated with the lexical items in the input sentence, and in the second stage the sentence is parsed with respect to this set. We evaluate this strategy with respect to two characteristics. First, the amount of filtering on the entire grammar is evaluated: once the first pass is performed, the parser uses only a subset of the grammar. Second, we evaluate the use of non-local information: the structures selected during the first pass encode the morphological value (and therefore the position in the string) of their {`}head{'}; this enables the parser to use non-local in form ation to guide its search. We take Lexicalized Tree Adjoining Grammars as an in stance of lexicalized grammar. We illustrate the organization of the grammar. Then we show how a general Earley-type TAG parser (Schabes and Joshi, 1988) can take advantage of lexicalization. Empirical data show that the filtering of the grammar and the non-local in formation provided by the two-pass strategy improve the performance of the parser. We explain how constraints over the elementary structures expressed by unification equations can be parsed by a simple extension of the Earley-type TAG parser. Lexicalization guarantees termination of the algorithm without special devices such as restrictors."
H89-2053,An Evaluation of Lexicalization in Parsing,1989,21,0,2,0,33217,aravind joshi,"Speech and Natural Language: Proceedings of a Workshop Held at Cape Cod, Massachusetts, October 15-18, 1989",0,"In this paper, we evaluate a two-pass parsing strategy proposed for the so-called 'lexicalized' grammar. In 'lexicalized' grammars (Schabes, Abeille and Joshi, 1988), each elementary structure is systematically associated with a lexical item called anchor. These structures specify extended domains of locality (as compared to CFGs) over which constraints can be stated. The 'grammar' consists of a lexicon where each lexical item is associated with a finite number of structures for which that item is the anchor. There are no separate grammar rules. There are, of course, 'rules' which tell us how these structures are combined.A general two-pass parsing strategy for 'lexicalized' grammars follows naturally. In the first stage, the parser selects a set of elementary structures associated with the lexical items in the input sentence, and in the second stage the sentence is parsed with respect to this set. We evaluate this strategy with respect to two characteristics. First, the amount of filtering on the entire grammar is evaluated: once the first pass is performed, the parser uses only a subset of the grammar. Second, we evaluate the use of non-local information: the structures selected during the first pass encode the morphological value (and therefore the position in the string) of their anchor; this enables the parser to use non-local information to guide its search.We take Lexicalized Tree Adjoining Grammars as an instance of lexicalized grammar. We illustrate the organization of the grammar. Then we show how a general Earley-type TAG parser (Schabes and Joshi, 1988) can take advantage of lexicalization. Empirical data show that the filtering of the grammar and the non-local information provided by the two-pass strategy improve the performance of the parser."
H89-1036,"Lexicalized {TAG}s, Parsing and Lexicons",1989,12,2,5,1,23421,anne abeille,"Speech and Natural Language: Proceedings of a Workshop Held at Philadelphia, {P}ennsylvania, {F}ebruary 21-23, 1989",0,"In our approach, each elementary structure is systematically associated with a lexical head. These structures specify extended domains of locality (as compared to a context-free grammar) over which constraints can be stated. These constraints either hold within the elementary structure itself or specify what other structures can be composed with a given elementary structure. The 'grammar' consists of a lexicon where each lexical item is associated with a finite number of structures for which that item is the head. There are no separate grammar rules. There are, of course, 'rules' which tell us how these structures are composed. A grammar of this form will be said to be 'lexicalized'. A 'lexicalized' grammar naturally follows from the extended domain of locality of TAGs.A general parsing strategy for 'lexicalized' grammars is discussed. In the first stage, the parser selects a set of elementary structures associated with the lexical items in the input sentence, and in the second stage the sentence is parsed with respect to this set. An Earley-type parser for TAGs has been has been developed. It can be adapted to take advantage of the two steps parsing strategy. The system parses unification formalisms that have a CFG skeleton and that have a TAG skeleton.Along with the development of an Earley-type parser for TAGs, lexicons for English are under development. A lexicons for French is also being developed. Subsets of these lexicons are being incrementally interfaced to the parser.We finally show how idioms are represented in lexicalized TAGs. We assign them regular syntactic structures while representing them semantically as one entry. We finally show how they can be parsed by a parsing strategy as mentioned above."
E89-1001,Parsing Idioms in Lexicalized {TAG}s,1989,14,57,2,1,23421,anne abeille,Fourth Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,"We show how idioms can be parsed in lexicalized TAGs. We rely on extensive studies of frozen phrases pursued at L.A.D.L. that show that idioms are pervasive in natural language and obey, generally speaking, the same morphological and syntactical patterns as 'free' structures. By idiom we mean a structure in which some items are lexically frozen and have a semantics that is not compositional. We thus consider idioms of different syntactic categories: NP, S, adverbials, compound prepositions... in both English and French.In lexicalized TAGs, the same grammar is used for idioms as for 'free' sentences. We assign them regular syntactic structures while representing them semantically as one non-compositional entry. Syntactic transformations and insertion of modifiers may thus apply to them as to any 'free' structures. Unlike previous approaches, their variability becomes the general case and their being totally frozen the exception. Idioms are generally represented by extended elementary trees with 'heads' made out of several items (that need not be contiguous) with one of the items serving as an index. When an idiomatic tree is selected by this index, lexical items are attached to some nodes in the tree. Idiomatic trees are selected by a single head node however the head value imposes lexical values on other nodes in the tree. This operation of attaching the head item of an idiom and its lexical parts is called lexical attachment. The resulting tree has the lexical items corresponding to the pieces of the idiom already attached to it.We generalize the parsing strategy defined for lexicalized TAG to the case of 'heads' made out of several items. We propose to parse idioms in two steps which are merged in the two steps parsing strategy that is defined for 'free' sentences. The first step performed during the lexical pass selects trees corresponding to the literal and idiomatic interpretation. However it is not always the case that the idiomatic trees are selected as possible candidates. We require that all basic pieces building the minimal idiomatic expression must be present in the input string (with possibly some order constraints). This condition is a necessary condition for the idiomatic reading but of course it is not sufficient. The second step performs the syntax analysis as in the usual case. During the second step, idiomatic reading might be rejected. Idioms are thus parsed as any 'free' sentences. Except during the selection process, idioms do not require any special parsing mechanism. We are also able to account for cases of ambiguity between idiomatic and literal interpretations.Factoring recursion from dependencies in TAGs allows discontinuous constituents to be parsed in an elegant way. We also show how regular 'transformations' are taken into account by the parser."
P88-1032,An {E}arley-Type Parsing Algorithm for {T}ree {A}djoining {G}rammars,1988,9,61,1,1,55905,yves schabes,26th Annual Meeting of the Association for Computational Linguistics,1,"We will describe an Earley-type parser for Tree Adjoining Grammars (TAGs). Although a CKY-type parser for TAGs has been developed earlier (Vijay-Shanker and Joshi, 1985), this is the first practical parser for TAGs because as is well known for CFGs, the average behavior of Earley-type parsers is superior to that of CKY-type parsers. The core of the algorithm is described. Then we discuss modifications of the parsing algorithm that can parse extensions of TAGs such as constraints on adjunction, substitution, and feature structures for TAGs. We show how with the use of substitution in TAGs the system is able to parse directly CFGs and TAGs. The system parses unification formalisms that have a CFG skeleton and also those with a TAG skeleton. Thus it also allows us to embed the essential aspects of PATR-II."
C88-2121,Parsing Strategies with {`}Lexicalized{'} Grammars: Application to {T}ree {A}djoining {G}rammars,1988,22,202,1,1,55905,yves schabes,{C}oling {B}udapest 1988 Volume 2: {I}nternational {C}onference on {C}omputational {L}inguistics,0,"In this paper we present a general parsing strategy that arose from the development of an Earley-type parsing algorithm for TAGs (Schabes and Joshi 1988) and from recent linguistic work in TAGs (Abeille 1988).In our approach elementary structures are associated with their lexical heads. These structures specify extended domains of locality (as compared to a context-free grammar) over which constraints can be stated. These constraints either hold within the elementary structure itself or specify what other structures can be composed with a given elementary structure.We state the conditions under which context-free based grammars can be 'lexicalized' without changing the linguistic structures originally produced. We argue that even if one extends the domain of locality of CFGs to trees, using only substitution does not give the freedom to choose the head of each structure. We show how adjunction allows us to 'lexicalize' a CFG freely.We then show how a 'lexicalized' grammar naturally follows from the extended domain of locality of TAGs and present some of the linguistic advantages of our approach.A novel general parsing strategy for 'lexicalized' grammars is discussed. In a first stage, the parser builds a set structures corresponding to the input sentence and in a second stage, the sentence is parsed with respect to this set. The strategy is independent of the linguistic theory adopted and of the underlying grammar formalism. However, we focus our attention on TAGs. Since the set of trees needed to parse an input sentence is supposed to be finite, the parser can use in principle any search strategy. Thus, in particular, a top-down strategy can be used since problems due to recursive structures are eliminated. The parser is also able to use non-local information to guide the search.We then explain how the Earley-type parser for TAGs can be modified to take advantage of this approach."
