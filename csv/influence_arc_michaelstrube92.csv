2020.codi-1.16,P15-1136,0,0.0485392,"Missing"
2020.codi-1.16,D16-1245,0,0.0385478,"Missing"
2020.codi-1.16,P16-1188,0,0.0188528,"egitimate to mitigate the obstacles, as they may falsely introduce mention overlaps in adversarial training and test sets, thus inflating the performance. 1 Table 1: Replacing ”the Gulf war” with ”the Gulf warfare” or ”the Gulf w¨arf¨are” addresses (1) exact match in the test example; (2) mention overlaps across examples. be the reason why coreference resolvers have little effect in downstream tasks. Introduction Resolution of coreferring expressions is a natural step for text understanding, but coreference resolvers appear to have a negligible effect in downstream NLP tasks (Yu and Ji, 2016; Durrett et al., 2016; Voita et al., 2018). For instance, Durrett et al. (2016) rewrite pronouns with their antecedents (e.g., he is replaced by Dominick Dunne), using the Berkeley Entity Resolution System (Durrett and Klein, 2014). However, this fails to improve the cross-sentence coherence of system summaries, although the resolver performs well on the OntoNotes 4.0 dataset (Pradhan et al., 2011). The CoNLL benchmark (Pradhan et al., 2012) reflects the recent advances of coreference resolution systems. Nevertheless, previous work (Moosavi and Strube, 2017) indicates that the progress on the CoNLL benchmark is in"
2020.codi-1.16,Q14-1037,0,0.0205489,"warfare” or ”the Gulf w¨arf¨are” addresses (1) exact match in the test example; (2) mention overlaps across examples. be the reason why coreference resolvers have little effect in downstream tasks. Introduction Resolution of coreferring expressions is a natural step for text understanding, but coreference resolvers appear to have a negligible effect in downstream NLP tasks (Yu and Ji, 2016; Durrett et al., 2016; Voita et al., 2018). For instance, Durrett et al. (2016) rewrite pronouns with their antecedents (e.g., he is replaced by Dominick Dunne), using the Berkeley Entity Resolution System (Durrett and Klein, 2014). However, this fails to improve the cross-sentence coherence of system summaries, although the resolver performs well on the OntoNotes 4.0 dataset (Pradhan et al., 2011). The CoNLL benchmark (Pradhan et al., 2012) reflects the recent advances of coreference resolution systems. Nevertheless, previous work (Moosavi and Strube, 2017) indicates that the progress on the CoNLL benchmark is inflated, as the training and test sets share a large size of mentions. This may As opposed to evaluating on standard benchmarks, recent work (Glockner et al., 2018; Pruthi et al., 2019; Eger et al., 2019; Eger a"
2020.codi-1.16,2020.aacl-main.79,1,0.704157,"2014). However, this fails to improve the cross-sentence coherence of system summaries, although the resolver performs well on the OntoNotes 4.0 dataset (Pradhan et al., 2011). The CoNLL benchmark (Pradhan et al., 2012) reflects the recent advances of coreference resolution systems. Nevertheless, previous work (Moosavi and Strube, 2017) indicates that the progress on the CoNLL benchmark is inflated, as the training and test sets share a large size of mentions. This may As opposed to evaluating on standard benchmarks, recent work (Glockner et al., 2018; Pruthi et al., 2019; Eger et al., 2019; Eger and Benz, 2020) investigates the generalization ability of NLP systems under adversarial attacks. For instance, Glockner et al. (2018) show that natural language inference systems fail blatantly when lexical changes, e.g., replacing a word by its synonym, occur in premises and hypotheses. Pruthi et al. (2019) observe that spelling errors distract text classification systems from correct prediction. Inspired by these works, we investigate published coreference resolvers in two realistic adversarial setups, which challenge (a) lexical inference ability to resolve coreferent mentions, where one mention is, e.g."
2020.codi-1.16,N19-1165,1,0.880844,"Missing"
2020.codi-1.16,P18-2103,0,0.0296253,"using the Berkeley Entity Resolution System (Durrett and Klein, 2014). However, this fails to improve the cross-sentence coherence of system summaries, although the resolver performs well on the OntoNotes 4.0 dataset (Pradhan et al., 2011). The CoNLL benchmark (Pradhan et al., 2012) reflects the recent advances of coreference resolution systems. Nevertheless, previous work (Moosavi and Strube, 2017) indicates that the progress on the CoNLL benchmark is inflated, as the training and test sets share a large size of mentions. This may As opposed to evaluating on standard benchmarks, recent work (Glockner et al., 2018; Pruthi et al., 2019; Eger et al., 2019; Eger and Benz, 2020) investigates the generalization ability of NLP systems under adversarial attacks. For instance, Glockner et al. (2018) show that natural language inference systems fail blatantly when lexical changes, e.g., replacing a word by its synonym, occur in premises and hypotheses. Pruthi et al. (2019) observe that spelling errors distract text classification systems from correct prediction. Inspired by these works, we investigate published coreference resolvers in two realistic adversarial setups, which challenge (a) lexical inference abil"
2020.codi-1.16,D19-1588,0,0.0245649,"Missing"
2020.codi-1.16,J13-4004,0,0.0764278,"Missing"
2020.codi-1.16,N18-2108,0,0.0289703,"Missing"
2020.codi-1.16,H05-1004,0,0.317241,"Missing"
2020.codi-1.16,P17-2003,1,0.924434,"a negligible effect in downstream NLP tasks (Yu and Ji, 2016; Durrett et al., 2016; Voita et al., 2018). For instance, Durrett et al. (2016) rewrite pronouns with their antecedents (e.g., he is replaced by Dominick Dunne), using the Berkeley Entity Resolution System (Durrett and Klein, 2014). However, this fails to improve the cross-sentence coherence of system summaries, although the resolver performs well on the OntoNotes 4.0 dataset (Pradhan et al., 2011). The CoNLL benchmark (Pradhan et al., 2012) reflects the recent advances of coreference resolution systems. Nevertheless, previous work (Moosavi and Strube, 2017) indicates that the progress on the CoNLL benchmark is inflated, as the training and test sets share a large size of mentions. This may As opposed to evaluating on standard benchmarks, recent work (Glockner et al., 2018; Pruthi et al., 2019; Eger et al., 2019; Eger and Benz, 2020) investigates the generalization ability of NLP systems under adversarial attacks. For instance, Glockner et al. (2018) show that natural language inference systems fail blatantly when lexical changes, e.g., replacing a word by its synonym, occur in premises and hypotheses. Pruthi et al. (2019) observe that spelling e"
2020.codi-1.16,W12-4501,0,0.275686,"tion of coreferring expressions is a natural step for text understanding, but coreference resolvers appear to have a negligible effect in downstream NLP tasks (Yu and Ji, 2016; Durrett et al., 2016; Voita et al., 2018). For instance, Durrett et al. (2016) rewrite pronouns with their antecedents (e.g., he is replaced by Dominick Dunne), using the Berkeley Entity Resolution System (Durrett and Klein, 2014). However, this fails to improve the cross-sentence coherence of system summaries, although the resolver performs well on the OntoNotes 4.0 dataset (Pradhan et al., 2011). The CoNLL benchmark (Pradhan et al., 2012) reflects the recent advances of coreference resolution systems. Nevertheless, previous work (Moosavi and Strube, 2017) indicates that the progress on the CoNLL benchmark is inflated, as the training and test sets share a large size of mentions. This may As opposed to evaluating on standard benchmarks, recent work (Glockner et al., 2018; Pruthi et al., 2019; Eger et al., 2019; Eger and Benz, 2020) investigates the generalization ability of NLP systems under adversarial attacks. For instance, Glockner et al. (2018) show that natural language inference systems fail blatantly when lexical changes"
2020.codi-1.16,W11-1901,0,0.0363748,"ect in downstream tasks. Introduction Resolution of coreferring expressions is a natural step for text understanding, but coreference resolvers appear to have a negligible effect in downstream NLP tasks (Yu and Ji, 2016; Durrett et al., 2016; Voita et al., 2018). For instance, Durrett et al. (2016) rewrite pronouns with their antecedents (e.g., he is replaced by Dominick Dunne), using the Berkeley Entity Resolution System (Durrett and Klein, 2014). However, this fails to improve the cross-sentence coherence of system summaries, although the resolver performs well on the OntoNotes 4.0 dataset (Pradhan et al., 2011). The CoNLL benchmark (Pradhan et al., 2012) reflects the recent advances of coreference resolution systems. Nevertheless, previous work (Moosavi and Strube, 2017) indicates that the progress on the CoNLL benchmark is inflated, as the training and test sets share a large size of mentions. This may As opposed to evaluating on standard benchmarks, recent work (Glockner et al., 2018; Pruthi et al., 2019; Eger et al., 2019; Eger and Benz, 2020) investigates the generalization ability of NLP systems under adversarial attacks. For instance, Glockner et al. (2018) show that natural language inference"
2020.codi-1.16,P19-1561,0,0.112757,"ty Resolution System (Durrett and Klein, 2014). However, this fails to improve the cross-sentence coherence of system summaries, although the resolver performs well on the OntoNotes 4.0 dataset (Pradhan et al., 2011). The CoNLL benchmark (Pradhan et al., 2012) reflects the recent advances of coreference resolution systems. Nevertheless, previous work (Moosavi and Strube, 2017) indicates that the progress on the CoNLL benchmark is inflated, as the training and test sets share a large size of mentions. This may As opposed to evaluating on standard benchmarks, recent work (Glockner et al., 2018; Pruthi et al., 2019; Eger et al., 2019; Eger and Benz, 2020) investigates the generalization ability of NLP systems under adversarial attacks. For instance, Glockner et al. (2018) show that natural language inference systems fail blatantly when lexical changes, e.g., replacing a word by its synonym, occur in premises and hypotheses. Pruthi et al. (2019) observe that spelling errors distract text classification systems from correct prediction. Inspired by these works, we investigate published coreference resolvers in two realistic adversarial setups, which challenge (a) lexical inference ability to resolve corefe"
2020.codi-1.16,S19-1021,0,0.0185025,"ts Modification Original → Modification S WAP D ELETE V ISUAL S YNONYM H YPONYM H YPERNYM people → peolpe rise → rse emergency → emergeˇncy next → upcoming people → workers pigeon → bird Table 2: Examples of text modification. antecedent the first African-American president of the US. The CoNLL dataset involves many such lexical overlaps in coreferent mentions. Furthermore, Moosavi and Strube (2017) find a large size of mentions are overlapping in the CoNLL training and test examples. Together, this shows that the CoNLL evaluation setup does require only little lexical inference requirements. Subramanian and Roth (2019) remove named entities overlapping in the training and test sets. In contrast, we choose a word overlap randomly from mentions and substitute it with its hyponym, hypernym and synonym, as found in WordNet (Miller, 1995). To prevent the meaning of a word substitution deviated from the original word, we make the substitution only when two words share one word sense (synset), obtained from adapted LESK algorithm (Banerjee and Pedersen, 2002). Orthographic Changes. Character-level (“lowlevel”) text changes, e.g., random swapping of characters (Pruthi et al., 2019), create surface form noise that o"
2020.codi-1.16,M95-1005,0,0.767328,"Missing"
2020.codi-1.16,P18-1117,0,0.027264,"the obstacles, as they may falsely introduce mention overlaps in adversarial training and test sets, thus inflating the performance. 1 Table 1: Replacing ”the Gulf war” with ”the Gulf warfare” or ”the Gulf w¨arf¨are” addresses (1) exact match in the test example; (2) mention overlaps across examples. be the reason why coreference resolvers have little effect in downstream tasks. Introduction Resolution of coreferring expressions is a natural step for text understanding, but coreference resolvers appear to have a negligible effect in downstream NLP tasks (Yu and Ji, 2016; Durrett et al., 2016; Voita et al., 2018). For instance, Durrett et al. (2016) rewrite pronouns with their antecedents (e.g., he is replaced by Dominick Dunne), using the Berkeley Entity Resolution System (Durrett and Klein, 2014). However, this fails to improve the cross-sentence coherence of system summaries, although the resolver performs well on the OntoNotes 4.0 dataset (Pradhan et al., 2011). The CoNLL benchmark (Pradhan et al., 2012) reflects the recent advances of coreference resolution systems. Nevertheless, previous work (Moosavi and Strube, 2017) indicates that the progress on the CoNLL benchmark is inflated, as the traini"
2020.codi-1.16,P16-1005,0,0.012354,"e sometimes not legitimate to mitigate the obstacles, as they may falsely introduce mention overlaps in adversarial training and test sets, thus inflating the performance. 1 Table 1: Replacing ”the Gulf war” with ”the Gulf warfare” or ”the Gulf w¨arf¨are” addresses (1) exact match in the test example; (2) mention overlaps across examples. be the reason why coreference resolvers have little effect in downstream tasks. Introduction Resolution of coreferring expressions is a natural step for text understanding, but coreference resolvers appear to have a negligible effect in downstream NLP tasks (Yu and Ji, 2016; Durrett et al., 2016; Voita et al., 2018). For instance, Durrett et al. (2016) rewrite pronouns with their antecedents (e.g., he is replaced by Dominick Dunne), using the Berkeley Entity Resolution System (Durrett and Klein, 2014). However, this fails to improve the cross-sentence coherence of system summaries, although the resolver performs well on the OntoNotes 4.0 dataset (Pradhan et al., 2011). The CoNLL benchmark (Pradhan et al., 2012) reflects the recent advances of coreference resolution systems. Nevertheless, previous work (Moosavi and Strube, 2017) indicates that the progress on the"
2020.coling-main.594,J08-1001,0,0.680772,"1 describes our lexical coherence module. To interpret the sentence being read, we update two components: a semantic centroid vector and a semantic similarity vector. The semantic centroid vector takes averaged representations of preceding sentences, and this vector represents their central point. We then measure the semantic similarity between the current sentence representation and the centroid vector. We use cosine similarity to measure semantic similarity. We iterate this procedure for all sentences. 1 Our code is available at: https://github.com/sdeva14/coling20-inc-lexi-cohe 6753 Model Barzilay and Lapata (2008) Guinaudeau and Strube (2013) Li and Jurafsky (2017) ∗Mesgar and Strube (2018) Lai and Tetreault (2018) Avg-XLNet-Sent Avg-XLNet-Doc Our Model-Sent Yahoo 38.0 40.0 53.5 47.3 54.9 58.0 60.5 57.3 Clinton 43.0 56.0 61.0 57.7 60.2 57.6 65.9 61.7 Enron 46.0 43.5 54.4 50.6 53.2 54.3 56.9 54.5 Yelp 45.5 53.0 49.1 54.6 54.4 55.9 59.0 56.9 Avg Acc 43.1 48.1 51.7 52.6 55.7 56.4 60.6 57.6 Table 1: GCDC Accuracy performance comparison (∗: our re-implementation). A convolutional layer is applied to the semantic similarity vector to extract a feature map which represents the patterns of changes in semantic"
2020.coling-main.594,N19-1423,0,0.178094,"between sentences on the graph. It encodes sentences as nodes and lexical relations between sentences as edges. This model, nevertheless, considers lexical items independently. Recent neural models adopt a modern machine learning-based technique (Liu and Lapata, 2019; Gupta and Durrett, 2019), the Transformer (Vaswani et al., 2017). It relates all items simultaneously to capture semantic relations in sequences. More recently, large-scale pretrained language models, Transformerbased models pretrained on the massive amounts of text, have led to significant improvements in a range of NLP tasks (Devlin et al., 2019). However, the Transformer processes texts in a way which is different from the way humans do it. Psycholinguistic experiments show that humans read texts incrementally (Marslen-Wilson, 1975; Kamide et al., 2003; Gibson and Warren, 2004). K¨ohn (2018) claim that NLP systems which follow this theory should interpret texts incrementally, too. Do neural models benefit from both pretrained language models and incremental sentence processing? To investigate this question, we propose a coherence model which interprets sentences incrementally to capture lexical relations. For the ongoing sentence bei"
2020.coling-main.594,K17-1017,0,0.0901686,"oherence models on formal texts (Barzilay and Lapata, 2008), GCDC is designed to evaluate coherence models on informal texts, such as emails or online reviews. The dataset contains four domains: Clinton and Enron for emails, Yahoo for questions and answers in an online forum, and Yelp for online reviews of businesses. The quality of the dataset is controlled to have evenly-distributed scores and a low correlation between discourse length and scores3 . 2 3 https://stanfordnlp.github.io/stanza/ The Pearson correlation between text length and scores is lower than 0.12 in all domains. 6754 Model ∗Dong et al. (2017) ∗Mesgar and Strube (2018) ∗Nadeem et al. (2019) Avg-XLNet-Sent Avg-XLNet-Doc Our Model-Sent 1 69.3 54.9 58.9 70.7 74.7 75.6 2 66.5 56.4 55.8 69.5 74.4 73.4 3 65.8 52.4 65.6 69.0 73.0 75.0 Prompt 4 5 66.4 68.9 56.1 55.3 61.3 57.8 67.5 72.4 73.8 75.6 73.5 76.8 6 64.2 55.5 57.5 70.9 75.7 75.2 7 67.1 56.0 52.4 70.1 71.9 73.5 8 65.7 57.3 52.8 69.0 71.0 72.8 Avg Acc 66.7 55.5 57.8 69.9 73.8 74.5 Table 2: TOEFL Accuracy performance comparison (∗: our re-implementation). Experimental setup: For GCDC, we perform the experiments following previous work (Lai and Tetreault, 2018). We perform 10-fold cros"
2020.coling-main.594,D08-1035,0,0.0662106,"ur model with the state of the art in each task and two variants of a simple baseline relying on a pretrained language model: the first baseline encoding sentences individually, and the second baseline encoding a whole text at once1 . 2 Related Work Morris and Hirst (1991) propose lexical chains which identify sequences of related words using a lexical knowledge base. To identify lexical relations without human annotation, generative models have been developed, which learn lexical distributions. However, they may not generalize well across multiple datasets drawn from different distributions (Eisenstein and Barzilay, 2008; McNamara et al., 2010). Mesgar and Strube (2016) propose a graph-based model to overcome these limitations using word embeddings pretrained on a large-scale dataset. They introduce a graph model to represent lexical relations between sentences, which encodes sentences as nodes and lexical relations between sentences as edges. This graph-based model captures k-node subgraphs of this graph and represents coherence patterns by the frequency of subgraphs. However, their model neglects context to capture lexical relations. Modeling lexical coherence has proven to be effective in diverse NLP appli"
2020.coling-main.594,J95-2003,0,0.551617,".5 72.4 73.8 75.6 73.5 76.8 6 64.2 55.5 57.5 70.9 75.7 75.2 7 67.1 56.0 52.4 70.1 71.9 73.5 8 65.7 57.3 52.8 69.0 71.0 72.8 Avg Acc 66.7 55.5 57.8 69.9 73.8 74.5 Table 2: TOEFL Accuracy performance comparison (∗: our re-implementation). Experimental setup: For GCDC, we perform the experiments following previous work (Lai and Tetreault, 2018). We perform 10-fold cross-validation, use the same evaluation measure, accuracy for 3-class classification, and use the same loss function, cross-entropy loss. Baseline models: Barzilay and Lapata (2008) propose the entity grid, based on Centering Theory (Grosz et al., 1995). This model considers the distribution of entities over sentences. Guinaudeau and Strube (2013) convert the supervised entity grid into an unsupervised graph-based model. Li and Jurafsky (2017) propose a neural model which uses cliques, sets of adjacent sentences, to discriminate the difference of sentences extracted from original articles and randomly permutated ones. Mesgar and Strube (2018) propose a neural coherence model which finds the two most similar RNN outputs to determine the most salient part of sentences to connect adjacent sentences. Lai and Tetreault (2018) show that a simple n"
2020.coling-main.594,P13-1010,1,0.923025,"herence module. To interpret the sentence being read, we update two components: a semantic centroid vector and a semantic similarity vector. The semantic centroid vector takes averaged representations of preceding sentences, and this vector represents their central point. We then measure the semantic similarity between the current sentence representation and the centroid vector. We use cosine similarity to measure semantic similarity. We iterate this procedure for all sentences. 1 Our code is available at: https://github.com/sdeva14/coling20-inc-lexi-cohe 6753 Model Barzilay and Lapata (2008) Guinaudeau and Strube (2013) Li and Jurafsky (2017) ∗Mesgar and Strube (2018) Lai and Tetreault (2018) Avg-XLNet-Sent Avg-XLNet-Doc Our Model-Sent Yahoo 38.0 40.0 53.5 47.3 54.9 58.0 60.5 57.3 Clinton 43.0 56.0 61.0 57.7 60.2 57.6 65.9 61.7 Enron 46.0 43.5 54.4 50.6 53.2 54.3 56.9 54.5 Yelp 45.5 53.0 49.1 54.6 54.4 55.9 59.0 56.9 Avg Acc 43.1 48.1 51.7 52.6 55.7 56.4 60.6 57.6 Table 1: GCDC Accuracy performance comparison (∗: our re-implementation). A convolutional layer is applied to the semantic similarity vector to extract a feature map which represents the patterns of changes in semantic similarities. Max-pooling is"
2020.coling-main.594,D19-1070,0,0.0235654,"coherence represents the cohesive effect achieved by lexical relations (Halliday and Hasan, 1976). Earlier work mainly focuses on capturing lexical relations using external resources (Morris and Hirst, 1991). Mesgar and Strube (2016) introduce a graph model, the latest model for lexical coherence, to represent lexical relations between sentences on the graph. It encodes sentences as nodes and lexical relations between sentences as edges. This model, nevertheless, considers lexical items independently. Recent neural models adopt a modern machine learning-based technique (Liu and Lapata, 2019; Gupta and Durrett, 2019), the Transformer (Vaswani et al., 2017). It relates all items simultaneously to capture semantic relations in sequences. More recently, large-scale pretrained language models, Transformerbased models pretrained on the massive amounts of text, have led to significant improvements in a range of NLP tasks (Devlin et al., 2019). However, the Transformer processes texts in a way which is different from the way humans do it. Psycholinguistic experiments show that humans read texts incrementally (Marslen-Wilson, 1975; Kamide et al., 2003; Gibson and Warren, 2004). K¨ohn (2018) claim that NLP systems"
2020.coling-main.594,P18-2070,0,0.0136302,"using word embeddings pretrained on a large-scale dataset. They introduce a graph model to represent lexical relations between sentences, which encodes sentences as nodes and lexical relations between sentences as edges. This graph-based model captures k-node subgraphs of this graph and represents coherence patterns by the frequency of subgraphs. However, their model neglects context to capture lexical relations. Modeling lexical coherence has proven to be effective in diverse NLP applications like summarization (Erkan and Radev, 2004), translation (Xiong et al., 2013), and discourse parsing (Jia et al., 2018). We believe that our study for lexical coherence can be beneficial in these applications. 3 Our Model Figure 1 shows our model architecture. Our model takes sentence representations using a pretrained language model. The model then feeds sentences into the lexical coherence module to produce the semantic centroid vector and the semantic similarity vector. We concatenate the two vectors to generate a model output through a feed-forward network. Sentence representations: We first encode input sentences using a pretrained language model to produce word representations. We take a sentence represe"
2020.coling-main.594,C18-1253,0,0.061484,"Missing"
2020.coling-main.594,W18-5023,0,0.259973,": a semantic centroid vector and a semantic similarity vector. The semantic centroid vector takes averaged representations of preceding sentences, and this vector represents their central point. We then measure the semantic similarity between the current sentence representation and the centroid vector. We use cosine similarity to measure semantic similarity. We iterate this procedure for all sentences. 1 Our code is available at: https://github.com/sdeva14/coling20-inc-lexi-cohe 6753 Model Barzilay and Lapata (2008) Guinaudeau and Strube (2013) Li and Jurafsky (2017) ∗Mesgar and Strube (2018) Lai and Tetreault (2018) Avg-XLNet-Sent Avg-XLNet-Doc Our Model-Sent Yahoo 38.0 40.0 53.5 47.3 54.9 58.0 60.5 57.3 Clinton 43.0 56.0 61.0 57.7 60.2 57.6 65.9 61.7 Enron 46.0 43.5 54.4 50.6 53.2 54.3 56.9 54.5 Yelp 45.5 53.0 49.1 54.6 54.4 55.9 59.0 56.9 Avg Acc 43.1 48.1 51.7 52.6 55.7 56.4 60.6 57.6 Table 1: GCDC Accuracy performance comparison (∗: our re-implementation). A convolutional layer is applied to the semantic similarity vector to extract a feature map which represents the patterns of changes in semantic similarities. Max-pooling is applied to the feature map, and this lets the model capture features seman"
2020.coling-main.594,D17-1019,0,0.0840705,"the sentence being read, we update two components: a semantic centroid vector and a semantic similarity vector. The semantic centroid vector takes averaged representations of preceding sentences, and this vector represents their central point. We then measure the semantic similarity between the current sentence representation and the centroid vector. We use cosine similarity to measure semantic similarity. We iterate this procedure for all sentences. 1 Our code is available at: https://github.com/sdeva14/coling20-inc-lexi-cohe 6753 Model Barzilay and Lapata (2008) Guinaudeau and Strube (2013) Li and Jurafsky (2017) ∗Mesgar and Strube (2018) Lai and Tetreault (2018) Avg-XLNet-Sent Avg-XLNet-Doc Our Model-Sent Yahoo 38.0 40.0 53.5 47.3 54.9 58.0 60.5 57.3 Clinton 43.0 56.0 61.0 57.7 60.2 57.6 65.9 61.7 Enron 46.0 43.5 54.4 50.6 53.2 54.3 56.9 54.5 Yelp 45.5 53.0 49.1 54.6 54.4 55.9 59.0 56.9 Avg Acc 43.1 48.1 51.7 52.6 55.7 56.4 60.6 57.6 Table 1: GCDC Accuracy performance comparison (∗: our re-implementation). A convolutional layer is applied to the semantic similarity vector to extract a feature map which represents the patterns of changes in semantic similarities. Max-pooling is applied to the feature"
2020.coling-main.594,P19-1500,0,0.0260147,"ted sentences. Lexical coherence represents the cohesive effect achieved by lexical relations (Halliday and Hasan, 1976). Earlier work mainly focuses on capturing lexical relations using external resources (Morris and Hirst, 1991). Mesgar and Strube (2016) introduce a graph model, the latest model for lexical coherence, to represent lexical relations between sentences on the graph. It encodes sentences as nodes and lexical relations between sentences as edges. This model, nevertheless, considers lexical items independently. Recent neural models adopt a modern machine learning-based technique (Liu and Lapata, 2019; Gupta and Durrett, 2019), the Transformer (Vaswani et al., 2017). It relates all items simultaneously to capture semantic relations in sequences. More recently, large-scale pretrained language models, Transformerbased models pretrained on the massive amounts of text, have led to significant improvements in a range of NLP tasks (Devlin et al., 2019). However, the Transformer processes texts in a way which is different from the way humans do it. Psycholinguistic experiments show that humans read texts incrementally (Marslen-Wilson, 1975; Kamide et al., 2003; Gibson and Warren, 2004). K¨ohn (20"
2020.coling-main.594,N16-1167,1,0.925211,"ural models relying on a pretrained language model, and our model in two downstream tasks. Our findings suggest that interpreting texts incrementally as humans could be useful to design more advanced models. 1 Introduction Coherence describes the semantic relation between elements of a text. It distinguishes a text as either a unified whole or a collection of unrelated sentences. Lexical coherence represents the cohesive effect achieved by lexical relations (Halliday and Hasan, 1976). Earlier work mainly focuses on capturing lexical relations using external resources (Morris and Hirst, 1991). Mesgar and Strube (2016) introduce a graph model, the latest model for lexical coherence, to represent lexical relations between sentences on the graph. It encodes sentences as nodes and lexical relations between sentences as edges. This model, nevertheless, considers lexical items independently. Recent neural models adopt a modern machine learning-based technique (Liu and Lapata, 2019; Gupta and Durrett, 2019), the Transformer (Vaswani et al., 2017). It relates all items simultaneously to capture semantic relations in sequences. More recently, large-scale pretrained language models, Transformerbased models pretraine"
2020.coling-main.594,D18-1464,1,0.912635,"we update two components: a semantic centroid vector and a semantic similarity vector. The semantic centroid vector takes averaged representations of preceding sentences, and this vector represents their central point. We then measure the semantic similarity between the current sentence representation and the centroid vector. We use cosine similarity to measure semantic similarity. We iterate this procedure for all sentences. 1 Our code is available at: https://github.com/sdeva14/coling20-inc-lexi-cohe 6753 Model Barzilay and Lapata (2008) Guinaudeau and Strube (2013) Li and Jurafsky (2017) ∗Mesgar and Strube (2018) Lai and Tetreault (2018) Avg-XLNet-Sent Avg-XLNet-Doc Our Model-Sent Yahoo 38.0 40.0 53.5 47.3 54.9 58.0 60.5 57.3 Clinton 43.0 56.0 61.0 57.7 60.2 57.6 65.9 61.7 Enron 46.0 43.5 54.4 50.6 53.2 54.3 56.9 54.5 Yelp 45.5 53.0 49.1 54.6 54.4 55.9 59.0 56.9 Avg Acc 43.1 48.1 51.7 52.6 55.7 56.4 60.6 57.6 Table 1: GCDC Accuracy performance comparison (∗: our re-implementation). A convolutional layer is applied to the semantic similarity vector to extract a feature map which represents the patterns of changes in semantic similarities. Max-pooling is applied to the feature map, and this lets the mod"
2020.coling-main.594,J91-1002,0,0.719933,"t in each task, simple neural models relying on a pretrained language model, and our model in two downstream tasks. Our findings suggest that interpreting texts incrementally as humans could be useful to design more advanced models. 1 Introduction Coherence describes the semantic relation between elements of a text. It distinguishes a text as either a unified whole or a collection of unrelated sentences. Lexical coherence represents the cohesive effect achieved by lexical relations (Halliday and Hasan, 1976). Earlier work mainly focuses on capturing lexical relations using external resources (Morris and Hirst, 1991). Mesgar and Strube (2016) introduce a graph model, the latest model for lexical coherence, to represent lexical relations between sentences on the graph. It encodes sentences as nodes and lexical relations between sentences as edges. This model, nevertheless, considers lexical items independently. Recent neural models adopt a modern machine learning-based technique (Liu and Lapata, 2019; Gupta and Durrett, 2019), the Transformer (Vaswani et al., 2017). It relates all items simultaneously to capture semantic relations in sequences. More recently, large-scale pretrained language models, Transfo"
2020.coling-main.594,W19-4450,0,0.0971545,"Lapata, 2008), GCDC is designed to evaluate coherence models on informal texts, such as emails or online reviews. The dataset contains four domains: Clinton and Enron for emails, Yahoo for questions and answers in an online forum, and Yelp for online reviews of businesses. The quality of the dataset is controlled to have evenly-distributed scores and a low correlation between discourse length and scores3 . 2 3 https://stanfordnlp.github.io/stanza/ The Pearson correlation between text length and scores is lower than 0.12 in all domains. 6754 Model ∗Dong et al. (2017) ∗Mesgar and Strube (2018) ∗Nadeem et al. (2019) Avg-XLNet-Sent Avg-XLNet-Doc Our Model-Sent 1 69.3 54.9 58.9 70.7 74.7 75.6 2 66.5 56.4 55.8 69.5 74.4 73.4 3 65.8 52.4 65.6 69.0 73.0 75.0 Prompt 4 5 66.4 68.9 56.1 55.3 61.3 57.8 67.5 72.4 73.8 75.6 73.5 76.8 6 64.2 55.5 57.5 70.9 75.7 75.2 7 67.1 56.0 52.4 70.1 71.9 73.5 8 65.7 57.3 52.8 69.0 71.0 72.8 Avg Acc 66.7 55.5 57.8 69.9 73.8 74.5 Table 2: TOEFL Accuracy performance comparison (∗: our re-implementation). Experimental setup: For GCDC, we perform the experiments following previous work (Lai and Tetreault, 2018). We perform 10-fold cross-validation, use the same evaluation measure, a"
2020.coling-main.594,D14-1162,0,0.0862372,"apture features semantically relevant to the centroid vector. Document representation: We concatenate the semantic centroid vector, updated on the last sentence, and the semantic similarity vector. Finally, a feed-forward network is applied on the representation to produce the output value. 4 Experiments 4.1 Implementation Details We implement our model using the PyTorch library and use the Stanford Stanza library2 for sentence tokenization. For the baselines that do not use the pretrained language model, we use Glove for word embeddings, the pretrained word embeddings trained on Google News (Pennington et al., 2014). For our model, we apply a convolutional layer whose kernel size is 3, stride is 2, and padding is 2 and an adaptive max-pooling layer reducing a vector to the length of 5 (see the supplementary material for more details). Many pretrained language models cannot encode long texts due to their training settings, or require a massive amount of memory to encode them. In this work, we employ XLNet for the pretrainedlanguage model (Yang et al., 2019). Unlike BERT (Devlin et al., 2019), since XLNet can handle any input sequence length, which is required for our datasets to encode a whole text at onc"
2020.coling-main.594,D16-1193,0,0.0115606,"7 to 10 of US middle schools, whereas the prompts in TOEFL are submitted for the standard English test for the entrance to US universities by non-native students. The prompts in TOEFL do not vary so much, the student population is more controlled, and the essays have a similar length. Experimental setup: We evaluate performance in-domain at the prompt level. We perform 5-fold crossvalidation. For 3-class classification, we use cross-entropy loss to train models and measure accuracy to evaluate models. We evaluate performance for 30 epochs on the validation set. Following previous work on AES (Taghipour and Ng, 2016), the model which reaches the best performance on the validation set is then applied to the test set (see the supplementary material for details). Baseline models: Dong et al. (2017) introduce a model which consists of a convolutional layer, followed by a recurrent layer, and an attention layer (Bahdanau et al., 2015). We also compare with the state of the art on TOEFL, Nadeem et al. (2019). Inspired by Dong et al. (2017), Nadeem et al. (2019) propose 4 https://kaggle.com/c/asap-aes/ 6755 a model which uses an attention layer to decide the relative weights automatically in adjacent words as we"
2020.coling-main.594,D13-1163,0,0.0209822,"h-based model to overcome these limitations using word embeddings pretrained on a large-scale dataset. They introduce a graph model to represent lexical relations between sentences, which encodes sentences as nodes and lexical relations between sentences as edges. This graph-based model captures k-node subgraphs of this graph and represents coherence patterns by the frequency of subgraphs. However, their model neglects context to capture lexical relations. Modeling lexical coherence has proven to be effective in diverse NLP applications like summarization (Erkan and Radev, 2004), translation (Xiong et al., 2013), and discourse parsing (Jia et al., 2018). We believe that our study for lexical coherence can be beneficial in these applications. 3 Our Model Figure 1 shows our model architecture. Our model takes sentence representations using a pretrained language model. The model then feeds sentences into the lexical coherence module to produce the semantic centroid vector and the semantic similarity vector. We concatenate the two vectors to generate a model output through a feed-forward network. Sentence representations: We first encode input sentences using a pretrained language model to produce word r"
2020.emnlp-main.604,J08-1001,0,0.0293191,"ntified trees of texts assigned to different quality scores. Finally, we investigate what our model learns in terms of theoretical claims1 . 1 Introduction Coherence describes the semantic relation between elements of a text. It identifies a text passage as either a unified whole or a collection of unrelated sentences. The most well-known formal theory, Centering theory, determines the most salient item in each sentence, the center or focus, and tracks the changes of the focus (Grosz et al., 1995). Prior studies of coherence have mainly focused on modeling local coherence in Centering theory (Barzilay and Lapata, 2008). They aim to identify the semantic relations between adjacent sentences. However, 1 Our code is available at: https://github.com/ sdeva14/emnlp20-centering-neural-hds coherence arises not only at the local level, but also at the document level giving insight into the structure of the discourse. Discourse structure represents the semantic organization of a text. Incorporating structural information into the model has been beneficial for diverse downstream tasks including text summarization (Marcu, 2000), translation (Guzm´an et al., 2014), sentiment analysis (Bhatia et al., 2015), and text cla"
2020.emnlp-main.604,D15-1263,0,0.015507,"theory (Barzilay and Lapata, 2008). They aim to identify the semantic relations between adjacent sentences. However, 1 Our code is available at: https://github.com/ sdeva14/emnlp20-centering-neural-hds coherence arises not only at the local level, but also at the document level giving insight into the structure of the discourse. Discourse structure represents the semantic organization of a text. Incorporating structural information into the model has been beneficial for diverse downstream tasks including text summarization (Marcu, 2000), translation (Guzm´an et al., 2014), sentiment analysis (Bhatia et al., 2015), and text classification (Ji and Smith, 2017). To identify discourse structure, earlier work adopts a supervised approach, relying on human annotations (Hernault et al., 2010; Wang et al., 2017). However, annotating discourse structure is time consuming and costly. It requires annotators to understand not only the local context surrounding the target sentence but also higher level relations. Learning latent structure has been proposed to alleviate this limitation. This approach induces the discourse structure from a text without annotations using an attention layer (Liu and Lapata, 2018). Rec"
2020.emnlp-main.604,C16-1179,0,0.0204166,"Missing"
2020.emnlp-main.604,P87-1022,0,0.784981,"ntences, we encode each sentence separately to identify centers of the sentence. To determine the forward-looking centers of the sentence at the word level, we extract diagonal elements of the matrix representing multi-head selfattention of the encoded sentence. We then select the top-k vectors obtained by XLNet as the forwardlooking centers in the extracted elements. The preferred center of a sentence is the top-1 item in the forward-looking centers. The backward-looking center of a sentence is the item most related to one of the forward-looking centers of the immediately preceding sentence (Brennan et al., 1987). We compare semantic similarity between the averaged word representations of the current sentence and each forward-looking center of the immediately preceding sentence. We use cosine similarity to measure semantic similarity. Previous work introduces concepts to describe the changes of focus. Grosz et al. (1995) describe three types of centering transitions: Continue, Retain, and Shifting, as shown in Table 1. Continue maintains the current focus, and Retain intends to change the focus to an item recognized in the current sentence. Shifting indicates that the focus is different from the previ"
2020.emnlp-main.604,N10-1099,0,0.0818812,"Missing"
2020.emnlp-main.604,W19-4828,0,0.0182266,"model is not affected by the performance of an external parser. Figure 2 gives an overview of our approach to identify the focus of sentences. To represent 7460 Cb(Si−1 ) ≈ Cb(Si ) Cb(Si ) ≈ Cp(Si ) Cb(Si ) 6= Cp(Si ) Cb(Si−1 ) 6= Cb(Si ) Continue Shifting Retain Table 1: Three types of centering transitions. the focus of a sentence, we model the backwardlooking center and forward-looking centers using scores computed by multi-head self-attention in XLNet. Recent work shows that multi-head attention of a pretrained language model represents important linguistic notions of the input sequence (Clark et al., 2019; Vig and Belinkov, 2019; Sen et al., 2020). It also claims that self-attention might be biased to specialized tokens used in training, <SEP>, <CLS> and the token of a punctuation mark, hence we only consider actual items by filtering these tokens. Following previous work, we use the averaged scores of the multi-head self-attention extracted from the last layer of the model. To identify the salient items of sentences, we encode each sentence separately to identify centers of the sentence. To determine the forward-looking centers of the sentence at the word level, we extract diagonal elements o"
2020.emnlp-main.604,P19-1062,0,0.233095,"nault et al., 2010; Wang et al., 2017). However, annotating discourse structure is time consuming and costly. It requires annotators to understand not only the local context surrounding the target sentence but also higher level relations. Learning latent structure has been proposed to alleviate this limitation. This approach induces the discourse structure from a text without annotations using an attention layer (Liu and Lapata, 2018). Recent work argues that, however, the learned trees have mostly little to no structure at the document level, and the model relies on specific linguistic cues (Ferracane et al., 2019). In this paper, we propose a coherence model inspired by Centering theory which takes structural information into consideration. Our model does not rely on human annotations to identify this information. Our model consists of two components: (1) a discourse segments parser which constructs structural relationship for discourse segments by tracking the changes of the focus between discourse segments, and (2) a structure-aware transformer which exploits structural information to update sentence representations. The discourse segments parser first identifies the hierarchical discourse segments o"
2020.emnlp-main.604,J95-2003,0,0.796381,"pretrained language model, achieves state-of-the-art performance on both tasks. We next statistically examine the identified trees of texts assigned to different quality scores. Finally, we investigate what our model learns in terms of theoretical claims1 . 1 Introduction Coherence describes the semantic relation between elements of a text. It identifies a text passage as either a unified whole or a collection of unrelated sentences. The most well-known formal theory, Centering theory, determines the most salient item in each sentence, the center or focus, and tracks the changes of the focus (Grosz et al., 1995). Prior studies of coherence have mainly focused on modeling local coherence in Centering theory (Barzilay and Lapata, 2008). They aim to identify the semantic relations between adjacent sentences. However, 1 Our code is available at: https://github.com/ sdeva14/emnlp20-centering-neural-hds coherence arises not only at the local level, but also at the document level giving insight into the structure of the discourse. Discourse structure represents the semantic organization of a text. Incorporating structural information into the model has been beneficial for diverse downstream tasks including"
2020.emnlp-main.604,J86-3001,0,0.794881,"most preferred item of Cf. Cb describes the focus of a sentence with regards to the previous context. The theory also defines centering transitions to describe the changes of focus by comparing two centers, Cp and Cb. We propose an algorithm to approximate this theory using a pretrained language model. Our algorithm first identifies the focus of sentences using multihead attention scores provided by the pretrained language model and semantic similarity between vector representations. Our algorithm then constructs hierarchical discourse segments using a focus stack – inspired by the concept of Grosz and Sidner (1986) – to track the changes of the focus between discourse segments. Secondly, we propose a structure-aware transformer to account for structural information. Vaswani et al. (2017) introduce the transformer, a model solely based on a self-attention mechanism. This mechanism relates all items to capture semantic relations in a sequence. In contrast, the self-attention of our transformer is restricted to considering sentences with regards to the identified hierarchical discourse segments. We first calculate document structure priors to allow self-attention to relate sentences connected in the identi"
2020.emnlp-main.604,P14-1065,0,0.0678077,"Missing"
2020.emnlp-main.604,P17-1092,0,0.0183451,"identify the semantic relations between adjacent sentences. However, 1 Our code is available at: https://github.com/ sdeva14/emnlp20-centering-neural-hds coherence arises not only at the local level, but also at the document level giving insight into the structure of the discourse. Discourse structure represents the semantic organization of a text. Incorporating structural information into the model has been beneficial for diverse downstream tasks including text summarization (Marcu, 2000), translation (Guzm´an et al., 2014), sentiment analysis (Bhatia et al., 2015), and text classification (Ji and Smith, 2017). To identify discourse structure, earlier work adopts a supervised approach, relying on human annotations (Hernault et al., 2010; Wang et al., 2017). However, annotating discourse structure is time consuming and costly. It requires annotators to understand not only the local context surrounding the target sentence but also higher level relations. Learning latent structure has been proposed to alleviate this limitation. This approach induces the discourse structure from a text without annotations using an attention layer (Liu and Lapata, 2018). Recent work argues that, however, the learned tre"
2020.emnlp-main.604,D15-1264,0,0.0134133,"ntence Representations : Averaged representations Adjacency Matrix : Structural Information Discourse Segment Parser sentence1 h sentence2 h sentencen h h Large-scale pretrained language model word1 word2 word3 wordn Figure 1: Our model architecture. structure-aware coherence modeling. We then examine the identified trees to investigate differences of texts in writing quality. We finally inspect identified centers to investigate what our model learns in terms of theoretical claims. 2 Related Work While unsupervised approaches for discourse parser have been developed (Marcu and Echihabi, 2002; Ji et al., 2015), earlier work mostly adopted a supervised approach to identify discourse structure relying on human annotations. Subba and Di Eugenio (2009) incorporate various linguistic features, including compositional semantics and part-of-speech information, to propose a discourse parser based on Inductive Logic Programming. Hernault et al. (2010) introduce a discourse parser which constructs discourse structure from a full input text. They train classifiers to identify discourse relations, and use them to build a tree structure of an input text. Feng and Hirst (2012) improve the tree building algorithm"
2020.emnlp-main.604,D14-1220,0,0.0185778,"Hernault et al. (2010) introduce a discourse parser which constructs discourse structure from a full input text. They train classifiers to identify discourse relations, and use them to build a tree structure of an input text. Feng and Hirst (2012) improve the tree building algorithm of this system by incorporating more linguistic features. Wang et al. (2017) introduce an SVM-based model that consists of two stages, one identifying discourse structure, and the other classifying types of relations between units. More recently, neural models have been developed to recognize discourse structure. Li et al. (2014) present a simple model based on a recursive neural network. Li et al. (2016) claim that this model suffers from a vanishing gradient problem caused by long sequences, and propose an attentionbased hierarchical neural network model. To alleviate the shortage of human annotations, Braud 7459 Top-N be ▁ o t ▁d n te ▁ p le o r pe e ▁ o lda n ▁ h t ▁ g il e a s fr s ▁ ▁le d an ▁ , a id a fr ss ▁ le , ▁ k ea w s ▁ le s ▁ are ▁ le op g pe n ▁ y o u se ▁ au n c io b e in ▁ o p t h is ▁ ▁ o ld h I ▁ ▁ ▁I ▁hold ▁t his ▁opinion ▁because ▁young ▁people ▁are ▁less ▁weak , ▁less ▁afraid , ▁and ▁less ▁fragi"
2020.emnlp-main.604,D16-1035,0,0.0208352,"se structure from a full input text. They train classifiers to identify discourse relations, and use them to build a tree structure of an input text. Feng and Hirst (2012) improve the tree building algorithm of this system by incorporating more linguistic features. Wang et al. (2017) introduce an SVM-based model that consists of two stages, one identifying discourse structure, and the other classifying types of relations between units. More recently, neural models have been developed to recognize discourse structure. Li et al. (2014) present a simple model based on a recursive neural network. Li et al. (2016) claim that this model suffers from a vanishing gradient problem caused by long sequences, and propose an attentionbased hierarchical neural network model. To alleviate the shortage of human annotations, Braud 7459 Top-N be ▁ o t ▁d n te ▁ p le o r pe e ▁ o lda n ▁ h t ▁ g il e a s fr s ▁ ▁le d an ▁ , a id a fr ss ▁ le , ▁ k ea w s ▁ le s ▁ are ▁ le op g pe n ▁ y o u se ▁ au n c io b e in ▁ o p t h is ▁ ▁ o ld h I ▁ ▁ ▁I ▁hold ▁t his ▁opinion ▁because ▁young ▁people ▁are ▁less ▁weak , ▁less ▁afraid , ▁and ▁less ▁fragile ▁t han ▁older ▁people ▁t end ▁t o ▁be Semantic similarity between each Cf"
2020.emnlp-main.604,Q18-1005,0,0.183789,"ysis (Bhatia et al., 2015), and text classification (Ji and Smith, 2017). To identify discourse structure, earlier work adopts a supervised approach, relying on human annotations (Hernault et al., 2010; Wang et al., 2017). However, annotating discourse structure is time consuming and costly. It requires annotators to understand not only the local context surrounding the target sentence but also higher level relations. Learning latent structure has been proposed to alleviate this limitation. This approach induces the discourse structure from a text without annotations using an attention layer (Liu and Lapata, 2018). Recent work argues that, however, the learned trees have mostly little to no structure at the document level, and the model relies on specific linguistic cues (Ferracane et al., 2019). In this paper, we propose a coherence model inspired by Centering theory which takes structural information into consideration. Our model does not rely on human annotations to identify this information. Our model consists of two components: (1) a discourse segments parser which constructs structural relationship for discourse segments by tracking the changes of the focus between discourse segments, and (2) a s"
2020.emnlp-main.604,Q13-1028,0,0.0464935,"Missing"
2020.emnlp-main.604,P02-1047,0,0.325223,"cture-aware Transformer Sentence Representations : Averaged representations Adjacency Matrix : Structural Information Discourse Segment Parser sentence1 h sentence2 h sentencen h h Large-scale pretrained language model word1 word2 word3 wordn Figure 1: Our model architecture. structure-aware coherence modeling. We then examine the identified trees to investigate differences of texts in writing quality. We finally inspect identified centers to investigate what our model learns in terms of theoretical claims. 2 Related Work While unsupervised approaches for discourse parser have been developed (Marcu and Echihabi, 2002; Ji et al., 2015), earlier work mostly adopted a supervised approach to identify discourse structure relying on human annotations. Subba and Di Eugenio (2009) incorporate various linguistic features, including compositional semantics and part-of-speech information, to propose a discourse parser based on Inductive Logic Programming. Hernault et al. (2010) introduce a discourse parser which constructs discourse structure from a full input text. They train classifiers to identify discourse relations, and use them to build a tree structure of an input text. Feng and Hirst (2012) improve the tree"
2020.emnlp-main.604,D18-1464,1,0.874027,"we use Glove for word embeddings, the pretrained word embeddings trained on Google News (Pennington et al., 2014). We set the top-n for selecting Cf to 3 and the semantic threshold to compare vector representations to 0.945 (see Appendix B for more training details and parameters). Due to memory constraints, we encode each sentence separately using XLNet instead of the whole document at once. Our dataset consists of long documents i.e., journal articles with more than 3,000 tokens. For employing the pretrained model, it is 7462 2 https://stanfordnlp.github.io/stanza/ Model Dong et al. (2017) Mesgar and Strube (2018) Liu and Lapata (2018) Averaged-XLNet XLNet + Wang et al. (2019) Our Model 1 69.30 56.25 55.60 70.73 71.65 75.10 2 66.47 55.94 55.80 69.48 71.50 73.35 3 65.84 55.20 65.60 68.98 71.71 74.75 Prompt 4 5 66.38 68.89 57.20 56.57 61.30 57.80 67.52 72.35 71.64 74.23 74.18 76.38 6 64.20 55.10 57.50 70.94 69.58 74.30 7 67.11 56.97 52.40 70.14 70.76 73.61 8 65.73 58.39 52.80 69.01 68.98 73.44 Avg Acc 66.74 56.45 57.80 69.89 71.26 74.39 Table 2: TOEFL Accuracy performance comparison on the test sets (see Appendix D for more details). practically infeasible to encode all words in a document at once due to"
2020.emnlp-main.604,D14-1162,0,0.0867281,"tions. The document attention identifies relative weights of updated sentence representations which enables our model to handle any document length. Finally, a feedforward network is applied to the representation to produce the output value. 4 Experiments 4.1 Implementation Details We implement our model using the PyTorch library and use the Stanford Stanza library2 for sentence tokenization. We employ XLNet for the pretrainedlanguage model. For the baselines that do not use the pretrained language model, we use Glove for word embeddings, the pretrained word embeddings trained on Google News (Pennington et al., 2014). We set the top-n for selecting Cf to 3 and the semantic threshold to compare vector representations to 0.945 (see Appendix B for more training details and parameters). Due to memory constraints, we encode each sentence separately using XLNet instead of the whole document at once. Our dataset consists of long documents i.e., journal articles with more than 3,000 tokens. For employing the pretrained model, it is 7462 2 https://stanfordnlp.github.io/stanza/ Model Dong et al. (2017) Mesgar and Strube (2018) Liu and Lapata (2018) Averaged-XLNet XLNet + Wang et al. (2019) Our Model 1 69.30 56.25 5"
2020.emnlp-main.604,2020.acl-main.419,0,0.020591,"an external parser. Figure 2 gives an overview of our approach to identify the focus of sentences. To represent 7460 Cb(Si−1 ) ≈ Cb(Si ) Cb(Si ) ≈ Cp(Si ) Cb(Si ) 6= Cp(Si ) Cb(Si−1 ) 6= Cb(Si ) Continue Shifting Retain Table 1: Three types of centering transitions. the focus of a sentence, we model the backwardlooking center and forward-looking centers using scores computed by multi-head self-attention in XLNet. Recent work shows that multi-head attention of a pretrained language model represents important linguistic notions of the input sequence (Clark et al., 2019; Vig and Belinkov, 2019; Sen et al., 2020). It also claims that self-attention might be biased to specialized tokens used in training, <SEP>, <CLS> and the token of a punctuation mark, hence we only consider actual items by filtering these tokens. Following previous work, we use the averaged scores of the multi-head self-attention extracted from the last layer of the model. To identify the salient items of sentences, we encode each sentence separately to identify centers of the sentence. To determine the forward-looking centers of the sentence at the word level, we extract diagonal elements of the matrix representing multi-head selfat"
2020.emnlp-main.604,N09-1064,0,0.0575244,"Missing"
2020.emnlp-main.604,D16-1193,0,0.0242696,"frequently used dataset for AES, the Automated Student Assessment Prize (ASAP) dataset3 . The prompts in ASAP are written by students in grade levels 7 to 10 of US middle schools. Many essays in ASAP consist of only a few sentences. In contrast, the prompts in TOEFL are submitted for the standard English test for the entrance to universities by non-native students. The prompts in TOEFL do not vary so much, the student population is more controlled, and the essays have a similar length (see Appendix A for more details). Evaluation Setup. We follow the evaluation setup of previous work on AES (Taghipour and Ng, 2016). For TOEFL, we evaluate performance with accuracy for the three-class classification problem with 5-fold cross-validation. We deploy the crossentropy loss for training. We use the ADAM optimizer with a learning rate of 0.003. We evaluate performance for 20 epochs on the validation set. The model which reaches the best accuracy on the validation set is then applied to the test set. We use a mini-batch size of 32 with random shuffle. 7463 3 https://kaggle.com/c/asap-aes/ 1.0 1.0 Our Model 0.8 0.6 Accuracy Accuracy 0.8 Accuracy for each essay score: Prompt 1 Liu and Lapata (2018) Averaged XLNet"
2020.emnlp-main.604,W19-4808,0,0.0265892,"ed by the performance of an external parser. Figure 2 gives an overview of our approach to identify the focus of sentences. To represent 7460 Cb(Si−1 ) ≈ Cb(Si ) Cb(Si ) ≈ Cp(Si ) Cb(Si ) 6= Cp(Si ) Cb(Si−1 ) 6= Cb(Si ) Continue Shifting Retain Table 1: Three types of centering transitions. the focus of a sentence, we model the backwardlooking center and forward-looking centers using scores computed by multi-head self-attention in XLNet. Recent work shows that multi-head attention of a pretrained language model represents important linguistic notions of the input sequence (Clark et al., 2019; Vig and Belinkov, 2019; Sen et al., 2020). It also claims that self-attention might be biased to specialized tokens used in training, <SEP>, <CLS> and the token of a punctuation mark, hence we only consider actual items by filtering these tokens. Following previous work, we use the averaged scores of the multi-head self-attention extracted from the last layer of the model. To identify the salient items of sentences, we encode each sentence separately to identify centers of the sentence. To determine the forward-looking centers of the sentence at the word level, we extract diagonal elements of the matrix representin"
2020.emnlp-main.604,D19-1098,0,0.294485,"Cpi ) 8: if simCbi ,Cbi−1 > tsim then 9: if simCpi ,Cbi > tsim then 10: isCont ← T rue 11: else 12: Push(f stack, Seg) 13: Seg ← {} 14: end if 15: break . Exit the loop 16: else 17: Pop(f stack) 18: isCont ← F alse 19: end if 20: end while 21: if ∼ isCont and f stack = then 22: Push(f stack, Seg) 23: Seg ← {} 24: end if 25: end for 26: Adj M at = Gen Ad Mat(Adj List) 27: return Adj M at 28: end procedure 7461 3.3 Structure-aware Transformer To take structural information into account, we propose a structure-aware transformer. Our structure-aware transformer is inspired by the TreeTransformer (Wang et al., 2019), which updates its hidden representations by inducing a tree-structure from a document. The Tree-Transformer generates constituent priors by calculating neighboring attention which represents the probability whether adjacent items are in the same constituent. The constituent priors constrain the self-attention of the transformer to follow the induced structure. Instead of inducing a tree structure, our model uses input structural information to generate document structure priors, which guide the self-attention of the transformer. The sentences which are not connected in the structure are cons"
2020.emnlp-main.604,P17-2029,0,0.105718,"-hds coherence arises not only at the local level, but also at the document level giving insight into the structure of the discourse. Discourse structure represents the semantic organization of a text. Incorporating structural information into the model has been beneficial for diverse downstream tasks including text summarization (Marcu, 2000), translation (Guzm´an et al., 2014), sentiment analysis (Bhatia et al., 2015), and text classification (Ji and Smith, 2017). To identify discourse structure, earlier work adopts a supervised approach, relying on human annotations (Hernault et al., 2010; Wang et al., 2017). However, annotating discourse structure is time consuming and costly. It requires annotators to understand not only the local context surrounding the target sentence but also higher level relations. Learning latent structure has been proposed to alleviate this limitation. This approach induces the discourse structure from a text without annotations using an attention layer (Liu and Lapata, 2018). Recent work argues that, however, the learned trees have mostly little to no structure at the document level, and the model relies on specific linguistic cues (Ferracane et al., 2019). In this paper"
2020.findings-emnlp.42,E17-1075,0,0.0759234,"correlations has become critical to improve performance. Large inventories tend to exhibit a hierarchical structure, either by an explicit tree-like arrangement of the labels (coarse labels at the top, finegrained at the bottom), or implicitly through the label distribution in the dataset (coarse labels appear more frequently than fine-grained ones). Prior work has integrated only explicit hierarchical information by formulating a hierarchy-aware loss (Murty et al., 2018; Xu and Barbosa, 2018) or by representing instances and labels in a joint Euclidean embedding space (Shimaoka et al., 2017; Abhishek et al., 2017). However, the resulting space is hard to interpret, and these methods fail to capture implicit relations in the label inventory. Hyperbolic space is naturally equipped for embedding symbolic data with hierarchical structures (Nickel and Kiela, 2017). Intuitively, that is because the amount of space grows exponentially as points move away from the origin. This mirrors the exponential growth of the number of nodes in trees with increasing distance from the root (Cho et al., 2019) (see Figure 1). 1 Code available at: https://github.com/nlpAThits/hyfi In this work, we propose a fully hyperbolic n"
2020.findings-emnlp.42,D14-1179,0,0.0135572,"Missing"
2020.findings-emnlp.42,W19-4319,1,0.76232,"Missing"
2020.findings-emnlp.42,P18-1010,0,0.165012,"xtremely large inventories, with hundreds (Gillick et al., 2014) or thousands of labels (Choi et al., 2018). Therefore, exploiting inter-label correlations has become critical to improve performance. Large inventories tend to exhibit a hierarchical structure, either by an explicit tree-like arrangement of the labels (coarse labels at the top, finegrained at the bottom), or implicitly through the label distribution in the dataset (coarse labels appear more frequently than fine-grained ones). Prior work has integrated only explicit hierarchical information by formulating a hierarchy-aware loss (Murty et al., 2018; Xu and Barbosa, 2018) or by representing instances and labels in a joint Euclidean embedding space (Shimaoka et al., 2017; Abhishek et al., 2017). However, the resulting space is hard to interpret, and these methods fail to capture implicit relations in the label inventory. Hyperbolic space is naturally equipped for embedding symbolic data with hierarchical structures (Nickel and Kiela, 2017). Intuitively, that is because the amount of space grows exponentially as points move away from the origin. This mirrors the exponential growth of the number of nodes in trees with increasing distance fr"
2020.findings-emnlp.42,N19-1250,0,0.677866,"in the Poincar´e model, Gulcehre et al. (2019) aggregate points in the Klein model, or Nickel and Kiela (2018) perform optimization in the Lorentz model). We address these issues. Our model encodes textual inputs, applies a novel attention mechanism, and performs multi-class multilabel classification, executing all operations in the Poincar´e model of hyperbolic space (§4). We evaluate the model on two datasets, namely Ultra-Fine (Choi et al., 2018) and OntoNotes (Gillick et al., 2014), and compare to Euclidean baselines as well as to state-of-the-art methods for the task (Xiong et al., 2019; Onoe and Durrett, 2019). The hyperbolic system has competitive performance when compared to an ELMo model (Peters et al., 2018) and a BERT model (Devlin et al., 2019) on very fine-grained types, with remarkable reduction of the parameter size (§6). Instead of relying on large pre-trained models, we impose a suitable inductive bias by choosing an adequate metric space to embed the data, which does not introduce extra burden on the parameter footprint. 1.00 1.00 y 0.75 y⊕x 0.50 0.75 x⊕y 0.50 0.25 M= 0.25 0.00  −1 −1.5 .2 .5  σ ⊗ (x) 0.00 x −0.25 −0.25 −0.50 −0.50 −0.75 −0.75 −1.00 x M ⊗x −1.00 −1.0 −0.5 0.0 0.5 1.0"
2020.findings-emnlp.42,D14-1162,0,0.0876335,"on is that the separation planes for these labels are located closer to the origin of the space. In this region, the spaces behave alike in terms of the distance calculation, and this similarity is reflected in the results as well. 6.1.1 Word Embeddings Ablation The input for both the Euclidean and hyperbolic models are Poincar´e GloVe embeddings, which are originally trained in hyperbolic space (Tifrea et al., 2019). This might favor the hyperbolic model, despite the application of the log0 map in the Euclidean case. Thus, we replace the hyperbolic embeddings by the regular GloVe embeddings (Pennington et al., 2014), and use exp0 on the hyperbolic model to project them into the ball. Table 4 shows that the tendency of the BASE hyperbolic model outperforming the Euclidean one A comparison of the metric spaces for different models on the test set is shown in Table 3. It can be seen that the hyperbolic model outperforms its Euclidean variants in all settings. It is notable that this trend holds even in high-dimensional spaces (500 dimensions for X L ARGE). Since the label inventory exhibits a clearly hierarchical structure, it perfectly suits the hyperbolic classification method. 466 Model Coarse Ma Mi Fine"
2020.findings-emnlp.42,N18-1202,0,0.347733,"18) perform optimization in the Lorentz model). We address these issues. Our model encodes textual inputs, applies a novel attention mechanism, and performs multi-class multilabel classification, executing all operations in the Poincar´e model of hyperbolic space (§4). We evaluate the model on two datasets, namely Ultra-Fine (Choi et al., 2018) and OntoNotes (Gillick et al., 2014), and compare to Euclidean baselines as well as to state-of-the-art methods for the task (Xiong et al., 2019; Onoe and Durrett, 2019). The hyperbolic system has competitive performance when compared to an ELMo model (Peters et al., 2018) and a BERT model (Devlin et al., 2019) on very fine-grained types, with remarkable reduction of the parameter size (§6). Instead of relying on large pre-trained models, we impose a suitable inductive bias by choosing an adequate metric space to embed the data, which does not introduce extra burden on the parameter footprint. 1.00 1.00 y 0.75 y⊕x 0.50 0.75 x⊕y 0.50 0.25 M= 0.25 0.00  −1 −1.5 .2 .5  σ ⊗ (x) 0.00 x −0.25 −0.25 −0.50 −0.50 −0.75 −0.75 −1.00 x M ⊗x −1.00 −1.0 −0.5 0.0 0.5 1.0 −1.0 −0.5 0.0 0.5 1.0 Figure 2: Visualization of M¨obius operations. Left: M¨obius addition (noncommutat"
2020.findings-emnlp.42,D16-1144,0,0.163365,"ble 7. The hyperbolic model, without requiring the explicit hierarchy provided in this dataset, achieves a competitive performance. Nonetheless, the advantages of the hyperbolic model are mitigated by the low multiplicity of fine-grained labels, and the lower hierarchy. 7 Related Work Type inventories for the task of fine-grained entity typing (Ling and Weld, 2012; Yosef et al., 2012) have grown in size and complexity (Del Corro et al., 2015; Choi et al., 2018). Researchers have tried to incorporate hierarchical information on the type distribution in different manners (Shimaoka et al., 2016; Ren et al., 2016a). Shimaoka et al. (2017) encode the hierarchy through a sparse matrix. Xu Computing time: M¨obius operations are more expensive than their Euclidean counterparts. Due to this, in our experiments we found the hyperbolic encoder to be twice slower, and the MLR 1.5 times slower than their Euclidean versions. 467 Model Shimaoka et al. (2017) AFET (Ren et al., 2016a) PLE (Ren et al., 2016b) BERT M ULTI TASK L ABEL GCN HY LARGE Acc. 51.7 55.1 57.2 51.8 59.5 59.6 47.4 Ma-F1 70.9 71.1 71.5 76.6 76.8 77.8 75.8 Mi-F1 64.9 64.7 66.1 69.1 71.8 72.2 69.4 Table 7: Total accuracy, macro and micro F1 scores"
2020.findings-emnlp.42,W03-0419,0,0.374078,"Missing"
2020.findings-emnlp.42,W16-1313,0,0.0176356,"th neural systems in Table 7. The hyperbolic model, without requiring the explicit hierarchy provided in this dataset, achieves a competitive performance. Nonetheless, the advantages of the hyperbolic model are mitigated by the low multiplicity of fine-grained labels, and the lower hierarchy. 7 Related Work Type inventories for the task of fine-grained entity typing (Ling and Weld, 2012; Yosef et al., 2012) have grown in size and complexity (Del Corro et al., 2015; Choi et al., 2018). Researchers have tried to incorporate hierarchical information on the type distribution in different manners (Shimaoka et al., 2016; Ren et al., 2016a). Shimaoka et al. (2017) encode the hierarchy through a sparse matrix. Xu Computing time: M¨obius operations are more expensive than their Euclidean counterparts. Due to this, in our experiments we found the hyperbolic encoder to be twice slower, and the MLR 1.5 times slower than their Euclidean versions. 467 Model Shimaoka et al. (2017) AFET (Ren et al., 2016a) PLE (Ren et al., 2016b) BERT M ULTI TASK L ABEL GCN HY LARGE Acc. 51.7 55.1 57.2 51.8 59.5 59.6 47.4 Ma-F1 70.9 71.1 71.5 76.6 76.8 77.8 75.8 Mi-F1 64.9 64.7 66.1 69.1 71.8 72.2 69.4 Table 7: Total accuracy, macro a"
2020.findings-emnlp.42,E17-1119,0,0.401225,"exploiting inter-label correlations has become critical to improve performance. Large inventories tend to exhibit a hierarchical structure, either by an explicit tree-like arrangement of the labels (coarse labels at the top, finegrained at the bottom), or implicitly through the label distribution in the dataset (coarse labels appear more frequently than fine-grained ones). Prior work has integrated only explicit hierarchical information by formulating a hierarchy-aware loss (Murty et al., 2018; Xu and Barbosa, 2018) or by representing instances and labels in a joint Euclidean embedding space (Shimaoka et al., 2017; Abhishek et al., 2017). However, the resulting space is hard to interpret, and these methods fail to capture implicit relations in the label inventory. Hyperbolic space is naturally equipped for embedding symbolic data with hierarchical structures (Nickel and Kiela, 2017). Intuitively, that is because the amount of space grows exponentially as points move away from the origin. This mirrors the exponential growth of the number of nodes in trees with increasing distance from the root (Cho et al., 2019) (see Figure 1). 1 Code available at: https://github.com/nlpAThits/hyfi In this work, we prop"
2020.findings-emnlp.42,N19-1084,0,0.632684,"9) learn embeddings in the Poincar´e model, Gulcehre et al. (2019) aggregate points in the Klein model, or Nickel and Kiela (2018) perform optimization in the Lorentz model). We address these issues. Our model encodes textual inputs, applies a novel attention mechanism, and performs multi-class multilabel classification, executing all operations in the Poincar´e model of hyperbolic space (§4). We evaluate the model on two datasets, namely Ultra-Fine (Choi et al., 2018) and OntoNotes (Gillick et al., 2014), and compare to Euclidean baselines as well as to state-of-the-art methods for the task (Xiong et al., 2019; Onoe and Durrett, 2019). The hyperbolic system has competitive performance when compared to an ELMo model (Peters et al., 2018) and a BERT model (Devlin et al., 2019) on very fine-grained types, with remarkable reduction of the parameter size (§6). Instead of relying on large pre-trained models, we impose a suitable inductive bias by choosing an adequate metric space to embed the data, which does not introduce extra burden on the parameter footprint. 1.00 1.00 y 0.75 y⊕x 0.50 0.75 x⊕y 0.50 0.25 M= 0.25 0.00  −1 −1.5 .2 .5  σ ⊗ (x) 0.00 x −0.25 −0.25 −0.50 −0.50 −0.75 −0.75 −1.00 x M ⊗x −1."
2020.findings-emnlp.42,N18-1002,0,0.199346,"tories, with hundreds (Gillick et al., 2014) or thousands of labels (Choi et al., 2018). Therefore, exploiting inter-label correlations has become critical to improve performance. Large inventories tend to exhibit a hierarchical structure, either by an explicit tree-like arrangement of the labels (coarse labels at the top, finegrained at the bottom), or implicitly through the label distribution in the dataset (coarse labels appear more frequently than fine-grained ones). Prior work has integrated only explicit hierarchical information by formulating a hierarchy-aware loss (Murty et al., 2018; Xu and Barbosa, 2018) or by representing instances and labels in a joint Euclidean embedding space (Shimaoka et al., 2017; Abhishek et al., 2017). However, the resulting space is hard to interpret, and these methods fail to capture implicit relations in the label inventory. Hyperbolic space is naturally equipped for embedding symbolic data with hierarchical structures (Nickel and Kiela, 2017). Intuitively, that is because the amount of space grows exponentially as points move away from the origin. This mirrors the exponential growth of the number of nodes in trees with increasing distance from the root (Cho et al."
2020.findings-emnlp.42,C12-2133,0,0.0634387,"Missing"
2020.lrec-1.697,J08-4004,0,0.044718,"Missing"
2020.lrec-1.697,N19-1423,0,0.0182501,"wo is the choice of context words. P RE W IN uses a predicate window of context words, which is a set of words originating from the dependency head of the PMW. The key intuition behind the predicate window is that the immediate context words are frequently noisy and redundant. The predicate window is a small and focused set of context words. The length of the predicate window is set to 5. 5.2. Results We evaluate each method using the following classification metrics: precision, recall and F1-score. The results for different word embeddings such as G LOV E (Pennington et al., 2014) and B ERT (Devlin et al., 2019) are reported. In the case of G LOV E, the vocabulary is made up of only the top 100,000 most-frequent words. The zero vector is used to represent the out-of-vocabulary words. We use the pretrained 50d (6B version) words vectors in our experiments. In contrast to G LOV E, B ERT embeddings are contextsensitive and hence are able to distinguish, for example, different senses of polysemous words from each other. We use the pre-trained base, uncased version of B ERT in our experiments. Instead of deploying B ERT as a classifier, we use the P RE W IN model and initialize it with the G LOV E-like wo"
2020.lrec-1.697,L18-1408,0,0.0319489,"Missing"
2020.lrec-1.697,J91-1003,0,0.908902,"Missing"
2020.lrec-1.697,L18-1079,0,0.0238504,"as weighted vectors of Wikipedia articles. Nastase et al. (2010) computes values of selectional preference features using the Wikipedia category network. Wikipedia is also used to construct datasets for various tasks such as coreference resolution (Ghaddar and Langlais, 2016), conflict-of-interest detection (Orizu and He, 2018), and concept relatedness (Dor et al., 2018). Ghaddar and Langlais (2018) and Mihalcea (2007) exploit the internal hyperlink structure of Wikipedia to build large, annotated corpora for the tasks of fine-grained entity typing and word sense disambiguation respectively. Ge et al. (2018) creates a resource composed of the major events in human history using event-related infobox templates of Wikipedia. The structural information from articles is used to harvest interevent relations. 2.2. Metonymy Resolution The task of metonymy resolution aims to identify words that are used metonymically and interpret them appropriately. Markert and Nissim (2002) resolves metonymy using co-occurrences, collocations and grammatical features. Nastase and Strube (2009) and Nastase et al. (2012) use selectional preference features, which are computed using external resources such as British Nati"
2020.lrec-1.697,L16-1021,0,0.0219132,"million articles in the English Wikipedia). Wikipedia follows a semi-structured format through its use of infobox templates, table of contents inside articles, category network and disambiguation pages. Wikipedia is used in NLP research because it is an excellent source of world knowledge. Gabrilovich and Markovitch (2007) computes word embeddings as weighted vectors of Wikipedia articles. Nastase et al. (2010) computes values of selectional preference features using the Wikipedia category network. Wikipedia is also used to construct datasets for various tasks such as coreference resolution (Ghaddar and Langlais, 2016), conflict-of-interest detection (Orizu and He, 2018), and concept relatedness (Dor et al., 2018). Ghaddar and Langlais (2018) and Mihalcea (2007) exploit the internal hyperlink structure of Wikipedia to build large, annotated corpora for the tasks of fine-grained entity typing and word sense disambiguation respectively. Ge et al. (2018) creates a resource composed of the major events in human history using event-related infobox templates of Wikipedia. The structural information from articles is used to harvest interevent relations. 2.2. Metonymy Resolution The task of metonymy resolution aims"
2020.lrec-1.697,L18-1699,0,0.0214584,"table of contents inside articles, category network and disambiguation pages. Wikipedia is used in NLP research because it is an excellent source of world knowledge. Gabrilovich and Markovitch (2007) computes word embeddings as weighted vectors of Wikipedia articles. Nastase et al. (2010) computes values of selectional preference features using the Wikipedia category network. Wikipedia is also used to construct datasets for various tasks such as coreference resolution (Ghaddar and Langlais, 2016), conflict-of-interest detection (Orizu and He, 2018), and concept relatedness (Dor et al., 2018). Ghaddar and Langlais (2018) and Mihalcea (2007) exploit the internal hyperlink structure of Wikipedia to build large, annotated corpora for the tasks of fine-grained entity typing and word sense disambiguation respectively. Ge et al. (2018) creates a resource composed of the major events in human history using event-related infobox templates of Wikipedia. The structural information from articles is used to harvest interevent relations. 2.2. Metonymy Resolution The task of metonymy resolution aims to identify words that are used metonymically and interpret them appropriately. Markert and Nissim (2002) resolves metonymy u"
2020.lrec-1.697,P17-1115,0,0.38824,"ed infobox templates of Wikipedia. The structural information from articles is used to harvest interevent relations. 2.2. Metonymy Resolution The task of metonymy resolution aims to identify words that are used metonymically and interpret them appropriately. Markert and Nissim (2002) resolves metonymy using co-occurrences, collocations and grammatical features. Nastase and Strube (2009) and Nastase et al. (2012) use selectional preference features, which are computed using external resources such as British National Corpus, Wikipedia, WordNet (Miller, 1995) and WikiNet (Nastase et al., 2010). Gritta et al. (2017) proposes a neural-networkbased model. This model is trained on a predicate window of context words, which is a set of words chosen with the the dependency head of the potentially metonymic word (PMW) as the starting point. The intuition behind the predicate window is the observation that the immediate context words are frequently noisy and hence, it is necessary to identify the right set of context words. The two existing datasets on metonymy are S EM E VAL (Markert and Nissim, 2007) and R ELOCA R (Gritta et al., 2017). The S EM E VAL data was sampled from BNC Version 1.0, and the R ELOCA R ("
2020.lrec-1.697,W98-0720,0,0.582259,"Missing"
2020.lrec-1.697,P92-1047,0,0.654288,"Missing"
2020.lrec-1.697,P19-1378,0,0.0133101,"majority class L OCATION, but do not exhibit a similar performance for the other classes. As a result, the macroaveraged results of all the methods are low when compared to the corresponding micro-averaged results. Second, the PMW is specified explicitly in the current experimental setting. This setting is easier because the classifier can exploit the context words that are indicative of a particular class to classify the PMW, while not performing anything strictly relevant to metonymy resolution (Zellers et al., 2019). A more challenging setting is where the PMW is not specified in advance (Mao et al., 2019). W I MC OR can be made to fit this new setting by considering every word (or phrase) in a sample as a PMW. This setting also enables a metonymy resolution system to be deployed on real-world data and be used for downstream tasks. 5684 5.3. 7. Discussion 5.3.1. Limitations of our Approach Metonymy is a linguistic phenomenon that can manifest itself in language in different ways such as multi-word expressions, proper nouns and common nouns (Littlemore, 2015). For instance, in the sentence “We need a couple of strong bodies on our team.” (Lakoff and Johnson, 1980), the noun phrase strong bodies"
2020.lrec-1.697,W02-1027,0,0.942754,"or et al., 2018). Ghaddar and Langlais (2018) and Mihalcea (2007) exploit the internal hyperlink structure of Wikipedia to build large, annotated corpora for the tasks of fine-grained entity typing and word sense disambiguation respectively. Ge et al. (2018) creates a resource composed of the major events in human history using event-related infobox templates of Wikipedia. The structural information from articles is used to harvest interevent relations. 2.2. Metonymy Resolution The task of metonymy resolution aims to identify words that are used metonymically and interpret them appropriately. Markert and Nissim (2002) resolves metonymy using co-occurrences, collocations and grammatical features. Nastase and Strube (2009) and Nastase et al. (2012) use selectional preference features, which are computed using external resources such as British National Corpus, Wikipedia, WordNet (Miller, 1995) and WikiNet (Nastase et al., 2010). Gritta et al. (2017) proposes a neural-networkbased model. This model is trained on a predicate window of context words, which is a set of words chosen with the the dependency head of the potentially metonymic word (PMW) as the starting point. The intuition behind the predicate windo"
2020.lrec-1.697,S07-1007,0,0.536493,"al resources such as British National Corpus, Wikipedia, WordNet (Miller, 1995) and WikiNet (Nastase et al., 2010). Gritta et al. (2017) proposes a neural-networkbased model. This model is trained on a predicate window of context words, which is a set of words chosen with the the dependency head of the potentially metonymic word (PMW) as the starting point. The intuition behind the predicate window is the observation that the immediate context words are frequently noisy and hence, it is necessary to identify the right set of context words. The two existing datasets on metonymy are S EM E VAL (Markert and Nissim, 2007) and R ELOCA R (Gritta et al., 2017). The S EM E VAL data was sampled from BNC Version 1.0, and the R ELOCA R (Real Location Retriever) data was sampled from Wikipedia. Both these corpora were compiled and labelled manually. 3. Corpus Details In this section, we describe how we extract data to construct W I MC OR. We then present some basic details of the W I MC OR corpus. In the rest of the paper, following the conventions in the literature, we refer to the word to be resolved as the PMW. Also, we refer to the title of a Wikipedia disambiguation page as the anchor text. 3.1. Resources In this"
2020.lrec-1.697,N07-1025,0,0.365537,"s, category network and disambiguation pages. Wikipedia is used in NLP research because it is an excellent source of world knowledge. Gabrilovich and Markovitch (2007) computes word embeddings as weighted vectors of Wikipedia articles. Nastase et al. (2010) computes values of selectional preference features using the Wikipedia category network. Wikipedia is also used to construct datasets for various tasks such as coreference resolution (Ghaddar and Langlais, 2016), conflict-of-interest detection (Orizu and He, 2018), and concept relatedness (Dor et al., 2018). Ghaddar and Langlais (2018) and Mihalcea (2007) exploit the internal hyperlink structure of Wikipedia to build large, annotated corpora for the tasks of fine-grained entity typing and word sense disambiguation respectively. Ge et al. (2018) creates a resource composed of the major events in human history using event-related infobox templates of Wikipedia. The structural information from articles is used to harvest interevent relations. 2.2. Metonymy Resolution The task of metonymy resolution aims to identify words that are used metonymically and interpret them appropriately. Markert and Nissim (2002) resolves metonymy using co-occurrences,"
2020.lrec-1.697,D09-1095,1,0.824516,"e of Wikipedia to build large, annotated corpora for the tasks of fine-grained entity typing and word sense disambiguation respectively. Ge et al. (2018) creates a resource composed of the major events in human history using event-related infobox templates of Wikipedia. The structural information from articles is used to harvest interevent relations. 2.2. Metonymy Resolution The task of metonymy resolution aims to identify words that are used metonymically and interpret them appropriately. Markert and Nissim (2002) resolves metonymy using co-occurrences, collocations and grammatical features. Nastase and Strube (2009) and Nastase et al. (2012) use selectional preference features, which are computed using external resources such as British National Corpus, Wikipedia, WordNet (Miller, 1995) and WikiNet (Nastase et al., 2010). Gritta et al. (2017) proposes a neural-networkbased model. This model is trained on a predicate window of context words, which is a set of words chosen with the the dependency head of the potentially metonymic word (PMW) as the starting point. The intuition behind the predicate window is the observation that the immediate context words are frequently noisy and hence, it is necessary to"
2020.lrec-1.697,nastase-etal-2010-wikinet,1,0.853323,"ce the task of metonymy resolution and the existing datasets on metonymy. 2.1. Wikipedia Wikipedia is a crowd-sourced, encyclopedic resource and is massive in size (as of 4 November 2019, there are over 5.9 million articles in the English Wikipedia). Wikipedia follows a semi-structured format through its use of infobox templates, table of contents inside articles, category network and disambiguation pages. Wikipedia is used in NLP research because it is an excellent source of world knowledge. Gabrilovich and Markovitch (2007) computes word embeddings as weighted vectors of Wikipedia articles. Nastase et al. (2010) computes values of selectional preference features using the Wikipedia category network. Wikipedia is also used to construct datasets for various tasks such as coreference resolution (Ghaddar and Langlais, 2016), conflict-of-interest detection (Orizu and He, 2018), and concept relatedness (Dor et al., 2018). Ghaddar and Langlais (2018) and Mihalcea (2007) exploit the internal hyperlink structure of Wikipedia to build large, annotated corpora for the tasks of fine-grained entity typing and word sense disambiguation respectively. Ge et al. (2018) creates a resource composed of the major events"
2020.lrec-1.697,D12-1017,1,0.88517,"Missing"
2020.lrec-1.697,P03-1008,0,0.769405,"list of class labels. The majority class classifier picks the label having the largest number of observations in the training set. These are simple classifiers because they do not make use of the context or any other information to make a decision. We use these classifiers to evaluate the performance of better informed classifiers. 5.1.2. Immediate context The words surrounding the PMW are very useful in detecting its metonymicity. Traditional machine-learning techniques in metonymy resolution relied on context-based features such as co-occurrences and collocations (Markert and Nissim, 2002; Nissim and Markert, 2003; Markert and Nissim, 2005). The I MM baseline is a neural-network-based model that resolves metonymy using the immediate context of the PMW. We created three variants of the I MM baseline: I MM 5, I MM 10 and I MM 50. The length of the context is different in each I MM variant. For instance, I MM 5 uses a context of length 5 words, from either side of the PMW. 5.1.3. Predicate window P RE W IN (Gritta et al., 2017) employs a neural-networkbased model. The model consists of four input layers in parallel: two LSTM layers for the right and left context words, and two dense layers for the depende"
2020.lrec-1.697,L18-1026,0,0.0192643,"s a semi-structured format through its use of infobox templates, table of contents inside articles, category network and disambiguation pages. Wikipedia is used in NLP research because it is an excellent source of world knowledge. Gabrilovich and Markovitch (2007) computes word embeddings as weighted vectors of Wikipedia articles. Nastase et al. (2010) computes values of selectional preference features using the Wikipedia category network. Wikipedia is also used to construct datasets for various tasks such as coreference resolution (Ghaddar and Langlais, 2016), conflict-of-interest detection (Orizu and He, 2018), and concept relatedness (Dor et al., 2018). Ghaddar and Langlais (2018) and Mihalcea (2007) exploit the internal hyperlink structure of Wikipedia to build large, annotated corpora for the tasks of fine-grained entity typing and word sense disambiguation respectively. Ge et al. (2018) creates a resource composed of the major events in human history using event-related infobox templates of Wikipedia. The structural information from articles is used to harvest interevent relations. 2.2. Metonymy Resolution The task of metonymy resolution aims to identify words that are used metonymically and in"
2020.lrec-1.697,D14-1162,0,0.0828833,"Missing"
2020.lrec-1.697,S07-1093,0,0.0979428,"Missing"
2020.lrec-1.697,P93-1012,0,0.880121,"Missing"
2020.lrec-1.697,P95-1026,0,0.290333,"list of metonymic associations we use to extract data. A metonymic pair and the corresponding anchor text is also given. 3.2.1. Metonymic Pair Extraction We use the Wikipedia disambiguation pages to harvest metonymic pairs. A metonymic pair hWL , WM i is a pair of Wikipedia articles that are referred to by the same natural title but denotes two different but strongly related concepts, such as Delft and Delft University of Technology. The samples in W I MC OR are generated using these metonymic pairs. Note that metonymy is different from other linguistic phenomena such as homonymy or polysemy (Yarowsky, 1995). For instance, the city of Paris in France and Paris Hilton, the singer, do not form a metonymic pair because of the lack of any strong relationship between these two entities, although both entities can be referred to by the same term Paris. On the other hand, Delft and Delft University of Technology form a metonymic pair because the university is located in the city and both the city and the university can be referred to by the same term Delft. As shown above, it is important to distinguish metonymic pairs in Wikipedia disambiguation pages from nonmetonymic (polysemous) pairs. Our two-step"
2020.lrec-1.697,P19-1472,0,0.0135271,"akes into account the proportion of each class in the data. The classifiers perform well for the majority class L OCATION, but do not exhibit a similar performance for the other classes. As a result, the macroaveraged results of all the methods are low when compared to the corresponding micro-averaged results. Second, the PMW is specified explicitly in the current experimental setting. This setting is easier because the classifier can exploit the context words that are indicative of a particular class to classify the PMW, while not performing anything strictly relevant to metonymy resolution (Zellers et al., 2019). A more challenging setting is where the PMW is not specified in advance (Mao et al., 2019). W I MC OR can be made to fit this new setting by considering every word (or phrase) in a sample as a PMW. This setting also enables a metonymy resolution system to be deployed on real-world data and be used for downstream tasks. 5684 5.3. 7. Discussion 5.3.1. Limitations of our Approach Metonymy is a linguistic phenomenon that can manifest itself in language in different ways such as multi-word expressions, proper nouns and common nouns (Littlemore, 2015). For instance, in the sentence “We need a coup"
2020.sdp-1.9,D18-1476,0,0.0629802,"Missing"
2021.codi-sharedtask.1,D19-1422,0,0.0559867,"variant that is only trained on bridging annotations. We evaluated their best-performing model, which was trained on the RST sub-corpus of ARRAU, on CODI - CRAC 2021 data.19 The baseline for Task 3 leverages a simple heuristic that only considers demonstrative pronouns (this, that) as anaphors and considers the immediately preceding clause/utterance in the conversation to be their antecedent. Although simplistic, the algorithm achieves respectable scores on the CODI CRAC 2021 development corpus. The performance KU_NLP submitted results for tasks 1 and 2. For identity anaphora, they leveraged Cui and Zhang (2019)’s model with an ELECTRA-large backbone 18 20 The necessary scripts are available from https:// github.com/sopankhosla/codi2021_scripts 21 Participants were allowed to create teams. https://github.com/lxucs/coref-hoi/ https://github.com/juntaoy/ dali-bridging 19 8 9 Discourse Deixis Resolution Bridging Resolution Anaphora Resolution Track - Joshi et al. Leverage baseline’s architecture to find the correct (2019) bridging antecedent in the gold setting. KU_NLP INRIA - Yu and Poe- A multi-pass sieve approach which used the baseline sio (2020) as one of the sieves and consists of a set of learnin"
2021.codi-sharedtask.1,E89-1022,0,0.449685,"v., USA; 2 Univ. of Essex, UK; 3 Intel Labs, USA; 4 UT Dallas, USA; 5 Queen Mary Univ., UK; 6 HITS, Germany; 7 AWS AI, Amazon, USA sopankh@amazon.com; j.yu@essex.ac.uk; ramesh.manuvinakurike@intel.com; vince@hlt.utdallas.edu; m.poesio@qmul.ac.uk; Michael.Strube@h-its.org; cprose@cs.cmu.edu Abstract such as ARRAU (Poesio et al., 2018; Uryupina et al., 2020). These include, e.g., bridging reference (Clark, 1977; Hou et al., 2018; Hou, 2020; Yu and Poesio, 2020; Kobayashi and Ng, 2021), discourse deixis (Webber, 1991; Marasovi´c et al., 2017; Kolhatkar et al., 2018) or split-antecedent anaphora (Eschenbach et al., 1989; Vala et al., 2016; Zhou and Choi, 2018; Yu et al., 2020b, 2021). There has been interest in other genres apart from news. This includes substantial research on annotating and resolving coreference in biomedical and other scientific domains (Cohen et al., 2017; Lu and Poesio, 2021) as well as in literary documents (Bamman et al., 2020). There are, however, language genres still understudied in the literature on anaphoric reference. Arguably the most important among these is conversational language in dialogue. Anaphora resolution in dialogue requires systems to handle grammatically incorrect"
2021.codi-sharedtask.1,J18-3007,0,0.521559,"chael Strube6 , and Carolyn Rosé1 1 Canegie Mellon Univ., USA; 2 Univ. of Essex, UK; 3 Intel Labs, USA; 4 UT Dallas, USA; 5 Queen Mary Univ., UK; 6 HITS, Germany; 7 AWS AI, Amazon, USA sopankh@amazon.com; j.yu@essex.ac.uk; ramesh.manuvinakurike@intel.com; vince@hlt.utdallas.edu; m.poesio@qmul.ac.uk; Michael.Strube@h-its.org; cprose@cs.cmu.edu Abstract such as ARRAU (Poesio et al., 2018; Uryupina et al., 2020). These include, e.g., bridging reference (Clark, 1977; Hou et al., 2018; Hou, 2020; Yu and Poesio, 2020; Kobayashi and Ng, 2021), discourse deixis (Webber, 1991; Marasovi´c et al., 2017; Kolhatkar et al., 2018) or split-antecedent anaphora (Eschenbach et al., 1989; Vala et al., 2016; Zhou and Choi, 2018; Yu et al., 2020b, 2021). There has been interest in other genres apart from news. This includes substantial research on annotating and resolving coreference in biomedical and other scientific domains (Cohen et al., 2017; Lu and Poesio, 2021) as well as in literary documents (Bamman et al., 2020). There are, however, language genres still understudied in the literature on anaphoric reference. Arguably the most important among these is conversational language in dialogue. Anaphora resolution in dialog"
2021.codi-sharedtask.1,D17-1018,0,0.0879633,"iple tasks focusing individually on these key relations. We discuss the evaluation scripts used to assess the system performance on these tasks, and provide a brief summary of the participating systems and the results obtained across 115 runs from six teams, with most submissions achieving significantly better results than our baseline methods. 1 Introduction The performance of models for single-antecedent anaphora resolution on the aspects of anaphoric interpretation annotated in the standard ONTONOTES dataset (Pradhan et al., 2012) has greatly improved in recent years (Wiseman et al., 2015; Lee et al., 2017, 2018; Kantor and Globerson, 2019; Joshi et al., 2020). So the attention of the community has started to turn to more complex cases of anaphora not found or not properly tested in ONTONOTES. Well-known examples of this trend are work on the cases of anaphora whose interpretation requires some form of commonsense knowledge tested by benchmarks for the Winograd Schema Challenge (Rahman and Ng, 2012; Liu et al., 2017; Sakaguchi et al., 2020), or the pronominal anaphors that cannot be resolved purely using gender, for which benchmarks such as GAP have been developed (Webster et al., 2018). GAP, h"
2021.codi-sharedtask.1,N18-2108,0,0.205145,"Missing"
2021.codi-sharedtask.1,2020.acl-main.132,0,0.135367,"urse Deixis in Dialogue Sopan Khosla1,7∗, Juntao Yu2 , Ramesh Manuvinakurike3 , Vincent Ng4 , Massimo Poesio5 , Michael Strube6 , and Carolyn Rosé1 1 Canegie Mellon Univ., USA; 2 Univ. of Essex, UK; 3 Intel Labs, USA; 4 UT Dallas, USA; 5 Queen Mary Univ., UK; 6 HITS, Germany; 7 AWS AI, Amazon, USA sopankh@amazon.com; j.yu@essex.ac.uk; ramesh.manuvinakurike@intel.com; vince@hlt.utdallas.edu; m.poesio@qmul.ac.uk; Michael.Strube@h-its.org; cprose@cs.cmu.edu Abstract such as ARRAU (Poesio et al., 2018; Uryupina et al., 2020). These include, e.g., bridging reference (Clark, 1977; Hou et al., 2018; Hou, 2020; Yu and Poesio, 2020; Kobayashi and Ng, 2021), discourse deixis (Webber, 1991; Marasovi´c et al., 2017; Kolhatkar et al., 2018) or split-antecedent anaphora (Eschenbach et al., 1989; Vala et al., 2016; Zhou and Choi, 2018; Yu et al., 2020b, 2021). There has been interest in other genres apart from news. This includes substantial research on annotating and resolving coreference in biomedical and other scientific domains (Cohen et al., 2017; Lu and Poesio, 2021) as well as in literary documents (Bamman et al., 2020). There are, however, language genres still understudied in the literature on an"
2021.codi-sharedtask.1,J18-2002,1,0.943726,"ridging, and Discourse Deixis in Dialogue Sopan Khosla1,7∗, Juntao Yu2 , Ramesh Manuvinakurike3 , Vincent Ng4 , Massimo Poesio5 , Michael Strube6 , and Carolyn Rosé1 1 Canegie Mellon Univ., USA; 2 Univ. of Essex, UK; 3 Intel Labs, USA; 4 UT Dallas, USA; 5 Queen Mary Univ., UK; 6 HITS, Germany; 7 AWS AI, Amazon, USA sopankh@amazon.com; j.yu@essex.ac.uk; ramesh.manuvinakurike@intel.com; vince@hlt.utdallas.edu; m.poesio@qmul.ac.uk; Michael.Strube@h-its.org; cprose@cs.cmu.edu Abstract such as ARRAU (Poesio et al., 2018; Uryupina et al., 2020). These include, e.g., bridging reference (Clark, 1977; Hou et al., 2018; Hou, 2020; Yu and Poesio, 2020; Kobayashi and Ng, 2021), discourse deixis (Webber, 1991; Marasovi´c et al., 2017; Kolhatkar et al., 2018) or split-antecedent anaphora (Eschenbach et al., 1989; Vala et al., 2016; Zhou and Choi, 2018; Yu et al., 2020b, 2021). There has been interest in other genres apart from news. This includes substantial research on annotating and resolving coreference in biomedical and other scientific domains (Cohen et al., 2017; Lu and Poesio, 2021) as well as in literary documents (Bamman et al., 2020). There are, however, language genres still understudied in the liter"
2021.codi-sharedtask.1,L16-1145,0,0.382038,"Missing"
2021.codi-sharedtask.1,2020.tacl-1.5,0,0.344926,"ons. We discuss the evaluation scripts used to assess the system performance on these tasks, and provide a brief summary of the participating systems and the results obtained across 115 runs from six teams, with most submissions achieving significantly better results than our baseline methods. 1 Introduction The performance of models for single-antecedent anaphora resolution on the aspects of anaphoric interpretation annotated in the standard ONTONOTES dataset (Pradhan et al., 2012) has greatly improved in recent years (Wiseman et al., 2015; Lee et al., 2017, 2018; Kantor and Globerson, 2019; Joshi et al., 2020). So the attention of the community has started to turn to more complex cases of anaphora not found or not properly tested in ONTONOTES. Well-known examples of this trend are work on the cases of anaphora whose interpretation requires some form of commonsense knowledge tested by benchmarks for the Winograd Schema Challenge (Rahman and Ng, 2012; Liu et al., 2017; Sakaguchi et al., 2020), or the pronominal anaphors that cannot be resolved purely using gender, for which benchmarks such as GAP have been developed (Webster et al., 2018). GAP, however, still focused on identity coreference. In addit"
2021.codi-sharedtask.1,D19-1588,0,0.0733603,"om the end-to-end neural coreference resolution model of Joshi et al. (2020). They recognized singletons, encoded speakers for all turns, and leveraged other out-of-domain datasets during training. Eval DD (Pred) UTD_NLP DFKI 42.70 20.97 35.35 17.43 39.64 23.76 35.43 23.86 38.3 21.5 Baseline 12.12 15.75 18.27 13.55 14.9 Table 5: Performance on Task 3 (Evaluation Phase) – Discourse Deixis (CoNLL Avg. F1) INRIA submitted an end-to-end transformerbased model fine-tuned for the bridging resolution task. They formulated the bridging problem as antecedent selection, and leveraged Lee et al. (2018); Joshi et al. (2019)’s architecture to find the correct antecedent. for mention detection. The resulting mention representation, created from the constituent token representations, is then fed to a pointer-network (Vinyals et al., 2015) based coreference resolution model for clustering. They solved the bridging resolution problem using a machine reading comprehension framework, where they constructed a query for each entity of the form – ""What is related of ENTITY?"". The input of their model is the query and the document (i.e., all utterances of dialogue), and the output is the entity span that is the answer for"
2021.codi-sharedtask.1,2021.crac-1.2,1,0.739024,".edu Abstract such as ARRAU (Poesio et al., 2018; Uryupina et al., 2020). These include, e.g., bridging reference (Clark, 1977; Hou et al., 2018; Hou, 2020; Yu and Poesio, 2020; Kobayashi and Ng, 2021), discourse deixis (Webber, 1991; Marasovi´c et al., 2017; Kolhatkar et al., 2018) or split-antecedent anaphora (Eschenbach et al., 1989; Vala et al., 2016; Zhou and Choi, 2018; Yu et al., 2020b, 2021). There has been interest in other genres apart from news. This includes substantial research on annotating and resolving coreference in biomedical and other scientific domains (Cohen et al., 2017; Lu and Poesio, 2021) as well as in literary documents (Bamman et al., 2020). There are, however, language genres still understudied in the literature on anaphoric reference. Arguably the most important among these is conversational language in dialogue. Anaphora resolution in dialogue requires systems to handle grammatically incorrect language suffering from disfluencies. Dialogue involves much more deictic reference, vaguer anaphoric and discourse deictic reference, speaker grounding of pronouns and long-distance conversation structure. These are complexities that are often missing in news or Wikipedia articles,"
2021.codi-sharedtask.1,P19-1066,0,0.0336391,"vidually on these key relations. We discuss the evaluation scripts used to assess the system performance on these tasks, and provide a brief summary of the participating systems and the results obtained across 115 runs from six teams, with most submissions achieving significantly better results than our baseline methods. 1 Introduction The performance of models for single-antecedent anaphora resolution on the aspects of anaphoric interpretation annotated in the standard ONTONOTES dataset (Pradhan et al., 2012) has greatly improved in recent years (Wiseman et al., 2015; Lee et al., 2017, 2018; Kantor and Globerson, 2019; Joshi et al., 2020). So the attention of the community has started to turn to more complex cases of anaphora not found or not properly tested in ONTONOTES. Well-known examples of this trend are work on the cases of anaphora whose interpretation requires some form of commonsense knowledge tested by benchmarks for the Winograd Schema Challenge (Rahman and Ng, 2012; Liu et al., 2017; Sakaguchi et al., 2020), or the pronominal anaphors that cannot be resolved purely using gender, for which benchmarks such as GAP have been developed (Webster et al., 2018). GAP, however, still focused on identity"
2021.codi-sharedtask.1,D17-1021,0,0.24606,"Missing"
2021.codi-sharedtask.1,2020.coling-main.331,1,0.631559,"atasets: the AMI corpus (Carletta, 2006), the LIGHT corpus (Urbanek et al., 2019), the PERSUASION corpus (Wang et al., 2019) and SWITCHBOARD (Godfrey et al., 1992). For each of these datasets, documents for about 15K tokens were annotated for development, and about the same number of tokens were annotated for testing, according to the ARRAU annotation scheme. Bridging references Annotating — indeed, even identifying — bridging references in a reliable way is difficult, which is one of the reasons why so few large-scale corpora for anaphora include this type of annotation (Poesio et al., 2016; Kobayashi and Ng, 2020). The ARRAU guidelines for bridging anaphora are based on experiments that were started by Vieira and Poesio (Poesio and Vieira, 1998) and continued in the GNOME project (Poesio, 2004). The ARRAU Release 1 guidelines followed the GNOME guidelines, but with an extension and a simplification. Annotators were asked to mark a markable as related to a particular antecedent if it stood to that antecedent in one of the relations identified in GNOME (indeed, the same examples Switchboard SWITCHBOARD8 (Godfrey et al., 1992) is one of the best known dialogue corpora. It consists of 1,155 five-minute spo"
2021.codi-sharedtask.1,J93-2004,0,0.0756707,"ther markable and thus form a singleton coreference chain. Moreover, in ARRAU non-referring markables are manually sub-classified into expletives, predicative, and quantifiers. In addition, all generic references are marked, including premodifiers when the entity referred to is mentioned again, e.g., in the case of ARRAU: Corpus and Annotation Scheme Genres The ARRAU corpus4 (Poesio and Artstein, 2008; Uryupina et al., 2020) was designed to cover a variety of genres. It includes a substantial amount of news text in a sub-corpus called RST, consisting of the entire subset of the Penn Treebank (Marcus et al., 1993) that was annotated in the RST treebank (Carlson et al., 2003). In addition to the news data, ARRAU includes three more sub-corpora. The TRAINS sub-corpus includes all the task-oriented dialogues in the TRAINS-93 corpus5 as well as the pilot dialogues in the socalled TRAINS-91 corpus. The PEAR sub-corpus consists of the complete collection of spoken nar6 The original intention had been to use the soon-to-bereleased ARRAU 3, but as the work on this version was still under way by the time the training data had to be released, ARRAU 2 was used instead–i.e., the exact same version used for the CRA"
2021.codi-sharedtask.1,2021.naacl-main.131,1,0.669111,"osla1,7∗, Juntao Yu2 , Ramesh Manuvinakurike3 , Vincent Ng4 , Massimo Poesio5 , Michael Strube6 , and Carolyn Rosé1 1 Canegie Mellon Univ., USA; 2 Univ. of Essex, UK; 3 Intel Labs, USA; 4 UT Dallas, USA; 5 Queen Mary Univ., UK; 6 HITS, Germany; 7 AWS AI, Amazon, USA sopankh@amazon.com; j.yu@essex.ac.uk; ramesh.manuvinakurike@intel.com; vince@hlt.utdallas.edu; m.poesio@qmul.ac.uk; Michael.Strube@h-its.org; cprose@cs.cmu.edu Abstract such as ARRAU (Poesio et al., 2018; Uryupina et al., 2020). These include, e.g., bridging reference (Clark, 1977; Hou et al., 2018; Hou, 2020; Yu and Poesio, 2020; Kobayashi and Ng, 2021), discourse deixis (Webber, 1991; Marasovi´c et al., 2017; Kolhatkar et al., 2018) or split-antecedent anaphora (Eschenbach et al., 1989; Vala et al., 2016; Zhou and Choi, 2018; Yu et al., 2020b, 2021). There has been interest in other genres apart from news. This includes substantial research on annotating and resolving coreference in biomedical and other scientific domains (Cohen et al., 2017; Lu and Poesio, 2021) as well as in literary documents (Bamman et al., 2020). There are, however, language genres still understudied in the literature on anaphoric reference. Arguably the most important"
2021.codi-sharedtask.1,D14-1056,0,0.107024,"2018). That shared task was articulated around three tasks: identity coreference (including identification of nonreferring expressions), bridging references, and discourse deixis. The organization of the shared task resulted in the development of an extended version of the Coreference Reference Scorer (Pradhan et al., 2014), which also scores non-referring expressions. Separate scorers were developed for bridging reference resolution, carrying out both mention-based evaluation and entity-based evaluation of bridging references, as done by Hou et al. (2018), and for discourse deixis, based on Kolhatkar and Hirst (2014). The present shared task was modeled on that. (5) There was not a moment to be lost: away went Alice like the wind, and was just in time to hear it say, as it turned a corner, ’Oh my ears and whiskers, how late it’s getting!’ She was close behind it when she turned the corner, but the Rabbit was no longer to be seen: she found herself in [a long, low hall, which was lit up by a row of lamps hanging from [the roof]]. There were doors all round the hall, but they were all locked; and when Alice had been all the way down one side and up the other, trying every door, she walked sadly down [the mi"
2021.codi-sharedtask.1,P12-1084,1,0.830857,"es of identity anaphora exist, as well as other types of anaphoric relations that are not annotated in ONTONOTES but are annotated in other corpora. Split-antecedent anaphora In ONTONOTES, plural reference is only marked when the antecedent is mentioned by a single noun phrase. However, split-antecedent anaphors are also possible (Eschenbach et al., 1989; Kamp and Reyle, 1 https://competitions.codalab.org/ competitions/30312 2 just in time to see it pop down a large rabbithole under the hedge. and Martí, 2010), ARRAU (Uryupina et al., 2020), GNOME (Poesio, 2004), GUM (Zeldes, 2017), IS NOTES (Markert et al., 2012), the Prague Dependency Treebank (Nedoluzhko, 2013), and TÜBA DZ (Versley, 2008). (See Poesio et al. (2016) for a more detailed survey and Nedoluzhko et al. (2021) for a more recent, extensive update.) Bridging references There are other forms of anaphoric reference besides identity, and there are now a number of corpora annotating (a subset of) these forms. Possibly the most studied of nonidentity anaphora is bridging reference or associative anaphora (Clark, 1977; Hawkins, 1978; Prince, 1981) as in (5), where bridging reference / associative anaphora the roof refers to an object which is rel"
2021.codi-sharedtask.1,J98-2001,1,0.56338,"CHBOARD (Godfrey et al., 1992). For each of these datasets, documents for about 15K tokens were annotated for development, and about the same number of tokens were annotated for testing, according to the ARRAU annotation scheme. Bridging references Annotating — indeed, even identifying — bridging references in a reliable way is difficult, which is one of the reasons why so few large-scale corpora for anaphora include this type of annotation (Poesio et al., 2016; Kobayashi and Ng, 2020). The ARRAU guidelines for bridging anaphora are based on experiments that were started by Vieira and Poesio (Poesio and Vieira, 1998) and continued in the GNOME project (Poesio, 2004). The ARRAU Release 1 guidelines followed the GNOME guidelines, but with an extension and a simplification. Annotators were asked to mark a markable as related to a particular antecedent if it stood to that antecedent in one of the relations identified in GNOME (indeed, the same examples Switchboard SWITCHBOARD8 (Godfrey et al., 1992) is one of the best known dialogue corpora. It consists of 1,155 five-minute spontaneous telephone conversations between two participants not previously acquainted with each other. In these conversations, callers q"
2021.codi-sharedtask.1,W00-1007,0,0.805051,"nt. . . The statement was the [US]1 ’s government first acknowledgment of what other groups, such as the International Monetary Fund, have been predicting for months. • an undersp-rel relation for ‘obvious cases of bridging that didn’t fit any other category’. Discourse deixis Discourse deixis in its full form is a very complex form of reference, both to annotate (Kolhatkar et al., 2018) and to resolve. Very few anaphoric annotation projects have attempted to annotate discourse deixis in its entirety (Kolhatkar et al., 2018). More typical is a partial annotation, as in (Byron and Allen, 1998; Navarretta, 2000), who annotated pronominal reference to abstract objects; in O NTO N OTES, where event anaphora was marked (Pradhan et al., 2007); and in the work of Kolhatkar and Hirst (2014), which focused on so-called shell nouns. In ARRAU, A coder specifying that a referring expression is discourseold is asked whether its antecedent was introduced using a phrase (markable) or a segment (discourse segment). Coders who choose segment have to mark a sequence of predefined clauses. (9) The Treasury report, which is required annually by a provision of the 1988 trade act, again took South Korea to task for its"
2021.codi-sharedtask.1,P14-2006,1,0.93955,"association such as identity of sense anaphora, etc. (Poesio, 2016). Some of these resources are of a sufficient size to support shared tasks. In particular, the AR RAU corpus was used as the dataset for the Shared Task on Anaphora Resolution with ARRAU in the CRAC 2018 Workshop (Poesio et al., 2018). That shared task was articulated around three tasks: identity coreference (including identification of nonreferring expressions), bridging references, and discourse deixis. The organization of the shared task resulted in the development of an extended version of the Coreference Reference Scorer (Pradhan et al., 2014), which also scores non-referring expressions. Separate scorers were developed for bridging reference resolution, carrying out both mention-based evaluation and entity-based evaluation of bridging references, as done by Hou et al. (2018), and for discourse deixis, based on Kolhatkar and Hirst (2014). The present shared task was modeled on that. (5) There was not a moment to be lost: away went Alice like the wind, and was just in time to hear it say, as it turned a corner, ’Oh my ears and whiskers, how late it’s getting!’ She was close behind it when she turned the corner, but the Rabbit was no"
2021.codi-sharedtask.1,W13-2313,0,0.0303269,"f anaphoric relations that are not annotated in ONTONOTES but are annotated in other corpora. Split-antecedent anaphora In ONTONOTES, plural reference is only marked when the antecedent is mentioned by a single noun phrase. However, split-antecedent anaphors are also possible (Eschenbach et al., 1989; Kamp and Reyle, 1 https://competitions.codalab.org/ competitions/30312 2 just in time to see it pop down a large rabbithole under the hedge. and Martí, 2010), ARRAU (Uryupina et al., 2020), GNOME (Poesio, 2004), GUM (Zeldes, 2017), IS NOTES (Markert et al., 2012), the Prague Dependency Treebank (Nedoluzhko, 2013), and TÜBA DZ (Versley, 2008). (See Poesio et al. (2016) for a more detailed survey and Nedoluzhko et al. (2021) for a more recent, extensive update.) Bridging references There are other forms of anaphoric reference besides identity, and there are now a number of corpora annotating (a subset of) these forms. Possibly the most studied of nonidentity anaphora is bridging reference or associative anaphora (Clark, 1977; Hawkins, 1978; Prince, 1981) as in (5), where bridging reference / associative anaphora the roof refers to an object which is related to / associated with, but not identical to, th"
2021.codi-sharedtask.1,W12-4501,0,0.677797,"ric relations: identity, bridging references and discourse deixis, we defined multiple tasks focusing individually on these key relations. We discuss the evaluation scripts used to assess the system performance on these tasks, and provide a brief summary of the participating systems and the results obtained across 115 runs from six teams, with most submissions achieving significantly better results than our baseline methods. 1 Introduction The performance of models for single-antecedent anaphora resolution on the aspects of anaphoric interpretation annotated in the standard ONTONOTES dataset (Pradhan et al., 2012) has greatly improved in recent years (Wiseman et al., 2015; Lee et al., 2017, 2018; Kantor and Globerson, 2019; Joshi et al., 2020). So the attention of the community has started to turn to more complex cases of anaphora not found or not properly tested in ONTONOTES. Well-known examples of this trend are work on the cases of anaphora whose interpretation requires some form of commonsense knowledge tested by benchmarks for the Winograd Schema Challenge (Rahman and Ng, 2012; Liu et al., 2017; Sakaguchi et al., 2020), or the pronominal anaphors that cannot be resolved purely using gender, for wh"
2021.codi-sharedtask.1,nissim-etal-2004-annotation,0,0.152087,"2) is one of the best known dialogue corpora. It consists of 1,155 five-minute spontaneous telephone conversations between two participants not previously acquainted with each other. In these conversations, callers question receivers on provided topics, such as child care, recycling, and news media. 440 speakers participate in these 1,155 conversations, producing 221,616 utterances. It was 7 Identity anaphora also includes split antecedent plural anaphoric reference. 8 https://catalog.ldc.upenn.edu/ LDC97S62 5 annotated for dialogue acts by Stolcke et al. (1997)9 and for information status by Nissim et al. (2004). exactly the same MMAX style – and by the same two annotators from the DALI team at Queen Mary University and University of Essex, Dr. Maris Camilleri and Dr. Paloma Carretero Garcia, who annotated and checked ARRAU Release 3, which is currently being prepared for release. However, due to time constraints, each document was only annotated by a single annotator, with spot checks carried out by the other annotator and Massimo Poesio (in ARRAU 3 each document was looked at by both annotators, and most documents were also independently checked by Massimo Poesio). To prepare the data for the share"
2021.codi-sharedtask.1,W04-0210,1,0.761359,"j didn’t fit [her]i . However, many other types of identity anaphora exist, as well as other types of anaphoric relations that are not annotated in ONTONOTES but are annotated in other corpora. Split-antecedent anaphora In ONTONOTES, plural reference is only marked when the antecedent is mentioned by a single noun phrase. However, split-antecedent anaphors are also possible (Eschenbach et al., 1989; Kamp and Reyle, 1 https://competitions.codalab.org/ competitions/30312 2 just in time to see it pop down a large rabbithole under the hedge. and Martí, 2010), ARRAU (Uryupina et al., 2020), GNOME (Poesio, 2004), GUM (Zeldes, 2017), IS NOTES (Markert et al., 2012), the Prague Dependency Treebank (Nedoluzhko, 2013), and TÜBA DZ (Versley, 2008). (See Poesio et al. (2016) for a more detailed survey and Nedoluzhko et al. (2021) for a more recent, extensive update.) Bridging references There are other forms of anaphoric reference besides identity, and there are now a number of corpora annotating (a subset of) these forms. Possibly the most studied of nonidentity anaphora is bridging reference or associative anaphora (Clark, 1977; Hawkins, 1978; Prince, 1981) as in (5), where bridging reference / associati"
2021.codi-sharedtask.1,D12-1071,1,0.763158,"e-antecedent anaphora resolution on the aspects of anaphoric interpretation annotated in the standard ONTONOTES dataset (Pradhan et al., 2012) has greatly improved in recent years (Wiseman et al., 2015; Lee et al., 2017, 2018; Kantor and Globerson, 2019; Joshi et al., 2020). So the attention of the community has started to turn to more complex cases of anaphora not found or not properly tested in ONTONOTES. Well-known examples of this trend are work on the cases of anaphora whose interpretation requires some form of commonsense knowledge tested by benchmarks for the Winograd Schema Challenge (Rahman and Ng, 2012; Liu et al., 2017; Sakaguchi et al., 2020), or the pronominal anaphors that cannot be resolved purely using gender, for which benchmarks such as GAP have been developed (Webster et al., 2018). GAP, however, still focused on identity coreference. In addition, more research has been carried out on aspects of anaphoric interpretation that go beyond identity anaphora but are covered by datasets ∗ Work done when the author was a student at CMU 1 Proceedings of the CODI-CRAC 2021 Shared Task on Anaphora, Bridging, and Discourse Deixis in Dialogue, pages 1–15 Punta Cana, Dominican Republic, November"
2021.codi-sharedtask.1,poesio-artstein-2008-anaphoric,1,0.800941,"lopment in the trial phase, whereas all other datasets were used for training.6 A limitation of most resources annotated for anaphora is that they mostly focus on expository text. The one substantial dataset of anaphoric relations in dialogue is ANCOR for French (Muzerelle et al., 2014), in which identity and bridging anaphora are annotated. Among the small number of English corpora that cover dialogue include ONTONOTES (Pradhan et al., 2012), which contains a small number of conversations annotated for identity anaphora and a small subtype of discourse deixis (as discussed earlier). ARRAU’s (Poesio and Artstein, 2008; Uryupina et al., 2020) TRAINS sub-corpus consists of task-oriented dialogues for identity, bridging, and discourse deixis. We include TRAINS in CODI - CRAC 2021 training data. The more recently released ONTOGUM (Zhu et al., 2021) builds upon the ONTONOTES schema and adds several new genres (including more spoken data) to the ONTONOTES family. Both identity anaphora and bridging are annotated in the dataset. 3 Annotation scheme The original annotation scheme used for Release 1 (Poesio and Artstein, 2008) is distributed with the dataset and is also available from the ARRAU corpus page. For the"
2021.codi-sharedtask.1,N19-1176,1,0.849468,"tings, and evaluation metrics in Section 4, and submission details in Section 5. This is followed by details of the baselines in Section 6 and participating systems in Section 7. We present a discussion of the performance of the systems on different tasks and sub-corpora in Section 8, and finally conclude this paper in Section 9. 2 1993), as in (2). These are also cases of plural identity coreference, but to sets composed of two or more entities introduced by separate noun phrases. Such references are annotated in, e.g., ARRAU (Uryupina et al., 2020), GUM (Zeldes, 2017) and Phrase Detectives (Poesio et al., 2019). (2) [John]1 met [Mary]2 . [He]1 greeted [her]2 . [They]1,2 went to the movies. Discourse deixis In ONTONOTES, event anaphora, a subtype of discourse deixis (Webber, 1991; Kolhatkar et al., 2018) is marked, as in (3) (where [that] arguably refers to the event of a white rabbit with pink ears running past Alice) but not the whole range of abstract anaphora, illustrated by, e.g., (4), where again arguably [this] refers to the fact that the Rabbit was able to talk. (Both examples from the Phrase Detectives corpus (Poesio et al., 2019).) (3) So she was considering in her own mind (as well as she"
2021.codi-sharedtask.1,2020.emnlp-main.686,0,0.790191,"f 55 individual participants registered for the CODI - CRAC 2021 shared task on CodaLab.21 Among them, five teams submitted results for Task 1, three submitted results for Task 2, and two submitted results for Task 3. Teams UTD_NLP, KU_NLP, DFKI_TalkingRobots, Emory_NLP, and INRIA submitted system description papers. We summarize their approaches below (and in Table 2): Baselines UTD_NLP participated in all three tasks. For identity anaphora, they deployed a pipeline architecture consisting of a mention detection component and an entity coreference component. The coreference component extends Xu and Choi (2020)’s implementation of Lee et al. (2018) by modifying the objective so that it can output singleton clusters, and enforces dialogue-specific constraints. They setup a similar architecture for discourse deixis. However, they slightly modified the objective function in Xu and Choi (2020) by classifying each span as a candidate anaphor, a candidate antecedent, or a non-mention in the mention detection stage, and resolving only candidate anaphors to candidate antecedents later. The team used a multi-pass sieve approach for bridging resolution to target same-head bridging links, with Yu and Poesio (2"
2021.codi-sharedtask.1,2020.lrec-1.1,1,0.560581,"ities that are often missing in news or Wikipedia articles, which form a large chunk of current datasets for coreference resolution. There has been some research on coreference in dialogue (Byron, 2002; Eckert and Strube, 2001; Müller, 2008), but very limited in scope (primarily related to pronominal interpretation), due to the lack of suitable corpora. The one language for which substantial corpora of coreference in dialogue exist is French: the ANCOR corpus (Muzerelle et al., 2014) has enabled the development of an end-to-end neural model for coreference interpretation in dialogue by Grobol (2020). For English, the one resource we are aware of fully annotated for anaphoric reference is the TRAINS corpora included in the ARRAU corpus (Uryupina et al., 2020). The objective of the CODI - CRAC 2021 Shared In this paper, we provide an overview of the CODI - CRAC 2021 Shared Task. The shared task focuses on detecting anaphoric relations in different genres of conversations. Using five conversational datasets, four of which have been newly annotated with a wide range of anaphoric relations: identity, bridging references and discourse deixis, we defined multiple tasks focusing individually on"
2021.codi-sharedtask.1,2020.coling-main.538,1,0.858079,"A; 5 Queen Mary Univ., UK; 6 HITS, Germany; 7 AWS AI, Amazon, USA sopankh@amazon.com; j.yu@essex.ac.uk; ramesh.manuvinakurike@intel.com; vince@hlt.utdallas.edu; m.poesio@qmul.ac.uk; Michael.Strube@h-its.org; cprose@cs.cmu.edu Abstract such as ARRAU (Poesio et al., 2018; Uryupina et al., 2020). These include, e.g., bridging reference (Clark, 1977; Hou et al., 2018; Hou, 2020; Yu and Poesio, 2020; Kobayashi and Ng, 2021), discourse deixis (Webber, 1991; Marasovi´c et al., 2017; Kolhatkar et al., 2018) or split-antecedent anaphora (Eschenbach et al., 1989; Vala et al., 2016; Zhou and Choi, 2018; Yu et al., 2020b, 2021). There has been interest in other genres apart from news. This includes substantial research on annotating and resolving coreference in biomedical and other scientific domains (Cohen et al., 2017; Lu and Poesio, 2021) as well as in literary documents (Bamman et al., 2020). There are, however, language genres still understudied in the literature on anaphoric reference. Arguably the most important among these is conversational language in dialogue. Anaphora resolution in dialogue requires systems to handle grammatically incorrect language suffering from disfluencies. Dialogue involves m"
2021.codi-sharedtask.1,D19-1062,0,0.248851,"duced (antecedent). For discourse-old mentions, an antecedent can be identified, either of type phrase (if the antecedent was introduced using a nominal markable) or segment (not introduced by a nominal markable, for discourse deixis).7 In addition, referring NPs can be marked as related to a previously mentioned discourse entity to identify them as examples of associative (bridging) anaphora. 3.2 New Data The annotated corpus prepared for the CODI - CRAC 2021 shared task consists of conversations from four well-known conversational datasets: the AMI corpus (Carletta, 2006), the LIGHT corpus (Urbanek et al., 2019), the PERSUASION corpus (Wang et al., 2019) and SWITCHBOARD (Godfrey et al., 1992). For each of these datasets, documents for about 15K tokens were annotated for development, and about the same number of tokens were annotated for testing, according to the ARRAU annotation scheme. Bridging references Annotating — indeed, even identifying — bridging references in a reliable way is difficult, which is one of the reasons why so few large-scale corpora for anaphora include this type of annotation (Poesio et al., 2016; Kobayashi and Ng, 2020). The ARRAU guidelines for bridging anaphora are based on"
2021.codi-sharedtask.1,2021.naacl-main.329,1,0.738872,"Missing"
2021.codi-sharedtask.1,P16-1216,0,0.0628664,"Missing"
2021.codi-sharedtask.1,2020.coling-main.315,1,0.910842,"in Dialogue Sopan Khosla1,7∗, Juntao Yu2 , Ramesh Manuvinakurike3 , Vincent Ng4 , Massimo Poesio5 , Michael Strube6 , and Carolyn Rosé1 1 Canegie Mellon Univ., USA; 2 Univ. of Essex, UK; 3 Intel Labs, USA; 4 UT Dallas, USA; 5 Queen Mary Univ., UK; 6 HITS, Germany; 7 AWS AI, Amazon, USA sopankh@amazon.com; j.yu@essex.ac.uk; ramesh.manuvinakurike@intel.com; vince@hlt.utdallas.edu; m.poesio@qmul.ac.uk; Michael.Strube@h-its.org; cprose@cs.cmu.edu Abstract such as ARRAU (Poesio et al., 2018; Uryupina et al., 2020). These include, e.g., bridging reference (Clark, 1977; Hou et al., 2018; Hou, 2020; Yu and Poesio, 2020; Kobayashi and Ng, 2021), discourse deixis (Webber, 1991; Marasovi´c et al., 2017; Kolhatkar et al., 2018) or split-antecedent anaphora (Eschenbach et al., 1989; Vala et al., 2016; Zhou and Choi, 2018; Yu et al., 2020b, 2021). There has been interest in other genres apart from news. This includes substantial research on annotating and resolving coreference in biomedical and other scientific domains (Cohen et al., 2017; Lu and Poesio, 2021) as well as in literary documents (Bamman et al., 2020). There are, however, language genres still understudied in the literature on anaphoric reference. Ar"
2021.codi-sharedtask.1,C18-1003,0,0.0784215,"USA; 4 UT Dallas, USA; 5 Queen Mary Univ., UK; 6 HITS, Germany; 7 AWS AI, Amazon, USA sopankh@amazon.com; j.yu@essex.ac.uk; ramesh.manuvinakurike@intel.com; vince@hlt.utdallas.edu; m.poesio@qmul.ac.uk; Michael.Strube@h-its.org; cprose@cs.cmu.edu Abstract such as ARRAU (Poesio et al., 2018; Uryupina et al., 2020). These include, e.g., bridging reference (Clark, 1977; Hou et al., 2018; Hou, 2020; Yu and Poesio, 2020; Kobayashi and Ng, 2021), discourse deixis (Webber, 1991; Marasovi´c et al., 2017; Kolhatkar et al., 2018) or split-antecedent anaphora (Eschenbach et al., 1989; Vala et al., 2016; Zhou and Choi, 2018; Yu et al., 2020b, 2021). There has been interest in other genres apart from news. This includes substantial research on annotating and resolving coreference in biomedical and other scientific domains (Cohen et al., 2017; Lu and Poesio, 2021) as well as in literary documents (Bamman et al., 2020). There are, however, language genres still understudied in the literature on anaphoric reference. Arguably the most important among these is conversational language in dialogue. Anaphora resolution in dialogue requires systems to handle grammatically incorrect language suffering from disfluencies. Di"
2021.codi-sharedtask.1,P19-1566,0,0.250523,", an antecedent can be identified, either of type phrase (if the antecedent was introduced using a nominal markable) or segment (not introduced by a nominal markable, for discourse deixis).7 In addition, referring NPs can be marked as related to a previously mentioned discourse entity to identify them as examples of associative (bridging) anaphora. 3.2 New Data The annotated corpus prepared for the CODI - CRAC 2021 shared task consists of conversations from four well-known conversational datasets: the AMI corpus (Carletta, 2006), the LIGHT corpus (Urbanek et al., 2019), the PERSUASION corpus (Wang et al., 2019) and SWITCHBOARD (Godfrey et al., 1992). For each of these datasets, documents for about 15K tokens were annotated for development, and about the same number of tokens were annotated for testing, according to the ARRAU annotation scheme. Bridging references Annotating — indeed, even identifying — bridging references in a reliable way is difficult, which is one of the reasons why so few large-scale corpora for anaphora include this type of annotation (Poesio et al., 2016; Kobayashi and Ng, 2020). The ARRAU guidelines for bridging anaphora are based on experiments that were started by Vieira and"
2021.codi-sharedtask.1,2021.acl-short.59,0,0.0207201,"gue is ANCOR for French (Muzerelle et al., 2014), in which identity and bridging anaphora are annotated. Among the small number of English corpora that cover dialogue include ONTONOTES (Pradhan et al., 2012), which contains a small number of conversations annotated for identity anaphora and a small subtype of discourse deixis (as discussed earlier). ARRAU’s (Poesio and Artstein, 2008; Uryupina et al., 2020) TRAINS sub-corpus consists of task-oriented dialogues for identity, bridging, and discourse deixis. We include TRAINS in CODI - CRAC 2021 training data. The more recently released ONTOGUM (Zhu et al., 2021) builds upon the ONTONOTES schema and adds several new genres (including more spoken data) to the ONTONOTES family. Both identity anaphora and bridging are annotated in the dataset. 3 Annotation scheme The original annotation scheme used for Release 1 (Poesio and Artstein, 2008) is distributed with the dataset and is also available from the ARRAU corpus page. For the second release (Uryupina et al., 2020), the guidelines for bridging were extended and genericity was also annotated using the GNOME guidelines, but a complete new manual was not produced. However, a fairly extensive description ca"
2021.codi-sharedtask.1,Q18-1042,0,0.0216415,"t al., 2015; Lee et al., 2017, 2018; Kantor and Globerson, 2019; Joshi et al., 2020). So the attention of the community has started to turn to more complex cases of anaphora not found or not properly tested in ONTONOTES. Well-known examples of this trend are work on the cases of anaphora whose interpretation requires some form of commonsense knowledge tested by benchmarks for the Winograd Schema Challenge (Rahman and Ng, 2012; Liu et al., 2017; Sakaguchi et al., 2020), or the pronominal anaphors that cannot be resolved purely using gender, for which benchmarks such as GAP have been developed (Webster et al., 2018). GAP, however, still focused on identity coreference. In addition, more research has been carried out on aspects of anaphoric interpretation that go beyond identity anaphora but are covered by datasets ∗ Work done when the author was a student at CMU 1 Proceedings of the CODI-CRAC 2021 Shared Task on Anaphora, Bridging, and Discourse Deixis in Dialogue, pages 1–15 Punta Cana, Dominican Republic, November 10, 2021. ©2021 Association for Computational Linguistics Task in Anaphora Resolution in Dialogue1 was to provide participants with the opportunity to develop automated approaches for corefer"
2021.codi-sharedtask.1,P15-1137,0,0.0532448,"eixis, we defined multiple tasks focusing individually on these key relations. We discuss the evaluation scripts used to assess the system performance on these tasks, and provide a brief summary of the participating systems and the results obtained across 115 runs from six teams, with most submissions achieving significantly better results than our baseline methods. 1 Introduction The performance of models for single-antecedent anaphora resolution on the aspects of anaphoric interpretation annotated in the standard ONTONOTES dataset (Pradhan et al., 2012) has greatly improved in recent years (Wiseman et al., 2015; Lee et al., 2017, 2018; Kantor and Globerson, 2019; Joshi et al., 2020). So the attention of the community has started to turn to more complex cases of anaphora not found or not properly tested in ONTONOTES. Well-known examples of this trend are work on the cases of anaphora whose interpretation requires some form of commonsense knowledge tested by benchmarks for the Winograd Schema Challenge (Rahman and Ng, 2012; Liu et al., 2017; Sakaguchi et al., 2020), or the pronominal anaphors that cannot be resolved purely using gender, for which benchmarks such as GAP have been developed (Webster et"
2021.newsum-1.5,N18-2097,0,0.0183126,"is used for F-score, Precision and Recall. ROUGE relies on different metrics that include n-gram (R - N) and Longest Common Sub-sequence - LCS (R - L) overlap (Lin, 2004). Unigram and bigram overlap (R -1,2) provide a reasonable estimation of informativeness, while R - L estimates the summaries’ fluency. In order to further investigate the linguistic quality of system summaries, two native German speakers with fluent English have evaluated the summaries for two parameters (details are present in Section 6.3). It is worth to be noted that previous monolingual scientific summarization studies (Cohan et al., 2018; Dangovski et al., 2019) have not considered the human evaluations due to its demanding nature. For human evaluation of scientific articles, human judges must read and comprehend long domain-specific articles with summaries 10 Github.com/fairseq (i-vi)Pypi/Sumy 12 (vii)Pypi/Bertext 11 44 R -1 SUM - BASIC TEXT- RANK KL - SUM LUHN LEX - RANK RANDOM LSA BERT PGN S2S TRF F P 28.67 26.29 24.96 25.67 26.53 28.05 26.51 28.74 22.82 18.82 17.73 19.25 20.11 21.20 18.97 23.56 22.25 20.94 25.53 47.88 54.98 40.95 R -2 R F P Extractive models 38.68 07.15 05.53 43.57 07.11 04.86 42.13 06.40 04.42 38.50 06.7"
2021.newsum-1.5,2020.lrec-1.820,0,0.201248,"the English Gigaword corpus along with the EnglishChinese translation corpus for training. Though these studies focused on CLS, there is no real cross-lingual dataset except for MultiLing and WikiLingua. The datasets are, however, limited in scope and cannot be used for our experiments. To the best of the authors’ knowledge, there is no real dataset created with the sole purpose of crosslingual abstractive summarization of scientific text. 2.1.1 Monolingual Datasets WIKIPEDIA has been widely used for the creation of MS datasets such as English multi-document summarization (Zopf et al., 2016; Antognini and Faltings, 2020; Gholipour Ghalandari et al., 2020), English and German single and multi-document summarization (Hättasch et al., 2020), and German single-document summarization (Frefel, 2020). As these datasets are designed for MS, it makes them inadequate for cross-lingual evaluation. 2.1.2 Multilingual Datasets The TAC MultiLing shared task is held biennially (2011-15) for multilingual multi-document summarization (Giannakopoulos et al., 2011; Giannakopoulos, 2013; Giannakopoulos et al., 2015). These corpora are composed of English Wikinews and translated into 9 languages. The final corpus (MultiLing&apos;15)"
2021.newsum-1.5,2020.findings-emnlp.428,0,0.0420407,"Missing"
2021.newsum-1.5,Q19-1008,0,0.0294914,"Missing"
2021.newsum-1.5,P19-1305,0,0.337087,"l to complement it. The Wikipedia dataset consists of English and German articles, which can be used for monolingual and cross-lingual summarization. Furthermore, we present a quantitative analysis of the datasets and results of empirical experiments with several existing extractive and abstractive summarization models. The results suggest the viability and usefulness of the proposed dataset for monolingual and crosslingual summarization. 1 Introduction The summarization research has recently shifted from monolingual summarization (MS) to crosslingual summarization (CLS) (Ouyang et al., 2019; Duan et al., 2019; Zhu et al., 2019). However, due to the absence of real cross-lingual datasets, recent CLS studies (Shen et al., 2018; Ouyang et al., 2019; Zhu et al., 2019; Pontes et al., 2020) are conducted on existing monolingual news datasets and off-theshelf machine translation (MT) systems which may introduce noise into pseudo-cross-lingual summarization (PCLS) data. As these CLS studies rely on only news data, the trained summarization models may not work well for other domains such as scientific texts. Although some efforts have been made for investigating the MS task on scientific papers (Vadapalli"
2021.newsum-1.5,2020.lrec-1.821,0,0.389157,"WikiLingua. The datasets are, however, limited in scope and cannot be used for our experiments. To the best of the authors’ knowledge, there is no real dataset created with the sole purpose of crosslingual abstractive summarization of scientific text. 2.1.1 Monolingual Datasets WIKIPEDIA has been widely used for the creation of MS datasets such as English multi-document summarization (Zopf et al., 2016; Antognini and Faltings, 2020; Gholipour Ghalandari et al., 2020), English and German single and multi-document summarization (Hättasch et al., 2020), and German single-document summarization (Frefel, 2020). As these datasets are designed for MS, it makes them inadequate for cross-lingual evaluation. 2.1.2 Multilingual Datasets The TAC MultiLing shared task is held biennially (2011-15) for multilingual multi-document summarization (Giannakopoulos et al., 2011; Giannakopoulos, 2013; Giannakopoulos et al., 2015). These corpora are composed of English Wikinews and translated into 9 languages. The final corpus (MultiLing&apos;15) size is 1500 documents in total for all languages. Ladhak et al. (2020) also create a multilingual dataset named WikiLingua from WikiHow in 18 languages. However, the author con"
2021.newsum-1.5,W13-3103,0,0.0245163,"atasets WIKIPEDIA has been widely used for the creation of MS datasets such as English multi-document summarization (Zopf et al., 2016; Antognini and Faltings, 2020; Gholipour Ghalandari et al., 2020), English and German single and multi-document summarization (Hättasch et al., 2020), and German single-document summarization (Frefel, 2020). As these datasets are designed for MS, it makes them inadequate for cross-lingual evaluation. 2.1.2 Multilingual Datasets The TAC MultiLing shared task is held biennially (2011-15) for multilingual multi-document summarization (Giannakopoulos et al., 2011; Giannakopoulos, 2013; Giannakopoulos et al., 2015). These corpora are composed of English Wikinews and translated into 9 languages. The final corpus (MultiLing&apos;15) size is 1500 documents in total for all languages. Ladhak et al. (2020) also create a multilingual dataset named WikiLingua from WikiHow in 18 languages. However, the author conducted experiments to generate English summaries from non-English articles. Although these datasets are multilingual, they are non-scientific, thus cannot be used for cross-lingual summarization of scientific texts. Moreover, the small size of the MultiLing makes it difficult to"
2021.newsum-1.5,N19-1204,0,0.370745,"kipedia Science Portal to complement it. The Wikipedia dataset consists of English and German articles, which can be used for monolingual and cross-lingual summarization. Furthermore, we present a quantitative analysis of the datasets and results of empirical experiments with several existing extractive and abstractive summarization models. The results suggest the viability and usefulness of the proposed dataset for monolingual and crosslingual summarization. 1 Introduction The summarization research has recently shifted from monolingual summarization (MS) to crosslingual summarization (CLS) (Ouyang et al., 2019; Duan et al., 2019; Zhu et al., 2019). However, due to the absence of real cross-lingual datasets, recent CLS studies (Shen et al., 2018; Ouyang et al., 2019; Zhu et al., 2019; Pontes et al., 2020) are conducted on existing monolingual news datasets and off-theshelf machine translation (MT) systems which may introduce noise into pseudo-cross-lingual summarization (PCLS) data. As these CLS studies rely on only news data, the trained summarization models may not work well for other domains such as scientific texts. Although some efforts have been made for investigating the MS task on scientific"
2021.newsum-1.5,W15-4638,0,0.0255621,"been widely used for the creation of MS datasets such as English multi-document summarization (Zopf et al., 2016; Antognini and Faltings, 2020; Gholipour Ghalandari et al., 2020), English and German single and multi-document summarization (Hättasch et al., 2020), and German single-document summarization (Frefel, 2020). As these datasets are designed for MS, it makes them inadequate for cross-lingual evaluation. 2.1.2 Multilingual Datasets The TAC MultiLing shared task is held biennially (2011-15) for multilingual multi-document summarization (Giannakopoulos et al., 2011; Giannakopoulos, 2013; Giannakopoulos et al., 2015). These corpora are composed of English Wikinews and translated into 9 languages. The final corpus (MultiLing&apos;15) size is 1500 documents in total for all languages. Ladhak et al. (2020) also create a multilingual dataset named WikiLingua from WikiHow in 18 languages. However, the author conducted experiments to generate English summaries from non-English articles. Although these datasets are multilingual, they are non-scientific, thus cannot be used for cross-lingual summarization of scientific texts. Moreover, the small size of the MultiLing makes it difficult to use for cross-lingual neural"
2021.newsum-1.5,N18-1065,0,0.0328846,"Missing"
2021.newsum-1.5,P17-1099,0,0.0491893,"ngs have dimensions of 512 and hidden layers have dimensions of 786. The model consists of encoder and decoder stacks, each having 6 layers and 8 multi-attention heads at the decoder side. To make the results comparable among all models, the same vocabulary size of 100K and 50K at the encoder and decoder sides are selected. The Adam optimizer is used at a learning rate of 0.0001 and with a residual dropout of 0.1. For all abstractive models, a beam search of size 4 is applied in the inference phase. For all abstractive models, the encoder and decoder length is fixed to 400 and 100 words as in See et al. (2017). All abstractive models are trained on a single Tesla P 40 GPU with 24 GB RAM. For training and inference, the S 2 S and TRF models take around 6 days, and the PGN model takes 3 days. Table 3: Percentage of novel n-grams in W- MS summaries. ing FairSeq10 : (i) TRANS - SUM - Translate-thenSummarize, and (ii) SUM - TRANS - Summarizethen-Translate. 5.2 Methods The following extractive methods are selected: (i) SUM - BASIC , (ii) LUHN , (iii) KL - SUM , (iv) LSA , (v) LEX - RANK , (vi) TEXT- RANK , and (vii) BERT 11,12 . The following abstractive models are chosen: (i) Attention based sequence to"
2021.newsum-1.5,2020.lrec-1.827,0,0.194386,"S, there is no real cross-lingual dataset except for MultiLing and WikiLingua. The datasets are, however, limited in scope and cannot be used for our experiments. To the best of the authors’ knowledge, there is no real dataset created with the sole purpose of crosslingual abstractive summarization of scientific text. 2.1.1 Monolingual Datasets WIKIPEDIA has been widely used for the creation of MS datasets such as English multi-document summarization (Zopf et al., 2016; Antognini and Faltings, 2020; Gholipour Ghalandari et al., 2020), English and German single and multi-document summarization (Hättasch et al., 2020), and German single-document summarization (Frefel, 2020). As these datasets are designed for MS, it makes them inadequate for cross-lingual evaluation. 2.1.2 Multilingual Datasets The TAC MultiLing shared task is held biennially (2011-15) for multilingual multi-document summarization (Giannakopoulos et al., 2011; Giannakopoulos, 2013; Giannakopoulos et al., 2015). These corpora are composed of English Wikinews and translated into 9 languages. The final corpus (MultiLing&apos;15) size is 1500 documents in total for all languages. Ladhak et al. (2020) also create a multilingual dataset named WikiLin"
2021.newsum-1.5,W16-1608,0,0.0223267,"he final corpus (MultiLing&apos;15) size is 1500 documents in total for all languages. Ladhak et al. (2020) also create a multilingual dataset named WikiLingua from WikiHow in 18 languages. However, the author conducted experiments to generate English summaries from non-English articles. Although these datasets are multilingual, they are non-scientific, thus cannot be used for cross-lingual summarization of scientific texts. Moreover, the small size of the MultiLing makes it difficult to use for cross-lingual neural models. 2.2 Cross-lingual Summarization Datasets Scientific Summarization Datasets Kim et al. (2016) build a dataset of introductionabstract pairs from ARXIV papers for abstractive summarization. Vadapalli et al. (2018b,a) collect a parallel corpus of 87K pairs of research paper titles, abstracts and corresponding blog titles for title generation. Nikolov et al. (2018) create two datasets from scientific articles, abstract-title pairs from MEDLINE for title generation and body-abstract pairs from PUBMED for abstract generation. Cohan 3 Dataset Construction 3.1 Spektrum Data is the German equivalent of the “Scientific American”, which began publishing in 19781 . SPEKTRUM 1 40 Spektrum.de/das-"
2021.newsum-1.5,D18-2028,0,0.120077,"al., 2019; Zhu et al., 2019). However, due to the absence of real cross-lingual datasets, recent CLS studies (Shen et al., 2018; Ouyang et al., 2019; Zhu et al., 2019; Pontes et al., 2020) are conducted on existing monolingual news datasets and off-theshelf machine translation (MT) systems which may introduce noise into pseudo-cross-lingual summarization (PCLS) data. As these CLS studies rely on only news data, the trained summarization models may not work well for other domains such as scientific texts. Although some efforts have been made for investigating the MS task on scientific papers (Vadapalli et al., 2018b; Nikolov et al., 2018; 39 Proceedings of the Third Workshop on New Frontiers in Summarization, pages 39–50 November 10, 2021. ©2021 Association for Computational Linguistics our dataset for MS and CLS. Moreover, linguistic quality is evaluated on a subset of the output summaries of the MS and CLS experiments by human judges. 2 et al. (2018) also collect a scientific dataset from (194K) and PUBMED (216K) articles for abstractive summarization. Dangovski et al. (2019) create a corpus of 60K science articles from ScienceDaily for summary generation. The datasets mentioned above consist of scien"
2021.newsum-1.5,2020.findings-emnlp.360,0,0.0403293,"erman single and multi-document summarization (Hättasch et al., 2020), and German single-document summarization (Frefel, 2020). As these datasets are designed for MS, it makes them inadequate for cross-lingual evaluation. 2.1.2 Multilingual Datasets The TAC MultiLing shared task is held biennially (2011-15) for multilingual multi-document summarization (Giannakopoulos et al., 2011; Giannakopoulos, 2013; Giannakopoulos et al., 2015). These corpora are composed of English Wikinews and translated into 9 languages. The final corpus (MultiLing&apos;15) size is 1500 documents in total for all languages. Ladhak et al. (2020) also create a multilingual dataset named WikiLingua from WikiHow in 18 languages. However, the author conducted experiments to generate English summaries from non-English articles. Although these datasets are multilingual, they are non-scientific, thus cannot be used for cross-lingual summarization of scientific texts. Moreover, the small size of the MultiLing makes it difficult to use for cross-lingual neural models. 2.2 Cross-lingual Summarization Datasets Scientific Summarization Datasets Kim et al. (2016) build a dataset of introductionabstract pairs from ARXIV papers for abstractive summ"
2021.newsum-1.5,W04-1013,0,0.0890519,"dimensions and hidden layers with 256 dimensions. The vocabulary size is 100K and 50K at the encoder and decoder sides, without the OOV words handling as used in the PGN model. In order to solve the OOV words, we choose BPE instead of the n-gram vocabulary. The Adam optimizer is used with a learning rate of 0.15 and a mini-batch of size 16. The models are trained for 40 epochs, and 5.3 Evaluation For automatic evaluation, ROUGE metric is used for F-score, Precision and Recall. ROUGE relies on different metrics that include n-gram (R - N) and Longest Common Sub-sequence - LCS (R - L) overlap (Lin, 2004). Unigram and bigram overlap (R -1,2) provide a reasonable estimation of informativeness, while R - L estimates the summaries’ fluency. In order to further investigate the linguistic quality of system summaries, two native German speakers with fluent English have evaluated the summaries for two parameters (details are present in Section 6.3). It is worth to be noted that previous monolingual scientific summarization studies (Cohan et al., 2018; Dangovski et al., 2019) have not considered the human evaluations due to its demanding nature. For human evaluation of scientific articles, human judge"
2021.newsum-1.5,D19-5411,0,0.0346563,"Missing"
2021.newsum-1.5,D19-1302,0,0.0748294,"The Wikipedia dataset consists of English and German articles, which can be used for monolingual and cross-lingual summarization. Furthermore, we present a quantitative analysis of the datasets and results of empirical experiments with several existing extractive and abstractive summarization models. The results suggest the viability and usefulness of the proposed dataset for monolingual and crosslingual summarization. 1 Introduction The summarization research has recently shifted from monolingual summarization (MS) to crosslingual summarization (CLS) (Ouyang et al., 2019; Duan et al., 2019; Zhu et al., 2019). However, due to the absence of real cross-lingual datasets, recent CLS studies (Shen et al., 2018; Ouyang et al., 2019; Zhu et al., 2019; Pontes et al., 2020) are conducted on existing monolingual news datasets and off-theshelf machine translation (MT) systems which may introduce noise into pseudo-cross-lingual summarization (PCLS) data. As these CLS studies rely on only news data, the trained summarization models may not work well for other domains such as scientific texts. Although some efforts have been made for investigating the MS task on scientific papers (Vadapalli et al., 2018b; Niko"
2021.newsum-1.5,C16-1145,0,0.0206531,"uage pair. They use the English Gigaword corpus along with the EnglishChinese translation corpus for training. Though these studies focused on CLS, there is no real cross-lingual dataset except for MultiLing and WikiLingua. The datasets are, however, limited in scope and cannot be used for our experiments. To the best of the authors’ knowledge, there is no real dataset created with the sole purpose of crosslingual abstractive summarization of scientific text. 2.1.1 Monolingual Datasets WIKIPEDIA has been widely used for the creation of MS datasets such as English multi-document summarization (Zopf et al., 2016; Antognini and Faltings, 2020; Gholipour Ghalandari et al., 2020), English and German single and multi-document summarization (Hättasch et al., 2020), and German single-document summarization (Frefel, 2020). As these datasets are designed for MS, it makes them inadequate for cross-lingual evaluation. 2.1.2 Multilingual Datasets The TAC MultiLing shared task is held biennially (2011-15) for multilingual multi-document summarization (Giannakopoulos et al., 2011; Giannakopoulos, 2013; Giannakopoulos et al., 2015). These corpora are composed of English Wikinews and translated into 9 languages. Th"
2021.sustainlp-1.4,D14-1162,0,0.0937952,"ES systems are not capable of assessing the quality of essays (Winerip, 2005; Ben-Simon and Bennett, 2007; Wolfe et al., 2016), but indeed work by adopting shallow heuristics for the majority of training examples. Perelman (2014) argues that machine learning-based systems rely on the factor of 1 2 https://github.com/sdeva14/ sustai21-counter-neural-essay-length https://kaggle.com/c/asap-aes/ 32 Proceedings of the 2nd Workshop on Simple and Efficient Natural Language Processing, pages 32–38 November 10, 2021. ©201 Association for Computational Linguistics 2 Essay Length and Scores in Datasets (Pennington et al., 2014). We use 100-dimensional Glove for all models on TOEFL. All other settings are identical with previous work. For our models, we test two variations for an RNN module with a Gated Recurrent Unit (GRU, Cho et al., 2014) and a large-scale natural language pretraining model (XLNet, Yang et al., 2019). XLNet not only outperforms BERT (Devlin et al., 2019) which has led to significant improvements in many NLP tasks, but – unlike BERT – XLNet can also handle any input sequence length, required for our datasets. We encode a whole text at once using the pretrained language model. Datasets: We use two e"
2021.sustainlp-1.4,D14-1179,0,0.0458938,"Missing"
2021.sustainlp-1.4,D15-1049,0,0.0297098,"e of the art on a standard dataset as well as on a second dataset. Our findings suggest that neural essay scoring systems should consider the characteristics of datasets to focus on text quality. 1 Introduction Automated essay scoring (AES) is the task of assigning a score for a given essay, aiming to replicate human scoring results. The public release of a standard dataset from a shared task1 increased the interest in this task significantly. There have been several systems applied to the standard dataset including machine learning-based systems employing diverse features (Chen and He, 2013; Phandi et al., 2015) and neural essay scoring systems (Taghipour and Ng, 2016; Dong et al., 2017). Previous work, nevertheless, has shown that AES systems are not capable of assessing the quality of essays (Winerip, 2005; Ben-Simon and Bennett, 2007; Wolfe et al., 2016), but indeed work by adopting shallow heuristics for the majority of training examples. Perelman (2014) argues that machine learning-based systems rely on the factor of 1 2 https://github.com/sdeva14/ sustai21-counter-neural-essay-length https://kaggle.com/c/asap-aes/ 32 Proceedings of the 2nd Workshop on Simple and Efficient Natural Language Proce"
2021.sustainlp-1.4,2020.emnlp-main.16,0,0.0862622,"Missing"
A00-2003,P98-1011,0,0.312679,"ent, such as animacy, clause type, thematic role, proximity, etc. Poesio et al. (1999) report that they were not able to annotate many of these factors reliably. On the basis of these annotations, they constructed decision trees for predicting surface forms of referring expressions based on these factors - with good results: all 28 personal pronouns in their corpus were generated correctly. Unfortunately, they do not evaluate the contribution of each of these factors, so we do not know which ones are important. Work on corpus-based approaches to anaphora resolution is more numerous. Ge et al. (1998) describe a supervised probabilistic pronoun resolution algorithm which is based on complete syntactic information. The factors they use include distance from last mention, syntactic function and context, agreement information, animacy of the referent, a simplified notion of selectional restrictions, Introduction Generating adequate referring expressions is an active research topic in Natural Language Generation. Adequate referring expressions are those that enable the user to quickly and unambiguously identify the discourse entity that the expression co-specifies with. In this paper, we conce"
A00-2003,W99-0611,0,0.0163131,"tain degree form of the antecedent. Since these factors can be annotated tel18 Agree Syn Class SynAnte FormAnte Dist Dist4 Par Ambig Agreement in person, gender, and number Syntactic function Sortal Class (cf. Tab. 2) Syntactic function of antecedent. ""F"" for first mention, ""N"" for deadend Form of antecedent (pers. pron., poss. pron., def. NP, indef. NP, proper name) Distance to last mention in units Dist reduced to 4 values (deadend, Dist=0, Dist= 1, Dist>=2) Parallelism (Syn=SynAnte) Number of competing discourse entities Table 1: Overview of factors and the length of the coreference chain. Cardie & Wagstaff (1999) describe an unsupervised algorithm for noun phrase coreference resolution. Their factors are taken from Ge et al. (1998), with two exceptions. First, they replace complete syntactic information with information about NP bracketing. Second, they use the sortal class of the referent which they determine on the basis of WordNet (Fellbaum, 1998). There has been no comparison between corpusbased approaches for anaphora resolution and more traditional algorithms based on focusing (Sidner, 1983) or centering (Grosz et al., 1995) except for Azzam et al. (1998). However, their comparison is flawed by"
A00-2003,J96-2004,0,0.0407259,"Missing"
A00-2003,W99-0107,1,0.836697,"referents account for the 4 longest coreference chains in CK (85, 96, 109, and 127 mentions). Two annotators (the authors, both trained linguists), hand-labeled the texts with co-specification information based on the specifications for the Message Understanding Coreference task (Hirschman & Chinchor (1997); for theoretical reasons, we did not mark reflexive pronouns and appositives as cospecifying). The MCUs were labelled by the second author. All referring expressions were annotated with agreement and sortal class information. Labels were placed using the GUI-based annotation tool REFEREE (DeCristofaro et al., 1999). The annotators developed the Sortal Class annotation guidelines on the basis of two training texts. Then, both labellers annotated two texts from each genre independently (eight in total). These eight texts were used to determine the reliability of the sortal class coding scheme. Since sortal class annotation is intrinsically hard, the annotators looked up the senses of the head noun of each referring NP that was not a pronoun or a proper name in WordNet. Each sense was mapped directly to one or more of the ten classes given in Table 2. The annotators then chose the adequate sense. The relia"
A00-2003,W98-1119,0,0.191913,"he antecedent, such as animacy, clause type, thematic role, proximity, etc. Poesio et al. (1999) report that they were not able to annotate many of these factors reliably. On the basis of these annotations, they constructed decision trees for predicting surface forms of referring expressions based on these factors - with good results: all 28 personal pronouns in their corpus were generated correctly. Unfortunately, they do not evaluate the contribution of each of these factors, so we do not know which ones are important. Work on corpus-based approaches to anaphora resolution is more numerous. Ge et al. (1998) describe a supervised probabilistic pronoun resolution algorithm which is based on complete syntactic information. The factors they use include distance from last mention, syntactic function and context, agreement information, animacy of the referent, a simplified notion of selectional restrictions, Introduction Generating adequate referring expressions is an active research topic in Natural Language Generation. Adequate referring expressions are those that enable the user to quickly and unambiguously identify the discourse entity that the expression co-specifies with. In this paper, we conce"
A00-2003,J95-2003,0,0.039353,"le 1: Overview of factors and the length of the coreference chain. Cardie & Wagstaff (1999) describe an unsupervised algorithm for noun phrase coreference resolution. Their factors are taken from Ge et al. (1998), with two exceptions. First, they replace complete syntactic information with information about NP bracketing. Second, they use the sortal class of the referent which they determine on the basis of WordNet (Fellbaum, 1998). There has been no comparison between corpusbased approaches for anaphora resolution and more traditional algorithms based on focusing (Sidner, 1983) or centering (Grosz et al., 1995) except for Azzam et al. (1998). However, their comparison is flawed by evaluating a syntax-based focus algorithm on the basis of insufficient syntactic information. For pronoun generation, the original centering model (Grosz et al., 1995) provides a rule which is supposed to decide whether a referring expression has to be realized as a pronoun. However, this rule applies only to the referring expression which is the backward-looking center (Cb) of the current utterance. With respect to all other referring expression in this utterance centering is underspecified. Yeh & Mellish (1997) propose a"
A00-2003,W99-0108,1,0.898145,"Missing"
A00-2003,J97-1007,0,0.160318,"centering (Grosz et al., 1995) except for Azzam et al. (1998). However, their comparison is flawed by evaluating a syntax-based focus algorithm on the basis of insufficient syntactic information. For pronoun generation, the original centering model (Grosz et al., 1995) provides a rule which is supposed to decide whether a referring expression has to be realized as a pronoun. However, this rule applies only to the referring expression which is the backward-looking center (Cb) of the current utterance. With respect to all other referring expression in this utterance centering is underspecified. Yeh & Mellish (1997) propose a set of handcrafted rules for the generation of anaphora (zero and personal pronouns, full NPs) in Chinese. However, the factors which appear to be important in their evaluation are similar to factors described by authors mentioned above: distance, syntactic constraints on zero pronouns, discourse structure, salience and animacy of discourse entities. 2.2 O u r Factors The factors we investigate in this paper only rely on annotations of NPs and their co-specification relations. We did not add any discourse structural annotation, because (1) the texts are extracts from larger texts wh"
A00-2003,P97-1005,0,\N,Missing
A00-2003,C98-1011,0,\N,Missing
C04-1110,P03-1048,0,0.0146477,"of semantic similarity metrics using the noun portion of WordNet as a knowledge source. So far, the noun senses have been disambiguated manually. The algorithm aims to extract utterances carrying the essential content of dialogues. We evaluate the system on 20 Switchboard dialogues. The results show that our system outperforms LEAD, RANDOM and TF*IDF baselines. 1 Introduction Research in automatic text summarization began in the late 1950s and has been receiving more attention again over the last decade. The maturity of this research area is indicated by recent large-scale evaluation efforts (Radev et al., 2003). In comparison, speech summarization is a rather new research area which emerged only a few years ago. However, the demand for speech summarization is growing because of the increasing availability of (digitally encoded) speech databases (e.g. spoken news, political speeches). Our research is concerned with the development of a system for automatically generating summaries of conversational speech. As a potential application we envision the automatic generation of meeting minutes. The approach to spoken dialogue summarization presented herein unifies corpus- and knowledge-based approaches to"
C04-1110,P00-1040,0,0.0266536,"n speech summarization focused mainly on single-speaker, written-to-be-spoken text (e.g. spoken news, political speeches, etc.). The methods were mostly derived from work on text summarization, but extended it by exploiting particular characteristics of spoken language, e.g. acoustic confidence scores or intonation. Difficulties arise because speech recognition systems are not perfect. Therefore, spoken dialogue summarization systems have to deal with errors in the input. There are no sentence boundaries in spoken language either. Work on spoken dialogue summarization is still in its infancy (Reithinger et al., 2000; Zechner, 2002). Multiparty dialogue is much more difficult to process than written text. In addition to the difficulties speech summarization has to face, spoken dialogue contains a whole range of dialogue phenomena as disfluencies, hesitations, interruptions, etc. Also, the information to be summarized may be contributed by different speakers (e.g. in questionanswer pairs). Finally, the language used in spoken dialogue differs from language used in texts. Because discourse participants are able to immediately clarify misunderstandings, the language used does not have to be that explicit. 3"
C04-1110,J02-4002,0,0.01784,"ion 4 provides information about the data used in our experiments, while Section 5 describes the experiments and the results together with their statistical significance. 2 Text, Speech and Dialogue Summarization Most research on automatic summarization dealt with written text. This work was based either on corpus-based, statistical methods or on knowledgebased techniques (for an overview over both strands of research see Mani & Maybury (1999)). Recent advances in text summarization are mostly due to statistical techniques with some additional usage of linguistic knowledge, e.g. (Marcu, 2000; Teufel & Moens, 2002), which can be applied to unrestricted input. Research on speech summarization focused mainly on single-speaker, written-to-be-spoken text (e.g. spoken news, political speeches, etc.). The methods were mostly derived from work on text summarization, but extended it by exploiting particular characteristics of spoken language, e.g. acoustic confidence scores or intonation. Difficulties arise because speech recognition systems are not perfect. Therefore, spoken dialogue summarization systems have to deal with errors in the input. There are no sentence boundaries in spoken language either. Work on"
C04-1110,J02-4003,0,0.0123953,"cused mainly on single-speaker, written-to-be-spoken text (e.g. spoken news, political speeches, etc.). The methods were mostly derived from work on text summarization, but extended it by exploiting particular characteristics of spoken language, e.g. acoustic confidence scores or intonation. Difficulties arise because speech recognition systems are not perfect. Therefore, spoken dialogue summarization systems have to deal with errors in the input. There are no sentence boundaries in spoken language either. Work on spoken dialogue summarization is still in its infancy (Reithinger et al., 2000; Zechner, 2002). Multiparty dialogue is much more difficult to process than written text. In addition to the difficulties speech summarization has to face, spoken dialogue contains a whole range of dialogue phenomena as disfluencies, hesitations, interruptions, etc. Also, the information to be summarized may be contributed by different speakers (e.g. in questionanswer pairs). Finally, the language used in spoken dialogue differs from language used in texts. Because discourse participants are able to immediately clarify misunderstandings, the language used does not have to be that explicit. 3 Semantic Similar"
C04-1110,O97-1002,0,\N,Missing
C10-1017,D08-1031,0,0.259086,"ioning Jie Cai and Michael Strube Natural Language Processing Group Heidelberg Institute for Theoretical Studies gGmbH (jie.cai|michael.strube)@h-its.org Abstract We describe a novel approach to coreference resolution which implements a global decision via hypergraph partitioning. In constrast to almost all previous approaches, we do not rely on separate classification and clustering steps, but perform coreference resolution globally in one step. Our hypergraph-based global model implemented within an endto-end coreference resolution system outperforms two strong baselines (Soon et al., 2001; Bengtson & Roth, 2008) using system mentions only. 1 Introduction Coreference resolution is the task of grouping mentions of entities into sets so that all mentions in one set refer to the same entity. Most recent approaches to coreference resolution divide this task into two steps: (1) a classification step which determines whether a pair of mentions is coreferent or which outputs a confidence value, and (2) a clustering step which groups mentions into entities based on the output of step 1. The classification steps of most approaches vary in the choice of the classifier (e.g. decision tree classifiers (Soon et al"
C10-1017,W10-4305,1,0.720457,"nd twinless key mentions, while keeping the size of the system mention set and the key mention set unchanged (which are different from 3 discards each other). For twinless mentions, Ball twinless key mentions for precision and twinless system mentions for recall. Discarding parts of the key mentions, however, makes the fair comparison of precision values difficult. B03 produces counter-intuitive precision by discarding all twinless system mentions. Although it penalizes the recall of all twinless key mentions, so that the Fscores are balanced, it is still too lenient (for further analyses see Cai & Strube (2010)). We devise two variants of the B 3 - and CEAF3 and CEAF algorithms, namely Bsys sys . For computing precision, the algorithms put all twinless true mentions into the response even if they were not extracted. All twinless system mentions which were deemed not coreferent are discarded. Only twinless system mentions which were mistakenly resolved are put into the key. Hence, the system is penalized for resolving mentions not found in the key. For recall the algorithms only consider mentions from the original key by discarding all the twinless system mentions and putting twinless true mentions i"
C10-1017,N07-1011,0,0.612343,"tation and applying graph clustering techniques (Nicolae & Nicolae, 2006), or by employing integer linear programming (Klenner, 2007; Denis & Baldridge, 2009). Since these methods base their global clustering step on a local pairwise model, some global information which could have guided step 2 is already lost. The twin-candidate model (Yang et al., 2008) replaces the pairwise model by learning preferences between two antecedent candidates in step 1 and applies tournament schemes instead of the clustering in step 2. There is little work which deviates from this two-step scheme. Culotta et al. (2007) introduce a first-order probabilistic model which implements features over sets of mentions and thus operates directly on entities. In this paper we describe a novel approach to coreference resolution which avoids the division into two steps and instead performs a global decision in one step. We represent a document as a hypergraph, where the vertices denote mentions and the edges denote relational features between mentions. Coreference resolution is performed globally in one step by partitioning the hypergraph into subhypergraphs so that all mentions in one subhypergraph refer to the same en"
C10-1017,H05-1013,0,0.0286108,"Missing"
C10-1017,H05-1004,0,0.056319,"response even if they were not extracted. All twinless system mentions which were deemed not coreferent are discarded. Only twinless system mentions which were mistakenly resolved are put into the key. Hence, the system is penalized for resolving mentions not found in the key. For recall the algorithms only consider mentions from the original key by discarding all the twinless system mentions and putting twinless true mentions into the response as singletons (algorithm details, simulations and comparison of different systems and metrics are provided in Cai & Strube (2010)). For CEAF sys , φ3 (Luo, 2005) 3 and CEAF is used. Bsys sys report results for endto-end coreference resolution systems adequately. 5.4 Baselines We compare COPA’s performance with two baselines: SOON – the BART (Versley et al., 2008) reimplementation of Soon et al. (2001) – and 148 MUC 3 Bsys CEAF sys MUC6 MUC7 ACE 2003 ACE 2004 MUC6 MUC7 ACE 2003 ACE 2004 MUC6 MUC7 ACE 2003 ACE 2004 SOON P 67.9 67.1 75.8 67.4 78.9 80.0 87.7 85.7 53.0 54.3 68.7 65.2 R 59.4 52.3 56.7 50.4 53.1 49.8 66.9 64.7 56.9 57.3 71.0 67.9 F 63.4 58.8 64.9 57.7 63.5 61.4 75.9 73.8 54.9 55.7 69.8 66.5 COPA with R2 partitioner R P F α⋆ 62.8 66.4 64.5 0."
C10-1017,P04-1018,0,0.27216,"ion pair anaphor and antecedent. It does not take any information of other mentions into account. However, it turned out that it is difficult to improve upon their results just by applying a more sophisticated learning method and without improving the features. We use a reimplementation of their system as first baseline. Bengtson & Roth (2008) push this approach to the limit by devising a much more informative feature set. They report the best results to date on the ACE 2004 data using true mentions. We use their system combined with our preprocessing components as second baseline. Luo et al. (2004) perform the clustering step within a Bell tree representation. Hence their system theoretically has access to all possible outcomes making it a potentially global system. However, the classification step is still based on a pairwise model. Also since the search space in the Bell tree is too large they have to apply search heuristics. Hence, their approach loses much of the power of a truly global approach. Culotta et al. (2007) introduce a first-order probabilistic model which implements features over sets of mentions. They use four features for their first-order model. The first is an enumer"
C10-1017,D08-1067,0,0.101154,"15 minutes, which is slightly faster than our duplicated SOON baseline. 6 Discussion and Outlook Most previous attempts to solve the coreference resolution task globally have been hampered by employing a local pairwise model in the classification step (step 1) while only the clustering step realizes a global approach, e.g. Luo et al. (2004), Nicolae & Nicolae (2006), Klenner (2007), Denis & Baldridge (2009), lesser so Culotta et al. (2007). It has been also observed that improvements in performance on true mentions do not necessarily translate into performance improvements on system mentions (Ng, 2008). In this paper we describe a coreference resolution system, COPA, which implements a global decision in one step via hypergraph partitioning. COPA looks at the whole graph at once which enables it to outperform two strong baselines (Soon et al., 2001; Bengtson & Roth, 2008). COPA’s hypergraph-based strategy can be taken as a general preference model, where the preference for one mention depends on information on all other mentions. We follow Stoyanov et al. (2009) and argue that evaluating the performance of coreference resolution systems on true mentions is unrealistic. Hence we integrate an"
C10-1017,P02-1014,0,0.647483,"ce resolution divide this task into two steps: (1) a classification step which determines whether a pair of mentions is coreferent or which outputs a confidence value, and (2) a clustering step which groups mentions into entities based on the output of step 1. The classification steps of most approaches vary in the choice of the classifier (e.g. decision tree classifiers (Soon et al., 2001), maximum entropy classification (Luo et al., 2004), SVM classifiers (Rahman & Ng, 2009)) and the number of features used (Soon et al. (2001) employ a set of twelve simple but effective features while e.g., Ng & Cardie (2002) and Bengtson & Roth (2008) devise much richer feature sets). The clustering step exhibits much more variation: Local variants utilize a closest-first decision (Soon et al., 2001), where a mention is resolved to its closest possible antecedent, or a best-first decision (Ng & Cardie, 2002), where a mention is resolved to its most confident antecedent (based on the confidence value returned by step 1). Global variants attempt to consider all possible clustering possibilites by creating and searching a Bell tree (Luo et al., 2004), by learning the optimal search strategy itself (Daum´e III & Marc"
C10-1017,W06-1633,0,0.897551,"its much more variation: Local variants utilize a closest-first decision (Soon et al., 2001), where a mention is resolved to its closest possible antecedent, or a best-first decision (Ng & Cardie, 2002), where a mention is resolved to its most confident antecedent (based on the confidence value returned by step 1). Global variants attempt to consider all possible clustering possibilites by creating and searching a Bell tree (Luo et al., 2004), by learning the optimal search strategy itself (Daum´e III & Marcu, 2005), by building a graph representation and applying graph clustering techniques (Nicolae & Nicolae, 2006), or by employing integer linear programming (Klenner, 2007; Denis & Baldridge, 2009). Since these methods base their global clustering step on a local pairwise model, some global information which could have guided step 2 is already lost. The twin-candidate model (Yang et al., 2008) replaces the pairwise model by learning preferences between two antecedent candidates in step 1 and applies tournament schemes instead of the clustering in step 2. There is little work which deviates from this two-step scheme. Culotta et al. (2007) introduce a first-order probabilistic model which implements featu"
C10-1017,D09-1101,0,0.0986868,"of grouping mentions of entities into sets so that all mentions in one set refer to the same entity. Most recent approaches to coreference resolution divide this task into two steps: (1) a classification step which determines whether a pair of mentions is coreferent or which outputs a confidence value, and (2) a clustering step which groups mentions into entities based on the output of step 1. The classification steps of most approaches vary in the choice of the classifier (e.g. decision tree classifiers (Soon et al., 2001), maximum entropy classification (Luo et al., 2004), SVM classifiers (Rahman & Ng, 2009)) and the number of features used (Soon et al. (2001) employ a set of twelve simple but effective features while e.g., Ng & Cardie (2002) and Bengtson & Roth (2008) devise much richer feature sets). The clustering step exhibits much more variation: Local variants utilize a closest-first decision (Soon et al., 2001), where a mention is resolved to its closest possible antecedent, or a best-first decision (Ng & Cardie, 2002), where a mention is resolved to its most confident antecedent (based on the confidence value returned by step 1). Global variants attempt to consider all possible clustering"
C10-1017,P09-1074,0,0.172181,"Missing"
C10-1017,M95-1005,0,0.835213,"Missing"
C10-1017,J08-3002,0,0.0496513,"value returned by step 1). Global variants attempt to consider all possible clustering possibilites by creating and searching a Bell tree (Luo et al., 2004), by learning the optimal search strategy itself (Daum´e III & Marcu, 2005), by building a graph representation and applying graph clustering techniques (Nicolae & Nicolae, 2006), or by employing integer linear programming (Klenner, 2007; Denis & Baldridge, 2009). Since these methods base their global clustering step on a local pairwise model, some global information which could have guided step 2 is already lost. The twin-candidate model (Yang et al., 2008) replaces the pairwise model by learning preferences between two antecedent candidates in step 1 and applies tournament schemes instead of the clustering in step 2. There is little work which deviates from this two-step scheme. Culotta et al. (2007) introduce a first-order probabilistic model which implements features over sets of mentions and thus operates directly on entities. In this paper we describe a novel approach to coreference resolution which avoids the division into two steps and instead performs a global decision in one step. We represent a document as a hypergraph, where the verti"
C10-1017,J01-4004,0,\N,Missing
C12-1050,E06-1002,0,0.268361,"Missing"
C12-1050,D12-1076,0,0.0356943,"Missing"
C12-1050,I11-1095,0,0.105206,"Missing"
C12-1050,C10-1032,0,0.0875677,"Missing"
C12-1050,H92-1045,0,0.0476517,"Missing"
C12-1050,D12-1010,0,0.0827122,"Missing"
C12-1050,P11-1115,0,0.0879506,"Missing"
C12-1050,W12-3016,0,0.066174,"Missing"
C12-1050,P12-2045,0,0.0246015,"Missing"
C12-1050,C10-2121,0,0.070166,"Missing"
C12-1050,P11-1138,0,0.21112,"Missing"
C12-1050,J98-1004,0,0.357854,"Missing"
C12-1050,D12-1011,0,0.0295438,"Missing"
C12-1050,P11-1080,0,0.0535157,"Missing"
C12-1050,P12-1040,0,0.0404617,"Missing"
C12-1050,C10-1150,0,0.124341,"Missing"
C12-1050,W10-3503,0,\N,Missing
C14-1061,W97-1306,0,0.496662,"n this paper, we propose a new precision oriented method for unsupervised coreference resolution. Our method evaluates the candidate entities of mentions based on the most precise relation of each mention and its candidate entity. Though we develop and evaluate our method for the English language, we intend to apply it to low resource languages in the future. Common coreference resolution approaches rely on a combination of different features for each decision (for an overview over such approaches, see Ng (2010)). However, a few approaches break down this combination having precision in mind (Baldwin, 1997; Zhou and Su, 2004; Haghighi and Klein, 2009; Lee et al., 2013). The idea of starting with high precision knowledge is used in various NLP tasks including parsing (Borghesi and Favareto, 1982), word alignment (Brown et al., 1993), and named entity classification (Collins and Singer, 1999) with different names like “islands of reliability”, “stepping stones”, and “cautiousness”. Lee et al. (2013) is a successful recent work that implements this idea as “sieve architecture”. Lee et al. (2013) first decide on the basis of more precise features, and then they extend these decisions by using less"
C14-1061,D08-1031,0,0.0273219,"hared task (Pradhan et al., 2012). This data set consists of 303 documents. • OntoNotes-Test: Test set of the OntoNotes data provided by the CoNLL2012 shared task (Pradhan et al., 2012). This data set consists of 322 documents. • ACE2004-nwire: Newswire subset of the ACE 2004 data set consisting of 128 documents. This split of ACE2004 has been utilized in previous work (Poon and Domingos, 2008; Finkel and Manning, 2008; Haghighi and Klein, 2009; Lee et al., 2013). • ACE2004-Culotta-Test: One of the test splits of the ACE 2004 data set that has been used in previous work (Culotta et al., 2007; Bengtson and Roth, 2008; Haghighi and Klein, 2009; Lee et al., 2013). This data set consists of 107 documents. • ACE2003-BNEWS: BNEWS subset of the ACE 2003 data set utilized in Ng (2008) and Kobdani et al. (2011) consisting of 51 documents. • ACE2003-NWIRE: NWIRE subset of the ACE 2003 data set utilized in Ng (2008) and Kobdani et al. (2011) consisting of 29 documents. 4.3 Preprocessing The mention detection of the Stanford coreference system (Lee et al., 2013) is used for the OntoNotes data sets. We use the predicted information in the OntoNotes data sets for named entity labels, and syntactic roles. For experimen"
C14-1061,W12-4503,0,0.139397,"Missing"
C14-1061,C82-1006,0,0.160831,"e relation of each mention and its candidate entity. Though we develop and evaluate our method for the English language, we intend to apply it to low resource languages in the future. Common coreference resolution approaches rely on a combination of different features for each decision (for an overview over such approaches, see Ng (2010)). However, a few approaches break down this combination having precision in mind (Baldwin, 1997; Zhou and Su, 2004; Haghighi and Klein, 2009; Lee et al., 2013). The idea of starting with high precision knowledge is used in various NLP tasks including parsing (Borghesi and Favareto, 1982), word alignment (Brown et al., 1993), and named entity classification (Collins and Singer, 1999) with different names like “islands of reliability”, “stepping stones”, and “cautiousness”. Lee et al. (2013) is a successful recent work that implements this idea as “sieve architecture”. Lee et al. (2013) first decide on the basis of more precise features, and then they extend these decisions by using less precise features in later sieves. In this system less precise knowledge is used for extending the decisions made by high precision knowledge. Our proposed inference method goes in the same dire"
C14-1061,J93-2003,0,0.0255109,"ntity. Though we develop and evaluate our method for the English language, we intend to apply it to low resource languages in the future. Common coreference resolution approaches rely on a combination of different features for each decision (for an overview over such approaches, see Ng (2010)). However, a few approaches break down this combination having precision in mind (Baldwin, 1997; Zhou and Su, 2004; Haghighi and Klein, 2009; Lee et al., 2013). The idea of starting with high precision knowledge is used in various NLP tasks including parsing (Borghesi and Favareto, 1982), word alignment (Brown et al., 1993), and named entity classification (Collins and Singer, 1999) with different names like “islands of reliability”, “stepping stones”, and “cautiousness”. Lee et al. (2013) is a successful recent work that implements this idea as “sieve architecture”. Lee et al. (2013) first decide on the basis of more precise features, and then they extend these decisions by using less precise features in later sieves. In this system less precise knowledge is used for extending the decisions made by high precision knowledge. Our proposed inference method goes in the same direction but in a different way. The pro"
C14-1061,W99-0611,0,0.116654,"). The success of statistical approaches in different NLP tasks together with the availability of coreference annotated corpora (like MUC-6 (Chinchor and Sundheim, 2003) and MUC-7 (Chinchor, 2001)) facilitated a shift from deploying rule-based methods to machine learning approaches in coreference research in the 1990s. The increasing importance of multilingual processing, brought the deployment of semi-supervised and unsupervised methods into attention for automatic processing of limited resource languages. There are several works which treat coreference resolution as an unsupervised problem (Cardie and Wagstaff, 1999; Angheluta et al., 2004; Haghighi and Klein, 2007; Ng, 2008; Poon and Domingos, 2008; Haghighi and Klein, 2009; Haghighi and Klein, 2010; Kobdani et al., 2011). We compare our results with the unsupervised systems of Haghighi and Klein (2007), Poon and Domingos (2008), Haghighi and Klein (2009), and Kobdani et al. (2011). The Haghighi and Klein (2010) approach is an almost unsupervised approach, and we do not include this system in our comparisons. We use the expectation maximization algorithm for unsupervised learning. EM has been previously used for coreference resolution (Cherry and Bergsm"
C14-1061,E09-1018,0,0.0867215,"2004; Haghighi and Klein, 2007; Ng, 2008; Poon and Domingos, 2008; Haghighi and Klein, 2009; Haghighi and Klein, 2010; Kobdani et al., 2011). We compare our results with the unsupervised systems of Haghighi and Klein (2007), Poon and Domingos (2008), Haghighi and Klein (2009), and Kobdani et al. (2011). The Haghighi and Klein (2010) approach is an almost unsupervised approach, and we do not include this system in our comparisons. We use the expectation maximization algorithm for unsupervised learning. EM has been previously used for coreference resolution (Cherry and Bergsma, 2005; Ng, 2008; Charniak and Elsner, 2009). Cherry and Bergsma (2005) and Charniak and Elsner (2009) use EM for pronoun resolution, and Ng (2008) models coreference resolution as EM clustering. The model parameters of Ng (2008) are of the form P (f1 , . . . , fk |Cij ), where fi is a feature, and Cij corresponds to the coreference decision of two mentions mi and mj . These parameters along with the entity set, are two sets of unknown variables in Ng (2008). He computes the posterior probabilities of entities in the E-step, and determines the parameters from the N-best clustering (i.e. estimated entities) in the M-step. Ng (2008) start"
C14-1061,W05-0612,0,0.325792,"nd Wagstaff, 1999; Angheluta et al., 2004; Haghighi and Klein, 2007; Ng, 2008; Poon and Domingos, 2008; Haghighi and Klein, 2009; Haghighi and Klein, 2010; Kobdani et al., 2011). We compare our results with the unsupervised systems of Haghighi and Klein (2007), Poon and Domingos (2008), Haghighi and Klein (2009), and Kobdani et al. (2011). The Haghighi and Klein (2010) approach is an almost unsupervised approach, and we do not include this system in our comparisons. We use the expectation maximization algorithm for unsupervised learning. EM has been previously used for coreference resolution (Cherry and Bergsma, 2005; Ng, 2008; Charniak and Elsner, 2009). Cherry and Bergsma (2005) and Charniak and Elsner (2009) use EM for pronoun resolution, and Ng (2008) models coreference resolution as EM clustering. The model parameters of Ng (2008) are of the form P (f1 , . . . , fk |Cij ), where fi is a feature, and Cij corresponds to the coreference decision of two mentions mi and mj . These parameters along with the entity set, are two sets of unknown variables in Ng (2008). He computes the posterior probabilities of entities in the E-step, and determines the parameters from the N-best clustering (i.e. estimated en"
C14-1061,W99-0613,0,0.238897,"he English language, we intend to apply it to low resource languages in the future. Common coreference resolution approaches rely on a combination of different features for each decision (for an overview over such approaches, see Ng (2010)). However, a few approaches break down this combination having precision in mind (Baldwin, 1997; Zhou and Su, 2004; Haghighi and Klein, 2009; Lee et al., 2013). The idea of starting with high precision knowledge is used in various NLP tasks including parsing (Borghesi and Favareto, 1982), word alignment (Brown et al., 1993), and named entity classification (Collins and Singer, 1999) with different names like “islands of reliability”, “stepping stones”, and “cautiousness”. Lee et al. (2013) is a successful recent work that implements this idea as “sieve architecture”. Lee et al. (2013) first decide on the basis of more precise features, and then they extend these decisions by using less precise features in later sieves. In this system less precise knowledge is used for extending the decisions made by high precision knowledge. Our proposed inference method goes in the same direction but in a different way. The probability of each coreference decision is computed based on a"
C14-1061,N07-1011,0,0.0268872,"ded by the CoNLL2012 shared task (Pradhan et al., 2012). This data set consists of 303 documents. • OntoNotes-Test: Test set of the OntoNotes data provided by the CoNLL2012 shared task (Pradhan et al., 2012). This data set consists of 322 documents. • ACE2004-nwire: Newswire subset of the ACE 2004 data set consisting of 128 documents. This split of ACE2004 has been utilized in previous work (Poon and Domingos, 2008; Finkel and Manning, 2008; Haghighi and Klein, 2009; Lee et al., 2013). • ACE2004-Culotta-Test: One of the test splits of the ACE 2004 data set that has been used in previous work (Culotta et al., 2007; Bengtson and Roth, 2008; Haghighi and Klein, 2009; Lee et al., 2013). This data set consists of 107 documents. • ACE2003-BNEWS: BNEWS subset of the ACE 2003 data set utilized in Ng (2008) and Kobdani et al. (2011) consisting of 51 documents. • ACE2003-NWIRE: NWIRE subset of the ACE 2003 data set utilized in Ng (2008) and Kobdani et al. (2011) consisting of 29 documents. 4.3 Preprocessing The mention detection of the Stanford coreference system (Lee et al., 2013) is used for the OntoNotes data sets. We use the predicted information in the OntoNotes data sets for named entity labels, and synta"
C14-1061,J93-1003,0,0.467455,"potheses: Hypothesis 0: P (C = 1|r = 1) = p = P (C = 1|r = 0) (6) Hypothesis 1: P (C = 1|r = 1) = p1 6= p2 = P (C = 1|r = 0) (7) where C ∈ {0, 1} is a random variable for coreference decisions. Hypothesis 0 (null hypothesis) formalizes independence (the coreference decisions are independent of relation r). Hypothesis 1 formalizes dependence, which in case p1  p2 indicates a strong positive association between r and C. This is the pattern that we are interested in. We use the G2 log-likelihood ratio statistics for testing these hypotheses. The statistics was introduced to the NLP community by Dunning (1993), and is defined as follows: −2 log λ = 2 · log L(H1) L(H0) (8) where L(H) is the likelihood of a hypothesis based on observed data assuming a binomial probability distribution for the existence of r between coreferring mentions. Asymptotically, −2 log λ is χ2 distributed with one degree of freedom. Assuming that we have the whole set of entities of input documents, we can use the maximum likelihood estimator to compute p1 , p2 , and p as follows: P P P n∈u r(m, n) u∈E m∈u P P n6=m p1 = y∈M r(x, y) x∈M y6=x P P P n∈u (1 − r(m, n)) u∈E m∈u P P n6=m p2 = y∈M (1 − r(x, y)) x∈M y6=x P P P n∈u 1 u∈"
C14-1061,D13-1203,0,0.0834998,"supervised systems on ACE data sets. 4.4 Results We evaluate our proposed model with the most commonly used metrics for coreference resolution: for the OntoNotes data sets MUC (Vilain et al., 1995), B 3 (Bagga and Baldwin, 1998), CEAF (Luo, 2005) and their average F1 as used in the CoNLL 2011 and 2012 shared tasks; for the ACE data sets MUC and B 3 . The experimental results for the OntoNotes and ACE data sets are presented in Tables 1 and 2, respectively. On the OntoNotes test set, we compare our method with the three best publicly available coreference systems including the Berkeley system (Durrett and Klein, 2013), the IMS system (Bj¨orkelund and Farkas, 2012), and the Stanford system (Lee et al., 2013; Recasens et al., 2013). The Berkeley and IMS systems are both supervised approaches with a rich set of lexical features. At the other hand, the Stanford system is a deterministic system with a set of entity-level features that needs to go through all mentions for incorporating each of the input features. The Stanford system is the winner of the CoNLL2011 shared 650 OntoNotes-Dev Same speaker > Compatible head match > Substring > String match > Proper head match > Acronym ACE2004-nwire Compatible head ma"
C14-1061,P08-2012,0,0.0225446,"e considered as coreference relations only in the ACE data. 4.2 Data We evaluate our method on the following data sets: • OntoNotes-Dev: Development set of the OntoNotes data provided by the CoNLL2012 shared task (Pradhan et al., 2012). This data set consists of 303 documents. • OntoNotes-Test: Test set of the OntoNotes data provided by the CoNLL2012 shared task (Pradhan et al., 2012). This data set consists of 322 documents. • ACE2004-nwire: Newswire subset of the ACE 2004 data set consisting of 128 documents. This split of ACE2004 has been utilized in previous work (Poon and Domingos, 2008; Finkel and Manning, 2008; Haghighi and Klein, 2009; Lee et al., 2013). • ACE2004-Culotta-Test: One of the test splits of the ACE 2004 data set that has been used in previous work (Culotta et al., 2007; Bengtson and Roth, 2008; Haghighi and Klein, 2009; Lee et al., 2013). This data set consists of 107 documents. • ACE2003-BNEWS: BNEWS subset of the ACE 2003 data set utilized in Ng (2008) and Kobdani et al. (2011) consisting of 51 documents. • ACE2003-NWIRE: NWIRE subset of the ACE 2003 data set utilized in Ng (2008) and Kobdani et al. (2011) consisting of 29 documents. 4.3 Preprocessing The mention detection of the St"
C14-1061,P05-1045,0,0.0278848,"sisting of 51 documents. • ACE2003-NWIRE: NWIRE subset of the ACE 2003 data set utilized in Ng (2008) and Kobdani et al. (2011) consisting of 29 documents. 4.3 Preprocessing The mention detection of the Stanford coreference system (Lee et al., 2013) is used for the OntoNotes data sets. We use the predicted information in the OntoNotes data sets for named entity labels, and syntactic roles. For experiments on the ACE data sets, gold mentions are used, so that comparison with previous work is possible. For preprocessing, the Stanford parser (Klein and Manning, 2003) and named entity recognizer (Finkel et al., 2005) are deployed. We also use the singleton detection of the Stanford system (Recasens et al., 2013) for the OntoNotes data sets. When both mentions are detected as a singleton by the singleton detection module, the value of all their corresponding relations will be set to zero. In other words, r(m, n) is set to zero when both n and m have been detected as a singleton. For examining the effect of the singleton detection module 649 R MUC P F1 Berkeley IMS Stanford This Work 67.48 65.23 63.95 65 72.97 70.10 65.43 64.27 This Work − Singleton & Genre 65.05 65.44 65.09 65.69 63.83 65.7 System Supervis"
C14-1061,P07-1107,0,0.113098,"nt NLP tasks together with the availability of coreference annotated corpora (like MUC-6 (Chinchor and Sundheim, 2003) and MUC-7 (Chinchor, 2001)) facilitated a shift from deploying rule-based methods to machine learning approaches in coreference research in the 1990s. The increasing importance of multilingual processing, brought the deployment of semi-supervised and unsupervised methods into attention for automatic processing of limited resource languages. There are several works which treat coreference resolution as an unsupervised problem (Cardie and Wagstaff, 1999; Angheluta et al., 2004; Haghighi and Klein, 2007; Ng, 2008; Poon and Domingos, 2008; Haghighi and Klein, 2009; Haghighi and Klein, 2010; Kobdani et al., 2011). We compare our results with the unsupervised systems of Haghighi and Klein (2007), Poon and Domingos (2008), Haghighi and Klein (2009), and Kobdani et al. (2011). The Haghighi and Klein (2010) approach is an almost unsupervised approach, and we do not include this system in our comparisons. We use the expectation maximization algorithm for unsupervised learning. EM has been previously used for coreference resolution (Cherry and Bergsma, 2005; Ng, 2008; Charniak and Elsner, 2009). Che"
C14-1061,D09-1120,0,0.482092,"cision oriented method for unsupervised coreference resolution. Our method evaluates the candidate entities of mentions based on the most precise relation of each mention and its candidate entity. Though we develop and evaluate our method for the English language, we intend to apply it to low resource languages in the future. Common coreference resolution approaches rely on a combination of different features for each decision (for an overview over such approaches, see Ng (2010)). However, a few approaches break down this combination having precision in mind (Baldwin, 1997; Zhou and Su, 2004; Haghighi and Klein, 2009; Lee et al., 2013). The idea of starting with high precision knowledge is used in various NLP tasks including parsing (Borghesi and Favareto, 1982), word alignment (Brown et al., 1993), and named entity classification (Collins and Singer, 1999) with different names like “islands of reliability”, “stepping stones”, and “cautiousness”. Lee et al. (2013) is a successful recent work that implements this idea as “sieve architecture”. Lee et al. (2013) first decide on the basis of more precise features, and then they extend these decisions by using less precise features in later sieves. In this sys"
C14-1061,N10-1061,0,0.104074,"6 (Chinchor and Sundheim, 2003) and MUC-7 (Chinchor, 2001)) facilitated a shift from deploying rule-based methods to machine learning approaches in coreference research in the 1990s. The increasing importance of multilingual processing, brought the deployment of semi-supervised and unsupervised methods into attention for automatic processing of limited resource languages. There are several works which treat coreference resolution as an unsupervised problem (Cardie and Wagstaff, 1999; Angheluta et al., 2004; Haghighi and Klein, 2007; Ng, 2008; Poon and Domingos, 2008; Haghighi and Klein, 2009; Haghighi and Klein, 2010; Kobdani et al., 2011). We compare our results with the unsupervised systems of Haghighi and Klein (2007), Poon and Domingos (2008), Haghighi and Klein (2009), and Kobdani et al. (2011). The Haghighi and Klein (2010) approach is an almost unsupervised approach, and we do not include this system in our comparisons. We use the expectation maximization algorithm for unsupervised learning. EM has been previously used for coreference resolution (Cherry and Bergsma, 2005; Ng, 2008; Charniak and Elsner, 2009). Cherry and Bergsma (2005) and Charniak and Elsner (2009) use EM for pronoun resolution, an"
C14-1061,J06-4003,0,0.0163222,"precise relations (relations that are randomly spread among coreferring and noncoreferring mentions) will get a lower score. The formulation of the log-likelihood ratio in Dunning (1993) is a two-tailed statistical test that if p1 and p2 significantly diverge from each other, the −2 log λ would get a high value. However, as mentioned above, we are just interested in the cases that p1 is much higher than p2 , because, otherwise, coreference links among the mentions which have the relation r in common are less frequent than expected. Therefore, we use the one-sidedness condition as discussed by Kiss and Strunk (2006) for the loglikelihood test. In this case, a relation r is selected as an informative relation for coreference resolution when the −2 log λ is larger than the desired threshold, and also p1 > p2 : ( −2 log λ if p1 > p2 IS(r) = (10) 0 otherwise We compute the values of {IS(r)} based on entities of the whole set of input documents in order to have a global estimation of the associations in the input data. In order to have a domain- or genrespecific model, one should learn different {IS(r)} for each different domain/genre. The domain/genre adaptation is discussed in more detail in the discussion"
C14-1061,P03-1054,0,0.00441391,"et utilized in Ng (2008) and Kobdani et al. (2011) consisting of 51 documents. • ACE2003-NWIRE: NWIRE subset of the ACE 2003 data set utilized in Ng (2008) and Kobdani et al. (2011) consisting of 29 documents. 4.3 Preprocessing The mention detection of the Stanford coreference system (Lee et al., 2013) is used for the OntoNotes data sets. We use the predicted information in the OntoNotes data sets for named entity labels, and syntactic roles. For experiments on the ACE data sets, gold mentions are used, so that comparison with previous work is possible. For preprocessing, the Stanford parser (Klein and Manning, 2003) and named entity recognizer (Finkel et al., 2005) are deployed. We also use the singleton detection of the Stanford system (Recasens et al., 2013) for the OntoNotes data sets. When both mentions are detected as a singleton by the singleton detection module, the value of all their corresponding relations will be set to zero. In other words, r(m, n) is set to zero when both n and m have been detected as a singleton. For examining the effect of the singleton detection module 649 R MUC P F1 Berkeley IMS Stanford This Work 67.48 65.23 63.95 65 72.97 70.10 65.43 64.27 This Work − Singleton & Genre"
C14-1061,P11-1079,0,0.0795299,"2003) and MUC-7 (Chinchor, 2001)) facilitated a shift from deploying rule-based methods to machine learning approaches in coreference research in the 1990s. The increasing importance of multilingual processing, brought the deployment of semi-supervised and unsupervised methods into attention for automatic processing of limited resource languages. There are several works which treat coreference resolution as an unsupervised problem (Cardie and Wagstaff, 1999; Angheluta et al., 2004; Haghighi and Klein, 2007; Ng, 2008; Poon and Domingos, 2008; Haghighi and Klein, 2009; Haghighi and Klein, 2010; Kobdani et al., 2011). We compare our results with the unsupervised systems of Haghighi and Klein (2007), Poon and Domingos (2008), Haghighi and Klein (2009), and Kobdani et al. (2011). The Haghighi and Klein (2010) approach is an almost unsupervised approach, and we do not include this system in our comparisons. We use the expectation maximization algorithm for unsupervised learning. EM has been previously used for coreference resolution (Cherry and Bergsma, 2005; Ng, 2008; Charniak and Elsner, 2009). Cherry and Bergsma (2005) and Charniak and Elsner (2009) use EM for pronoun resolution, and Ng (2008) models core"
C14-1061,J94-4002,0,0.193292,"on Maximization (EM) algorithm. Overall, our inference method first finds the most precise relation that a mention has with its candidate entity based on the computed informativeness scores. It then computes the probability of joining the mention to the entity based on this best relation and its distribution among all candidate entities. We empirically validate our approach on the OntoNotes and ACE data sets, showing that despite being entirely unsupervised, our system performs well on all benchmark data sets. 2 Related Work Early coreference resolution systems were mainly rule-based systems (Lappin and Leass, 1994; Baldwin, 1997). The success of statistical approaches in different NLP tasks together with the availability of coreference annotated corpora (like MUC-6 (Chinchor and Sundheim, 2003) and MUC-7 (Chinchor, 2001)) facilitated a shift from deploying rule-based methods to machine learning approaches in coreference research in the 1990s. The increasing importance of multilingual processing, brought the deployment of semi-supervised and unsupervised methods into attention for automatic processing of limited resource languages. There are several works which treat coreference resolution as an unsuper"
C14-1061,J13-4004,0,0.305459,"unsupervised coreference resolution. Our method evaluates the candidate entities of mentions based on the most precise relation of each mention and its candidate entity. Though we develop and evaluate our method for the English language, we intend to apply it to low resource languages in the future. Common coreference resolution approaches rely on a combination of different features for each decision (for an overview over such approaches, see Ng (2010)). However, a few approaches break down this combination having precision in mind (Baldwin, 1997; Zhou and Su, 2004; Haghighi and Klein, 2009; Lee et al., 2013). The idea of starting with high precision knowledge is used in various NLP tasks including parsing (Borghesi and Favareto, 1982), word alignment (Brown et al., 1993), and named entity classification (Collins and Singer, 1999) with different names like “islands of reliability”, “stepping stones”, and “cautiousness”. Lee et al. (2013) is a successful recent work that implements this idea as “sieve architecture”. Lee et al. (2013) first decide on the basis of more precise features, and then they extend these decisions by using less precise features in later sieves. In this system less precise kn"
C14-1061,P04-1018,0,0.0513107,"e form P (f1 , . . . , fk |Cij ), where fi is a feature, and Cij corresponds to the coreference decision of two mentions mi and mj . These parameters along with the entity set, are two sets of unknown variables in Ng (2008). He computes the posterior probabilities of entities in the E-step, and determines the parameters from the N-best clustering (i.e. estimated entities) in the M-step. Ng (2008) starts from an initial guess about the entities and determines the parameters based on this initial guess (M-step). In order to compute the N-best clustering, Ng (2008) uses the Bell tree approach of Luo et al. (2004). The informativeness scores of mention pair relations (Section 3.2.1) are our unknown parameters. Our inference method only requires the ranking of the informativeness scores (and not their exact values). Therefore, it is much easier to estimate the ranking of these parameters than parameters like P (f1 , . . . , fk |Cij ), and our search space for finding an optimized ranking of the informativeness scores is very small. Since it is easier to have an initial guess about the ranking of informativeness scores (rather than guessing an initial entity set), we start from an E-step with a random ra"
C14-1061,H05-1004,0,0.208049,"68.3 62.0 56.1 71.4 62.8 65.0 69.5 67.1 70.35 65.9 89.56 70.2 78.80 68.0 This Work Haghighi07 Poon08 Haghighi09 ACE2004-nwire 74.77 84.53 79.35 62.3 66.7 64.2 71.3 70.5 70.9 75.09 77.0 76.5 74.21 74.5 87.50 79.4 80.31 76.9 This Work Haghighi09 ACE2004-Culotta-Test 68.88 82.42 75.04 73.62 77.7 74.8 79.6 78.5 88.87 79.6 80.53 79.0 Table 2: Comparison with other unsupervised systems on ACE data sets. 4.4 Results We evaluate our proposed model with the most commonly used metrics for coreference resolution: for the OntoNotes data sets MUC (Vilain et al., 1995), B 3 (Bagga and Baldwin, 1998), CEAF (Luo, 2005) and their average F1 as used in the CoNLL 2011 and 2012 shared tasks; for the ACE data sets MUC and B 3 . The experimental results for the OntoNotes and ACE data sets are presented in Tables 1 and 2, respectively. On the OntoNotes test set, we compare our method with the three best publicly available coreference systems including the Berkeley system (Durrett and Klein, 2013), the IMS system (Bj¨orkelund and Farkas, 2012), and the Stanford system (Lee et al., 2013; Recasens et al., 2013). The Berkeley and IMS systems are both supervised approaches with a rich set of lexical features. At the ot"
C14-1061,D08-1067,0,0.639481,"the availability of coreference annotated corpora (like MUC-6 (Chinchor and Sundheim, 2003) and MUC-7 (Chinchor, 2001)) facilitated a shift from deploying rule-based methods to machine learning approaches in coreference research in the 1990s. The increasing importance of multilingual processing, brought the deployment of semi-supervised and unsupervised methods into attention for automatic processing of limited resource languages. There are several works which treat coreference resolution as an unsupervised problem (Cardie and Wagstaff, 1999; Angheluta et al., 2004; Haghighi and Klein, 2007; Ng, 2008; Poon and Domingos, 2008; Haghighi and Klein, 2009; Haghighi and Klein, 2010; Kobdani et al., 2011). We compare our results with the unsupervised systems of Haghighi and Klein (2007), Poon and Domingos (2008), Haghighi and Klein (2009), and Kobdani et al. (2011). The Haghighi and Klein (2010) approach is an almost unsupervised approach, and we do not include this system in our comparisons. We use the expectation maximization algorithm for unsupervised learning. EM has been previously used for coreference resolution (Cherry and Bergsma, 2005; Ng, 2008; Charniak and Elsner, 2009). Cherry and Be"
C14-1061,P10-1142,0,0.13974,"employing unsupervised methods has become a popular and important area of research in NLP. In this paper, we propose a new precision oriented method for unsupervised coreference resolution. Our method evaluates the candidate entities of mentions based on the most precise relation of each mention and its candidate entity. Though we develop and evaluate our method for the English language, we intend to apply it to low resource languages in the future. Common coreference resolution approaches rely on a combination of different features for each decision (for an overview over such approaches, see Ng (2010)). However, a few approaches break down this combination having precision in mind (Baldwin, 1997; Zhou and Su, 2004; Haghighi and Klein, 2009; Lee et al., 2013). The idea of starting with high precision knowledge is used in various NLP tasks including parsing (Borghesi and Favareto, 1982), word alignment (Brown et al., 1993), and named entity classification (Collins and Singer, 1999) with different names like “islands of reliability”, “stepping stones”, and “cautiousness”. Lee et al. (2013) is a successful recent work that implements this idea as “sieve architecture”. Lee et al. (2013) first d"
C14-1061,D08-1068,0,0.385512,"ability of coreference annotated corpora (like MUC-6 (Chinchor and Sundheim, 2003) and MUC-7 (Chinchor, 2001)) facilitated a shift from deploying rule-based methods to machine learning approaches in coreference research in the 1990s. The increasing importance of multilingual processing, brought the deployment of semi-supervised and unsupervised methods into attention for automatic processing of limited resource languages. There are several works which treat coreference resolution as an unsupervised problem (Cardie and Wagstaff, 1999; Angheluta et al., 2004; Haghighi and Klein, 2007; Ng, 2008; Poon and Domingos, 2008; Haghighi and Klein, 2009; Haghighi and Klein, 2010; Kobdani et al., 2011). We compare our results with the unsupervised systems of Haghighi and Klein (2007), Poon and Domingos (2008), Haghighi and Klein (2009), and Kobdani et al. (2011). The Haghighi and Klein (2010) approach is an almost unsupervised approach, and we do not include this system in our comparisons. We use the expectation maximization algorithm for unsupervised learning. EM has been previously used for coreference resolution (Cherry and Bergsma, 2005; Ng, 2008; Charniak and Elsner, 2009). Cherry and Bergsma (2005) and Charniak"
C14-1061,W12-4501,0,0.0371885,". The compatibility is measured in terms of number, gender, person, animacy, and named entity label. This approach corresponds to the pronoun resolution strategy of the Stanford system. The differences between the relations of the OntoNotes and ACE corpora is due to the fact that these two corpora have different annotation schemes. Some of the relations mentioned (e.g. Apposition) are considered as coreference relations only in the ACE data. 4.2 Data We evaluate our method on the following data sets: • OntoNotes-Dev: Development set of the OntoNotes data provided by the CoNLL2012 shared task (Pradhan et al., 2012). This data set consists of 303 documents. • OntoNotes-Test: Test set of the OntoNotes data provided by the CoNLL2012 shared task (Pradhan et al., 2012). This data set consists of 322 documents. • ACE2004-nwire: Newswire subset of the ACE 2004 data set consisting of 128 documents. This split of ACE2004 has been utilized in previous work (Poon and Domingos, 2008; Finkel and Manning, 2008; Haghighi and Klein, 2009; Lee et al., 2013). • ACE2004-Culotta-Test: One of the test splits of the ACE 2004 data set that has been used in previous work (Culotta et al., 2007; Bengtson and Roth, 2008; Haghighi"
C14-1061,P14-2006,1,0.844011,"that outperforms both the Stanford and IMS systems. Despite being totally unsupervised and using pairwise features, the results of our system are on par with those of the Stanford system (according to the approximate randomization test, there is no significant difference). The comparison with this state-of-the-art rule based system (Lee et al., 2013), indicates the effectiveness of our coreference resolution approach, as it uses the same preprocessing modules and a simpler and smaller set of features. All results in the Table 1 are reported using the scorer-v71 of the CoNLL-2012 shared task (Pradhan et al., 2014). On the ACE data sets, we compare our performance to those of the unsupervised systems mentioned in Section 2. As Table 2 shows, our method considerably outperforms other unsupervised systems on all data sets (except only for the MUC measure on the ACE2004-Culotta-Test data set). 5 Discussion 5.1 Informativeness Score As discussed in Section 3.2.1, we determine the discriminative power of mention pair relations in coreference decisions based on the informativeness score (Equation 10), in which the statistical test is computed on the unsupervised estimated set of entities. The resulting rankin"
C14-1061,N13-1071,0,0.0863088,"Missing"
C14-1061,M95-1005,0,0.705069,"g08 Kobdani11 (UNSEL) ACE2003-BNEWS 67.36 84.72 75.05 56.8 68.3 62.0 56.1 71.4 62.8 65.0 69.5 67.1 70.35 65.9 89.56 70.2 78.80 68.0 This Work Haghighi07 Poon08 Haghighi09 ACE2004-nwire 74.77 84.53 79.35 62.3 66.7 64.2 71.3 70.5 70.9 75.09 77.0 76.5 74.21 74.5 87.50 79.4 80.31 76.9 This Work Haghighi09 ACE2004-Culotta-Test 68.88 82.42 75.04 73.62 77.7 74.8 79.6 78.5 88.87 79.6 80.53 79.0 Table 2: Comparison with other unsupervised systems on ACE data sets. 4.4 Results We evaluate our proposed model with the most commonly used metrics for coreference resolution: for the OntoNotes data sets MUC (Vilain et al., 1995), B 3 (Bagga and Baldwin, 1998), CEAF (Luo, 2005) and their average F1 as used in the CoNLL 2011 and 2012 shared tasks; for the ACE data sets MUC and B 3 . The experimental results for the OntoNotes and ACE data sets are presented in Tables 1 and 2, respectively. On the OntoNotes test set, we compare our method with the three best publicly available coreference systems including the Berkeley system (Durrett and Klein, 2013), the IMS system (Bj¨orkelund and Farkas, 2012), and the Stanford system (Lee et al., 2013; Recasens et al., 2013). The Berkeley and IMS systems are both supervised approach"
C14-1061,C04-1075,0,0.0309115,"e propose a new precision oriented method for unsupervised coreference resolution. Our method evaluates the candidate entities of mentions based on the most precise relation of each mention and its candidate entity. Though we develop and evaluate our method for the English language, we intend to apply it to low resource languages in the future. Common coreference resolution approaches rely on a combination of different features for each decision (for an overview over such approaches, see Ng (2010)). However, a few approaches break down this combination having precision in mind (Baldwin, 1997; Zhou and Su, 2004; Haghighi and Klein, 2009; Lee et al., 2013). The idea of starting with high precision knowledge is used in various NLP tasks including parsing (Borghesi and Favareto, 1982), word alignment (Brown et al., 1993), and named entity classification (Collins and Singer, 1999) with different names like “islands of reliability”, “stepping stones”, and “cautiousness”. Lee et al. (2013) is a successful recent work that implements this idea as “sieve architecture”. Lee et al. (2013) first decide on the basis of more precise features, and then they extend these decisions by using less precise features in"
C16-1215,W06-0901,0,0.398593,"ata sets share many event types, and events occur only intrasentential. However, some event types in TAC are more fine-grained, e.g., TRANSPORT in ACE was split into TRANSPORT-PERSON and TRANSPORT-ARTIFACT in TAC. 3 Related Work The base system we use is the one described in Li et al. (2014)2 . To our knowledge it is the only system to predict entity mentions, relations, event triggers, and event arguments jointly. It achieves state-of-the-art performance in all four tasks. Many approaches to event extraction, including our base system, do not cross sentence boundaries (Grishman et al., 2005; Ahn, 2006; Lu and Roth, 2012; Li et al., 2013; Li et al., 2014). Only a few 1 2 Some arguments are mentions of points in time, amounts of money, etc. This system is a combination of the systems described in Li et al. (2013) and Li and Ji (2014). 2280 approaches go beyond sentences (Liao and Grishman, 2010; Hong et al., 2011) or beyond documents (Ji and Grishman, 2008) in order to exploit richer contexts for the extraction of events. Recently, three deep learning systems were proposed. Nguyen and Grishman (2015) use a Convolutional Neural Network (CNN) to detect event triggers. They achieve good trigger"
C16-1215,P11-1056,0,0.0222949,"namic features, which depend on previous decisions. Table 1 contains a feature summary struc2282 Common Static Mentions Triggers Arguments Relations Common Triggers Dynamic Arguments Relations Joint Argument-Relation lexical information* (segment tokens, context, dependencies) brown clusters, gazetteer entries* character suffixes of length 3 and 4* possible FrameNet frames in case of verbs* trigger type, mention and mention context trigger-mention dependency/constituency paths entity types, contexts, extent overlaps mention-mention dependency/constituency paths syntactico-semantic structures (Chan and Roth, 2011) (event or entity) type bigrams consistencies (same text=same type, coordinations, pronouns) dependency path between trigger pairs characterize mentions filling the same role in the same event characterize triggers sharing a mention characterize triangles like A-C, B-C characterize constructions where the parts tend to have same relation types, e.g., coordinations relation types and overlapping argument types Table 1: A description of the base system’s feature set. Static features marked with * apply to the entire segment and not to each token in the segment. tured in this way. All features ar"
C16-1215,P15-1017,0,0.0655644,"2 Some arguments are mentions of points in time, amounts of money, etc. This system is a combination of the systems described in Li et al. (2013) and Li and Ji (2014). 2280 approaches go beyond sentences (Liao and Grishman, 2010; Hong et al., 2011) or beyond documents (Ji and Grishman, 2008) in order to exploit richer contexts for the extraction of events. Recently, three deep learning systems were proposed. Nguyen and Grishman (2015) use a Convolutional Neural Network (CNN) to detect event triggers. They achieve good trigger detection performance, but they do not tackle the full event task. Chen et al. (2015) propose a more complicated version of a CNN which is able to detect multiple arguments in addition to triggers. Their system is a pipeline system (it predicts triggers and arguments separately) and suffers from error propagation. Nguyen et al. (2016) combine the advantages of joint models, explicit feature engineering and neural networks. They propose a joint, bi-directional recurrent network which additionally uses the features in Li et al. (2013). However, both Chen et al. (2015) and Nguyen et al. (2016) rely on gold entity mentions for trigger prediction. The cross-sentential systems propo"
C16-1215,J14-1002,0,0.0327829,"ures In this subsection we describe the new features we use. First, we will describe new local features. Then, we will describe the global features needed for our Incremental Global Inference. Table 2 summarizes the features we describe in the following. The first column reports feature types (local or global), the second column reports class requirements (triggers or arguments), the third column reports conditions under which features fire, and the last column gives short descriptions. Local Features The first local feature we introduce to event extraction is based on so-called hidden units (Das et al., 2014). The hidden units of an event type are the exclusive triggers it has in the training data. Hidden units define a semantic space for their event types. Measuring semantic relations of the node under consideration with this space helps to find triggers never seen during training. For example, let the candidate trigger be meeting and the candidate event type be MEET. We have several hidden units of MEET sharing semantic relations with the predominant sense of meeting: convention and summit as WordNet hypernyms (Fellbaum, 1998), and meet as a morphological variant. We can draw features from the h"
C16-1215,P13-1166,0,0.0220438,"Missing"
C16-1215,H92-1045,0,0.839865,"event, which in turn makes it hard to predict that he is the Person of this event. If the system looks at the entire document and knows that he and Sam Waksal are coreferent, it can better infer that the Defendant of a SENTENCE event can be the Person of an ARREST-JAIL event, which in turn makes it easier to infer that he is this person. In this paper, we present a method to incorporate the global, document-wide context into the decision process of a system that predicts entity mentions, events, and relations jointly. We use features that are based on the ‘one sense per discourse’ assumption (Gale et al., 1992), a concept widely used in Word Sense Disambiguation (e.g., Navigli and Lapata (2007)), and on the coherence of roles an entity plays in different events. We show that our method robustly increases performance on two datasets, namely ACE 2005 and TAC 2015. This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/ License details: http:// 2279 Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 2279–2289, Osaka, Japan, December 11-17 2016. Org-aff Agent That ’s why we"
C16-1215,P11-1113,0,0.0823713,"Missing"
C16-1215,N12-1015,0,0.0319804,"he number of configurations grows exponentially with the number of tokens. Following Li et al. (2014) we approximate the enumeration by using beam search. We first expand configurations with new nodes and keep only the best z configurations. These are then expanded with arcs. During this process we again keep only the best z configurations. After arc generation, we move to the next position. In training mode the procedure updates feature weights as soon as yi , a prefix of the gold configuration, is not predictable anymore because it is no longer part of the beam. This is called early update. Huang et al. (2012) proved that, in structured perceptrons, standard updates may lead to bad performance with inexact search strategies such as beam search because there may be updates which actually lower the score of the gold solution, thus leading the model in a wrong direction. Early updates prevent this problem at the expense of higher training time. Note that we exit the procedure when early updates happen. There are two positions where early updates occur, namely after node generation and during arc generation (Lines 6 and 12). 4.3 Features The feature set of our base system is complex because each subtas"
C16-1215,P08-1030,0,0.627748,"tity mentions, relations, event triggers, and event arguments jointly. It achieves state-of-the-art performance in all four tasks. Many approaches to event extraction, including our base system, do not cross sentence boundaries (Grishman et al., 2005; Ahn, 2006; Lu and Roth, 2012; Li et al., 2013; Li et al., 2014). Only a few 1 2 Some arguments are mentions of points in time, amounts of money, etc. This system is a combination of the systems described in Li et al. (2013) and Li and Ji (2014). 2280 approaches go beyond sentences (Liao and Grishman, 2010; Hong et al., 2011) or beyond documents (Ji and Grishman, 2008) in order to exploit richer contexts for the extraction of events. Recently, three deep learning systems were proposed. Nguyen and Grishman (2015) use a Convolutional Neural Network (CNN) to detect event triggers. They achieve good trigger detection performance, but they do not tackle the full event task. Chen et al. (2015) propose a more complicated version of a CNN which is able to detect multiple arguments in addition to triggers. Their system is a pipeline system (it predicts triggers and arguments separately) and suffers from error propagation. Nguyen et al. (2016) combine the advantages"
C16-1215,P14-1038,0,0.0206539,"The base system we use is the one described in Li et al. (2014)2 . To our knowledge it is the only system to predict entity mentions, relations, event triggers, and event arguments jointly. It achieves state-of-the-art performance in all four tasks. Many approaches to event extraction, including our base system, do not cross sentence boundaries (Grishman et al., 2005; Ahn, 2006; Lu and Roth, 2012; Li et al., 2013; Li et al., 2014). Only a few 1 2 Some arguments are mentions of points in time, amounts of money, etc. This system is a combination of the systems described in Li et al. (2013) and Li and Ji (2014). 2280 approaches go beyond sentences (Liao and Grishman, 2010; Hong et al., 2011) or beyond documents (Ji and Grishman, 2008) in order to exploit richer contexts for the extraction of events. Recently, three deep learning systems were proposed. Nguyen and Grishman (2015) use a Convolutional Neural Network (CNN) to detect event triggers. They achieve good trigger detection performance, but they do not tackle the full event task. Chen et al. (2015) propose a more complicated version of a CNN which is able to detect multiple arguments in addition to triggers. Their system is a pipeline system (i"
C16-1215,P13-1008,0,0.110978,"s, and events occur only intrasentential. However, some event types in TAC are more fine-grained, e.g., TRANSPORT in ACE was split into TRANSPORT-PERSON and TRANSPORT-ARTIFACT in TAC. 3 Related Work The base system we use is the one described in Li et al. (2014)2 . To our knowledge it is the only system to predict entity mentions, relations, event triggers, and event arguments jointly. It achieves state-of-the-art performance in all four tasks. Many approaches to event extraction, including our base system, do not cross sentence boundaries (Grishman et al., 2005; Ahn, 2006; Lu and Roth, 2012; Li et al., 2013; Li et al., 2014). Only a few 1 2 Some arguments are mentions of points in time, amounts of money, etc. This system is a combination of the systems described in Li et al. (2013) and Li and Ji (2014). 2280 approaches go beyond sentences (Liao and Grishman, 2010; Hong et al., 2011) or beyond documents (Ji and Grishman, 2008) in order to exploit richer contexts for the extraction of events. Recently, three deep learning systems were proposed. Nguyen and Grishman (2015) use a Convolutional Neural Network (CNN) to detect event triggers. They achieve good trigger detection performance, but they do"
C16-1215,D14-1198,0,0.0773566,"onal Conference on Computational Linguistics: Technical Papers, pages 2279–2289, Osaka, Japan, December 11-17 2016. Org-aff Agent That ’s why we Org founded the National Patient Safety Foundation PER START-ORG ORG Figure 1: The configuration of a sentence. Depicted are two entities, a trigger, and the semantic relations and event argument relations between them. The paper is structured as follows. Section 2 describes the task in more detail. Section 3 puts our work in context of other approaches to event extraction. Section 4 describes an intra-sentential, state-ofthe-art system introduced by Li et al. (2014). In Section 5 we present Incremental Global Inference, a multi-pass procedure that makes the global context of a document accessible to the joint decoding of our intra-sentential event extractor. Section 6 reports evaluation results and Section 7 gives conclusions. 2 Task Description Event extraction is an information extraction task where mentions of predefined event types are extracted from texts. We follow the task definition of the Automatic Content Extraction (ACE) program of 2005 which defines 33 event types, organized in eight categories. We also evaluate on another dataset, namely on"
C16-1215,P10-1081,0,0.660297,"l. (2014)2 . To our knowledge it is the only system to predict entity mentions, relations, event triggers, and event arguments jointly. It achieves state-of-the-art performance in all four tasks. Many approaches to event extraction, including our base system, do not cross sentence boundaries (Grishman et al., 2005; Ahn, 2006; Lu and Roth, 2012; Li et al., 2013; Li et al., 2014). Only a few 1 2 Some arguments are mentions of points in time, amounts of money, etc. This system is a combination of the systems described in Li et al. (2013) and Li and Ji (2014). 2280 approaches go beyond sentences (Liao and Grishman, 2010; Hong et al., 2011) or beyond documents (Ji and Grishman, 2008) in order to exploit richer contexts for the extraction of events. Recently, three deep learning systems were proposed. Nguyen and Grishman (2015) use a Convolutional Neural Network (CNN) to detect event triggers. They achieve good trigger detection performance, but they do not tackle the full event task. Chen et al. (2015) propose a more complicated version of a CNN which is able to detect multiple arguments in addition to triggers. Their system is a pipeline system (it predicts triggers and arguments separately) and suffers from"
C16-1215,P12-1088,0,0.0313681,"are many event types, and events occur only intrasentential. However, some event types in TAC are more fine-grained, e.g., TRANSPORT in ACE was split into TRANSPORT-PERSON and TRANSPORT-ARTIFACT in TAC. 3 Related Work The base system we use is the one described in Li et al. (2014)2 . To our knowledge it is the only system to predict entity mentions, relations, event triggers, and event arguments jointly. It achieves state-of-the-art performance in all four tasks. Many approaches to event extraction, including our base system, do not cross sentence boundaries (Grishman et al., 2005; Ahn, 2006; Lu and Roth, 2012; Li et al., 2013; Li et al., 2014). Only a few 1 2 Some arguments are mentions of points in time, amounts of money, etc. This system is a combination of the systems described in Li et al. (2013) and Li and Ji (2014). 2280 approaches go beyond sentences (Liao and Grishman, 2010; Hong et al., 2011) or beyond documents (Ji and Grishman, 2008) in order to exploit richer contexts for the extraction of events. Recently, three deep learning systems were proposed. Nguyen and Grishman (2015) use a Convolutional Neural Network (CNN) to detect event triggers. They achieve good trigger detection performa"
C16-1215,P15-2060,0,0.0597305,"ches to event extraction, including our base system, do not cross sentence boundaries (Grishman et al., 2005; Ahn, 2006; Lu and Roth, 2012; Li et al., 2013; Li et al., 2014). Only a few 1 2 Some arguments are mentions of points in time, amounts of money, etc. This system is a combination of the systems described in Li et al. (2013) and Li and Ji (2014). 2280 approaches go beyond sentences (Liao and Grishman, 2010; Hong et al., 2011) or beyond documents (Ji and Grishman, 2008) in order to exploit richer contexts for the extraction of events. Recently, three deep learning systems were proposed. Nguyen and Grishman (2015) use a Convolutional Neural Network (CNN) to detect event triggers. They achieve good trigger detection performance, but they do not tackle the full event task. Chen et al. (2015) propose a more complicated version of a CNN which is able to detect multiple arguments in addition to triggers. Their system is a pipeline system (it predicts triggers and arguments separately) and suffers from error propagation. Nguyen et al. (2016) combine the advantages of joint models, explicit feature engineering and neural networks. They propose a joint, bi-directional recurrent network which additionally uses"
C16-1215,N16-1034,0,0.0934298,"or beyond documents (Ji and Grishman, 2008) in order to exploit richer contexts for the extraction of events. Recently, three deep learning systems were proposed. Nguyen and Grishman (2015) use a Convolutional Neural Network (CNN) to detect event triggers. They achieve good trigger detection performance, but they do not tackle the full event task. Chen et al. (2015) propose a more complicated version of a CNN which is able to detect multiple arguments in addition to triggers. Their system is a pipeline system (it predicts triggers and arguments separately) and suffers from error propagation. Nguyen et al. (2016) combine the advantages of joint models, explicit feature engineering and neural networks. They propose a joint, bi-directional recurrent network which additionally uses the features in Li et al. (2013). However, both Chen et al. (2015) and Nguyen et al. (2016) rely on gold entity mentions for trigger prediction. The cross-sentential systems proposed in Liao and Grishman (2010) and Yang and Mitchell (2016) are closest to ours. We will describe them in the following. Liao and Grishman (2010) propose a pipeline system that performs easy-first global inference. Local classifiers find triggers and"
C16-1215,N16-1033,0,0.255938,"of a CNN which is able to detect multiple arguments in addition to triggers. Their system is a pipeline system (it predicts triggers and arguments separately) and suffers from error propagation. Nguyen et al. (2016) combine the advantages of joint models, explicit feature engineering and neural networks. They propose a joint, bi-directional recurrent network which additionally uses the features in Li et al. (2013). However, both Chen et al. (2015) and Nguyen et al. (2016) rely on gold entity mentions for trigger prediction. The cross-sentential systems proposed in Liao and Grishman (2010) and Yang and Mitchell (2016) are closest to ours. We will describe them in the following. Liao and Grishman (2010) propose a pipeline system that performs easy-first global inference. Local classifiers find triggers and arguments based solely on local information. Confident decisions are collected and used to inform global trigger and argument classifiers. In the local, intra-sentential phase the system performs pattern matching to align the entity mention context of a trigger to some known patterns. Similar to our approach, confident decisions are collected during the intra-sentential pass and used to infer harder cases"
C18-2012,Q18-1008,0,0.0220959,"d to be loaded every time. Experiments involving large numbers of WECs are not uncommon: Baroni et al. (2014) employed 48 different WECs, while Levy et al. (2015) used as many as 672. More recently, Wendlandt et al. (2018) explored the (in)stability of word embeddings by evaluating WECs trained for all combinations of three algorithms (two of them involving a random component), five vector sizes (dimensions), and seven data sets. In order to include the effect of randomness, five sets of WECs with different initializations were trained for the two algorithms, resulting in 385 WECs altogether. Antoniak and Mimno (2018) focused on training corpora, in particular on the effect of three different sampling methods. They trained WECs for all combinations of these three methods, four algorithms, six data sets, and two segmentation sizes. To tackle the effect of randomness, they trained repeatedly for 50 This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/ License details: http:// 53 Proceedings of the 27th International Conference on Computational Linguistics: System Demonstrations, pages 53–57 Santa Fe, New Mexico, USA, August 20-26, 2018. tim"
C18-2012,P14-1023,0,0.0493351,"milarity between WECs: two or more WECs might be identical except for their training window sizes, or except for the fact that some additional postprocessing was applied to one of them. For intrinsic and extrinsic evaluation (Schnabel et al., 2015; Nayak et al., 2016) of the effect of different training parameters on WECs, these parameters need to be accessible explicitly, and not just on the level of file names. 2. Experiments with large numbers of WECs do not scale and are inefficient if entire files need to be loaded every time. Experiments involving large numbers of WECs are not uncommon: Baroni et al. (2014) employed 48 different WECs, while Levy et al. (2015) used as many as 672. More recently, Wendlandt et al. (2018) explored the (in)stability of word embeddings by evaluating WECs trained for all combinations of three algorithms (two of them involving a random component), five vector sizes (dimensions), and seven data sets. In order to include the effect of randomness, five sets of WECs with different initializations were trained for the two algorithms, resulting in 385 WECs altogether. Antoniak and Mimno (2018) focused on training corpora, in particular on the effect of three different samplin"
C18-2012,S17-2001,0,0.0141945,"d only) WEC # potential result for second WEC ] Schematic WOMBAT result format. 3 Sample Use Cases At this point, the wombat.analyse library contains only a few methods (cf. below). Our focus has been on developing a stable, generic, and efficient code base, on top of which more complex and useful functionality (incl. further visualizations, nearest neighbors, etc.) can be implemented. 3.1 Global Sentence Similarity In order to demonstrate WOMBAT in an actual end-to-end use case, we applied it to a sentence pair similarity ranking task, using the data set from task 1, track 5 of SemEval-2017 (Cer et al., 2017). The data set consists of 250 tab-separated, raw sentence pairs. Since we focus on preprocessing and vector retrieval, we implement a simple baseline approach only, in which sentences are represented as the average vector of their respective word vectors (excluding stop words) and the pairwise distances are computed as cosine distance. The result is a list containing, for each WEC, an ordered list of tuples of the form &lt;distance, sentence1, sentence2&gt;. The following code implements the whole process. The distance metric in the pairwise distances(...) method is provided as a parameter, and can"
C18-2012,P13-1166,0,0.0324111,"Missing"
C18-2012,Q15-1016,0,0.039992,"cal except for their training window sizes, or except for the fact that some additional postprocessing was applied to one of them. For intrinsic and extrinsic evaluation (Schnabel et al., 2015; Nayak et al., 2016) of the effect of different training parameters on WECs, these parameters need to be accessible explicitly, and not just on the level of file names. 2. Experiments with large numbers of WECs do not scale and are inefficient if entire files need to be loaded every time. Experiments involving large numbers of WECs are not uncommon: Baroni et al. (2014) employed 48 different WECs, while Levy et al. (2015) used as many as 672. More recently, Wendlandt et al. (2018) explored the (in)stability of word embeddings by evaluating WECs trained for all combinations of three algorithms (two of them involving a random component), five vector sizes (dimensions), and seven data sets. In order to include the effect of randomness, five sets of WECs with different initializations were trained for the two algorithms, resulting in 385 WECs altogether. Antoniak and Mimno (2018) focused on training corpora, in particular on the effect of three different sampling methods. They trained WECs for all combinations of"
C18-2012,W16-2504,0,0.0184549,"y, and robustness. 1. Writing code in which WECs are easily and unambiguously identified is difficult when each WEC is treated as a monolith in the file system.This way of identifying WECs completely disregards – and, in the worst case, obscures – the fact that these resources might share some of their meta data, resulting in different degrees of similarity between WECs: two or more WECs might be identical except for their training window sizes, or except for the fact that some additional postprocessing was applied to one of them. For intrinsic and extrinsic evaluation (Schnabel et al., 2015; Nayak et al., 2016) of the effect of different training parameters on WECs, these parameters need to be accessible explicitly, and not just on the level of file names. 2. Experiments with large numbers of WECs do not scale and are inefficient if entire files need to be loaded every time. Experiments involving large numbers of WECs are not uncommon: Baroni et al. (2014) employed 48 different WECs, while Levy et al. (2015) used as many as 672. More recently, Wendlandt et al. (2018) explored the (in)stability of word embeddings by evaluating WECs trained for all combinations of three algorithms (two of them involvi"
C18-2012,D15-1036,0,0.0243033,"transparency, efficiency, and robustness. 1. Writing code in which WECs are easily and unambiguously identified is difficult when each WEC is treated as a monolith in the file system.This way of identifying WECs completely disregards – and, in the worst case, obscures – the fact that these resources might share some of their meta data, resulting in different degrees of similarity between WECs: two or more WECs might be identical except for their training window sizes, or except for the fact that some additional postprocessing was applied to one of them. For intrinsic and extrinsic evaluation (Schnabel et al., 2015; Nayak et al., 2016) of the effect of different training parameters on WECs, these parameters need to be accessible explicitly, and not just on the level of file names. 2. Experiments with large numbers of WECs do not scale and are inefficient if entire files need to be loaded every time. Experiments involving large numbers of WECs are not uncommon: Baroni et al. (2014) employed 48 different WECs, while Levy et al. (2015) used as many as 672. More recently, Wendlandt et al. (2018) explored the (in)stability of word embeddings by evaluating WECs trained for all combinations of three algorithms"
C18-2012,N18-1190,0,0.0190075,"or the fact that some additional postprocessing was applied to one of them. For intrinsic and extrinsic evaluation (Schnabel et al., 2015; Nayak et al., 2016) of the effect of different training parameters on WECs, these parameters need to be accessible explicitly, and not just on the level of file names. 2. Experiments with large numbers of WECs do not scale and are inefficient if entire files need to be loaded every time. Experiments involving large numbers of WECs are not uncommon: Baroni et al. (2014) employed 48 different WECs, while Levy et al. (2015) used as many as 672. More recently, Wendlandt et al. (2018) explored the (in)stability of word embeddings by evaluating WECs trained for all combinations of three algorithms (two of them involving a random component), five vector sizes (dimensions), and seven data sets. In order to include the effect of randomness, five sets of WECs with different initializations were trained for the two algorithms, resulting in 385 WECs altogether. Antoniak and Mimno (2018) focused on training corpora, in particular on the effect of three different sampling methods. They trained WECs for all combinations of these three methods, four algorithms, six data sets, and two"
C96-1084,C92-1023,0,0.181619,"Missing"
C96-1084,P86-1004,0,\N,Missing
C96-1084,H86-1011,0,\N,Missing
C96-1084,C94-2126,0,\N,Missing
C96-1084,E95-1033,1,\N,Missing
C96-1084,J95-2003,0,\N,Missing
C96-1084,P96-1036,1,\N,Missing
C98-2199,P87-1022,0,0.930185,"and intra-sentential anaphora. The model is the basis for an algorithm which operates incrementally, word by word. immediately. Therefore, the S-list integrates in the simplest manner preferences for inter- and intrasentential anaphora, making further specifications for processing complex sentences unnecessary. Section 2 describes the centering model as the relevant background for my proposal. In Section 3, I introduce my model, its only data structure, the S-list, and the accompanying algorithm. In Section 4, I compare the results of my algorithm with the results of the centering algorithm (Brennan et al., 1987) with and without specifications for complex sentences (Kamcyama, 1998). 1 2 Abstract Introduction I propose a model for determining the hearer&apos;s attentional state in understanding discourse. My proposal is inspired by the centering model (Grosz et al., 1983; 1995) and draws on the conclusions of Strube & Hahn&apos;s (1996) approach for the ranking of the forward-looking center list for German. Their approach has been proven as the point of departure for a new model which is valid for English as well. The use of the centering transitions in Brennan et al.&apos;s (1987) algorithm prevents it from being a"
C98-2199,P83-1007,0,0.630037,"ations for processing complex sentences unnecessary. Section 2 describes the centering model as the relevant background for my proposal. In Section 3, I introduce my model, its only data structure, the S-list, and the accompanying algorithm. In Section 4, I compare the results of my algorithm with the results of the centering algorithm (Brennan et al., 1987) with and without specifications for complex sentences (Kamcyama, 1998). 1 2 Abstract Introduction I propose a model for determining the hearer&apos;s attentional state in understanding discourse. My proposal is inspired by the centering model (Grosz et al., 1983; 1995) and draws on the conclusions of Strube & Hahn&apos;s (1996) approach for the ranking of the forward-looking center list for German. Their approach has been proven as the point of departure for a new model which is valid for English as well. The use of the centering transitions in Brennan et al.&apos;s (1987) algorithm prevents it from being applied incrementally (cf. Kehler (1997)). In my approach, I propose to replace the functions of the backward-looking center and the centering transitions by the order among the elements of the list of salient discourse entities (S-list). The S-list ranking c"
C98-2199,J95-2003,0,0.144269,"d a few rules and constraints. Each utterance Ui is assigned a list of forward-looking centers, C f (Ui), and a unique backward-looking center, Cb(Ui). A ranking imposed on the elements of the C f reflects the assumption that the most highly ranked element of C f (Ui) (the preferred center Cp(Ui)) is most likely to be the Cb(Ui+l). The most highly ranked element of Cf(Ui) that is realized in Ui+l (i.e., is associated with an expression that has a valid interpretation in the underlying semantic representation) is the Cb(Ui+l). Therefore, the ranking on the Cf plays a crucial role in the model. Grosz et al. (1995) and Brennan et al. (1987) use grammatical relations to rank the Cf(i.e., subj -~ obj -&lt; ...) but state that other factors might also play a role. For their centering algorithm, Brennan et al. (1987, henceforth BFP-algorithm) extend the notion of centering transition relations, which hold across adjacent utterances, to differentiate types of shift (cf. Table 1 taken from Walker et al. (1994)). Cb(U~) = Cp(Ud cb(u~) ¢ cp(ud Cb( U~) = Cb(U~_ ~) O R no Cb(Ui-l) Cb(Ui) ¢ CONTINUE SMOOTH-SHIFT RETAIN ROUGH-SHIFT 3 An Alternative to Centering 3.1 The Model The realization and the structure of my mod"
C98-2199,C92-1023,0,0.129924,"Missing"
C98-2199,J97-3006,0,0.536524,"ations for complex sentences (Kamcyama, 1998). 1 2 Abstract Introduction I propose a model for determining the hearer&apos;s attentional state in understanding discourse. My proposal is inspired by the centering model (Grosz et al., 1983; 1995) and draws on the conclusions of Strube & Hahn&apos;s (1996) approach for the ranking of the forward-looking center list for German. Their approach has been proven as the point of departure for a new model which is valid for English as well. The use of the centering transitions in Brennan et al.&apos;s (1987) algorithm prevents it from being applied incrementally (cf. Kehler (1997)). In my approach, I propose to replace the functions of the backward-looking center and the centering transitions by the order among the elements of the list of salient discourse entities (S-list). The S-list ranking criteria define a preference for hearer-old over hearer-new discourse entities (Prince, 1981) generalizing Strube & Hahn&apos;s (1996) approach. Because of these ranking criteria, I can account for the difference in salience between definite NPs (mostly hearer-old) and indefinite NPs (mostly hearer-new). The S-list is not a local data structure associated with individual utterances. T"
C98-2199,J94-4002,0,0.0962831,"Hemingway. Up in Michigan. In. The Complete Short Stories of Ernest Hemingway New York: Charles Scribner&apos;s Sons, 1987, p.60. The focus model (Sidner, 1983; Suri & McCoy, 1994) accounts for e v o k e d discourse entities explicitly because it uses the discourse focus, which is determined by a successful anaphora resolution. Incremental processing is not a topic of these papers. Even models which use salience measures for determining the antecedents of pronoun use the concept of e v o k e d discourse entities. Hajiaov~i et al. (1992) assign the highest value to an evoked discourse entity. Also Lappin & Leass (1994), who give the subject of the current sentence the highest weight, have an implicit notion of evokedness. The salience weight degrades from one sentence to another by a factor of two which implies that a repeatedly mentioned discourse entity gets a higher weight than a b r a n d - n e w subject. 6 Conclusions In this paper, I proposed a model for determining the heater&apos;s attentional state which is based on the distinction between hearer-old and hearer-new discourse entities. I showed that my model, though it omits the backward-looking center and the centering transitions, does not lose any of"
C98-2199,P96-1036,1,0.829112,"Missing"
C98-2199,J94-2006,0,0.286255,"nces for anaphoric antecedents. Kameyama&apos;s specifications reduce the complexity in that the Cf-lists in general are shorter after splitting up a sentence into clauses. Therefore, the Interpretation. 1256 5 Comparison to Related Approaches Kameyama&apos;s (1998) version of centering also omits the centering transitions. But she uses the Cb and a ranking over simplified transitions preventing the incremental application of her model. I°In: Ernest Hemingway. Up in Michigan. In. The Complete Short Stories of Ernest Hemingway New York: Charles Scribner&apos;s Sons, 1987, p.60. The focus model (Sidner, 1983; Suri & McCoy, 1994) accounts for e v o k e d discourse entities explicitly because it uses the discourse focus, which is determined by a successful anaphora resolution. Incremental processing is not a topic of these papers. Even models which use salience measures for determining the antecedents of pronoun use the concept of e v o k e d discourse entities. Hajiaov~i et al. (1992) assign the highest value to an evoked discourse entity. Also Lappin & Leass (1994), who give the subject of the current sentence the highest weight, have an implicit notion of evokedness. The salience weight degrades from one sentence to"
C98-2199,P89-1031,0,0.69859,"inal case, said last week S: [SMIRGAE: attorney, CASEE: case, CURT1SE; him, CS COURT(j: CS Court, JUDGEE: judge ] that he had doubts about the psychiatric reports that said Mr. Curtis would never improve. S: [SMIRGAE: he, CASEE: case, REPORTSE: reports, CURTISE: Mr. Curtis, DOUBTSBN: doubts] Table 5: Analysis for (3) 4 Some Empirical Data In the first experiment, I compare my algorithm with the BFP-algorithm which was in a second experiment extended by the constraints for complex sentences as described by Kameyama (1998). Method. I use the following guidelines for the hand-simulated analysis (Walker, 1989). I do not assume any world knowledge as part of the anaphora resolution process. Only agreement criteria, binding and sortal constraints are applied. I do not account for false positives and error chains. Following Walker (1989), a segment is defined as a paragraph unless its first sentence has a pronoun in subject position or a pronoun where none of the preceding sentence-internal noun phrases matches its syntactic features. At the beginning of a segment, anaphora resolution is preferentially performed within the same utterance. My algorithm starts with an empty S-list at the beginning of a"
C98-2199,J94-2003,0,0.177934,"n Ui+l (i.e., is associated with an expression that has a valid interpretation in the underlying semantic representation) is the Cb(Ui+l). Therefore, the ranking on the Cf plays a crucial role in the model. Grosz et al. (1995) and Brennan et al. (1987) use grammatical relations to rank the Cf(i.e., subj -~ obj -&lt; ...) but state that other factors might also play a role. For their centering algorithm, Brennan et al. (1987, henceforth BFP-algorithm) extend the notion of centering transition relations, which hold across adjacent utterances, to differentiate types of shift (cf. Table 1 taken from Walker et al. (1994)). Cb(U~) = Cp(Ud cb(u~) ¢ cp(ud Cb( U~) = Cb(U~_ ~) O R no Cb(Ui-l) Cb(Ui) ¢ CONTINUE SMOOTH-SHIFT RETAIN ROUGH-SHIFT 3 An Alternative to Centering 3.1 The Model The realization and the structure of my model departs significantly from the centering model: • The model consists of one construct with one operation: the list of salient discourse entities (S-list) with an insertion operation. • The S-list describes the attentional state of the hearer at any given point in processing a discourse. Cb(Ui-1) Table 1: Transition Types Brennan et al. (1987) modify the second of two rules on center movem"
D08-1019,J05-3002,0,0.645174,"he same person should be merged into a single one. An example of a fused sentence (3) with the source sentences (1,2) is given below: We present a novel unsupervised sentence fusion method which we apply to a corpus of biographies in German. Given a group of related sentences, we align their dependency trees and build a dependency graph. Using integer linear programming we compress this graph to a new tree, which we then linearize. We use GermaNet and Wikipedia for checking semantic compatibility of co-arguments. In an evaluation with human judges our method outperforms the fusion approach of Barzilay & McKeown (2005) with respect to readability. 1 (1) Bohr studierte an der Universit¨at Kopenhagen Bohr studied at the University Copenhagen und erlangte dort seine Doktorw¨urde. there his PhD and got Introduction Automatic text summarization is a rapidly developing field in computational linguistics. Summarization systems can be classified as either extractive or abstractive ones (Sp¨arck Jones, 1999). To date, most systems are extractive: sentences are selected from one or several documents and then ordered. This method exhibits problems, because input sentences very often overlap and complement each other a"
D08-1019,A00-1031,0,0.0162719,"s. 3 Data The comparable corpus we work with is a collection of about 400 biographies in German gathered from the Internet2 . These biographies describe 140 different people, and the number of articles for one person ranges from 2 to 4, being 3 on average. Despite obvious similarities between articles about one person, neither identical content nor identical ordering of information can be expected. Fully automatic preprocessing in our system comprises the following steps: sentence boundaries are identified with a Perl CPAN module3 . Then the sentences are split into tokens and the TnT tagger (Brants, 2000) and the TreeTagger (Schmid, 1997) are used for tagging and lemmatization respectively. Finally, the biographies are parsed with the CDG dependency parser (Foth & Menzel, 2006). We also identify references to the biographee (pronominal as well as proper names) and temporal expressions (absolute and relative) with a few rules. 4 Our Method Groups of related sentences serve as input to a sentence fusion system and thus need to be identified first (4.1). Then the dependency trees of the sentences are modified (4.2) and aligned (4.3). Syntactic importance (4.4) and word informativeness (4.5) score"
D08-1019,P07-1041,1,0.522498,"f both the labels from h to w and from h to u are preserved. 182 The “overgenerate-and-rank” approach to statistical surface realization is very common (Langkilde & Knight, 1998). Unfortunately, in its simplest and most popular version, it ignores syntactical constraints and may produce ungrammatical output. For example, an inviolable rule of German grammar states that the finite verb must be in the second position in the main clause. Since it is hard to enforce such rules with an ngram language model, syntax-informed linearization methods have been developed for German (Ringger et al., 2004; Filippova & Strube, 2007). We apply our recent method to order constituents and, using the CMU toolkit (Clarkson & Rosenfeld, 1997), build a trigram language model from Wikipedia (approx. 1GB plain text) to find the best word order within constituents. Some constraints on word order are inferred from the input. Only interclause punctuation is generated. 5 Experiments and Evaluation We choose Barzilay & McKeown’s system as a nontrivial baseline since, to our knowledge, there is no other system which outperforms theirs (Sec. 5.1). It is important for us to evaluate the fusion part of our system, so the input and the lin"
D08-1019,W08-1105,1,0.906621,"nd-crafted rules, but decide to retain a dependency based on its syntactic importance score. The second point concerns integrating semantics. Being definitely important, ”this source of information remains relatively unused in work on aggregation1 within NLG” (Reiter & Dale, 2000, p.141). To our knowledge, in the text-to-text generation field, we are the first to use semantic information not only for alignment but also for aggregation in that we check coarguments’ compatibility. Apart from that, our method is not limited to sentence fusion and can be easily applied to sentence compression. In Filippova & Strube (2008) we compress English sentences with the same approach and achieve state-of-the-art performance. The paper is organized as follows: Section 2 gives an overview of related work and Section 3 presents our data. Section 4 introduces our method and Section 5 describes the experiments and discusses the results of the evaluation. The conclusions follow in the final section. 2 Related Work Most studies on text-to-text generation concern sentence compression where the input consists of exactly one sentence (Jing, 2001; Hori & Furui, 2004; Clarke & Lapata, 2008, inter alia). In such setting, redundancy,"
D08-1019,P06-1041,0,0.0147803,"ople, and the number of articles for one person ranges from 2 to 4, being 3 on average. Despite obvious similarities between articles about one person, neither identical content nor identical ordering of information can be expected. Fully automatic preprocessing in our system comprises the following steps: sentence boundaries are identified with a Perl CPAN module3 . Then the sentences are split into tokens and the TnT tagger (Brants, 2000) and the TreeTagger (Schmid, 1997) are used for tagging and lemmatization respectively. Finally, the biographies are parsed with the CDG dependency parser (Foth & Menzel, 2006). We also identify references to the biographee (pronominal as well as proper names) and temporal expressions (absolute and relative) with a few rules. 4 Our Method Groups of related sentences serve as input to a sentence fusion system and thus need to be identified first (4.1). Then the dependency trees of the sentences are modified (4.2) and aligned (4.3). Syntactic importance (4.4) and word informativeness (4.5) scores are used to extract a new dependency tree from a graph of aligned trees (4.6). Finally, the tree is linearized (4.7). 4.1 Sentence Alignment Sentence alignment for comparable"
D08-1019,P05-3002,0,0.015849,"flattened to facilitate alignment. A locally optimal pairwise alignment of modified dependency trees is recursively found with WordNet and a paraphrase lexicon. From the alignment costs the centroid of the group is identified. Then this tree is augmented with information from other trees given that it appears in at least half of the sentences from this group. A rule-based pruning module prunes optional constituents, such as PPs or relative clauses. The linearization of the resulting tree (or graph) is done with a trigram language model. To adapt this system to German, we use the GermaNet API (Gurevych & Niederlich, 2005) instead of WordNet. We do not use a paraphrase lexicon, because there is no comparable corpus of sufficient size available for German. We readjust the alignment parameters of the system to prevent dissimilar nodes from being aligned. The input to the algorithm is generated as described in Sec. 4.1. The linearization is done as described in Sec. 4.7. In cases when there is a graph to linearize, all possible trees covering the maximum number of nodes are extracted from it and linearized. The most probable string is selected as the final output with a language model. For the rest of the reimplem"
D08-1019,P08-2049,0,0.504296,"ut decide to retain a dependency based on its syntactic importance score. The second point concerns integrating semantics. Being definitely important, ”this source of information remains relatively unused in work on aggregation1 within NLG” (Reiter & Dale, 2000, p.141). To our knowledge, in the text-to-text generation field, we are the first to use semantic information not only for alignment but also for aggregation in that we check coarguments’ compatibility. Apart from that, our method is not limited to sentence fusion and can be easily applied to sentence compression. In Filippova & Strube (2008) we compress English sentences with the same approach and achieve state-of-the-art performance. The paper is organized as follows: Section 2 gives an overview of related work and Section 3 presents our data. Section 4 introduces our method and Section 5 describes the experiments and discusses the results of the evaluation. The conclusions follow in the final section. 2 Related Work Most studies on text-to-text generation concern sentence compression where the input consists of exactly one sentence (Jing, 2001; Hori & Furui, 2004; Clarke & Lapata, 2008, inter alia). In such setting, redundancy,"
D08-1019,P98-1116,0,0.0335958,"rding to GermaNet, physics (Physik) is a hyponym of science (Wissenschaft). (12) blocks taking both pleasure (Freude) and Bohr because rel(Freude,Bohr) = 0.17. math and physics are neither in ISA, nor part-of relation and are sufficiently related (rel(Mathematik, Physik) = 0.67) to become conjuncts. META constraints (equations (13) and (14)) guarl antee that yw,u = xlh,w × xlh,u i.e. they ensure that the semantic constraints are applied only if both the labels from h to w and from h to u are preserved. 182 The “overgenerate-and-rank” approach to statistical surface realization is very common (Langkilde & Knight, 1998). Unfortunately, in its simplest and most popular version, it ignores syntactical constraints and may produce ungrammatical output. For example, an inviolable rule of German grammar states that the finite verb must be in the second position in the main clause. Since it is hard to enforce such rules with an ngram language model, syntax-informed linearization methods have been developed for German (Ringger et al., 2004; Filippova & Strube, 2007). We apply our recent method to order constituents and, using the CMU toolkit (Clarkson & Rosenfeld, 1997), build a trigram language model from Wikipedia"
D08-1019,kunze-lemnitzer-2002-germanet,0,0.00747476,"org/∼holsten/ Lingua-DE-Sentence-0.07/Sentence.pm 179 demonstrate the efficacy of a sentence-based tf*idf score when applied to comparable corpora. Following them, we define the similarity of two sentences sim(s1 , s2 ) as P wS (t) · wS (t) S1 · S2 = qPt 1 P 2 |S1 |· |S2 | w2 (t) w2 (t) t S1 t (1) S2 where S is the set of all lemmas but stop-words from s, and wS (t) is the weight of the term t: 1 (2) Nt where S(t) is the indicator function of S, Nt is the number of sentences in the biographies of one person which contain t. We enhance the similarity measure by looking up synonymy in GermaNet (Lemnitzer & Kunze, 2002). We discard identical or nearly identical sentences (sim(s1 , s2 ) &gt; 0.8) and greedily build sentence clusters using a hierarchical groupwiseaverage technique. As a result, one sentence may belong to one cluster at most. These sentence clusters serve as input to the fusion algorithm. wS (t) = S(t) 4.2 Dependency Tree Modification We apply a set of transformations to a dependency tree to emphasize its important properties and eliminate unimportant ones. These transformations are necessary for the compression stage. An example of a dependency tree and its modifed version are given in Fig. 1. pr"
D08-1019,W05-1612,0,0.456322,"to sentence fusion. Barzilay & McKeown (2005) present a sentence fusion method for multi-document news summarization which crucially relies on the assumption that information appearing in many sources is important. Consequently, their method produces an intersection of input sentences by, first, finding the centroid of the input, second, augmenting it with information from other sentences and, finally, pruning a predefined set of constituents (e.g. PPs). The resulting structure is not necessarily a tree and allows for extraction of several trees, each of which can be linearized in many ways. Marsi & Krahmer (2005) extend the approach of Barzilay & McKeown to do not only intersection but also union fusion. Like Barzilay & McKeown (2005), they find the best linearization with a language model which, as they point out, often produces inadequate rankings being unable to deal with word order, agreement and subcategorization constraints. In our work we aim at producing a valid dependency tree structure so that most grammaticality issues are resolved before the linearization stage. Wan et al. (2007) introduce a global revision method of how a novel sentence can be generated from a set of input words. They for"
D08-1019,E06-1021,0,0.0384916,"entence Alignment Sentence alignment for comparable corpora requires methods different from those used in machine translation for parallel corpora. For example, given two biographies of a person, one of them may follow the timeline from birth to death whereas the other may group events thematically or tell only about the scientific contribution of the person. Thus one cannot assume that the sentence order or the content is the same in two biographies. Shallow methods like word or bigram overlap, (weighted) cosine or Jaccard similarity are appealing as they are cheap and robust. In particular, Nelken & Schieber (2006) 2 http://de.wikipedia.org, http://home. datacomm.ch/biografien, http://biographie. net/de, http://www.weltchronik.de/ws/bio/ main.htm, http://www.brockhaus-suche.de/ suche 3 http://search.cpan.org/∼holsten/ Lingua-DE-Sentence-0.07/Sentence.pm 179 demonstrate the efficacy of a sentence-based tf*idf score when applied to comparable corpora. Following them, we define the similarity of two sentences sim(s1 , s2 ) as P wS (t) · wS (t) S1 · S2 = qPt 1 P 2 |S1 |· |S2 | w2 (t) w2 (t) t S1 t (1) S2 where S is the set of all lemmas but stop-words from s, and wS (t) is the weight of the term t: 1 (2) Nt"
D08-1019,kassner-etal-2008-acquiring,1,\N,Missing
D08-1019,C04-1097,0,\N,Missing
D08-1019,C98-1112,0,\N,Missing
D09-1095,S07-1007,0,0.739036,"proach suffers from data sparseness. To address this problem, Nissim and Markert (2003) proposed a word similarity-based method. They use Lin’s thesaurus (Lin, 1998) to determine how close two lexical heads are, and use this instead of the more restrictive identity constraint when comparing two instances. This technique is complex, requiring smoothing, multiple iterations over the thesaurus and hybrid methods to allow a back-off to grammatical roles. The supervised approach to resolving metonymies was encouraged by the metonymy resolution task at the semantic evaluation exercise SemEval 2007 (Markert and Nissim, 2007). The participating systems in this task were varied. Most of them (four out of five) have used supervised machine learning techniques. The systems that beat the baseline used either the grammatical annotations provided by the organizers (Farkas et al., 2007; Nicolae et al., 2007), or a robust and deep (not freely available) parser (Brun et al., 2007). These systems represented instances in a manner similar to (Nissim and Markert, 2005). They used additional manually built resources – WordNet, FrameNet, Levin’s verb classes, manually built lists of “trigger” words – to generalize the existing"
D09-1095,S07-1109,0,0.196903,"ple iterations over the thesaurus and hybrid methods to allow a back-off to grammatical roles. The supervised approach to resolving metonymies was encouraged by the metonymy resolution task at the semantic evaluation exercise SemEval 2007 (Markert and Nissim, 2007). The participating systems in this task were varied. Most of them (four out of five) have used supervised machine learning techniques. The systems that beat the baseline used either the grammatical annotations provided by the organizers (Farkas et al., 2007; Nicolae et al., 2007), or a robust and deep (not freely available) parser (Brun et al., 2007). These systems represented instances in a manner similar to (Nissim and Markert, 2005). They used additional manually built resources – WordNet, FrameNet, Levin’s verb classes, manually built lists of “trigger” words – to generalize the existing features. Brun et al. (2007) also used the British National Corpus (BNC) for computing the distance between words based on their syntactic distribution. While lexical resources and corpora are used to estimate word similarity, all these systems rely exclusively on the data provided by the organizers – instance representation captures only information"
D09-1095,de-marneffe-etal-2006-generating,0,0.0127061,"Missing"
D09-1095,I08-2105,1,0.926103,"ing a PMW towards each of PMW’s possible readings. The technique employed is adapted from unsupervised word sense disambiguation (WSD). In short, we use the local grammatical context as it is commonly used in WSD approaches, to guide the system in choosing the reading that fits best. The benefits of using grammatical information for automatic WSD were first explored by Yarowsky (1995) and Resnik (1996) in unsupervised approaches to disambiguating single words in context. The method described here uses automatically induced selectional preferences, computed from sense-untagged data, similar to Nastase (2008). 3 Data We work with the data from the metonymy resolution task at SemEval 2007 (Markert and Nissim, 2007), generated based on a scheme developed by Markert and Nissim (2003). The metonymy resolution task at SemEval 2007 consisted of two subtasks – one for resolving country names, the other for companies. For each subtask there is a training and a test portion. Figure 1 shows the text fragment for one sample, and Table 1 the data statistics. The reading column shows the possible interpretations of a PMW for countries and companies respectively. For example, org-for-product would be the interp"
D09-1095,S07-1033,0,0.544548,"y constraint when comparing two instances. This technique is complex, requiring smoothing, multiple iterations over the thesaurus and hybrid methods to allow a back-off to grammatical roles. The supervised approach to resolving metonymies was encouraged by the metonymy resolution task at the semantic evaluation exercise SemEval 2007 (Markert and Nissim, 2007). The participating systems in this task were varied. Most of them (four out of five) have used supervised machine learning techniques. The systems that beat the baseline used either the grammatical annotations provided by the organizers (Farkas et al., 2007; Nicolae et al., 2007), or a robust and deep (not freely available) parser (Brun et al., 2007). These systems represented instances in a manner similar to (Nissim and Markert, 2005). They used additional manually built resources – WordNet, FrameNet, Levin’s verb classes, manually built lists of “trigger” words – to generalize the existing features. Brun et al. (2007) also used the British National Corpus (BNC) for computing the distance between words based on their syntactic distribution. While lexical resources and corpora are used to estimate word similarity, all these systems rely exclusiv"
D09-1095,S07-1101,0,0.577899,"stems in this task were varied. Most of them (four out of five) have used supervised machine learning techniques. The systems that beat the baseline used either the grammatical annotations provided by the organizers (Farkas et al., 2007; Nicolae et al., 2007), or a robust and deep (not freely available) parser (Brun et al., 2007). These systems represented instances in a manner similar to (Nissim and Markert, 2005). They used additional manually built resources – WordNet, FrameNet, Levin’s verb classes, manually built lists of “trigger” words – to generalize the existing features. Brun et al. (2007) also used the British National Corpus (BNC) for computing the distance between words based on their syntactic distribution. While lexical resources and corpora are used to estimate word similarity, all these systems rely exclusively on the data provided by the organizers – instance representation captures only information that can be derived from or between the data points provided. The approach presented here goes beyond the given data, and induces from corpora measures that allow the system to determine that appear as this verb’s subjects, and estimate from this the preferences excel has fo"
D09-1095,J91-1003,0,0.834572,"the 1930s (Stern, 1931), and are increasingly recognized as an important phenomenon to tackle in the interest of higher level language processing tasks, such as anaphora resolution (Harabagiu, 1998; Markert and Hahn, 2002), question answering (Stallard, 1993) or machine translation (Kamei and Wakao, 1992). Until the early 90s, the main view about metonymies was that they violate semantic constraints in their immediate context. To resolve metonymies then amounts to detecting violated constraints, usually from those imposed by the verbs on their arguments (Pustejovsky, 1991; Hobbs et al., 1993; Fass, 1991). Markert and Hahn (2002) showed that this approach misses metonymies which do not violate selectional restrictions. In this case referential cohesion relations may indicate that the literal reading is not appropriate and give clues about the intended metonymic interpretation. Markert and Nissim (2003) have combined observations from the linguistic analysis of metonymies with results of corpus studies. Linguistic research has postulated that (i) conven911 what are the preferences of the words surrounding a PMW towards each of PMW’s possible readings. The technique employed is adapted from unsu"
D09-1095,P03-1008,0,0.923863,"ontext, and are applied to the information/knowledge evoked by w. The local constraints come from the words with which w is (grammatically) related to. The global constraints come from the domain/topic of the text, discourse relations that span across sentences. Metonymic words have a rather small number of possible interpretations (also called readings) which occur frequently (Markert and Nissim, 2002). Idiosyncratic interpretations are also possible, but very rare. One can view the possible interpretations of a potentially metonymic word (PMW) as corresponding to the word’s possible senses (Nissim and Markert, 2003), bringing the task close to word sense disambiguation. The approach to metonymy resolution presented here is supervised, with unsupervised feature enrichment. We apply techniques inspired by unsupervised word sense disambiguation, which allow us to go beyond the annotated data provided in training, and quantify the restrictions imposed on the interpretation of a PMW by its grammatically related neighbours through collocation information extracted from corpora. The only annotation required for the corpora are automatically induced part-of-speech tags from which we obtain grammatical relations"
D09-1095,W98-0720,0,0.828656,"this context – organization-for-people or organization-for-product. The paper continues with related work in Section 2 and the description of the data in Section 3. The representation used is introduced in Section 4. The results and the discussion are presented in Section 5. The paper wraps up with conclusions and future work. 2 Related Work Analysis of metonymies as a linguistic phenomenon dates back at least to the 1930s (Stern, 1931), and are increasingly recognized as an important phenomenon to tackle in the interest of higher level language processing tasks, such as anaphora resolution (Harabagiu, 1998; Markert and Hahn, 2002), question answering (Stallard, 1993) or machine translation (Kamei and Wakao, 1992). Until the early 90s, the main view about metonymies was that they violate semantic constraints in their immediate context. To resolve metonymies then amounts to detecting violated constraints, usually from those imposed by the verbs on their arguments (Pustejovsky, 1991; Hobbs et al., 1993; Fass, 1991). Markert and Hahn (2002) showed that this approach misses metonymies which do not violate selectional restrictions. In this case referential cohesion relations may indicate that the lit"
D09-1095,P92-1047,0,0.829483,"work in Section 2 and the description of the data in Section 3. The representation used is introduced in Section 4. The results and the discussion are presented in Section 5. The paper wraps up with conclusions and future work. 2 Related Work Analysis of metonymies as a linguistic phenomenon dates back at least to the 1930s (Stern, 1931), and are increasingly recognized as an important phenomenon to tackle in the interest of higher level language processing tasks, such as anaphora resolution (Harabagiu, 1998; Markert and Hahn, 2002), question answering (Stallard, 1993) or machine translation (Kamei and Wakao, 1992). Until the early 90s, the main view about metonymies was that they violate semantic constraints in their immediate context. To resolve metonymies then amounts to detecting violated constraints, usually from those imposed by the verbs on their arguments (Pustejovsky, 1991; Hobbs et al., 1993; Fass, 1991). Markert and Hahn (2002) showed that this approach misses metonymies which do not violate selectional restrictions. In this case referential cohesion relations may indicate that the literal reading is not appropriate and give clues about the intended metonymic interpretation. Markert and Nissi"
D09-1095,J91-4003,0,0.170387,"stic phenomenon dates back at least to the 1930s (Stern, 1931), and are increasingly recognized as an important phenomenon to tackle in the interest of higher level language processing tasks, such as anaphora resolution (Harabagiu, 1998; Markert and Hahn, 2002), question answering (Stallard, 1993) or machine translation (Kamei and Wakao, 1992). Until the early 90s, the main view about metonymies was that they violate semantic constraints in their immediate context. To resolve metonymies then amounts to detecting violated constraints, usually from those imposed by the verbs on their arguments (Pustejovsky, 1991; Hobbs et al., 1993; Fass, 1991). Markert and Hahn (2002) showed that this approach misses metonymies which do not violate selectional restrictions. In this case referential cohesion relations may indicate that the literal reading is not appropriate and give clues about the intended metonymic interpretation. Markert and Nissim (2003) have combined observations from the linguistic analysis of metonymies with results of corpus studies. Linguistic research has postulated that (i) conven911 what are the preferences of the words surrounding a PMW towards each of PMW’s possible readings. The techni"
D09-1095,P93-1012,0,0.853879,"oduct. The paper continues with related work in Section 2 and the description of the data in Section 3. The representation used is introduced in Section 4. The results and the discussion are presented in Section 5. The paper wraps up with conclusions and future work. 2 Related Work Analysis of metonymies as a linguistic phenomenon dates back at least to the 1930s (Stern, 1931), and are increasingly recognized as an important phenomenon to tackle in the interest of higher level language processing tasks, such as anaphora resolution (Harabagiu, 1998; Markert and Hahn, 2002), question answering (Stallard, 1993) or machine translation (Kamei and Wakao, 1992). Until the early 90s, the main view about metonymies was that they violate semantic constraints in their immediate context. To resolve metonymies then amounts to detecting violated constraints, usually from those imposed by the verbs on their arguments (Pustejovsky, 1991; Hobbs et al., 1993; Fass, 1991). Markert and Hahn (2002) showed that this approach misses metonymies which do not violate selectional restrictions. In this case referential cohesion relations may indicate that the literal reading is not appropriate and give clues about the inten"
D09-1095,W02-1027,0,0.87833,"stract parts, the cockpit and front seat are some of them, and this provides the discourse links between the two sentences. Constraints on the interpretation of a word w in context comes both from the local and global context, and are applied to the information/knowledge evoked by w. The local constraints come from the words with which w is (grammatically) related to. The global constraints come from the domain/topic of the text, discourse relations that span across sentences. Metonymic words have a rather small number of possible interpretations (also called readings) which occur frequently (Markert and Nissim, 2002). Idiosyncratic interpretations are also possible, but very rare. One can view the possible interpretations of a potentially metonymic word (PMW) as corresponding to the word’s possible senses (Nissim and Markert, 2003), bringing the task close to word sense disambiguation. The approach to metonymy resolution presented here is supervised, with unsupervised feature enrichment. We apply techniques inspired by unsupervised word sense disambiguation, which allow us to go beyond the annotated data provided in training, and quantify the restrictions imposed on the interpretation of a PMW by its gram"
D09-1095,P95-1026,0,0.255858,"Markert and Nissim (2003) have combined observations from the linguistic analysis of metonymies with results of corpus studies. Linguistic research has postulated that (i) conven911 what are the preferences of the words surrounding a PMW towards each of PMW’s possible readings. The technique employed is adapted from unsupervised word sense disambiguation (WSD). In short, we use the local grammatical context as it is commonly used in WSD approaches, to guide the system in choosing the reading that fits best. The benefits of using grammatical information for automatic WSD were first explored by Yarowsky (1995) and Resnik (1996) in unsupervised approaches to disambiguating single words in context. The method described here uses automatically induced selectional preferences, computed from sense-untagged data, similar to Nastase (2008). 3 Data We work with the data from the metonymy resolution task at SemEval 2007 (Markert and Nissim, 2007), generated based on a scheme developed by Markert and Nissim (2003). The metonymy resolution task at SemEval 2007 consisted of two subtasks – one for resolving country names, the other for companies. For each subtask there is a training and a test portion. Figure 1"
D12-1017,S07-1109,0,0.0948328,"t of older work in metonymy resolution such as Hobbs et al. (1993), Fass (1991), Markert and Hahn (2002) or the use of a generative lexicon and its relations in Pustejovsky (1991), which also are unsupervised. However, these approaches lacked scalability due to the use of small hand-modeled knowledge bases which our use of a very large Wikipedia-derived ontology overcomes. In addition, most of these approaches (Fass, 1991; Hobbs et al., 1993; Pustejovsky, 1991; Harabagiu, 1998) rely on the view that metonymies violate selectional restrictions in their immediate, local context, usually those 1 Brun et al. (2007) is semi-supervised but again relies on the local grammatical context. 185 imposed by the verbs on their arguments. As can be seen in the Example 2, this misses metonymies which do not violate selectional restrictions. Nastase and Strube (2009) use more flexible probabilistic selectional preferences instead of strict constraint violations as well as WordNet as a larger taxonomy but are also restricted to the local context. Markert and Hahn (2002) do propose a treatment of metonymies that takes into account the larger discourse in the form of anaphoric relations between a metonymy and the prior"
D12-1017,S07-1033,0,0.279243,"ntext, dependent on the semantic class studied and (ii) that an unsupervised approach — although lower than the supervised one — outperforms the supervised most frequent reading baseline and performs close to a standard supervised model with the basic set of lexico-syntactic features (Nissim and Markert, 2005). 2 Related Work The word sense disambiguation setting for metonymy resolution as developed by Nissim and Markert (2005) and used for the SemEval 2007 task (Markert and Nissim, 2009) uses a small, prespecified number of frequently occurring readings. The approaches building on this work (Farkas et al., 2007; Nicolae et al., 2007, among others) are supervised, mostly using shallow surface features as well as grammatical relations.1 Most effective in the SemEval task as summarized in Markert and Nissim (2009) has been the local, grammatical context, with the two systems relying on the global context or the local/global context in a BOW model (Leveling, 2007; Poibeau, 2007) not outperforming the most frequent reading baseline. We believe that might be due to the lack of a link between the local and global context in these approaches — in our work, we condition the global context on the abstractions"
D12-1017,J91-1003,0,0.806461,"ation in that the flexibility of our framework allows us to incorporate a wider context than in most prior approaches. Let us consider the indications for metonymic readings and its interpretation in Example 1, on the one hand, and Example 2, on the other hand. In Example 1, the grammatical relation to the verb defeat and the verb’s selectional preferences indicate the metonymy. We will call all such grammatically related words and the grammatical relations the local context of the PMW. Such types of local context have been used by most prior approaches (Pustejovsky, 1991; Hobbs et al., 1993; Fass, 1991; Nastase and Strube, 2009, among others). However, Example 2 shows that the local context can be ambiguous or often weak, such as the verb to be. In these examples, the wider context (database, key184 word) is a better indication for a metonymy but has not been satisfactorily integrated in prior approaches (see Section 2). We here call all words surrounding the PMW but not grammatically related to it the global context. In our approach we integrate both the local and the global context in our probabilistic framework. For the local context, we compute the selectional preferences for the words"
D12-1017,W98-0720,0,0.49228,"s limited to interpretation. Our view of relations in a concept network being the interpretations of metonymies is strongly reminiscent of older work in metonymy resolution such as Hobbs et al. (1993), Fass (1991), Markert and Hahn (2002) or the use of a generative lexicon and its relations in Pustejovsky (1991), which also are unsupervised. However, these approaches lacked scalability due to the use of small hand-modeled knowledge bases which our use of a very large Wikipedia-derived ontology overcomes. In addition, most of these approaches (Fass, 1991; Hobbs et al., 1993; Pustejovsky, 1991; Harabagiu, 1998) rely on the view that metonymies violate selectional restrictions in their immediate, local context, usually those 1 Brun et al. (2007) is semi-supervised but again relies on the local grammatical context. 185 imposed by the verbs on their arguments. As can be seen in the Example 2, this misses metonymies which do not violate selectional restrictions. Nastase and Strube (2009) use more flexible probabilistic selectional preferences instead of strict constraint violations as well as WordNet as a larger taxonomy but are also restricted to the local context. Markert and Hahn (2002) do propose a"
D12-1017,P03-1069,0,0.0257898,"tures as well as grammatical relations.1 Most effective in the SemEval task as summarized in Markert and Nissim (2009) has been the local, grammatical context, with the two systems relying on the global context or the local/global context in a BOW model (Leveling, 2007; Poibeau, 2007) not outperforming the most frequent reading baseline. We believe that might be due to the lack of a link between the local and global context in these approaches — in our work, we condition the global context on the abstractions and selectional preferences yielded by the local context and achieve better results. Lapata (2003), Shutova (2009) as well as Roberts and Harabagiu (2011) deal with the issue of logical metonymy, where the participant stands in for the full event: e.g. Mary enjoyed the book., where book stands in for reading the book, and this missing event (reading) can be inferred from a corpus. Utiyama et al. (2000), Lapata (2003) propose a probabilistic model for finding the correct interpretation of such metonymies in an unsupervised manner. However, these event type metonymies differ from the problem dealt with in our paper and the SemEval 2007 task in that their recognition (i.e. their distinction f"
D12-1017,S07-1031,0,0.309658,"tting for metonymy resolution as developed by Nissim and Markert (2005) and used for the SemEval 2007 task (Markert and Nissim, 2009) uses a small, prespecified number of frequently occurring readings. The approaches building on this work (Farkas et al., 2007; Nicolae et al., 2007, among others) are supervised, mostly using shallow surface features as well as grammatical relations.1 Most effective in the SemEval task as summarized in Markert and Nissim (2009) has been the local, grammatical context, with the two systems relying on the global context or the local/global context in a BOW model (Leveling, 2007; Poibeau, 2007) not outperforming the most frequent reading baseline. We believe that might be due to the lack of a link between the local and global context in these approaches — in our work, we condition the global context on the abstractions and selectional preferences yielded by the local context and achieve better results. Lapata (2003), Shutova (2009) as well as Roberts and Harabagiu (2011) deal with the issue of logical metonymy, where the participant stands in for the full event: e.g. Mary enjoyed the book., where book stands in for reading the book, and this missing event (reading) c"
D12-1017,J98-2002,0,0.0128516,"ed with a parallelized version of Ensemble4 (Surdeanu and Manning, 2010), and we extracted G, the set of all grammatical relations of the type (verb, dependency, hyperlink) and (adjective, dependency, hyperlink), with the hyperlinks resolved to their corresponding node (concept) in the network ( |G |= 1,578,413 triples). For each verb and adjective in the extracted collocations, and for each of their dependency relations, their collocates were generalized in the network defined by the hypernym/hyponym relations in WikiNet following a method similar to the Minimum Description Length principle (Li and Abe, 1998). Essentially, we aimed to determine a small set of (more general) concepts that describe the set of collocates for a word w and grammatical relation r. Starting from the concept collocates gathered, we go upwards following WikiNet’s is a links, and for each node found that covers at least N concept collocates (N is a parameter, N=2 in the experiments presented here), the MDL score of the node is computed (Algorithm 2). We place a limit M on the number of upward steps in the hierarchy (M =3 in our experiments). The disjoint set of nodes that has the lowest overall MDL score is chosen (Γ), and"
D12-1017,D09-1095,1,0.884177,"t the flexibility of our framework allows us to incorporate a wider context than in most prior approaches. Let us consider the indications for metonymic readings and its interpretation in Example 1, on the one hand, and Example 2, on the other hand. In Example 1, the grammatical relation to the verb defeat and the verb’s selectional preferences indicate the metonymy. We will call all such grammatically related words and the grammatical relations the local context of the PMW. Such types of local context have been used by most prior approaches (Pustejovsky, 1991; Hobbs et al., 1993; Fass, 1991; Nastase and Strube, 2009, among others). However, Example 2 shows that the local context can be ambiguous or often weak, such as the verb to be. In these examples, the wider context (database, key184 word) is a better indication for a metonymy but has not been satisfactorily integrated in prior approaches (see Section 2). We here call all words surrounding the PMW but not grammatically related to it the global context. In our approach we integrate both the local and the global context in our probabilistic framework. For the local context, we compute the selectional preferences for the words related to the PMW from a"
D12-1017,nastase-etal-2010-wikinet,1,0.885388,"Missing"
D12-1017,S07-1101,0,0.312209,"ork in metonymy resolution such as Hobbs et al. (1993), Fass (1991), Markert and Hahn (2002) or the use of a generative lexicon and its relations in Pustejovsky (1991), which also are unsupervised. However, these approaches lacked scalability due to the use of small hand-modeled knowledge bases which our use of a very large Wikipedia-derived ontology overcomes. In addition, most of these approaches (Fass, 1991; Hobbs et al., 1993; Pustejovsky, 1991; Harabagiu, 1998) rely on the view that metonymies violate selectional restrictions in their immediate, local context, usually those 1 Brun et al. (2007) is semi-supervised but again relies on the local grammatical context. 185 imposed by the verbs on their arguments. As can be seen in the Example 2, this misses metonymies which do not violate selectional restrictions. Nastase and Strube (2009) use more flexible probabilistic selectional preferences instead of strict constraint violations as well as WordNet as a larger taxonomy but are also restricted to the local context. Markert and Hahn (2002) do propose a treatment of metonymies that takes into account the larger discourse in the form of anaphoric relations between a metonymy and the prior"
D12-1017,S07-1093,0,0.266348,"my resolution as developed by Nissim and Markert (2005) and used for the SemEval 2007 task (Markert and Nissim, 2009) uses a small, prespecified number of frequently occurring readings. The approaches building on this work (Farkas et al., 2007; Nicolae et al., 2007, among others) are supervised, mostly using shallow surface features as well as grammatical relations.1 Most effective in the SemEval task as summarized in Markert and Nissim (2009) has been the local, grammatical context, with the two systems relying on the global context or the local/global context in a BOW model (Leveling, 2007; Poibeau, 2007) not outperforming the most frequent reading baseline. We believe that might be due to the lack of a link between the local and global context in these approaches — in our work, we condition the global context on the abstractions and selectional preferences yielded by the local context and achieve better results. Lapata (2003), Shutova (2009) as well as Roberts and Harabagiu (2011) deal with the issue of logical metonymy, where the participant stands in for the full event: e.g. Mary enjoyed the book., where book stands in for reading the book, and this missing event (reading) can be inferred f"
D12-1017,J91-4003,0,0.727503,"anking showcases our second major innovation in that the flexibility of our framework allows us to incorporate a wider context than in most prior approaches. Let us consider the indications for metonymic readings and its interpretation in Example 1, on the one hand, and Example 2, on the other hand. In Example 1, the grammatical relation to the verb defeat and the verb’s selectional preferences indicate the metonymy. We will call all such grammatically related words and the grammatical relations the local context of the PMW. Such types of local context have been used by most prior approaches (Pustejovsky, 1991; Hobbs et al., 1993; Fass, 1991; Nastase and Strube, 2009, among others). However, Example 2 shows that the local context can be ambiguous or often weak, such as the verb to be. In these examples, the wider context (database, key184 word) is a better indication for a metonymy but has not been satisfactorily integrated in prior approaches (see Section 2). We here call all words surrounding the PMW but not grammatically related to it the global context. In our approach we integrate both the local and the global context in our probabilistic framework. For the local context, we compute the select"
D12-1017,D11-1091,0,0.0239153,"Most effective in the SemEval task as summarized in Markert and Nissim (2009) has been the local, grammatical context, with the two systems relying on the global context or the local/global context in a BOW model (Leveling, 2007; Poibeau, 2007) not outperforming the most frequent reading baseline. We believe that might be due to the lack of a link between the local and global context in these approaches — in our work, we condition the global context on the abstractions and selectional preferences yielded by the local context and achieve better results. Lapata (2003), Shutova (2009) as well as Roberts and Harabagiu (2011) deal with the issue of logical metonymy, where the participant stands in for the full event: e.g. Mary enjoyed the book., where book stands in for reading the book, and this missing event (reading) can be inferred from a corpus. Utiyama et al. (2000), Lapata (2003) propose a probabilistic model for finding the correct interpretation of such metonymies in an unsupervised manner. However, these event type metonymies differ from the problem dealt with in our paper and the SemEval 2007 task in that their recognition (i.e. their distinction from literal occurrences) is achieved simply by grammatic"
D12-1017,P09-3001,0,0.164701,"s grammatical relations.1 Most effective in the SemEval task as summarized in Markert and Nissim (2009) has been the local, grammatical context, with the two systems relying on the global context or the local/global context in a BOW model (Leveling, 2007; Poibeau, 2007) not outperforming the most frequent reading baseline. We believe that might be due to the lack of a link between the local and global context in these approaches — in our work, we condition the global context on the abstractions and selectional preferences yielded by the local context and achieve better results. Lapata (2003), Shutova (2009) as well as Roberts and Harabagiu (2011) deal with the issue of logical metonymy, where the participant stands in for the full event: e.g. Mary enjoyed the book., where book stands in for reading the book, and this missing event (reading) can be inferred from a corpus. Utiyama et al. (2000), Lapata (2003) propose a probabilistic model for finding the correct interpretation of such metonymies in an unsupervised manner. However, these event type metonymies differ from the problem dealt with in our paper and the SemEval 2007 task in that their recognition (i.e. their distinction from literal occu"
D12-1017,N10-1091,0,0.0159107,"S // Remove hyponyms. for all {(c, c0 ) ∈ S 0 |(c0 , is a, c) ∈ W kN } do // update frequency f of c fc = fc + fc0 , f ∈ S delete c0 return S 0 187 discourse” assumption – a phrase that appears associated with a hyperlink once in the article body will be associated with the same hyperlink throughout the article (this applies to the article title as well, which is not hyperlinked in the article itself). This new version of the corpus was then split into sentences, and those without hyperlinks were removed. The remaining 18 million sentences were parsed with a parallelized version of Ensemble4 (Surdeanu and Manning, 2010), and we extracted G, the set of all grammatical relations of the type (verb, dependency, hyperlink) and (adjective, dependency, hyperlink), with the hyperlinks resolved to their corresponding node (concept) in the network ( |G |= 1,578,413 triples). For each verb and adjective in the extracted collocations, and for each of their dependency relations, their collocates were generalized in the network defined by the hypernym/hyponym relations in WikiNet following a method similar to the Minimum Description Length principle (Li and Abe, 1998). Essentially, we aimed to determine a small set of (mo"
D12-1017,C00-2128,0,0.0528555,"requent reading baseline. We believe that might be due to the lack of a link between the local and global context in these approaches — in our work, we condition the global context on the abstractions and selectional preferences yielded by the local context and achieve better results. Lapata (2003), Shutova (2009) as well as Roberts and Harabagiu (2011) deal with the issue of logical metonymy, where the participant stands in for the full event: e.g. Mary enjoyed the book., where book stands in for reading the book, and this missing event (reading) can be inferred from a corpus. Utiyama et al. (2000), Lapata (2003) propose a probabilistic model for finding the correct interpretation of such metonymies in an unsupervised manner. However, these event type metonymies differ from the problem dealt with in our paper and the SemEval 2007 task in that their recognition (i.e. their distinction from literal occurrences) is achieved simply by grammatical patterns (a noun instead of a gerund or to-infinitive following the verb) and the problem is limited to interpretation. Our view of relations in a concept network being the interpretations of metonymies is strongly reminiscent of older work in meto"
D12-1017,bentivogli-etal-2010-building,0,\N,Missing
D13-1077,J08-1001,0,0.0257015,"ition Using a Rich Linguistic Feature Set Yufang Hou1 , Katja Markert2 , Michael Strube1 1 Heidelberg Institute for Theoretical Studies gGmbH, Heidelberg, Germany (yufang.hou|michael.strube)@h-its.org 2 School of Computing, University of Leeds, UK scskm@leeds.ac.uk Abstract Example 1 cannot be established. This is a problem both for coherence theories such as Centering (Grosz et al., 1995) (where bridging is therefore incorporated as an indirect realization of previous entities) as well as applications relying on entity coherence modelling, such as readability assessment or sentence ordering (Barzilay and Lapata, 2008). Recognizing bridging anaphora is difficult due to the wide variation within the phenomenon, the resulting lack of easily identifiable surface markers and their relative rarity. We develop linguistically motivated discourse structure, lexico-semantic and genericity detection features and integrate these into a cascaded minority preference algorithm that models bridging recognition as a subtask of learning finegrained information status (IS). We substantially improve bridging recognition without impairing performance on other IS classes. 1 Introduction In bridging or associative anaphora (Clar"
D13-1077,W12-1632,0,0.374131,"ve anaphora where anaphor and antecedent are in a similarity/exclusion relation, indicated by anaphor modifiers such as other or similar (Modjeska et al., 2003). 2 Examples are from OntoNotes (Weischedel et al., 2011). Bridging anaphora are set in boldface; antecedents in italics. Full bridging resolution needs (i) recognition that a bridging anaphor is present and (ii) identification of the antecedent and contiguity relation. In recent work, these two tasks have been tackled separately, with bridging recognition handled as part of information status (IS) classification (Markert et al., 2012; Cahill and Riester, 2012; Rahman and Ng, 2012). Each mention in a text gets assigned one IS class that describes its accessibility to the reader at a given point in a text, bridging being one possible class. We stay within this framework. Bridging recognition is a difficult task, so that we had to report very low results on this IS class in previous work (Markert et al., 2012). This is due to the phenomenon’s variety, leading to a lack of clear surface features for recognition. Instead, we formulate in this paper novel discourse structure and lexicosemantic features as well as features that distinguish bridging from"
D13-1077,caselli-prodanof-2006-annotating,0,0.843433,"Missing"
D13-1077,J95-2003,0,0.178395,"phors are often licensed because of discourse structure Markert et al. (2012) local feature set f 1 FullPrevMention (b) f 2 FullPreMentionTime (n) f 3 PartialPreMention (b) f 4 ContentWordPreMention (b) f 5 Determiner (n) f 6 NPtype (n) f 7 NPlength (int) f 8 GrammaticalRole (n) f 9 NPNumber (n) f 10 PreModByCompMarker (b) f 11 SemanticClass (n) Markert et al. (2012) relational feature set f 12 HasChild (r) f 13 Precedes (r) Table 1: Markert et al.’s (2012) feature set, b indicates binary, n nominal, r relational features. and/or lexical or world knowledge. With regard to discourse structure, Grosz et al. (1995) observe that bridging is often needed to establish entity coherence between two adjacent sentences (Examples 1, 2, 4, 5, 6, 7 and 9). With regard to lexical and world knowledge, relational noun phrases (Examples 3, 4, 8 and 9), building parts (Example 1), set membership elements (Example 7), or, more rarely, temporal/spatial modification (Example 6) may favor a bridging reading. Motivated by these observations, we develop discourse structure and lexico-semantic features indicating bridging anaphora as well as features designed to separate genericity from bridging. 3.2 Features In Markert et a"
D13-1077,C96-1084,1,0.857455,"Missing"
D13-1077,N13-1111,1,0.837932,"Missing"
D13-1077,W03-2606,1,0.866159,"Missing"
D13-1077,P12-1084,1,0.81519,"concentrates on antecedent selection only (Poesio and Vieira, 1998; Poesio et al., 2004a; Markert et al., 2003; Lassalle and Denis, 2011; Hou et al., 2013), assuming that bridging recognition has already been performed. Previous work on recognition is either limited to definite NPs based on heuristics evaluated on small datasets (Hahn et al., 1996; Vieira and Poesio, 2000), or models it as a subtask of learning fine-grained IS (Rahman and Ng, 2012; Markert et al., 2012; Cahill and Riester, 2012). Results within this latter framework for bridging have been mixed: We reported in Markert et al. (2012) low results for bridging in written news text whereas Rahman and Ng (2012) report high results for the four subcategories of bridging annotated in the Switchboard dialogue corpus by Nissim et al. (2004). We believe this discrepancy to be due to differences in corpus size and genre as well as in bridging definition. Bridging in Switchboard includes non-anaphoric, syntactically linked part-of and set-member relationships (such as the building’s lobby), as well as comparative anaphora, the latter being marked by surface indicators such as other, another etc. Both types are much easier to identif"
D13-1077,W03-1023,1,0.897984,"idging anaphors with the antecedent One building.2 (1) One building was upgraded to red status while people were taking things out, and a resident called up the stairs to his girlfriend, telling her to keep sending things down to the lobby. Bridging is an important problem as it affects linguistic theory and applications alike. For example, without bridging resolution, entity coherence between the first and second coordinated clause in 1 We exclude comparative anaphora where anaphor and antecedent are in a similarity/exclusion relation, indicated by anaphor modifiers such as other or similar (Modjeska et al., 2003). 2 Examples are from OntoNotes (Weischedel et al., 2011). Bridging anaphora are set in boldface; antecedents in italics. Full bridging resolution needs (i) recognition that a bridging anaphor is present and (ii) identification of the antecedent and contiguity relation. In recent work, these two tasks have been tackled separately, with bridging recognition handled as part of information status (IS) classification (Markert et al., 2012; Cahill and Riester, 2012; Rahman and Ng, 2012). Each mention in a text gets assigned one IS class that describes its accessibility to the reader at a given poin"
D13-1077,nissim-etal-2004-annotation,0,0.297955,"ion has already been performed. Previous work on recognition is either limited to definite NPs based on heuristics evaluated on small datasets (Hahn et al., 1996; Vieira and Poesio, 2000), or models it as a subtask of learning fine-grained IS (Rahman and Ng, 2012; Markert et al., 2012; Cahill and Riester, 2012). Results within this latter framework for bridging have been mixed: We reported in Markert et al. (2012) low results for bridging in written news text whereas Rahman and Ng (2012) report high results for the four subcategories of bridging annotated in the Switchboard dialogue corpus by Nissim et al. (2004). We believe this discrepancy to be due to differences in corpus size and genre as well as in bridging definition. Bridging in Switchboard includes non-anaphoric, syntactically linked part-of and set-member relationships (such as the building’s lobby), as well as comparative anaphora, the latter being marked by surface indicators such as other, another etc. Both types are much easier to identify than anaphoric bridging cases.3 In addition, many non-anaphoric lexical cohesion cases have been annotated as bridging in Switchbard as well. We also separate bridging recognition and antecedent select"
D13-1077,W06-1612,0,0.474791,"Missing"
D13-1077,N13-1099,0,0.0899123,"Missing"
D13-1077,J98-2001,0,0.913747,"Missing"
D13-1077,P04-1019,0,0.875524,"lassify eight finegrained IS categories for NPs in written text: old, new and 6 mediated categories (syntactic, worldKnowledge, bridging, comparative, aggregate and function). This feature set (Table 1, f 1-f 13) works well to identify old, new and several mediated categories. However, it fails to recognize most bridging anaphora which we try to remedy in this work by including more diverse features. Discourse structure features (Table 2, f 1-f 3). Bridging occurs frequently in sentences where otherwise there would no entity coherence to previous sentences/clauses (see Grosz et al. (1995) and Poesio et al. (2004b) for discussions about bridging, entity coherence and centering transitions in the Centering framework). This is especially true for topic NPs (Halliday and Hasan, 1976) in such sentences. We follow these insights by identifying coherence gap sentences (see Examples 1, 4, 5, 6, 7, 9 and also 2): a sentence has a coherence gap (f 1) if it has none 816 new local features for bridging discourse f 1 IsCoherenceGap (b) structure f 2 IsSentFirstMention (b) f 3 IsDocFirstMention (b) semantics f 4 IsWordNetRelationalNoun (b) f 5 IsInquirerRoleNoun (b) f 6 IsBuildingPart (b) f 7 IsSetElement (b) f 8"
D13-1077,J04-3003,0,0.241221,"Missing"
D13-1077,P10-1005,0,0.258526,"fy set membership bridging cases (see Example 7), by checking whether the NP head is a number or indefinite pronoun (such as none, one, some) or modified by each, one. However, not all numbers are bridging cases (such as 1976) and we use f 9 to exclude such cases. Lassalle and Denis (2011) note that some bridging anaphors are indicated by spatial or temporal modifications (see Example 6). We use f 8 to detect this by compiling 20 such adjectives from Inquirer. Features to detect generic nouns (Table 2, f 11f 15). Generic NPs (Example 10) are easily confused with bridging anaphora. Inspired by Reiter and Frank (2010) who build on linguistic research, we develop features (f 11-f 15) to exclude generics. First, hypothetical entities are likely to refer to generic entities (Mitchell et al., 2002), We approximate this by determining whether the NP appears in an if-clause (f 11). Also the clause tense and mood may play a role to decide genericity (Reiter and Frank, 2010). This is often reflected by the main verb of a clause, so we extract its POS tag (f 12). Some NPs are commonly used generically, such as children, men, or the dollar. The ACE-2 corpus (distinct from our corpus) contains generic annotation . We"
D13-1077,J00-4003,0,0.403319,", 2013). 814 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 814–820, c Seattle, Washington, USA, 18-21 October 2013. 2013 Association for Computational Linguistics 2 Related Work Most bridging research concentrates on antecedent selection only (Poesio and Vieira, 1998; Poesio et al., 2004a; Markert et al., 2003; Lassalle and Denis, 2011; Hou et al., 2013), assuming that bridging recognition has already been performed. Previous work on recognition is either limited to definite NPs based on heuristics evaluated on small datasets (Hahn et al., 1996; Vieira and Poesio, 2000), or models it as a subtask of learning fine-grained IS (Rahman and Ng, 2012; Markert et al., 2012; Cahill and Riester, 2012). Results within this latter framework for bridging have been mixed: We reported in Markert et al. (2012) low results for bridging in written news text whereas Rahman and Ng (2012) report high results for the four subcategories of bridging annotated in the Switchboard dialogue corpus by Nissim et al. (2004). We believe this discrepancy to be due to differences in corpus size and genre as well as in bridging definition. Bridging in Switchboard includes non-anaphoric, synt"
D14-1221,P12-1041,0,0.0124753,"etermining which mentions in a text refer to the same entity. State-of-the-art approaches include both learningbased (Fernandes et al., 2012; Bj¨orkelund and Farkas, 2012; Durrett and Klein, 2013) and deterministic models (Lee et al., 2013; Martschat, 2013). These approaches achieve state-of-the-art performance mainly relying on morphosyntactic and lexical factors. However, consider the following example. we provide a solid foundation for work on employing knowledge sources for improving recall for coreference resolution (Ponzetto and Strube, 2006; Rahman and Ng, 2011; Ratinov and Roth, 2012; Bansal and Klein, 2012, inter alia). In particular, we make the following contributions: We present a novel framework for coreference resolution error analysis. This yields a formal foundation for previous work on link-based error analysis (Uryupina, 2008; Martschat, 2013) and complements work on transformation-based error analysis (Kummerfeld and Klein, 2013). We apply the method proposed in this paper to perform a recall error analysis of four state-ofthe-art systems, encompassing deterministic and learning-based approaches. In particular, we identify and characterize a set of challenging errors common to all sys"
D14-1221,J13-4004,0,0.368347,"te-of-the-art English coreference resolution systems. Our analysis highlights differences between the systems and identifies that the majority of recall errors for nouns and names are shared by all systems. We characterize this set of common challenging errors in terms of a broad range of lexical and semantic properties. 1 Introduction Coreference resolution is the task of determining which mentions in a text refer to the same entity. State-of-the-art approaches include both learningbased (Fernandes et al., 2012; Bj¨orkelund and Farkas, 2012; Durrett and Klein, 2013) and deterministic models (Lee et al., 2013; Martschat, 2013). These approaches achieve state-of-the-art performance mainly relying on morphosyntactic and lexical factors. However, consider the following example. we provide a solid foundation for work on employing knowledge sources for improving recall for coreference resolution (Ponzetto and Strube, 2006; Rahman and Ng, 2011; Ratinov and Roth, 2012; Bansal and Klein, 2012, inter alia). In particular, we make the following contributions: We present a novel framework for coreference resolution error analysis. This yields a formal foundation for previous work on link-based error analysis"
D14-1221,D08-1031,0,0.0577355,"sing Edges by Distance. The most straight-forward way to decide on an edge is to take the edge with smallest mention distance between source and target. This is the approach taken by Martschat (2013). 4 Choosing Edges by Accessibility. However, the distance-based approach may lead to unintuitive results. Let us consider again the BARACK O BAMA example from Figure 1. When choosing edges by distance, we would extract the error (the president, he). However, such links with a nonpronominal anaphor and a pronominal antecedent are difficult to process and considered unreliable (Ng and Cardie, 2002; Bengtson and Roth, 2008). On the other hand, the missed link (the president, Obama) constitutes a well-defined hyponymy relation which can be found in knowledge bases and is easily interpretable by humans. Uryupina (Uryupina, 2007; Uryupina, 2008) presents a recall error analysis where she takes the “intuitively easiest” missing link to analyze (Uryupina, 2007, p. 196). How can we formalize such an intuition? We will employ a notion grounded in accessibility theory (Ariel, 1988). Names and nouns refer to less accessible entities than pronouns do. For such anaphors, we prefer descriptive (name/nominal) antecedents. In"
D14-1221,H05-1004,0,0.278491,"r analysis complements insights obtained via evaluation metrics. We follow Chen and Ng (2013) and distinguish between linguistically agnostic metrics, which do not employ linguistic information during scoring, and linguistically informed metrics, which employ linguistic information similar as we do when computing spanning trees. We limit the discussion of linguistically agnostic metrics to the three most popular evaluation metrics whose average constitutes the official score in the CoNLL shared tasks on coreference resolution: MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998) and CEAFe (Luo, 2005).10 Our framework bears most similarities to the MUC metric, as both are based on the same linkbased entity representation. In particular, when we divide the number of errors extracted from an entity by the size of a spanning tree for that entity, we obtain a score linearly related11 to the MUC score for that entity (recall for reference entities, precision for system entities). B3 and CEAFe are not founded on a link-based structure. B3 computes recall by computing the relative overlap of reference and system entity for each reference mention, and then normalizes by the number of mentions. CEA"
D14-1221,W12-4503,0,0.0264011,"Missing"
D14-1221,W12-4511,1,0.893648,"Missing"
D14-1221,C10-1017,1,0.911376,"Missing"
D14-1221,P13-3012,1,0.338193,"ish coreference resolution systems. Our analysis highlights differences between the systems and identifies that the majority of recall errors for nouns and names are shared by all systems. We characterize this set of common challenging errors in terms of a broad range of lexical and semantic properties. 1 Introduction Coreference resolution is the task of determining which mentions in a text refer to the same entity. State-of-the-art approaches include both learningbased (Fernandes et al., 2012; Bj¨orkelund and Farkas, 2012; Durrett and Klein, 2013) and deterministic models (Lee et al., 2013; Martschat, 2013). These approaches achieve state-of-the-art performance mainly relying on morphosyntactic and lexical factors. However, consider the following example. we provide a solid foundation for work on employing knowledge sources for improving recall for coreference resolution (Ponzetto and Strube, 2006; Rahman and Ng, 2011; Ratinov and Roth, 2012; Bansal and Klein, 2012, inter alia). In particular, we make the following contributions: We present a novel framework for coreference resolution error analysis. This yields a formal foundation for previous work on link-based error analysis (Uryupina, 2008;"
D14-1221,W12-4504,0,0.0375268,"Missing"
D14-1221,I13-1193,0,0.231717,"ot able to analyze differences. They furthermore focus on describing many different error classes, instead of closely investigating particular phenomena. Evaluation Metrics. We extract recall and precision errors. How does our error analysis framework relate to coreference resolution evaluation metrics, which quantify recall and precision errors? We first observe a fundamental difference: evaluation metrics deal with scoring coreference chains, they provide no means of extracting recall or precision errors. Therefore our analysis complements insights obtained via evaluation metrics. We follow Chen and Ng (2013) and distinguish between linguistically agnostic metrics, which do not employ linguistic information during scoring, and linguistically informed metrics, which employ linguistic information similar as we do when computing spanning trees. We limit the discussion of linguistically agnostic metrics to the three most popular evaluation metrics whose average constitutes the official score in the CoNLL shared tasks on coreference resolution: MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998) and CEAFe (Luo, 2005).10 Our framework bears most similarities to the MUC metric, as both are based on t"
D14-1221,D13-1203,0,0.503885,"pply to perform a recall error analysis of four state-of-the-art English coreference resolution systems. Our analysis highlights differences between the systems and identifies that the majority of recall errors for nouns and names are shared by all systems. We characterize this set of common challenging errors in terms of a broad range of lexical and semantic properties. 1 Introduction Coreference resolution is the task of determining which mentions in a text refer to the same entity. State-of-the-art approaches include both learningbased (Fernandes et al., 2012; Bj¨orkelund and Farkas, 2012; Durrett and Klein, 2013) and deterministic models (Lee et al., 2013; Martschat, 2013). These approaches achieve state-of-the-art performance mainly relying on morphosyntactic and lexical factors. However, consider the following example. we provide a solid foundation for work on employing knowledge sources for improving recall for coreference resolution (Ponzetto and Strube, 2006; Rahman and Ng, 2011; Ratinov and Roth, 2012; Bansal and Klein, 2012, inter alia). In particular, we make the following contributions: We present a novel framework for coreference resolution error analysis. This yields a formal foundation for"
D14-1221,W12-4502,0,0.0495566,"Missing"
D14-1221,D13-1027,0,0.583632,"hosyntactic and lexical factors. However, consider the following example. we provide a solid foundation for work on employing knowledge sources for improving recall for coreference resolution (Ponzetto and Strube, 2006; Rahman and Ng, 2011; Ratinov and Roth, 2012; Bansal and Klein, 2012, inter alia). In particular, we make the following contributions: We present a novel framework for coreference resolution error analysis. This yields a formal foundation for previous work on link-based error analysis (Uryupina, 2008; Martschat, 2013) and complements work on transformation-based error analysis (Kummerfeld and Klein, 2013). We apply the method proposed in this paper to perform a recall error analysis of four state-ofthe-art systems, encompassing deterministic and learning-based approaches. In particular, we identify and characterize a set of challenging errors common to all systems, and discuss strengths and weaknesses of each system regarding specific error types. We also present a brief precision error analysis. A toolkit which implements the framework proposed in this paper is available for download.1 2 A Link-Based Analysis Framework In this section we discuss challenges in coreference resolution error anal"
D14-1221,J94-4002,0,0.562022,"Missing"
D14-1221,P02-1014,0,0.825787,"sing edges. 2072 Choosing Edges by Distance. The most straight-forward way to decide on an edge is to take the edge with smallest mention distance between source and target. This is the approach taken by Martschat (2013). 4 Choosing Edges by Accessibility. However, the distance-based approach may lead to unintuitive results. Let us consider again the BARACK O BAMA example from Figure 1. When choosing edges by distance, we would extract the error (the president, he). However, such links with a nonpronominal anaphor and a pronominal antecedent are difficult to process and considered unreliable (Ng and Cardie, 2002; Bengtson and Roth, 2008). On the other hand, the missed link (the president, Obama) constitutes a well-defined hyponymy relation which can be found in knowledge bases and is easily interpretable by humans. Uryupina (Uryupina, 2007; Uryupina, 2008) presents a recall error analysis where she takes the “intuitively easiest” missing link to analyze (Uryupina, 2007, p. 196). How can we formalize such an intuition? We will employ a notion grounded in accessibility theory (Ariel, 1988). Names and nouns refer to less accessible entities than pronouns do. For such anaphors, we prefer descriptive (nam"
D14-1221,P10-1142,0,0.0136857,"best according to all metrics, followed by Multigraph. StanfordSieve is the worst performing system: the gap to BerkeleyCoref is five points in average score. 4.3 Discussion Although we analyze recent systems on a recently published coreference data set, we believe that the results of our analysis will have implications for coreference in general. The data set is the largest and most genre-diverse coreference corpus so far. The systems we investigate represent major directions in coreference resolution model research, and make use of large and diverse feature sets proposed in the literature (Ng, 2010). Experimental Settings Previous studies identified the presence of recall errors as a main bottleneck for improving performance (Raghunathan et al., 2010; Durrett and Klein, 2013; Kummerfeld and Klein, 2013). This is also evidenced by the CoNLL shared tasks on coreference resolution (Pradhan et al., 2011; Pradhan et al., 2012), where most competitive systems had higher precision than recall. This indicates that an analysis of recall errors helps to understand and improve the state of the art. Hence, we focus on analyzing recall errors, and complement this by a brief analysis of precision erro"
D14-1221,N06-1025,1,0.86961,"Missing"
D14-1221,W11-1901,0,0.0449531,"ysis will have implications for coreference in general. The data set is the largest and most genre-diverse coreference corpus so far. The systems we investigate represent major directions in coreference resolution model research, and make use of large and diverse feature sets proposed in the literature (Ng, 2010). Experimental Settings Previous studies identified the presence of recall errors as a main bottleneck for improving performance (Raghunathan et al., 2010; Durrett and Klein, 2013; Kummerfeld and Klein, 2013). This is also evidenced by the CoNLL shared tasks on coreference resolution (Pradhan et al., 2011; Pradhan et al., 2012), where most competitive systems had higher precision than recall. This indicates that an analysis of recall errors helps to understand and improve the state of the art. Hence, we focus on analyzing recall errors, and complement this by a brief analysis of precision errors. We analyze errors of the four systems presented in the previous section on the CoNLL’12 English development data. To extract recall errors we employ the spanning tree algorithm which chooses edges by accessibility. We obtain precision errors from the pairwise output of the systems. 5.2 A Recall Error"
D14-1221,W12-4501,0,0.326433,"ts. Inspired by Ariel’s degrees of accessibility, we choose a target for a given anaphor mi as follows: • If mi is a pronoun, choose the closest preceding mention. • If mi is not a pronoun, choose the closest preceding proper name. If no such mention exists, choose the closest preceding common noun. If no such mention exists, choose the closest preceding mention. Applied to the example from Figure 1, this algorithm extracts the error (the president, Obama).3 We analyze the errors of the systems on the English development data of the CoNLL’12 shared task on multilingual coreference resolution (Pradhan et al., 2012). This corpus contains 343 documents, spanning seven genres: bible texts, broadcast conversation, broadcast news, magazine texts, news wire, telephone conversations and web logs. 3.2 Precision Errors Virtually all approaches to coreference resolution obtain entities by outputting pairs of anaphor and antecedent, subject to the constraint that one anaphor has at most one antecedent. We use this information to build spanning trees for system entities: these spanning trees consist of exactly the edges which correspond to anaphor/antecedent pairs in the system output. 3 A similar procedure was use"
D14-1221,P14-2006,1,0.836092,"the metrics are not link-based, they do not provide means to extract link-based errors. We leave determining whether the framework of these metrics exhibits a useful notion of errors to future work. 10 These are linguistically agnostic since they do not differ between different mention or entity types when evaluating. 11 via the transformation x 7→ 1 − x Recent work considered devising evaluation metrics which take linguistic information into account. Chen and Ng (2013) inject linguistic knowledge into existing evaluation metrics by weighting links in an entity representation graph. Tuggener (2014) devises scoring algorithms tailored for particular applications by redefining the notion of a correct link. While both of these works focus on scoring, they weight or explicitly define links in the reference and system entities, thereby they in principle allow error extraction. However, the authors do not attempt this and it is not clear whether the errors extracted that way are useful for analysis and system development. 7 Conclusions We presented a novel link-based framework for coreference resolution error analysis, which extends and complements previous work. We applied the framework to a"
D14-1221,D10-1048,0,0.0364984,"ts in average score. 4.3 Discussion Although we analyze recent systems on a recently published coreference data set, we believe that the results of our analysis will have implications for coreference in general. The data set is the largest and most genre-diverse coreference corpus so far. The systems we investigate represent major directions in coreference resolution model research, and make use of large and diverse feature sets proposed in the literature (Ng, 2010). Experimental Settings Previous studies identified the presence of recall errors as a main bottleneck for improving performance (Raghunathan et al., 2010; Durrett and Klein, 2013; Kummerfeld and Klein, 2013). This is also evidenced by the CoNLL shared tasks on coreference resolution (Pradhan et al., 2011; Pradhan et al., 2012), where most competitive systems had higher precision than recall. This indicates that an analysis of recall errors helps to understand and improve the state of the art. Hence, we focus on analyzing recall errors, and complement this by a brief analysis of precision errors. We analyze errors of the four systems presented in the previous section on the CoNLL’12 English development data. To extract recall errors we employ t"
D14-1221,P11-1082,0,0.0168164,"ction Coreference resolution is the task of determining which mentions in a text refer to the same entity. State-of-the-art approaches include both learningbased (Fernandes et al., 2012; Bj¨orkelund and Farkas, 2012; Durrett and Klein, 2013) and deterministic models (Lee et al., 2013; Martschat, 2013). These approaches achieve state-of-the-art performance mainly relying on morphosyntactic and lexical factors. However, consider the following example. we provide a solid foundation for work on employing knowledge sources for improving recall for coreference resolution (Ponzetto and Strube, 2006; Rahman and Ng, 2011; Ratinov and Roth, 2012; Bansal and Klein, 2012, inter alia). In particular, we make the following contributions: We present a novel framework for coreference resolution error analysis. This yields a formal foundation for previous work on link-based error analysis (Uryupina, 2008; Martschat, 2013) and complements work on transformation-based error analysis (Kummerfeld and Klein, 2013). We apply the method proposed in this paper to perform a recall error analysis of four state-ofthe-art systems, encompassing deterministic and learning-based approaches. In particular, we identify and characteri"
D14-1221,D12-1113,0,0.0118649,"olution is the task of determining which mentions in a text refer to the same entity. State-of-the-art approaches include both learningbased (Fernandes et al., 2012; Bj¨orkelund and Farkas, 2012; Durrett and Klein, 2013) and deterministic models (Lee et al., 2013; Martschat, 2013). These approaches achieve state-of-the-art performance mainly relying on morphosyntactic and lexical factors. However, consider the following example. we provide a solid foundation for work on employing knowledge sources for improving recall for coreference resolution (Ponzetto and Strube, 2006; Rahman and Ng, 2011; Ratinov and Roth, 2012; Bansal and Klein, 2012, inter alia). In particular, we make the following contributions: We present a novel framework for coreference resolution error analysis. This yields a formal foundation for previous work on link-based error analysis (Uryupina, 2008; Martschat, 2013) and complements work on transformation-based error analysis (Kummerfeld and Klein, 2013). We apply the method proposed in this paper to perform a recall error analysis of four state-ofthe-art systems, encompassing deterministic and learning-based approaches. In particular, we identify and characterize a set of challenging"
D14-1221,E14-4045,0,0.0437068,"ities. As the metrics are not link-based, they do not provide means to extract link-based errors. We leave determining whether the framework of these metrics exhibits a useful notion of errors to future work. 10 These are linguistically agnostic since they do not differ between different mention or entity types when evaluating. 11 via the transformation x 7→ 1 − x Recent work considered devising evaluation metrics which take linguistic information into account. Chen and Ng (2013) inject linguistic knowledge into existing evaluation metrics by weighting links in an entity representation graph. Tuggener (2014) devises scoring algorithms tailored for particular applications by redefining the notion of a correct link. While both of these works focus on scoring, they weight or explicitly define links in the reference and system entities, thereby they in principle allow error extraction. However, the authors do not attempt this and it is not clear whether the errors extracted that way are useful for analysis and system development. 7 Conclusions We presented a novel link-based framework for coreference resolution error analysis, which extends and complements previous work. We applied the framework to a"
D14-1221,uryupina-2008-error,0,0.485988,"Martschat, 2013). These approaches achieve state-of-the-art performance mainly relying on morphosyntactic and lexical factors. However, consider the following example. we provide a solid foundation for work on employing knowledge sources for improving recall for coreference resolution (Ponzetto and Strube, 2006; Rahman and Ng, 2011; Ratinov and Roth, 2012; Bansal and Klein, 2012, inter alia). In particular, we make the following contributions: We present a novel framework for coreference resolution error analysis. This yields a formal foundation for previous work on link-based error analysis (Uryupina, 2008; Martschat, 2013) and complements work on transformation-based error analysis (Kummerfeld and Klein, 2013). We apply the method proposed in this paper to perform a recall error analysis of four state-ofthe-art systems, encompassing deterministic and learning-based approaches. In particular, we identify and characterize a set of challenging errors common to all systems, and discuss strengths and weaknesses of each system regarding specific error types. We also present a brief precision error analysis. A toolkit which implements the framework proposed in this paper is available for download.1 2"
D14-1221,J00-4003,0,0.0296372,"ether the head matches between the mentions or not. In 341 cases the heads match. For many of these cases, parsing errors propagate and prevent the systems from recognizing the correct mention boundaries. In order to get a better understanding of the errors for nouns with different heads, we randomly extracted 50 of the 494 pairs and investigated the relation that holds between the heads. In 23 cases, the heads were related via hyponymy. In 10 cases they were synonyms. The remaining 17 cases involve many different phenomena, for example meronymy. This confirms findings from previous research (Vieira and Poesio, 2000). Hence, looking up lexical relations, especially hyponymy, might be helpful to solve these cases. 5.4.2 Differences between the Systems In order to analyze differences between the systems, we compare the recall errors they make. The information how recall errors differ between systems will enable us to understand individual strengths and weaknesses. Exemplarily, we will have a look at the differences in the errors when the anaphor is a common noun and the antecedent is a proper name. By system design and by the total error numbers (Table 4) we expect the learning-based systems to have a sligh"
D14-1221,M95-1005,0,0.215001,"o means of extracting recall or precision errors. Therefore our analysis complements insights obtained via evaluation metrics. We follow Chen and Ng (2013) and distinguish between linguistically agnostic metrics, which do not employ linguistic information during scoring, and linguistically informed metrics, which employ linguistic information similar as we do when computing spanning trees. We limit the discussion of linguistically agnostic metrics to the three most popular evaluation metrics whose average constitutes the official score in the CoNLL shared tasks on coreference resolution: MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998) and CEAFe (Luo, 2005).10 Our framework bears most similarities to the MUC metric, as both are based on the same linkbased entity representation. In particular, when we divide the number of errors extracted from an entity by the size of a spanning tree for that entity, we obtain a score linearly related11 to the MUC score for that entity (recall for reference entities, precision for system entities). B3 and CEAFe are not founded on a link-based structure. B3 computes recall by computing the relative overlap of reference and system entity for each reference mention"
D14-1222,W03-2607,0,0.115884,"five astronauts and touchdown) and the antecedent (The space shuttle Atlantis) establish (local) entity coherence.1 (1) The space shuttle Atlantis landed at a desert air strip at Edwards Air Force Base, Calif., ending a five-day mission that dispatched the Jupiter-bound Galileo space probe. The 1 Examples are from OntoNotes (Weischedel et al., 2011). Bridging anaphora are typed in boldface; antecedents in italics. Bridging or associative anaphora has been widely discussed in the linguistic literature (Clark, 1975; Prince, 1981; Gundel et al., 1993; L¨obner, 1998). Poesio and Vieira (1998) and Bunescu (2003) include cases where antecedent and anaphor are coreferent but do not share the same head noun (different-head coreference). We follow our previous work (Hou et al., 2013b) and restrict bridging to non-coreferential cases. We also exclude comparative anaphora (Modjeska et al., 2003). Bridging resolution includes two subtasks: (1) recognizing bridging anaphors and (2) finding the correct antecedent among candidates. In recent empirical work, these two subtasks have been tackled separately: (Markert et al., 2012; Cahill and Riester, 2012; Rahman and Ng, 2012; Hou et al., 2013a) handle bridging r"
D14-1222,W12-1632,0,0.25603,"1; Gundel et al., 1993; L¨obner, 1998). Poesio and Vieira (1998) and Bunescu (2003) include cases where antecedent and anaphor are coreferent but do not share the same head noun (different-head coreference). We follow our previous work (Hou et al., 2013b) and restrict bridging to non-coreferential cases. We also exclude comparative anaphora (Modjeska et al., 2003). Bridging resolution includes two subtasks: (1) recognizing bridging anaphors and (2) finding the correct antecedent among candidates. In recent empirical work, these two subtasks have been tackled separately: (Markert et al., 2012; Cahill and Riester, 2012; Rahman and Ng, 2012; Hou et al., 2013a) handle bridging recognition as part of information status (IS) classification, while (Poesio et al., 1997; Poesio et al., 2004; Markert et al., 2003; Lassalle and Denis, 2011; Hou et al., 2013b) concentrate on antecedent selection only, assuming that bridging recognition has already been performed. One exception is Vieira and Poesio (2000). They propose a rule-based system for processing definite NPs. However, they include different-head coreference into bridging. They report results for the whole anaphora resolution but do not report results for bridg"
D14-1222,W06-3915,0,0.334756,"re bridging anaphors are limited to definite NPs. They report preliminary results using the CoNLL scorer. However, we think the coreference resolution system and the evaluation metric for coreference resolution are 2082 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2082–2093, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics not suitable for bridging resolution since bridging is not a set problem. Another vein of research for bridging resolution focuses on formal semantics. Asher and Lascarides (1998) and Cimiano (2006) model bridging by integrating discourse structure and semantics from a formal semantics viewpoint. However, the implementation of such a theoretical framework is beyond the current capabilities of NLP since it depends heavily on commonsense entailment. In this paper, we propose a rule-based system for unrestricted bridging resolution. The system consists of eight rules which we carefully design based on linguistic intuitions, i.e., how the nature of bridging is reflected by various lexical, syntactic and semantic features. We evaluate our rulebased system on a corpus where bridging is reliabl"
D14-1222,T75-2034,0,0.828489,"shing entity coherence in a text. In Example 1, the links between the bridging anaphors (The five astronauts and touchdown) and the antecedent (The space shuttle Atlantis) establish (local) entity coherence.1 (1) The space shuttle Atlantis landed at a desert air strip at Edwards Air Force Base, Calif., ending a five-day mission that dispatched the Jupiter-bound Galileo space probe. The 1 Examples are from OntoNotes (Weischedel et al., 2011). Bridging anaphora are typed in boldface; antecedents in italics. Bridging or associative anaphora has been widely discussed in the linguistic literature (Clark, 1975; Prince, 1981; Gundel et al., 1993; L¨obner, 1998). Poesio and Vieira (1998) and Bunescu (2003) include cases where antecedent and anaphor are coreferent but do not share the same head noun (different-head coreference). We follow our previous work (Hou et al., 2013b) and restrict bridging to non-coreferential cases. We also exclude comparative anaphora (Modjeska et al., 2003). Bridging resolution includes two subtasks: (1) recognizing bridging anaphors and (2) finding the correct antecedent among candidates. In recent empirical work, these two subtasks have been tackled separately: (Markert e"
D14-1222,N13-1111,1,0.526785,"r strip at Edwards Air Force Base, Calif., ending a five-day mission that dispatched the Jupiter-bound Galileo space probe. The 1 Examples are from OntoNotes (Weischedel et al., 2011). Bridging anaphora are typed in boldface; antecedents in italics. Bridging or associative anaphora has been widely discussed in the linguistic literature (Clark, 1975; Prince, 1981; Gundel et al., 1993; L¨obner, 1998). Poesio and Vieira (1998) and Bunescu (2003) include cases where antecedent and anaphor are coreferent but do not share the same head noun (different-head coreference). We follow our previous work (Hou et al., 2013b) and restrict bridging to non-coreferential cases. We also exclude comparative anaphora (Modjeska et al., 2003). Bridging resolution includes two subtasks: (1) recognizing bridging anaphors and (2) finding the correct antecedent among candidates. In recent empirical work, these two subtasks have been tackled separately: (Markert et al., 2012; Cahill and Riester, 2012; Rahman and Ng, 2012; Hou et al., 2013a) handle bridging recognition as part of information status (IS) classification, while (Poesio et al., 1997; Poesio et al., 2004; Markert et al., 2003; Lassalle and Denis, 2011; Hou et al.,"
D14-1222,P13-1116,0,0.0350868,"5; 1998) interprets bridging anaphora as a particular kind of functional concept, which in a given situation assign a necessarily unique correlate to a (implicit) possessor argument. He distinguishes between relational nouns (e.g. parts terms, kinship terms, role terms) and sortal nouns and points out that relational nouns are more frequently used as bridging anaphora than sortal nouns. Rule1 to Rule4 in our system aim to resolve such relational nouns. We design Rule5 and Rule6 to capture set bridging. Finally, Rule7 and Rule8 are motivated by previous work on implicit semantic role labeling (Laparra and Rigau, 2013) which focuses on few predicates. For all NPs in a document, each rule r is applied separately to predict a set of potential bridging links. Every rule has its own constraints on bridging anaphora and antecedents respectively. Bridging anaphors are diverse with regard to syntactic form and function: they can be modified by definite or indefinite determiners (Table 1), furthermore they can take the subject (e.g. Example 3 and Example 6) or other positions (e.g. Example 2 and Example 4) in sentences. The only frequent syntactic property shared is that bridging anaphors most often have a simple i"
D14-1222,W03-2606,1,0.92684,"coreference). We follow our previous work (Hou et al., 2013b) and restrict bridging to non-coreferential cases. We also exclude comparative anaphora (Modjeska et al., 2003). Bridging resolution includes two subtasks: (1) recognizing bridging anaphors and (2) finding the correct antecedent among candidates. In recent empirical work, these two subtasks have been tackled separately: (Markert et al., 2012; Cahill and Riester, 2012; Rahman and Ng, 2012; Hou et al., 2013a) handle bridging recognition as part of information status (IS) classification, while (Poesio et al., 1997; Poesio et al., 2004; Markert et al., 2003; Lassalle and Denis, 2011; Hou et al., 2013b) concentrate on antecedent selection only, assuming that bridging recognition has already been performed. One exception is Vieira and Poesio (2000). They propose a rule-based system for processing definite NPs. However, they include different-head coreference into bridging. They report results for the whole anaphora resolution but do not report results for bridging resolution only. Another exception is R¨osiger and Teufel (2014). They apply a coreference resolution system with several additional semantic features to find bridging links in scientifi"
D14-1222,P12-1084,1,0.522065,"based system are incorporated into mlSystem ruleFeats as the features. mlSystem ruleFeats + atomFeats We augment mlSystem ruleFeats with more features from our previous work (Markert et al., 2012; Hou et al., 2013a; Hou et al., 2013b) on bridging anaphora recognition and antecedent selection. Some of these features overlap with the atomic features used in the rule-based system. Table 4 shows all the features we use for recognizing bridging anaphora. “∗” indicates the resources are used in the rule-based system. We apply them to the first element a of a pairwise instance (a, c). Markert et al. (2012) and Hou et results of learning-based approaches on the same test set as the rule-based system. 2088 Markert et al. local feature set f 1 FullPrevMention (b) ∗ f 2 FullPreMentionTime (n) f 4 ContentWordPreMention (b) f 5 Determiner (n) ∗ f 7 NPlength (int) f 8 GrammaticalRole (n) ∗ f 10 PreModByCompMarker (b) ∗ Hou et al. local feature set features to identify bridging anaphora f 1 IsCoherenceGap (b) f 2 IsSentFirstMention (b) f 4 IsWordNetRelationalNoun (b) ∗ f 5 IsInquirerRoleNoun (b) f 7 IsSetElement (b) ∗ f 8 PreModSpatialTemporal (b) f 10 PreModifiedByCountry (b) ∗ f 11 AppearInIfClause ("
D14-1222,meyers-etal-2004-annotating,0,0.0537321,"c relations. Such syntactic patterns are also explored in Poesio et al. (2004) to resolve meronymy bridging. 4 We use Gigaword (Parker et al., 2011) with automatic POS tag and NP chunk information. Rule2: relative person NPs. This rule is used to capture the bridging relation between a relative (e.g. The husband) and its antecedent (e.g. She). A list of 110 such relative nouns is extracted from WordNet. However, some relative nouns are frequently used generically instead of being bridging, such as children. To exclude such cases, we compute the argument taking ratio α for an NP using NomBank (Meyers et al., 2004). For each NP, α is calculated via its head frequency in the NomBank annotation divided by the head’s total frequency in the WSJ corpus in which the NomBank annotation is conducted. The value of α reflects how likely an NP is to take arguments. For instance, the value of α is 0.90 for husband but 0.31 for children. To predict bridging anaphora more accurately, a conservative constraint is used. An NP from A is added to Ar2 if: (1) its head appears in the relative person list; (2) its argument taking ratio α is bigger than 0.5; and (3) it does not contain any nominal or adjective pre-modificati"
D14-1222,W03-1023,1,0.817243,"leo space probe. The 1 Examples are from OntoNotes (Weischedel et al., 2011). Bridging anaphora are typed in boldface; antecedents in italics. Bridging or associative anaphora has been widely discussed in the linguistic literature (Clark, 1975; Prince, 1981; Gundel et al., 1993; L¨obner, 1998). Poesio and Vieira (1998) and Bunescu (2003) include cases where antecedent and anaphor are coreferent but do not share the same head noun (different-head coreference). We follow our previous work (Hou et al., 2013b) and restrict bridging to non-coreferential cases. We also exclude comparative anaphora (Modjeska et al., 2003). Bridging resolution includes two subtasks: (1) recognizing bridging anaphors and (2) finding the correct antecedent among candidates. In recent empirical work, these two subtasks have been tackled separately: (Markert et al., 2012; Cahill and Riester, 2012; Rahman and Ng, 2012; Hou et al., 2013a) handle bridging recognition as part of information status (IS) classification, while (Poesio et al., 1997; Poesio et al., 2004; Markert et al., 2003; Lassalle and Denis, 2011; Hou et al., 2013b) concentrate on antecedent selection only, assuming that bridging recognition has already been performed."
D14-1222,P02-1014,0,0.463912,"Missing"
D14-1222,J98-2001,0,0.691542,"n the bridging anaphors (The five astronauts and touchdown) and the antecedent (The space shuttle Atlantis) establish (local) entity coherence.1 (1) The space shuttle Atlantis landed at a desert air strip at Edwards Air Force Base, Calif., ending a five-day mission that dispatched the Jupiter-bound Galileo space probe. The 1 Examples are from OntoNotes (Weischedel et al., 2011). Bridging anaphora are typed in boldface; antecedents in italics. Bridging or associative anaphora has been widely discussed in the linguistic literature (Clark, 1975; Prince, 1981; Gundel et al., 1993; L¨obner, 1998). Poesio and Vieira (1998) and Bunescu (2003) include cases where antecedent and anaphor are coreferent but do not share the same head noun (different-head coreference). We follow our previous work (Hou et al., 2013b) and restrict bridging to non-coreferential cases. We also exclude comparative anaphora (Modjeska et al., 2003). Bridging resolution includes two subtasks: (1) recognizing bridging anaphors and (2) finding the correct antecedent among candidates. In recent empirical work, these two subtasks have been tackled separately: (Markert et al., 2012; Cahill and Riester, 2012; Rahman and Ng, 2012; Hou et al., 2013a"
D14-1222,W97-1301,0,0.895913,"share the same head noun (different-head coreference). We follow our previous work (Hou et al., 2013b) and restrict bridging to non-coreferential cases. We also exclude comparative anaphora (Modjeska et al., 2003). Bridging resolution includes two subtasks: (1) recognizing bridging anaphors and (2) finding the correct antecedent among candidates. In recent empirical work, these two subtasks have been tackled separately: (Markert et al., 2012; Cahill and Riester, 2012; Rahman and Ng, 2012; Hou et al., 2013a) handle bridging recognition as part of information status (IS) classification, while (Poesio et al., 1997; Poesio et al., 2004; Markert et al., 2003; Lassalle and Denis, 2011; Hou et al., 2013b) concentrate on antecedent selection only, assuming that bridging recognition has already been performed. One exception is Vieira and Poesio (2000). They propose a rule-based system for processing definite NPs. However, they include different-head coreference into bridging. They report results for the whole anaphora resolution but do not report results for bridging resolution only. Another exception is R¨osiger and Teufel (2014). They apply a coreference resolution system with several additional semantic f"
D14-1222,P04-1019,0,0.921756,"noun (different-head coreference). We follow our previous work (Hou et al., 2013b) and restrict bridging to non-coreferential cases. We also exclude comparative anaphora (Modjeska et al., 2003). Bridging resolution includes two subtasks: (1) recognizing bridging anaphors and (2) finding the correct antecedent among candidates. In recent empirical work, these two subtasks have been tackled separately: (Markert et al., 2012; Cahill and Riester, 2012; Rahman and Ng, 2012; Hou et al., 2013a) handle bridging recognition as part of information status (IS) classification, while (Poesio et al., 1997; Poesio et al., 2004; Markert et al., 2003; Lassalle and Denis, 2011; Hou et al., 2013b) concentrate on antecedent selection only, assuming that bridging recognition has already been performed. One exception is Vieira and Poesio (2000). They propose a rule-based system for processing definite NPs. However, they include different-head coreference into bridging. They report results for the whole anaphora resolution but do not report results for bridging resolution only. Another exception is R¨osiger and Teufel (2014). They apply a coreference resolution system with several additional semantic features to find bridg"
D14-1222,E12-1081,0,0.0897781,"obner, 1998). Poesio and Vieira (1998) and Bunescu (2003) include cases where antecedent and anaphor are coreferent but do not share the same head noun (different-head coreference). We follow our previous work (Hou et al., 2013b) and restrict bridging to non-coreferential cases. We also exclude comparative anaphora (Modjeska et al., 2003). Bridging resolution includes two subtasks: (1) recognizing bridging anaphors and (2) finding the correct antecedent among candidates. In recent empirical work, these two subtasks have been tackled separately: (Markert et al., 2012; Cahill and Riester, 2012; Rahman and Ng, 2012; Hou et al., 2013a) handle bridging recognition as part of information status (IS) classification, while (Poesio et al., 1997; Poesio et al., 2004; Markert et al., 2003; Lassalle and Denis, 2011; Hou et al., 2013b) concentrate on antecedent selection only, assuming that bridging recognition has already been performed. One exception is Vieira and Poesio (2000). They propose a rule-based system for processing definite NPs. However, they include different-head coreference into bridging. They report results for the whole anaphora resolution but do not report results for bridging resolution only."
D14-1222,E14-3006,0,0.091871,"Missing"
D14-1222,J01-4004,0,0.578686,"6) or other positions (e.g. Example 2 and Example 4) in sentences. The only frequent syntactic property shared is that bridging anaphors most often have a simple internal structure concerning modification. Therefore we first create an initial list of potential bridging anaphora A which excludes NPs which have a complex syntactic structure. An NP is added to A if it does not contain any other NPs and do not have modifications strongly indicating comparative NPs (such as other symptoms)3 . Since head match is a strong indicator of coreference anaphora for definite NPs (Vieira and Poesio, 2000; Soon et al., 2001), we further exclude definite NPs from A if they have the same head as a previous NP. Then a set of potential bridging anaphors Ar is chosen from A based on r’s constraints on bridging anaphora. Finally, for each potential bridging anaphor ana ∈ 3 A small list of 10 markers such as such, another . . . and the presence of adjectives or adverbs in the comparative form are used to predict comparative NPs. 2084 Ar , a single best antecedent ante from a list of candidate NPs (Cana ) is chosen if the rule’s constraint on antecedents is applied successfully. Every rule has its own scope to form the a"
D14-1222,J00-4003,0,0.446504,"ding site. Fog shrouded the base before touchdown. Bridging resolution plays an important role in establishing (local) entity coherence. This paper proposes a rule-based approach for the challenging task of unrestricted bridging resolution, where bridging anaphors are not limited to definite NPs and semantic relations between anaphors and their antecedents are not restricted to meronymic relations. The system consists of eight rules which target different relations based on linguistic insights. Our rule-based system significantly outperforms a reimplementation of a previous rule-based system (Vieira and Poesio, 2000). Furthermore, it performs better than a learning-based approach which has access to the same knowledge resources as the rule-based system. Additionally, incorporating the rules and more features into the learning-based system yields a minor improvement over the rule-based system. 1 Introduction Bridging resolution recovers the various nonidentity relations between anaphora and antecedents. It plays an important role in establishing entity coherence in a text. In Example 1, the links between the bridging anaphors (The five astronauts and touchdown) and the antecedent (The space shuttle Atlanti"
D14-1222,D13-1077,1,\N,Missing
D15-1226,N15-1113,0,0.0230438,"nformation. Finally, summaries should be coherent and of high readability. We introduce a completely unsupervised graphbased summarization using latent drichlet allocation (LDA, Blei and Lafferty (2009)). LDA is a simple model for topic modeling where topic probabilities are assigned words in documents. The probabilities can be used to measure the semantic relatedness between words and hence the topical coherence of a document. We use topical coherence as a means to ensure the coherence of extractive single-document summaries. Remus and Biemann (2013) apply LDA to compute lexical chains while Gorinski and Lapata (2015) also develop a graph-based summarization system which takes coherence into account. Our work is based on the bipartite entity graph introduced by Guinaudeau and Strube (2013). However, in their graph one set of nodes corresponds to entities whereas in our graph it corresponds to topics. The entity graph has already been used by Parveen and Strube (2015) for summarization. Their graph is unweighted and sparse, whereas our topical graph is weighted and dense. We apply our topical graph on the dataset introduced by Parveen and Strube (2015). This dataset contains scientific articles from the jou"
D15-1226,N13-1119,0,0.015523,"repetitive information, so summaries should not include redundant information. Finally, summaries should be coherent and of high readability. We introduce a completely unsupervised graphbased summarization using latent drichlet allocation (LDA, Blei and Lafferty (2009)). LDA is a simple model for topic modeling where topic probabilities are assigned words in documents. The probabilities can be used to measure the semantic relatedness between words and hence the topical coherence of a document. We use topical coherence as a means to ensure the coherence of extractive single-document summaries. Remus and Biemann (2013) apply LDA to compute lexical chains while Gorinski and Lapata (2015) also develop a graph-based summarization system which takes coherence into account. Our work is based on the bipartite entity graph introduced by Guinaudeau and Strube (2013). However, in their graph one set of nodes corresponds to entities whereas in our graph it corresponds to topics. The entity graph has already been used by Parveen and Strube (2015) for summarization. Their graph is unweighted and sparse, whereas our topical graph is weighted and dense. We apply our topical graph on the dataset introduced by Parveen and"
D15-1226,P13-1010,1,0.841278,"tion, so summaries should not include redundant information. Finally, summaries should be coherent and of high readability. We introduce a completely unsupervised graphbased summarization using latent drichlet allocation (LDA, Blei and Lafferty (2009)). LDA is a simple model for topic modeling where topic probabilities are assigned words in documents. The probabilities can be used to measure the semantic relatedness between words and hence the topical coherence of a document. We use topical coherence as a means to ensure the coherence of extractive single-document summaries. Remus and Biemann (2013) apply LDA to compute lexical chains while Gorinski and Lapata (2015) also develop a graph-based summarization system which takes coherence into account. Our work is based on the bipartite entity graph introduced by Guinaudeau and Strube (2013). However, in their graph one set of nodes corresponds to entities whereas in our graph it corresponds to topics. The entity graph has already been used by Parveen and Strube (2015) for summarization. Their graph is unweighted and sparse, whereas our topical graph is weighted and dense. We apply our topical graph on the dataset introduced by Parveen and"
D15-1226,P03-1054,0,0.0396287,"d author’s abstracts independently. To compare with the state-of-the-art in single-document summarization, we also evaluate on DUC 2002 data. 1951 3.2 We use the XML version of PLOS Medicine articles. We extract the contents excluding figures, tables and references. Editor’s summary and authors’ abstract are separated from the content for evaluation. The PLOS Medicine XML provides explicit full forms when abbreviations are introduced. We replace abbreviations with their full form in the summary. We then remove nonalphabetical characters. After this we parse articles using the Stanford parser (Klein and Manning, 2003). We perform pronoun resolution using the coreference resolution system by Martschat (2013)2 . We use gensim to generate the topics. For generating topics we use a dataset containing scientific articles from biology, which contains 221,385 documents and about 50 million sentences3 . We also use Wikipedia to compare with topics from a general domain. The HITS algorithm is applied on the bipartite graph for computing sentence importance. We calculate the coherence values of sentences on weighted one-mode projection graphs. The importance and coherence of a sentence is used in the optimization ph"
D15-1226,W04-1013,0,0.0633777,"e a dataset containing scientific articles from biology, which contains 221,385 documents and about 50 million sentences3 . We also use Wikipedia to compare with topics from a general domain. The HITS algorithm is applied on the bipartite graph for computing sentence importance. We calculate the coherence values of sentences on weighted one-mode projection graphs. The importance and coherence of a sentence is used in the optimization phase4 which returns a binary value for each sentence. 3.3 Results Results on PLOS Medicine are shown in Tables 1 and 2. We evaluate using ROUGE-SU4 and ROUGE-2 (Lin, 2004). We limit the length of the summaries to five sentences and the number of topics to 2000 in the topical graph. We also experimented with varying numbers of topics, i.e. 500, 1000 and 2000, and varying summary length limits. The results changed only marginally. The general trends remained the same. We compare our system with four different baselines and two versions of the entity graph. Lead selects the top five sentences, Random five sentences randomly. MMR is an implementation of maximal marginal relevance (Carbonell and Goldstein, 1998). TextRank is the graph-based system by Mihalcea and Ta"
D15-1226,P13-3012,0,0.0427296,"zation, we also evaluate on DUC 2002 data. 1951 3.2 We use the XML version of PLOS Medicine articles. We extract the contents excluding figures, tables and references. Editor’s summary and authors’ abstract are separated from the content for evaluation. The PLOS Medicine XML provides explicit full forms when abbreviations are introduced. We replace abbreviations with their full form in the summary. We then remove nonalphabetical characters. After this we parse articles using the Stanford parser (Klein and Manning, 2003). We perform pronoun resolution using the coreference resolution system by Martschat (2013)2 . We use gensim to generate the topics. For generating topics we use a dataset containing scientific articles from biology, which contains 221,385 documents and about 50 million sentences3 . We also use Wikipedia to compare with topics from a general domain. The HITS algorithm is applied on the bipartite graph for computing sentence importance. We calculate the coherence values of sentences on weighted one-mode projection graphs. The importance and coherence of a sentence is used in the optimization phase4 which returns a binary value for each sentence. 3.3 Results Results on PLOS Medicine a"
D15-1226,W04-3252,0,\N,Missing
D16-1074,P11-1051,0,0.0167255,"xtRank and ECoh . This confirms that using coherence patterns for sentence extraction yields more coherent summaries. 4 Related Work Summarizing scientific articles is as difficult as multi-document summarization because scientific articles are tend to be long and the important information is spread all over the article unlike information in news articles (Teufel and Moens, 2002). There are various approaches for summarizing scientific articles. Citations have been used by many researchers for summarization in this domain (Elkiss et al., 2008; Mohammad et al., 2009; Qazvinian and Radev, 2008; Abu-Jbara and Radev, 2011). Nanba and Okumura (2000) develop rules for categorizing citations by analyzing citation sentences. Newman (2001) analyzes the structure using a citation network. Similarly, Siddharthan and Teufel (2007) discover scientific attributions using citations. Discourse structure (but not necessarily coherence) has been used by Teufel and Moens (2002), Liakata et al. (2013) and others for summarizing scientific articles. Several state-of-the-art extractive summarization systems implement summarization as maximizing an objective function using constraints. McDonald (2007) interprets text summarizatio"
D16-1074,J08-1001,0,0.263306,"two gold summaries. DUC 2002 articles are shorter than PLOS Medicine articles (25 vs. 154 sentences average length). We use all (300) DUC 2005 human summaries to mine coherence patterns and to calculate their weights. 3.2 Experimental Setup First, we extract the text of an article. We remove figures, tables, references and non-alphabetical characters. Then we use the Stanford parser (Klein and 2 http://www.ncbi.nlm.nih.gov/pmc/tools/ ftp/ Manning, 2003) to determine sentence boundaries. We apply the Brown coherence toolkit (Elsner and Charniak, 2011) to convert the articles into entity grids (Barzilay and Lapata, 2008) which then are transformed into entity graphs. We use gSpan (Yan and Han, 2002) to extract all subgraphs from the projection graphs of the abstracts of the PubMed corpus. It is possible that patterns with a large number of nodes are not at all present in the projection graph. Hence, we use coherence patterns with 3 and 4 nodes, referred to as CP3 and CP4 , respectively. We use Gurobi (Gurobi Optimization, Inc., 2014) to solve the MIP problem. We use a pronoun resolution system (Martschat, 2013) to replace all pronouns in the summary with their antecedents. We determine the best values for λI"
D16-1074,P10-1084,0,0.0170457,"Missing"
D16-1074,P16-1046,0,0.00616999,"o significant difference between the ROUGE scores of using CP3 and CP4 on DUC 2002. Thus, we only report the results of using CP3 on DUC 2002. In Table 3, LREG is a baseline system usSystems Baselines Lead DUC 2002 Best TextRank LREG State-of-the-art Mead ILPphrase URANK UniformLink (k = 10) ECoh TCoh NN-SE Our Model CP3 R-1 R-2 R-SU4 0.459 0.480 0.470 0.438 0.180 0.228 0.195 0.207 0.201 0.445 0.454 0.485 0.471 0.485 0.481 0.474 0.200 0.213 0.215 0.201 0.230 0.243 0.230 0.210 0.490 0.247 0.258 0.217 0.253 0.242 Table 3: ROUGE scores on DUC 2002. ing logistic regression and hand-made features (Cheng and Lapata, 2016). We compare our model to previously published state-of-the-art systems. These systems show reasonable performance on the DUC 2002 summarization task. ILPphrase is a phrase-based extraction model, which selects important phrases and combines them via integer linear programming (Woodsend and Lapata, 2010). URANK utilizes a unified ranking process for single-document and multi-document summarization tasks (Wan, 2010). UniformLink (k=10), considers similar documents for document expansion in the single-document summarization task (Wan and Xiao, 2010). The more recent system, NN-SE, utilizes a neu"
D16-1074,N13-1136,0,0.258186,"Upper Bound in Table 2 represents maximum ROUGE scores that can be achieved in extractive summarization on the PLOS Medicine dataset. It is calculated by considering the whole scientific article as a summary and the corresponding editor’s summary as the gold standard. The Upper Bound scores are not very high showing that a significant improvement in ROUGE scores on the PLOS Medicine dataset is difficult. Thus, the performance achieved by our systems, CP3 and CP4 , is a considerable improvement on the PLOS Medicine dataset. Furthermore, we apply CP3 on the dataset introduced by Liakata et al. (2013). The dataset consists of 28 scientific articles from the chemistry domain. The state-of-the-art system on this dataset is CoreSC, which is developed by Liakata et al. (2013). CoreSC considers discourse information while summarizing a scientific article. The ROUGE-1 score of CP3 (0.96) is significantly better than CoreSC (0.75) and Microsoft Office Word 2007 AutoSumarize (0.73) (Garc´ıa-Hern´andez et al., 2009), in respect of abstracts. This shows that our system per778 forms well in other domains. We further calculate the average number of sentences per summary obtained by Mead and CP3 . On a"
D16-1074,P11-2022,0,0.039823,"articles for summarization. Every article is accompanied by at least two gold summaries. DUC 2002 articles are shorter than PLOS Medicine articles (25 vs. 154 sentences average length). We use all (300) DUC 2005 human summaries to mine coherence patterns and to calculate their weights. 3.2 Experimental Setup First, we extract the text of an article. We remove figures, tables, references and non-alphabetical characters. Then we use the Stanford parser (Klein and 2 http://www.ncbi.nlm.nih.gov/pmc/tools/ ftp/ Manning, 2003) to determine sentence boundaries. We apply the Brown coherence toolkit (Elsner and Charniak, 2011) to convert the articles into entity grids (Barzilay and Lapata, 2008) which then are transformed into entity graphs. We use gSpan (Yan and Han, 2002) to extract all subgraphs from the projection graphs of the abstracts of the PubMed corpus. It is possible that patterns with a large number of nodes are not at all present in the projection graph. Hence, we use coherence patterns with 3 and 4 nodes, referred to as CP3 and CP4 , respectively. We use Gurobi (Gurobi Optimization, Inc., 2014) to solve the MIP problem. We use a pronoun resolution system (Martschat, 2013) to replace all pronouns in th"
D16-1074,C12-1056,0,0.0126586,"ourse structure (but not necessarily coherence) has been used by Teufel and Moens (2002), Liakata et al. (2013) and others for summarizing scientific articles. Several state-of-the-art extractive summarization systems implement summarization as maximizing an objective function using constraints. McDonald (2007) interprets text summarization as a global inference problem, where he is maximizing the importance score of a summary by considering the length constraint. Similarly, various approaches for summarization are based on optimization using ILP (Gillick et al., 2009; Nishikawa et al., 2010; Galanis et al., 2012; Parveen and Strube, 2015). Until now, only few works have considered coherence while summarizing scientific articles. AbuJbara and Radev (2011) work on citation based summarization. They preprocess the citation sentences to filter out irrelevant sentences or sentence fragments, then extract sentences for the summary. Eventually, they refine the summary sentences to improve readability. Jha et al. (2015) consider Minimum Independent Discourse Contexts (MIDC) to solve the problem of non-coherence in extractive summarization. However, none of them deals with the problem of coherence within the"
D16-1074,N15-1113,0,0.0186908,"hen extract sentences for the summary. Eventually, they refine the summary sentences to improve readability. Jha et al. (2015) consider Minimum Independent Discourse Contexts (MIDC) to solve the problem of non-coherence in extractive summarization. However, none of them deals with the problem of coherence within the task of sentence selection. Sentence selection and ensuring the coherence of summaries are not tightly integrated in their techniques. They model coherence in summarization by only considering adjacent sentences. There are few methods (Hirao et al., 2013; Parveen and Strube, 2015; Gorinski and Lapata, 2015) which integrate coherence in optimization. These methods do not take into account the overall structure of the summary. Unlike earlier methods, we incorporate coherence patterns in optimization. 780 5 Conclusion We introduce a novel graph-based approach to generate coherent summaries of scientific articles. Our approach takes care of coherence distinctively by coherence patterns. We have experimented with PLOS Medicine and DUC 2002. The results show that the approach is robust, works on both scientific and news documents and with input documents of different length. It considerably outperform"
D16-1074,D15-1013,0,0.0135893,"e best weights for the PLOS Medicine development set. Weights for the DUC 2002 development set are λI = 0.5, λR = 0.2 and λc = 0.3. 3.3 Results We evaluate our model in two ways. First, we use ROUGE scores to compare our model with other models. Second, we explicitly evaluate the coherence of the summaries by human judgements. 3.3.1 ROUGE Assessment The ROUGE score (Lin, 2004) is a standard evaluation score in automatic text summarization. It calculates the overlap between gold summary and system summary. In automatic text summarization ROUGE 1, ROUGE 2 and ROUGE SU4 are usually reported (see Graham (2015) for an assessment of evaluation metrics for summarization). We compare our system (CP3 and CP4 ) with four baselines: Lead, Random, Maximal Marginal Relevance (MMR) and TextRank. Lead selects adjacent sentences from the beginning of an input article. Random selects sentences randomly. MMR (Carbonell and Goldstein, 1998) uses a trade-off between relevance and redundancy. TextRank is a graph-based system using sentences as nodes and edges weighted by cosine similarity between sentences (Mihalcea and Tarau, 2004). We compare our system with three state-of-the-art systems: ECoh (Parveen and Strub"
D16-1074,P13-1010,1,0.734707,"e evaluate our model on DUC 2002 to compare with state-ofthe-art systems. Our experimental results show that using coherence patterns for summarization produces more informative (but not redundant) and coherent summaries as compared to several baseline methods and state-of-the-art methods based on ROUGE scores and human judgements. 773 2 Method We solve the task of creating coherent summaries by employing coherence patterns. We tightly integrate determining importance, non-redundancy and coherence by applying global optimization, i.e., MIP. 2.1 Document Representation We use the entity graph (Guinaudeau and Strube, 2013) to represent scientific articles. The entity graph is a bipartite graph which consists of entities and sentences as two disjoint sets of nodes (Figure 2, ii). Entity nodes are connected only with sentence nodes and not among each other. An entity node is connected with a sentence node if and only if the entity is present in the sentence. Entities are the head nouns of noun phrases. We perform a one-mode projection on sentence nodes to create a directed one-mode projection graph (Figure 2, iii). Two sentence nodes in the onemode projection graph are connected if they share at least one entity"
D16-1074,N09-1041,0,0.0247913,"ent system, NN-SE, utilizes a neural network hierarchical document encoder and an attention-based extractor to extract sentences from a document for a summary (Cheng and Lapata, 2016). ROUGE scores of our approach on this dataset are better than baselines and state-of-theart systems. This shows that our system performs well even in a different genre (robust) and with considerably shorter input documents (scalable). 3.3.2 Coherence Assessment ROUGE scores do not evaluate summary coherence, since ROUGE only calculates overlapping recall scores and does not consider the structure of the summary. Haghighi and Vanderwende (2009), Celikyilmaz and Hakkani-T¨ur (2010) and Christensen et al. (2013) evaluate the overall summary quality by asking human subjects to rank system generated 779 summaries. Parveen and Strube (2015) and Parveen et al. (2015) assess the coherence by asking human assessors to rank system generated summaries and compare their system with baseline systems. We perform summary coherence assessment by asking one Postdoc, two PhD students and one Masters student from the field of natural language processing. We provide them with the output summaries of four different systems for ten articles. We ask them"
D16-1074,D13-1158,0,0.0225942,"irrelevant sentences or sentence fragments, then extract sentences for the summary. Eventually, they refine the summary sentences to improve readability. Jha et al. (2015) consider Minimum Independent Discourse Contexts (MIDC) to solve the problem of non-coherence in extractive summarization. However, none of them deals with the problem of coherence within the task of sentence selection. Sentence selection and ensuring the coherence of summaries are not tightly integrated in their techniques. They model coherence in summarization by only considering adjacent sentences. There are few methods (Hirao et al., 2013; Parveen and Strube, 2015; Gorinski and Lapata, 2015) which integrate coherence in optimization. These methods do not take into account the overall structure of the summary. Unlike earlier methods, we incorporate coherence patterns in optimization. 780 5 Conclusion We introduce a novel graph-based approach to generate coherent summaries of scientific articles. Our approach takes care of coherence distinctively by coherence patterns. We have experimented with PLOS Medicine and DUC 2002. The results show that the approach is robust, works on both scientific and news documents and with input doc"
D16-1074,C10-2060,0,0.052281,"Missing"
D16-1074,P03-1054,0,0.0456043,"Missing"
D16-1074,D13-1070,0,0.198481,"Upper Bound in Table 2 represents maximum ROUGE scores that can be achieved in extractive summarization on the PLOS Medicine dataset. It is calculated by considering the whole scientific article as a summary and the corresponding editor’s summary as the gold standard. The Upper Bound scores are not very high showing that a significant improvement in ROUGE scores on the PLOS Medicine dataset is difficult. Thus, the performance achieved by our systems, CP3 and CP4 , is a considerable improvement on the PLOS Medicine dataset. Furthermore, we apply CP3 on the dataset introduced by Liakata et al. (2013). The dataset consists of 28 scientific articles from the chemistry domain. The state-of-the-art system on this dataset is CoreSC, which is developed by Liakata et al. (2013). CoreSC considers discourse information while summarizing a scientific article. The ROUGE-1 score of CP3 (0.96) is significantly better than CoreSC (0.75) and Microsoft Office Word 2007 AutoSumarize (0.73) (Garc´ıa-Hern´andez et al., 2009), in respect of abstracts. This shows that our system per778 forms well in other domains. We further calculate the average number of sentences per summary obtained by Mead and CP3 . On a"
D16-1074,W04-1013,0,0.0151003,"We use a pronoun resolution system (Martschat, 2013) to replace all pronouns in the summary with their antecedents. We determine the best values for λI , λR , and λc on the development sets. λI = 0.4, λR = 0.3, and λc = 0.3 are the best weights for the PLOS Medicine development set. Weights for the DUC 2002 development set are λI = 0.5, λR = 0.2 and λc = 0.3. 3.3 Results We evaluate our model in two ways. First, we use ROUGE scores to compare our model with other models. Second, we explicitly evaluate the coherence of the summaries by human judgements. 3.3.1 ROUGE Assessment The ROUGE score (Lin, 2004) is a standard evaluation score in automatic text summarization. It calculates the overlap between gold summary and system summary. In automatic text summarization ROUGE 1, ROUGE 2 and ROUGE SU4 are usually reported (see Graham (2015) for an assessment of evaluation metrics for summarization). We compare our system (CP3 and CP4 ) with four baselines: Lead, Random, Maximal Marginal Relevance (MMR) and TextRank. Lead selects adjacent sentences from the beginning of an input article. Random selects sentences randomly. MMR (Carbonell and Goldstein, 1998) uses a trade-off between relevance and redu"
D16-1074,P13-3012,0,0.0291072,"coherence toolkit (Elsner and Charniak, 2011) to convert the articles into entity grids (Barzilay and Lapata, 2008) which then are transformed into entity graphs. We use gSpan (Yan and Han, 2002) to extract all subgraphs from the projection graphs of the abstracts of the PubMed corpus. It is possible that patterns with a large number of nodes are not at all present in the projection graph. Hence, we use coherence patterns with 3 and 4 nodes, referred to as CP3 and CP4 , respectively. We use Gurobi (Gurobi Optimization, Inc., 2014) to solve the MIP problem. We use a pronoun resolution system (Martschat, 2013) to replace all pronouns in the summary with their antecedents. We determine the best values for λI , λR , and λc on the development sets. λI = 0.4, λR = 0.3, and λc = 0.3 are the best weights for the PLOS Medicine development set. Weights for the DUC 2002 development set are λI = 0.5, λR = 0.2 and λc = 0.3. 3.3 Results We evaluate our model in two ways. First, we use ROUGE scores to compare our model with other models. Second, we explicitly evaluate the coherence of the summaries by human judgements. 3.3.1 ROUGE Assessment The ROUGE score (Lin, 2004) is a standard evaluation score in automati"
D16-1074,S15-1036,1,0.787272,"ed to one another such that it becomes coherent and easy to read. In this work, we focus on the coherence aspect of summarization. We use discourse entities as the unit of information that relate sentences. Here, discourse entities are referred to as head nouns of noun phrases (see Section 2). The main goal is to extract sentences which refer to those entities which are important and unique, and also to entities which connect the extracted sentences in a coherent manner. Entities in connected sentences can be used to create linguistically motivated coherence patterns (Daneˇs, 1974). Recently, Mesgar and Strube (2015) modeled these coherence patterns by subgraphs of the graph representation (nodes represent sentences and edges represent entity connections among sentences) of documents. They show that the frequency of coherence patterns can be used as features for coherence. The key idea of this paper is to apply coherence patterns to long scientific articles to extract (possibly) non-adjacent sentences which, however, are already coherent. Based on the assumption that ab772 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 772–783, c Austin, Texas, November 1-5,"
D16-1074,N09-1066,0,0.0175721,"ful. It also performs substantially better than TextRank and ECoh . This confirms that using coherence patterns for sentence extraction yields more coherent summaries. 4 Related Work Summarizing scientific articles is as difficult as multi-document summarization because scientific articles are tend to be long and the important information is spread all over the article unlike information in news articles (Teufel and Moens, 2002). There are various approaches for summarizing scientific articles. Citations have been used by many researchers for summarization in this domain (Elkiss et al., 2008; Mohammad et al., 2009; Qazvinian and Radev, 2008; Abu-Jbara and Radev, 2011). Nanba and Okumura (2000) develop rules for categorizing citations by analyzing citation sentences. Newman (2001) analyzes the structure using a citation network. Similarly, Siddharthan and Teufel (2007) discover scientific attributions using citations. Discourse structure (but not necessarily coherence) has been used by Teufel and Moens (2002), Liakata et al. (2013) and others for summarizing scientific articles. Several state-of-the-art extractive summarization systems implement summarization as maximizing an objective function using co"
D16-1074,C00-2160,0,0.0944644,"rms that using coherence patterns for sentence extraction yields more coherent summaries. 4 Related Work Summarizing scientific articles is as difficult as multi-document summarization because scientific articles are tend to be long and the important information is spread all over the article unlike information in news articles (Teufel and Moens, 2002). There are various approaches for summarizing scientific articles. Citations have been used by many researchers for summarization in this domain (Elkiss et al., 2008; Mohammad et al., 2009; Qazvinian and Radev, 2008; Abu-Jbara and Radev, 2011). Nanba and Okumura (2000) develop rules for categorizing citations by analyzing citation sentences. Newman (2001) analyzes the structure using a citation network. Similarly, Siddharthan and Teufel (2007) discover scientific attributions using citations. Discourse structure (but not necessarily coherence) has been used by Teufel and Moens (2002), Liakata et al. (2013) and others for summarizing scientific articles. Several state-of-the-art extractive summarization systems implement summarization as maximizing an objective function using constraints. McDonald (2007) interprets text summarization as a global inference pr"
D16-1074,C10-2105,0,0.0136061,"ns using citations. Discourse structure (but not necessarily coherence) has been used by Teufel and Moens (2002), Liakata et al. (2013) and others for summarizing scientific articles. Several state-of-the-art extractive summarization systems implement summarization as maximizing an objective function using constraints. McDonald (2007) interprets text summarization as a global inference problem, where he is maximizing the importance score of a summary by considering the length constraint. Similarly, various approaches for summarization are based on optimization using ILP (Gillick et al., 2009; Nishikawa et al., 2010; Galanis et al., 2012; Parveen and Strube, 2015). Until now, only few works have considered coherence while summarizing scientific articles. AbuJbara and Radev (2011) work on citation based summarization. They preprocess the citation sentences to filter out irrelevant sentences or sentence fragments, then extract sentences for the summary. Eventually, they refine the summary sentences to improve readability. Jha et al. (2015) consider Minimum Independent Discourse Contexts (MIDC) to solve the problem of non-coherence in extractive summarization. However, none of them deals with the problem of"
D16-1074,D15-1226,1,0.228773,"randomly. MMR (Carbonell and Goldstein, 1998) uses a trade-off between relevance and redundancy. TextRank is a graph-based system using sentences as nodes and edges weighted by cosine similarity between sentences (Mihalcea and Tarau, 2004). We compare our system with three state-of-the-art systems: ECoh (Parveen and Strube, 2015), TCoh 777 Systems Baselines Lead Random MMR TextRank State-of-the-art ECoh TCoh Mead Our Model CP3 R-SU4 R-2 0.067 0.048 0.069 0.068 0.055 0.031 0.048 0.048 0.131 0.129 0.084 0.098 0.095 0.068 0.135 0.103 Table 1: PLOS Medicine, editor’s summaries with 5 sentences. (Parveen et al., 2015), and Mead (Radev et al., 2004). ECoh uses entity graphs which consists of entities and sentences, and TCoh uses topical graphs where entities are replaced by the topics. They both use the outdegree of sentence nodes in the unweighted and the weighted projection graph, respectively, as the coherence measure of each sentence. Mead employs a linear combination of three features: centroid score, position score and overlap score. The linear combination is used to add sentences to the summary up to the required length. The centroid score gives the highest score to the most central sentence in the c"
D16-1074,C08-1087,0,0.0127593,"ubstantially better than TextRank and ECoh . This confirms that using coherence patterns for sentence extraction yields more coherent summaries. 4 Related Work Summarizing scientific articles is as difficult as multi-document summarization because scientific articles are tend to be long and the important information is spread all over the article unlike information in news articles (Teufel and Moens, 2002). There are various approaches for summarizing scientific articles. Citations have been used by many researchers for summarization in this domain (Elkiss et al., 2008; Mohammad et al., 2009; Qazvinian and Radev, 2008; Abu-Jbara and Radev, 2011). Nanba and Okumura (2000) develop rules for categorizing citations by analyzing citation sentences. Newman (2001) analyzes the structure using a citation network. Similarly, Siddharthan and Teufel (2007) discover scientific attributions using citations. Discourse structure (but not necessarily coherence) has been used by Teufel and Moens (2002), Liakata et al. (2013) and others for summarizing scientific articles. Several state-of-the-art extractive summarization systems implement summarization as maximizing an objective function using constraints. McDonald (2007)"
D16-1074,radev-etal-2004-mead,0,0.0144882,"dstein, 1998) uses a trade-off between relevance and redundancy. TextRank is a graph-based system using sentences as nodes and edges weighted by cosine similarity between sentences (Mihalcea and Tarau, 2004). We compare our system with three state-of-the-art systems: ECoh (Parveen and Strube, 2015), TCoh 777 Systems Baselines Lead Random MMR TextRank State-of-the-art ECoh TCoh Mead Our Model CP3 R-SU4 R-2 0.067 0.048 0.069 0.068 0.055 0.031 0.048 0.048 0.131 0.129 0.084 0.098 0.095 0.068 0.135 0.103 Table 1: PLOS Medicine, editor’s summaries with 5 sentences. (Parveen et al., 2015), and Mead (Radev et al., 2004). ECoh uses entity graphs which consists of entities and sentences, and TCoh uses topical graphs where entities are replaced by the topics. They both use the outdegree of sentence nodes in the unweighted and the weighted projection graph, respectively, as the coherence measure of each sentence. Mead employs a linear combination of three features: centroid score, position score and overlap score. The linear combination is used to add sentences to the summary up to the required length. The centroid score gives the highest score to the most central sentence in the cluster of sentences, the positi"
D16-1074,N07-1040,0,0.0235258,"mmarization because scientific articles are tend to be long and the important information is spread all over the article unlike information in news articles (Teufel and Moens, 2002). There are various approaches for summarizing scientific articles. Citations have been used by many researchers for summarization in this domain (Elkiss et al., 2008; Mohammad et al., 2009; Qazvinian and Radev, 2008; Abu-Jbara and Radev, 2011). Nanba and Okumura (2000) develop rules for categorizing citations by analyzing citation sentences. Newman (2001) analyzes the structure using a citation network. Similarly, Siddharthan and Teufel (2007) discover scientific attributions using citations. Discourse structure (but not necessarily coherence) has been used by Teufel and Moens (2002), Liakata et al. (2013) and others for summarizing scientific articles. Several state-of-the-art extractive summarization systems implement summarization as maximizing an objective function using constraints. McDonald (2007) interprets text summarization as a global inference problem, where he is maximizing the importance score of a summary by considering the length constraint. Similarly, various approaches for summarization are based on optimization us"
D16-1074,J02-4002,0,0.0214312,"Hence, these summaries are as coherent as the author intends them to be, but they are not informative. However, CP3 is very close in coherence to Lead indicating that our strategy is successful. It also performs substantially better than TextRank and ECoh . This confirms that using coherence patterns for sentence extraction yields more coherent summaries. 4 Related Work Summarizing scientific articles is as difficult as multi-document summarization because scientific articles are tend to be long and the important information is spread all over the article unlike information in news articles (Teufel and Moens, 2002). There are various approaches for summarizing scientific articles. Citations have been used by many researchers for summarization in this domain (Elkiss et al., 2008; Mohammad et al., 2009; Qazvinian and Radev, 2008; Abu-Jbara and Radev, 2011). Nanba and Okumura (2000) develop rules for categorizing citations by analyzing citation sentences. Newman (2001) analyzes the structure using a citation network. Similarly, Siddharthan and Teufel (2007) discover scientific attributions using citations. Discourse structure (but not necessarily coherence) has been used by Teufel and Moens (2002), Liakata"
D16-1074,C10-1128,0,0.00533712,"4 0.200 0.213 0.215 0.201 0.230 0.243 0.230 0.210 0.490 0.247 0.258 0.217 0.253 0.242 Table 3: ROUGE scores on DUC 2002. ing logistic regression and hand-made features (Cheng and Lapata, 2016). We compare our model to previously published state-of-the-art systems. These systems show reasonable performance on the DUC 2002 summarization task. ILPphrase is a phrase-based extraction model, which selects important phrases and combines them via integer linear programming (Woodsend and Lapata, 2010). URANK utilizes a unified ranking process for single-document and multi-document summarization tasks (Wan, 2010). UniformLink (k=10), considers similar documents for document expansion in the single-document summarization task (Wan and Xiao, 2010). The more recent system, NN-SE, utilizes a neural network hierarchical document encoder and an attention-based extractor to extract sentences from a document for a summary (Cheng and Lapata, 2016). ROUGE scores of our approach on this dataset are better than baselines and state-of-theart systems. This shows that our system performs well even in a different genre (robust) and with considerably shorter input documents (scalable). 3.3.2 Coherence Assessment ROUGE"
D16-1074,P10-1058,0,0.0407198,"NN-SE Our Model CP3 R-1 R-2 R-SU4 0.459 0.480 0.470 0.438 0.180 0.228 0.195 0.207 0.201 0.445 0.454 0.485 0.471 0.485 0.481 0.474 0.200 0.213 0.215 0.201 0.230 0.243 0.230 0.210 0.490 0.247 0.258 0.217 0.253 0.242 Table 3: ROUGE scores on DUC 2002. ing logistic regression and hand-made features (Cheng and Lapata, 2016). We compare our model to previously published state-of-the-art systems. These systems show reasonable performance on the DUC 2002 summarization task. ILPphrase is a phrase-based extraction model, which selects important phrases and combines them via integer linear programming (Woodsend and Lapata, 2010). URANK utilizes a unified ranking process for single-document and multi-document summarization tasks (Wan, 2010). UniformLink (k=10), considers similar documents for document expansion in the single-document summarization task (Wan and Xiao, 2010). The more recent system, NN-SE, utilizes a neural network hierarchical document encoder and an attention-based extractor to extract sentences from a document for a summary (Cheng and Lapata, 2016). ROUGE scores of our approach on this dataset are better than baselines and state-of-theart systems. This shows that our system performs well even in a di"
D16-1074,W04-3252,0,\N,Missing
D17-1138,N16-1115,1,0.901932,"Missing"
D17-1138,W17-1501,1,0.887783,"Missing"
D17-1138,M95-1005,0,0.282004,"on pairs. Lines and boxes represent quartiles, diamonds outliers, points subsamples with jitter. Coreferent mention pairs are more similar than non-coreferent mention pairs with a Matthews correlation coefficient of 0.30, indicating weak to moderate correlation. other knowledge sources in a non-linear way. We select the ranking model of deep-coref (Clark and Manning, 2016b) as our baseline. deep-coref is a neural model that combines the input features through several hidden layers. Baseline in Table 1 reports our baseline results on the CoNLL 2012 test set. The results are reported using MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998), CEAF e (Luo, 2005), the average F1 score of these three metrics, i.e. CoNLL score, and LEA (Moosavi and Strube, 2016b). deepcoref includes the embeddings of the dependency governor of mentions. Combined with the relative position of a mention to its governor, deep-coref may be able to implicitly capture selectional preferences to some extent. −gov in Table 1 represents deep-coref performance when governors are not incorporated. As we can see, the exclusion of the governor information does not affect the performance. This result shows that the implicit modeling o"
D17-1138,N16-1114,0,0.0322814,"e makes on its arguments.” For example, selectional preferences allow resolving the pronoun it in the text “The Titanic hit an iceberg. It sank quickly.” Here, the predicate sink ‘prefers’ certain subject arguments over others: It is plausible that a ship sinks, but implausible that an iceberg does. Work on the automatic acquisition of selectional preferences has shown considerable progress (Dagan and Itai, 1990; Resnik, 1993; Agirre and Martinez, 2001; Pantel et al., 2007; Erk, 2007; Ritter et al., 2010; Van de Cruys, 2014). However, today’s coreference resolvers (Martschat and Strube, 2015; Wiseman et al., 2016; Clark and Manning, 2016a, i.a.) capture selectional preferences only ∗ These authors contributed equally to this work. The claim by Kehler et al. (2004) is based on selectional preferences extracted from a, by current standards, small number of 2.8m predicateargument pairs. Furthermore, they employ a simple (linear) maximum entropy classifier, which requires manual definition of feature combinations and is unlikely to fully capture the complex interaction between selectional preferences and other coreference features. Therefore, it is worth revisiting how a better selectional preference mode"
D17-1138,N07-1071,0,0.0118818,"al work on “Resolving Pronominal References” Hobbs (1978) proposed a semantic approach that requires reasoning about the “demands the predicate makes on its arguments.” For example, selectional preferences allow resolving the pronoun it in the text “The Titanic hit an iceberg. It sank quickly.” Here, the predicate sink ‘prefers’ certain subject arguments over others: It is plausible that a ship sinks, but implausible that an iceberg does. Work on the automatic acquisition of selectional preferences has shown considerable progress (Dagan and Itai, 1990; Resnik, 1993; Agirre and Martinez, 2001; Pantel et al., 2007; Erk, 2007; Ritter et al., 2010; Van de Cruys, 2014). However, today’s coreference resolvers (Martschat and Strube, 2015; Wiseman et al., 2016; Clark and Manning, 2016a, i.a.) capture selectional preferences only ∗ These authors contributed equally to this work. The claim by Kehler et al. (2004) is based on selectional preferences extracted from a, by current standards, small number of 2.8m predicateargument pairs. Furthermore, they employ a simple (linear) maximum entropy classifier, which requires manual definition of feature combinations and is unlikely to fully capture the complex interac"
D17-1138,D14-1162,0,0.0852303,"should be arguments that tend to fill the same slots of similar predicates, and predicate slots that have similar arguments. For example, captain should be close to pilot, ship to airplane, the subject of steer close to both captain and pilot, and also to, e.g., the subject of drive. Such a space allows judging the plausibility of unseen predicate-argument pairs.2 We construct this space via dependency-based word embeddings (Levy and Goldberg, 2014). To see why this choice is better-suited for modeling selectional preferences than alternatives such as word2vec (Mikolov et al., 2013) or GloVe (Pennington et al., 2014), consider the following example: captain :: ←−− nsubj steers −−→ dobj pilot ←−− nsubj steers −−→ dobj ship :: airplane Here, captain and ship, have high syntagmatic similarity, i.e., these words are semantically related and tend to occur close to each other. This also holds for pilot and airplane. In contrast, captain and pilot, as well as ship and airplane have high paradigmatic similarity, i.e., they are seman2 Prior work generalizes to unseen predicate-argument pairs via WordNet synsets (Resnik, 1993), a generalization corpus (Erk, 2007), or tensor factorization (Van de Cruys, 2010). Close"
D17-1138,P10-1044,0,0.0561899,"Missing"
D17-1138,L16-1376,0,0.039277,"h corpus with the Stanford CoreNLP dependency parser (Manning et al., 2014). After filtering, this yields ca. 1.4 billion phrase-context pairs such as (Titanic, sank@nsubj) from GigaWord and ca. 12.9 million entity type-context pairs such as (/product/ship, sank@nsubj) from Wikilinks. Finally, we train dependency-based embeddings using the generalized word2vec version by Levy and Goldberg (2014), obtaining distributed representations of selectional preferences. To identify fine-grained types of named entities at test time, we first perform entity linking using the system by Heinzerling et al. (2016), then query Freebase (Bollacker et al., 2008) for entity types and apply the mapping to fine-grained types by Ling and Weld. The plausibility of an argument filling a particular predicate slot can now be computed via the cosine similarity of their associated embeddings. For example, in our trained model, the similarity of (Titanic, sank@nsubj) is 0.11 while the similarity of (iceberg, sank@nsubj) is -0.005, indicating that an iceberg sinking is less plausible. 4 Do Selectional Preferences Benefit Coreference Resolution? We now investigate the effect of incorporating selectional preferences, i"
D17-1138,N04-1002,0,0.144028,"Missing"
D17-1138,D08-1094,0,\N,Missing
D17-1138,C90-3063,0,\N,Missing
D17-1138,P07-1028,0,\N,Missing
D17-1138,H05-1004,0,\N,Missing
D17-1138,W01-0703,0,\N,Missing
D17-1138,P16-1061,0,\N,Missing
D17-1138,P16-1060,1,\N,Missing
D17-1138,N04-1037,0,\N,Missing
D18-1018,D08-1031,0,0.0482651,"used here. E.g. there is a dedicated workshop for this topic https: //sites.google.com/view/relsnnlp. 3 We refer to features that are based on linguistic intu2 ∗ This author is currently employed by the Ubiquitous Knowledge Processing (UKP) Lab, Technische Universit¨at Darmstadt, https://www.ukp.tu-darmstadt.de. 193 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 193–203 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics were shown to be important for coreference resolution, e.g. Uryupina (2007) and Bengtson and Roth (2008), state-of-the-art systems no longer use them and mainly rely on word embeddings and deep neural networks. Since all recent systems are using neural networks, we focus on the effect of linguistic features on a neural coreference resolver. The contributions of this paper are as follows: Roth (2008), and Recasens and Hovy (2009) also study the importance of features in coreference resolution. Apart from the mentioned studies, which are mainly about the importance of individual features, studies like Bj¨orkelund and Farkas (2012), Fernandes et al. (2012), and Uryupina and Moschitti (2015) generat"
D18-1018,W12-4503,0,0.0297973,"Missing"
D18-1018,K16-1023,0,0.139917,"set. However, the superiority of a coreference resolver on the CoNLL evaluation sets does not necessarily indicate that it also performs better on new datasets. For instance, the ranking model of Clark and Manning (2016a), the reinforcement learning model of Clark and Manning (2016b) and the end-to-end model of Lee et al. (2017) are three recent coreference resolvers, among which the model of Lee et al. (2017) performs the best and that of Clark and Manning (2016b) performs the second best on the CoNLL development and test sets. However, if we evaluate these systems on the WikiCoref dataset (Ghaddar and Langlais, 2016a), which is consistent with CoNLL with regard to coreference definition and annotation scheme, the performance ranking would be in a reverse order1 . In Moosavi and Strube (2017a), we investigate the generalization problem in coreference resolution and show that there is a large overlap between the coreferring mentions in the CoNLL training and evaluation sets. Therefore, higher scores on the CoNLL evaluation sets do not necessarily indicate a better coreference model. They may be due to better memorization of the training data. As a result, despite the remarkable improvements in coreference"
D18-1018,L16-1021,0,0.298158,"set. However, the superiority of a coreference resolver on the CoNLL evaluation sets does not necessarily indicate that it also performs better on new datasets. For instance, the ranking model of Clark and Manning (2016a), the reinforcement learning model of Clark and Manning (2016b) and the end-to-end model of Lee et al. (2017) are three recent coreference resolvers, among which the model of Lee et al. (2017) performs the best and that of Clark and Manning (2016b) performs the second best on the CoNLL development and test sets. However, if we evaluate these systems on the WikiCoref dataset (Ghaddar and Langlais, 2016a), which is consistent with CoNLL with regard to coreference definition and annotation scheme, the performance ranking would be in a reverse order1 . In Moosavi and Strube (2017a), we investigate the generalization problem in coreference resolution and show that there is a large overlap between the coreferring mentions in the CoNLL training and evaluation sets. Therefore, higher scores on the CoNLL evaluation sets do not necessarily indicate a better coreference model. They may be due to better memorization of the training data. As a result, despite the remarkable improvements in coreference"
D18-1018,P16-1061,0,0.494671,"heoretical Studies gGmbH 2 Research Training Group AIPHES moosavi@ukp.informatik.tu-darmstadt.de, michael.strube@h-its.org Abstract the CoNLL official test set. However, the superiority of a coreference resolver on the CoNLL evaluation sets does not necessarily indicate that it also performs better on new datasets. For instance, the ranking model of Clark and Manning (2016a), the reinforcement learning model of Clark and Manning (2016b) and the end-to-end model of Lee et al. (2017) are three recent coreference resolvers, among which the model of Lee et al. (2017) performs the best and that of Clark and Manning (2016b) performs the second best on the CoNLL development and test sets. However, if we evaluate these systems on the WikiCoref dataset (Ghaddar and Langlais, 2016a), which is consistent with CoNLL with regard to coreference definition and annotation scheme, the performance ranking would be in a reverse order1 . In Moosavi and Strube (2017a), we investigate the generalization problem in coreference resolution and show that there is a large overlap between the coreferring mentions in the CoNLL training and evaluation sets. Therefore, higher scores on the CoNLL evaluation sets do not necessarily indi"
D18-1018,D16-1245,0,0.0947133,"ube (2017a), we investigate the generalization problem in coreference resolution and show that there is a large overlap between the coreferring mentions in the CoNLL training and evaluation sets. Therefore, higher scores on the CoNLL evaluation sets do not necessarily indicate a better coreference model. They may be due to better memorization of the training data. As a result, despite the remarkable improvements in coreference resolution, the use of coreference resolution in other applications is mainly limited to the use of simple rule-based systems, e.g. Lapata and Barzilay (2005),Yu and Ji (2016), and Elsner and Charniak (2008). In this paper, we explore the role of linguistic features for improving generalization. The incorporation of linguistic features is considered as a potential solution for building more generalizable NLP systems2 . While linguistic features3 Coreference resolution is an intermediate step for text understanding. It is used in tasks and domains for which we do not necessarily have coreference annotated corpora. Therefore, generalization is of special importance for coreference resolution. However, while recent coreference resolvers have notable improvements on th"
D18-1018,D17-1018,0,0.498879,"Generalization Capability of Neural Coreference Resolvers Nafise Sadat Moosavi1,2∗ and Michael Strube1 1 Heidelberg Institute for Theoretical Studies gGmbH 2 Research Training Group AIPHES moosavi@ukp.informatik.tu-darmstadt.de, michael.strube@h-its.org Abstract the CoNLL official test set. However, the superiority of a coreference resolver on the CoNLL evaluation sets does not necessarily indicate that it also performs better on new datasets. For instance, the ranking model of Clark and Manning (2016a), the reinforcement learning model of Clark and Manning (2016b) and the end-to-end model of Lee et al. (2017) are three recent coreference resolvers, among which the model of Lee et al. (2017) performs the best and that of Clark and Manning (2016b) performs the second best on the CoNLL development and test sets. However, if we evaluate these systems on the WikiCoref dataset (Ghaddar and Langlais, 2016a), which is consistent with CoNLL with regard to coreference definition and annotation scheme, the performance ranking would be in a reverse order1 . In Moosavi and Strube (2017a), we investigate the generalization problem in coreference resolution and show that there is a large overlap between the core"
D18-1018,D08-1069,0,0.0375366,"the most significant effect. Since pairwise features have the most significant effect, we also perform an experiment in which only pairwise features are incorporated in the “top-pairs” model, i.e. “+pairwise”. The results of “-pairwise” compared to “+pairwise” show that pairwise feature-values have a significant impact, but only when they are considered in combination with other EPM 12 We observe that using larger Θl values will result in many over-specified patterns. 13 Following the previous studies that show different features are of different importance for various types of mentions, e.g. Denis and Baldridge (2008) and Moosavi and Strube (2017b), we mine a separate set of patterns for each type of anaphor. These resulting feature-values are the union of informative feature-values for all types of anaphora. 14 We set the minimum frequency, maximum pattern length and score+ threshold parameters of JIM to 20, 5 and 0.6. 200 e2edeep-coref ranking reinforce top-pairs +EPM single ensemble G&L R 57.72 62.12 56.31 58.23 60.14 59.58 66.06 MUC P F1 R 69.57 63.10 41.42 58.98 60.51 46.98 71.74 63.09 39.78 74.05 65.20 43.33 64.46 62.22 45.20 71.60 65.04 44.64 62.93 64.46 57.73 B3 P 58.30 45.79 61.85 63.90 51.75 60.9"
D18-1018,H05-1004,0,0.0738597,"wn – Animacy: animate, inanimate, unknown – Named entity type: person, location, organization, date, time, number, etc. – Dependency relation: enhanced dependency relation (Schuster and Manning, 2016) of the head word to its parent – POS tags of the first, last, head, two words preceding and following of each mention 5 Pairwise features include: Impact of Linguistic Features In this section, we examine the effect of employing all linguistic features described in Section 4 in a neural coreference resolver, i.e. deep-coref. We use MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998), CEAF e (Luo, 2005), LEA (Moosavi and Strube, 2016), and the CoNLL score (Pradhan et al., 2014), i.e. the average F1 value of MUC, B3 , and CEAF e , for evaluations. The results of employing those features in deepcoref’s “ranking” and “top-pairs” models on the – Head match: both mentions have the same head, e.g. “red hat” and “the hat” – String of one mention is contained in the other, e.g. “Mary’s hat” and “Mary” – Head of one mention is contained in the other, e.g. “Mary’s hat” and “hat” – Acronym, e.g. “Heidelberg Institute for Theoretical Studies” and “HITS” 6 The CoNLL score of the e2e-coref single model on"
D18-1018,J93-1003,0,0.161822,"to the order in A. 6.3 2. Set the current node (T ) to the root. Informativeness Measures We use a discriminative power and an information novelty measure for determining informativeness. We also use a frequency measure which determines the required minimum frequency of a pattern in training samples. It helps to avoid overfitting to the properties of the training data. Discriminative power: We use the G2 likelihood ratio test (Agresti, 2007) in order to choose patterns whose association with the class variable is statistically significant.10 The G2 test is successfully used for text analysis (Dunning, 1993). Information Novelty: A large number of redundant patterns can be generated by adding irrelevant items to a base pattern that is discriminative itself. ¯i ], where ak is the first 3. Consider Xi = [ak |X ¯i = Xi − ak . (ordered) item of xi , and X If T has a child n that contains ak then increment supportn (ak , c(Xi )) by one. Otherwise, create a new node n that contains ak with supportn (ak , c(Xi )) = 1. Add n to the tree as a child of T . ¯i is non-empty, set T to n. Assign Xi = 4. If X ¯i and go to step 3. X As an example, assume D contains the following two samples: X1 ={ana-type=NAM, a"
D18-1018,P14-5010,0,0.00521483,"proper, nominal or pronominal – Fine mention type: proper, definite or indefinite nominal, or the citation form of pronouns The last three features are similar to the discourselevel features discussed by Uryupina (2007), which are created by combining proximity, agreement and salience properties. She shows that such features are useful for resolving pronouns. we estimate proximity by considering the distance of two mentions. The salience is also incorporated by discriminating subject or object antecedents. We do not use any gold information. All features are extracted using Stanford CoreNLP (Manning et al., 2014). – Gender: female, male, neutral, unknown – Number: singular, plural, unknown – Animacy: animate, inanimate, unknown – Named entity type: person, location, organization, date, time, number, etc. – Dependency relation: enhanced dependency relation (Schuster and Manning, 2016) of the head word to its parent – POS tags of the first, last, head, two words preceding and following of each mention 5 Pairwise features include: Impact of Linguistic Features In this section, we examine the effect of employing all linguistic features described in Section 4 in a neural coreference resolver, i.e. deep-cor"
D18-1018,P16-1060,1,0.868597,"ube (2017a), we investigate the generalization problem in coreference resolution and show that there is a large overlap between the coreferring mentions in the CoNLL training and evaluation sets. Therefore, higher scores on the CoNLL evaluation sets do not necessarily indicate a better coreference model. They may be due to better memorization of the training data. As a result, despite the remarkable improvements in coreference resolution, the use of coreference resolution in other applications is mainly limited to the use of simple rule-based systems, e.g. Lapata and Barzilay (2005),Yu and Ji (2016), and Elsner and Charniak (2008). In this paper, we explore the role of linguistic features for improving generalization. The incorporation of linguistic features is considered as a potential solution for building more generalizable NLP systems2 . While linguistic features3 Coreference resolution is an intermediate step for text understanding. It is used in tasks and domains for which we do not necessarily have coreference annotated corpora. Therefore, generalization is of special importance for coreference resolution. However, while recent coreference resolvers have notable improvements on th"
D18-1018,S15-1034,0,0.0657461,"a (2007) and Bengtson and Roth (2008), state-of-the-art systems no longer use them and mainly rely on word embeddings and deep neural networks. Since all recent systems are using neural networks, we focus on the effect of linguistic features on a neural coreference resolver. The contributions of this paper are as follows: Roth (2008), and Recasens and Hovy (2009) also study the importance of features in coreference resolution. Apart from the mentioned studies, which are mainly about the importance of individual features, studies like Bj¨orkelund and Farkas (2012), Fernandes et al. (2012), and Uryupina and Moschitti (2015) generate new features by combining basic features. Bj¨orkelund and Farkas (2012) do not use a systematic approach for combining features. Fernandes et al. (2012) use the Entropy guided Feature Induction (EFI) approach (Fernandes and Milidi´u, 2012) to automatically generate discriminative feature combinations. The first step is to train a decision tree on a dataset in which each sample consists of features describing a mention pair. The EFI approach traverses the tree from the root in a depth-first order and recursively builds feature combinations. Each pattern that is generated by EFI starts"
D18-1018,P17-2003,1,0.804347,"ion Capability of Neural Coreference Resolvers Nafise Sadat Moosavi1,2∗ and Michael Strube1 1 Heidelberg Institute for Theoretical Studies gGmbH 2 Research Training Group AIPHES moosavi@ukp.informatik.tu-darmstadt.de, michael.strube@h-its.org Abstract the CoNLL official test set. However, the superiority of a coreference resolver on the CoNLL evaluation sets does not necessarily indicate that it also performs better on new datasets. For instance, the ranking model of Clark and Manning (2016a), the reinforcement learning model of Clark and Manning (2016b) and the end-to-end model of Lee et al. (2017) are three recent coreference resolvers, among which the model of Lee et al. (2017) performs the best and that of Clark and Manning (2016b) performs the second best on the CoNLL development and test sets. However, if we evaluate these systems on the WikiCoref dataset (Ghaddar and Langlais, 2016a), which is consistent with CoNLL with regard to coreference definition and annotation scheme, the performance ranking would be in a reverse order1 . In Moosavi and Strube (2017a), we investigate the generalization problem in coreference resolution and show that there is a large overlap between the core"
D18-1018,M95-1005,0,0.0587318,"le, male, neutral, unknown – Number: singular, plural, unknown – Animacy: animate, inanimate, unknown – Named entity type: person, location, organization, date, time, number, etc. – Dependency relation: enhanced dependency relation (Schuster and Manning, 2016) of the head word to its parent – POS tags of the first, last, head, two words preceding and following of each mention 5 Pairwise features include: Impact of Linguistic Features In this section, we examine the effect of employing all linguistic features described in Section 4 in a neural coreference resolver, i.e. deep-coref. We use MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998), CEAF e (Luo, 2005), LEA (Moosavi and Strube, 2016), and the CoNLL score (Pradhan et al., 2014), i.e. the average F1 value of MUC, B3 , and CEAF e , for evaluations. The results of employing those features in deepcoref’s “ranking” and “top-pairs” models on the – Head match: both mentions have the same head, e.g. “red hat” and “the hat” – String of one mention is contained in the other, e.g. “Mary’s hat” and “Mary” – Head of one mention is contained in the other, e.g. “Mary’s hat” and “hat” – Acronym, e.g. “Heidelberg Institute for Theoretical Studies” and “HITS”"
D18-1018,W17-1501,1,0.90247,"ranking model of Clark and Manning (2016a), the reinforcement learning model of Clark and Manning (2016b) and the end-to-end model of Lee et al. (2017) are three recent coreference resolvers, among which the model of Lee et al. (2017) performs the best and that of Clark and Manning (2016b) performs the second best on the CoNLL development and test sets. However, if we evaluate these systems on the WikiCoref dataset (Ghaddar and Langlais, 2016a), which is consistent with CoNLL with regard to coreference definition and annotation scheme, the performance ranking would be in a reverse order1 . In Moosavi and Strube (2017a), we investigate the generalization problem in coreference resolution and show that there is a large overlap between the coreferring mentions in the CoNLL training and evaluation sets. Therefore, higher scores on the CoNLL evaluation sets do not necessarily indicate a better coreference model. They may be due to better memorization of the training data. As a result, despite the remarkable improvements in coreference resolution, the use of coreference resolution in other applications is mainly limited to the use of simple rule-based systems, e.g. Lapata and Barzilay (2005),Yu and Ji (2016), a"
D18-1018,P02-1014,0,0.207617,"7)’s thesis is one of the most thorough analyses of linguistically motivated features for coreference resolution. She examines a large set of linguistic features, i.e. string match, syntactic knowledge, semantic compatibility, discourse structure and salience, and investigates their interaction with coreference relations. She shows that even imperfect linguistic features, which are extracted using error-prone preprocessing modules, boost the performance and argues that coreference resolvers could and should benefit from linguistic theories. Her claims are based on analyses on the MUC dataset. Ng and Cardie (2002), Yang et al. (2004), Ponzetto and Strube (2006), Bengtson and Baseline Coreference Resolver deep-coref (Clark and Manning, 2016a) and e2ecoref (Lee et al., 2017) are among the best performing coreference resolvers from which e2ecoref performs better on the CoNLL test set. deepcoref is a pipelined system, i.e. a mention detection first determines the list of candidate mentions with their corresponding features. It contains various coreference models including the mention-pair, mention-ranking, and entity-based models. The mention-ranking model of deepcoref has three variations: (1) “ranking” u"
D18-1018,P15-1137,0,0.0826861,"on and Baseline Coreference Resolver deep-coref (Clark and Manning, 2016a) and e2ecoref (Lee et al., 2017) are among the best performing coreference resolvers from which e2ecoref performs better on the CoNLL test set. deepcoref is a pipelined system, i.e. a mention detection first determines the list of candidate mentions with their corresponding features. It contains various coreference models including the mention-pair, mention-ranking, and entity-based models. The mention-ranking model of deepcoref has three variations: (1) “ranking” uses the slack-rescaled max-margin training objective of Wiseman et al. (2015), (2) “reinforce” is a variation of the “ranking” model in which the hyperparameters are set in a reinforcement learning framework (Sutton and Barto, 1998), and (3) “topitions, e.g. string match, or are acquired from linguistic preprocessing modules, e.g. POS tags, as linguistic features. 4 The EPM code is available at https://github. com/ns-moosavi/epm 5 194 http://www.borgelt.net/jim.html pairs” is a simple variation of the “ranking” model that uses a probabilistic objective function and is used for pretraining the “ranking” model. e2e-coref is an end-to-end system that jointly models mentio"
D18-1018,N06-1025,1,0.688513,"ally motivated features for coreference resolution. She examines a large set of linguistic features, i.e. string match, syntactic knowledge, semantic compatibility, discourse structure and salience, and investigates their interaction with coreference relations. She shows that even imperfect linguistic features, which are extracted using error-prone preprocessing modules, boost the performance and argues that coreference resolvers could and should benefit from linguistic theories. Her claims are based on analyses on the MUC dataset. Ng and Cardie (2002), Yang et al. (2004), Ponzetto and Strube (2006), Bengtson and Baseline Coreference Resolver deep-coref (Clark and Manning, 2016a) and e2ecoref (Lee et al., 2017) are among the best performing coreference resolvers from which e2ecoref performs better on the CoNLL test set. deepcoref is a pipelined system, i.e. a mention detection first determines the list of candidate mentions with their corresponding features. It contains various coreference models including the mention-pair, mention-ranking, and entity-based models. The mention-ranking model of deepcoref has three variations: (1) “ranking” uses the slack-rescaled max-margin training objec"
D18-1018,P04-1017,0,0.0205253,"the most thorough analyses of linguistically motivated features for coreference resolution. She examines a large set of linguistic features, i.e. string match, syntactic knowledge, semantic compatibility, discourse structure and salience, and investigates their interaction with coreference relations. She shows that even imperfect linguistic features, which are extracted using error-prone preprocessing modules, boost the performance and argues that coreference resolvers could and should benefit from linguistic theories. Her claims are based on analyses on the MUC dataset. Ng and Cardie (2002), Yang et al. (2004), Ponzetto and Strube (2006), Bengtson and Baseline Coreference Resolver deep-coref (Clark and Manning, 2016a) and e2ecoref (Lee et al., 2017) are among the best performing coreference resolvers from which e2ecoref performs better on the CoNLL test set. deepcoref is a pipelined system, i.e. a mention detection first determines the list of candidate mentions with their corresponding features. It contains various coreference models including the mention-pair, mention-ranking, and entity-based models. The mention-ranking model of deepcoref has three variations: (1) “ranking” uses the slack-rescal"
D18-1018,P14-2006,1,0.932991,"Missing"
D18-1018,P16-1005,0,0.0133135,"vi and Strube (2017a), we investigate the generalization problem in coreference resolution and show that there is a large overlap between the coreferring mentions in the CoNLL training and evaluation sets. Therefore, higher scores on the CoNLL evaluation sets do not necessarily indicate a better coreference model. They may be due to better memorization of the training data. As a result, despite the remarkable improvements in coreference resolution, the use of coreference resolution in other applications is mainly limited to the use of simple rule-based systems, e.g. Lapata and Barzilay (2005),Yu and Ji (2016), and Elsner and Charniak (2008). In this paper, we explore the role of linguistic features for improving generalization. The incorporation of linguistic features is considered as a potential solution for building more generalizable NLP systems2 . While linguistic features3 Coreference resolution is an intermediate step for text understanding. It is used in tasks and domains for which we do not necessarily have coreference annotated corpora. Therefore, generalization is of special importance for coreference resolution. However, while recent coreference resolvers have notable improvements on th"
D18-1018,L16-1376,0,0.0335953,"Missing"
D18-1464,P05-1018,0,0.297566,"results show that our model achieves the state-of-the-art result for readability assessment on the examined dataset (De Clercq and Hoste, 2016); and the combination of our coherence features with other essay scoring features signiﬁcantly improves the performance of the examined essay scorer (Phandi et al., 2015). 2 Related Work Early work on coherence captures different types of relations: entity-based (Grosz et al., 1995; Barzilay and Lapata, 2008), lexical-based (Beigman Klebanov and Flor, 2013; Somasundaran et al., 2014; Zhang et al., 2015), etc. Among these models, the entity-grid model (Barzilay and Lapata, 2005, 2008) has received a lot of attention. In this model, entities are deﬁned, heuristically, by applying a string match over head nouns of all NPs in a text. The model, then, deﬁnes all possible changes over syntactic roles of entities in adjacent sentences as coherence patterns. The entity-grid model has been extended both by expanding its entity extraction phase (Elsner and Charniak, 2011; Feng and Hirst, 2012) and by deﬁning other types of patterns (Lin et al., 2011; Louis and Nenkova, 2012; Ji and Eisenstein, 2014; Guinaudeau and Strube, 2013). Recently, Tien Nguyen and Joty (2017) fed enti"
D18-1464,J08-1001,0,0.872307,"collections of sentences. A potential application of coherence models is text quality assessment. Examples include readability assessment (Pitler and Nenkova, 2008; Li and Hovy, 2014) and essay scoring (Miltsakaki and Kukich, 2004; Burstein et al., 2010). Here, we address the problem of local coherence modeling, which captures text relatedness at the level of sentence-to-sentence transitions. Several approaches to local coherence modeling have been proposed. Entity-based methods principally relate adjacent sentences by means of entities, which are mentioned as noun phrases, NPs, in sentences (Barzilay and Lapata, 2008; Elsner and Charniak, 2011; Guinaudeau and Strube, ∗ This author is currently employed by the Ubiquitous Knowledge Processing (UKP) Lab, Technische Universit¨at Darmstadt, https://www.ukp.tu-darmstadt.de. 2013; Tien Nguyen and Joty, 2017). Lexical models connect sentences based on semantic relations between words in sentences (Beigman Klebanov and Shamir, 2006; Heilman et al., 2007; Mesgar and Strube, 2016). Both of these approaches suffer from different weaknesses. The entity-based models require an entity detection system, a coreference model, and a syntactic parser. These subsystems need t"
D18-1464,P13-1113,0,0.0676244,"s an ablation. CohEmb has no RNN layer, so the model is built directly on word embeddings. In this model, relations between sentences are made over only content words by eliminating all stop words. 4 Implementation Details Model conﬁgurations. Our model is implemented in PyTorch3 with CUDA 8.0 support. In preprocessing we apply zero-padding to all sentences and documents to make their length equal. The vocabulary is limited to the 4000 most frequent words in the training data and all other words are replaced with the unknown token. We use the pre-trained word embeddings released by Zou et al. (2013), which are employed by 3 https://pytorch.org state-of-the-art essay scoring systems. The dimensions of word embeddings and LSTM cells are 50 and 300, respectively. The convolution layer uses one ﬁlter with size 4. However, optimizing hyperparameters for each task may lead to better performance. For selecting two vectors with the highest similarity from the LSTM states of two adjacent sentences, we capture the similarity between any pair of LSTM states of the sentences as an element in a vector, and then apply a max-pooling layer to this vector of similarities to identify the pair with maximal"
D18-1464,N10-1099,0,0.293294,"ating our coherence model into an essay scorer improves its performance. An important task for evaluating a coherence model is readability assessment (Li and Hovy, 2014; Petersen et al., 2015; Todirascu et al., 2016). The more coherent a text, the faster to read and easier to understand it is. Early readability formulas were based on superﬁcial text features such as average word lengths (Kincaid et al., 1975). These formulas systematically ignore many important factors that affect readability such as discourse coherence (Barzilay and Lapata, 2008). Schwarm and Ostendorf (2005) and Feng et al. (2010) recast readability assessment as a ranking task, and employ different semantic (e.g. language model perplexity scores) and syntactic (e.g. the average number of NPs) features to solve this task. Pitler and Nenkova (2008) show that discourse coherence features are more informative than other features for ranking texts with respect to their readability. Following the related work on coherence modeling (Barzilay and Lapata, 2008; Mesgar and Strube, 2015), we evaluate our coherence model on this task. Another popular task for evaluating coherence models is essay scoring (Beigman Klebanov and Flor"
D18-1464,P16-1046,0,0.0298215,", z) to be differentiable. 4331 sim(f�i , f�i+1 ) d= , l where l is the length of input vectors, which is used to prevent large numbers (Vaswani et al., 2017), and sim is the similarity function (Section 3.2). The task of this layer is to check if the salient information that is shared by two adjacent sentences is salient in the subsequent sentence or not. The last layer of our model is a convolutional layer to automatically extract and represent patterns of semantic changes in a text. CNNs have proven useful for various NLP tasks (Collobert et al., 2011; Kim, 2014; Kalchbrenner et al., 2014; Cheng and Lapata, 2016) because of their effectiveness in identifying patterns in their input (Xu et al., 2015). In the case of coherence, the convolution layer can identify coherence patterns that correlate with ﬁnal tasks (Tien Nguyen and Joty, 2017). We use a temporal narrow convolution by applying a kernel ﬁlter k of width h to a window of h adjacent transitions over sentences to produce a new coherence feature. This ﬁlter is applied to each possible window of transitions in a text to produce a feature map p�, which is a coherence vector. Since we use a standard convolution layer, we explain details of the CNN f"
D18-1464,J16-3004,0,0.318266,"Missing"
D18-1464,K17-1017,0,0.346793,"nd Lapata, 2005, 2008) has received a lot of attention. In this model, entities are deﬁned, heuristically, by applying a string match over head nouns of all NPs in a text. The model, then, deﬁnes all possible changes over syntactic roles of entities in adjacent sentences as coherence patterns. The entity-grid model has been extended both by expanding its entity extraction phase (Elsner and Charniak, 2011; Feng and Hirst, 2012) and by deﬁning other types of patterns (Lin et al., 2011; Louis and Nenkova, 2012; Ji and Eisenstein, 2014; Guinaudeau and Strube, 2013). Recently, Tien Nguyen and Joty (2017) fed entity grid representations of texts to a convolutional neural network (CNN) in order to overcome the limitation of predeﬁned coherence patterns and extract patterns automatically. However, all of these models limit relations between sentences to entities that are shared by sentences. This makes the performance of these models dependent on the performance of other tools like coreference resolution systems and syntactic parsers. Our coherence model, in contrast, is based on relations between any embedded semantic information in sentences, and does not require entity annotations. A similar"
D18-1464,P11-2022,0,0.636344,"potential application of coherence models is text quality assessment. Examples include readability assessment (Pitler and Nenkova, 2008; Li and Hovy, 2014) and essay scoring (Miltsakaki and Kukich, 2004; Burstein et al., 2010). Here, we address the problem of local coherence modeling, which captures text relatedness at the level of sentence-to-sentence transitions. Several approaches to local coherence modeling have been proposed. Entity-based methods principally relate adjacent sentences by means of entities, which are mentioned as noun phrases, NPs, in sentences (Barzilay and Lapata, 2008; Elsner and Charniak, 2011; Guinaudeau and Strube, ∗ This author is currently employed by the Ubiquitous Knowledge Processing (UKP) Lab, Technische Universit¨at Darmstadt, https://www.ukp.tu-darmstadt.de. 2013; Tien Nguyen and Joty, 2017). Lexical models connect sentences based on semantic relations between words in sentences (Beigman Klebanov and Shamir, 2006; Heilman et al., 2007; Mesgar and Strube, 2016). Both of these approaches suffer from different weaknesses. The entity-based models require an entity detection system, a coreference model, and a syntactic parser. These subsystems need to be perfect to gain the be"
D18-1464,C10-2032,0,0.0550761,"that integrating our coherence model into an essay scorer improves its performance. An important task for evaluating a coherence model is readability assessment (Li and Hovy, 2014; Petersen et al., 2015; Todirascu et al., 2016). The more coherent a text, the faster to read and easier to understand it is. Early readability formulas were based on superﬁcial text features such as average word lengths (Kincaid et al., 1975). These formulas systematically ignore many important factors that affect readability such as discourse coherence (Barzilay and Lapata, 2008). Schwarm and Ostendorf (2005) and Feng et al. (2010) recast readability assessment as a ranking task, and employ different semantic (e.g. language model perplexity scores) and syntactic (e.g. the average number of NPs) features to solve this task. Pitler and Nenkova (2008) show that discourse coherence features are more informative than other features for ranking texts with respect to their readability. Following the related work on coherence modeling (Barzilay and Lapata, 2008; Mesgar and Strube, 2015), we evaluate our coherence model on this task. Another popular task for evaluating coherence models is essay scoring (Beigman Klebanov and Flor"
D18-1464,E12-1032,0,0.0759705,"., 1995; Barzilay and Lapata, 2008), lexical-based (Beigman Klebanov and Flor, 2013; Somasundaran et al., 2014; Zhang et al., 2015), etc. Among these models, the entity-grid model (Barzilay and Lapata, 2005, 2008) has received a lot of attention. In this model, entities are deﬁned, heuristically, by applying a string match over head nouns of all NPs in a text. The model, then, deﬁnes all possible changes over syntactic roles of entities in adjacent sentences as coherence patterns. The entity-grid model has been extended both by expanding its entity extraction phase (Elsner and Charniak, 2011; Feng and Hirst, 2012) and by deﬁning other types of patterns (Lin et al., 2011; Louis and Nenkova, 2012; Ji and Eisenstein, 2014; Guinaudeau and Strube, 2013). Recently, Tien Nguyen and Joty (2017) fed entity grid representations of texts to a convolutional neural network (CNN) in order to overcome the limitation of predeﬁned coherence patterns and extract patterns automatically. However, all of these models limit relations between sentences to entities that are shared by sentences. This makes the performance of these models dependent on the performance of other tools like coreference resolution systems and syntac"
D18-1464,J95-2003,0,0.914845,"e if the combination of coherence vectors produced by our model and other essay scoring features proposed by Phandi et al. (2015) improves the performance of the essay scorer. The experimental results show that our model achieves the state-of-the-art result for readability assessment on the examined dataset (De Clercq and Hoste, 2016); and the combination of our coherence features with other essay scoring features signiﬁcantly improves the performance of the examined essay scorer (Phandi et al., 2015). 2 Related Work Early work on coherence captures different types of relations: entity-based (Grosz et al., 1995; Barzilay and Lapata, 2008), lexical-based (Beigman Klebanov and Flor, 2013; Somasundaran et al., 2014; Zhang et al., 2015), etc. Among these models, the entity-grid model (Barzilay and Lapata, 2005, 2008) has received a lot of attention. In this model, entities are deﬁned, heuristically, by applying a string match over head nouns of all NPs in a text. The model, then, deﬁnes all possible changes over syntactic roles of entities in adjacent sentences as coherence patterns. The entity-grid model has been extended both by expanding its entity extraction phase (Elsner and Charniak, 2011; Feng an"
D18-1464,P13-1010,1,0.917724,"s an ablation. CohEmb has no RNN layer, so the model is built directly on word embeddings. In this model, relations between sentences are made over only content words by eliminating all stop words. 4 Implementation Details Model conﬁgurations. Our model is implemented in PyTorch3 with CUDA 8.0 support. In preprocessing we apply zero-padding to all sentences and documents to make their length equal. The vocabulary is limited to the 4000 most frequent words in the training data and all other words are replaced with the unknown token. We use the pre-trained word embeddings released by Zou et al. (2013), which are employed by 3 https://pytorch.org state-of-the-art essay scoring systems. The dimensions of word embeddings and LSTM cells are 50 and 300, respectively. The convolution layer uses one ﬁlter with size 4. However, optimizing hyperparameters for each task may lead to better performance. For selecting two vectors with the highest similarity from the LSTM states of two adjacent sentences, we capture the similarity between any pair of LSTM states of the sentences as an element in a vector, and then apply a max-pooling layer to this vector of similarities to identify the pair with maximal"
D18-1464,N07-1058,0,0.028847,"ions. Several approaches to local coherence modeling have been proposed. Entity-based methods principally relate adjacent sentences by means of entities, which are mentioned as noun phrases, NPs, in sentences (Barzilay and Lapata, 2008; Elsner and Charniak, 2011; Guinaudeau and Strube, ∗ This author is currently employed by the Ubiquitous Knowledge Processing (UKP) Lab, Technische Universit¨at Darmstadt, https://www.ukp.tu-darmstadt.de. 2013; Tien Nguyen and Joty, 2017). Lexical models connect sentences based on semantic relations between words in sentences (Beigman Klebanov and Shamir, 2006; Heilman et al., 2007; Mesgar and Strube, 2016). Both of these approaches suffer from different weaknesses. The entity-based models require an entity detection system, a coreference model, and a syntactic parser. These subsystems need to be perfect to gain the best performance of entity-based coherence models. The weakness of the lexical models is that they consider words independently, i.e. regardless of context in that words appear. More concretely, such lexical models take sentences as a bag of words. Recent deep learning coherence work (Li and Hovy, 2014; Li and Jurafsky, 2017) adopts recursive and recurrent n"
D18-1464,P15-1162,0,0.0250881,"Missing"
D18-1464,P14-1002,0,0.0216817,"2014; Zhang et al., 2015), etc. Among these models, the entity-grid model (Barzilay and Lapata, 2005, 2008) has received a lot of attention. In this model, entities are deﬁned, heuristically, by applying a string match over head nouns of all NPs in a text. The model, then, deﬁnes all possible changes over syntactic roles of entities in adjacent sentences as coherence patterns. The entity-grid model has been extended both by expanding its entity extraction phase (Elsner and Charniak, 2011; Feng and Hirst, 2012) and by deﬁning other types of patterns (Lin et al., 2011; Louis and Nenkova, 2012; Ji and Eisenstein, 2014; Guinaudeau and Strube, 2013). Recently, Tien Nguyen and Joty (2017) fed entity grid representations of texts to a convolutional neural network (CNN) in order to overcome the limitation of predeﬁned coherence patterns and extract patterns automatically. However, all of these models limit relations between sentences to entities that are shared by sentences. This makes the performance of these models dependent on the performance of other tools like coreference resolution systems and syntactic parsers. Our coherence model, in contrast, is based on relations between any embedded semantic informat"
D18-1464,P14-1062,0,0.044092,"as g(z) = max(0, z) − min(0, z) to be differentiable. 4331 sim(f�i , f�i+1 ) d= , l where l is the length of input vectors, which is used to prevent large numbers (Vaswani et al., 2017), and sim is the similarity function (Section 3.2). The task of this layer is to check if the salient information that is shared by two adjacent sentences is salient in the subsequent sentence or not. The last layer of our model is a convolutional layer to automatically extract and represent patterns of semantic changes in a text. CNNs have proven useful for various NLP tasks (Collobert et al., 2011; Kim, 2014; Kalchbrenner et al., 2014; Cheng and Lapata, 2016) because of their effectiveness in identifying patterns in their input (Xu et al., 2015). In the case of coherence, the convolution layer can identify coherence patterns that correlate with ﬁnal tasks (Tien Nguyen and Joty, 2017). We use a temporal narrow convolution by applying a kernel ﬁlter k of width h to a window of h adjacent transitions over sentences to produce a new coherence feature. This ﬁlter is applied to each possible window of transitions in a text to produce a feature map p�, which is a coherence vector. Since we use a standard convolution layer, we exp"
D18-1464,D14-1181,0,0.00839935,"mplemented as g(z) = max(0, z) − min(0, z) to be differentiable. 4331 sim(f�i , f�i+1 ) d= , l where l is the length of input vectors, which is used to prevent large numbers (Vaswani et al., 2017), and sim is the similarity function (Section 3.2). The task of this layer is to check if the salient information that is shared by two adjacent sentences is salient in the subsequent sentence or not. The last layer of our model is a convolutional layer to automatically extract and represent patterns of semantic changes in a text. CNNs have proven useful for various NLP tasks (Collobert et al., 2011; Kim, 2014; Kalchbrenner et al., 2014; Cheng and Lapata, 2016) because of their effectiveness in identifying patterns in their input (Xu et al., 2015). In the case of coherence, the convolution layer can identify coherence patterns that correlate with ﬁnal tasks (Tien Nguyen and Joty, 2017). We use a temporal narrow convolution by applying a kernel ﬁlter k of width h to a window of h adjacent transitions over sentences to produce a new coherence feature. This ﬁlter is applied to each possible window of transitions in a text to produce a feature map p�, which is a coherence vector. Since we use a standar"
D18-1464,W18-5023,0,0.146717,"tences in the window. Text coherence is evaluated by sliding the window over sentences and aggregating their coherence probabilities. Similarly, Li and Jurafsky (2017) study the same model at a larger scale and use a sequence-to-sequence approach in which the model is trained to generate the next sentence given the current sentence and vice versa. Our approach differs from these methods; we represent coherence by a vector of coherence patterns. Moreover, our model takes distant relations between words in a text into account by relating two semantic states of sentences that are highly similar. Lai and Tetreault (2018) compare the performance of the aforementioned coherence models on texts from different domains. They conclude that the neural coherence models, which are explained above, surpass examined nonneural coherence models such as the entity-based models and the lexical-based model. Unlike their 4329 evaluation method, which predicts the coherence level of a text, we rank two texts with respect to their coherence levels for the readability assessment task. We also show that integrating our coherence model into an essay scorer improves its performance. An important task for evaluating a coherence mode"
D18-1464,D14-1218,0,0.689871,"sentences. Our experiments demonstrate that our approach is beneﬁcial for two downstream tasks: Readability assessment, in which our model achieves new state-of-the-art results; and essay scoring, in which the combination of our coherence vectors and other taskdependent features signiﬁcantly improves the performance of a strong essay scorer. 1 Introduction Coherence is a key factor that distinguishes well-written texts from random collections of sentences. A potential application of coherence models is text quality assessment. Examples include readability assessment (Pitler and Nenkova, 2008; Li and Hovy, 2014) and essay scoring (Miltsakaki and Kukich, 2004; Burstein et al., 2010). Here, we address the problem of local coherence modeling, which captures text relatedness at the level of sentence-to-sentence transitions. Several approaches to local coherence modeling have been proposed. Entity-based methods principally relate adjacent sentences by means of entities, which are mentioned as noun phrases, NPs, in sentences (Barzilay and Lapata, 2008; Elsner and Charniak, 2011; Guinaudeau and Strube, ∗ This author is currently employed by the Ubiquitous Knowledge Processing (UKP) Lab, Technische Universit"
D18-1464,D17-1019,0,0.219332,"Beigman Klebanov and Shamir, 2006; Heilman et al., 2007; Mesgar and Strube, 2016). Both of these approaches suffer from different weaknesses. The entity-based models require an entity detection system, a coreference model, and a syntactic parser. These subsystems need to be perfect to gain the best performance of entity-based coherence models. The weakness of the lexical models is that they consider words independently, i.e. regardless of context in that words appear. More concretely, such lexical models take sentences as a bag of words. Recent deep learning coherence work (Li and Hovy, 2014; Li and Jurafsky, 2017) adopts recursive and recurrent neural networks for computing semantic vectors for sentences. Coherence models that use recursive neural networks suffer from a severe dependence on external resources, e.g. a syntactic parser to construct their recursion structure. Coherence models that purely rely on the recurrent neural networks process words sequentially within a text. However, in such models, long-distance dependencies between words cannot be captured effectively due to the limits of the memorization capability of recurrent networks. Our motivation is to overcome these limitations. We use t"
D18-1464,P11-1100,0,0.414571,"Missing"
D18-1464,D12-1106,0,0.152367,"013; Somasundaran et al., 2014; Zhang et al., 2015), etc. Among these models, the entity-grid model (Barzilay and Lapata, 2005, 2008) has received a lot of attention. In this model, entities are deﬁned, heuristically, by applying a string match over head nouns of all NPs in a text. The model, then, deﬁnes all possible changes over syntactic roles of entities in adjacent sentences as coherence patterns. The entity-grid model has been extended both by expanding its entity extraction phase (Elsner and Charniak, 2011; Feng and Hirst, 2012) and by deﬁning other types of patterns (Lin et al., 2011; Louis and Nenkova, 2012; Ji and Eisenstein, 2014; Guinaudeau and Strube, 2013). Recently, Tien Nguyen and Joty (2017) fed entity grid representations of texts to a convolutional neural network (CNN) in order to overcome the limitation of predeﬁned coherence patterns and extract patterns automatically. However, all of these models limit relations between sentences to entities that are shared by sentences. This makes the performance of these models dependent on the performance of other tools like coreference resolution systems and syntactic parsers. Our coherence model, in contrast, is based on relations between any e"
D18-1464,S15-1036,1,0.950037,"e, our model incorporates words in their sentence context and models (roughly) distant relations between words. We evaluate our model on two tasks: readability assessment and essay scoring. Both have been frequently used for coherence evaluation (Barzilay and Lapata, 2008; Miltsakaki and Kukich, 2004). Readability assessment is a ranking task where we compare the rankings given by the model against human judgments. Essay scoring is a regression task, in which we investigate if the combination of coherence vectors produced by our model and other essay scoring features proposed by Phandi et al. (2015) improves the performance of the essay scorer. The experimental results show that our model achieves the state-of-the-art result for readability assessment on the examined dataset (De Clercq and Hoste, 2016); and the combination of our coherence features with other essay scoring features signiﬁcantly improves the performance of the examined essay scorer (Phandi et al., 2015). 2 Related Work Early work on coherence captures different types of relations: entity-based (Grosz et al., 1995; Barzilay and Lapata, 2008), lexical-based (Beigman Klebanov and Flor, 2013; Somasundaran et al., 2014; Zhang"
D18-1464,N16-1167,1,0.832814,"es to local coherence modeling have been proposed. Entity-based methods principally relate adjacent sentences by means of entities, which are mentioned as noun phrases, NPs, in sentences (Barzilay and Lapata, 2008; Elsner and Charniak, 2011; Guinaudeau and Strube, ∗ This author is currently employed by the Ubiquitous Knowledge Processing (UKP) Lab, Technische Universit¨at Darmstadt, https://www.ukp.tu-darmstadt.de. 2013; Tien Nguyen and Joty, 2017). Lexical models connect sentences based on semantic relations between words in sentences (Beigman Klebanov and Shamir, 2006; Heilman et al., 2007; Mesgar and Strube, 2016). Both of these approaches suffer from different weaknesses. The entity-based models require an entity detection system, a coreference model, and a syntactic parser. These subsystems need to be perfect to gain the best performance of entity-based coherence models. The weakness of the lexical models is that they consider words independently, i.e. regardless of context in that words appear. More concretely, such lexical models take sentences as a bag of words. Recent deep learning coherence work (Li and Hovy, 2014; Li and Jurafsky, 2017) adopts recursive and recurrent neural networks for computi"
D18-1464,D15-1049,0,0.289724,"ms. Furthermore, our model incorporates words in their sentence context and models (roughly) distant relations between words. We evaluate our model on two tasks: readability assessment and essay scoring. Both have been frequently used for coherence evaluation (Barzilay and Lapata, 2008; Miltsakaki and Kukich, 2004). Readability assessment is a ranking task where we compare the rankings given by the model against human judgments. Essay scoring is a regression task, in which we investigate if the combination of coherence vectors produced by our model and other essay scoring features proposed by Phandi et al. (2015) improves the performance of the essay scorer. The experimental results show that our model achieves the state-of-the-art result for readability assessment on the examined dataset (De Clercq and Hoste, 2016); and the combination of our coherence features with other essay scoring features signiﬁcantly improves the performance of the examined essay scorer (Phandi et al., 2015). 2 Related Work Early work on coherence captures different types of relations: entity-based (Grosz et al., 1995; Barzilay and Lapata, 2008), lexical-based (Beigman Klebanov and Flor, 2013; Somasundaran et al., 2014; Zhang"
D18-1464,D08-1020,0,0.887351,"ion that relates adjacent sentences. Our experiments demonstrate that our approach is beneﬁcial for two downstream tasks: Readability assessment, in which our model achieves new state-of-the-art results; and essay scoring, in which the combination of our coherence vectors and other taskdependent features signiﬁcantly improves the performance of a strong essay scorer. 1 Introduction Coherence is a key factor that distinguishes well-written texts from random collections of sentences. A potential application of coherence models is text quality assessment. Examples include readability assessment (Pitler and Nenkova, 2008; Li and Hovy, 2014) and essay scoring (Miltsakaki and Kukich, 2004; Burstein et al., 2010). Here, we address the problem of local coherence modeling, which captures text relatedness at the level of sentence-to-sentence transitions. Several approaches to local coherence modeling have been proposed. Entity-based methods principally relate adjacent sentences by means of entities, which are mentioned as noun phrases, NPs, in sentences (Barzilay and Lapata, 2008; Elsner and Charniak, 2011; Guinaudeau and Strube, ∗ This author is currently employed by the Ubiquitous Knowledge Processing (UKP) Lab,"
D18-1464,P05-1065,0,0.138056,"ity assessment task. We also show that integrating our coherence model into an essay scorer improves its performance. An important task for evaluating a coherence model is readability assessment (Li and Hovy, 2014; Petersen et al., 2015; Todirascu et al., 2016). The more coherent a text, the faster to read and easier to understand it is. Early readability formulas were based on superﬁcial text features such as average word lengths (Kincaid et al., 1975). These formulas systematically ignore many important factors that affect readability such as discourse coherence (Barzilay and Lapata, 2008). Schwarm and Ostendorf (2005) and Feng et al. (2010) recast readability assessment as a ranking task, and employ different semantic (e.g. language model perplexity scores) and syntactic (e.g. the average number of NPs) features to solve this task. Pitler and Nenkova (2008) show that discourse coherence features are more informative than other features for ranking texts with respect to their readability. Following the related work on coherence modeling (Barzilay and Lapata, 2008; Mesgar and Strube, 2015), we evaluate our coherence model on this task. Another popular task for evaluating coherence models is essay scoring (Be"
D18-1464,C14-1090,0,0.435396,"proposed by Phandi et al. (2015) improves the performance of the essay scorer. The experimental results show that our model achieves the state-of-the-art result for readability assessment on the examined dataset (De Clercq and Hoste, 2016); and the combination of our coherence features with other essay scoring features signiﬁcantly improves the performance of the examined essay scorer (Phandi et al., 2015). 2 Related Work Early work on coherence captures different types of relations: entity-based (Grosz et al., 1995; Barzilay and Lapata, 2008), lexical-based (Beigman Klebanov and Flor, 2013; Somasundaran et al., 2014; Zhang et al., 2015), etc. Among these models, the entity-grid model (Barzilay and Lapata, 2005, 2008) has received a lot of attention. In this model, entities are deﬁned, heuristically, by applying a string match over head nouns of all NPs in a text. The model, then, deﬁnes all possible changes over syntactic roles of entities in adjacent sentences as coherence patterns. The entity-grid model has been extended both by expanding its entity extraction phase (Elsner and Charniak, 2011; Feng and Hirst, 2012) and by deﬁning other types of patterns (Lin et al., 2011; Louis and Nenkova, 2012; Ji an"
D18-1464,D16-1193,0,0.0518441,"ys 1783 1800 1726 1772 1805 1800 1569 723 Genre argumentative argumentative response response response response narrative narrative Avg. Len. 350 350 150 150 150 150 250 650 Table 2: Some properties of the dataset used for the essay scoring experiment. Evaluation metric. ASAP adopted Quadratic Weighted Kappa (QWK) as the ofﬁcial evaluation metric. This metric measures the agreement between scores predicted by a system and scores assigned by humans. QWK considers chance agreements and penalizes large disagreements more than small agreements. We use an implementation of QWK that is described in Taghipour and Ng (2016). The formulation of QWK are explained in Appendix D. The ﬁnal reported QWK is the average over QWKs of all prompts. We perform a paired t-test to determine if improvements are statistically signiﬁcant (p &lt; .05). Results. Table 3 shows the results of different systems for the essay scoring task. Both EASE & CohEmb, and EASE & CohLSTM signiﬁcantly improve EASE, conﬁrming that our proposed representation for coherence is beneﬁcial for essay scoring and 6 https://www.kaggle.com/c/asap-aes/ data improves the performance of the examined essay scoring system. Our model does not beat the state-of-the"
D18-1464,P17-1121,0,0.716259,"t al., 2010). Here, we address the problem of local coherence modeling, which captures text relatedness at the level of sentence-to-sentence transitions. Several approaches to local coherence modeling have been proposed. Entity-based methods principally relate adjacent sentences by means of entities, which are mentioned as noun phrases, NPs, in sentences (Barzilay and Lapata, 2008; Elsner and Charniak, 2011; Guinaudeau and Strube, ∗ This author is currently employed by the Ubiquitous Knowledge Processing (UKP) Lab, Technische Universit¨at Darmstadt, https://www.ukp.tu-darmstadt.de. 2013; Tien Nguyen and Joty, 2017). Lexical models connect sentences based on semantic relations between words in sentences (Beigman Klebanov and Shamir, 2006; Heilman et al., 2007; Mesgar and Strube, 2016). Both of these approaches suffer from different weaknesses. The entity-based models require an entity detection system, a coreference model, and a syntactic parser. These subsystems need to be perfect to gain the best performance of entity-based coherence models. The weakness of the lexical models is that they consider words independently, i.e. regardless of context in that words appear. More concretely, such lexical models"
D18-1464,C16-1094,0,0.301701,"volutional neural network (CNN) in order to overcome the limitation of predeﬁned coherence patterns and extract patterns automatically. However, all of these models limit relations between sentences to entities that are shared by sentences. This makes the performance of these models dependent on the performance of other tools like coreference resolution systems and syntactic parsers. Our coherence model, in contrast, is based on relations between any embedded semantic information in sentences, and does not require entity annotations. A similar approach to ours is proposed by Mesgar and Strube (2016). Their approach encodes lexical relations between sentences in a text via a graph. Sentences are encoded by nodes, and lexical semantic relations between sentences are represented by edges. Coherence patterns are obtained by applying a subgraph mining method to graph representations of all texts in a corpus. This model involves words individually and independent of their sentence context. Our model uses a RNN layer over words in sentences to incorporate context information. Our approach for extracting coherence patterns also differs from this model as we employ CNNs rather than graph mining."
D18-1464,D16-1157,0,0.0230961,"and Hi−1 . Since this is the details of implementation, we explain matrix-based equations in Appendix B. The absolute value in the similarity function is used to encode semantic relatedness between associated information with vectors, which is independent of the sign of the similarity function (Manning and Sch¨utze, 1999). We represent semantic information that relates two adjacent sentences by accumulating its selected LSTM states in the corresponding sentences. Since averaging in the vector space is an effective way to accumulate information represented in some vectors (Iyyer et al., 2015; Wieting et al., 2016), we compute the average of two identiﬁed vectors among the LSTM states of two adjacent sentences to represent semantic information shared by the sentences. More concretely, the vector representation of what relates sentence si to its immediately preceding sentence is obtained by averaging a vector of Hi and a vector of Hi−1 that are identiﬁed as highly similar: �u + �v f�i = avg(�u, �v ) = , 2 where �u and �v are selected vectors. f�i is the vector representation of what connects si to its immediately preceding sentence. 3.3 Coherence Representations Since sentences in a coherent text are abo"
D18-1464,W15-0626,0,0.166936,"e, our model incorporates words in their sentence context and models (roughly) distant relations between words. We evaluate our model on two tasks: readability assessment and essay scoring. Both have been frequently used for coherence evaluation (Barzilay and Lapata, 2008; Miltsakaki and Kukich, 2004). Readability assessment is a ranking task where we compare the rankings given by the model against human judgments. Essay scoring is a regression task, in which we investigate if the combination of coherence vectors produced by our model and other essay scoring features proposed by Phandi et al. (2015) improves the performance of the essay scorer. The experimental results show that our model achieves the state-of-the-art result for readability assessment on the examined dataset (De Clercq and Hoste, 2016); and the combination of our coherence features with other essay scoring features signiﬁcantly improves the performance of the examined essay scorer (Phandi et al., 2015). 2 Related Work Early work on coherence captures different types of relations: entity-based (Grosz et al., 1995; Barzilay and Lapata, 2008), lexical-based (Beigman Klebanov and Flor, 2013; Somasundaran et al., 2014; Zhang"
D18-1464,N15-1115,0,0.101882,"(2015) improves the performance of the essay scorer. The experimental results show that our model achieves the state-of-the-art result for readability assessment on the examined dataset (De Clercq and Hoste, 2016); and the combination of our coherence features with other essay scoring features signiﬁcantly improves the performance of the examined essay scorer (Phandi et al., 2015). 2 Related Work Early work on coherence captures different types of relations: entity-based (Grosz et al., 1995; Barzilay and Lapata, 2008), lexical-based (Beigman Klebanov and Flor, 2013; Somasundaran et al., 2014; Zhang et al., 2015), etc. Among these models, the entity-grid model (Barzilay and Lapata, 2005, 2008) has received a lot of attention. In this model, entities are deﬁned, heuristically, by applying a string match over head nouns of all NPs in a text. The model, then, deﬁnes all possible changes over syntactic roles of entities in adjacent sentences as coherence patterns. The entity-grid model has been extended both by expanding its entity extraction phase (Elsner and Charniak, 2011; Feng and Hirst, 2012) and by deﬁning other types of patterns (Lin et al., 2011; Louis and Nenkova, 2012; Ji and Eisenstein, 2014; G"
D18-1464,D13-1141,0,0.0229785,"hEmb that is an ablation. CohEmb has no RNN layer, so the model is built directly on word embeddings. In this model, relations between sentences are made over only content words by eliminating all stop words. 4 Implementation Details Model conﬁgurations. Our model is implemented in PyTorch3 with CUDA 8.0 support. In preprocessing we apply zero-padding to all sentences and documents to make their length equal. The vocabulary is limited to the 4000 most frequent words in the training data and all other words are replaced with the unknown token. We use the pre-trained word embeddings released by Zou et al. (2013), which are employed by 3 https://pytorch.org state-of-the-art essay scoring systems. The dimensions of word embeddings and LSTM cells are 50 and 300, respectively. The convolution layer uses one ﬁlter with size 4. However, optimizing hyperparameters for each task may lead to better performance. For selecting two vectors with the highest similarity from the LSTM states of two adjacent sentences, we capture the similarity between any pair of LSTM states of the sentences as an element in a vector, and then apply a max-pooling layer to this vector of similarities to identify the pair with maximal"
D19-5542,W18-1112,0,0.247006,"Missing"
D19-5542,W16-0307,0,0.054813,"Missing"
D19-5542,W14-4012,0,0.16458,"Missing"
D19-5542,P16-2096,0,0.0244129,"., Linguistic Inquiry and Word Count (LIWC) (Pennebaker et al., 2001) – a widely used feature extractor for various studies regarding mental health (Rude et al., 2004; Coppersmith et al., 2014; Sekuli´c et al., 2018; Zomick et al., 2019). Recently, Song et al. (2018) built a feature attention network for depression detection on Reddit, showing high interpretability, but low improvement in accuracy. Orabi et al. (2018) concatenate all the tweets of a Twitter user in a single document and experiment with various deep neu4.1 Ethical considerations Acknowledging the social impact of NLP research (Hovy and Spruit, 2016), mental health analysis must be approached carefully as it is an extremely ˇ sensitive matter (Suster et al., 2017). In order to acquire the SMHD dataset, we comply to the Data Usage Agreement, made to protect the users’ privacy. We do not attempt to contact the users in the dataset, nor identify or link them with other user information. 5 Conclusion In this study, we experimented with hierarchical attention networks for the task of predicting mental health status of Reddit users. For the disorders with a fair amount of diagnosed users, a HAN proves to be better than the baselines. However, t"
D19-5542,C18-1126,0,0.141859,"Missing"
D19-5542,W18-0607,0,0.0346572,"ategories. We observe similar patterns in features shown 324 Pers. pronouns Affective Social Biological Informal Other unigrams bigrams I, my, her, your, they like, nice, love, bad friend, boyfriend, girl, guy pain, sex, skin, sleep, porn omg, lol, shit, fuck, cool advice, please, reddit I’ve never, your thoughts I love my dad, my girlfriend, my ex your pain, a doctor, a therapist tl dr, holy shit thank you, your advice ral models for depression detection. Some of the previous studies use deep learning methods on a post level to infer general information about a user (Kshirsagar et al., 2017; Ive et al., 2018; Ruder et al., 2016), or detect different mental health concepts in posts themselves (Rojas-Barahona et al., 2018), while we focus on utilizing all of the users’ text. Yates et al. (2017) use a CNN on a postlevel to extract features, which are then concatenated to get a user representation used for selfharm and depression assessment. A CNN requires a fixed length of posts, putting constraints on the data available to the model, while a HAN utilizes all of the data from posts of arbitrary lengths. A social media user can be modeled as collection of their posts, so we look at neural models for"
D19-5542,W14-3207,0,0.0680092,"e Twitter data (Coppersmith et al., 2015a, 2014; Benton et al., 2017; Coppersmith et al., 2015b), a recent stream turns to Reddit as a richer source of high-volume data (De Choudhury and De, 2014; Shen and Rudzicz, 2017; ˇ Gjurkovi´c and Snajder, 2018; Cohan et al., 2018; Sekuli´c et al., 2018; Zirikly et al., 2019). Previous approaches to author’s mental health prediction usually relied on linguistic and stylistic features, e.g., Linguistic Inquiry and Word Count (LIWC) (Pennebaker et al., 2001) – a widely used feature extractor for various studies regarding mental health (Rude et al., 2004; Coppersmith et al., 2014; Sekuli´c et al., 2018; Zomick et al., 2019). Recently, Song et al. (2018) built a feature attention network for depression detection on Reddit, showing high interpretability, but low improvement in accuracy. Orabi et al. (2018) concatenate all the tweets of a Twitter user in a single document and experiment with various deep neu4.1 Ethical considerations Acknowledging the social impact of NLP research (Hovy and Spruit, 2016), mental health analysis must be approached carefully as it is an extremely ˇ sensitive matter (Suster et al., 2017). In order to acquire the SMHD dataset, we comply to t"
D19-5542,W15-1201,0,0.0439065,"processes, Cohan et al. (2018) report significant differences between depressed and control group, similar to some other disorders. Except the above mentioned words and their abbreviations, among most commonly attended are swear words, as well as other forms of informal language. The attention mechanism’s weighting suggests that words and phrases proved important in previous studies, using lexical features and linear models, are relevant for the HAN as well. 4 Related Work In recent years, social media has been a valuable source for psychological research. While most studies use Twitter data (Coppersmith et al., 2015a, 2014; Benton et al., 2017; Coppersmith et al., 2015b), a recent stream turns to Reddit as a richer source of high-volume data (De Choudhury and De, 2014; Shen and Rudzicz, 2017; ˇ Gjurkovi´c and Snajder, 2018; Cohan et al., 2018; Sekuli´c et al., 2018; Zirikly et al., 2019). Previous approaches to author’s mental health prediction usually relied on linguistic and stylistic features, e.g., Linguistic Inquiry and Word Count (LIWC) (Pennebaker et al., 2001) – a widely used feature extractor for various studies regarding mental health (Rude et al., 2004; Coppersmith et al., 2014; Sekuli´c et al"
D19-5542,Y18-1070,0,0.0177271,"et al., 2015b), a recent stream turns to Reddit as a richer source of high-volume data (De Choudhury and De, 2014; Shen and Rudzicz, 2017; ˇ Gjurkovi´c and Snajder, 2018; Cohan et al., 2018; Sekuli´c et al., 2018; Zirikly et al., 2019). Previous approaches to author’s mental health prediction usually relied on linguistic and stylistic features, e.g., Linguistic Inquiry and Word Count (LIWC) (Pennebaker et al., 2001) – a widely used feature extractor for various studies regarding mental health (Rude et al., 2004; Coppersmith et al., 2014; Sekuli´c et al., 2018; Zomick et al., 2019). Recently, Song et al. (2018) built a feature attention network for depression detection on Reddit, showing high interpretability, but low improvement in accuracy. Orabi et al. (2018) concatenate all the tweets of a Twitter user in a single document and experiment with various deep neu4.1 Ethical considerations Acknowledging the social impact of NLP research (Hovy and Spruit, 2016), mental health analysis must be approached carefully as it is an extremely ˇ sensitive matter (Suster et al., 2017). In order to acquire the SMHD dataset, we comply to the Data Usage Agreement, made to protect the users’ privacy. We do not atte"
D19-5542,W17-3108,0,0.020679,"er the most common LIWC categories. We observe similar patterns in features shown 324 Pers. pronouns Affective Social Biological Informal Other unigrams bigrams I, my, her, your, they like, nice, love, bad friend, boyfriend, girl, guy pain, sex, skin, sleep, porn omg, lol, shit, fuck, cool advice, please, reddit I’ve never, your thoughts I love my dad, my girlfriend, my ex your pain, a doctor, a therapist tl dr, holy shit thank you, your advice ral models for depression detection. Some of the previous studies use deep learning methods on a post level to infer general information about a user (Kshirsagar et al., 2017; Ive et al., 2018; Ruder et al., 2016), or detect different mental health concepts in posts themselves (Rojas-Barahona et al., 2018), while we focus on utilizing all of the users’ text. Yates et al. (2017) use a CNN on a postlevel to extract features, which are then concatenated to get a user representation used for selfharm and depression assessment. A CNN requires a fixed length of posts, putting constraints on the data available to the model, while a HAN utilizes all of the data from posts of arbitrary lengths. A social media user can be modeled as collection of their posts, so we look at"
D19-5542,W18-0609,0,0.144102,"and Snajder, 2018; Cohan et al., 2018; Sekuli´c et al., 2018; Zirikly et al., 2019). Previous approaches to author’s mental health prediction usually relied on linguistic and stylistic features, e.g., Linguistic Inquiry and Word Count (LIWC) (Pennebaker et al., 2001) – a widely used feature extractor for various studies regarding mental health (Rude et al., 2004; Coppersmith et al., 2014; Sekuli´c et al., 2018; Zomick et al., 2019). Recently, Song et al. (2018) built a feature attention network for depression detection on Reddit, showing high interpretability, but low improvement in accuracy. Orabi et al. (2018) concatenate all the tweets of a Twitter user in a single document and experiment with various deep neu4.1 Ethical considerations Acknowledging the social impact of NLP research (Hovy and Spruit, 2016), mental health analysis must be approached carefully as it is an extremely ˇ sensitive matter (Suster et al., 2017). In order to acquire the SMHD dataset, we comply to the Data Usage Agreement, made to protect the users’ privacy. We do not attempt to contact the users in the dataset, nor identify or link them with other user information. 5 Conclusion In this study, we experimented with hierarchi"
D19-5542,W17-1610,0,0.0186737,"udies regarding mental health (Rude et al., 2004; Coppersmith et al., 2014; Sekuli´c et al., 2018; Zomick et al., 2019). Recently, Song et al. (2018) built a feature attention network for depression detection on Reddit, showing high interpretability, but low improvement in accuracy. Orabi et al. (2018) concatenate all the tweets of a Twitter user in a single document and experiment with various deep neu4.1 Ethical considerations Acknowledging the social impact of NLP research (Hovy and Spruit, 2016), mental health analysis must be approached carefully as it is an extremely ˇ sensitive matter (Suster et al., 2017). In order to acquire the SMHD dataset, we comply to the Data Usage Agreement, made to protect the users’ privacy. We do not attempt to contact the users in the dataset, nor identify or link them with other user information. 5 Conclusion In this study, we experimented with hierarchical attention networks for the task of predicting mental health status of Reddit users. For the disorders with a fair amount of diagnosed users, a HAN proves to be better than the baselines. However, the results worsen as the data available decreases, suggesting that traditional approaches remain better for smaller"
D19-5542,N16-1174,0,0.693001,"}@h-its.org Abstract Diagnoses (SMHD) dataset (Cohan et al., 2018), consisting of thousands of Reddit users diagnosed with one or more mental illnesses. The contribution of our work is threefold. First, we adapt a deep neural model, proved to be successful in large-scale document classification, for user classification on social media, outperforming previously set benchmarks for four out of nine disorders. In contrast to the majority of preceding studies on mental health prediction in social media, which relied mostly on traditional classifiers, we employ Hierarchical Attention Network (HAN) (Yang et al., 2016). Second, we explore the limitations of the model in terms of data needed for successful classification, specifically, the number of users and number of posts per user. Third, through the attention mechanism of the model, we analyze the most relevant phrases for the classification and compare them to previous work in the field. We find similarities between lexical features and ngrams identified by the attention mechanism, supporting previous analyses. Mental health poses a significant challenge for an individual’s well-being. Text analysis of rich resources, like social media, can contribute t"
D19-5542,N16-1000,0,0.43166,"ayer. It employs GRU-based sequence encoders (Cho et al., 2014) on sentence and document level, yielding a document representation in the end. The word sequence encoder produces a representation of a given sentence, which then is forwarded to a sentence sequence encoder that, given a sequence of encoded sentences, returns a document representation. Both, word sequence and sentence sequence encoders, apply attention mechanisms on top to help the encoder more accurately aggregate the representation of given sequence. For details of the architecture we refer the interested readers to Yang et al. (2016). In this work, we model a user as a document, enabling an intuitive adaptation of the HAN. Just as a document is a sequence of sentences, we propose to model a social media user as a sequence of posts. Similarly, we identify posts as sentences, both being a sequence of tokens. This interpretation enables us to apply the HAN, which had great success in document classification, to user classification on social media. Table 1: Number of users in SMHD dataset per condition and the average number of posts per user (with std.). Reddit) they post in. Diagnosed users’ language is normalized by removi"
D19-5542,D14-1162,0,0.0899145,"used in Cohan et al. (2018). Multiple sub-datasets with different control groups not only provide us with unbiased results, but also show how results of a Results Experimental Setup The HAN uses two layers of bidirectional GRU units with hidden size of 150, each of them followed by a 100 dimensional attention mechanism. The first layer encodes posts, while the second one encodes a user as a sequence of encoded posts. The output layer is 50-dimensional fullyconnected network, with binary cross entropy as a loss function. We initialize the input layer with 300 dimensional GloVe word embeddings (Pennington et al., 2014). We train the model with Adam (Kingma and Ba, 2014), with an initial learning rate of 10−4 and a batch size of 32 for 50 epochs. The model that proves best on the development set is selected. We implement the baselines as in Cohan et al. (2018). Logistic regression and the linear SVM 323 Logistic Regression Linear SVM Supervised FastText HAN Depression ADHD Anxiety Bipolar PTSD Autism OCD Schizo Eating 59.00 58.64 58.38 68.28 51.02 50.08 48.80 64.27 62.34 61.69 60.17 69.24 61.87 61.30 56.53 67.42 69.34 69.91 61.08 68.59 55.57 55.35 49.52 53.09 59.49 58.56 54.16 58.51 56.31 57.43 46.73 53.68 7"
D19-5542,W18-5606,0,0.0227677,"Informal Other unigrams bigrams I, my, her, your, they like, nice, love, bad friend, boyfriend, girl, guy pain, sex, skin, sleep, porn omg, lol, shit, fuck, cool advice, please, reddit I’ve never, your thoughts I love my dad, my girlfriend, my ex your pain, a doctor, a therapist tl dr, holy shit thank you, your advice ral models for depression detection. Some of the previous studies use deep learning methods on a post level to infer general information about a user (Kshirsagar et al., 2017; Ive et al., 2018; Ruder et al., 2016), or detect different mental health concepts in posts themselves (Rojas-Barahona et al., 2018), while we focus on utilizing all of the users’ text. Yates et al. (2017) use a CNN on a postlevel to extract features, which are then concatenated to get a user representation used for selfharm and depression assessment. A CNN requires a fixed length of posts, putting constraints on the data available to the model, while a HAN utilizes all of the data from posts of arbitrary lengths. A social media user can be modeled as collection of their posts, so we look at neural models for large-scale text classification. Liu et al. (2018) split a document into chunks and use a combination of CNNs and R"
D19-5542,D17-1322,0,0.045285,"nd, boyfriend, girl, guy pain, sex, skin, sleep, porn omg, lol, shit, fuck, cool advice, please, reddit I’ve never, your thoughts I love my dad, my girlfriend, my ex your pain, a doctor, a therapist tl dr, holy shit thank you, your advice ral models for depression detection. Some of the previous studies use deep learning methods on a post level to infer general information about a user (Kshirsagar et al., 2017; Ive et al., 2018; Ruder et al., 2016), or detect different mental health concepts in posts themselves (Rojas-Barahona et al., 2018), while we focus on utilizing all of the users’ text. Yates et al. (2017) use a CNN on a postlevel to extract features, which are then concatenated to get a user representation used for selfharm and depression assessment. A CNN requires a fixed length of posts, putting constraints on the data available to the model, while a HAN utilizes all of the data from posts of arbitrary lengths. A social media user can be modeled as collection of their posts, so we look at neural models for large-scale text classification. Liu et al. (2018) split a document into chunks and use a combination of CNNs and RNNs for document classification. While this approach proves to be success"
D19-5542,W19-3003,0,0.0252391,"The attention mechanism’s weighting suggests that words and phrases proved important in previous studies, using lexical features and linear models, are relevant for the HAN as well. 4 Related Work In recent years, social media has been a valuable source for psychological research. While most studies use Twitter data (Coppersmith et al., 2015a, 2014; Benton et al., 2017; Coppersmith et al., 2015b), a recent stream turns to Reddit as a richer source of high-volume data (De Choudhury and De, 2014; Shen and Rudzicz, 2017; ˇ Gjurkovi´c and Snajder, 2018; Cohan et al., 2018; Sekuli´c et al., 2018; Zirikly et al., 2019). Previous approaches to author’s mental health prediction usually relied on linguistic and stylistic features, e.g., Linguistic Inquiry and Word Count (LIWC) (Pennebaker et al., 2001) – a widely used feature extractor for various studies regarding mental health (Rude et al., 2004; Coppersmith et al., 2014; Sekuli´c et al., 2018; Zomick et al., 2019). Recently, Song et al. (2018) built a feature attention network for depression detection on Reddit, showing high interpretability, but low improvement in accuracy. Orabi et al. (2018) concatenate all the tweets of a Twitter user in a single docume"
D19-5542,W19-3009,0,0.556288,"nosed and receive no treatment for their illness. In line with WHO’s Mental Health Action Plan (Saxena et al., 2013), the natural language processing community helps the gathering of information and evidence on mental conditions, focusing on text analysis of authors affected by mental illnesses. Researchers can utilize large amounts of text on social media sites to get a deeper understanding of mental health and develop models for early detection of various mental disorders (De Choudhury et al., 2013a; Coppersmith et al., 2014; Gkotsis et al., 2016; Benton et al., 2017; Sekuli´c et al., 2018; Zomick et al., 2019). In this work, we experiment with the Self-reported Mental Health 2 2.1 Dataset and the Model Self-reported Mental Health Diagnoses Dataset The SMHD dataset (Cohan et al., 2018) is a largescale dataset of Reddit posts from users with one or multiple mental health conditions. The users were identified by constructing patterns for discovering self-reported diagnoses of nine different mental disorders. For example, if a user writes “I was officially diagnosed with depression last year”, she/he/other would be considered to suffer from depression. Nine or more control users, which are meant to rep"
D19-5542,W18-6211,1,0.823365,"Missing"
D19-5542,W17-3107,0,0.0570489,"iations, among most commonly attended are swear words, as well as other forms of informal language. The attention mechanism’s weighting suggests that words and phrases proved important in previous studies, using lexical features and linear models, are relevant for the HAN as well. 4 Related Work In recent years, social media has been a valuable source for psychological research. While most studies use Twitter data (Coppersmith et al., 2015a, 2014; Benton et al., 2017; Coppersmith et al., 2015b), a recent stream turns to Reddit as a richer source of high-volume data (De Choudhury and De, 2014; Shen and Rudzicz, 2017; ˇ Gjurkovi´c and Snajder, 2018; Cohan et al., 2018; Sekuli´c et al., 2018; Zirikly et al., 2019). Previous approaches to author’s mental health prediction usually relied on linguistic and stylistic features, e.g., Linguistic Inquiry and Word Count (LIWC) (Pennebaker et al., 2001) – a widely used feature extractor for various studies regarding mental health (Rude et al., 2004; Coppersmith et al., 2014; Sekuli´c et al., 2018; Zomick et al., 2019). Recently, Song et al. (2018) built a feature attention network for depression detection on Reddit, showing high interpretability, but low improvemen"
D19-5542,N03-1031,0,\N,Missing
E06-2015,J96-1002,0,0.0475453,"Missing"
E06-2015,W05-0620,0,0.0826244,"Missing"
E06-2015,J02-3001,0,0.0296174,"brunnenweg 33 69118 Heidelberg, Germany http://www.eml-research.de/nlp/ Abstract Extending a machine learning based coreference resolution system with a feature capturing automatically generated information about semantic roles improves its performance. 1 Introduction The last years have seen a boost of work devoted to the development of machine learning based coreference resolution systems (Soon et al., 2001; Ng & Cardie, 2002; Kehler et al., 2004, inter alia). Similarly, many researchers have explored techniques for robust, broad coverage semantic parsing in terms of semantic role labeling (Gildea & Jurafsky, 2002; Carreras & M`arquez, 2005, SRL henceforth). This paper explores whether coreference resolution can benefit from SRL, more specifically, which phenomena are affected by such information. The motivation comes from the fact that current coreference resolution systems are mostly relying on rather shallow features, such as the distance between the coreferent expressions, string matching, and linguistic form. On the other hand, the literature emphasizes since the very beginning the relevance of world knowledge and inference (Charniak, 1973). As an example, consider a sentence from the Automatic Co"
E06-2015,H05-1003,0,0.0621186,"Missing"
E06-2015,N04-1037,0,0.270315,"the report predicate, and It being the AGENT of say, could trigger the (semantic parallelism based) inference required to correctly link the two expressions, in contrast to anchoring the pronoun to Moscow. SRL provides the semantic relationships that constituents have with predicates, thus allowing us to include document-level event descriptive information into the relations holding between referring expressions (REs). This layer of semantic context abstracts from the specific lexical expressions used, and therefore represents a higher level of abstraction than predicate argument statistics (Kehler et al., 2004) and Latent Semantic Analysis used as a model of world knowledge (Klebanov & Wiemer-Hastings, 2002). In this respect, the present work is closer in spirit to Ji et al. (2005), who explore the employment of the ACE 2004 relation ontology as a semantic filter. 2 Coreference Resolution Using SRL 2.1 Corpora Used The system was initially prototyped using the MUC-6 and MUC-7 data sets (Chinchor & Sundheim, 2003; Chinchor, 2001), using the standard partitioning of 30 texts for training and 20-30 texts for testing. Then, we developed and tested the system with the ACE 2003 Training Data corpus (Mitch"
E06-2015,P02-1014,0,0.1233,"80 465 420 #coref ch. 904 399 354 #pron. 1037 358 329 NWIRE #comm. nouns 1210 485 484 #prop. names 2023 923 712 Table 1: Partitions of the ACE 2003 training data corpus ing a chunker and a named entity recognizer is applied to the text in order to identify the noun phrases, which are further taken as REs to be used for instance generation. Instances are created following Soon et al. (2001). During testing the classifier imposes a partitioning on the available REs by clustering each set of expressions labeled as coreferent into the same coreference chain. 2.3 Baseline System Features Following Ng & Cardie (2002), our baseline system reimplements the Soon et al. (2001) system. The system uses 12 features. Given a pair of candidate referring expressions REi and REj the features are computed as follows2 . (a) Lexical features STRING MATCH T if REi and REj have the same spelling, else F. ALIAS T if one RE is an alias of the other; else F. (b) Grammatical features I PRONOUN T if REi is a pronoun; else F. J PRONOUN T if REj is a pronoun; else F. J DEF T if REj starts with the; else F. J DEM T if REj starts with this, that, these, or those; else F. NUMBER T if both REi and REj agree in number; else F. GENDE"
E06-2015,J05-1004,0,0.0586635,"antic class matching. Unfortunately, a simple WordNet semantic class lookup exhibits problems such as coverage and sense disambiguation3 , which make the WN CLASS feature very noisy. As a consequence, we propose in the following to enrich the semantic knowledge made available to the classifier by using SRL information. In our experiments we use the ASSERT parser (Pradhan et al., 2004), an SVM based semantic role tagger which uses a full syntactic analysis to automatically identify all verb predicates in a sentence together with their semantic arguments, which are output as PropBank arguments (Palmer et al., 2005). It is often the case that the semantic arguments output by the parser do not align with any of the previously identified noun phrases. In this case, we pass a semantic role label to a RE only in case the two phrases share the same head. Labels have the form “ARG1 pred1 . . . ARGn predn ” for n semantic roles filled by a constituent, where each semantic argument label ARGi is always defined with respect to a predicate lemma predi . Given such level of semantic information available at the RE level, we introduce two new features4 . I SEMROLE the semantic predicate pairs of REi . role argument3"
E06-2015,N04-1030,0,0.0220541,"(d) Distance features DISTANCE how many sentences REi and REj are apart. 2.4 Semantic Role Features The baseline system employs only a limited amount of semantic knowledge. In particular, semantic information is limited to WordNet semantic class matching. Unfortunately, a simple WordNet semantic class lookup exhibits problems such as coverage and sense disambiguation3 , which make the WN CLASS feature very noisy. As a consequence, we propose in the following to enrich the semantic knowledge made available to the classifier by using SRL information. In our experiments we use the ASSERT parser (Pradhan et al., 2004), an SVM based semantic role tagger which uses a full syntactic analysis to automatically identify all verb predicates in a sentence together with their semantic arguments, which are output as PropBank arguments (Palmer et al., 2005). It is often the case that the semantic arguments output by the parser do not align with any of the previously identified noun phrases. In this case, we pass a semantic role label to a RE only in case the two phrases share the same head. Labels have the form “ARG1 pred1 . . . ARGn predn ” for n semantic roles filled by a constituent, where each semantic argument l"
E06-2015,J01-4004,0,0.454657,"lud1 We used the training data corpus only, as the availability of the test data was restricted to ACE participants. 143 TRAIN. DEVEL TEST #coref ch. 587 201 228 #pron. 876 315 291 BNEWS #comm. nouns 572 163 238 #prop. names 980 465 420 #coref ch. 904 399 354 #pron. 1037 358 329 NWIRE #comm. nouns 1210 485 484 #prop. names 2023 923 712 Table 1: Partitions of the ACE 2003 training data corpus ing a chunker and a named entity recognizer is applied to the text in order to identify the noun phrases, which are further taken as REs to be used for instance generation. Instances are created following Soon et al. (2001). During testing the classifier imposes a partitioning on the available REs by clustering each set of expressions labeled as coreferent into the same coreference chain. 2.3 Baseline System Features Following Ng & Cardie (2002), our baseline system reimplements the Soon et al. (2001) system. The system uses 12 features. Given a pair of candidate referring expressions REi and REj the features are computed as follows2 . (a) Lexical features STRING MATCH T if REi and REj have the same spelling, else F. ALIAS T if one RE is an alias of the other; else F. (b) Grammatical features I PRONOUN T if REi"
E06-2015,M95-1005,0,0.0729664,"ototyping we experimented unpairing the arguments from the predicates, which yielded worse results. This is supported by the PropBank arguments always being defined with respect to a target predicate. Binarizing the features — i.e. do REi and REj have the same argument or predicate label with respect to their closest predicate? — also gave worse results. 144 original Soon et al. duplicated baseline R 58.6 MUC-6 P F1 67.3 62.3 64.9 65.6 65.3 R 56.1 MUC-7 P F1 65.5 60.4 55.1 68.5 61.1 baseline +SRL role 3.1 argumentExperiments Performance Metrics We report in the following tables the MUC score (Vilain et al., 1995). Scores in Table 2 are computed for all noun phrases appearing in either the key or the system response, whereas Tables 3 and 4 refer to scoring only those phrases which appear in both the key and the response. We discard therefore those responses not present in the key, as we are interested here in establishing the upper limit of the improvements given by SRL. We also report the accuracy score for all three types of ACE mentions, namely pronouns, common nouns and proper names. Accuracy is the percentage of REs of a given mention type correctly resolved divided by the total number of REs of t"
E14-1052,W06-2911,0,0.0360272,"rant and the scope-aware approach. previous Markov Logic based approach for joint concept disambiguation and clustering (Fahrni and Strube, 2012). In contrast to us, most approaches for lexical disambiguation use either one model for all mentions (Milne and Witten, 2008; Ratinov et al., 2011) or a separate model for each mention or concept which requires a lot of training data (e.g. Bryl et al. (2010)). Only a few approaches try to learn specific models for groups of mentions, although none of them is discourse-motivated as ours: Mihalcea and Csomai (2005) learn a specific model for each POS, Ando (2006) uses alternating structure optimization to simultantanously learn a number of WSD problems and Dhillon and Ungar (2009) improve feature selection for WSD by integrating knowledge from similar words. achieves competitive performance without training on TAC data. On all data sets, the joint scope-aware approach consistently outperforms the scope-ignorant approach ceteris paribus. 5 Related Work Joint approaches have been successful in the past in NLP (e.g. Meza-Ruiz and Riedel (2009)). The idea of augmenting a model with additional latent variables to increase its expressiveness is known as hid"
E14-1052,P09-2065,0,0.0153985,"d clustering (Fahrni and Strube, 2012). In contrast to us, most approaches for lexical disambiguation use either one model for all mentions (Milne and Witten, 2008; Ratinov et al., 2011) or a separate model for each mention or concept which requires a lot of training data (e.g. Bryl et al. (2010)). Only a few approaches try to learn specific models for groups of mentions, although none of them is discourse-motivated as ours: Mihalcea and Csomai (2005) learn a specific model for each POS, Ando (2006) uses alternating structure optimization to simultantanously learn a number of WSD problems and Dhillon and Ungar (2009) improve feature selection for WSD by integrating knowledge from similar words. achieves competitive performance without training on TAC data. On all data sets, the joint scope-aware approach consistently outperforms the scope-ignorant approach ceteris paribus. 5 Related Work Joint approaches have been successful in the past in NLP (e.g. Meza-Ruiz and Riedel (2009)). The idea of augmenting a model with additional latent variables to increase its expressiveness is known as hidden or latent variable learning (Smith, 2011) and is a promising research direction with successful applications in e.g."
E14-1052,C10-1032,0,0.051046,"flexibility and their previous success for many tasks. Blunsom et al. (2008) for instance use latent variables in the context of discriminative machine translation and model the derivation as a latent variable. Chang et al. (2010) is close to our approach, as their latent variable approach also uses ILP. Poon and Domingos (2008) also use latent variables with Markov Logic, although with a completely different aim, i.e. for unsupervised coreference resolution. Most approaches that use Wikipedia as a resource for disambiguation focus on named entities (Bunescu and Pas¸ca, 2006; Cucerzan, 2007; Dredze et al., 2010; Ji and Grishman, 2011; Hachey et al., 2013; Hoffart et al., 2011), while only a few disambiguate common and proper nouns like us (Csomai and Mihalcea, 2008; Milne and Witten, 2008; Zhou et al., 2010; Ratinov et al., 2011; Cheng and Roth, 2013). We build upon our 6 Conclusions In this paper, we discuss the relationship between cohesion and concept disambiguation and propose a cohesive scope-aware disambiguation approach. We distinguish between three different cohesive scopes (local, intermediate and global) and model the scope assignment and the disambiguation jointly using latent variables i"
E14-1052,P08-1024,0,0.0676643,"e are templates not formulas (Section 3.2)): (1) a template for formulas that add information for scope assignment (f 8) and (2) a template for formulas that add information for disambiguation (f 9). All formulas with scope-parametrized weights that are relevant for the concept prediction task are defined for the predicate relatesScopeToConcept. This enables us to activate the relevant Weight Learning with Latent Variables Since no annotations are available for the scope distinction, we face a latent variable learning problem. For learning weights in this situation we follow Poon and Domingos (2008). We split our hidden predicates into two parts: V are the ones for which the ground truth is known (concepts) and U are the ones for which there is no annotation (scopes). Let O be the observed predicates. Let o and v be the values of O and V in the training data. u denotes values assigned to U . Weight learning finds a w that maximizes the conditional log-likelihood Lw (o, v) = log Pw (V = v|O = o) X = log Pw (V = v, U = u|O = o), u where the sum is over all possible values of U . Although Lw (o, v) is not convex, a local optimum can be found via gradient descent by iteratively solving wt+1"
E14-1052,C12-1050,1,0.343661,"ediction task, i.e. the disambiguation. In this paper, we focus on concept and entity disambiguation1 with respect to an inventory derived from Wikipedia and compare (1) to a stateof-the-art approach that treats all mentions alike and uses the same features for disambiguation, (2) to a pipeline-based approach, and (3) to other state-of-the-art approaches (Section 4). While early work disambiguated concepts using the local context (Csomai and Mihalcea, 2008), current research focuses on exploiting the global document context (Milne and Witten, 2008; Kulkarni et al., 2009; Ratinov et al., 2011; Fahrni and Strube, 2012; Cheng and Roth, 2013). Although such global approaches try to balance between local and global context, they treat all mentions alike, i.e., they apply the same model and the same weighting of local and global context features for disambiguating all mentions (Section 5). This paper takes a discourse-oriented perspective for disambiguating common and proper noun mentions with respect to Wikipedia. Our novel approach models the relationship between disambiguation and aspects of cohesion using Markov Logic Networks with latent variables. Considering cohesive aspects consistently improves the di"
E14-1052,E06-1002,0,0.367743,"Missing"
E14-1052,N03-1013,0,0.0286475,"argument is usually more likely to be of local scope. A passive construction – e.g. “the thief was catched by the police” – is a way to reduce the prominency of the agent (e.g. police). The agent tends to be of local scope. Conjunctions are often used for exemplifications. Therefore mentions in conjunctions are often less prominent. In NPs with prepositional or genitive modifiers usually at most one part – either the modifying NP or the head – has intermediate or global scope. The more frequent the head of a mention appears in the text – also as a derivation, e.g. a verb, according to CatVar (Habash and Dorr, 2003) –, the more prominent it is. The earlier a mention appears in text, the more likely it is to exhibit global cohesive scope (cf. the hard-to-be-beat lead baseline in summarization (Radev et al., 2003)). Table 2: Features for cohesive scope distinction. m, m1 , m2 denote mentions, q a score. The predicates are plugged in the template formula f 8 in Table 1. Data set WP Training WP Dev ACE 2005 ACE 2004 TAC 2011 MSNBC No. of Mentions 56,372 9,992 29,300 306 2,250 756 NonNILs 53,097 9,375 27,184 257 1,124 629 NILs 3,275 617 2,116 49 1,126 127 Avg. Ambiguity 2.31 2.28 6.52 5.04 6.32 5.29 date conc"
E14-1052,N09-1041,0,0.0119271,"e is local, as it lacks some cohesive ties with mentions in other sentences. If it had been disambiguated to S CHOLARLY PAPER, its scope would be global. This reciprocal relationship between discourse structure and meaning has also been discussed by Asher and Lascarides (1995). They use rhetorical relations for structuring discourse while we rely on the notion of lexical cohesion and model scope assignment and disambiguation jointly. Our notion of scope is related to work on lexical chains (Morris and Hirst, 1991; Nelken and Shieber, 2006; Mihalcea, 2006) and to work in content modeling, e.g. Haghighi and Vanderwende (2009) distinguish content vocabulary and document-specific vocabulary. 3 3.1 Markov Logic Networks Markov Logic (ML) incorporates first-order logic and probabilities (Domingos and Lowd, 2009). A Markov Logic Network (MLN) is a first-order knowledge base and consists of a set of pairs (Fi , wi ), where Fi is a first-order formula and wi ∈ R is the weight of formula Fi . It is a template for constructing a Markov Network. This Markov Network has a binary node for each possible grounding for each predicate of the MLN. If the grounding of the predicate is true, the binary node’s value is set to 1, othe"
E14-1052,D13-1184,0,0.622337,"isambiguation. In this paper, we focus on concept and entity disambiguation1 with respect to an inventory derived from Wikipedia and compare (1) to a stateof-the-art approach that treats all mentions alike and uses the same features for disambiguation, (2) to a pipeline-based approach, and (3) to other state-of-the-art approaches (Section 4). While early work disambiguated concepts using the local context (Csomai and Mihalcea, 2008), current research focuses on exploiting the global document context (Milne and Witten, 2008; Kulkarni et al., 2009; Ratinov et al., 2011; Fahrni and Strube, 2012; Cheng and Roth, 2013). Although such global approaches try to balance between local and global context, they treat all mentions alike, i.e., they apply the same model and the same weighting of local and global context features for disambiguating all mentions (Section 5). This paper takes a discourse-oriented perspective for disambiguating common and proper noun mentions with respect to Wikipedia. Our novel approach models the relationship between disambiguation and aspects of cohesion using Markov Logic Networks with latent variables. Considering cohesive aspects consistently improves the disambiguation results on"
E14-1052,D11-1072,0,0.39269,"Missing"
E14-1052,P11-1115,0,0.239288,"r previous success for many tasks. Blunsom et al. (2008) for instance use latent variables in the context of discriminative machine translation and model the derivation as a latent variable. Chang et al. (2010) is close to our approach, as their latent variable approach also uses ILP. Poon and Domingos (2008) also use latent variables with Markov Logic, although with a completely different aim, i.e. for unsupervised coreference resolution. Most approaches that use Wikipedia as a resource for disambiguation focus on named entities (Bunescu and Pas¸ca, 2006; Cucerzan, 2007; Dredze et al., 2010; Ji and Grishman, 2011; Hachey et al., 2013; Hoffart et al., 2011), while only a few disambiguate common and proper nouns like us (Csomai and Mihalcea, 2008; Milne and Witten, 2008; Zhou et al., 2010; Ratinov et al., 2011; Cheng and Roth, 2013). We build upon our 6 Conclusions In this paper, we discuss the relationship between cohesion and concept disambiguation and propose a cohesive scope-aware disambiguation approach. We distinguish between three different cohesive scopes (local, intermediate and global) and model the scope assignment and the disambiguation jointly using latent variables in the framework of MLN."
E14-1052,P06-1055,0,0.022092,"ection for WSD by integrating knowledge from similar words. achieves competitive performance without training on TAC data. On all data sets, the joint scope-aware approach consistently outperforms the scope-ignorant approach ceteris paribus. 5 Related Work Joint approaches have been successful in the past in NLP (e.g. Meza-Ruiz and Riedel (2009)). The idea of augmenting a model with additional latent variables to increase its expressiveness is known as hidden or latent variable learning (Smith, 2011) and is a promising research direction with successful applications in e.g. syntactic parsing (Petrov et al., 2006), statistical machine translation (Blunsom et al., 2008) and sentiment analysis (Yessenalina et al., 2010; Trivedi and Eisenstein, 2013). For latent variable learning generative approaches (Petrov et al., 2006), large margin methods (Smith, 2011) and conditional log-linear models have been proposed. We focus here on conditional log-linear models due to their flexibility and their previous success for many tasks. Blunsom et al. (2008) for instance use latent variables in the context of discriminative machine translation and model the derivation as a latent variable. Chang et al. (2010) is close"
E14-1052,D08-1068,0,0.527039,"the output which is not relevant except for supporting the prediction of the target (Smith, 2011). In our approach, the different cohesive scopes are modeled by latent variables. Each mention to be disambiguated is assigned a scope s. All feature weights are parametrized by scope s. The parameters for the disambiguation and scope assignment tasks are learned jointly and are guided by the annotations available for the disambiguation task. Markov Logic Networks can be represented as log-linear models, when grounded, and are therefore straightforward to extend with latent variables (Smith, 2011; Poon and Domingos, 2008). In addition, global features can be conveniently integrated. chains). In this paper, we focus on concept-level cohesion and assume that each concept referred to by a mention can exhibit cohesive ties with concepts from other lexical units. The cohesive scope of a mention is the text span within which a concept referred to by a mention shows such cohesive ties. We distinguish three broad categories of cohesive scopes: (1) Mentions with local cohesive scope exhibit cohesive ties with lexical units in the same sentence; (2) mentions with intermediate cohesive scope show cohesive ties both withi"
E14-1052,P11-1138,0,0.659421,"able for the target prediction task, i.e. the disambiguation. In this paper, we focus on concept and entity disambiguation1 with respect to an inventory derived from Wikipedia and compare (1) to a stateof-the-art approach that treats all mentions alike and uses the same features for disambiguation, (2) to a pipeline-based approach, and (3) to other state-of-the-art approaches (Section 4). While early work disambiguated concepts using the local context (Csomai and Mihalcea, 2008), current research focuses on exploiting the global document context (Milne and Witten, 2008; Kulkarni et al., 2009; Ratinov et al., 2011; Fahrni and Strube, 2012; Cheng and Roth, 2013). Although such global approaches try to balance between local and global context, they treat all mentions alike, i.e., they apply the same model and the same weighting of local and global context features for disambiguating all mentions (Section 5). This paper takes a discourse-oriented perspective for disambiguating common and proper noun mentions with respect to Wikipedia. Our novel approach models the relationship between disambiguation and aspects of cohesion using Markov Logic Networks with latent variables. Considering cohesive aspects con"
E14-1052,N09-1018,0,0.0194841,"mentions, although none of them is discourse-motivated as ours: Mihalcea and Csomai (2005) learn a specific model for each POS, Ando (2006) uses alternating structure optimization to simultantanously learn a number of WSD problems and Dhillon and Ungar (2009) improve feature selection for WSD by integrating knowledge from similar words. achieves competitive performance without training on TAC data. On all data sets, the joint scope-aware approach consistently outperforms the scope-ignorant approach ceteris paribus. 5 Related Work Joint approaches have been successful in the past in NLP (e.g. Meza-Ruiz and Riedel (2009)). The idea of augmenting a model with additional latent variables to increase its expressiveness is known as hidden or latent variable learning (Smith, 2011) and is a promising research direction with successful applications in e.g. syntactic parsing (Petrov et al., 2006), statistical machine translation (Blunsom et al., 2008) and sentiment analysis (Yessenalina et al., 2010; Trivedi and Eisenstein, 2013). For latent variable learning generative approaches (Petrov et al., 2006), large margin methods (Smith, 2011) and conditional log-linear models have been proposed. We focus here on condition"
E14-1052,P05-3014,0,0.0205566,"rity of the article name to the context – both in the scope-ignorant and the scope-aware approach. previous Markov Logic based approach for joint concept disambiguation and clustering (Fahrni and Strube, 2012). In contrast to us, most approaches for lexical disambiguation use either one model for all mentions (Milne and Witten, 2008; Ratinov et al., 2011) or a separate model for each mention or concept which requires a lot of training data (e.g. Bryl et al. (2010)). Only a few approaches try to learn specific models for groups of mentions, although none of them is discourse-motivated as ours: Mihalcea and Csomai (2005) learn a specific model for each POS, Ando (2006) uses alternating structure optimization to simultantanously learn a number of WSD problems and Dhillon and Ungar (2009) improve feature selection for WSD by integrating knowledge from similar words. achieves competitive performance without training on TAC data. On all data sets, the joint scope-aware approach consistently outperforms the scope-ignorant approach ceteris paribus. 5 Related Work Joint approaches have been successful in the past in NLP (e.g. Meza-Ruiz and Riedel (2009)). The idea of augmenting a model with additional latent variabl"
E14-1052,N13-1100,0,0.0230805,"ll data sets, the joint scope-aware approach consistently outperforms the scope-ignorant approach ceteris paribus. 5 Related Work Joint approaches have been successful in the past in NLP (e.g. Meza-Ruiz and Riedel (2009)). The idea of augmenting a model with additional latent variables to increase its expressiveness is known as hidden or latent variable learning (Smith, 2011) and is a promising research direction with successful applications in e.g. syntactic parsing (Petrov et al., 2006), statistical machine translation (Blunsom et al., 2008) and sentiment analysis (Yessenalina et al., 2010; Trivedi and Eisenstein, 2013). For latent variable learning generative approaches (Petrov et al., 2006), large margin methods (Smith, 2011) and conditional log-linear models have been proposed. We focus here on conditional log-linear models due to their flexibility and their previous success for many tasks. Blunsom et al. (2008) for instance use latent variables in the context of discriminative machine translation and model the derivation as a latent variable. Chang et al. (2010) is close to our approach, as their latent variable approach also uses ILP. Poon and Domingos (2008) also use latent variables with Markov Logic,"
E14-1052,P10-2062,0,0.0294255,"training on TAC data. On all data sets, the joint scope-aware approach consistently outperforms the scope-ignorant approach ceteris paribus. 5 Related Work Joint approaches have been successful in the past in NLP (e.g. Meza-Ruiz and Riedel (2009)). The idea of augmenting a model with additional latent variables to increase its expressiveness is known as hidden or latent variable learning (Smith, 2011) and is a promising research direction with successful applications in e.g. syntactic parsing (Petrov et al., 2006), statistical machine translation (Blunsom et al., 2008) and sentiment analysis (Yessenalina et al., 2010; Trivedi and Eisenstein, 2013). For latent variable learning generative approaches (Petrov et al., 2006), large margin methods (Smith, 2011) and conditional log-linear models have been proposed. We focus here on conditional log-linear models due to their flexibility and their previous success for many tasks. Blunsom et al. (2008) for instance use latent variables in the context of discriminative machine translation and model the derivation as a latent variable. Chang et al. (2010) is close to our approach, as their latent variable approach also uses ILP. Poon and Domingos (2008) also use late"
E14-1052,J91-1002,0,0.392168,". In the example (Section 1), paper in read yesterday’s paper refers to the concept N EWSPAPER. Its scope is local, as it lacks some cohesive ties with mentions in other sentences. If it had been disambiguated to S CHOLARLY PAPER, its scope would be global. This reciprocal relationship between discourse structure and meaning has also been discussed by Asher and Lascarides (1995). They use rhetorical relations for structuring discourse while we rely on the notion of lexical cohesion and model scope assignment and disambiguation jointly. Our notion of scope is related to work on lexical chains (Morris and Hirst, 1991; Nelken and Shieber, 2006; Mihalcea, 2006) and to work in content modeling, e.g. Haghighi and Vanderwende (2009) distinguish content vocabulary and document-specific vocabulary. 3 3.1 Markov Logic Networks Markov Logic (ML) incorporates first-order logic and probabilities (Domingos and Lowd, 2009). A Markov Logic Network (MLN) is a first-order knowledge base and consists of a set of pairs (Fi , wi ), where Fi is a first-order formula and wi ∈ R is the weight of formula Fi . It is a template for constructing a Markov Network. This Markov Network has a binary node for each possible grounding fo"
E14-1052,C10-1150,0,0.0213867,"variable. Chang et al. (2010) is close to our approach, as their latent variable approach also uses ILP. Poon and Domingos (2008) also use latent variables with Markov Logic, although with a completely different aim, i.e. for unsupervised coreference resolution. Most approaches that use Wikipedia as a resource for disambiguation focus on named entities (Bunescu and Pas¸ca, 2006; Cucerzan, 2007; Dredze et al., 2010; Ji and Grishman, 2011; Hachey et al., 2013; Hoffart et al., 2011), while only a few disambiguate common and proper nouns like us (Csomai and Mihalcea, 2008; Milne and Witten, 2008; Zhou et al., 2010; Ratinov et al., 2011; Cheng and Roth, 2013). We build upon our 6 Conclusions In this paper, we discuss the relationship between cohesion and concept disambiguation and propose a cohesive scope-aware disambiguation approach. We distinguish between three different cohesive scopes (local, intermediate and global) and model the scope assignment and the disambiguation jointly using latent variables in the framework of MLN. The joint scope-aware approach significantly improves over both a state-of-the-art and a pipeline-based approach using the same features for the disambiguation task. For future"
E14-1052,W10-3503,0,\N,Missing
E14-1052,2003.mtsummit-systems.9,0,\N,Missing
E14-1052,P03-1048,0,\N,Missing
E14-1052,D07-1074,0,\N,Missing
E17-1078,E06-1002,0,0.478319,"Missing"
E17-1078,P14-2076,0,0.0353975,"Missing"
E17-1078,D07-1074,0,0.876744,"stract whether they rank all candidates of all mentions simultaneously by incorporating a global coherence measure into the optimization goal (“global inference”). While linguistically well-founded in the concept of lexical cohesion (Halliday and Hasan, 1976), global inference approaches (Kulkarni et al., 2009; Hoffart et al., 2011a) do not scale well with number of mentions and number of candidate entities. In contrast, local approaches do not suffer from scalability issues, since they only optimize the similarity between mention context and candidate KB entry text (Bunescu and Pas¸ca, 2006; Cucerzan, 2007), usually also including a popularity prior2 (Milne and Witten, 2008; Spitkovsky and Chang, 2012). Recent local approaches achieve state-of-the-art results by using convolutional neural networks to capture similarity at multiple context sizes (Francis-Landau et al., 2016), but, by definition, fail to take global coherence into account. To avoid the trade-off between the efficiency of local inference on the one hand and the coherence benefits of global inference on the other, we propose a two-stage approach: In the first stage, candidate entities are ranked by a fast, local inferencebased EL sy"
E17-1078,P15-4007,1,0.862146,"ference system models mention and entity context with a convolutional neural network (CNN). The CNN captures semantic similarity of a given mention’s context at different granularities (small context window, paragraph, document) and the entity context derived from the entity’s Wikipedia page. PH (Pershina et al., 2015): This global inference system applies Personal PageRank to a graph whose nodes represent candidate entities and whose edges indicate if a link between the corresponding Wikipedia articles exists. PH achieves the best CoNLL performance among the systems in our evaluation. TAC-1 (Heinzerling and Strube, 2015): This system uses local and pairwise inference in an easyfirst, incremental rule-based approach. Features are based on popularity priors, contextual occurrence of keywords, entity type, and relational evidence. TAC-2 (Sil et al., 2015): This system employs a global inference approach which partitions a document into sets of mentions that appear near each other. The partitioning is motivated by the intuition that a given mention’s immediate context provides the most salient information for disambiguation, and drastically reduces the search space during global optimization. TAC-3 (Dai et al., 2"
E17-1078,Q14-1037,0,0.127759,"Table 5: Results on CoNLL and TAC15 test sets. Baseline shows performance of the original systems, After verification shows performance after application of our automatic verification method, and ∆ shows the corresponding change. Bold font indicates best results for each metric and system. On CoNLL, the precision increase is less pronounced, arguably owing to the already higher baseline precision, which leaves less room for improvement. Since EL is usually performed as part of a larger task, such as knowledge base completion, search, or as part of a more comprehensive entity analysis system (Durrett and Klein, 2014), good precision is highly desirable in order to minimize error propagation to other system components and downstream applications. 3.3 System Prec Rec F1 FL baseline FL filter FL rerank 85.3 89.2 87.9 85.2 84.7 85.6 85.2 86.9 86.7 Table 6: Comparison of filtering and candidate entity reranking performance on the CoNLL test set. filtering, while reranking increases both precision and recall. Candidate Reranking 3.4 We resort to the binary decision of either retaining or removing an entity linked by an EL system if no candidate entities and no meaningful confidence scores are available. This is"
E17-1078,W14-5201,0,0.0231896,"Missing"
E17-1078,D11-1072,0,0.203629,"Missing"
E17-1078,N16-1150,0,0.423721,", global inference approaches (Kulkarni et al., 2009; Hoffart et al., 2011a) do not scale well with number of mentions and number of candidate entities. In contrast, local approaches do not suffer from scalability issues, since they only optimize the similarity between mention context and candidate KB entry text (Bunescu and Pas¸ca, 2006; Cucerzan, 2007), usually also including a popularity prior2 (Milne and Witten, 2008; Spitkovsky and Chang, 2012). Recent local approaches achieve state-of-the-art results by using convolutional neural networks to capture similarity at multiple context sizes (Francis-Landau et al., 2016), but, by definition, fail to take global coherence into account. To avoid the trade-off between the efficiency of local inference on the one hand and the coherence benefits of global inference on the other, we propose a two-stage approach: In the first stage, candidate entities are ranked by a fast, local inferencebased EL system. In the second stage these results are used to create a semantic profile of the given text, derived from rich data the KB contains about the top-ranked candidates. Since the linking precision of current EL systems is relatively high, we trust that this profile is rea"
E17-1078,P14-5010,0,0.00479715,"Missing"
E17-1078,spitkovsky-chang-2012-cross,0,0.150255,"ng a global coherence measure into the optimization goal (“global inference”). While linguistically well-founded in the concept of lexical cohesion (Halliday and Hasan, 1976), global inference approaches (Kulkarni et al., 2009; Hoffart et al., 2011a) do not scale well with number of mentions and number of candidate entities. In contrast, local approaches do not suffer from scalability issues, since they only optimize the similarity between mention context and candidate KB entry text (Bunescu and Pas¸ca, 2006; Cucerzan, 2007), usually also including a popularity prior2 (Milne and Witten, 2008; Spitkovsky and Chang, 2012). Recent local approaches achieve state-of-the-art results by using convolutional neural networks to capture similarity at multiple context sizes (Francis-Landau et al., 2016), but, by definition, fail to take global coherence into account. To avoid the trade-off between the efficiency of local inference on the one hand and the coherence benefits of global inference on the other, we propose a two-stage approach: In the first stage, candidate entities are ranked by a fast, local inferencebased EL system. In the second stage these results are used to create a semantic profile of the given text,"
E17-1078,S10-1071,0,0.0867794,"Missing"
E17-1078,P13-4007,0,0.0156959,"on. TAC-3 (Dai et al., 2015): This local inference system models mentions and entity context with a CNN and word embeddings. The systems were chosen for their popularity (AIDA, SL), performance on CoNLL (FL, PH), and performance on TAC15 (TAC systems). Unless stated otherwise, we use system output provided by authors for CoNLL systems, and provided by the workshop organizers for TAC15 systems.9 Our evaluation does not include (Globerson et al., 2016) and (Yamada et al., 2016), who report better performance on CoNLL than PH, but were unable to make system output available. 3.1 tion; DKPro WSD (Miller et al., 2013) for modeling entity mentions and links, and using Freebase (Bollacker et al., 2008) and YAGO (Hoffart et al., 2011a) as knowledge bases. After feature extraction, we train a random forest classifier10 for each dataset, one using FL system results for the CoNLL development set (216 documents) and one using TAC-1 results for the TAC15 training set (168 documents). For evaluation, we apply the verifier trained on FL CoNLL development results to the test set results of the FL and AIDA systems, and a verifier trained on PH training data to the PH test set results. For the test set output of TAC sy"
E17-1078,K16-1025,0,0.0132162,"mmediate context provides the most salient information for disambiguation, and drastically reduces the search space during global optimization. TAC-3 (Dai et al., 2015): This local inference system models mentions and entity context with a CNN and word embeddings. The systems were chosen for their popularity (AIDA, SL), performance on CoNLL (FL, PH), and performance on TAC15 (TAC systems). Unless stated otherwise, we use system output provided by authors for CoNLL systems, and provided by the workshop organizers for TAC15 systems.9 Our evaluation does not include (Globerson et al., 2016) and (Yamada et al., 2016), who report better performance on CoNLL than PH, but were unable to make system output available. 3.1 tion; DKPro WSD (Miller et al., 2013) for modeling entity mentions and links, and using Freebase (Bollacker et al., 2008) and YAGO (Hoffart et al., 2011a) as knowledge bases. After feature extraction, we train a random forest classifier10 for each dataset, one using FL system results for the CoNLL development set (216 documents) and one using TAC-1 results for the TAC15 training set (168 documents). For evaluation, we apply the verifier trained on FL CoNLL development results to the test set"
E17-1078,Q14-1019,0,0.0305856,"TAC15 dataset consists of different text genres: clean newswire articles, and noisy discussion forum threads. Analysis of verification performance on these two genres reveals that verification has the biggest impact on noisy text (Table 7, bottom), while the improvement is smaller for two systems on clean text, and even slightly negative for one system, namely the global inference system TAC-2 (Table 7, top). 4 Related Work Global coherence has been successfully employed for EL in a number of seminal works (Kulkarni et al., 2009; Hoffart et al., 2011b; Han et al., 2011), and more recently by Moro et al. (2014), Pershina et al. (2015), and Globerson et al. (2016), among others. These approaches maximize global coherence based on a general notion of semantic relatedness, while considering a fixed number of candidate entities for each mentions. Our approach differs from these in in two regards. Firstly, we introduce specific aspects of coherence, namely entity type coherence, geographic coherence, and tem835 Baseline Rec F1 After verification Prec Rec F1 Genre System Prec News TAC-1 TAC-2 TAC-3 66.5 69.7 63.0 60.3 59.9 59.1 63.2 64.4 61.0 75.8 79.3 71.3 57.0 53.9 54.3 Forum TAC-1 TAC-2 TAC-3 76.0 73.1"
E17-1078,N15-1026,0,0.0325173,"Missing"
E17-1078,D16-1201,0,0.0658539,"Missing"
E17-1078,P11-1138,0,0.269279,"achey et al., 2014) for TAC15. This metric measures precision, recall, and F 1 of matching entity links and mention spans. 3.2 Results and Discussion Evaluation results are shown in Table 5. Our method improves the linking performance of all evaluated EL systems. The impact is most noticeable for the systems that only use local and pairwise inference, namely FL (+1.9 F 1), TAC1 (+2.4 F 1), TAC-3 (+1.1 F 1). The improved TAC-1 result (68.1F 1) is the best published linking score on the TAC15 dataset. Improvements are smaller for the global inference systems, AIDA, HP, and TAC-2. In contrast to Ratinov et al. (2011), who report only a very small increase in linking performance when incorporating global features into a local inferencebased system, our results indicate that global features are useful and lead to considerable improvements. As expected, improvements are caused by increased precision, due to filtering out likely linking mistakes. The fact that this increase is not accompanied by a commensurate decrease in recall, shows that our method predicts wrong linking decisions with high accuracy. On TAC15, we observe considerable improvements in linking precision of up to 10.4 percent. Setup and Implem"
E95-1033,C94-1061,1,0.895913,"Missing"
E95-1033,C88-1026,0,0.0554151,"dy is presented in Hahn (1992)). 2 DG Constraints on Anaphora In this section, we present, quite informally, some constraints on intra-sentential anaphora in terms of dependency grammar (DG). We will reconsider these constraints in Section 3, where our grammar model is dealt with in more depth. We provide here a definition of d-binding and two constraints which describe the use of reflexive pronouns and anaphors (personal pronouns and definite noun phrases). These constraints cover approximately the same phenomena as the binding theory of GB (Chomsky (1981); for a computational treatment, cf. Correa (1988)). Dependency structures, by definition, refer to the sentence level of linguistic description only. The relation of dependency holds between a lexical head and one or several modifiers of that head, such that the occurrence of a head allows for the occurrence of one or several modifiers (in some 238 pre-specified linear ordering), but not vice versa. Speaking in terms of dependency structure representations, the head always precedes and, thus, (transitively) governs its associated modifiers in the dependency tree. This basic notion of government must be further refined for the description of"
E95-1033,J86-3001,0,0.0173051,"airly general linguistic tasks, such as establishing dependencies, properly arranging coordinations, and, of course, resolving anaphors. Consequently, any of these subprotocols constitutes part of the grammar specification proper. We shall illustrate the linguistic aspects of word actor-based parsing by introducing the basic data structures for text-level anaphora as acquaintances of specific word actors, and then turn to the general message-passing protocol that accounts for intra- as well as inter-sentential anaphora. Our exposition builds on the well-known focusing mechanism (Sidner, 1983; Grosz and Sidner, 1986). Accordingly, we distinguish each sentence's unique focus, a complementary list of alternate potential loci, and a history list composed of discourse elements not in the list of potential loci, but occurring in previous sentences of the current discourse segment. These data structures are realized as acquaintances of sentence delimiters to restrict the search space beyond the sentence to the relevant word actors. The protocol level of analysis encompasses the procedural interpretation of the declarative constraints given in Section 2. At that level, in the case of reflexive pronouns, the sear"
E95-1033,C92-1008,1,0.838158,"with crucial linguistic phenomena which cause considerable problems for current GB theory, 3. goes beyond GB in that it allows the treatment of anaphora at the text level of description within the same grammar formalism as is used for sentence level anaphora, and, 4. goes beyond the anaphora-centered treatment of text structure characteristic of the DRT approach in that it already accounts for the resolution of text-level ellipsis (sometimes also referred to as functional anaphora, cf. Hahn and Strube (1995)) and the interpretation of text macro structures (a preliminary study is presented in Hahn (1992)). 2 DG Constraints on Anaphora In this section, we present, quite informally, some constraints on intra-sentential anaphora in terms of dependency grammar (DG). We will reconsider these constraints in Section 3, where our grammar model is dealt with in more depth. We provide here a definition of d-binding and two constraints which describe the use of reflexive pronouns and anaphors (personal pronouns and definite noun phrases). These constraints cover approximately the same phenomena as the binding theory of GB (Chomsky (1981); for a computational treatment, cf. Correa (1988)). Dependency str"
E95-1033,C92-1023,0,0.148716,"ved (Lappin and Leass, 1994), but still is a source of lots of problems. Third, unlike our approach, even the current SG model for anaphora resolution does not incorporate conceptual knowledge and global discourse structures (for reasons discussed by Lappin and Laess). This decision might nevertheless cause trouble if more conceptually rooted text cohesion and coherence structures have to be accounted for (e.g., textual ellipses). A particular problem we have not yet solved, the plausible ranking of single antecedents from a candidate set, is dealt with in depth by Lappin and Laess (1994) and Hajicova et al. (1992). Both define salience metrics capable of ordering alternative antecedents according to structural criteria, several of which can directly be attributed to the topological structure and topic/comment annotations of the underlying dependency trees. 243 6 Conclusions We have outlined a model of anaphora resolution which is founded on a dependency-based grammar model. This model accounts for sentence-level anaphora, with constraints adapted from GB, as well as text-level anaphora, with concepts close to Grosz-Sidner-style focus models. The associated text parser is based on the actor computation"
E95-1033,P89-1032,0,0.016866,"mong concepts, e.g., (MOTnERBOARD, has-cpu, CPU) E permit. Furthermore, object.attribute denotes the value of the property attribute at object and the symbol self refers to the current lexical item. The ParseTalk specification language, in addition, incorporates topological primitives for relations within dependency trees. The relations left and head denote ""~ occurs left of y"" and ""x is head of y', resp. These primitive relations can be considered declarative equivalents to the procedural specifications used in several tree-walking algorithms for anaphora resolution, e.g., by Hobbs (1978) or Ingria and Stallard (1989). Note that in the description below tel + and rel* denote the transitive and transitive/reflexive closure of a relation rel, respectively. Major G r a m m a t i c a l Predicates The ParseTalk model of DG (Hahn et al., 1994) exploits inheritance as a major abstraction mechanism. The entire lexical system is organized as a hierarchy of lexical classes (isac denoting the subclass relation among lexical classes), with concrete lexical items forming the leave nodes of the corresponding lexicon grammar graph. Valency constraints are attached to each lexical item, on which the local computation of c"
E95-1033,J94-4002,0,0.0937059,"mar (SG), a slight theory variant of DG. In particular, they treat pronominal coreference and anaphora (i.e., reflexives and reciprocals). Our approach methodologically differs in three major aspects from that study: First, unlike the SG proposal, which is based on a second-pass algorithm operating on fully parsed clauses to determine anaphoric relationships, our proposal is basically an incremental single-pass parsing model. Most importan't, however, is that our model incorporates the text-level of anaphora resolution, a shortcoming of the original SG approach that has recently been removed (Lappin and Leass, 1994), but still is a source of lots of problems. Third, unlike our approach, even the current SG model for anaphora resolution does not incorporate conceptual knowledge and global discourse structures (for reasons discussed by Lappin and Laess). This decision might nevertheless cause trouble if more conceptually rooted text cohesion and coherence structures have to be accounted for (e.g., textual ellipses). A particular problem we have not yet solved, the plausible ranking of single antecedents from a candidate set, is dealt with in depth by Lappin and Laess (1994) and Hajicova et al. (1992). Both"
E95-1033,J90-4001,0,0.101558,"eover, conceptual criteria have to be met as in the case of nominal anaphors which must subsume their antecedents at the conceptual level. Similarly, for pronominal anaphors the selected antecedent must be permitted in those conceptual roles connecting the pronominal anaphors and its grammatical head. The DG constraints for the use of reflexives and intra-sentential anaphora cover approximately the same phenomena as GB, but the structures used by DG analysis are less complex than those of GB and do not require the formal machinery of empty categories, binding chains and complex movements (cf. Lappin and McCord (1990, p.205) for a similar argument). Hence, our proposal provides a more tractable basis for implementation. 3 guage. The concept hierarchy consists of a set of concept names ~"" = {COMPUTERSYSTEM, NOTEBOOK, MOTHERBOARD, ...} and a subclass relation isaF = {(NOTEBOOK, COMPUTERSYSTEM), (PCI-MOTHERBOARD, MOTHERBOARD), ...} C x 9r. roles C f"" x 9v is the set of relations with role names ""R = {has-part, has-cpu, ...} and denotes the established relations in the knowledge base, while R characterizes the labels of admitted conceptual relations. The relation permit C 9v x ""R x 9r characterizes the range"
E95-1033,C94-1080,1,0.840754,"horicAntecedentOf (cf. Box 3), determines the candidate set of possible antecedents for (pro)nominal anaphors, and 4 x isPotentialAnaphoricAntecedentOf y :~:~ --,3 z: (z d-binds x A z d-binds y) R e s o l u t i o n of A n a p h o r a NomAnaphorTest (defNP, ante):~ ante isac* Noun A ((defNP.features sel~agr
um) U(ante.featuressel~ag,
um) # _L) ^ ante.concept isaF* defNP.concept The ParseTaik environment builds on the actor computation model (Agha and Hewitt, 1987) as background for the procedural interpretation of lexicalized dependency specifications in terms of so-called word actors (of. Schacht et al. 1994; Hahn et al. 1994). Word actors combine objectoriented features with concurrency yielding strict lexical distribution and distributed computation in a methodologically clean way. The model assumes word actors to communicate via asynchronous message passing. An actor can send messages only to other actors it knows about, its socalled acquaintances. The arrival of a message at an actor is called an event; it triggers the execution of a method that is composed of atomic actions - among them the evaluation of grammatical predicates. As we will show, the specification of a particular message proto"
E99-1006,P92-1004,0,0.0319043,"117) 43 6 Comparison to Related Work Both Webber(1991) and Asher (1993) describe the phenomenon of abstract object anaphora and present restrictions on the set of potential antecedents. They do not, however, concern themselves with the problem of how to classify a certain pronoun or demonstrative as individual or abstract. Also, as they do not give preferences on the set of potential candidates, their approaches are not intended as attempts to resolve abstract object anaphora. Concerning anaphora resolution in dialogues, only little research has been carried out in this area to our knowledge. LuperFoy (1992) does not present a corpus study, meaning that statistics about the distribution of individual and abstract object anaphora or about the success rate of her approach are not available. Byron and Stent (1998) present extensions of the centering model (Grosz et al., 1995) for spoken dialogue and identify several problems with the model. We have chosen Strube's (1998) model for the resolution of individual anaphora as basis because it avoids the problems encountered by Byron & Stent, who also do not present data on the resolution of pronouns in dialogues and do not mention abstract object anaphor"
E99-1006,J97-1005,0,0.078926,"above the horizontal line show how often a particular class was actually marked as such by both annotators. In the rows below the line, N shows the total number of markables, while Z gives the number of agreements between the annotations. PA is percent agreement between the annotators, PE expected agreement by chance. Finally, ~ is computed by the formula P A - P E / 1 - P E . Dialogue Acts. First, turns were segmented into dialogue act units. We turned the segmentation task into a classification task by using boundaries between dialogue acts as one class and non-boundaries as the other (see Passonneau and Litman (1997) for a similar practice). In Table l, Non-Bound. and Bound. give the number of non-boundaries and boundaries actually marked by the annotators, N is the total number of possible boundary sites, while Z gives the number of agreements between the annotations. Non-Bound. Bound. N Z PA PE SW2403 3372 454 1913 1877 0.9812 0.7908 0.9100 SW3117 3332 452 1892 1866 0.9863 0.7896 0.9347 SW3241 1717 241 979 962 0.9826 0.7841 0.9200 E 8421 1147 4784 4705 0.9835 0.7890 0.9217 Table I : Dialogue Act Units Table 2 shows the results of the comparison between the annotations with respect to the classification"
E99-1006,P98-2241,0,0.251137,"owever, concern themselves with the problem of how to classify a certain pronoun or demonstrative as individual or abstract. Also, as they do not give preferences on the set of potential candidates, their approaches are not intended as attempts to resolve abstract object anaphora. Concerning anaphora resolution in dialogues, only little research has been carried out in this area to our knowledge. LuperFoy (1992) does not present a corpus study, meaning that statistics about the distribution of individual and abstract object anaphora or about the success rate of her approach are not available. Byron and Stent (1998) present extensions of the centering model (Grosz et al., 1995) for spoken dialogue and identify several problems with the model. We have chosen Strube's (1998) model for the resolution of individual anaphora as basis because it avoids the problems encountered by Byron & Stent, who also do not present data on the resolution of pronouns in dialogues and do not mention abstract object anaphora. Dagan and Itai (1991) describe a corpus-based approach to the resolution of pronouns, which is evaluated for the neuter pronoun &quot;it&quot;. Again, abstract object anaphora are not mentioned. 7 Conclusions and F"
E99-1006,J97-1002,0,0.0475363,"we define this domain in pragmatic terms. We assume that discourse entities enter the joint discourse model and are available for subsequent reference when common ground between the discourse participants is established. Our model builds on the observation that certain dialogue acts - in particular acknowledgments - signal that common ground is achieved. Our assumptions are based on Clark's (1989) theory of contributions (cf. also Traum (1994)). Each dialogue is divided into short, clearly defined dialogue acts - Initiations I and Acknowledgments A - based on the top of the hierarchy given in Carletta et al. (1997). Each sentence and each conjoined clause counts as a separate I, even if they are part of the same turn. A's do not convey semantic content but have a pragmatic function (e.g., backchannel). In addition there are utterances which function as an A but also have semantic content - these are labelled as A/I. A single I is paired with an A and they jointly form a Synchronising Unit (SU). In longer turns, each main clause functions as a separate unit along with its subordinate clauses. Single I's constitute SU's by themselves and do not require explicit acknowledgment. The assumption is that by le"
E99-1006,J96-2004,0,0.045627,"key for the test of the algorithm described in Section 4.3. 5.1 Annotation Our data consisted of five randomly selected dialogues from the Switchboard corpus of spoken telephone conversations (LDC, 1993). Two dialogues (SW2041, SW4877) were used to train the two annotators (the authors), and three further dialogues for testing (SW2403, SW3117, SW3241). The training dialogues were used for improving the annotation manual and for clarifying the annotation in borderline cases. After each step the annotations were compared using the ~ statistic as reliability measure for all classification tasks (Carletta, 1996). A t~ of 0.68 < ~ < 0.80 allows tentative conclusions while ~ &gt; 0.80 indicates reliability between the annotators. In the following tables, the rows on above the horizontal line show how often a particular class was actually marked as such by both annotators. In the rows below the line, N shows the total number of markables, while Z gives the number of agreements between the annotations. PA is percent agreement between the annotators, PE expected agreement by chance. Finally, ~ is computed by the formula P A - P E / 1 - P E . Dialogue Acts. First, turns were segmented into dialogue act units."
E99-1006,J95-2003,0,0.238131,"ertain pronoun or demonstrative as individual or abstract. Also, as they do not give preferences on the set of potential candidates, their approaches are not intended as attempts to resolve abstract object anaphora. Concerning anaphora resolution in dialogues, only little research has been carried out in this area to our knowledge. LuperFoy (1992) does not present a corpus study, meaning that statistics about the distribution of individual and abstract object anaphora or about the success rate of her approach are not available. Byron and Stent (1998) present extensions of the centering model (Grosz et al., 1995) for spoken dialogue and identify several problems with the model. We have chosen Strube's (1998) model for the resolution of individual anaphora as basis because it avoids the problems encountered by Byron & Stent, who also do not present data on the resolution of pronouns in dialogues and do not mention abstract object anaphora. Dagan and Itai (1991) describe a corpus-based approach to the resolution of pronouns, which is evaluated for the neuter pronoun &quot;it&quot;. Again, abstract object anaphora are not mentioned. 7 Conclusions and Future Work In this paper we presented a method for resolving ab"
E99-1006,P98-2204,1,0.832561,"ve <sil&gt; five boxcars of oranges there S-List: [Coming, 5 boxcars of oranges] u: uh no they're are already waiting for me there (d92a-4.3) Figure 1: Unacknowledged Turns Speaker u's second turn is an I which is not followed by an A. This means that the entity referred to in that utterance (orange warehouse) is immediately removed from the joint discourse model. Thus there in the final two turns co-specifies with Coming and not the most recent orange warehouse. 4 How to Resolve Discourse Deictic Anaphora We now turn to our method of anaphora resolution, which extends the algorithm presented in Strube (1998), in order to be able to account for discourse deictic anaphora as well as individual anaphora. 4.1 Anaphor-anteeedent Compatibility As indicated in Section 2, information provided by the subcategorisation frame of the anaphor's predicate can be used to determine the type of the referent. In the algorithm, we make use of the notion of anaphorantecedent Compatibility to distinguish between discourse deictic and individual reference. Certain predicates (notably verbs of propositional attitude) require one of their arguments to have a referent whose meaning is correlated with sentences, e.g., is"
E99-1006,C98-2236,0,\N,Missing
E99-1006,C98-2199,1,\N,Missing
I11-1038,J90-1003,0,0.146633,"om. The reviews are subdivided according to their topics. We included the three categories ”Cell Phones & Service”, ”Gourmet Food” and ”Kitchen & Housewares”. 340 • Taboada and Grieve’s Turney Adjective List (TGL) Taboada and Grieve (2004) created a list containing adjectives and their polarities based on a method described by Turney (2002). They first query a search engine for the adjective together with some manually chosen clearly positive adjectives, using the nearoperator, then they do the same with a list of negative adjectives. Finally, they calculate the point-wise mutual information (Church and Hanks, 1990) between the queries. the same amount of positive and negative labels, we take the overall polarity of the whole review as label. 4.2 Polarity Features For each segment, we estimate prior positivity and negativity scores using state-of-the-art sentiment classification methods. There are two basic ways to classify polarity. One of the most common approaches is to train a classifier on labeled data that works with a bag-of-words model or uses similar features. However, named approach will have difficulties with the short text segments our system is focused on. Another method for polarity classif"
I11-1038,P09-1075,0,0.0120844,"arge cell phone is likely bad. To take domain-dependence into account, we compile a list of common positive and negative unigrams as well as punctuation marks for each of the three topics separately. Since we need 40 reviews per topic for the evaluation only the remaining reviews are used to compile the unigram lexicon. From this data, we calculate the ratio of all occurrences of a unigram in positive reviews to its occurrences in negative reviews and use this ratio as the positivity and negativity scores, respectively. 4.2.1 Discourse Parsing We employ the discourse parser HILDA developed by duVerle and Prendinger (2009). It performs two tasks: First, it splits the review text into discourse segments which constitute the basic entities our system classifies. Second, it determines the discourse relations between segments. The actual output of HILDA is the discourse tree of a text. We convert the tree structure to a linear sequence of relations between neighboring segments. HILDA uses the set of relation labels described by Soricut and Marcu (2003) which is coarser-grained than RST and consists of 18 labels. For the experiments, we distinguished two types of relations: relations labeled as contrast and all othe"
I11-1038,H05-1043,0,0.790878,"ects of the product. Moreover, different opinions can even be uttered Integrating Neighborhood Relations. As discourse segments consist of only a few tokens, they 1 Please note that in this work we do not distinguish between C ONCESSION and C ONTRAST relations and consider both as C ONTRAST relations. In the following, we will refer to all other kind of relations as N O C ONTRAST relations. 336 Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 336–344, c Chiang Mai, Thailand, November 8 – 13, 2011. 2011 AFNLP sentence level, the system of the latter - Popescu and Etzioni (2005) - extracts opinion phrases on the subsentence level for product features. Their approaches have in common that they first extract features of a product, like the size of a camera or its weight. Then, they look for opinion words describing these features. Finally, the polarity of these terms and, thus, of the feature is determined. An even finer-grained system is presented in Kessler and Nicolov (2009). The approach aims at classifying both sentiment expressions as well as their targets using a rich set of linguistic features. However, they have not implemented the component that detects and a"
I11-1038,esuli-sebastiani-2006-sentiwordnet,0,0.00895088,"equence of relations between neighboring segments. HILDA uses the set of relation labels described by Soricut and Marcu (2003) which is coarser-grained than RST and consists of 18 labels. For the experiments, we distinguished two types of relations: relations labeled as contrast and all other relations. We refer to this class as ncontrast. We model these two relations in the Markov logic framework as described in Section 3.2. We want to investigate whether the use of a discourse parser is improving fine-grained sentiment analysis. The discourse segments determined by • SentiWordNet (SWN) SWN (Esuli and Sebastiani, 2006) is a lexical resource that contains positivity-scores, negativity-scores and objectivity-scores for WordNet (Fellbaum, 1998) synsets. The scores are between 0.0 and 1.0 and all three scores for a synset sum up to 1.0. For our system, we only regard positivity scores and negativity scores. We use a part-of-speech tagger and take the first word sense. 3 We used the negation indicators no, cannot, not, none, nothing, nowhere, neither, nor, nobody, hardly, scarcely, barely and all negations of auxiliaries modals ending on n’t, like don’t or won’t. 341 majority baseline SVM MLN polarity MLN neighb"
I11-1038,P09-1026,0,0.0117884,"scores from different sentiment lexicons with information about discourse relations between neighboring segments, and evaluate the method on product reviews. 2 The impact of discourse relations for sentiment analysis is investigated in Asher et al. (2009). The authors conduct a manual study in which they represent opinions in text as shallow semantic feature structures. These are combined to an overall opinion using hand-written rules based on manually annotated discourse relations. An interdependent classification scenario to determine polarity as well as discourse relations is presented in Somasundaran and Wiebe (2009). In their approach, text is modeled as opinion graphs including discourse information. In Somasundaran et al. (2009) the authors try alternative machine learning scenarios with combinations of supervised and unsupervised methods for the same task. However, they do not determine discourse relations automatically but use manual annotations. Related Work Methods for fine-grained sentiment analysis are developed by Hu and Liu (2004), Ding et al. (2008) and Popescu and Etzioni (2005). While the approaches of the former two operate on the 337 3 Statistical-Relational Representation A Markov logic n"
I11-1038,D09-1018,0,0.0227588,"valuate the method on product reviews. 2 The impact of discourse relations for sentiment analysis is investigated in Asher et al. (2009). The authors conduct a manual study in which they represent opinions in text as shallow semantic feature structures. These are combined to an overall opinion using hand-written rules based on manually annotated discourse relations. An interdependent classification scenario to determine polarity as well as discourse relations is presented in Somasundaran and Wiebe (2009). In their approach, text is modeled as opinion graphs including discourse information. In Somasundaran et al. (2009) the authors try alternative machine learning scenarios with combinations of supervised and unsupervised methods for the same task. However, they do not determine discourse relations automatically but use manual annotations. Related Work Methods for fine-grained sentiment analysis are developed by Hu and Liu (2004), Ding et al. (2008) and Popescu and Etzioni (2005). While the approaches of the former two operate on the 337 3 Statistical-Relational Representation A Markov logic network is a set of pairs (Fi , wi ) where each Fi is a first-order formula and each wi a real-valued weight associate"
I11-1038,N03-1030,0,0.105826,"tive reviews and use this ratio as the positivity and negativity scores, respectively. 4.2.1 Discourse Parsing We employ the discourse parser HILDA developed by duVerle and Prendinger (2009). It performs two tasks: First, it splits the review text into discourse segments which constitute the basic entities our system classifies. Second, it determines the discourse relations between segments. The actual output of HILDA is the discourse tree of a text. We convert the tree structure to a linear sequence of relations between neighboring segments. HILDA uses the set of relation labels described by Soricut and Marcu (2003) which is coarser-grained than RST and consists of 18 labels. For the experiments, we distinguished two types of relations: relations labeled as contrast and all other relations. We refer to this class as ncontrast. We model these two relations in the Markov logic framework as described in Section 3.2. We want to investigate whether the use of a discourse parser is improving fine-grained sentiment analysis. The discourse segments determined by • SentiWordNet (SWN) SWN (Esuli and Sebastiani, 2006) is a lexical resource that contains positivity-scores, negativity-scores and objectivity-scores fo"
I11-1038,P11-2100,0,0.0854821,"Missing"
I11-1038,P02-1053,0,0.0033802,"4.1 Gold Standard Data We chose a subset of the the Multi-Domain Sentiment Dataset arranged by Blitzer et al. (2007) and annotated it for our purpose. The Multi-Domain Sentiment Dataset consists of user-written product reviews downloaded from the web page http://amazon.com. The reviews are subdivided according to their topics. We included the three categories ”Cell Phones & Service”, ”Gourmet Food” and ”Kitchen & Housewares”. 340 • Taboada and Grieve’s Turney Adjective List (TGL) Taboada and Grieve (2004) created a list containing adjectives and their polarities based on a method described by Turney (2002). They first query a search engine for the adjective together with some manually chosen clearly positive adjectives, using the nearoperator, then they do the same with a list of negative adjectives. Finally, they calculate the point-wise mutual information (Church and Hanks, 1990) between the queries. the same amount of positive and negative labels, we take the overall polarity of the whole review as label. 4.2 Polarity Features For each segment, we estimate prior positivity and negativity scores using state-of-the-art sentiment classification methods. There are two basic ways to classify pola"
I11-1038,P06-2063,0,0.0494804,"Missing"
I11-1038,H05-2017,0,\N,Missing
I11-1038,P07-1056,0,\N,Missing
I11-2001,O97-1002,0,0.259058,"Missing"
I11-2001,nastase-etal-2010-wikinet,1,0.745591,"sisting of graphical and textual browsing tools. This allows the user to inspect the knowledge base to which WikiNetTK is applied. The application-oriented part of the toolkit provides various functionalities: access to various types of information in the knowledge base as well as methods for computing association paths and relatedness measures. The system is applied to a large-scale multilingual concept network obtained by extracting and combining various sources of information from Wikipedia. 1 2 Data WikiNetTK is applied to WikiNet, a repository of world knowledge extracted from Wikipedia (Nastase et al., 2010). It is derived from the category and article network, disambiguation, redirect, cross-language, infobox and textual content of Wikipedia pages. It is organized as a concept network – it separates concepts and their lexicalizations, and contains relations between concepts – in a manner similar to WordNet. Concepts have lexicalizations in numerous languages. With WikiNet’s 3.7 Million concepts and 40 Million relations (instantiating 656 relation types), efficiency in data management becomes an issue. Manual analysis of the data is also problematic. WikiNetTK addresses both these issues. A fast"
I11-2001,J06-1003,0,\N,Missing
I17-1083,P13-1008,0,0.419409,"Automatic Content Extraction evaluation (ACE 2005) defines three challenging sub-tasks: Entity mention detection, the task of finding mentions of predefined entity types like persons and organizations; event trigger detection, the task of finding words, mostly verbs or nominalizations, indicating an event from a set of predefined event types; and event argument identification, the identification of entity mentions1 playing a role in the events, as well as the identification of the roles they play. When we look at the evaluations in three of the most influential recent event extraction papers (Li et al., 2013, 2014; Chen et al., 2015; Nguyen et al., 2016) we note that argument identification performance is low, ranging from 52.7 to 55.4 F1 . There are multiple reasons for the low performance. First, argument identification suffers from 1. We observe that syntactic complexity is a crucial factor for argument identification. Argument identification performance highly correlates with dependency path length (Section 3.1). 1 In this work we make no distinction between ‘entity’, ‘time’, and ‘place’ for the sake of simplicity. 822 Proceedings of the The 8th International Joint Conference on Natural Langu"
I17-1083,P15-1017,0,0.207898,"ction evaluation (ACE 2005) defines three challenging sub-tasks: Entity mention detection, the task of finding mentions of predefined entity types like persons and organizations; event trigger detection, the task of finding words, mostly verbs or nominalizations, indicating an event from a set of predefined event types; and event argument identification, the identification of entity mentions1 playing a role in the events, as well as the identification of the roles they play. When we look at the evaluations in three of the most influential recent event extraction papers (Li et al., 2013, 2014; Chen et al., 2015; Nguyen et al., 2016) we note that argument identification performance is low, ranging from 52.7 to 55.4 F1 . There are multiple reasons for the low performance. First, argument identification suffers from 1. We observe that syntactic complexity is a crucial factor for argument identification. Argument identification performance highly correlates with dependency path length (Section 3.1). 1 In this work we make no distinction between ‘entity’, ‘time’, and ‘place’ for the sake of simplicity. 822 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 822"
I17-1083,N16-1034,0,0.126415,"CE 2005) defines three challenging sub-tasks: Entity mention detection, the task of finding mentions of predefined entity types like persons and organizations; event trigger detection, the task of finding words, mostly verbs or nominalizations, indicating an event from a set of predefined event types; and event argument identification, the identification of entity mentions1 playing a role in the events, as well as the identification of the roles they play. When we look at the evaluations in three of the most influential recent event extraction papers (Li et al., 2013, 2014; Chen et al., 2015; Nguyen et al., 2016) we note that argument identification performance is low, ranging from 52.7 to 55.4 F1 . There are multiple reasons for the low performance. First, argument identification suffers from 1. We observe that syntactic complexity is a crucial factor for argument identification. Argument identification performance highly correlates with dependency path length (Section 3.1). 1 In this work we make no distinction between ‘entity’, ‘time’, and ‘place’ for the sake of simplicity. 822 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 822–831, c Taipei, Taiwan"
I17-1083,W02-1001,0,0.0143844,"s about 20 seconds per epoch. mi exi yi = P xj j mj e The above equation gives the probability for a particular argument type, yi , where x is the input vector to softmax, and m is a binary vector indicating allowed types. Note that yi > 0 only if the respective argument type is allowed. The top part of Figure 3 visualizes the softmax component. The input vector is first reduced to 29 dimensions (28 argument types and one ‘null’ type), multiplied with the restriction mask, and forwarded to our modified softmax. Parameter Averaging Inspired by the Averaged Perceptron (Freund and Shapire, 1999; Collins, 2002) we do not use the learned parameters directly for prediction. Instead, in each epoch we keep a moving average of the parameters: θV = αθT + (1 − α)θV −1 Here, θT is the current weight vector after training epoch T , θV −1 is the averaged weight vector before the new update, and α is the fraction of how much θT influences θV . We set α = 0.1. θV is then used during testing. Note that this procedure does not change the training in any way. With the formulation above, older weight vectors have less influence on θV than more recent vectors. After every training epoch, we evaluate θV on the develo"
I17-1083,L16-1376,0,0.0705768,"baseline in the setting we investigate in this paper. Xu et al. (2015) use LSTMs for relation extraction. Similar to our work, they use LSTMs to compute a representation of the shortest dependency path connecting two related entities. While the general learning scheme is similar to our biLSTM component, they represent dependency paths differently. Instead of one lexicalized path, they construct four different representations, using only dependency labels, only words, only part-of-speech tags, and only WordNet categories. This results in four LSTMs whose output is concatenated. Roth and Lapata (2016) use dependency pathencoding LSTMs for semantic role labeling (SRL). Event extraction and SRL are similar in terms of structures: SRL also involves finding ‘triggers’ and ‘arguments’. They differ however in the nature of these structures. For example, potential arguments in SRL may be assigned to every verb in a sentence, while in event extraction, potential arguments must be assigned only to event triggers. Similar to our work, Roth and Lapata (2016) use an LSTM which processes a dependency path connecting predicate and argument. Their dependency paths mix words, part-ofspeech tags and depend"
I17-1083,D15-1206,0,0.403385,"ake no distinction between ‘entity’, ‘time’, and ‘place’ for the sake of simplicity. 822 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 822–831, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP Argument type Victim Instrument Artifact Attacker Target Giver 2. We propose to represent dependency paths with bidirectional Long Short-term Memory networks (biLSTMs) in order to account for their sequential and compositional nature. Using LSTMs to learn dependency path representations proved effective in other areas like relation extraction (Xu et al., 2015) and semantic role labeling (Roth and Lapata, 2016). We investigate their use for argument identification. 2.1 can be divided into segment level templates and sentence level templates. Segment level templates capture characteristics of the mentions within one event, e.g., the words between two mentions sharing a role in one event, or the head and modifier of nominal modifications like ‘IBM CEO’. Sentence level templates capture characteristics of events sharing mentions, e.g., the roles such a mention fills, or the dependency path connecting the two triggers. The system uses two dozen feature"
I17-1083,P08-1030,0,0.041595,"ution over all argument types. We pick the class with the highest probability as our final result. However, choosing between all classes is unnecessary because not all combinations of event type, entity type and argument type are possible. For example, the argument type Vehicle can only be assigned to TRANSPORT events, and only mentions with entity type veh can be possible fillers. We modify softmax to assign zero probability to classes which are disallowed: 5.1 Experiments Data, Evaluation Metrics and Parameters We evaluate on ACE 2005 and use the same data split as most previous approaches (Ji and Grishman, 2008; Nguyen et al., 2016, inter alia). We follow standard evaluation procedures: An event argument is correct, if its span and role match a reference argument (Ji and Grishman, 2008). Section 1 gives an overview over the annotations provided in ACE 2005. Because we want to measure argument identification performance by itself, we must ensure that compared systems use the same entity mention and trigger predictions; the best way to ensure this is to set both to gold. Using gold entity mentions is a common setting in event extraction (Li et al., 2013; Chen et al., 2015; Nguyen et al., 2016, inter a"
I17-1083,N15-1011,0,0.0122308,"ce. The sentence is important because the lexical contexts of trigger and mention convey valuable information for argument identification. This means that in our case a filter of width h produces features for the entire sentence, c = [c1, c2, . . . , cn−h+1 ]. We apply max-pooling afterwards, i.e., the final output of one CNN filter is given by cˆ = max(c). Our CNN uses 20 filters for filter widths 2,3,4. The middle-right part of Figure 3 exemplifies a CNN with 3 filters and filter width 2. duce bag-of-words-like representations. They were successfully applied to many NLP problems (Kim, 2014; Johnson and Zhang, 2015, inter alia). Input to our CNN is a tokenized sentence where each word at position i is replaced by a vector xi which is almost identical to the definition of vi above. The only difference is that xi contains only word embeddings. CNNs apply filters (also called kernels) to a fixed number of consecutive input vectors. Filters are then moved by a certain offset (also called a stride) and re-applied. In our case, one filter produces one feature for one position i: ci = σ(W · xi:i+h−1 + s) where σ is a non-linearity (tanh in our case), W is a weight matrix, s ∈ R is 827 5 Final Classification Fi"
I17-1083,D14-1181,0,0.00345697,"nt a sentence. The sentence is important because the lexical contexts of trigger and mention convey valuable information for argument identification. This means that in our case a filter of width h produces features for the entire sentence, c = [c1, c2, . . . , cn−h+1 ]. We apply max-pooling afterwards, i.e., the final output of one CNN filter is given by cˆ = max(c). Our CNN uses 20 filters for filter widths 2,3,4. The middle-right part of Figure 3 exemplifies a CNN with 3 filters and filter width 2. duce bag-of-words-like representations. They were successfully applied to many NLP problems (Kim, 2014; Johnson and Zhang, 2015, inter alia). Input to our CNN is a tokenized sentence where each word at position i is replaced by a vector xi which is almost identical to the definition of vi above. The only difference is that xi contains only word embeddings. CNNs apply filters (also called kernels) to a fixed number of consecutive input vectors. Filters are then moved by a certain offset (also called a stride) and re-applied. In our case, one filter produces one feature for one position i: ci = σ(W · xi:i+h−1 + s) where σ is a non-linearity (tanh in our case), W is a weight matrix, s ∈ R is 827"
I17-1083,D14-1198,0,0.0293789,"Missing"
J03-4005,N01-1025,0,0.015143,"ecall trade-off. Linguistically justified features have the inherent benefit of supporting natural explanations of the system induction, although evaluation usually does not account for them. In my view, the (linguistic) interpretation of different aspects of SVM learning in TC is an important research direction. The study of the relationship between training materials (as represented under a linguistic perspective) and the choice of kernel functions optimal for the task is a promising research line. 4. Late Reflections Support vector machines are widely adopted today for several tasks (e.g., Kudo and Matsumoto 2001). They seem to reproduce effectively the induction of separation functions from training data. The SVM inductive setting (of which the transductive one is just a derivation) is a straightforward approach to TC induction, and this book is proof. Although it leaves a large set of open problems, the book must be seen as a relevant contribution in the area of machine learning for natural language. It is a powerful means of getting acquainted with theoretical and methodological knowledge for text classification. Unfortunately, obvious gaps highly affect its completeness as a handbook in courses on"
J03-4005,J94-4002,0,0.199498,"Missing"
J03-4005,J01-4004,0,0.0776062,"Missing"
J03-4005,J00-4003,0,0.05796,"Missing"
J03-4005,M95-1005,0,0.0978105,"Missing"
J18-2002,J08-4004,0,0.0411832,"idging recognition and antecedent selection (Section 6). This is the first full bridging resolution system that attempts the unrestricted phenomenon in a real setting. All our experiments are performed on ISNotes and therefore all our claims hold only for the news genre. Although we believe the benefit of joint optimization to hold across other genres, several of our features are optimized for that particular corpus 2 We consider annotation highly reliable when κ exceeds 0.80 and marginally reliable when between 0.67 and 0.80 (Carletta 1996). The interpretation of κ is still under discussion (Artstein and Poesio 2008). 239 Computational Linguistics Volume 44, Number 2 and therefore our figures indicate the best possible performance of our approach. The adaptation to other corpora will likely need additional fine-tuning.3 Connection to previous conference publications. This article synthesizes Markert, Hou, and Strube (2012) and Hou et al. (2013a, 2013b). It provides more technical details, error analyses, and also includes the following new aspects. For the corpus, we now include a detailed analysis of our bridging cases (Section 3.3). In bridging recognition, we now use Markov Logic Networks instead of it"
J18-2002,P98-1013,0,0.260293,"Missing"
J18-2002,J08-1001,0,0.0199865,"to recognize bridging anaphors and find links to their antecedents. In Example (1), the bridging anaphors The windows, The carpets and walls can be felicitously used thanks to their part-of relation to their antecedent the Polish center.1 (1) If Mr. McDonough’s plans get executed, as much as possible of the Polish center will be made from aluminum, steel and glass recycled from Warsaw’s abundant rubble. [2 sent.] The windows will open. The carpets won’t be glued down and walls will be coated with non-toxic finishes. Bridging plays an important role in establishing entity coherence in a text. Barzilay and Lapata (2008) model local coherence with the entity grid based on coreference only. However, Example (1) does not exhibit any coreferential entity coherence, and therefore entity coherence can only be established when bridging is resolved. Furthermore, text understanding applications such as textual entailment (Mirkin, Dagan, and Pado´ 2010), context question answering (Voorhees 2001), and opinion mining (Kobayashi, Inui, and Matsumoto 2007) have been shown to benefit from bridging resolution. The main contributions presented in this article lie in the following aspects: 1. We present an English corpus (IS"
J18-2002,bjorkelund-etal-2014-extended,0,0.371063,"Missing"
J18-2002,P11-1151,0,0.0783664,"Missing"
J18-2002,P09-1092,0,0.0900894,"Missing"
J18-2002,W12-1632,0,0.674482,"Missing"
J18-2002,J96-2004,0,0.556789,"lly, we evaluate bridging resolution as a pipeline consisting of bridging recognition and antecedent selection (Section 6). This is the first full bridging resolution system that attempts the unrestricted phenomenon in a real setting. All our experiments are performed on ISNotes and therefore all our claims hold only for the news genre. Although we believe the benefit of joint optimization to hold across other genres, several of our features are optimized for that particular corpus 2 We consider annotation highly reliable when κ exceeds 0.80 and marginally reliable when between 0.67 and 0.80 (Carletta 1996). The interpretation of κ is still under discussion (Artstein and Poesio 2008). 239 Computational Linguistics Volume 44, Number 2 and therefore our figures indicate the best possible performance of our approach. The adaptation to other corpora will likely need additional fine-tuning.3 Connection to previous conference publications. This article synthesizes Markert, Hou, and Strube (2012) and Hou et al. (2013a, 2013b). It provides more technical details, error analyses, and also includes the following new aspects. For the corpus, we now include a detailed analysis of our bridging cases (Section"
J18-2002,caselli-prodanof-2006-annotating,0,0.739279,"Missing"
J18-2002,W06-3915,0,0.0297969,"us studies in Section 2.2. Section 2.3 reviews automatic algorithms for bridging resolution and Section 2.4 discusses bridging and implicit semantic role labeling. 2.1 Bridging: Theoretical Studies Theoretical studies on bridging include linguistic (Hawkins 1978; Prince 1981, 1992), psycholinguistic (Clark 1975; Clark and Haviland 1977; Garrod and Sanford 1982), pragmatic and cognitive (Erku¨ and Gundel 1987; Gundel, Hedberg, and Zacharski 2000; Matsui 2000; Schwarz 2000), as well as formal accounts (Hobbs et al. 1993; Bos, ¨ Buitelaar, and Mineur 1995; Asher and Lascarides 1998; Lobner 1998; Cimiano 2006; Irmer 2009). Our concept of bridging is closest to the notions of associative anaphora in Hawkins (1978) and (noncontained) inferrables in Prince (1981): noun phrases (NPs) that are not coreferent to a previous mention but the referent of which is identifiable via a lexicosemantic, frame, or encyclopedic relation to a previous mention, with this relation not being syntactically expressed. Relation types used are very diverse and antecedents can be noun phrases, verb phrases, or even whole sentences (Clark 1975; Asher and Lascarides 1998, inter alia). ¨ Several studies, such as Hawkins (1978)"
J18-2002,T75-2034,0,0.583996,"referring back to the antecedent “The business,” which itself refers back to “The Bakersfield Supermarket.” Both of these two anaphors refer to the same entity as their antecedents. Differently, the bridging anaphor “friends” does not refer to the same entity as its antecedent “its owner.” The phenomena illustrated in (1) and (2) have attracted a lot of interest under the heading of coreference resolution (Hobbs 1978; Hirschman and Chinchor 1997; Soon, Ng, and Lim 2001; Lee et al. 2013, 2017, inter alia). This article, however, focuses on the phenomenon illustrated in (3), known as bridging (Clark 1975) or associative anaphora (Hawkins 1978). Bridging anaphors are anaphoric noun phrases that are not coreferent but instead linked via associative relations to the antecedent. Bridging resolution has to recognize bridging anaphors and find links to their antecedents. In Example (1), the bridging anaphors The windows, The carpets and walls can be felicitously used thanks to their part-of relation to their antecedent the Polish center.1 (1) If Mr. McDonough’s plans get executed, as much as possible of the Polish center will be made from aluminum, steel and glass recycled from Warsaw’s abundant rub"
J18-2002,J93-1003,0,0.136081,"Missing"
J18-2002,C12-1050,1,0.885472,"Missing"
J18-2002,J95-2003,0,0.789352,"Missing"
J18-2002,J86-3001,0,0.388309,"Missing"
J18-2002,C96-1084,1,0.561768,"Missing"
J18-2002,C16-1177,1,0.663164,"and Lobner (1998), limit bridging to definite NPs; we, however, believe that there is no clear difference in information status between the windows, on the one hand, and walls, on the other hand, in Example (1).6 3 Unfortunately, as we explain in Section 2.2, no other English corpus that is immediately usable for the full problem of bridging resolution is currently available for us to test our system on. 4 Quantitative results for bridging recognition are very similar to the previous framework, however. 5 We do not include the work that we conducted previously (Hou, Markert, and Strube 2014; Hou 2016), as these follow very different paradigms, using rule-based and neural network approaches, respectively. None of these approaches outperform our work in this article. 6 Prince (1992) also gives examples of indefinite bridging cases, so our observation is not new. 240 Hou, Markert, and Strube Unrestricted Bridging Resolution Our bridging notion differs from Clark (1975) in that we do not include coreferential cases: We believe coreference is different both from an IS viewpoint (always being discourse-old) as well as from a computational perspective in that coreference needs different methods t"
J18-2002,D13-1077,1,0.667552,"ther genres, several of our features are optimized for that particular corpus 2 We consider annotation highly reliable when κ exceeds 0.80 and marginally reliable when between 0.67 and 0.80 (Carletta 1996). The interpretation of κ is still under discussion (Artstein and Poesio 2008). 239 Computational Linguistics Volume 44, Number 2 and therefore our figures indicate the best possible performance of our approach. The adaptation to other corpora will likely need additional fine-tuning.3 Connection to previous conference publications. This article synthesizes Markert, Hou, and Strube (2012) and Hou et al. (2013a, 2013b). It provides more technical details, error analyses, and also includes the following new aspects. For the corpus, we now include a detailed analysis of our bridging cases (Section 3.3). In bridging recognition, we now use Markov Logic Networks instead of iterative collective classification to unify the approaches to the two tasks.4 With regard to antecedent selection, we introduce several new features as well as the notion of using the discourse scope of an anaphor to adjust the set of potential antecedents it can refer back to (Section 5.3). We also now consider different evaluation"
J18-2002,N13-1111,1,0.870902,"ther genres, several of our features are optimized for that particular corpus 2 We consider annotation highly reliable when κ exceeds 0.80 and marginally reliable when between 0.67 and 0.80 (Carletta 1996). The interpretation of κ is still under discussion (Artstein and Poesio 2008). 239 Computational Linguistics Volume 44, Number 2 and therefore our figures indicate the best possible performance of our approach. The adaptation to other corpora will likely need additional fine-tuning.3 Connection to previous conference publications. This article synthesizes Markert, Hou, and Strube (2012) and Hou et al. (2013a, 2013b). It provides more technical details, error analyses, and also includes the following new aspects. For the corpus, we now include a detailed analysis of our bridging cases (Section 3.3). In bridging recognition, we now use Markov Logic Networks instead of iterative collective classification to unify the approaches to the two tasks.4 With regard to antecedent selection, we introduce several new features as well as the notion of using the discourse scope of an anaphor to adjust the set of potential antecedents it can refer back to (Section 5.3). We also now consider different evaluation"
J18-2002,D07-1114,0,0.0301264,"Missing"
J18-2002,P13-1116,0,0.0447125,"Missing"
J18-2002,J13-4004,0,0.0308425,"(the antecedent). Figure 1 shows an excerpt of a news article with three anaphoric references: “its” is a pronominal anaphor referring back to the antecedent “The business,” which itself refers back to “The Bakersfield Supermarket.” Both of these two anaphors refer to the same entity as their antecedents. Differently, the bridging anaphor “friends” does not refer to the same entity as its antecedent “its owner.” The phenomena illustrated in (1) and (2) have attracted a lot of interest under the heading of coreference resolution (Hobbs 1978; Hirschman and Chinchor 1997; Soon, Ng, and Lim 2001; Lee et al. 2013, 2017, inter alia). This article, however, focuses on the phenomenon illustrated in (3), known as bridging (Clark 1975) or associative anaphora (Hawkins 1978). Bridging anaphors are anaphoric noun phrases that are not coreferent but instead linked via associative relations to the antecedent. Bridging resolution has to recognize bridging anaphors and find links to their antecedents. In Example (1), the bridging anaphors The windows, The carpets and walls can be felicitously used thanks to their part-of relation to their antecedent the Polish center.1 (1) If Mr. McDonough’s plans get executed,"
J18-2002,D17-1018,0,0.10828,"Missing"
J18-2002,P12-1084,1,0.891789,"Missing"
J18-2002,W03-2606,1,0.847151,"Missing"
J18-2002,D14-1221,1,0.882604,"Missing"
J18-2002,meyers-etal-2004-annotating,0,0.0855715,"s” in (implicit) semantic role labeling, for example, One man in Example (2). Still, employees do occasionally try to smuggle out a gem or two. One man wrapped several diamonds in the knot of his tie. (2) In addition, implicit semantic role labeling for nominal predicates tries to link all possible implicit core roles for the nominal predicate in question. Yet not every nominal predicate under consideration is a bridging anaphor. Despite differences between implicit semantic role labeling and bridging resolution, these two tasks can benefit from each other. We explore statistics from NomBank (Meyers et al. 2004) to predict bridging anaphors (Section 4.3.2). Some of our features for bridging antecedent selection are inspired by Laparra and Rigau (2013) (Section 5.2.2). 3. ISNotes: A Corpus for Information Status ISNotes contains 50 texts from the Wall Street Journal portion of OntoNotes (Weischedel et al. 2011), in which all mentions (10,980 overall) are annotated for IS. The corpus can be downloaded from http://www.h-its.org/en/research/nlp/isnotes-corpus/. 3.1 ISNotes Annotation Scheme Information status in ISNotes. Information status describes the degree to which a discourse entity is available to"
J18-2002,E06-1015,0,0.0604157,"Missing"
J18-2002,W09-3017,0,0.475177,"Missing"
J18-2002,P02-1014,0,0.38831,"Missing"
J18-2002,W06-1612,0,0.531154,"Missing"
J18-2002,nissim-etal-2004-annotation,0,0.927923,"t only used for definite expressions. They achieved a kappa score of 0.78 for six top-level categories. However, the confusion matrix in Riester, Lorenz, and Seemann (2010) shows that the anaphoric bridging category is frequently confused with other categories: The two annotators agreed on fewer than a third of bridging anaphors. These previous corpus studies on bridging differ from ours in several ways. First, the definition of bridging is sometimes extended to include coreferential NPs 241 Computational Linguistics Volume 44, Number 2 with lexical variety (Vieira 1998) or non-anaphoric NPs (Nissim et al. 2004). Second, they put more restrictions on bridging than we do, limiting to definite NP anaphora (Poesio and Vieira 1998; Gardent and Manu´elian 2005; Caselli and Prodanof 2006; Riester, Lorenz, and Seemann 2010), to NP antecedents (all prior work), or to few relation types between anaphor and antecedent (Poesio 2004). Apart from these differences in definition of bridging, often reliability is not measured or low, especially for bridging recognition (Fraurud 1990; Poesio and Vieira 1998; Gardent and Manu´elian 2005; Nedoluzhko, M´ırovsky, ` and Pajas 2009; Riester, Lorenz, and Seemann 2010). 2.3"
J18-2002,N13-1099,0,0.0686676,"Missing"
J18-2002,N04-3012,0,0.0942611,"Missing"
J18-2002,W03-2605,0,0.16295,"Missing"
J18-2002,W04-2327,0,0.43661,"Missing"
J18-2002,poesio-etal-2002-acquiring,0,0.370081,"Missing"
J18-2002,P04-1019,0,0.924307,"Missing"
J18-2002,J04-3003,0,0.433952,"Missing"
J18-2002,J98-2001,0,0.846898,"he confusion matrix in Riester, Lorenz, and Seemann (2010) shows that the anaphoric bridging category is frequently confused with other categories: The two annotators agreed on fewer than a third of bridging anaphors. These previous corpus studies on bridging differ from ours in several ways. First, the definition of bridging is sometimes extended to include coreferential NPs 241 Computational Linguistics Volume 44, Number 2 with lexical variety (Vieira 1998) or non-anaphoric NPs (Nissim et al. 2004). Second, they put more restrictions on bridging than we do, limiting to definite NP anaphora (Poesio and Vieira 1998; Gardent and Manu´elian 2005; Caselli and Prodanof 2006; Riester, Lorenz, and Seemann 2010), to NP antecedents (all prior work), or to few relation types between anaphor and antecedent (Poesio 2004). Apart from these differences in definition of bridging, often reliability is not measured or low, especially for bridging recognition (Fraurud 1990; Poesio and Vieira 1998; Gardent and Manu´elian 2005; Nedoluzhko, M´ırovsky, ` and Pajas 2009; Riester, Lorenz, and Seemann 2010). 2.3 Bridging: Computational Approaches Most computational approaches for resolving bridging focus on antecedent selectio"
J18-2002,W97-1301,0,0.869618,"Missing"
J18-2002,prasad-etal-2008-penn,0,0.178129,"Missing"
J18-2002,D11-1099,0,0.0607682,"Missing"
J18-2002,E12-1081,0,0.687179,"Missing"
J18-2002,P10-1005,0,0.0780066,"Missing"
J18-2002,riester-etal-2010-recursive,0,0.25416,"Missing"
J18-2002,E14-3006,0,0.0396792,"Missing"
J18-2002,S10-1008,0,0.0267878,"le Labeling Semantic role labeling is the task of assigning semantic roles (such as Agent or Theme) to the semantic arguments associated with a predicate (e.g., a verb or a noun). In frame semantics (Baker, Fillmore, and Lowe 1998), core semantic roles (also called Core Frame Elements) are essential to the meaning of semantic situations while non-core semantic roles (e.g., time, manner) are less central. The majority of work on semantic role labeling only recognizes semantic arguments from the sentence where the predicate is present and thus ignores arguments from the wider discourse context. Ruppenhofer et al. (2010) organized a shared task to 243 Computational Linguistics Volume 44, Number 2 address the issue of non-local (implicit) argument identification for nominal and verbal predicates. There is partial overlap between bridging resolution and implicit semantic role labeling (i.e., in some bridging cases, antecedents are implicit semantic roles of bridging anaphors). However, bridging resolution considers all possible nominal bridging anaphors in running text. Some bridging anaphors are not considered “nominal predicates” in (implicit) semantic role labeling, for example, One man in Example (2). Still"
J18-2002,D09-1151,0,0.575118,"Missing"
J18-2002,D09-1018,0,0.0894786,"Missing"
J18-2002,J01-4004,0,0.672971,"Missing"
J18-2002,P14-2120,0,0.0662945,"Missing"
J18-2002,J00-4003,0,0.539882,"Missing"
J18-2002,P97-1072,0,0.83894,"Missing"
J18-2002,D14-1222,1,\N,Missing
J99-3001,P87-1022,0,0.972084,"Missing"
J99-3001,C88-1021,0,0.0910327,"e comprehensive theory of discourse understanding based on linguistic, attentional, and intentional layers, the centering model can be considered the first principled attempt to deal with preference orders for plausible antecedent selection for anaphors. Its predecessors were entirely heuristic approaches to anaphora resolution. These were concerned with various criteria--beyond strictly grammatical constraints such as agreement--for the optimization of the referent selection process based on preferential choices. An elaborate description of several of these preference criteria is supplied by Carbonell and Brown (1988) who discuss, among others, heuristics involving case role filling, semantic and pragmatic alignment, syntactic parallelism, syntactic topicalization, and intersentential recency. Given such a wealth of criteria one may either try to order them a priori in terms of importance or--as was proposed by the majority of researchers in this field-define several scoring functions that compute flexible orderings on the fly. These combine the variety of available evidence, each one usually annotated by a specific weight factor, and, finally, map the weights to a single salience score (Rich and LuperFoy"
J99-3001,T75-2034,0,0.865862,"Missing"
J99-3001,C90-3063,0,0.0650446,"ap the weights to a single salience score (Rich and LuperFoy 1988; Haji~ovG KuboG and Kubo~ 1992; Lappin and Leass 1994) These heuristics helped to improve the performance of discourse-understanding systems through significant reductions of the available search-space for antecedents. Their major drawback is that they require a great deal of skilled hand-crafting that, unfortunately, usually does not scale in broader application domains. Hence, proposals were made to replace these high-level ""symbolic"" categories by statistically interpreted occurrence patterns derived from large text corpora (Dagan and Itai 1990). Preferences then reflect patterns of statistically significant lexical usage rather than introspective abstractions of linguistic patterns such as syntactic parallelism or pragmatic alignment. Among the heuristic approaches to anaphora resolution, those which consider the identification of heuristics a machine learning (ML) problem are particularly interesting, since their heuristics dynamically adapt to the textual data. Furthermore, ML procedures operate on incomplete parses (hence, they accept noisy data), which dis337 Computational Linguistics Volume 25, Number 3 tinguishes them from the"
J99-3001,C86-1119,0,0.0864974,"Missing"
J99-3001,E99-1006,1,0.82756,"Missing"
J99-3001,P83-1007,0,0.469828,"description level. Computational linguists have recognized the need to account for referential ambiguities in discourse and have developed various theories centered around the notion of discourse focus (Grosz 1977; Sidner 1983). In a seminal paper, Grosz and Sidner (1986) wrapped up the results of their research and formulated a model in which three levels of discourse coherence are distinguished--attention, intention, and discourse segment structure. While this paper gives a comprehensive picture of a complex, yet not explicitly spelled-out theory of discourse coherence, the centering model (Grosz, Joshi, and Weinstein, 1983, 1995) marked a major step in clarifying the relationship between attentional states and (local) discourse segment structure. More precisely, the centering model accounts for the interactions between local coherence and preferential choices of referring expressions. It relates differences in coherence (in part) to varying demands on inferences as required by different types of referring expressions, given a particular attentional state of the hearer in a discourse setting (Grosz, Joshi, and Weinstein 1995, 204-205). The claim is made then that the lower the inference load put on the hearer, t"
J99-3001,J95-2003,0,0.902021,"Missing"
J99-3001,J86-3001,0,0.270633,"Philadelphia, PA 19104, USA t Computational Linguistics Group, Text Understanding Lab, Werthmannplatz 1, 79085 Freiburg, Germany (~) 1999 Association for Computational Linguistics Computational Linguistics Volume 25, Number 3 terns of language use and, thus, introduces the level of discourse context and further pragmatic factors as a complementary description level. Computational linguists have recognized the need to account for referential ambiguities in discourse and have developed various theories centered around the notion of discourse focus (Grosz 1977; Sidner 1983). In a seminal paper, Grosz and Sidner (1986) wrapped up the results of their research and formulated a model in which three levels of discourse coherence are distinguished--attention, intention, and discourse segment structure. While this paper gives a comprehensive picture of a complex, yet not explicitly spelled-out theory of discourse coherence, the centering model (Grosz, Joshi, and Weinstein, 1983, 1995) marked a major step in clarifying the relationship between attentional states and (local) discourse segment structure. More precisely, the centering model accounts for the interactions between local coherence and preferential choic"
J99-3001,1997.iwpt-1.14,1,0.84595,"Missing"
J99-3001,P97-1014,1,0.845678,"ense, i.e., it allows only the consideration of immediately adjacent centering structures for establishing proper referential links. In order to extend that theory to the level of global coherence, various steps have to be taken. At the referential level, mechanisms have to be introduced to account for reference relationships that extend beyond the immediately preceding utterance. Empirical evidence for such phenomena exists in the literature and we also found the need to have such a mechanism available for longer texts. The extension of functional centering to these phenomena is presented in Hahn and Strube (1997), while Walker (1998) builds upon the centering algorithm described in Brennan, Friedman, and Pollard (1987). At the level of discourse pragmatics, a richer notion than mere reference between terms is needed to account for coherence relations such as those aimed at by Rhetorical Structure Theory 340 Strube and Hahn Functional Centering (Mann and T h o m p s o n 1988). In addition, an explicit relation to basic notions from speech act theory is also missing, t h o u g h it should be considered vital for the global coherence of discourse (Grosz and Sidner 1986). In general, it might become incre"
J99-3001,C92-1023,0,0.037871,"Missing"
J99-3001,P92-1002,0,0.0129328,"al and nominal anaphora and even functional anaphora based on the proposal we have developed in this article. It does not, however, take into account several ""hard"" issues such as plural anaphora, generic definite noun phrases, propositional anaphora, and deictic forms (but see Eckert and Strube [1999] for a treatment of discourse-deictic anaphora in dialogues within a centering-type framework). These shortcomings might be traced back to the fact that the centering model, up to now, did not consider the role of the (main) verb of the utterance under scrutiny. Other cases, such as VP anaphora (Hardt 1992), temporal anaphora (Kameyama, Passonneau, and Poesio 1993; Hitzeman, Moens, and Grover 1995) have already been examined within the centering model. The particular phenomenon of paycheck anaphora is described by Hardt (1996), though he uses only a rather simplified centering model for this work. Other cases are only dealt with in the focusing framework such as propositional anaphora (Dahl and Ball 1990). Evaluations of the centering model have so far only been carried out manually. This is clearly no longer rewarding, so appropriate computational support environments have to be provided. What"
J99-3001,C96-1088,0,0.0137294,"ses, propositional anaphora, and deictic forms (but see Eckert and Strube [1999] for a treatment of discourse-deictic anaphora in dialogues within a centering-type framework). These shortcomings might be traced back to the fact that the centering model, up to now, did not consider the role of the (main) verb of the utterance under scrutiny. Other cases, such as VP anaphora (Hardt 1992), temporal anaphora (Kameyama, Passonneau, and Poesio 1993; Hitzeman, Moens, and Grover 1995) have already been examined within the centering model. The particular phenomenon of paycheck anaphora is described by Hardt (1996), though he uses only a rather simplified centering model for this work. Other cases are only dealt with in the focusing framework such as propositional anaphora (Dahl and Ball 1990). Evaluations of the centering model have so far only been carried out manually. This is clearly no longer rewarding, so appropriate computational support environments have to be provided. What we have in mind is a kind of discourse structure bank and associated workbenches comparable to grammar workbenches and parse treebanks. Aone and Bennett (1994), for example, report on a GUI-based Discourse Tagging Tool (DTT)"
J99-3001,E95-1035,0,0.0202363,"Missing"
J99-3001,C96-1094,0,0.0090448,"s theme and rheme in the sense of the functional sentence perspective (FSP) (Firbas 1974). Viewed from this perspective, the theme/rheme-hierarchy of utterance Ui is determined by the Cf(Ui_l). Elements of Ui that are contained in Cf(Ui-1) are less rhematic than those not contained in Cf(Ui-1). He then concludes that the Cb(Ui) must be the theme of the current utterance. Rambow does not exploit the information structure of utterances to determine the Cf ranking but formulates it on the basis of linear textual precedence among the relevant discourse entities. In order to analyze Turkish texts, Hoffman (1996, 1998) distinguishes between the information structure of utterances and centering, since both constructs are assigned different functions for text understanding. A hearer exploits the information structure of an utterance to update his discourse model, and he applies the centering constraints in order to connect the current utterance to the previous discourse. Hoffman describes the information structure of an utterance in terms of topic (theme) and comment (rheme). The comment is split again into focus and (back)ground (see also Vallduvi [1990] and Vallduvf and Engdahl [1996]). Based on prev"
J99-3001,P86-1031,0,0.0567532,"Missing"
J99-3001,P93-1010,0,0.0837112,"Missing"
J99-3001,C96-1021,0,0.0237166,"theory of preferential anaphora resolution, one should clearly stress the different goals behind heuristics-based systems, such as the ones just discussed, and the model of centering. Heuristic approaches combine introspectively acquired descriptive evidence and attempt to optimize reference resolution performance by proper evidence ""engineering"". This is often done in an admittedly ad hoc way, requiring tricky retuning when new evidence is added (Rich and LuperFoy 1988). On the other hand, many of these systems work in a real-world environment (Rich and LuperFoy 1988; Lappin and Leass 1994; Kennedy and Boguraev 1996) in which noisy data and incomplete, sometimes even faulty, analysis results have to be accounted for. The centering model differs from these considerations in that it aims at unfolding a unified theory of discourse coherence at the linguistic, attentional, and intentional level (Grosz and Sidner 1986); hence, the search for a more principled, theory-based solution, but also the need for (almost) perfect linguistic analyses in terms of parsing and semantic interpretation. 7. C o n c l u s i o n In this paper, we provided a novel account for ordering the forward-looking center list, a major con"
J99-3001,J94-4002,0,0.806272,"s involving case role filling, semantic and pragmatic alignment, syntactic parallelism, syntactic topicalization, and intersentential recency. Given such a wealth of criteria one may either try to order them a priori in terms of importance or--as was proposed by the majority of researchers in this field-define several scoring functions that compute flexible orderings on the fly. These combine the variety of available evidence, each one usually annotated by a specific weight factor, and, finally, map the weights to a single salience score (Rich and LuperFoy 1988; Haji~ovG KuboG and Kubo~ 1992; Lappin and Leass 1994) These heuristics helped to improve the performance of discourse-understanding systems through significant reductions of the available search-space for antecedents. Their major drawback is that they require a great deal of skilled hand-crafting that, unfortunately, usually does not scale in broader application domains. Hence, proposals were made to replace these high-level ""symbolic"" categories by statistically interpreted occurrence patterns derived from large text corpora (Dagan and Itai 1990). Preferences then reflect patterns of statistically significant lexical usage rather than introspec"
J99-3001,A88-1003,0,0.0854674,"l and Brown (1988) who discuss, among others, heuristics involving case role filling, semantic and pragmatic alignment, syntactic parallelism, syntactic topicalization, and intersentential recency. Given such a wealth of criteria one may either try to order them a priori in terms of importance or--as was proposed by the majority of researchers in this field-define several scoring functions that compute flexible orderings on the fly. These combine the variety of available evidence, each one usually annotated by a specific weight factor, and, finally, map the weights to a single salience score (Rich and LuperFoy 1988; Haji~ovG KuboG and Kubo~ 1992; Lappin and Leass 1994) These heuristics helped to improve the performance of discourse-understanding systems through significant reductions of the available search-space for antecedents. Their major drawback is that they require a great deal of skilled hand-crafting that, unfortunately, usually does not scale in broader application domains. Hence, proposals were made to replace these high-level ""symbolic"" categories by statistically interpreted occurrence patterns derived from large text corpora (Dagan and Itai 1990). Preferences then reflect patterns of statis"
J99-3001,P96-1057,1,0.647789,"appropriate for different languages. In fact, Walker, Iida, and Cote (1994) hypothesize that the Cf ranking criteria are the only language-dependent factors within the centering model. Though evidence for many additional criteria for the Cf ranking have been brought forward in the literature, to some extent consensus has emerged that grammatical roles play a major role in making ranking decisions (e.g., whether the referential expression appears as the grammatical subject, direct object, or indirect object of an utterance). Our own work on the centering model 1 (Strube and Hahn 1996; Hahn and Strube 1996) brings in evidence from German, a free-word-order language in which grammatical role information is far less predictive of the organization of centers than for fixed-word-order languages such as English. In establishing proper referential relations, we found the functional information structure of the utterances to be much more relevant. By this we mean indicators of whether or not a discourse entity in the current utterance refers to another discourse entity already introduced by previous utterances in the discourse. Borrowing terminology from Prince (1981, 1992), an entity that does refer t"
J99-3001,P98-2204,1,0.91819,"in the fact that nominal anaphors are far more constrained by conceptual criteria than pronominal ones. Thus, the chance of properly resolving a nominal anaphor, even when ranked at a lower position in the center lists, is greater than for pronominal anaphors. By shifting our evaluation criteria away from resolution success data to structural conditions reflecting the proper ordering of center lists (in particular, we focus on the most highly ranked item of the forward-looking centers), these criteria are intended to compensate for the a significant improvement in the results, is proposed in Strube (1998). 21 Mtiller, Heiner. 1974. Geschichten aus der Produktion 2. Rotbuch Verlag, Berlin. (""Liebesgeschichte,"" pages 57~2.) 330 Strube and Hahn Functional Centering Table 18 Quantitative distribution of centering transitions. Grammatical Grammatical & F A ante &gt; FA FunC 167 158 41 23 102 226 24 37 197 131 35 26 309 25 51 4 17 42 9 7 28 32 9 6 37 28 7 3 43 23 8 1 50 12 13 0 CONTINUE RETAIN SMOOTH-SHIFT ROUGH-SHIFT 31 19 15 14 31 19 17 12 32 18 15 14 32 18 16 13 36 15 18 10 CONTINUE RETAIN SMOOTH-SHIFT ROUGH-SHIFT 97 330 56 60 226 209 67 41 171 272 46 54 272 172 59 40 395 52 82 14 Transition Types N"
J99-3001,E95-1033,1,0.919915,"ketchily dealt with in the centering literature, e.g., by asserting that the entity in question ""is realized but not directly realized"" (Grosz, Joshi, and Weinstein 1995, 217). Furthermore, the distinction between these two kinds of realization is not part of the centering mechanisms but delegated to the underlying semantic theory. We will develop arguments for how to discern inferable discourse entities and relate them properly to their antecedent at the center level. The ordering constraints we supply account for all of the types of anaphora mentioned above, including (pro)nominal anaphora (Strube and Hahn 1995; Hahn and Strube 1996). This claim will be validated by a substantial body of empirical data in Section 5. Our third contribution relates to the w a y the results of centering-based anaphora resolution are usually evaluated. Basically, we argue that rather than counting resolution rates for anaphora or comparing isolated transition types holding among head positions in the center lists--preferred transition types stand for a high degree of local coherence, while less preferred ones signal that the underlying discourse might lack coherence--one should consider adjacent transition pairs and ann"
J99-3001,P96-1036,1,0.927266,"ers about the ranking criteria appropriate for different languages. In fact, Walker, Iida, and Cote (1994) hypothesize that the Cf ranking criteria are the only language-dependent factors within the centering model. Though evidence for many additional criteria for the Cf ranking have been brought forward in the literature, to some extent consensus has emerged that grammatical roles play a major role in making ranking decisions (e.g., whether the referential expression appears as the grammatical subject, direct object, or indirect object of an utterance). Our own work on the centering model 1 (Strube and Hahn 1996; Hahn and Strube 1996) brings in evidence from German, a free-word-order language in which grammatical role information is far less predictive of the organization of centers than for fixed-word-order languages such as English. In establishing proper referential relations, we found the functional information structure of the utterances to be much more relevant. By this we mean indicators of whether or not a discourse entity in the current utterance refers to another discourse entity already introduced by previous utterances in the discourse. Borrowing terminology from Prince (1981, 1992), an e"
J99-3001,J94-2006,0,0.572774,"tatus of discourse entities to determine the current discourse focus. However, a 336 Strube and Hahn Functional Centering common area of criticism of these approaches is the diversity of data structures they require. These data structures are likely to hide the underlying linguistic regularities, because they promote the mix of preference and data structure considerations in the focusing algorithms. As an example, Sidner (1983, 292ff.) distinguishes between an Actor Focus and a Discourse Focus, as well as corresponding lists, viz. Potential Actor Focus List and Potential Discourse Focus List. Suri and McCoy (1994) in their RAFT/RAPR approach use grammatical roles for ordering the focus lists and make a distinction between Subject Focus, Current Focus, and corresponding lists. Both focusing algorithms prefer an element that represents the Focus to the elements in the list when the anaphoric expression under consideration is not the agent (for Sidner) or the subject (for Suri and McCoy). Relating these approaches to our proposal, they already exhibit a weak preference for a single hearer-old (more precisely, evoked) discourse element. Dahl and Ball (1990), describing the anaphora resolution module of the"
J99-3001,P89-1031,0,0.226704,"st set for success rate evaluation. Hemingway NYT English 274 153 2785 302 233 4546 576 386 7331 3rd pers. & poss. pron. sentences words Writers FAZ 299 186 3195 320 394 8005 German 619 580 11200 5.1.2 Method. The evaluation was carried out manually by the authors, supported by a small-scale discourse annotation tool. We used the following guidelines for our evaluation: We did not assume any world knowledge as part of the anaphora resolution process. Only agreement criteria and sortal constraints were applied. We did not account for false positives and error chains, but marked the latter (see Walker 1989). We use Kameyama&apos;s (1998) specifications for dealing with complex sentences (for a description, see Section 3). Following Walker (1989), a discourse segment is defined as a paragraph unless its first sentence has a pronoun in subject position or a pronoun whose syntactic features do not match the syntactic features of any of the preceding sentence-internal noun phrases. Also, at the beginning of a segment, anaphora resolution is preferentially performed within the same utterance. According to the preference for intersentential candidates in the original centering model, we defined the followi"
J99-3001,J94-2003,0,0.325741,"Missing"
J99-3001,J90-3001,0,\N,Missing
J99-3001,C98-2199,1,\N,Missing
judea-etal-2012-concept,S07-1007,0,\N,Missing
judea-etal-2012-concept,N10-1091,0,\N,Missing
judea-etal-2012-concept,H92-1045,0,\N,Missing
judea-etal-2012-concept,I11-2001,1,\N,Missing
judea-etal-2012-concept,nastase-etal-2010-wikinet,1,\N,Missing
K19-1021,E17-1088,0,0.0541429,"data). Following that, we motivate our selection of test languages and outline the subword-informed representation methods compared in our evaluation. Types of Data Scarcity. The majority of languages in the world still lack basic language technology, and progress in natural language processing is largely hindered by the lack of annotated task data that can guide machine learning models (Agi´c et al., 2016; Ponti et al., 2018). However, many languages face another challenge: the lack of large unannotated text corpora that can be used to induce useful general features such as word embeddings (Adams et al., 2017; Fang and Cohn, 2017; Ponti et al., 2018):2 i.e. WE data. The absence of data has over the recent years materialized the proxy fallacy. That is, methods tailored for low-resource languages are typically tested only by proxy, simulating low-data regimes exclusively on resource-rich languages (Agi´c et al., 2017). While this type of evaluation is useful for analyzing the main properties of the intended lowresource methods in controlled in vitro conditions, a complete evaluation should also provide results on true low-resource languages in vivo. In this paper we therefore conduct both types of e"
K19-1021,Q16-1022,0,0.0419405,"Missing"
K19-1021,P17-2093,0,0.0157947,"at, we motivate our selection of test languages and outline the subword-informed representation methods compared in our evaluation. Types of Data Scarcity. The majority of languages in the world still lack basic language technology, and progress in natural language processing is largely hindered by the lack of annotated task data that can guide machine learning models (Agi´c et al., 2016; Ponti et al., 2018). However, many languages face another challenge: the lack of large unannotated text corpora that can be used to induce useful general features such as word embeddings (Adams et al., 2017; Fang and Cohn, 2017; Ponti et al., 2018):2 i.e. WE data. The absence of data has over the recent years materialized the proxy fallacy. That is, methods tailored for low-resource languages are typically tested only by proxy, simulating low-data regimes exclusively on resource-rich languages (Agi´c et al., 2017). While this type of evaluation is useful for analyzing the main properties of the intended lowresource methods in controlled in vitro conditions, a complete evaluation should also provide results on true low-resource languages in vivo. In this paper we therefore conduct both types of evaluation. Note that"
K19-1021,E17-2040,0,0.0287154,"Missing"
K19-1021,C18-1139,0,0.0675297,"Missing"
K19-1021,E17-2067,0,0.0188003,"word’s meaning can be inferred from the meaning of its constituent (i.e., subword) parts. Instead of treating each word as an atomic unit, subword-informed neural architectures reduce data sparsity by relying on parameterization at the level of subwords (Bojanowski et al., 2017; Pinter et al., 2017; Chaudhary et al., 2018; Kudo, 2018). An increasing body of work focuses on various aspects of subword-informed representation learning such as segmentation of words into subwords and composing subword embeddings into word representations (Lazaridou et al., 2013; Cotterell and Sch¨utze, 2015, 2018; Avraham and Goldberg, 2017; Vania and Lopez, 2017; Kim et al., 2018; Zhang et al., 2018; Zhao et al., 2018, inter alia).1 The increased parameter sharing ability of such models is especially relevant for learning embeddings of rare and unseen words. Therefore, the importance of subword-level knowledge should be even more pronounced in low-data regimes for truly low-resource languages. Yet, a systematic study focusing exactly on the usefulness of subword information in such settings is currently missing in the literature. In this work, we fill this gap by providing a comprehensive analysis of subword-informed representa"
K19-1021,Q17-1010,0,0.661817,"tity typing (Zhu et al., 2019), neural machine translation (Sennrich et al., 2016; Luong and Manning, 2016; Lample et al., 2018; Durrani et al., 2019), or general and rare word similarity (Pilehvar et al., 2018; Zhu et al., 2019). The subword-informed word representation architectures leverage the internal structure of words and assume that a word’s meaning can be inferred from the meaning of its constituent (i.e., subword) parts. Instead of treating each word as an atomic unit, subword-informed neural architectures reduce data sparsity by relying on parameterization at the level of subwords (Bojanowski et al., 2017; Pinter et al., 2017; Chaudhary et al., 2018; Kudo, 2018). An increasing body of work focuses on various aspects of subword-informed representation learning such as segmentation of words into subwords and composing subword embeddings into word representations (Lazaridou et al., 2013; Cotterell and Sch¨utze, 2015, 2018; Avraham and Goldberg, 2017; Vania and Lopez, 2017; Kim et al., 2018; Zhang et al., 2018; Zhao et al., 2018, inter alia).1 The increased parameter sharing ability of such models is especially relevant for learning embeddings of rare and unseen words. Therefore, the importance of"
K19-1021,D18-1366,0,0.0341798,"translation (Sennrich et al., 2016; Luong and Manning, 2016; Lample et al., 2018; Durrani et al., 2019), or general and rare word similarity (Pilehvar et al., 2018; Zhu et al., 2019). The subword-informed word representation architectures leverage the internal structure of words and assume that a word’s meaning can be inferred from the meaning of its constituent (i.e., subword) parts. Instead of treating each word as an atomic unit, subword-informed neural architectures reduce data sparsity by relying on parameterization at the level of subwords (Bojanowski et al., 2017; Pinter et al., 2017; Chaudhary et al., 2018; Kudo, 2018). An increasing body of work focuses on various aspects of subword-informed representation learning such as segmentation of words into subwords and composing subword embeddings into word representations (Lazaridou et al., 2013; Cotterell and Sch¨utze, 2015, 2018; Avraham and Goldberg, 2017; Vania and Lopez, 2017; Kim et al., 2018; Zhang et al., 2018; Zhao et al., 2018, inter alia).1 The increased parameter sharing ability of such models is especially relevant for learning embeddings of rare and unseen words. Therefore, the importance of subword-level knowledge should be even more"
K19-1021,D17-1078,0,0.0241869,"ecific Data: Task Data. The maximum number of training instances for all languages is again provided in Table 1. As before, for 4 languages we simulate low-resource settings by taking only a sample of the available task data: for FGET we work with 200, 2K or 20K training instances which roughly correspond to training regimes of different data availability, while we select 300,3 1K, and 10K sentences for NER and MGET. Again, for the remaining 12 languages, we use all the available data to run the experiments. We adopt existing data splits into training, development, and test portions for MTAG (Cotterell and Heigold, 2017), and random splits for FGET (Heinzerling and Strube, 2018; Zhu et al., 2019) and NER (Pan et al., 2017). 219 3 With a smaller number of instances (e.g., 100), NER and model training was unstable and resulted in near-zero performance across multiple runs. MGET A large number of data points for scarcity simulations allow us to trace how performance on the three tasks varies in relation to the availability of WE data versus task data, and what data source is more important for the final performance. Embedding Training Setup. When training our subword-informed representations, we argue that keepi"
K19-1021,N18-2085,0,0.0176097,"ithout any available data (Kornai, 2013; Ponti et al., 2018) is a challenge left for future work. (Selection of) Languages. Both sources of data scarcity potentially manifest in degraded task performance for low-resource languages: our goal is to analyze the extent to which these factors affect downstream tasks across morphologically diverse language types that naturally come with varying data sizes to train their respective embeddings and task-based models. Our selection of test languages is therefore guided by the following goals: a) following recent initiatives (e.g. in language modeling) (Cotterell et al., 2018; Gerz et al., 2018), we aim to ensure coverage of different genealogical and typological properties; b) we aim to cover low-resource languages with varying amounts of available WE data and task-specific data. We select 16 languages in total spanning 4 broad In what follows, we further motivate our work by analyzing two different sources of data scarcity: 217 2 For instance, as of April 2019, Wikipedia is available only in 304 out of the estimated 7,000 existing languages. Agglutinative EMB FGET NER MTAG BERT Fusional Introflexive Isolating BM BXR MYV TE TR ZU EN FO GA GOT MT RUE AM HE YO ZH 4"
K19-1021,N15-1140,0,0.04425,"Missing"
K19-1021,Q18-1003,0,0.0380649,"Missing"
K19-1021,D18-1029,1,0.899767,"Missing"
K19-1021,L18-1550,0,0.0288291,"ubword-informed models are universally useful across all language types, with large gains over subword-agnostic embeddings. They also suggest that the effective use of subwords largely depends on the language (type) and the task at hand, as well as on the amount of available data for training the embeddings and taskbased models, where having sufficient in-task data is a more critical requirement. 1 Introduction and Motivation Recent studies have confirmed the usefulness of leveraging subword-level information in learning word representations (Peters et al., 2018; Heinzerling and Strube, 2018; Grave et al., 2018; Zhu et al., 2019, inter alia), and in a range of tasks such as sequence tagging (Lample et al., 2016; Akbik ∗ et al., 2018; Devlin et al., 2019), fine-grained entity typing (Zhu et al., 2019), neural machine translation (Sennrich et al., 2016; Luong and Manning, 2016; Lample et al., 2018; Durrani et al., 2019), or general and rare word similarity (Pilehvar et al., 2018; Zhu et al., 2019). The subword-informed word representation architectures leverage the internal structure of words and assume that a word’s meaning can be inferred from the meaning of its constituent (i.e., subword) parts. In"
K19-1021,L18-1473,1,0.706967,". Our main results show that subword-informed models are universally useful across all language types, with large gains over subword-agnostic embeddings. They also suggest that the effective use of subwords largely depends on the language (type) and the task at hand, as well as on the amount of available data for training the embeddings and taskbased models, where having sufficient in-task data is a more critical requirement. 1 Introduction and Motivation Recent studies have confirmed the usefulness of leveraging subword-level information in learning word representations (Peters et al., 2018; Heinzerling and Strube, 2018; Grave et al., 2018; Zhu et al., 2019, inter alia), and in a range of tasks such as sequence tagging (Lample et al., 2016; Akbik ∗ et al., 2018; Devlin et al., 2019), fine-grained entity typing (Zhu et al., 2019), neural machine translation (Sennrich et al., 2016; Luong and Manning, 2016; Lample et al., 2018; Durrani et al., 2019), or general and rare word similarity (Pilehvar et al., 2018; Zhu et al., 2019). The subword-informed word representation architectures leverage the internal structure of words and assume that a word’s meaning can be inferred from the meaning of its constituent (i.e."
K19-1021,P82-1020,0,0.744642,"Missing"
K19-1021,C18-1216,0,0.0212074,"its constituent (i.e., subword) parts. Instead of treating each word as an atomic unit, subword-informed neural architectures reduce data sparsity by relying on parameterization at the level of subwords (Bojanowski et al., 2017; Pinter et al., 2017; Chaudhary et al., 2018; Kudo, 2018). An increasing body of work focuses on various aspects of subword-informed representation learning such as segmentation of words into subwords and composing subword embeddings into word representations (Lazaridou et al., 2013; Cotterell and Sch¨utze, 2015, 2018; Avraham and Goldberg, 2017; Vania and Lopez, 2017; Kim et al., 2018; Zhang et al., 2018; Zhao et al., 2018, inter alia).1 The increased parameter sharing ability of such models is especially relevant for learning embeddings of rare and unseen words. Therefore, the importance of subword-level knowledge should be even more pronounced in low-data regimes for truly low-resource languages. Yet, a systematic study focusing exactly on the usefulness of subword information in such settings is currently missing in the literature. In this work, we fill this gap by providing a comprehensive analysis of subword-informed representation learning focused on low-resource set"
K19-1021,P18-1007,0,0.0258865,"t al., 2016; Luong and Manning, 2016; Lample et al., 2018; Durrani et al., 2019), or general and rare word similarity (Pilehvar et al., 2018; Zhu et al., 2019). The subword-informed word representation architectures leverage the internal structure of words and assume that a word’s meaning can be inferred from the meaning of its constituent (i.e., subword) parts. Instead of treating each word as an atomic unit, subword-informed neural architectures reduce data sparsity by relying on parameterization at the level of subwords (Bojanowski et al., 2017; Pinter et al., 2017; Chaudhary et al., 2018; Kudo, 2018). An increasing body of work focuses on various aspects of subword-informed representation learning such as segmentation of words into subwords and composing subword embeddings into word representations (Lazaridou et al., 2013; Cotterell and Sch¨utze, 2015, 2018; Avraham and Goldberg, 2017; Vania and Lopez, 2017; Kim et al., 2018; Zhang et al., 2018; Zhao et al., 2018, inter alia).1 The increased parameter sharing ability of such models is especially relevant for learning embeddings of rare and unseen words. Therefore, the importance of subword-level knowledge should be even more pronounced in"
K19-1021,N16-1030,0,0.0378679,"d-agnostic embeddings. They also suggest that the effective use of subwords largely depends on the language (type) and the task at hand, as well as on the amount of available data for training the embeddings and taskbased models, where having sufficient in-task data is a more critical requirement. 1 Introduction and Motivation Recent studies have confirmed the usefulness of leveraging subword-level information in learning word representations (Peters et al., 2018; Heinzerling and Strube, 2018; Grave et al., 2018; Zhu et al., 2019, inter alia), and in a range of tasks such as sequence tagging (Lample et al., 2016; Akbik ∗ et al., 2018; Devlin et al., 2019), fine-grained entity typing (Zhu et al., 2019), neural machine translation (Sennrich et al., 2016; Luong and Manning, 2016; Lample et al., 2018; Durrani et al., 2019), or general and rare word similarity (Pilehvar et al., 2018; Zhu et al., 2019). The subword-informed word representation architectures leverage the internal structure of words and assume that a word’s meaning can be inferred from the meaning of its constituent (i.e., subword) parts. Instead of treating each word as an atomic unit, subword-informed neural architectures reduce data spars"
K19-1021,N19-1423,0,0.646923,"at the effective use of subwords largely depends on the language (type) and the task at hand, as well as on the amount of available data for training the embeddings and taskbased models, where having sufficient in-task data is a more critical requirement. 1 Introduction and Motivation Recent studies have confirmed the usefulness of leveraging subword-level information in learning word representations (Peters et al., 2018; Heinzerling and Strube, 2018; Grave et al., 2018; Zhu et al., 2019, inter alia), and in a range of tasks such as sequence tagging (Lample et al., 2016; Akbik ∗ et al., 2018; Devlin et al., 2019), fine-grained entity typing (Zhu et al., 2019), neural machine translation (Sennrich et al., 2016; Luong and Manning, 2016; Lample et al., 2018; Durrani et al., 2019), or general and rare word similarity (Pilehvar et al., 2018; Zhu et al., 2019). The subword-informed word representation architectures leverage the internal structure of words and assume that a word’s meaning can be inferred from the meaning of its constituent (i.e., subword) parts. Instead of treating each word as an atomic unit, subword-informed neural architectures reduce data sparsity by relying on parameterization at the le"
K19-1021,D18-1549,0,0.0214746,"ning the embeddings and taskbased models, where having sufficient in-task data is a more critical requirement. 1 Introduction and Motivation Recent studies have confirmed the usefulness of leveraging subword-level information in learning word representations (Peters et al., 2018; Heinzerling and Strube, 2018; Grave et al., 2018; Zhu et al., 2019, inter alia), and in a range of tasks such as sequence tagging (Lample et al., 2016; Akbik ∗ et al., 2018; Devlin et al., 2019), fine-grained entity typing (Zhu et al., 2019), neural machine translation (Sennrich et al., 2016; Luong and Manning, 2016; Lample et al., 2018; Durrani et al., 2019), or general and rare word similarity (Pilehvar et al., 2018; Zhu et al., 2019). The subword-informed word representation architectures leverage the internal structure of words and assume that a word’s meaning can be inferred from the meaning of its constituent (i.e., subword) parts. Instead of treating each word as an atomic unit, subword-informed neural architectures reduce data sparsity by relying on parameterization at the level of subwords (Bojanowski et al., 2017; Pinter et al., 2017; Chaudhary et al., 2018; Kudo, 2018). An increasing body of work focuses on variou"
K19-1021,N19-1154,0,0.0223364,"nd taskbased models, where having sufficient in-task data is a more critical requirement. 1 Introduction and Motivation Recent studies have confirmed the usefulness of leveraging subword-level information in learning word representations (Peters et al., 2018; Heinzerling and Strube, 2018; Grave et al., 2018; Zhu et al., 2019, inter alia), and in a range of tasks such as sequence tagging (Lample et al., 2016; Akbik ∗ et al., 2018; Devlin et al., 2019), fine-grained entity typing (Zhu et al., 2019), neural machine translation (Sennrich et al., 2016; Luong and Manning, 2016; Lample et al., 2018; Durrani et al., 2019), or general and rare word similarity (Pilehvar et al., 2018; Zhu et al., 2019). The subword-informed word representation architectures leverage the internal structure of words and assume that a word’s meaning can be inferred from the meaning of its constituent (i.e., subword) parts. Instead of treating each word as an atomic unit, subword-informed neural architectures reduce data sparsity by relying on parameterization at the level of subwords (Bojanowski et al., 2017; Pinter et al., 2017; Chaudhary et al., 2018; Kudo, 2018). An increasing body of work focuses on various aspects of subword-in"
K19-1021,P13-1149,0,0.0307702,"leverage the internal structure of words and assume that a word’s meaning can be inferred from the meaning of its constituent (i.e., subword) parts. Instead of treating each word as an atomic unit, subword-informed neural architectures reduce data sparsity by relying on parameterization at the level of subwords (Bojanowski et al., 2017; Pinter et al., 2017; Chaudhary et al., 2018; Kudo, 2018). An increasing body of work focuses on various aspects of subword-informed representation learning such as segmentation of words into subwords and composing subword embeddings into word representations (Lazaridou et al., 2013; Cotterell and Sch¨utze, 2015, 2018; Avraham and Goldberg, 2017; Vania and Lopez, 2017; Kim et al., 2018; Zhang et al., 2018; Zhao et al., 2018, inter alia).1 The increased parameter sharing ability of such models is especially relevant for learning embeddings of rare and unseen words. Therefore, the importance of subword-level knowledge should be even more pronounced in low-data regimes for truly low-resource languages. Yet, a systematic study focusing exactly on the usefulness of subword information in such settings is currently missing in the literature. In this work, we fill this gap by p"
K19-1021,E14-2006,0,0.124496,"r (iii) both, we analyse how different data regimes affect the final task performance. 2) We experiment with 16 languages representing 4 diverse morphological types, with a focus on truly low-resource languages such as Zulu, Rusyn, Buryat, or Bambara. 3) We experiment with a variety of subword-informed representation architectures, where the focus is on unsupervised, widely portable language-agnostic methods such as the ones based on character n-grams (Luong and Manning, 2016; Bojanowski et al., 2017), Byte Pair Encodings (BPE) (Sennrich et al., 2016; Heinzerling and Strube, 2018), Morfessor (Smit et al., 2014), or BERT-style pretraining and fine-tuning (Devlin et al., 2019) which relies on WordPieces (Wu et al., 2016). We demonstrate that by tuning subword-informed models in low-resource settings we can obtain substantial gains over subwordagnostic models such as skip-gram with negative sampling (Mikolov et al., 2013) across the board. The main goal of this study is to identify viable and effective subword-informed approaches for truly low-resource languages and offer modeling guidance in relation to the target task, the language at hand, and the (un)availability of general and/or task-specific tra"
K19-1021,W18-1205,0,0.0212108,"e word itself can be appended to the subword sequence and embedded into the subword space in order to incorporate word-level information (Bojanowski et al., 2017). To encode subword order, s can be further enriched by a trainable position embedding p. We use addition to combine subword and position embeddings, namely s := s + p, which has become the de-facto standard method to encode positional information (Gehring et al., 2017; Vaswani et al., 2017; Devlin et al., 2019). Finally, the subword embedding sequence is passed to a composition function, which computes the final word representation. Li et al. (2018) and Zhu et al. (2019) have empirically verified that composition by simple addition, among other more complex composition functions, is a robust choice. Therefore, we use addition in all our experiments. Similar to Bojanowski et al. (2017); Zhu et al. (2019), we adopt skip-gram with negative sampling 218 Component Segmentation Option Morfessor BPE char n-gram Label morf bpeX charn Word token exclusion inclusion ww+ Position embedding exclusion additive pp+ addition add Composition function resentations, and can benefit from the information. Table 2: Components for constructing subwordinformed"
K19-1021,P16-1100,0,0.0620352,"Missing"
K19-1021,P17-1178,0,0.10163,"before, for 4 languages we simulate low-resource settings by taking only a sample of the available task data: for FGET we work with 200, 2K or 20K training instances which roughly correspond to training regimes of different data availability, while we select 300,3 1K, and 10K sentences for NER and MGET. Again, for the remaining 12 languages, we use all the available data to run the experiments. We adopt existing data splits into training, development, and test portions for MTAG (Cotterell and Heigold, 2017), and random splits for FGET (Heinzerling and Strube, 2018; Zhu et al., 2019) and NER (Pan et al., 2017). 219 3 With a smaller number of instances (e.g., 100), NER and model training was unstable and resulted in near-zero performance across multiple runs. MGET A large number of data points for scarcity simulations allow us to trace how performance on the three tasks varies in relation to the availability of WE data versus task data, and what data source is more important for the final performance. Embedding Training Setup. When training our subword-informed representations, we argue that keeping hyper-parameters fixed across different data points will possibly result in underfitting for larger d"
K19-1021,N18-1202,0,0.0352715,"representation method. Our main results show that subword-informed models are universally useful across all language types, with large gains over subword-agnostic embeddings. They also suggest that the effective use of subwords largely depends on the language (type) and the task at hand, as well as on the amount of available data for training the embeddings and taskbased models, where having sufficient in-task data is a more critical requirement. 1 Introduction and Motivation Recent studies have confirmed the usefulness of leveraging subword-level information in learning word representations (Peters et al., 2018; Heinzerling and Strube, 2018; Grave et al., 2018; Zhu et al., 2019, inter alia), and in a range of tasks such as sequence tagging (Lample et al., 2016; Akbik ∗ et al., 2018; Devlin et al., 2019), fine-grained entity typing (Zhu et al., 2019), neural machine translation (Sennrich et al., 2016; Luong and Manning, 2016; Lample et al., 2018; Durrani et al., 2019), or general and rare word similarity (Pilehvar et al., 2018; Zhu et al., 2019). The subword-informed word representation architectures leverage the internal structure of words and assume that a word’s meaning can be inferred from the me"
K19-1021,D18-1169,0,0.130939,"a more critical requirement. 1 Introduction and Motivation Recent studies have confirmed the usefulness of leveraging subword-level information in learning word representations (Peters et al., 2018; Heinzerling and Strube, 2018; Grave et al., 2018; Zhu et al., 2019, inter alia), and in a range of tasks such as sequence tagging (Lample et al., 2016; Akbik ∗ et al., 2018; Devlin et al., 2019), fine-grained entity typing (Zhu et al., 2019), neural machine translation (Sennrich et al., 2016; Luong and Manning, 2016; Lample et al., 2018; Durrani et al., 2019), or general and rare word similarity (Pilehvar et al., 2018; Zhu et al., 2019). The subword-informed word representation architectures leverage the internal structure of words and assume that a word’s meaning can be inferred from the meaning of its constituent (i.e., subword) parts. Instead of treating each word as an atomic unit, subword-informed neural architectures reduce data sparsity by relying on parameterization at the level of subwords (Bojanowski et al., 2017; Pinter et al., 2017; Chaudhary et al., 2018; Kudo, 2018). An increasing body of work focuses on various aspects of subword-informed representation learning such as segmentation of words"
K19-1021,D17-1010,0,0.0259855,"2019), neural machine translation (Sennrich et al., 2016; Luong and Manning, 2016; Lample et al., 2018; Durrani et al., 2019), or general and rare word similarity (Pilehvar et al., 2018; Zhu et al., 2019). The subword-informed word representation architectures leverage the internal structure of words and assume that a word’s meaning can be inferred from the meaning of its constituent (i.e., subword) parts. Instead of treating each word as an atomic unit, subword-informed neural architectures reduce data sparsity by relying on parameterization at the level of subwords (Bojanowski et al., 2017; Pinter et al., 2017; Chaudhary et al., 2018; Kudo, 2018). An increasing body of work focuses on various aspects of subword-informed representation learning such as segmentation of words into subwords and composing subword embeddings into word representations (Lazaridou et al., 2013; Cotterell and Sch¨utze, 2015, 2018; Avraham and Goldberg, 2017; Vania and Lopez, 2017; Kim et al., 2018; Zhang et al., 2018; Zhao et al., 2018, inter alia).1 The increased parameter sharing ability of such models is especially relevant for learning embeddings of rare and unseen words. Therefore, the importance of subword-level knowle"
K19-1021,P16-2067,0,0.096051,"Missing"
K19-1021,P16-1162,0,0.314176,"well as on the amount of available data for training the embeddings and taskbased models, where having sufficient in-task data is a more critical requirement. 1 Introduction and Motivation Recent studies have confirmed the usefulness of leveraging subword-level information in learning word representations (Peters et al., 2018; Heinzerling and Strube, 2018; Grave et al., 2018; Zhu et al., 2019, inter alia), and in a range of tasks such as sequence tagging (Lample et al., 2016; Akbik ∗ et al., 2018; Devlin et al., 2019), fine-grained entity typing (Zhu et al., 2019), neural machine translation (Sennrich et al., 2016; Luong and Manning, 2016; Lample et al., 2018; Durrani et al., 2019), or general and rare word similarity (Pilehvar et al., 2018; Zhu et al., 2019). The subword-informed word representation architectures leverage the internal structure of words and assume that a word’s meaning can be inferred from the meaning of its constituent (i.e., subword) parts. Instead of treating each word as an atomic unit, subword-informed neural architectures reduce data sparsity by relying on parameterization at the level of subwords (Bojanowski et al., 2017; Pinter et al., 2017; Chaudhary et al., 2018; Kudo, 2018)"
K19-1021,P17-1184,0,0.0193316,"ed from the meaning of its constituent (i.e., subword) parts. Instead of treating each word as an atomic unit, subword-informed neural architectures reduce data sparsity by relying on parameterization at the level of subwords (Bojanowski et al., 2017; Pinter et al., 2017; Chaudhary et al., 2018; Kudo, 2018). An increasing body of work focuses on various aspects of subword-informed representation learning such as segmentation of words into subwords and composing subword embeddings into word representations (Lazaridou et al., 2013; Cotterell and Sch¨utze, 2015, 2018; Avraham and Goldberg, 2017; Vania and Lopez, 2017; Kim et al., 2018; Zhang et al., 2018; Zhao et al., 2018, inter alia).1 The increased parameter sharing ability of such models is especially relevant for learning embeddings of rare and unseen words. Therefore, the importance of subword-level knowledge should be even more pronounced in low-data regimes for truly low-resource languages. Yet, a systematic study focusing exactly on the usefulness of subword information in such settings is currently missing in the literature. In this work, we fill this gap by providing a comprehensive analysis of subword-informed representation learning focused o"
K19-1021,D15-1083,0,0.0434509,"Missing"
K19-1021,C18-1153,0,0.0422342,"Missing"
K19-1021,D18-1059,0,0.0518042,"Instead of treating each word as an atomic unit, subword-informed neural architectures reduce data sparsity by relying on parameterization at the level of subwords (Bojanowski et al., 2017; Pinter et al., 2017; Chaudhary et al., 2018; Kudo, 2018). An increasing body of work focuses on various aspects of subword-informed representation learning such as segmentation of words into subwords and composing subword embeddings into word representations (Lazaridou et al., 2013; Cotterell and Sch¨utze, 2015, 2018; Avraham and Goldberg, 2017; Vania and Lopez, 2017; Kim et al., 2018; Zhang et al., 2018; Zhao et al., 2018, inter alia).1 The increased parameter sharing ability of such models is especially relevant for learning embeddings of rare and unseen words. Therefore, the importance of subword-level knowledge should be even more pronounced in low-data regimes for truly low-resource languages. Yet, a systematic study focusing exactly on the usefulness of subword information in such settings is currently missing in the literature. In this work, we fill this gap by providing a comprehensive analysis of subword-informed representation learning focused on low-resource setups. Our study centers on the following"
K19-1021,N19-1097,1,0.876896,"Missing"
kassner-etal-2008-acquiring,A97-1014,0,\N,Missing
kassner-etal-2008-acquiring,kunze-lemnitzer-2002-germanet,0,\N,Missing
kassner-etal-2008-acquiring,P05-1045,0,\N,Missing
L18-1473,Q17-1010,0,0.128628,"f rare words or words not seen during training at all is a difficult challenge in natural language processing. As a makeshift solution, systems have typically replaced such words with a generic UNK token. Recently, based on the assumption that a word’s meaning can be reconstructed from its parts, several subwordbased methods have been proposed to deal with the unknown word problem: character-based recurrent neural networks (RNN) (Luong and Manning, 2016), character-based convolutional neural networks (CNN) (Chiu and Nichols, 2016), word embeddings enriched with subword information (FastText) (Bojanowski et al., 2017), and byte-pair encoding (BPE) (Sennrich et al., 2016), among others. While pre-trained FastText embeddings are publicly available, embeddings for BPE units are commonly trained on a per-task basis (e.g. a specific language pair for machinetranslation) and not published for general use. In this work we present BPEmb, a collection of pre-trained subword embeddings in 275 languages, and make the following contributions: • We publish BPEmb, a collection of pre-trained bytepair embeddings in 275 languages; • We show the utility of BPEmb in a fine-grained entity typing task; and • We show that BPEm"
L18-1473,Q16-1026,0,0.0203632,"dings, byte-pair encoding, multilingual 1. Introduction Learning good representations of rare words or words not seen during training at all is a difficult challenge in natural language processing. As a makeshift solution, systems have typically replaced such words with a generic UNK token. Recently, based on the assumption that a word’s meaning can be reconstructed from its parts, several subwordbased methods have been proposed to deal with the unknown word problem: character-based recurrent neural networks (RNN) (Luong and Manning, 2016), character-based convolutional neural networks (CNN) (Chiu and Nichols, 2016), word embeddings enriched with subword information (FastText) (Bojanowski et al., 2017), and byte-pair encoding (BPE) (Sennrich et al., 2016), among others. While pre-trained FastText embeddings are publicly available, embeddings for BPE units are commonly trained on a per-task basis (e.g. a specific language pair for machinetranslation) and not published for general use. In this work we present BPEmb, a collection of pre-trained subword embeddings in 275 languages, and make the following contributions: • We publish BPEmb, a collection of pre-trained bytepair embeddings in 275 languages; • We"
L18-1473,P16-1061,0,0.0203378,"hows the lowest variance, i.e., it robustly yields good results across many different hyperparameter settings. In contrast, BPEmb and characterbased models show higher variance, i.e., they require more careful hyper-parameter tuning to achieve good results. Architectures. Averaging a mention’s associated embeddings is the worst architecture choice. This is expected for character-based models, but somewhat surprising for tokenbased models, given the fact that averaging is a common method for representing mentions in tasks such as entity typing (Shimaoka et al., 2017) or coreference resolution (Clark and Manning, 2016). RNNs perform slightly better than CNNs, at the cost of much longer training time. FastText BPEmb ∆ English German Russian French Spanish 62.9 65.5 71.2 64.5 66.6 65.4 66.2 70.7 63.9 66.5 2.5 0.7 -0.5 -0.6 -0.1 Chinese Japanese 71.0 62.3 72.0 61.4 1.0 -0.9 Tibetan Burmese Vietnamese Khmer Thai Lao Malay Tagalog 37.9 65.0 81.0 61.5 63.5 44.9 75.9 63.4 41.4 64.6 81.0 52.6 63.8 47.0 76.3 62.6 3.5 -0.4 0.0 -8.9 0.3 2.1 0.4 -1.2 Language Table 2: Entity typing scores for five high-resource languages (top), two high-resource languages without explicit tokenization, and eight medium- to low-resource"
L18-1473,P16-1100,0,0.0154261,"is available at https://github.com/bheinzerling/bpemb. Keywords: subword embeddings, byte-pair encoding, multilingual 1. Introduction Learning good representations of rare words or words not seen during training at all is a difficult challenge in natural language processing. As a makeshift solution, systems have typically replaced such words with a generic UNK token. Recently, based on the assumption that a word’s meaning can be reconstructed from its parts, several subwordbased methods have been proposed to deal with the unknown word problem: character-based recurrent neural networks (RNN) (Luong and Manning, 2016), character-based convolutional neural networks (CNN) (Chiu and Nichols, 2016), word embeddings enriched with subword information (FastText) (Bojanowski et al., 2017), and byte-pair encoding (BPE) (Sennrich et al., 2016), among others. While pre-trained FastText embeddings are publicly available, embeddings for BPE units are commonly trained on a per-task basis (e.g. a specific language pair for machinetranslation) and not published for general use. In this work we present BPEmb, a collection of pre-trained subword embeddings in 275 languages, and make the following contributions: • We publish"
L18-1473,P14-5010,0,0.00475365,"Missing"
L18-1473,D14-1162,0,0.0802965,"Missing"
L18-1473,D17-1035,0,0.0118348,"as measured by textual content; b) Chinese and Japanese, i.e. two high-resource languages without tokenization markers; and c) eight medium- to lowresource Asian languages. Experimental Setup. We evaluate entity typing performance with the average of strict, loose micro, and loose macro precision (Ling and Weld, 2012). For each combination of SU and encoding architecture, we perform a Tree-structured Parzen Estimator hyper-parameter search (Bergstra et al., 2011) with at least 1000 hyper-parameter search trials (English, at least 50 trials for other languages) and report score distributions (Reimers and Gurevych, 2017). See Table 3 for hyper-parameter ranges. 4. 4.1. Results and Discussion Subwords vs. Characters vs. Tokens Figure 1 shows our main result for English: score distributions of 1000+ trials for each SU and architecture. Token-based results using two sets of pre-trained embeddings (Mikolov et al., 2013; Pennington et al., 2014) are included for comparison. Subword units. BPEmb outperforms all other subword units across all architectures (BPE-RNN mean score 0.624 ± 0.029, max. 0.65). FastText performs slightly worse (FastText-RNN mean 0.617 ± 0.007, max. 0.63)6 , (Shimaoka et al., 2017). Data. We"
L18-1473,E17-1074,0,0.0453511,"Missing"
L18-1473,P16-1162,0,0.0248542,"a difficult challenge in natural language processing. As a makeshift solution, systems have typically replaced such words with a generic UNK token. Recently, based on the assumption that a word’s meaning can be reconstructed from its parts, several subwordbased methods have been proposed to deal with the unknown word problem: character-based recurrent neural networks (RNN) (Luong and Manning, 2016), character-based convolutional neural networks (CNN) (Chiu and Nichols, 2016), word embeddings enriched with subword information (FastText) (Bojanowski et al., 2017), and byte-pair encoding (BPE) (Sennrich et al., 2016), among others. While pre-trained FastText embeddings are publicly available, embeddings for BPE units are commonly trained on a per-task basis (e.g. a specific language pair for machinetranslation) and not published for general use. In this work we present BPEmb, a collection of pre-trained subword embeddings in 275 languages, and make the following contributions: • We publish BPEmb, a collection of pre-trained bytepair embeddings in 275 languages; • We show the utility of BPEmb in a fine-grained entity typing task; and • We show that BPEmb performs as well as, and for some languages better t"
L18-1473,E17-1119,0,0.0203966,"Missing"
L18-1473,E17-1055,0,0.0412815,"Missing"
mieskes-strube-2006-part,J93-2004,0,\N,Missing
mieskes-strube-2006-part,zavrel-daelemans-2000-bootstrapping,0,\N,Missing
mieskes-strube-2006-part,N03-1033,0,\N,Missing
mieskes-strube-2006-part,W00-1308,0,\N,Missing
mieskes-strube-2006-part,W02-1001,0,\N,Missing
mieskes-strube-2006-part,W03-0407,0,\N,Missing
mieskes-strube-2006-part,P98-1081,0,\N,Missing
mieskes-strube-2006-part,C98-1078,0,\N,Missing
mieskes-strube-2006-part,A00-1031,0,\N,Missing
mieskes-strube-2006-part,P02-1063,0,\N,Missing
mieskes-strube-2006-part,J99-4003,0,\N,Missing
mieskes-strube-2008-parameters,W97-0703,0,\N,Missing
mieskes-strube-2008-parameters,J97-1005,0,\N,Missing
mieskes-strube-2008-parameters,W98-1123,0,\N,Missing
mieskes-strube-2008-parameters,J91-1002,0,\N,Missing
mieskes-strube-2008-parameters,E06-1007,0,\N,Missing
mieskes-strube-2008-parameters,J97-1003,0,\N,Missing
mieskes-strube-2008-parameters,W06-2914,0,\N,Missing
mieskes-strube-2008-parameters,J02-4003,0,\N,Missing
mieskes-strube-2008-parameters,H05-1122,0,\N,Missing
mieskes-strube-2008-parameters,W02-0214,0,\N,Missing
mieskes-strube-2008-parameters,P03-1071,0,\N,Missing
mieskes-strube-2008-parameters,N03-3009,0,\N,Missing
mieskes-strube-2008-parameters,J01-1002,0,\N,Missing
mieskes-strube-2008-parameters,P01-1064,0,\N,Missing
mieskes-strube-2008-parameters,P93-1041,0,\N,Missing
mieskes-strube-2008-parameters,P99-1046,0,\N,Missing
mieskes-strube-2008-parameters,E06-1035,0,\N,Missing
mieskes-strube-2008-three,N04-4040,0,\N,Missing
mieskes-strube-2008-three,P99-1053,0,\N,Missing
mieskes-strube-2008-three,J99-4003,0,\N,Missing
mieskes-strube-2008-three,P04-1005,0,\N,Missing
mieskes-strube-2008-three,J02-4002,0,\N,Missing
mueller-etal-2008-knowledge,nissim-etal-2004-annotation,0,\N,Missing
mueller-etal-2008-knowledge,J99-3001,1,\N,Missing
mueller-etal-2008-knowledge,W97-1301,0,\N,Missing
mueller-etal-2008-knowledge,C96-1084,1,\N,Missing
mueller-etal-2008-knowledge,P07-1103,0,\N,Missing
mueller-etal-2008-knowledge,zesch-etal-2008-extracting,0,\N,Missing
mueller-etal-2008-knowledge,P07-2027,0,\N,Missing
muller-strube-2002-api,bird-etal-2000-atlas,0,\N,Missing
muller-strube-2002-api,M95-1005,0,\N,Missing
muller-strube-2002-api,W01-1612,1,\N,Missing
muller-strube-2002-api,A97-2017,0,\N,Missing
N06-1025,J96-1002,0,0.0286631,"Missing"
N06-1025,J02-3001,0,0.0686141,"Missing"
N06-1025,gimenez-marquez-2004-svmtool,0,0.0210029,"Missing"
N06-1025,N01-1008,0,0.470214,"s that constituents have with predicates, thus allowing us to include such documentlevel event descriptive information into the relations holding between referring expressions (REs). Instead of exploring different kinds of data representations, task definitions or machine learning techniques (Ng & Cardie, 2002; Yang et al., 2003; Luo et al., 2004) we focus on a few promising semantic features which we evaluate in a controlled environment. That way we try to overcome the plateauing in performance in coreference resolution observed by Kehler et al. (2004). 2 Related Work Vieira & Poesio (2000), Harabagiu et al. (2001), and Markert & Nissim (2005) explore the use of WordNet for different coreference resolution subtasks, such as resolving bridging reference, otherand definite NP anaphora, and MUC-style coreference resolution. All of them present systems which infer coreference relations from a set of potential antecedents by means of a WordNet search. Our approach to WordNet here is to cast the search results in terms of semantic similarity measures. Their output can be used as features for a learner. These measures are not specifically developed for coreference resolution but simply taken ‘off-the-shelf’ an"
N06-1025,H05-1003,0,0.054662,"e best of our knowledge, we do not know of any previous work using Wikipedia or SRL for coreference resolution. In the case of SRL, this layer of semantic context abstracts from the specific lexical expressions used, and therefore represents a higher level of abstraction than (still related) work involving predicate argument statistics. Kehler et al. (2004) observe no significant improvement due to predicate argument statistics. The improvement reported by Yang et al. (2005) is rather caused by their 193 twin-candidate model than by the semantic knowledge. Employing SRL is closer in spirit to Ji et al. (2005), who explore the employment of the ACE 2004 relation ontology as a semantic filter. 3 Coreference Resolution Using Semantic Knowledge Sources 3.1 Corpora Used To establish a competitive coreference resolver, the system was initially prototyped using the MUC-6 and MUC-7 data sets (Chinchor & Sundheim, 2003; Chinchor, 2001), using the standard partitioning of 30 texts for training and 20-30 texts for testing. Then, we moved on and developed and tested the system with the ACE 2003 Training Data corpus (Mitchell et al., 2003)1 . Both the Newswire (NWIRE) and Broadcast News (BNEWS) sections where"
N06-1025,O97-1002,0,0.00579472,"eatures In the baseline system semantic information is limited to WordNet semantic class matching. Unfortunately, a WordNet semantic class lookup exhibits problems such as coverage, sense proliferation and ambiguity4 , which make the WN CLASS feature very noisy. We enrich the semantic information available to the classifier by using semantic similarity measures based on the WordNet taxonomy (Pedersen et al., 2004). The measures we use include path length based measures (Rada et al., 1989; Wu & Palmer, 1994; Leacock & Chodorow, 1998), as well as ones based on information content (Resnik, 1995; Jiang & Conrath, 1997; Lin, 1998). In our case, the measures are obtained by computing the similarity scores between the head lemmata of each potential antecedent-anaphor pair. In order to overcome the sense disambiguation problem, we factorise over all possible sense pairs: given a candidate pair, we take the cross product of each antecedent and anaphor sense to form pairs of synsets. For each measure WN SIMILARITY, we compute the similarity score for all synset pairs, and create the following features. WN SIMILARITY BEST the highest similarity score from all hSENSEREi ,n , SENSEREj ,m i synset pairs. WN SIMILARI"
N06-1025,N04-1037,0,0.39369,"the pronoun to Moscow. SRL provides the semantic relationships that constituents have with predicates, thus allowing us to include such documentlevel event descriptive information into the relations holding between referring expressions (REs). Instead of exploring different kinds of data representations, task definitions or machine learning techniques (Ng & Cardie, 2002; Yang et al., 2003; Luo et al., 2004) we focus on a few promising semantic features which we evaluate in a controlled environment. That way we try to overcome the plateauing in performance in coreference resolution observed by Kehler et al. (2004). 2 Related Work Vieira & Poesio (2000), Harabagiu et al. (2001), and Markert & Nissim (2005) explore the use of WordNet for different coreference resolution subtasks, such as resolving bridging reference, otherand definite NP anaphora, and MUC-style coreference resolution. All of them present systems which infer coreference relations from a set of potential antecedents by means of a WordNet search. Our approach to WordNet here is to cast the search results in terms of semantic similarity measures. Their output can be used as features for a learner. These measures are not specifically develope"
N06-1025,W00-0730,0,0.10097,"Missing"
N06-1025,P04-1018,0,0.810468,"n extension of a machine learning based coreference resolution system which uses features induced from different semantic knowledge sources. These features represent knowledge mined from WordNet and Wikipedia, as well as information about semantic role labels. We show that semantic features indeed improve the performance on different referring expression types such as pronouns and common nouns. (1) 1 Introduction The last years have seen a boost of work devoted to the development of machine learning based coreference resolution systems (Soon et al., 2001; Ng & Cardie, 2002; Yang et al., 2003; Luo et al., 2004, inter alia). While machine learning has proved to yield performance rates fully competitive with rule based systems, current coreference resolution systems are mostly relying on rather shallow features, such as the distance between the coreferent expressions, string matching, and linguistic form. However, the literature emphasizes since the very beginning the relevance of world knowledge and inference for coreference resolution (Charniak, 1973). This paper explores whether coreference resolution can benefit from semantic knowledge sources. More specifically, whether a machine learning based"
N06-1025,J05-3004,0,0.0691769,"predicates, thus allowing us to include such documentlevel event descriptive information into the relations holding between referring expressions (REs). Instead of exploring different kinds of data representations, task definitions or machine learning techniques (Ng & Cardie, 2002; Yang et al., 2003; Luo et al., 2004) we focus on a few promising semantic features which we evaluate in a controlled environment. That way we try to overcome the plateauing in performance in coreference resolution observed by Kehler et al. (2004). 2 Related Work Vieira & Poesio (2000), Harabagiu et al. (2001), and Markert & Nissim (2005) explore the use of WordNet for different coreference resolution subtasks, such as resolving bridging reference, otherand definite NP anaphora, and MUC-style coreference resolution. All of them present systems which infer coreference relations from a set of potential antecedents by means of a WordNet search. Our approach to WordNet here is to cast the search results in terms of semantic similarity measures. Their output can be used as features for a learner. These measures are not specifically developed for coreference resolution but simply taken ‘off-the-shelf’ and applied to our task without"
N06-1025,P02-1014,0,0.929837,"2003 data. In this paper we present an extension of a machine learning based coreference resolution system which uses features induced from different semantic knowledge sources. These features represent knowledge mined from WordNet and Wikipedia, as well as information about semantic role labels. We show that semantic features indeed improve the performance on different referring expression types such as pronouns and common nouns. (1) 1 Introduction The last years have seen a boost of work devoted to the development of machine learning based coreference resolution systems (Soon et al., 2001; Ng & Cardie, 2002; Yang et al., 2003; Luo et al., 2004, inter alia). While machine learning has proved to yield performance rates fully competitive with rule based systems, current coreference resolution systems are mostly relying on rather shallow features, such as the distance between the coreferent expressions, string matching, and linguistic form. However, the literature emphasizes since the very beginning the relevance of world knowledge and inference for coreference resolution (Charniak, 1973). This paper explores whether coreference resolution can benefit from semantic knowledge sources. More specifical"
N06-1025,J05-1004,0,0.0236358,"ng features. WIKI RELATEDNESS BEST the highest relatedness score from all hCREi ,n , CREj ,m i category pairs. WIKI RELATEDNESS AVG the average relatedness score from all hCREi ,n , CREj ,m i category pairs. 3.6 Semantic Role Features The last semantic knowledge enhancement for the baseline system uses SRL information. In our experiments we use the ASSERT parser (Pradhan et al., 2004), an SVM based semantic role tagger which uses a full syntactic analysis to automatically identify all verb predicates in a sentence together with their semantic arguments, which are output as PropBank arguments (Palmer et al., 2005). It is often the case that the semantic arguments output by 196 the parser do not align with any of the previously identified noun phrases. In this case, we pass a semantic role label to a RE only when the two phrases share the same head. Labels have the form “ARG1 pred1 . . . ARGn predn ” for n semantic roles filled by a constituent, where each semantic argument label is always defined with respect to a predicate. Given such level of semantic information available at the RE level, we introduce two new features6 . I SEMROLE the semantic predicate pairs of REi . role argumentJ SEMROLE the sema"
N06-1025,N04-3012,0,0.0360973,"e U(nknown), T(rue) and F(alse). Note that in contrast to Ng & Cardie (2002) we interpret ALIAS as a lexical feature, as it solely relies on string comparison and acronym string matching. 194 3.4 WordNet Features In the baseline system semantic information is limited to WordNet semantic class matching. Unfortunately, a WordNet semantic class lookup exhibits problems such as coverage, sense proliferation and ambiguity4 , which make the WN CLASS feature very noisy. We enrich the semantic information available to the classifier by using semantic similarity measures based on the WordNet taxonomy (Pedersen et al., 2004). The measures we use include path length based measures (Rada et al., 1989; Wu & Palmer, 1994; Leacock & Chodorow, 1998), as well as ones based on information content (Resnik, 1995; Jiang & Conrath, 1997; Lin, 1998). In our case, the measures are obtained by computing the similarity scores between the head lemmata of each potential antecedent-anaphor pair. In order to overcome the sense disambiguation problem, we factorise over all possible sense pairs: given a candidate pair, we take the cross product of each antecedent and anaphor sense to form pairs of synsets. For each measure WN SIMILARI"
N06-1025,N04-1030,0,0.0618894,"pairs. That is, we take the cross product of each antecedent and anaphor category to form pairs of ‘Wikipedia synsets’. For each measure WIKI RELATEDNESS, we compute the relatedness score for all category pairs, and create the following features. WIKI RELATEDNESS BEST the highest relatedness score from all hCREi ,n , CREj ,m i category pairs. WIKI RELATEDNESS AVG the average relatedness score from all hCREi ,n , CREj ,m i category pairs. 3.6 Semantic Role Features The last semantic knowledge enhancement for the baseline system uses SRL information. In our experiments we use the ASSERT parser (Pradhan et al., 2004), an SVM based semantic role tagger which uses a full syntactic analysis to automatically identify all verb predicates in a sentence together with their semantic arguments, which are output as PropBank arguments (Palmer et al., 2005). It is often the case that the semantic arguments output by 196 the parser do not align with any of the previously identified noun phrases. In this case, we pass a semantic role label to a RE only when the two phrases share the same head. Labels have the form “ARG1 pred1 . . . ARGn predn ” for n semantic roles filled by a constituent, where each semantic argument"
N06-1025,J01-4004,0,0.940881,"nt Extraction (ACE) 2003 data. In this paper we present an extension of a machine learning based coreference resolution system which uses features induced from different semantic knowledge sources. These features represent knowledge mined from WordNet and Wikipedia, as well as information about semantic role labels. We show that semantic features indeed improve the performance on different referring expression types such as pronouns and common nouns. (1) 1 Introduction The last years have seen a boost of work devoted to the development of machine learning based coreference resolution systems (Soon et al., 2001; Ng & Cardie, 2002; Yang et al., 2003; Luo et al., 2004, inter alia). While machine learning has proved to yield performance rates fully competitive with rule based systems, current coreference resolution systems are mostly relying on rather shallow features, such as the distance between the coreferent expressions, string matching, and linguistic form. However, the literature emphasizes since the very beginning the relevance of world knowledge and inference for coreference resolution (Charniak, 1973). This paper explores whether coreference resolution can benefit from semantic knowledge sourc"
N06-1025,J00-4003,0,0.134349,"he semantic relationships that constituents have with predicates, thus allowing us to include such documentlevel event descriptive information into the relations holding between referring expressions (REs). Instead of exploring different kinds of data representations, task definitions or machine learning techniques (Ng & Cardie, 2002; Yang et al., 2003; Luo et al., 2004) we focus on a few promising semantic features which we evaluate in a controlled environment. That way we try to overcome the plateauing in performance in coreference resolution observed by Kehler et al. (2004). 2 Related Work Vieira & Poesio (2000), Harabagiu et al. (2001), and Markert & Nissim (2005) explore the use of WordNet for different coreference resolution subtasks, such as resolving bridging reference, otherand definite NP anaphora, and MUC-style coreference resolution. All of them present systems which infer coreference relations from a set of potential antecedents by means of a WordNet search. Our approach to WordNet here is to cast the search results in terms of semantic similarity measures. Their output can be used as features for a learner. These measures are not specifically developed for coreference resolution but simply"
N06-1025,M95-1005,0,0.252567,"pred1 . . . ARGn predn ” for n semantic roles filled by a constituent, where each semantic argument label is always defined with respect to a predicate. Given such level of semantic information available at the RE level, we introduce two new features6 . I SEMROLE the semantic predicate pairs of REi . role argumentJ SEMROLE the semantic predicate pairs of REj . role argumentFor the ACE 2003 data, 11,406 of 32,502 automatically extracted noun phrases were tagged with 2,801 different argument-predicate pairs. 4 Experiments 4.1 Performance Metrics We report in the following tables the MUC score (Vilain et al., 1995). Scores in Table 2 are computed for all noun phrases appearing in either the key or the system response, whereas Tables 3 and 4 refer to scoring only those phrases which appear in both the key and the response. We therefore discard those responses not present in the key, as we are interested in establishing the upper limit of the improvements given by our semantic features. That is, we want to define a baseline against which to establish the contribution of the semantic information sources explored here for coreference resolution. In addition, we report the accuracy score for all three types"
N06-1025,P94-1019,0,0.471927,"a lexical feature, as it solely relies on string comparison and acronym string matching. 194 3.4 WordNet Features In the baseline system semantic information is limited to WordNet semantic class matching. Unfortunately, a WordNet semantic class lookup exhibits problems such as coverage, sense proliferation and ambiguity4 , which make the WN CLASS feature very noisy. We enrich the semantic information available to the classifier by using semantic similarity measures based on the WordNet taxonomy (Pedersen et al., 2004). The measures we use include path length based measures (Rada et al., 1989; Wu & Palmer, 1994; Leacock & Chodorow, 1998), as well as ones based on information content (Resnik, 1995; Jiang & Conrath, 1997; Lin, 1998). In our case, the measures are obtained by computing the similarity scores between the head lemmata of each potential antecedent-anaphor pair. In order to overcome the sense disambiguation problem, we factorise over all possible sense pairs: given a candidate pair, we take the cross product of each antecedent and anaphor sense to form pairs of synsets. For each measure WN SIMILARITY, we compute the similarity score for all synset pairs, and create the following features. W"
N06-1025,P05-1021,0,0.134489,"contrast to Harabagiu et al. (2001), who weight WordNet relations differently in order to compute the confidence measure of the path. To the best of our knowledge, we do not know of any previous work using Wikipedia or SRL for coreference resolution. In the case of SRL, this layer of semantic context abstracts from the specific lexical expressions used, and therefore represents a higher level of abstraction than (still related) work involving predicate argument statistics. Kehler et al. (2004) observe no significant improvement due to predicate argument statistics. The improvement reported by Yang et al. (2005) is rather caused by their 193 twin-candidate model than by the semantic knowledge. Employing SRL is closer in spirit to Ji et al. (2005), who explore the employment of the ACE 2004 relation ontology as a semantic filter. 3 Coreference Resolution Using Semantic Knowledge Sources 3.1 Corpora Used To establish a competitive coreference resolver, the system was initially prototyped using the MUC-6 and MUC-7 data sets (Chinchor & Sundheim, 2003; Chinchor, 2001), using the standard partitioning of 30 texts for training and 20-30 texts for testing. Then, we moved on and developed and tested the syst"
N06-1025,P03-1023,0,0.783702,"paper we present an extension of a machine learning based coreference resolution system which uses features induced from different semantic knowledge sources. These features represent knowledge mined from WordNet and Wikipedia, as well as information about semantic role labels. We show that semantic features indeed improve the performance on different referring expression types such as pronouns and common nouns. (1) 1 Introduction The last years have seen a boost of work devoted to the development of machine learning based coreference resolution systems (Soon et al., 2001; Ng & Cardie, 2002; Yang et al., 2003; Luo et al., 2004, inter alia). While machine learning has proved to yield performance rates fully competitive with rule based systems, current coreference resolution systems are mostly relying on rather shallow features, such as the distance between the coreferent expressions, string matching, and linguistic form. However, the literature emphasizes since the very beginning the relevance of world knowledge and inference for coreference resolution (Charniak, 1973). This paper explores whether coreference resolution can benefit from semantic knowledge sources. More specifically, whether a machi"
N09-2057,P07-1041,1,0.926466,"or translating into a different language. A syntactic tree emerging as a result of such operations has to be linearized to a string of words before it can be output to the end-user. The simple and most widely used trigram LM has become a standard tool for tree linearization in English (Langkilde & Knight, 1998). For languages with less rigid word order, LM-based approaches have been shown to perform poorly (e.g., Marsi & Krahmer (2005) for Dutch), and methods relying on a range of linguistic features have been successfully applied instead (see Uchimoto et al. (2000) and Ringger et al. (2004), Filippova & Strube (2007) for Japanese and German resp.). To our knowledge, none of the linearization studies have compared a LM-based method with We show that such a differentiated approach is beneficial and that the proposed combination outperforms the method which relies solely on a LM. Hence, our results challenge the widespread attitude that trigram LMs provide an appropriate way to linearize syntactic trees in English but also indicate that they perform well in linearizing subtrees corresponding to phrases. 2 LM-based Approach Trigram models are easy to build and use, and it has been shown that more sophisticate"
N09-2057,P03-1054,0,0.0136479,"e goal of our experiments is to check the following hypotheses: 1. That trigram LMs are well-suited for phrase linearization. 2. That there is a considerable drop in performance when one uses them for linearization on the clause level. 3. That an approach which uses a richer representation on the clause level is more appropriate. 4.1 Data We take a subset of the TIPSTER3 corpus – all Wall Street Journal articles from the period of 1987-92 (approx. 72 mill. words) – and automatically annotate them with sentence boundaries, part of speech tags and dependency relations using the Stanford parser (Klein & Manning, 2003). We reserve a small subset of about 600 articles (340,000 words) for testing and use the rest to build a trigram LM with the CMU toolkit (Clarkson & Rosenfeld, 1997, with Good-Turing smoothing and vocabulary size of 30,000). To train the maximum entropy classifiers we use about 41,000 sentences. 4.2 Evaluation To test the trigram-based approach, we generate all possible permutations of clause constituents, place 3 Description at http://www.ldc.upenn.edu/ Catalog/CatalogEntry.jsp?catalogId= LDC93T3A. 227 the verb right after the subject and then rank the resulting strings with the LM taking th"
N09-2057,P98-1116,0,0.0178898,"finds the order of clause constituents (i.e., constituents dependent on a finite verb) with a maximum entropy classifier trained on a range of linguistic features. 1 Introduction To date, many natural language processing applications rely on syntactic representations and also modify them by compressing, fusing, or translating into a different language. A syntactic tree emerging as a result of such operations has to be linearized to a string of words before it can be output to the end-user. The simple and most widely used trigram LM has become a standard tool for tree linearization in English (Langkilde & Knight, 1998). For languages with less rigid word order, LM-based approaches have been shown to perform poorly (e.g., Marsi & Krahmer (2005) for Dutch), and methods relying on a range of linguistic features have been successfully applied instead (see Uchimoto et al. (2000) and Ringger et al. (2004), Filippova & Strube (2007) for Japanese and German resp.). To our knowledge, none of the linearization studies have compared a LM-based method with We show that such a differentiated approach is beneficial and that the proposed combination outperforms the method which relies solely on a LM. Hence, our results ch"
N09-2057,J06-4002,0,0.0074111,"tion in word order and it might happen that the generated order is not necessarily wrong if different from the original one, we do not expect this to happen often and evaluate the performance rigorously: only the original order counts as the correct one. The default evaluation metric is perphrase/per-clause accuracy: acc = |correct| |total| Other metrics we use to measure how different a generated order of N elements is from the correct one are: 1. Kendall’s τ , τ = 1 − 4 N (Nt −1) where t is the minimum number of interchanges of consecutive elements to achieve the right order (Kendall, 1938; Lapata, 2006). m where m 2. Edit distance related di, di = 1 − N is the minimum number of deletions combined with insertions to get to the right order (Ringger et al., 2004). E.g., on the phrase level, the incorrectly generated phrase the all brothers of my neighbor (’1-0-2-3-45’) gets τ = 0.87, di = 0.83. Likewise, given the input sentence from (1a), the incorrectly generated order of the four clause constituents in (1c) – ’1-02-3’ – gets τ of 0.67 and di of 0.75. 4.3 Results The results of the experiments on the phrase and the clause levels are presented in Tables 1 and 2 respectively. From the total of"
N09-2057,W05-1612,0,0.03431,"n a range of linguistic features. 1 Introduction To date, many natural language processing applications rely on syntactic representations and also modify them by compressing, fusing, or translating into a different language. A syntactic tree emerging as a result of such operations has to be linearized to a string of words before it can be output to the end-user. The simple and most widely used trigram LM has become a standard tool for tree linearization in English (Langkilde & Knight, 1998). For languages with less rigid word order, LM-based approaches have been shown to perform poorly (e.g., Marsi & Krahmer (2005) for Dutch), and methods relying on a range of linguistic features have been successfully applied instead (see Uchimoto et al. (2000) and Ringger et al. (2004), Filippova & Strube (2007) for Japanese and German resp.). To our knowledge, none of the linearization studies have compared a LM-based method with We show that such a differentiated approach is beneficial and that the proposed combination outperforms the method which relies solely on a LM. Hence, our results challenge the widespread attitude that trigram LMs provide an appropriate way to linearize syntactic trees in English but also in"
N09-2057,C04-1097,0,0.735808,"y compressing, fusing, or translating into a different language. A syntactic tree emerging as a result of such operations has to be linearized to a string of words before it can be output to the end-user. The simple and most widely used trigram LM has become a standard tool for tree linearization in English (Langkilde & Knight, 1998). For languages with less rigid word order, LM-based approaches have been shown to perform poorly (e.g., Marsi & Krahmer (2005) for Dutch), and methods relying on a range of linguistic features have been successfully applied instead (see Uchimoto et al. (2000) and Ringger et al. (2004), Filippova & Strube (2007) for Japanese and German resp.). To our knowledge, none of the linearization studies have compared a LM-based method with We show that such a differentiated approach is beneficial and that the proposed combination outperforms the method which relies solely on a LM. Hence, our results challenge the widespread attitude that trigram LMs provide an appropriate way to linearize syntactic trees in English but also indicate that they perform well in linearizing subtrees corresponding to phrases. 2 LM-based Approach Trigram models are easy to build and use, and it has been s"
N09-2057,C00-2126,0,0.0567224,"ions and also modify them by compressing, fusing, or translating into a different language. A syntactic tree emerging as a result of such operations has to be linearized to a string of words before it can be output to the end-user. The simple and most widely used trigram LM has become a standard tool for tree linearization in English (Langkilde & Knight, 1998). For languages with less rigid word order, LM-based approaches have been shown to perform poorly (e.g., Marsi & Krahmer (2005) for Dutch), and methods relying on a range of linguistic features have been successfully applied instead (see Uchimoto et al. (2000) and Ringger et al. (2004), Filippova & Strube (2007) for Japanese and German resp.). To our knowledge, none of the linearization studies have compared a LM-based method with We show that such a differentiated approach is beneficial and that the proposed combination outperforms the method which relies solely on a LM. Hence, our results challenge the widespread attitude that trigram LMs provide an appropriate way to linearize syntactic trees in English but also indicate that they perform well in linearizing subtrees corresponding to phrases. 2 LM-based Approach Trigram models are easy to build"
N09-2057,C98-1112,0,\N,Missing
N13-1111,J08-1001,0,0.269857,"Missing"
N13-1111,W12-1632,0,0.285534,"Missing"
N13-1111,caselli-prodanof-2006-annotating,0,0.493834,"nts improves significantly over the local model. 2 Related Work Prior corpus-linguistic studies on bridging are beset by three main problems. First, reliability is not measured or low (Fraurud, 1990; Poesio, 2003; Gardent and Manu´elian, 2005; Riester et al., 2010).3 Second, annotated corpora are small (Poesio et al., 2004; Korzen and Buch-Kromann, 2011). Third, they are often based on strong untested assumptions about bridging anaphora types, antecedent types or relations, such as limiting it to definite NP anaphora (Poesio and Vieira, 1998; Poesio et al., 2004; Gardent and Manu´elian, 2005; Caselli and Prodanof, 2006; Riester et al., 2010; Lassalle and Denis, 2011), to NP antecedents (all prior work) or to part3 Although the overall information status scheme in Riester et al. (2010) achieved high agreement, their confusion matrix shows that the anaphoric bridging category (BRI) is frequently confused with other categories so that the two annotators agreed on only less than a third of bridging anaphors. 908 of relations between anaphor and antecedent (Markert et al., 2003; Poesio et al., 2004). In our own work (Markert et al., 2012) we established a corpus that circumvents these problems, i.e. human bridgi"
N13-1111,C12-1050,1,0.8319,"Markov networks. Given different sets of constants, an MLN will produce different ground Markov networks which may vary in size but have the same structure and parameters. For a ground Markov network, the probability distribution over possible worlds x is given by ! X 1 P (X = x) = exp wi ni (x) Z i (1) 5 Features 5.1 where ni (x) is the number of true groundings of Fi in x. The normalization factor Z is the partition function. MLNs have been applied to many NLP tasks and achieved good performance by leveraging rich relations among objects (Poon and Domingos, 2008; Meza-Ruiz and Riedel, 2009; Fahrni and Strube, 2012, inter alia). We use thebeast5 to learn weights for the formulas and to perform inference. thebeast employs cutting plane inference (Riedel, 2008) to improve the accuracy and efficiency of MAP inference for Markov logic. With MLNs, we model bridging resolution globally on the discourse level: given the set M of all anaphors and sets of local antecedent candidates Em for each anaphor m ∈ M , we select antecedents for S all anaphors from E = m∈M Em at the same time. Table 1 shows the hidden predicates and formulas used. Each formula is associated with a weight. The 5 polarity of the weights is"
N13-1111,J12-4003,0,0.0578218,"Missing"
N13-1111,W03-2606,1,0.905518,"Missing"
N13-1111,P12-1084,1,0.653732,"so that the two annotators agreed on only less than a third of bridging anaphors. 908 of relations between anaphor and antecedent (Markert et al., 2003; Poesio et al., 2004). In our own work (Markert et al., 2012) we established a corpus that circumvents these problems, i.e. human bridging recognition was reliable, it contains a medium number of bridging cases that allows generalisable statistics and we did not limit bridging anaphora or antecedents according to their syntactic type or relations between them. However, we only discussed human agreement on bridging recognition in Markert et al. (2012), disregarding antecedent annotation. We also did not discuss the different types of bridging in the corpus. We will remedy this in Section 3. Automatic work on bridging distinguishes between recognition (Vieira and Poesio, 2000; Rahman and Ng, 2012; Cahill and Riester, 2012; Markert et al., 2012) and antecedent selection. Work on antecedent selection suffers from focusing on subproblems, e.g. only part-of bridging (Poesio et al., 2004; Markert et al., 2003) or definite NP anaphora (Lassalle and Denis, 2011). Most relevant for us is Lassalle and Denis (2011) who restrict anaphora to definite d"
N13-1111,N09-1018,0,0.0531989,"a template for constructing Markov networks. Given different sets of constants, an MLN will produce different ground Markov networks which may vary in size but have the same structure and parameters. For a ground Markov network, the probability distribution over possible worlds x is given by ! X 1 P (X = x) = exp wi ni (x) Z i (1) 5 Features 5.1 where ni (x) is the number of true groundings of Fi in x. The normalization factor Z is the partition function. MLNs have been applied to many NLP tasks and achieved good performance by leveraging rich relations among objects (Poon and Domingos, 2008; Meza-Ruiz and Riedel, 2009; Fahrni and Strube, 2012, inter alia). We use thebeast5 to learn weights for the formulas and to perform inference. thebeast employs cutting plane inference (Riedel, 2008) to improve the accuracy and efficiency of MAP inference for Markov logic. With MLNs, we model bridging resolution globally on the discourse level: given the set M of all anaphors and sets of local antecedent candidates Em for each anaphor m ∈ M , we select antecedents for S all anaphors from E = m∈M Em at the same time. Table 1 shows the hidden predicates and formulas used. Each formula is associated with a weight. The 5 po"
N13-1111,P10-1123,0,0.0778411,"rud, 1990; Poesio, 2003; Gardent and Manu´elian, 2005; Riester et al., 2010).3 Second, annotated corpora are small (Poesio et al., 2004; Korzen and Buch-Kromann, 2011). Third, they are often based on strong untested assumptions about bridging anaphora types, antecedent types or relations, such as limiting it to definite NP anaphora (Poesio and Vieira, 1998; Poesio et al., 2004; Gardent and Manu´elian, 2005; Caselli and Prodanof, 2006; Riester et al., 2010; Lassalle and Denis, 2011), to NP antecedents (all prior work) or to part3 Although the overall information status scheme in Riester et al. (2010) achieved high agreement, their confusion matrix shows that the anaphoric bridging category (BRI) is frequently confused with other categories so that the two annotators agreed on only less than a third of bridging anaphors. 908 of relations between anaphor and antecedent (Markert et al., 2003; Poesio et al., 2004). In our own work (Markert et al., 2012) we established a corpus that circumvents these problems, i.e. human bridging recognition was reliable, it contains a medium number of bridging cases that allows generalisable statistics and we did not limit bridging anaphora or antecedents acc"
N13-1111,W03-1023,1,0.936625,"Missing"
N13-1111,nissim-etal-2004-annotation,0,0.406101,"ates (e.g. Gerber and Chai (2012)). We consider all bridging anaphors in running text. The closest work to ours interpreting implicit role filling as anaphora resolution is Silberer and Frank (2012). 3 Corpus for Bridging: An Overview We use the dataset we created in Markert et al. (2012) with almost 11,000 NPs annotated for information status including 663 bridging NPs and their antecedents in 50 texts taken from the WSJ portion of the OntoNotes corpus (Weischedel et al., 2011). Bridging anaphora can be any noun phrase. They are not limited to definite NPs as in previous work. In contrast to Nissim et al. (2004), antecedents are annotated and can be noun phrases, verb phrases or even clauses. Our bridging annotation is also not limited with regards to semantic relations between anaphor and antecedent. In Markert et al. (2012) we achieved high agreement for the overall information status annotation scheme between three annotators (κ between 75 and 80, dependent on annotator pairs) as well as for all subcategories, including bridging (κ over 60 for all annotator pairings, over 70 for two expert annotators). Here, we add the following new results: • Agreement for selecting bridging antecedents was aroun"
N13-1111,J98-2001,0,0.826665,"esolution may also be important for textual entailment (Mirkin et al., 2010). Bridging resolution can be divided into two tasks, recognizing that a bridging anaphor is present and finding the correct antecedent among a list of candidates. These two tasks have frequently been handled in a pipeline with most research concentrating on antecedent selection only. We also handle only the task of antecedent selection. Previous work on antecedent selection for bridging anaphora is restricted. It makes strong untested assumptions about bridging anaphora types or relations, limiting it to definite NPs (Poesio and Vieira, 1998; Poesio et al., 2004; Lassalle and Denis, 2011) or to part-of relations between anaphor and antecedent (Poesio et al., 2004; Markert et al., 2003; Lassalle and Denis, 2011). We break new ground by considering all relations and anaphora/antecedent types and show that the variety of bridging anaphora is much higher than reported previously. Following work on coreference resolution, we apply a local pairwise model (Soon et al., 2001) for antecedent selection. We then develop novel semantic, syntactic and salience features for this task, showing strong improvements over one of the best known 907"
N13-1111,P04-1019,0,0.116393,"ortant for textual entailment (Mirkin et al., 2010). Bridging resolution can be divided into two tasks, recognizing that a bridging anaphor is present and finding the correct antecedent among a list of candidates. These two tasks have frequently been handled in a pipeline with most research concentrating on antecedent selection only. We also handle only the task of antecedent selection. Previous work on antecedent selection for bridging anaphora is restricted. It makes strong untested assumptions about bridging anaphora types or relations, limiting it to definite NPs (Poesio and Vieira, 1998; Poesio et al., 2004; Lassalle and Denis, 2011) or to part-of relations between anaphor and antecedent (Poesio et al., 2004; Markert et al., 2003; Lassalle and Denis, 2011). We break new ground by considering all relations and anaphora/antecedent types and show that the variety of bridging anaphora is much higher than reported previously. Following work on coreference resolution, we apply a local pairwise model (Soon et al., 2001) for antecedent selection. We then develop novel semantic, syntactic and salience features for this task, showing strong improvements over one of the best known 907 Proceedings of NAACL-"
N13-1111,W03-2605,0,0.0691816,"allows us to: • model constraints that certain anaphora are likely to share the same antecedent; • model the global semantic connectivity of a salient potential antecedent to all anaphora in a text; • consider the union of potential antecedents for all anaphora instead of a static window-sized constraint. We show that this global model with the same local features but enhanced with global constraints improves significantly over the local model. 2 Related Work Prior corpus-linguistic studies on bridging are beset by three main problems. First, reliability is not measured or low (Fraurud, 1990; Poesio, 2003; Gardent and Manu´elian, 2005; Riester et al., 2010).3 Second, annotated corpora are small (Poesio et al., 2004; Korzen and Buch-Kromann, 2011). Third, they are often based on strong untested assumptions about bridging anaphora types, antecedent types or relations, such as limiting it to definite NP anaphora (Poesio and Vieira, 1998; Poesio et al., 2004; Gardent and Manu´elian, 2005; Caselli and Prodanof, 2006; Riester et al., 2010; Lassalle and Denis, 2011), to NP antecedents (all prior work) or to part3 Although the overall information status scheme in Riester et al. (2010) achieved high ag"
N13-1111,D08-1068,0,0.018588,"ght. It can be viewed as a template for constructing Markov networks. Given different sets of constants, an MLN will produce different ground Markov networks which may vary in size but have the same structure and parameters. For a ground Markov network, the probability distribution over possible worlds x is given by ! X 1 P (X = x) = exp wi ni (x) Z i (1) 5 Features 5.1 where ni (x) is the number of true groundings of Fi in x. The normalization factor Z is the partition function. MLNs have been applied to many NLP tasks and achieved good performance by leveraging rich relations among objects (Poon and Domingos, 2008; Meza-Ruiz and Riedel, 2009; Fahrni and Strube, 2012, inter alia). We use thebeast5 to learn weights for the formulas and to perform inference. thebeast employs cutting plane inference (Riedel, 2008) to improve the accuracy and efficiency of MAP inference for Markov logic. With MLNs, we model bridging resolution globally on the discourse level: given the set M of all anaphors and sets of local antecedent candidates Em for each anaphor m ∈ M , we select antecedents for S all anaphors from E = m∈M Em at the same time. Table 1 shows the hidden predicates and formulas used. Each formula is associ"
N13-1111,riester-etal-2010-recursive,0,0.0828271,"rud, 1990; Poesio, 2003; Gardent and Manu´elian, 2005; Riester et al., 2010).3 Second, annotated corpora are small (Poesio et al., 2004; Korzen and Buch-Kromann, 2011). Third, they are often based on strong untested assumptions about bridging anaphora types, antecedent types or relations, such as limiting it to definite NP anaphora (Poesio and Vieira, 1998; Poesio et al., 2004; Gardent and Manu´elian, 2005; Caselli and Prodanof, 2006; Riester et al., 2010; Lassalle and Denis, 2011), to NP antecedents (all prior work) or to part3 Although the overall information status scheme in Riester et al. (2010) achieved high agreement, their confusion matrix shows that the anaphoric bridging category (BRI) is frequently confused with other categories so that the two annotators agreed on only less than a third of bridging anaphors. 908 of relations between anaphor and antecedent (Markert et al., 2003; Poesio et al., 2004). In our own work (Markert et al., 2012) we established a corpus that circumvents these problems, i.e. human bridging recognition was reliable, it contains a medium number of bridging cases that allows generalisable statistics and we did not limit bridging anaphora or antecedents acc"
N13-1111,S10-1008,0,0.0203059,"Missing"
N13-1111,S12-1001,0,0.0204899,"Missing"
N13-1111,J01-4004,0,0.0867864,"Missing"
N13-1111,J00-4003,0,0.592405,"Missing"
N15-3002,D13-1203,0,0.150608,"Missing"
N15-3002,P14-5002,0,0.0366592,"Missing"
N15-3002,D13-1027,0,0.0308905,"to one or more systems. This allows for an assessment of the qualitative usefulness of the new feature. Depend9 Related Work Compared to our original implementation of the error analysis framework (Martschat and Strube, 2014), we made the analysis interface more userfriendly and provide more analysis functionality. Furthermore, while our original implementation did not include any visualization capabilities, we now allow for both data visualization and document visualization. We are aware of two other software packages for coreference resolution error analysis. Our toolkit complements these. Kummerfeld and Klein (2013) present a toolkit which extracts errors from transformation of reference to system entities. Hence, their definition of what an error is not rooted in a pairwise representation, and is therefore conceptually different from our definition. They do not provide any visualization components. ICE (G¨artner et al., 2014) is a toolkit for coreference visualization and corpus analysis. In particular, the toolkit visualizes recall and precision errors in a tree-based visualization of coreference clusters. Compared to ICE, we provide more extensive functionality for error analysis and can accommodate f"
N15-3002,J13-4004,0,0.055329,"lution approaches. First, it includes multigraph, which is a deterministic approach using a few strong features (Martschat and Strube, 2014). Second, it includes a mention-pair approach (Soon et al., 2001) with a large feature set, trained via a perceptron on the CoNLL’12 English training data. System MUC B3 CEAFe Average StanfordSieve BerkeleyCoref 64.96 70.27 54.49 59.29 51.24 56.11 56.90 61.89 multigraph mention-pair 69.13 69.09 58.61 57.84 56.06 53.56 61.28 60.16 Table 1: Comparison of systems on CoNLL’12 English development data. In Table 1, we compare both approaches with StanfordSieve (Lee et al., 2013), the winner of the CoNLL-2011 shared task, and BerkeleyCoref (Durrett and Klein, 2013), a state-of-the-art structured machine learning approach. The systems are evaluated via the CoNLL scorer (Pradhan et al., 2014). Both implemented approaches achieve competitive performance. Due to their modular implementation, both approaches are easily extensible with new features and with training or inference schemes. They therefore can serve as a good starting point for system development and analysis. 7 3.3 analysis The core of this module is the ErrorAnalysis class, which extracts and manages errors e"
N15-3002,D14-1221,1,0.691901,"ny thierry.goeckel@iqser.com Abstract We present a toolkit for coreference resolution error analysis. It implements a recently proposed analysis framework and contains rich components for analyzing and visualizing recall and precision errors. 1 Introduction Coreference resolution is the task of determining which mentions in a text refer to the same entity. Both the natural language processing engineer (who needs a coreference resolution system for the problem at hand) and the coreference resolution researcher need tools to facilitate and support system development, comparison and analysis. In Martschat and Strube (2014), we propose a framework for error analysis for coreference resolution. In this paper, we present cort1 , an implementation of this framework, and show how it can be useful for engineers and researchers. cort is released as open source and is available for download2 . 2 Error Analysis Framework Due to the set-based nature of coreference resolution, it is not clear how to extract errors when an entity is not correctly identified. The idea underlying the analysis framework of Martschat and Strube (2014) is to employ spanning trees in a graph-based entity representation. 1 2 Short for coreference"
N15-3002,W12-4501,0,0.0647549,"l, 1990) for reference entities. For system entity spanning trees, we take each output pair as an edge. 3 Architecture Our toolkit is available as a Python library. It consists of three modules: the core module provides mention extraction and preprocessing, the coreference module implements features for and approaches to coreference resolution, and the analysis module implements the error analysis framework described above and ships with other analysis and visualization utilities. 3.1 core All input and output must conform to the format of the CoNLL-2012 shared task on coreference resolution (Pradhan et al., 2012). We employ a rule-based mention extractor, which also computes a rich set of mention attributes, including tokens, head, part-ofspeech tags, named entity tags, gender, number, se6 Proceedings of NAACL-HLT 2015, pages 6–10, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics (a) Obama (b) (c) m1 m1 his he m4 m2 m3 m1 m4 n3 the president m2 m3 n1 n2 the president m3 the president Figure 1: (a) a reference entity r (represented as a complete one-directional graph) and its spanning tree Tr , (b) a set S of three system entities, (c) the errors: all edges in T"
N15-3002,P14-2006,1,0.66373,"ser.com Abstract We present a toolkit for coreference resolution error analysis. It implements a recently proposed analysis framework and contains rich components for analyzing and visualizing recall and precision errors. 1 Introduction Coreference resolution is the task of determining which mentions in a text refer to the same entity. Both the natural language processing engineer (who needs a coreference resolution system for the problem at hand) and the coreference resolution researcher need tools to facilitate and support system development, comparison and analysis. In Martschat and Strube (2014), we propose a framework for error analysis for coreference resolution. In this paper, we present cort1 , an implementation of this framework, and show how it can be useful for engineers and researchers. cort is released as open source and is available for download2 . 2 Error Analysis Framework Due to the set-based nature of coreference resolution, it is not clear how to extract errors when an entity is not correctly identified. The idea underlying the analysis framework of Martschat and Strube (2014) is to employ spanning trees in a graph-based entity representation. 1 2 Short for coreference"
N15-3002,P11-1080,0,0.06216,"Missing"
N15-3002,J01-4004,0,0.328799,"m2 m3 m1 m4 n3 the president m2 m3 n1 n2 the president m3 the president Figure 1: (a) a reference entity r (represented as a complete one-directional graph) and its spanning tree Tr , (b) a set S of three system entities, (c) the errors: all edges in Tr which are not in S. mantic class, grammatical function, coarse mention type and fine-grained mention type. 3.2 coreference cort ships with two coreference resolution approaches. First, it includes multigraph, which is a deterministic approach using a few strong features (Martschat and Strube, 2014). Second, it includes a mention-pair approach (Soon et al., 2001) with a large feature set, trained via a perceptron on the CoNLL’12 English training data. System MUC B3 CEAFe Average StanfordSieve BerkeleyCoref 64.96 70.27 54.49 59.29 51.24 56.11 56.90 61.89 multigraph mention-pair 69.13 69.09 58.61 57.84 56.06 53.56 61.28 60.16 Table 1: Comparison of systems on CoNLL’12 English development data. In Table 1, we compare both approaches with StanfordSieve (Lee et al., 2013), the winner of the CoNLL-2011 shared task, and BerkeleyCoref (Durrett and Klein, 2013), a state-of-the-art structured machine learning approach. The systems are evaluated via the CoNLL sc"
N16-1115,P87-1022,0,0.585642,"64.11 64.72 66.56 69.71 70.69 72.63 73.28 72.60 R 49.53 49.09 48.65 54.02 53.62 54.64 55.46 55.83 B3 P 55.21 56.84 60.32 59.47 62.02 66.78 66.9 66.07 F1 52.22 52.68 53.86 56.61 57.52 60.11 60.65 60.52 R 53.06 52.54 48.71 51.88 51.42 52.85 52.07 54.88 CEAFe P 44.82 46.55 51.48 52.17 55.07 60.3 62.23 59.41 F1 48.59 49.37 50.06 52.02 53.18 56.33 56.7 57.05 Avg. F1 54.97 55.59 56.83 59.45 60.46 63.02 63.54 63.39 Table 4: Results on the English test set. All the improvements made by our singleton detection models are statistically significant. cency, focus (Grosz and Sidner, 1986), and centering (Brennan et al., 1987) are examples of useful linguistic features for coreference resolution which have the additional benefit of being applicable to different languages. For example, Hobbs’ algorithm and agreement features are being used successfully in the Stanford system (Lee et al., 2013). However, apart from features like these, a large number of linguistically motivated features have been proposed which either do not have a significant impact or are only applicable to a specific language or domain. Therefore, designing general linguistic features which provide information that is not captured by surface featu"
N16-1115,P15-1136,0,0.0140737,"eton detection is a more recent method (Recasens et al., 2013; Ma et al., 2014; Marneffe et al., 2015). 1005 Proceedings of NAACL-HLT 2016, pages 1005–1011, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics Anaphoricity detection examines whether a phrase is anaphoric. Singleton detection examines whether a phrase belongs to a coreference chain regardless of being anaphor or antecedent. Therefore, anaphoricity detection only prunes the search space of anaphors while singleton detection prunes the search space of both anaphors and antecedents. Except for Clark and Manning (2015), all of the state-of-the-art coreference resolvers explicitly model anaphoricity detection (Martschat and Strube, 2015; Wiseman et al., 2015; Peng et al., 2015). Therefore, modeling search space pruning as singleton detection can provide additional information for the state-of-the-art coreference resolution systems. In this paper we propose a simple but efficient singleton detection model. We first perform intrinsic evaluations and show that our simple model significantly improves the state-of-the-art results in singleton detection by a large margin. We then evaluate our singleton model extri"
N16-1115,N07-1030,0,0.0893558,"Missing"
N16-1115,D13-1203,0,0.148442,"elf, are required in order to improve the correct detection of the first mentions of entities. Table 3 shows the ratio of recall errors for each mention type. For our Surface system, this ratio is more or less the same for different mention types. However, Confident’s main source of recall errors is the detection of non-coreferent nominal mentions. 2.3 Discussion Our results significantly outperform the results of Marneffe et al. (2015) who use both surface features and a set of hand-engineered features targeting different linguistic phenomena related to the task. Our findings are mirrored by Durrett and Klein (2013)’s work on the coreference resolution task. Durrett and Klein (2013) show that a coreference resolution system that uses surface features can outperform those using hand-engineered linguistic features. Linguistic features like syntactic nearness (on which Hobbs’ algorithm (Hobbs, 1978) is based), morpho-syntactic and semantic agreement (e.g. number, gender and semantic class agreements), reBaseline +Stanford Singleton +Preprocess Pruning Pairwise Cort +Preprocess Pruning Latent Ranking +Pruning Feature Wiseman et al. (2015) Stanford R 64.58 64.26 64.27 68.46 68.19 68.55 68.81 69.31 MUC P 63.65"
N16-1115,P07-1029,0,0.351239,"inguistic features, we select a simple and small set of shallow features: 1. lemmas of all words included in the mention; 2. lemmas of the two previous/next words before/after the mention; 3. part-of-speech tags of all words of the mention; 4. part-of-speech tags of the two previous/next words before/after the mention; 5. complete mention string; 6. length of the mention in words; 7. mention type (proper, nominal, pronominal); 8. whether the whole string of the mention appears again in the document; 1006 9. whether the head of the mention appears again in the document. We use an anchored SVM (Goldberg and Elhadad, 2007) with a polynomial kernel of degree two for classification. When only few features are available, anchored SVMs generalize much better than softmargin-SVMs (Goldberg and Elhadad, 2009). In our experiments, we use a count threshold for discarding vary rare lexical features that occur fewer than 10 times. Similar to Marneffe et al. (2015), we use three different configurations for evaluation. The Surface configuration only uses the shallow features. The Combined configuration uses the surface features plus the linguistic features introduced by Marneffe et al. (2015). The linguistic features of M"
N16-1115,D09-1119,0,0.458924,"ion; 3. part-of-speech tags of all words of the mention; 4. part-of-speech tags of the two previous/next words before/after the mention; 5. complete mention string; 6. length of the mention in words; 7. mention type (proper, nominal, pronominal); 8. whether the whole string of the mention appears again in the document; 1006 9. whether the head of the mention appears again in the document. We use an anchored SVM (Goldberg and Elhadad, 2007) with a polynomial kernel of degree two for classification. When only few features are available, anchored SVMs generalize much better than softmargin-SVMs (Goldberg and Elhadad, 2009). In our experiments, we use a count threshold for discarding vary rare lexical features that occur fewer than 10 times. Similar to Marneffe et al. (2015), we use three different configurations for evaluation. The Surface configuration only uses the shallow features. The Combined configuration uses the surface features plus the linguistic features introduced by Marneffe et al. (2015). The linguistic features of Marneffe et al. (2015) also include some pairwise combinations of the single features. Since our SVM with a polynomial kernel of degree two implicitly models feature pairs, we only incl"
N16-1115,J86-3001,0,0.438764,"9 69.01 71.01 73.38 77.22 78.37 76.23 F1 64.11 64.72 66.56 69.71 70.69 72.63 73.28 72.60 R 49.53 49.09 48.65 54.02 53.62 54.64 55.46 55.83 B3 P 55.21 56.84 60.32 59.47 62.02 66.78 66.9 66.07 F1 52.22 52.68 53.86 56.61 57.52 60.11 60.65 60.52 R 53.06 52.54 48.71 51.88 51.42 52.85 52.07 54.88 CEAFe P 44.82 46.55 51.48 52.17 55.07 60.3 62.23 59.41 F1 48.59 49.37 50.06 52.02 53.18 56.33 56.7 57.05 Avg. F1 54.97 55.59 56.83 59.45 60.46 63.02 63.54 63.39 Table 4: Results on the English test set. All the improvements made by our singleton detection models are statistically significant. cency, focus (Grosz and Sidner, 1986), and centering (Brennan et al., 1987) are examples of useful linguistic features for coreference resolution which have the additional benefit of being applicable to different languages. For example, Hobbs’ algorithm and agreement features are being used successfully in the Stanford system (Lee et al., 2013). However, apart from features like these, a large number of linguistically motivated features have been proposed which either do not have a significant impact or are only applicable to a specific language or domain. Therefore, designing general linguistic features which provide information"
N16-1115,J13-4004,0,0.0145995,"48 52.17 55.07 60.3 62.23 59.41 F1 48.59 49.37 50.06 52.02 53.18 56.33 56.7 57.05 Avg. F1 54.97 55.59 56.83 59.45 60.46 63.02 63.54 63.39 Table 4: Results on the English test set. All the improvements made by our singleton detection models are statistically significant. cency, focus (Grosz and Sidner, 1986), and centering (Brennan et al., 1987) are examples of useful linguistic features for coreference resolution which have the additional benefit of being applicable to different languages. For example, Hobbs’ algorithm and agreement features are being used successfully in the Stanford system (Lee et al., 2013). However, apart from features like these, a large number of linguistically motivated features have been proposed which either do not have a significant impact or are only applicable to a specific language or domain. Therefore, designing general linguistic features which provide information that is not captured by surface features deserve more attention in order to gain higher recall and better generalization. We combine a simple set of surface features with a standard machine learning model that can handle a large number of surface features. This leads to a new state-of-the-art singleton dete"
N16-1115,H05-1004,0,0.138187,"rious coreference 1 http://github.com/smartschat/cort 1008 resolution models, i.e. ranking, antecedent trees, and pairwise. The pairwise model is the most commonly used model in coreference resolution, and latent ranking is the best performing model for coreference resolution to date (Wiseman et al., 2015; Martschat and Strube, 2015). 3.1 Results Table 4 shows the results of integrating singleton detection into different coreference resolution approaches. We evaluate the systems on the CoNLL 2012 English test set using the M U C (Vilain et al., 1995), B 3 (Bagga and Baldwin, 1998), and CEAFe (Luo, 2005) measures as provided by the CoNLL coreference scorer version 8.01 (Pradhan et al., 2014). According to the approximate randomization test (Noreen, 1989), all of the improvements made by our singleton detection module are statistically significant (p &lt; 0.05). Baseline shows the result of the Stanford system without using singleton detection. +Stanford Singleton is the result of the Stanford system including its singleton detection module (Recasens et al., 2013). +Preprocess Pruning is the result when our Confident model from Section 2 is used. The singleton detection modules of Recasens et al."
N16-1115,D14-1225,0,0.039521,"Missing"
N16-1115,Q15-1029,1,0.928765,"ngs of NAACL-HLT 2016, pages 1005–1011, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics Anaphoricity detection examines whether a phrase is anaphoric. Singleton detection examines whether a phrase belongs to a coreference chain regardless of being anaphor or antecedent. Therefore, anaphoricity detection only prunes the search space of anaphors while singleton detection prunes the search space of both anaphors and antecedents. Except for Clark and Manning (2015), all of the state-of-the-art coreference resolvers explicitly model anaphoricity detection (Martschat and Strube, 2015; Wiseman et al., 2015; Peng et al., 2015). Therefore, modeling search space pruning as singleton detection can provide additional information for the state-of-the-art coreference resolution systems. In this paper we propose a simple but efficient singleton detection model. We first perform intrinsic evaluations and show that our simple model significantly improves the state-of-the-art results in singleton detection by a large margin. We then evaluate our singleton model extrinsically on coreference resolution showing that search space pruning improves different coreference resolution models."
N16-1115,C14-1061,1,0.839862,"Discussion Recent improvements in coreference resolution have been made by exploring more complex learning and inference strategies, a larger number of features, and joint processing. There are also technically viable solutions for improving the performance of a coreference resolver which do not work in prac1009 tice. For instance, since coreference resolution is a set partitioning problem, entity-based models seem to be more suitable for coreference resolution than mention-pair models. However, entity-based models do not necessarily perform better than mentionpair models (e.g. Ng (2010) and Moosavi and Strube (2014)). The same is true for incorporating more semantic-level information in a coreference resolution system (e.g. Durrett and Klein (2013)). In this paper, we show that coreference resolution can also simply be improved by performing search space pruning. The significant gap between the performance of the latent ranking model on gold mentions and on system mentions indicates that there is still room for further improvements in search space pruning. 4 Conclusions We achieve new state-of-the-art results for singleton detection by only using shallow features and simple classifiers. We also show that"
N16-1115,C02-1139,0,0.481583,"Missing"
N16-1115,N09-1065,0,0.409376,"Missing"
N16-1115,P10-1142,0,0.0266509,"chat/cort. 3.2 Discussion Recent improvements in coreference resolution have been made by exploring more complex learning and inference strategies, a larger number of features, and joint processing. There are also technically viable solutions for improving the performance of a coreference resolver which do not work in prac1009 tice. For instance, since coreference resolution is a set partitioning problem, entity-based models seem to be more suitable for coreference resolution than mention-pair models. However, entity-based models do not necessarily perform better than mentionpair models (e.g. Ng (2010) and Moosavi and Strube (2014)). The same is true for incorporating more semantic-level information in a coreference resolution system (e.g. Durrett and Klein (2013)). In this paper, we show that coreference resolution can also simply be improved by performing search space pruning. The significant gap between the performance of the latent ranking model on gold mentions and on system mentions indicates that there is still room for further improvements in search space pruning. 4 Conclusions We achieve new state-of-the-art results for singleton detection by only using shallow features and simple"
N16-1115,P14-2006,1,0.856323,"improvements in coreference resolution have been made by exploring more complex learning and inference strategies, a larger number of features, and joint processing. There are also technically viable solutions for improving the performance of a coreference resolver which do not work in prac1009 tice. For instance, since coreference resolution is a set partitioning problem, entity-based models seem to be more suitable for coreference resolution than mention-pair models. However, entity-based models do not necessarily perform better than mentionpair models (e.g. Ng (2010) and Moosavi and Strube (2014)). The same is true for incorporating more semantic-level information in a coreference resolution system (e.g. Durrett and Klein (2013)). In this paper, we show that coreference resolution can also simply be improved by performing search space pruning. The significant gap between the performance of the latent ranking model on gold mentions and on system mentions indicates that there is still room for further improvements in search space pruning. 4 Conclusions We achieve new state-of-the-art results for singleton detection by only using shallow features and simple classifiers. We also show that"
N16-1115,N13-1071,0,0.343458,"Missing"
N16-1115,M95-1005,0,0.535104,"seline because Cort is a framework that allows evaluations on various coreference 1 http://github.com/smartschat/cort 1008 resolution models, i.e. ranking, antecedent trees, and pairwise. The pairwise model is the most commonly used model in coreference resolution, and latent ranking is the best performing model for coreference resolution to date (Wiseman et al., 2015; Martschat and Strube, 2015). 3.1 Results Table 4 shows the results of integrating singleton detection into different coreference resolution approaches. We evaluate the systems on the CoNLL 2012 English test set using the M U C (Vilain et al., 1995), B 3 (Bagga and Baldwin, 1998), and CEAFe (Luo, 2005) measures as provided by the CoNLL coreference scorer version 8.01 (Pradhan et al., 2014). According to the approximate randomization test (Noreen, 1989), all of the improvements made by our singleton detection module are statistically significant (p &lt; 0.05). Baseline shows the result of the Stanford system without using singleton detection. +Stanford Singleton is the result of the Stanford system including its singleton detection module (Recasens et al., 2013). +Preprocess Pruning is the result when our Confident model from Section 2 is us"
N16-1115,P15-1137,0,0.325423,"rating our pruning method in one of the state-of-the-art coreference resolution systems, we achieve the best reported overall score on the CoNLL 2012 English test set. A version of our pruning method is available with the Cort coreference resolution source code. 1 Introduction Coreference resolution is the task of clustering referring expressions in a text so that each resulting cluster represents an entity. It is a very challenging task in natural language processing and it is still far from being solved, i.e. the best reported overall CoNLL score on the CoNLL 2012 English test set is 63.39 (Wiseman et al., 2015). Text spans referring to an entity are called mentions. Mentions are the primary objects in a coreference resolution system. As with most previous work on coreference resolution, we only consider mentions that are noun phrases. However, not all of the noun phrases are mentions. A noun phrase may not refer to any entity at all. The pronoun it in the sentence it is raining is an example of a non-referential noun phrase. Noun phrases which do refer to an entity (mentions) can be further divided into two catThe latent ranking model is the best performing model for coreference resolution to date ("
N16-1115,D09-1102,0,0.52657,"Missing"
N16-1167,J08-1001,0,0.588401,"unting out his money, The queen was in the parlour, eating bread and honey. We model lexical coherence between sentences by a lexical coherence graph (LCG). We consider subgraphs of this graph coherence patterns and use their frequency as features representing the connectivity of the graph and, hence, the coherence of a text (Mesgar and Strube, 2015). An important task for evaluating a coherence model is readability assessment. The goal of this task is to rate texts based on their readability. The more coherent a text, the faster to read and easier to understand it is. Other coherence models (Barzilay and Lapata, 2008; Guinaudeau and Strube, 2013; Mesgar and Strube, 2014) are also evaluated on this task. Pitler and Nenkova (2008) use the entity grid (Barzilay and Lapata, 2008) to capture the coherence of a text for readability assessment. Mesgar and Strube (2015) extend the entity graph (Guinaudeau and Strube, 2013) as coherence model to measure the readability of texts. They encode coherence as a vector of frequencies of subgraphs of the graph representation of a text. We build upon their method and represent the connectivity of sentences in our LCG model by a vector of frequencies of subgraphs. Although"
N16-1167,P13-1010,1,0.767094,"ueen was in the parlour, eating bread and honey. We model lexical coherence between sentences by a lexical coherence graph (LCG). We consider subgraphs of this graph coherence patterns and use their frequency as features representing the connectivity of the graph and, hence, the coherence of a text (Mesgar and Strube, 2015). An important task for evaluating a coherence model is readability assessment. The goal of this task is to rate texts based on their readability. The more coherent a text, the faster to read and easier to understand it is. Other coherence models (Barzilay and Lapata, 2008; Guinaudeau and Strube, 2013; Mesgar and Strube, 2014) are also evaluated on this task. Pitler and Nenkova (2008) use the entity grid (Barzilay and Lapata, 2008) to capture the coherence of a text for readability assessment. Mesgar and Strube (2015) extend the entity graph (Guinaudeau and Strube, 2013) as coherence model to measure the readability of texts. They encode coherence as a vector of frequencies of subgraphs of the graph representation of a text. We build upon their method and represent the connectivity of sentences in our LCG model by a vector of frequencies of subgraphs. Although using the frequency of subgra"
N16-1167,P13-2121,0,0.108683,"texts. They encode coherence as a vector of frequencies of subgraphs of the graph representation of a text. We build upon their method and represent the connectivity of sentences in our LCG model by a vector of frequencies of subgraphs. Although using the frequency of subgraphs of the lexical coherence graph encodes coherence features well, the subgraph frequency method, in general, is suffering from a sparsity problem when the subgraphs get larger. Large subgraphs capture more structural information, but they occur only rarely. We resolve this sparsity issue by adapting KneserNey smoothing (Heafield et al., 2013) to smooth subgraph counts (Section 3). We estimate the probability of unseen subgraphs, i.e. coherence patterns. This prediction lets us measure the coherence of a text even when its corresponding graph representation contains a subgraph which does not occur in the training data. If the unseen coherence pattern is similar to seen ones, smoothing gives it closer probability to seen coherence patterns in comparison to dissimilar unseen ones. This is due to the base probability factor in Kneser-Ney smoothing. 1415 We evaluate our LCG model on the two readability datasets provided by Pitler and N"
N16-1167,J13-4004,0,0.0289288,"Missing"
N16-1167,W14-3701,1,0.873543,"ng bread and honey. We model lexical coherence between sentences by a lexical coherence graph (LCG). We consider subgraphs of this graph coherence patterns and use their frequency as features representing the connectivity of the graph and, hence, the coherence of a text (Mesgar and Strube, 2015). An important task for evaluating a coherence model is readability assessment. The goal of this task is to rate texts based on their readability. The more coherent a text, the faster to read and easier to understand it is. Other coherence models (Barzilay and Lapata, 2008; Guinaudeau and Strube, 2013; Mesgar and Strube, 2014) are also evaluated on this task. Pitler and Nenkova (2008) use the entity grid (Barzilay and Lapata, 2008) to capture the coherence of a text for readability assessment. Mesgar and Strube (2015) extend the entity graph (Guinaudeau and Strube, 2013) as coherence model to measure the readability of texts. They encode coherence as a vector of frequencies of subgraphs of the graph representation of a text. We build upon their method and represent the connectivity of sentences in our LCG model by a vector of frequencies of subgraphs. Although using the frequency of subgraphs of the lexical coheren"
N16-1167,S15-1036,1,0.773945,"be induced by word em1414 Proceedings of NAACL-HLT 2016, pages 1414–1423, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics bedding models (Mikolov et al., 2013; Pennington et al., 2014). . . . The king was in his counting-house, counting out his money, The queen was in the parlour, eating bread and honey. We model lexical coherence between sentences by a lexical coherence graph (LCG). We consider subgraphs of this graph coherence patterns and use their frequency as features representing the connectivity of the graph and, hence, the coherence of a text (Mesgar and Strube, 2015). An important task for evaluating a coherence model is readability assessment. The goal of this task is to rate texts based on their readability. The more coherent a text, the faster to read and easier to understand it is. Other coherence models (Barzilay and Lapata, 2008; Guinaudeau and Strube, 2013; Mesgar and Strube, 2014) are also evaluated on this task. Pitler and Nenkova (2008) use the entity grid (Barzilay and Lapata, 2008) to capture the coherence of a text for readability assessment. Mesgar and Strube (2015) extend the entity graph (Guinaudeau and Strube, 2013) as coherence model to"
N16-1167,D14-1162,0,0.100267,"Missing"
N16-1167,D08-1020,0,0.717651,"tences by a lexical coherence graph (LCG). We consider subgraphs of this graph coherence patterns and use their frequency as features representing the connectivity of the graph and, hence, the coherence of a text (Mesgar and Strube, 2015). An important task for evaluating a coherence model is readability assessment. The goal of this task is to rate texts based on their readability. The more coherent a text, the faster to read and easier to understand it is. Other coherence models (Barzilay and Lapata, 2008; Guinaudeau and Strube, 2013; Mesgar and Strube, 2014) are also evaluated on this task. Pitler and Nenkova (2008) use the entity grid (Barzilay and Lapata, 2008) to capture the coherence of a text for readability assessment. Mesgar and Strube (2015) extend the entity graph (Guinaudeau and Strube, 2013) as coherence model to measure the readability of texts. They encode coherence as a vector of frequencies of subgraphs of the graph representation of a text. We build upon their method and represent the connectivity of sentences in our LCG model by a vector of frequencies of subgraphs. Although using the frequency of subgraphs of the lexical coherence graph encodes coherence features well, the subgraph freq"
N16-1167,N15-1115,0,0.19958,"tions between sentences are obtained by information on entites shared by sentences. Guinaudeau and Strube (2013) perform a one-mode projection on sentence nodes and use the average out-degree of the onemode projection graph to quantify the coherence of the given text. Mesgar and Strube (2015) represent the connectivity of the one-mode projection graph by a vector whose elements are the frequencies of subgraphs in projection graphs. This encoding works much better than the entity graph for the readability task on the P&N dataset and even outperforms Pitler and Nenkova (2008) by a large margin. Zhang et al. (2015) state that the entity graph model is limited, because it only captures mentions which refer to the same entity (the entity graph uses a very restricted version of coreference resolution to determine entities). Zhang et al. (2015) use world knowledge YAGO (Hoffart et al., 2013), WikiPedia (Denoyer and Gallinari, 2006) and FreeBase (Bollacker et al., 2008) to capture the semantic relatedness between entities even if they do not refer to the same entity. Main issues with using world knowledge are: the choice knowledge sources, selection of knowledge from the source, coverage, and languagedepende"
nastase-etal-2010-wikinet,W06-2810,0,\N,Missing
nastase-etal-2010-wikinet,wentland-etal-2008-building,0,\N,Missing
P02-1045,P95-1017,0,0.0856336,"tion if Co-Training can significantly reduce the amount of manual labeling work and still produce a classifier with an acceptable performance. 1 Introduction A major obstacle for natural language processing systems which analyze natural language texts or utterances is the need to identify the entities referred to by means of referring expressions. Among referring expressions, pronouns and definite noun phrases (NPs) are the most prominent. Supervised machine learning algorithms were used for pronoun resolution with good results (Ge et al., 1998), and for definite NPs with fairly good results (Aone and Bennett, 1995; McCarthy and Lehnert, 1995; Soon et al., 2001). However, the deficiency of supervised machine learning approaches is the need for an unknown amount of annotated training data for optimal performance. So, researchers in NLP began to experiment with weakly supervised machine learning algorithms such as Co-Training (Blum and Mitchell, 1998). Among others Co-Training was applied to document classification (Blum and Mitchell, 1998), namedentity recognition (Collins and Singer, 1999), noun phrase bracketing (Pierce and Cardie, 2001), and statistical parsing (Sarkar, 2001). In this paper we apply C"
P02-1045,W98-1119,0,0.0604837,"assifier for reference resolution. We are concerned with the question if Co-Training can significantly reduce the amount of manual labeling work and still produce a classifier with an acceptable performance. 1 Introduction A major obstacle for natural language processing systems which analyze natural language texts or utterances is the need to identify the entities referred to by means of referring expressions. Among referring expressions, pronouns and definite noun phrases (NPs) are the most prominent. Supervised machine learning algorithms were used for pronoun resolution with good results (Ge et al., 1998), and for definite NPs with fairly good results (Aone and Bennett, 1995; McCarthy and Lehnert, 1995; Soon et al., 2001). However, the deficiency of supervised machine learning approaches is the need for an unknown amount of annotated training data for optimal performance. So, researchers in NLP began to experiment with weakly supervised machine learning algorithms such as Co-Training (Blum and Mitchell, 1998). Among others Co-Training was applied to document classification (Blum and Mitchell, 1998), namedentity recognition (Collins and Singer, 1999), noun phrase bracketing (Pierce and Cardie,"
P02-1045,W01-0501,0,0.279095,"s (Ge et al., 1998), and for definite NPs with fairly good results (Aone and Bennett, 1995; McCarthy and Lehnert, 1995; Soon et al., 2001). However, the deficiency of supervised machine learning approaches is the need for an unknown amount of annotated training data for optimal performance. So, researchers in NLP began to experiment with weakly supervised machine learning algorithms such as Co-Training (Blum and Mitchell, 1998). Among others Co-Training was applied to document classification (Blum and Mitchell, 1998), namedentity recognition (Collins and Singer, 1999), noun phrase bracketing (Pierce and Cardie, 2001), and statistical parsing (Sarkar, 2001). In this paper we apply Co-Training to the problem of reference resolution in German texts from the tourism domain in order to provide answers to the following questions: Does Co-Training work at all for this task (when compared to conventional C4.5 decision tree learning)? How much labeled training data is required for achieving a reasonable performance? First, we discuss features that have been found to be relevant for the task of reference resolution, and describe the feature set that we are using (Section 2). Then we briefly introduce the Co-Trainin"
P02-1045,N01-1023,0,0.0825249,"ly good results (Aone and Bennett, 1995; McCarthy and Lehnert, 1995; Soon et al., 2001). However, the deficiency of supervised machine learning approaches is the need for an unknown amount of annotated training data for optimal performance. So, researchers in NLP began to experiment with weakly supervised machine learning algorithms such as Co-Training (Blum and Mitchell, 1998). Among others Co-Training was applied to document classification (Blum and Mitchell, 1998), namedentity recognition (Collins and Singer, 1999), noun phrase bracketing (Pierce and Cardie, 2001), and statistical parsing (Sarkar, 2001). In this paper we apply Co-Training to the problem of reference resolution in German texts from the tourism domain in order to provide answers to the following questions: Does Co-Training work at all for this task (when compared to conventional C4.5 decision tree learning)? How much labeled training data is required for achieving a reasonable performance? First, we discuss features that have been found to be relevant for the task of reference resolution, and describe the feature set that we are using (Section 2). Then we briefly introduce the Co-Training paradigm (Section 3), which is followe"
P02-1045,W98-1117,0,0.0112533,"is the ratio between p and n, i.e., the number of positive and negative instances added in each iteration. These values are commonly chosen in such a way as to reflect the empirical class distribution of the respective instances. 4 Data 4.1 Text Corpus Our corpus consists of 250 short German texts (total 36924 tokens, 9399 NPs, 2179 anaphoric NPs) about sights, historic events and persons in Heidelberg. The average length of the texts was 149 tokens. The texts were POS-tagged using TnT (Brants, 2000). A basic identification of markables (i.e. NPs) was obtained by using the NP-Chunker Chunkie (Skut and Brants, 1998). The POS-tagger was also used for assigning attributes to markables (e.g. the NP form). The automatic annotation was followed by a man10. 11. 12. 13. 14. 15. 16. Document level features document number (1 . . . 250) doc id NP-level features grammatical function of antecedent (subject, object, other) ante gram func form of antecedent (definite NP, indefinite NP, personal pronoun, ante npform demonstrative pronoun, possessive pronoun, proper name) agreement in person, gender, number ante agree ante semanticclass semantic class of antecedent (human, concrete object, abstract object) ana gram fun"
P02-1045,J01-4004,0,0.517685,"unt of manual labeling work and still produce a classifier with an acceptable performance. 1 Introduction A major obstacle for natural language processing systems which analyze natural language texts or utterances is the need to identify the entities referred to by means of referring expressions. Among referring expressions, pronouns and definite noun phrases (NPs) are the most prominent. Supervised machine learning algorithms were used for pronoun resolution with good results (Ge et al., 1998), and for definite NPs with fairly good results (Aone and Bennett, 1995; McCarthy and Lehnert, 1995; Soon et al., 2001). However, the deficiency of supervised machine learning approaches is the need for an unknown amount of annotated training data for optimal performance. So, researchers in NLP began to experiment with weakly supervised machine learning algorithms such as Co-Training (Blum and Mitchell, 1998). Among others Co-Training was applied to document classification (Blum and Mitchell, 1998), namedentity recognition (Collins and Singer, 1999), noun phrase bracketing (Pierce and Cardie, 2001), and statistical parsing (Sarkar, 2001). In this paper we apply Co-Training to the problem of reference resolutio"
P02-1045,A00-1031,0,0.00594228,"to be disjoint or even conditionally independent (but cf. Nigam and Ghani (2000)). Another important factor is the ratio between p and n, i.e., the number of positive and negative instances added in each iteration. These values are commonly chosen in such a way as to reflect the empirical class distribution of the respective instances. 4 Data 4.1 Text Corpus Our corpus consists of 250 short German texts (total 36924 tokens, 9399 NPs, 2179 anaphoric NPs) about sights, historic events and persons in Heidelberg. The average length of the texts was 149 tokens. The texts were POS-tagged using TnT (Brants, 2000). A basic identification of markables (i.e. NPs) was obtained by using the NP-Chunker Chunkie (Skut and Brants, 1998). The POS-tagger was also used for assigning attributes to markables (e.g. the NP form). The automatic annotation was followed by a man10. 11. 12. 13. 14. 15. 16. Document level features document number (1 . . . 250) doc id NP-level features grammatical function of antecedent (subject, object, other) ante gram func form of antecedent (definite NP, indefinite NP, personal pronoun, ante npform demonstrative pronoun, possessive pronoun, proper name) agreement in person, gender, num"
P02-1045,W02-1040,1,0.589839,"Missing"
P02-1045,W99-0611,0,0.171509,"Missing"
P02-1045,M95-1005,0,0.041231,"de by a simple resolution algorithm that cause in a real-world setting, information about a pronoun’s semantic class obviously is not available prior to its resolution. 2 This filter applies only if the anaphor is a pronoun. This restriction is necessary because German allows for cases where an antecedent is referred back to by a non-pronoun anaphor which has a different grammatical gender. looks for an antecedent from the current position upwards until it finds one or reaches the beginning. Hence, our results are only indirectly comparable with the ones obtained by an evaluation according to Vilain et al. (1995). However, in this paper we only compare results of this direct binary antecedentanaphor pair decision. The remaining texts were split in two sets of 50 resp. 150 texts. From the first, our labeled training set was produced by removing all instances with class DN and TP. The second set was used as our unlabeled training set. From this set, no instances were removed because no knowledge whatsoever about the data can be assumed in a realistic setting. 5 Experiments and Results For our experiments we implemented the standard Co-Training algorithm (as described in Section 3) in Java using the Weka"
P02-1045,J96-2004,0,0.0383182,"Missing"
P02-1045,W99-0613,0,0.0683603,"s were used for pronoun resolution with good results (Ge et al., 1998), and for definite NPs with fairly good results (Aone and Bennett, 1995; McCarthy and Lehnert, 1995; Soon et al., 2001). However, the deficiency of supervised machine learning approaches is the need for an unknown amount of annotated training data for optimal performance. So, researchers in NLP began to experiment with weakly supervised machine learning algorithms such as Co-Training (Blum and Mitchell, 1998). Among others Co-Training was applied to document classification (Blum and Mitchell, 1998), namedentity recognition (Collins and Singer, 1999), noun phrase bracketing (Pierce and Cardie, 2001), and statistical parsing (Sarkar, 2001). In this paper we apply Co-Training to the problem of reference resolution in German texts from the tourism domain in order to provide answers to the following questions: Does Co-Training work at all for this task (when compared to conventional C4.5 decision tree learning)? How much labeled training data is required for achieving a reasonable performance? First, we discuss features that have been found to be relevant for the task of reference resolution, and describe the feature set that we are using (Se"
P03-1022,M95-1005,0,\N,Missing
P03-1022,W99-0207,0,\N,Missing
P03-1022,A97-1052,0,\N,Missing
P03-1022,J01-4003,0,\N,Missing
P03-1022,P02-1014,0,\N,Missing
P03-1022,W02-1040,1,\N,Missing
P03-1022,J01-4004,0,\N,Missing
P03-1022,P02-1011,0,\N,Missing
P03-1022,P02-1045,1,\N,Missing
P07-1041,J05-3002,0,0.022809,"in two steps: First, the best candidate for the initial sentence position is chosen. Then, the order for the remaining constituents is determined. The first task is more difficult than the second one because of properties of the German sentence-initial position. Experiments show a significant improvement over competing approaches. Our algorithm is also more efficient than these. 1 Introduction Many natural languages allow variation in the word order. This is a challenge for natural language generation and machine translation systems, or for text summarizers. E.g., in text-to-text generation (Barzilay & McKeown, 2005; Marsi & Krahmer, 2005; Wan et al., 2005), new sentences are fused from dependency structures of input sentences. The last step of sentence fusion is linearization of the resulting parse. Even for English, which is a language with fixed word order, this is not a trivial task. German has a relatively free word order. This concerns the order of constituents1 within sentences while the order of words within constituents is relatively rigid. The grammar only partially prescribes how constituents dependent on the verb should be ordered, and for many clauses each of the n! possible permutations of"
P07-1041,A00-1031,0,0.00425814,"e probability of an ordered tree. Using “Markov grammars” as the starting point and conditioning on the syntactic category only, they expand a non-terminal node C by predicting its daughters from left to right: http://de.wikipedia.org entwickelte außerdem ADV (conn) Lummer SUBJ (pers) eine Quecksilberdampflampe OBJA um herzustellen SUB monochromatisches Licht Figure 1: The representation of the sentence in Example 1 boundaries are identified with a Perl CPAN module4 whose performance we improved by extending the list of abbreviations. Next, the sentences are split into tokens. The TnT tagger (Brants, 2000) and the TreeTagger (Schmid, 1997) are used for tagging and lemmatization. Finally, the articles are parsed with the CDG dependency parser (Foth & Menzel, 2006). Named entities are classified according to their semantic type using lists and category information from Wikipedia: person (pers), location (loc), organization (org), or undefined named entity (undef ne). Temporal expressions (Oktober 1915, danach (after that) etc.) are identified automatically by a set of patterns. Inevitable during automatic annotation, errors at one of the preprocessing stages cause errors at the ordering stage. Di"
P07-1041,P06-1041,0,0.0470126,"al node C by predicting its daughters from left to right: http://de.wikipedia.org entwickelte außerdem ADV (conn) Lummer SUBJ (pers) eine Quecksilberdampflampe OBJA um herzustellen SUB monochromatisches Licht Figure 1: The representation of the sentence in Example 1 boundaries are identified with a Perl CPAN module4 whose performance we improved by extending the list of abbreviations. Next, the sentences are split into tokens. The TnT tagger (Brants, 2000) and the TreeTagger (Schmid, 1997) are used for tagging and lemmatization. Finally, the articles are parsed with the CDG dependency parser (Foth & Menzel, 2006). Named entities are classified according to their semantic type using lists and category information from Wikipedia: person (pers), location (loc), organization (org), or undefined named entity (undef ne). Temporal expressions (Oktober 1915, danach (after that) etc.) are identified automatically by a set of patterns. Inevitable during automatic annotation, errors at one of the preprocessing stages cause errors at the ordering stage. Distinguishing between main and subordinate clauses, we split the total of about 19 000 sentences into training, development and test sets (Table 1). Clauses with"
P07-1041,W06-1402,0,0.0926221,"ed by the findings of the Prague School (Sgall et al., 1986) and Systemic Functional Linguistics (Halliday, 1985), they focus on the role that information structure plays in constituent ordering. Kruijff-Korbayov´a et al. (2002) address the task of word order generation in the same vein. Similar to ours, their algorithm recognizes the special role of the sentence-initial position which they reserve for the theme – the point of departure of the message. Unfortunately, they did not implement their algorithm, and it is hard to judge how well the system would perform on real data. Harbusch et al. (2006) present a generation workbench, which has the goal of producing not the most appropriate order, but all grammatical ones. They also do not provide experimental results. The work of Uchimoto et al. (2000) is done on the free word order language Japanese. They determine the order of phrasal units dependent on the same modifiee. Their approach is similar to ours in that they aim at regenerating the original order from a dependency parse, but differs in the scope of the problem as they regenerate the order of modifers for all and not only for the top clausal node. Using a maximum entropy framewor"
P07-1041,W01-0810,0,0.31785,"Missing"
P07-1041,W00-1905,0,0.0276664,"lay a role. It has also been observed that there are preferences for a particular order. The preferences summarized below have mo320 Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 320–327, c Prague, Czech Republic, June 2007. 2007 Association for Computational Linguistics tivated our choice of features: • constituents in the nominative case precede those in other cases, and dative constituents often precede those in the accusative case (Uszkoreit, 1987; Keller, 2000); • the verb arguments’ order depends on the verb’s subcategorization properties (Kurz, 2000); • constituents with a definite article precede those with an indefinite one (Weber & Mu¨ ller, 2004); • pronominalized constituents precede nonpronominalized ones (Kempen & Harbusch, 2004); • animate referents precede inanimate ones (Pappert et al., 2007); • short constituents precede longer ones (Kimball, 1973); • the preferred topic position is right after the verb (Frey, 2004); • the initial position is usually occupied by scene-setting elements and topics (Speyer, 2005). • there is a default order based on semantic properties of constituents (Sgall et al., 1986): Actor < Temporal < Space"
P07-1041,P98-1116,0,0.078729,"ich orders not only verb arguments but all kinds of constituents, and evaluate it on a corpus of biographies. For each parsed sentence in the test set, our maximumentropy-based algorithm aims at reproducing the order found in the original text. We investigate the importance of different linguistic factors and suggest an algorithm to constituent ordering which first determines the sentence initial constituent and then orders the remaining ones. We provide evidence that the task requires language-specific knowledge to achieve better results and point to the most difficult part of it. Similar to Langkilde & Knight (1998) we utilize statistical methods. Unlike overgeneration approaches (Varges & Mellish, 2001, inter alia) which select the best of all possible outputs ours is more efficient, because we do not need to generate every permutation. 2 Theoretical Premises 2.1 Background It has been suggested that several factors have an influence on German constituent order. Apart from the constraints posed by the grammar, information structure, surface form, and discourse status have also been shown to play a role. It has also been observed that there are preferences for a particular order. The preferences summariz"
P07-1041,J06-4002,0,0.0225549,"wever, like τ , agr and inv can give a positive score to an ungrammatical order. Hence, none of the evaluation metrics describes the performance perfectly. Human evaluation which reliably distinguishes between appropriate, acceptable, grammatical and ingrammatical orders was out of choice because of its high cost. 6.2 Results Results 6.1 Evaluation Metrics We use several metrics to evaluate our systems and the baselines. The first is per-sentence accuracy (acc) which is the proportion of correctly regenerated sentences. Kendall’s τ , which has been used for evaluating sentence ordering tasks (Lapata, 2006), is the second metric we use. τ is calculated as 1 − 4 N (Nt −1) , where t is the number of interchanges of consecutive elements to arrange N elements in the right order. τ is sensitive to near misses and assigns abdc (almost correct order) a score of 0.66 while dcba (inverse order) gets −1. Note that it is questionable whether this metric is as appropriate for word ordering tasks as for sentence ordering ones because a near miss might turn out to be ungrammatical whereas a more different order stays acceptable. Apart from acc and τ , we also adopt the metrics used by Uchimoto et al. (2000) a"
P07-1041,W05-1612,0,0.0273982,"best candidate for the initial sentence position is chosen. Then, the order for the remaining constituents is determined. The first task is more difficult than the second one because of properties of the German sentence-initial position. Experiments show a significant improvement over competing approaches. Our algorithm is also more efficient than these. 1 Introduction Many natural languages allow variation in the word order. This is a challenge for natural language generation and machine translation systems, or for text summarizers. E.g., in text-to-text generation (Barzilay & McKeown, 2005; Marsi & Krahmer, 2005; Wan et al., 2005), new sentences are fused from dependency structures of input sentences. The last step of sentence fusion is linearization of the resulting parse. Even for English, which is a language with fixed word order, this is not a trivial task. German has a relatively free word order. This concerns the order of constituents1 within sentences while the order of words within constituents is relatively rigid. The grammar only partially prescribes how constituents dependent on the verb should be ordered, and for many clauses each of the n! possible permutations of n constituents is gramm"
P07-1041,C04-1097,0,0.693727,"ne. Data The data we work with is a collection of biographies from the German version of Wikipedia3 . Fully automatic preprocessing in our system comprises the following steps: First, a list of people of a certain Wikipedia category is taken and an article is extracted for every person. Second, sentence 3 Only reference orders are assumed to be correct. n Y i=1 4 i=1 j=1 2 is 1 if word wi precedes wi+j in the reference sentence, 0 otherwise. The features they use are akin to those which play a role in determining German word order. We use their approach as a non-trivial baseline in our study. Ringger et al. (2004) aim at regenerating the order of constituents as well as the order within them for German and French technical manuals. Utilizing syntactic, semantic, sub-categorization and length features, they test several statistical models to find the order which maximizes the probability of an ordered tree. Using “Markov grammars” as the starting point and conditioning on the syntactic category only, they expand a non-terminal node C by predicting its daughters from left to right: http://de.wikipedia.org entwickelte außerdem ADV (conn) Lummer SUBJ (pers) eine Quecksilberdampflampe OBJA um herzustellen S"
P07-1041,C00-2126,0,0.663736,"dering. Kruijff-Korbayov´a et al. (2002) address the task of word order generation in the same vein. Similar to ours, their algorithm recognizes the special role of the sentence-initial position which they reserve for the theme – the point of departure of the message. Unfortunately, they did not implement their algorithm, and it is hard to judge how well the system would perform on real data. Harbusch et al. (2006) present a generation workbench, which has the goal of producing not the most appropriate order, but all grammatical ones. They also do not provide experimental results. The work of Uchimoto et al. (2000) is done on the free word order language Japanese. They determine the order of phrasal units dependent on the same modifiee. Their approach is similar to ours in that they aim at regenerating the original order from a dependency parse, but differs in the scope of the problem as they regenerate the order of modifers for all and not only for the top clausal node. Using a maximum entropy framework, they choose the most probable order from the set of all permutations of n words by the following formula: P (1|h) = P ({Wi,i+j = 1|1 ≤ i ≤ n − 1, 1 ≤ j ≤ n − i}|h) ≈ n−1 Y n−i Y P (Wi,i+j = 1|hi,i+j )"
P07-1041,N01-1001,0,0.0224623,"s of biographies. For each parsed sentence in the test set, our maximumentropy-based algorithm aims at reproducing the order found in the original text. We investigate the importance of different linguistic factors and suggest an algorithm to constituent ordering which first determines the sentence initial constituent and then orders the remaining ones. We provide evidence that the task requires language-specific knowledge to achieve better results and point to the most difficult part of it. Similar to Langkilde & Knight (1998) we utilize statistical methods. Unlike overgeneration approaches (Varges & Mellish, 2001, inter alia) which select the best of all possible outputs ours is more efficient, because we do not need to generate every permutation. 2 Theoretical Premises 2.1 Background It has been suggested that several factors have an influence on German constituent order. Apart from the constraints posed by the grammar, information structure, surface form, and discourse status have also been shown to play a role. It has also been observed that there are preferences for a particular order. The preferences summarized below have mo320 Proceedings of the 45th Annual Meeting of the Association of Computat"
P07-1041,W05-1628,0,0.0275081,"initial sentence position is chosen. Then, the order for the remaining constituents is determined. The first task is more difficult than the second one because of properties of the German sentence-initial position. Experiments show a significant improvement over competing approaches. Our algorithm is also more efficient than these. 1 Introduction Many natural languages allow variation in the word order. This is a challenge for natural language generation and machine translation systems, or for text summarizers. E.g., in text-to-text generation (Barzilay & McKeown, 2005; Marsi & Krahmer, 2005; Wan et al., 2005), new sentences are fused from dependency structures of input sentences. The last step of sentence fusion is linearization of the resulting parse. Even for English, which is a language with fixed word order, this is not a trivial task. German has a relatively free word order. This concerns the order of constituents1 within sentences while the order of words within constituents is relatively rigid. The grammar only partially prescribes how constituents dependent on the verb should be ordered, and for many clauses each of the n! possible permutations of n constituents is grammatical. 1 Hencefort"
P07-1041,W04-1911,0,0.0453516,"Missing"
P07-1041,C98-1112,0,\N,Missing
P07-2013,P05-3002,0,0.0196779,"e accessed using robust Open Source applications, e.g. the MediaWiki software5 , integrated within a Linux, Apache, MySQL and PHP (LAMP) software bundle. The architecture of the API consists of the following modules: 1. RDBMS: at the lowest level, the encyclopedia content is stored in a relational database management system (e.g. MySQL). 2. MediaWiki: a suite of PHP routines for interacting with the RDBMS. 3. WWW-Wikipedia Perl library6 : responsible for 4 In contrast to WordNet::Similarity, which due to the structural variations between the respective wordnets was reimplemented for German by Gurevych & Niederlich (2005). 5 6 http://www.mediawiki.org http://search.cpan.org/dist/WWW-Wikipedia 50 Using the API The API provides factory classes for querying Wikipedia, in order to retrieve encyclopedia entries as well as relatedness scores for word pairs. In practice, the Java library provides a simple programmatic interface. Users can accordingly access the library using only a few methods given in the factory classes, e.g. getPage(word) for retrieving Wikipedia articles titled word or getRelatedness(word1,word2), for computing the relatedness between word1 and word2, and display(path) for displaying a path found"
P07-2013,I05-1082,0,0.0217934,"in the categorization graph. Examples of programmatic usage of the API are presented in Figure 3. In addition, the software distribution includes UNIX shell scripts to access the API interactively from a terminal, i.e. it does not require any knowledge of Java. 6 Application scenarios Semantic relatedness measures have proven useful in many NLP applications such as word sense disambiguation (Kohomban & Lee, 2005; Patwardhan et al., 2005), information retrieval (Finkelstein et al., 2002), information extraction pattern induction (Stevenson & Greenwood, 2005), interpretation of noun compounds (Kim & Baldwin, 2005), parap u k o l e c i t e r s a a : b t a D ) e : g a p ( t y e r s e u t l q u L s iid k e Q R S W : a e M t c : lld a e c j b e o u e l c o i m t r A P P H r e H v : r e s b e W t x : e t s t e p u u q k e r r a m P a T ik d iW k e H p iW :  e W l a y c r e h : c l u u r t q d a c ) o e e e s m k s j n b r n o h o t i l m t r l y a l e r d e r p P e o n a P g d a : d e n s C t a e a P i n c r R o o m  e L g i t e o s M c r n t a X f a r o t t c h p s : x ( p e s e y t a e u e r r r y q e y g s r e C r u o r t e a P q g l t P u R e a b L T s  t e Q ilr L e a r M R S H C C e X : p : : : a r"
P07-2013,P05-1005,0,0.0161109,"etrieving Wikipedia articles titled word or getRelatedness(word1,word2), for computing the relatedness between word1 and word2, and display(path) for displaying a path found between two Wikipedia articles in the categorization graph. Examples of programmatic usage of the API are presented in Figure 3. In addition, the software distribution includes UNIX shell scripts to access the API interactively from a terminal, i.e. it does not require any knowledge of Java. 6 Application scenarios Semantic relatedness measures have proven useful in many NLP applications such as word sense disambiguation (Kohomban & Lee, 2005; Patwardhan et al., 2005), information retrieval (Finkelstein et al., 2002), information extraction pattern induction (Stevenson & Greenwood, 2005), interpretation of noun compounds (Kim & Baldwin, 2005), parap u k o l e c i t e r s a a : b t a D ) e : g a p ( t y e r s e u t l q u L s iid k e Q R S W : a e M t c : lld a e c j b e o u e l c o i m t r A P P H r e H v : r e s b e W t x : e t s t e p u u q k e r r a m P a T ik d iW k e H p iW :  e W l a y c r e h : c l u u r t q d a c ) o e e e s m k s j n b r n o h o t i l m t r l y a l e r d e r p P e o n a P g d a : d e n s C t a e a P i n c"
P07-2013,P05-3019,0,0.0154194,"ticles titled word or getRelatedness(word1,word2), for computing the relatedness between word1 and word2, and display(path) for displaying a path found between two Wikipedia articles in the categorization graph. Examples of programmatic usage of the API are presented in Figure 3. In addition, the software distribution includes UNIX shell scripts to access the API interactively from a terminal, i.e. it does not require any knowledge of Java. 6 Application scenarios Semantic relatedness measures have proven useful in many NLP applications such as word sense disambiguation (Kohomban & Lee, 2005; Patwardhan et al., 2005), information retrieval (Finkelstein et al., 2002), information extraction pattern induction (Stevenson & Greenwood, 2005), interpretation of noun compounds (Kim & Baldwin, 2005), parap u k o l e c i t e r s a a : b t a D ) e : g a p ( t y e r s e u t l q u L s iid k e Q R S W : a e M t c : lld a e c j b e o u e l c o i m t r A P P H r e H v : r e s b e W t x : e t s t e p u u q k e r r a m P a T ik d iW k e H p iW :  e W l a y c r e h : c l u u r t q d a c ) o e e e s m k s j n b r n o h o t i l m t r l y a l e r d e r p P e o n a P g d a : d e n s C t a e a P i n c r R o o m  e L g i t e o"
P07-2013,N04-3012,0,0.0414517,"Michael Strube EML Research gGmbH Schloss-Wolfsbrunnenweg 33 69118 Heidelberg, Germany http://www.eml-research.de/nlp 3 Abstract We present an API for computing the semantic relatedness of words in Wikipedia. 1 The API computes semantic relatedness by: 1. taking a pair of words as input; Introduction The last years have seen a large amount of work in Natural Language Processing (NLP) using measures of semantic similarity and relatedness. We believe that the extensive usage of such measures derives also from the availability of robust and freely available software that allows to compute them (Pedersen et al., 2004, WordNet::Similarity). In Ponzetto & Strube (2006) and Strube & Ponzetto (2006) we proposed to take the Wikipedia categorization system as a semantic network which served as basis for computing the semantic relatedness of words. In the following we present the API we used in our previous work, hoping that it will encourage further research in NLP using Wikipedia1 . 2 The Application Programming Interface Measures of Semantic Relatedness Approaches to measuring semantic relatedness that use lexical resources transform these resources into a network or graph and compute relatedness using paths"
P07-2013,N06-1025,1,0.816102,"brunnenweg 33 69118 Heidelberg, Germany http://www.eml-research.de/nlp 3 Abstract We present an API for computing the semantic relatedness of words in Wikipedia. 1 The API computes semantic relatedness by: 1. taking a pair of words as input; Introduction The last years have seen a large amount of work in Natural Language Processing (NLP) using measures of semantic similarity and relatedness. We believe that the extensive usage of such measures derives also from the availability of robust and freely available software that allows to compute them (Pedersen et al., 2004, WordNet::Similarity). In Ponzetto & Strube (2006) and Strube & Ponzetto (2006) we proposed to take the Wikipedia categorization system as a semantic network which served as basis for computing the semantic relatedness of words. In the following we present the API we used in our previous work, hoping that it will encourage further research in NLP using Wikipedia1 . 2 The Application Programming Interface Measures of Semantic Relatedness Approaches to measuring semantic relatedness that use lexical resources transform these resources into a network or graph and compute relatedness using paths in it (see Budanitsky & Hirst (2006) for an extensi"
P07-2013,P05-1047,0,0.0198923,"path) for displaying a path found between two Wikipedia articles in the categorization graph. Examples of programmatic usage of the API are presented in Figure 3. In addition, the software distribution includes UNIX shell scripts to access the API interactively from a terminal, i.e. it does not require any knowledge of Java. 6 Application scenarios Semantic relatedness measures have proven useful in many NLP applications such as word sense disambiguation (Kohomban & Lee, 2005; Patwardhan et al., 2005), information retrieval (Finkelstein et al., 2002), information extraction pattern induction (Stevenson & Greenwood, 2005), interpretation of noun compounds (Kim & Baldwin, 2005), parap u k o l e c i t e r s a a : b t a D ) e : g a p ( t y e r s e u t l q u L s iid k e Q R S W : a e M t c : lld a e c j b e o u e l c o i m t r A P P H r e H v : r e s b e W t x : e t s t e p u u q k e r r a m P a T ik d iW k e H p iW :  e W l a y c r e h : c l u u r t q d a c ) o e e e s m k s j n b r n o h o t i l m t r l y a l e r d e r p P e o n a P g d a : d e n s C t a e a P i n c r R o o m  e L g i t e o s M c r n t a X f a r o t t c h p s : x ( p e s e y t a e u e r r r y q e y g s r e C r u o r t e a P q g l t P u R e a b"
P07-2013,P94-1019,0,0.0196934,"me approach with Roget’s Thesaurus while Hirst & St-Onge (1998) apply a similar strategy to WordNet. 1 The software can be freely downloaded at http://www. eml-research.de/nlp/download/wikipediasimilarity.php. 2. retrieving the Wikipedia articles they refer to (via a disambiguation strategy based on the link structure of the articles); 3. computing paths in the Wikipedia categorization graph between the categories the articles are assigned to; 4. returning as output the set of paths found, scored according to some measure definition. The implementation includes path-length (Rada et al., 1989; Wu & Palmer, 1994; Leacock & Chodorow, 1998), information-content (Resnik, 1995; Seco et al., 2004) and text-overlap (Lesk, 1986; Banerjee & Pedersen, 2003) measures, as described in Strube & Ponzetto (2006). The API is built on top of several modules and can be used for tasks other than Wikipedia-based relatedness computation. On a basic usage level, it can be used to retrieve Wikipedia articles by name, optionally using disambiguation patterns, as well as to find a ranked set of articles satisfying a search query (via integration with the Lucene2 text search engine). Additionally, it provides functionality f"
P07-2013,J06-1003,0,\N,Missing
P09-2044,C08-1006,0,0.0113481,"ty (see Pang & Lee (2008) for an overview). However, even opinions have to be backed up by facts to be effective as arguments. Distinguishing facts from fiction requires to detect subtle variations in the use of linguistic devices such as linguistic hedges which indicate that speakers do not back up their opinions with facts (Lakoff, 1973; Hyland, 1998). Many NLP applications could benefit from identifying linguistic hedges, e.g. question answering systems (Riloff et al., 2003), information extraction from biomedical documents (Medlock & Briscoe, 2007; Szarvas, 2008), and deception detection (Bachenko et al., 2008). While NLP research on classifying linguistic hedges has been restricted to analysing biomedical documents, the above (incomplete) list of applications suggests that domain- and languageindependent approaches for hedge detection need to be developed. We investigate Wikipedia as a source of training data for hedge classification. We adopt Wikipedia’s notion of weasel words which we argue to be closely related to hedges and private states. Many Wikipedia articles contain a specific weasel tag, so that Wikipedia can be viewed 3 Weasel Words Wikipedia editors are advised to avoid weasel words, be"
P09-2044,A00-1031,0,0.0163691,"ntexts and the corpus in general and b) on the average distance the word has to a weasel tag, if found in a weasel context. We assume that a word is an indicator for a weasel if it occurs close before a weasel tag. The final scoring function for each word in the training set 2 http://en.wikipedia.org/wiki/ Wikipedia:Avoid_weasel_words 3 http://download.wikipedia.org/ 174 is thus: Score(w) = RelF (w) + AvgDist(w) with RelF (w) = W (w) log2 (C(w)) 2. Passive constructions (“It is believed”, “It is considered”) (1) 3. Adverbs (“Often”, “Probably”) We POS-tagged the test data with the TnT tagger (Brants, 2000) and developed finite state automata to detect such constellations. We combine these syntactic patterns with the word-scoring function from above. If a pattern is found, only the head of the pattern (i.e., adverbs, main verbs for passive patterns, nouns and quantifiers for numerically underspecified subjects) is assigned a score. The scoring function adding syntactic patterns (asp) for each sentence is: (2) and W (w) AvgDist(w) = PW (w) dist(w, weaseltagj ) (3) W (w) denotes the number of times word w occurred in the context of a weasel tag, whereas C(w) denotes the total number of times w occ"
P09-2044,J96-2004,0,0.10215,"to be a much higher number of potential weasel words which had not yet been tagged leading to false positives. Therefore, we also annotated a small sample manually. One of the authors, two linguists and one computer scientist annotated 100 sentences each, 50 of which were the same for all annotators to enable measuring agreement. The annotators labeled the data independently and following annotation guidelines which were mainly adopted from the Wikipedia style guide with only small adjustments to match our pre-processed data. We then used Cohen’s Kappa (κ) to determine the level of agreement (Carletta, 1996). Table 4 shows the agreement between each possible pair of annotators. The overall inter-annotator agreement was κ = 0.65, which is similar to what Light et al. (2004) report but worse than Medlock & Briscoe’s (2007) results. As Gold standard we merged all four annotations sets. From the 50 overlapping instances, we removed those where less than three annotators had agreed on one category, resulting in a set of 246 sentences for evaluation. (1) Others argue {{weasel-inline}} that the news media are simply catering to public demand. (2) ...therefore America is viewed by some {{weasel-inline}}"
P09-2044,P07-1125,0,0.714429,"ady annotated corpus using its tagged weasel words which mark sentences and phrases as non-factual. We evaluate the quality of Wikipedia as training data for hedge detection, as well as shallow linguistic features. 1 2 Related Work Research on hedge detection in NLP has been focused almost exclusively on the biomedical domain. Light et al. (2004) present a study on annotating hedges in biomedical documents. They show that the phenomenon can be annotated tentatively reliably by non-domain experts when using a two-way distinction. They also perform first experiments on automatic classification. Medlock & Briscoe (2007) develop a weakly supervised system for hedge classification in a very narrow subdomain in the life sciences. They start with a small set of seed examples known to indicate hedging. Then they iterate and acquire more training seeds without much manual intervention (step 2 in their seed generation procedure indicates that there is some manual intervention). Their best system results in a 0.76 precision/recall break-even-point (BEP). While Medlock & Briscoe use words as features, Szarvas (2008) extends their work to n-grams. He also applies his method to (slightly) out of domain data and observe"
P09-2044,W03-0404,0,0.0115756,"sing is dealing with identifying, extracting and classifying facts, recent years have seen a surge in research on sentiment and subjectivity (see Pang & Lee (2008) for an overview). However, even opinions have to be backed up by facts to be effective as arguments. Distinguishing facts from fiction requires to detect subtle variations in the use of linguistic devices such as linguistic hedges which indicate that speakers do not back up their opinions with facts (Lakoff, 1973; Hyland, 1998). Many NLP applications could benefit from identifying linguistic hedges, e.g. question answering systems (Riloff et al., 2003), information extraction from biomedical documents (Medlock & Briscoe, 2007; Szarvas, 2008), and deception detection (Bachenko et al., 2008). While NLP research on classifying linguistic hedges has been restricted to analysing biomedical documents, the above (incomplete) list of applications suggests that domain- and languageindependent approaches for hedge detection need to be developed. We investigate Wikipedia as a source of training data for hedge classification. We adopt Wikipedia’s notion of weasel words which we argue to be closely related to hedges and private states. Many Wikipedia ar"
P09-2044,P08-1033,0,0.288643,"sing a two-way distinction. They also perform first experiments on automatic classification. Medlock & Briscoe (2007) develop a weakly supervised system for hedge classification in a very narrow subdomain in the life sciences. They start with a small set of seed examples known to indicate hedging. Then they iterate and acquire more training seeds without much manual intervention (step 2 in their seed generation procedure indicates that there is some manual intervention). Their best system results in a 0.76 precision/recall break-even-point (BEP). While Medlock & Briscoe use words as features, Szarvas (2008) extends their work to n-grams. He also applies his method to (slightly) out of domain data and observes a considerable drop in performance. Introduction While most research in natural language processing is dealing with identifying, extracting and classifying facts, recent years have seen a surge in research on sentiment and subjectivity (see Pang & Lee (2008) for an overview). However, even opinions have to be backed up by facts to be effective as arguments. Distinguishing facts from fiction requires to detect subtle variations in the use of linguistic devices such as linguistic hedges which"
P09-2044,W04-3103,0,\N,Missing
P12-1084,J08-4004,0,0.0219377,"e annotated by Annotator A. Finally, Annotator A carried out consistency checks over all texts. – The gold standard includes 10,980 true mentions (see Table 3). Table 1: Agreement Results κ Non-mention κ Old κ New κ Mediated/Knowledge κ Mediated/Synt κ Mediated/Aggregate κ Mediated/Func κ Mediated/Comp κ Mediated/Bridging A-B 81.5 80.5 76.6 82.1 88.4 87.0 6.0 81.8 70.8 A-C 78.9 83.2 74.0 78.4 87.8 85.4 83.2 78.3 60.6 Gold Standard B-C 86.0 79.3 74.3 74.1 87.6 86.0 6.9 81.2 62.3 Texts Mentions old Table 2: Agreement Results for individual categories percentage agreement, we measured Cohen’s κ (Artstein and Poesio, 2008) between all 3 possible annotator pairings. We also report single-category agreement for each category, where all categories but one are merged and then κ is computed as usual. Table 1 shows agreement results for the overall scheme at the coarse-grained (4 categories: non-mention, old, new, mediated) and the fine-grained level (9 categories: non-mention, old, new and the 6 mediated subtypes). The results show that the scheme is overall reliable, with not too many differences between the different annotator pairings.7 Table 2 shows the individual category agreement for all 9 categories. We achi"
P12-1084,J08-1001,0,0.0964698,"Notes corpus (Weischedel et al., 2011). We also report the first results on fine-grained IS classification by modelling further distinctions within the category of mediated mentions, such as comparative and bridging anaphora (see Examples 1 and 2, re795 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 795–804, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics spectively).2 Fine-grained IS is a prerequisite to full bridging/comparative anaphora resolution, and therefore necessary to fill gaps in entity grids (Barzilay and Lapata, 2008) based on coreference only. Thus, Examples 1 and 2 do not exhibit any coreferential entity coherence but coherence can be established when the comparative anaphor others is resolved to others than freeway survivor Buck Helm, and the bridging anaphor the streets is resolved to the streets of Oranjemund, respectively. (1) the condition of freeway survivor Buck Helm . . . , improved, hospital officials said. Rescue crews, however, gave up hope that others would be found. (2) Oranjemund, the mine headquarters, is a lonely corporate oasis of 9,000 residents. Jackals roam the streets at night . . ."
P12-1084,P09-1092,0,0.293161,"nly on the mention itself but also on other mentions in the vicinity and solve the task by collectively classifying the information status of all mentions. Our approach strongly outperforms reimplementations of previous work. IS can be beneficial for a number of NLP tasks, though the results have been mixed. Nenkova et al. (2007) used IS as a feature for generating pitch accent in conversational speech. As IS is restricted to noun phrases, while pitch accent can be assigned to any word in an utterance, the experiments were not conclusive. For determining constituent order of German sentences, Cahill and Riester (2009) incorporate features modeling IS to good effect. Rahman and Ng (2011) showed that IS is a useful feature for coreference resolution. 1 Introduction Speakers present already known and yet to be established information according to principles referred to as information structure (Prince, 1981; Lambrecht, 1994; Kruijff-Korbayov´a and Steedman, 2003, inter alia). While information structure affects all kinds of constituents in a sentence, we here adopt the more restricted notion of information status which concerns only discourse entities realized as noun phrases, i.e. mentions1 . Information sta"
P12-1084,W11-1907,1,0.885694,"Missing"
P12-1084,J96-2004,0,0.15904,"ifferent annotator pairings.7 Table 2 shows the individual category agreement for all 9 categories. We achieve high reliability for most categories.8 Particularly interesting is the fact that hearer-old entities (mediated/knowledge) can be identified reliably although all annotators had substantially different backgrounds. The reliability of the category bridging is more annotatordependent, although still higher, sometimes considerably, than other previous attempts at bridg7 Often, annotation is considered highly reliable when κ exceeds 0.80 and marginally reliable when between 0.67 and 0.80 (Carletta, 1996). However, the interpretation of κ is still under discussion (Artstein and Poesio, 2008). 8 The low reliability of the rare category func, when involving Annotator B, was explained by Annotator B forgetting about this category after having used it once. Pair A-C achieved high reliability (κ 83.2 for pair A-C). 798 coref generic deictic pr mediated world knowledge syntactic aggregate func comparative bridging new 50 10,980 3237 3,143 94 3,708 924 1,592 211 65 253 663 4,035 Table 3: Gold Standard Distribution 4 Features In this Section, we describe both the local as well as the relational featur"
P12-1084,N07-1030,0,0.0609095,"Missing"
P12-1084,P07-1041,1,0.847197,"Missing"
P12-1084,J95-2003,0,0.784558,"Missing"
P12-1084,J05-3004,1,0.860388,"cal classification problem whereas we look at dependencies between the IS status of different mentions, leading to collective classification. In addition, they only distinguish the three main categories old, mediated and new. Finally, we work on news corpora which poses different problems from dialogue. Anaphoricity determination (Ng, 2009; Zhou and Kong, 2009) identifies many or most old mentions. However, no distinction between mediated and new mentions is made. Most approaches to bridging resolution (Meyer and Dale, 2002; Poesio et al., 2004) or comparative anaphora (Modjeska et al., 2003; Markert and Nissim, 2005) address only the selection of the antecedent for the bridging/comparative anaphor, not its recognition. Sasano and Kurohashi (2009) do also tackle bridging recognition, but they depend on languagespecific non-transferrable features for Japanese. 3 Corpus Creation 3.1 ing most generics as well as newly introduced, specific mentions such as Example 7. Annotation Scheme Our scheme follows Nissim et al. (2004) in distinguishing three major IS categories old, new and mediated. A mention is old if it is either coreferential with an already introduced entity or a generic or deictic pronoun. We follo"
P12-1084,W03-1023,1,0.870658,"classification as a local classification problem whereas we look at dependencies between the IS status of different mentions, leading to collective classification. In addition, they only distinguish the three main categories old, mediated and new. Finally, we work on news corpora which poses different problems from dialogue. Anaphoricity determination (Ng, 2009; Zhou and Kong, 2009) identifies many or most old mentions. However, no distinction between mediated and new mentions is made. Most approaches to bridging resolution (Meyer and Dale, 2002; Poesio et al., 2004) or comparative anaphora (Modjeska et al., 2003; Markert and Nissim, 2005) address only the selection of the antecedent for the bridging/comparative anaphor, not its recognition. Sasano and Kurohashi (2009) do also tackle bridging recognition, but they depend on languagespecific non-transferrable features for Japanese. 3 Corpus Creation 3.1 ing most generics as well as newly introduced, specific mentions such as Example 7. Annotation Scheme Our scheme follows Nissim et al. (2004) in distinguishing three major IS categories old, new and mediated. A mention is old if it is either coreferential with an already introduced entity or a generic o"
P12-1084,N07-1002,0,0.0225811,"ersational dialogue. We here introduce the task of classifying finegrained information status and work on written text. We add a fine-grained information status layer to the Wall Street Journal portion of the OntoNotes corpus. We claim that the information status of a mention depends not only on the mention itself but also on other mentions in the vicinity and solve the task by collectively classifying the information status of all mentions. Our approach strongly outperforms reimplementations of previous work. IS can be beneficial for a number of NLP tasks, though the results have been mixed. Nenkova et al. (2007) used IS as a feature for generating pitch accent in conversational speech. As IS is restricted to noun phrases, while pitch accent can be assigned to any word in an utterance, the experiments were not conclusive. For determining constituent order of German sentences, Cahill and Riester (2009) incorporate features modeling IS to good effect. Rahman and Ng (2011) showed that IS is a useful feature for coreference resolution. 1 Introduction Speakers present already known and yet to be established information according to principles referred to as information structure (Prince, 1981; Lambrecht, 1"
P12-1084,N09-1065,0,0.0165875,"n a subproblem of IS only, namely the hearer-old/hearer-new distinctions for person proper names. Nissim (2006) and Rahman and Ng (2011) both present algorithms for IS detection on Nissim et al.’s (2004) Switchboard corpus. Both papers treat IS classification as a local classification problem whereas we look at dependencies between the IS status of different mentions, leading to collective classification. In addition, they only distinguish the three main categories old, mediated and new. Finally, we work on news corpora which poses different problems from dialogue. Anaphoricity determination (Ng, 2009; Zhou and Kong, 2009) identifies many or most old mentions. However, no distinction between mediated and new mentions is made. Most approaches to bridging resolution (Meyer and Dale, 2002; Poesio et al., 2004) or comparative anaphora (Modjeska et al., 2003; Markert and Nissim, 2005) address only the selection of the antecedent for the bridging/comparative anaphor, not its recognition. Sasano and Kurohashi (2009) do also tackle bridging recognition, but they depend on languagespecific non-transferrable features for Japanese. 3 Corpus Creation 3.1 ing most generics as well as newly introduced,"
P12-1084,nissim-etal-2004-annotation,0,0.330285,"and yet to be established information according to principles referred to as information structure (Prince, 1981; Lambrecht, 1994; Kruijff-Korbayov´a and Steedman, 2003, inter alia). While information structure affects all kinds of constituents in a sentence, we here adopt the more restricted notion of information status which concerns only discourse entities realized as noun phrases, i.e. mentions1 . Information status (IS henceforth) describes the degree to which a discourse entity is available to the hearer with regard to the speaker’s assumptions about the hearer’s knowledge and beliefs (Nissim et al., 2004). Old mentions are known to the hearer and have been referred 1 Since not all noun phrases are referential, we call noun phrases which carry information status mentions. Previous work on learning IS (Nissim, 2006; Rahman and Ng, 2011) is restricted in several ways. It deals with conversational dialogue, in particular with the corpus annotated by Nissim et al. (2004). However, many applications that can profit from IS concentrate on written texts, such as summarization. For example, Siddharthan et al. (2011) show that solving the IS subproblem of whether a person proper name is already known to"
P12-1084,W06-1612,0,0.456196,"all kinds of constituents in a sentence, we here adopt the more restricted notion of information status which concerns only discourse entities realized as noun phrases, i.e. mentions1 . Information status (IS henceforth) describes the degree to which a discourse entity is available to the hearer with regard to the speaker’s assumptions about the hearer’s knowledge and beliefs (Nissim et al., 2004). Old mentions are known to the hearer and have been referred 1 Since not all noun phrases are referential, we call noun phrases which carry information status mentions. Previous work on learning IS (Nissim, 2006; Rahman and Ng, 2011) is restricted in several ways. It deals with conversational dialogue, in particular with the corpus annotated by Nissim et al. (2004). However, many applications that can profit from IS concentrate on written texts, such as summarization. For example, Siddharthan et al. (2011) show that solving the IS subproblem of whether a person proper name is already known to the reader improves automatic summarization of news. Therefore, we here model IS in written text, creating a new dataset which adds an IS layer to the already existing comprehensive annotation in the OntoNotes c"
P12-1084,P04-1035,0,0.00646741,"Missing"
P12-1084,P04-1019,0,0.672562,"2004) Switchboard corpus. Both papers treat IS classification as a local classification problem whereas we look at dependencies between the IS status of different mentions, leading to collective classification. In addition, they only distinguish the three main categories old, mediated and new. Finally, we work on news corpora which poses different problems from dialogue. Anaphoricity determination (Ng, 2009; Zhou and Kong, 2009) identifies many or most old mentions. However, no distinction between mediated and new mentions is made. Most approaches to bridging resolution (Meyer and Dale, 2002; Poesio et al., 2004) or comparative anaphora (Modjeska et al., 2003; Markert and Nissim, 2005) address only the selection of the antecedent for the bridging/comparative anaphor, not its recognition. Sasano and Kurohashi (2009) do also tackle bridging recognition, but they depend on languagespecific non-transferrable features for Japanese. 3 Corpus Creation 3.1 ing most generics as well as newly introduced, specific mentions such as Example 7. Annotation Scheme Our scheme follows Nissim et al. (2004) in distinguishing three major IS categories old, new and mediated. A mention is old if it is either coreferential w"
P12-1084,W04-2327,0,0.0843417,"his to the higher syntactic complexity and semantic vagueness in the commentary corpus. Riester et al. (2010) annotated a 2 All examples in this paper are from the OntoNotes corpus. The mention in question is typed in boldface; antecedents, where applicable, are displayed in italics. 796 German news corpus marginally reliable (κ = 0.66) for their overall scheme but their confusion matrix shows even lower reliability for several subcategories, most importantly deixis and bridging. While standard coreference corpora do not contain IS annotation, some corpora annotated for bridging are emerging (Poesio, 2004; Korzen and Buch-Kromann, 2011) but they are (i) not annotated for comparative anaphora or other IS categories, (ii) often not tested for reliability or reach only low reliability, (iii) often very small (Poesio, 2004). To the best of our knowledge, we therefore present the first English corpus reliably annotated for a wide range of IS categories as well as full anaphoric information for three main anaphora types (coreference, bridging, comparative). Automatic recognition of IS. Vieira and Poesio (2000) describe heuristics for processing definite descriptions in news text. As their approach i"
P12-1084,P96-1039,0,0.0656944,"Missing"
P12-1084,D11-1099,0,0.485024,"lve the task by collectively classifying the information status of all mentions. Our approach strongly outperforms reimplementations of previous work. IS can be beneficial for a number of NLP tasks, though the results have been mixed. Nenkova et al. (2007) used IS as a feature for generating pitch accent in conversational speech. As IS is restricted to noun phrases, while pitch accent can be assigned to any word in an utterance, the experiments were not conclusive. For determining constituent order of German sentences, Cahill and Riester (2009) incorporate features modeling IS to good effect. Rahman and Ng (2011) showed that IS is a useful feature for coreference resolution. 1 Introduction Speakers present already known and yet to be established information according to principles referred to as information structure (Prince, 1981; Lambrecht, 1994; Kruijff-Korbayov´a and Steedman, 2003, inter alia). While information structure affects all kinds of constituents in a sentence, we here adopt the more restricted notion of information status which concerns only discourse entities realized as noun phrases, i.e. mentions1 . Information status (IS henceforth) describes the degree to which a discourse entity i"
P12-1084,riester-etal-2010-recursive,0,0.579882,"the challenge of modeling IS via collective classification, using several novel linguistically motivated features. We reimplement Nissim’s (2006) and Rahman and Ng’s (2011) approaches as baselines and show that our approach outperforms these by a large margin for both coarse- and finegrained IS classification. 2 Related Work IS annotation schemes and corpora. We enhance the approach in Nissim et al. (2004) in two major ways (see also Section 3.1). First, comparative anaphora are not specifically handled in Nissim et al. (2004) (and follow-on work such as Ritz et al. (2008) and Riester et al. (2010)), although some of them might be included in their respective bridging subcategories. Second, we apply the annotation scheme reliably to a new genre, namely news. This is a non-trivial extension: Ritz et al. (2008) applied a variation of the Nissim et al. (2004) scheme to a small set of 220 NPs in a German news/commentary corpus but found that reliability then dropped significantly to the range of κ = 0.55 to 0.60. They attributed this to the higher syntactic complexity and semantic vagueness in the commentary corpus. Riester et al. (2010) annotated a 2 All examples in this paper are from the"
P12-1084,ritz-etal-2008-annotation,0,0.0923975,"Missing"
P12-1084,D09-1151,0,0.0852425,"sification. In addition, they only distinguish the three main categories old, mediated and new. Finally, we work on news corpora which poses different problems from dialogue. Anaphoricity determination (Ng, 2009; Zhou and Kong, 2009) identifies many or most old mentions. However, no distinction between mediated and new mentions is made. Most approaches to bridging resolution (Meyer and Dale, 2002; Poesio et al., 2004) or comparative anaphora (Modjeska et al., 2003; Markert and Nissim, 2005) address only the selection of the antecedent for the bridging/comparative anaphor, not its recognition. Sasano and Kurohashi (2009) do also tackle bridging recognition, but they depend on languagespecific non-transferrable features for Japanese. 3 Corpus Creation 3.1 ing most generics as well as newly introduced, specific mentions such as Example 7. Annotation Scheme Our scheme follows Nissim et al. (2004) in distinguishing three major IS categories old, new and mediated. A mention is old if it is either coreferential with an already introduced entity or a generic or deictic pronoun. We follow the OntoNotes (Weischedel et al., 2011) definition of coreference to be able to integrate our annotations with it. This definition"
P12-1084,J11-4007,0,0.152808,"he hearer with regard to the speaker’s assumptions about the hearer’s knowledge and beliefs (Nissim et al., 2004). Old mentions are known to the hearer and have been referred 1 Since not all noun phrases are referential, we call noun phrases which carry information status mentions. Previous work on learning IS (Nissim, 2006; Rahman and Ng, 2011) is restricted in several ways. It deals with conversational dialogue, in particular with the corpus annotated by Nissim et al. (2004). However, many applications that can profit from IS concentrate on written texts, such as summarization. For example, Siddharthan et al. (2011) show that solving the IS subproblem of whether a person proper name is already known to the reader improves automatic summarization of news. Therefore, we here model IS in written text, creating a new dataset which adds an IS layer to the already existing comprehensive annotation in the OntoNotes corpus (Weischedel et al., 2011). We also report the first results on fine-grained IS classification by modelling further distinctions within the category of mediated mentions, such as comparative and bridging anaphora (see Examples 1 and 2, re795 Proceedings of the 50th Annual Meeting of the Associa"
P12-1084,D09-1018,0,0.194881,"2 coarse-grained classes, including location, organisation, person and several classes for numbers (such as date, money or percent). 4.2 Relations for Collective Classification Both Nissim (2006) and Rahman and Ng (2011) classify each mention individually in a standard supervised ML setting, not considering potential dependencies between the IS categories of different 9 We changed the value of “full prev mention” from “numeric’ to {yes, no, NA}. 799 mentions. However, collective or joint classification has made substantial impact in other NLP tasks, such as opinion mining (Pang and Lee, 2004; Somasundaran et al., 2009), text categorization (Yang et al., 2002; Taskar et al., 2002) and the related task of coreference resolution (Denis and Baldridge, 2007). We investigate two types of relations between mentions that might impact on IS classification. Syntactic parent-child relations. Two mediated subcategories account for accessibility via syntactic links to another old or mediated mention: mediated/synt is used when at least one child of a mention is mediated or old, with child relations restricted to pre- or postnominal possessives as well as PP children in our scheme (see Section 3.1). mediated/aggregate is"
P12-1084,J00-4003,0,0.491739,"rd coreference corpora do not contain IS annotation, some corpora annotated for bridging are emerging (Poesio, 2004; Korzen and Buch-Kromann, 2011) but they are (i) not annotated for comparative anaphora or other IS categories, (ii) often not tested for reliability or reach only low reliability, (iii) often very small (Poesio, 2004). To the best of our knowledge, we therefore present the first English corpus reliably annotated for a wide range of IS categories as well as full anaphoric information for three main anaphora types (coreference, bridging, comparative). Automatic recognition of IS. Vieira and Poesio (2000) describe heuristics for processing definite descriptions in news text. As their approach is restricted to definites, they only analyse a subset of the mentions we consider carrying IS. Siddharthan et al. (2011) also concentrate on a subproblem of IS only, namely the hearer-old/hearer-new distinctions for person proper names. Nissim (2006) and Rahman and Ng (2011) both present algorithms for IS detection on Nissim et al.’s (2004) Switchboard corpus. Both papers treat IS classification as a local classification problem whereas we look at dependencies between the IS status of different mentions,"
P12-1084,D09-1102,0,0.0171281,"blem of IS only, namely the hearer-old/hearer-new distinctions for person proper names. Nissim (2006) and Rahman and Ng (2011) both present algorithms for IS detection on Nissim et al.’s (2004) Switchboard corpus. Both papers treat IS classification as a local classification problem whereas we look at dependencies between the IS status of different mentions, leading to collective classification. In addition, they only distinguish the three main categories old, mediated and new. Finally, we work on news corpora which poses different problems from dialogue. Anaphoricity determination (Ng, 2009; Zhou and Kong, 2009) identifies many or most old mentions. However, no distinction between mediated and new mentions is made. Most approaches to bridging resolution (Meyer and Dale, 2002; Poesio et al., 2004) or comparative anaphora (Modjeska et al., 2003; Markert and Nissim, 2005) address only the selection of the antecedent for the bridging/comparative anaphor, not its recognition. Sasano and Kurohashi (2009) do also tackle bridging recognition, but they depend on languagespecific non-transferrable features for Japanese. 3 Corpus Creation 3.1 ing most generics as well as newly introduced, specific mentions such"
P13-1010,J95-2003,0,0.776281,"on three tasks: sentence ordering, summary coherence rating and readability assessment. The performance is comparable to entity grid based approaches though these rely on a computationally expensive training phase and face data sparsity problems. 1 Introduction Many NLP applications which process or generate texts rely on information about local coherence, i.e. information about which entities occur in which sentence and how the entities are distributed in the text. This led to the development of many theories and models accounting for local coherence. One popular model, the centering model (Grosz et al., 1995), uses a ranking of discourse entities realized in particular sentences and computes transitions between adjacent sentences to provide insight in the felicity of texts. Centering models local coherence rather generally and has been applied to the generation of referring expressions (Kibble and Power, 2004), to resolve pronouns (Brennan et al., 1987, inter alia), to score essays (Miltsakaki and Kukich, 2004), to arrange sentences in the correct order (Karamanis et al., 2009), and to many other tasks. Poesio et al. (2004) observe that it is not clear how to set parameters in the centering model"
P13-1010,W03-1004,0,0.0108165,"assessment task to test if the entity grid model can be used for style classification. They combined their model with Schwarm and Ostendorf’s (2005) readability features and use Support Vector Machines to classify documents in two categories. With the same intention, we evaluate the ability of our model to differentiate “easy to read” documents from difficult ones. 4.3.1 Experimental Settings The objective of the readability assessment task is to evaluate how difficult to read a document is. We perform this task on the data used by Barzilay and Lapata (2008), a corpus collected originally by Barzilay and Elhadad (2003) from the Encyclopedia Britannica and its version for children, the Britannica Elementary. Both versions contain 107 articles. In Encyclopedia Britannica, documents are composed by an average of 83.1 sentences while they contain 36.6 sentences in Britannica Elementary. Although these texts are not explicitly annotated with grade levels, they represent two broad readability categories. In order to estimate the complexity of a document, our model computes the local coherence score for each article in the two categories. The article associated with the higher score is considered to be the more re"
P13-1010,P05-1018,0,0.0739386,": Entity Grid representation of summary M local entity transitions of the entities present or absent in the sentence. To make this representation accessible to machine learning algorithms, Barzilay and Lapata (2008) compute for each document the probability of each transition and generate feature vectors representing the sentences. Coherence assessment is then formulated as a ranking learning problem where the ranking function is learned with SVMlight (Joachims, 2002). The entity grid approach has already been applied to many applications relying on local coherence estimation: summary rating (Barzilay and Lapata, 2005), essay scoring (Burstein et al., 2010) or story generation (McIntyre and Lapata, 2010). It was also used successfully in combination with other systems or features. Soricut and Marcu (2006) show that the entity grid model is a critical component in their sentence ordering model for discourse generation. Barzilay and Lapata (2008) combine the entity grid with readability-related features to discriminate documents between easy- and difficult-to-read categories. Lin et al. (2011) use discourse relations to transform the entity grid representation into a discourse role matrix that is used to gene"
P13-1010,J09-1003,0,0.0279437,"is led to the development of many theories and models accounting for local coherence. One popular model, the centering model (Grosz et al., 1995), uses a ranking of discourse entities realized in particular sentences and computes transitions between adjacent sentences to provide insight in the felicity of texts. Centering models local coherence rather generally and has been applied to the generation of referring expressions (Kibble and Power, 2004), to resolve pronouns (Brennan et al., 1987, inter alia), to score essays (Miltsakaki and Kukich, 2004), to arrange sentences in the correct order (Karamanis et al., 2009), and to many other tasks. Poesio et al. (2004) observe that it is not clear how to set parameters in the centering model so that optimal performance in different tasks and languages can be achieved. Barzilay and Lapata (2008) criticize research on centering to be too dependent on manually annotated input. This led them to propose a local coherence model relying on a more parsimonious representation, the entity grid model. In order to overcome these problems we propose to represent entities in a graph and then model local coherence by applying centrality measures to the nodes in the graph (Sec"
P13-1010,J08-1001,0,0.091395,"grid is a two dimensional array where the rows represent sentences and the columns discourse entities. From this grid Barzilay and Lapata (2008) derive probabilities of transitions between adjacent sentences which are used as features for machine learning algorithms. They evaluate this approach successfully on sentence ordering, summary coherence rating, and readability assessment. However, their approach has some disadvantages which they point out themselves: data sparsity, domain dependence and computational complexity, especially in terms of feature space issues while building their model (Barzilay and Lapata (2008, p.8, p.10, p.30), Elsner and Charniak (2011, p.126, p.127)). We propose a computationally efficient graph-based approach for local coherence modeling. We evaluate our system on three tasks: sentence ordering, summary coherence rating and readability assessment. The performance is comparable to entity grid based approaches though these rely on a computationally expensive training phase and face data sparsity problems. 1 Introduction Many NLP applications which process or generate texts rely on information about local coherence, i.e. information about which entities occur in which sentence and"
P13-1010,J04-4001,0,0.0148895,"Missing"
P13-1010,P87-1022,0,0.563822,"Missing"
P13-1010,P11-1100,0,0.271816,"cal coherence by applying centrality measures to the nodes in the graph (Section 3). We claim that a graph is a more powerful representation for local coherence than the entity grid (Barzilay and Lapata, 2008) which is restricted to transitions between adjacent sentences. The graph can easily span the entire text without leading to computational complexity and data sparsity problems. Similar to the application of graph-based methods in other areas of NLP (e.g. work on word sense disambiguation by Navigli and Lapata (2010); for an overview over graph-based methods in NLP see Mihalcea and Radev (2011)) we model local coherence by relying only on centrality measures applied to the nodes in the graph. We apply our graph-based model to the three tasks handled by Barzilay and Lapata (2008) to show that it provides the same flexibility over disparate tasks as the entity grid model: sentence ordering (Section 4.1), summary coherence ranking (Section 4.2), and readability assessment (Section 4.3). In the 93 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 93–103, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics GOVERN"
P13-1010,N10-1099,0,0.462758,"me these problems we propose to represent entities in a graph and then model local coherence by applying centrality measures to the nodes in the graph (Section 3). We claim that a graph is a more powerful representation for local coherence than the entity grid (Barzilay and Lapata, 2008) which is restricted to transitions between adjacent sentences. The graph can easily span the entire text without leading to computational complexity and data sparsity problems. Similar to the application of graph-based methods in other areas of NLP (e.g. work on word sense disambiguation by Navigli and Lapata (2010); for an overview over graph-based methods in NLP see Mihalcea and Radev (2011)) we model local coherence by relying only on centrality measures applied to the nodes in the graph. We apply our graph-based model to the three tasks handled by Barzilay and Lapata (2008) to show that it provides the same flexibility over disparate tasks as the entity grid model: sentence ordering (Section 4.1), summary coherence ranking (Section 4.2), and readability assessment (Section 4.3). In the 93 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 93–103, c Sofia, B"
P13-1010,W12-4511,1,0.375689,"Missing"
P13-1010,de-marneffe-etal-2006-generating,0,0.0348952,"Missing"
P13-1010,P10-1158,0,0.0413094,"ent or absent in the sentence. To make this representation accessible to machine learning algorithms, Barzilay and Lapata (2008) compute for each document the probability of each transition and generate feature vectors representing the sentences. Coherence assessment is then formulated as a ranking learning problem where the ranking function is learned with SVMlight (Joachims, 2002). The entity grid approach has already been applied to many applications relying on local coherence estimation: summary rating (Barzilay and Lapata, 2005), essay scoring (Burstein et al., 2010) or story generation (McIntyre and Lapata, 2010). It was also used successfully in combination with other systems or features. Soricut and Marcu (2006) show that the entity grid model is a critical component in their sentence ordering model for discourse generation. Barzilay and Lapata (2008) combine the entity grid with readability-related features to discriminate documents between easy- and difficult-to-read categories. Lin et al. (2011) use discourse relations to transform the entity grid representation into a discourse role matrix that is used to generate feature vectors for machine learning algorithms similarly to Barzilay and Lapata ("
P13-1010,P11-2022,0,0.525415,"s represent sentences and the columns discourse entities. From this grid Barzilay and Lapata (2008) derive probabilities of transitions between adjacent sentences which are used as features for machine learning algorithms. They evaluate this approach successfully on sentence ordering, summary coherence rating, and readability assessment. However, their approach has some disadvantages which they point out themselves: data sparsity, domain dependence and computational complexity, especially in terms of feature space issues while building their model (Barzilay and Lapata (2008, p.8, p.10, p.30), Elsner and Charniak (2011, p.126, p.127)). We propose a computationally efficient graph-based approach for local coherence modeling. We evaluate our system on three tasks: sentence ordering, summary coherence rating and readability assessment. The performance is comparable to entity grid based approaches though these rely on a computationally expensive training phase and face data sparsity problems. 1 Introduction Many NLP applications which process or generate texts rely on information about local coherence, i.e. information about which entities occur in which sentence and how the entities are distributed in the text"
P13-1010,N07-1055,0,0.060746,"transform the entity grid representation into a discourse role matrix that is used to generate feature vectors for machine learning algorithms similarly to Barzilay and Lapata (2008). Several studies propose to extend the entity grid model using different strategies for entity selection. Filippova and Strube (2007) aim to improve the entity grid model performance by grouping entities by means of semantic relatedness. In their studies, Elsner and Charniak extend the number and type of entities selected and consider that each entity has to be dealt with accordingly with its information status (Elsner et al., 2007) or its namedentity category (Elsner and Charniak, 2011). Finally, they include a heuristic coreference resolution component by linking mentions which share a The Entity Grid Model Barzilay and Lapata (2005; 2008) introduced the entity grid, a method for local coherence modeling that captures the distribution of discourse entities across sentences in a text. An entity grid is a two dimensional array, where rows correspond to sentences and columns to discourse entities. For each discourse entity ej and each sentence si in the text, the corresponding grid cell cij contains information about the"
P13-1010,J04-3003,0,0.0217308,"Missing"
P13-1010,W07-2321,1,0.854165,"t the entity grid model is a critical component in their sentence ordering model for discourse generation. Barzilay and Lapata (2008) combine the entity grid with readability-related features to discriminate documents between easy- and difficult-to-read categories. Lin et al. (2011) use discourse relations to transform the entity grid representation into a discourse role matrix that is used to generate feature vectors for machine learning algorithms similarly to Barzilay and Lapata (2008). Several studies propose to extend the entity grid model using different strategies for entity selection. Filippova and Strube (2007) aim to improve the entity grid model performance by grouping entities by means of semantic relatedness. In their studies, Elsner and Charniak extend the number and type of entities selected and consider that each entity has to be dealt with accordingly with its information status (Elsner et al., 2007) or its namedentity category (Elsner and Charniak, 2011). Finally, they include a heuristic coreference resolution component by linking mentions which share a The Entity Grid Model Barzilay and Lapata (2005; 2008) introduced the entity grid, a method for local coherence modeling that captures the"
P13-1010,W12-4501,0,0.0161443,"the insertion task, proposed by Elsner and Charniak (2011), we evaluate the ability of our system to retrieve the original position of a sentence previously removed from a document. For this, each sentence is removed in turn and a local coherence score is computed for every possible reinsertion position. The system output is considered as correct if the document associated with the highest local coherence score is the one in which the sentence is reinserted in the correct position. These two tasks were performed on documents extracted from the English test part of the CoNLL 2012 shared task (Pradhan et al., 2012). This corpus, composed by documents of multiple news sources – spoken or written – was preferred to the ACCIDENTS and EARTHQUAKES corpora used by Barzilay and Lapata (2008) for two reasons. First, as mentioned by Elsner and Charniak (2008), these corpora use a very constrained style and are not typical of normal informative documents3 . Second, we want to evaluate the influence of automatically performed coreference resolution in a controlled fashion. The coreference resolution system used performs well on the CoNLL 2012 data. In this dataset, documents composed by the concatenation of differ"
P13-1010,P05-1065,0,0.0242592,"Missing"
P13-1010,P06-2103,0,0.312361,"zilay and Lapata (2008) compute for each document the probability of each transition and generate feature vectors representing the sentences. Coherence assessment is then formulated as a ranking learning problem where the ranking function is learned with SVMlight (Joachims, 2002). The entity grid approach has already been applied to many applications relying on local coherence estimation: summary rating (Barzilay and Lapata, 2005), essay scoring (Burstein et al., 2010) or story generation (McIntyre and Lapata, 2010). It was also used successfully in combination with other systems or features. Soricut and Marcu (2006) show that the entity grid model is a critical component in their sentence ordering model for discourse generation. Barzilay and Lapata (2008) combine the entity grid with readability-related features to discriminate documents between easy- and difficult-to-read categories. Lin et al. (2011) use discourse relations to transform the entity grid representation into a discourse role matrix that is used to generate feature vectors for machine learning algorithms similarly to Barzilay and Lapata (2008). Several studies propose to extend the entity grid model using different strategies for entity se"
P13-1010,P08-2011,0,\N,Missing
P14-2006,W12-4501,1,0.648168,"ity in this subfield of NLP can be gauged by: (i) the continual addition of corpora manually annotated for coreference—The OntoNotes corpus (Pradhan et al., 2007; Weischedel et al., 2011) in the general domain, as well as the i2b2 (Uzuner et al., 2012) and THYME (Styler et al., 2014) corpora in the clinical domain would be a few examples of such emerging corpora; and (ii) ongoing proposals for refining the existing metrics to make them more informative (Holen, 2013; Chen and Ng, 2013). The CoNLL-2011/2012 shared tasks on coreference resolution using the OntoNotes corpus (Pradhan et al., 2011; Pradhan et al., 2012) were an attempt to standardize the evaluation settings by providing a benchmark annotated corpus, scorer, and state-of-the-art system results that would allow future systems to compare against them. Following the timely emphasis on end-to-end evaluation, the official track used predicted mentions and measured performance using five coreference measures: MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998), CEAFe (Luo, 2005), CEAFm (Luo, 2005), and BLANC (Recasens and Hovy, 2011). The arithmetic mean of the first three was the task’s final score. An unfortunate setback to these evaluations"
P14-2006,W10-4305,1,0.648818,"ainst them. Following the timely emphasis on end-to-end evaluation, the official track used predicted mentions and measured performance using five coreference measures: MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998), CEAFe (Luo, 2005), CEAFm (Luo, 2005), and BLANC (Recasens and Hovy, 2011). The arithmetic mean of the first three was the task’s final score. An unfortunate setback to these evaluations had its root in three issues: (i) the multiple variations of two of the scoring metrics—B3 and CEAF— used by the community to handle predicted mentions; (ii) a buggy implementation of the Cai and Strube (2010) proposal that tried to reconcile these variations; and (iii) the erroneous computation of The definitions of two coreference scoring metrics—B3 and CEAF—are underspecified with respect to predicted, as opposed to key (or gold) mentions. Several variations have been proposed that manipulate either, or both, the key and predicted mentions in order to get a one-to-one mapping. On the other hand, the metric BLANC was, until recently, limited to scoring partitions of key mentions. In this paper, we (i) argue that mention manipulation for scoring predicted mentions is unnecessary, and potentially h"
P14-2006,D09-1101,1,0.646014,"e mapping between the key and predicted mentions, assuming that the original measures cannot be applied to predicted mentions. Below we first provide an overview of these variations and then discuss the unnecessity of this assumption. Coining the term twinless mentions for those mentions that are either spurious or missing from the predicted mention set, Stoyanov et al. (2009) proposed two variations to B3 — B3all and B30 —to handle them. In the first variation, all predicted twinless mentions are retained, whereas the latter discards them and penalizes recall for twinless predicted mentions. Rahman and Ng (2009) proposed another variation by removing “all and only those twinless system mentions that are singletons before applying B3 and CEAF.” Following upon this line of research, Cai and Strube (2010) proposed a unified solution for both B3 and CEAF m , leaving the question of handling CEAF e as future work because “it produces unintuitive results.” The essence of their solution involves manipulating twinless key and predicted mentions by adding them either from the predicted partition to the key partition or vice versa, depending on whether one is computing precision or recall. The Cai and Strube ("
P14-2006,I13-1193,1,0.853507,"very hard to grasp the state of the art for the task of coreference. This has been expounded in Stoyanov et al. (2009). The activity in this subfield of NLP can be gauged by: (i) the continual addition of corpora manually annotated for coreference—The OntoNotes corpus (Pradhan et al., 2007; Weischedel et al., 2011) in the general domain, as well as the i2b2 (Uzuner et al., 2012) and THYME (Styler et al., 2014) corpora in the clinical domain would be a few examples of such emerging corpora; and (ii) ongoing proposals for refining the existing metrics to make them more informative (Holen, 2013; Chen and Ng, 2013). The CoNLL-2011/2012 shared tasks on coreference resolution using the OntoNotes corpus (Pradhan et al., 2011; Pradhan et al., 2012) were an attempt to standardize the evaluation settings by providing a benchmark annotated corpus, scorer, and state-of-the-art system results that would allow future systems to compare against them. Following the timely emphasis on end-to-end evaluation, the official track used predicted mentions and measured performance using five coreference measures: MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998), CEAFe (Luo, 2005), CEAFm (Luo, 2005), and BLANC (Recas"
P14-2006,P11-1082,1,0.341706,"he OntoNotes corpus, and by the i2b2 2011 shared task on coreference resolution using an assortment of clinical notes corpora (Uzuner et al., 2012).1 It was later identified by Recasens et al. (2013) that there was a bug in the implementation of this variation in the scorer used for the CoNLL-2011/2012 tasks. We have not tested the correctness of this variation in the scoring package used for the i2b2 shared task. However, it turns out that the CEAF metric (Luo, 2005) was always intended to work seamlessly on predicted mentions, and so has been the case with the B3 metric.2 In a latter paper, Rahman and Ng (2011) correctly state that “CEAF can compare partitions with twinless mentions without any modification.” We will look at this further in Section 4.3. We argue that manipulations of key and response mentions/entities, as is done in the existing B3 variations, not only confound the evaluation process, but are also subject to abuse and can seriously jeopardize the fidelity of the evaluVariations of Scoring Measures Two commonly used coreference scoring metrics —B3 and CEAF—are underspecified in their application for scoring predicted, as opposed to key mentions. The examples in the papers describing"
P14-2006,W09-2411,1,0.899331,"Missing"
P14-2006,doddington-etal-2004-automatic,0,0.0166341,"ith this implementation. This will help the community accurately measure and compare new end-to-end coreference resolution algorithms. 1 Introduction Coreference resolution is a key task in natural language processing (Jurafsky and Martin, 2008) aiming to detect the referential expressions (mentions) in a text that point to the same entity. Roughly over the past two decades, research in coreference (for the English language) had been plagued by individually crafted evaluations based on two central corpora—MUC (Hirschman and Chinchor, 1997; Chinchor and Sundheim, 2003; Chinchor, 2001) and ACE (Doddington et al., 2004). Experimental parameters ranged from using perfect (gold, or key) mentions as input for 30 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 30–35, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics the BLANC metric for partitions of predicted mentions. Different interpretations as to how to compute B3 and CEAF scores for coreference systems when predicted mentions do not perfectly align with key mentions—which is usually the case— led to variations of these metrics that manipulate the gold st"
P14-2006,N13-1071,1,0.782346,"Missing"
P14-2006,P09-1074,0,0.498213,"arnegie Mellon University, Pittsburgh, PA 5 HLTRI, University of Texas at Dallas, Richardson, TX, 6 HITS, Heidelberg, Germany 3 sameer.pradhan@childrens.harvard.edu, {xql,recasens}@google.com, hovy@cmu.edu, vince@hlt.utdallas.edu, michael.strube@h-its.org Abstract purely testing the quality of the entity linking algorithm, to an end-to-end evaluation where predicted mentions are used. Given the range of evaluation parameters and disparity between the annotation standards for the two corpora, it was very hard to grasp the state of the art for the task of coreference. This has been expounded in Stoyanov et al. (2009). The activity in this subfield of NLP can be gauged by: (i) the continual addition of corpora manually annotated for coreference—The OntoNotes corpus (Pradhan et al., 2007; Weischedel et al., 2011) in the general domain, as well as the i2b2 (Uzuner et al., 2012) and THYME (Styler et al., 2014) corpora in the clinical domain would be a few examples of such emerging corpora; and (ii) ongoing proposals for refining the existing metrics to make them more informative (Holen, 2013; Chen and Ng, 2013). The CoNLL-2011/2012 shared tasks on coreference resolution using the OntoNotes corpus (Pradhan et"
P14-2006,N13-2001,0,0.0493029,"pora, it was very hard to grasp the state of the art for the task of coreference. This has been expounded in Stoyanov et al. (2009). The activity in this subfield of NLP can be gauged by: (i) the continual addition of corpora manually annotated for coreference—The OntoNotes corpus (Pradhan et al., 2007; Weischedel et al., 2011) in the general domain, as well as the i2b2 (Uzuner et al., 2012) and THYME (Styler et al., 2014) corpora in the clinical domain would be a few examples of such emerging corpora; and (ii) ongoing proposals for refining the existing metrics to make them more informative (Holen, 2013; Chen and Ng, 2013). The CoNLL-2011/2012 shared tasks on coreference resolution using the OntoNotes corpus (Pradhan et al., 2011; Pradhan et al., 2012) were an attempt to standardize the evaluation settings by providing a benchmark annotated corpus, scorer, and state-of-the-art system results that would allow future systems to compare against them. Following the timely emphasis on end-to-end evaluation, the official track used predicted mentions and measured performance using five coreference measures: MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998), CEAFe (Luo, 2005), CEAFm (Luo, 200"
P14-2006,Q14-1012,1,0.042996,"Missing"
P14-2006,P14-2005,1,0.929886,"ne-to-one mention mapping (Stoyanov et al., 2009; Cai and Strube, 2010). Some of these variations arguably produce rather unintuitive results, while others are not faithful to the original measures. In this paper, we address the issues in scoring coreference partitions of predicted mentions. Specifically, we justify our decision to go back to the original scoring algorithms by arguing that manipulation of key or predicted mentions is unnecessary and could in fact produce unintuitive results. We demonstrate the use of our recent extension of BLANC that can seamlessly handle predicted mentions (Luo et al., 2014). We make available an open-source, thoroughly-tested reference implementation of the main coreference evaluation measures that do not involve mention manipulation and is faithful to the original intentions of the proposers of these metrics. We republish the CoNLL-2011/2012 results based on this scorer, so that future systems can use it for evaluation and have the CoNLL results available for comparison. The rest of the paper is organized as follows. Section 2 provides an overview of the variations of the existing measures. We present our newly updated coreference scoring package in Section 3 t"
P14-2006,M95-1005,0,0.887578,"proposals for refining the existing metrics to make them more informative (Holen, 2013; Chen and Ng, 2013). The CoNLL-2011/2012 shared tasks on coreference resolution using the OntoNotes corpus (Pradhan et al., 2011; Pradhan et al., 2012) were an attempt to standardize the evaluation settings by providing a benchmark annotated corpus, scorer, and state-of-the-art system results that would allow future systems to compare against them. Following the timely emphasis on end-to-end evaluation, the official track used predicted mentions and measured performance using five coreference measures: MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998), CEAFe (Luo, 2005), CEAFm (Luo, 2005), and BLANC (Recasens and Hovy, 2011). The arithmetic mean of the first three was the task’s final score. An unfortunate setback to these evaluations had its root in three issues: (i) the multiple variations of two of the scoring metrics—B3 and CEAF— used by the community to handle predicted mentions; (ii) a buggy implementation of the Cai and Strube (2010) proposal that tried to reconcile these variations; and (iii) the erroneous computation of The definitions of two coreference scoring metrics—B3 and CEAF—are underspecified"
P14-2006,H05-1004,1,0.706057,"re informative (Holen, 2013; Chen and Ng, 2013). The CoNLL-2011/2012 shared tasks on coreference resolution using the OntoNotes corpus (Pradhan et al., 2011; Pradhan et al., 2012) were an attempt to standardize the evaluation settings by providing a benchmark annotated corpus, scorer, and state-of-the-art system results that would allow future systems to compare against them. Following the timely emphasis on end-to-end evaluation, the official track used predicted mentions and measured performance using five coreference measures: MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998), CEAFe (Luo, 2005), CEAFm (Luo, 2005), and BLANC (Recasens and Hovy, 2011). The arithmetic mean of the first three was the task’s final score. An unfortunate setback to these evaluations had its root in three issues: (i) the multiple variations of two of the scoring metrics—B3 and CEAF— used by the community to handle predicted mentions; (ii) a buggy implementation of the Cai and Strube (2010) proposal that tried to reconcile these variations; and (iii) the erroneous computation of The definitions of two coreference scoring metrics—B3 and CEAF—are underspecified with respect to predicted, as opposed to key (or"
P14-2006,W11-1901,1,0.482275,"al. (2009). The activity in this subfield of NLP can be gauged by: (i) the continual addition of corpora manually annotated for coreference—The OntoNotes corpus (Pradhan et al., 2007; Weischedel et al., 2011) in the general domain, as well as the i2b2 (Uzuner et al., 2012) and THYME (Styler et al., 2014) corpora in the clinical domain would be a few examples of such emerging corpora; and (ii) ongoing proposals for refining the existing metrics to make them more informative (Holen, 2013; Chen and Ng, 2013). The CoNLL-2011/2012 shared tasks on coreference resolution using the OntoNotes corpus (Pradhan et al., 2011; Pradhan et al., 2012) were an attempt to standardize the evaluation settings by providing a benchmark annotated corpus, scorer, and state-of-the-art system results that would allow future systems to compare against them. Following the timely emphasis on end-to-end evaluation, the official track used predicted mentions and measured performance using five coreference measures: MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998), CEAFe (Luo, 2005), CEAFm (Luo, 2005), and BLANC (Recasens and Hovy, 2011). The arithmetic mean of the first three was the task’s final score. An unfortunate setbac"
P14-2006,J00-4006,0,\N,Missing
P14-2006,D08-1067,1,\N,Missing
P14-2006,S10-1001,1,\N,Missing
P15-4007,H05-1004,0,0.260564,"rube Heidelberg Institute for Theoretical Studies gGmbH Schloss-Wolfsbrunnenweg 35 69118 Heidelberg, Germany Benjamin Heinzerling Research Training Group AIPHES Heidelberg Institute for Theoretical Studies gGmbH Schloss-Wolfsbrunnenweg 35 69118 Heidelberg, Germany michael.strube@h-its.org benjamin.heinzerling@h-its.org Abstract coreference resolution (mentions linked by coreference relations to the same entity), these metrics include ones originally proposed for evaluation of coreference resolutions systems, such as the MUC score (Vilain et al., 1995), B 3 (Bagga and Baldwin, 1998), and CEAF (Luo, 2005) and variants thereof (Cai and Strube, 2010). While such metrics, which express system performance in numeric terms of precision, recall, and F1 scores, are well-suited for comparing systems, they are of limited use to EL system developers trying to identify problem areas and components whose improvement will likely result in the largest performance increase. To address this problem, we present the Visual Entity Explorer (VEX), an interactive tool for visually exploring the results produced by an EL system. To our knowledge, there exist no other dedicated tools for visualizing the output of EL"
P15-4007,P14-5010,0,0.00448987,"categorized as a FP entity, since the system erroneously linked them to the KB entry for the city of Allen, Texas, resulting in a system entity that does not intersect with any gold entity. The system commits a similar mistake for the mention “Paul”. 3.5 Insights This analysis of only a few examples has already revealed several categories of errors, either committed by the EL system or resulting from gold annotation mistakes: • mention detection errors due to non-standard letter case, which suggest incorporating truecasing (Lita et al., 2003) and/or a caseless named entity recognition model (Manning et al., 2014) into the mention detection process could improve performance; • mention detection errors due to off-by-one errors involving punctuation, which suggest the need for clear and consistently applied annotation guidelines, enabling developers to add hard-coded, task-specific postprocessing rules for dealing with such cases; • mention detection errors due to missing gold standard annotations, which suggest performing a simple string match against already annotated mentions to find cases of unannotated mentions could significantly improve the gold standard at little cost; Implementation 4.1 Design D"
P15-4007,D14-1221,1,0.901689,"Missing"
P15-4007,W10-4305,1,0.845762,"oretical Studies gGmbH Schloss-Wolfsbrunnenweg 35 69118 Heidelberg, Germany Benjamin Heinzerling Research Training Group AIPHES Heidelberg Institute for Theoretical Studies gGmbH Schloss-Wolfsbrunnenweg 35 69118 Heidelberg, Germany michael.strube@h-its.org benjamin.heinzerling@h-its.org Abstract coreference resolution (mentions linked by coreference relations to the same entity), these metrics include ones originally proposed for evaluation of coreference resolutions systems, such as the MUC score (Vilain et al., 1995), B 3 (Bagga and Baldwin, 1998), and CEAF (Luo, 2005) and variants thereof (Cai and Strube, 2010). While such metrics, which express system performance in numeric terms of precision, recall, and F1 scores, are well-suited for comparing systems, they are of limited use to EL system developers trying to identify problem areas and components whose improvement will likely result in the largest performance increase. To address this problem, we present the Visual Entity Explorer (VEX), an interactive tool for visually exploring the results produced by an EL system. To our knowledge, there exist no other dedicated tools for visualizing the output of EL systems or similar representations. VEX is"
P15-4007,M95-1005,0,0.502413,"Missing"
P15-4007,P14-2076,0,0.0775836,"Missing"
P15-4007,P03-1020,0,\N,Missing
P15-4011,P02-1014,0,0.771198,"rick.claus|michael.strube)@h-its.org Abstract 2 We present cort, a modular toolkit for devising, implementing, comparing and analyzing approaches to coreference resolution. The toolkit allows for a unified representation of popular coreference resolution approaches by making explicit the structures they operate on. Several of the implemented approaches achieve state-ofthe-art performance. 1 A Framework for Coreference Resolution In this section we briefly describe a structured prediction framework for coreference resolution. 2.1 Motivation The popular mention pair approach (Soon et al., 2001; Ng and Cardie, 2002) operates on a list of mention pairs. Each mention pair is considered individually for learning and prediction. In contrast, antecedent tree models (Yu and Joachims, 2009; Fernandes et al., 2014; Bj¨orkelund and Kuhn, 2014) operate on a tree which encodes all anaphorantecedent decisions in a document. Conceptually, both approaches have in common that the structures they employ are not annotated in the data (in coreference resolution, the annotation consists of a mapping of mentions to entity identifiers). Hence, we can view both approaches as instantiations of a generic structured prediction a"
P15-4011,W12-4513,0,0.230222,"g Our aim is to learn a prediction function f that, given an input document x ∈ X , predicts a pair (h, z) ∈ H ×Z. h is the (unobserved) latent structure encoding the coreference relations between mentions in x. z is the mapping of mentions to entity identifiers (which is observed in the training data). Usually, z is obtained from h by taking the transitive closure over coreference decisions encoded in h. H and Z are the spaces containing all such structures and mappings. 2.3 Representation For a document x ∈ X , we write Mx = {m1 , . . . , mn } for the mentions in x. Following previous work (Chang et al., 2012; Fernandes et al., 2014), we make use of a dummy mention which we denote as m0 . If m0 is predicted as the 1 http://smartschat.de/software http://pypi.python.org/pypi. Install it via pip install cort. 2 61 Proceedings of ACL-IJCNLP 2015 System Demonstrations, pages 61–66, c Beijing, China, July 26-31, 2015. 2015 ACL and AFNLP antecedent of a mention mi , we consider mi nonanaphoric. We define Mx0 = {m0 } ∪ Mx . Inspired by previous work (Bengtson and Roth, 2008; Fernandes et al., 2014; Martschat and Strube, 2014), we adopt a graph-based representation of the latent structures h ∈ H. In partic"
P15-4011,W12-4501,0,0.6666,"le binary classification models on mention pairs (Soon et al., 2001) to complex structured prediction approaches (Durrett and Klein, 2013; Fernandes et al., 2014). In this paper, we present a toolkit that implements a framework that unifies these approaches: in the framework, we obtain a unified representation of many coreference approaches by making explicit the latent structures they operate on. Our toolkit provides an interface for defining structures for coreference resolution, which we use to implement several popular approaches. An evaluation of the approaches on CoNLL shared task data (Pradhan et al., 2012) shows that they obtain state-of-the-art results. The toolkit also can perform end-to-end coreference resolution. We implemented this functionality on top of the coreference resolution error analysis toolkit cort (Martschat et al., 2015). Hence, this toolkit now provides functionality for devising, implementing, comparing and analyzing approaches to coreference resolution. cort is released as open source1 and is available from the Python Package Index2 . 2.2 Setting Our aim is to learn a prediction function f that, given an input document x ∈ X , predicts a pair (h, z) ∈ H ×Z. h is the (unobse"
P15-4011,W02-1001,0,0.150163,"ntion mi , we consider mi nonanaphoric. We define Mx0 = {m0 } ∪ Mx . Inspired by previous work (Bengtson and Roth, 2008; Fernandes et al., 2014; Martschat and Strube, 2014), we adopt a graph-based representation of the latent structures h ∈ H. In particular, we express structures by labeled directed graphs with vertex set Mx0 . for each substructure (instead of one maximization problem for the whole structure). To learn the parameter vector θ ∈ Rd from training data, we employ a latent structured perceptron (Sun et al., 2009) with cost-augmented inference (Crammer et al., 2006) and averaging (Collins, 2002). m1 m3 We now describe our implementation of the framework presented in the previous section. m4 3.1 m0 m2 3 Implementation By expressing approaches in the framework, researchers can quickly devise, implement, compare and analyze approaches for coreference resolution. To facilitate development, it should be as easy as possible to define a coreference resolution approach. We first describe the general architecture of our toolkit before giving a detailed description of how to implement specific coreference resolution approaches. m5 Figure 1: Latent structure underlying the mention ranking and t"
P15-4011,P14-2006,1,0.908175,"antecedents. . System Entities 9. Thousands of East Germans fled to Czechoslovakia after the East Berlin ORTEGA class RankingPerceptron( perceptrons.Perceptron): def argmax(self, substructure, arc_information): best_arc, best_arc_score,  best_cons_arc, best_cons_arc_score,  consistent = self.find_best_arcs( substructure, arc_information) government lifted travel restrictions . Contras 3.2.3 analysis elections president 10. The ban on crossborder movement was imposed last month after a To support system development, this module implements the error analysis framework of Martschat and Strube (2014). Users can extract, analyze and visualize recall and precision errors of the systems they are working on. Figure 2 shows a screenshot of the visualization. A more detailed description can be found in Martschat et al. (2015). massive exodus of emigres to West Germany . rebels Bush country 11. Also , a Communist official for the first time said the future of the Berlin balloting return ([best_arc], [], [best_arc_score], [best_cons_arc], [], [best_cons_arc_score], consistent) 3.2.4 coreference This module provides features for coreference resolution and implements the machine learning framework"
P15-4011,D08-1069,0,0.141246,"approach. We first describe the general architecture of our toolkit before giving a detailed description of how to implement specific coreference resolution approaches. m5 Figure 1: Latent structure underlying the mention ranking and the antecedent tree approach. The black nodes and arcs represent one substructure for the mention ranking approach. Figure 1 shows a structure underlying the mention ranking and the antecedent tree approach. An arc between two mentions signals coreference. For antecedent trees (Fernandes et al., 2014), the whole structure is considered, while for mention ranking (Denis and Baldridge, 2008; Chang et al., 2012) only the antecedent decision for one anaphor is examined. This can be expressed via an appropriate segmentation into subgraphs which we refer to as substructures. One such substructure encoding the antecedent decision for m3 is colored black in the figure. Via arc labels we can express additional information. For example, mention pair models (Soon et al., 2001) distinguish between positive and negative instances. This can be modeled by labeling arcs with appropriate labels, such as + and −. 2.4 Aims 3.2 Architecture The toolkit is implemented in Python. It can process raw"
P15-4011,J01-4004,0,0.861126,"stian.martschat|patrick.claus|michael.strube)@h-its.org Abstract 2 We present cort, a modular toolkit for devising, implementing, comparing and analyzing approaches to coreference resolution. The toolkit allows for a unified representation of popular coreference resolution approaches by making explicit the structures they operate on. Several of the implemented approaches achieve state-ofthe-art performance. 1 A Framework for Coreference Resolution In this section we briefly describe a structured prediction framework for coreference resolution. 2.1 Motivation The popular mention pair approach (Soon et al., 2001; Ng and Cardie, 2002) operates on a list of mention pairs. Each mention pair is considered individually for learning and prediction. In contrast, antecedent tree models (Yu and Joachims, 2009; Fernandes et al., 2014; Bj¨orkelund and Kuhn, 2014) operate on a tree which encodes all anaphorantecedent decisions in a document. Conceptually, both approaches have in common that the structures they employ are not annotated in the data (in coreference resolution, the annotation consists of a mapping of mentions to entity identifiers). Hence, we can view both approaches as instantiations of a generic s"
P15-4011,D13-1203,0,0.623591,"ches have in common that the structures they employ are not annotated in the data (in coreference resolution, the annotation consists of a mapping of mentions to entity identifiers). Hence, we can view both approaches as instantiations of a generic structured prediction approach with latent variables. Introduction Coreference resolution is the task of determining which mentions in a text refer to the same entity. Machine learning approaches to coreference resolution range from simple binary classification models on mention pairs (Soon et al., 2001) to complex structured prediction approaches (Durrett and Klein, 2013; Fernandes et al., 2014). In this paper, we present a toolkit that implements a framework that unifies these approaches: in the framework, we obtain a unified representation of many coreference approaches by making explicit the latent structures they operate on. Our toolkit provides an interface for defining structures for coreference resolution, which we use to implement several popular approaches. An evaluation of the approaches on CoNLL shared task data (Pradhan et al., 2012) shows that they obtain state-of-the-art results. The toolkit also can perform end-to-end coreference resolution. We"
P15-4011,J13-4004,0,0.0169051,"text and CoNLL shared task data. It achieves state-ofthe-art performance on the shared task data. The framework and toolkit presented in this paper help researchers to devise, analyze and compare representations for coreference resolution. Acknowledgements We thank Benjamin Heinzerling for helpful comments on drafts of this paper. This work has been funded by the Klaus Tschira Foundation, Germany. The first author has been supported by a HITS Ph.D. scholarship. Related Work Many researchers on coreference resolution release an implementation of the coreference model described in their paper (Lee et al., 2013; Durrett and Klein, 2013; Bj¨orkelund and Kuhn, 2014, inter alia). However, these implementations implement only one approach following one paradigm (such as mention ranking or antecedent trees). Similarly to cort, research toolkits such as BART (Versley et al., 2008) or Reconcile (Stoyanov et al., 2009) provide a framework to implement and compare coreference resolution approaches. In contrast to these toolkits, we make the latent structure underlying coreference approaches explicit, which facilitates development of new approaches and renders the development more transparent. Furthermore, we"
P15-4011,P08-4003,0,0.0613057,"Missing"
P15-4011,H05-1004,0,0.0408153,"ntion Pair Ranking: Closest Ranking: Latent Antecedent Trees 67.16 67.96 68.13 65.34 71.48 76.61 76.72 78.12 69.25 72.03 72.17 71.16 51.97 54.07 54.22 50.23 60.55 64.98 66.12 67.36 55.93 59.03 59.58 57.54 51.02 51.45 52.33 49.76 51.89 59.02 59.47 58.43 51.45 54.97 55.67 53.75 58.88 62.01 62.47 60.82 Table 1: Results of different systems and models on CoNLL-2012 English test data. Models below the dashed lines are implemented in our toolkit. 6 scorer (Pradhan et al., 2014), which computes the average of the evaluation metrics MUC (Vilain et al., 1995), B3 , (Bagga and Baldwin, 1998) and CEAFe (Luo, 2005). The models are trained on the concatenation of training and development data. The evaluation of the models is shown in Table 1. To put the numbers into context, we compare with Fernandes et al. (2014), the winning system of the CoNLL-2012 shared task, and the state-ofthe-art system of Bj¨orkelund and Kuhn (2014). The mention pair model performs decently, while the antecedent tree model exhibits performance comparable to Fernandes et al. (2014), who use a very similar model. The ranking models outperform Bj¨orkelund and Kuhn (2014), obtaining state-of-the-art performance. 5 Conclusions We pre"
P15-4011,M95-1005,0,0.464918,"1 57.58 58.58 50.82 52.27 57.28 59.40 53.86 55.61 60.65 61.63 Mention Pair Ranking: Closest Ranking: Latent Antecedent Trees 67.16 67.96 68.13 65.34 71.48 76.61 76.72 78.12 69.25 72.03 72.17 71.16 51.97 54.07 54.22 50.23 60.55 64.98 66.12 67.36 55.93 59.03 59.58 57.54 51.02 51.45 52.33 49.76 51.89 59.02 59.47 58.43 51.45 54.97 55.67 53.75 58.88 62.01 62.47 60.82 Table 1: Results of different systems and models on CoNLL-2012 English test data. Models below the dashed lines are implemented in our toolkit. 6 scorer (Pradhan et al., 2014), which computes the average of the evaluation metrics MUC (Vilain et al., 1995), B3 , (Bagga and Baldwin, 1998) and CEAFe (Luo, 2005). The models are trained on the concatenation of training and development data. The evaluation of the models is shown in Table 1. To put the numbers into context, we compare with Fernandes et al. (2014), the winning system of the CoNLL-2012 shared task, and the state-ofthe-art system of Bj¨orkelund and Kuhn (2014). The mention pair model performs decently, while the antecedent tree model exhibits performance comparable to Fernandes et al. (2014), who use a very similar model. The ranking models outperform Bj¨orkelund and Kuhn (2014), obtain"
P15-4011,D14-1221,1,0.868541,"x ∈ X , we write Mx = {m1 , . . . , mn } for the mentions in x. Following previous work (Chang et al., 2012; Fernandes et al., 2014), we make use of a dummy mention which we denote as m0 . If m0 is predicted as the 1 http://smartschat.de/software http://pypi.python.org/pypi. Install it via pip install cort. 2 61 Proceedings of ACL-IJCNLP 2015 System Demonstrations, pages 61–66, c Beijing, China, July 26-31, 2015. 2015 ACL and AFNLP antecedent of a mention mi , we consider mi nonanaphoric. We define Mx0 = {m0 } ∪ Mx . Inspired by previous work (Bengtson and Roth, 2008; Fernandes et al., 2014; Martschat and Strube, 2014), we adopt a graph-based representation of the latent structures h ∈ H. In particular, we express structures by labeled directed graphs with vertex set Mx0 . for each substructure (instead of one maximization problem for the whole structure). To learn the parameter vector θ ∈ Rd from training data, we employ a latent structured perceptron (Sun et al., 2009) with cost-augmented inference (Crammer et al., 2006) and averaging (Collins, 2002). m1 m3 We now describe our implementation of the framework presented in the previous section. m4 3.1 m0 m2 3 Implementation By expressing approaches in the f"
P15-4011,N15-3002,1,0.804524,"Missing"
P15-4011,D08-1031,0,\N,Missing
P15-4011,J14-4004,0,\N,Missing
P15-4011,P14-1005,0,\N,Missing
P16-1060,W11-1901,0,0.0124221,"ut entities with various kinds of coreference errors. Several evaluation metrics have been introduced for coreference resolution (Vilain et al., 1995; Bagga and Baldwin, 1998; Luo, 2005; Recasens and Hovy, 2011; Tuggener, 2014). Metrics that are being used widely are MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998), CEAF (Luo, 2005), and BLANC (Recasens and Hovy, 2011). There are known flaws for each of these metrics. Besides, the agreement between all these metrics is relatively low (Holen, 2013), and it is not clear which metric is the most reliable. The CoNLL-2011/2012 shared tasks (Pradhan et al., 2011; Pradhan et al., 2012) ranked participating systems using an average of three metrics, i.e. MUC, B3 , and CEAF, following a proposal by (Denis and Baldridge, 2009a). Averaging three unreliable scores does not result in a reliable one. Besides, when an average score is used for comparisons, it is not possible to analyse recall and precision to determine which output is more precise and which one covers more coreference information. This is indeed a requirement for coreference resolvers to be used in end-tasks. Therefore, averaging individual metrics is nothing but a compromise. As mentioned by"
P16-1060,P15-1136,0,0.187137,"xisting metrics. We have begun the process of integrating the LEA metric in the official CoNLL scorer1 so as to continue the progress made in recent years to produce replicable evaluation metrics. In order to use the LEA metric, there is no additional requirement than that of the CoNLL scorer v8.01 2 . 2 to meet the specific requirements of text summarization and question answering. In this section, we show that the recall and precision of the B3 , CEAF and BLANC metrics are neither interpretable nor reliable. We choose the output of the state-of-the-art coreference resolver of Wiseman et al. (2015) on the CoNLL 2012 English test set as the base output. The CoNLL 2012 English test set contains 222 documents (comprising 348 partially annotated sections). This test set contains 19,764 coreferring mentions that belong to 4,532 different entities. In Table 1, Base represents the scores of (Wiseman et al., 2015) on the CoNLL 2012 test set. All reported scores in this paper are computed by the official CoNLL scorer v8.01 (Pradhan et al., 2014). Assume Mk,r is the set of mentions that exists in both key and response entities. Let Lk (m) and Lr (m) be the set of coreference links of mention m in"
P16-1060,W12-4501,0,0.128861,"us kinds of coreference errors. Several evaluation metrics have been introduced for coreference resolution (Vilain et al., 1995; Bagga and Baldwin, 1998; Luo, 2005; Recasens and Hovy, 2011; Tuggener, 2014). Metrics that are being used widely are MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998), CEAF (Luo, 2005), and BLANC (Recasens and Hovy, 2011). There are known flaws for each of these metrics. Besides, the agreement between all these metrics is relatively low (Holen, 2013), and it is not clear which metric is the most reliable. The CoNLL-2011/2012 shared tasks (Pradhan et al., 2011; Pradhan et al., 2012) ranked participating systems using an average of three metrics, i.e. MUC, B3 , and CEAF, following a proposal by (Denis and Baldridge, 2009a). Averaging three unreliable scores does not result in a reliable one. Besides, when an average score is used for comparisons, it is not possible to analyse recall and precision to determine which output is more precise and which one covers more coreference information. This is indeed a requirement for coreference resolvers to be used in end-tasks. Therefore, averaging individual metrics is nothing but a compromise. As mentioned by Luo (2005), interpreta"
P16-1060,P14-2006,1,0.667762,"gletons. Therefore, the mention identification effect does not apply to LEA recall and precision. As a result, one can trust LEA recall or precision. 6 – LEA takes the importance of missing/extra entities into account. Therefore, unlike CEAF e , it differentiates between the outputs missing the most prominent and the smallest entities. – LEA considers resolved coreference relations instead of resolved mentions. Therefore, the existence of repeated mentions in different response entities is not troublesome for LEA. An Illustrative Example In this section, we use the example from Pradhan et al. (2014) to show the process of computing the LEA scores. In this example, K = {k1 = {a, b, c}, k2 = {d, e, f, g}} is the set of key entities and R = {r1 = {a, b}, r2 = {c, d}, r3 = {f, g, h, i}} is the set of response entities. Here we assume that importance corresponds to entity size. Hence, importance(k1 ) = 3 and importance(k2 ) = 4. The sets of coreference links in k1 and k2 are {ab, ac, bc} and {de, df, dg, ef, eg, f g}, respectively. Therefore, link(k1 ) = 3 and link(k2 ) = 6. ab is the only common link between k1 and r1 . There are no common links between k1 and the two other response entities"
P16-1060,N13-2001,0,0.184216,"s imperative for the evaluation metrics to be reliable. However, it is not a trivial task to score output entities with various kinds of coreference errors. Several evaluation metrics have been introduced for coreference resolution (Vilain et al., 1995; Bagga and Baldwin, 1998; Luo, 2005; Recasens and Hovy, 2011; Tuggener, 2014). Metrics that are being used widely are MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998), CEAF (Luo, 2005), and BLANC (Recasens and Hovy, 2011). There are known flaws for each of these metrics. Besides, the agreement between all these metrics is relatively low (Holen, 2013), and it is not clear which metric is the most reliable. The CoNLL-2011/2012 shared tasks (Pradhan et al., 2011; Pradhan et al., 2012) ranked participating systems using an average of three metrics, i.e. MUC, B3 , and CEAF, following a proposal by (Denis and Baldridge, 2009a). Averaging three unreliable scores does not result in a reliable one. Besides, when an average score is used for comparisons, it is not possible to analyse recall and precision to determine which output is more precise and which one covers more coreference information. This is indeed a requirement for coreference resolver"
P16-1060,P09-1074,0,0.0364501,"tion, it1 , it2 , it3 } rj cr1 cr2 respectively. CEAF e rewards recall and precision by (|ki |+|r2j |)×|K |and (|ki |+|r2j |)×|R |, respectively. If instead of the number of common mentions, MUC 92.30 60.00 B3 66.66 40.00 CEAFm 50.00 66.66 CEAFe 44.44 66.66 BLANC 60.00 32.29 Table 3: F1 scores for Table 2’s response entities. 635 The other response entity is only used for penalizing the precision of cr1 . This counterintuitive result is only because of the stringent constraint of CEAF that the mapping of key to response entities should be one-to-one. Another problem with CEAF e , mentioned by Stoyanov et al. (2009), is that it weights entities equally regardless of their sizes. The system that does not detect entity (1), the most prominent entity of Figure 1, gets the same score as that of a system which does not detect entity (4) of size 2. 3.4 LEA evaluates a set of entities as follows: P ei ∈E (importance(ei ) × resolution-score(ei )) P ek ∈E importance(ek ) We consider the size of an entity as a measure of importance, i.e. importance(e) = |e|. Therefore, the more prominent entities of the text get higher importance values. However, according to the end-task or domain used, one can choose other impor"
P16-1060,P14-2005,0,0.0442232,"on factors besides ei ’s size, e.g. ei ’s entity type or ei ’s mention types. For example, as suggested by Holen (2013), each mention carries different information values, and considering this information could benefit the quantitative evaluation of coreference resolution. The importance measure of LEA is the appropriate place to incorporate this kind of information. Entity e with n mentions has link(e) = n × (n−1)/2 unique coreference links. The resolution score of key entity ki is computed as the fraction of correctly resolved coreference links of ki : BLANC BLANC (Recasens and Hovy, 2011; Luo et al., 2014) is a link-based metric that adapts the Rand index (Rand, 1971) to coreference resolution evaluation. Let Ck and Cr be the sets of coreference links in the key and response entities, respectively. Assume Nk and Nr are the sets of non-coreference links in the key and response entities, respectively. Recall and precision of coreference links are computed as: Rc = |Ck ∩ Cr | , |Ck | Pc = resolution-score(ki ) = |Ck ∩ Cr | |Cr | rj ∈R For each ki , LEA checks all the response entities to see whether they are partial matches for ki . rj is a partial match for ki , if it contains at least one of the"
P16-1060,H05-1004,0,0.407999,"e for Theoretical Studies gGmbH Schloss-Wolfsbrunnenweg 35 69118 Heidelberg, Germany {nafise.moosavi|michael.strube}@h-its.org Abstract set is useful for improving the recall or precision of a coreference resolver. Therefore, evaluation metrics play an important role in the advancement of the underlying technology. It is imperative for the evaluation metrics to be reliable. However, it is not a trivial task to score output entities with various kinds of coreference errors. Several evaluation metrics have been introduced for coreference resolution (Vilain et al., 1995; Bagga and Baldwin, 1998; Luo, 2005; Recasens and Hovy, 2011; Tuggener, 2014). Metrics that are being used widely are MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998), CEAF (Luo, 2005), and BLANC (Recasens and Hovy, 2011). There are known flaws for each of these metrics. Besides, the agreement between all these metrics is relatively low (Holen, 2013), and it is not clear which metric is the most reliable. The CoNLL-2011/2012 shared tasks (Pradhan et al., 2011; Pradhan et al., 2012) ranked participating systems using an average of three metrics, i.e. MUC, B3 , and CEAF, following a proposal by (Denis and Baldridge, 2009a)"
P16-1060,Q15-1029,1,0.869373,"1 ) = 3 and link(k2 ) = 6. ab is the only common link between k1 and r1 . There are no common links between k1 and the two other response entities. Similarly, k2 has one common link with r3 and it has no common links with r1 or r2 . Therefore, resolution-score(k1 ) = 1+0+0 3 and resolution-score(k2 ) = 0+0+1 . As a result 6 LEA recall is computed as: P importance(ki ) × resolution-score(ki ) P importance(kj ) = 3× 1 3 +4× 3+4 1 6 + 2 × 0+0 1 +4× 2+2+4 0+1 6 ≈ 0.33 Evaluation on Real Data Table 4 shows the scores of the state-of-the-art coreference resolvers developed by Wiseman et al. (2015), Martschat and Strube (2015), and Peng et al. (2015). Clark and Manning (2015)’s resolver is also among the state-of-the-art systems but we did not have access to their output. Considering the average score of MUC, B3 , and CEAF e , Martschat, and Peng perform equally. However, according to LEA, Martschat performs significantly better based on an approximate randomization test (Noreen, 1989). CEAF e also agrees with LEA for this ranking. However, CEAF e recall and precision are similar for Peng while based on LEA, Peng’s precision is marginally better than recall. In addition to the state-of-the-art systems, we report th"
P16-1060,E14-4045,0,0.0411781,"oss-Wolfsbrunnenweg 35 69118 Heidelberg, Germany {nafise.moosavi|michael.strube}@h-its.org Abstract set is useful for improving the recall or precision of a coreference resolver. Therefore, evaluation metrics play an important role in the advancement of the underlying technology. It is imperative for the evaluation metrics to be reliable. However, it is not a trivial task to score output entities with various kinds of coreference errors. Several evaluation metrics have been introduced for coreference resolution (Vilain et al., 1995; Bagga and Baldwin, 1998; Luo, 2005; Recasens and Hovy, 2011; Tuggener, 2014). Metrics that are being used widely are MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998), CEAF (Luo, 2005), and BLANC (Recasens and Hovy, 2011). There are known flaws for each of these metrics. Besides, the agreement between all these metrics is relatively low (Holen, 2013), and it is not clear which metric is the most reliable. The CoNLL-2011/2012 shared tasks (Pradhan et al., 2011; Pradhan et al., 2012) ranked participating systems using an average of three metrics, i.e. MUC, B3 , and CEAF, following a proposal by (Denis and Baldridge, 2009a). Averaging three unreliable scores does n"
P16-1060,K15-1002,0,0.0243526,"the only common link between k1 and r1 . There are no common links between k1 and the two other response entities. Similarly, k2 has one common link with r3 and it has no common links with r1 or r2 . Therefore, resolution-score(k1 ) = 1+0+0 3 and resolution-score(k2 ) = 0+0+1 . As a result 6 LEA recall is computed as: P importance(ki ) × resolution-score(ki ) P importance(kj ) = 3× 1 3 +4× 3+4 1 6 + 2 × 0+0 1 +4× 2+2+4 0+1 6 ≈ 0.33 Evaluation on Real Data Table 4 shows the scores of the state-of-the-art coreference resolvers developed by Wiseman et al. (2015), Martschat and Strube (2015), and Peng et al. (2015). Clark and Manning (2015)’s resolver is also among the state-of-the-art systems but we did not have access to their output. Considering the average score of MUC, B3 , and CEAF e , Martschat, and Peng perform equally. However, according to LEA, Martschat performs significantly better based on an approximate randomization test (Noreen, 1989). CEAF e also agrees with LEA for this ranking. However, CEAF e recall and precision are similar for Peng while based on LEA, Peng’s precision is marginally better than recall. In addition to the state-of-the-art systems, we report the scores of boundary cas"
P16-1060,P15-1137,0,0.0866084,"oblems of the existing metrics. We have begun the process of integrating the LEA metric in the official CoNLL scorer1 so as to continue the progress made in recent years to produce replicable evaluation metrics. In order to use the LEA metric, there is no additional requirement than that of the CoNLL scorer v8.01 2 . 2 to meet the specific requirements of text summarization and question answering. In this section, we show that the recall and precision of the B3 , CEAF and BLANC metrics are neither interpretable nor reliable. We choose the output of the state-of-the-art coreference resolver of Wiseman et al. (2015) on the CoNLL 2012 English test set as the base output. The CoNLL 2012 English test set contains 222 documents (comprising 348 partially annotated sections). This test set contains 19,764 coreferring mentions that belong to 4,532 different entities. In Table 1, Base represents the scores of (Wiseman et al., 2015) on the CoNLL 2012 test set. All reported scores in this paper are computed by the official CoNLL scorer v8.01 (Pradhan et al., 2014). Assume Mk,r is the set of mentions that exists in both key and response entities. Let Lk (m) and Lr (m) be the set of coreference links of mention m in"
P17-2003,D14-1221,1,0.913802,"op of their lexical features does not result in a significant improvement. deep-coref, the state-of-the-art coreference resolver, follows the same approach. Clark and Manning (2016b) capture the required information for resolving coreference relations by using a large number of lexical features and a small set of nonlexical features including string match, distance, mention type, speaker and genre features. The main difference is that Clark and Manning (2016b) use word embeddings instead of the exact surface forms that are used by Durrett and Klein (2013). Based on the error analysis by cort (Martschat and Strube, 2014), in comparison to systems that do not use word embeddings, deep-coref has fewer recall and precision errors especially for pronouns. For example, deep-coref correctly recognizes around 83 percent of non-anaphoric “it” in the CoNLL development set. This could be a direct result of a better context representation by word embeddings. 3 Out-of-Domain Evaluation Aside from the evident success of lexical features, it is debatable how well the knowledge that is mainly captured by the lexical information of the training data can be generalized to other domains. As reported by Ghaddar and Langlais (20"
P17-2003,D16-1245,0,0.246344,"oblem in which each resulting partition refers to an entity. As shown by Durrett and Klein (2013), lexical features implicitly model some linguistic phenomena, which were previously modeled by heuristic features, but at a finer level of granularity. However, we question whether the knowledge that is mainly captured by lexical features can be generalized to other domains. The introduction of the CoNLL dataset enabled a significant boost in the performance of coreference resolvers, i.e. about 10 percent difference between the CoNLL score of the currently best coreference resolver, deep-coref by Clark and Manning (2016b), and the winner of the CoNLL 2011 shared task, the Stanford rule-based system 2 Lexical Features The large difference in performance between coreference resolvers that use lexical features and ones which do not, implies the importance of lexical features. Durrett and Klein (2013) show that lexical features implicitly capture some phenomena, e.g. definiteness and syntactic roles, which were previously modeled by heuristic features. Durrett and Klein (2013) use exact surface forms as lexical features. However, when word embeddings are used instead of surface forms, the use of lexical features"
P17-2003,Q15-1029,1,0.510455,"e results on the CoNLL test set and WikiCoref. et al., 1995), B3 (Bagga and Baldwin, 1998), CEAF e (Luo, 2005), the average F1 score of these three metrics, i.e. CoNLL score, and LEA (Moosavi and Strube, 2016). berkeley is the mention-ranking model of Durrett and Klein (2013) with the FINAL feature set including the head, first, last, preceding and following words of a mention, the ancestry, length, gender and number of a mention, distance of two mentions, whether the anaphor and antecedent are nested, same speaker and a small set of string match features. cort is the mention-ranking model of Martschat and Strube (2015). cort uses the following set of features: the head, first, last, preceding and following words of a mention, the ancestry, length, gender, number, type, semantic class, dependency relation and dependency governor of a mention, the named entity type of the head word, distance of two mentions, same speaker, whether the anaphor and antecedent are nested, and a set of string match features. berkeley and cort scores in Table 1 are taken from Ghaddar and Langlais (2016a). deep-coref is the mention-ranking model of Clark and Manning (2016b). deep-coref incorporates a large set of embeddings, i.e. em"
P17-2003,P16-1061,0,0.130108,"oblem in which each resulting partition refers to an entity. As shown by Durrett and Klein (2013), lexical features implicitly model some linguistic phenomena, which were previously modeled by heuristic features, but at a finer level of granularity. However, we question whether the knowledge that is mainly captured by lexical features can be generalized to other domains. The introduction of the CoNLL dataset enabled a significant boost in the performance of coreference resolvers, i.e. about 10 percent difference between the CoNLL score of the currently best coreference resolver, deep-coref by Clark and Manning (2016b), and the winner of the CoNLL 2011 shared task, the Stanford rule-based system 2 Lexical Features The large difference in performance between coreference resolvers that use lexical features and ones which do not, implies the importance of lexical features. Durrett and Klein (2013) show that lexical features implicitly capture some phenomena, e.g. definiteness and syntactic roles, which were previously modeled by heuristic features. Durrett and Klein (2013) use exact surface forms as lexical features. However, when word embeddings are used instead of surface forms, the use of lexical features"
P17-2003,P16-1060,1,0.768355,"47.88 38.18 49.08 42.47 51.47 49.54 41.40 53.08 47.11 38.46 50.31 F1 F1 CoNLL Avg. F1 R LEA P F1 49.34 54.72 56.82 58.45 58.90 55.60 61.24 63.37 65.39 65.60 43.72 49.66 50.40 54.55 54.55 51.53 59.17 64.46 65.35 65.68 47.30 54.00 56.57 59.46 59.60 46.44 43.92 42.48 46.54 46.52 43.60 51.77 51.01 49.94 52.65 53.14 51.01 38.79 40.36 38.22 34.11 48.92 50.73 55.98 57.15 43.27 44.95 45.43 42.72 Table 1: Comparison of the results on the CoNLL test set and WikiCoref. et al., 1995), B3 (Bagga and Baldwin, 1998), CEAF e (Luo, 2005), the average F1 score of these three metrics, i.e. CoNLL score, and LEA (Moosavi and Strube, 2016). berkeley is the mention-ranking model of Durrett and Klein (2013) with the FINAL feature set including the head, first, last, preceding and following words of a mention, the ancestry, length, gender and number of a mention, distance of two mentions, whether the anaphor and antecedent are nested, same speaker and a small set of string match features. cort is the mention-ranking model of Martschat and Strube (2015). cort uses the following set of features: the head, first, last, preceding and following words of a mention, the ancestry, length, gender, number, type, semantic class, dependency r"
P17-2003,D13-1203,0,0.0970106,"show that if coreference resolvers mainly rely on lexical features, they can hardly generalize to unseen domains. Furthermore, we show that the current coreference resolution evaluation is clearly flawed by only evaluating on a specific split of a specific dataset in which there is a notable overlap between the training, development and test sets. 1 Introduction Similar to many other tasks, lexical features are a major source of information in current coreference resolvers. Coreference resolution is a set partitioning problem in which each resulting partition refers to an entity. As shown by Durrett and Klein (2013), lexical features implicitly model some linguistic phenomena, which were previously modeled by heuristic features, but at a finer level of granularity. However, we question whether the knowledge that is mainly captured by lexical features can be generalized to other domains. The introduction of the CoNLL dataset enabled a significant boost in the performance of coreference resolvers, i.e. about 10 percent difference between the CoNLL score of the currently best coreference resolver, deep-coref by Clark and Manning (2016b), and the winner of the CoNLL 2011 shared task, the Stanford rule-based"
P17-2003,P14-2006,1,0.82196,"Missing"
P17-2003,M95-1005,0,0.0957849,"Missing"
P17-2003,L16-1021,0,0.34054,"the anaphor and antecedent are nested, same speaker and a small set of string match features. cort is the mention-ranking model of Martschat and Strube (2015). cort uses the following set of features: the head, first, last, preceding and following words of a mention, the ancestry, length, gender, number, type, semantic class, dependency relation and dependency governor of a mention, the named entity type of the head word, distance of two mentions, same speaker, whether the anaphor and antecedent are nested, and a set of string match features. berkeley and cort scores in Table 1 are taken from Ghaddar and Langlais (2016a). deep-coref is the mention-ranking model of Clark and Manning (2016b). deep-coref incorporates a large set of embeddings, i.e. embeddings of the head, first, last, two previous/following words, and the dependency governor of a mention in addition to the averaged embeddings of the five previous/following words, all words of the mention, sentence words, and document words. deep-coref also incorporates type, length, and position of a mention, whether the mention is nested in any other mention, distance of two mentions, speaker features and a small set of string match features. For deep-coref ["
P17-2003,J13-4004,0,0.0265899,"Missing"
P17-2003,N15-1098,0,0.0351304,"classifier could learn that “Haiti” is a country and “Guangzhou” is a city. However, it is questionable how useful word knowledge is if it is mainly based on the training data. The coreference relation of two nominal noun phrases with no head match can be very hard to resolve. The resolution of such pairs has been referred to as capturing semantic similarity (Clark and Manning, 2016b). deep-coref links 49 such pairs on the development set. Among all these links, only 5 pairs are unseen on the training set and all of them are incorrect links. The effect of lexical features is also analyzed by Levy et al. (2015) for tasks like hypernymy and entailment. They show that state-of-the-art classifiers memorize words from the training data. The classifiers benefit from this lexical memorization when there are common words between the training and test sets. 5 Acknowledgments The authors would like to thank Kevin Clark for answering all of our questions regarding deepcoref. We would also like to thank the three anonymous reviewers for their thoughtful comments. This work has been funded by the Klaus Tschira Foundation, Heidelberg, Germany. The first author has been supported by a Heidelberg Institute for The"
P17-2003,H05-1004,0,0.0293398,"3 54.20 63.44 63.18 54.43 64.17 WikiCoref 47.90 50.89 42.70 47.51 48.06 40.44 46.45 47.88 38.18 49.08 42.47 51.47 49.54 41.40 53.08 47.11 38.46 50.31 F1 F1 CoNLL Avg. F1 R LEA P F1 49.34 54.72 56.82 58.45 58.90 55.60 61.24 63.37 65.39 65.60 43.72 49.66 50.40 54.55 54.55 51.53 59.17 64.46 65.35 65.68 47.30 54.00 56.57 59.46 59.60 46.44 43.92 42.48 46.54 46.52 43.60 51.77 51.01 49.94 52.65 53.14 51.01 38.79 40.36 38.22 34.11 48.92 50.73 55.98 57.15 43.27 44.95 45.43 42.72 Table 1: Comparison of the results on the CoNLL test set and WikiCoref. et al., 1995), B3 (Bagga and Baldwin, 1998), CEAF e (Luo, 2005), the average F1 score of these three metrics, i.e. CoNLL score, and LEA (Moosavi and Strube, 2016). berkeley is the mention-ranking model of Durrett and Klein (2013) with the FINAL feature set including the head, first, last, preceding and following words of a mention, the ancestry, length, gender and number of a mention, distance of two mentions, whether the anaphor and antecedent are nested, same speaker and a small set of string match features. cort is the mention-ranking model of Martschat and Strube (2015). cort uses the following set of features: the head, first, last, preceding and fol"
P19-1027,C18-1139,0,0.0341765,".5 89.2 92.9 88.6 85.2 96.5 82.1 81.6 83.1 73.9 58.7 97.9 92.8 91.0 94.0 89.7 88.7 4 Limitations. While extensive, our evaluation is not without limitations. Throughout this study, we have used a Wikipedia edition in a given language as a sample of that language. The degree to which this sample is representative varies, and low-resource Wikipedias in particular contain large fractions of “foreign” text and noise, which propagates into embeddings and datasets. Our evaluation did not include other subword representations, most notably ELMo (Peters et al., 2018) and contextual string embeddings (Akbik et al., 2018), since, even though they are languageagnostic in principle, pretrained models are only available in a few languages. Conclusions. We have presented a large-scale study of contextual and non-contextual subword embeddings, in which we trained monolingual and multilingual NER models in 265 languages and POS-tagging models in 27 languages. BPE vocabulary size has a large effect on model quality, both in monolingual settings and with a large vocabulary shared among 265 languages. As a rule of thumb, a smaller vocabulary size is better for small datasets and larger vocabulary sizes better for large"
P19-1027,K16-1006,0,0.0266798,"g methods compared in our evaluation, as well as the NER baseline system (Pan17). BERT: Contextual Subword Embeddings One of the drawbacks of the subword embeddings introduced above, and of pretrained word embeddings in general, is their lack of context. For example, with a non-contextual representation, the embedding of the word play will be the same both in the phrase a play by Shakespeare and the phrase to play Chess, even though play in the first phrase is a noun with a distinctly different meaning than the verb play in the second phrase. Contextual word representations (Dai and Le, 2015; Melamud et al., 2016; Ramachandran et al., 2017; Peters et al., 2018; Radford et al., 2018; Howard and Ruder, 2018) overcome this shortcoming via pretrained language models. Instead of representing a word or subword by a lookup of a learned embedding, which is the same regardless of context, a contextual representation is obtained by encoding the word in context using a neural language model (Bengio et al., 2003). Neural language models typically employ a sequence encoder such as a bidirectional LSTM (Hochreiter and Schmidhuber, 1997) or Transformer (Vaswani et al., 2017). In such a model, each word or subword in"
P19-1027,D18-2012,0,0.0226795,"017) and other tasks which involve the costly operation of taking a softmax over the entire output vocabulary (see Morin and Bengio, 2005; Li et al., 2019). BPE vocabulary sizes between 8k and 64k are common in neural machine translation. Multilingual BERT operates on a subword vocabulary of size 100k which is shared among 104 languages. Even with shared sym10 Specifically, we extract up to 500k randomly selected paragraphs from articles in each Wikipedia edition, yielding 16GB of text in 265 languages. Then, we train BPE models with vocabulary sizes 100k, 320k, and 1000k using SentencePiece (Kudo and Richardson, 2018), and finally train 300d subword embeddings using GloVe. 11 With this limit, training takes about a week on one NVIDIA P40 GPU. 278 Figure 5: Shared multilingual byte-pair embedding space pretrained (left) and after NER model training (right), 2-d UMAP projection (McInnes et al., 2018). As there is no 1-to-1 correspondence between BPE symbols and languages in a shared multilingual vocabulary, it is not possible to color BPE symbols by language. Instead, we color symbols by Unicode code point. This yields a coloring in which, for example, BPE symbols consisting of characters from the Latin alph"
P19-1027,L16-1262,0,0.0503098,"Missing"
P19-1027,P17-1178,0,0.0961415,"t pretrained subword representations introduced in §2 on two tasks: NER and POS tagging. Our multilingual evaluation is split in four parts. After devising a sequence tagging architecture (§3.1), we investigate an important hyper-parameter in BPE-based subword segmentation: the BPE vocabulary size (§3.2). Then, we conduct NER experiments on two sets of languages (see Table 2): 265 languages supported by FastText and BPEmb (§3.3) and the 101 languages supported by all methods including BERT (§3.4). Our experiments conclude with POS tagging on 27 languages (§3.4). Data. For NER, we use WikiAnn (Pan et al., 2017), a dataset containing named entity mention and three-class entity type annotations in 282 languages. WikiAnn was automatically generated by extracting and classifying entity mentions from inter-article links on Wikipedia. Because of this, WikiAnn suffers from problems such as skewed entity type distributions in languages with small Wikipedias (see Figure 6 in Appendix A), as well as wrong entity types due to automatic type classification. These issues notwithstanding, WikiAnn is the only available NER dataset that covers almost all languages supported by the subword representations compared i"
P19-1027,P16-1162,0,0.0151816,"Introduction Rare and unknown words pose a difficult challenge for embedding methods that rely on seeing a word frequently during training (Bullinaria and Levy, 2007; Luong et al., 2013). Subword segmentation methods avoid this problem by assuming a word’s meaning can be inferred from the meaning of its parts. Linguistically motivated subword approaches first split words into morphemes and then represent word meaning by composing morpheme embeddings (Luong et al., 2013). More recently, character-ngram approaches (Luong and Manning, 2016; Bojanowski et al., 2017) and Byte Pair Encoding (BPE) (Sennrich et al., 2016) have grown in popularity, likely due to their computational simplicity and language-agnosticity.1 Sequence tagging with subwords. Subword information has long been recognized as an important feature in sequence tagging tasks such as named entity recognition (NER) and part-ofspeech (POS) tagging. For example, the suffix -ly often indicates adverbs in English POS tagging and English NER may exploit that professions often end in suffixes like -ist (journalist, cyclist) or companies in suffixes like -tech or soft. In early systems, these observations were operationalized with manually compiled li"
P19-1027,D14-1162,0,0.0791663,"to new symbols. E.g., when applied to English text, BPE merges the characters h and e into the new byte-pair symbol he, then the pair consisting of the character t and the byte-pair symbol he into the new symbol the and so on. These merge operations are learned from a large background corpus. The set of byte-pair symbols learned in this fashion is called the BPE vocabulary. Applying BPE, i.e. iteratively performing learned merge operations, segments a text into subwords (see BPE segmentations for vocabulary sizes vs1000 to vs100000 in Table 1). By employing an embedding algorithm, e.g. GloVe (Pennington et al., 2014), to train embeddings on such a subword-segmented text, one obtains • We present a large-scale evaluation of multilingual subword representations on two sequence tagging tasks; • We find that subword vocabulary size matters and give recommendations for choosing it; • We find that different methods have different strengths: Monolingual BPEmb works best in medium- and high-resource settings, multilingual non-contextual subword embeddings are best in low-resource languages, while multilingual BERT gives good or best results across languages. 2 FastText: Character-ngram Embeddings Subword Embeddin"
P19-1027,N18-1089,0,0.28185,"ntity type annotations in 282 languages. WikiAnn was automatically generated by extracting and classifying entity mentions from inter-article links on Wikipedia. Because of this, WikiAnn suffers from problems such as skewed entity type distributions in languages with small Wikipedias (see Figure 6 in Appendix A), as well as wrong entity types due to automatic type classification. These issues notwithstanding, WikiAnn is the only available NER dataset that covers almost all languages supported by the subword representations compared in this work. For POS tagging, we follow Plank et al. (2016); Yasunaga et al. (2018) and use annotations from the Universal Dependencies project (Nivre et al., 2016). These annotations take the form of language-universal POS tags (Petrov et al., 2012), such as noun, verb, adjective, determiner, and numeral. 3.1 Sequence Tagging Architecture Our sequence tagging architecture is depicted in Figure 1. The architecture is modular and allows encoding text using one or more subword embedding methods. The model receives a sequence of tokens as input, here Magnus Carlsen played. After subword segmentation and an embedding 3 https://nlp.h-its.org/bpemb/ https://github.com/google-resea"
P19-1027,N18-1202,0,0.378904,"the NER baseline system (Pan17). BERT: Contextual Subword Embeddings One of the drawbacks of the subword embeddings introduced above, and of pretrained word embeddings in general, is their lack of context. For example, with a non-contextual representation, the embedding of the word play will be the same both in the phrase a play by Shakespeare and the phrase to play Chess, even though play in the first phrase is a noun with a distinctly different meaning than the verb play in the second phrase. Contextual word representations (Dai and Le, 2015; Melamud et al., 2016; Ramachandran et al., 2017; Peters et al., 2018; Radford et al., 2018; Howard and Ruder, 2018) overcome this shortcoming via pretrained language models. Instead of representing a word or subword by a lookup of a learned embedding, which is the same regardless of context, a contextual representation is obtained by encoding the word in context using a neural language model (Bengio et al., 2003). Neural language models typically employ a sequence encoder such as a bidirectional LSTM (Hochreiter and Schmidhuber, 1997) or Transformer (Vaswani et al., 2017). In such a model, each word or subword in the input sequence is encoded into a vector rep"
P19-1027,petrov-etal-2012-universal,0,0.0197181,"of this, WikiAnn suffers from problems such as skewed entity type distributions in languages with small Wikipedias (see Figure 6 in Appendix A), as well as wrong entity types due to automatic type classification. These issues notwithstanding, WikiAnn is the only available NER dataset that covers almost all languages supported by the subword representations compared in this work. For POS tagging, we follow Plank et al. (2016); Yasunaga et al. (2018) and use annotations from the Universal Dependencies project (Nivre et al., 2016). These annotations take the form of language-universal POS tags (Petrov et al., 2012), such as noun, verb, adjective, determiner, and numeral. 3.1 Sequence Tagging Architecture Our sequence tagging architecture is depicted in Figure 1. The architecture is modular and allows encoding text using one or more subword embedding methods. The model receives a sequence of tokens as input, here Magnus Carlsen played. After subword segmentation and an embedding 3 https://nlp.h-its.org/bpemb/ https://github.com/google-research/ bert/blob/f39e881/multilingual.md 4 275 lookup, subword embeddings are encoded with an encoder specific to the respective subword method. For BERT, this is a pret"
P19-1027,W03-0424,0,\N,Missing
P19-1027,W13-3512,0,\N,Missing
P19-1027,N16-1030,0,\N,Missing
P19-1027,Q17-1010,0,\N,Missing
P19-1027,D17-1039,0,\N,Missing
P19-1027,N19-1423,0,\N,Missing
P19-1027,Q16-1026,0,\N,Missing
P19-1027,P05-1045,0,\N,Missing
P19-1027,P02-1062,0,\N,Missing
P19-1408,P16-1061,0,0.0772022,"broadcast news, broadcast conversation, telephone conversation, magazine, weblogs, and Bible genres while the annotated documents in WikiCoref are selected from Wikipedia. 6.2 Results Table 4 shows the maximum vs. minimum span evaluations of several recent coreference resolvers on the CoNLL-2012 test set and the WikiCoref dataset. The examined coreference resolvers are as follows: the Stanford rule-based system (Lee et al., 2013), the coreference resolver of Peng et al. (2015), the ranking model of cort (Martschat and Strube, 2015), the ranking and reinforcement learning models of deep-coref (Clark and Manning, 2016a,b), the single and ensemble models of Lee et al. (2017), and the current stateof-the-art system by Lee et al. (2018). We make the following observations based on the results of Table 4: Using minimum spans in coreference evaluation strongly affects the comparisons in the cross-dataset setting. The results on the WikiCoref dataset show that mention boundary detection errors specifically affect coreference scores in cross-dataset evaluations. The ranking of systems is very different by using maximum vs. minimum spans. The reinforcement learning model of deep-coref, i.e., deep-coref RL, has the"
P19-1408,D16-1245,0,0.0305313,"Missing"
P19-1408,L16-1021,0,0.0159792,"the CoNLL score (Pradhan et al., 2014), i.e. the average F1 value of MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998), and CEAF e (Luo, 2005), and the LEA F1 (Moosavi and Strube, 2016) score.10 Minimum spans are detected using both MINA and Collins’ head finding rules. All examined coreference resolvers are trained on the CoNLL-2012 training data. For indomain evaluations, models are evaluated on the CoNLL-2012 test data and minimum spans are extracted using gold parse trees, which are provided in CoNLL-2012.11 For cross-dataset evaluations, models are tested on the WikiCoref dataset (Ghaddar and Langlais, 2016). For extracting minimum spans, we parse WikiCoref by the Stanford neural parser. This dataset is annotated using the same annotation guidelines as that of CoNLL-2012, however, it contains documents from a different domain. 10 We use the python implementation that is available at https://github.com/ns-moosavi/coval. 11 We also examined the in-domain results of Table 4 based on the system parse trees of CoNLL-2012 instead of gold parse trees. The differences between scores based on MINA spans that are extracted from gold vs. those that are extracted from system parse trees were only about 0.2 p"
P19-1408,D17-1018,0,0.169234,"annotated using the same annotation guidelines as that of CoNLL-2012, however, it contains documents from a different domain. 10 We use the python implementation that is available at https://github.com/ns-moosavi/coval. 11 We also examined the in-domain results of Table 4 based on the system parse trees of CoNLL-2012 instead of gold parse trees. The differences between scores based on MINA spans that are extracted from gold vs. those that are extracted from system parse trees were only about 0.2 points. 4173 max CoNLL MINA Stanford rule-based cort Peng et al. deep-coref ranking deep-coref RL Lee et al. 2017 single Lee et al. 2017 ensemble Lee et al. 2018 55.60 (8) 63.03 (7) 63.05 (6) 65.59 (5) 65.81 (4) 67.23 (3) 68.87 (2) 72.96 (1) 57.55 (8) 64.60 (6) 63.50 (7) 67.29 (5) 67.50 (4) 68.55 (3) 70.12 (2) 74.26 (1) Stanford rule-based deep-coref ranking deep-coref RL Lee et al. 2017 single Lee et al. 2017 ensemble Lee et al. 2018 51.78 (4) 52.90 (3) 50.73 (5) 50.38 (6) 53.63 (2) 57.89 (1) 53.79 (5) 55.16 (2) 54.26 (4) 52.16 (6) 55.03 (3) 59.90 (1) head max CoNLL-2012 test set 57.38 (8) 47.31 (8) 64.51 (6) 56.10 (6) 63.54 (7) 55.22 (7) 67.09 (5) 59.58 (5) 67.36 (4) 59.76 (4) 68.53 (3) 61.24 (3) 70.05"
P19-1408,N18-2108,0,0.0860512,"Missing"
P19-1408,H05-1004,0,0.0716784,"l parser, makes MINA spans, as well as detected heads, more consistent compared to MIN spans. In addition to the above two properties, i.e. the length of minimum spans and their consistency with MIN annotations, we also check that MINA Experimental Setup In this section, we investigate how the use of minimum spans instead of maximum spans in coreference evaluation affects the results in in-domain as well as cross-dataset evaluations. For comparisons, we use the CoNLL score (Pradhan et al., 2014), i.e. the average F1 value of MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998), and CEAF e (Luo, 2005), and the LEA F1 (Moosavi and Strube, 2016) score.10 Minimum spans are detected using both MINA and Collins’ head finding rules. All examined coreference resolvers are trained on the CoNLL-2012 training data. For indomain evaluations, models are evaluated on the CoNLL-2012 test data and minimum spans are extracted using gold parse trees, which are provided in CoNLL-2012.11 For cross-dataset evaluations, models are tested on the WikiCoref dataset (Ghaddar and Langlais, 2016). For extracting minimum spans, we parse WikiCoref by the Stanford neural parser. This dataset is annotated using the same"
P19-1408,Q15-1029,1,0.833242,"ferent based on maximum vs. MINA spans are highlighted. CoNLL-2012 contains the newswire, broadcast news, broadcast conversation, telephone conversation, magazine, weblogs, and Bible genres while the annotated documents in WikiCoref are selected from Wikipedia. 6.2 Results Table 4 shows the maximum vs. minimum span evaluations of several recent coreference resolvers on the CoNLL-2012 test set and the WikiCoref dataset. The examined coreference resolvers are as follows: the Stanford rule-based system (Lee et al., 2013), the coreference resolver of Peng et al. (2015), the ranking model of cort (Martschat and Strube, 2015), the ranking and reinforcement learning models of deep-coref (Clark and Manning, 2016a,b), the single and ensemble models of Lee et al. (2017), and the current stateof-the-art system by Lee et al. (2018). We make the following observations based on the results of Table 4: Using minimum spans in coreference evaluation strongly affects the comparisons in the cross-dataset setting. The results on the WikiCoref dataset show that mention boundary detection errors specifically affect coreference scores in cross-dataset evaluations. The ranking of systems is very different by using maximum vs. minim"
P19-1408,P16-1060,1,0.892913,"as well as detected heads, more consistent compared to MIN spans. In addition to the above two properties, i.e. the length of minimum spans and their consistency with MIN annotations, we also check that MINA Experimental Setup In this section, we investigate how the use of minimum spans instead of maximum spans in coreference evaluation affects the results in in-domain as well as cross-dataset evaluations. For comparisons, we use the CoNLL score (Pradhan et al., 2014), i.e. the average F1 value of MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998), and CEAF e (Luo, 2005), and the LEA F1 (Moosavi and Strube, 2016) score.10 Minimum spans are detected using both MINA and Collins’ head finding rules. All examined coreference resolvers are trained on the CoNLL-2012 training data. For indomain evaluations, models are evaluated on the CoNLL-2012 test data and minimum spans are extracted using gold parse trees, which are provided in CoNLL-2012.11 For cross-dataset evaluations, models are tested on the WikiCoref dataset (Ghaddar and Langlais, 2016). For extracting minimum spans, we parse WikiCoref by the Stanford neural parser. This dataset is annotated using the same annotation guidelines as that of CoNLL-201"
P19-1408,P17-2003,1,0.701581,"ed on our analyses, MINA spans are compatible with those that are manually annotated by experts. By using MINA, we can benefit from minimum span evaluation for all corpora without introducing additional annotation costs. While the use of MINA spans already benefits in-domain evaluation, by reducing the gap between the performance on gold vs. system mentions, it has a more significant impact on crossdataset evaluation, in which detected maximum mention boundaries are noisier due to domain shift. Cross-dataset coreference evaluation is used to assess the generalization of coreference resolvers (Moosavi and Strube, 2017, 2018). Coreference resolution is a mid-step for text understanding in downstream tasks, e.g., question answering, text summarization, and information retrieval. Therefore, generalization is an important property for coreference resolvers because downstream datasets are not necessarily from the same domain as those of coreference-annotated corpora. When coreference resolvers are applied to a new domain, detected maximum boundaries become noisier, e.g., gold and system mentions differ by 4168 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4168–41"
P19-1408,P03-1054,0,0.0953729,"contain their corresponding MIN annotations in MUC and ARRAU. MINA and head words are detected using the parse trees of the Stanford PCFG parser. flight attendants more than 10000 NP MUC-7 92.4 90.0 NP QP QP MUC-6 95.6 92.9 and QP NP 329 million in NP cash marketable securities Figure 12: The system parse trees of two mentions from ARRAU. MINA spans are boldfaced. “securities” and “cash” are annotated as MIN for the left and right mentions, respectively. In order to investigate the effect of using a different parser, we perform the experiment of Table 2 using the Stanford English PCFG parser (Klein and Manning, 2003). The results are reported in Table 3. As we see, the use of a better parser, i.e., the Stanford neural parser, makes MINA spans, as well as detected heads, more consistent compared to MIN spans. In addition to the above two properties, i.e. the length of minimum spans and their consistency with MIN annotations, we also check that MINA Experimental Setup In this section, we investigate how the use of minimum spans instead of maximum spans in coreference evaluation affects the results in in-domain as well as cross-dataset evaluations. For comparisons, we use the CoNLL score (Pradhan et al., 201"
P19-1408,D18-1018,1,0.829277,"Missing"
P19-1408,J13-4004,0,0.144027,"Coref. The ranking of corresponding scores is specified in parentheses. Rankings which are different based on maximum vs. MINA spans are highlighted. CoNLL-2012 contains the newswire, broadcast news, broadcast conversation, telephone conversation, magazine, weblogs, and Bible genres while the annotated documents in WikiCoref are selected from Wikipedia. 6.2 Results Table 4 shows the maximum vs. minimum span evaluations of several recent coreference resolvers on the CoNLL-2012 test set and the WikiCoref dataset. The examined coreference resolvers are as follows: the Stanford rule-based system (Lee et al., 2013), the coreference resolver of Peng et al. (2015), the ranking model of cort (Martschat and Strube, 2015), the ranking and reinforcement learning models of deep-coref (Clark and Manning, 2016a,b), the single and ensemble models of Lee et al. (2017), and the current stateof-the-art system by Lee et al. (2018). We make the following observations based on the results of Table 4: Using minimum spans in coreference evaluation strongly affects the comparisons in the cross-dataset setting. The results on the WikiCoref dataset show that mention boundary detection errors specifically affect coreference"
P19-1408,K15-1002,0,0.286272,"Example 1. S VP NP NP This News Corp. has PP NP NP an extensive presence in PP , of NP Background NP this country course Figure 2: System parse tree of Example 1. A system that uses the system parse tree for mention detection links “that presence” to “an extensive presence, of course in this country” and gets penalized based on recall and precision. This penalty is the same as that of a system that links “that presence” to “this News Corp.”. Recall drops because of not recognizing “an extensive presence” and precision drops because of detecting a spurious mention. The coreference resolver of Peng et al. (2015) is developed around the idea that working with mention heads is more robust compared to working with maximum mention boundaries. In this regard, they develop a system that resolves coreference relations based on mention heads. The resolved mention heads are then expanded to full mention boundaries using a separate classifier that is trained to do so. Peng et al. (2015) also report the evaluation scores using both maximum mention boundaries and mention heads. Peng et al. (2015) extract mention heads using Collins’ head finder rules (Collins, 1999). They use gold 1 http://www-nlpir.nist.gov/rel"
P19-1408,W12-4501,0,0.0382966,"ngle mention. In order to decouple coreference evaluation from maximum boundary detection complexities, smaller corpora like MUC (Hirschman and Chinchor, 1997), ACE (Mitchell et al., 2002), and ARRAU (Uryupina et al., 2016) explicitly annotate the minimum span as well as the maximum logical span of each mention. The annotated minimum spans indicate the minimum strings that a coreference resolver must identify for the corresponding mentions. This solution comes with an additional annotation cost. As a result, the annotation of minimum spans has been discarded in larger corpora like CoNLL-2012 (Pradhan et al., 2012). In this paper, we propose MINA, a MINimum span extraction Algorithm that automatically determines minimum spans from constituency-based parse trees. Based on our analyses, MINA spans are compatible with those that are manually annotated by experts. By using MINA, we can benefit from minimum span evaluation for all corpora without introducing additional annotation costs. While the use of MINA spans already benefits in-domain evaluation, by reducing the gap between the performance on gold vs. system mentions, it has a more significant impact on crossdataset evaluation, in which detected maximu"
P19-1408,P13-1045,0,0.0156155,"ns, we use the provided parse trees in the key file.7 7 If the key file does not include parse information, we parse it with the Stanford parser. For the experiments of this section, we use the MUC-6, MUC-7, ARRAU, and CoNLL-2012 8 If the boundary of a mention is not recognized as a single phrase in the parse tree, as it is the case for the system mention, we add a dummy root (“X” in the right subtree of Figure 7) to include the whole span into a single phrase. 4171 ·104 corpora, from which MUC and ARRAU contain manually annotated minimum spans. We use the Stanford neural constituency parser (Socher et al., 2013) for getting system parse trees, unless otherwise stated. For the ARRAU corpus, we use mentions of the training split of the RST Discourse Treebank subpart. As a baseline, we also evaluate the syntactic head of mentions, based on Collins’ rules, as the minimum span.9 max-span min-span Count 1 0.5 0 How does the length of evaluated spans change by using MINA? Table 1 shows the average length of maximum spans vs. that of MINA spans on the training splits of the MUC-6, MUC-7 and ARRAU corpora as well as the development set of the CoNLL-2012 dataset. For the CoNLL-2012 dataset, we use the provided"
P19-1408,L16-1326,1,0.86508,"s is to specify the largest span of each mention. The problem with using maximum spans in coreference evaluation is that a single mention may have different maximum boundaries based on gold vs. automatically detected syntactic structures. For instance, variations in prepositional phrase attachment, which is a known challenge in syntactic parsing, will lead to different maximum boundaries for a single mention. In order to decouple coreference evaluation from maximum boundary detection complexities, smaller corpora like MUC (Hirschman and Chinchor, 1997), ACE (Mitchell et al., 2002), and ARRAU (Uryupina et al., 2016) explicitly annotate the minimum span as well as the maximum logical span of each mention. The annotated minimum spans indicate the minimum strings that a coreference resolver must identify for the corresponding mentions. This solution comes with an additional annotation cost. As a result, the annotation of minimum spans has been discarded in larger corpora like CoNLL-2012 (Pradhan et al., 2012). In this paper, we propose MINA, a MINimum span extraction Algorithm that automatically determines minimum spans from constituency-based parse trees. Based on our analyses, MINA spans are compatible wi"
P19-1408,M95-1005,0,0.661987,"As we see, the use of a better parser, i.e., the Stanford neural parser, makes MINA spans, as well as detected heads, more consistent compared to MIN spans. In addition to the above two properties, i.e. the length of minimum spans and their consistency with MIN annotations, we also check that MINA Experimental Setup In this section, we investigate how the use of minimum spans instead of maximum spans in coreference evaluation affects the results in in-domain as well as cross-dataset evaluations. For comparisons, we use the CoNLL score (Pradhan et al., 2014), i.e. the average F1 value of MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998), and CEAF e (Luo, 2005), and the LEA F1 (Moosavi and Strube, 2016) score.10 Minimum spans are detected using both MINA and Collins’ head finding rules. All examined coreference resolvers are trained on the CoNLL-2012 training data. For indomain evaluations, models are evaluated on the CoNLL-2012 test data and minimum spans are extracted using gold parse trees, which are provided in CoNLL-2012.11 For cross-dataset evaluations, models are tested on the WikiCoref dataset (Ghaddar and Langlais, 2016). For extracting minimum spans, we parse WikiCoref by the Stanford n"
P19-1408,J03-4003,0,\N,Missing
P19-1408,P14-2006,1,\N,Missing
P96-1036,P87-1022,0,0.986711,"on the forward-looking centers has emerged, one that reflects well-known regularities of fixed word order languages such as English. With the exception of Walker et al. (1990; 1994) for Japanese, Turan (1995) for Turkish, Rambow (1993) for German and Cote (1996) for English, only grammatical roles are considered and the (partial) ordering in Table 11 is taken for granted. I subject > dir-object > indir-object > complement(s) > adjunct(s) I Table 1: Grammatical Role Based Ranking on the C! ~Table 1 contains the most explicit ordering of grammatical roles we are aware of and has been taken from Brennan et al. (1987). Often, the distinction between complements and adjuncts is collapsed into the category ""others"" (c.f., e.g., Grosz et al. (1995)). Our work on the resolution of anaphora (Strube & Hahn, 1995; Hahn & Strube, 1996) and textual ellipsis (Hahn et al., 1996), however, is based on German, a free word order language, in which grammatical role information is far less predictive for the organization of centers. Rather, for establishing proper referential relations, the functional information structure of the utterances becomes crucial (different perspectives on functional analysis are brought forward"
P96-1036,P83-1007,0,0.955222,"order language (German) we propose a fundamental revision of the principles guiding the ordering of discourse entities in the forward-looking centers within the centering model. We claim that grammatical role criteria should be replaced by indicators of the functional information structure of the utterances, i.e., the distinction between context-bound and unbound discourse elements. This claim is backed up by an empirical evaluation of functional centering. 1 Introduction The centering model has evolved as a methodology for the description and explanation of the local coherence of discourse (Grosz et al., 1983; 1995), with focus on pronominal and nominal anaphora. Though several cross-linguistic studies have been carded out (cf. the enumeration in Grosz et al. (1995)), an almost canonical scheme for the ordering on the forward-looking centers has emerged, one that reflects well-known regularities of fixed word order languages such as English. With the exception of Walker et al. (1990; 1994) for Japanese, Turan (1995) for Turkish, Rambow (1993) for German and Cote (1996) for English, only grammatical roles are considered and the (partial) ordering in Table 11 is taken for granted. I subject > dir-ob"
P96-1036,J95-2003,0,0.14679,"he centering model. We claim that grammatical role criteria should be replaced by indicators of the functional information structure of the utterances, i.e., the distinction between context-bound and unbound discourse elements. This claim is backed up by an empirical evaluation of functional centering. 1 Introduction The centering model has evolved as a methodology for the description and explanation of the local coherence of discourse (Grosz et al., 1983; 1995), with focus on pronominal and nominal anaphora. Though several cross-linguistic studies have been carded out (cf. the enumeration in Grosz et al. (1995)), an almost canonical scheme for the ordering on the forward-looking centers has emerged, one that reflects well-known regularities of fixed word order languages such as English. With the exception of Walker et al. (1990; 1994) for Japanese, Turan (1995) for Turkish, Rambow (1993) for German and Cote (1996) for English, only grammatical roles are considered and the (partial) ordering in Table 11 is taken for granted. I subject > dir-object > indir-object > complement(s) > adjunct(s) I Table 1: Grammatical Role Based Ranking on the C! ~Table 1 contains the most explicit ordering of grammatical"
P96-1036,C96-1084,1,0.854411,"996) for English, only grammatical roles are considered and the (partial) ordering in Table 11 is taken for granted. I subject > dir-object > indir-object > complement(s) > adjunct(s) I Table 1: Grammatical Role Based Ranking on the C! ~Table 1 contains the most explicit ordering of grammatical roles we are aware of and has been taken from Brennan et al. (1987). Often, the distinction between complements and adjuncts is collapsed into the category ""others"" (c.f., e.g., Grosz et al. (1995)). Our work on the resolution of anaphora (Strube & Hahn, 1995; Hahn & Strube, 1996) and textual ellipsis (Hahn et al., 1996), however, is based on German, a free word order language, in which grammatical role information is far less predictive for the organization of centers. Rather, for establishing proper referential relations, the functional information structure of the utterances becomes crucial (different perspectives on functional analysis are brought forward in Dane~ (1974b) and Dahl (1974)). We share the notion of functional information structure as developed by Dane~ (1974a). He distinguishes between two crucial dichotomies, viz. given information vs. new information (constituting the information structure"
P96-1036,C92-1023,0,0.282948,"Missing"
P96-1036,P86-1031,0,0.841204,"English language, we considered all exampies from Grosz et al. (1995) and Brennan et al. (1987); for Japanese we took the data from Walker et al. (1994)). Surprisingly enough, all examples of Grosz et al. (1995) passed the test successfully. Only with respect to the troublesome Alfa Romeo driving scenario (cf. Brennan et al. (1987, p.157)) our constraints fail to properly rank the elements of the third sentence C! of that example. 7 Note also that these results were achieved without having recourse to extra constraints, e.g., the shared property constraint to account for anaphora parallelism (Kameyama, 1986). We applied our constraints to Japanese examples in the same way. Again we abandoned all extra constraints set up in these studies, e.g., the Zero Topic Assignment (ZTA) rule and the special role of empathy 7In essence, the very specific problem addressed by that example seems to be that Friedman has not been previously introduced in the local discourse segment and is only accessible via the global focus. CONTINUE - cheap cheap RETAIN expensive cheap CONTINUE RETAIN expensive expensive expensive SMOOTH-SHIFT cheap ROUGH-SHIFT expensive expensive SMOOTH-SHIFT ROUGH-SHIFT i expensNe cheap expen"
P96-1036,P96-1057,1,0.840409,"Rambow (1993) for German and Cote (1996) for English, only grammatical roles are considered and the (partial) ordering in Table 11 is taken for granted. I subject > dir-object > indir-object > complement(s) > adjunct(s) I Table 1: Grammatical Role Based Ranking on the C! ~Table 1 contains the most explicit ordering of grammatical roles we are aware of and has been taken from Brennan et al. (1987). Often, the distinction between complements and adjuncts is collapsed into the category ""others"" (c.f., e.g., Grosz et al. (1995)). Our work on the resolution of anaphora (Strube & Hahn, 1995; Hahn & Strube, 1996) and textual ellipsis (Hahn et al., 1996), however, is based on German, a free word order language, in which grammatical role information is far less predictive for the organization of centers. Rather, for establishing proper referential relations, the functional information structure of the utterances becomes crucial (different perspectives on functional analysis are brought forward in Dane~ (1974b) and Dahl (1974)). We share the notion of functional information structure as developed by Dane~ (1974a). He distinguishes between two crucial dichotomies, viz. given information vs. new informatio"
P96-1036,E95-1033,1,0.768722,", Turan (1995) for Turkish, Rambow (1993) for German and Cote (1996) for English, only grammatical roles are considered and the (partial) ordering in Table 11 is taken for granted. I subject > dir-object > indir-object > complement(s) > adjunct(s) I Table 1: Grammatical Role Based Ranking on the C! ~Table 1 contains the most explicit ordering of grammatical roles we are aware of and has been taken from Brennan et al. (1987). Often, the distinction between complements and adjuncts is collapsed into the category ""others"" (c.f., e.g., Grosz et al. (1995)). Our work on the resolution of anaphora (Strube & Hahn, 1995; Hahn & Strube, 1996) and textual ellipsis (Hahn et al., 1996), however, is based on German, a free word order language, in which grammatical role information is far less predictive for the organization of centers. Rather, for establishing proper referential relations, the functional information structure of the utterances becomes crucial (different perspectives on functional analysis are brought forward in Dane~ (1974b) and Dahl (1974)). We share the notion of functional information structure as developed by Dane~ (1974a). He distinguishes between two crucial dichotomies, viz. given informat"
P96-1036,J94-2006,0,0.878125,"Rotbuch Verlag, pp. 57-63. SA performance evaluation of the current anaphora and ellipsis resolution capacities of our system is reported in Hahn et al. (1996). 273 constraint that elliptical antecedents are ranked higher than elliptical expressions (short: ""ante > express""). For the evaluation of a centering algorithm on naturally occurring text it is necessary to specify how to deal with complex sentences. In particular, methods for the interaction between intra- and intersentential anaphora resolution have to be defined, since the centering model is concerned only with the latter case (see Suri & McCoy (1994)). We use an approach as described by Strube (1996) for the evaluation. Since most of the anaphors in these texts are nominal anaphors, the resolution of which is much more restricted than that of pronominal anaphors, the rate of success for the whole anaphora resolution process is not significant enough for a proper evaluation of the functional constraints. The reason for this lies in the fact that nominal anaphors are far more constrained by conceptual criteria than pronominal anaphors. So the chance to properly resolve a nominal anaphor, even at lower ranked positions in the center lists, i"
P96-1036,J94-2003,0,\N,Missing
P96-1057,P87-1022,0,0.738853,"Missing"
P96-1057,J95-2003,0,0.693432,"nded for the resolution of anaphora at the sentence level. However, the centering framework is not fully specified to handie complex sentences (Suri & McCoy, 1994). This underspocification corresponds to the lack of a precise definition of the expression utterance, a term always used but intentionally left undefined1. Therefore, the centering algorithms currently under discussion are not able to handle naturally occurring discourse. Possible strategies for treating sentence-level anaphora within the centering framework are 1. processing sentences linearly one clause at a time (as suggested by Grosz et al. (1995)), 2. preference for sentence-external antecedents which are proposed by the centering mechanism, 3. preference for sentence-internal antecedents which are filtered by the usual binding criteria, 4. a mixed-mode which prefers only a particular set of sentence-internal over sentence-external antecedents (e.g. Suri & McCoy (1994)). The question arises as to which strategy fits best for the interaction between the resolution of intra- and intersentential anaphora. In my contribution, evidence for a mixed-mode strategy is brought forward, which favors a particular set of sentence-internal antecede"
P96-1057,C92-1023,0,0.245816,"Missing"
P96-1057,J94-4002,0,0.276166,"Missing"
P96-1057,E95-1033,1,0.771321,"Missing"
P96-1057,J94-2006,0,0.398246,"d the centering model for the resolution of intia-sentential anaphora and specify how to handle complex sentences. An empirical evaluation indicates that the functional information structure guides the search for an antecedent within the sentence. 1 Introduction The centering model (Grosz et al., 1995) focuses on the resolution of inter-sentential anaphora. Since intra-sentential anaphora occur at high rates in realworld texts, the model has to be extended for the resolution of anaphora at the sentence level. However, the centering framework is not fully specified to handie complex sentences (Suri & McCoy, 1994). This underspocification corresponds to the lack of a precise definition of the expression utterance, a term always used but intentionally left undefined1. Therefore, the centering algorithms currently under discussion are not able to handle naturally occurring discourse. Possible strategies for treating sentence-level anaphora within the centering framework are 1. processing sentences linearly one clause at a time (as suggested by Grosz et al. (1995)), 2. preference for sentence-external antecedents which are proposed by the centering mechanism, 3. preference for sentence-internal antecedent"
P96-1057,P89-1031,0,0.40352,"Mflller, Geschichten aus der Produktion 2, Berlin: Rotbuch Verlag, pp.57-63. aWe do not consider dialogues with elliptical utterances. 379 3. Compute the C! (Un), considering only the elements of the matrix clause of Un. 3 Evaluation In order to evaluate the functional approach to the resolution of intra-sentential anaphora within the centering model, we compared it to the other approaches mentioned in Section 1, employing the test set referred to in Table 2. Note that we tried to eliminate error chaining and false positives (for some remarks on evaluating discourse processing algorithms, cf. Walker (1989); we consider her results as a starting point for our proposal). First, we examine the errors which all strategies have in common (for the success rate, cf. Table 4). 99 errors are caused by underspecification at different levels, e.g., prepositional anaphors (16), plural anaphors (8), anaphors which refer to a member of a set (14), sentence anaphors (21), and anaphors which refer to a global focus (12) are not yet included in the mechanism. In 9 cases, any strategy will choose the false antecedent. The most interesting cases are the ones for which the performance of the different strategies v"
P96-1057,P96-1036,1,\N,Missing
P97-1014,P87-1022,0,0.504615,"and incorporation of discourse structure beyond the level of immediately adjacent utterances within the centering framework. Two recent studies deal with this topic in order to relate attentional and intentional structures on a larger scale of global discourse coherence. Passonneau (1996) proposes an algorithm for the generation of referring expressions and Walker (1996a) integrates centering into a cache model of attentional state. Both studies, among other things, deal with the supposition whether a correlation exists between particular centering transitions (which were first introduced by Brennan et al. (1987); cf. Table 1) and intentionbased discourse segments. In particular, the role of SHIFT-type transitions is examined from the perspective of whether they not only indicate a shift of the topic between two immediately successive utterances but also signal (intention-based) segment boundaries. The data in both studies reveal that only a weak correlation between the SHIFT transitions and segment boundaries can be observed. This finding precludes a reliable prediction of segment boundaries based on the occurrence of 1Our notion of referentialdiscourse segment should not be confounded with the inten"
P97-1014,J95-2003,0,0.851237,"tation has on the validity of anaphora resolution (cf. Section 5 for a discussion of evaluation results). We specify an algorithm that builds up a hierarchy of referential discourse segments from local centering data. The spatial extension and nesting of these discourse segments constrain the reachability of potential antecedents of an anaphoric expression beyond the local level of adjacent center pairs. Thus, the centering model is scaled up to the level of the global referential structure of discourse. An empirical evaluation of the algorithm is supplied. 1 Introduction The centering model (Grosz et al., 1995) has evolved as a major methodology for computational discourse analysis. It provides simple, yet powerful data structures, constraints and rules for the local coherence of discourse. As far as anaphora resolution is concerned, e.g., the model requires to consider those discourse entities as potential antecedents for anaphoric expressions in the current utterance Ui, which are available in the forward-looking centers of the immediately preceding utterance Ui- 1. No constraints or rules are formulated, however, that account for anaphoric relationships which spread out over non-adjacent utteranc"
P97-1014,J86-3001,0,0.595101,"based discourse segments. In particular, the role of SHIFT-type transitions is examined from the perspective of whether they not only indicate a shift of the topic between two immediately successive utterances but also signal (intention-based) segment boundaries. The data in both studies reveal that only a weak correlation between the SHIFT transitions and segment boundaries can be observed. This finding precludes a reliable prediction of segment boundaries based on the occurrence of 1Our notion of referentialdiscourse segment should not be confounded with the intentional one originating from Grosz & Sidner (1986), for reasons discussed in Section 2. SHIFTS and vice versa. In order to accommodate to these empirical results divergent solutions are proposed. Passonneau suggests that the centering data structures need to be modified appropriately, while Walker concludes that the local centering data should be left as they are and further be complemented by a cache mechanism. She thus intends to extend the scope of centering in accordance with cognitively plausible limits of the attentional span. Walker, finally, claims that the content of the cache, rather than the intentional discourse segment structure,"
P97-1014,P94-1002,0,0.0171891,"h are competing models of the attentional state. Centered segmentation has also the additional advantage of restricting the search space of anaphoric antecedents to those discourse entities actually referred to in the discourse, while the cache model allows unrestricted retrieval in the main or long-term memory. Text segmentation procedures (more with an information retrieval motivation, rather than being related to reference resolution tasks) have also been proposed for a coarse-grained partitioning of texts into contiguous, nonoverlapping blocks and assigning content labels to these blocks (Hearst, 1994). The methodological basis of these studies are lexical cohesion indicators (Morris & Hirst, 1991) combined with word-level co-occurrence statistics. Since the labelling is one-dimensional, this approximates our use of preferred centers of discourse segments. These studies, however, lack the fine-grained information of the contents of Cf lists also needed for proper reference resolution. Finally, many studies on discourse segmentation highlight the role of cue words for signaling segment boundaries (cf., e.g., the discussion in Passonneau & Litman (1993)). However useful this strategy might be"
P97-1014,J91-1002,0,0.0120573,"onal advantage of restricting the search space of anaphoric antecedents to those discourse entities actually referred to in the discourse, while the cache model allows unrestricted retrieval in the main or long-term memory. Text segmentation procedures (more with an information retrieval motivation, rather than being related to reference resolution tasks) have also been proposed for a coarse-grained partitioning of texts into contiguous, nonoverlapping blocks and assigning content labels to these blocks (Hearst, 1994). The methodological basis of these studies are lexical cohesion indicators (Morris & Hirst, 1991) combined with word-level co-occurrence statistics. Since the labelling is one-dimensional, this approximates our use of preferred centers of discourse segments. These studies, however, lack the fine-grained information of the contents of Cf lists also needed for proper reference resolution. Finally, many studies on discourse segmentation highlight the role of cue words for signaling segment boundaries (cf., e.g., the discussion in Passonneau & Litman (1993)). However useful this strategy might be, we see the danger that such a surface-level description may actually hide structural regularitie"
P97-1014,P93-1020,0,0.10593,"blocks and assigning content labels to these blocks (Hearst, 1994). The methodological basis of these studies are lexical cohesion indicators (Morris & Hirst, 1991) combined with word-level co-occurrence statistics. Since the labelling is one-dimensional, this approximates our use of preferred centers of discourse segments. These studies, however, lack the fine-grained information of the contents of Cf lists also needed for proper reference resolution. Finally, many studies on discourse segmentation highlight the role of cue words for signaling segment boundaries (cf., e.g., the discussion in Passonneau & Litman (1993)). However useful this strategy might be, we see the danger that such a surface-level description may actually hide structural regularities at deeper levels of investigation illustrated by access mechanisms for centering data at different levels of discourse segmentation. 7 Acknowledgments. We like to thank our colleagues in the CLIF group for fruitful discussions and instant support, Joe Bush who polished the text as a native speaker, the three anonymous reviewers for their critical comments, and, in particular, Bonnie Webber for supplying invaluable comments to an earlier draft of this paper"
P97-1014,P95-1005,0,0.0394336,"Missing"
P97-1014,P96-1036,1,0.876436,"segment levels succeeds. (b) Close the embedded segment and open a new, parallel one: If none of the anaphoric expressions under consideration co-specify the IsReachable(ante, s, Ui ) if ante 6 C/(s, Ui-1) else if ante E C/(s - 1, Uosts_,.~,a]) else if (3v E N : ante =~tr Cp(v, UDsI.... a]) ^ v < ( s - 1)) A (-~Sv&apos; 6 N : ante A v < v&apos;) =,t,- else Cp(v&apos;,UDst~,.~ndl) Table 4: Reachability of the Anaphoric Antecedent Finally, the function Lift(s, i) (cf. Table 5) determines the appropriate discourse segment level, s, of an utter2The Cf lists in the functional centering model are totally ordered (Strobe & Hahn, 1996, p.272) and we here implicitly assume that they are accessed in the total order given. 106 C p ( 8 - 1, U[8_l.end]), then the entire C! at this segment level is checked for the given utterance. If an antecedent matches, the segment which contains Ui- 1 is ultimately closed, since Ui opens a parallel segment at the same level of embedding. Subsequent anaphora checks exclude any of the preceding parallel segments from the search for a valid antecedent and just visit the currently open one. (c) Open new, embedded segment: If there is no matching antecedent in hierarchically reachable segments, t"
P97-1014,P96-1000,0,0.602609,"nts on valid antecedents are placed by the global discourse structure previous utterances are embedded in. We want to emphasize from the beginning that our proposal considers only the referential properties underlying 104 2 Global Discourse Structure There have been only few attempts at dealing with the recognition and incorporation of discourse structure beyond the level of immediately adjacent utterances within the centering framework. Two recent studies deal with this topic in order to relate attentional and intentional structures on a larger scale of global discourse coherence. Passonneau (1996) proposes an algorithm for the generation of referring expressions and Walker (1996a) integrates centering into a cache model of attentional state. Both studies, among other things, deal with the supposition whether a correlation exists between particular centering transitions (which were first introduced by Brennan et al. (1987); cf. Table 1) and intentionbased discourse segments. In particular, the role of SHIFT-type transitions is examined from the perspective of whether they not only indicate a shift of the topic between two immediately successive utterances but also signal (intention-base"
P97-1014,J94-2006,0,0.270977,"could not find an embedding of more than seven levels. 6 Related Work There has always been an implicit relationship between the local perspective of centering and the global view of focusing on discourse structure (cf. the discussion in Grosz et al. (1995)). However, work establishing an explicit account of how both can be joined in a computational model has not been done so far. The efforts of Sidner (1983), e.g., have provided a variety of different focus data structures to be used for reference resolution. This multiplicity and the on-going growth of the number of different entities (cf. Suri & McCoy (1994)) mirrors an increase in explanatory constructs that we consider a methodological drawback to this approach because they can hardly be kept control of. Our model, due to its hierarchical nature implements a stack behavior that is also inherent to the above mentioned proposals. We refrain, however, from establishing a new data type (even worse, different types of stacks) that has to be managed on its own. There is no need for extra computations to determine the ""segment focus"", since that is implicitly given in the local centering data already available in our model. A recent attempt at introdu"
P97-1014,J96-2005,0,0.327174,"ormulated, however, that account for anaphoric relationships which spread out over non-adjacent utterances. Hence, it is unclear how discourse elements which appear in utterances preceding utterance Ui-1 are taken into consideration as potential antecedents for anaphoric expressions in Ui. The extension of the search space for antecedents is by no means a trivial enterprise. A simple linear backward search of all preceding centering structures, e.g., may not only turn out to establish illegal references but also contradicts the cognitive principles underlying the limited attention constraint (Walker, 1996b). The solution we propose starts from the observation that additional constraints on valid antecedents are placed by the global discourse structure previous utterances are embedded in. We want to emphasize from the beginning that our proposal considers only the referential properties underlying 104 2 Global Discourse Structure There have been only few attempts at dealing with the recognition and incorporation of discourse structure beyond the level of immediately adjacent utterances within the centering framework. Two recent studies deal with this topic in order to relate attentional and int"
P98-2204,P87-1022,0,0.98077,"and intra-sentential anaphora. The model is the basis for an algorithm which operates incrementally, word by word. immediately. Therefore, the S-list integrates in the simplest manner preferences for inter- and intrasentential anaphora, making further specifications for processing complex sentences unnecessary. Section 2 describes the centering model as the relevant background for my proposal. In Section 3, I introduce my model, its only data structure, the S-list, and the accompanying algorithm. In Section 4, I compare the results of my algorithm with the results of the centering algorithm (Brennan et al., 1987) with and without specifications for complex sentences (Kameyama, 1998). 1 2 Abstract Introduction I propose a model for determining the heater's attentional state in understanding discourse. My proposal is inspired by the centering model (Grosz et al., 1983; 1995) and draws on the conclusions of Strube & Hahn's (1996) approach for the ranking of the forward-looking center list for German. Their approach has been proven as the point of departure for a new model which is valid for English as well. The use of the centering transitions in Brennan et al.'s (1987) algorithm prevents it from being a"
P98-2204,P83-1007,0,0.80744,"ations for processing complex sentences unnecessary. Section 2 describes the centering model as the relevant background for my proposal. In Section 3, I introduce my model, its only data structure, the S-list, and the accompanying algorithm. In Section 4, I compare the results of my algorithm with the results of the centering algorithm (Brennan et al., 1987) with and without specifications for complex sentences (Kameyama, 1998). 1 2 Abstract Introduction I propose a model for determining the heater's attentional state in understanding discourse. My proposal is inspired by the centering model (Grosz et al., 1983; 1995) and draws on the conclusions of Strube & Hahn's (1996) approach for the ranking of the forward-looking center list for German. Their approach has been proven as the point of departure for a new model which is valid for English as well. The use of the centering transitions in Brennan et al.'s (1987) algorithm prevents it from being applied incrementally (cf. Kehler (1997)). In my approach, I propose to replace the functions of the backward-looking center and the centering transitions by the order among the elements of the list of salient discourse entities (S-list). The S-list ranking c"
P98-2204,J95-2003,0,0.155384,"d a few rules and constraints. Each utterance Ui is assigned a list of forward-looking centers, C f (Ui), and a unique backward-looking center, Cb(Ui). A ranking imposed on the elements of the C f reflects the assumption that the most highly ranked element of C f (Ui) (the preferred center Cp(Ui)) is most likely to be the Cb(Ui+l). The most highly ranked element of Cf(Ui) that is realized in Ui+x (i.e., is associated with an expression that has a valid interpretation in the underlying semantic representation) is the Cb(Ui+l). Therefore, the ranking on the Cf plays a crucial role in the model. Grosz et al. (1995) and Brennan et al. (1987) use grammatical relations to rank the Cf (i.e., subj -.< obj -< ...) but state that other factors might also play a role. For their centering algorithm, Brennan et al. (1987, henceforth BFP-algorithm) extend the notion of centering transition relations, which hold across adjacent utterances, to differentiate types of shift (cf. Table 1 taken from Walker et al. (1994)). Cb(Ui) = Cb(Ui-1) OR no Cb(Ui-1) Cb(Ui) = Cp(Vi) Cb(Ui) y£ Cp(t:i) CONTINUE Cb(Ui) Cb(Vi-1) SMOOTH-SHIFT RETAIN ROUGH-SHIFT Table 1: Transition Types Brennan et al. (1987) modify the second of two rule"
P98-2204,C92-1023,0,0.118417,"Missing"
P98-2204,J97-3006,0,0.332077,"ations for complex sentences (Kameyama, 1998). 1 2 Abstract Introduction I propose a model for determining the heater's attentional state in understanding discourse. My proposal is inspired by the centering model (Grosz et al., 1983; 1995) and draws on the conclusions of Strube & Hahn's (1996) approach for the ranking of the forward-looking center list for German. Their approach has been proven as the point of departure for a new model which is valid for English as well. The use of the centering transitions in Brennan et al.'s (1987) algorithm prevents it from being applied incrementally (cf. Kehler (1997)). In my approach, I propose to replace the functions of the backward-looking center and the centering transitions by the order among the elements of the list of salient discourse entities (S-list). The S-list ranking criteria define a preference for hearer-old over hearer-new discourse entities (Prince, 1981) generalizing Strube & Hahn's (1996) approach. Because of these ranking criteria, I can account for the difference in salience between definite NPs (mostly hearer-old) and indefinite NPs (mostly hearer-new). The S-list is not a local data structure associated with individual utterances. T"
P98-2204,P96-1036,1,0.76186,"l) discourse entities which are realized in the current and the previous utterance. • The elements of the S-list are ranked according to their information status. The order among the elements provides directly the preference for the interpretation of anaphoric expressions. In contrast to the centering model, my model does not need a construct which looks back; it does not need transitions and transition ranking criteria. Instead of using the Cb to account for local coherence, in my model this is achieved by comparing the first element of the S-list with the preceding state. 3.2 S-List Ranking Strube & Hahn (1996) rank the Cfaccording to the information status of discourse entities. I here generalize these ranking criteria by redefining them in Prince's (1981; 1992) terms. I distinguish between three different sets of expressions, hearer-old discourse entities (OLD), mediated discourse entities (MED), and hearer-new discourse entities (NEW). These sets consist of the elements of Prince's familiarity scale (Prince, 1981, p.245). OLD consists of evoked (E) and unused (U) discourse entities while NEW consists of brand-new (BN) discourse entities. MED consists of inferrables (I), containing inferrables (I"
P98-2204,J94-2006,0,0.271943,"ntential preferences for anaphoric antecedents. Kameyama's specifications reduce the complexity in that the Cf-lists in general are shorter after splitting up a sentence into clauses. Therefore, the 1256 5 Comparison to Related Approaches Kameyama's (1998) version of centering also omits the centering transitions. But she uses the Cb and a ranking over simplified transitions preventing the incremental application of her model. l°In: Emest Hemingway. Up in Michigan. ln. The Complete Short Stories of Ernest Hemingway. New York: Charles Scribner's Sons, 1987, p.60. The focus model (Sidner, 1983; Suri & McCoy, 1994) accounts for evoked discourse entities explicitly because it uses the discourse focus, which is determined by a successful anaphora resolution. Incremental processing is not a topic of these papers. Even models which use salience measures for determining the antecedents of pronoun use the concept of evoked discourse entities. Haji~ov~i et al. (1992) assign the highest value to an evoked discourse entity. Also Lappin & Leass (1994), who give the subject of the current sentence the highest weight, have an implicit notion of evokedness. The salience weight degrades from one sentence to another b"
P98-2204,P89-1031,0,0.639854,"inal case, said last week S: [SMIRGAE: attorney, CASEE: case, CURTISE: him, CS COURTu: CS Court, JUDGEE: judge ] that he had doubts about the psychiatric reports that said Mr. Curtis would never improve. S: [SMIRGAE: he, CASEE: case, REPORTSE: reports, CURTISE: Mr. Curtis, DOUBTSBN: doubts] Table 5: Analysis for (3) 4 Some Empirical Dat:i In the first experiment, I compare my algorithm with the BFP-algorithm which was in a second experiment extended by the constraints for complex sentences as described by Kameyama (1998). Method. I use the following guidelines for the hand-simulated analysis (Walker, 1989). I do not assume any world knowledge as part of the anaphora resolution process. Only agreement criteria, binding and sortal constraints are applied. I do not account for false positives and error chains. Following Walker (1989), a segment is defined as a paragraph unless its first sentence has a pronoun in subject position or a pronoun where none of the preceding sentence-internal noun phrases matches its syntactic features. At the beginning of a segment, anaphora resolution is preferentially performed within the same utterance. My algorithm starts with an empty S-list at the beginning of a"
P98-2204,J94-2003,0,0.146583,"Ui+x (i.e., is associated with an expression that has a valid interpretation in the underlying semantic representation) is the Cb(Ui+l). Therefore, the ranking on the Cf plays a crucial role in the model. Grosz et al. (1995) and Brennan et al. (1987) use grammatical relations to rank the Cf (i.e., subj -.< obj -< ...) but state that other factors might also play a role. For their centering algorithm, Brennan et al. (1987, henceforth BFP-algorithm) extend the notion of centering transition relations, which hold across adjacent utterances, to differentiate types of shift (cf. Table 1 taken from Walker et al. (1994)). Cb(Ui) = Cb(Ui-1) OR no Cb(Ui-1) Cb(Ui) = Cp(Vi) Cb(Ui) y£ Cp(t:i) CONTINUE Cb(Ui) Cb(Vi-1) SMOOTH-SHIFT RETAIN ROUGH-SHIFT Table 1: Transition Types Brennan et al. (1987) modify the second of two rules on center movement and realization which were defined by Grosz et al. (1983; 1995): Rule 1: If some element of Cf(Ui-1) is realized as a pronoun in Ui, then so is Cb(Ui). Rule 2"" Transition states are ordered. CONTINUE is preferred to RETAIN is preferred to SMOOTHSHIFT is preferred to ROUGH-SHIFT. The BFP-algorithm (cf. Walker et al. (1994)) consists of three basic steps: 1. G E N E R A T E"
P98-2204,J94-4002,0,\N,Missing
Q15-1029,D08-1031,0,0.322513,"Mx , where m0 precedes every mi ∈ Mx (Chang et al., 2012; Fernandes et al., 2014). m0 plays the role of a dummy mention for anaphoricity detection: if m0 is chosen as the antecedent, the corresponding mention is deemed as non-anaphoric. This enables joint coreference resolution and anaphoricity determination. 3.3 The Latent Space Hx for an Input x Let x ∈ X be some document. As we saw in the previous section, approaches to coreference resolution predict a latent structure which is not annotated in the data but is used to infer coreference information. Inspired by previous work on coreference (Bengtson and Roth, 2008; Fernandes et al., 2014; Martschat and Strube, 2014), we now develop a graph-based representation for these structures. A valid latent structure for the document x is a labeled directed graph h = (V, A, LA ) where • the set of nodes are the mentions, V = Mx0 , • the set of edges A consists of links between mentions pointing back in the text, A ⊆ {(mj , mi ) |j &gt; i} ⊆ Mx × Mx0 . • LA : A → L assigns a label ` ∈ L to each edge. L is a finite set of labels, for example signaling coreference or non-coreference. We split h into subgraphs (called substructures from now on), which we notate as h = h"
Q15-1029,P06-1005,0,0.0529827,". The shared task organizers provide the training/development/ test split. We use the 2802 training documents for training the models, and evaluate and analyze the models on the development set containing 343 documents. The 349 test set documents are only used for final evaluation. We work in a setting that corresponds to the shared task’s closed track (Pradhan et al., 2012). That is, we make use of the automatically created annotation layers (parse trees, NE information, ...) shipped with the data. As additional resources we use only WordNet 3.0 (Fellbaum, 1998) and the number/gender data of Bergsma and Lin (2006). For evaluation we follow the practice of the CoNLL-2012 shared task and employ the reference implementation of the CoNLL scorer (Pradhan et al., 2014) which computes the popular evaluation metrics MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998), CEAFe (Luo, 2005) and their average. The average is the metric for ranking the systems in the CoNLL shared tasks on coreference resolution (Pradhan et al., 2011; Pradhan et al., 2012). 5.2 Features We employ a rich set of features frequently used in the literature (Ng and Cardie, 2002; Bengtson and Roth, 2008; Bj¨orkelund and Kuhn, 2014). The"
Q15-1029,P14-1005,0,0.559924,"Missing"
Q15-1029,C10-1017,1,0.665587,"by predicting whether sets of mentions are coreferent, going beyond pairwise predictions. While a detailed discussion of such approaches is beyond the scope of this paper, we now briefly describe how we can generalize the proposed framework to accommodate for such approaches. When viewing coreference resolution as prediction of latent structures, entity-based models operate on structures that relate sets of mentions to each other. This can be expressed by hypergraphs, which are graphs where edges can link more than two nodes. Hypergraphs have already been used to model coreference resolution (Cai and Strube, 2010; Sapena, 2012). To model entity-based approaches, we extend the valid latent structures to labeled directed hypergraphs. These are tuples h = (V, A, LA ), where • the set of nodes are the mentions, V = Mx0 , • the set of edges A ⊆ 2V × 2V consists of directed hyperedges linking two sets of mentions, • LA : A → L assigns a label ` ∈ L to each edge. L is a finite set of labels. For example, the entity-mention model (Yang et al., 2008) predicts coreference in a left-to-right fashion. For each anaphor mj , it considers the set Ej ⊆ 2{m0 ,...,mj−1 } of preceding partial entities that have been est"
Q15-1029,W12-4513,0,0.0632993,"cture to performance, while fixing parameters such as preprocessing and features. In particular, we analyze approaches to coreference resolution and point out that they mainly differ in the structures they operate on. We then note that these structures are not annotated in the training data (Section 2). Motivated by this observation, we develop a machine learning framework for structured prediction with latent variables for coreference resolution (Section 3). We formalize the mention pair model (Soon et al., 2001; Ng and Cardie, 2002), mention ranking architectures (Denis and Baldridge, 2008; Chang et al., 2012) and antecedent trees (Fernandes et al., 2014) in our framework and highlight key differences and similarities (Section 4). Finally, we present an extensive comparison and analysis of the implemented approaches, both quantitative and qualitative (Sections 5 and 6). Our analysis shows that a mention ranking architecture with latent antecedents performs best, mainly due to its ability to structurally model determining anaphoricity. Finally, we briefly describe how entity-centric approaches fit into our framework (Section 7). An open source toolkit which implements the machine learning framework"
Q15-1029,W02-1001,0,0.116192,"08; Chang et al., 2012) and antecedent trees (Yu and Joachims, 2009; Fernandes et al., 2014; Bj¨orkelund and Kuhn, 2014). We characterize each approach by the latent structure it operates on during learning and inference (we assume that all approaches we consider share the same features). Furthermore, we also discuss the factorization into substructures and typical cost functions used in the literature. 4.1 Mention Pair Model We first consider the mention pair model. In its original formulation, it extracts mention pairs from the 2 We also shuffle the data before each epoch and use averaging (Collins, 2002). data and labels these as positive or negative. During testing, all pairs are extracted and some clustering algorithm such as closest-first or best-first is applied to the list of pairs. During training, some heuristic is applied to help balancing positive and negative examples. The most popular heuristic is to take the closest antecedent of an anaphor as a positive example, and all pairs in between as negative examples. Latent Structure. In our framework, we can represent the mention pair model as a labeled graph. In particular, let the set of edges be all backwardpointing edges, i.e. A = {("
Q15-1029,N07-1011,0,0.011702,"ramework generalizes approaches to coreference resolution which employed specific latent structures for representation, such as latent antecedents (Chang et al., 2012) and antecedent trees (Fernandes et al., 2014). We give a unified representation of such approaches and show that seemingly disparate approaches such as the mention pair model also fit in a framework based on latent structures. Only few studies systematically compare approaches to coreference resolution. Most previous work highlights the improved expressive power of the presented model by a comparison to a mention pair baseline (Culotta et al., 2007; Denis and Baldridge, 2008; Cai and Strube, 2010). Rahman and Ng (2011) consider a series of models with increasing expressiveness, ranging from a mention pair to a cluster-ranking model. However, they do not develop a unified framework for comparing approaches, and their analysis is not qualitative. Fernandes et al. (2014) compare variations of antecedent tree models, including different loss functions and a version with a fixed structure. They only consider antecedent trees and also do not provide a qualitative analysis. Kummerfeld and Klein (2013) and Martschat and Strube (2014) present a"
Q15-1029,D08-1069,0,0.1245,"tion of the underlying structure to performance, while fixing parameters such as preprocessing and features. In particular, we analyze approaches to coreference resolution and point out that they mainly differ in the structures they operate on. We then note that these structures are not annotated in the training data (Section 2). Motivated by this observation, we develop a machine learning framework for structured prediction with latent variables for coreference resolution (Section 3). We formalize the mention pair model (Soon et al., 2001; Ng and Cardie, 2002), mention ranking architectures (Denis and Baldridge, 2008; Chang et al., 2012) and antecedent trees (Fernandes et al., 2014) in our framework and highlight key differences and similarities (Section 4). Finally, we present an extensive comparison and analysis of the implemented approaches, both quantitative and qualitative (Sections 5 and 6). Our analysis shows that a mention ranking architecture with latent antecedents performs best, mainly due to its ability to structurally model determining anaphoricity. Finally, we briefly describe how entity-centric approaches fit into our framework (Section 7). An open source toolkit which implements the machin"
Q15-1029,D13-1203,0,0.808508,"of the mention ranking approach is that it considers each anaphor in isolation, but all candidate antecedents at once. We therefore define substructures as follows. The jth substructure is the graph hj with nodes Vj = {m0 , . . . , mj } and Aj = {(mj , mi ) |there is i with j &gt; i s.t. (mj , mi ) ∈ A}. Aj contains the antecedent decision for mj . One such substructure encoding the antecedent decision for m3 is colored black in Figure 2. Cost Function. Cost functions for the mention ranking model can reward the resolution of specific classes. The most sophisticated cost function was proposed by Durrett and Klein (2013), who distinguish between three errors: finding an antecedent for a non-anaphoric mention, misclassifying an anaphoric mention as non-anaphoric, and finding a wrong antecedent for an anaphoric mention. We will use a variant of this cost function in our experiments (described in Section 5.3). 4.3 Antecedent Trees Finally, we consider antecedent trees. This structure encodes all antecedent decisions for all anaphors. In our framework they can be understood as an extension of the mention ranking approach to the document level. So far, research did not investigate constraints on the space of laten"
Q15-1029,Q14-1037,0,0.0291207,"CoNLL’12 English development and test data. In order to put the numbers into context, we also report the results of Bj¨orkelund and Kuhn (2014), who present a system that implements an antecedent tree model with non-local features. Their system is the highestperforming system on the CoNLL data which operates in a closed track setting. We also compare with Fernandes et al. (2014), the winning system of the CoNLL-2012 shared task (Pradhan et al., 2012)3 . Both systems were trained on training data for evaluating on the development set, and on the concatena3 We do not compare with the system of Durrett and Klein (2014) since it uses Wikipedia as an additional resource, and therefore does not work under the closed track setting. Its performance is 61.71 average F1 (71.24 MUC F1 , 58.71 B3 F1 and 55.18 CEAFe F1 ) on CoNLL-2012 English test data. 412 tion of training and development data for evaluating on the test set. Despite its simplicity, the mention pair model yields reasonable performance. The gap to Bj¨orkelund and Kuhn (2014) is roughly 2.8 points in average F1 score on test data. Compared to the mention pair model, the variants of the mention ranking model improve the results for all metrics, largely"
Q15-1029,P08-2012,0,0.0100924,"nput x ∈ X and a weight vector θ ∈ Rd , we obtain the prediction by solving the arg max equation described in the previous subsection. This can be viewed as searching the output space Hx ×Zx for the highest scoring output pair (h, z). The details of the search procedure depend on the space Hx of latent structures and the factorization into substructures. For the structures we consider in this paper, the maximization can be solved exactly via greedy search. For structures with complex constraints like transitivity, more complex or even approximate search methods need to be used (Klenner, 2007; Finkel and Manning, 2008). 3.7 Learning We assume a supervised learning setting with latent variables, i.e., we have a training set of documents n  o D= x(i) , z (i) |i = 1, . . . , m at our disposal. Note that the latent structures are not encoded in this training set. In principle we would like to directly optimize for the evaluation metric we are interested in. Unfortunately, the evaluation metrics used in coreference do not allow for efficient optimization based on mention pairs, since they operate on the entity level. For example, the CEAFe metric (Luo, 2005) needs to compute optimal entity alignments between g"
Q15-1029,W01-1812,0,0.0203385,"; Lappin and Leass, 1994) to increasingly sophisticated machine learning models. While early approaches cast the problem as binary classification of mention pairs (Soon et al., 2001), recent approaches make use of complex structures to represent coreference relations (Yu and Joachims, 2009; Fernandes et al., 2014). The aim of this paper is to devise a framework for coreference resolution that leads to a unified representation of different approaches to coreference resolution in terms of the structure they operate on. Previous work in other areas of natural language processing such as parsing (Klein and Manning, 2001) and machine translation (Lopez, 2009) has shown that providing unified representations of approaches to a problem deepens its understanding and can also lead to empirical improvements. By implementing popular approaches in this framework, we can highlight structural differences and similarities between them. Furthermore, this establishes a setting to systematically analyze the contribution of the underlying structure to performance, while fixing parameters such as preprocessing and features. In particular, we analyze approaches to coreference resolution and point out that they mainly differ i"
Q15-1029,D13-1027,0,0.0250111,"model by a comparison to a mention pair baseline (Culotta et al., 2007; Denis and Baldridge, 2008; Cai and Strube, 2010). Rahman and Ng (2011) consider a series of models with increasing expressiveness, ranging from a mention pair to a cluster-ranking model. However, they do not develop a unified framework for comparing approaches, and their analysis is not qualitative. Fernandes et al. (2014) compare variations of antecedent tree models, including different loss functions and a version with a fixed structure. They only consider antecedent trees and also do not provide a qualitative analysis. Kummerfeld and Klein (2013) and Martschat and Strube (2014) present a largescale qualitative comparison of coreference systems, but they do not investigate the influence of the latent structures the systems operate on. Furthermore, the systems in their studies differ in terms of mention extraction and feature sets. 416 9 Conclusions We observed that many approaches to coreference resolution can be uniformly represented by the latent structure they operate on. We devised a framework that accounts for such structures, and showed how we can express the mention pair model, the mention ranking model and antecedent trees in t"
Q15-1029,J94-4002,0,0.431484,"esentation of different approaches to coreference resolution in terms of the structure they operate on. We represent several coreference resolution approaches proposed in the literature in our framework and evaluate their performance. Finally, we conduct a systematic analysis of the output of these approaches, highlighting differences and similarities. 1 Introduction Coreference resolution is the task of determining which mentions in a text are used to refer to the same real-world entity. The era of statistical natural language processing saw the shift from rule-based approaches (Hobbs, 1976; Lappin and Leass, 1994) to increasingly sophisticated machine learning models. While early approaches cast the problem as binary classification of mention pairs (Soon et al., 2001), recent approaches make use of complex structures to represent coreference relations (Yu and Joachims, 2009; Fernandes et al., 2014). The aim of this paper is to devise a framework for coreference resolution that leads to a unified representation of different approaches to coreference resolution in terms of the structure they operate on. Previous work in other areas of natural language processing such as parsing (Klein and Manning, 2001)"
Q15-1029,J13-4004,0,0.077139,"e strategy, employing antecedent trees leads to 415 a more precision-oriented approach, which significantly improves precision at the expense of recall. 7 Beyond Pairwise Predictions In this paper we concentrated on representing and analyzing the most prevalent approaches to coreference resolution, which are based on predicting whether pairs of mentions are coreferent. Hence, we choose graphs as latent structures and let the feature functions factor over edges in the graph, which correspond to pairs of mentions. However, entity-based approaches (Rahman and Ng, 2011; Stoyanov and Eisner, 2012; Lee et al., 2013, inter alia) obtain coreference chains by predicting whether sets of mentions are coreferent, going beyond pairwise predictions. While a detailed discussion of such approaches is beyond the scope of this paper, we now briefly describe how we can generalize the proposed framework to accommodate for such approaches. When viewing coreference resolution as prediction of latent structures, entity-based models operate on structures that relate sets of mentions to each other. This can be expressed by hypergraphs, which are graphs where edges can link more than two nodes. Hypergraphs have already bee"
Q15-1029,E09-1061,0,0.0247189,"cated machine learning models. While early approaches cast the problem as binary classification of mention pairs (Soon et al., 2001), recent approaches make use of complex structures to represent coreference relations (Yu and Joachims, 2009; Fernandes et al., 2014). The aim of this paper is to devise a framework for coreference resolution that leads to a unified representation of different approaches to coreference resolution in terms of the structure they operate on. Previous work in other areas of natural language processing such as parsing (Klein and Manning, 2001) and machine translation (Lopez, 2009) has shown that providing unified representations of approaches to a problem deepens its understanding and can also lead to empirical improvements. By implementing popular approaches in this framework, we can highlight structural differences and similarities between them. Furthermore, this establishes a setting to systematically analyze the contribution of the underlying structure to performance, while fixing parameters such as preprocessing and features. In particular, we analyze approaches to coreference resolution and point out that they mainly differ in the structures they operate on. We t"
Q15-1029,H05-1004,0,0.889789,"thods need to be used (Klenner, 2007; Finkel and Manning, 2008). 3.7 Learning We assume a supervised learning setting with latent variables, i.e., we have a training set of documents n  o D= x(i) , z (i) |i = 1, . . . , m at our disposal. Note that the latent structures are not encoded in this training set. In principle we would like to directly optimize for the evaluation metric we are interested in. Unfortunately, the evaluation metrics used in coreference do not allow for efficient optimization based on mention pairs, since they operate on the entity level. For example, the CEAFe metric (Luo, 2005) needs to compute optimal entity alignments between gold and system entities. These alignments do not factor with respect to mention pairs. We therefore have to use some surrogate loss. Algorithm 1 Structured latent perceptron with costaugmented inference. Input: Training set D, a cost function c, number of epochs n. function P ERCEPTRON(D, c, n) set θ = (0, . . . , 0) for epoch = 1, . . . , n do for (x, z) ∈ D do for each substructure do ˆ opt,i = arg max hθ, φ(x, hi , z)i h hi ∈const(Hx,z,i ) ˆ i , zˆ) = (h arg max (hθ, φ(x, hi , z)i (hi ,z)∈Hx,i ×Zx ˆ opt,i , z)) + c(x, hi , h ˆ i does not"
Q15-1029,D14-1221,1,0.836443,"., 2012; Fernandes et al., 2014). m0 plays the role of a dummy mention for anaphoricity detection: if m0 is chosen as the antecedent, the corresponding mention is deemed as non-anaphoric. This enables joint coreference resolution and anaphoricity determination. 3.3 The Latent Space Hx for an Input x Let x ∈ X be some document. As we saw in the previous section, approaches to coreference resolution predict a latent structure which is not annotated in the data but is used to infer coreference information. Inspired by previous work on coreference (Bengtson and Roth, 2008; Fernandes et al., 2014; Martschat and Strube, 2014), we now develop a graph-based representation for these structures. A valid latent structure for the document x is a labeled directed graph h = (V, A, LA ) where • the set of nodes are the mentions, V = Mx0 , • the set of edges A consists of links between mentions pointing back in the text, A ⊆ {(mj , mi ) |j &gt; i} ⊆ Mx × Mx0 . • LA : A → L assigns a label ` ∈ L to each edge. L is a finite set of labels, for example signaling coreference or non-coreference. We split h into subgraphs (called substructures from now on), which we notate as h = h1 ⊕. . .⊕hn , with hi = (Vi , Ai , LAi ) ∈ Hx,i , whe"
Q15-1029,P02-1014,0,0.871848,"shes a setting to systematically analyze the contribution of the underlying structure to performance, while fixing parameters such as preprocessing and features. In particular, we analyze approaches to coreference resolution and point out that they mainly differ in the structures they operate on. We then note that these structures are not annotated in the training data (Section 2). Motivated by this observation, we develop a machine learning framework for structured prediction with latent variables for coreference resolution (Section 3). We formalize the mention pair model (Soon et al., 2001; Ng and Cardie, 2002), mention ranking architectures (Denis and Baldridge, 2008; Chang et al., 2012) and antecedent trees (Fernandes et al., 2014) in our framework and highlight key differences and similarities (Section 4). Finally, we present an extensive comparison and analysis of the implemented approaches, both quantitative and qualitative (Sections 5 and 6). Our analysis shows that a mention ranking architecture with latent antecedents performs best, mainly due to its ability to structurally model determining anaphoricity. Finally, we briefly describe how entity-centric approaches fit into our framework (Sect"
Q15-1029,W11-1901,0,0.0278227,"ly created annotation layers (parse trees, NE information, ...) shipped with the data. As additional resources we use only WordNet 3.0 (Fellbaum, 1998) and the number/gender data of Bergsma and Lin (2006). For evaluation we follow the practice of the CoNLL-2012 shared task and employ the reference implementation of the CoNLL scorer (Pradhan et al., 2014) which computes the popular evaluation metrics MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998), CEAFe (Luo, 2005) and their average. The average is the metric for ranking the systems in the CoNLL shared tasks on coreference resolution (Pradhan et al., 2011; Pradhan et al., 2012). 5.2 Features We employ a rich set of features frequently used in the literature (Ng and Cardie, 2002; Bengtson and Roth, 2008; Bj¨orkelund and Kuhn, 2014). The set consists of the following features: • the mention type (name, def. noun, indef. noun, citation form of pronoun, demonstrative) of anaphor, antecedent and both, • gender, number, semantic class, named entity class, grammatical function and length in words of anaphor, antecedent and both, • semantic head, first/last/preceding/next token of anaphor, antecedent and both, • distance between anaphor and antecedent"
Q15-1029,W12-4501,0,0.695264,"rison with a qualitative analysis of the influence of the structures on the output. 5.1 Data and Evaluation Metrics The aim of our evaluation is to assess the effectiveness and competitiveness of the models implemented in our framework in a realistic coreference setting, i.e. without using gold information such as 410 gold mentions. As all models we consider share the same preprocessing and features, this allows for a fair comparison of the individual structures. We train, evaluate and analyze the models on the English data of the CoNLL-2012 shared task on multilingual coreference resolution (Pradhan et al., 2012). The shared task organizers provide the training/development/ test split. We use the 2802 training documents for training the models, and evaluate and analyze the models on the development set containing 343 documents. The 349 test set documents are only used for final evaluation. We work in a setting that corresponds to the shared task’s closed track (Pradhan et al., 2012). That is, we make use of the automatically created annotation layers (parse trees, NE information, ...) shipped with the data. As additional resources we use only WordNet 3.0 (Fellbaum, 1998) and the number/gender data of"
Q15-1029,P14-2006,1,0.689439,"est sets. When evaluating on the test set, we train on the concatenation of the training and development set. After preliminary experiments with the ranking model with closest antecedents on the development set, we set the number of perceptron epochs to 5 and set λ = 100 in the cost function. We assess statistical significance of the difference in F1 score for two approaches via an approximate randomization test (Noreen, 1989). We say an improvement is statistically significant if p < 0.05. MUC Model R P B3 F1 R P CEAFe F1 R P F1 Average F1 CoNLL-2012 English development data Fernandes et al. (2014) Bj¨orkelund and Kuhn (2014) 64.88 68.58 74.74 73.04 69.46 70.74 51.85 57.97 65.35 62.28 57.83 60.03 51.50 54.57 57.72 59.23 54.43 56.80 60.57 62.52 Mention Pair Ranking: Closest Ranking: Latent Antecedent Trees 66.68 67.85 68.02 65.91 71.71 76.66 76.73 77.92 69.10 71.99∗ 72.11× 71.41 53.57 55.33 55.61 52.72 62.44 65.45 66.91 67.98 57.67 59.97∗ 60.74† 59.39 52.56 53.16 54.48 52.13 53.87 61.28 61.36 60.82 53.21 56.93∗ 57.72†× 56.14 59.99 62.96 63.52 62.31 CoNLL-2012 English test data Fernandes et al. (2014) Bj¨orkelund and Kuhn (2014) 65.83 67.46 75.91 74.30 70.51 70.72 51.55 54.96 65.19 62."
Q15-1029,J01-4004,0,0.652698,"posed in the literature in our framework and evaluate their performance. Finally, we conduct a systematic analysis of the output of these approaches, highlighting differences and similarities. 1 Introduction Coreference resolution is the task of determining which mentions in a text are used to refer to the same real-world entity. The era of statistical natural language processing saw the shift from rule-based approaches (Hobbs, 1976; Lappin and Leass, 1994) to increasingly sophisticated machine learning models. While early approaches cast the problem as binary classification of mention pairs (Soon et al., 2001), recent approaches make use of complex structures to represent coreference relations (Yu and Joachims, 2009; Fernandes et al., 2014). The aim of this paper is to devise a framework for coreference resolution that leads to a unified representation of different approaches to coreference resolution in terms of the structure they operate on. Previous work in other areas of natural language processing such as parsing (Klein and Manning, 2001) and machine translation (Lopez, 2009) has shown that providing unified representations of approaches to a problem deepens its understanding and can also lead"
Q15-1029,C12-1154,0,0.0498386,"ult cases. Due to the update strategy, employing antecedent trees leads to 415 a more precision-oriented approach, which significantly improves precision at the expense of recall. 7 Beyond Pairwise Predictions In this paper we concentrated on representing and analyzing the most prevalent approaches to coreference resolution, which are based on predicting whether pairs of mentions are coreferent. Hence, we choose graphs as latent structures and let the feature functions factor over edges in the graph, which correspond to pairs of mentions. However, entity-based approaches (Rahman and Ng, 2011; Stoyanov and Eisner, 2012; Lee et al., 2013, inter alia) obtain coreference chains by predicting whether sets of mentions are coreferent, going beyond pairwise predictions. While a detailed discussion of such approaches is beyond the scope of this paper, we now briefly describe how we can generalize the proposed framework to accommodate for such approaches. When viewing coreference resolution as prediction of latent structures, entity-based models operate on structures that relate sets of mentions to each other. This can be expressed by hypergraphs, which are graphs where edges can link more than two nodes. Hypergraph"
Q15-1029,P09-1074,0,0.0217029,"ng err. corr. err. corr. err. corr. err. corr. err. corr. err. corr. err. corr. 885 587 640 595 2673 2620 2664 2628 83 93 92 57 79 96 102 82 1055 494 567 442 1098 960 1038 924 836 873 862 836 2479 2521 2461 2398 289 324 318 318 1546 1692 1692 1691 864 844 835 757 1408 1510 1594 1557 175 121 42 37 115 97 43 36 Table 4: Precision errors (err.) and correct links (corr.) of model variants on CoNLL-2012 English development data. gate error classes, and compare the models in terms of how they handle these error classes. This is a practice common in the analysis of coreference resolution approaches (Stoyanov et al., 2009; Martschat and Strube, 2014). We distinguish between errors where both mentions are a proper name or a common noun, errors where the anaphor is a pronoun and the remaining errors. Tables 3 and 4 summarize recall and precision errors for subcategories of these classes6 . We now compare individual models. 6.2 Mention Ranking vs. Mention Pair For pairs of proper names and pairs of common nouns, employing the ranking model instead of the mention pair model leads to a large decrease in precision errors, but an increase in recall errors. For pronouns and mixed pairs, we can observe decreases in rec"
Q15-1029,M95-1005,0,0.906017,"9 test set documents are only used for final evaluation. We work in a setting that corresponds to the shared task’s closed track (Pradhan et al., 2012). That is, we make use of the automatically created annotation layers (parse trees, NE information, ...) shipped with the data. As additional resources we use only WordNet 3.0 (Fellbaum, 1998) and the number/gender data of Bergsma and Lin (2006). For evaluation we follow the practice of the CoNLL-2012 shared task and employ the reference implementation of the CoNLL scorer (Pradhan et al., 2014) which computes the popular evaluation metrics MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998), CEAFe (Luo, 2005) and their average. The average is the metric for ranking the systems in the CoNLL shared tasks on coreference resolution (Pradhan et al., 2011; Pradhan et al., 2012). 5.2 Features We employ a rich set of features frequently used in the literature (Ng and Cardie, 2002; Bengtson and Roth, 2008; Bj¨orkelund and Kuhn, 2014). The set consists of the following features: • the mention type (name, def. noun, indef. noun, citation form of pronoun, demonstrative) of anaphor, antecedent and both, • gender, number, semantic class, named entity class, gramm"
Q15-1029,P08-1096,0,0.0773436,"can be expressed by hypergraphs, which are graphs where edges can link more than two nodes. Hypergraphs have already been used to model coreference resolution (Cai and Strube, 2010; Sapena, 2012). To model entity-based approaches, we extend the valid latent structures to labeled directed hypergraphs. These are tuples h = (V, A, LA ), where • the set of nodes are the mentions, V = Mx0 , • the set of edges A ⊆ 2V × 2V consists of directed hyperedges linking two sets of mentions, • LA : A → L assigns a label ` ∈ L to each edge. L is a finite set of labels. For example, the entity-mention model (Yang et al., 2008) predicts coreference in a left-to-right fashion. For each anaphor mj , it considers the set Ej ⊆ 2{m0 ,...,mj−1 } of preceding partial entities that have been established so far (such as e = {m1 , m3 , m6 }). In terms of our framework, substructures for this approach are hypergraphs with hyperedges ({mj } , e) for e ∈ Ej , encoding the decision to which partial entity mj refers. The definitions of features and the decoding problem carry over from the graph-based framework (we drop the edge factorization assumption for features). Learning requires adaptations to cope with the dependency betwee"
Q15-1029,J13-4003,0,\N,Missing
Q15-1029,J14-4004,0,\N,Missing
S15-1018,W06-0901,0,0.100968,"Semantics (*SEM 2015), pages 159–164, Denver, Colorado, June 4–5, 2015. place target place instrument victim instrument target In Baghdad, a cameraman died when an American tank fired on the Palestine Hotel. DIE ATTACK Figure 1: A sentence with two event instances, a DIE event triggered by the word “died”, and an ATTACK event triggered by “fired”. Three arguments are shared by both events. model. Finally, we evaluate the retrained system on the ACE 2005 data (Walker et al., 2006). 2 Related Work Many approaches to event extraction do not cross sentence boundaries, e.g. Grishman et al. (2005), Ahn (2006), Lu and Roth (2012), Li et al. (2013) and Li et al. (2014). Only few approaches, like Ji and Grishman (2008) and Liao and Grishman (2010) go beyond sentences and even beyond documents in order to exploit richer context for the extraction of events. While early systems usually predict triggers and arguments independently, more recent work employs joint inference, i.e., predicts triggers and arguments (or only arguments) jointly, e.g., Lu and Roth (2012), Li et al. (2013), and Li et al. (2014). 3 Approach We make use of SEMAFOR, a state-of-the-art frame-semantic parsing system (Das et al., 2010"
S15-1018,W02-1001,0,0.0835078,"the maximum entropy to the perceptron framework. This was done because the perceptron gives better performance for SEMAFORE and is considerably faster, e.g., the argument model can be trained in a few seconds instead of several hours. The new models have a simpler form because we do not have to compute probabilities anymore. The new trigger model is defined as X ei = argmax θ&gt; g 0 (e, l, ti , x) e∈Ei l∈L (6) e &gt; ? + θ g (e, ti , x). The new argument model is defined as Ai (rk ) = argmax ψ &gt; h(s, rk , ei , ti , x). s∈S (7) Weights θ and ψ are learned using a variant of the averaged perceptron (Collins, 2002), where we store feature vectors only after each pass through the training data. 3.4 Features For the trigger model, SEMAFOR’s features include lemmas (of trigger tokens and of the head governor), dependencies of the head, if the head is equal to or has semantic relations with any hidden unit, as well 2 The threshold was determined on development data. as the type of these relations3 . Additionally, we include unigrams and bigrams around the trigger in a window of two. Following Li et al. (2013), we also look at the mention nearest to the trigger. We include its entity type and its string repr"
S15-1018,N10-1138,0,0.0155696,"005), Ahn (2006), Lu and Roth (2012), Li et al. (2013) and Li et al. (2014). Only few approaches, like Ji and Grishman (2008) and Liao and Grishman (2010) go beyond sentences and even beyond documents in order to exploit richer context for the extraction of events. While early systems usually predict triggers and arguments independently, more recent work employs joint inference, i.e., predicts triggers and arguments (or only arguments) jointly, e.g., Lu and Roth (2012), Li et al. (2013), and Li et al. (2014). 3 Approach We make use of SEMAFOR, a state-of-the-art frame-semantic parsing system (Das et al., 2010)1 . We retrain it to predict ACE events, i.e., triggers with event types and arguments for their roles, and make adaptions to better prepare it for event extraction. We call the new system SEMAFORE . 3.1 Trigger Classification In order to classify triggers (single or multiple tokens), the original SEMAFOR uses a log-linear model. To cope with unknown triggers the model includes a latent variable iterating over triggers seen in training (called hidden units). At inference time, hidden units serve as prototypes for unknown words. The model is defined as X ei = argmax pθ (e, l |ti , x). (1) e∈Ei"
S15-1018,J14-1002,0,0.0324484,"d place are filled with the arguments “cameraman”, “American tank”, and “Baghdad”, respectively. For ATTACK, the role target has two arguments, namely “cameraman” and “Palestine hotel”, the roles instrument, and place have the arguments, “American tank”, and “Baghdad”, respectively. Three arguments are shared. One of them, “cameraman”, plays different roles in the events, namely victim of DIE and target of ATTACK. Frame-semantic parsing is the task of extracting semantic predicate-argument structures from texts. It is built on the theory of frame semantics and FrameNet (Fillmore et al., 2003; Das et al., 2014). As in event extraction, frames occur within sentences and have triggers and roles (called lexical units and frame elements). Our hypothesis is that the two tasks are structurally identical. From a computational point of view, they differ only in feature types. We can use the same approach and infrastructure to tackle both. Based on this hypothesis, we retrain a framesemantic parsing system, SEMAFOR, for event extraction. We describe differences between frame-semantic parsing and event extraction and the adaptions needed to better prepare SEMAFOR for the new task. We also describe a bias in t"
S15-1018,P08-1030,0,0.266154,"Missing"
S15-1018,P13-1008,0,0.420584,"–164, Denver, Colorado, June 4–5, 2015. place target place instrument victim instrument target In Baghdad, a cameraman died when an American tank fired on the Palestine Hotel. DIE ATTACK Figure 1: A sentence with two event instances, a DIE event triggered by the word “died”, and an ATTACK event triggered by “fired”. Three arguments are shared by both events. model. Finally, we evaluate the retrained system on the ACE 2005 data (Walker et al., 2006). 2 Related Work Many approaches to event extraction do not cross sentence boundaries, e.g. Grishman et al. (2005), Ahn (2006), Lu and Roth (2012), Li et al. (2013) and Li et al. (2014). Only few approaches, like Ji and Grishman (2008) and Liao and Grishman (2010) go beyond sentences and even beyond documents in order to exploit richer context for the extraction of events. While early systems usually predict triggers and arguments independently, more recent work employs joint inference, i.e., predicts triggers and arguments (or only arguments) jointly, e.g., Lu and Roth (2012), Li et al. (2013), and Li et al. (2014). 3 Approach We make use of SEMAFOR, a state-of-the-art frame-semantic parsing system (Das et al., 2010)1 . We retrain it to predict ACE even"
S15-1018,D14-1198,0,0.525003,"o, June 4–5, 2015. place target place instrument victim instrument target In Baghdad, a cameraman died when an American tank fired on the Palestine Hotel. DIE ATTACK Figure 1: A sentence with two event instances, a DIE event triggered by the word “died”, and an ATTACK event triggered by “fired”. Three arguments are shared by both events. model. Finally, we evaluate the retrained system on the ACE 2005 data (Walker et al., 2006). 2 Related Work Many approaches to event extraction do not cross sentence boundaries, e.g. Grishman et al. (2005), Ahn (2006), Lu and Roth (2012), Li et al. (2013) and Li et al. (2014). Only few approaches, like Ji and Grishman (2008) and Liao and Grishman (2010) go beyond sentences and even beyond documents in order to exploit richer context for the extraction of events. While early systems usually predict triggers and arguments independently, more recent work employs joint inference, i.e., predicts triggers and arguments (or only arguments) jointly, e.g., Lu and Roth (2012), Li et al. (2013), and Li et al. (2014). 3 Approach We make use of SEMAFOR, a state-of-the-art frame-semantic parsing system (Das et al., 2010)1 . We retrain it to predict ACE events, i.e., triggers wi"
S15-1018,P10-1081,0,0.109411,"et In Baghdad, a cameraman died when an American tank fired on the Palestine Hotel. DIE ATTACK Figure 1: A sentence with two event instances, a DIE event triggered by the word “died”, and an ATTACK event triggered by “fired”. Three arguments are shared by both events. model. Finally, we evaluate the retrained system on the ACE 2005 data (Walker et al., 2006). 2 Related Work Many approaches to event extraction do not cross sentence boundaries, e.g. Grishman et al. (2005), Ahn (2006), Lu and Roth (2012), Li et al. (2013) and Li et al. (2014). Only few approaches, like Ji and Grishman (2008) and Liao and Grishman (2010) go beyond sentences and even beyond documents in order to exploit richer context for the extraction of events. While early systems usually predict triggers and arguments independently, more recent work employs joint inference, i.e., predicts triggers and arguments (or only arguments) jointly, e.g., Lu and Roth (2012), Li et al. (2013), and Li et al. (2014). 3 Approach We make use of SEMAFOR, a state-of-the-art frame-semantic parsing system (Das et al., 2010)1 . We retrain it to predict ACE events, i.e., triggers with event types and arguments for their roles, and make adaptions to better prep"
S15-1018,P12-1088,0,0.0311348,"SEM 2015), pages 159–164, Denver, Colorado, June 4–5, 2015. place target place instrument victim instrument target In Baghdad, a cameraman died when an American tank fired on the Palestine Hotel. DIE ATTACK Figure 1: A sentence with two event instances, a DIE event triggered by the word “died”, and an ATTACK event triggered by “fired”. Three arguments are shared by both events. model. Finally, we evaluate the retrained system on the ACE 2005 data (Walker et al., 2006). 2 Related Work Many approaches to event extraction do not cross sentence boundaries, e.g. Grishman et al. (2005), Ahn (2006), Lu and Roth (2012), Li et al. (2013) and Li et al. (2014). Only few approaches, like Ji and Grishman (2008) and Liao and Grishman (2010) go beyond sentences and even beyond documents in order to exploit richer context for the extraction of events. While early systems usually predict triggers and arguments independently, more recent work employs joint inference, i.e., predicts triggers and arguments (or only arguments) jointly, e.g., Lu and Roth (2012), Li et al. (2013), and Li et al. (2014). 3 Approach We make use of SEMAFOR, a state-of-the-art frame-semantic parsing system (Das et al., 2010)1 . We retrain it t"
S15-1036,J08-1001,0,0.896048,"ntify the difficulty of text understanding. Possible applications of readability assessment are automatic text summarization and simplification systems. Measuring readability can also be used in question answering and knowledge extraction systems to prune texts with low readability (Kate et al., 2010). Many different text features have been used to assess readability. They include shallow features (Flesch, 1948; Kincaid et al., 1975), language modeling features (Si and Callan, 2001; CollinsThompson and Callan, 2004), syntactic features (Schwarm and Ostendorf, 2005) and text flow or coherence (Barzilay and Lapata, 2008; Pitler and Nenkova, 2008). In a coherent text each sentence The main goal of this paper is to introduce novel graph-based coherence features for assessing readability. To achieve this goal, we use the entity graph coherence model by Guinaudeau and Strube (2013) (Section 3.1.1) and follow two ideas. The first main idea is to use a graph representation of rhetorical relations between sentences of a text (Section 3.1.2) and to merge the entity graph and the rhetorical graph (Section 3.1.3). Hence we enrich the entity graph and consequently consider the distribution of two aspects of coherence ("
S15-1036,C12-1017,0,0.235448,"Missing"
S15-1036,N04-1025,0,0.117781,"Missing"
S15-1036,E09-1027,0,0.0451675,"Missing"
S15-1036,J95-2003,0,0.5247,"es. In other words, at least two entities are mentioned in one sentence and the subsequent ones are about these entities. st su st su st su st 4 su sv sv sv sv sg1 sg2 sg3 sg4 Experiments 4.1 Figure 8: Feasible 3-node subgraph coherence features. Node labels illustrate the order of sentences. Sentence st occurs before sentence su , and sentence su occurs before sentence sv (i.e. t < u < v). • sg2 : Indicates that entities in st and su get connected to each other in sv . • sg3 : Each sentence tends to refer to the most prominent entity (focus of attention) in preceding sentences (Sidner, 1983; Grosz et al., 1995). The absence of a connection between st and sv indicates that the entity connecting st and su is different from the entity connecting su and sv . Therefore this subgraph approximately corresponds to the shift of the focus of attention. • sg4 : Merges sg1 and sg3 and represents all connections of these two subgraphs. We use these feasible 3-node subgraphs and compute the graph signature, Φ, of each G ∈ ζ . We propose each ϕ ∈ Φ (i.e. relative frequency of each subgraph in G) as a connectivity feature of graph G to measure text coherence. Frequent large subgraphs. Since we observe a strong corr"
S15-1036,P13-1010,1,0.92223,"s with low readability (Kate et al., 2010). Many different text features have been used to assess readability. They include shallow features (Flesch, 1948; Kincaid et al., 1975), language modeling features (Si and Callan, 2001; CollinsThompson and Callan, 2004), syntactic features (Schwarm and Ostendorf, 2005) and text flow or coherence (Barzilay and Lapata, 2008; Pitler and Nenkova, 2008). In a coherent text each sentence The main goal of this paper is to introduce novel graph-based coherence features for assessing readability. To achieve this goal, we use the entity graph coherence model by Guinaudeau and Strube (2013) (Section 3.1.1) and follow two ideas. The first main idea is to use a graph representation of rhetorical relations between sentences of a text (Section 3.1.2) and to merge the entity graph and the rhetorical graph (Section 3.1.3). Hence we enrich the entity graph and consequently consider the distribution of two aspects of coherence (i.e. entities and discourse relations) simultaneously. The second main idea is to apply subgraph mining algorithms to find frequent subgraphs (i.e. patterns) in texts (Section 3.2). Subgraph mining has been successfully applied to other tasks, e.g. image processi"
S15-1036,N07-1058,0,0.133807,"Missing"
S15-1036,J09-1003,0,0.0209939,"11 show that different patterns of edges in subgraphs capture readability judgments. Stoddard (1991, p.29) explains this by the ambiguity node phenomenon: “[...] in some cases, there may be more than one logical, possible node for a given cohesive element in a text, in which case, a reader may see the resulting ambiguity but not be able to 6 Although, the proposed features can be applied on all kind of presented graphs, we evaluate them (except outdegree) only on projections of the entity graph model. We leave the application to the other graph representations for future work. 7 This supports Karamanis et al. (2009) who report that NOCB transitions in the centering model can be used for the sentence ordering task. 315 sg1 sg2 sg3 sg4 sg5 sg6 sg7 sg8 sg9 sg10 sg11 sg12 sg13 sg14 sg15 sg16 sg17 sg18 sg19 sg20 sg21 sg22 sg23 sg24 number of edges 6 5 5 4 5 5 5 4 5 4 4 4 4 4 3 4 3 3 3 3 4 3 3 3 ρ 0.103 −0.212 −0.176 −0.257 −0.140 0.200 −0.402 −0.317 0.153 −0.238 −0.509 0.449 −0.045 −0.033 −0.358 −0.068 −0.308 −0.546 −0.601 0.094 0.068 −0.374 −0.314 0.100 p value 0.609 0.288 0.380 0.196 0. 486 0.317 0.038 0.107 0.446 0.232 0.007 0.019 0.824 0.870 0.067 0.736 0.118 0.003 0.001 0.641 0.736 0.055 0.111 0.620 Tabl"
S15-1036,C10-1062,0,0.120345,"Missing"
S15-1036,P11-1100,0,0.198111,"[news] show we saw [yesterday] even displayed 25 federal [officials] meeting around a [table]. S4: We recall that the [mayor] of [Charleston] complained bitterly about the federal [bureaucracy]’s response to [Hurricane Hugo]. S5: The [sense] grows that modern public [bureaucracies] simply don’t perform their assigned [functions] well. Table 1: A sample text from the Wall Street Journal dataset (Pitler and Nenkova, 2008). s3 s1 1 s1 s3 s1 s4 s2 s3 s3 s1 1 1 s2 s4 1 s2 1 s4 1 s5 s5 s5 PuER PwER PuDR experiments as well. Thus, we do explain further details of PwER here. Discourse Relation Graph Lin et al. (2011) and Lin (2011) use Rhetorical Structure Theory (RST) to describe and model coherence by considering the transitions between discourse relations. Inspired by the entity grid they expand the relation sequence into a two-dimensional matrix whose rows and columns are sentences and entities, respectively. The cell hsi , e j i corresponds to the set of discourse relations entity e j is involved with in sentence si . These methods are based on entity transitions which, however, are intuitively implausible, because discourse relations connect sentences (or elementary discourse units). Since discourse"
S15-1036,H94-1020,0,0.109426,"trix, we compute the graph signature of each G ∈ ζ and take each element of the graph signature as a coherence feature. 313 Data We use the dataset created by Pitler and Nenkova (2008) which consists of randomly selected articles from the Wall Street Journal corpus. The articles were rated by three humans on a scale from 1 to 5 for readability based on quality measures that are designed to estimate the coherence of articles. The final readability score of each article is the average of these three ratings. We exclude three files from this dataset: wsj-0382 does not exist in the Penn Treebank (Marcus et al., 1994)1 . wsj-2090 does not exist in the Penn Discource Treebank (Prasad et al., 2008). wsj-1398 is a poem. 4.2 Settings Entity graph. We use the gold parse trees in the Penn Treebank (Marcus et al., 1994) to extract all nouns in a document as mentions. We consider nouns with identical stem2 as coreferent. We divide the edge weight between two sentence nodes si and s j by their distance j − i to decrease the importance of links that exist between non-adjacent sentences. Discourse relation graph. We use gold PDTB-style discourse relations (Prasad et al., 2008). We filter out EntRel and NoRel relation"
S15-1036,D08-1020,0,0.546728,"O CHARLESTON ... MAYOR NEWS GEAR HOURS BUREAUCRACY ... WASHINGTON MAKERS POLICY s5 s4 ... OFFICIALS PRESS ... ASSOCIATED s3 s2 TABLE s1 Figure 1: The entity graph representation of the text in Table 1. Dark entities are shared by the sentences. 2 Readability Assessment The quality of a text depends on different factors which make the text easier to read. These factors range from shallow features like word length to semantic features like coherence. Readability assessment leads to two problems: distinguishing and recognizing readability levels of texts and predicting human readability ratings. Pitler and Nenkova (2008) use all entity transitions of the entity grid model (Barzilay and Lapata, 2008) as coherence features. They compute the correlation between them and readability ratings and show that none of them is significantly correlated with human readability judgments. Indeed, none of these features on its own is a good predictor to measure coherence and to predict readability as well. 3 Method We introduce the graph representation of a text and propose to use these graphs to model coherence. 3.1 3.1.1 Graphs Entity Graph Guinaudeau and Strube (2013) describe a graphbased version of the entity grid (Barz"
S15-1036,prasad-etal-2008-penn,0,0.173509,"ns. Hence we do not report on their performance. Inspired by linear regression models we combine the weighted graphs by adding (+) the edge weights in PwER and PwDR (Figure 4). s3 s1 Relation Implicit Expansion Explicit Comparison Implicit Expansion Implicit Temporal Implicit Contingency 1 s2 1 Figure 2: PuER : unweighted, and PwER : weighted projection graphs. In the weighted projection all edge weights are equal to one, because all sentences share one entity. 3.1.2 s4 Arg2 S2 S2 S3 S4 S5 1 s1 1 s2 s4 PuER ∨ PuDR 1 1 1 s2 1 s5 s3 1 s4 2 s5 PwER + PwDR Table 2: PDTB-style discourse relations (Prasad et al., 2008) of the sample text in Table 1 Figure 4: Combined entity and discourse relation graphs. A discourse relation graph is PuDR = (V, R), where V is the set of sentence nodes and R is the edge set which represents all discourse relations in the text. Two sentence nodes are adjacent if and only if they are connected by at least one discourse relation. Intra-sentential discourse relations are represented as self-edges. We define PwDR as a weighted discourse relation graph whose edge weights are 3.2 311 Coherence Features We use the proposed graphs to introduce novel coherence features. Average outdeg"
S15-1036,P05-1065,0,0.167803,"tors can be used by readability assessment methods to quantify the difficulty of text understanding. Possible applications of readability assessment are automatic text summarization and simplification systems. Measuring readability can also be used in question answering and knowledge extraction systems to prune texts with low readability (Kate et al., 2010). Many different text features have been used to assess readability. They include shallow features (Flesch, 1948; Kincaid et al., 1975), language modeling features (Si and Callan, 2001; CollinsThompson and Callan, 2004), syntactic features (Schwarm and Ostendorf, 2005) and text flow or coherence (Barzilay and Lapata, 2008; Pitler and Nenkova, 2008). In a coherent text each sentence The main goal of this paper is to introduce novel graph-based coherence features for assessing readability. To achieve this goal, we use the entity graph coherence model by Guinaudeau and Strube (2013) (Section 3.1.1) and follow two ideas. The first main idea is to use a graph representation of rhetorical relations between sentences of a text (Section 3.1.2) and to merge the entity graph and the rhetorical graph (Section 3.1.3). Hence we enrich the entity graph and consequently c"
W01-1612,W99-0107,1,0.808851,"DTTool and CLinkA, it also allows for the extension of the tag set, so that in principle the handling of different coreference phenomena is possible. The tool (like the other two mentioned before) processes SGML files, into which annotation tags are inserted directly during annotation. We regard this approach to annotation as a drawback, because it mixes the basic data (i.e. the texts to be annotated) with the annotation itself. This can give rise to problems, e.g. in cases where alternative annotations of the same data are to be compared. Referee, a Tcl/Tk program for coreference annotation (DeCristofaro et al., 1999), is better in this respect in that it writes the annotations to a separate file, leaving the annotated text itself unaltered. The format of this annotation file, however, is highly idiosyncratic, rendering very difficult the subsequent analysis of the annotation. Moreover, this tool also represents cospecification in terms of antecedence only, making it impossible to annotate the former without specifying the latter. On the other hand, Referee directly supports the definition of user-definable attributes. Finally, the MATE Workbench 5 is the most ambitious tool that we considered for the impl"
W01-1612,orasan-2000-clinka,0,0.059321,"Missing"
W01-1612,J00-4003,0,0.239022,"tool is required which implements the annotation scheme in a robust and efficient way. We review a selection of existing tools, then present MMAX (Multi-Modal Annotation in XML), our versatile Java tool (Section 4), and we demonstrate how the annotation scheme for anaphoric and bridging relations can be implemented in MMAX (Section 5). 2 Definition In general, anaphoric as well as bridging relations hold between specifying expressions. These are those expressions that specify (i.e. are used to refer to) a particular extra-linguistic entity. In what follows, we briefly discuss the approach of (Vieira & Poesio, 2000) and present our own definition. Since (Vieira & Poesio, 2000) address the problem of bridging annotation, they try to find an operational and easily applicable definition. This is the main motivation for choosing (Vieira & Poesio, 2000) (and not e.g. (Clark, 1975), who introduced the term bridging) as the background of our discussion. In the following discussion, two features of pairs of specifying expressions will be important. The first one is cospecification (Sidner, 1983), also known as coreference, a relation holding between two or more specifying expressions which specify the same extra"
W01-1612,M95-1005,0,0.0300474,"in this way, the concept antecedent becomes free to be used only in those cases where it is both relevant and unambiguously decidable. It is important to note that no relevant information appears to be lost here: Supplied that the linear order of markables within the text is preserved, it should be possible to establish an antecedent to any anaphoric expression from a set of cospecifying expressions annotated within the scheme described above. Moreover, the important task of evaluating the annotation scheme is not affected either, because common evaluation algorithms for anaphor annotations (Vilain et al., 1995) do not depend on antecedence information, but treat anaphoric expressions as cospecifying equivalence classes. What is even more important is that by the same means we can render optional the explicit specification of bridging antecedents as well. Two cases can be distinguished here: Whenever only a single candidate for antecedence exists, specifying it is trivial. Thus, the only cases where uncertainty as to the correct antecedent of a bridging expression can arise appear to be those in which multiple cospecifying candidates are available. Since bridging (as we define it) is a relation not b"
W02-0207,J96-2004,0,0.201028,"Missing"
W02-0207,W01-1612,1,0.888456,"Missing"
W02-0207,rapp-strube-2002-iterative,1,0.78873,"Missing"
W02-1040,P95-1017,0,0.196999,"sets consisting of definite noun phrases and proper names, respectively. When applied to the whole data set the feature produced a smaller but still significant improvement. 1 Introduction For the automatic understanding of written or spoken natural language it is crucial to be able to identify the entities referred to by referring expressions. The most common and thus most important types of referring expressions are pronouns and definite noun phrases (NPs). Supervised machine learning algorithms have been used for pronoun resolution (Ge et al., 1998) and for the resolution of definite NPs (Aone and Bennett, 1995; McCarthy and Lehnert, 1995; Soon et al., 2001). An unsupervised approach to the resolution of definite NPs was applied by Cardie and Wagstaff (1999). However, though machine learning algorithms may deduce to make best use of a given set of features for a given problem, it is a linguistic question and a non-trivial task to identify a set of features which describe the data sufficiently. We report on experiments in the resolution of anaphoric expressions in general, including definite noun phrases, proper names, and personal, possessive and demonstrative pronouns. Based on the work mentioned a"
W02-1040,A00-1031,0,0.00350817,"feature (used for proper names and acronyms) an appositive feature Table 1: Features used by Soon et al. – – – – – – – – – position (NPs are numbered sequentially) pronoun type (nom., acc., possessive, ambiguous) article (indefinite, definite, none) appositive (yes, no) number (singular, plural) proper name (yes, no) semantic class (based on WordNet: time, city, animal, human, object; based on a separate algorithm: number, money, company) gender (masculine, feminine, either, neuter) animacy (anim, inanim) Table 2: Features used by Cardie and Wagstaff kens. The texts were POS-tagged using TnT (Brants, 2000). A basic identification of markables (referring expressions, i.e. NPs) was obtained by using the NP-Chunker Chunkie (Skut and Brants, 1998). The POS-tagger was also used for assigning attributes like e.g. the NP form to markables. The automatic annotation was followed by a manual correction and annotation phase in which the markables were annotated with further tags (e.g. semantic class). In this phase manual coreference annotation was performed as well. In our annotation coreference is represented in terms of a member attribute on markables. Markables with the same value in this attribute ar"
W02-1040,W99-0611,0,0.387307,"ill significant improvement. 1 Introduction For the automatic understanding of written or spoken natural language it is crucial to be able to identify the entities referred to by referring expressions. The most common and thus most important types of referring expressions are pronouns and definite noun phrases (NPs). Supervised machine learning algorithms have been used for pronoun resolution (Ge et al., 1998) and for the resolution of definite NPs (Aone and Bennett, 1995; McCarthy and Lehnert, 1995; Soon et al., 2001). An unsupervised approach to the resolution of definite NPs was applied by Cardie and Wagstaff (1999). However, though machine learning algorithms may deduce to make best use of a given set of features for a given problem, it is a linguistic question and a non-trivial task to identify a set of features which describe the data sufficiently. We report on experiments in the resolution of anaphoric expressions in general, including definite noun phrases, proper names, and personal, possessive and demonstrative pronouns. Based on the work mentioned above we started with a feature set including NP-level and coreference-level features. Applied to the whole data set these features led only to moderat"
W02-1040,J96-2004,0,0.0241605,"as also used for assigning attributes like e.g. the NP form to markables. The automatic annotation was followed by a manual correction and annotation phase in which the markables were annotated with further tags (e.g. semantic class). In this phase manual coreference annotation was performed as well. In our annotation coreference is represented in terms of a member attribute on markables. Markables with the same value in this attribute are considered coreferring expressions. The annotation was performed by two students. The reliability of the annotations was checked using the kappa statistic (Carletta, 1996). 3.2 Data Generation The problem of coreference resolution can easily be formulated as a binary classification: Given a pair of potential anaphor and potential antecedent, classify as positive if the antecedent is in fact the closest antecedent, and as negative otherwise. In anaphoric chains only the immediately adjacent pairs are classified as positive. We generated data suitable as input to a machine learning algorithm from our corpus using a straightforward algorithm which combined potential anaphors and their potential antecedents. We then applied the following filters to the resulting pa"
W02-1040,W98-1119,0,0.0220444,". This feature yielded a significant improvement for data sets consisting of definite noun phrases and proper names, respectively. When applied to the whole data set the feature produced a smaller but still significant improvement. 1 Introduction For the automatic understanding of written or spoken natural language it is crucial to be able to identify the entities referred to by referring expressions. The most common and thus most important types of referring expressions are pronouns and definite noun phrases (NPs). Supervised machine learning algorithms have been used for pronoun resolution (Ge et al., 1998) and for the resolution of definite NPs (Aone and Bennett, 1995; McCarthy and Lehnert, 1995; Soon et al., 2001). An unsupervised approach to the resolution of definite NPs was applied by Cardie and Wagstaff (1999). However, though machine learning algorithms may deduce to make best use of a given set of features for a given problem, it is a linguistic question and a non-trivial task to identify a set of features which describe the data sufficiently. We report on experiments in the resolution of anaphoric expressions in general, including definite noun phrases, proper names, and personal, posse"
W02-1040,W98-1117,0,0.0200167,"(NPs are numbered sequentially) pronoun type (nom., acc., possessive, ambiguous) article (indefinite, definite, none) appositive (yes, no) number (singular, plural) proper name (yes, no) semantic class (based on WordNet: time, city, animal, human, object; based on a separate algorithm: number, money, company) gender (masculine, feminine, either, neuter) animacy (anim, inanim) Table 2: Features used by Cardie and Wagstaff kens. The texts were POS-tagged using TnT (Brants, 2000). A basic identification of markables (referring expressions, i.e. NPs) was obtained by using the NP-Chunker Chunkie (Skut and Brants, 1998). The POS-tagger was also used for assigning attributes like e.g. the NP form to markables. The automatic annotation was followed by a manual correction and annotation phase in which the markables were annotated with further tags (e.g. semantic class). In this phase manual coreference annotation was performed as well. In our annotation coreference is represented in terms of a member attribute on markables. Markables with the same value in this attribute are considered coreferring expressions. The annotation was performed by two students. The reliability of the annotations was checked using the"
W02-1040,J01-4004,0,0.920926,"names, respectively. When applied to the whole data set the feature produced a smaller but still significant improvement. 1 Introduction For the automatic understanding of written or spoken natural language it is crucial to be able to identify the entities referred to by referring expressions. The most common and thus most important types of referring expressions are pronouns and definite noun phrases (NPs). Supervised machine learning algorithms have been used for pronoun resolution (Ge et al., 1998) and for the resolution of definite NPs (Aone and Bennett, 1995; McCarthy and Lehnert, 1995; Soon et al., 2001). An unsupervised approach to the resolution of definite NPs was applied by Cardie and Wagstaff (1999). However, though machine learning algorithms may deduce to make best use of a given set of features for a given problem, it is a linguistic question and a non-trivial task to identify a set of features which describe the data sufficiently. We report on experiments in the resolution of anaphoric expressions in general, including definite noun phrases, proper names, and personal, possessive and demonstrative pronouns. Based on the work mentioned above we started with a feature set including NP-"
W02-1040,J00-4003,0,0.0456088,"recall. The substring match feature was not too helpful either as it does not trigger in many cases. So, we investigated ways to raise the recall of the string ident and substring match features without losing too much precision. A look at some relevant cases (Table 5) suggested that a large number of anaphoric definite NPs shared some substring with their antecedent, but they were not identical nor completely included. What is needed is a weakened form of the string ident and substring match features. Soon et al. (2001) removed determiners before comparing the strings. Other researchers like Vieira and Poesio (2000) used information about the syntactic structure and compared only the syntactic heads of the phrases. However, the feature used by Soon et al. (2001) is neither sufficient nor language dependent, the one used by Vieira and Poesio (2000) is not cheap since it relies on a syntactic analysis. We were looking for a feature which gave us the improvements of the features used by other researchers without their associated costs. Hence we considered the minimum edit distance (MED) (Wagner and Fischer, 1974), which has been used for spelling correction and in speech recognizer evaluations (termed “accu"
W02-1040,M95-1005,0,0.146074,"Missing"
W02-1040,J00-4006,0,\N,Missing
W03-2117,rapp-strube-2002-iterative,1,\N,Missing
W03-2117,soria-etal-2002-advanced,0,\N,Missing
W03-2117,P03-1022,1,\N,Missing
W03-2117,muller-strube-2002-api,1,\N,Missing
W03-2117,salmon-alt-vieira-2002-nominal,0,\N,Missing
W03-2117,P02-1045,1,\N,Missing
W05-0618,P04-1051,0,0.0379499,". Reiter (1994), Reiter & Dale (2000)), and has been successfully used to combine standard preprocessing tasks such as part-of-speech tagging, chunking and named entity recognition (e.g. Buchholz et al. (1999), Soon et al. (2001)). In this paper we address the problem of aggregating the outputs of classifiers solving different NLP tasks. We compare pipeline-based processing with discrete optimization modeling used in the field of computer vision and image recognition (Kleinberg & Tardos, 2000; Chekuri et al., 2001) and recently applied in NLP by Roth & Yih (2004), Punyakanok et al. (2004) and Althaus et al. (2004). Whereas Roth and Yih used optimization to solve two tasks only, and Punyakanok et al. and Althaus et al. focused on a single task, we propose a general formulation capable of combining a large number of different NLP tasks. We apply the proposed model to solving numerous tasks in the generation process and compare it with two pipeline-based systems. The paper is structured as follows: in Section 2 we discuss the use of classifiers for handling NLP tasks and point to the limitations of pipeline processing. In Section 3 we present a general discrete optimization model whose application in NLG"
W05-0618,W99-0629,0,0.0103126,"lve such tasks facilitates building complex applications out of many light components. The architecture of choice for such systems has become a pipeline, with strict ordering of the processing stages. An example of a generic pipeline architecture is GATE (Cunningham et al., 1997) which provides an infrastructure for building NLP applications. Sequential processing has also been used in several NLG systems (e.g. Reiter (1994), Reiter & Dale (2000)), and has been successfully used to combine standard preprocessing tasks such as part-of-speech tagging, chunking and named entity recognition (e.g. Buchholz et al. (1999), Soon et al. (2001)). In this paper we address the problem of aggregating the outputs of classifiers solving different NLP tasks. We compare pipeline-based processing with discrete optimization modeling used in the field of computer vision and image recognition (Kleinberg & Tardos, 2000; Chekuri et al., 2001) and recently applied in NLP by Roth & Yih (2004), Punyakanok et al. (2004) and Althaus et al. (2004). Whereas Roth and Yih used optimization to solve two tasks only, and Punyakanok et al. and Althaus et al. focused on a single task, we propose a general formulation capable of combining a"
W05-0618,A97-1035,0,0.0302603,"tterance. Each such mapping is typically split into a number of different tasks handled by separate modules. As noted by Daelemans & van den Bosch (1998), individual decisions that these tasks involve can be formulated as classification problems falling in either of two groups: disambiguation or segmentation. The use of machine-learning to solve such tasks facilitates building complex applications out of many light components. The architecture of choice for such systems has become a pipeline, with strict ordering of the processing stages. An example of a generic pipeline architecture is GATE (Cunningham et al., 1997) which provides an infrastructure for building NLP applications. Sequential processing has also been used in several NLG systems (e.g. Reiter (1994), Reiter & Dale (2000)), and has been successfully used to combine standard preprocessing tasks such as part-of-speech tagging, chunking and named entity recognition (e.g. Buchholz et al. (1999), Soon et al. (2001)). In this paper we address the problem of aggregating the outputs of classifiers solving different NLP tasks. We compare pipeline-based processing with discrete optimization modeling used in the field of computer vision and image recogni"
W05-0618,C04-1197,0,0.11702,"d in several NLG systems (e.g. Reiter (1994), Reiter & Dale (2000)), and has been successfully used to combine standard preprocessing tasks such as part-of-speech tagging, chunking and named entity recognition (e.g. Buchholz et al. (1999), Soon et al. (2001)). In this paper we address the problem of aggregating the outputs of classifiers solving different NLP tasks. We compare pipeline-based processing with discrete optimization modeling used in the field of computer vision and image recognition (Kleinberg & Tardos, 2000; Chekuri et al., 2001) and recently applied in NLP by Roth & Yih (2004), Punyakanok et al. (2004) and Althaus et al. (2004). Whereas Roth and Yih used optimization to solve two tasks only, and Punyakanok et al. and Althaus et al. focused on a single task, we propose a general formulation capable of combining a large number of different NLP tasks. We apply the proposed model to solving numerous tasks in the generation process and compare it with two pipeline-based systems. The paper is structured as follows: in Section 2 we discuss the use of classifiers for handling NLP tasks and point to the limitations of pipeline processing. In Section 3 we present a general discrete optimization model"
W05-0618,W94-0319,0,0.0119288,"vidual decisions that these tasks involve can be formulated as classification problems falling in either of two groups: disambiguation or segmentation. The use of machine-learning to solve such tasks facilitates building complex applications out of many light components. The architecture of choice for such systems has become a pipeline, with strict ordering of the processing stages. An example of a generic pipeline architecture is GATE (Cunningham et al., 1997) which provides an infrastructure for building NLP applications. Sequential processing has also been used in several NLG systems (e.g. Reiter (1994), Reiter & Dale (2000)), and has been successfully used to combine standard preprocessing tasks such as part-of-speech tagging, chunking and named entity recognition (e.g. Buchholz et al. (1999), Soon et al. (2001)). In this paper we address the problem of aggregating the outputs of classifiers solving different NLP tasks. We compare pipeline-based processing with discrete optimization modeling used in the field of computer vision and image recognition (Kleinberg & Tardos, 2000; Chekuri et al., 2001) and recently applied in NLP by Roth & Yih (2004), Punyakanok et al. (2004) and Althaus et al."
W05-0618,W04-2401,0,0.603244,"g has also been used in several NLG systems (e.g. Reiter (1994), Reiter & Dale (2000)), and has been successfully used to combine standard preprocessing tasks such as part-of-speech tagging, chunking and named entity recognition (e.g. Buchholz et al. (1999), Soon et al. (2001)). In this paper we address the problem of aggregating the outputs of classifiers solving different NLP tasks. We compare pipeline-based processing with discrete optimization modeling used in the field of computer vision and image recognition (Kleinberg & Tardos, 2000; Chekuri et al., 2001) and recently applied in NLP by Roth & Yih (2004), Punyakanok et al. (2004) and Althaus et al. (2004). Whereas Roth and Yih used optimization to solve two tasks only, and Punyakanok et al. and Althaus et al. focused on a single task, we propose a general formulation capable of combining a large number of different NLP tasks. We apply the proposed model to solving numerous tasks in the generation process and compare it with two pipeline-based systems. The paper is structured as follows: in Section 2 we discuss the use of classifiers for handling NLP tasks and point to the limitations of pipeline processing. In Section 3 we present a general d"
W05-0618,J01-4004,0,0.0259485,"es building complex applications out of many light components. The architecture of choice for such systems has become a pipeline, with strict ordering of the processing stages. An example of a generic pipeline architecture is GATE (Cunningham et al., 1997) which provides an infrastructure for building NLP applications. Sequential processing has also been used in several NLG systems (e.g. Reiter (1994), Reiter & Dale (2000)), and has been successfully used to combine standard preprocessing tasks such as part-of-speech tagging, chunking and named entity recognition (e.g. Buchholz et al. (1999), Soon et al. (2001)). In this paper we address the problem of aggregating the outputs of classifiers solving different NLP tasks. We compare pipeline-based processing with discrete optimization modeling used in the field of computer vision and image recognition (Kleinberg & Tardos, 2000; Chekuri et al., 2001) and recently applied in NLP by Roth & Yih (2004), Punyakanok et al. (2004) and Althaus et al. (2004). Whereas Roth and Yih used optimization to solve two tasks only, and Punyakanok et al. and Althaus et al. focused on a single task, we propose a general formulation capable of combining a large number of dif"
W05-0618,W98-1223,0,0.0330136,"Missing"
W05-0633,J02-3001,0,0.12467,"tactic and semantic information. Accordingly, we make use of both clausal, chunk and deep syntactic (tree structure) features, named entity information, as well as statistical representations for lexical item encoding. The set of features and their encoding reflect the necessity of limiting the complexity and dimensionality of the input space. They also provide the classifier with enough information. We explore here the use of a minimal set of compact features for semantic role prediction, and show that a feature-based 2.1.1 Tree node mapping of semantic arguments and named entities Following Gildea & Jurafsky (2002), (i) labels matching more than one constituent due to nonbranching nodes are taken as labels of higher constituents, (ii) in cases of labels with no corresponding parse constituent, these are assigned to the partial match given by the constituent spanning the shortest portion of the sentence beginning at the label’s span left boundary and lying entirely within it. We drop the role or named entity label if such suitable constituent could not be found2 . 1 All other processing steps assume a uniform treatment of both training and test data. 2 The percentage of roles for which no valid tree node"
W05-0633,J05-1004,0,0.0330164,"et. 1 2 System description 2.1 Preprocessing During preprocessing the predicates’ semantic arguments are mapped to the nodes in the parse trees, a set of hand-crafted shallow tree pruning rules are applied, probability distributions for feature representation are generated from training data1 , and feature vectors are extracted. Those are finally fed into the classifier for semantic role classification. Introduction This paper presents a system for the CoNLL 2005 Semantic Role Labeling shared task (Carreras & M`arquez, 2005), which is based on the current release of the English PropBank data (Palmer et al., 2005). For the 2005 edition of the shared task are available both syntactic and semantic information. Accordingly, we make use of both clausal, chunk and deep syntactic (tree structure) features, named entity information, as well as statistical representations for lexical item encoding. The set of features and their encoding reflect the necessity of limiting the complexity and dimensionality of the input space. They also provide the classifier with enough information. We explore here the use of a minimal set of compact features for semantic role prediction, and show that a feature-based 2.1.1 Tree"
W05-0633,J03-4003,0,\N,Missing
W05-0633,W05-0620,0,\N,Missing
W05-1611,P04-1051,0,0.0689794,"Missing"
W05-1611,N01-1002,0,0.0538633,"Missing"
W05-1611,1993.eamt-1.6,0,0.0652112,"Missing"
W05-1611,P84-1107,0,0.528028,"Missing"
W05-1611,W03-2304,0,0.0482625,"Missing"
W05-1611,W90-0109,0,0.169206,"Missing"
W05-1611,W94-0319,0,0.115822,"Missing"
W05-1611,W04-2401,0,0.0299071,"Missing"
W05-1611,W98-0315,0,0.0687875,"Missing"
W06-1632,A00-1031,0,0.0273323,"Missing"
W06-1632,A00-2004,0,0.0244195,"uistically Motivated Features for Paragraph Boundary Identification Katja Filippova and Michael Strube EML Research gGmbH Schloss-Wolfsbrunnenweg 33 69118 Heidelberg, Germany http://www.eml-research.de/nlp Abstract (PBI) could be useful for a number of different applications, such as producing the layout for transcripts provided by speech recognizers and optical character recognition systems, and determining the layout of documents generated for output devices with different screen size. Though related to the task of topic segmentation which stimulated a large number of studies (Hearst, 1997; Choi, 2000; Galley et al., 2003, inter alia), paragraph segmentation has not been thoroughly investigated so far. We explain this by the fact that paragraphs are considered a stylistic phenomenon and that there is no unanimous opinion on what the function of the paragraph is. Some authors (Irmscher (1972) as cited by Stark (1988)) suggest that paragraph structure is arbitrary and can not be determined based solely on the properties of the text. Still, psycholinguistic studies report that humans agree, at least to some extent, on placing boundaries between paragraphs. These studies also note that paragra"
W06-1632,W05-0618,1,0.83053,"ependent on the paragraph structure and not the other way round. At the same time they mention speech and optical character recognition tasks as possible application domains for the PBI. There, pronouns are already given and need not be regenerated, hence for such applications features which utilize pronouns are absolutely appropriate. Unlike the recognition tasks, for multi-document summarization both decisions have to be made, and the order of the two tasks is not self-evident. The best decision would probably be to decide simultaneously on both using optimization methods (Roth & Yih, 2004; Marciniak & Strube, 2005). Generating pronouns before inserting boundaries seems as reasonable as doing it the other way round. prevSCue, currSCue: This feature is the connective itself (NA in case of none). prevSCueClass, currSCueClass: This feature represents the semantic class of the cue word or phrase as assigned by the IDS Mannheim. There are 25 values, including NA in case of no connective, altogether, with the most frequent values being temporal, concessive, conclusive, etc. prevSProCue, currSProCue: The third binary feature marks whether the connective is proadverbial or not (NA if there is no connective). Bei"
W06-1632,J02-1002,0,0.0187081,"it to the model. We iterate the process until all features are added to the three-feature system. Thus, we optimize the default setting and obtain the information on what the paragraph structure crucially depends. 5 The results of our system and the baselines for different classifiers (BT stands for BoosTexter and Ti for TiMBL) are summarized in Table 4. Accuracy is calculated by dividing the number of matches over the total number of test instances. Precision, recall and F-measure are obtained by considering true positives, false positives and false negatives. The latter metric, WindowDiff (Pevzner & Hearst, 2002), is supposed to overcome the disadvantage of the F-measure which penalizes near misses as harsh as more serious mistakes. The value of WindowDiff varies between 0 and 1, where a lesser count corresponds to better performance. The significance of our results was computed using the  test. All results are significantly better (on the      level or below) than both baselines and the reimplemented version of Sporleder & Lapata’s (2006) algorithm whose performance on our data is comparable to what the authors reported on their corpus of German fiction. Interestingly, TiMBL does much better t"
W06-1632,W06-2305,0,0.0887402,"Missing"
W06-1632,W04-2401,0,0.0166075,"ronominalization dependent on the paragraph structure and not the other way round. At the same time they mention speech and optical character recognition tasks as possible application domains for the PBI. There, pronouns are already given and need not be regenerated, hence for such applications features which utilize pronouns are absolutely appropriate. Unlike the recognition tasks, for multi-document summarization both decisions have to be made, and the order of the two tasks is not self-evident. The best decision would probably be to decide simultaneously on both using optimization methods (Roth & Yih, 2004; Marciniak & Strube, 2005). Generating pronouns before inserting boundaries seems as reasonable as doing it the other way round. prevSCue, currSCue: This feature is the connective itself (NA in case of none). prevSCueClass, currSCueClass: This feature represents the semantic class of the cue word or phrase as assigned by the IDS Mannheim. There are 25 values, including NA in case of no connective, altogether, with the most frequent values being temporal, concessive, conclusive, etc. prevSProCue, currSProCue: The third binary feature marks whether the connective is proadverbial or not (NA if t"
W06-1632,W03-1009,0,0.185329,"Missing"
W06-1632,W04-3210,0,0.415984,"rbitrary and can not be determined based solely on the properties of the text. Still, psycholinguistic studies report that humans agree, at least to some extent, on placing boundaries between paragraphs. These studies also note that paragraph boundaries are informative and make the reader perceive paragraphinitial sentences as being important (Stark, 1988). In contrast to topic segmentation, paragraph segmentation has the advantage that large amounts of annotated data are readily availabe for supervised learning. In this paper we describe our approach to paragraph segmentation. Previous work (Sporleder & Lapata, 2004; 2006) mainly focused on superficial and easily obtainable surface features like punctuation, quotes, distance and words in the sentence. Their approach was claimed to be domain- and language-independent. Our hypothesis, however, is that linguistically motivated features, which we compute automatically, provide a better paragraph segmentation than Sporleder & Lapata’s surface ones, though our approach may loose some of the In this paper we propose a machinelearning approach to paragraph boundary identification which utilizes linguistically motivated features. We investigate the relation betwe"
W06-1632,J97-1003,0,0.168416,"by collocations, we explore the role of discourse cues, pronominalization and information structure. The task of topic segmentation is closely related to the task of paragraph segmentation. If there is a topic boundary, it is very likely that it coincides with a paragraph boundary. However, the reverse is not true and one topic can extend over several paragraphs. So, if determined reliably, topic boundaries could be used as high precision, low recall predictors for paragraph boundaries. Still, there is an important difference: While work on topic segmentation mainly depends on content words (Hearst, 1997) and relations between them which are computed using lexical chains (Galley et al., 2003), paragraph segmentation as a stylistic phenomenon may depend equally likely on function words. Hence, paragraph segmentation is a task which encompasses the traditional borders between content and style. Related Work Compared to other text segmentation tasks, e.g. topic segmentation, PBI has received relatively little attention. We are aware of three studies which approach the problem from different perspectives. Bolshakov & Gelbukh (2001) assume that splitting text into paragraphs is determined by text c"
W06-1632,1996.amta-1.36,0,0.145192,"ing the layout for transcripts provided by speech recognizers and optical character recognition systems, and determining the layout of documents generated for output devices with different screen size. Though related to the task of topic segmentation which stimulated a large number of studies (Hearst, 1997; Choi, 2000; Galley et al., 2003, inter alia), paragraph segmentation has not been thoroughly investigated so far. We explain this by the fact that paragraphs are considered a stylistic phenomenon and that there is no unanimous opinion on what the function of the paragraph is. Some authors (Irmscher (1972) as cited by Stark (1988)) suggest that paragraph structure is arbitrary and can not be determined based solely on the properties of the text. Still, psycholinguistic studies report that humans agree, at least to some extent, on placing boundaries between paragraphs. These studies also note that paragraph boundaries are informative and make the reader perceive paragraphinitial sentences as being important (Stark, 1988). In contrast to topic segmentation, paragraph segmentation has the advantage that large amounts of annotated data are readily availabe for supervised learning. In this paper we"
W06-1632,P03-1071,0,\N,Missing
W07-2321,P05-1018,0,0.101419,"gGmbH Schloss-Wolfsbrunnenweg 33 69118 Heidelberg, Germany http://www.eml-research.de/nlp/ Abstract method is based on how patterns of entity distribution differ for coherent and incoherent texts. It utilizes information of three kinds: coreference, salience and syntax. As a suggestion for future work, Barzilay & Lapata hypothesize that integrating semantic knowledge for entity grouping (as opposed to coreference) should improve the results. So, the purpose of the current study is threefold: This paper reports on work in progress on extending the entity-based approach on measuring coherence (Barzilay & Lapata, 2005; Lapata & Barzilay, 2005) from coreference to semantic relatedness. We use a corpus of manually annotated German newspaper text (T¨uBa-D/Z) and aim at improving the performance by grouping related entities with the WikiRelate! API (Strube & Ponzetto, 2006). • to check how the method performs on a language other than English; 1 Introduction • to estimate the contribution of the three knowledge sources on mannualy annotated data; Evaluation is a well-known problem for Natural Language Generation (NLG). Human labor required to evaluate the output of a NLG system is expensive since every text sho"
W07-2321,I05-1067,0,0.0529163,"Missing"
W07-2321,J06-4002,0,0.0563013,"Missing"
W08-1105,P06-4020,0,0.0136535,"entence He is right it is the adjective right which is the root of the tree. Unlike that, sentences from the German corpora always have a verb as the root. To unify the formats, we write a set of rules to make the verb the root of the tree in all cases. Corpora and Annotation We apply our method to sentences from two corpora in English and German. These are presented below. English Compression Corpus: The English data we use is a document-based compression corpus from the British National Corpus and American News Text Corpus which consists of 82 news stories3 . We parsed the corpus with RASP (Briscoe et al., 2006) and with the Stanford PCFG parser (Klein & Manning, 2003). The output of the former is a set of dependency relations whereas the latter provides an option for converting the output into dependency format (de Marneffe et al., 2006) which we use. ¨ TuBa-D/Z: The German corpus we use is a collection of 1,000 newspaper articles (Telljohann et al., 2003)4 . Sentence boundaries, morphology, dependency structure and anaphoric relations are manually annotated in this corpus. RASP has been used by Clarke & Lapata (2008) whose state of the art results we compare with ours. We use not only RASP but also"
W08-1105,P06-1048,0,0.0615674,"Missing"
W08-1105,de-marneffe-etal-2006-generating,0,0.0759308,"Missing"
W08-1105,P07-1041,1,0.384468,"e of the constraints on word order in this language. One of the rules of German grammar states that in the main clause the inflected part of the verb occupies the second position, the first position being occupied by exactly one constituent. Therefore, if the sentence initial position in a source sentence is occupied by a constituent which got pruned off as a result of compression, the verb becomes the first element of the sentence which results in an undesirable output. There are linearization methods developed for German which find an optimal word order for a sentence (Ringger et al., 2004; Filippova & Strube, 2007). We use our recent method to linearize compressed trees. 4 (48 vs. 15 of RASP) which is not overly large (compared with the 106 grammatical relations of the Link Parser). This is important for our system which relies on syntactic information when making pruning decisions. A comparison between the Stanford parser and two dependency parsers, MiniPar and Link Parser, showed a decent performance of the former (de Marneffe et al., 2006). It is also of interest to see to what extent the choice of the parser influences the results. Apart from the corpora listed above, we use the Tipster corpus to ca"
W08-1105,P06-1041,0,0.021354,"onditional probabilities of syntactic labels given head lemmas as well as word significance scores. The significance score is calculated from the total number of 128 million nouns and verbs. Conditional probabilities are calculated from a much smaller portion of Tipster (about 6 million tokens). The latter number is comparable to the size of the data set we use to compute the probabilities for German. There, we use a corpus of about 4,000 articles from the German Wikipedia to calculate conditional probabilities and significance scores. The corpus is parsed with the highly accurate CDG parser (Foth & Menzel, 2006) and has the same dependency format as T¨uBa-D/Z (Versley, 2005). Although all corpora are annotated with dependency relations, there are considerable differences between the annotation of the English and German data sets. The phrase to dependency structure conversion done by the Stanford parser makes the semantic head of the constituent its syntactic head. For example, in the sentence He is right it is the adjective right which is the root of the tree. Unlike that, sentences from the German corpora always have a verb as the root. To unify the formats, we write a set of rules to make the verb"
W08-1105,N07-1023,0,0.0793227,"Missing"
W08-1105,P05-1036,0,0.0477236,"a subset of the words of S, such that it is grammatical and preserves essential information from S? There are many applications which would benefit from a robust compression system, such as subtitle generation, compression for mobile devices with a limited screen size, or news digests. Given that to date most text and speech summarization systems are extractive, sentence compression techniques are a common way to deal with redundancy in their output. In recent years, a number of approaches to sentence compression have been developed (Jing, 2001; Knight & Marcu, 2002; Gagnon & Da Sylva, 2005; Turner & Charniak, 2005; Clarke & Lapata, 2008, inter alia). Many explicitly rely on a language model, usually a trigram model, to produce grammatical output (Knight & Marcu, 2002; Hori & Furui, 2004; Turner & Charniak, 2005; Galley & McKIn this paper we present a novel unsupervised approach to sentence compression which is motivated by the belief that the grammaticality of the output can be better ensured by compressing trees. In particular, given a dependency tree, we want to prune subtrees which are neither obligatory syntactic arguments, nor contribute important information to the content of the sentence. A tree"
W08-1105,P03-1054,0,0.00894068,"e root of the tree. Unlike that, sentences from the German corpora always have a verb as the root. To unify the formats, we write a set of rules to make the verb the root of the tree in all cases. Corpora and Annotation We apply our method to sentences from two corpora in English and German. These are presented below. English Compression Corpus: The English data we use is a document-based compression corpus from the British National Corpus and American News Text Corpus which consists of 82 news stories3 . We parsed the corpus with RASP (Briscoe et al., 2006) and with the Stanford PCFG parser (Klein & Manning, 2003). The output of the former is a set of dependency relations whereas the latter provides an option for converting the output into dependency format (de Marneffe et al., 2006) which we use. ¨ TuBa-D/Z: The German corpus we use is a collection of 1,000 newspaper articles (Telljohann et al., 2003)4 . Sentence boundaries, morphology, dependency structure and anaphoric relations are manually annotated in this corpus. RASP has been used by Clarke & Lapata (2008) whose state of the art results we compare with ours. We use not only RASP but also the Stanford parser for several reasons. Apart from being"
W08-1105,E06-1038,0,0.303545,"icality and to decide whether a constituent is obligatory or may be pruned are to utilize a subcategorization lexicon (Jing, 2001), or to define a set of generally prunable constituents. Gagnon & Da Sylva (2005) prune dependency trees by removing prepositional complements of the verb, subordinate clauses and noun appositions. Apparently, this does not guarantee grammaticality in all cases. It may also eliminate important information from the tree. Most approaches are supervised and require training data to learn which words or constituents can be dropped from a sentence (Riezler et al., 2003; McDonald, 2006). However, it is difficult to obtain training data. Still, there are few unsupervised methods. For example, Hori & Furui (2004) introduce a scoring function which relies on such information sources as word significance score and language model. A compression of a given length which maximizes the scoring function is then found with dynamic programming. Clarke & Lapata (2008) present another unsupervised approach. They formulate the task as an optimization problem and solve it with integer linear programming. Two scores contribute to their objective function – a trigram language model score and"
W08-1105,N03-1026,0,0.0401705,"ways to ensure grammaticality and to decide whether a constituent is obligatory or may be pruned are to utilize a subcategorization lexicon (Jing, 2001), or to define a set of generally prunable constituents. Gagnon & Da Sylva (2005) prune dependency trees by removing prepositional complements of the verb, subordinate clauses and noun appositions. Apparently, this does not guarantee grammaticality in all cases. It may also eliminate important information from the tree. Most approaches are supervised and require training data to learn which words or constituents can be dropped from a sentence (Riezler et al., 2003; McDonald, 2006). However, it is difficult to obtain training data. Still, there are few unsupervised methods. For example, Hori & Furui (2004) introduce a scoring function which relies on such information sources as word significance score and language model. A compression of a given length which maximizes the scoring function is then found with dynamic programming. Clarke & Lapata (2008) present another unsupervised approach. They formulate the task as an optimization problem and solve it with integer linear programming. Two scores contribute to their objective function – a trigram language"
W08-1105,C04-1097,0,\N,Missing
W09-2814,W08-1109,0,0.0246511,"Missing"
W09-2814,P98-1013,0,0.0420771,"Missing"
W09-2814,W09-0628,0,0.0304965,"learned graphs, we will generate directions for a wide range of possible routes. Dale et al. (2005) developed a system that takes GIS data as input and uses a pipeline architecture to generate verbal route directions. In contrast to their approach, our approach will be based on an integrated architecture allowing for more interaction between the different stages of generation. The idea of combining verbal directions with scenes from a virtual 3D environment has recently lead to a new framework for evaluating NLG systems: The Challenge on Generating Instructions in Virtual Environments (GIVE) (Byron et al., 2009) is planned to become a regular event for the NLG community. This work describes first steps towards building a system that synchronously generates multimodal (textual and visual) route directions for pedestrians. We pursue a corpus-based approach for building a generation model that produces natural instructions in multiple languages. We conducted an empirical study to collect verbal route directions, and annotated the acquired texts on different levels. Here we describe the experimental setting and an analysis of the collected data. 1 Michael Strube† Introduction Route directions guide a per"
W09-2814,W07-1203,0,0.0433801,"Missing"
W09-2814,J09-2005,0,0.0246238,"Missing"
W09-2814,kunze-lemnitzer-2002-germanet,0,0.0204592,"Missing"
W09-2814,W05-0618,1,0.78264,"Missing"
W09-2814,J93-4001,0,0.0302683,"Missing"
W09-2814,P09-4010,1,0.701736,"surface generation model. We observed a variety of coherence-inducing elements that are generic in nature and thus seem well-suited for a corpusbased generation model. As other languages are known to exhibit differences in verbal realization of directions (von Stutterheim et al., 2002), we have to extend our data collection in order to generate systematic linguistic variations from a single underlying semantic structure for all languages. The linguistic annotation levels of frames and roles, syntactic dependencies, and basic word categories have been tested successfully with a similar corpus (Roth & Frank, 2009). The next steps will consist in the alignment of physical routes and landmarks with semantic representations in an integrated generation architecture. Table 3: Frequencies of temporal adverbs indicating linear (&gt; t ) and reversed linear order (&lt; t ) the following action or situation is not supposed to take place (e.g. Gehen Sie vorher rechts ‘beforehand turn right’). Backward-looking event anaphors and references to result states: We also found explicit references to past events (e.g. Nach dem Durchqueren ‘after traversing’) and result states of events, e.g. the adverbial phrase unten angekom"
W09-2814,burchardt-etal-2006-salto,1,\N,Missing
W09-2814,J03-1003,0,\N,Missing
W09-2814,C98-1013,0,\N,Missing
W10-4305,W06-1633,0,0.0235601,"e task description of the recent SemEval task on coreference resolution at http://stel.ub.edu/ semeval2010-coref), or ignores an important part of the problem by evaluating on key mentions only (Ponzetto & Strube, 2006; Bengtson & Roth, 2008, inter alia). We follow here Stoyanov et al. (2009, p.657) in arguing that such evaluations are “an unrealistic surrogate for the original problem” and ask researchers to evaluate end-toend coreference resolution systems. However, the evaluation of end-to-end coreference resolution systems has been inconsistent making it impossible to compare the results. Nicolae & Nicolae (2006) evaluate using the MUC score (Vilain et al., 1995) and the CEAF algorithm 2 Coreference Evaluation Metrics We discuss the problems which arise when applying the most prevalent coreference resolution evaluation metrics to end-to-end systems and propose our variants which overcome those problems. We provide detailed analyses of illustrative examples. 2.1 MUC The MUC score (Vilain et al., 1995) counts the minimum number of links between mentions to be inserted or deleted when mapping a system response to a gold standard key set. Although pairwise links capture the information in a set, they cann"
W10-4305,N06-1025,1,0.833119,"expression is referential and can take part in a coreferential relationship, and (2) deciding whether mentions are coreferent or not. Most recent research on coreference resolution simplifies the resolution task by providing the system with key mentions, i.e. already annotated mentions (Luo et al. (2004), Denis & Baldridge (2007), Culotta et al. (2007), Haghighi & Klein (2007), inter alia; see also the task description of the recent SemEval task on coreference resolution at http://stel.ub.edu/ semeval2010-coref), or ignores an important part of the problem by evaluating on key mentions only (Ponzetto & Strube, 2006; Bengtson & Roth, 2008, inter alia). We follow here Stoyanov et al. (2009, p.657) in arguing that such evaluations are “an unrealistic surrogate for the original problem” and ask researchers to evaluate end-toend coreference resolution systems. However, the evaluation of end-to-end coreference resolution systems has been inconsistent making it impossible to compare the results. Nicolae & Nicolae (2006) evaluate using the MUC score (Vilain et al., 1995) and the CEAF algorithm 2 Coreference Evaluation Metrics We discuss the problems which arise when applying the most prevalent coreference resol"
W10-4305,D09-1101,0,0.884664,"hen exposed to different mention taggers as shown in Tables 2 and 3. B3all manages to penalize erroneous resolutions for twinless system mentions, however, it ignores twinless key mentions when measuring precision. In Table 2, System 1 and System 2 generate the same outputs, except that the mention tagger in System 2 also extracts mention c. Intuitively, the same numbers are expected for both systems. However, B3all gives a higher precision to System 2, which results in a higher F-score. We assume that Rahman & Ng apply a strategy similar to B3all after the removing step (this is not clear in Rahman & Ng (2009)). While it avoids the problem with singleton twinless system mentions, B3r&n still suffers from the problem dealing with twinless key mentions, as illustrated in Table 2. 2.2.2 B3sys We here propose a coreference resolution evaluation metric, B3sys , which deals with system mentions more adequately (see the rows labeled B3sys in Tables 1, 2, 3, 4 and 5). We put all twinless key mentions into the response as singletons which enables B3sys to penalize non-resolved coreferent key mentions without penalizing non-resolved singleton key mentions, and also avoids the problem B3all and B3r&n have as"
W10-4305,P09-1074,0,0.174129,"Missing"
W10-4305,M95-1005,0,0.649288,"rence resolution at http://stel.ub.edu/ semeval2010-coref), or ignores an important part of the problem by evaluating on key mentions only (Ponzetto & Strube, 2006; Bengtson & Roth, 2008, inter alia). We follow here Stoyanov et al. (2009, p.657) in arguing that such evaluations are “an unrealistic surrogate for the original problem” and ask researchers to evaluate end-toend coreference resolution systems. However, the evaluation of end-to-end coreference resolution systems has been inconsistent making it impossible to compare the results. Nicolae & Nicolae (2006) evaluate using the MUC score (Vilain et al., 1995) and the CEAF algorithm 2 Coreference Evaluation Metrics We discuss the problems which arise when applying the most prevalent coreference resolution evaluation metrics to end-to-end systems and propose our variants which overcome those problems. We provide detailed analyses of illustrative examples. 2.1 MUC The MUC score (Vilain et al., 1995) counts the minimum number of links between mentions to be inserted or deleted when mapping a system response to a gold standard key set. Although pairwise links capture the information in a set, they cannot represent singleton entities, i.e. entities, whi"
W10-4305,J08-3002,0,0.0273048,"Missing"
W10-4305,D08-1031,0,0.185797,"l and can take part in a coreferential relationship, and (2) deciding whether mentions are coreferent or not. Most recent research on coreference resolution simplifies the resolution task by providing the system with key mentions, i.e. already annotated mentions (Luo et al. (2004), Denis & Baldridge (2007), Culotta et al. (2007), Haghighi & Klein (2007), inter alia; see also the task description of the recent SemEval task on coreference resolution at http://stel.ub.edu/ semeval2010-coref), or ignores an important part of the problem by evaluating on key mentions only (Ponzetto & Strube, 2006; Bengtson & Roth, 2008, inter alia). We follow here Stoyanov et al. (2009, p.657) in arguing that such evaluations are “an unrealistic surrogate for the original problem” and ask researchers to evaluate end-toend coreference resolution systems. However, the evaluation of end-to-end coreference resolution systems has been inconsistent making it impossible to compare the results. Nicolae & Nicolae (2006) evaluate using the MUC score (Vilain et al., 1995) and the CEAF algorithm 2 Coreference Evaluation Metrics We discuss the problems which arise when applying the most prevalent coreference resolution evaluation metric"
W10-4305,N07-1011,0,0.285471,"s dealing with system mentions, i.e. automatically determined mentions. Our experiments show that our variants lead to intuitive and reliable results. 1 Introduction The coreference resolution problem can be divided into two steps: (1) determining mentions, i.e., whether an expression is referential and can take part in a coreferential relationship, and (2) deciding whether mentions are coreferent or not. Most recent research on coreference resolution simplifies the resolution task by providing the system with key mentions, i.e. already annotated mentions (Luo et al. (2004), Denis & Baldridge (2007), Culotta et al. (2007), Haghighi & Klein (2007), inter alia; see also the task description of the recent SemEval task on coreference resolution at http://stel.ub.edu/ semeval2010-coref), or ignores an important part of the problem by evaluating on key mentions only (Ponzetto & Strube, 2006; Bengtson & Roth, 2008, inter alia). We follow here Stoyanov et al. (2009, p.657) in arguing that such evaluations are “an unrealistic surrogate for the original problem” and ask researchers to evaluate end-toend coreference resolution systems. However, the evaluation of end-to-end coreference resolution sy"
W10-4305,N07-1030,0,0.0286055,"resolution systems dealing with system mentions, i.e. automatically determined mentions. Our experiments show that our variants lead to intuitive and reliable results. 1 Introduction The coreference resolution problem can be divided into two steps: (1) determining mentions, i.e., whether an expression is referential and can take part in a coreferential relationship, and (2) deciding whether mentions are coreferent or not. Most recent research on coreference resolution simplifies the resolution task by providing the system with key mentions, i.e. already annotated mentions (Luo et al. (2004), Denis & Baldridge (2007), Culotta et al. (2007), Haghighi & Klein (2007), inter alia; see also the task description of the recent SemEval task on coreference resolution at http://stel.ub.edu/ semeval2010-coref), or ignores an important part of the problem by evaluating on key mentions only (Ponzetto & Strube, 2006; Bengtson & Roth, 2008, inter alia). We follow here Stoyanov et al. (2009, p.657) in arguing that such evaluations are “an unrealistic surrogate for the original problem” and ask researchers to evaluate end-toend coreference resolution systems. However, the evaluation of end-to-end coreference resolution sy"
W10-4305,P07-1107,0,0.0174283,"i.e. automatically determined mentions. Our experiments show that our variants lead to intuitive and reliable results. 1 Introduction The coreference resolution problem can be divided into two steps: (1) determining mentions, i.e., whether an expression is referential and can take part in a coreferential relationship, and (2) deciding whether mentions are coreferent or not. Most recent research on coreference resolution simplifies the resolution task by providing the system with key mentions, i.e. already annotated mentions (Luo et al. (2004), Denis & Baldridge (2007), Culotta et al. (2007), Haghighi & Klein (2007), inter alia; see also the task description of the recent SemEval task on coreference resolution at http://stel.ub.edu/ semeval2010-coref), or ignores an important part of the problem by evaluating on key mentions only (Ponzetto & Strube, 2006; Bengtson & Roth, 2008, inter alia). We follow here Stoyanov et al. (2009, p.657) in arguing that such evaluations are “an unrealistic surrogate for the original problem” and ask researchers to evaluate end-toend coreference resolution systems. However, the evaluation of end-to-end coreference resolution systems has been inconsistent making it impossible"
W10-4305,H05-1004,0,0.63043,"ce twinless system mentions do not have mappings in key, they contribute nothing to the mapping similarity. So, resolution mistakes for system mentions are not calculated, and moreover, the precision is easily skewed by the number of output entities. CEAForig reports very low precision for system mentions (see also Stoyanov et al. (2009)). . = 0.556 Thus the F-score number is calculated as: FBsys =2× 3 0.611×0.556 0.611+0.556 . = 0.582 B3sys indicates more adequately the performance of end-to-end coreference resolution systems. It is not easily tricked by different mention taggers3 . 2.3 CEAF Luo (2005) criticizes the B3 algorithm for using entities more than one time, because B3 computes precision and recall of mentions by comparing entities containing that mention. Hence Luo proposes the CEAF algorithm which aligns entities in key and response. CEAF applies a similarity metric (which could be either mention based or entity based) for each pair of entities (i.e. a set of mentions) to measure the goodness of each possible alignment. The best mapping is used for calculating CEAF precision, recall and F-measure. Luo proposes two entity based similarity metrics (Equation 3 and 4) for an entity"
W10-4305,P04-1018,0,0.0998019,"eference resolution systems dealing with system mentions, i.e. automatically determined mentions. Our experiments show that our variants lead to intuitive and reliable results. 1 Introduction The coreference resolution problem can be divided into two steps: (1) determining mentions, i.e., whether an expression is referential and can take part in a coreferential relationship, and (2) deciding whether mentions are coreferent or not. Most recent research on coreference resolution simplifies the resolution task by providing the system with key mentions, i.e. already annotated mentions (Luo et al. (2004), Denis & Baldridge (2007), Culotta et al. (2007), Haghighi & Klein (2007), inter alia; see also the task description of the recent SemEval task on coreference resolution at http://stel.ub.edu/ semeval2010-coref), or ignores an important part of the problem by evaluating on key mentions only (Ponzetto & Strube, 2006; Bengtson & Roth, 2008, inter alia). We follow here Stoyanov et al. (2009, p.657) in arguing that such evaluations are “an unrealistic surrogate for the original problem” and ask researchers to evaluate end-toend coreference resolution systems. However, the evaluation of end-to-end"
W10-4305,D08-1067,0,\N,Missing
W10-4305,J01-4004,0,\N,Missing
W11-1907,C10-1017,1,0.596586,"ple strategy of assigning edge weights, only a reasonable size of training data is needed. Learner. Instead of connecting nodes with the target relation as usually done in graph models, COPA builds the graph directly out of low dimensional features without assuming a distance metric. 3.2.2 HGResolver In order to partition the hypergraph we adopt a spectral clustering algorithm (Agarwal et al., 2005). All experimental results are obtained using symmetric Laplacians (Lsym ) (von Luxburg, 2007). We apply the recursive variant of spectral clustering, recursive 2-way partitioning (R2 partitioner) (Cai and Strube, 2010). This method does not need any information about the number of target sets (the number k of clusters). Instead a stopping criterion α⋆ has to be provided which is adjusted on development data. 3.3 Complexity of HGResolver Since edge weights are assigned using simple descriptive statistics, the time HGResolver needs for building the graph Laplacian matrix is not substantial. For eigensolving, we use an open source library provided by the Colt project1 which implements a Householder-QL algorithm to solve the eigenvalue decomposition. When applied to the symmetric graph Laplacian, the complexity"
W11-1907,P05-1022,0,0.0450688,"olkit (Versley et al., 2008). Documents are transformed into the MMAX2-format (M¨uller and Strube, 2006) which allows for easy visualization and (linguistic) debugging. Each document is stored in several XML-files representing different layers of annotations. These annotations are created by a pipeline of preprocessing components. We use the Stanford MaxentTagger (Toutanova et al., 2003) for partof-speech tagging, and the Stanford Named Entity Recognizer (Finkel et al., 2005) for annotating named entities. In order to derive syntactic information, we use the Charniak/Johnson reranking parser (Charniak and Johnson, 2005) com56 Proceedings of the 15th Conference on Computational Natural Language Learning: Shared Task, pages 56–60, c Portland, Oregon, 23-24 June 2011. 2011 Association for Computational Linguistics bined with a constituent-to-dependency conversion Tool (http://nlp.cs.lth.se/software/ treebank_converter). The preprocessing models are not trained on CoNLL data, so we only participated in the open task. We have implemented an in-house mention detector, which makes use of the parsing output, the partof-speech tags, as well as the chunks from the Yamcha Chunker (Kudoh and Matsumoto, 2000). For the On"
W11-1907,P05-1045,0,0.0151856,"ake extensive use of information beyond information from the closed class setting. 2 Preprocessing COPA is implemented on top of the BART-toolkit (Versley et al., 2008). Documents are transformed into the MMAX2-format (M¨uller and Strube, 2006) which allows for easy visualization and (linguistic) debugging. Each document is stored in several XML-files representing different layers of annotations. These annotations are created by a pipeline of preprocessing components. We use the Stanford MaxentTagger (Toutanova et al., 2003) for partof-speech tagging, and the Stanford Named Entity Recognizer (Finkel et al., 2005) for annotating named entities. In order to derive syntactic information, we use the Charniak/Johnson reranking parser (Charniak and Johnson, 2005) com56 Proceedings of the 15th Conference on Computational Natural Language Learning: Shared Task, pages 56–60, c Portland, Oregon, 23-24 June 2011. 2011 Association for Computational Linguistics bined with a constituent-to-dependency conversion Tool (http://nlp.cs.lth.se/software/ treebank_converter). The preprocessing models are not trained on CoNLL data, so we only participated in the open task. We have implemented an in-house mention detector, w"
W11-1907,W00-0730,0,0.0565432,"ng parser (Charniak and Johnson, 2005) com56 Proceedings of the 15th Conference on Computational Natural Language Learning: Shared Task, pages 56–60, c Portland, Oregon, 23-24 June 2011. 2011 Association for Computational Linguistics bined with a constituent-to-dependency conversion Tool (http://nlp.cs.lth.se/software/ treebank_converter). The preprocessing models are not trained on CoNLL data, so we only participated in the open task. We have implemented an in-house mention detector, which makes use of the parsing output, the partof-speech tags, as well as the chunks from the Yamcha Chunker (Kudoh and Matsumoto, 2000). For the OntoNotes data, the mention detector annotates the biggest noun phrase spans. 3 COPA: Coreference Partitioner The COPA system consists of modules which derive hyperedges from features and assign edge weights indicating a positive correlation with the coreference relation, and resolution modules which create a hypergraph representation for the testing data and perform partitioning to produce subhypergraphs, each of which represents an entity. 3.1 HyperEdgeCreator COPA needs training data only for computing the hyperedge weights. Hyperedges represent features. Each hyperedge correspond"
W11-1907,P09-1074,0,0.0477475,"not agree in semantic class (only the Object, Date and Person top categories derived from WordNet (Fellbaum, 1998) are used). (4) N Mod: Two mentions have the same syntactic heads, and the anaphor has a pre-modifier which does not occur in the antecedent and does not contradict the antecedent. (5) N DSPrn: Two first person pronouns in direct speeches assigned to different speakers. (6) N ContraSubjObj: Two mentions are in the subject and object positions of the same verb, and the anaphor is a non-possesive pronoun. 4.2 Positive Features The majority of well studied coreference features (e.g. Stoyanov et al. (2009)) are actually positive coreference indicators. In our system, the mentions which participate in positive relations are included in the graph representation. (7) StrMatch Npron & (8) StrMatch Pron: After discarding stop words, if the strings of mentions completely match and are not pronouns, they are put into edges of the StrMatch Npron type. When the matched mentions are pronouns, they are put into the StrMatch Pron type edges. (9) Alias: After discarding stop words, if mentions are aliases of each other (i.e. proper names with 58 partial match, full names and acronyms, etc.). (10) HeadMatch:"
W11-1907,N03-1033,0,0.0139958,"e CoNLL shared task on modeling unrestricted coreference (Pradhan et al., 2011). We did not make extensive use of information beyond information from the closed class setting. 2 Preprocessing COPA is implemented on top of the BART-toolkit (Versley et al., 2008). Documents are transformed into the MMAX2-format (M¨uller and Strube, 2006) which allows for easy visualization and (linguistic) debugging. Each document is stored in several XML-files representing different layers of annotations. These annotations are created by a pipeline of preprocessing components. We use the Stanford MaxentTagger (Toutanova et al., 2003) for partof-speech tagging, and the Stanford Named Entity Recognizer (Finkel et al., 2005) for annotating named entities. In order to derive syntactic information, we use the Charniak/Johnson reranking parser (Charniak and Johnson, 2005) com56 Proceedings of the 15th Conference on Computational Natural Language Learning: Shared Task, pages 56–60, c Portland, Oregon, 23-24 June 2011. 2011 Association for Computational Linguistics bined with a constituent-to-dependency conversion Tool (http://nlp.cs.lth.se/software/ treebank_converter). The preprocessing models are not trained on CoNLL data, so"
W11-1907,P08-4003,0,0.208733,"venience. While COPA has been developed originally to perform coreference resolution on MUC and ACE data (Cai and Strube, 2010), the move to the OntoNotes data (Weischedel et al., 2011) required mainly to update the mention detector and the feature set. Since several off-the-shelf preprocessing components are used, COPA participated in the open setting of the CoNLL shared task on modeling unrestricted coreference (Pradhan et al., 2011). We did not make extensive use of information beyond information from the closed class setting. 2 Preprocessing COPA is implemented on top of the BART-toolkit (Versley et al., 2008). Documents are transformed into the MMAX2-format (M¨uller and Strube, 2006) which allows for easy visualization and (linguistic) debugging. Each document is stored in several XML-files representing different layers of annotations. These annotations are created by a pipeline of preprocessing components. We use the Stanford MaxentTagger (Toutanova et al., 2003) for partof-speech tagging, and the Stanford Named Entity Recognizer (Finkel et al., 2005) for annotating named entities. In order to derive syntactic information, we use the Charniak/Johnson reranking parser (Charniak and Johnson, 2005)"
W11-1907,W11-1901,0,\N,Missing
W12-4511,P06-1005,0,0.152267,"transform the predicted constituent parse trees into dependency parse trees. We work with two different dependency representations, one obtained via the converter implemented 1 http://nlp.stanford.edu/software/ lex-parser.shtml 101 in Stanford’s NLP suite2 , the other using LTH’s constituent-to-dependency conversion tool3 . For pronouns, we determine number and gender using tables containing a mapping of pronouns to their gender and number. 2.2.1 English For English, number and gender for common nouns are computed via a comparison of head lemma to head and using the number and gender data of Bergsma and Lin (2006). Quantified noun phrases are always plural. We compute semantic classes via a WordNet (Fellbaum, 1998) lookup. 2.2.2 Chinese For Chinese, we simply determine number and gender by searching for the corresponding designators, since plural mentions mostly end with 们, while 先生 (sir) and 女士 (lady) often suggest gender information. To identify demonstrative and definite noun phrases, we check whether they start with a definite/demonstrative indicator (e.g. 这 (this) or 那 (that)). We use lists of named entities extracted from the training data to determine named entities and their semantic class in d"
W12-4511,W11-1907,1,0.738437,"Missing"
W12-4511,D09-1120,0,0.0710331,"Again, we go through M in a left-to-right fashion. If for two mentions mi , mj with i &gt; j a negative relation R− holds, no edge between mi and mj can be built. Otherwise we add an edge from mi to mj for every positive relation R+ such that R+ (mi , mj ) is true. The structure obtained by this construction is a directed multigraph. We handle copula relations similar to Lee et al. (2011): if mi is this and the pair (mi , mj ) is in a copula relation (like This is the World), we remove mj and replace mj in all edges involving it by mi . For Chinese, we handle “role appositives” as introduced by Haghighi and Klein (2009) analogously. 2.4.1 Assigning Weights to Edges Initially, any edge (mi , mj ) induced by the relation R has the weight wR computed as described in Section 2.3. If R is a transitive relation, we divide the weight by the number of mentions connected by this relation. This corresponds to the way edge weights are assigned during the spectral embedding in Cai et al. (2011b). If R is a relation sensitive to distance like compatibility between a common/proper noun and a pronoun, the weight is altered according to the distance between mi and mj . 2.4.2 An Example We demonstrate the graph construction"
W12-4511,W11-1902,0,0.300143,"ng the SemanticHeadFinder from the Stanford Parser1 . The head for a proper noun starts at the first token tagged as a noun until a punctuation, preposition or subclause is encountered. Coordinations have the CC tagged token as head and quantifying noun phrases have the quantifier as head. In a postprocessing step we filter out adjectival use of nations and named entities with semantic class Money, Percent or Cardinal. We discard mentions whose head is embedded in another mention’s head. Pleonastic pronouns are identified and discarded via a modified version of the patterns used by Lee et al. (2011). 2.1.2 Chinese For Chinese we detect four mention types: common noun, proper noun, pronoun and coordination. The head detection for Chinese is provided by the SunJurafskyChineseHeadFinder from the Standford Parser, except for proper nouns whose head is set to the mention’s rightmost token. The remaining processing is similar to the mention detection for English. 2.2 Preprocessing We extract the information in the provided annotation layers and transform the predicted constituent parse trees into dependency parse trees. We work with two different dependency representations, one obtained via th"
W12-4511,W12-4501,0,0.108768,"ent, where the nodes correspond to mentions and the edges correspond to relations between the mentions. Entities are obtained via greedy clustering. We participated in the closed tasks for English and Chinese. Our system ranked second in the English closed task. 1 2 Introduction Coreference resolution is the task of determining which mentions in a text refer to the same entity. This paper describes HITS’ system for the CoNLL2012 Shared Task on multilingual unrestricted coreference resolution, where the goal is to build a system for coreference resolution in an end-to-end multilingual setting (Pradhan et al., 2012). We participated in the closed tasks for English and Chinese and focused on English. Our system ranked second in the English closed task. Being conceptually similar to and building upon Cai et al. (2011b), our system is based on a directed multigraph representation of a document. A multigraph is a graph where two nodes can be connected by more than one edge. In our model, nodes represent mentions and edges are built from relations between the mentions. The entities to be inferred correspond to clusters in the multigraph. Architecture Our system is implemented on top of the BART toolkit (Versl"
W12-4511,P08-4003,0,0.0675141,"2012). We participated in the closed tasks for English and Chinese and focused on English. Our system ranked second in the English closed task. Being conceptually similar to and building upon Cai et al. (2011b), our system is based on a directed multigraph representation of a document. A multigraph is a graph where two nodes can be connected by more than one edge. In our model, nodes represent mentions and edges are built from relations between the mentions. The entities to be inferred correspond to clusters in the multigraph. Architecture Our system is implemented on top of the BART toolkit (Versley et al., 2008). To compute the coreference clusters in a document, we first extract a set of mentions M = {m1 , . . . , mn } ordered according to their position in the text (Section 2.1). We then build a directed multigraph where the set of nodes is M and edges are induced by relations between mentions (Section 2.4). The relations we use in our system are coreference indicators like string matching or alias (Section 3). For every relation R, we compute a weight wR using the training data (Section 2.3). We then assign the weight wR to any edge that is induced by the relation R. Depending on distance and conn"
W14-3701,P05-1000,0,0.344475,"he weight of an edge which connects sentence si and entity ej . If w(si , ej ) = 1, then this edge indicates that there is a mention of ej in sentence si . In order to realize the insight from Grosz et al. (1995) that certain syntactic roles are more important than others, the syntactic role of ej in si can be mapped to an integer value (Guinaudeau and Strube, 2013): ( Introduction Guinaudeau and Strube (2013) introduce a graph based model (henceforth called entity graph) to compute local entity coherence. Despite being unsupervised, the entity graph performs on par with Barzilay and Lapata’s (2005; 2008) supervised entity grid on the tasks of sentence ordering, summary coherence rating and readability assessment. The entity graph also overcomes shortcomings of the entity grid with regard to computational complexity, data sparsity and domain dependence. The entity graph is a bipartite graph where one set of nodes represents entities and the other set of nodes represents the sentences of a document. Guinaudeau and Strube (2013) apply a one mode projection on sentence nodes (Newman, 2010) and then compute the average out-degree of sentence nodes to determine how coherent a document is. They desc"
W14-3701,P05-1018,0,0.0338128,"Missing"
W14-3701,J08-1001,0,0.648893,"e∈Eik 1 Proceedings of TextGraphs-9: the workshop on Graph-based Methods for Natural Language Processing, pages 1–5, c October 29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics Figure 2 shows the three kinds of one-mode projections used in the entity graph. 1 S1 S2 S1 1 1 S2 9 S3 P PW 1 6 S2 e1 e3 e5 e4 7 7 1 8 1 8 1 3 e6 1 7 2 7 e7 e8 1 7 e9 e10 Acc For PAcc we divide the weight of each edge by the sum of all edges’ weights of a sentence. This gives the importance of each entity in a sentence relative to the sentence’s other entities (see Figure 3). While the entity grid (Barzilay and Lapata, 2008) uses information about sentences which do not share entities by means of the “- -” transition, the entity graph cannot employ this negative information. Here, we propose a normalization for the entity graph and its corresponding one-mode projections which is based on the relative importance of entities and, in turn, the relative importance of sentences. Including negative information allows to normalize the importance of entities according to sentence length (measured in terms of entity mentions), and hence to capture distance information between mentions of the same entity. This brings the e"
W14-3701,P11-2022,0,0.0175531,"rmalized projections 4 Sentence Ordering This task consists of two subtasks: discrimination and insertion. In both subtasks we evaluate whether our model can distinguish between the correct order of sentences in a document and an incorrect one. Experimental setup and data follow Guinaudeau and Strube (2013) (61 documents from the English test part of the CoNLL 2012 shared task (Pradhan et al., 2012)). For discrimination we use 20 permutations of each text. Table 1 shows the results. Results for Guinaudeau and Strube (2013), G&S, are reproduced, results for Barzilay and Lapata (2008), B&L, and Elsner and Charniak (2011), E&C, were reproduced by Guinaudeau and Strube (2013). The unweighted graph, PU , does not need normalization. Hence the results for the entity graph and the normalized entity graph are identical. Normalization improves the results for the weighted graphs PW and PAcc with PAcc outperforming B&L considerably and closely approaching E&L. Sentence insertion is more difficult than discrimination. Following Elsner and Charniak (2011), we use two measures for evaluation: Accuracy (Acc.) and the average proportion of correct insertions per document (Ins.). 64 S1 4 1 S3 4.1 27 2 8 Acc F 0.496 0.496 0"
W14-3701,J95-2003,0,0.17581,"ummary coherence rating and readability assessment. In all tasks normalization improves the results. 1 The Entity Graph The entity graph (Guinaudeau and Strube, 2013), G = (V, E), represents the relations between sentences and entities in a text, where node set V contains all sentences and entities in a text and E is the set of all edges between sentences and entities. Let function w(si , ej ) indicate the weight of an edge which connects sentence si and entity ej . If w(si , ej ) = 1, then this edge indicates that there is a mention of ej in sentence si . In order to realize the insight from Grosz et al. (1995) that certain syntactic roles are more important than others, the syntactic role of ej in si can be mapped to an integer value (Guinaudeau and Strube, 2013): ( Introduction Guinaudeau and Strube (2013) introduce a graph based model (henceforth called entity graph) to compute local entity coherence. Despite being unsupervised, the entity graph performs on par with Barzilay and Lapata’s (2005; 2008) supervised entity grid on the tasks of sentence ordering, summary coherence rating and readability assessment. The entity graph also overcomes shortcomings of the entity grid with regard to computati"
W14-3701,P13-1010,1,0.910061,"entences and entities in a text, where node set V contains all sentences and entities in a text and E is the set of all edges between sentences and entities. Let function w(si , ej ) indicate the weight of an edge which connects sentence si and entity ej . If w(si , ej ) = 1, then this edge indicates that there is a mention of ej in sentence si . In order to realize the insight from Grosz et al. (1995) that certain syntactic roles are more important than others, the syntactic role of ej in si can be mapped to an integer value (Guinaudeau and Strube, 2013): ( Introduction Guinaudeau and Strube (2013) introduce a graph based model (henceforth called entity graph) to compute local entity coherence. Despite being unsupervised, the entity graph performs on par with Barzilay and Lapata’s (2005; 2008) supervised entity grid on the tasks of sentence ordering, summary coherence rating and readability assessment. The entity graph also overcomes shortcomings of the entity grid with regard to computational complexity, data sparsity and domain dependence. The entity graph is a bipartite graph where one set of nodes represents entities and the other set of nodes represents the sentences of a document."
W14-3701,D08-1020,0,0.157071,"Missing"
W14-3701,W12-4501,0,0.0380218,"graph, so the number of edges of all paths are equal to two. Figure 6 shows the normalized projections where the weights have been computed by the above formula. S1 1 S2 S1 S2 S3 P P U 8 S2 W S3 23 56 P Acc Figure 6: Normalized projections 4 Sentence Ordering This task consists of two subtasks: discrimination and insertion. In both subtasks we evaluate whether our model can distinguish between the correct order of sentences in a document and an incorrect one. Experimental setup and data follow Guinaudeau and Strube (2013) (61 documents from the English test part of the CoNLL 2012 shared task (Pradhan et al., 2012)). For discrimination we use 20 permutations of each text. Table 1 shows the results. Results for Guinaudeau and Strube (2013), G&S, are reproduced, results for Barzilay and Lapata (2008), B&L, and Elsner and Charniak (2011), E&C, were reproduced by Guinaudeau and Strube (2013). The unweighted graph, PU , does not need normalization. Hence the results for the entity graph and the normalized entity graph are identical. Normalization improves the results for the weighted graphs PW and PAcc with PAcc outperforming B&L considerably and closely approaching E&L. Sentence insertion is more difficult"
W14-3701,P05-1065,0,0.0649516,"Missing"
W14-3703,J08-1001,0,0.0448912,"sing the graphical representation of the text followed by a description how to quantify the importance of sentences in the input texts. We then discuss the ILP technique which optimizes the importance of sentences and redundancy. 3.1 Graphical representation of text The graphical representation of a text makes it more expressive than a traditional tf-idf depiction for summarization. A graph can easily capture the essence of the whole text without leading to high computational complexity. Guinaudeau and Strube (2013) introduced a bipartite graph representation of text based on the entity grid (Barzilay and Lapata, 2008) representation of text. The projection of this bipartite graph representation has been used for calculating the local coherence of a text (Guinaudeau and Strube, 2013). The basic intuition to use a bipartite graph for summarization is that it contains entity transitions similar to lexical chains (Barzilay and Elhadad, 1999). An appropriate measure to determine the importance of sentences by considering strong entity transitions indicates the information central to a text better than simply giving scores on the basis of most frequent words. The unweighted bipartite graph G = (Vs , Ve , L) cont"
W14-3703,P11-2022,0,0.140115,"al chains (Barzilay and Elhadad, 1999). An appropriate measure to determine the importance of sentences by considering strong entity transitions indicates the information central to a text better than simply giving scores on the basis of most frequent words. The unweighted bipartite graph G = (Vs , Ve , L) contains two sets of nodes, Vs corresponding to the sentences from the input text and Ve corresonding to the entities, and a set of edges represented by L. Figure 1 shows a model summary from the DUC 2006 data, which is transformed into an entity grid in Figure 2 (Barzilay and Lapata, 2008; Elsner and Charniak, 2011). Here, cells are filled with the syntactic role a mention of an entity occupies in a sentence. Subjects are denoted by S, objects by O and all other roles by X. If an entity is not mentioned in a sentence then the corresponding cell contains “-”. In the corresponding bipartite graph (Figure 3), edges are created between a sentence and an entity only if the entity is mentioned in a sentence (the cell in entity grid is not “-”). Since this is a dyadic graph, there are no edges between nodes of the same set. 3.2 • Hub update rule: Update each node’s hub score to be equal to the sum of the author"
W14-3703,P11-1049,0,0.0246198,"to summary) and 0 (doesn’t belong to summary) (Shen et al., 2007; Gong and Liu, 2001). Here, the sentences are projected as feature vectors and conditional random fields are used to classify them. During document processing, most informative sentences are selected by the summarizer (Shen et al., 2007). Fattah and Ren (2009) also considMcDonald (2007) proposes a new ILP optimization method for extractive summarization. He introduces an objective function which maximizes the importance of sentences and minimizes the similarity of sentences. ILP methods for optimization have also been adopted by Berg-Kirkpatrick et al. (2011),Woodsend and Lapata (2012) and Galanis et al. (2012). Until now, Galanis et al. (2012) have reported the highest scores for multi-document summarization on DUC2005 and DUC2007. However, their approach is not completely unsupervised. 16 3 Our method trast to the local information specific to a vertex, graphical ranking algorithms take (graph-) global information to calculate the rank of a node. The Hyperlink-Induced Topic Search algorithm (HITS, also known as Hubs and Authorities) by Kleinberg (1999) is used to rank sentences in our method. This algorithm considers two types of nodes, hence it"
W14-3703,C12-1056,0,0.177707,"nt way. A widely used technique is the greedy approach introduced by Carbonell and Goldstein (1998) and Goldstein et al. (2000). They compute a relevance score for all sentences with regard to the topic, start by extracting the most relevant sentence, and then iteratively extract further sentences which are relevant to the topic and at the same time most dissimilar to already extracted sentences. Later more fundamental optimization methods have been widely used in multi-document summarization, e.g. Integer Linear Programming (ILP) (McDonald, 2007; Gillick et al., 2009; Nishikawa et al., 2010; Galanis et al., 2012). Unlike most other approaches (Galanis et al., 2012) has also taken into account the readability of the final summary. In this work, we introduce an extractive topic based multi-document summarization system which represents documents graphically and Introduction Topic-based multi-document summarization aims to create a single summary from a set of given documents while considering the topic of interest. The input documents can be created by querying an information retrieval or search engine for a particular topic and retaining highly ranked documents, or by clustering documents of a large co"
W14-3703,P10-1084,0,0.048571,"Missing"
W14-3703,P13-3012,0,0.021334,"orming ILP. ILP returns a binary value, i.e., if a sentence should be included in the summary it returns 1, if not it returns 0. We set λ1 = 0.7 and λ2 = 0.3 for all datasets. We did not choose the optimal values, but rather opted for ones which favor importance over non-redundancy. We did not observe significant differences between different λ values as long as λ1 > λ2 (see Table 2). The sentences in the output summary are ordered according to their ranks. If the output summary contains pronouns, we perform pronoun resolution in the source documents using the coreference resolution system by Martschat (2013). If pronoun and antecedent occur in the same sentence, we leave the pronoun. If the antecedent occurs in an earlier sentence, we replace the pronoun in the summary by the first element of the coreference chain the pronoun belongs to. Except for setting λ1 and λ2 on DUC 2005, our approach is unsupervised, as there is no traning data required. The recall (ROUGE) scores on different datasets are shown in Table 3. Table 3 shows that our system would have performed very well in the DUC 2005 and DUC 2006 competitions with ranks in the top 3 and well in the DUC 2007 competition. Since the competitio"
W14-3703,W00-0405,0,0.779642,"should not contain the same information twice. 3. Readability: A summary should have good readability (syntactically well formed, no dangling pronouns, coherent, . . . ). Generally, multi-document summarization systems differ from each other on the basis of document representation, sentence selection method or on the requirements for the output summary. Popular methods for document representation include graph-based representations (e.g. LexRank (Erkan and Radev, 2004) and TextRank (Mihalcea and Tarau, 2004)) and tf-idf vector-based representations (Luhn, 1958; Nenkova and Vanderwende, 2005; Goldstein et al., 2000). These document representations act as input for the next phase and provide information about the importance of individual sentences. Sentence selection is the crucial phase of the summarizer where sentence redundancy must be handled in an efficient way. A widely used technique is the greedy approach introduced by Carbonell and Goldstein (1998) and Goldstein et al. (2000). They compute a relevance score for all sentences with regard to the topic, start by extracting the most relevant sentence, and then iteratively extract further sentences which are relevant to the topic and at the same time"
W14-3703,P13-1010,1,0.847586,"ction describes the technique, which we adopted for summarization. We start by discussing the graphical representation of the text followed by a description how to quantify the importance of sentences in the input texts. We then discuss the ILP technique which optimizes the importance of sentences and redundancy. 3.1 Graphical representation of text The graphical representation of a text makes it more expressive than a traditional tf-idf depiction for summarization. A graph can easily capture the essence of the whole text without leading to high computational complexity. Guinaudeau and Strube (2013) introduced a bipartite graph representation of text based on the entity grid (Barzilay and Lapata, 2008) representation of text. The projection of this bipartite graph representation has been used for calculating the local coherence of a text (Guinaudeau and Strube, 2013). The basic intuition to use a bipartite graph for summarization is that it contains entity transitions similar to lexical chains (Barzilay and Elhadad, 1999). An appropriate measure to determine the importance of sentences by considering strong entity transitions indicates the information central to a text better than simply"
W14-3703,N09-1041,0,0.0322732,"0.16735 (5) Table 3: System performance (and rank) on the DUC 2005, 2006 and 2007 (main) data. The number in parenthesis after the DUC year indicates the number of competing systems. (2012) report the best results on DUC 2005 data. While their ROUGE-2 score is slightly better than ours, we outperform them in terms of ROUGESU4 (0.14105 vs. 0.13640), where, to our knowledge, our results are the highest reported so far. However, their results on DUC 2007 (ROUGE-2 0.12517 and ROUGE-SU4 0.17603) are still quite a bit better than our results. On the DUC 2006 data we outperform the HIERSUM system by Haghighi and Vanderwende (2009) on ROUGE2 (0.08969 vs. 0.086) as well as on ROUGESU4 (0.15070 vs. 0.143). On the DUC 2007 data, our results are worse than theirs on ROUGE2 (0.10928 vs. 0.118) and on par on ROUGESU4 (0.16735 vs. 0.167). The system which won the DUC 2007 task, PYTHY by Toutanova et al. (2007), performs similar to HIERSUM and hence slightly better than our system on these data. The recent work by Suzuki and Fukumoto (2014) evaluates also on DUC 2007 but reports only ROUGE1 scores. We obtain a ROUGE-1 score of 0.448 on DUC 2007 which is better than Suzuki and Fukumoto (2014) (0.438) as well as PYTHY (0.426). Th"
W14-3703,P04-3020,0,0.0398705,"eas clustering reduces the redundancy. Related work A graph-based representation of documents for summarization is adopted by various approaches. For instance, TextRank by Mihalcea and Tarau (2004) applies the PageRank algorithm (Brin and Page, 1998) to extract important sentences for single document summarization. This ranking algorithm proclaims the importance of a sentence by considering the global information which is computed recursively from the entire graph. Later, the graph is converted into a weighted graph in which the weights are calculated by measuring the similarity of sentences (Mihalcea, 2004). Similarly, in the LexRank approach (Erkan and Radev, 2004), documents are represented as a similarity graph in which the sentences are nodes and these sentences are then ranked according to centrality measures. The three centrality measures used are degree, LexRank with threshold and continuous LexRank. LexRank is a measure to calculate ranks using the similarity graph of sentences. It is also known as lexical PageRank. The summarization approach developed by Gong and Liu (2001) is also based on ranking sentences where important sentences are selected using a relevance measure and latent sem"
W14-3703,E14-1075,0,0.0117929,"d by the Maximum Marginal Relevance (MMR) approach of Carbonell and Goldstein (1998)) and applied it to clusters of passages. Similarly, influenced by the SumBasic system (Nenkova and Vanderwende, 2005), Yih et al. (2007) developed a system which assigns a score to each term on the basis of position and frequency information and selects the sentence having highest score. Other approaches are based on an estimate of word importance (e.g. Lin and Hovy (2000)) or the log likelihood ratio test which identifies the importance of words using a supervised model that considers a rich set of features (Hong and Nenkova, 2014). Finally, Barzilay and Elhadad (1999) extract sentences which are strongly connected by lexical chains for summarization. The second approach deals with relevance and redundancy seperately. For instance, McKeown et al. (1999) create clusters of similar sentences and pick the representative one from every cluster. The representative sentence of a cluster of sentences takes care of the requirement to extract relevant information whereas clustering reduces the redundancy. Related work A graph-based representation of documents for summarization is adopted by various approaches. For instance, Text"
W14-3703,C10-2105,0,0.0193056,"be handled in an efficient way. A widely used technique is the greedy approach introduced by Carbonell and Goldstein (1998) and Goldstein et al. (2000). They compute a relevance score for all sentences with regard to the topic, start by extracting the most relevant sentence, and then iteratively extract further sentences which are relevant to the topic and at the same time most dissimilar to already extracted sentences. Later more fundamental optimization methods have been widely used in multi-document summarization, e.g. Integer Linear Programming (ILP) (McDonald, 2007; Gillick et al., 2009; Nishikawa et al., 2010; Galanis et al., 2012). Unlike most other approaches (Galanis et al., 2012) has also taken into account the readability of the final summary. In this work, we introduce an extractive topic based multi-document summarization system which represents documents graphically and Introduction Topic-based multi-document summarization aims to create a single summary from a set of given documents while considering the topic of interest. The input documents can be created by querying an information retrieval or search engine for a particular topic and retaining highly ranked documents, or by clustering"
W14-3703,P03-1054,0,0.0237746,"SU4 and ROUGE 2 as evaluation metrics. ROUGE returns recall, precision and F-score of a system, but usually only recall is used in for evaluating automatic summarization systems, because the final summary does not contain many words. Hence, if the recall is high then the summarization system is working well. Document statistics is provided in Table 1. i=1 X Experiments 4.2 Experimental setup We use raw documents from the various DUC datasets as input for our system. We remove nonalphabetical characters from the documents. Then we obtain a clean sentence split by means of the Stanford parser (Klein and Manning, 2003) so that the sentences are compatible with the next steps. (6) iSj In constraint 6, Sj is a set of sentences containing entity yj . This constraint shows that, if an entity yj is selected then at least one sentence is seP lected which contains it (yj = 1, ∴ xi ≥ 1). If 1 http://www-nlpir.nist.gov/projects/ duc/index.html 19 λ1 λ1 λ1 λ1 λ1 = 0.5 & λ2 = 0.6 & λ2 = 0.7 & λ2 = 0.8 & λ2 = 0.9 & λ2 = 0.5 = 0.4 = 0.3 = 0.2 = 0.1 ROUGE-2 0.07950 0.07956 0.07975 0.07976 0.07985 ROUGE-SU4 0.14060 0.14071 0.14105 0.14106 0.14107 Table 2: Results on different λ’s on DUC 2005 We use the Brown coherence to"
W14-3703,C00-1072,0,0.242002,"approaches. Firstly, relevance and redundancy can be optimized simultaneously. For instance, Goldstein et al. (2000) developed a metric named MMR-MD (influenced by the Maximum Marginal Relevance (MMR) approach of Carbonell and Goldstein (1998)) and applied it to clusters of passages. Similarly, influenced by the SumBasic system (Nenkova and Vanderwende, 2005), Yih et al. (2007) developed a system which assigns a score to each term on the basis of position and frequency information and selects the sentence having highest score. Other approaches are based on an estimate of word importance (e.g. Lin and Hovy (2000)) or the log likelihood ratio test which identifies the importance of words using a supervised model that considers a rich set of features (Hong and Nenkova, 2014). Finally, Barzilay and Elhadad (1999) extract sentences which are strongly connected by lexical chains for summarization. The second approach deals with relevance and redundancy seperately. For instance, McKeown et al. (1999) create clusters of similar sentences and pick the representative one from every cluster. The representative sentence of a cluster of sentences takes care of the requirement to extract relevant information where"
W14-3703,P14-2040,0,0.0200637,"o far. However, their results on DUC 2007 (ROUGE-2 0.12517 and ROUGE-SU4 0.17603) are still quite a bit better than our results. On the DUC 2006 data we outperform the HIERSUM system by Haghighi and Vanderwende (2009) on ROUGE2 (0.08969 vs. 0.086) as well as on ROUGESU4 (0.15070 vs. 0.143). On the DUC 2007 data, our results are worse than theirs on ROUGE2 (0.10928 vs. 0.118) and on par on ROUGESU4 (0.16735 vs. 0.167). The system which won the DUC 2007 task, PYTHY by Toutanova et al. (2007), performs similar to HIERSUM and hence slightly better than our system on these data. The recent work by Suzuki and Fukumoto (2014) evaluates also on DUC 2007 but reports only ROUGE1 scores. We obtain a ROUGE-1 score of 0.448 on DUC 2007 which is better than Suzuki and Fukumoto (2014) (0.438) as well as PYTHY (0.426). The best ROUGE-1 score reported to date has been reported by Celikyilmaz and Hakkani-T¨ur (2010) with 0.456. The difference between this score and our score of 0.448 is rather small. 5 Discussion Several approaches have been proposed for topic based multi-document summarization on the DUC datasets we use for our experiments. The best results to date have been obtained by supervised and semi-supervised system"
W14-3703,W04-1013,0,0.0302959,"X xi ≥ yj , f orj = 1, . . . , m Datasets Datasets used for our experiments are DUC2005 (Dang, 2005), DUC2006 (Dang, 2006) and DUC20071 . Each dataset contains group of related documents. Each group of documents contains one related topic or a query consisting of a few sentences. In DUC, the final summary should respond to the corresponding topic. Also, the summary cannot exceed the maximum allowed length. For instance, in DUC2005, 250 words are allowed in the final summary. Every document cluster has corresponding human summaries for evaluating system summaries on the basis of ROUGE scores (Lin, 2004). The sources of DUC datasets are Los Angeles Times, Financial Times of London, Associated Press, New York Times and Xinhua news agency. We employ ROUGE SU4 and ROUGE 2 as evaluation metrics. ROUGE returns recall, precision and F-score of a system, but usually only recall is used in for evaluating automatic summarization systems, because the final summary does not contain many words. Hence, if the recall is high then the summarization system is working well. Document statistics is provided in Table 1. i=1 X Experiments 4.2 Experimental setup We use raw documents from the various DUC datasets a"
W14-3703,D12-1022,0,0.0191326,"ng to summary) (Shen et al., 2007; Gong and Liu, 2001). Here, the sentences are projected as feature vectors and conditional random fields are used to classify them. During document processing, most informative sentences are selected by the summarizer (Shen et al., 2007). Fattah and Ren (2009) also considMcDonald (2007) proposes a new ILP optimization method for extractive summarization. He introduces an objective function which maximizes the importance of sentences and minimizes the similarity of sentences. ILP methods for optimization have also been adopted by Berg-Kirkpatrick et al. (2011),Woodsend and Lapata (2012) and Galanis et al. (2012). Until now, Galanis et al. (2012) have reported the highest scores for multi-document summarization on DUC2005 and DUC2007. However, their approach is not completely unsupervised. 16 3 Our method trast to the local information specific to a vertex, graphical ranking algorithms take (graph-) global information to calculate the rank of a node. The Hyperlink-Induced Topic Search algorithm (HITS, also known as Hubs and Authorities) by Kleinberg (1999) is used to rank sentences in our method. This algorithm considers two types of nodes, hence it is well suited to rank sen"
W16-0518,W15-0607,0,0.0302779,"Missing"
W16-0518,N10-1019,0,0.0276862,"misjudge the expressiveness of the results. In fact, correctly assigning 1.0 to only one faulty sentence and 0.5 to all other sentences yields a score of 0.8571. The result is not as extreme if precision and recall are computed based on the mean absolute error, which results in 0.6667. This, still, clearly overestimates the quality of the results. 7 Related Work As Daudaraviˇcius (2015) states, a lot of scientists authoring scientific papers are nonnative English speakers. This insight suggests a relation of automatic evaluation of scientific writing to the field of language learner systems. Gamon (2010) mainly addresses article and preposition errors, which have shown to be frequent errors in the dataset provided for the AESW 2016 Shared Task, too. He uses language models on both a lexical and a syntactical level to find more likely alternatives for prepositions and articles with respect to the linguistic environment they occur in. He also bases some features on ratios of language model outcomes, rather than on individual probabilities, which is an approach that underlies many of our real-valued features. Tetreault et al. (2010) examine how helpful parser output features are when modeling pr"
W16-0518,D08-1020,0,0.0408496,"-division. 3.3 c(lii+n−1 ) = . i+n−1 c(li+1 ) p(wi |ti ) = c(pred(h) = g) , 0 g 0 ∈C c(pred(h) = g ) p(pred(h) = g |h) = P Features We implement a total of 82 features based on the data analysis described in Section 2. These features can be classified into three sets, depending on their range. Features 1–14 (see Table 1) are integervalued, features 15–55 (see Table 2) are binary, and features 56–82 (see Table 3) are real-valued. Most of the integer-valued features originate in readability research and address the coherence of documents, but they may also be helpful to assess sentence quality (Pitler and Nenkova, 2008). It is plausible that long sentences or sentences with a very high parse tree should be shortened or split into more sentences in order to simplify their syntax. Thus they account for those cases where phrases are deleted in favor of conciseness. Many occurrences of constituents such as VP, SBAR or NP are likely to ID 1 2 3 4 5 6 7 8 9 10 11 12 13 14 Definition number of definite articles (token the with POS-tag DT) number of pronouns (tokens with POS-tag PRP) number of SBAR (subtrees of syntax tree with root SBAR) number of VP (subtrees of syntax tree with root VP) number of NP (subtrees of"
W16-0518,P10-2065,0,0.0268112,"evaluation of scientific writing to the field of language learner systems. Gamon (2010) mainly addresses article and preposition errors, which have shown to be frequent errors in the dataset provided for the AESW 2016 Shared Task, too. He uses language models on both a lexical and a syntactical level to find more likely alternatives for prepositions and articles with respect to the linguistic environment they occur in. He also bases some features on ratios of language model outcomes, rather than on individual probabilities, which is an approach that underlies many of our real-valued features. Tetreault et al. (2010) examine how helpful parser output features are when modeling preposition usage. They present several phrase structure and dependency-based features, including left and right contexts of constituents in parse trees and the 170 lexicals modified by a prepositional phrase. For our features we extract those ideas from these works that seem the most promising for the challenge we encounter. But they both hold inspiration for even more features than those we implement in the course of our participation in the shared task and will be reconsidered in future work. 8 Conclusions To detect spurious sent"
W16-4317,S12-1051,0,0.031087,"ively close to our model. Volkova et al. (2016) use a very coarse notion of geolocation and find differences in emotions across different countries. Bertrand et al. (2013) use geolocation in tweets to perform sentiment analysis. They base their work on The First Law of Geography: “everything is related to everything else, but near things are more related than distant things” (Tobler, 1970, p.4). We also follow this law. Our semi-supervised approach is based on the idea that similar tweets should be labeled with similar emotions. However, approaches for computing “Semantic Textual Similarity” (Agirre et al., 2012) are not applicable as emotions are not expressed that much through content words but through the text’s linguistic and stylistic properties. Hence, our features are closer to ones used in linguistic style analysis as used in, e.g., work on authorship attribution on tweets (Layton et al., 2010; Silva et al., 2011; Macleod and Grant, 2012; Schwartz et al., 2013). Linguistic style analysis also has been applied to sentiment analysis in tweets (Pak and Paroubek, 2010; Kouloumpis et al., 2011; Brody and Diakopoulos, 2011). 1 https://blog.twitter.com/2009/location-location-location) 154 everybody h"
W16-4317,D11-1052,0,0.262402,"this work are described in detail in a companion paper (Resch et al., 2016). 2 Related Work Emotion recognition can be viewed as a subtask of sentiment analysis (Liu and Zhang, 2012). It is, however, more complex as it addresses multiple emotions, and, hence, requires a multi-class classification (Kozareva et al., 2007), instead of the binary or gradual polarity categories used mostly in sentiment analysis. Sentiment analysis on Twitter data has attracted a lot of research (Strapparava and Mihalcea, 2008; Davidov et al., 2010; Bollen et al., 2011; Roberts et al., 2012; Pak and Paroubek, 2010; Brody and Diakopoulos, 2011; Kouloumpis et al., 2011) with, e.g., several years of shared tasks at SemEval and more than 30 participating teams at the SemEval 2016 Task 4. Still, the results are still far from perfect and quite a bit worse than results on reviews (Nakov et al., 2016). Existing work classifying emotions in tweets is supervised and requires large amounts of annotated data (Roberts et al., 2012; Mohammad and Kiritchenko, 2014; Volkova and Bachrach, 2016) or heuristics deriving emotions from hashtags to label emotions in tweets (Davidov et al., 2010). We, in contrast, apply a semi-supervised method which re"
W16-4317,N13-1037,0,0.0131528,"set of positive, negative, and neutral tweets. We also use hashtags as a feature to compute the similarity between tweets. Emojis have an even stronger emotional content than hashtags. Hence, we use them for the same purpose. Style Features. POS tags do not directly convey emotion information, but their distribution within a text has been shown to reveal a text’s polarity (Pak and Paroubek, 2010). However, the POS tagger used (Owoputi et al., 2013) does not tag adjectives correctly. Hence we can only use adverbs in our feature set. Spelling features take spelling pecularities as intensifiers (Eisenstein, 2013; Kouloumpis et al., 2011): the number of words containing character repetitions and the number of words written in only capital letters. We take punctuation as an encoding of emotional content (as suggested by Davidov et al. (2010)). We compare exclamation, question, and quotation marks as sequences and as counts. 4.1.2 Normalising and Aggregating Results We normalize the results from the feature groups by applying the sigmoid function f (x) = x/(1 + |x|), a function that does not depend on a maximum value. The normalized results from all feature groups are aggregated. This value is normalize"
W16-4317,S07-1072,0,0.0375865,"on microblogs (Schwartz et al., 2013). We choose the time and geolocation around the Boston Marathon Bombing because we expect to harvest a larger fraction of highly emotional tweets than usual. See Table 1 for a few examples from our dataset some of which express emotions. The GIScience aspects of this work are described in detail in a companion paper (Resch et al., 2016). 2 Related Work Emotion recognition can be viewed as a subtask of sentiment analysis (Liu and Zhang, 2012). It is, however, more complex as it addresses multiple emotions, and, hence, requires a multi-class classification (Kozareva et al., 2007), instead of the binary or gradual polarity categories used mostly in sentiment analysis. Sentiment analysis on Twitter data has attracted a lot of research (Strapparava and Mihalcea, 2008; Davidov et al., 2010; Bollen et al., 2011; Roberts et al., 2012; Pak and Paroubek, 2010; Brody and Diakopoulos, 2011; Kouloumpis et al., 2011) with, e.g., several years of shared tasks at SemEval and more than 30 participating teams at the SemEval 2016 Task 4. Still, the results are still far from perfect and quite a bit worse than results on reviews (Nakov et al., 2016). Existing work classifying emotions"
W16-4317,P12-3005,0,0.0115301,"tps://dev.twitter.com/rest/public) via spatial search queries (Harvard University, Center for Geographic Analysis, 2016). This provides us with all georeferenced tweets from a particular area instead of just a sample as would have been the case if the Streaming API would have been used with spatial information (Boyd and Crawford, 2012). We select tweets from April 8th, 2013 to April 22nd, 2013, georeferenced within a bounding box containing Boston with: xmin: -71.21, ymin: 42.29, xmax: -70.95, ymax: 42.25. Preprocessing comprises language detection by two language detectors (McCandless, 2010; Lui and Baldwin, 2012) so that only tweets are kept which are identified by at least one detector as English, removing tweets without content (i.e., tweets being empty after filtering URLs and @mentions). After preprocessing 195,380 georeferenced tweets remain. 3.2 Emotion Annotation We choose Ekman’s six basic emotions happiness, anger, sadness, disgust, surprise, and fear (Ekman and Friesen, 1971) plus none as a basis for our annotation. These categories have been used in related work (Strapparava and Mihalcea, 2008; Roberts et al., 2012; Purver and Battersby, 2012). We train seven naive (neither experts in psych"
W16-4317,P07-1103,0,0.0288678,"Missing"
W16-4317,N13-1039,0,0.0300858,"Missing"
W16-4317,pak-paroubek-2010-twitter,0,0.872709,"he GIScience aspects of this work are described in detail in a companion paper (Resch et al., 2016). 2 Related Work Emotion recognition can be viewed as a subtask of sentiment analysis (Liu and Zhang, 2012). It is, however, more complex as it addresses multiple emotions, and, hence, requires a multi-class classification (Kozareva et al., 2007), instead of the binary or gradual polarity categories used mostly in sentiment analysis. Sentiment analysis on Twitter data has attracted a lot of research (Strapparava and Mihalcea, 2008; Davidov et al., 2010; Bollen et al., 2011; Roberts et al., 2012; Pak and Paroubek, 2010; Brody and Diakopoulos, 2011; Kouloumpis et al., 2011) with, e.g., several years of shared tasks at SemEval and more than 30 participating teams at the SemEval 2016 Task 4. Still, the results are still far from perfect and quite a bit worse than results on reviews (Nakov et al., 2016). Existing work classifying emotions in tweets is supervised and requires large amounts of annotated data (Roberts et al., 2012; Mohammad and Kiritchenko, 2014; Volkova and Bachrach, 2016) or heuristics deriving emotions from hashtags to label emotions in tweets (Davidov et al., 2010). We, in contrast, apply a se"
W16-4317,E12-1049,0,0.0235723,"tection by two language detectors (McCandless, 2010; Lui and Baldwin, 2012) so that only tweets are kept which are identified by at least one detector as English, removing tweets without content (i.e., tweets being empty after filtering URLs and @mentions). After preprocessing 195,380 georeferenced tweets remain. 3.2 Emotion Annotation We choose Ekman’s six basic emotions happiness, anger, sadness, disgust, surprise, and fear (Ekman and Friesen, 1971) plus none as a basis for our annotation. These categories have been used in related work (Strapparava and Mihalcea, 2008; Roberts et al., 2012; Purver and Battersby, 2012). We train seven naive (neither experts in psychology nor in NLP) subjects to annotate tweets and to perform an initial reliability study. It turns out that two annotators are not up to the task (after computing pairwise κ (Fleiss, 1971) between annotators). So we continue with five annotators who annotated 261 randomly selected tweets. Also, the κ scores for disgust and surprise are very low. See Table 2 for a few examples which caused arguments among annotators during the first phase of the annotation. Hence we change the annotation manual so that the likely to be confused emotions anger and"
W16-4317,P15-2104,0,0.0198872,", 2016) or heuristics deriving emotions from hashtags to label emotions in tweets (Davidov et al., 2010). We, in contrast, apply a semi-supervised method which requires only little annotated data. While Bollen et al. (2011) label discrete emotions, they do not classify single tweets but examine the whole Twitter community jointly. Roberts et al. (2012) and Bollen et al. (2011) use the temporal dimension, but neglect the spatial dimension (georeferencing of single tweets had been introduced only in 20091 ). There is only little work in NLP dealing with geolocation in tweets. Han et al. (2014), Rahimi et al. (2015b) and Rahimi et al. (2015a) use tweets to predict geolocation, the reverse of our setting. However, Rahimi et al. (2015a) use a model based on Modified Adsorption which is relatively close to our model. Volkova et al. (2016) use a very coarse notion of geolocation and find differences in emotions across different countries. Bertrand et al. (2013) use geolocation in tweets to perform sentiment analysis. They base their work on The First Law of Geography: “everything is related to everything else, but near things are more related than distant things” (Tobler, 1970, p.4). We also follow this law"
W16-4317,N15-1153,0,0.0142204,", 2016) or heuristics deriving emotions from hashtags to label emotions in tweets (Davidov et al., 2010). We, in contrast, apply a semi-supervised method which requires only little annotated data. While Bollen et al. (2011) label discrete emotions, they do not classify single tweets but examine the whole Twitter community jointly. Roberts et al. (2012) and Bollen et al. (2011) use the temporal dimension, but neglect the spatial dimension (georeferencing of single tweets had been introduced only in 20091 ). There is only little work in NLP dealing with geolocation in tweets. Han et al. (2014), Rahimi et al. (2015b) and Rahimi et al. (2015a) use tweets to predict geolocation, the reverse of our setting. However, Rahimi et al. (2015a) use a model based on Modified Adsorption which is relatively close to our model. Volkova et al. (2016) use a very coarse notion of geolocation and find differences in emotions across different countries. Bertrand et al. (2013) use geolocation in tweets to perform sentiment analysis. They base their work on The First Law of Geography: “everything is related to everything else, but near things are more related than distant things” (Tobler, 1970, p.4). We also follow this law"
W16-4317,roberts-etal-2012-empatweet,0,0.658834,"ch express emotions. The GIScience aspects of this work are described in detail in a companion paper (Resch et al., 2016). 2 Related Work Emotion recognition can be viewed as a subtask of sentiment analysis (Liu and Zhang, 2012). It is, however, more complex as it addresses multiple emotions, and, hence, requires a multi-class classification (Kozareva et al., 2007), instead of the binary or gradual polarity categories used mostly in sentiment analysis. Sentiment analysis on Twitter data has attracted a lot of research (Strapparava and Mihalcea, 2008; Davidov et al., 2010; Bollen et al., 2011; Roberts et al., 2012; Pak and Paroubek, 2010; Brody and Diakopoulos, 2011; Kouloumpis et al., 2011) with, e.g., several years of shared tasks at SemEval and more than 30 participating teams at the SemEval 2016 Task 4. Still, the results are still far from perfect and quite a bit worse than results on reviews (Nakov et al., 2016). Existing work classifying emotions in tweets is supervised and requires large amounts of annotated data (Roberts et al., 2012; Mohammad and Kiritchenko, 2014; Volkova and Bachrach, 2016) or heuristics deriving emotions from hashtags to label emotions in tweets (Davidov et al., 2010). We,"
W16-4317,P05-1065,0,0.0142815,"Missing"
W16-4317,D13-1193,0,0.0682871,"and ANGER / DISGUST (merged in one category, see Section 3.2). Additionally, we utilize a NONE class to catch all other cases. In this paper, we focus on computing the similarity between two nodes, i.e. tweets, which is used to construct the graph (Figure 1). The similarity score is utilized as edge weight. On the resulting graph we apply Modified Adsorption (Talukdar and Crammer, 2009), a semi-supervised label-propagation algorithm. Features are derived from and extend work on Twitter sentiment analysis and Twitter writing style analysis such as work on authorship attribution on microblogs (Schwartz et al., 2013). We choose the time and geolocation around the Boston Marathon Bombing because we expect to harvest a larger fraction of highly emotional tweets than usual. See Table 1 for a few examples from our dataset some of which express emotions. The GIScience aspects of this work are described in detail in a companion paper (Resch et al., 2016). 2 Related Work Emotion recognition can be viewed as a subtask of sentiment analysis (Liu and Zhang, 2012). It is, however, more complex as it addresses multiple emotions, and, hence, requires a multi-class classification (Kozareva et al., 2007), instead of the"
W16-4317,P16-1148,0,0.107059,"has attracted a lot of research (Strapparava and Mihalcea, 2008; Davidov et al., 2010; Bollen et al., 2011; Roberts et al., 2012; Pak and Paroubek, 2010; Brody and Diakopoulos, 2011; Kouloumpis et al., 2011) with, e.g., several years of shared tasks at SemEval and more than 30 participating teams at the SemEval 2016 Task 4. Still, the results are still far from perfect and quite a bit worse than results on reviews (Nakov et al., 2016). Existing work classifying emotions in tweets is supervised and requires large amounts of annotated data (Roberts et al., 2012; Mohammad and Kiritchenko, 2014; Volkova and Bachrach, 2016) or heuristics deriving emotions from hashtags to label emotions in tweets (Davidov et al., 2010). We, in contrast, apply a semi-supervised method which requires only little annotated data. While Bollen et al. (2011) label discrete emotions, they do not classify single tweets but examine the whole Twitter community jointly. Roberts et al. (2012) and Bollen et al. (2011) use the temporal dimension, but neglect the spatial dimension (georeferencing of single tweets had been introduced only in 20091 ). There is only little work in NLP dealing with geolocation in tweets. Han et al. (2014), Rahimi"
W17-1501,P08-1002,0,0.030165,"odules, use deep neural networks, word embeddings and a small set of features describing surface properties of mentions. While it is shown that this small set of features has significant impact on the overall performance (Clark and Manning, 2016a), their use is very limited in the state-of-the-art systems in comparison to the embedding features. 2.1 Non-Referential Mentions Non-referential mentions do not refer to an entity. These mentions only fill a syntactic position. For instance, “it” in “it is raining” is a nonreferential mention. The approaches proposed by Evans (2001), M¨uller (2006), Bergsma et al. (2008), Bergsma and Yarowsky (2011) are examples of detecting non-referential cases of the pronoun it. Byron and Gegg-Harrison (2004) present a more general approach for detecting nonreferential noun phrases. 1 Proceedings of the 2nd Workshop on Coreference Resolution Beyond OntoNotes (CORBON 2017), co-located with EACL 2017, pages 1–7, c Valencia, Spain, April 4, 2017. 2017 Association for Computational Linguistics 2.2 coreferent vs. non-coreferent. Coreferent mentions are those mentions that appear in a coreference chain. A non-coreferent mention therefore can be a non-referential noun phrase or a"
W17-1501,P16-1061,0,0.480659,"ina, 2009). However, they are not common in comparison to the above categories. Introduction Coreference resolution is the task of finding different mentions that refer to the same entity in a given text. Anaphoricity detection is an important step for coreference resolution. An anaphoricity detection module discriminates mentions that are coreferent with one of the previous mentions. If a system recognizes mention m as non-anaphoric, it does not need to make any coreferent links for the pairs in which m is the anaphor. The current state-of-the-art coreference resolvers (Wiseman et al., 2016; Clark and Manning, 2016a; Clark and Manning, 2016b), as well as their anaphoricity detection modules, use deep neural networks, word embeddings and a small set of features describing surface properties of mentions. While it is shown that this small set of features has significant impact on the overall performance (Clark and Manning, 2016a), their use is very limited in the state-of-the-art systems in comparison to the embedding features. 2.1 Non-Referential Mentions Non-referential mentions do not refer to an entity. These mentions only fill a syntactic position. For instance, “it” in “it is raining” is a nonreferen"
W17-1501,D16-1245,0,0.0608578,"Missing"
W17-1501,D08-1069,0,0.307234,"Missing"
W17-1501,P82-1020,0,0.660936,"Missing"
W17-1501,N09-1065,0,0.0244254,"ady evoked in the discourse. Except for first mentions of coreference chains, other coreferent mentions are discourse-old. For instance, “this philosopher” and the second “Plato” in Example 2.1 are discourse-old mentions. A mention is inferable if the hearer can infer the identity of the mention from another entity that has already been evoked in the discourse. “the windows” in Example 2.2 is an inferable mention. Example 2.2. I walked into the room. The windows were all open. The detection of discourse-old mentions is commonly referred to as anaphoricity detection (e.g. Zhou and Kong (2009), Ng (2009), Wiseman et al. (2015), Lassalle and Denis (2015), inter alia) while the task of anaphoric mention detection, based on its original definition, is of no use for coreference resolution. Mentions whose interpretations do not depend on previous mentions are called non-anaphoric mentions (van Deemter and Kibble, 2000). For example, both ”Plato”s in Example 2.1 are non-anaphoric. For consistency with the coreference literature, we refer to the task of discourse-old mention detection as anaphoricity detection. Currently, all the state-of-the-art coreference resolvers learn anaphoricity detection jo"
W17-1501,D14-1162,0,0.0866674,"ance of the mention detection module is of crucial importance for anaphoricity detection. Therefore, it is important that the compared anaphoricity detectors use the same mention detection. Implementation Details joint LSTM LSTM∗ SVM Hyperparameters are tuned on the CoNLL 2012 development set. We minimize the cross entropy loss using gradient-based optimization and the Adam update rule (Kingma and Ba, 2014). We use minibatches of size 50. A dropout (Hinton et al., 2012) with a rate of 0.3 is applied to the output of LSTM. We initialize the embeddings with the 300-dimensional Glove embeddings (Pennington et al., 2014). The size of LSTM’s hidden layer is Non-Anaphoric R P F1 90.71 92.64 91.66 90.51 87.31 88.88 92.42 92.61 92.51 Anaphoric R P 81.81 77.18 85.00 81.48 72.64 78.64 84.66 84.30 F1 79.43 83.20 75.52 84.48 Table 1: Results on the CoNLL 2012 test set. The LSTM model that is described in Section 3.2 is denoted as LSTM in Table 1. In order to investigate the effect of the used surface 3 5 features, we also report the results of the LSTM model without using these features (LSTM∗ ). We analyze the output of the LSTM and SVM models on the CoNLL 2012 test set to see how well they perform for different typ"
W17-1501,P13-1049,0,0.0602618,"Missing"
W17-1501,N13-1071,0,0.0808517,"Missing"
W17-1501,D14-1221,1,0.826586,"uns very much while it mainly affects the detection of anaphoric proper names by about 24 percent. In order to see whether the same pattern holds for coreference resolution, we compare the recall and precision errors of the best coreference system that only uses surface features, i.e. cort (Martschat and Strube, 2015) with singleton features (Moosavi and Strube, 2016) 2 , and the stateof-the-art deep coreference resolver, i.e. deepcoref (Clark and Manning, 2016a). The comparison of the errors for the CoNLL 2012 test set is shown in Table 4. We use the error analysis tool of cort introduced by Martschat and Strube (2014) for the results of Table 4. As can be seen from Table 4, while deep-coref is significantly better than cort for resolving common nouns and specially pronouns, its result does not go far beyond that of cort when it comes to resolving proper names. The following observations can be drawn from the results of Table 1: (1) our LSTM model outperforms the joint model while using less features and being trained independently, (2) the results of the LSTM∗ model is considerably lower than those of LSTM, especially for recognizing anaphoric mentions, and (3) the simple SVM model outperforms the neural m"
W17-1501,Q15-1029,1,0.843842,"e performance of LSTM and SVM for recognizing anaphoric pronouns. SVM detects anaphoric proper names better while LSTM is better at recognizing anaphoric common nouns. We also analyze the output of LSTM∗ . As can be seen, the incorporation of surface features does not affect the detection of anaphoric pronouns very much while it mainly affects the detection of anaphoric proper names by about 24 percent. In order to see whether the same pattern holds for coreference resolution, we compare the recall and precision errors of the best coreference system that only uses surface features, i.e. cort (Martschat and Strube, 2015) with singleton features (Moosavi and Strube, 2016) 2 , and the stateof-the-art deep coreference resolver, i.e. deepcoref (Clark and Manning, 2016a). The comparison of the errors for the CoNLL 2012 test set is shown in Table 4. We use the error analysis tool of cort introduced by Martschat and Strube (2014) for the results of Table 4. As can be seen from Table 4, while deep-coref is significantly better than cort for resolving common nouns and specially pronouns, its result does not go far beyond that of cort when it comes to resolving proper names. The following observations can be drawn from"
W17-1501,J00-4005,0,0.106177,"Missing"
W17-1501,N16-1115,1,0.882024,"Missing"
W17-1501,P15-1137,0,0.0378652,"in the discourse. Except for first mentions of coreference chains, other coreferent mentions are discourse-old. For instance, “this philosopher” and the second “Plato” in Example 2.1 are discourse-old mentions. A mention is inferable if the hearer can infer the identity of the mention from another entity that has already been evoked in the discourse. “the windows” in Example 2.2 is an inferable mention. Example 2.2. I walked into the room. The windows were all open. The detection of discourse-old mentions is commonly referred to as anaphoricity detection (e.g. Zhou and Kong (2009), Ng (2009), Wiseman et al. (2015), Lassalle and Denis (2015), inter alia) while the task of anaphoric mention detection, based on its original definition, is of no use for coreference resolution. Mentions whose interpretations do not depend on previous mentions are called non-anaphoric mentions (van Deemter and Kibble, 2000). For example, both ”Plato”s in Example 2.1 are non-anaphoric. For consistency with the coreference literature, we refer to the task of discourse-old mention detection as anaphoricity detection. Currently, all the state-of-the-art coreference resolvers learn anaphoricity detection jointly with coreference"
W17-1501,E06-1007,0,0.0919472,"Missing"
W17-1501,C02-1139,0,0.111577,"definition, is of no use for coreference resolution. Mentions whose interpretations do not depend on previous mentions are called non-anaphoric mentions (van Deemter and Kibble, 2000). For example, both ”Plato”s in Example 2.1 are non-anaphoric. For consistency with the coreference literature, we refer to the task of discourse-old mention detection as anaphoricity detection. Currently, all the state-of-the-art coreference resolvers learn anaphoricity detection jointly with coreference resolution (Wiseman et al., 2015; Wiseman et al., 2016; Clark and Manning, 2016a). The approaches proposed by Ng and Cardie (2002), Ng (2004), Ng (2009), Zhou and Kong (2009), Uryupina (2009) are examples of independent anaphoricity detection approaches. 2.3 Anaphoricity Detection Models 3.1 Joint Model As one of the neural models for anaphoricity detection, we consider the anaphoricity module of deep-coref1 , the state-of-the-art coreference resolution system introduced by Clark and Manning (2016a). This model has three layers for encoding different types of information regarding a mention. The first layer encodes the word embeddings of the head, first, last, two previous/following words, and the syntactic parent of the"
W17-1501,N16-1114,0,0.161798,"se-new mentions (Uryupina, 2009). However, they are not common in comparison to the above categories. Introduction Coreference resolution is the task of finding different mentions that refer to the same entity in a given text. Anaphoricity detection is an important step for coreference resolution. An anaphoricity detection module discriminates mentions that are coreferent with one of the previous mentions. If a system recognizes mention m as non-anaphoric, it does not need to make any coreferent links for the pairs in which m is the anaphor. The current state-of-the-art coreference resolvers (Wiseman et al., 2016; Clark and Manning, 2016a; Clark and Manning, 2016b), as well as their anaphoricity detection modules, use deep neural networks, word embeddings and a small set of features describing surface properties of mentions. While it is shown that this small set of features has significant impact on the overall performance (Clark and Manning, 2016a), their use is very limited in the state-of-the-art systems in comparison to the embedding features. 2.1 Non-Referential Mentions Non-referential mentions do not refer to an entity. These mentions only fill a syntactic position. For instance, “it” in “it is"
W17-1501,D09-1102,0,0.0211951,"an entity that is already evoked in the discourse. Except for first mentions of coreference chains, other coreferent mentions are discourse-old. For instance, “this philosopher” and the second “Plato” in Example 2.1 are discourse-old mentions. A mention is inferable if the hearer can infer the identity of the mention from another entity that has already been evoked in the discourse. “the windows” in Example 2.2 is an inferable mention. Example 2.2. I walked into the room. The windows were all open. The detection of discourse-old mentions is commonly referred to as anaphoricity detection (e.g. Zhou and Kong (2009), Ng (2009), Wiseman et al. (2015), Lassalle and Denis (2015), inter alia) while the task of anaphoric mention detection, based on its original definition, is of no use for coreference resolution. Mentions whose interpretations do not depend on previous mentions are called non-anaphoric mentions (van Deemter and Kibble, 2000). For example, both ”Plato”s in Example 2.1 are non-anaphoric. For consistency with the coreference literature, we refer to the task of discourse-old mention detection as anaphoricity detection. Currently, all the state-of-the-art coreference resolvers learn anaphoricity d"
W17-1501,D09-1119,0,\N,Missing
W17-1501,P07-1029,0,\N,Missing
W17-1501,L16-1021,0,\N,Missing
W17-4803,W15-2501,0,0.0139172,"g to its frequency in the training documents and sums this up for all subgraphs – as our feature model score of our coherence model. 4.1 dev 3,003 - Table 2: Statistics on the datasets used. train is the news commentary v10 corpus, dev is the 2012 newstest development data, and test is the DiscoMT 2015 test data. The number (#) of tokens corresponds to the English (target) side. sgki ∈FSG 4 train 200,239 - Experiments Datasets We use the WMT 2015 (Bojar et al., 2015) dataset for training and development of the sentence-level translation and language models4 , and the DiscoMT 2015 Shared Task (Hardmeier et al., 2015) dataset for mining subgraphs (coherence patterns) and as our test data (Table 2). We run experiments on the language pair French-English. Coherence patterns are extracted from the 1551 DiscoMT training documents using GloVe word embeddings. We extract all k-node subgraphs for k ∈ {3, 4, 5} using GASTON 5 (Nijssen and Kok, 2004, 2005). 4 We use Moses to translate sentences independently and initialize the translation state in Docent. 5 http://liacs.leidenuniv.nl/ nijssensgr/gaston/iccs.html. ˜ 6 We choose this threshold to make a balance between processing time and translation performance. 30"
W17-4803,P13-4033,0,0.201075,"Missing"
W17-4803,J08-1001,0,0.711916,"linguistic point of view also the discourse-wide context must be taken into account to have a high-quality translation (Hatim and Mason, 1990; Hardmeier et al., 2012). The current paradigm of machine translation needs to be improved as it does not consider any discourse coherence phenomena that establish a text’s connectedness (Sim Smith et al., 2015). One of the active research topics in modeling coherence focuses on entity connections over sentences based on Centering Theory (Grosz et al., 1995). Previous research on coherence modeling shows its application mainly in readability assessment (Barzilay and Lapata, 2008; Pitler and Nenkova, 2008). Recently, Parveen et al. (2016) showed that the graph-based coherence model can be utilized to generate more coherent summaries of scientific articles. The main goal of this paper is to integrate coherence features with a statistical machine translation system to improve the quality of the output translation. To achieve this goal, we combine the graph-based coherence representation by Guinaudeau and Strube (2013) and its extensions (Mesgar and Strube, 2015, 2016) into the documentlevel machine translation decoder Docent (Hardmeier et al., 2012, 2013). Docent define"
W17-4803,W11-2123,0,0.0770616,"f K feature functions hk (S), each with a constant weight λk , such that K X f(S) = λk hk (S). (2) 3.3 k=1 Docent uses simulated annealing, a stochastic variant of the hill climbing algorithm (Khachaturyan et al., 1981), for either accepting or rejecting operations for maximizing its objective function (Hardmeier, 2012) . Docent already implements some sentencelocal feature models that are similar to those found in traditional sentence-level decoders. These include phrase translation scores provided by the phrase table (Koehn et al., 2003), n-gram language model scores implemented with KenLM (Heafield, 2011), a word penalty score, and an unlexicalised distortion cost model with geometric decay (Koehn et al., 2003). Our idea is to add a new document-level coherence function hcoh (S), namely a graph-based coherence model to the objective function represented in Equation. 2. In the next subsection, we describe this model in more detail. 3.2 Integrating the Coherence Model With Docent For extracting coherence patterns we use the target documents3 of the training set of the DiscoMT dataset. We extract all k-node subgraphs for k ∈ {3, 4, 5}. We limit the size of subgraphs to 3-, 4-, and 5-node as Mesga"
W17-4803,W14-3348,0,0.015548,"ow the standard machine translation procedure of evaluation, measuring BLEU (Papineni et al., 2002) for every system. BLEU is an ngram based co-occurrence metric that operates with modified n-gram precision scores. The document n-gram precision scores are averaged using the geometric mean of these scores with n-grams up to length N and positive weights summing to one. The result is multiplied by an exponential brevity penalty factor that penalizes a translation if it does not match the reference translations in length, word choice, and word order. We also calculate Meteor (Lavie et al., 2004; Denkowski and Lavie, 2014) as it is a widely used evaluation metric as well. In contrast to BLEU, Meteor is a word-based metric that takes recall into account as well. Meteor creates a word alignment between a pair of strings that is incrementally produced using a sequence of various wordmapping modules, including the exact module, the Porter stem module, and the WordNet synonymy module (Lavie and Agarwal, 2007). Because Meteor has been shown to have a higher correlation with human judgements than BLEU (Lavie et al., 2004), it is a useful alternative evaluation metric for our purposes. As it also considers stemmed word"
W17-4803,P07-2045,0,0.0284798,"nother, and more difficult, text generation system. We can also evaluate which feature is more beneficial for machine translation. 2.2 3.1 Method Docent We use Docent (Hardmeier et al., 2012, 2013) as the baseline. It explicitly has no notion of coherence. Docent is a document-level decoder that treats a translation not as a bag of sentences but instead has a translation hypothesis for the whole document at each step. The initial hypothesis can either be generated randomly from the translation table or it can be initialized with the result of any standard sentence-level decoder such as Moses (Koehn et al., 2007). Docent first independently translates all sentences of the input document. Then it starts to modify the translation of sentences with respect to the other translated sentences. Three basic operations modify the translation of sentences: changephrase-translation, swap-phrases, and resegment. Change-phrase-translations replaces the translation of a single phrase with a random translation for the same source phrase. Swap-phrases changes the word order without affecting the phrase translations by exchanging two phrases in a sentence. Coherence in Machine Translation Coherence modeling in machine"
W17-4803,W08-0509,0,0.0303123,"tion on the test set of DiscoMT. So, given the coherence graph representation of an intermediate state of the translated document (during the test phase), GS , and the set of all extracted subgraphs of the training documents, FSG = {sgk1 , sgk2 , ..., sgkm } where k ∈ {3, 4, 5}, and their weights, hcoh (S) is defined as follow: X hcoh (S) = 4.2 test 12 2,093 174 4,458,256 63,778 48,122 Experimental Setup We train our systems using the Moses decoder (Koehn et al., 2007). After standard preprocessing of the data, we train a 3-gram language model using KenLM (Heafield, 2011). We use the MGIZA++ (Gao and Vogel, 2008) word aligner and employ standard grow-diag-fast-and symmetrization. Tuning is done on the development data via minimum error rate training (Och, 2003). After training the language model and creating the phrase table with Moses, we use these to initialize our translation systems. We use the lcurvedocent binary of Docent, which outputs Docent’s learning curve, i.e., files for the intermediate decoding states. This additionally allows us to investigate the learning curves with regard to how our coherence feature behaves over time. We prune the translation table by only retaining all phrase trans"
W17-4803,J95-2003,0,0.674517,"ersion. The translated sentences should be coherently connected to each other in the target document as well. From a linguistic point of view also the discourse-wide context must be taken into account to have a high-quality translation (Hatim and Mason, 1990; Hardmeier et al., 2012). The current paradigm of machine translation needs to be improved as it does not consider any discourse coherence phenomena that establish a text’s connectedness (Sim Smith et al., 2015). One of the active research topics in modeling coherence focuses on entity connections over sentences based on Centering Theory (Grosz et al., 1995). Previous research on coherence modeling shows its application mainly in readability assessment (Barzilay and Lapata, 2008; Pitler and Nenkova, 2008). Recently, Parveen et al. (2016) showed that the graph-based coherence model can be utilized to generate more coherent summaries of scientific articles. The main goal of this paper is to integrate coherence features with a statistical machine translation system to improve the quality of the output translation. To achieve this goal, we combine the graph-based coherence representation by Guinaudeau and Strube (2013) and its extensions (Mesgar and"
W17-4803,N03-1017,0,0.0161386,"The scoring function can be further decomposed into a linear combination of K feature functions hk (S), each with a constant weight λk , such that K X f(S) = λk hk (S). (2) 3.3 k=1 Docent uses simulated annealing, a stochastic variant of the hill climbing algorithm (Khachaturyan et al., 1981), for either accepting or rejecting operations for maximizing its objective function (Hardmeier, 2012) . Docent already implements some sentencelocal feature models that are similar to those found in traditional sentence-level decoders. These include phrase translation scores provided by the phrase table (Koehn et al., 2003), n-gram language model scores implemented with KenLM (Heafield, 2011), a word penalty score, and an unlexicalised distortion cost model with geometric decay (Koehn et al., 2003). Our idea is to add a new document-level coherence function hcoh (S), namely a graph-based coherence model to the objective function represented in Equation. 2. In the next subsection, we describe this model in more detail. 3.2 Integrating the Coherence Model With Docent For extracting coherence patterns we use the target documents3 of the training set of the DiscoMT dataset. We extract all k-node subgraphs for k ∈ {3"
W17-4803,P13-1010,1,0.911778,"ver sentences based on Centering Theory (Grosz et al., 1995). Previous research on coherence modeling shows its application mainly in readability assessment (Barzilay and Lapata, 2008; Pitler and Nenkova, 2008). Recently, Parveen et al. (2016) showed that the graph-based coherence model can be utilized to generate more coherent summaries of scientific articles. The main goal of this paper is to integrate coherence features with a statistical machine translation system to improve the quality of the output translation. To achieve this goal, we combine the graph-based coherence representation by Guinaudeau and Strube (2013) and its extensions (Mesgar and Strube, 2015, 2016) into the documentlevel machine translation decoder Docent (Hardmeier et al., 2012, 2013). Docent defines an initial translation of the source document and modifies the translation of sentences aiming to maximize an objective function. This function measures the quality of the Although coherence is an important aspect of any text generation system, it has received little attention in the context of machine translation (MT) so far. We hypothesize that the quality of document-level translation can be improved if MT models take into account the s"
W17-4803,W07-0734,0,0.0629073,"lied by an exponential brevity penalty factor that penalizes a translation if it does not match the reference translations in length, word choice, and word order. We also calculate Meteor (Lavie et al., 2004; Denkowski and Lavie, 2014) as it is a widely used evaluation metric as well. In contrast to BLEU, Meteor is a word-based metric that takes recall into account as well. Meteor creates a word alignment between a pair of strings that is incrementally produced using a sequence of various wordmapping modules, including the exact module, the Porter stem module, and the WordNet synonymy module (Lavie and Agarwal, 2007). Because Meteor has been shown to have a higher correlation with human judgements than BLEU (Lavie et al., 2004), it is a useful alternative evaluation metric for our purposes. As it also considers stemmed words and information from WordNet to determine synonymous words between a candidate and a reference translation, the metric is interesting with regard to surface variation with the same semantic content and how this affects the evaluation of our coherence model (as its graph construction is semantically grounded). 5 5.1 sg6 sg7 s1 s3 s1 s3 s1 s3 s2 s4 s2 s4 s2 s4 sg8 sg9 sg10 s1 s3 s1 s3 s"
W17-4803,lavie-etal-2004-significance,0,0.041863,"tion Metrics We follow the standard machine translation procedure of evaluation, measuring BLEU (Papineni et al., 2002) for every system. BLEU is an ngram based co-occurrence metric that operates with modified n-gram precision scores. The document n-gram precision scores are averaged using the geometric mean of these scores with n-grams up to length N and positive weights summing to one. The result is multiplied by an exponential brevity penalty factor that penalizes a translation if it does not match the reference translations in length, word choice, and word order. We also calculate Meteor (Lavie et al., 2004; Denkowski and Lavie, 2014) as it is a widely used evaluation metric as well. In contrast to BLEU, Meteor is a word-based metric that takes recall into account as well. Meteor creates a word alignment between a pair of strings that is incrementally produced using a sequence of various wordmapping modules, including the exact module, the Porter stem module, and the WordNet synonymy module (Lavie and Agarwal, 2007). Because Meteor has been shown to have a higher correlation with human judgements than BLEU (Lavie et al., 2004), it is a useful alternative evaluation metric for our purposes. As it"
W17-4803,J12-4004,0,0.0155326,"rget documents. We base our coherence patterns on the characteristics of the target language as there is a theory within Translation Studies that “textual relations obtaining in the original are often modified [...] in favour of (more) habitual options offered by a target culture” (Toury, 1995). Toury (1995) calls this the law of growing standardization which seeks to describe and explain the acceptability of the translation in the receiving culture (Venuti, 2004). This law seems suitable in the context of subgraph mining as it is also already reflected in the language model of any MT system (Lembersky et al., 2012). For computing the weights of subgraphs, we divide the count of each k-node subgraph by the toGraph-based Coherence Model Our coherence model is based on the lexical graph representation (Mesgar and Strube, 2016). For any given document, we first filter out stop words using the provided stop word list by Salton (1971). 3 29 We experiment on translation from French to English. tal counts of subgraphs for that k. For each k, this gives the following vector: ϕ(sgk , G) = (w(sgk1 , G), ..., w(sgkm , G)), We use the twelve test documents of DiscoMT as the test data because these are much longer, o"
W17-4803,E17-3017,0,0.0638111,"Missing"
W17-4803,D15-1106,0,0.0201699,"hree basic operations modify the translation of sentences: changephrase-translation, swap-phrases, and resegment. Change-phrase-translations replaces the translation of a single phrase with a random translation for the same source phrase. Swap-phrases changes the word order without affecting the phrase translations by exchanging two phrases in a sentence. Coherence in Machine Translation Coherence modeling in machine translation is an (almost) desideratum . To the best of our knowledge, there are only a handful of publications in this direction. The one relevant to our approach is the work by Lin et al. (2015) as it constitutes an application of a coherence model in the context of machine translation, as opposed to more theoretical papers on the state of coherence in machine translation (Sim Smith et al., 2016). Lin et al. (2015) develop a sentence-level Recurrent Neural Network Language Model (RNNLM) that takes a sentence as input and tries to predict the next one based on the sentence history vector. By modeling sequences of sentences, the vector is able to model local coherence within RNNLM.2 Given the 10-best results of all sen2 They consider the “log probability of a given document as its cohe"
W17-4803,S15-1036,1,0.830332,"al., 1995). Previous research on coherence modeling shows its application mainly in readability assessment (Barzilay and Lapata, 2008; Pitler and Nenkova, 2008). Recently, Parveen et al. (2016) showed that the graph-based coherence model can be utilized to generate more coherent summaries of scientific articles. The main goal of this paper is to integrate coherence features with a statistical machine translation system to improve the quality of the output translation. To achieve this goal, we combine the graph-based coherence representation by Guinaudeau and Strube (2013) and its extensions (Mesgar and Strube, 2015, 2016) into the documentlevel machine translation decoder Docent (Hardmeier et al., 2012, 2013). Docent defines an initial translation of the source document and modifies the translation of sentences aiming to maximize an objective function. This function measures the quality of the Although coherence is an important aspect of any text generation system, it has received little attention in the context of machine translation (MT) so far. We hypothesize that the quality of document-level translation can be improved if MT models take into account the semantic relations among sentences during tra"
W17-4803,W15-2507,0,0.0228716,"owever, it is insufficient to just sequentially and independently translate sentences of the source document and concatenate them as the translated version. The translated sentences should be coherently connected to each other in the target document as well. From a linguistic point of view also the discourse-wide context must be taken into account to have a high-quality translation (Hatim and Mason, 1990; Hardmeier et al., 2012). The current paradigm of machine translation needs to be improved as it does not consider any discourse coherence phenomena that establish a text’s connectedness (Sim Smith et al., 2015). One of the active research topics in modeling coherence focuses on entity connections over sentences based on Centering Theory (Grosz et al., 1995). Previous research on coherence modeling shows its application mainly in readability assessment (Barzilay and Lapata, 2008; Pitler and Nenkova, 2008). Recently, Parveen et al. (2016) showed that the graph-based coherence model can be utilized to generate more coherent summaries of scientific articles. The main goal of this paper is to integrate coherence features with a statistical machine translation system to improve the quality of the output t"
W17-4803,N16-1167,1,0.866775,"Hardmeier et al., 2012, 2013). Docent defines an initial translation of the source document and modifies the translation of sentences aiming to maximize an objective function. This function measures the quality of the Although coherence is an important aspect of any text generation system, it has received little attention in the context of machine translation (MT) so far. We hypothesize that the quality of document-level translation can be improved if MT models take into account the semantic relations among sentences during translation. We integrate the graph-based coherence model proposed by Mesgar and Strube (2016) with Docent1 (Hardmeier et al., 2012; Hardmeier, 2014) a document-level machine translation system. The application of this graph-based coherence modeling approach is novel in the context of machine translation. We evaluate the coherence model and its effects on the quality of the machine translation. The result of our experiments shows that our coherence model slightly improves the quality of translation in terms of the average Meteor score. 1 Introduction Coherence represents semantic connectivity of texts with regard to grammatical and lexical relations between sentences. It is an essentia"
W17-4803,W16-3407,0,0.0820677,"slation for the same source phrase. Swap-phrases changes the word order without affecting the phrase translations by exchanging two phrases in a sentence. Coherence in Machine Translation Coherence modeling in machine translation is an (almost) desideratum . To the best of our knowledge, there are only a handful of publications in this direction. The one relevant to our approach is the work by Lin et al. (2015) as it constitutes an application of a coherence model in the context of machine translation, as opposed to more theoretical papers on the state of coherence in machine translation (Sim Smith et al., 2016). Lin et al. (2015) develop a sentence-level Recurrent Neural Network Language Model (RNNLM) that takes a sentence as input and tries to predict the next one based on the sentence history vector. By modeling sequences of sentences, the vector is able to model local coherence within RNNLM.2 Given the 10-best results of all sen2 They consider the “log probability of a given document as its coherence score” (Lin et al., 2015). 28 Then, we calculate the cosine relatedness of all remaining word pairs of all sentence pairs using the 840 billion token pre-trained word embeddings of GloVe (Pennington"
W17-4803,P03-1021,0,0.0632108,"nd the set of all extracted subgraphs of the training documents, FSG = {sgk1 , sgk2 , ..., sgkm } where k ∈ {3, 4, 5}, and their weights, hcoh (S) is defined as follow: X hcoh (S) = 4.2 test 12 2,093 174 4,458,256 63,778 48,122 Experimental Setup We train our systems using the Moses decoder (Koehn et al., 2007). After standard preprocessing of the data, we train a 3-gram language model using KenLM (Heafield, 2011). We use the MGIZA++ (Gao and Vogel, 2008) word aligner and employ standard grow-diag-fast-and symmetrization. Tuning is done on the development data via minimum error rate training (Och, 2003). After training the language model and creating the phrase table with Moses, we use these to initialize our translation systems. We use the lcurvedocent binary of Docent, which outputs Docent’s learning curve, i.e., files for the intermediate decoding states. This additionally allows us to investigate the learning curves with regard to how our coherence feature behaves over time. We prune the translation table by only retaining all phrase translations with a probability greater than 0.0001 during training. In our configuration file for Docent, we set to use the simulated annealing algorithm w"
W17-4803,P02-1040,0,0.0992979,"nguage pair French-English. Coherence patterns are extracted from the 1551 DiscoMT training documents using GloVe word embeddings. We extract all k-node subgraphs for k ∈ {3, 4, 5} using GASTON 5 (Nijssen and Kok, 2004, 2005). 4 We use Moses to translate sentences independently and initialize the translation state in Docent. 5 http://liacs.leidenuniv.nl/ nijssensgr/gaston/iccs.html. ˜ 6 We choose this threshold to make a balance between processing time and translation performance. 30 4.3 sg5 Evaluation Metrics We follow the standard machine translation procedure of evaluation, measuring BLEU (Papineni et al., 2002) for every system. BLEU is an ngram based co-occurrence metric that operates with modified n-gram precision scores. The document n-gram precision scores are averaged using the geometric mean of these scores with n-grams up to length N and positive weights summing to one. The result is multiplied by an exponential brevity penalty factor that penalizes a translation if it does not match the reference translations in length, word choice, and word order. We also calculate Meteor (Lavie et al., 2004; Denkowski and Lavie, 2014) as it is a widely used evaluation metric as well. In contrast to BLEU, M"
W17-4803,D16-1074,1,0.86336,"taken into account to have a high-quality translation (Hatim and Mason, 1990; Hardmeier et al., 2012). The current paradigm of machine translation needs to be improved as it does not consider any discourse coherence phenomena that establish a text’s connectedness (Sim Smith et al., 2015). One of the active research topics in modeling coherence focuses on entity connections over sentences based on Centering Theory (Grosz et al., 1995). Previous research on coherence modeling shows its application mainly in readability assessment (Barzilay and Lapata, 2008; Pitler and Nenkova, 2008). Recently, Parveen et al. (2016) showed that the graph-based coherence model can be utilized to generate more coherent summaries of scientific articles. The main goal of this paper is to integrate coherence features with a statistical machine translation system to improve the quality of the output translation. To achieve this goal, we combine the graph-based coherence representation by Guinaudeau and Strube (2013) and its extensions (Mesgar and Strube, 2015, 2016) into the documentlevel machine translation decoder Docent (Hardmeier et al., 2012, 2013). Docent defines an initial translation of the source document and modifies"
W17-4803,D14-1162,0,0.0808395,"evant background literature (Section 2). We then describe the graph-based coherence model and how we integrate its coherence features with Docent (Section 3). Section 4 outlines the datasets and the experimental setup. We discuss results in Section 5. Conclusions and possible future work are in Section 6. 2 2.1 Mesgar and Strube (2016) extend the entity graph to the lexical graph: two sentences may be semantically connected because at least two words of them are semantically associated to each other. They compute semantic relatedness between all content word pairs using GloVe word embeddings (Pennington et al., 2014). If there is a word pair whose word vectors have a cosine relatedness greater than a threshold, two sentences are considered to be connected. They quantify the coherence of texts via frequency of subgraphs of the lexical graphs. It outperforms the entity graph coherence model on readability assessment. Related Work Entity Graph Guinaudeau and Strube (2013) present a graphbased version of the entity grid (Barzilay and Lapata, 2008). It models the interaction between entities and sentences as a bipartite graph. In this representation, one set of nodes corresponds to sentences, whereas the other"
W17-4803,D08-1020,0,0.034769,"so the discourse-wide context must be taken into account to have a high-quality translation (Hatim and Mason, 1990; Hardmeier et al., 2012). The current paradigm of machine translation needs to be improved as it does not consider any discourse coherence phenomena that establish a text’s connectedness (Sim Smith et al., 2015). One of the active research topics in modeling coherence focuses on entity connections over sentences based on Centering Theory (Grosz et al., 1995). Previous research on coherence modeling shows its application mainly in readability assessment (Barzilay and Lapata, 2008; Pitler and Nenkova, 2008). Recently, Parveen et al. (2016) showed that the graph-based coherence model can be utilized to generate more coherent summaries of scientific articles. The main goal of this paper is to integrate coherence features with a statistical machine translation system to improve the quality of the output translation. To achieve this goal, we combine the graph-based coherence representation by Guinaudeau and Strube (2013) and its extensions (Mesgar and Strube, 2015, 2016) into the documentlevel machine translation decoder Docent (Hardmeier et al., 2012, 2013). Docent defines an initial translation of"
W17-4803,D11-1034,0,\N,Missing
W19-4319,E17-1075,0,0.200417,"rse types such as “person” near the top, to more specific, fine types such as “politician” in the middle, to even more specific, ultrafine entity types such as “diplomat” at the bottom (see Figure 1). By virtue of such a hierarchy, a model learning about diplomats will be able to transfer this knowledge to related entities such as politicians. Prior work integrated hierarchical entity type information by formulating a hierarchy-aware loss (Ren et al., 2016; Murty et al., 2018; Xu and Barbosa, 2018) or by representing words and types in a joint Euclidean embedding space (Shimaoka et al., 2017; Abhishek et al., 2017). Noting that it is impossible to embed arbitrary hierarchies in Euclidean space, Nickel and Kiela (2017) propose hyperbolic space as an alternative and show that hyperbolic embeddings accurately encode hierarchical information. Intuitively (and as explained in more detail in Section 2), this is because distances in hyperbolic space grow exponentially as one moves away from the origin, just like the number of elements in a hierarchy grows exponentially with its depth. While the intrinsic advantages of hyperbolic embeddings are well-established, their usefulness in downstream tasks is, so far,"
W19-4319,C16-1017,0,0.0184276,"Hy Eu Hy Eu Coarse Ma Mi 83.0 81.9 82.2 82.2 81.7 81.8 81.7 81.7 Fine Ma Mi 24.0 23.9 28.8 28.7 27.1 27.1 30.6 30.6 7 Type inventories for the task of fine-grained entity typing (Ling and Weld, 2012; Gillick et al., 2014; Yosef et al., 2012) have grown in size and complexity (Del Corro et al., 2015; Murty et al., 2017; Choi et al., 2018). Systems have tried to incorporate hierarchical information on the type distribution in different manners. Shimaoka et al. (2017) encode the hierarchy through a sparse matrix. Xu and Barbosa (2018) model the relations through a hierarchy-aware loss function. Ma et al. (2016) and Abhishek et al. (2017) learn embeddings for labels and feature representations into a joint space in order to facilitate information sharing among them. Our work resembles Xiong et al. (2019) since they derive hierarchical information in an unrestricted fashion, through type co-occurrence statistics from the dataset. These models operate under Euclidean assumptions. Instead, we impose a hyperbolic geometry to enrich the hierarchical information. Hyperbolic spaces have been applied mostly on complex and social networks modeling (Krioukov et al., 2010; Verbeek and Suri, 2016). In the field"
W19-4319,P18-1009,0,0.0589787,"n Entity typing classifies textual mentions of entities according to their semantic class. The task has progressed from finding company names (Rau, 1991), to recognizing coarse classes (person, location, organization, and other, Tjong Kim Sang and De Meulder, 2003), to fine-grained inventories of about one hundred types, with finer-grained types proving beneficial in applications such as relation extraction (Yaghoobzadeh et al., 2017) and question answering (Yavuz et al., 2016). The trend towards larger inventories has culminated in ultra-fine and open entity typing with thousands of classes (Choi et al., 2018; Zhou et al., 2018). However, large type inventories pose a challenge for the common approach of casting entity typing as a multi-label classification task (Yogatama et al., 2015; Shimaoka et al., 2016), since exploiting inter-type correlations becomes more 1 Code available at: https://github.com/ nlpAThits/figet-hyperbolic-space 169 Proceedings of the 4th Workshop on Representation Learning for NLP (RepL4NLP-2019), pages 169–180 c Florence, Italy, August 2, 2019. 2019 Association for Computational Linguistics (a) Euclidean Space. point approaches infinity in hyperbolic space, its norm approa"
W19-4319,H92-1116,0,0.133072,"under different geometries, we also experiment on the OntoNotes dataset (Gillick et al., 2014) as it is a standard benchmark for entity typing. 4.2 Deriving the Hierarchies The two methods we analyze to derive a hierarchical structure from the type inventory are the following. Knowledge base alignment: Hierarchical information can be provided explicitly, by aligning the type labels to a knowledge base schema. In this case the types follow the tree-like structure of the ontology curated by experts. On the Ultra-Fine dataset, the type vocabulary T (i.e. noun phrases) is extracted from WordNet (Miller, 1992). Nouns in WordNet are organized into a deep hierarchy, defined by hypernym or “IS A” relationships. By aligning the type labels to the hypernym structure existing in WordNet, we obtain a type hierarchy. In this case, all paths lead to the root type entity. 5 Experiments We perform experiments on the Ultra-Fine (Choi et al., 2018) and OntoNotes (Gillick et al., 2014) datasets to evaluate which kind of hierarchical information is better suited for entity typing, and under which geometry the hierarchy can be better exploited. 5.1 Setup For evaluation we run experiments on the UltraFine dataset w"
W19-4319,D15-1103,0,0.058221,"Missing"
W19-4319,P18-1010,0,0.0352869,"of types increases. A natural solution for dealing with a large number of types is to organize them in hierarchy ranging from general, coarse types such as “person” near the top, to more specific, fine types such as “politician” in the middle, to even more specific, ultrafine entity types such as “diplomat” at the bottom (see Figure 1). By virtue of such a hierarchy, a model learning about diplomats will be able to transfer this knowledge to related entities such as politicians. Prior work integrated hierarchical entity type information by formulating a hierarchy-aware loss (Ren et al., 2016; Murty et al., 2018; Xu and Barbosa, 2018) or by representing words and types in a joint Euclidean embedding space (Shimaoka et al., 2017; Abhishek et al., 2017). Noting that it is impossible to embed arbitrary hierarchies in Euclidean space, Nickel and Kiela (2017) propose hyperbolic space as an alternative and show that hyperbolic embeddings accurately encode hierarchical information. Intuitively (and as explained in more detail in Section 2), this is because distances in hyperbolic space grow exponentially as one moves away from the origin, just like the number of elements in a hierarchy grows exponentially w"
W19-4319,W18-1708,0,0.102734,"ptured via the distance and orientation in the space, and generality, via the norm of the embeddings. To mitigate the instability in the derivative of the hyperbolic distance2 we follow the approach proposed in Sala et al. (2018) and minimize the square of the distance, which does have a continuous derivative in B n . Thus, in the Poincar´e Model we minimize the distance for two points u, v ∈ B n defined as: Projecting into the Ball To learn a projection function that embeds our feature representation in the target space, we apply a variation of the re-parameterization technique introduced in Dhingra et al. (2018). The reparameterization involves computing a direction vector r and a norm magnitude λ from e(m, c) as follows: r r = ϕdir (e(m, c)), r = , krk (2) λ = ϕnorm (e(m, c)), 0 Optimization of the Model λ = σ(λ), 0 where ϕdir : Rn → Rn , ϕnorm : Rn → R can be arbitrary functions, whose parameters will be optimized during training, and σ is the sigmoid function that ensures the resulting norm λ ∈ (0, 1). The re-parameterized embedding is defined as v = λr, which lies in S n . By making use of this simple technique, the embeddings are guaranteed to lie in the Poincar´e ball. This avoids the need to c"
W19-4319,W03-0419,0,0.505216,"Missing"
W19-4319,D14-1162,0,0.0912839,"a of Choi et al. (2018) based on Shimaoka et al. (2016). We replace the location embedding of the original encoder with a word position embedding pi to reflect relative distances between the i-th word and the entity mention. This modification induces a bias on the attention layer to focus less on the mention and more on the context. Finally we apply a standard Bi-LSTM and a self-attentive encoder (McCann et al., 2017) on top to get the context representation C ∈ Rdc . For the mention representation we derive features from a character-level CNN, concatenate them with the Glove word embeddings (Pennington et al., 2014) of the mention, and combine them with a similar self-attentive encoder. The mention representation is denoted as M ∈ Rdm . The final representation is achieved by the concatenation of mention and context [M ; C] ∈ Rdm +dc . 3.5 3.6 We aim to find projection functions fi that embed the instance representations closer to the respective target types, in a given vector space S n . As target space S n we use the Poincar´e Ball B n and compare it with the Euclidean unit ball Rn . Both B n and Rn are metric spaces, therefore they are equipped with a distance function, namely the hyperbolic distance"
W19-4319,N19-1084,0,0.0660545,", 2012; Gillick et al., 2014; Yosef et al., 2012) have grown in size and complexity (Del Corro et al., 2015; Murty et al., 2017; Choi et al., 2018). Systems have tried to incorporate hierarchical information on the type distribution in different manners. Shimaoka et al. (2017) encode the hierarchy through a sparse matrix. Xu and Barbosa (2018) model the relations through a hierarchy-aware loss function. Ma et al. (2016) and Abhishek et al. (2017) learn embeddings for labels and feature representations into a joint space in order to facilitate information sharing among them. Our work resembles Xiong et al. (2019) since they derive hierarchical information in an unrestricted fashion, through type co-occurrence statistics from the dataset. These models operate under Euclidean assumptions. Instead, we impose a hyperbolic geometry to enrich the hierarchical information. Hyperbolic spaces have been applied mostly on complex and social networks modeling (Krioukov et al., 2010; Verbeek and Suri, 2016). In the field of Natural Language Processing, they have been employed to learn embeddings for Question Answering (Tay et al., 2018), in Neural Machine Translation (Gulcehre et al., 2019), and to model language"
W19-4319,N18-1002,0,0.182891,"A natural solution for dealing with a large number of types is to organize them in hierarchy ranging from general, coarse types such as “person” near the top, to more specific, fine types such as “politician” in the middle, to even more specific, ultrafine entity types such as “diplomat” at the bottom (see Figure 1). By virtue of such a hierarchy, a model learning about diplomats will be able to transfer this knowledge to related entities such as politicians. Prior work integrated hierarchical entity type information by formulating a hierarchy-aware loss (Ren et al., 2016; Murty et al., 2018; Xu and Barbosa, 2018) or by representing words and types in a joint Euclidean embedding space (Shimaoka et al., 2017; Abhishek et al., 2017). Noting that it is impossible to embed arbitrary hierarchies in Euclidean space, Nickel and Kiela (2017) propose hyperbolic space as an alternative and show that hyperbolic embeddings accurately encode hierarchical information. Intuitively (and as explained in more detail in Section 2), this is because distances in hyperbolic space grow exponentially as one moves away from the origin, just like the number of elements in a hierarchy grows exponentially with its depth. While th"
W19-4319,E17-1111,0,0.031604,"Missing"
W19-4319,D16-1015,0,0.0563015,"c advantages of hyperbolic embeddings are well-established, their usefulness in downstream tasks is, so far, less clear. We beIntroduction Entity typing classifies textual mentions of entities according to their semantic class. The task has progressed from finding company names (Rau, 1991), to recognizing coarse classes (person, location, organization, and other, Tjong Kim Sang and De Meulder, 2003), to fine-grained inventories of about one hundred types, with finer-grained types proving beneficial in applications such as relation extraction (Yaghoobzadeh et al., 2017) and question answering (Yavuz et al., 2016). The trend towards larger inventories has culminated in ultra-fine and open entity typing with thousands of classes (Choi et al., 2018; Zhou et al., 2018). However, large type inventories pose a challenge for the common approach of casting entity typing as a multi-label classification task (Yogatama et al., 2015; Shimaoka et al., 2016), since exploiting inter-type correlations becomes more 1 Code available at: https://github.com/ nlpAThits/figet-hyperbolic-space 169 Proceedings of the 4th Workshop on Representation Learning for NLP (RepL4NLP-2019), pages 169–180 c Florence, Italy, August 2, 2"
W19-4319,W16-1313,0,0.409837,"tion, organization, and other, Tjong Kim Sang and De Meulder, 2003), to fine-grained inventories of about one hundred types, with finer-grained types proving beneficial in applications such as relation extraction (Yaghoobzadeh et al., 2017) and question answering (Yavuz et al., 2016). The trend towards larger inventories has culminated in ultra-fine and open entity typing with thousands of classes (Choi et al., 2018; Zhou et al., 2018). However, large type inventories pose a challenge for the common approach of casting entity typing as a multi-label classification task (Yogatama et al., 2015; Shimaoka et al., 2016), since exploiting inter-type correlations becomes more 1 Code available at: https://github.com/ nlpAThits/figet-hyperbolic-space 169 Proceedings of the 4th Workshop on Representation Learning for NLP (RepL4NLP-2019), pages 169–180 c Florence, Italy, August 2, 2019. 2019 Association for Computational Linguistics (a) Euclidean Space. point approaches infinity in hyperbolic space, its norm approaches one in the Poincar´e disk model. In the general n-dimensional case, the disk model becomes the Poincar´e ball (Chamberlain et al., 2017) B n = {x ∈ Rn |kxk &lt; 1}, where k · k denotes the Euclidean no"
W19-4319,P15-2048,0,0.0196207,"e classes (person, location, organization, and other, Tjong Kim Sang and De Meulder, 2003), to fine-grained inventories of about one hundred types, with finer-grained types proving beneficial in applications such as relation extraction (Yaghoobzadeh et al., 2017) and question answering (Yavuz et al., 2016). The trend towards larger inventories has culminated in ultra-fine and open entity typing with thousands of classes (Choi et al., 2018; Zhou et al., 2018). However, large type inventories pose a challenge for the common approach of casting entity typing as a multi-label classification task (Yogatama et al., 2015; Shimaoka et al., 2016), since exploiting inter-type correlations becomes more 1 Code available at: https://github.com/ nlpAThits/figet-hyperbolic-space 169 Proceedings of the 4th Workshop on Representation Learning for NLP (RepL4NLP-2019), pages 169–180 c Florence, Italy, August 2, 2019. 2019 Association for Computational Linguistics (a) Euclidean Space. point approaches infinity in hyperbolic space, its norm approaches one in the Poincar´e disk model. In the general n-dimensional case, the disk model becomes the Poincar´e ball (Chamberlain et al., 2017) B n = {x ∈ Rn |kxk &lt; 1}, where k · k"
W19-4319,E17-1119,0,0.159159,"nging from general, coarse types such as “person” near the top, to more specific, fine types such as “politician” in the middle, to even more specific, ultrafine entity types such as “diplomat” at the bottom (see Figure 1). By virtue of such a hierarchy, a model learning about diplomats will be able to transfer this knowledge to related entities such as politicians. Prior work integrated hierarchical entity type information by formulating a hierarchy-aware loss (Ren et al., 2016; Murty et al., 2018; Xu and Barbosa, 2018) or by representing words and types in a joint Euclidean embedding space (Shimaoka et al., 2017; Abhishek et al., 2017). Noting that it is impossible to embed arbitrary hierarchies in Euclidean space, Nickel and Kiela (2017) propose hyperbolic space as an alternative and show that hyperbolic embeddings accurately encode hierarchical information. Intuitively (and as explained in more detail in Section 2), this is because distances in hyperbolic space grow exponentially as one moves away from the origin, just like the number of elements in a hierarchy grows exponentially with its depth. While the intrinsic advantages of hyperbolic embeddings are well-established, their usefulness in downs"
W19-4319,C12-2133,0,0.165729,"Missing"
W19-4319,D18-1231,0,0.0136701,"ssifies textual mentions of entities according to their semantic class. The task has progressed from finding company names (Rau, 1991), to recognizing coarse classes (person, location, organization, and other, Tjong Kim Sang and De Meulder, 2003), to fine-grained inventories of about one hundred types, with finer-grained types proving beneficial in applications such as relation extraction (Yaghoobzadeh et al., 2017) and question answering (Yavuz et al., 2016). The trend towards larger inventories has culminated in ultra-fine and open entity typing with thousands of classes (Choi et al., 2018; Zhou et al., 2018). However, large type inventories pose a challenge for the common approach of casting entity typing as a multi-label classification task (Yogatama et al., 2015; Shimaoka et al., 2016), since exploiting inter-type correlations becomes more 1 Code available at: https://github.com/ nlpAThits/figet-hyperbolic-space 169 Proceedings of the 4th Workshop on Representation Learning for NLP (RepL4NLP-2019), pages 169–180 c Florence, Italy, August 2, 2019. 2019 Association for Computational Linguistics (a) Euclidean Space. point approaches infinity in hyperbolic space, its norm approaches one in the Poin"
W99-0107,J96-2004,0,0.0552804,"Missing"
W99-0107,A97-1051,0,0.0801584,"Missing"
W99-0107,W98-1119,0,0.0872904,"Missing"
W99-0107,H94-1020,0,0.0191554,"a Parsed Corpus All of the applications discussed in section I depend on having a corpus of reliably marked expressions, features, and relations. In order to determine that these dimensions have been &apos;*reliably marked&quot;, we need to measure agreement between two codeas marking the same text. One way to increase the relinb&apos;dity of the coding (regardless of the method used to measure reliability) is to automate part of the coding process. Our system can extract a number of markings, features and relations from the parsed, part-of-speech-tagged corpora of the type found in in the Penn Treebank 2 (Marcus et al., 1994). Use of the Treebank data means we can find most of the markables and many of the necessary features before giving the task to a human coder. We do not try to extract any of the co-reference information from the parsed corpora. Dialog Acts. To determine the domain ofanaphoric antecedent~ the dialog must be divided into short piece. We have chosen to use units based on dialog acts for this task. Therefore, turns have to be segmcnted into dialog ~ t units. Our study of anaphoric extnssious reveals that in a dialog between two participants A and B, the DE&apos;s introduced by A are not added to the s"
W99-0107,W99-0108,1,0.899707,"Missing"
W99-0107,J97-1005,0,0.0606602,"Missing"
W99-0107,J98-2001,0,0.015582,"hod depends on counting how many co-refereace links must be added to one coder&apos;s equivalence classes to wansform the set into that found by the other c~ler. We adopt this method and enable the tool to perform t l ~ computation between any two codings which fully agree on the underlying set o f markahies. Finally. we can measure feature-value agreement by viewing the featme assignment task as a kind o f classification task and then computing Kappa (a), which measures how well the coders a . g ~ l compared to their random eJcpected agreemeat4(CaHetta, 1996). We conform to the method proposed in Poesio & Vieira (1998) for computing actual and expected agreement. (Again we assume the coders have already agreed on the set of +markables.) Suppose we are considering a given feature 2The intent is thatthe coders will achieve a high degree of consistencyif the manual is clear, and then if the manual accurately represents the desired coding style, consistency among coden impliesaccuracyof all the coding~ SAgreemem among a set of n &gt; 2 coders is usmdly calculated as a function of the . ~ pail&apos;wise agreements, so we will discuss only the pa/rwise case here, realizing that the full comlmmion is straightforward. • At"
W99-0107,P98-2204,1,0.827841,"exactly which features are most predictive of co-reference. So. we will try to mark a set of features which is a superset of the necessary features. Drawing on the feature sets used in Connolly et al. (1997) and Ge et al. (1998), we believe the following factors might indicate co-referenco: • Syntactic role (e.g. Subject, Object, Prepositional Object,...), • Pronominalization (yea or no), • Distancebetween EA and Ee (an integer), • Definiteness (yes or no), • Semantic role (e.g. indicating location, manner, time,...), • Nesting depth of an N&apos;P (an integer), • Information status (as defined by Strube (1998)) of the DE, • Gender, Number, Animacy. The tool must allow the coder to assign values for these features to each marked expression, but should not demand that every expression has a value assigned for every feature. Since we cannot claim that this set Of features is exhaustive, the tool must allow further features to be added by the user. Since reliability of feature assignment is important, the tool should have the abifity to extract as many features aspossible automatically (for example, from a parsed corpus). In addition since some features must be hand-marked, the tool must have the abili"
W99-0107,M95-1005,0,0.0632856,"he expressions which are not common to the two codings. This will make it easier to visualize the differences between the codings and reach perfect agreement of markables. The second kind of agreement measures agreement between two coders&apos; co-reference codings. We require that the two coders have the same set of markables before comparing their co-reference annotations, so achieving markable agreement of I is a prerequisite for this calculation. As discussed in section 2.3, the co-reference relation divides the set of markables into equivalence classes. A model-theoretic algorithm proposed by Vilain et al. (1995) uses these Co-reference classes to define a precision and recall metric which yields intuitively plausible results and is easy to calculate. The method depends on counting how many co-refereace links must be added to one coder&apos;s equivalence classes to wansform the set into that found by the other c~ler. We adopt this method and enable the tool to perform t l ~ computation between any two codings which fully agree on the underlying set o f markahies. Finally. we can measure feature-value agreement by viewing the featme assignment task as a kind o f classification task and then computing Kappa"
W99-0107,E99-1006,1,\N,Missing
W99-0107,J86-3001,0,\N,Missing
W99-0107,C98-2199,1,\N,Missing
W99-0108,J95-2003,0,0.584355,"Missing"
W99-0108,J86-3001,0,0.258267,"Missing"
W99-0108,J88-2003,0,0.0392824,"Missing"
W99-0108,J88-2004,0,0.0254088,"Missing"
W99-0108,P87-1022,0,0.560726,"Missing"
W99-0108,P95-1005,0,0.0456911,"Missing"
W99-0108,P98-2204,1,0.912273,"Missing"
W99-0108,J94-2006,1,0.893995,"Missing"
W99-0108,J94-2004,0,0.0364681,"Missing"
W99-0108,C98-2199,1,\N,Missing
