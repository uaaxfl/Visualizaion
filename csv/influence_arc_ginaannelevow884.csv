2007.sigdial-1.41,W01-1601,0,0.0234935,"and search for multimodal data and a powerful framework for web-based, distributed collaborative annotation and analysis. The flexibility and capabilities of the system have been demonstrated through archiving Talkbank and other spoken discourse and dialogue data and performing joint multimodal analysis of lexical, prosodic, turntaking, and other multimodal factors. 1 A number of systems have been developed to manage and support annotation of multimodal data, including Annotation Graphs (Bird and Liberman, 2001), Exmeralda (Schmidt, 2004), NITE XML Toolkit (Carletta et al., 2003), Multitool (Allwood et al., 2001), Anvil (Kipp, 2001), and Elan (Wittenburg et al., 2006). Introduction Recent research programs in multimodal environments, including understanding and analysis of multi-party meeting data and oral history recording projects, have created an explosion of multimodal data sets, including video and audio recordings, transcripts and other annotations, and increased interest in annotation and analysis of such data. However, multimodal data poses particular challenges including a broad range of annotation and analysis measures, large storage requirements for media data, and increased computational c"
2007.sigdial-1.41,wittenburg-etal-2006-elan,0,0.0422725,"ork for web-based, distributed collaborative annotation and analysis. The flexibility and capabilities of the system have been demonstrated through archiving Talkbank and other spoken discourse and dialogue data and performing joint multimodal analysis of lexical, prosodic, turntaking, and other multimodal factors. 1 A number of systems have been developed to manage and support annotation of multimodal data, including Annotation Graphs (Bird and Liberman, 2001), Exmeralda (Schmidt, 2004), NITE XML Toolkit (Carletta et al., 2003), Multitool (Allwood et al., 2001), Anvil (Kipp, 2001), and Elan (Wittenburg et al., 2006). Introduction Recent research programs in multimodal environments, including understanding and analysis of multi-party meeting data and oral history recording projects, have created an explosion of multimodal data sets, including video and audio recordings, transcripts and other annotations, and increased interest in annotation and analysis of such data. However, multimodal data poses particular challenges including a broad range of annotation and analysis measures, large storage requirements for media data, and increased computational complexity of media The framework described here, develop"
2018.gwc-1.35,P06-4018,0,\N,Missing
2021.computel-1.12,P16-2096,0,0.0213441,"evelopment of successful speaker identification or diarization technology for endangered languages, and should there be other recordings of the same individuals available elsewhere on the web, such technology would make it possible to link the different recordings to the same speaker identity, potentially deanonymyzing otherwise anonymous deposits. Beyond the harms to privacy that this represents, it also opens up further risks, such as the potential to amass sufficiently large amounts of data to create deep fakes in the voice of the recorded speakers. We find that the potential for dual use (Hovy and Spruit, 2016) is inherent in speech technology developed for endangered language documentation. That is, alongside the positive value the technology can bring by facilitating language documentation and revitalization, there is also the risk of harmful use. Different speaker communities and indeed different speakers may view these risks differently. What is particularly vexing is that the development of new technology can reshape what risks a speaker is taking on when consenting to be recorded for archival data. We recommend that computational linguists and archivists communicate about the state of technolo"
2021.computel-1.12,W06-1421,0,0.196392,"Missing"
2021.computel-1.12,P10-1102,0,0.0474457,"Table 2. Where available, we provide the ISO639-3 language codes.4 In addition to genre and phonological typological profile, we expect further kinds of variation across these languages and data sets. For example, speaker communities likely vary in their conventions around overlap between turns of different speakers (how much overlap is allowed before it is considered a rude interruption? do speakers need leave some silence after another’s turn? are listeners expected to provide audible backchannels? (Clancy et al., 1996; Levow et al., 2010; Duncan et al., 2010; Tannen, 1994; Goldberg, 1990; Laskowski, 2010)), and recordings likely vary in terms of the amount and type of ambient noise included (animals, traffic, wind). Unfortunately, we do not have access to either type of information and so will have to assume that these data sets do vary along these dimensions. Both speaker diarization and speaker identification have been the subject of ongoing shared task evaluations (Ryant et al., 2018, 2019; NIST, 2016). Earlier work on diarization focused on telephone conversations (Godfrey et al., 1992), broadcast news, and multiparty meetings (Janin et al., 2003). Recent tasks and data sets have refocused"
2021.computel-1.12,D18-1512,0,0.0330966,"Missing"
2021.computel-1.12,W17-0106,1,0.882084,"Missing"
C98-1117,P92-1008,0,0.00931861,"ems have only recently been developed, little work has been done on error correction dialogs in this context. Two areas of related research that have been investigated are the identification of self-repairs and disfluencies, where the speaker self-interrupts to change an utterThe field trial involved a group of nineteen ance in progress, and some preliminary efforts subjects. Four of the participants were members in the study of corrections in speech intmt. of the system development staff, fourteen were In analyzing and identifying self repairs, volunteers drawn from Sun Microsystems&apos; staff, (Bear et al., 1992) and (Heeman and Allen, and a final class of subjects consisted of onefound that the most effective methods 1994) time guest users There were three female and relied on identifying shared textual regions besixteen male subjects. the reparandum and the repair. However, tween All interactions with the system were these techniques are limited to those instances recorded and digitized in standard telephone where a reliable recognition string is available; audio quality format at 8kHz sampling in 8-bit in general, that is not tile case for most speech nm-law encoding during the conversation. In rec"
C98-1117,P94-1041,0,0.0346834,"Missing"
C98-1117,J93-3003,0,0.0353436,"eatures, duration, pause, and pitch, are described below. Figure 2: Contrasting Falling (top) and Rising (bottom) Pitch Contours insertion and lengthening appear in Figure 1. 5.3 Pitch To derive pitch features, we first apply the F0 (fundamental frequency) analysis function from the Entropic ESPS Waves+ system (Se(;rest and Doddington, 1993) to produce a basic pitch track. Most of the related work reported above had found relationships between the magnitude of pitch features and discourse function rather than presence of accent type, used more heavily by (Pierrehumbert and Hirschberg, 1990), (Hirschberg and Litman, 1993). Thus, we chose to concentrate on pitch features of the former type. A trained analyst examines the pitch track to remove any points of doubling or halving clue to pitch tracker error, non-speech sounds, and excessive glottalization of > 5 sample points. We compute several derived measures using simple algorithms to obtain F0 maximum, F0 minimum, F0 range, final F0 contour, slope of maximum pitch rise, slope of maximum pitch fall, and sum of the slopes of the steepest rise and fall. Figure 2 depicts a basic pitch contour. 5.4 Amplitude Amplitude, measuring the loudness of an utterance, is als"
C98-1117,J90-3003,0,\N,Missing
C98-1117,P88-1023,0,\N,Missing
D07-1037,W01-0514,0,0.0143914,"words that occur in tj and have another occurrence within n previous sentences. Due to this simplification, we compute the score based on inner products. Once we make the transition to inner products, we can use hybrid indexing and compute semantic cohesion score beyond term repetition. 4 Related Approaches We compare our approach to the LCseg algorithm which uses lexical chains to estimate topic boundaries (Galley et al., 2003). Hybrid indexing allows us to compute semantic cohesion score rather than the lexical cohesion score based on word repetitions. Choi at al. used LSA for segmentation (Choi et al., 2001). LSA (Deerwester et al., 1990) is a special case of spectral embedding and Choi at al. (Choi et al., 2001) used all vocabulary words to compute low-dimensional document vectors. We use GLSA (Matveeva et al., 2005) because it computes term vectors as opposed to the dual document-term representation with LSA and uses a different matrix of pair-wise similarities. Furthermore, Choi at al. (Choi et al., 2001) used clustering to predict boundaries whereas we used the average similarity scores. s1: The Cuban news agency Prensa Latina called Clinton ’s announcement Friday that Cubans picked up at sea"
D07-1037,P03-1071,0,0.157548,"itions to evaluate coherence. Since the sentences covering the same story represent a coherent discourse segment, they typically contain the same or related words. Repeated words build lexical chains that are consequently used to estimate lexical coherence. This can be done either by analyzing the number of overlapping lexical chains (Hearst, 1994) or by building a short-range and long-range language model (Beeferman et al., 1999). More recently, topic segmentation with lexical chains has been successfully applied to segmentation of news stories, multi-party conversation and audio recordings (Galley et al., 2003). We present a domain-independent unsupervised topic segmentation approach based on hybrid document indexing. Lexical chains have been successfully employed to evaluate lexical cohesion of text segments and to predict topic boundaries. Our approach is based in the notion of semantic cohesion. It uses spectral embedding to estimate semantic association between content nouns over a span of multiple text segments. Our method significantly outperforms the baseline on the topic segmentation task and achieves performance comparable to state-of-the-art methods that incorporate domain specific informa"
D07-1037,P94-1002,0,0.775607,"on different ways of estimating lexical coherence of text segments, such as semantic similarity between words (Kozima, 1993), similarity between blocks of text (Hearst, 1994), and adaptive language models (Beeferman et al., 1999). These approaches use word repetitions to evaluate coherence. Since the sentences covering the same story represent a coherent discourse segment, they typically contain the same or related words. Repeated words build lexical chains that are consequently used to estimate lexical coherence. This can be done either by analyzing the number of overlapping lexical chains (Hearst, 1994) or by building a short-range and long-range language model (Beeferman et al., 1999). More recently, topic segmentation with lexical chains has been successfully applied to segmentation of news stories, multi-party conversation and audio recordings (Galley et al., 2003). We present a domain-independent unsupervised topic segmentation approach based on hybrid document indexing. Lexical chains have been successfully employed to evaluate lexical cohesion of text segments and to predict topic boundaries. Our approach is based in the notion of semantic cohesion. It uses spectral embedding to estima"
D07-1037,P93-1041,0,0.700671,"Missing"
D07-1037,W06-3810,1,0.778235,"ctor space. These methods use pairwise relations between the data points encoded in a similarity matrix. The main step is to find an embedding for the data that preserves the original similarities. GLSA We use Generalized Latent Semantic Analysis (GLSA) (Matveeva et al., 2005) to compute spectral embedding for nouns. GLSA computes term vectors and since we would like to use spectral embedding for nouns, it is well-suited for our approach. GLSA extends the ideas of LSA by defining different ways to obtain the similarities matrix and has been shown to outperform LSA on a number of applications (Matveeva and Levow, 2006). GLSA begins with a matrix of pair-wise term similarities S, computes its eigenvectors U and uses the first k of them to represent terms and documents, for details see (Matveeva et al., 2005). The justification for this approach is the theorem by Eckart and Young (Golub and Reinsch, 1971) stating that inner product similarities between the term vectors based on the eigenvectors of S represent the best elementwise approximation to the entries in S. In other words, the inner product similarity in the GLSA space preserves the semantic similarities in S. Since our representation will try to prese"
D07-1037,J02-1002,0,0.131931,"Missing"
D07-1037,wayne-2000-multilingual,0,0.0213153,"een content nouns over a span of multiple text segments. Our method significantly outperforms the baseline on the topic segmentation task and achieves performance comparable to state-of-the-art methods that incorporate domain specific information. 1 Introduction The goal of topic segmentation is to discover story boundaries in the stream of text or audio recordings. Story is broadly defined as segment of text containing topically related sentences. In particular, the task may require segmenting a stream of broadcast news, addressed by the Topic Detection and Tracking (TDT) evaluation project (Wayne, 2000; Allan, 2002). In this case topically related sentences belong to the same news story. While we are considering TDT data sets in this paper, we would like to pose the problem more broadly and consider a domainindependent approach to topic segmentation. Previous research on topic segmentation has shown that lexical coherence is a reliable indicator of topical relatedness. Therefore, many approaches When the task is to segment long streams of text containing stories which may continue at a later point in time, for example developing news stories, building of lexical chains becomes intricate. In"
dorr-etal-2000-building,W98-1426,0,\N,Missing
dorr-etal-2000-building,dorr-etal-2000-chinese,1,\N,Missing
dorr-etal-2000-building,dorr-katsova-1998-lexical,1,\N,Missing
dorr-etal-2000-building,C94-1038,0,\N,Missing
dorr-etal-2000-building,olsen-etal-1998-enhancing,1,\N,Missing
dorr-etal-2000-building,A97-1021,1,\N,Missing
dorr-etal-2000-building,P98-1046,0,\N,Missing
dorr-etal-2000-building,C98-1046,0,\N,Missing
dorr-etal-2000-building,P98-1116,0,\N,Missing
dorr-etal-2000-building,C98-1112,0,\N,Missing
dorr-etal-2000-building,dorr-etal-1998-thematic,1,\N,Missing
dorr-etal-2000-chinese,C94-1038,0,\N,Missing
dorr-etal-2000-chinese,olsen-etal-1998-enhancing,1,\N,Missing
dorr-etal-2000-chinese,A97-1021,1,\N,Missing
dorr-etal-2000-chinese,P98-1046,0,\N,Missing
dorr-etal-2000-chinese,C98-1046,0,\N,Missing
E06-2017,N03-1032,0,0.203919,"rds in V , obtain a matrix of pair-wise similarities, S, using the large corpus W 3. Obtain the matrix U T of low dimensional vector space representation of terms that preserves the similarities in S, U T ∈ Rk×|V | 4. Compute document vectors by taking linear ˆ = UT D combinations of term vectors D ˆ are documents in the kThe columns of D dimensional space. In step 2 we used point-wise mutual information (PMI) as the co-occurrence based measure of semantic associations between pairs of the vocabulary terms. PMI has been successfully applied to semantic proximity tests for words (Turney, 2001; Terra and Clarke, 2003) and was also successfully used as a measure of term similarity to compute document clusters (Pantel and Lin, 2002). In 152 our preliminary experiments, the GLSA with PMI showed a better performance than with other cooccurrence based measures such as the likelihood ratio, and χ2 test. PMI between random variables representing two words, w1 and w2 , is computed as P (W1 = 1, W2 = 1) . P (W1 = 1)P (W2 = 1) (4) We used the singular value decomposition (SVD) in step 3 to compute GLSA term vectors. LSA (Deerwester et al., 1990) and some other related dimensionality reduction techniques, e.g. Locali"
H01-1050,1998.amta-tutorials.5,0,\N,Missing
H01-1050,W98-1005,0,\N,Missing
H01-1050,J95-4004,0,\N,Missing
H01-1050,A97-1029,0,\N,Missing
I05-3010,N04-4035,1,0.829582,"average pitch for all tones is reduced, and relative tone height is largely preserved.3 Thus a final high tone is readily distinguishable from a final low tone, if the listener can interpret the syllable as turn-final. The contrasts appear in Figure 5. Turning to tone contour, we find likewise little change between non-final and final contours, with the contours running parallel, but at a much lower pitch.4 For illustration, mid-rising and high falling tones are shown in Figure 6. Comparable behavior has been observed at other discourse boundaries such as story boundaries in newswire speech. (Levow, 2004). Figure 4: Intensity contrasts between initial syllables in smooth turn transitions and interruptions. Values for smooth transitions are in black, interruptions in grey. prosodic cues to take the floor by interruption. These descriptive analyses demonstrate that intonational cues to turn-taking do play a role in a tone language. Not only does intensity play a significant role, but pitch also is employed to distinguish initiation and finality, in spite of its concurrent use in determining lexical identity. In the following section, we describe the effects on tone height and tone shape caused b"
I05-3010,W04-3209,0,0.0426646,"Missing"
I05-3010,H01-1073,0,0.0742116,"ealization of lexical tone (Section 4). Finally we will apply these prosodic contrasts to enable classification of words for finality and interruption status (Section 5). 2 2.1 Prosodic Features For the subsequent analysis, the conversations were divided into chunks based on the turn and overlap time-stamps. Using a Chinese character-to-pinyin dictionary and a handconstructed mapping of pinyin sequences to ARPABET phonemes, the transcribed text was then force-aligned to the corresponding audio segments using the language porting mechanism in the University of Colorado Sonic speech recognizer (Pellom et al., 2001). The resulting alignment provided phone, syllable, and word durations as well as silence positions and durations. Pitch and intensity values for voiced regions were computed using the functions ”To Pitch” and ”To Intensity” in the freely available Praat acoustic analysis software package(Boersma, 2001). We then computed normalized pitch and intensity values based on log-scaled z-score normalization of each conversation side. Based on the above alignment, we then computed maximum and mean pitch and intensity values for each syllable and word for all voiced regions. Given the presence of lexica"
I08-1029,P04-1086,0,0.59989,"l these coarticulatory effects, and recent approaches have primarily employed local classifiers, such as decision trees (Sun, 2002; Rosenberg and Hirschberg, 2006) or Support Vector Machines (Levow, 2005). Another body of work on pitch accent recognition has focused on exploitation of lexical and syntactic information to predict ToBI labels, for example for speech synthesis. These approaches explored a range of machine learning techniques from local classifiers such as decision trees (Sun, 2002) and RIPPER (Pan and McKeown, 1998) to sequence models such as Conditional Random Fields 217 (CRFs)(Gregory and Altun, 2004) more recently. The systems often included features that captured local or longer range context, such as n-gram probabilities, neighboring words, or even indicators of prior mention. (Chen et al., 2004; Rangarajan Sridhar et al., 2007) explored the integration of based prosodic and lexico-syntactic evidence in GMM-based and maximum entropy models respectively. Here we explore the use of contextual acoustic and lexical models within a sequence learning framework. We analyze the interaction of different feature types on prediction of prosodic labels using linear-chain CRFs. We demonstrate improv"
I08-1029,P05-1056,0,0.0277309,"and exploits inter-label dependencies. Furthermore, under the ToBI labeling guidelines, the presence of a boundary tone dictates the co-occurrence of a phrase accent label. To capture these relations between labels of different types, we also consider factorial models. Conditional Random Fields (Lafferty et al., 2001) are a class of graphical models which are undirected and conditionally trained. While they can represent long term dependencies, most applications have employed first-order linear chains for language and speech processing tasks including POS tagging, sentence boundary detection (Liu et al., 2005), and even text-oriented pitch accent prediction(Gregory and Altun, 2004). The models capture sequential label-label relations, but unlike HMMs, the conditionally trained model can more tractably support larger text-based feature sets. Factorial CRFs (Sutton, 2006; McCallum et al., 2003) augment the linear sequence model with additional cotemporal labels, so that multiple (factors) labels are predicted at each time step and dependencies between them can be modeled. Examples of linear-chain and factorial CRFs appear in Figure 1. In the linear chain example, the fi items correspond to the featur"
I08-1029,P98-2165,0,0.0153999,"4; Dusterhoff et al., 1999) employed Hidden Markov Models, they did not explicitly model these coarticulatory effects, and recent approaches have primarily employed local classifiers, such as decision trees (Sun, 2002; Rosenberg and Hirschberg, 2006) or Support Vector Machines (Levow, 2005). Another body of work on pitch accent recognition has focused on exploitation of lexical and syntactic information to predict ToBI labels, for example for speech synthesis. These approaches explored a range of machine learning techniques from local classifiers such as decision trees (Sun, 2002) and RIPPER (Pan and McKeown, 1998) to sequence models such as Conditional Random Fields 217 (CRFs)(Gregory and Altun, 2004) more recently. The systems often included features that captured local or longer range context, such as n-gram probabilities, neighboring words, or even indicators of prior mention. (Chen et al., 2004; Rangarajan Sridhar et al., 2007) explored the integration of based prosodic and lexico-syntactic evidence in GMM-based and maximum entropy models respectively. Here we explore the use of contextual acoustic and lexical models within a sequence learning framework. We analyze the interaction of different feat"
I08-1029,N07-1001,0,0.202568,"cent recognition has focused on exploitation of lexical and syntactic information to predict ToBI labels, for example for speech synthesis. These approaches explored a range of machine learning techniques from local classifiers such as decision trees (Sun, 2002) and RIPPER (Pan and McKeown, 1998) to sequence models such as Conditional Random Fields 217 (CRFs)(Gregory and Altun, 2004) more recently. The systems often included features that captured local or longer range context, such as n-gram probabilities, neighboring words, or even indicators of prior mention. (Chen et al., 2004; Rangarajan Sridhar et al., 2007) explored the integration of based prosodic and lexico-syntactic evidence in GMM-based and maximum entropy models respectively. Here we explore the use of contextual acoustic and lexical models within a sequence learning framework. We analyze the interaction of different feature types on prediction of prosodic labels using linear-chain CRFs. We demonstrate improved recognition by integration of textual and acoustic cues, well-supported by the sequence model. Finally we consider the joint prediction of multiple prosodic label types, finding improvement for joint modeling in the case of feature"
I08-1029,C98-2160,0,\N,Missing
N04-4035,J86-3001,0,0.111389,"duction Natural spoken discourse is composed a sequence of utterances, not independently generated or randomly strung together, but rather organized according to basic structural principles. This structure in turn guides the interpretation of individual utterances and the discourse as a whole. Formal written discourse signals a hierarchical, tree-based discourse structure explicitly by the division of the text into chapters, sections, paragraphs, and sentences. This structure, in turn, identifies domains for interpretation; many systems for anaphora resolution rely on some notion of locality (Grosz and Sidner, 1986). Similarly, this structure represents topical organization, and thus would be useful in information retrieval to select documents where the primary sections are on-topic, and, for summarization, to select information covering the different aspects of the topic. Unfortunately, spoken discourse does not include the orthographic conventions that signal structural organization in written discourse. Instead, one must infer the hierarchical structure of spoken discourse from other cues. Prior research (Nakatani et al., 1995; Swerts, 1997) has shown that human labelers can more sharply, consistently"
N04-4035,P94-1002,0,0.183656,"ost robust, and segmented data sets are publicly available. Furthermore, we apply prosodic-based segmentation to Mandarin Chinese. Not only is the use of prosodic cues to topic segmentation much less well-studied in general than is the use of text cues, but the use of prosodic cues has been largely limited to English and other European languages. 2 Related Work Most prior research on automatic topic segmentation has been applied to clean text only and thus used textual features. Text-based segmentation approaches have utilized term-based similarity measures computed across candidate segments (Hearst, 1994) and also discourse markers to identify discourse structure (Marcu, 2000). The Topic Detection and Tracking (TDT) evaluations focused on segmentation of both text and speech sources. This framework introduced new challenges in dealing with errorful automatic transcriptions as well as new opportunities to exploit cues in the original speech. The most successful approach (Beeferman et al., 1999) produced automatic segmentations that yielded retrieval results comparable to those with manual segmentations, using text and silence features. (Tur et al., 2001) applied both a prosody-only and a mixed"
N04-4035,J01-1002,0,0.342437,"measures computed across candidate segments (Hearst, 1994) and also discourse markers to identify discourse structure (Marcu, 2000). The Topic Detection and Tracking (TDT) evaluations focused on segmentation of both text and speech sources. This framework introduced new challenges in dealing with errorful automatic transcriptions as well as new opportunities to exploit cues in the original speech. The most successful approach (Beeferman et al., 1999) produced automatic segmentations that yielded retrieval results comparable to those with manual segmentations, using text and silence features. (Tur et al., 2001) applied both a prosody-only and a mixed text-prosody model to segmentation of TDT English broadcast news, with the best results combining text and prosodic features. (Hirschberg and Nakatani, 1998) also examined automatic topic segmentation based on prosodic cues, in the domain of English broadcast news. Work in discourse analysis (Nakatani et al., 1995; Swerts, 1997) in both English and Dutch has identified features such as changes in pitch range, intensity, and speaking rate associated with segment boundaries and with boundaries of different strengths. They also demonstrated that access to"
N04-4035,wayne-2000-multilingual,0,0.0610329,"ll-established (Ross and Ostendorf, 1996) that for robust analysis pitch and intensity should be normalized by speaker, since, for example, average pitch is largely incomparable for male and female speakers. In the absence of speaker identification software, we approximate speaker normalization with story-based   , assuming one normalization, computed as   speaker per topic1 . For duration, we consider both absolute and normalized word duration, where average word duration is used as the mean in the calculation above. 5 Data Set We utilize the Topic Detection and Tracking (TDT) 3 (Wayne, 2000) collection Mandarin Chinese broadcast news audio corpus as our data set. Story segmentation in Mandarin and English broadcast news and newswire text was one of the TDT tasks and also an enabling technology for other retrieval tasks. We use the segment boundaries provided with the corpus as our gold standard labeling. Our collection comprises 3014 stories drawn from approximately 113 hours over three months (OctoberDecember 1998) of news broadcasts from the Voice of America (VOA) in Mandarin Chinese. The transcriptions span approximately 740,000 words. The audio is stored in NIST Sphere format"
N06-1029,W04-3010,0,0.0427417,"ployed phrase structure (Fujisaki, 1983), several recent approaches to tone recognition in East Asian languages (Wang and Seneff, 2000; Zhou et al., 2004) have incorporated elements of local and broad range contextual influence on tone. Many of these techniques create explicit context-dependent models of the phone, tone, or accent for each context in which they appear, either using the tone sequence for left or right context or using a simplified high-low contrast, as is natural for integration in a Hidden Markov Model speech recognition framework. In pitch accent recognition, recent work by (Hasegawa-Johnson et al., 2004) has integrated pitch accent and boundary tone recognition with speech recognition using prosodically conditioned models within an HMM framework, improving both speech and prosodic recognition. Since these approaches are integrated with HMM speech recognition models, standard HMM training procedures which rely upon large labeled training sets are used for tone recognition as well. Other tone and pitch accent recognition approaches using other classification frameworks such as support vector machines (Thubthong and Kijsirikul, 2001) and decision trees with boosting and bagging (Sun, 2002) have"
N06-1029,H01-1073,0,0.0255002,"igh, low, and downstepped high for experimentation. 2.2 Mandarin Chinese Tone Data Mandarin Chinese is a language with lexical tone in which each syllable carries a tone and the meaning of the syllable is jointly determined by the tone and segmental information. Mandarin Chinese has four canonical lexical tones, typically described as follows: 1) high level, 2) mid-rising, 3) low fallingrising, and 4) high falling.1 The canonical pitch con1 For the experiments in this paper, we exclude the neutral tone, which appears on unstressed syllables, because the clear Colorado Sonic speech recognizer (Pellom et al., 2001). A mapping from the transcriptions to English phone sequences supported by Sonic was created using a Chinese character-pinyin pronunciation dictionary and a manually constructed mapping from pinyin sequences to the closest corresponding English phone sequences.3 2.3 Figure 1: Contours for canonical Mandarin tones tours for these tones appear in Figure 1. We employ data from two distinct sources in the experiments reported here. 2.2.1 Read Speech The first data set is very clean speech data drawn from a collection of read speech collected under laboratory conditions by (Xu, 1999). In these mat"
P09-2068,N07-1001,0,0.0631099,"Missing"
P09-2068,I08-1029,1,\N,Missing
P11-2108,W09-3936,0,0.0268995,"nation manifests in the timing of contributions from the conversational participants, through turntaking and back-channels. (Duncan, 1972) proposed an analysis of turn-taking as rule-governed, supported by a range of prosodic and non-verbal cues. Several computational approaches have investigated prosodic and verbal cues to these phenomena. (Shriberg et al., 2001) found that prosodic cues could aid in the identification of jump-in points in multi-party meetings. (Cathcart et al., 2003) employed features such as pause duration and part-ofspeech (POS) tag sequences for back-channel prediction. (Gravano and Hirschberg, 2009) investigated back-channel-inviting cues in task-oriented dialog, identifying increases in pitch and intensity as well as certain POS patterns as key contributors. In multi-lingual comparisons, (Ward and Tsukuhara, 2000; Ward and Al Bayyari, 2007; Rivera and Ward, 2007) found pitch patterns, including periods of low pitch or drops in pitch, to be associated with eliciting back-channels across Japanese, English, Arabic, and Spanish. (Herrera et al., 2010) collected a corpus of multi-party interactions among American English, Mexican Spanish, and Arabic speakers to investigate cross-cultural dif"
P11-2108,H01-1073,0,0.0177425,"t viewed a six minute film, the “Pear Film” (Chafe, 1975), developed for language-independent elicitation. In the role of Speaker, this participant then related the story to the active and engaged Listener, who understood that they would need to retell the story themselves later. We have collected 114 elicitations: 45 Arabic, 32 Mexican Spanish, and 37 American English. All recordings have been fully transcribed and time-aligned to the audio using a semi-automated procedure. We convert an initial manual coarse transcription at the phrase level to a full word and phone alignment using CUSonic (Pellom et al., 2001), applying its language porting functionality to Spanish and Arabic. In addition, word and phrase level English glosses were manually created for the Spanish and Arabic data. Manual annotation of a broad range of nonverbal cues, including gaze, blink, head nod and tilt, fidget, and coverbal gestures, is underway. For the experiments presented in the remainder of this paper, we employ a set of 45 vetted dyads, 15 in each language. Analysis of cross-cultural differences in narrative length, rate of listener verbal contributions, and the use of pitch and intensity in eliciting listener vocalizati"
P98-1122,P92-1008,0,0.0124567,"Missing"
P98-1122,P94-1041,0,0.0367621,"Missing"
P98-1122,J93-3003,0,0.03571,"es, duration, pause, and pitch, are described below. iL°,. Figure 2: Contrasting Falling (top) and Rising (bottom) Pitch Contours insertion and lengthening appear in Figure 1. 5.3 Pitch To derive pitch features, we first apply the F0 (fundamental frequency) analysis function from the Entropic ESPS Waves+ system (Secrest and Doddington, 1993) to produce a basic pitch track. Most of the related work reported above had found relationships between the magnitude of pitch features and discourse function rather than presence of accent type, used more heavily by (Pierrehumbert and Hirschberg, 1990), (Hirschberg and Litman, 1993). Thus, we chose to concentrate on pitch features of the former type. A trained analyst examines the pitch track to remove any points of doubling or halving due to pitch tracker error, non-speech sounds, and excessive glottalization of > 5 sample points. We compute several derived measures using simple algorithms to obtain F0 maximum, F0 minimum, F0 range, final F0 contour, slope of maximum pitch rise, slope of m a x i m u m pitch fall, and sum of the slopes of the steepest rise and fall. Figure 2 depicts a basic pitch contour. 5.4 Descriptive Acoustic Analysis Amplitude Amplitude, measuring t"
P98-1122,J90-3003,0,\N,Missing
P98-1122,P88-1023,0,\N,Missing
S15-2075,S15-2053,0,0.166441,"Missing"
S15-2075,J02-3001,0,0.129868,"Heuristic Filter eliminates some systematic errors in the Semantics Classifier. 2.1 Featurization Many of the features used in our system were inspired by the system produced by Toutanova et al. (2008), which used many features from prior work. This was a top-performing system and we incorporated each of the features that applied to the dependency parsing framework adopted in this task. We then augmented this feature set with a number of novel additional features. Many of these were adaptations of Semantic Role Labeling (SRL) features from the phrase-structure to dependency parsing paradigm (Gildea and Jurafsky 2002, Surdeanu et 434 al. 2003, Pradhan et al. 2004). Others were added to generalize better to unseen verbs, which is critical for our task. Some of our features depend on having a phrasestructure parse node corresponding to the candidate dependency parse node. Since dependency parse nodes each correspond to a token in the sentence, the tokens corresponding to the candidate node and its descendants in the dependency parse tree were identified. Then, in the phrase-structure parse tree, the lowest ancestor to all of these tokens was taken to be the phrase-structure parse node best corresponding to"
S15-2075,P14-5010,0,0.00348852,"Britain could or should abandon development, either for itself or for the developing world.” This subtask involves taking Our system consists of a pipelined five-component system plus source data and resources. A system diagram is shown in Figure 1. A cascading series of MaxEnt classifiers are used to identify arguments, their syntactic labels, and then their semantic labels. Each token in an input sentence was a training example. Sketch Engine (Kilgarriff 2014) was used to help with featurization. All sentences in the training data were parsed and POS tagged using the Stanford CoreNLP tools (Manning et al. 2014). This data was used to generate features which are then supplied to an Argument Identification Classifier (AIC) that identifies whether or not a particular token is one of the relevant verb’s arguments. For the tokens identified as arguments to the verb, a Syntax Classifier identifies the syntactic role of the token. This is done using a multi-class MaxEnt model with the same features as the AIC plus features derived from the AIC’s predictions. A similar Semantics Classifier follows, taking the Syntax 433 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pag"
S15-2075,N04-1030,0,0.155332,"Missing"
S15-2075,P03-1002,0,0.176615,"Missing"
S15-2075,J08-2002,0,0.0461896,"Missing"
S15-2075,W04-3212,0,0.0449037,"d part of speech of the parent of the parse node corresponding to the candidate node  Node-LCA Partial Path: The Path between the candidate node and the lowest common ancestor between the candidate node and the verb  PP Parent Head Word: The head word of the parent node in the syntax tree, if that parent is a prepositional phrase.  PP NP Head Word/POS: If the syntax parse node representing the candidate node is a PP, the head word and POS of the rightmost NP directly under the PP. Finally, baseline features that consisted entirely of pairs of already-mentioned features were also taken from Xue and Palmer (2004):  Predicate Lemma & Path  Predicate Lemma & Head Word of Phrase  Predicate Lemma & Phrase Type  Voice & Position  Predicate Lemma & PP Parent Head Word We added additional features adapted from the aforementioned features to generalize better given the sparse training data relative to other SRL tasks:  Head POS of Phrase: the tagged POS of the Head Word of Phrase  Head Lemma of Phrase: the lemma of the Head Word of Phrase  First/Last Lemma: the lemma of the first and last word under the candidate parse node  Left/Right Sister Head Lemma: the lemmas of the Left/Right Sister Head Words"
W03-1110,H01-1050,1,0.842267,"for matching as cognates in retrieval even when no explicit translation is available. Recent side experiments on preand post-translation query expansion on the EnglishChinese pair show a similar pattern of effectiveness for post-translation expansion over pre-translation expansion (Levow et al., Under Review). A further complication is caused by the fact that Mandarin Chinese is written without white space separating words. As a result, some segmentation process must be performed to identify words for translation, even though indexing and retrieval can be performed effectively on -gram units (Meng et al., 2001). This segmentation process typically relies on a list of terms that may appear in legal segmentations. Just as in the case of translation, these term lists often lack good coverage of proper names. Thus, these terms may not be identified for translation, expansion, or even transcription by an automatic speech recognition system that also depends on word lists as models. These constraints limit the effectiveness of pre-translation expansion. In post-translation expansion, however, these problems are much less significant. In English, white-space delimited terms are available and largely suffic"
W04-1115,J86-3001,0,0.208023,"tion Natural spoken discourse is composed of a sequence of utterances, not independently generated or randomly strung together, but rather organized according to basic structural principles. This structure in turn guides the interpretation of individual utterances and the discourse as a whole. Formal written discourse signals a hierarchical, tree-based discourse structure explicitly by the division of the text into chapters, sections, paragraphs, and sentences. This structure, in turn, identifies domains for interpretation; many systems for anaphora resolution rely on some notion of locality (Grosz and Sidner, 1986). Similarly, this structure represents topical organization, and thus would be useful in information retrieval to select documents where the primary sections are on-topic, and, for summarization, to select information covering the different aspects of the topic. Unfortunately, spoken discourse does not include the orthographic conventions that signal structural organization in written discourse. Instead, one must infer the hierarchical structure of spoken discourse from other cues. Prior research (Nakatani et al., 1995; Swerts, 1997) has shown that human labelers can more sharply, consistently"
W04-1115,P94-1002,0,0.049365,"ts are publicly available. Furthermore, we apply prosodic-based segmentation to Mandarin Chinese, in addition to textual features. Not only is the use of prosodic cues to topic segmentation much less well-studied in general than is the use of text cues, but the use of prosodic cues has been largely limited to English and other European languages. 2 Related Work Most prior research on automatic topic segmentation has been applied to clean text only and thus used textual features. Text-based segmentation approaches have utilized term-based similarity measures computed across candidate segments (Hearst, 1994) and also discourse markers to identify discourse structure (Marcu, 2000). The Topic Detection and Tracking (TDT) evaluations focused on segmentation of both text and speech sources. This framework introduced new challenges in dealing with errorful automatic transcriptions as well as new opportunities to exploit cues in the original speech. The most successful approach (Beeferman et al., 1999) produced automatic segmentations that yielded retrieval results approaching those with manual segmentations, using text and silence features. (Tur et al., 2001) applied both a prosody-only and a mixed te"
W04-1115,J01-1002,0,0.0235931,"y measures computed across candidate segments (Hearst, 1994) and also discourse markers to identify discourse structure (Marcu, 2000). The Topic Detection and Tracking (TDT) evaluations focused on segmentation of both text and speech sources. This framework introduced new challenges in dealing with errorful automatic transcriptions as well as new opportunities to exploit cues in the original speech. The most successful approach (Beeferman et al., 1999) produced automatic segmentations that yielded retrieval results approaching those with manual segmentations, using text and silence features. (Tur et al., 2001) applied both a prosody-only and a mixed text-prosody model to segmentation of TDT English broadcast news, with the best results combining text and prosodic features. (Hirschberg and Nakatani, 1998) also examined automatic topic segmentation based on prosodic cues, in the domain of English broadcast news, while (Hirschberg et al., 2001) applied similar cues to segmentation of voicemail. Work in discourse analysis (Nakatani et al., 1995; Swerts, 1997) in both English and Dutch has identified features such as changes in pitch range, intensity, and speaking rate associated with segment boundaries"
W04-1115,wayne-2000-multilingual,0,0.0145562,"associated with each syllable. This additional use of pitch raises the question of the cross-linguistic applicability of the prosodic cues, especially pitch cues, identified for non-tone languages. Specifically, do we find intonational cues in tone languages? The fact that emphasis is marked intonationally by expansion of pitch range even in the presence of Mandarin lexical tone (Shen, 1989) suggests encouragingly that prosodic, intonational cues to other aspects of information structure might also prove robust in tone languages. 4 Data Set We utilize the Topic Detection and Tracking (TDT) 3 (Wayne, 2000) collection Mandarin Chinese broadcast news audio corpus as our data set. Story segmentation in Mandarin and English broadcast news and newswire text was one of the TDT tasks and also an enabling technology for other retrieval tasks. We use the segment boundaries provided with the corpus as our gold standard labeling. Our collection comprises 3014 news stories drawn from approximately 113 hours over three months (OctoberDecember 1998) of news broadcasts from the Voice of America (VOA) in Mandarin Chinese, with 800 regions of other program material including musical interludes and teasers. The"
W04-2318,P98-1122,1,0.763469,", in conjunction with textual cues to topicality, by (Tur et al., 2001), where large pitch differences between pre- and post- boundary positions play the most significant role among prosodic cues. In spoken dialogue, research has focused on the identification of dialogue acts and dialogue games. Integration of textual and prosodic cues, such as particular pitch accent or contour types, have been found useful for identifying act type(Shriberg et al., 1998; Taylor et al., 1998). Specific classes of dialogue act, such as corrections (request repair), have received particular interest in work by (Levow, 1998; Swerts et al., 2000) in the context of human-computer error resolution. Recent work on the ICSI multi-party meeting recorder data has demonstrated some very preliminary results on multi-party segmentation (Galley et al., 2003); prosodic information in this case was limited to silence duration. With the exception of work on error resolution, most work on dialogue has focused human-human interaction and on identification of particular act or game types. Here we concentrate on the general question of discourse segmentation in voice-only human-computer interaction. We ask whether the cues to seg"
W04-2318,J97-1005,0,0.0286138,"rded and digitized in standard telephone audio quality format at 8kHz sampling in 8-bit mu-law encoding during the conversation. In addition, speech recognition results, parser results, and synthesized responses were logged. A paid assistant then produced a correct verbatim transcript of all user utterances. Overall there were 7752 user utterances recorded. 1 Designing SpeechActs: Issues in Speech User Interface Design (Yankelovich et al., 1995) p. 2 3.2 Data Coding and Extraction Consistent discourse segmentation can be difficult even for trained experts (Nakatani et al., 1995; Swerts, 1997; Passoneau and Litman, 1997), and differences in depth of nesting for discourse structure appear to be the most problematic. As a result, we chose to examine utterances whose segment and topic initiating status would be relatively unambiguous. As the SpeechActs system consists of 6 different applications, we chose to focus on changes from application to application as reliable indicators of topic initiation. These commands are either simply the name of the desirable application, as in “Mail” or “Calendar”, possibly with an optional politeness term, or a switch command, such as “Switch to” and the name of the application."
W04-2318,J86-3001,0,0.459669,"er minimum pitch. These results suggest that even in the artificial environment of human-computer dialogue, prosodic cues robustly signal discourse segment structure, comparably to the contrastive uses of pitch and amplitude identified in natural monologues. Keywords Dialogue Systems, Discourse structure, Prosody in understanding 1 Introduction Contemporary theories of discourse, both computational and descriptive, postulate a tree-structured hierarchical model of discourse. These structures may be viewed as corresponding to“intentional” structure of discourse segment purposes in the view of (Grosz and Sidner, 1986), to plan and subplan structure directly in the view of (Allen and Litman, 1990) , to nuclei and satellite rhetorical relations in the Rhetorical Structure Theory of (Mann and Thompson, 1987), or to information structures as in (Traum and Hinkelman, 1992). Despite this diversity of views on the sources of structural organization, these theories agree on the decomposition of discourse into segments and subsegments in a hierarchical structure. Discourse segments help to establish the domain of interpretation for referents or anaphors. (Grosz, 1977) Discourse segmentation can also provide guidanc"
W04-2318,J01-1002,0,0.257257,"extracted based heavily on cue phrases and discourse markers by (Marcu, 2000). In spoken monologue, prosodic cues to discourse structure and segmentation have been explored by (Nakatani et al., 1995; Swerts, 1997). Increases in pitch range, amplitude, and silence duration appear to signal discourse segment boundaries across different domains - voicemail, broadcast news, descriptive narrative - and across different languages, such as English and Dutch. Comparable prosodic cues have been applied to the related task of news story segmentation, in conjunction with textual cues to topicality, by (Tur et al., 2001), where large pitch differences between pre- and post- boundary positions play the most significant role among prosodic cues. In spoken dialogue, research has focused on the identification of dialogue acts and dialogue games. Integration of textual and prosodic cues, such as particular pitch accent or contour types, have been found useful for identifying act type(Shriberg et al., 1998; Taylor et al., 1998). Specific classes of dialogue act, such as corrections (request repair), have received particular interest in work by (Levow, 1998; Swerts et al., 2000) in the context of human-computer erro"
W04-2318,P94-1002,0,\N,Missing
W04-2318,P03-1071,0,\N,Missing
W04-2318,C98-1117,1,\N,Missing
W04-2906,J86-3001,0,0.0520999,"tion Natural spoken discourse is composed of a sequence of utterances, not independently generated or randomly strung together, but rather organized according to basic structural principles. This structure in turn guides the interpretation of individual utterances and the discourse as a whole. Formal written discourse signals a hierarchical, tree-based discourse structure explicitly by the division of the text into chapters, sections, paragraphs, and sentences. This structure, in turn, identifies domains for interpretation; many systems for anaphora resolution rely on some notion of locality (Grosz and Sidner, 1986). Similarly, this structure represents topical organization, and thus would be useful in information retrieval to select documents where the primary sections are on-topic, and, for summarization, to select information covering the different aspects of the topic. Unfortunately, spoken discourse does not include the orthographic conventions that signal structural organization in written discourse. Instead, one must infer the hierarchical structure of spoken discourse from other cues. Prior research (Nakatani et al., 1995; Swerts, 1997) has shown that human labelers can more sharply, consistently"
W04-2906,P94-1002,0,0.0608871,"text-based, and mixed cue-based segmentation for Mandarin Chinese, to assess the relative utility of the cues for a tone language. Not only is the use of prosodic cues to topic segmentation much less wellstudied in general than is the use of text cues, but the use of prosodic cues has been largely limited to English and other European languages. 2 Related Work Most prior research on automatic topic segmentation has been applied to clean text only and thus used textual features. Text-based segmentation approaches have utilized term-based similarity measures computed across candidate segments (Hearst, 1994) and also discourse markers to identify discourse structure (Marcu, 2000). The Topic Detection and Tracking (TDT) evaluations focused on segmentation of both text and speech sources. This framework introduced new challenges in dealing with errorful automatic transcriptions as well as new opportunities to exploit cues in the original speech. The most successful approach (Beeferman et al., 1999) produced automatic segmentations that yielded retrieval results comparable to those with manual segmentations, using text and silence features. (Tur et al., 2001) applied both a prosody-only and a mixed"
W04-2906,N04-4035,1,0.0943029,"an in the calculation above. Mandarin Chinese is a tone language in which lexical identity is determined by a pitch contour - or tone - associated with each syllable. This additional use of pitch raises the question of the cross-linguistic applicability of the prosodic cues, especially pitch cues, identified for non-tone languages. Specifically, do we find intonational cues in tone languages? We have found highly significant differences&quot;!  based  #$ on paired t-test two-tailed, ( ) for words in segment-final position, relative to the same word in non-final positions. (Levow, 2004). Specifically, word duration, normalized mean pitch, and normalized mean intensity all differ significantly for words in topicfinal position relative to occurrences throughout the story. Word duration increases, while both pitch and intensity decrease. Importantly, reduction in pitch as a signal of topic finality is robust across the typological contrast of tone and non-tone languages, such as English (Nakatani et al., 1995) and Dutch (Swerts, 1997). 1 This is an imperfect approximation as some stories include off-site interviews, but seems a reasonable choice in the absence of automatic spea"
W04-2906,J01-1002,0,0.152214,"measures computed across candidate segments (Hearst, 1994) and also discourse markers to identify discourse structure (Marcu, 2000). The Topic Detection and Tracking (TDT) evaluations focused on segmentation of both text and speech sources. This framework introduced new challenges in dealing with errorful automatic transcriptions as well as new opportunities to exploit cues in the original speech. The most successful approach (Beeferman et al., 1999) produced automatic segmentations that yielded retrieval results comparable to those with manual segmentations, using text and silence features. (Tur et al., 2001) applied both a prosody-only and a mixed text-prosody model to segmentation of TDT English broadcast news, with the best results combining text and prosodic features. (Hirschberg and Nakatani, 1998) also examined automatic topic segmentation based on prosodic cues, in the domain of English broadcast news. Work in discourse analysis (Nakatani et al., 1995; Swerts, 1997) in both English and Dutch has identified features such as changes in pitch range, intensity, and speaking rate associated with segment boundaries and with boundaries of different strengths. 3 Data Set We utilize the Topic Detect"
W04-2906,wayne-2000-multilingual,0,0.175294,"ly and a mixed text-prosody model to segmentation of TDT English broadcast news, with the best results combining text and prosodic features. (Hirschberg and Nakatani, 1998) also examined automatic topic segmentation based on prosodic cues, in the domain of English broadcast news. Work in discourse analysis (Nakatani et al., 1995; Swerts, 1997) in both English and Dutch has identified features such as changes in pitch range, intensity, and speaking rate associated with segment boundaries and with boundaries of different strengths. 3 Data Set We utilize the Topic Detection and Tracking (TDT) 3 (Wayne, 2000) collection Mandarin Chinese broadcast news audio corpus as our data set. Story segmentation in Mandarin and English broadcast news and newswire text was one of the TDT tasks and also an enabling technology for other retrieval tasks. We use the segment boundaries provided with the corpus as our gold standard labeling. Our collection comprises 3014 stories drawn from approximately 113 hours over three months (OctoberDecember 1998) of news broadcasts from the Voice of America (VOA) in Mandarin Chinese. The transcriptions span approximately 740,000 words. The audio is stored in NIST Sphere format"
W06-0115,I05-3017,0,0.195124,"nts a significant challenge since it is typically written without such separation. Word segmentation has thus long been the focus of significant research because of its role as a necessary pre-processing phase for the tasks above. However, word segmentation remains a significant challenge both for the difficulty of the task itself and because standards for segmentation vary and human segmenters may often disagree. SIGHAN, the Special Interest Group for Chinese Language Processing of the Association for Computational Linguistics, conducted two prior word segmentation bakeoffs, in 2003 and 2005(Emerson, 2005), which established benchmarks for word segmentation against which other systems are judged. The bakeoff presentations at SIGHAN workshops highlighted new approaches in the field as well as the crucial importance of handling out-of-vocabulary (OOV) words. 2 Details of the Evaluation 2.1 Corpora Five corpora were provided for the evaluation: three in Simplified characters and two in traditional characters. The Simplified character corpora were provided by Microsoft Research Asia (MSRA) for WS and NER, by University of Pennsylvania/University of Colorado (UPUC) for WS, and by the Linguistic Data"
W06-0115,1997.eamt-1.1,0,0.0451743,"Missing"
W06-0115,W02-2024,0,0.0446316,"separated form, test data tifying the corpora and tasks of interest. Trainwas created by automatically deleting line-internal ing data was released for download from the webwhitespace. sites (both SIGHAN and LDC) on April 17, 2006. Primary training and truth data for named Test data was released on May 15, 2006 and reentity recognition were converted from the sults were due 14:00 GMT on May 17. Scores for provided XML format to a two-column format all submitted runs were emailed to the individual similar to that used in the CoNLL 2002 NER groups by May 19, and were made available to all task(Sang, 2002) adapted for Chinese, where groups on a web page a few days later. the first column is the current character and Groups could participate in either or both of two the second column the corresponding tag. Fortracks for each task and corpus: mat details may be found at the bakeoff website (http://www.sighan.org/bakeoff2006/). • In the open track, participants could use any For consistency, we tagged only ”<NAMEX>” external data they chose in addition to the mentions, of either (PER)SON, (LOC)ATION, provided training data. Such data could in(ORG)ANIZATION, or (G)EO-(P)OLITICAL 1 clude external le"
W06-0115,W03-1719,0,0.0605486,"Missing"
W06-0115,W03-1726,0,\N,Missing
W06-3810,J98-1004,0,0.209087,"Missing"
W08-0213,W01-1601,0,0.0283319,"Framework 2.1 Motivation Recent research programs in multi-modal environments, including understanding and analysis of multi-party meeting data and oral history recording projects, have created an explosion of multi-modal data sets, including video and audio recordings, transcripts and other annotations, and increased interest in annotation and analysis of such data. A number of systems have been developed to manage and support annotation of multi-modal data, including Annotation Graphs (Bird and Liberman, 2001), Exmeralda (Schmidt, 2004), NITE XML Toolkit (Carletta et al., 2003), Multitool (Allwood et al., 2001), Anvil (Kipp, 2001), and Elan (Wittenburg et al., 2006). The Social Informatics Data Grid (SIDGrid), developed under the NSF Cyberinfrastructure Program, aims to extend the capabilities of such systems by focusing on support for largescale, extensible distributed data annotation, sharing, and analysis. The system is open-source and multi-platform and based on existing open-source software and standards. The system greatly eases the integration of annotation with analysis though userdefined functions both on the client-side for data exploration and on the TeraGrid for large-scale distributed d"
W08-0213,P04-3031,0,0.0389401,"local installation of software, shielding the user from problems due to complex installations, variations in platforms and operating systems, and abstruse command-line syntax. In addition, the web-based archive provides simple mechanisms to browse and download a range of data sources. The students all found the archive, download, and transformation mechanisms easy to use, regardless of prior programming experience. It is important to remember that the goal of this environment is not to replace existing software systems for Natural Language Processing, such the Natural Language Toolkit (NLTK) (Bird and Loper, 2004), but rather to provide a simpler interface to such software tools and to support their application to potentially large data sets, irrespective of the processing power of the individual user’ system. 4.2 Enabling Large-Scale Experimentation A second goal is to enable larger-scale experimentation by both expert and non-expert users. The use of the web-based portal to the TeraGrid provides such opportunities. The portal provides access to highly distributed parallel processing capabilities. For example, in the case of the segmentation of the oral history interviews above, the user can select se"
W08-0213,P04-1088,0,0.0670955,"Missing"
W08-0213,wittenburg-etal-2006-elan,0,0.0171695,"multi-modal environments, including understanding and analysis of multi-party meeting data and oral history recording projects, have created an explosion of multi-modal data sets, including video and audio recordings, transcripts and other annotations, and increased interest in annotation and analysis of such data. A number of systems have been developed to manage and support annotation of multi-modal data, including Annotation Graphs (Bird and Liberman, 2001), Exmeralda (Schmidt, 2004), NITE XML Toolkit (Carletta et al., 2003), Multitool (Allwood et al., 2001), Anvil (Kipp, 2001), and Elan (Wittenburg et al., 2006). The Social Informatics Data Grid (SIDGrid), developed under the NSF Cyberinfrastructure Program, aims to extend the capabilities of such systems by focusing on support for largescale, extensible distributed data annotation, sharing, and analysis. The system is open-source and multi-platform and based on existing open-source software and standards. The system greatly eases the integration of annotation with analysis though userdefined functions both on the client-side for data exploration and on the TeraGrid for large-scale distributed data processing. A web-accessible repository supports dat"
W08-0213,P03-1071,0,\N,Missing
W08-0213,P06-4018,0,\N,Missing
W08-0213,W02-0109,0,\N,Missing
W12-1811,W07-0305,0,0.0193755,"and Cole, 1997) has recently shifted to a commercial footing. Similarly, several industry platforms have provided free non-commercial VoiceXML hosting, as a simple spoken dialog development environment. However, at least one of these systems has recently shifted to a paid-only status. The environment changes rapidly. Of three freely available academic systems and five VoiceXML platforms listed in a 2009 survey (Jokinen and McTear, 2009), two have already gone to paid status as of late 2011. Two frameworks have emerged in recent years as popular SDS frameworks: the Ravenclaw/Olympus framework (Bohus et al., 2007) and VoiceXML, hosted on one of the industrial platforms, such as Nuance’s Cafe or Voxeo1 . However, they do seem to address the needs of different user groups. Ravenclaw/Olympus has been more widely adopted in the research community: it is robust, flexible, extensible, open-source, provides diverse use cases, and has an active support and development community. In contrast, the VoiceXML platforms have proven popular in an instructional setting, as attested by the large number of online homework assignments employing VoiceXML. These VoiceXML frameworks offer very simple, easy-to-use environmen"
W12-1811,H01-1073,0,0.112884,"l needs for spoken dialog systems frameworks to bridge gaps in dialog systems for instructional use. A variety of systems have been developed that address many of these needs, but all suffer from signif21 NAACL-HLT 2012 Workshop on Future directions and needs in the Spoken Dialog Community: Tools and Data, pages 21–22, c Montr´eal, Canada, June 7, 2012. 2012 Association for Computational Linguistics icant limitations. Availability and affordability have posed some of the knottiest problems. For example, many of the Galaxy Communicator research systems, such as those by University of Colorado (Pellom et al., 2001), MIT, and CMU, were made available to the research community. However, many of the systems are no longer available, usable, or supported, as research groups have disbanded and systems architectures have changed. Maintaining systems over time requires group and community commitment, facilitated by an open-source framework. Other toolkits and frameworks have become problematic due to conflicts between availability and affordability. The long-popular CSLU toolkit (Sutton and Cole, 1997) has recently shifted to a commercial footing. Similarly, several industry platforms have provided free non-com"
W17-0106,W10-2211,0,0.0319097,"systems will pick this information up from the training data. 4 45 able in need of normalization than there is material that establishes correct practices. On the other hand, there are fewer individual authors, meaning that author identification can potentially lead to greater gains, and supplementary material like audio is likely to be available for at least some of the texts (because much endangered language text is transcribed from audio recordings). The first-pass IGT production shared task resembles earlier shared tasks on morphological analysis, most notably the Morpho Challenge series (Kurimo et al., 2010). It differs, however, in working with words in context (rather than word lists), and in going beyond segmentation of words into morphemes to associating morphemes with particular glosses. The presence of the translation line also provides a new source of information in producing the glosses, not available in previous shared tasks. Finally, the task of producing word-glosses is a novel one, with connections to low-resource machine translation. texts (both old and new) to a consistent format can solve many practical problems communities face. 5 Of the outputs provided by the first-pass IGT prod"
W17-0106,W14-3605,0,0.0207445,"rce scenarios. These contrast with the typical simulated low-resource scenarios in that the latter involve decisions about which data to keep, and this might not be representative of what an actual low-resource situation might be like. Each task has additional inherent research interest of its own, as detailed below. The “Grandma’s hatbox” shared task suite spans a range of speech processing technologies, including language identification, speaker identiThe orthographic regularization shared task builds on other work on orthographic regularization in widely spoken languages (see, for example (Mohit et al., 2014; Rozovskaya et al., 2015; Baldwin et al., 2015) on social media text and Dale and Kilgariff (2011) on text produced by language learners), but pushes the frontiers of work in this area in several ways: While this proposed shared task has much in common with these previous shared tasks, endangered language text normalization poses additional interesting problems. In languages like English or Arabic, there is usually a single, established orthography in which almost all users have formal schooling and extensive digital corpora in this orthography that establish “correct” practices. Endangered l"
W17-0106,W15-4319,0,0.0611856,"Missing"
W17-0106,W06-1421,0,0.386565,"l@alumni.ubc.ca, Shobhana.Chelliah@unt.edu, maxwell@umiacs.umd.edu Abstract ties if we are to actually reap the potential benefits of current research in the former for the latter. We propose that a particularly efficient and effective way to achieve this alignment of interest is through a set of “Shared Task Evaluation Challenges” (STECs) for the speech and language processing communities based on data already collected and annotated in language documentation efforts. STECs have been a primary driver of progress in natural language processing (NLP) and speech technology over several decades (Belz and Kilgarriff, 2006). A STEC involves standardized data for training (or otherwise developing) NLP/speech systems and then a held-out, also standardized, set of test data as well as implemented evaluation metrics for evaluating the systems submitted by the participating groups. This system is productive because the groups developing the algorithms benefit from independently curated data sets to test their systems on as well as independent evaluation of the systems, while the organizers of the shared task are able to focus effort on questions of interest to them without directly funding system development. Organiz"
W17-0106,brugman-russel-2004-annotating,0,0.0478615,"l employ two evaluation metrics: F1 scores averaged across all time segments in the corpus and all speakers in the corpus, respectively. As above, in the case of overlapped speech, both (all) speakers should be labeled. 3. Genre classification For a span of audio, identify which of a fixed inventory of genres, e.g., elicitation, monolog, LRL dialog, reading, or chanting, is present. This inventory will be provided by the organizers along with the training data. Training and test samples will be provided 42 as well as alignment can readily be encoded in formats readable by tools such as ELAN2 (Brugman and Russel, 2004), which has seen increasing adoption for endangered language research and which provides an easy visual interface to timealigned speech and language annotations. The techniques developed through the “Grandma’s hatbox” ensemble of shared tasks have potential benefit for field linguists, researchers in endangered languages, and language archivists. For those collecting data, these techniques can accelerate the process of transcription and alignment of speech data. They can also facilitate consistent metadata extraction from recordings, including language and speaker information, recording dates"
W17-0106,W15-3204,0,0.0147756,"contrast with the typical simulated low-resource scenarios in that the latter involve decisions about which data to keep, and this might not be representative of what an actual low-resource situation might be like. Each task has additional inherent research interest of its own, as detailed below. The “Grandma’s hatbox” shared task suite spans a range of speech processing technologies, including language identification, speaker identiThe orthographic regularization shared task builds on other work on orthographic regularization in widely spoken languages (see, for example (Mohit et al., 2014; Rozovskaya et al., 2015; Baldwin et al., 2015) on social media text and Dale and Kilgariff (2011) on text produced by language learners), but pushes the frontiers of work in this area in several ways: While this proposed shared task has much in common with these previous shared tasks, endangered language text normalization poses additional interesting problems. In languages like English or Arabic, there is usually a single, established orthography in which almost all users have formal schooling and extensive digital corpora in this orthography that establish “correct” practices. Endangered languages often only have"
W17-0106,W11-2838,0,0.0881554,"Missing"
W17-0106,E09-1099,1,0.689325,"hared tasks: Realism Whereas shared tasks in speech and NLP are often somewhat artificial, it is critical to our goals that our shared tasks closely model the actual computational needs of working linguists. It directly follows from this design principle that the software contributed by shared task participants should work off-the-shelf for stakeholders in documentary materials who are interested in using it later (e.g., linguists, speaker Accessibility of the shared task The shared tasks must have relatively low barriers to entry, in 1 There are some notable exceptions, including Xia et al’s (2009) work on language identification in IGT harvested from linguistics papers on the web (Xia et al., 2009), the Zero Resource Speech Challenge (Versteegh et al., 2015), and the recent BABEL (Harper, 2014) program. 40 now turn to the explanation of our three proposed shared tasks. order to encourage broad participation. This can be ensured by having the shared task organizers provide baseline systems which provide working, if not necessarily high-performing, solutions to the task. The baseline systems not only establish the basic feasibility of the tasks, but also provide a starting point for team"
W18-1206,P16-1225,0,0.0942725,"Missing"
W18-1206,N16-1038,0,0.0678252,"Missing"
W18-1206,D14-1162,0,0.101227,"ure vectors that encode the submorphemes occurring in a surface form. We use sparse regularization to select relevant features from this model, which enables it to automatically choose a subset of the submorpheme features that predict the vectors (our predicted phonesthemes). Specifically, we regularize our linear regression model with the elastic net (Zou and Hastie, 2005). We used scikit-learn (Pedregosa et al., 2011) to train our models, and we tune the L1 and L2 regularization strengths on held-out error in 5-fold cross-validation. 3 Data For our experiments, we use 300-dimensional GloVe (Pennington et al., 2014) English word embeddings trained on the cased Common Crawl. Many of the terms in the set of pretrained vectors are not English words. As a first attempt toward removing non-English words and named entities, we discard types that are not alphabetical or not completely lowercased. In addition, it’s unlikely that rare words or very common words will contribute to the formation of sound-meaning associations (Hutchins, 1998). To further filter these rare or common words (and remove additional nonEnglish types), we remove types that either occur less than 1000 times in the Gigaword corpus or in more"
W99-0405,P98-1032,0,0.206954,"cooperative listener; however, they result in an over-optimistic assessment of the speaker&apos;s actual proficiency. We arguably do not wish to build this sort of cooperation into an automated Introduction Computer-mediated language assessment appeals to educators and language evaluators because it has the potential for making language assessment widely available with minimal human effort and limited expense. Fairly robust results (n &apos;~ 0.8) have been achieved in the commercial domain modeling the human rater results, with both the Electronic Essay Rater (erater) system for written essay scoring (Burstein et al., 1998), and the PhonePass pronunciation assessment (Ordinate, 1998). There are at least three reasons why it is not possible to model the human rating process. First, there is a mismatch between what the technology is able to handle and what people manipulate, especially in the assessment of speech features. Second, we lack a wellarticulated model of the human process, often characterized as holistic. Certain assessment features have been identified, but their rela1 24 assessment system, though it is likely desirable for other sorts of human-computer interaction systems. Furthermore, if we focus on"
W99-0405,W95-0102,0,0.0265511,"Missing"
W99-0405,1993.tmi-1.14,0,0.0302336,"e student may be asked to count to twenty in the target language or to enumerate the items in a pictured context, such as a classroom scene. Within these tasks one can test for the presence and number of specific desired vocabulary items, yielding another measure of lexical knowledge. Simple measures with numerical values include number of words in the speech sample and number of distinct words. In addition, examinees at this level frequently rely on vocabulary items from English in their answers. A deeper type of knowledge may be captured by the lexicon in Lexical Conceptual Structure (LCS) (Dorr, 1993b; Dorr, 1993a; Jackendoff, 1983). The LCS is an interlingual framework for representing semantic elements that have syntactic reflexes3 LCSs have been ported from English into a variety of languages, including Spanish, requiring a minimum of adaptation in even unrelated languages (e.g. Chinese (Olsen et al., 1998)). The representation indicates the argument-taking properties of verbs (hit requires an object; smile does not), selectional constraints (the subject of fear and the object of frighten are animate), thematic information of arguments (the subject of frighten is an agent; the object i"
W99-0405,P92-1017,0,0.0174522,"Missing"
W99-0405,olsen-etal-1998-enhancing,1,\N,Missing
W99-0405,C98-1032,0,\N,Missing
