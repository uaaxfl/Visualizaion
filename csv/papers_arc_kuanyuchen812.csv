2021.rocling-1.5,A Flexible and Extensible Framework for Multiple Answer Modes Question Answering,2021,-1,-1,13,0,2300,chengchung fan,Proceedings of the 33rd Conference on Computational Linguistics and Speech Processing (ROCLING 2021),0,"This paper presents a framework to answer the questions that require various kinds of inference mechanisms (such as Extraction, Entailment-Judgement, and Summarization). Most of the previous approaches adopt a rigid framework which handles only one inference mechanism. Only a few of them adopt several answer generation modules for providing different mechanisms; however, they either lack an aggregation mechanism to merge the answers from various modules, or are too complicated to be implemented with neural networks. To alleviate the problems mentioned above, we propose a divide-and-conquer framework, which consists of a set of various answer generation modules, a dispatch module, and an aggregation module. The answer generation modules are designed to provide different inference mechanisms, the dispatch module is used to select a few appropriate answer generation modules to generate answer candidates, and the aggregation module is employed to select the final answer. We test our framework on the 2020 Formosa Grand Challenge Contest dataset. Experiments show that the proposed framework outperforms the state-of-the-art Roberta-large model by about 11.4{\%}."
2021.rocling-1.22,A {BERT}-based {S}iamese-structured Retrieval Model,2021,-1,-1,2,0,2361,hungyun chiang,Proceedings of the 33rd Conference on Computational Linguistics and Speech Processing (ROCLING 2021),0,"Due to the development of deep learning, the natural language processing tasks have made great progresses by leveraging the bidirectional encoder representations from Transformers (BERT). The goal of information retrieval is to search the most relevant results for the user{'}s query from a large set of documents. Although BERT-based retrieval models have shown excellent results in many studies, these models usually suffer from the need for large amounts of computations and/or additional storage spaces. In view of the flaws, a BERT-based Siamese-structured retrieval model (BESS) is proposed in this paper. BESS not only inherits the merits of pre-trained language models, but also can generate extra information to compensate the original query automatically. Besides, the reinforcement learning strategy is introduced to make the model more robust. Accordingly, we evaluate BESS on three public-available corpora, and the experimental results demonstrate the efficiency of the proposed retrieval model."
2021.rocling-1.46,ntust-nlp-1 at {ROCLING}-2021 Shared Task: Educational Texts Dimensional Sentiment Analysis using Pretrained Language Models,2021,-1,-1,6,0,2429,yiwei wang,Proceedings of the 33rd Conference on Computational Linguistics and Speech Processing (ROCLING 2021),0,"This technical report aims at the ROCLING 2021 Shared Task: Dimensional Sentiment Analysis for Educational Texts. In order to predict the affective states of Chinese educational texts, we present a practical framework by employing pre-trained language models, such as BERT and MacBERT. Several valuable observations and analyses can be drawn from a series of experiments. From the results, we find that MacBERT-based methods can deliver better results than BERT-based methods on the verification set. Therefore, we average the prediction results of several models obtained using different settings as the final output."
2021.rocling-1.47,ntust-nlp-2 at {ROCLING}-2021 Shared Task: {BERT}-based semantic analyzer with word-level information,2021,-1,-1,2,0,2434,kehan lu,Proceedings of the 33rd Conference on Computational Linguistics and Speech Processing (ROCLING 2021),0,"In this paper, we proposed a BERT-based dimensional semantic analyzer, which is designed by incorporating with word-level information. Our model achieved three of the best results in four metrics on {``}ROCLING 2021 Shared Task: Dimensional Sentiment Analysis for Educational Texts{''}. We conducted a series of experiments to compare the effectiveness of different pre-trained methods. Besides, the results also proofed that our method can significantly improve the performances than classic methods. Based on the experiments, we also discussed the impact of model architectures and datasets."
2020.rocling-1.8,A Preliminary Study on Using Meta-learning Technique for Information Retrieval,2020,-1,-1,2,0,15580,chongen lin,Proceedings of the 32nd Conference on Computational Linguistics and Speech Processing (ROCLING 2020),0,None
2020.rocling-1.16,A Preliminary Study on Leveraging Meta Learning Technique for Code-switching Speech Recognition,2020,-1,-1,2,0,15597,fuhao yu,Proceedings of the 32nd Conference on Computational Linguistics and Speech Processing (ROCLING 2020),0,None
2019.rocling-1.1,åºæ¼ç¹å¾µç²åº¦ä¹è¨ç·´ç­ç¥æ¼ä¸­æå£èªåç­ç³»çµ±ä¹æç¨(A Feature-granularity Training Strategy for {C}hinese Spoken Question Answering),2019,-1,-1,2,1,2302,shangbao luo,Proceedings of the 31st Conference on Computational Linguistics and Speech Processing (ROCLING 2019),0,None
2019.rocling-1.2,æ°ç©çåºåçææ¶æ§æ¼ä¸­æéå¯«å¼æè¦ä¹ç ç©¶(Novel Sequence Generation Framework for {C}hinese Abstractive Summarization),2019,-1,-1,2,0,27199,chinyueh chien,Proceedings of the 31st Conference on Computational Linguistics and Speech Processing (ROCLING 2019),0,None
2019.rocling-1.3,{EBSUM}: åºæ¼{BERT} çå¼·å¥æ§æ½åå¼æè¦æ³({EBSUM}: An Enhanced {BERT}-based Extractive Summarization Framework),2019,-1,-1,2,0,27200,zhengyu wu,Proceedings of the 31st Conference on Computational Linguistics and Speech Processing (ROCLING 2019),0,None
2019.rocling-1.4,{GAL}s: åºæ¼å°æå¼å­¸ç¿ä¹æ´åå¼æè¦æ³ ({GAL}s: A {GAN}-based Listwise Summarizer),2019,-1,-1,2,0,2301,chiachih kuo,Proceedings of the 31st Conference on Computational Linguistics and Speech Processing (ROCLING 2019),0,None
2019.ijclclp-2.1,åºæ¼ç¹å¾µç²åº¦ä¹è¨ç·´ç­ç¥æ¼ä¸­æå£èªåç­ç³»çµ±ä¹æç¨ (A Feature-granularity Training Strategy for {C}hinese Spoken Question Answering),2019,-1,-1,2,1,2302,shangbao luo,"International Journal of Computational Linguistics {\\&} {C}hinese Language Processing, Volume 24, Number 2, December 2019",0,None
2019.ijclclp-2.2,{EBSUM}: åºæ¼ {BERT} çå¼·å¥æ§æ½åå¼æè¦æ³ ({EBSUM}: An Enhanced {BERT}-based Extractive Summarization Framework),2019,-1,-1,2,0,27200,zhengyu wu,"International Journal of Computational Linguistics {\\&} {C}hinese Language Processing, Volume 24, Number 2, December 2019",0,None
O18-1014,æªç»éè©ä¹åéè¡¨ç¤ºæ³æ¨¡åæ¼ä¸­ææ©å¨é±è®çè§£ä¹æç¨ (An {OOV} Word Embedding Framework for {C}hinese Machine Reading Comprehension) [In {C}hinese],2018,0,0,3,1,2302,shangbao luo,Proceedings of the 30th Conference on Computational Linguistics and Speech Processing ({ROCLING} 2018),0,None
2018.ijclclp-2.5,æªç»éè©ä¹åéè¡¨ç¤ºæ³æ¨¡åæ¼ä¸­ææ©å¨é±è®çè§£ä¹æç¨ (An {OOV} Word Embedding Framework for {C}hinese Machine Reading Comprehension),2018,0,0,4,1,2302,shangbao luo,"International Journal of Computational Linguistics {\\&} {C}hinese Language Processing, Volume 23, Number 2, December 2018",0,None
O17-3002,èªé³æä»¶æª¢ç´¢ä½¿ç¨é¡ç¥ç¶ç¶²è·¯æè¡ (On the Use of Neural Network Modeling Techniques for Spoken Document Retrieval) [In {C}hinese],2017,0,0,3,0,2341,tienhong lo,"International Journal of Computational Linguistics {\\&} {C}hinese Language Processing, Volume 22, Number 2, {D}ecember 2017-Special Issue on Selected Papers from {ROCLING} {XXIX}",0,None
O17-2001,ç¶ä»£éç£ç£å¼æ¹æ³ä¹æ¯è¼æ¼ç¯éå¼èªé³æè¦ (An Empirical Comparison of Contemporary Unsupervised Approaches for Extractive Speech Summarization) [In {C}hinese],2017,0,0,2,0.58699,19036,shihhung liu,"International Journal of Computational Linguistics {\\&} {C}hinese Language Processing, Volume 22, Number 1, June 2017",0,None
O17-1015,ä½¿ç¨æ¥è©¢æåæ¢ç´¢èé¡ç¥ç¶ç¶²è·¯æ¼èªé³æä»¶æª¢ç´¢ä¹ç ç©¶ (Exploring Query Intent and Neural Network modeling Techniques for Spoken Document Retrieval) [In {C}hinese],2017,0,0,4,0,2341,tienhong lo,Proceedings of the 29th Conference on Computational Linguistics and Speech Processing ({ROCLING} 2017),0,None
O16-3004,è©ä¼°å°ºåº¦ç¸éæä½³åæ¹æ³æ¼è¯èªé¯èª¤ç¼é³æª¢æ¸¬ä¹ç ç©¶ (Evaluation Metric-related Optimization Methods for {M}andarin Mispronunciation Detection) [In {C}hinese],2016,0,0,5,0,34556,yaochi hsu,"International Journal of Computational Linguistics {\\&} {C}hinese Language Processing, Volume 21, Number 2, {D}ecember 2016",0,None
O16-3006,èåå¤ä»»åå­¸ç¿é¡ç¥ç¶ç¶²è·¯è²å­¸æ¨¡åè¨ç·´æ¼æè­°èªé³è¾¨è­ä¹ç ç©¶ (Leveraging Multi-Task Learning with Neural Network Based Acoustic Modeling for Improved Meeting Speech Recognition) [In {C}hinese],2016,0,0,5,0,34557,minghan yang,"International Journal of Computational Linguistics {\\&} {C}hinese Language Processing, Volume 21, Number 2, {D}ecember 2016",0,None
O16-1002,èåå¤ä»»åå­¸ç¿é¡ç¥ç¶ç¶²è·¯è²å­¸æ¨¡åè¨ç·´æ¼æè­°èªé³è¾¨è­ä¹ç ç©¶(Leveraging Multi-task Learning with Neural Network Based Acoustic Modeling for Improved Meeting Speech Recognition) [In {C}hinese],2016,0,0,6,0,34557,minghan yang,Proceedings of the 28th Conference on Computational Linguistics and Speech Processing ({ROCLING} 2016),0,None
O16-1012,éç¨åºåå°åºåçææ¶æ§æ¼éå¯«å¼èªåæè¦(Exploiting Sequence-to-Sequence Generation Framework for Automatic Abstractive Summarization)[In {C}hinese],2016,0,0,3,0,25636,yulun hsieh,Proceedings of the 28th Conference on Computational Linguistics and Speech Processing ({ROCLING} 2016),0,None
C16-1035,Learning to Distill: The Essence Vector Modeling Framework,2016,25,5,1,1,2312,kuanyu chen,"Proceedings of {COLING} 2016, the 26th International Conference on Computational Linguistics: Technical Papers",0,"In the context of natural language processing, representation learning has emerged as a newly active research subject because of its excellent performance in many applications. Learning representations of words is a pioneering study in this school of research. However, paragraph (or sentence and document) embedding learning is more suitable/reasonable for some tasks, such as sentiment classification and document summarization. Nevertheless, as far as we are aware, there is only a dearth of research focusing on launching unsupervised paragraph embedding methods. Classic paragraph embedding methods infer the representation of a given paragraph by considering all of the words occurring in the paragraph. Consequently, those stop or function words that occur frequently may mislead the embedding learning process to produce a misty paragraph representation. Motivated by these observations, our major contributions are twofold. First, we propose a novel unsupervised paragraph embedding method, named the essence vector (EV) model, which aims at not only distilling the most representative information from a paragraph but also excluding the general background information to produce a more informative low-dimensional vector representation for the paragraph. We evaluate the proposed EV model on benchmark sentiment classification and multi-document summarization tasks. The experimental results demonstrate the effectiveness and applicability of the proposed embedding method. Second, in view of the increasing importance of spoken content processing, an extension of the EV model, named the denoising essence vector (D-EV) model, is proposed. The D-EV model not only inherits the advantages of the EV model but also can infer a more robust representation for a given spoken paragraph against imperfect speech recognition. The utility of the D-EV model is evaluated on a spoken document summarization task, confirming the effectiveness of the proposed embedding method in relation to several well-practiced and state-of-the-art summarization methods."
O15-3004,ç¯éå¼èªé³æä»¶æè¦ä½¿ç¨è¡¨ç¤ºæ³å­¸ç¿æè¡ (Extractive Spoken Document Summarization with Representation Learning Techniques) [In {C}hinese],2015,0,0,2,0,32687,kaiwun shih,"International Journal of Computational Linguistics {\\&} {C}hinese Language Processing, Volume 20, Number 2, {D}ecember 2015 - Special Issue on Selected Papers from {ROCLING} {XXVII}",0,None
O15-3005,èª¿è®é »è­åè§£æè¡æ¼å¼·å¥èªé³è¾¨è­ä¹ç ç©¶ (Investigating Modulation Spectrum Factorization Techniques for Robust Speech Recognition) [In {C}hinese],2015,0,0,3,0,37547,tinghao chang,"International Journal of Computational Linguistics {\\&} {C}hinese Language Processing, Volume 20, Number 2, {D}ecember 2015 - Special Issue on Selected Papers from {ROCLING} {XXVII}",0,None
O15-1001,è¡¨ç¤ºæ³å­¸ç¿æè¡æ¼ç¯éå¼èªé³æä»¶æè¦ä¹ç ç©¶(A Study on Representation Learning Techniques for Extractive Spoken Document Summarization) [In {C}hinese],2015,0,0,3,0,32687,kaiwun shih,Proceedings of the 27th Conference on Computational Linguistics and Speech Processing ({ROCLING} 2015),0,None
O15-1002,ä½¿ç¨è©åéè¡¨ç¤ºèæ¦å¿µè³è¨æ¼ä¸­æå¤§è©å½é£çºèªé³è¾¨è­ä¹èªè¨æ¨¡åèª¿é©(Exploring Word Embedding and Concept Information for Language Model Adaptation in {M}andarin Large Vocabulary Continuous Speech Recognition) [In {C}hinese],2015,0,0,2,0,37556,ssucheng chen,Proceedings of the 27th Conference on Computational Linguistics and Speech Processing ({ROCLING} 2015),0,None
O15-1008,å¯è®æ§é æ¸¬æ¼ä¸­å°å­¸åèªææç§æ¸ååªè¯èª²å¤è®ç©ä¹ç ç©¶(A Study of Readability Prediction on Elementary and Secondary {C}hinese Textbooks and Excellent Extracurricular Reading Materials) [In {C}hinese],2015,0,0,2,0,37566,yinian liu,Proceedings of the 27th Conference on Computational Linguistics and Speech Processing ({ROCLING} 2015),0,None
O15-1010,èª¿è®é »è­åè§£ä¹æ¹è¯æ¼å¼·å¥æ§èªé³è¾¨è­(Several Refinements of Modulation Spectrum Factorization for Robust Speech Recognition) [In {C}hinese],2015,0,0,3,0,37547,tinghao chang,Proceedings of the 27th Conference on Computational Linguistics and Speech Processing ({ROCLING} 2015),0,None
O14-1002,æ¢ç©¶æ°ç©èªå¥æ¨¡ååæè¡æ¼ç¯éå¼èªé³æè¦ (Investigating Novel Sentence Modeling Techniques for Extractive Speech Summarization) [In {C}hinese],2014,0,0,2,0.949107,19036,shihhung liu,Proceedings of the 26th Conference on Computational Linguistics and Speech Processing ({ROCLING} 2014),0,None
D14-1156,Leveraging Effective Query Modeling Techniques for Speech Recognition and Summarization,2014,47,7,1,1,2312,kuanyu chen,Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP}),0,"Statistical language modeling (LM) that purports to quantify the acceptability of a given piece of text has long been an interesting yet challenging research area. In particular, language modeling for information retrieval (IR) has enjoyed remarkable empirical success; one emerging stream of the LM approach for IR is to employ the pseudo-relevance feedback process to enhance the representation of an input query so as to improve retrieval effectiveness. This paper presents a continuation of such a general line of research and the main contribution is threefold. First, we propose a principled framework which can unify the relationships among several widely-used query modeling formulations. Second, on top of the successfully developed framework, we propose an extended query modeling formulation by incorporating critical query-specific information cues to guide the model estimation. Third, we further adopt and formalize such a framework to the speech recognition and summarization tasks. A series of empirical experiments reveal the feasibility of such an LM framework and the performance merits of the deduced models on these two tasks."
W13-4414,A Study of Language Modeling for {C}hinese Spelling Check,2013,25,10,1,1,2312,kuanyu chen,Proceedings of the Seventh {SIGHAN} Workshop on {C}hinese Language Processing,0,"Chinese spelling check (CSC) is still an open problem today. To the best of our knowledge, language modeling is widely used in CSC because of its simplicity and fair predictive power, but most systems only use the conventional n-gram models. Our work in this paper continues this general line of research by further exploring different ways to glean extra semantic clues and Web resources to enhance the CSC performance in an unsupervised fashion. Empirical results demonstrate the utility of our CSC system."
O13-1001,æ¹è¯èªå¥æ¨¡åæè¡æ¼ç¯éå¼èªé³æè¦ä¹ç ç©¶ (Improved Sentence Modeling Techniques for Extractive Speech Summarization) [In {C}hinese],2013,0,0,2,0.949107,19036,shihhung liu,Proceedings of the 25th Conference on Computational Linguistics and Speech Processing ({ROCLING} 2013),0,None
I13-1158,"Semantic Na{\\\\\i}ve {B}ayes Classifier for Document Classification""",2013,21,7,3,0,41718,how jing,Proceedings of the Sixth International Joint Conference on Natural Language Processing,0,"In this paper, we propose a semantic naive Bayes classifier (SNBC) to improve the conventional naive Bayes classifier (NBC) by incorporating xe2x80x9cdocument-levelxe2x80x9d semantic information for document classification (DC). To capture the semantic information from each document, we develop semantic feature extraction and modeling algorithms. For semantic feature extraction, we first apply a log-Bilinear document modeling (LBDM) algorithm to transform each word into a semantic vector, and then apply principal component analysis (PCA) to the semantic space formed by the word vectors to extract a set of semantic features for each document. For semantic modeling, a semantic model is constructed using the semantic features of the training documents. In the testing phase, SNBC systematically integrates the semantic model and the conventional NBC to perform DC. The results of experiments on the 20 News-groups and WebKB datasets confirm that, with the semantic score, SNBC consistently outperforms NBC with various language modeling approaches."
O11-1001,å¯¦è­æ¢ç©¶å¤ç¨®éå¥å¼èªè¨æ¨¡åæ¼èªé³è¾¨è­ä¹ç ç©¶ (Empirical Comparisons of Various Discriminative Language Models for Speech Recognition) [In {C}hinese],2011,27,1,3,0,44725,minhsuan lai,Proceedings of the 23rd Conference on Computational Linguistics and Speech Processing ({ROCLING} 2011),0,None
O09-1014,ä¸»é¡èªè¨æ¨¡åæ¼å¤§è©å½é£çºèªé³è¾¨è­ä¹ç ç©¶ (On the Use of Topic Models for Large-Vocabulary Continuous Speech Recognition) [In {C}hinese],2009,0,0,1,1,2312,kuanyu chen,Proceedings of the 21st Conference on Computational Linguistics and Speech Processing,0,None
