2020.acl-main.102,D19-1431,0,0.0677503,"Missing"
2020.acl-main.102,P17-2057,0,0.0612691,"TVabaO5zqzYn/LPdE51d3G9PdNrohYiRti//LNlP/1qVokBjjVNQRUU6oZVR0zWXLdFXVz+0tVkjKkxCncp3hGmGnnrM+29ghdu+qtp+NvWqlYtWdGm+Nd3ZIG7P4c5zxoHlVdp+peHldqjhl1EXvYxyHN8wQ1nKOOhp7jI57wbF1Ywrqz7j+lVsF4dvFtWQ8fKeySgg==</latexit> Base Batch Pretrained Encoder 3.1 Dynamic Memory Induction Network Wbase Base Set Support Set Meta Learning 3 Supervised Learning 2019; Hu et al., 2019), including text classification (Yu et al., 2018; Rios and Kavuluru, 2018; Xu et al., 2019; Geng et al., 2019; Gao et al., 2019; Ye and Ling, 2019). Memory mechanism has shown to be very effective in many NLP tasks (Tang et al., 2016; Das et al., 2017; Madotto et al., 2018). In the fewshot learning scenario, researchers have applied memory networks to store the encoded contextual information in each meta episode (Santoro et al., 2016; Cai et al., 2018; Kaiser et al., 2017). Specifically Qi et al. (2018) and Gidaris and Komodakis (2018) build a two-stage training procedure and regard the supervisely learned class representation as a memory component. Sample Vector Class Vector Classification Score Query Query Vector Figure 1: An overview of Dynamic Memory Induction Network with a 3-way 2-shot example. rectional Transformer encoder based on"
2020.acl-main.102,N19-1423,0,0.272796,"inn et al., 2017; Vinyals et al., 2016; Snell et al., 2017; Sung et al., 2018; Allen et al., 2019). Researchers have also investigated few-shot learning in various NLP tasks (Dou et al., 2019; Wu et al., 2019; Gu et al., 2018; Chen et al., 2019; Obamuyide and Vlachos, 1087 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1087–1094 c July 5 - 10, 2020. 2020 Association for Computational Linguistics Pre-trained Encoder We expect that developing few-shot text classifier should benefit from the recent advance on pretrained models (Peters et al., 2018; Devlin et al., 2019; Radford et al.). Unlike recent work (Geng et al., 2019), we employ BERT-base (Devlin et al., 2019) for sentence encoding , which has been used in recent few-shot learning models (Bao et al., 2019; Soares et al., 2019). The model architecture of BERT (Devlin et al., 2019) is a multi-layer bidiQuery Set Classifier 3.2 Class 3 Query-enhanced Induction Module An overview of our Dynamic Memory Induction Networks (DMIN) is shown in Figure 1, which is built on the two-stage few-shot framework Gidaris and Komodakis (2018). In the supervised learning stage (upper, green subfigure), a subset of classe"
2020.acl-main.102,D19-1403,1,0.814385,"., 2019), in which key information is often lost when switching between meta-tasks. Recent solutions (Gidaris and Komodakis, 2018) leverage a memory component to maintain models’ learning experience, e.g., by finding from a supervised stage the content that is similar to the unseen classes, leading to the state-of-the-art performance. However, the memory weights are static ∗ Corresponding author. during inference and the capability of the model is still limited when adapted to new classes. Another prominent challenge is the instance-level diversity caused by various reasons (Gao et al., 2019; Geng et al., 2019), resulting in the difficulty of finding a fixed prototype for a class (Allen et al., 2019). Recent research has shown that models can benefit from query-aware methods (Gao et al., 2019). In this paper we propose Dynamic Memory Induction Networks (DMIN) to further tackle the above challenges. DMIN utilizes dynamic routing (Sabour et al., 2017; Geng et al., 2019) to render more flexibility to memory-based few-shot learning (Gidaris and Komodakis, 2018) in order to better adapt the support sets, by leveraging the routing component’s capacity in automatically adjusting the coupling coefficients d"
2020.acl-main.102,D18-1398,0,0.0464872,"Missing"
2020.acl-main.102,P19-1402,0,0.0179397,"X/oBb/S/xD/QvvDNOQS2iE5KcOfecO3Pv9dMwENJxXgvWwuLS8kpxtbS2vrG5Vd7eaYokzxhvsCRMsrbvCR4GMW/IQIa8nWbci/yQt/zRmYq3bnkmgiS+kuOUdyNvGAeDgHmSqHarN1Huaa9ccaqOXvY8cA2owKx6Un7BNfpIwJAjAkcMSTiEB0FPBy4cpMR1MSEuIxToOMcUJfLmpOKk8Igd0XdIu45hY9qrnEK7GZ0S0puR08YBeRLSZYTVabaO5zqzYn/LPdE51d3G9PdNrohYiRti//LNlP/1qVokBjjVNQRUU6oZVR0zWXLdFXVz+0tVkjKkxCncp3hGmGnnrM+29ghdu+qtp+NvWqlYtWdGm+Nd3ZIG7P4c5zxoHlVdp+peHldqjhl1EXvYxyHN8wQ1nKOOhp7jI57wbF1Ywrqz7j+lVsF4dvFtWQ8fKeySgg==</latexit> Base Batch Pretrained Encoder 3.1 Dynamic Memory Induction Network Wbase Base Set Support Set Meta Learning 3 Supervised Learning 2019; Hu et al., 2019), including text classification (Yu et al., 2018; Rios and Kavuluru, 2018; Xu et al., 2019; Geng et al., 2019; Gao et al., 2019; Ye and Ling, 2019). Memory mechanism has shown to be very effective in many NLP tasks (Tang et al., 2016; Das et al., 2017; Madotto et al., 2018). In the fewshot learning scenario, researchers have applied memory networks to store the encoded contextual information in each meta episode (Santoro et al., 2016; Cai et al., 2018; Kaiser et al., 2017). Specifically Qi et al. (2018) and Gidaris and Komodakis (2018) build a two-stage training procedure and regard the superv"
2020.acl-main.102,P18-1136,0,0.0417247,"1d3G9PdNrohYiRti//LNlP/1qVokBjjVNQRUU6oZVR0zWXLdFXVz+0tVkjKkxCncp3hGmGnnrM+29ghdu+qtp+NvWqlYtWdGm+Nd3ZIG7P4c5zxoHlVdp+peHldqjhl1EXvYxyHN8wQ1nKOOhp7jI57wbF1Ywrqz7j+lVsF4dvFtWQ8fKeySgg==</latexit> Base Batch Pretrained Encoder 3.1 Dynamic Memory Induction Network Wbase Base Set Support Set Meta Learning 3 Supervised Learning 2019; Hu et al., 2019), including text classification (Yu et al., 2018; Rios and Kavuluru, 2018; Xu et al., 2019; Geng et al., 2019; Gao et al., 2019; Ye and Ling, 2019). Memory mechanism has shown to be very effective in many NLP tasks (Tang et al., 2016; Das et al., 2017; Madotto et al., 2018). In the fewshot learning scenario, researchers have applied memory networks to store the encoded contextual information in each meta episode (Santoro et al., 2016; Cai et al., 2018; Kaiser et al., 2017). Specifically Qi et al. (2018) and Gidaris and Komodakis (2018) build a two-stage training procedure and regard the supervisely learned class representation as a memory component. Sample Vector Class Vector Classification Score Query Query Vector Figure 1: An overview of Dynamic Memory Induction Network with a 3-way 2-shot example. rectional Transformer encoder based on the original Transforme"
2020.acl-main.102,P19-1589,0,0.126494,"Missing"
2020.acl-main.102,N18-1202,0,0.0311914,"ishra et al., 2017; Finn et al., 2017; Vinyals et al., 2016; Snell et al., 2017; Sung et al., 2018; Allen et al., 2019). Researchers have also investigated few-shot learning in various NLP tasks (Dou et al., 2019; Wu et al., 2019; Gu et al., 2018; Chen et al., 2019; Obamuyide and Vlachos, 1087 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1087–1094 c July 5 - 10, 2020. 2020 Association for Computational Linguistics Pre-trained Encoder We expect that developing few-shot text classifier should benefit from the recent advance on pretrained models (Peters et al., 2018; Devlin et al., 2019; Radford et al.). Unlike recent work (Geng et al., 2019), we employ BERT-base (Devlin et al., 2019) for sentence encoding , which has been used in recent few-shot learning models (Bao et al., 2019; Soares et al., 2019). The model architecture of BERT (Devlin et al., 2019) is a multi-layer bidiQuery Set Classifier 3.2 Class 3 Query-enhanced Induction Module An overview of our Dynamic Memory Induction Networks (DMIN) is shown in Figure 1, which is built on the two-stage few-shot framework Gidaris and Komodakis (2018). In the supervised learning stage (upper, green subfigure"
2020.acl-main.102,D18-1352,0,0.0307115,"YokzxhvsCRMsrbvCR4GMW/IQIa8nWbci/yQt/zRmYq3bnkmgiS+kuOUdyNvGAeDgHmSqHarN1Huaa9ccaqOXvY8cA2owKx6Un7BNfpIwJAjAkcMSTiEB0FPBy4cpMR1MSEuIxToOMcUJfLmpOKk8Igd0XdIu45hY9qrnEK7GZ0S0puR08YBeRLSZYTVabaO5zqzYn/LPdE51d3G9PdNrohYiRti//LNlP/1qVokBjjVNQRUU6oZVR0zWXLdFXVz+0tVkjKkxCncp3hGmGnnrM+29ghdu+qtp+NvWqlYtWdGm+Nd3ZIG7P4c5zxoHlVdp+peHldqjhl1EXvYxyHN8wQ1nKOOhp7jI57wbF1Ywrqz7j+lVsF4dvFtWQ8fKeySgg==</latexit> Base Batch Pretrained Encoder 3.1 Dynamic Memory Induction Network Wbase Base Set Support Set Meta Learning 3 Supervised Learning 2019; Hu et al., 2019), including text classification (Yu et al., 2018; Rios and Kavuluru, 2018; Xu et al., 2019; Geng et al., 2019; Gao et al., 2019; Ye and Ling, 2019). Memory mechanism has shown to be very effective in many NLP tasks (Tang et al., 2016; Das et al., 2017; Madotto et al., 2018). In the fewshot learning scenario, researchers have applied memory networks to store the encoded contextual information in each meta episode (Santoro et al., 2016; Cai et al., 2018; Kaiser et al., 2017). Specifically Qi et al. (2018) and Gidaris and Komodakis (2018) build a two-stage training procedure and regard the supervisely learned class representation as a memory component. Sample Vector C"
2020.acl-main.102,P19-1279,0,0.119062,"., 2018; Chen et al., 2019; Obamuyide and Vlachos, 1087 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1087–1094 c July 5 - 10, 2020. 2020 Association for Computational Linguistics Pre-trained Encoder We expect that developing few-shot text classifier should benefit from the recent advance on pretrained models (Peters et al., 2018; Devlin et al., 2019; Radford et al.). Unlike recent work (Geng et al., 2019), we employ BERT-base (Devlin et al., 2019) for sentence encoding , which has been used in recent few-shot learning models (Bao et al., 2019; Soares et al., 2019). The model architecture of BERT (Devlin et al., 2019) is a multi-layer bidiQuery Set Classifier 3.2 Class 3 Query-enhanced Induction Module An overview of our Dynamic Memory Induction Networks (DMIN) is shown in Figure 1, which is built on the two-stage few-shot framework Gidaris and Komodakis (2018). In the supervised learning stage (upper, green subfigure), a subset of classes in training data are selected as the base sets, consisting of Cbase number of base classes, which is used to finetune a pretrained sentence encoder and to train a classifier. In the meta-learning stage (bottom, orange"
2020.acl-main.102,D16-1021,0,0.036069,"7GZ0S0puR08YBeRLSZYTVabaO5zqzYn/LPdE51d3G9PdNrohYiRti//LNlP/1qVokBjjVNQRUU6oZVR0zWXLdFXVz+0tVkjKkxCncp3hGmGnnrM+29ghdu+qtp+NvWqlYtWdGm+Nd3ZIG7P4c5zxoHlVdp+peHldqjhl1EXvYxyHN8wQ1nKOOhp7jI57wbF1Ywrqz7j+lVsF4dvFtWQ8fKeySgg==</latexit> Base Batch Pretrained Encoder 3.1 Dynamic Memory Induction Network Wbase Base Set Support Set Meta Learning 3 Supervised Learning 2019; Hu et al., 2019), including text classification (Yu et al., 2018; Rios and Kavuluru, 2018; Xu et al., 2019; Geng et al., 2019; Gao et al., 2019; Ye and Ling, 2019). Memory mechanism has shown to be very effective in many NLP tasks (Tang et al., 2016; Das et al., 2017; Madotto et al., 2018). In the fewshot learning scenario, researchers have applied memory networks to store the encoded contextual information in each meta episode (Santoro et al., 2016; Cai et al., 2018; Kaiser et al., 2017). Specifically Qi et al. (2018) and Gidaris and Komodakis (2018) build a two-stage training procedure and regard the supervisely learned class representation as a memory component. Sample Vector Class Vector Classification Score Query Query Vector Figure 1: An overview of Dynamic Memory Induction Network with a 3-way 2-shot example. rectional Transformer"
2020.acl-main.102,D19-1444,0,0.121976,"Missing"
2020.acl-main.102,D19-1164,0,0.0311847,"n a memory matrix M (here Wbase ) and sample vector q ∈ Rd , the algorithm aims to adapt the sample vector based on memory M learned in the supervised learning stage. q 0 = DM R(M, q). (1) First, for each entry mi ∈ M , the standard matrix-transformation and squash operations in dynamic routing (Sabour et al., 2017) are applied on the inputs: m ˆ ij qˆj = squash(Wj mi + bj ), (2) = squash(Wj q + bj ), (3) where the transformation weights Wj and bias bj are shared across the inputs to fit the few-shot learning scenario. We then calculate the Pearson Correlation Coefficients (PCCs) (Hunt, 1986; Yang et al., 2019) between m ˆ i and qˆj . pij = tanh(P CCs(m ˆ ij , qˆj )), Cov(x1 , x2 ) P CCs = . σx1 σx2 (4) (5) αij = αij + pij m ˆ i vj . vˆj = (dij + pij )mij , We update the coupling coefficients αij and pij with Eq. 6 and Eq. 7, and finally output the adapted vector q 0 as in Algorithm 1. The Dynamic Memory Module (DMM) aims to use DMR to adapt sample vectors ec,s , guided by the memory Wbase . That is, the resulting adapted sample vector is computed with e0c,s = DM R(Wbase , ec,s ). Query-enhanced Induction Module  After the sample vectors e0c,s s=1,...,K are adapted (8) and query vectors {eq }L q=1"
2020.acl-main.102,P19-1277,0,0.027559,"caqOXvY8cA2owKx6Un7BNfpIwJAjAkcMSTiEB0FPBy4cpMR1MSEuIxToOMcUJfLmpOKk8Igd0XdIu45hY9qrnEK7GZ0S0puR08YBeRLSZYTVabaO5zqzYn/LPdE51d3G9PdNrohYiRti//LNlP/1qVokBjjVNQRUU6oZVR0zWXLdFXVz+0tVkjKkxCncp3hGmGnnrM+29ghdu+qtp+NvWqlYtWdGm+Nd3ZIG7P4c5zxoHlVdp+peHldqjhl1EXvYxyHN8wQ1nKOOhp7jI57wbF1Ywrqz7j+lVsF4dvFtWQ8fKeySgg==</latexit> Base Batch Pretrained Encoder 3.1 Dynamic Memory Induction Network Wbase Base Set Support Set Meta Learning 3 Supervised Learning 2019; Hu et al., 2019), including text classification (Yu et al., 2018; Rios and Kavuluru, 2018; Xu et al., 2019; Geng et al., 2019; Gao et al., 2019; Ye and Ling, 2019). Memory mechanism has shown to be very effective in many NLP tasks (Tang et al., 2016; Das et al., 2017; Madotto et al., 2018). In the fewshot learning scenario, researchers have applied memory networks to store the encoded contextual information in each meta episode (Santoro et al., 2016; Cai et al., 2018; Kaiser et al., 2017). Specifically Qi et al. (2018) and Gidaris and Komodakis (2018) build a two-stage training procedure and regard the supervisely learned class representation as a memory component. Sample Vector Class Vector Classification Score Query Query Vector Figure 1: An overview"
2020.acl-main.102,N18-1109,0,0.147015,"8kpxtbS2vrG5Vd7eaYokzxhvsCRMsrbvCR4GMW/IQIa8nWbci/yQt/zRmYq3bnkmgiS+kuOUdyNvGAeDgHmSqHarN1Huaa9ccaqOXvY8cA2owKx6Un7BNfpIwJAjAkcMSTiEB0FPBy4cpMR1MSEuIxToOMcUJfLmpOKk8Igd0XdIu45hY9qrnEK7GZ0S0puR08YBeRLSZYTVabaO5zqzYn/LPdE51d3G9PdNrohYiRti//LNlP/1qVokBjjVNQRUU6oZVR0zWXLdFXVz+0tVkjKkxCncp3hGmGnnrM+29ghdu+qtp+NvWqlYtWdGm+Nd3ZIG7P4c5zxoHlVdp+peHldqjhl1EXvYxyHN8wQ1nKOOhp7jI57wbF1Ywrqz7j+lVsF4dvFtWQ8fKeySgg==</latexit> Base Batch Pretrained Encoder 3.1 Dynamic Memory Induction Network Wbase Base Set Support Set Meta Learning 3 Supervised Learning 2019; Hu et al., 2019), including text classification (Yu et al., 2018; Rios and Kavuluru, 2018; Xu et al., 2019; Geng et al., 2019; Gao et al., 2019; Ye and Ling, 2019). Memory mechanism has shown to be very effective in many NLP tasks (Tang et al., 2016; Das et al., 2017; Madotto et al., 2018). In the fewshot learning scenario, researchers have applied memory networks to store the encoded contextual information in each meta episode (Santoro et al., 2016; Cai et al., 2018; Kaiser et al., 2017). Specifically Qi et al. (2018) and Gidaris and Komodakis (2018) build a two-stage training procedure and regard the supervisely learned class representation as a memory c"
2020.acl-main.102,D19-1112,0,\N,Missing
2020.acl-main.57,P19-1361,0,0.282887,"nt performance degradation in dialog systems, which ∗ Corresponding author may harm the users’ experience and result in loss of customers in commercial applications. Therefore, both fast adaptability and reliable performance are strongly desirable for practical system deployment. Fast adaptability reflects the efficiency of adapting dialog systems to domains with low-resource data. Reliable performance reflects the robustness of handling unpredictable user behaviors in online services. To boost the online performance of dialog systems, there have been some recent work (Rajendran et al., 2019; Wang et al., 2019; Lu et al., 2019) on designing end-to-end models in a human-machine joint-teaming manner. For instance, the dialog system in (Rajendran et al., 2019) can identify an ongoing dialog during testing when the system might fail and transfer it to a human agent. But all these methods are trained with sufficient data, which hinders the possibility of rapidly prototyping the models in new domains with restricted resources. In this paper, we formulate the low-resource goal-oriented dialog learning as a few-shot learning problem, where a limited numbers of dialogs are used for training and the remainin"
2020.acl-main.57,P17-1062,0,0.0323554,"2 shows the adaptation results for different models on MultiWOZ 2.1. It can be seen that MDS still largely outperforms other models with the adaptation of 10 dialogs. The degradation of per-turn accuracy from extended-bAbI to MultiWOZ is reasonable since the user utterance is more diverse and the dialog policy is more flexible. 4 Related Work End-to-end neural approaches of building dialog systems have attracted increasing research interest. The work of (Bordes et al., 2017) is the first attempt to solve goal-oriented dialog tasks with endto-end models. Further improvements has been made in (Williams et al., 2017) to combine explicit domain-specific knowledge and implicit RNN features. Luo et al. (2019) take user personalities into consideration for better user satisfaction. Rajendran et al. (2018) learn dialogs with multiple possible answers. Our work is inspired by the work of (Rajendran et al., 2019; Wang et al., 2019), which Mem MetaMem Mem+C MDS-switch MDSrand MDSmle MDS Adapt with 10 dialogs accuracy request 56.87±1.63 62.78±2.05 80.59±3.13 64.50±3.75 74.78±4.35 80.92±3.02 83.52±3.30 n.a. n.a. 38.18±5.01 n.a. 38.34 37.91±4.20 38.34±6.96 Table 2: Few-shot test results on MultiWOZ 2.1. propose to s"
2020.acl-main.57,W18-5001,0,0.050042,"Missing"
2020.acl-main.664,W14-3348,0,0.0358431,"Missing"
2020.acl-main.664,D17-1159,0,0.159016,"vel” label (obj1 , pred, obj2 ) extracted from captions, there may exist multiple object regions corresponding to obj1 and obj2 . In this paper, we propose to use weakly supervised multi-instance learning to detect if a bag of object (region) pairs in an image contain certain predicates, e.g., predicates appearing in ground-truth captions here (or in other applications, they can be any given predicates under concerns). Based on that we can construct caption-guided visual relationship graphs. Once the visual relationship graphs (VRG) are built, we propose to adapt graph convolution operations (Marcheggiani and Titov, 2017) to obtain representation for object nodes and predicate nodes. These nodes can be viewed as image representation units used for generation. During generation, we further incorporate visual relationships—we propose multi-task learning for jointly predicting word and tag sequences, where each word in a caption could be assigned with a tag, i.e., object, predicate, or none, which takes as input the graph node features from the above visual relationship graphs. The motivation for predicting a tag in each step is to regularize which types of information should be taken into more consideration for"
2020.acl-main.664,P02-1040,0,0.112926,"Missing"
2020.acl-main.664,W15-2812,0,0.0260432,"with regard to the ground truth caption S ∗ = {w1∗ , w2∗ , ..., wT∗ }: LXE = − log p(S ∗ |I) =− T X ∗ log p(wt∗ |w<t , I) (1) (2) t=1 The model is further tuned with a Reinforcement Learning (RL) objective (Rennie et al., 2017) to maximize the reward of the generated sentence S: JRL = ES∼p(S|I) (d(S, S ∗ )) (3) 3.1 3.1.1 Caption-Guided Visual Relationship Graph (CGVRG) with Weakly Supervised Learning Extracting Visual Relationship Triples and Detecting Objects The process of constructing CGVRG first extracts relationship triples from captions using textual scene graph parser as described in (Schuster et al., 2015). Our framework employs Faster RCNN (Ren et al., 2015) to recognize instances of objects and returns a set of image regions for objects: V = {v1 , v2 , · · · , vn }. 3.1.2 Constructing CGVRG The main focus of CGVRG is constructing visual relationship graphs. As discussed in introduction, the existing approaches use pre-trained VRD (visual relationship detection) models, which often ignore key relationships needed for captioning. This gap can be even more prominent if the domain/data used to train image-captioning is farther from where VRD is pretrained. A major challenge to use predicate tripl"
2020.acl-main.664,P19-1650,0,0.0524296,"Missing"
2020.coling-main.101,D14-1059,0,0.397868,"the presence of noise and uncertainty. The majority of research efforts are based on some abstract logical forms such as the first-order logic (FOL) or its fragments. For natural language, obtaining such a representation is known to face many thorny challenges. Natural logic instead aims to sidestep some of the challenges by performing inferences over surface forms of text based on monotonicity or projectivity (Van Benthem, 1986; Valencia, 1991; MacCartney and Manning, 2009; Icard and Moss, 2014), and has been applied to tasks such as natural language inference (MacCartney and Manning, 2009; Angeli and Manning, 2014) and question answering (Angeli et al., 2016). In this work we explore differentiable natural logic models that integrate natural logic with neural networks, with the aim to keep the backbone of inference based on the natural logic formalism, while introducing subsymbolic vector representations and neural components into the framework. Combining the advantages of neural networks with natural logic needs to take several basic problems into consideration. Two problems flow directly from this objective: 1) How (and where) to leverage the strength of neural networks in the natural logic formalism,"
2020.coling-main.101,P16-1042,0,0.281442,"ty of research efforts are based on some abstract logical forms such as the first-order logic (FOL) or its fragments. For natural language, obtaining such a representation is known to face many thorny challenges. Natural logic instead aims to sidestep some of the challenges by performing inferences over surface forms of text based on monotonicity or projectivity (Van Benthem, 1986; Valencia, 1991; MacCartney and Manning, 2009; Icard and Moss, 2014), and has been applied to tasks such as natural language inference (MacCartney and Manning, 2009; Angeli and Manning, 2014) and question answering (Angeli et al., 2016). In this work we explore differentiable natural logic models that integrate natural logic with neural networks, with the aim to keep the backbone of inference based on the natural logic formalism, while introducing subsymbolic vector representations and neural components into the framework. Combining the advantages of neural networks with natural logic needs to take several basic problems into consideration. Two problems flow directly from this objective: 1) How (and where) to leverage the strength of neural networks in the natural logic formalism, and; 2) How to alleviate the issue of a lack"
2020.coling-main.101,D15-1075,0,0.031026,"and unifies them to consider implicatives (Nairn et al., 2006), which is a state-of-the-art natural logic formalism that has been used for multiple NLP tasks (MacCartney, 2009; Angeli and Manning, 2014). In this work we explore neural natural logic based on this formalism. We will briefly review the background in Section 3. 2.3 Natural Language Inference Previous work often studies natural logic in natural language inference (NLI). NLI (Dagan et al., 2005; Iftene and Balahur-Dobrescu, 2007; MacCartney and Manning, 2008; MacCartney and Manning, 2009; MacCartney, 2009; Angeli and Manning, 2014; Bowman et al., 2015), also known as recognizing textual entailment (RTE), aims to model the logical relationships between two sentences, e.g., as a binary (entailment vs. non-entailment) or three-way classification (entailment, contradiction, and neutral). Recently deep learning algorithms have been proposed (Bowman et al., 2015; Chen et al., 2017a; Chen et al., 2017b; Chen et al., 2017c; Chen et al., 2018; Peters et al., 2018; Yoon et al., 2018; Kiela et al., 2018; Talman et al., 2018; Yang et al., 2019; Devlin et al., 2019). In this paper we will describe and evaluate our neural natural logic models on NLI. The"
2020.coling-main.101,P17-1152,1,0.908432,"tural Language Inference Previous work often studies natural logic in natural language inference (NLI). NLI (Dagan et al., 2005; Iftene and Balahur-Dobrescu, 2007; MacCartney and Manning, 2008; MacCartney and Manning, 2009; MacCartney, 2009; Angeli and Manning, 2014; Bowman et al., 2015), also known as recognizing textual entailment (RTE), aims to model the logical relationships between two sentences, e.g., as a binary (entailment vs. non-entailment) or three-way classification (entailment, contradiction, and neutral). Recently deep learning algorithms have been proposed (Bowman et al., 2015; Chen et al., 2017a; Chen et al., 2017b; Chen et al., 2017c; Chen et al., 2018; Peters et al., 2018; Yoon et al., 2018; Kiela et al., 2018; Talman et al., 2018; Yang et al., 2019; Devlin et al., 2019). In this paper we will describe and evaluate our neural natural logic models on NLI. The proposed model may also be further extended to other tasks in which natural logic has been applied, e.g., question answering (Angeli et al., 2016). 3 Background This section briefly reviews the natural logic formalism (MacCartney and Manning, 2009) that our work is based on. For more details, we refer readers to (MacCartney an"
2020.coling-main.101,W17-5307,1,0.925613,"tural Language Inference Previous work often studies natural logic in natural language inference (NLI). NLI (Dagan et al., 2005; Iftene and Balahur-Dobrescu, 2007; MacCartney and Manning, 2008; MacCartney and Manning, 2009; MacCartney, 2009; Angeli and Manning, 2014; Bowman et al., 2015), also known as recognizing textual entailment (RTE), aims to model the logical relationships between two sentences, e.g., as a binary (entailment vs. non-entailment) or three-way classification (entailment, contradiction, and neutral). Recently deep learning algorithms have been proposed (Bowman et al., 2015; Chen et al., 2017a; Chen et al., 2017b; Chen et al., 2017c; Chen et al., 2018; Peters et al., 2018; Yoon et al., 2018; Kiela et al., 2018; Talman et al., 2018; Yang et al., 2019; Devlin et al., 2019). In this paper we will describe and evaluate our neural natural logic models on NLI. The proposed model may also be further extended to other tasks in which natural logic has been applied, e.g., question answering (Angeli et al., 2016). 3 Background This section briefly reviews the natural logic formalism (MacCartney and Manning, 2009) that our work is based on. For more details, we refer readers to (MacCartney an"
2020.coling-main.101,C18-1154,1,0.841234,"logic in natural language inference (NLI). NLI (Dagan et al., 2005; Iftene and Balahur-Dobrescu, 2007; MacCartney and Manning, 2008; MacCartney and Manning, 2009; MacCartney, 2009; Angeli and Manning, 2014; Bowman et al., 2015), also known as recognizing textual entailment (RTE), aims to model the logical relationships between two sentences, e.g., as a binary (entailment vs. non-entailment) or three-way classification (entailment, contradiction, and neutral). Recently deep learning algorithms have been proposed (Bowman et al., 2015; Chen et al., 2017a; Chen et al., 2017b; Chen et al., 2017c; Chen et al., 2018; Peters et al., 2018; Yoon et al., 2018; Kiela et al., 2018; Talman et al., 2018; Yang et al., 2019; Devlin et al., 2019). In this paper we will describe and evaluate our neural natural logic models on NLI. The proposed model may also be further extended to other tasks in which natural logic has been applied, e.g., question answering (Angeli et al., 2016). 3 Background This section briefly reviews the natural logic formalism (MacCartney and Manning, 2009) that our work is based on. For more details, we refer readers to (MacCartney and Manning, 2008; MacCartney and Manning, 2009; MacCartney, 2"
2020.coling-main.101,D16-1146,0,0.0644083,"Missing"
2020.coling-main.101,N19-1423,0,0.17901,"Manning, 2008; MacCartney and Manning, 2009; MacCartney, 2009; Angeli and Manning, 2014; Bowman et al., 2015), also known as recognizing textual entailment (RTE), aims to model the logical relationships between two sentences, e.g., as a binary (entailment vs. non-entailment) or three-way classification (entailment, contradiction, and neutral). Recently deep learning algorithms have been proposed (Bowman et al., 2015; Chen et al., 2017a; Chen et al., 2017b; Chen et al., 2017c; Chen et al., 2018; Peters et al., 2018; Yoon et al., 2018; Kiela et al., 2018; Talman et al., 2018; Yang et al., 2019; Devlin et al., 2019). In this paper we will describe and evaluate our neural natural logic models on NLI. The proposed model may also be further extended to other tasks in which natural logic has been applied, e.g., question answering (Angeli et al., 2016). 3 Background This section briefly reviews the natural logic formalism (MacCartney and Manning, 2009) that our work is based on. For more details, we refer readers to (MacCartney and Manning, 2008; MacCartney and Manning, 2009; MacCartney, 2009; Angeli et al., 2016). Monotonicity is a pervasive feature of natural language and an essential concept in natural log"
2020.coling-main.101,P17-1097,0,0.117639,"ural networks, with the aim to keep the backbone of inference based on the natural logic formalism, while introducing subsymbolic vector representations and neural components into the framework. Combining the advantages of neural networks with natural logic needs to take several basic problems into consideration. Two problems flow directly from this objective: 1) How (and where) to leverage the strength of neural networks in the natural logic formalism, and; 2) How to alleviate the issue of a lack of intermediate supervision for training sub-components, which may lead to the spurious problem (Guu et al., 2017; Min et al., 2019) in the end-to-end training. We explore a framework in which module networks (Andreas et al., 2016; Gupta et al., 2020) are leveraged to model the natural logic operations, which is enhanced with a memory module component to ∗ Equal contribution. This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/. License details: http:// 1172 Proceedings of the 28th International Conference on Computational Linguistics, pages 1172–1185 Barcelona, Spain (Online), December 8-13, 2020 Relation Relation Name Example Set The"
2020.coling-main.101,2014.lilt-9.7,0,0.148125,"y, while symbolic models often render superior explainability and interpretability but are brittle and prone to fail in the presence of noise and uncertainty. The majority of research efforts are based on some abstract logical forms such as the first-order logic (FOL) or its fragments. For natural language, obtaining such a representation is known to face many thorny challenges. Natural logic instead aims to sidestep some of the challenges by performing inferences over surface forms of text based on monotonicity or projectivity (Van Benthem, 1986; Valencia, 1991; MacCartney and Manning, 2009; Icard and Moss, 2014), and has been applied to tasks such as natural language inference (MacCartney and Manning, 2009; Angeli and Manning, 2014) and question answering (Angeli et al., 2016). In this work we explore differentiable natural logic models that integrate natural logic with neural networks, with the aim to keep the backbone of inference based on the natural logic formalism, while introducing subsymbolic vector representations and neural components into the framework. Combining the advantages of neural networks with natural logic needs to take several basic problems into consideration. Two problems flow d"
2020.coling-main.101,D18-1176,0,0.0132997,"l., 2005; Iftene and Balahur-Dobrescu, 2007; MacCartney and Manning, 2008; MacCartney and Manning, 2009; MacCartney, 2009; Angeli and Manning, 2014; Bowman et al., 2015), also known as recognizing textual entailment (RTE), aims to model the logical relationships between two sentences, e.g., as a binary (entailment vs. non-entailment) or three-way classification (entailment, contradiction, and neutral). Recently deep learning algorithms have been proposed (Bowman et al., 2015; Chen et al., 2017a; Chen et al., 2017b; Chen et al., 2017c; Chen et al., 2018; Peters et al., 2018; Yoon et al., 2018; Kiela et al., 2018; Talman et al., 2018; Yang et al., 2019; Devlin et al., 2019). In this paper we will describe and evaluate our neural natural logic models on NLI. The proposed model may also be further extended to other tasks in which natural logic has been applied, e.g., question answering (Angeli et al., 2016). 3 Background This section briefly reviews the natural logic formalism (MacCartney and Manning, 2009) that our work is based on. For more details, we refer readers to (MacCartney and Manning, 2008; MacCartney and Manning, 2009; MacCartney, 2009; Angeli et al., 2016). Monotonicity is a pervasive featu"
2020.coling-main.101,P19-1028,0,0.0179857,"op inference. The model can effectively learn natural logic operations in the end-to-end training paradigm.1 2 Related Work 2.1 Neural Symbolic Models A growing number of research efforts have recently revisited the long-standing problem of bringing together the complementary advantages of neural networks and symbolic methods. There are at least two approaches that have received intensive attention. One uses symbolic constraints as regularizers to equip neural models with the corresponding inductive bias (Demeester et al., 2016; Diligenti et al., 2017; Donadello et al., 2017; Xu et al., 2018; Li and Srikumar, 2019). Another approach develops differentiable end-to-end trained frameworks based on symbolic models. For example, the work in (Rockt¨aschel and Riedel, 2017; Weber et al., 2019; Minervini et al., 2020) proposes a differentiable backward-chaining algorithm, and Dong et al. (2019) adopt probabilistic tensor representations for logic predicates and mimic the forward-chaining proof. Evans and Grefenstette (2018) treat inductive logic programming as a satisfiability problem and Manhaeve et al. (2018) combine high-level symbolic oriented reasoning with low-level neural perception models. The second ap"
2020.coling-main.101,C08-1066,0,0.172192,"009; Icard, 2012; Angeli et al., 2016) has a long history that is traceable to the syllogisms of Aristotle. It aims to model a subset of logical inferences by operating directly on the surface form and structure of language, based on monotonicity or projectivity (Van Benthem, 1986; Valencia, 1991; MacCartney and Manning, 2009; Icard and Moss, 2014), rather than deduction on the abstract forms such as the first-order logic (FOL) or its fragments—it is well known that deriving logic forms for natural language is a very challenging task. In natural language processing, the framework proposed in (MacCartney and Manning, 2008; MacCartney and Manning, 2009) extends monotonicity-based models (van Benthem, 1988; Valencia, 1991) to incorporate semantic exclusion and unifies them to consider implicatives (Nairn et al., 2006), which is a state-of-the-art natural logic formalism that has been used for multiple NLP tasks (MacCartney, 2009; Angeli and Manning, 2014). In this work we explore neural natural logic based on this formalism. We will briefly review the background in Section 3. 2.3 Natural Language Inference Previous work often studies natural logic in natural language inference (NLI). NLI (Dagan et al., 2005; Ift"
2020.coling-main.101,W09-3714,0,0.351828,"e robust to noise and ambiguity, while symbolic models often render superior explainability and interpretability but are brittle and prone to fail in the presence of noise and uncertainty. The majority of research efforts are based on some abstract logical forms such as the first-order logic (FOL) or its fragments. For natural language, obtaining such a representation is known to face many thorny challenges. Natural logic instead aims to sidestep some of the challenges by performing inferences over surface forms of text based on monotonicity or projectivity (Van Benthem, 1986; Valencia, 1991; MacCartney and Manning, 2009; Icard and Moss, 2014), and has been applied to tasks such as natural language inference (MacCartney and Manning, 2009; Angeli and Manning, 2014) and question answering (Angeli et al., 2016). In this work we explore differentiable natural logic models that integrate natural logic with neural networks, with the aim to keep the backbone of inference based on the natural logic formalism, while introducing subsymbolic vector representations and neural components into the framework. Combining the advantages of neural networks with natural logic needs to take several basic problems into considerati"
2020.coling-main.101,D19-1284,0,0.0595061,"h the aim to keep the backbone of inference based on the natural logic formalism, while introducing subsymbolic vector representations and neural components into the framework. Combining the advantages of neural networks with natural logic needs to take several basic problems into consideration. Two problems flow directly from this objective: 1) How (and where) to leverage the strength of neural networks in the natural logic formalism, and; 2) How to alleviate the issue of a lack of intermediate supervision for training sub-components, which may lead to the spurious problem (Guu et al., 2017; Min et al., 2019) in the end-to-end training. We explore a framework in which module networks (Andreas et al., 2016; Gupta et al., 2020) are leveraged to model the natural logic operations, which is enhanced with a memory module component to ∗ Equal contribution. This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/. License details: http:// 1172 Proceedings of the 28th International Conference on Computational Linguistics, pages 1172–1185 Barcelona, Spain (Online), December 8-13, 2020 Relation Relation Name Example Set Theoretic Definition x"
2020.coling-main.101,W06-3907,0,0.0819632,"ctive logic programming as a satisfiability problem and Manhaeve et al. (2018) combine high-level symbolic oriented reasoning with low-level neural perception models. The second approach is more interesting to us for exploring powerful reasoning models with built-in explainability. Unlike the existing work based on abstract logical forms, this paper explores the integration of neural networks with natural logic. 1 Our code is available at https://github.com/feng-yufei/Neural-Natural-Logic 1173 2.2 Natural Logic Natural logic (Lakoff, 1970; van Benthem, 1988; Valencia, 1991; Van Benthem, 1995; Nairn et al., 2006; MacCartney, 2009; Icard, 2012; Angeli et al., 2016) has a long history that is traceable to the syllogisms of Aristotle. It aims to model a subset of logical inferences by operating directly on the surface form and structure of language, based on monotonicity or projectivity (Van Benthem, 1986; Valencia, 1991; MacCartney and Manning, 2009; Icard and Moss, 2014), rather than deduction on the abstract forms such as the first-order logic (FOL) or its fragments—it is well known that deriving logic forms for natural language is a very challenging task. In natural language processing, the framewor"
2020.coling-main.101,D14-1162,0,0.0849236,"Missing"
2020.coling-main.101,N18-1202,0,0.0497234,"anguage inference (NLI). NLI (Dagan et al., 2005; Iftene and Balahur-Dobrescu, 2007; MacCartney and Manning, 2008; MacCartney and Manning, 2009; MacCartney, 2009; Angeli and Manning, 2014; Bowman et al., 2015), also known as recognizing textual entailment (RTE), aims to model the logical relationships between two sentences, e.g., as a binary (entailment vs. non-entailment) or three-way classification (entailment, contradiction, and neutral). Recently deep learning algorithms have been proposed (Bowman et al., 2015; Chen et al., 2017a; Chen et al., 2017b; Chen et al., 2017c; Chen et al., 2018; Peters et al., 2018; Yoon et al., 2018; Kiela et al., 2018; Talman et al., 2018; Yang et al., 2019; Devlin et al., 2019). In this paper we will describe and evaluate our neural natural logic models on NLI. The proposed model may also be further extended to other tasks in which natural logic has been applied, e.g., question answering (Angeli et al., 2016). 3 Background This section briefly reviews the natural logic formalism (MacCartney and Manning, 2009) that our work is based on. For more details, we refer readers to (MacCartney and Manning, 2008; MacCartney and Manning, 2009; MacCartney, 2009; Angeli et al., 2"
2020.coling-main.101,P19-1618,0,0.0583581,"Missing"
2020.coling-main.101,W19-4804,0,0.20633,"≡ @ A # | | | # A # A # ` ` # ` @ @ # # # # # # # # # # Table 3: Relation aggregation table (Icard, 2012). Relations listed in the first column are aggregated with those listed in the first row, yielding the relations in the corresponding entries in the table. capture contextual information. At the lexical and local relation learning layers, we constrain the network to predict the seven natural logic relations. The entire model is differentiable and end-to-end trained. We evaluate and analyze the proposed model on the monotonicity subset of Semantic Fragments (Richardson et al., 2020), HELP (Yanaka et al., 2019b) and MED (Yanaka et al., 2019a). We also extend MED to generate a dataset to help evaluate 2-hop inference. The model can effectively learn natural logic operations in the end-to-end training paradigm.1 2 Related Work 2.1 Neural Symbolic Models A growing number of research efforts have recently revisited the long-standing problem of bringing together the complementary advantages of neural networks and symbolic methods. There are at least two approaches that have received intensive attention. One uses symbolic constraints as regularizers to equip neural models with the corresponding inductive"
2020.coling-main.101,S19-1027,0,0.209999,"≡ @ A # | | | # A # A # ` ` # ` @ @ # # # # # # # # # # Table 3: Relation aggregation table (Icard, 2012). Relations listed in the first column are aggregated with those listed in the first row, yielding the relations in the corresponding entries in the table. capture contextual information. At the lexical and local relation learning layers, we constrain the network to predict the seven natural logic relations. The entire model is differentiable and end-to-end trained. We evaluate and analyze the proposed model on the monotonicity subset of Semantic Fragments (Richardson et al., 2020), HELP (Yanaka et al., 2019b) and MED (Yanaka et al., 2019a). We also extend MED to generate a dataset to help evaluate 2-hop inference. The model can effectively learn natural logic operations in the end-to-end training paradigm.1 2 Related Work 2.1 Neural Symbolic Models A growing number of research efforts have recently revisited the long-standing problem of bringing together the complementary advantages of neural networks and symbolic methods. There are at least two approaches that have received intensive attention. One uses symbolic constraints as regularizers to equip neural models with the corresponding inductive"
2020.emnlp-main.628,D15-1075,0,0.336411,".g., tables in relational databases or in the HTML format is also ubiquitous. Performing fact validation based on structured data is important yet challenging and further study is highly desirable. Fig. 1 depicts a simplified example in which systems are expected to decide whether the facts in the table support the natural language statement. In addition to its importance in applications, the task presents research challenges of fundamental interests—the problem inherently involves both informal inference based on language understanding (Dagan et al., 2005; MacCartney and Manning, 2009, 2008; Bowman et al., 2015, 2016) and symbolic operations such as mathematical operations (e.g., count and max). Recently, pre-trained language models such as BERT (Devlin et al., 2019) have shown superior performances in natural language inference by leveraging knowledge from large text datasets and can capture complicated semantic and syntactic information among premises and hypotheses (Radford, 2018; Radford et al., 2019; Liu et al., 2019; Dong et al., 2019b). 7810 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 7810–7825, c November 16–20, 2020. 2020 Association for Com"
2020.emnlp-main.628,P16-1139,0,0.0560132,"Missing"
2020.emnlp-main.628,P17-1171,0,0.0269024,"n is mainly based on collecting and using evidences from unstructured text data (Liu et al., 2020; Nie et al., 2019; Hanselowski et al., 2018; Yoneda et al., 2018). FEVER (Thorne et al., 2018) is one of the most influential benchmark datasets built to evaluate systems in checking claims by retrieving Wikipedia articles and extracting evidence sentences. Recent proposed FEVER 2.0 (Thorne et al., 2019) has a more challenging dataset to verify factoid claims and an adversarial attack task. Some previous models are developed on the official baseline (Thorne et al., 2018) with three step pipeline (Chen et al., 2017a) for fact verification (Hanselowski et al., 2018; Yoneda et al., 2018; Yin and Roth, 2018; Nie et al., 2019). Others formulates fact verification as graph reasoning (Zhou et al., 2019; Liu et al., 2020). Natural language inference (NLI) task is also a verification problem which is fully based on unstructured text data (Dagan et al., 2005, 2010; Bowman et al., 2015; Parikh et al., 2016; Chen et al., 2017c; Ghaeini et al., 2018; Peters et al., 2018). Neural models proposed for NLI have been shown to be effective (Parikh et al., 2016; Chen et al., 2017d,e; Ghaeini et al., 2018; Peters et al., 2"
2020.emnlp-main.628,N18-1202,0,0.00910292,"s a tree consisting of multiple executable symbolic operations opi . An example of programs is shown in the center of Fig. 2. An operation opi = (opi .t, opi .arg) contains an operator opi .t (e.g., max in the figure) and arguments opi .arg relevant to table T (e.g., all rows and tournaments played), and the execution of an operation yields an output/answer ans (e.g., 29). Before building the model, we follow the previous work (Chen et al., 2020) and perform rule-based entity linking and latent program search to obtain a set of candidate programs Z = {zi }N i=1 . Specifically, entity linking (Nie et al., 2018) detects relevant entities (i.e., cells in evidence table T ) in statement S using a set of string matching rules. And the latent program search algorithm finds all valid combinations of pre-defined operations and detected entities by traversing and executing them recursively through the evidence table T . Task Formulation and Notations Formally, given a structured evidence table T and a statement S, the fact verification task aims to predict whether T entails S or refutes it. The evidence table T = {Ti,j |i ≤ R, j ≤ C} has R rows and C columns, and Ti,j is the value in the (i, j)-th cell. Ti,"
2020.emnlp-main.628,D17-1317,0,0.0203213,"ear Tournaments Played Avg. Score Scoring Rank 2007 22 72.46 81 2008 29 71.65 22 2009 25 71.90 34 2010 18 73.42 92 2011 11 74.42 125 Statement Ji-young Oh played more tournament in 2008 than any other year. Label ENTAILED Program eq { max { all_rows ; tournaments played } ; hop { filter_eq { all_rows ; year ; 2008 } ; tournaments played } } = True Figure 1: An example of fact verification over tables. Introduction With the overwhelming information available on the Internet, fact verification has become crucial for many applications such as detecting fake news, rumors, and political deception (Rashkin et al., 2017; Thorne et al., 2018; Goodrich et al., 2019; Vaibhav et al., 2019; Kry´sci´nski et al., 2019), among others. Existing research has mainly focused on collecting ∗ Equal contribution to this work. The work was done during the second author’s visiting to Queen’s University. and processing evidences from unstructured text data (Liu et al., 2020; Nie et al., 2019; Hanselowski et al., 2018; Yoneda et al., 2018), which is only one type of data where important facts exist. Structured and semi-structured data, e.g., tables in relational databases or in the HTML format is also ubiquitous. Performing fa"
2020.emnlp-main.628,P17-1041,0,0.0329891,"rograms for fact verification. LogicalFactChecker utilizes inherent structures of programs to prune irrelevant information in evidence tables and modularize symbolic operations with module networks. Different from theirs, our proposed framework verbalizes the accumulated evidences from program execution to support the final verification decision with graph attention networks. Semantic Parsing. A line of work uses program synthesis or logic forms to address different natural language processing problems, such as question answering (Berant et al., 2013; Berant and Liang, 2014), code generation (Yin and Neubig, 2017), SQL synthesis (Zhong et al., 2017; Yu et al., 2018) and mathematical problem solving (Kushman et al., 2014; Shi et al., 2015). Traditional semantic parsing methods greatly rely on rules and lexicons to parse texts into structured representations (Zettlemoyer and Collins, 2005; Berant et al., 2013; Artzi and Zettlemoyer, 2013). Recent semantic parsing methods strives to leverage the power of neural 7811 Program Selection Verbalization with Program Execution Selected Program: eq { max { all_rows ; tournaments played } ; hop { filter_eq { all_rows ; year ; 2008 } ; tournaments played } } Candid"
2020.emnlp-main.628,D18-1010,0,0.0396718,"., 2020; Nie et al., 2019; Hanselowski et al., 2018; Yoneda et al., 2018). FEVER (Thorne et al., 2018) is one of the most influential benchmark datasets built to evaluate systems in checking claims by retrieving Wikipedia articles and extracting evidence sentences. Recent proposed FEVER 2.0 (Thorne et al., 2019) has a more challenging dataset to verify factoid claims and an adversarial attack task. Some previous models are developed on the official baseline (Thorne et al., 2018) with three step pipeline (Chen et al., 2017a) for fact verification (Hanselowski et al., 2018; Yoneda et al., 2018; Yin and Roth, 2018; Nie et al., 2019). Others formulates fact verification as graph reasoning (Zhou et al., 2019; Liu et al., 2020). Natural language inference (NLI) task is also a verification problem which is fully based on unstructured text data (Dagan et al., 2005, 2010; Bowman et al., 2015; Parikh et al., 2016; Chen et al., 2017c; Ghaeini et al., 2018; Peters et al., 2018). Neural models proposed for NLI have been shown to be effective (Parikh et al., 2016; Chen et al., 2017d,e; Ghaeini et al., 2018; Peters et al., 2018), including models incorporating external knowledge (Chen et al., 2017b; Yang et al., 2"
2020.emnlp-main.628,W18-5515,0,0.0296693,"oduction With the overwhelming information available on the Internet, fact verification has become crucial for many applications such as detecting fake news, rumors, and political deception (Rashkin et al., 2017; Thorne et al., 2018; Goodrich et al., 2019; Vaibhav et al., 2019; Kry´sci´nski et al., 2019), among others. Existing research has mainly focused on collecting ∗ Equal contribution to this work. The work was done during the second author’s visiting to Queen’s University. and processing evidences from unstructured text data (Liu et al., 2020; Nie et al., 2019; Hanselowski et al., 2018; Yoneda et al., 2018), which is only one type of data where important facts exist. Structured and semi-structured data, e.g., tables in relational databases or in the HTML format is also ubiquitous. Performing fact validation based on structured data is important yet challenging and further study is highly desirable. Fig. 1 depicts a simplified example in which systems are expected to decide whether the facts in the table support the natural language statement. In addition to its importance in applications, the task presents research challenges of fundamental interests—the problem inherently involves both informal"
2020.emnlp-main.628,D18-1425,0,0.120813,"inherent structures of programs to prune irrelevant information in evidence tables and modularize symbolic operations with module networks. Different from theirs, our proposed framework verbalizes the accumulated evidences from program execution to support the final verification decision with graph attention networks. Semantic Parsing. A line of work uses program synthesis or logic forms to address different natural language processing problems, such as question answering (Berant et al., 2013; Berant and Liang, 2014), code generation (Yin and Neubig, 2017), SQL synthesis (Zhong et al., 2017; Yu et al., 2018) and mathematical problem solving (Kushman et al., 2014; Shi et al., 2015). Traditional semantic parsing methods greatly rely on rules and lexicons to parse texts into structured representations (Zettlemoyer and Collins, 2005; Berant et al., 2013; Artzi and Zettlemoyer, 2013). Recent semantic parsing methods strives to leverage the power of neural 7811 Program Selection Verbalization with Program Execution Selected Program: eq { max { all_rows ; tournaments played } ; hop { filter_eq { all_rows ; year ; 2008 } ; tournaments played } } Candidate Programs Program_n Graph-based Verification [CLS]"
2020.emnlp-main.628,2020.acl-main.539,0,0.267934,"2018; Peters et al., 2018), including models incorporating external knowledge (Chen et al., 2017b; Yang et al., 2019). Our work focuses on fact verification based on structured tables (Chen et al., 2020). For verification performed on structured data, Chen et al. (2020) propose a typical baseline (TableBERT), which is a semantic matching model taking a linearized table T and statement S as input and employs BERT for verification. The other model (LPA) proposed in (Chen et al., 2020) uses Transformer blocks to compute semantic similarity between a statement and program. A contemporaneous work (Zhong et al., 2020) proposes LogicalFactChecker aiming to leverage programs for fact verification. LogicalFactChecker utilizes inherent structures of programs to prune irrelevant information in evidence tables and modularize symbolic operations with module networks. Different from theirs, our proposed framework verbalizes the accumulated evidences from program execution to support the final verification decision with graph attention networks. Semantic Parsing. A line of work uses program synthesis or logic forms to address different natural language processing problems, such as question answering (Berant et al.,"
2020.emnlp-main.628,P19-1085,0,0.0677117,", 2018) is one of the most influential benchmark datasets built to evaluate systems in checking claims by retrieving Wikipedia articles and extracting evidence sentences. Recent proposed FEVER 2.0 (Thorne et al., 2019) has a more challenging dataset to verify factoid claims and an adversarial attack task. Some previous models are developed on the official baseline (Thorne et al., 2018) with three step pipeline (Chen et al., 2017a) for fact verification (Hanselowski et al., 2018; Yoneda et al., 2018; Yin and Roth, 2018; Nie et al., 2019). Others formulates fact verification as graph reasoning (Zhou et al., 2019; Liu et al., 2020). Natural language inference (NLI) task is also a verification problem which is fully based on unstructured text data (Dagan et al., 2005, 2010; Bowman et al., 2015; Parikh et al., 2016; Chen et al., 2017c; Ghaeini et al., 2018; Peters et al., 2018). Neural models proposed for NLI have been shown to be effective (Parikh et al., 2016; Chen et al., 2017d,e; Ghaeini et al., 2018; Peters et al., 2018), including models incorporating external knowledge (Chen et al., 2017b; Yang et al., 2019). Our work focuses on fact verification based on structured tables (Chen et al., 2020). Fo"
2020.findings-emnlp.127,P17-1055,0,0.0514115,"Missing"
2020.findings-emnlp.127,D19-1193,1,0.595805,"User 2: User 1: User 2: User 1: User 2: User 1: User 2: User 2: User 1: User 1: User 2: User 2: Figure 1: An example from CMU DoG dataset (Zhou et al., 2018a). Words in the same color are related. Introduction Building a conversational agent with intelligence has received significant attention with the emergence of personal assistants such as Apple Siri, Google Now and Microsoft Cortana. One approach is to building retrieval-based chatbots, which aims to select a potential response from a set of candidates given the conversation context (Lowe et al., 2015; Wu et al., 2017; Zhou et al., 2018b; Gu et al., 2019a; Tao et al., 2019; Gu et al., 2020a). ∗ Corresponding author. Conversation Hi how are you today? I am good. How are you? Pretty good. Have you seen the inception? No, I have not but have heard of it. What is it about? It’s about extractors that perform experiments using military technology on people to retrieve info about their targets. Sounds interesting. Do you know which actors are in it? I haven’t watched it either or seen a preview. But it’s sciﬁ so it might be good. Ugh Leonardo DiCaprio is the main character. He plays as Don Cobb. I’m not a big sciﬁ fan but there are a few movies I st"
2020.findings-emnlp.127,P82-1020,0,0.662382,"Missing"
2020.findings-emnlp.127,W15-4640,0,0.303826,"table by visualizing the knowledge grounding process. 1 User 2: User 1: User 2: User 1: User 2: User 1: User 2: User 2: User 1: User 1: User 2: User 2: Figure 1: An example from CMU DoG dataset (Zhou et al., 2018a). Words in the same color are related. Introduction Building a conversational agent with intelligence has received significant attention with the emergence of personal assistants such as Apple Siri, Google Now and Microsoft Cortana. One approach is to building retrieval-based chatbots, which aims to select a potential response from a set of candidates given the conversation context (Lowe et al., 2015; Wu et al., 2017; Zhou et al., 2018b; Gu et al., 2019a; Tao et al., 2019; Gu et al., 2020a). ∗ Corresponding author. Conversation Hi how are you today? I am good. How are you? Pretty good. Have you seen the inception? No, I have not but have heard of it. What is it about? It’s about extractors that perform experiments using military technology on people to retrieve info about their targets. Sounds interesting. Do you know which actors are in it? I haven’t watched it either or seen a preview. But it’s sciﬁ so it might be good. Ugh Leonardo DiCaprio is the main character. He plays as Don Cobb."
2020.findings-emnlp.127,D18-1298,0,0.137783,"Missing"
2020.findings-emnlp.127,D14-1162,0,0.0822238,"Missing"
2020.findings-emnlp.127,P19-1001,0,0.158965,"er 2: User 1: User 2: User 1: User 2: User 2: User 1: User 1: User 2: User 2: Figure 1: An example from CMU DoG dataset (Zhou et al., 2018a). Words in the same color are related. Introduction Building a conversational agent with intelligence has received significant attention with the emergence of personal assistants such as Apple Siri, Google Now and Microsoft Cortana. One approach is to building retrieval-based chatbots, which aims to select a potential response from a set of candidates given the conversation context (Lowe et al., 2015; Wu et al., 2017; Zhou et al., 2018b; Gu et al., 2019a; Tao et al., 2019; Gu et al., 2020a). ∗ Corresponding author. Conversation Hi how are you today? I am good. How are you? Pretty good. Have you seen the inception? No, I have not but have heard of it. What is it about? It’s about extractors that perform experiments using military technology on people to retrieve info about their targets. Sounds interesting. Do you know which actors are in it? I haven’t watched it either or seen a preview. But it’s sciﬁ so it might be good. Ugh Leonardo DiCaprio is the main character. He plays as Don Cobb. I’m not a big sciﬁ fan but there are a few movies I still enjoy in that g"
2020.findings-emnlp.127,D13-1096,0,0.58867,"al and cross attentions for representing contexts and knowledge, together with an iteratively referring network for scoring response candidates. (2) Experimental results on two datasets demonstrate that our proposed model outperforms state-of-the-art models on the accuracy of response selection. (3) Empirical analysis further verifies the effectiveness of our proposed method. 2 2.1 Related Work Response Selection Response selection is an important problem in building retrieval-based chatbots. Existing work on response selection can be categorized according to processing single-turn dialogues (Wang et al., 2013) or multi-turn ones (Lowe et al., 2015; Wu et al., 2017; Zhang et al., 2018b; Zhou et al., 2018b; Gu et al., 2019a; Tao et al., 2019; Gu et al., 2020a,b). Recent studies focused on multiturn conversations, a more practical setup for real applications. Wu et al. (2017) proposed the sequential matching network (SMN) which accumulated the utterance-response matching information by 1413 a recurrent neural network. Zhou et al. (2018b) proposed the deep attention matching network (DAM) to construct representations at different granularities with stacked self-attention. Gu et al. (2019a) proposed the"
2020.findings-emnlp.127,P17-1046,0,0.217006,"g the knowledge grounding process. 1 User 2: User 1: User 2: User 1: User 2: User 1: User 2: User 2: User 1: User 1: User 2: User 2: Figure 1: An example from CMU DoG dataset (Zhou et al., 2018a). Words in the same color are related. Introduction Building a conversational agent with intelligence has received significant attention with the emergence of personal assistants such as Apple Siri, Google Now and Microsoft Cortana. One approach is to building retrieval-based chatbots, which aims to select a potential response from a set of candidates given the conversation context (Lowe et al., 2015; Wu et al., 2017; Zhou et al., 2018b; Gu et al., 2019a; Tao et al., 2019; Gu et al., 2020a). ∗ Corresponding author. Conversation Hi how are you today? I am good. How are you? Pretty good. Have you seen the inception? No, I have not but have heard of it. What is it about? It’s about extractors that perform experiments using military technology on people to retrieve info about their targets. Sounds interesting. Do you know which actors are in it? I haven’t watched it either or seen a preview. But it’s sciﬁ so it might be good. Ugh Leonardo DiCaprio is the main character. He plays as Don Cobb. I’m not a big sci"
2020.findings-emnlp.127,P18-1205,0,0.132955,"ledge-Grounded Chatbots Chit-chat models suffer from the lack of explicit long-term memory as they are typically trained to produce an utterance given only a very recent dialogue history. Recently, some studies show that chit-chat models can be more diverse and engaging by conditioning them on the background knowledge. Zhang et al. (2018a) released the PERSONA-CHAT dataset which employs the speakers’ profile information as the background knowledge. Zhou et al. (2018a) built the CMU DoG dataset which adopts the Wikipedia articles about popular movies as the background knowledge. Mazar´e et al. (2018) proposed to pretrain a model using a large-scale corpus based on Reddit. Zhao et al. (2019) proposed the document-grounded matching network (DGMN) which fused each context utterance with each knowledge entry for representing them. Gu et al. (2019b) proposed a dually interactive matching network (DIM) which performed the interactive matching between responses and contexts and between responses and knowledge respectively. The FIRE model proposed in this paper makes two major improvements to the state-of-the-art DIM model (Gu et al., 2019b). First, a context filter and a knowledge filter are bui"
2020.findings-emnlp.127,C18-1317,0,0.0289668,"ith an iteratively referring network for scoring response candidates. (2) Experimental results on two datasets demonstrate that our proposed model outperforms state-of-the-art models on the accuracy of response selection. (3) Empirical analysis further verifies the effectiveness of our proposed method. 2 2.1 Related Work Response Selection Response selection is an important problem in building retrieval-based chatbots. Existing work on response selection can be categorized according to processing single-turn dialogues (Wang et al., 2013) or multi-turn ones (Lowe et al., 2015; Wu et al., 2017; Zhang et al., 2018b; Zhou et al., 2018b; Gu et al., 2019a; Tao et al., 2019; Gu et al., 2020a,b). Recent studies focused on multiturn conversations, a more practical setup for real applications. Wu et al. (2017) proposed the sequential matching network (SMN) which accumulated the utterance-response matching information by 1413 a recurrent neural network. Zhou et al. (2018b) proposed the deep attention matching network (DAM) to construct representations at different granularities with stacked self-attention. Gu et al. (2019a) proposed the interactive matching network (IMN) to perform the bidirectional and global"
2020.findings-emnlp.127,D18-1076,0,0.160413,"ledge-Grounded Chatbots Chit-chat models suffer from the lack of explicit long-term memory as they are typically trained to produce an utterance given only a very recent dialogue history. Recently, some studies show that chit-chat models can be more diverse and engaging by conditioning them on the background knowledge. Zhang et al. (2018a) released the PERSONA-CHAT dataset which employs the speakers’ profile information as the background knowledge. Zhou et al. (2018a) built the CMU DoG dataset which adopts the Wikipedia articles about popular movies as the background knowledge. Mazar´e et al. (2018) proposed to pretrain a model using a large-scale corpus based on Reddit. Zhao et al. (2019) proposed the document-grounded matching network (DGMN) which fused each context utterance with each knowledge entry for representing them. Gu et al. (2019b) proposed a dually interactive matching network (DIM) which performed the interactive matching between responses and contexts and between responses and knowledge respectively. The FIRE model proposed in this paper makes two major improvements to the state-of-the-art DIM model (Gu et al., 2019b). First, a context filter and a knowledge filter are bui"
2020.findings-emnlp.127,P18-1103,0,0.0889667,"ferring network for scoring response candidates. (2) Experimental results on two datasets demonstrate that our proposed model outperforms state-of-the-art models on the accuracy of response selection. (3) Empirical analysis further verifies the effectiveness of our proposed method. 2 2.1 Related Work Response Selection Response selection is an important problem in building retrieval-based chatbots. Existing work on response selection can be categorized according to processing single-turn dialogues (Wang et al., 2013) or multi-turn ones (Lowe et al., 2015; Wu et al., 2017; Zhang et al., 2018b; Zhou et al., 2018b; Gu et al., 2019a; Tao et al., 2019; Gu et al., 2020a,b). Recent studies focused on multiturn conversations, a more practical setup for real applications. Wu et al. (2017) proposed the sequential matching network (SMN) which accumulated the utterance-response matching information by 1413 a recurrent neural network. Zhou et al. (2018b) proposed the deep attention matching network (DAM) to construct representations at different granularities with stacked self-attention. Gu et al. (2019a) proposed the interactive matching network (IMN) to perform the bidirectional and global interactions betwee"
2020.semeval-1.39,2020.semeval-1.63,0,0.0364258,"atures from text-to-image generation using Gradient Boosted Decision Tree, and give better results in post-evaluation. • Qiaoning (Liu, 2020) and JUST (Mohammed and Abdullah, 2020) use several ensembles of BERT, ALBERT, XLNet and RoBERTa. • BUT-FIT (Jon et al., 2020), LMVE (Liu et al., 2020), Lijunyi (Li et al., 2020) use ALBERT as the encoder. BUT-FIT uses back-translation from Czech for data augmentation, and LMVE uses hint sentences, back-translation from French and intra-subtask transfer learning between Subtasks A and B to enhance their system. • UAICS (Cusmuliuc et al., 2020), DEEPYANG (Bai and Zhou, 2020), YNU-oxz (Ou et al., 2020), KDE-SenseForce (Mendbayar and Aono, 2020), CUHK (Wang et al., 2020) JBNU (Na and Lee, 2020), SWAGex (Rim and Okazaki, 2020) are BERT-based. JBNU puts an BiLSTM on top of BERT, and SWAGex finetunes BERT with SWAG data. CUHK uses a Multitask Learning framework MTDNN (Liu et al., 2019b), adopting the “Explain, Reason and Predict” system. 313 Team Human BUT-FIT Solomon KaLM panaali* JUSTers cdjhz* JBNU ANA LMVE+ BLEU 22.4 19.3 18.5 17.2 16.1 16.0 15.9 15.7 12.9 Rank 1 2 3 4 5 6 7 8 9 Human 2.58 1.84 1.84 2.08 1.22 1.94 1.75 1.80 2.10 1.78 Rank 4 4 2 11 3 8 6 1 7 Team C"
2020.semeval-1.39,2020.acl-main.130,1,0.90634,"ts to generate natural language statements that obey or violate their commonsense knowledge about the world. 1 Many existing tasks embed the evaluation of commonsense understanding in other tasks such as coreference resolution (Levesque et al., 2012; Morgenstern and Ortiz, 2015), subsequent event prediction (Roemmele et al., 2011), ordinal common-sense inference (Zhang et al., 2017), situations with adversarial generations (Zellers et al., 2018), event validation (Wang et al., 2018), reading comprehension (Mostafazadeh et al., 2016; Ostermann et al., 2018b; Ostermann et al., 2018a), dialogue (Cui et al., 2020) and QA (Davis, 2016; Talmor et al., 2018; Mihaylov et al., 2018). They verify whether a system is equipped with common sense by testing whether the system can give a correct answer when the input does not contain such knowledge. The above tasks do not directly evaluate commonsense validation and they do not explicitly identify the key factor required in a commonsense validation process. The SemEval-2020 Task 4 includes three subtasks on testing whether a system can distinguish natural language statements that make sense from those that do not, and probe the reasons. In the first subtask, a *"
2020.semeval-1.39,2020.semeval-1.77,0,0.0358153,"RoBERTa features with additional features from text-to-image generation using Gradient Boosted Decision Tree, and give better results in post-evaluation. • Qiaoning (Liu, 2020) and JUST (Mohammed and Abdullah, 2020) use several ensembles of BERT, ALBERT, XLNet and RoBERTa. • BUT-FIT (Jon et al., 2020), LMVE (Liu et al., 2020), Lijunyi (Li et al., 2020) use ALBERT as the encoder. BUT-FIT uses back-translation from Czech for data augmentation, and LMVE uses hint sentences, back-translation from French and intra-subtask transfer learning between Subtasks A and B to enhance their system. • UAICS (Cusmuliuc et al., 2020), DEEPYANG (Bai and Zhou, 2020), YNU-oxz (Ou et al., 2020), KDE-SenseForce (Mendbayar and Aono, 2020), CUHK (Wang et al., 2020) JBNU (Na and Lee, 2020), SWAGex (Rim and Okazaki, 2020) are BERT-based. JBNU puts an BiLSTM on top of BERT, and SWAGex finetunes BERT with SWAG data. CUHK uses a Multitask Learning framework MTDNN (Liu et al., 2019b), adopting the “Explain, Reason and Predict” system. 313 Team Human BUT-FIT Solomon KaLM panaali* JUSTers cdjhz* JBNU ANA LMVE+ BLEU 22.4 19.3 18.5 17.2 16.1 16.0 15.9 15.7 12.9 Rank 1 2 3 4 5 6 7 8 9 Human 2.58 1.84 1.84 2.08 1.22 1.94 1.75 1.80 2.10 1.78"
2020.semeval-1.39,2020.semeval-1.61,0,0.0336443,"75.8 10 11 12 13 14 15 16 17 18 Team Acc. Rank Masked Reasoner KDE SenseForce SSN-NLP TakeLab* UoR dania* CUHK bhu* praveenjoshi007* 73.5 72.8 68.3 66.8 65.9 55.5 51.2 36.4 32.6 19 20 21 22 23 24 25 26 27 Table 5: Subtask B results of all the submitted systems. Those marked with * did not submit system description paper. Human performance are based on the trial data instead of the test data used by the participating systems. al., 2002). They also explore several prompt templates to constructs as the inputs to the model. • Solomon (Srivastava et al., 2020), KaLM (Wan and Huang, 2020), CS-NET (Dash et al., 2020), JUSTers (Fadel et al., 2020), CS-NLP (Saeedi et al., 2020), UI (Doxolodeo and Mahendra, 2020), TR (Teo, 2020) UoR (Markchom et al., 2020), Masked Reasoner (Lu, 2020) have similar model architecture, with RoBERTa as the encoder. In addition, UoR finetunes the pretrained language model on NLI and STS dataset, and UI finetunes on MNLI data. TR combines RoBERTa features with additional features from text-to-image generation using Gradient Boosted Decision Tree, and give better results in post-evaluation. • Qiaoning (Liu, 2020) and JUST (Mohammed and Abdullah, 2020) use several ensembles of BERT,"
2020.semeval-1.39,2020.semeval-1.78,0,0.130264,"LP TakeLab* UoR dania* CUHK bhu* praveenjoshi007* 73.5 72.8 68.3 66.8 65.9 55.5 51.2 36.4 32.6 19 20 21 22 23 24 25 26 27 Table 5: Subtask B results of all the submitted systems. Those marked with * did not submit system description paper. Human performance are based on the trial data instead of the test data used by the participating systems. al., 2002). They also explore several prompt templates to constructs as the inputs to the model. • Solomon (Srivastava et al., 2020), KaLM (Wan and Huang, 2020), CS-NET (Dash et al., 2020), JUSTers (Fadel et al., 2020), CS-NLP (Saeedi et al., 2020), UI (Doxolodeo and Mahendra, 2020), TR (Teo, 2020) UoR (Markchom et al., 2020), Masked Reasoner (Lu, 2020) have similar model architecture, with RoBERTa as the encoder. In addition, UoR finetunes the pretrained language model on NLI and STS dataset, and UI finetunes on MNLI data. TR combines RoBERTa features with additional features from text-to-image generation using Gradient Boosted Decision Tree, and give better results in post-evaluation. • Qiaoning (Liu, 2020) and JUST (Mohammed and Abdullah, 2020) use several ensembles of BERT, ALBERT, XLNet and RoBERTa. • BUT-FIT (Jon et al., 2020), LMVE (Liu et al., 2020), Lijunyi (Li"
2020.semeval-1.39,2020.semeval-1.66,0,0.186821,"18 Team Acc. Rank Masked Reasoner KDE SenseForce SSN-NLP TakeLab* UoR dania* CUHK bhu* praveenjoshi007* 73.5 72.8 68.3 66.8 65.9 55.5 51.2 36.4 32.6 19 20 21 22 23 24 25 26 27 Table 5: Subtask B results of all the submitted systems. Those marked with * did not submit system description paper. Human performance are based on the trial data instead of the test data used by the participating systems. al., 2002). They also explore several prompt templates to constructs as the inputs to the model. • Solomon (Srivastava et al., 2020), KaLM (Wan and Huang, 2020), CS-NET (Dash et al., 2020), JUSTers (Fadel et al., 2020), CS-NLP (Saeedi et al., 2020), UI (Doxolodeo and Mahendra, 2020), TR (Teo, 2020) UoR (Markchom et al., 2020), Masked Reasoner (Lu, 2020) have similar model architecture, with RoBERTa as the encoder. In addition, UoR finetunes the pretrained language model on NLI and STS dataset, and UI finetunes on MNLI data. TR combines RoBERTa features with additional features from text-to-image generation using Gradient Boosted Decision Tree, and give better results in post-evaluation. • Qiaoning (Liu, 2020) and JUST (Mohammed and Abdullah, 2020) use several ensembles of BERT, ALBERT, XLNet and RoBERTa. •"
2020.semeval-1.39,P17-1025,0,0.0502593,"gue reasoning in the commonsense area (Cui et al., 2020). Those questions are not easy to answer without specializing certain domain knowledge, while our questions are based on daily common sense. Some datasets focus on non-sentential eventual plausibility (Wang et al., 2018; Porada et al., 2019), such as “gorilla-ride-camel”. In contrast, our dataset is based on statements which includes events, descriptions, assertion etc, not merely events, such as “China’s territory is larger than Japan’s”. And some datasets concentrate on limited attributes or actions of world knowledge, such as physics (Forbes and Choi, 2017). Our dataset concerns general commonsense knowledge beyond just physical common sense, the sentence in our task “Tom’s mom become (happy)/(upset) when Tom gets high grades in the exam” is about social and emotional common sense. For our first task, those statements that conforms to commonsense can also be phrased as being plausible. Thus our first task is similar to plausibility tests, despite that plausibility has a broader scope while our focus is on commonsense only. More importantly, compared with our work, the above tasks do not directly estimate general common sense or ask the logical r"
2020.semeval-1.39,2020.semeval-1.46,0,0.1313,"(Saeedi et al., 2020), UI (Doxolodeo and Mahendra, 2020), TR (Teo, 2020) UoR (Markchom et al., 2020), Masked Reasoner (Lu, 2020) have similar model architecture, with RoBERTa as the encoder. In addition, UoR finetunes the pretrained language model on NLI and STS dataset, and UI finetunes on MNLI data. TR combines RoBERTa features with additional features from text-to-image generation using Gradient Boosted Decision Tree, and give better results in post-evaluation. • Qiaoning (Liu, 2020) and JUST (Mohammed and Abdullah, 2020) use several ensembles of BERT, ALBERT, XLNet and RoBERTa. • BUT-FIT (Jon et al., 2020), LMVE (Liu et al., 2020), Lijunyi (Li et al., 2020) use ALBERT as the encoder. BUT-FIT uses back-translation from Czech for data augmentation, and LMVE uses hint sentences, back-translation from French and intra-subtask transfer learning between Subtasks A and B to enhance their system. • UAICS (Cusmuliuc et al., 2020), DEEPYANG (Bai and Zhou, 2020), YNU-oxz (Ou et al., 2020), KDE-SenseForce (Mendbayar and Aono, 2020), CUHK (Wang et al., 2020) JBNU (Na and Lee, 2020), SWAGex (Rim and Okazaki, 2020) are BERT-based. JBNU puts an BiLSTM on top of BERT, and SWAGex finetunes BERT with SWAG data. C"
2020.semeval-1.39,2020.semeval-1.45,0,0.0262627,"on these sequences with a next token prediction objective. At the test time, based on the statement, the model generates the reason tokens until the end-of-sentence token is generated. • KaLM (Wan and Huang, 2020) uses the sequence-to-sequence architecture BART. To enhance the source side statement, they extract keywords from the statement and search for evidence from Wiktionary.2 After that, they concatenate the evidence along with the original statement as the source sentence for the generation. This approach proves effective and makes their system second-best for human evaluations. • ANA (Konar et al., 2020) has the highest human evaluation score with a multitask learning framework. Specifically, they use a decoder-only transformer based on GPT-2 as the backbone model, and train 2 Wiktionary version: enwiktionary-20200220 314 the model with two self-attention heads: one for language models and another for classification. They then use data from both task B and task C to calculate language model loss and classification loss. Furthermore, they use OMCS at the pretraining stage and use CoS-E (Rajani et al., 2019) and OpenBook (Mihaylov et al., 2018) at the task-specific training stage. • Solomon (Sr"
2020.semeval-1.39,2020.semeval-1.69,0,0.0316577,"20), TR (Teo, 2020) UoR (Markchom et al., 2020), Masked Reasoner (Lu, 2020) have similar model architecture, with RoBERTa as the encoder. In addition, UoR finetunes the pretrained language model on NLI and STS dataset, and UI finetunes on MNLI data. TR combines RoBERTa features with additional features from text-to-image generation using Gradient Boosted Decision Tree, and give better results in post-evaluation. • Qiaoning (Liu, 2020) and JUST (Mohammed and Abdullah, 2020) use several ensembles of BERT, ALBERT, XLNet and RoBERTa. • BUT-FIT (Jon et al., 2020), LMVE (Liu et al., 2020), Lijunyi (Li et al., 2020) use ALBERT as the encoder. BUT-FIT uses back-translation from Czech for data augmentation, and LMVE uses hint sentences, back-translation from French and intra-subtask transfer learning between Subtasks A and B to enhance their system. • UAICS (Cusmuliuc et al., 2020), DEEPYANG (Bai and Zhou, 2020), YNU-oxz (Ou et al., 2020), KDE-SenseForce (Mendbayar and Aono, 2020), CUHK (Wang et al., 2020) JBNU (Na and Lee, 2020), SWAGex (Rim and Okazaki, 2020) are BERT-based. JBNU puts an BiLSTM on top of BERT, and SWAGex finetunes BERT with SWAG data. CUHK uses a Multitask Learning framework MTDNN (Liu e"
2020.semeval-1.39,P19-1441,0,0.13007,"his figure is mostly based on Team Solomon’s system. For Subtask B and C, the connector can be simply “No, ”, to help in constraining the model to learn a choice that explains the unreasonability of the statement. For Subtask A and B, the pretrained models are finetuned on the task-specific data with MLM-objective, and then trained as a binary classification task to score each input. For Subtask C, the cross-entropy loss of next-token-prediction is used to train the model, and beam search is used at inference. adopted the pretrained language models such as BERT (Devlin et al., 2018), RoBERTa (Liu et al., 2019c), XLNET (Yang et al., 2019) and ALBERT (Lan et al., 2019) as the encoder of the model, and then finetune on the training set of the task. See Figure 1 for the most commonly-used model architectures for Subtask A and B. Also, the top-performing systems take advantage of external knowledge graphs such as ConceptNet (Speer et al., 2017), or unstructured text containing commonsense knowledge. Below we introduce in detail several top-performing systems and their main features. • CN-HIT-IT.NLP (Zhang et al., 2020) ranks top in Subtask A. They use a variant of K-BERT (Liu et al., 2019a) as the enco"
2020.semeval-1.39,2020.semeval-1.70,0,0.0390468,"(Doxolodeo and Mahendra, 2020), TR (Teo, 2020) UoR (Markchom et al., 2020), Masked Reasoner (Lu, 2020) have similar model architecture, with RoBERTa as the encoder. In addition, UoR finetunes the pretrained language model on NLI and STS dataset, and UI finetunes on MNLI data. TR combines RoBERTa features with additional features from text-to-image generation using Gradient Boosted Decision Tree, and give better results in post-evaluation. • Qiaoning (Liu, 2020) and JUST (Mohammed and Abdullah, 2020) use several ensembles of BERT, ALBERT, XLNet and RoBERTa. • BUT-FIT (Jon et al., 2020), LMVE (Liu et al., 2020), Lijunyi (Li et al., 2020) use ALBERT as the encoder. BUT-FIT uses back-translation from Czech for data augmentation, and LMVE uses hint sentences, back-translation from French and intra-subtask transfer learning between Subtasks A and B to enhance their system. • UAICS (Cusmuliuc et al., 2020), DEEPYANG (Bai and Zhou, 2020), YNU-oxz (Ou et al., 2020), KDE-SenseForce (Mendbayar and Aono, 2020), CUHK (Wang et al., 2020) JBNU (Na and Lee, 2020), SWAGex (Rim and Okazaki, 2020) are BERT-based. JBNU puts an BiLSTM on top of BERT, and SWAGex finetunes BERT with SWAG data. CUHK uses a Multitask Lear"
2020.semeval-1.39,2020.semeval-1.50,0,0.0260087,"Srivastava et al., 2020), KaLM (Wan and Huang, 2020), CS-NET (Dash et al., 2020), JUSTers (Fadel et al., 2020), CS-NLP (Saeedi et al., 2020), UI (Doxolodeo and Mahendra, 2020), TR (Teo, 2020) UoR (Markchom et al., 2020), Masked Reasoner (Lu, 2020) have similar model architecture, with RoBERTa as the encoder. In addition, UoR finetunes the pretrained language model on NLI and STS dataset, and UI finetunes on MNLI data. TR combines RoBERTa features with additional features from text-to-image generation using Gradient Boosted Decision Tree, and give better results in post-evaluation. • Qiaoning (Liu, 2020) and JUST (Mohammed and Abdullah, 2020) use several ensembles of BERT, ALBERT, XLNet and RoBERTa. • BUT-FIT (Jon et al., 2020), LMVE (Liu et al., 2020), Lijunyi (Li et al., 2020) use ALBERT as the encoder. BUT-FIT uses back-translation from Czech for data augmentation, and LMVE uses hint sentences, back-translation from French and intra-subtask transfer learning between Subtasks A and B to enhance their system. • UAICS (Cusmuliuc et al., 2020), DEEPYANG (Bai and Zhou, 2020), YNU-oxz (Ou et al., 2020), KDE-SenseForce (Mendbayar and Aono, 2020), CUHK (Wang et al., 2020) JBNU (Na and Lee, 2020),"
2020.semeval-1.39,2020.semeval-1.49,0,0.038915,".6 19 20 21 22 23 24 25 26 27 Table 5: Subtask B results of all the submitted systems. Those marked with * did not submit system description paper. Human performance are based on the trial data instead of the test data used by the participating systems. al., 2002). They also explore several prompt templates to constructs as the inputs to the model. • Solomon (Srivastava et al., 2020), KaLM (Wan and Huang, 2020), CS-NET (Dash et al., 2020), JUSTers (Fadel et al., 2020), CS-NLP (Saeedi et al., 2020), UI (Doxolodeo and Mahendra, 2020), TR (Teo, 2020) UoR (Markchom et al., 2020), Masked Reasoner (Lu, 2020) have similar model architecture, with RoBERTa as the encoder. In addition, UoR finetunes the pretrained language model on NLI and STS dataset, and UI finetunes on MNLI data. TR combines RoBERTa features with additional features from text-to-image generation using Gradient Boosted Decision Tree, and give better results in post-evaluation. • Qiaoning (Liu, 2020) and JUST (Mohammed and Abdullah, 2020) use several ensembles of BERT, ALBERT, XLNet and RoBERTa. • BUT-FIT (Jon et al., 2020), LMVE (Liu et al., 2020), Lijunyi (Li et al., 2020) use ALBERT as the encoder. BUT-FIT uses back-translation f"
2020.semeval-1.39,2020.semeval-1.52,0,0.0366318,"3.5 72.8 68.3 66.8 65.9 55.5 51.2 36.4 32.6 19 20 21 22 23 24 25 26 27 Table 5: Subtask B results of all the submitted systems. Those marked with * did not submit system description paper. Human performance are based on the trial data instead of the test data used by the participating systems. al., 2002). They also explore several prompt templates to constructs as the inputs to the model. • Solomon (Srivastava et al., 2020), KaLM (Wan and Huang, 2020), CS-NET (Dash et al., 2020), JUSTers (Fadel et al., 2020), CS-NLP (Saeedi et al., 2020), UI (Doxolodeo and Mahendra, 2020), TR (Teo, 2020) UoR (Markchom et al., 2020), Masked Reasoner (Lu, 2020) have similar model architecture, with RoBERTa as the encoder. In addition, UoR finetunes the pretrained language model on NLI and STS dataset, and UI finetunes on MNLI data. TR combines RoBERTa features with additional features from text-to-image generation using Gradient Boosted Decision Tree, and give better results in post-evaluation. • Qiaoning (Liu, 2020) and JUST (Mohammed and Abdullah, 2020) use several ensembles of BERT, ALBERT, XLNet and RoBERTa. • BUT-FIT (Jon et al., 2020), LMVE (Liu et al., 2020), Lijunyi (Li et al., 2020) use ALBERT as the encoder. BUT"
2020.semeval-1.39,D18-1260,0,0.367889,"ate their commonsense knowledge about the world. 1 Many existing tasks embed the evaluation of commonsense understanding in other tasks such as coreference resolution (Levesque et al., 2012; Morgenstern and Ortiz, 2015), subsequent event prediction (Roemmele et al., 2011), ordinal common-sense inference (Zhang et al., 2017), situations with adversarial generations (Zellers et al., 2018), event validation (Wang et al., 2018), reading comprehension (Mostafazadeh et al., 2016; Ostermann et al., 2018b; Ostermann et al., 2018a), dialogue (Cui et al., 2020) and QA (Davis, 2016; Talmor et al., 2018; Mihaylov et al., 2018). They verify whether a system is equipped with common sense by testing whether the system can give a correct answer when the input does not contain such knowledge. The above tasks do not directly evaluate commonsense validation and they do not explicitly identify the key factor required in a commonsense validation process. The SemEval-2020 Task 4 includes three subtasks on testing whether a system can distinguish natural language statements that make sense from those that do not, and probe the reasons. In the first subtask, a * Equal Contribution This work is licensed under a Creative Commons"
2020.semeval-1.39,2020.semeval-1.75,0,0.0348917,"20), KaLM (Wan and Huang, 2020), CS-NET (Dash et al., 2020), JUSTers (Fadel et al., 2020), CS-NLP (Saeedi et al., 2020), UI (Doxolodeo and Mahendra, 2020), TR (Teo, 2020) UoR (Markchom et al., 2020), Masked Reasoner (Lu, 2020) have similar model architecture, with RoBERTa as the encoder. In addition, UoR finetunes the pretrained language model on NLI and STS dataset, and UI finetunes on MNLI data. TR combines RoBERTa features with additional features from text-to-image generation using Gradient Boosted Decision Tree, and give better results in post-evaluation. • Qiaoning (Liu, 2020) and JUST (Mohammed and Abdullah, 2020) use several ensembles of BERT, ALBERT, XLNet and RoBERTa. • BUT-FIT (Jon et al., 2020), LMVE (Liu et al., 2020), Lijunyi (Li et al., 2020) use ALBERT as the encoder. BUT-FIT uses back-translation from Czech for data augmentation, and LMVE uses hint sentences, back-translation from French and intra-subtask transfer learning between Subtasks A and B to enhance their system. • UAICS (Cusmuliuc et al., 2020), DEEPYANG (Bai and Zhou, 2020), YNU-oxz (Ou et al., 2020), KDE-SenseForce (Mendbayar and Aono, 2020), CUHK (Wang et al., 2020) JBNU (Na and Lee, 2020), SWAGex (Rim and Okazaki, 2020) are BERT"
2020.semeval-1.39,N16-1098,0,0.222829,"e. In our task, we take an operational definition of making sense by asking human subjects to generate natural language statements that obey or violate their commonsense knowledge about the world. 1 Many existing tasks embed the evaluation of commonsense understanding in other tasks such as coreference resolution (Levesque et al., 2012; Morgenstern and Ortiz, 2015), subsequent event prediction (Roemmele et al., 2011), ordinal common-sense inference (Zhang et al., 2017), situations with adversarial generations (Zellers et al., 2018), event validation (Wang et al., 2018), reading comprehension (Mostafazadeh et al., 2016; Ostermann et al., 2018b; Ostermann et al., 2018a), dialogue (Cui et al., 2020) and QA (Davis, 2016; Talmor et al., 2018; Mihaylov et al., 2018). They verify whether a system is equipped with common sense by testing whether the system can give a correct answer when the input does not contain such knowledge. The above tasks do not directly evaluate commonsense validation and they do not explicitly identify the key factor required in a commonsense validation process. The SemEval-2020 Task 4 includes three subtasks on testing whether a system can distinguish natural language statements that make"
2020.semeval-1.39,2020.semeval-1.65,0,0.193562,"iaoning (Liu, 2020) and JUST (Mohammed and Abdullah, 2020) use several ensembles of BERT, ALBERT, XLNet and RoBERTa. • BUT-FIT (Jon et al., 2020), LMVE (Liu et al., 2020), Lijunyi (Li et al., 2020) use ALBERT as the encoder. BUT-FIT uses back-translation from Czech for data augmentation, and LMVE uses hint sentences, back-translation from French and intra-subtask transfer learning between Subtasks A and B to enhance their system. • UAICS (Cusmuliuc et al., 2020), DEEPYANG (Bai and Zhou, 2020), YNU-oxz (Ou et al., 2020), KDE-SenseForce (Mendbayar and Aono, 2020), CUHK (Wang et al., 2020) JBNU (Na and Lee, 2020), SWAGex (Rim and Okazaki, 2020) are BERT-based. JBNU puts an BiLSTM on top of BERT, and SWAGex finetunes BERT with SWAG data. CUHK uses a Multitask Learning framework MTDNN (Liu et al., 2019b), adopting the “Explain, Reason and Predict” system. 313 Team Human BUT-FIT Solomon KaLM panaali* JUSTers cdjhz* JBNU ANA LMVE+ BLEU 22.4 19.3 18.5 17.2 16.1 16.0 15.9 15.7 12.9 Rank 1 2 3 4 5 6 7 8 9 Human 2.58 1.84 1.84 2.08 1.22 1.94 1.75 1.80 2.10 1.78 Rank 4 4 2 11 3 8 6 1 7 Team CN-HIT-IT.NLP+ SWAGex UI TMLab* CUHK SSN-NLP UoR+ Masked Reasoner+ BLEU Rank Human Rank 9.7 7.1 5.5 5.4 4.3 2.2 0.9 0.6 1"
2020.semeval-1.39,P19-1459,0,0.0624979,"Missing"
2020.semeval-1.39,L18-1564,0,0.169229,"operational definition of making sense by asking human subjects to generate natural language statements that obey or violate their commonsense knowledge about the world. 1 Many existing tasks embed the evaluation of commonsense understanding in other tasks such as coreference resolution (Levesque et al., 2012; Morgenstern and Ortiz, 2015), subsequent event prediction (Roemmele et al., 2011), ordinal common-sense inference (Zhang et al., 2017), situations with adversarial generations (Zellers et al., 2018), event validation (Wang et al., 2018), reading comprehension (Mostafazadeh et al., 2016; Ostermann et al., 2018b; Ostermann et al., 2018a), dialogue (Cui et al., 2020) and QA (Davis, 2016; Talmor et al., 2018; Mihaylov et al., 2018). They verify whether a system is equipped with common sense by testing whether the system can give a correct answer when the input does not contain such knowledge. The above tasks do not directly evaluate commonsense validation and they do not explicitly identify the key factor required in a commonsense validation process. The SemEval-2020 Task 4 includes three subtasks on testing whether a system can distinguish natural language statements that make sense from those that d"
2020.semeval-1.39,S18-1119,0,0.191276,"operational definition of making sense by asking human subjects to generate natural language statements that obey or violate their commonsense knowledge about the world. 1 Many existing tasks embed the evaluation of commonsense understanding in other tasks such as coreference resolution (Levesque et al., 2012; Morgenstern and Ortiz, 2015), subsequent event prediction (Roemmele et al., 2011), ordinal common-sense inference (Zhang et al., 2017), situations with adversarial generations (Zellers et al., 2018), event validation (Wang et al., 2018), reading comprehension (Mostafazadeh et al., 2016; Ostermann et al., 2018b; Ostermann et al., 2018a), dialogue (Cui et al., 2020) and QA (Davis, 2016; Talmor et al., 2018; Mihaylov et al., 2018). They verify whether a system is equipped with common sense by testing whether the system can give a correct answer when the input does not contain such knowledge. The above tasks do not directly evaluate commonsense validation and they do not explicitly identify the key factor required in a commonsense validation process. The SemEval-2020 Task 4 includes three subtasks on testing whether a system can distinguish natural language statements that make sense from those that d"
2020.semeval-1.39,2020.semeval-1.80,0,0.0328145,"ration using Gradient Boosted Decision Tree, and give better results in post-evaluation. • Qiaoning (Liu, 2020) and JUST (Mohammed and Abdullah, 2020) use several ensembles of BERT, ALBERT, XLNet and RoBERTa. • BUT-FIT (Jon et al., 2020), LMVE (Liu et al., 2020), Lijunyi (Li et al., 2020) use ALBERT as the encoder. BUT-FIT uses back-translation from Czech for data augmentation, and LMVE uses hint sentences, back-translation from French and intra-subtask transfer learning between Subtasks A and B to enhance their system. • UAICS (Cusmuliuc et al., 2020), DEEPYANG (Bai and Zhou, 2020), YNU-oxz (Ou et al., 2020), KDE-SenseForce (Mendbayar and Aono, 2020), CUHK (Wang et al., 2020) JBNU (Na and Lee, 2020), SWAGex (Rim and Okazaki, 2020) are BERT-based. JBNU puts an BiLSTM on top of BERT, and SWAGex finetunes BERT with SWAG data. CUHK uses a Multitask Learning framework MTDNN (Liu et al., 2019b), adopting the “Explain, Reason and Predict” system. 313 Team Human BUT-FIT Solomon KaLM panaali* JUSTers cdjhz* JBNU ANA LMVE+ BLEU 22.4 19.3 18.5 17.2 16.1 16.0 15.9 15.7 12.9 Rank 1 2 3 4 5 6 7 8 9 Human 2.58 1.84 1.84 2.08 1.22 1.94 1.75 1.80 2.10 1.78 Rank 4 4 2 11 3 8 6 1 7 Team CN-HIT-IT.NLP+ SWAGex UI TML"
2020.semeval-1.39,P02-1040,0,0.114832,"Missing"
2020.semeval-1.39,D19-6015,0,0.0352032,"t elementary level science that are related to the questions. The AI2 Reasoning Challenge (ARC) (Clark et al., 2018) gives thousands of questions with different knowledge types, as well as a relevant 14M-sentence corpus, mixed with science facts and other narrative sentences. MuTual provides a dataset for Multi-Turn dialogue reasoning in the commonsense area (Cui et al., 2020). Those questions are not easy to answer without specializing certain domain knowledge, while our questions are based on daily common sense. Some datasets focus on non-sentential eventual plausibility (Wang et al., 2018; Porada et al., 2019), such as “gorilla-ride-camel”. In contrast, our dataset is based on statements which includes events, descriptions, assertion etc, not merely events, such as “China’s territory is larger than Japan’s”. And some datasets concentrate on limited attributes or actions of world knowledge, such as physics (Forbes and Choi, 2017). Our dataset concerns general commonsense knowledge beyond just physical common sense, the sentence in our task “Tom’s mom become (happy)/(upset) when Tom gets high grades in the exam” is about social and emotional common sense. For our first task, those statements that con"
2020.semeval-1.39,P19-1487,0,0.0441511,"proach proves effective and makes their system second-best for human evaluations. • ANA (Konar et al., 2020) has the highest human evaluation score with a multitask learning framework. Specifically, they use a decoder-only transformer based on GPT-2 as the backbone model, and train 2 Wiktionary version: enwiktionary-20200220 314 the model with two self-attention heads: one for language models and another for classification. They then use data from both task B and task C to calculate language model loss and classification loss. Furthermore, they use OMCS at the pretraining stage and use CoS-E (Rajani et al., 2019) and OpenBook (Mihaylov et al., 2018) at the task-specific training stage. • Solomon (Srivastava et al., 2020), JUSTers (Fadel et al., 2020), SWAGex (Rim and Okazaki, 2020), UI (Doxolodeo and Mahendra, 2020) and CUHK (Wang et al., 2020) use GPT or GPT-2 finetuned on the task training data. JBNU (Na and Lee, 2020) uses UniLM, which incorporates three LM tasks: unidirectional LM, bidirectional LM and sequence-to-sequence prediction LM, and only use one of the reference correct reasons. UI does not use the training data and treats the generation as a Cloze task. SSN-NLP (S, 2020) uses the seq2seq"
2020.semeval-1.39,P18-1043,0,0.0303324,"sts, despite that plausibility has a broader scope while our focus is on commonsense only. More importantly, compared with our work, the above tasks do not directly estimate general common sense or ask the logical reasons behind the correct answers and questions. In recent years, some large-scale commonsense inference knowledge resources have been developed, which may be helpful in commonsense reasoning tasks. Atomic (Sap et al., 2018) presents a large-scale everyday commonsense knowledge graph, which has nine if-then relations with variables, including causes, effects, and so on. Event2Mind (Rashkin et al., 2018) proposes a new corpus and task, aiming to find out the mentioned/unmentioned people’s intents and reactions under various daily circumstances. These datasets are not directly useful for our benchmark since they focus only on a small domain. ConceptNet is a seminal knowledge graph that has been upgraded over time (Liu and Singh, 2004; Havasi et al., 2007; Speer and Havasi, 2013; Speer et al., 2017). ConceptNet constructs triples using labeled edges as relations and various words and/or phrases as entities. It also has the sentences describing the corresponding triples. In contrast to these dat"
2020.semeval-1.39,2020.semeval-1.51,0,0.208096,"T (Mohammed and Abdullah, 2020) use several ensembles of BERT, ALBERT, XLNet and RoBERTa. • BUT-FIT (Jon et al., 2020), LMVE (Liu et al., 2020), Lijunyi (Li et al., 2020) use ALBERT as the encoder. BUT-FIT uses back-translation from Czech for data augmentation, and LMVE uses hint sentences, back-translation from French and intra-subtask transfer learning between Subtasks A and B to enhance their system. • UAICS (Cusmuliuc et al., 2020), DEEPYANG (Bai and Zhou, 2020), YNU-oxz (Ou et al., 2020), KDE-SenseForce (Mendbayar and Aono, 2020), CUHK (Wang et al., 2020) JBNU (Na and Lee, 2020), SWAGex (Rim and Okazaki, 2020) are BERT-based. JBNU puts an BiLSTM on top of BERT, and SWAGex finetunes BERT with SWAG data. CUHK uses a Multitask Learning framework MTDNN (Liu et al., 2019b), adopting the “Explain, Reason and Predict” system. 313 Team Human BUT-FIT Solomon KaLM panaali* JUSTers cdjhz* JBNU ANA LMVE+ BLEU 22.4 19.3 18.5 17.2 16.1 16.0 15.9 15.7 12.9 Rank 1 2 3 4 5 6 7 8 9 Human 2.58 1.84 1.84 2.08 1.22 1.94 1.75 1.80 2.10 1.78 Rank 4 4 2 11 3 8 6 1 7 Team CN-HIT-IT.NLP+ SWAGex UI TMLab* CUHK SSN-NLP UoR+ Masked Reasoner+ BLEU Rank Human Rank 9.7 7.1 5.5 5.4 4.3 2.2 0.9 0.6 10 11 12 13 14 15 16 17 1.74 1.75"
2020.semeval-1.39,2020.semeval-1.73,0,0.0561971,"Missing"
2020.semeval-1.39,2020.semeval-1.62,0,0.0343034,"soner KDE SenseForce SSN-NLP TakeLab* UoR dania* CUHK bhu* praveenjoshi007* 73.5 72.8 68.3 66.8 65.9 55.5 51.2 36.4 32.6 19 20 21 22 23 24 25 26 27 Table 5: Subtask B results of all the submitted systems. Those marked with * did not submit system description paper. Human performance are based on the trial data instead of the test data used by the participating systems. al., 2002). They also explore several prompt templates to constructs as the inputs to the model. • Solomon (Srivastava et al., 2020), KaLM (Wan and Huang, 2020), CS-NET (Dash et al., 2020), JUSTers (Fadel et al., 2020), CS-NLP (Saeedi et al., 2020), UI (Doxolodeo and Mahendra, 2020), TR (Teo, 2020) UoR (Markchom et al., 2020), Masked Reasoner (Lu, 2020) have similar model architecture, with RoBERTa as the encoder. In addition, UoR finetunes the pretrained language model on NLI and STS dataset, and UI finetunes on MNLI data. TR combines RoBERTa features with additional features from text-to-image generation using Gradient Boosted Decision Tree, and give better results in post-evaluation. • Qiaoning (Liu, 2020) and JUST (Mohammed and Abdullah, 2020) use several ensembles of BERT, ALBERT, XLNet and RoBERTa. • BUT-FIT (Jon et al., 2020), LM"
2020.semeval-1.39,K17-1004,0,0.0537086,"Missing"
2020.semeval-1.39,P18-2119,0,0.0175848,"uation. Situations with Adversarial Generations (SWAG) (Zellers et al., 2018) request a system to choose the most likely-to-happen alternative after a specific situation. Those datasets emphasize the pre-situations and/or the after-situations of certain situations, but not on the reasons why they occur or are caused. Besides, our dataset is not limited to events or situations. It concerns a broader commonsense setting, which includes events, descriptions, assertion etc. Some datasets are inspired by reading comprehension. The Story Cloze Test and ROCStories Corpora (Mostafazadeh et al., 2016; Sharma et al., 2018) aim to figure out the right ending from two candidate sentences after a four-sentence story. For a narrative text, MCScript (Ostermann et al., 2018a) gives various types of questions and pairs of answer candidates for each question. Most questions require knowledge beyond the facts mentioned in the text. Compared to those reading comprehension tasks, our benchmark encourages people to use any external resources they want. Some other datasets evolve from QA problems and care more about factual commonsense knowledge. SQUABU (Davis, 2016) provides a small hand-constructed test of commonsense and"
2020.semeval-1.39,2020.semeval-1.74,0,0.141097,"b* UI ehsantaher* uzh* 91.4 90.8 89.0 85.3 84.6 82.0 80.5 79.3 75.8 10 11 12 13 14 15 16 17 18 Team Acc. Rank Masked Reasoner KDE SenseForce SSN-NLP TakeLab* UoR dania* CUHK bhu* praveenjoshi007* 73.5 72.8 68.3 66.8 65.9 55.5 51.2 36.4 32.6 19 20 21 22 23 24 25 26 27 Table 5: Subtask B results of all the submitted systems. Those marked with * did not submit system description paper. Human performance are based on the trial data instead of the test data used by the participating systems. al., 2002). They also explore several prompt templates to constructs as the inputs to the model. • Solomon (Srivastava et al., 2020), KaLM (Wan and Huang, 2020), CS-NET (Dash et al., 2020), JUSTers (Fadel et al., 2020), CS-NLP (Saeedi et al., 2020), UI (Doxolodeo and Mahendra, 2020), TR (Teo, 2020) UoR (Markchom et al., 2020), Masked Reasoner (Lu, 2020) have similar model architecture, with RoBERTa as the encoder. In addition, UoR finetunes the pretrained language model on NLI and STS dataset, and UI finetunes on MNLI data. TR combines RoBERTa features with additional features from text-to-image generation using Gradient Boosted Decision Tree, and give better results in post-evaluation. • Qiaoning (Liu, 2020) and JUST (Moh"
2020.semeval-1.39,2020.semeval-1.76,0,0.021044,"aveenjoshi007* 73.5 72.8 68.3 66.8 65.9 55.5 51.2 36.4 32.6 19 20 21 22 23 24 25 26 27 Table 5: Subtask B results of all the submitted systems. Those marked with * did not submit system description paper. Human performance are based on the trial data instead of the test data used by the participating systems. al., 2002). They also explore several prompt templates to constructs as the inputs to the model. • Solomon (Srivastava et al., 2020), KaLM (Wan and Huang, 2020), CS-NET (Dash et al., 2020), JUSTers (Fadel et al., 2020), CS-NLP (Saeedi et al., 2020), UI (Doxolodeo and Mahendra, 2020), TR (Teo, 2020) UoR (Markchom et al., 2020), Masked Reasoner (Lu, 2020) have similar model architecture, with RoBERTa as the encoder. In addition, UoR finetunes the pretrained language model on NLI and STS dataset, and UI finetunes on MNLI data. TR combines RoBERTa features with additional features from text-to-image generation using Gradient Boosted Decision Tree, and give better results in post-evaluation. • Qiaoning (Liu, 2020) and JUST (Mohammed and Abdullah, 2020) use several ensembles of BERT, ALBERT, XLNet and RoBERTa. • BUT-FIT (Jon et al., 2020), LMVE (Liu et al., 2020), Lijunyi (Li et al., 2020) us"
2020.semeval-1.39,2020.semeval-1.67,0,0.0678589,"89.0 85.3 84.6 82.0 80.5 79.3 75.8 10 11 12 13 14 15 16 17 18 Team Acc. Rank Masked Reasoner KDE SenseForce SSN-NLP TakeLab* UoR dania* CUHK bhu* praveenjoshi007* 73.5 72.8 68.3 66.8 65.9 55.5 51.2 36.4 32.6 19 20 21 22 23 24 25 26 27 Table 5: Subtask B results of all the submitted systems. Those marked with * did not submit system description paper. Human performance are based on the trial data instead of the test data used by the participating systems. al., 2002). They also explore several prompt templates to constructs as the inputs to the model. • Solomon (Srivastava et al., 2020), KaLM (Wan and Huang, 2020), CS-NET (Dash et al., 2020), JUSTers (Fadel et al., 2020), CS-NLP (Saeedi et al., 2020), UI (Doxolodeo and Mahendra, 2020), TR (Teo, 2020) UoR (Markchom et al., 2020), Masked Reasoner (Lu, 2020) have similar model architecture, with RoBERTa as the encoder. In addition, UoR finetunes the pretrained language model on NLI and STS dataset, and UI finetunes on MNLI data. TR combines RoBERTa features with additional features from text-to-image generation using Gradient Boosted Decision Tree, and give better results in post-evaluation. • Qiaoning (Liu, 2020) and JUST (Mohammed and Abdullah, 2020) us"
2020.semeval-1.39,N18-2049,0,0.283234,"rstand whether a given statement makes sense. In our task, we take an operational definition of making sense by asking human subjects to generate natural language statements that obey or violate their commonsense knowledge about the world. 1 Many existing tasks embed the evaluation of commonsense understanding in other tasks such as coreference resolution (Levesque et al., 2012; Morgenstern and Ortiz, 2015), subsequent event prediction (Roemmele et al., 2011), ordinal common-sense inference (Zhang et al., 2017), situations with adversarial generations (Zellers et al., 2018), event validation (Wang et al., 2018), reading comprehension (Mostafazadeh et al., 2016; Ostermann et al., 2018b; Ostermann et al., 2018a), dialogue (Cui et al., 2020) and QA (Davis, 2016; Talmor et al., 2018; Mihaylov et al., 2018). They verify whether a system is equipped with common sense by testing whether the system can give a correct answer when the input does not contain such knowledge. The above tasks do not directly evaluate commonsense validation and they do not explicitly identify the key factor required in a commonsense validation process. The SemEval-2020 Task 4 includes three subtasks on testing whether a system can"
2020.semeval-1.39,P19-1393,1,0.559422,"nces. These datasets are not directly useful for our benchmark since they focus only on a small domain. ConceptNet is a seminal knowledge graph that has been upgraded over time (Liu and Singh, 2004; Havasi et al., 2007; Speer and Havasi, 2013; Speer et al., 2017). ConceptNet constructs triples using labeled edges as relations and various words and/or phrases as entities. It also has the sentences describing the corresponding triples. In contrast to these datasets, we investigate the evaluation of common sense, rather than building a resource. Before organizing this shared-task, a pilot study (Wang et al., 2019) has been performed, showing that there is still a significant gap between human and machine performance when no training data is provided, despite that the models have already been pretrained with over 100 million natural language sentences. In our task here, we also provide training data with human annotations. 6 Summary This paper summarizes SemEval-2020 Task 4: Commonsense Validation and Explanation. In this task, we construct a dataset that consists of 11,997 instances and 83,986 sentences. The task attracted around 40 participating teams, out of which 31 teams submit their system papers."
2020.semeval-1.39,2020.semeval-1.47,0,0.159388,"s in post-evaluation. • Qiaoning (Liu, 2020) and JUST (Mohammed and Abdullah, 2020) use several ensembles of BERT, ALBERT, XLNet and RoBERTa. • BUT-FIT (Jon et al., 2020), LMVE (Liu et al., 2020), Lijunyi (Li et al., 2020) use ALBERT as the encoder. BUT-FIT uses back-translation from Czech for data augmentation, and LMVE uses hint sentences, back-translation from French and intra-subtask transfer learning between Subtasks A and B to enhance their system. • UAICS (Cusmuliuc et al., 2020), DEEPYANG (Bai and Zhou, 2020), YNU-oxz (Ou et al., 2020), KDE-SenseForce (Mendbayar and Aono, 2020), CUHK (Wang et al., 2020) JBNU (Na and Lee, 2020), SWAGex (Rim and Okazaki, 2020) are BERT-based. JBNU puts an BiLSTM on top of BERT, and SWAGex finetunes BERT with SWAG data. CUHK uses a Multitask Learning framework MTDNN (Liu et al., 2019b), adopting the “Explain, Reason and Predict” system. 313 Team Human BUT-FIT Solomon KaLM panaali* JUSTers cdjhz* JBNU ANA LMVE+ BLEU 22.4 19.3 18.5 17.2 16.1 16.0 15.9 15.7 12.9 Rank 1 2 3 4 5 6 7 8 9 Human 2.58 1.84 1.84 2.08 1.22 1.94 1.75 1.80 2.10 1.78 Rank 4 4 2 11 3 8 6 1 7 Team CN-HIT-IT.NLP+ SWAGex UI TMLab* CUHK SSN-NLP UoR+ Masked Reasoner+ BLEU Rank Human Rank 9.7 7.1 5"
2020.semeval-1.39,2020.semeval-1.42,0,0.0194753,"triples visible only to the corresponding entity. They use ConceptNet as the commonsense repository to extract the triples for the statements. • ECNU-SenseMaker (Zhao et al., 2020) ranks top in Subtask B. They use Knowledge-enhanced Graph Attention Network to leverage heterogeneous knowledge from both the structured knowledge base (i.e. ConceptNet) and the unstructured text to better improve the commonsense understanding. Like CN-HIT-IT.NLP, their model is also based on K-BERT. In addition, they use unstructured text from ConceptNet and Subtask C to pretrain the language model. • IIE-NLP-NUT (Xing et al., 2020) uses RoBERTa as the encoder, and conduct a second pretraining on the original RoBERTa model with the textual corpus from Open Mind Common Sense (Singh et 312 Team Human CN-HIT-IT.NLP ECNU-SenseMaker IIE-NLP-NUT nlpx* Solomon Qiaoning BUT-FIT olenet* KaLM CS-NET fkerem* JUSTers CS-NLP Acc. 99.1 97.0 96.7 96.4 96.4 96.0 95.9 95.8 95.5 95.3 94.8 94.4 92.9 92.7 Rank 1 2 3 3 5 6 7 8 9 10 11 12 13 Team Acc. Rank panaali* ZhengxianFan* LMVE Warren* TMLab* UAICS JUST eggy* UI Armins* DEEPYANG WUY* YNU-oxz 92.5 92.4 90.4 90.4 89.2 89.1 89.1 89.0 88.2 87.1 85.1 84.2 83.6 14 15 16 16 18 19 19 21 22 23 2"
2020.semeval-1.39,D18-1009,0,0.310879,"tial to gauge how well computers can understand whether a given statement makes sense. In our task, we take an operational definition of making sense by asking human subjects to generate natural language statements that obey or violate their commonsense knowledge about the world. 1 Many existing tasks embed the evaluation of commonsense understanding in other tasks such as coreference resolution (Levesque et al., 2012; Morgenstern and Ortiz, 2015), subsequent event prediction (Roemmele et al., 2011), ordinal common-sense inference (Zhang et al., 2017), situations with adversarial generations (Zellers et al., 2018), event validation (Wang et al., 2018), reading comprehension (Mostafazadeh et al., 2016; Ostermann et al., 2018b; Ostermann et al., 2018a), dialogue (Cui et al., 2020) and QA (Davis, 2016; Talmor et al., 2018; Mihaylov et al., 2018). They verify whether a system is equipped with common sense by testing whether the system can give a correct answer when the input does not contain such knowledge. The above tasks do not directly evaluate commonsense validation and they do not explicitly identify the key factor required in a commonsense validation process. The SemEval-2020 Task 4 includes three su"
2020.semeval-1.39,2020.semeval-1.60,0,0.0190605,"ce. adopted the pretrained language models such as BERT (Devlin et al., 2018), RoBERTa (Liu et al., 2019c), XLNET (Yang et al., 2019) and ALBERT (Lan et al., 2019) as the encoder of the model, and then finetune on the training set of the task. See Figure 1 for the most commonly-used model architectures for Subtask A and B. Also, the top-performing systems take advantage of external knowledge graphs such as ConceptNet (Speer et al., 2017), or unstructured text containing commonsense knowledge. Below we introduce in detail several top-performing systems and their main features. • CN-HIT-IT.NLP (Zhang et al., 2020) ranks top in Subtask A. They use a variant of K-BERT (Liu et al., 2019a) as the encoder to enhance language representations through knowledge graphs. K-BERT is a Transformer-based model, which enhances the language representations of the text by injecting relevant triples from a knowledge graph to form a knowledge-rich sentence tree, and then uses a mask-Transformer to make the triples visible only to the corresponding entity. They use ConceptNet as the commonsense repository to extract the triples for the statements. • ECNU-SenseMaker (Zhao et al., 2020) ranks top in Subtask B. They use Know"
2020.semeval-1.39,2020.semeval-1.48,0,0.0351203,"heir main features. • CN-HIT-IT.NLP (Zhang et al., 2020) ranks top in Subtask A. They use a variant of K-BERT (Liu et al., 2019a) as the encoder to enhance language representations through knowledge graphs. K-BERT is a Transformer-based model, which enhances the language representations of the text by injecting relevant triples from a knowledge graph to form a knowledge-rich sentence tree, and then uses a mask-Transformer to make the triples visible only to the corresponding entity. They use ConceptNet as the commonsense repository to extract the triples for the statements. • ECNU-SenseMaker (Zhao et al., 2020) ranks top in Subtask B. They use Knowledge-enhanced Graph Attention Network to leverage heterogeneous knowledge from both the structured knowledge base (i.e. ConceptNet) and the unstructured text to better improve the commonsense understanding. Like CN-HIT-IT.NLP, their model is also based on K-BERT. In addition, they use unstructured text from ConceptNet and Subtask C to pretrain the language model. • IIE-NLP-NUT (Xing et al., 2020) uses RoBERTa as the encoder, and conduct a second pretraining on the original RoBERTa model with the textual corpus from Open Mind Common Sense (Singh et 312 Tea"
2020.semeval-1.40,2020.semeval-1.57,0,0.0303607,"or question answering, and nearly all of them benefit from pretraining. An exception is the 6th ranked team, Anderson Sung, that use a multi-stack, birdirectional LSTM architecture to some success (Sung et al., 2020), showing that non-transformer approaches are viable for the task. Similarly, habi-akl experiments with a BiLSTM Conditional Random Fields (CRF) model for Subtask-2, but find that a BERT based model with a multilayer perceptron classifier outperforms the LSTM and conclude that the semi-supervised systems show a better level of understanding of challenging counterfactual forms (Abi Akl et al., 2020). One approach among the top models formulates the problem as an extractive question answering (QA) task, with the target being extracting the answer from the given context towards a specific question. The others formulate the task as a sequence labeling task. In the top 4 systems, half of the teams took the QA approach, and the other half took the sequence labelling approach. The choice between BERT and RoBERTa has split almost evenly amongst most of the participants. As is the case for Subtask-1, many teams sought to build on top of the pre-trained models and add additional upper layer struc"
2020.semeval-1.40,2020.semeval-1.56,0,0.138924,"ined neural models, which have achieved the state-of-the-art results across many natural language processing (NLP) tasks (Devlin et al., 2018; Radford et al., 2018; Radford et al., 2019; Yang et al., 2019; Liu et al., 2019; Lan et al., 2019), which we believe is an effective way to integrate additional external knowledge that does not exist in the training data, including common sense. Some participants like shngt experimented with classic machine learning methods like SVM and gradient boosted random forests, and found that model performance plateaued at an F1 score of around 60 percent (Anil Ojha et al., 2020), showing that these methods cannot capture counterfactual reasoning as well as the pre-trained models. One team, Serena, use a non-transformer approach, basing their system on Ordered Neurons LSTM (ON-LSTM) with Hierarchical Attention Network (HAN) and a Pooling operation is done for dimensionality reduction (Ou et al., 2020). Their system struggles with data imbalance and they conclude that transformer networks can improve their performance. Among the top 8 competitors, BERT and RoBERTa based systems are most popular, being used as the primary models or as a part of their final ensemble in 5"
2020.semeval-1.40,2020.semeval-1.88,0,0.0176087,"le the task better. The best results are achieved by team Martin, with an F1 score of 88.2 and an exact match score of 57.5 (Fajcik et al., 2020). To predict the start and ending positions of antecedents and consequents, the model utilizes an ensemble of RoBERTa models and extend it in the same manner as how BERT was extended for the SQuAD dataset (Rajpurkar et al., 2016). The second-place system pouria babvey uses a sequence labelling approach: the authors develop the model on top of BERT with a multi-head attention layer and label masking to capture mutual information between nearby labels (Babvey et al., 2020). Label masking, in which only part of the labels is fed during training and the rest have to be predicted, has shown to be particularly effective for improving accuracy, which can be seen as a form of regularization. In addition, a multi-stage algorithm is used to gradually improve certainty in predictions after each step. The third-place system, Roger, formulates the problem as a query-based question answering problem, where antecedents and consequents are extracted after an antecedent and consequent query are supplied along with the original statement into BERT. Pointer networks are further"
2020.semeval-1.40,2020.semeval-1.82,0,0.039236,"Missing"
2020.semeval-1.40,2020.semeval-1.53,0,0.0342751,"the answer from the given context towards a specific question. The others formulate the task as a sequence labeling task. In the top 4 systems, half of the teams took the QA approach, and the other half took the sequence labelling approach. The choice between BERT and RoBERTa has split almost evenly amongst most of the participants. As is the case for Subtask-1, many teams sought to build on top of the pre-trained models and add additional upper layer structures to handle the task better. The best results are achieved by team Martin, with an F1 score of 88.2 and an exact match score of 57.5 (Fajcik et al., 2020). To predict the start and ending positions of antecedents and consequents, the model utilizes an ensemble of RoBERTa models and extend it in the same manner as how BERT was extended for the SQuAD dataset (Rajpurkar et al., 2016). The second-place system pouria babvey uses a sequence labelling approach: the authors develop the model on top of BERT with a multi-head attention layer and label masking to capture mutual information between nearby labels (Babvey et al., 2020). Label masking, in which only part of the labels is fed during training and the rest have to be predicted, has shown to be p"
2020.semeval-1.40,2020.semeval-1.86,0,0.0624451,"Missing"
2020.semeval-1.40,2020.semeval-1.81,0,0.0615568,"Missing"
2020.semeval-1.40,2020.semeval-1.85,0,0.0927996,"Missing"
2020.semeval-1.40,2020.semeval-1.89,0,0.034283,"es not exist in the training data, including common sense. Some participants like shngt experimented with classic machine learning methods like SVM and gradient boosted random forests, and found that model performance plateaued at an F1 score of around 60 percent (Anil Ojha et al., 2020), showing that these methods cannot capture counterfactual reasoning as well as the pre-trained models. One team, Serena, use a non-transformer approach, basing their system on Ordered Neurons LSTM (ON-LSTM) with Hierarchical Attention Network (HAN) and a Pooling operation is done for dimensionality reduction (Ou et al., 2020). Their system struggles with data imbalance and they conclude that transformer networks can improve their performance. Among the top 8 competitors, BERT and RoBERTa based systems are most popular, being used as the primary models or as a part of their final ensemble in 5 and 4 of the top 8 participants’ systems respectively. XLNet and ALBERT are less popular 326 choices, but they are also used in the first and second of the top 8 participating systems respectively. In addition, the top models adopt ensemble strategies, and in most cases, achieve better performance than that of individual clas"
2020.semeval-1.40,2020.semeval-1.55,0,0.0429014,"r, formulates the problem as a query-based question answering problem, where antecedents and consequents are extracted after an antecedent and consequent query are supplied along with the original statement into BERT. Pointer networks are further used to predict the start and ending positions (Lu et al., 2020) . A unique approach for Subtask-2 is used by 7th placed team, rajaswa patil, where they use a base architecture for both subtasks. They first train with a binary-classification module for Subtask-1, then replace it with a regression-module and further fine-tune the system for Subtask-2 (Patil and Baths, 2020), leveraging the commonality between the two tasks. We can observe that there is still a gap between the performance of exact match and F1, which is mainly due to the fact that Exact Match is sensitive to non-essential phrases in predictions even the core parts are identified correctly. 5 Related Work Modelling counterfactual thinking has started to attract more interest. One of the previous works closest to ours is (Son et al., 2017), in which a small-scale counterfactual tweet dataset is collected from social media. There are three main differences between that dataset and ours. First, there"
2020.semeval-1.40,D19-1509,0,0.0521885,"Missing"
2020.semeval-1.40,D16-1264,0,0.0460134,"lling approach. The choice between BERT and RoBERTa has split almost evenly amongst most of the participants. As is the case for Subtask-1, many teams sought to build on top of the pre-trained models and add additional upper layer structures to handle the task better. The best results are achieved by team Martin, with an F1 score of 88.2 and an exact match score of 57.5 (Fajcik et al., 2020). To predict the start and ending positions of antecedents and consequents, the model utilizes an ensemble of RoBERTa models and extend it in the same manner as how BERT was extended for the SQuAD dataset (Rajpurkar et al., 2016). The second-place system pouria babvey uses a sequence labelling approach: the authors develop the model on top of BERT with a multi-head attention layer and label masking to capture mutual information between nearby labels (Babvey et al., 2020). Label masking, in which only part of the labels is fed during training and the rest have to be predicted, has shown to be particularly effective for improving accuracy, which can be seen as a form of regularization. In addition, a multi-stage algorithm is used to gradually improve certainty in predictions after each step. The third-place system, Roge"
2020.semeval-1.40,P17-2103,0,0.122702,"making the filtering stage too rigorous. 1 In the official evaluation period of Subtask-1, the ranking is based on F1. 324 Token-based Filtering The template set consists of two subsets that jointly work to find candidate potential counterfactual statements when they are used to search through news articles. The first subset focuses on word token patterns and the second subset leverages POS tag-based patterns. The full list of token-based patterns are listed in Appendix A. Some of the patterns are based on the previous research which revealed common counterfactual constructions (Hobbs, 2005; Son et al., 2017; Rouvoli et al., 2019). POS-based Filtering The second subset of templates utilize patterns based on part-of-speech tags. We identified five counterfactual forms based on (Janocko et al., 2016) and coverted them into POS-based patterns to increase the chances of identifying true counterfactual statements. The details of the POSbased rules are presented in Appendix B. To apply the rules, we tokenize each sentence and conduct POS tagging with the NLTK library (Bird et al., 2009). Then we extract the sentences which match one of the pre-defined patterns. By applying both the token-based and POS-"
2020.semeval-1.40,2020.semeval-1.43,0,0.188472,"Missing"
2020.semeval-1.40,2020.semeval-1.83,0,0.0711281,"Missing"
2021.acl-short.111,N19-1423,0,0.049222,"w a slot is roughly operated in the current dialog context and connected with all possible tokens regarding its values in the schema. The dialog context encoder is used for the parameter initialization of the base part of a DST model. The pre-trained corpus is constructed from MultiWOZ2.1 dialogs (Eric et al., 2020) and the off-the-shelf synthesized dialogs (Campagna et al., 2020), which contains 337,346 dialog data in total. We also leverage the language modelling (LM) loss as an auxiliary loss Laux to learn contextual representations of natural language. To be specific, we use the MLM loss (Devlin et al., 2019) as Laux for transformer-based DST modes and the summation of both forward and backward LM losses (Peters et al., 2018) for RNN-based DST models. We only use the original MultiWOZ2.1 dialogs to optimize Laux , considering that synthesized data is not suitable for natural language modelling. However, both the original and synthesized data are used to optimize Lseq and Lcls . 3.3 The Review Module The process of review often help a learner consolidate difficult concepts newly learned. We design a review module to consider mispredicted examples as the concepts that the DST model has not grasped d"
2021.acl-short.111,2020.lrec-1.53,0,0.516868,"mation, a curriculum module that optimizes the model with CL, and a review module that augments mispredicted data to reinforce the CL training. We show that our proposed approach improves DST performance over both a transformerbased and RNN-based DST model (TripPy and TRADE) and achieves new state-of-the-art results on WOZ2.0 and MultiWOZ2.1. 1 Figure 1: An easy and a hard dialog example for DST. Introduction Dialog state tracking (DST) extracts users’ goals in task-oriented dialog systems, where dialog states are often represented in terms of a set of slot-value pairs (Williams et al., 2016; Eric et al., 2020). Due to the language variety of multi-turn dialogs, the concepts of slots and values are often indirectly expressed in the conversation (such as co-references, ellipsis, and diverse appearances), which are a major bottleneck for improving DST performance (Gao et al., 2019; Hu et al., 2020). Many existing DST methods have focused on designing better model architectures to tackle the problems (Dai et al., 2018; Wu et al., 2019; Kim et al., 2020), but still neglect the full exploitation of two important aspects of structural information. The first is curriculum structure in a dataset. Such a str"
2021.acl-short.111,W19-5932,0,0.0172906,"and achieves new state-of-the-art results on WOZ2.0 and MultiWOZ2.1. 1 Figure 1: An easy and a hard dialog example for DST. Introduction Dialog state tracking (DST) extracts users’ goals in task-oriented dialog systems, where dialog states are often represented in terms of a set of slot-value pairs (Williams et al., 2016; Eric et al., 2020). Due to the language variety of multi-turn dialogs, the concepts of slots and values are often indirectly expressed in the conversation (such as co-references, ellipsis, and diverse appearances), which are a major bottleneck for improving DST performance (Gao et al., 2019; Hu et al., 2020). Many existing DST methods have focused on designing better model architectures to tackle the problems (Dai et al., 2018; Wu et al., 2019; Kim et al., 2020), but still neglect the full exploitation of two important aspects of structural information. The first is curriculum structure in a dataset. Such a structure relies on a measure of the difficulty of examples, which can be used to guide the ∗ Corresponding author model training in an easy-to-hard manner, imitating the meaningful learning order in human curricula. This paradigm is called curriculum learning (CL) (Bengio et"
2021.acl-short.111,2020.sigdial-1.4,0,0.0310204,"Missing"
2021.acl-short.111,C18-1105,0,0.0278682,"al., 2020), reading comprehension (Tay et al., 2019) and open-domain chatbots (Bao et al., 2020; Cai et al., 2020; Su et al., 2020). Yet, the research on using CL in task-oriented dialog systems is limited. There has been some work (Saito, 2018; Zhao et al., 2021) on using CL in dialog policy learning, but applying CL to DST has not been investigated. Learning a structural inductive bias during pretraining has been shown beneficial in downstream tasks that require parsing semantics, such as textto-SQL (Yu et al., 2021) and table cell recognition (Wang et al., 2020). There are also many works (Hou et al., 2018; Yoo et al., 2020; Yin et al., 2020) on dialog augmentation. We aim to integrate these methods to build a general CL framework for DST. 6 Conclusion In this paper, we propose a model-agnostic framework named as schema-aware curriculum learning for DST, which exploits both the curriculum 883 References Siqi Bao, Huang He, Fan Wang, Hua Wu, Haifeng Wang, Wenquan Wu, Zhen Guo, Zhibin Liu, and Xinchao Xu. 2020. Plato-2: Towards building an open-domain chatbot via curriculum learning. arXiv preprint arXiv:2006.16779. Yoshua Bengio, J´erˆome Louradour, Ronan Collobert, and Jason Weston. 2009. Curri"
2021.acl-short.111,2020.acl-main.567,0,0.0308017,"state-of-the-art results on WOZ2.0 and MultiWOZ2.1. 1 Figure 1: An easy and a hard dialog example for DST. Introduction Dialog state tracking (DST) extracts users’ goals in task-oriented dialog systems, where dialog states are often represented in terms of a set of slot-value pairs (Williams et al., 2016; Eric et al., 2020). Due to the language variety of multi-turn dialogs, the concepts of slots and values are often indirectly expressed in the conversation (such as co-references, ellipsis, and diverse appearances), which are a major bottleneck for improving DST performance (Gao et al., 2019; Hu et al., 2020). Many existing DST methods have focused on designing better model architectures to tackle the problems (Dai et al., 2018; Wu et al., 2019; Kim et al., 2020), but still neglect the full exploitation of two important aspects of structural information. The first is curriculum structure in a dataset. Such a structure relies on a measure of the difficulty of examples, which can be used to guide the ∗ Corresponding author model training in an easy-to-hard manner, imitating the meaningful learning order in human curricula. This paradigm is called curriculum learning (CL) (Bengio et al., 2009) and ha"
2021.acl-short.111,2020.acl-main.53,0,0.0359253,"users’ goals in task-oriented dialog systems, where dialog states are often represented in terms of a set of slot-value pairs (Williams et al., 2016; Eric et al., 2020). Due to the language variety of multi-turn dialogs, the concepts of slots and values are often indirectly expressed in the conversation (such as co-references, ellipsis, and diverse appearances), which are a major bottleneck for improving DST performance (Gao et al., 2019; Hu et al., 2020). Many existing DST methods have focused on designing better model architectures to tackle the problems (Dai et al., 2018; Wu et al., 2019; Kim et al., 2020), but still neglect the full exploitation of two important aspects of structural information. The first is curriculum structure in a dataset. Such a structure relies on a measure of the difficulty of examples, which can be used to guide the ∗ Corresponding author model training in an easy-to-hard manner, imitating the meaningful learning order in human curricula. This paradigm is called curriculum learning (CL) (Bengio et al., 2009) and has been shown useful in various other problems (Wang et al., 2021). DST training examples also vary greatly in their difficulty levels. As shown in Figure 1,"
2021.acl-short.111,P19-1546,0,0.0447439,"Missing"
2021.acl-short.111,2020.acl-main.41,0,0.0724363,"Missing"
2021.acl-short.111,N18-1202,0,0.017011,"in the schema. The dialog context encoder is used for the parameter initialization of the base part of a DST model. The pre-trained corpus is constructed from MultiWOZ2.1 dialogs (Eric et al., 2020) and the off-the-shelf synthesized dialogs (Campagna et al., 2020), which contains 337,346 dialog data in total. We also leverage the language modelling (LM) loss as an auxiliary loss Laux to learn contextual representations of natural language. To be specific, we use the MLM loss (Devlin et al., 2019) as Laux for transformer-based DST modes and the summation of both forward and backward LM losses (Peters et al., 2018) for RNN-based DST models. We only use the original MultiWOZ2.1 dialogs to optimize Laux , considering that synthesized data is not suitable for natural language modelling. However, both the original and synthesized data are used to optimize Lseq and Lcls . 3.3 The Review Module The process of review often help a learner consolidate difficult concepts newly learned. We design a review module to consider mispredicted examples as the concepts that the DST model has not grasped during CL, and utilize a schema-based data augmenter to produce similar cases from the examples. Specifically, the DST m"
2021.acl-short.111,W18-5707,0,0.0427423,"Missing"
2021.acl-short.111,2020.acl-main.563,0,0.0626527,"Missing"
2021.acl-short.111,N10-1116,0,0.0129957,"into our curriculum design: 1) current dialog turn number t; 2) the total token number of (Rt , Ut ); 3) the number of mentioned name entities like ‘hotel names’ in Zt ; 4) the number of newly added or changed slots in Yt . We set the maximum values of above factors as 7/50/4/6 respectively, and normalize all factors into rtrul,i ∈ [0, 1], where i indicates the i-th factor. Finally, the hybrid difficulty calculated P4score isrul,i jointly as rthyb = P α0 rmod + α r , where t i=1 i t 4 hyb r ∈ [0, 1] and i=0 αi = 1. 3.1.2 The Training Scheduler We adopt a widely used strategy called baby step (Spitkovsky et al., 2010) to organize the scored data for CL. Specifically, we divide the score uniformly into N intervals and distribute the sorted data into N buckets accordingly. The optimization starts from the easiest bucket as the initial training stage. After reaching a fixed number of maximum epochs or convergence, the next bucket is merged 880 Figure 2: An overview of the SaCLog training procedures. into the current training subset and shuffled for the next training stage. In our experiment, we set the maximum number of epochs as 3, and treat as the convergence if the training loss ceases to decrease and the"
2021.acl-short.111,P19-1486,0,0.0621828,"Missing"
2021.acl-short.111,E17-1042,0,0.0497929,"Missing"
2021.acl-short.111,P19-1078,0,0.138292,"ng (DST) extracts users’ goals in task-oriented dialog systems, where dialog states are often represented in terms of a set of slot-value pairs (Williams et al., 2016; Eric et al., 2020). Due to the language variety of multi-turn dialogs, the concepts of slots and values are often indirectly expressed in the conversation (such as co-references, ellipsis, and diverse appearances), which are a major bottleneck for improving DST performance (Gao et al., 2019; Hu et al., 2020). Many existing DST methods have focused on designing better model architectures to tackle the problems (Dai et al., 2018; Wu et al., 2019; Kim et al., 2020), but still neglect the full exploitation of two important aspects of structural information. The first is curriculum structure in a dataset. Such a structure relies on a measure of the difficulty of examples, which can be used to guide the ∗ Corresponding author model training in an easy-to-hard manner, imitating the meaningful learning order in human curricula. This paradigm is called curriculum learning (CL) (Bengio et al., 2009) and has been shown useful in various other problems (Wang et al., 2021). DST training examples also vary greatly in their difficulty levels. As"
2021.acl-short.111,2020.findings-emnlp.95,0,0.0267103,"icitly via multi-round interactions, requiring a complex inference process to find the value ‘golden house’ referred by the slot ‘restaurant-name’. However, CL has been rarely studied in DST, and models are often trained with dialog data in a random order. In addition, schema structure is prominent in multi-domain task-oriented dialogs. A schema is specified by a collection of all possible slots and their values, which describes semantic relations among them. Some previous work utilized the structure via an extra schema graph in a regular training process (Chen et al., 2020; Zhu et al., 2020; Wu et al., 2020). We propose to incorporate schema information into CL through a pre-curriculum process, in which a DST model can be pre-trained with schema-related objectives to prepare for upcom879 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Short Papers), pages 879–885 August 1–6, 2021. ©2021 Association for Computational Linguistics ing DST examples. To reinforce the CL training, we can also expand those examples with frequent mispredictions during CL based upon the schema, enabling the"
2021.acl-short.111,2020.acl-main.542,0,0.0871591,"Missing"
2021.acl-short.111,2020.acl-main.620,0,0.0747792,"Missing"
2021.acl-short.111,2020.findings-emnlp.68,0,0.0237234,"her intention implicitly via multi-round interactions, requiring a complex inference process to find the value ‘golden house’ referred by the slot ‘restaurant-name’. However, CL has been rarely studied in DST, and models are often trained with dialog data in a random order. In addition, schema structure is prominent in multi-domain task-oriented dialogs. A schema is specified by a collection of all possible slots and their values, which describes semantic relations among them. Some previous work utilized the structure via an extra schema graph in a regular training process (Chen et al., 2020; Zhu et al., 2020; Wu et al., 2020). We propose to incorporate schema information into CL through a pre-curriculum process, in which a DST model can be pre-trained with schema-related objectives to prepare for upcom879 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Short Papers), pages 879–885 August 1–6, 2021. ©2021 Association for Computational Linguistics ing DST examples. To reinforce the CL training, we can also expand those examples with frequent mispredictions during CL based upon the sch"
2021.acl-short.36,P19-1441,0,0.0977086,"als et al., 2015; Karpathy and FeiFei, 2015; Donahue et al., 2015; Yang et al., 2016). To improve the performance on reference-based automatic evaluation metrics, previous work has used visual attention mechanism (Anderson et al., 2018; Lu et al., 2017; Pedersoli et al., 2017; Xu et al., 2015; Pan et al., 2020), explicit high-level attributes detection (Yao et al., 2017; Wu et al., 2016; You et al., 2016), reinforcement learning methods (Rennie et al., 2017; Ranzato et al., 2015; Liu et al., 2018a), contrastive or adversarial learning (Dai and Lin, 2017; Dai et al., 2017), multistep decoding (Liu et al., 2019a; Gu et al., 2018), weighted training by word-image correlation (Ding et al., 2019) and scene graph detection (Yao et al., 2018; Yang et al., 2019; Shi et al., 2020). The work of (Luo et al., 2018; Liu et al., 2018b) is most related to ours, which uses retrieval loss as a rewarding signal to encourage descriptive captioning. Different from the above approaches, our method explicitly explore the different descriptiveness in references using NLI models and incorporate the information into the training objectives to guide the model to generate more informative sentences. We build our method on t"
2021.acl-short.36,2021.ccl-1.108,0,0.0191482,"Missing"
2021.acl-short.36,2020.acl-main.664,1,0.586126,"evious work has used visual attention mechanism (Anderson et al., 2018; Lu et al., 2017; Pedersoli et al., 2017; Xu et al., 2015; Pan et al., 2020), explicit high-level attributes detection (Yao et al., 2017; Wu et al., 2016; You et al., 2016), reinforcement learning methods (Rennie et al., 2017; Ranzato et al., 2015; Liu et al., 2018a), contrastive or adversarial learning (Dai and Lin, 2017; Dai et al., 2017), multistep decoding (Liu et al., 2019a; Gu et al., 2018), weighted training by word-image correlation (Ding et al., 2019) and scene graph detection (Yao et al., 2018; Yang et al., 2019; Shi et al., 2020). The work of (Luo et al., 2018; Liu et al., 2018b) is most related to ours, which uses retrieval loss as a rewarding signal to encourage descriptive captioning. Different from the above approaches, our method explicitly explore the different descriptiveness in references using NLI models and incorporate the information into the training objectives to guide the model to generate more informative sentences. We build our method on top of the existing methods to verify the effectiveness. Applications of NLI There are basically three major application types for NLI, (1) Direct application of train"
2021.acl-short.36,W18-5501,0,0.0378197,"Missing"
2021.acl-short.36,P02-1040,0,0.110643,"he CIDEr reward item rcd in r(ˆ c, xi ) as shown in equation (2) by replacing U (c |xi ) with q(c |xi ): 0 rcd (ˆ c, xi ) = n X q(cji |xi )·CD(ˆ c, cji ) (5) j=1 where CD denotes the CIDEr similarity score. 4 4.1 Experiment Setup Dataset and Evaluation Metrics We perform experiments on the Karpathy split of the MSCOCO dataset (Lin et al., 2014; Karpathy and Fei-Fei, 2015). We employ a wide range of conventional image caption evaluation metrics, i.e., SPICE(SP) (Anderson et al., 2016), CIDEr(CD) (Vedantam et al., 2015), METEOR(ME) (Denkowski and Lavie, 2014), ROUGE-L(RG) (Lin, 2004), and BLEU (Papineni et al., 2002) to evaluate the generated captions. Following (Liu et al., 2019a), we also use the caption generated cˆ to retrieve image x using a separately trained image-matching model (Lee et al., 2018). The retrieval evaluation is based on 1K images (Lee et al., 2018) from the Karpathy test set. Retrieval performances are measured by R@K (K = 1, 5), i.e., whether x is retrieved within the top K retrieved images. We also perform human evaluation on descriptiveness, fluency, and fidelity. Implementation Details To make a fair comparison, we use the same experiment setup that the compared baselines used. S"
2021.acl-short.36,N18-1202,0,0.0101114,"ess. Applications of NLI There are basically three major application types for NLI, (1) Direct application of trained NLI models. Trained NLI models are directly used in Fact Extraction and Verification (Thorne et al., 2018) to decide whether a piece of evidence supports a claim (Nie et al., 2019) and generation of longer sentences as a discriminator (Holtzman et al., 2018) to prevent a text decoder from contradicting itself; (2) NLI as a research and evaluation task for new methods. It is widely used as a major evaluation when developing novel language model pretraining (Devlin et al., 2018; Peters et al., 2018; Liu et al., 2019c); (3) NLI as a pre-training task in transfer learning. Training neural network models on NLI corpora and then fine-tuning them on target tasks often yields substantial improvements in performance (Liu et al., 2019b; Phang et al., 2018). Figure 1: A NLI matrix and inference graph. 3 Our Method The goal of image captioning is to train conditional generation model pθ (c |x) based on training instances (xi , Ci )m i=1 in a training dataset and Ci = {c1i , · · · , cni }, where m is the number of training instances and n is the number of reference captions for an image. The typic"
2021.acl-short.36,Q14-1006,0,0.0461166,"the observation that more descriptive captions often result in better discriminativity in retrieval. 1 In the paper, we explore to develop better descriptive image captioning models from a novel perspective— considering that among different captions of an image, descriptive captions are more likely to entail less descriptive ones, we develop descriptive image captioning models that leverage natural language inference (NLI, or also known as recognizing textual entailment) (Dagan et al., 2005; MacCartney and Manning, 2009; Bowman et al., 2015), which can utilize multiple references of captions (Young et al., 2014; Lin et al., 2014) to guide the models to produce more descriptive captions. Specifically, the proposed model first predicts NLI relations for all pairs of references, i.e., entailment or neutral2 . Built on that, we construct inference graphs and employ a PageRank algorithm to estimate descriptiveness scores for individual captions. We use reference sampling and weighted designated rewards to incorporate the descriptiveness signal into the Maximum Likelihood Estimation and Reinforcement Learning phase, respectively, to guide captioning models to produce descriptive captions. Extensive experi"
2021.emnlp-main.181,N19-1423,0,0.452151,"ssifier. After updating the parameters of the session classifier, a new set of data with high confidence will be retrieved from the predicted disentanglement results of the session classifier and used for updating the message-pair classifier. As shown in Figure 2, the above process is iteratively performed by updating one classifier with the other until the performance of session classifier stops increasing. We conduct experiments on the large public Movie Dialogue Dataset (Liu et al., 2020). Experimental results demonstrate that our proposed method outperforms strong baselines based on BERT (Devlin et al., 2019) in two-step settings, and achieves competitive results compared to those of the state-of-the-art supervised end-to-end methods. Moreover, we apply the disentangled conversations predicted by our method to the downstream task of multi-party response selection and get significant improvements compared to a baseline system.3 In summary, our main contributions are three-fold: sessions and calculate the matching degree between a session and a message in an end-to-end manner. Though end-to-end methods have been proved to be more flexible and can achieve better performance (Liu et al., 2020), these"
2021.emnlp-main.181,P08-1095,0,0.462644,", the inherent property that multiple topics are often discussed in one channel hinders an efficient access to the conversational content. In the example shown in Figure 1, people or intelligent systems have to selectively read the messages related to the topics they are interested in from hundreds of messages in the chat channel. With the goal of automatically grouping messages with the same topic into one session, conversation disentanglement has proved to be a prerequisite for understanding multi-party conversations and solving the corresponding downstream tasks such as response selection (Elsner and Charniak, 2008; Lowe et al., 2017; Jia et al., 2020; Wang et al., 2020). Previous research on conversation disentanglement can be roughly divided into two categories: (1) two-step methods, and (2) end-to-end methods. In the two-step methods (Elsner and Charniak, 2011, 2008; Jiang et al., 2018), a model first retrieves the “local” relations between two messages 1 Introduction by utilizing either feature engineering approaches With the continuing growth of Internet and social or deep learning methods, and then a clustering media, online group chat channels, e.g., Slack1 and algorithm is employed to divide an"
2021.emnlp-main.181,J10-3004,0,0.113025,"Missing"
2021.emnlp-main.181,P11-1118,0,0.0257567,"he topics they are interested in from hundreds of messages in the chat channel. With the goal of automatically grouping messages with the same topic into one session, conversation disentanglement has proved to be a prerequisite for understanding multi-party conversations and solving the corresponding downstream tasks such as response selection (Elsner and Charniak, 2008; Lowe et al., 2017; Jia et al., 2020; Wang et al., 2020). Previous research on conversation disentanglement can be roughly divided into two categories: (1) two-step methods, and (2) end-to-end methods. In the two-step methods (Elsner and Charniak, 2011, 2008; Jiang et al., 2018), a model first retrieves the “local” relations between two messages 1 Introduction by utilizing either feature engineering approaches With the continuing growth of Internet and social or deep learning methods, and then a clustering media, online group chat channels, e.g., Slack1 and algorithm is employed to divide an entire converWhatsapp2 , among many others, have become in- sation into separate sessions based on the message creasingly popular and played a significant social pair relations. In contrast, end-to-end methods (Tan 1 et al., 2019; Yu and Joty, 2020) cap"
2021.emnlp-main.181,2020.emnlp-main.150,0,0.362871,"re often discussed in one channel hinders an efficient access to the conversational content. In the example shown in Figure 1, people or intelligent systems have to selectively read the messages related to the topics they are interested in from hundreds of messages in the chat channel. With the goal of automatically grouping messages with the same topic into one session, conversation disentanglement has proved to be a prerequisite for understanding multi-party conversations and solving the corresponding downstream tasks such as response selection (Elsner and Charniak, 2008; Lowe et al., 2017; Jia et al., 2020; Wang et al., 2020). Previous research on conversation disentanglement can be roughly divided into two categories: (1) two-step methods, and (2) end-to-end methods. In the two-step methods (Elsner and Charniak, 2011, 2008; Jiang et al., 2018), a model first retrieves the “local” relations between two messages 1 Introduction by utilizing either feature engineering approaches With the continuing growth of Internet and social or deep learning methods, and then a clustering media, online group chat channels, e.g., Slack1 and algorithm is employed to divide an entire converWhatsapp2 , among many o"
2021.emnlp-main.181,N18-1164,0,0.0805554,"from hundreds of messages in the chat channel. With the goal of automatically grouping messages with the same topic into one session, conversation disentanglement has proved to be a prerequisite for understanding multi-party conversations and solving the corresponding downstream tasks such as response selection (Elsner and Charniak, 2008; Lowe et al., 2017; Jia et al., 2020; Wang et al., 2020). Previous research on conversation disentanglement can be roughly divided into two categories: (1) two-step methods, and (2) end-to-end methods. In the two-step methods (Elsner and Charniak, 2011, 2008; Jiang et al., 2018), a model first retrieves the “local” relations between two messages 1 Introduction by utilizing either feature engineering approaches With the continuing growth of Internet and social or deep learning methods, and then a clustering media, online group chat channels, e.g., Slack1 and algorithm is employed to divide an entire converWhatsapp2 , among many others, have become in- sation into separate sessions based on the message creasingly popular and played a significant social pair relations. In contrast, end-to-end methods (Tan 1 et al., 2019; Yu and Joty, 2020) capture the “global” https://s"
2021.emnlp-main.181,D14-1162,0,0.0850412,"ement of different methods. We can observe that: (1) for two-step methods, BERT 5.1.2 Implementation Details has a very poor performance without finetuning, We adopt BERT (Devlin et al., 2019) (the uncased while after finetuned on our pseudo dataset, its base version) as the message-pair classifier. For the performance gets improved with a relatively large session classifier, we set the hidden dimension to margin. (2) Utilizing the pseudo pairs generated be 300, and the word embeddings are initialized by a pretrained DialoGPT can further improve the ret . We consider with 300-d GloVe vectors (Pennington et al., 2014). performance of BERT based on Dm For training, we use Adam (Kingma and Ba, 2015) this is because the messages from one speaker are for optimization; the learning rate is set to be 1e-5 usually not contiguous in a conversation, while for the message-pair classifier, 1e-4 for initializing DialoGPT can directly produce a response to a the session classifier, and 1e-5 for updating the message, which is beneficial to BERT on captursession classifier with reinforcement learning. We ing the differences of two messages. (3) During 2351 Iteration Base 1 2 3 NMI 24.96 29.80 29.87 29.71 1-1 54.26 56.24"
2021.emnlp-main.181,2020.emnlp-main.148,0,0.0165841,"d conversation disentanglement in a completely unsupervised fashion, which can be easily adapted to downstream tasks and used in a wide variety of applications. Dialogue Structure Learning One problem that may be related to conversation disentanglement is dialogue structure learning (Zhai and Williams, 2014; Shi et al., 2019). Both are related to understanding multi-party conversation structures but they are different tasks. Dialogue structure learning aims to discover latent dialogue topics and construct an implicit utterance dependency tree to represent a multi-party dialogue’s turn taking (Qiu et al., 2020), while the goal of conversation disentanglement is to learn an explicit dividing scheme that separates intermingled messages into sessions. complementary views and utilizes two models to iteratively provide pseudo training signals to each other. Our method consists of a message-pair classifier and a session classifier, which respectively view the unannotated dataset from the perspective of the local relations between two messages and that of the context-aware relations between a session and a message. To the best of our knowledge, this is the first work that utilizes co-training in the resear"
2021.emnlp-main.181,N19-1178,0,0.0133486,"pervised thread detection in email systems based on two-step methods (Wu and Oard, 2005; Erera and Carmel, 2008; Domeniconi et al., 2016), but these methods use handcrafted features which cannot be extended to various datasets. Compared with previous works, our method can conduct end-to-end conversation disentanglement in a completely unsupervised fashion, which can be easily adapted to downstream tasks and used in a wide variety of applications. Dialogue Structure Learning One problem that may be related to conversation disentanglement is dialogue structure learning (Zhai and Williams, 2014; Shi et al., 2019). Both are related to understanding multi-party conversation structures but they are different tasks. Dialogue structure learning aims to discover latent dialogue topics and construct an implicit utterance dependency tree to represent a multi-party dialogue’s turn taking (Qiu et al., 2020), while the goal of conversation disentanglement is to learn an explicit dividing scheme that separates intermingled messages into sessions. complementary views and utilizes two models to iteratively provide pseudo training signals to each other. Our method consists of a message-pair classifier and a session"
2021.emnlp-main.181,D19-1682,0,0.0516411,"tasks to boost their performance (Jia et al., 2020; Wang et al., 2020). Previous methods on conversation disentanglement are mostly performed in a supervised fashion, which can be classified as two categories: (1) two-step approaches and (2) end-to-end methods. The two-step methods (Elsner and Charniak, 2008, 2010, 2011; Chen et al., 2017; Jiang et al., 2018; Kummerfeld et al., 2019) firstly retrieve the relations between two messages, e.g., “reply-to” relations (Guo et al., 2018; Zhu et al., 2020), and then adopt a clustering algorithm to construct individual sessions. The end-to-end models (Tan et al., 2019; Liu et al., 2020; Yu and Joty, 2020), instead, perform the disentanglement operation in an end-to-end manner, where the context information of detached sessions will be exploited to classify a message to a session. End-to-end models tend to achieve better performance than two-step models, but both often need large annotated data to get fully trained (Liu et al., 2020), which is expensive to obtain and thus encourages the demand on unsupervised algorithms. A few preliminary studies perform unsupervised thread detection in email systems based on two-step methods (Wu and Oard, 2005; Erera and C"
2021.emnlp-main.181,2020.emnlp-main.533,0,0.0998043,"Missing"
2021.emnlp-main.181,N18-1113,0,0.0276953,"e the relations between two messages. The relation scores will be used as rewards for updating the session classifier during co-training. 2. A session classifier which can perform end-toend conversation disentanglement by retrieving the relations between a message and a session. The predicted results will be used to build new pseudo data to train the messagepair classifier during co-training. Co-training Co-training (Blum and Mitchell, 1998; Nigam and Ghani, 2000) has been widely used as a low-resource learning algorithm in nat3. A co-training algorithm involving the ural language processing (Wu et al., 2018; Chen message-pair classifier and the session et al., 2018), which assumes that the data has two classifier. Two classifiers will help to update 2347 each other until the performance of the session classifier stops growing. We will introduce the details of the three components in following sections. 4.1 Message-pair Classifier The message-pair classifier is a binary classifier which we denote as Fm in the remainder of this paper. Due to the lack of annotated data in unsupervised settings, the goal of Fm is to predict if two messages are in the same session; i.e., whether they talk about the s"
2021.emnlp-main.181,N16-1174,0,0.0467201,"d a message that indicates if the message belongs to the session or not. Given the current context of a session as T = [m1 , · · · , m|T |] and a message m, the goal of Ft is to decide if m can be appended to T or not. 4.2.1 Model For each message mj ∈ T , we obtain its sentence embedding vmj by a Bidirectional LSTM network (Hochreiter and Schmidhuber, 1997) and a multilayer perceptron (MLP): 2348 → − − v mj , ← v mj = BiLSTM(mj ) − − v = MLP([→ v ,← v ]) mj mj mj (5) (6) After obtaining sentence embeddings of all the messages in T as [vm1 , · · · vm|T |], we adopt a self attention mechanism (Yang et al., 2016) to calculate the session embedding vT by aggregating the information from different messages. Specifically, umj = tanh(w · vmj + b) exp(umj ) αmj = P j exp(umj ) X vT = αmj vmj (7) (8) (9) j where w and b are trainable parameters. For the message m, we use the same Bidirectional LSTM network and MLP as in Equation 5 and 6 to obtain its sentence embedding vm . Then the probability of m belonging to T is calculated with the dot product between vm and vT : pt = Sigmoid(vT · vm ) (10) We abbreviate the above process as: pt (T, m) = Ft (vT , vm ) (11) Ft is trained to minimize the cross-entropy lo"
2021.emnlp-main.181,2020.emnlp-main.512,0,0.250772,"ner and Charniak, 2011, 2008; Jiang et al., 2018), a model first retrieves the “local” relations between two messages 1 Introduction by utilizing either feature engineering approaches With the continuing growth of Internet and social or deep learning methods, and then a clustering media, online group chat channels, e.g., Slack1 and algorithm is employed to divide an entire converWhatsapp2 , among many others, have become in- sation into separate sessions based on the message creasingly popular and played a significant social pair relations. In contrast, end-to-end methods (Tan 1 et al., 2019; Yu and Joty, 2020) capture the “global” https://slack.com/ 2 https://www.whatsapp.com/ information contained in the context of detached 2345 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 2345–2356 c November 7–11, 2021. 2021 Association for Computational Linguistics relations between messages pairs as rewards message-pair classi er new pseudo data with high con dence session classi er test performance Figure 2: Illustration of our proposed co-training framework. model, which will be separately initialized with pseudo data built from the unannotated corpus and upda"
2021.emnlp-main.181,P14-1004,0,0.0237756,"nary studies perform unsupervised thread detection in email systems based on two-step methods (Wu and Oard, 2005; Erera and Carmel, 2008; Domeniconi et al., 2016), but these methods use handcrafted features which cannot be extended to various datasets. Compared with previous works, our method can conduct end-to-end conversation disentanglement in a completely unsupervised fashion, which can be easily adapted to downstream tasks and used in a wide variety of applications. Dialogue Structure Learning One problem that may be related to conversation disentanglement is dialogue structure learning (Zhai and Williams, 2014; Shi et al., 2019). Both are related to understanding multi-party conversation structures but they are different tasks. Dialogue structure learning aims to discover latent dialogue topics and construct an implicit utterance dependency tree to represent a multi-party dialogue’s turn taking (Qiu et al., 2020), while the goal of conversation disentanglement is to learn an explicit dividing scheme that separates intermingled messages into sessions. complementary views and utilizes two models to iteratively provide pseudo training signals to each other. Our method consists of a message-pair classi"
2021.emnlp-main.181,2020.acl-demos.30,0,0.0670153,"Missing"
2021.emnlp-main.307,D14-1059,0,0.021819,"ertain level of abstraction. This not only aligns with the idea that knowledge should be widely applicable but also provides the necessary means to reduce the ambiguity in sentences involving multiple people, objects, etc. 2. The symbols of the predicates are usually retained in the sentence whenever it is possible and natural to do so. will not help the reasoning process. For relevance, we adopt one of the following transformations for each knowledge sentence. Negation. Negating the meaning of the sentence will create a contradicting statement, following the Natural Logic inference system of Angeli and Manning (2014). For example, “X is tall enough” is transformed into “X is not tall enough”. Swapped or Replaced. Swap the positions of the two parties either in the antecedent or the consequent. Or replace one with the other. For example, “X is larger than Y” becomes “Y is larger than X”; and “X is angry” becomes “Y is angry”. Changed. Change the content of the sentence while preserving relevance. For example, “person X is a suspect of criminal” becomes “person X is hurt by a criminal”. Others. Use multiple transformations. We manually provide a pair of true and false knowledge sentences based on the verifi"
2021.emnlp-main.307,N19-1423,0,0.0610604,"Missing"
2021.emnlp-main.307,D12-1071,0,0.0925469,"Missing"
2021.emnlp-main.307,P19-1478,0,0.0202953,"u.edu.cn, ymliu@mail.sysu.edu.cn, xiaodan.zhu@queensu.ca Abstract which achieved near-human performance (Sakaguchi et al., 2020). The recent success of neural language modStill, the opacity of the NLMs has raised quesels (NLMs) on the Winograd Schema Chaltions about whether they truly capture common lenge has called for further investigation of the commonsense reasoning ability of these sense or merely exploit biases. Such concerns were models. Previous diagnostic datasets rely on confirmed for the natural language inference task, crowd-sourcing which fails to provide coherent as McCoy et al. (2019) discovered an LM could commonsense crucial for solving WSC probprovide correct answers using fallible heuristics. lems. To better evaluate NLMs, we propose To investigate if NLMs understand the reasons for a logic-based framework that focuses on highsolving WSC, Zhang et al. (2020) crowd-sourced quality commonsense knowledge. Specifically, reasons for the WSC problems and built a new we identify and collect formal knowledge fordataset WinoWhy. mulas verified by theorem provers and translate such formulas into natural language sentences. While in the correct direction, crowd-sourcing is Based"
2021.emnlp-main.307,2020.emnlp-main.664,0,0.0297448,"Missing"
2021.emnlp-main.307,D19-1335,0,0.0302622,"Missing"
2021.emnlp-main.307,W18-5446,0,0.067363,"Missing"
2021.emnlp-main.307,N18-1101,0,0.0949684,"Missing"
2021.emnlp-main.307,2020.acl-main.508,0,0.0331881,"ther they truly capture common lenge has called for further investigation of the commonsense reasoning ability of these sense or merely exploit biases. Such concerns were models. Previous diagnostic datasets rely on confirmed for the natural language inference task, crowd-sourcing which fails to provide coherent as McCoy et al. (2019) discovered an LM could commonsense crucial for solving WSC probprovide correct answers using fallible heuristics. lems. To better evaluate NLMs, we propose To investigate if NLMs understand the reasons for a logic-based framework that focuses on highsolving WSC, Zhang et al. (2020) crowd-sourced quality commonsense knowledge. Specifically, reasons for the WSC problems and built a new we identify and collect formal knowledge fordataset WinoWhy. mulas verified by theorem provers and translate such formulas into natural language sentences. While in the correct direction, crowd-sourcing is Based on these true knowledge sentences, adfar from perfect for collecting reliable explanations. versarial false ones are generated. We propose Consider the following WinoWhy example. a new dataset named W INO L OGIC with these Example 1 (WinoWhy). The trophy doesn’t fit into sentences."
2021.emnlp-main.320,2020.acl-main.426,0,0.0413374,"emotion in the next upcoming turn, with- emotional state in conversations and automatically out knowing the participant’s response yet. An learn whether the participant keeps the historical example of the task is shown in Figure 1. Emo- emotional state or is affected by others. tion inference is a necessary step for applications In addition, we propose an ensemble strategy such as dialogue planning, dialogue generation, to further enhance the model performance. Since and interpretability, among others (Lin et al., 2008; the exact response of the participant in the next Hasegawa et al., 2013; Gaonkar et al., 2020). For upcoming turn is unknown, there may be multiple example, in a human-machine conversation sce- potential emotional reactions. We run the models nario, if a chatbot tries to say something to cheer with different random seeds to generate multiple ∗ Corresponding author. candidate results, and then train a fusion classifier 3935 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 3935–3941 c November 7–11, 2021. 2021 Association for Computational Linguistics Surprise Neutral Anger 5) What about the scene 7) You fell with the kangaroo? asleep! 3) Whic"
2021.emnlp-main.320,2020.findings-emnlp.224,0,0.281022,"t teristics of emotional propagation in converupcoming utterance, in which the utterance in the sations, such as persistence and contagiousnext turn is not given. Hasegawa et al. (2013) studness. In this study, we focus on investigating the task of emotion inference in multi-turn ied a similar task to the emotion inference, howconversations by modeling the propagation of ever they only took two turns as context and the emotional states among participants in the conmulti-party multi-turn scenario was not considered. versation history, and propose an addresseeBothe et al. (2017) and Wang et al. (2020) estiaware module to automatically learn whether mate the sentiment polarity (negative or positive) the participant keeps the historical emotional of the next utterance, while our work anticipates the state or is affected by others in the next upcomfine-grained emotion, such as happy, sad, angry, ing turn. In addition, we propose an ensemexcited, and frustrated, etc. ble strategy to further enhance the model performance. Empirical studies on three different Although extensive related work has been conbenchmark conversation datasets demonstrate ducted, emotion inference in multi-turn conversath"
2021.emnlp-main.320,D19-1015,0,0.0317435,"Missing"
2021.emnlp-main.320,P13-1095,0,0.205348,"to perceive emotion of a given utterance, including bc-LSTM and reason about the future feelings of partic(Poria et al., 2017), DialogueRNN (Majumder et al., ipants, due to the lack of utterance informa2019), DialogueGCN (Zhong et al., 2019), COStion from the future. Moreover, it is crucial MIC (Ghosal et al., 2020), etc., while the emotion for emotion inference to capture the characinference task is to predict the emotion of the next teristics of emotional propagation in converupcoming utterance, in which the utterance in the sations, such as persistence and contagiousnext turn is not given. Hasegawa et al. (2013) studness. In this study, we focus on investigating the task of emotion inference in multi-turn ied a similar task to the emotion inference, howconversations by modeling the propagation of ever they only took two turns as context and the emotional states among participants in the conmulti-party multi-turn scenario was not considered. versation history, and propose an addresseeBothe et al. (2017) and Wang et al. (2020) estiaware module to automatically learn whether mate the sentiment polarity (negative or positive) the participant keeps the historical emotional of the next utterance, while our"
2021.emnlp-main.320,N18-1193,0,0.0158979,"GloVe-based encoder and the RoBERTa-based encoder respectively. The encoding process can be simplified as: u1 , u2 , · · · , um = CNN/RoBERTa(U1 , U2 , · · · , Um ), (2) where (U1 , U2 , · · · , Um ) is the conversation history, Ut is the utterance at time t and ut ∈ RH is the corresponding utterance representation encoded by CNN/RoBERTa, H = 100/1024. Addressee-Aware Module To infer and anticipate the participant’s emotion, it is important to model the emotion shift in conversations. In this work, we consider two basic characteristics of emotion: persistence and contagiousness (Picard, 1995; Hazarika et al., 2018), as the basis of inferring participant’s emotion. • Persistence. Participants may be consistently affected by their own mood and keep the existing emotional state for a period of time. For example, if a participant’s car breaks down, then the emotion of this participant may be sad for a long period of time in the conversation. • Contagiousness. The emotional states of participants are interactive, influential and contagious to each other. For example, a sad participant can be encouraged or comforted by others to be happy. Thus, the addressee pam+1 either keeps her/his own Feature Extraction:"
2021.emnlp-main.320,D14-1181,0,0.0137176,". • Persistence. Participants may be consistently affected by their own mood and keep the existing emotional state for a period of time. For example, if a participant’s car breaks down, then the emotion of this participant may be sad for a long period of time in the conversation. • Contagiousness. The emotional states of participants are interactive, influential and contagious to each other. For example, a sad participant can be encouraged or comforted by others to be happy. Thus, the addressee pam+1 either keeps her/his own Feature Extraction: First, we employ both a GloVe-based CNN encoder (Kim, 2014; Penning- historical emotional state or is affected by othton et al., 2014) and a RoBERTa Large encoder ers. In this paper, an addressee-aware module is 3936 proposed for both a sequence-based and a graphbased model to model these two kinds of emotion flow simultaneously. Sequence-based Model: We first categorize each utterance ut in the conversation history (u1 , u2 , · · · , ut , · · · , um ) into two types according to whether the utterance ut was spoken by the addressee pam+1 or others. Two different LSTM units, LST Mstore and LST Maf f ect , are then employed to control the different emo"
2021.emnlp-main.320,2021.ccl-1.108,0,0.0510708,"Missing"
2021.emnlp-main.320,D14-1162,0,0.0852696,"irectional LSTM model. DialogueRNN (Majumder et al., 2019) is an RNN-based model, which uses three separate GRU networks to keep track of the individual speaker states. DialogueGCN (Ghosal et al., 2019) uses a relational GCN to model the relation between utterances. COSMIC (Ghosal et al., 2020) is the state-of-the-art model in emotion recognition in conversations, which incorporates different elements of commonsense. All the baseline methods in our experiments use the same input features (Eq 2) as our proposed methods to ensure a fair comparison (300 dimensional pretrained 840B GloVe vectors (Pennington et al., 2014) for the GloVe-based models, and 1024 dimensional RoBERTa-Large (Liu et al., 2019) for the RoBERTa-based models). 4.3 Experimental Settings We use the batch size of 16, learning rate of 0.001, where ef am+1 ∈ RC is the output emotion proba- and dropout rate of 0.2 to train the inference models. bility distribution of the ensemble strategy. esam+1 , Cross entropy is used as the optimization objective eg am+1 , eiam+1 are the output emotion probability function of the model, and the optimization algodistributions of DialogInfer-S, DialogInfer-G, and rithm is Adam (Kingma and Ba, 2015). The hidde"
2021.emnlp-main.320,P17-1081,0,0.197917,", then the chatbot must predict the emotional consequence first, and avoid Emotion inference in multi-turn conversations words that may offend you or elicit negative emoaims to predict the participant’s emotion in tion on you. the next upcoming turn without knowing the Previous studies on emotion analysis in converparticipant’s response yet, and is a necessary sations have mainly focused on recognizing the step for applications such as dialogue planning. However, it is a severe challenge to perceive emotion of a given utterance, including bc-LSTM and reason about the future feelings of partic(Poria et al., 2017), DialogueRNN (Majumder et al., ipants, due to the lack of utterance informa2019), DialogueGCN (Zhong et al., 2019), COStion from the future. Moreover, it is crucial MIC (Ghosal et al., 2020), etc., while the emotion for emotion inference to capture the characinference task is to predict the emotion of the next teristics of emotional propagation in converupcoming utterance, in which the utterance in the sations, such as persistence and contagiousnext turn is not given. Hasegawa et al. (2013) studness. In this study, we focus on investigating the task of emotion inference in multi-turn ied a si"
2021.emnlp-main.320,P19-1050,0,0.0624086,"3935 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 3935–3941 c November 7–11, 2021. 2021 Association for Computational Linguistics Surprise Neutral Anger 5) What about the scene 7) You fell with the kangaroo? asleep! 3) Which part exactly? Joey Chandler Dialogue 1) You liked it? You really liked it? Neutral Emotion : 2) Oh, yeah! Joy 4) The whole thing! Can we go? Neutral What is Chandler&apos;s most likely emotion at turn 8? (Sadness) 6) I was surprised to see a kangaroo in a world war epic. Surprise Figure 1: A dialogue example in the MELD dataset (Poria et al., 2019). The task of emotion inference in multi-turn conversations is to predict Chandler’s emotion in the next upcoming turn (8) based on the previous 7 turns of the dialogue. to automatically select the final result most suitable for the current context and dialogue scene from the candidates. The main novelty and contribution of this work is that we propose an addressee-aware module for the emotion inference task to model the emotional characteristics and anticipate the emotion trend in multi-turn conversations. Moreover, an ensemble strategy is proposed to further enhance the model performance. Th"
2021.emnlp-main.320,2020.coling-main.221,0,0.0335176,"n of the next teristics of emotional propagation in converupcoming utterance, in which the utterance in the sations, such as persistence and contagiousnext turn is not given. Hasegawa et al. (2013) studness. In this study, we focus on investigating the task of emotion inference in multi-turn ied a similar task to the emotion inference, howconversations by modeling the propagation of ever they only took two turns as context and the emotional states among participants in the conmulti-party multi-turn scenario was not considered. versation history, and propose an addresseeBothe et al. (2017) and Wang et al. (2020) estiaware module to automatically learn whether mate the sentiment polarity (negative or positive) the participant keeps the historical emotional of the next utterance, while our work anticipates the state or is affected by others in the next upcomfine-grained emotion, such as happy, sad, angry, ing turn. In addition, we propose an ensemexcited, and frustrated, etc. ble strategy to further enhance the model performance. Empirical studies on three different Although extensive related work has been conbenchmark conversation datasets demonstrate ducted, emotion inference in multi-turn conversath"
2021.emnlp-main.320,D19-1016,0,0.0365608,"Missing"
2021.emnlp-main.86,D15-1075,0,0.0184181,"ency among conversation utterances. Second, the matching in SPD is In summary, our contributions in this paper are established between two sets of sentences which three-fold. (1) We propose a new task, Speaker requires a complicated many-to-many matching Persona Detection (SPD), and construct a dataset framework. This has not been explored yet, for studying this problem, which make the first attempt to detect speaker personas from converas previous studies have been conducted either between a pair of sentences (i.e., one-to-one) sational texts by persona matching. (2) Many (Wang et al., 2013; Bowman et al., 2015; Williams baseline methods have be established for the SPD et al., 2018), or between a set of sentences and task. (3) We propose U2P matching networks with a single sentence (i.e., many-to-one) (Lowe et al., a fine granularity to explore the matching between 2015; Lai et al., 2017; Wu et al., 2017). Third, there two sets of sentences, which outperform their C2P exists dynamic redundancy among conversation counterparts with a coarse granularity on the SPD utterances and persona profiles. Specifically, the task in our experiments. We hope the task and informativeness of each conversation uttera"
2021.emnlp-main.86,P17-1152,1,0.926676,"baselines. Furthermore, we propose utterance-to-profile (U2P) matching networks which are established at a fine granularity by treating each sentence in either contexts or personas individually. They first obtain the representation for each sentence and then derive the representations of contexts and personas through an aggregation operation. In this paper, (1) sentence-encoding-based framework such as the encoder of bag-of-words (BOW) (Joulin et al., 2017), BiLSTM (Hochreiter and Schmidhuber, 1997) and Transformer (Vaswani et al., 2017), and (2) cross-attention-based framework such as ESIM (Chen et al., 2017) and (3) pretraining-based framework such as BERT (Devlin et al., 2019) are considered when building either C2P and U2P models. The experimental results comparing the C2P and U2P models demonstrate the effectiveness of the latter for solving the SPD task, which relies on the many-to-many matching between two sets of sentences. In addition to the performance improvement, U2P models yield interpretability by explicitly scoring each utterance-profile pair and performing the aggregation operation. In our proposed SPD task, a conversation context is composed of multiple utterances and a persona is"
2021.emnlp-main.86,N19-1423,0,0.0513966,"ng networks which are established at a fine granularity by treating each sentence in either contexts or personas individually. They first obtain the representation for each sentence and then derive the representations of contexts and personas through an aggregation operation. In this paper, (1) sentence-encoding-based framework such as the encoder of bag-of-words (BOW) (Joulin et al., 2017), BiLSTM (Hochreiter and Schmidhuber, 1997) and Transformer (Vaswani et al., 2017), and (2) cross-attention-based framework such as ESIM (Chen et al., 2017) and (3) pretraining-based framework such as BERT (Devlin et al., 2019) are considered when building either C2P and U2P models. The experimental results comparing the C2P and U2P models demonstrate the effectiveness of the latter for solving the SPD task, which relies on the many-to-many matching between two sets of sentences. In addition to the performance improvement, U2P models yield interpretability by explicitly scoring each utterance-profile pair and performing the aggregation operation. In our proposed SPD task, a conversation context is composed of multiple utterances and a persona is composed of multiple profiles, which brings three challenges to existin"
2021.emnlp-main.86,2020.findings-emnlp.127,1,0.843972,"Profile 4: Autumn is my favorite season. Rank: Context-to-Persona (C2P) or Utterance-to-Profile (U2P) Matching Networks Persona of speaker candidate #3 …… #1 R #2 Q #3 Q #4 Q …… Persona of speaker candidate #4 …… …… Figure 1: Illustration of our proposed SPD task. The matching network judges whether a persona candidate matches with the conversational texts of the speaker. is given. Zhang et al. (2018); Liu et al. (2020); Song et al. (2021) built dialogue agents to perceive the user’s persona and then generated personalized responses. Mazare et al. (2018); Gu et al. (2019); Hua et al. (2020); Gu et al. (2020); Zhu et al. (2021) built matching networks to select a response matching not only the conversational context, but 1 Introduction also the user’s persona. Recently, human-machine conversation has However, these personas are pre-defined and achieved great success (Lowe et al., 2015; Serban difficult to obtain before a conversation, because et al., 2016; Wu et al., 2017; Madotto et al., 2018; the speaker might not want to fill out a specific Tao et al., 2019; Zhang et al., 2020; Roller et al., table to show its persona due to privacy issues. 2021), and has been applied to various scenarios, Henc"
2021.emnlp-main.86,D19-1193,1,0.852313,"file 3: My parents live in bora bora. Profile 4: Autumn is my favorite season. Rank: Context-to-Persona (C2P) or Utterance-to-Profile (U2P) Matching Networks Persona of speaker candidate #3 …… #1 R #2 Q #3 Q #4 Q …… Persona of speaker candidate #4 …… …… Figure 1: Illustration of our proposed SPD task. The matching network judges whether a persona candidate matches with the conversational texts of the speaker. is given. Zhang et al. (2018); Liu et al. (2020); Song et al. (2021) built dialogue agents to perceive the user’s persona and then generated personalized responses. Mazare et al. (2018); Gu et al. (2019); Hua et al. (2020); Gu et al. (2020); Zhu et al. (2021) built matching networks to select a response matching not only the conversational context, but 1 Introduction also the user’s persona. Recently, human-machine conversation has However, these personas are pre-defined and achieved great success (Lowe et al., 2015; Serban difficult to obtain before a conversation, because et al., 2016; Wu et al., 2017; Madotto et al., 2018; the speaker might not want to fill out a specific Tao et al., 2019; Zhang et al., 2020; Roller et al., table to show its persona due to privacy issues. 2021), and has be"
2021.emnlp-main.86,D18-1514,0,0.0335405,"Missing"
2021.emnlp-main.86,E17-2068,0,0.0583848,"context-to-persona (C2P) matching networks, which are established at a coarse granularity by concatenating two sets of sentences respectively, are employed as baselines. Furthermore, we propose utterance-to-profile (U2P) matching networks which are established at a fine granularity by treating each sentence in either contexts or personas individually. They first obtain the representation for each sentence and then derive the representations of contexts and personas through an aggregation operation. In this paper, (1) sentence-encoding-based framework such as the encoder of bag-of-words (BOW) (Joulin et al., 2017), BiLSTM (Hochreiter and Schmidhuber, 1997) and Transformer (Vaswani et al., 2017), and (2) cross-attention-based framework such as ESIM (Chen et al., 2017) and (3) pretraining-based framework such as BERT (Devlin et al., 2019) are considered when building either C2P and U2P models. The experimental results comparing the C2P and U2P models demonstrate the effectiveness of the latter for solving the SPD task, which relies on the many-to-many matching between two sets of sentences. In addition to the performance improvement, U2P models yield interpretability by explicitly scoring each utterance-"
2021.emnlp-main.86,I17-1011,0,0.104009,"onstruct a dataset framework. This has not been explored yet, for studying this problem, which make the first attempt to detect speaker personas from converas previous studies have been conducted either between a pair of sentences (i.e., one-to-one) sational texts by persona matching. (2) Many (Wang et al., 2013; Bowman et al., 2015; Williams baseline methods have be established for the SPD et al., 2018), or between a set of sentences and task. (3) We propose U2P matching networks with a single sentence (i.e., many-to-one) (Lowe et al., a fine granularity to explore the matching between 2015; Lai et al., 2017; Wu et al., 2017). Third, there two sets of sentences, which outperform their C2P exists dynamic redundancy among conversation counterparts with a coarse granularity on the SPD utterances and persona profiles. Specifically, the task in our experiments. We hope the task and informativeness of each conversation utterance datasets will invite more research on detecting changes when inferring different profiles in a speaker profiles from conversational texts. 1127 2 Related Work Speaker Profile in Text Maintaining the consistency between a speaker and its utterances is an important issue in many"
2021.emnlp-main.86,P16-1094,0,0.0769729,"Missing"
2021.emnlp-main.86,2020.acl-main.131,0,0.0589738,"Profile 4: They call me a bean counter. Persona of speaker candidate #2 Profile 1: I love to meet new people. Profile 2: I have a turtle named timothy. Profile 3: My parents live in bora bora. Profile 4: Autumn is my favorite season. Rank: Context-to-Persona (C2P) or Utterance-to-Profile (U2P) Matching Networks Persona of speaker candidate #3 …… #1 R #2 Q #3 Q #4 Q …… Persona of speaker candidate #4 …… …… Figure 1: Illustration of our proposed SPD task. The matching network judges whether a persona candidate matches with the conversational texts of the speaker. is given. Zhang et al. (2018); Liu et al. (2020); Song et al. (2021) built dialogue agents to perceive the user’s persona and then generated personalized responses. Mazare et al. (2018); Gu et al. (2019); Hua et al. (2020); Gu et al. (2020); Zhu et al. (2021) built matching networks to select a response matching not only the conversational context, but 1 Introduction also the user’s persona. Recently, human-machine conversation has However, these personas are pre-defined and achieved great success (Lowe et al., 2015; Serban difficult to obtain before a conversation, because et al., 2016; Wu et al., 2017; Madotto et al., 2018; the speaker mi"
2021.emnlp-main.86,W15-4640,0,0.163515,"etwork judges whether a persona candidate matches with the conversational texts of the speaker. is given. Zhang et al. (2018); Liu et al. (2020); Song et al. (2021) built dialogue agents to perceive the user’s persona and then generated personalized responses. Mazare et al. (2018); Gu et al. (2019); Hua et al. (2020); Gu et al. (2020); Zhu et al. (2021) built matching networks to select a response matching not only the conversational context, but 1 Introduction also the user’s persona. Recently, human-machine conversation has However, these personas are pre-defined and achieved great success (Lowe et al., 2015; Serban difficult to obtain before a conversation, because et al., 2016; Wu et al., 2017; Madotto et al., 2018; the speaker might not want to fill out a specific Tao et al., 2019; Zhang et al., 2020; Roller et al., table to show its persona due to privacy issues. 2021), and has been applied to various scenarios, Hence, the cold-start problem may hinder the such as customer service and conversational persona-aware response prediction in practice. To recommendation engine. It is well-known that a tackle this issue, our intuition is that the personal user’s persona can help the machine to genera"
2021.emnlp-main.86,P18-1136,0,0.0280293,"ang et al. (2018); Liu et al. (2020); Song et al. (2021) built dialogue agents to perceive the user’s persona and then generated personalized responses. Mazare et al. (2018); Gu et al. (2019); Hua et al. (2020); Gu et al. (2020); Zhu et al. (2021) built matching networks to select a response matching not only the conversational context, but 1 Introduction also the user’s persona. Recently, human-machine conversation has However, these personas are pre-defined and achieved great success (Lowe et al., 2015; Serban difficult to obtain before a conversation, because et al., 2016; Wu et al., 2017; Madotto et al., 2018; the speaker might not want to fill out a specific Tao et al., 2019; Zhang et al., 2020; Roller et al., table to show its persona due to privacy issues. 2021), and has been applied to various scenarios, Hence, the cold-start problem may hinder the such as customer service and conversational persona-aware response prediction in practice. To recommendation engine. It is well-known that a tackle this issue, our intuition is that the personal user’s persona can help the machine to generate information such as hobbies or occupations may more appropriate responses. Many studies be mentioned explici"
2021.emnlp-main.86,D18-1298,0,0.283843,"tle named timothy. Profile 3: My parents live in bora bora. Profile 4: Autumn is my favorite season. Rank: Context-to-Persona (C2P) or Utterance-to-Profile (U2P) Matching Networks Persona of speaker candidate #3 …… #1 R #2 Q #3 Q #4 Q …… Persona of speaker candidate #4 …… …… Figure 1: Illustration of our proposed SPD task. The matching network judges whether a persona candidate matches with the conversational texts of the speaker. is given. Zhang et al. (2018); Liu et al. (2020); Song et al. (2021) built dialogue agents to perceive the user’s persona and then generated personalized responses. Mazare et al. (2018); Gu et al. (2019); Hua et al. (2020); Gu et al. (2020); Zhu et al. (2021) built matching networks to select a response matching not only the conversational context, but 1 Introduction also the user’s persona. Recently, human-machine conversation has However, these personas are pre-defined and achieved great success (Lowe et al., 2015; Serban difficult to obtain before a conversation, because et al., 2016; Wu et al., 2017; Madotto et al., 2018; the speaker might not want to fill out a specific Tao et al., 2019; Zhang et al., 2020; Roller et al., table to show its persona due to privacy issues."
2021.emnlp-main.86,D14-1162,0,0.0855248,"and the whole set of profiles. 5 5.1 Experiments Evaluation Metrics We adopted the evaluation metrics popularly used by other text retrieval tasks (Lowe et al., 2015; Wu et al., 2017; Zhang et al., 2018). Each model was tasked with selecting the k best-matched ones from n available candidates, and we calculated the recall of the true positive one among the k selected candidates, denoted as Rn @k. In addition, mean reciprocal rank, denoted as MRRn , was also calculated, which was the average of reciprocal ranks of retrieval results among n available candidates. 5.2 Training Details embeddings (Pennington et al., 2014) and were not updated during training. All hidden states of the LSTM had 200 dimensions. The MLP at the prediction layer had 256 hidden units with ReLU (Nair and Hinton, 2010) activation. Dropout (Srivastava et al., 2014) with a rate of 0.2 was applied to the word embeddings and all hidden layers. The maximum utterance length, maximum number of utterances in a context, maximum profile length and maximum number of profiles in a persona were set to 20, 8, 15 and 5 respectively for PMPC. Zeros were padded if the number of utterances in a context and the number of profiles in a persona were less t"
2021.emnlp-main.86,P17-1046,1,0.899183,"ker. is given. Zhang et al. (2018); Liu et al. (2020); Song et al. (2021) built dialogue agents to perceive the user’s persona and then generated personalized responses. Mazare et al. (2018); Gu et al. (2019); Hua et al. (2020); Gu et al. (2020); Zhu et al. (2021) built matching networks to select a response matching not only the conversational context, but 1 Introduction also the user’s persona. Recently, human-machine conversation has However, these personas are pre-defined and achieved great success (Lowe et al., 2015; Serban difficult to obtain before a conversation, because et al., 2016; Wu et al., 2017; Madotto et al., 2018; the speaker might not want to fill out a specific Tao et al., 2019; Zhang et al., 2020; Roller et al., table to show its persona due to privacy issues. 2021), and has been applied to various scenarios, Hence, the cold-start problem may hinder the such as customer service and conversational persona-aware response prediction in practice. To recommendation engine. It is well-known that a tackle this issue, our intuition is that the personal user’s persona can help the machine to generate information such as hobbies or occupations may more appropriate responses. Many studie"
2021.emnlp-main.86,2021.eacl-main.24,0,0.0738384,"Missing"
2021.emnlp-main.86,2021.acl-long.14,0,0.0144326,"ll me a bean counter. Persona of speaker candidate #2 Profile 1: I love to meet new people. Profile 2: I have a turtle named timothy. Profile 3: My parents live in bora bora. Profile 4: Autumn is my favorite season. Rank: Context-to-Persona (C2P) or Utterance-to-Profile (U2P) Matching Networks Persona of speaker candidate #3 …… #1 R #2 Q #3 Q #4 Q …… Persona of speaker candidate #4 …… …… Figure 1: Illustration of our proposed SPD task. The matching network judges whether a persona candidate matches with the conversational texts of the speaker. is given. Zhang et al. (2018); Liu et al. (2020); Song et al. (2021) built dialogue agents to perceive the user’s persona and then generated personalized responses. Mazare et al. (2018); Gu et al. (2019); Hua et al. (2020); Gu et al. (2020); Zhu et al. (2021) built matching networks to select a response matching not only the conversational context, but 1 Introduction also the user’s persona. Recently, human-machine conversation has However, these personas are pre-defined and achieved great success (Lowe et al., 2015; Serban difficult to obtain before a conversation, because et al., 2016; Wu et al., 2017; Madotto et al., 2018; the speaker might not want to fill"
2021.emnlp-main.86,P18-1205,0,0.0433843,"Missing"
2021.emnlp-main.86,2020.acl-demos.30,0,0.0431148,"the user’s persona and then generated personalized responses. Mazare et al. (2018); Gu et al. (2019); Hua et al. (2020); Gu et al. (2020); Zhu et al. (2021) built matching networks to select a response matching not only the conversational context, but 1 Introduction also the user’s persona. Recently, human-machine conversation has However, these personas are pre-defined and achieved great success (Lowe et al., 2015; Serban difficult to obtain before a conversation, because et al., 2016; Wu et al., 2017; Madotto et al., 2018; the speaker might not want to fill out a specific Tao et al., 2019; Zhang et al., 2020; Roller et al., table to show its persona due to privacy issues. 2021), and has been applied to various scenarios, Hence, the cold-start problem may hinder the such as customer service and conversational persona-aware response prediction in practice. To recommendation engine. It is well-known that a tackle this issue, our intuition is that the personal user’s persona can help the machine to generate information such as hobbies or occupations may more appropriate responses. Many studies be mentioned explicitly or implicitly during a investigate how to predict a response if a persona conversati"
2021.emnlp-main.86,P19-1001,0,0.0121693,"agents to perceive the user’s persona and then generated personalized responses. Mazare et al. (2018); Gu et al. (2019); Hua et al. (2020); Gu et al. (2020); Zhu et al. (2021) built matching networks to select a response matching not only the conversational context, but 1 Introduction also the user’s persona. Recently, human-machine conversation has However, these personas are pre-defined and achieved great success (Lowe et al., 2015; Serban difficult to obtain before a conversation, because et al., 2016; Wu et al., 2017; Madotto et al., 2018; the speaker might not want to fill out a specific Tao et al., 2019; Zhang et al., 2020; Roller et al., table to show its persona due to privacy issues. 2021), and has been applied to various scenarios, Hence, the cold-start problem may hinder the such as customer service and conversational persona-aware response prediction in practice. To recommendation engine. It is well-known that a tackle this issue, our intuition is that the personal user’s persona can help the machine to generate information such as hobbies or occupations may more appropriate responses. Many studies be mentioned explicitly or implicitly during a investigate how to predict a response if"
2021.emnlp-main.86,D13-1096,0,0.229676,"he long-term dependency among conversation utterances. Second, the matching in SPD is In summary, our contributions in this paper are established between two sets of sentences which three-fold. (1) We propose a new task, Speaker requires a complicated many-to-many matching Persona Detection (SPD), and construct a dataset framework. This has not been explored yet, for studying this problem, which make the first attempt to detect speaker personas from converas previous studies have been conducted either between a pair of sentences (i.e., one-to-one) sational texts by persona matching. (2) Many (Wang et al., 2013; Bowman et al., 2015; Williams baseline methods have be established for the SPD et al., 2018), or between a set of sentences and task. (3) We propose U2P matching networks with a single sentence (i.e., many-to-one) (Lowe et al., a fine granularity to explore the matching between 2015; Lai et al., 2017; Wu et al., 2017). Third, there two sets of sentences, which outperform their C2P exists dynamic redundancy among conversation counterparts with a coarse granularity on the SPD utterances and persona profiles. Specifically, the task in our experiments. We hope the task and informativeness of eac"
2021.emnlp-main.86,N18-1101,0,0.016555,"uman. Profiles between two pieces of text is a basic problem in in a persona co-refer to each other, and each many natural language understanding tasks. The profile describes different properties of a coherent existing matching or classification tasks can be persona. Due to the natural dataset creating generally categorized into two main categories that method, the conversational texts conditioned establish the relationship either (1) between a pair on the given personas can intuitively reflect of sentences (Wang et al., 2013; Bowman et al., characteristics of the speaker, which creates 2015; Williams et al., 2018), or (2) between a set the natural matching relationship between the of sentences and a single sentence (Lowe et al., conversational texts and the persona of a speaker. 2015; Lai et al., 2017; Wu et al., 2017; Han et al., Thus, we assume that Persona-Chat is suitable 2018). We name them one-to-one and many-to-one for studying the SPD task. Since each dialogue matching in this paper. in Persona-Chat was performed between two Different from the studies mention above, our speakers, we can consider one of them as human proposed SPD task is a new many-to-many seman- speaker and the other as intelli"
2021.findings-emnlp.171,W14-3348,0,0.028836,"Missing"
2021.findings-emnlp.171,Q18-1031,0,0.0456509,"Missing"
2021.findings-emnlp.171,K19-1009,0,0.0904329,"n the widely used image captioning benchmarks. The proposed models achieve substantial improvement over the compared baselines on both composition related evaluation metrics and conventional image captioning metrics. Figure 1: Comparison of compositional generalization in generated descriptions between human and machine (Anderson et al., 2018). formance under a variety of text-similarity based metrics. However, when verbalising the visual semantic concepts into natural language sentences, these models still fall short of compositional generalization for images with novel concept combinations (Nikolaus et al., 2019). Note that making systematic generalizations (Lake and Baroni, 2018; Janssen and Partee, 1997) from limited data is an essential property of human language. As shown in Figure 1, the visual instances of “horse” and “cow” as well as the scene containing concept com1 Introduction binations of “cow eat” have been observed during Generating a textual description for a given image, training. While the existing models can often genera problem known as image captioning (Chen et al., ate “horse on” for the picture, it would be effortless for humans to generate a caption containing “horse 2015), requi"
2021.findings-emnlp.171,P02-1040,0,0.111503,"Missing"
2021.findings-emnlp.171,2021.acl-short.36,1,0.743665,"nd conventional evaluation metrics (from 109.9 to 114.3 on CIDEr). 2 Related Work Image Caption Generation Image captioning aims at generating visually grounded descriptions for images. Current models often leverage a CNN or variants as the image encoder and an RNN or transformer as the decoder to generate sentences (Vinyals et al., 2015; Karpathy and Fei-Fei, 2015; Donahue et al., 2015; Yang et al., 2016; Huang et al., 2019). Previous work has used a visual attention mechanism (Anderson et al., 2018; Pu et al., 2018; Lu et al., 2017; Pedersoli et al., 2017; Xu et al., 2015; Pan et al., 2020; Shi et al., 2021b), explicit high-level attributes detection (Yao et al., 2017; Wu et al., 2016; You et al., 2016) to align visual and textual features. For the learning method, people use reinforcement learning methods (Rennie et al., 2017; Ranzato et al., 2015; Liu et al., 2018), or contrastive or adversarial learning (Dai and Lin, 2017; Dai et al., 2017) to generate descriptive captions (Luo et al., 2018; Shi et al., 2021a) with improved quality. The distribution shift between training and test stages also has received a lot of attention, such as generating captions with novel concepts (Lu et al., 2018; Ag"
2021.findings-emnlp.171,2020.acl-main.664,1,0.715772,"problem known as image captioning (Chen et al., ate “horse on” for the picture, it would be effortless for humans to generate a caption containing “horse 2015), requires a conditional generation model to recognize salient visual regions, e.g., object (Ander- eat” even this combination has not been observed son et al., 2018) or scene graph detection (Yao et al., during training. It is partly due to the fact that current language generation models rely heavily 2018), align visual features with textual tokens (Lu on the surface distributional characteristics of the et al., 2017; Pu et al., 2018; Shi et al., 2020), and captions and hence are discouraged from generatverbalize them in a natural language sentence (Xu et al., 2015; Lu et al., 2018). Current state-of-the- ing unseen concept combinations (Holtzman et al., 2019; Nikolaus et al., 2019). art image captioning models benefit from powerful neural autoregressive generation models, attention To remedy the problem, we propose to leverage mechanisms, and progress in object or scene graph prototype-based generation approaches (Guu et al., detection. They have achieved significant progress 2018; Hashimoto et al., 2018) which can explicin obtaining visua"
2021.findings-emnlp.90,2020.acl-main.655,0,0.0274574,"e crucial to many applications such as de- simplified example in Figure 1. To avoid manutecting fake news and rumor (Rashkin et al., 2017; ally annotating gold decompositions, we design a program-guided pipeline to collect pseudo deThorne et al., 2018; Goodrich et al., 2019; Vaibhav compositions for training generation models by et al., 2019; Kryscinski et al., 2020). While existing distinguishing four major decomposition types and research mainly focuses on verification based on designing templates accordingly. The programs we unstructured text (Hanselowski et al., 2018; Yoneda et al., 2018; Liu et al., 2020; Nie et al., 2019), a re- used are parsed from statements with a weakly sucent trend is to explore structured data as evidence, pervised parser with the training signals from final verification labels. Figure 1 shows a statementwhich is ubiquitous in our daily life. program example. We adapt table-based natural Verification performed with structured data presents research challenges of fundamental inter- language understanding systems to solve the decomposed subproblems. After obtaining the anests, as it involves both informal inference based on swers to subproblems, we combine them in a pair"
2021.findings-emnlp.90,P19-1613,0,0.0212284,"ns. The ablation results in Table 3 indicate that data augmentation and the use of type in- split a complex sentence into a sequence of shorter sentences while preserving original meanings (Aha5 We also conduct significance tests over both the base and roni and Goldberg, 2018; Botha et al., 2018; Guo large models (the proposed model vs. TAPAS), with the oneet al., 2020). In QA task, question decomposition tail t-test. For the base model, the p-value is 4.7e-6 and for the large model, 3.2e-7. has been applied to help answer multi-hop ques1048 tions (Iyyer et al., 2016; Talmor and Berant, 2018; Min et al., 2019; Wolfson et al., 2020; Perez et al., 2020). Our work mainly focuses on decomposing statements for table-based fact verification with pseudo supervision from programs. Julian Eisenschlos, Syrine Krichene, and Thomas Müller. 2020. Understanding tables with intermediate pre-training. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 281–296, Online. Association for Computational Linguistics. 5 Ben Goodrich, Vinay Rao, Peter J. Liu, and Mohammad Saleh. 2019. Assessing the factual accuracy of generated text. In Proceedings of the 25th ACM SIGKDD International Conferen"
2021.findings-emnlp.90,D17-1064,0,0.0215052,"Chen et al., 2020). Unlike the pseudo decomposition dataset as the hold-out validation set, based on which we use BLEU-4 (Pa- the previous work (Chen et al., 2020; Zhong et al., pineni et al., 2002) to measure the generation qual- 2020; Shi et al., 2020; Zhang et al., 2020; Yang et al., 2020; Eisenschlos et al., 2020), we propose a ity. We also sample 100 decomposable cases from the TABFACT test set and ask three crowd work- framework to verify statements via decomposition. Sentence decomposition takes the form of Spliters to judge whether the model produces plausible and-Rephrase proposed by Narayan et al. (2017) to decompositions. The ablation results in Table 3 indicate that data augmentation and the use of type in- split a complex sentence into a sequence of shorter sentences while preserving original meanings (Aha5 We also conduct significance tests over both the base and roni and Goldberg, 2018; Botha et al., 2018; Guo large models (the proposed model vs. TAPAS), with the oneet al., 2020). In QA task, question decomposition tail t-test. For the base model, the p-value is 4.7e-6 and for the large model, 3.2e-7. has been applied to help answer multi-hop ques1048 tions (Iyyer et al., 2016; Talmor an"
2021.findings-emnlp.90,P02-1040,0,0.109069,"Missing"
2021.findings-emnlp.90,P15-1142,0,0.0255544,"formed decompositions are involved in the subsequent process to enhance the downstream verification. In case some substatements need further decomposition, it can be implemented by resending them to our pipeline3 . 2.3 Solving Subproblems We adapt TAPAS (Eisenschlos et al., 2020), a SOTA model on table-based fact verification and QA task, to solve the decomposed subproblems. Verifying sub-statements is formulated as a binary classification with the TAPAS model fine-tuned on the TAB FACT (Chen et al., 2020) dataset. To answer each sub-question, we use the TAPAS finetuned on WikiTableQuestions (Pasupat and Liang, 2015) dataset. We combine the subproblems and their answers in a pairwise manner to obtain the inN termediate evidence E = {ei }N i=1 = {(di , ai )}i=1 , an example evidence is shown in Figure 1. 3 In most cases, there is no need to perform iterative decomposition, and we leave finer-grained decomposition for future research. 2.4 Recombining Intermediate Evidence Downstream tasks can utilize the intermediate evidence in various ways. In this paper, we train a model to fuse the evidence E together with the statement S and table T for table-based fact verification4 . Specifically, we jointly encode S"
2021.findings-emnlp.90,2020.emnlp-main.713,0,0.01754,"cate that data augmentation and the use of type in- split a complex sentence into a sequence of shorter sentences while preserving original meanings (Aha5 We also conduct significance tests over both the base and roni and Goldberg, 2018; Botha et al., 2018; Guo large models (the proposed model vs. TAPAS), with the oneet al., 2020). In QA task, question decomposition tail t-test. For the base model, the p-value is 4.7e-6 and for the large model, 3.2e-7. has been applied to help answer multi-hop ques1048 tions (Iyyer et al., 2016; Talmor and Berant, 2018; Min et al., 2019; Wolfson et al., 2020; Perez et al., 2020). Our work mainly focuses on decomposing statements for table-based fact verification with pseudo supervision from programs. Julian Eisenschlos, Syrine Krichene, and Thomas Müller. 2020. Understanding tables with intermediate pre-training. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 281–296, Online. Association for Computational Linguistics. 5 Ben Goodrich, Vinay Rao, Peter J. Liu, and Mohammad Saleh. 2019. Assessing the factual accuracy of generated text. In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, KD"
2021.findings-emnlp.90,D17-1317,0,0.0227835,"as BERT (Devlin et al., 2019) have presented superior performances on verifying simple statements while still struggling with complex ones: a performance gap exists between the simple and complex tracks (Chen et al., 2020). 1 Introduction In this paper, we propose to decompose complex statements into simpler subproblems to imFact verification aims to validate if a statement prove table-based fact verification, as shown in a is entailed or refuted by given evidence. It has become crucial to many applications such as de- simplified example in Figure 1. To avoid manutecting fake news and rumor (Rashkin et al., 2017; ally annotating gold decompositions, we design a program-guided pipeline to collect pseudo deThorne et al., 2018; Goodrich et al., 2019; Vaibhav compositions for training generation models by et al., 2019; Kryscinski et al., 2020). While existing distinguishing four major decomposition types and research mainly focuses on verification based on designing templates accordingly. The programs we unstructured text (Hanselowski et al., 2018; Yoneda et al., 2018; Liu et al., 2020; Nie et al., 2019), a re- used are parsed from statements with a weakly sucent trend is to explore structured data as ev"
2021.findings-emnlp.90,2020.coling-main.466,0,0.029933,"ion of Decompositions. We use both an 2018; Hanselowski et al., 2018; Yoneda et al., 2018; automated metric and human validation to evaluate Thorne et al., 2019; Nie et al., 2019; Liu et al., the decomposition quality. For the automated met2020). Our work focuses on fact verification based ric, we randomly sample 1,000 training cases from on structured tables (Chen et al., 2020). Unlike the pseudo decomposition dataset as the hold-out validation set, based on which we use BLEU-4 (Pa- the previous work (Chen et al., 2020; Zhong et al., pineni et al., 2002) to measure the generation qual- 2020; Shi et al., 2020; Zhang et al., 2020; Yang et al., 2020; Eisenschlos et al., 2020), we propose a ity. We also sample 100 decomposable cases from the TABFACT test set and ask three crowd work- framework to verify statements via decomposition. Sentence decomposition takes the form of Spliters to judge whether the model produces plausible and-Rephrase proposed by Narayan et al. (2017) to decompositions. The ablation results in Table 3 indicate that data augmentation and the use of type in- split a complex sentence into a sequence of shorter sentences while preserving original meanings (Aha5 We also conduct signi"
2021.findings-emnlp.90,N18-1059,0,0.0233168,"l. (2017) to decompositions. The ablation results in Table 3 indicate that data augmentation and the use of type in- split a complex sentence into a sequence of shorter sentences while preserving original meanings (Aha5 We also conduct significance tests over both the base and roni and Goldberg, 2018; Botha et al., 2018; Guo large models (the proposed model vs. TAPAS), with the oneet al., 2020). In QA task, question decomposition tail t-test. For the base model, the p-value is 4.7e-6 and for the large model, 3.2e-7. has been applied to help answer multi-hop ques1048 tions (Iyyer et al., 2016; Talmor and Berant, 2018; Min et al., 2019; Wolfson et al., 2020; Perez et al., 2020). Our work mainly focuses on decomposing statements for table-based fact verification with pseudo supervision from programs. Julian Eisenschlos, Syrine Krichene, and Thomas Müller. 2020. Understanding tables with intermediate pre-training. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 281–296, Online. Association for Computational Linguistics. 5 Ben Goodrich, Vinay Rao, Peter J. Liu, and Mohammad Saleh. 2019. Assessing the factual accuracy of generated text. In Proceedings of the 25th ACM SIGKDD Inte"
2021.findings-emnlp.90,N18-1074,0,0.0648288,"Missing"
2021.findings-emnlp.90,D19-6601,0,0.0297931,"Missing"
2021.findings-emnlp.90,D19-5316,0,0.033582,"Missing"
2021.findings-emnlp.90,2020.tacl-1.13,0,0.0203435,"esults in Table 3 indicate that data augmentation and the use of type in- split a complex sentence into a sequence of shorter sentences while preserving original meanings (Aha5 We also conduct significance tests over both the base and roni and Goldberg, 2018; Botha et al., 2018; Guo large models (the proposed model vs. TAPAS), with the oneet al., 2020). In QA task, question decomposition tail t-test. For the base model, the p-value is 4.7e-6 and for the large model, 3.2e-7. has been applied to help answer multi-hop ques1048 tions (Iyyer et al., 2016; Talmor and Berant, 2018; Min et al., 2019; Wolfson et al., 2020; Perez et al., 2020). Our work mainly focuses on decomposing statements for table-based fact verification with pseudo supervision from programs. Julian Eisenschlos, Syrine Krichene, and Thomas Müller. 2020. Understanding tables with intermediate pre-training. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 281–296, Online. Association for Computational Linguistics. 5 Ben Goodrich, Vinay Rao, Peter J. Liu, and Mohammad Saleh. 2019. Assessing the factual accuracy of generated text. In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discov"
2021.findings-emnlp.90,2020.emnlp-main.628,1,0.847391,"el training. Since semantic parsers can map statements into executable programs that not only capture the semantics but also reveal the compositional structures of the statements, we propose a program-guided pipeline to construct a pseudo decomposition dataset. 2.2.1 Constructing Pseudo Decompositions Program Acquisition. Following Chen et al. (2020), we use latent program algorithm (LPA) to parse each statement S into a set of candidate programs Z = {zi }K i=1 . To select the most semantically consistent program z ∗ among all candidates and mitigate the impact of spurious programs, we follow Yang et al. (2020) to optimize the program selection model with a margin loss, which is detailed in Appendix A.1. By further removing programs that are labelinconsistent or cannot be split into two isolated sub-programs from the root operator, we obtain the remaining (T, S, z) triples as the source of data construction1 . Decomposition Templates. Programs are formal, unambiguous meaning representations for the corresponding statements. Designed to support 1 These triples do not involve any tables or statements in the dev/test set of the dataset used in this paper. S princes park venue recorded the highest crowd"
2021.findings-emnlp.90,W18-5515,0,0.0166053,"vidence. It has become crucial to many applications such as de- simplified example in Figure 1. To avoid manutecting fake news and rumor (Rashkin et al., 2017; ally annotating gold decompositions, we design a program-guided pipeline to collect pseudo deThorne et al., 2018; Goodrich et al., 2019; Vaibhav compositions for training generation models by et al., 2019; Kryscinski et al., 2020). While existing distinguishing four major decomposition types and research mainly focuses on verification based on designing templates accordingly. The programs we unstructured text (Hanselowski et al., 2018; Yoneda et al., 2018; Liu et al., 2020; Nie et al., 2019), a re- used are parsed from statements with a weakly sucent trend is to explore structured data as evidence, pervised parser with the training signals from final verification labels. Figure 1 shows a statementwhich is ubiquitous in our daily life. program example. We adapt table-based natural Verification performed with structured data presents research challenges of fundamental inter- language understanding systems to solve the decomposed subproblems. After obtaining the anests, as it involves both informal inference based on swers to subproblems, we comb"
2021.findings-emnlp.90,2020.emnlp-main.126,0,0.066611,"Missing"
2021.findings-emnlp.90,2020.acl-main.539,0,0.03303,"Missing"
2021.naacl-main.83,D18-1352,0,0.265099,"Classification through Reinforced Label Hierarchy Reasoning Hui Liu1 Danqing Zhang2 Bing Yin2 Xiaodan Zhu1 1 Ingenuity Labs Research Institute & ECE, Queen’s University, Canada {hui.liu, xiaodan.zhu}@queensu.ca 2 Amazon.com Inc, Palo Alto, CA, USA {danqinz, alexbyin}@amazon.com Abstract Exploiting label hierarchies has become a promising approach to tackling the zero-shot multi-label text classification (ZS-MTC) problem. Conventional methods aim to learn a matching model between text and labels, using a graph encoder to incorporate label hierarchies to obtain effective label representations (Rios and Kavuluru, 2018). More recently, pretrained models like BERT (Devlin et al., 2018) have been used to convert classification tasks into a textual entailment task (Yin et al., 2019). This approach is naturally suitable for the ZSMTC task. However, pretrained models are underexplored in the existing work because they do not generate individual vector representations for text or labels, making it unintuitive to combine them with conventional graph encoding methods. In this paper, we explore to improve pretrained models with label hierarchies on the ZS-MTC task. We propose a Reinforced Label Hierarchy Reasoning (R"
2021.naacl-main.83,2020.emnlp-main.235,0,0.138786,"Bikes Figure 1: An example of label hierarchy and predictions with logical errors. Circled labels are model predictions without incorporating label hierarchy. The existing zero-shot learning for multi-label text classification (ZS-MTC) mostly learns a matching model between the feature space of text and the label space (Ye et al., 2020). In order to learn effective representations for labels, a majority of existing work incorporates label hierarchies via a label encoder designed as Graph Neural Networks (GNNs) that can aggregate the neighboring information for labels (Chalkidis et al., 2020; Lu et al., 2020). Recently, pretrained models like BERT (Devlin et al., 2018) have been widely used as strong matching models due to their superior representation ability (Qiao et al., 2019). They have been applied to convert a classification task to a textual entailment task, by treating the text to be classified as the premise, and its label as the hypothesis, which is naturally suitable for the ZS-MTC study (Yin et al., 2019). However, the problem of this approach is 1 Introduction that pretrained models cannot generate individual vector representations for labels—a label is couMulti-label text classificat"
2021.naacl-main.83,D19-1042,0,0.117417,"r contributions as follows: matching-score-based rollback algorithm to introduce the structural information of label hierarchies to pretrained models in both the training and inference stage. • Experiments with different pretrained models are performed on three real-life datasets. We show the effectiveness of our proposed approach and provide detailed analyses. 2 Related Work Exploiting the prior distribution of the label space has proven to be an effective method to tackle the multi-label text classification problem because it can provide the model with information about the label structure. Mao et al. (2019); Huang et al. (2019) took the explicitly represented label hierarchy as the structural information, while Wu et al. (2019) assumed the prior distribution to be implicit and trained their model to learn the distribution during learning. Leveraging the label hierarchy to tackle ZSMTC has shown to be promising in previous work, which mostly aimed to learn a matching model between texts and labels. Chalkidis et al. (2020, 2019); Xie et al. (2019) adopted Label-Wise Attention Networks to encourage interactions between text and labels. Rios and Kavuluru (2018); Lu et al. (2020) used Graph Neural Ne"
2021.naacl-main.83,D19-1444,0,0.0217724,"hies to pretrained models in both the training and inference stage. • Experiments with different pretrained models are performed on three real-life datasets. We show the effectiveness of our proposed approach and provide detailed analyses. 2 Related Work Exploiting the prior distribution of the label space has proven to be an effective method to tackle the multi-label text classification problem because it can provide the model with information about the label structure. Mao et al. (2019); Huang et al. (2019) took the explicitly represented label hierarchy as the structural information, while Wu et al. (2019) assumed the prior distribution to be implicit and trained their model to learn the distribution during learning. Leveraging the label hierarchy to tackle ZSMTC has shown to be promising in previous work, which mostly aimed to learn a matching model between texts and labels. Chalkidis et al. (2020, 2019); Xie et al. (2019) adopted Label-Wise Attention Networks to encourage interactions between text and labels. Rios and Kavuluru (2018); Lu et al. (2020) used Graph Neural Networks to capture the structural information in the label hierarchy. However, few existing works investigate the effectiven"
2021.naacl-main.83,D17-1060,0,0.0277914,"ing inference. In our work, we will investigate such a method and see that the hierarchical inference method is not optimal for pretrained models on the ZS-MTC task because it broadcasts errors top-down in the label hierarchy. Path reasoning is effective for exploiting explicit • We demonstrate that pretrained models out- relationships in structured data, which can be comperform conventional methods on ZS-MTC. bined with reinforcement learning, e.g., knowledge • We design a novel Reinforced Label Hier- graph reasoning (Wan et al., 2020; Xian et al., 2019; archy Reasoning (RLHR) approach and a Xiong et al., 2017). We propose to introduce the 1 label hierarchy to pretrained models through path Code and data available at https://github.com/ layneins/Zero-shot-RLHR reasoning, with the aim to strengthen the intercon1052 nections between labels. To the best of our knowledge, our work is the first to improve pretrained models through label hierarchies for ZS-MTC. 3 3.1 Problem Formulation Label Hierarchy Reasoning In general, a label hierarchy is defined as G = (L, E), where L and E are a set of labels and relations, respectively. The latter represent parentchild relations between labels. The root of G is a"
2021.naacl-main.83,2020.acl-main.272,0,0.0308304,"n the ZS-MTC task. Root Active Life Bike Rentals Shopping Local Services Sporting Goods Missing Correct Bike Repair Wrong Input: It’s no doubt the best store to get a bike if you want to do bicycling on weekends!!! Labels: Active Life Shopping Bike Rentals Sporting Goods Bikes Bikes Figure 1: An example of label hierarchy and predictions with logical errors. Circled labels are model predictions without incorporating label hierarchy. The existing zero-shot learning for multi-label text classification (ZS-MTC) mostly learns a matching model between the feature space of text and the label space (Ye et al., 2020). In order to learn effective representations for labels, a majority of existing work incorporates label hierarchies via a label encoder designed as Graph Neural Networks (GNNs) that can aggregate the neighboring information for labels (Chalkidis et al., 2020; Lu et al., 2020). Recently, pretrained models like BERT (Devlin et al., 2018) have been widely used as strong matching models due to their superior representation ability (Qiao et al., 2019). They have been applied to convert a classification task to a textual entailment task, by treating the text to be classified as the premise, and its"
2021.naacl-main.83,D19-1404,0,0.282029,"Canada {hui.liu, xiaodan.zhu}@queensu.ca 2 Amazon.com Inc, Palo Alto, CA, USA {danqinz, alexbyin}@amazon.com Abstract Exploiting label hierarchies has become a promising approach to tackling the zero-shot multi-label text classification (ZS-MTC) problem. Conventional methods aim to learn a matching model between text and labels, using a graph encoder to incorporate label hierarchies to obtain effective label representations (Rios and Kavuluru, 2018). More recently, pretrained models like BERT (Devlin et al., 2018) have been used to convert classification tasks into a textual entailment task (Yin et al., 2019). This approach is naturally suitable for the ZSMTC task. However, pretrained models are underexplored in the existing work because they do not generate individual vector representations for text or labels, making it unintuitive to combine them with conventional graph encoding methods. In this paper, we explore to improve pretrained models with label hierarchies on the ZS-MTC task. We propose a Reinforced Label Hierarchy Reasoning (RLHR) approach to encourage interdependence among labels in the hierarchies during training. Meanwhile, to overcome the weakness of flat predictions, we design a ro"
2021.semeval-1.4,P17-1168,0,0.313381,"(↓ 14.4) 7 ZJUKLAB 87.9 - 8 IIE-NLP-Eyas 87.5 82.1(↓ 5.4) 9 hzxx1997 86.7 - 10 XRJL 86.7 81.8(↓ 4.9) 11 noobs 86.2 78.6(↓ 7.6) 12 godrevl 83.1 - 13 ReCAM@IITK 82.1 80.7(↓ 1.4) 14 DeepBlueAI 81.8 76.3(↓ 5.5) 15 LRG 75.3 61.8(↓ 13.5) 16 xuliang 74.7 - 17 Llf1206571288 72.8 - 18 Qing 71.4 - 19 NEUer 56.6 51.8(↓ 4.8) 20 CCLAB 46.3 35.2(↓ 11.1) 21 UoR 42.0 39.4(↓ 2.6) 22 munia 19.3 - 23 BaoShanCollege 19.0 - Subtask 1: ReCAM-Imperceptibility Table 6 shows all the official submissions and most of them outperform the baseline model. The baseline used for Subtask 1 is the Gated-Attention (GA) Reader (Dhingra et al., 2017). The GA Reader uses a multi-layer iterated architecture with a gated attention mechanism to derive better query-aware passage representation. The motivation behind using GA Reader is to have a simple comparison between our task and the CNN/Daily Mail reading comprehension dataset since GA Reader achieves reasonably good performance on the CNN/Daily Mail reading comprehension dataset. Note that the last column of the table lists the accuracy (Acc. Cross) for models trained on the Subtask 2 training data and tested on the Subtask 1 testset. We will discuss those results later in Section 4.3. Th"
2021.semeval-1.4,2021.semeval-1.105,0,0.360297,"g structured knowledge through unimportant edges, they propose a noise reduction strategy. owlmx used the MRC Psycholinguistic Database to obtain a measurement of imperceptibility abstractness. Different pre-processing techniques were proposed in multiple systems. ZJUKLAB (Xie et al., 2021a) used a sliding window to limit input length in training. PINGAN Omini-Sinitic (Wang et al., 2021) used the cycle noisy label detection algorithm to make models more robust. Much interesting analysis regarding the failure cases and data distribution was discussed in several system description papers. XRJL (Jiang et al., 2021) found that for a few questions, common Rank Team Acc. Acc. Cross - GA Reader 24.3 - 1 PINGANOmini-Sinitic 95.3 94.2 (↓ 1.1) 2 SRC-B-roc 94.9 93.9(↓ 1.0) 3 tt123 93.4 85.8(↓ 7.6) 4 ECNU-ICA-1 93.0 92.8(↓ 0.2) 5 cxn 92.9 - 6 ZJUKLAB 92.8 - 7 nxc 92.7 - 8 hzxx1997 90.2 - 9 XRJL 90.0 87.6(↓ 2.4) 10 IIE-NLP-Eyas 89.6 84.1(↓ 5.5) 11 ReCAM@IITK 87.6 85.2(↓ 2.4) 12 noobs 87.1 82.4(↓ 4.7) 13 DeepBlueAI 86.2 80.7(↓ 5.5) 14 xuliang 81.0 - 15 LRG 77.8 65.6(↓ 12.2) 16 Yotta 71.6 - 17 sayazzad 68.3 - 18 itanhisada 67.7 - 19 NEUer 66.9 45.0(↓ 21.9) 20 YaA@JUST 66.1 - 21 NLP-IIS@UT 64.4 - 22 CCLAB 48.1 31.8("
2021.semeval-1.4,D16-1241,0,0.185346,"ment analysis, summarization, and word sense disambiguation. In the past decade, significant advancement has been seen in developing computational models for semantics, based on deep neural networks. In this shared task, we aim to help assess the capability of the state-of-the-art deep learning models on representing and modelling abstract concepts in a specific reading comprehension setup. We introduce SemEval-2021 Task 4, Reading Comprehension of Abstract Meaning (ReCAM). Specifically, we design this shared task by following the machine reading comprehension framework (Hermann et al., 2015; Onishi et al., 2016; Hill et al., 2016), in which computers are given a passage Di as well as a human summary Si to comprehend. If a model can digest the passage as humans do, we expect it to predict the abstract word used in the summary, if the abstract word is masked. Unlike the previous work that requires computers to predict concrete concepts, e.g., named entities, in our task we ask models to fill in abstract words removed from human summaries. During the SemEval-2021 official evaluation period, we received 23 submissions to Subtask 1 and 28 submissions to Subtask 2. The participating teams additionally mad"
2021.semeval-1.4,Q18-1023,0,0.0577215,"Missing"
2021.semeval-1.4,D14-1162,0,0.0987847,"efinitions of abstractness. 3 Finding Imperceptible Concepts Abstractness Scorer for Imperceptibility Following Turney et al. (2011), we use the MRC Psycholinguistic Database (Coltheart, 1981), which includes 4,295 words rated with a degree of abstractness by human subjects, to train our abstractness scorer for imperceptibility. The rating of the words in the MRC Psycholinguistic Database ranges from 158 (highly abstract) to 670 (highly concrete). We linearly scale the rating to the range of 0 (highly abstract) to 1 (highly concrete). The neural regression model accepts fixed Glove embedding (Pennington et al., 2014) as input and predicts the abstractness rating score between 0 and 1. Our regression model is a three-layer network that consists of two nonlinear hidden layers with the ReLU activation and a sigmoid output layer. The mean square error (MSE) is used as the training loss. To test the regression model’s performance, we randomly split the MRC Psycholinguistic Database into train and test set with the size of 2,148 and 1,877, respectively. Table 2 shows the final performance of the neural regression model on the MRC database. We use the Pearson correlation between ratings predicted by models and o"
2021.semeval-1.4,W17-2623,0,0.0608683,"Missing"
2021.semeval-1.4,D11-1063,0,0.616621,"uring the SemEval-2021 official evaluation period, we received 23 submissions to Subtask 1 and 28 to Subtask 2. The participating teams additionally made 29 submissions to Subtask 3. The leaderboard and competition website can be found at https://competitions .codalab.org/competitions/26153. The data and baseline code are available at https://github.com/boyuanzheng010/ SemEval2021-Reading-Comprehensionof-Abstract-Meaning. 1 Introduction 2 Humans use words with abstract meaning in their daily life. In the past, research efforts have been exerted to better understand and model abstract meaning (Turney et al., 2011; Theijssen et al., Task Description We organize our shared task based on two typical definitions of abstractness, named as imperceptibility and nonspecificity in this paper, implemented in Subtask 1 and Subtask 2, respectively. Subtask 3 further evaluates models’ generalizability over the two definitions of abstractness. ∗ This work was performed when Boyuan Zheng visited Queen’s University. 37 Proceedings of the 15th International Workshop on Semantic Evaluation (SemEval-2021), pages 37–50 Bangkok, Thailand (online), August 5–6, 2021. ©2021 Association for Computational Linguistics Passage 3"
2021.semeval-1.4,2021.semeval-1.109,0,0.445334,"asonably good performance on the CNN/Daily Mail reading comprehension dataset. Note that the last column of the table lists the accuracy (Acc. Cross) for models trained on the Subtask 2 training data and tested on the Subtask 1 testset. We will discuss those results later in Section 4.3. The best result in Subtask 1 was achieved by team SRC-B-roc (Zhang et al., 2021) with an accuracy of 0.951. The system was built on a pre-trained ELECTRA discriminator and it further applied upper attention and auto-denoising mechanism to process long sequences. The second-placed system, PINGAN omini-Sinitic (Wang et al., 2021), adopted an ensemble of ELECTRA-based models with task-adaptive pre-training and a mutlihead attention based multiple-choice classifier. ECNU-ICA-1 (Liu et al., 2021) ranked third in this subtask with a knowledge-enhanced Graph Attention Network and a semantic space transformation strategy. Most teams in Subtask 1 utilize pre-trained language models (PLM), like BERT (Devlin et al., 2019), ALBERT (Lan et al., 2020), DistilBERT (Sanh et al., 2019), RoBERTa (Liu et al., 2019), ELECTRA (Clark et al., 2020), DeBERTa (He et al., 2020), XLNet (Yang et al., 2019), T5 (Raffel et al., 2020). SRC-B-roc"
2021.semeval-1.4,2021.semeval-1.108,0,0.280938,"ELECTRA outperforms BERT and ALBERT by large margins, which may be due to the different learning objecTable 6: Official results of Subtask 1 and Subtask 3. Acc is the accuracy of the models trained on the Subtask 1 training data and tested on the Subtask 1 testset. Acc. cross is the accuracy of models trained on the Subtask 2 training data and tested on the Subtask 1 testset. tives of these pre-trained models. Most participating systems performed intermediate task pre-training (Pruksachatkun et al., 2020) for their language models. For example, CNN/Daily Mail dataset was selected by ZJUKLAB (Xie et al., 2021a) to further pretrain their language models. The CNN/Daily Mail dataset and Newsroom dataset boost model performance on both Subtask 1 and Subtask 2. Data augmentation methods are also popular among participants. ZJUKLAB (Xie et al., 2021a) performed negative data augmentation with a 41 language model to leverage misleading words. IIE-NLP-Eyas (Xie et al., 2021b) adopted template-based input reconstruction methods to augment their dataset and further fine-tuned their language models based on the dataset. Most teams also used an ensemble of multiple pre-trained language models to further enhan"
2021.semeval-1.4,2021.semeval-1.22,0,0.0559165,"Missing"
2021.semeval-1.50,2021.semeval-1.39,0,0.0315653,"tudies. Verifying statements based on tables is a challenging task since the researches on understanding tables are not enough compared with works on free-texts, and the methods to train models understanding free-text and table jointly need further studies as well. While TABFACT contains a huge number of statements and tables, the tables in the TABFACT dataset are relatively simple since they do not have hierarchical column heads as tables in scientific papers do and the contents in the tables are easier to understand compared to the tables in scientific papers as well. In the SemEval task 9 (Wang et al., 2021), the goal is to develop a system that can verify statements (subtask A) and find evidence (subtask B) based on the tables extracted from scientific tables. Our team is more interested in the verifying task and only participated in subtask A. Different from data in TABFACT, in subtask A, we are also required to classify a new type of statement that cannot be entailed or refuted based on the given table, named “unknown” type. Since no statements of this type are given in the training data, the classification of this type of statement becomes a core difficulty of the subtask. This paper describe"
C10-2177,J97-1003,0,0.389377,"2 0.3 0.4 Word error rate WindowDiff under different WERs B−ALN HG−ALN AUDIO 0.4 WindowDiff utilizes the hierarchical structures of slides and global distribution of words, i.e., the HG-ALN model, reduces both Pk and WindowDiff scores over the baseline model, B-ALN. As discussed earlier, the baseline is a re-implementation of standard dynamic time warping based only on a pre-order walk of the slides, while the HG-ALN model incorporates also hierarchical bullet constraints and global word distribution. Table 1 also presents the performance of a typical topic segmentation algorithm, TextTiling (Hearst, 1997). Note that similar to (Malioutov et al., 2007), we force the number of predicted topic segments to be the target number, i.e., in our task, the number of bullets. The results show that both the Pk and WindowDiff scores of TextTiling are significantly higher than those of the alignment algorithms. Our manual analysis suggests that many segments are as short as several utterances and the difference between two consecutive segments is too subtle to be captured by a lexical cohesionbased method such as TextTiling. For comparison, We also present the results of uniform segmentation (UNI), which si"
C10-2177,W06-1644,0,0.33355,"Missing"
C10-2177,J02-4006,0,0.125056,"w the tree structure in advance and therefore we know that the starting position of the next sibling bullet is the ending boundary for the current bullet. 4 Our approaches Our task is to find the correspondence between slide bullets and a speech sequence or its transcripts. Research on finding correspondences between parallel texts pervades natural language processing. For example, aligning bilingual sentence pairs is an essential step in training machine translation models. In text summarization, the correspondence between human-written summaries and their original texts has been identified (Jing, 2002), too. In speech recognition, forced alignment is applied to align speech and transcripts. In this paper, we keep the general framework of alignment in solving our problem. Our solution, however, should be flexible to consider multiple constraints such as those conveyed in hierarchical bullet structures and global word distribution. Accordingly, the model proposed in this paper depends on two orthogonal strategies to ensure efficiency and richness of the model. First of all, we formulate all our solutions within a classic dynamic programming framework to enforce computational efficiency (secti"
C10-2177,P07-1064,0,0.360242,"gnores the hierarchical structure of bullets within slides. We also explore the impact of speech recognition errors on this task. Furthermore, we study the feasibility of directly aligning a structure to raw speech, as opposed to a transcript. 2 Related work Topic/slide boundary detection The previous work most directly related to ours is research that attempts to find flat structures of spoken documents, such as topic and slide boundaries. For example, the work of (Chen and Heng, 2003; Ruddarraju, 2006; Zhu et al., 2008) aims to find slide boundaries in the corresponding lecture transcripts. Malioutov et al. (2007) developed an approach to detecting topic boundaries of lecture recordings by finding repeated acoustic patterns. None of this work, however, has involved hierarchical structures that exist at different levels of a document. In addition, researchers have also analyzed other multimedia channels, e.g., video (Liu et al., 2002; Wang et al., 2003; Fan et al., 2006), to detect slide transitions. Such approaches, however, are unlikely to find semantic structures that are more detailed than slide transitions, e.g., the bullet hierarchical structures that we are interested in. Building tables-of-conte"
C10-2177,A00-2025,0,0.595888,"rarchical and global features and the improvement is consistent on transcripts with different WERs. Directly imposing such hierarchical structures onto raw speech without using transcripts yields competitive results. The efficiency and convenience of reading spoken documents are affected by at least two facts. First, the quality of transcripts can impair browsing efficiency, e.g., as shown in (Stark et al., 2000; Munteanu et al., 2006), though if the goal is only to browse salient excerpts, recognition errors on the extracts can be reduced by considering the confidence scores assigned by ASR (Zechner and Waibel, 2000; Hori and Furui, 2003). 1 Introduction Though speech has long served as a basic method of human communication, revisiting and browsing speech content had never been a possibility before human can record their own voice. Recent technological advances in recording, compressing, and distributing such archives have led to the consistently increasing availability of spoken content. Along with this availability comes a demand for better ways to browse such archives, which is inherently more difficult than browsing text. In relying on human beings’ ability to browse text, a solution is therefore to"
C10-2177,J02-1002,0,0.448335,"Missing"
C10-2177,P07-1069,0,\N,Missing
C16-1089,P14-1113,0,0.0280547,"Middle: Pick the middle N keyphrases, These may strike the right balance: some similarity with the rest, i.e. not noisy, but not too similar, i.e. specific to a document. This separation is somewhat crude, but further investigation in Section 4.2 show that further refinements do not yield better performance. 2.3 Hierarchy-based Discrimintive Keyphrase Extraction In order to use hierarchical semantic information to extract discriminative keyphrases, we first need to model the hierarchical information between keyphrases. We generalize the linear projection for hierarchical relations proposed by Fu et al. (2014), by using a Deep Belief Network (DBN) to model this relationship on keyphrase embeddings. A d-dimensional vector for each keyphrase is obtained using again word2vec, as in Section 2.1. The hierarchical information between two keyphrases p and q is then modeled as a binary classification problem: From a 2 × d dimensional input containing the embeddings p and q, the model predicts whether q is a child of p (positive class) or not (negative class). DBNs are deep learning models consisting of multiple layers of hidden variables, often used to obtain abstract representations (e.g., features) for r"
C16-1089,W03-1028,0,0.561144,"ses are a static property of documents, that is, a given document would always produce a fixed set of keyphrases. Many approaches were developped for that purpose. For example, the Keyphrase Extraction Algorithm, or KEA (Witten et al., 1998), uses a supervised learning method (Na¨ıve Bayes) to predict keyphrases based on their lexical features. Turney (2000) developed a genetic algorithm (GenEx) to extract keyphrases, and showed that this outperformed the well-known C4.5 algorithm. More recent work on supervised keyphrase extraction used, e.g., a combination of lexical and syntactic features (Hulth, 2003) or other statistical classifiers such as support vector machine (SVM) (Zhang et al., 2006) or conditional random fields (CRF) (Zhang et al., 2008). Unsupervised methods were also proposed, based on a graph-based ranking model (Mihalcea and Tarau, 2004), or using co-occurrences (Matsuo and Ishizuka, 2004), enriched with WordNet (Martinez-Romo et al., 2016). Unsupervised keyphrase extraction was also applied to shorter texts from twitter, using multiple random walks to topic context (Zhao et al., 2011) or unsupervised feature extraction (Marujo et al., 2015). The use of hierarchical information"
C16-1089,P15-2105,0,0.0246097,"Missing"
C16-1089,P08-1028,0,0.0420522,"|p |is the number of words in p. For example, the embedding for Machine Learning is the average of the vector 1 https://code.google.com/archive/p/word2vec/ 933 representations for Machine and for Learning. The similarity between two phrases p and q is: P p i qi hp, qi cosine(p, q) = p =qP i P hp, pihq, qi ( i p2i )( i qi2 ) (2) where i runs over the dimensions of the chosen embedding space, and h·, ·i is the scalar product notation. Note that despite its simplicity, arithmetic average has been found to be very effective among many alternatives when combining word vectors to represent phrases (Mitchell and Lapata, 2008). 2.2 Similarity-based Discriminative Keyphrase Extraction Now equipped with a similarity between keyphrases, we turn to extracting discriminative keyphrases for a document. In the similarity-based approach, we consider every candidate keyphrase p, and compare it to all other keyphrases from the group of document by computing the average similarity score between p and all other keyphrases q from all documents in the group. In our example, this would be all expertise keyphrases extracted for all researchers in the group considered: sScore(p) = 1 X cosine(p, q), C (3) q∈K where K is the set of k"
C16-1089,P11-1039,0,0.0242735,"ork on supervised keyphrase extraction used, e.g., a combination of lexical and syntactic features (Hulth, 2003) or other statistical classifiers such as support vector machine (SVM) (Zhang et al., 2006) or conditional random fields (CRF) (Zhang et al., 2008). Unsupervised methods were also proposed, based on a graph-based ranking model (Mihalcea and Tarau, 2004), or using co-occurrences (Matsuo and Ishizuka, 2004), enriched with WordNet (Martinez-Romo et al., 2016). Unsupervised keyphrase extraction was also applied to shorter texts from twitter, using multiple random walks to topic context (Zhao et al., 2011) or unsupervised feature extraction (Marujo et al., 2015). The use of hierarchical information to extract keyphrases was explored in (Smatana and Butka, 2016; Berend, 2016). Although these supervised and unsupervised methods achieve improved performance, little work has been done to generate discriminative keyphrases based on other documents in the group. This work is licenced under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/ License details: http:// 932 Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics"
C16-1089,W04-3252,0,\N,Missing
C18-1154,D15-1075,0,0.615253,"spans of text such as sentences or documents. The approaches to computing sentence embedding generally fall into two categories. The first consists of learning sentence embedding with unsupervised learning, e.g., auto-encoder-based models (Socher et al., 2011), Paragraph Vector (Le and Mikolov, 2014), SkipThought vectors (Kiros et al., 2015), FastSent (Hill et al., 2016), among others. The second category consists of models trained with supervised learning, such as convolution neural networks (CNN) (Kim, 2014; Kalchbrenner et al., 2014), recurrent neural networks (RNN) (Conneau et al., 2017; Bowman et al., 2015), and tree-structure recursive networks (Socher et al., 2013; Zhu et al., 2015; Tai et al., 2015), just to name a few. Pooling is an essential component of a wide variety of sentence representation and embedding models. For example, in recurrent-neural-network-based models, pooling is often used to aggregate hidden states at different time steps (i.e., words in a sentence) to obtain sentence embedding. Convolutional neural networks (CNN) also often uses max or mean pooling to obtain a fixed-size sentence embedding. In this paper we explore generalized pooling methods to enhance sentence embedd"
C18-1154,P16-1139,0,0.0551971,"Missing"
C18-1154,P17-1152,1,0.911507,"as input and predicts the level of sentiment in terms of the number of stars, from 1 to 5 stars, where 5-star means the most positive. We use the same data split as in Lin et al. (2017), i.e., 500,000 samples for training, 2,000 for development, and 2,000 for testing. 4.2 Training Details We implement our algorithm with Theano (Theano Development Team, 2016) framework. We use the development set (in-domain development set for MultiNLI) to select models for testing. To help replicate our results, we publish our code3 , which is developed from our codebase for multiple tasks (Chen et al., 2018; Chen et al., 2017a; Chen et al., 2016; Zhang et al., 2017). Specifically, we use Adam (Kingma and Ba, 2014) for optimization. The initial learning rate is 4e-4 for SNLI and MultiNLI, 2e-3 for Age dataset, 1e-3 for Yelp dataset. For SNLI and MultiNLI dataset, stacked BiLSTMs have 3 layers. For Age and Yelp dataset, stacked BiLSTMs have 1 layer. The hidden states of BiLSTMs for each direction and MLP are 300 dimension, except for SNLI whose dimensions are 600. We clip the norm of gradients to make it smaller than 10 for SNLI and MultiNLI, and 0.5 for Age and Yelp dataset. The character embedding has 15 dimension"
C18-1154,W17-5307,1,0.937976,"as input and predicts the level of sentiment in terms of the number of stars, from 1 to 5 stars, where 5-star means the most positive. We use the same data split as in Lin et al. (2017), i.e., 500,000 samples for training, 2,000 for development, and 2,000 for testing. 4.2 Training Details We implement our algorithm with Theano (Theano Development Team, 2016) framework. We use the development set (in-domain development set for MultiNLI) to select models for testing. To help replicate our results, we publish our code3 , which is developed from our codebase for multiple tasks (Chen et al., 2018; Chen et al., 2017a; Chen et al., 2016; Zhang et al., 2017). Specifically, we use Adam (Kingma and Ba, 2014) for optimization. The initial learning rate is 4e-4 for SNLI and MultiNLI, 2e-3 for Age dataset, 1e-3 for Yelp dataset. For SNLI and MultiNLI dataset, stacked BiLSTMs have 3 layers. For Age and Yelp dataset, stacked BiLSTMs have 1 layer. The hidden states of BiLSTMs for each direction and MLP are 300 dimension, except for SNLI whose dimensions are 600. We clip the norm of gradients to make it smaller than 10 for SNLI and MultiNLI, and 0.5 for Age and Yelp dataset. The character embedding has 15 dimension"
C18-1154,P18-1224,1,0.784416,"hich takes reviews as input and predicts the level of sentiment in terms of the number of stars, from 1 to 5 stars, where 5-star means the most positive. We use the same data split as in Lin et al. (2017), i.e., 500,000 samples for training, 2,000 for development, and 2,000 for testing. 4.2 Training Details We implement our algorithm with Theano (Theano Development Team, 2016) framework. We use the development set (in-domain development set for MultiNLI) to select models for testing. To help replicate our results, we publish our code3 , which is developed from our codebase for multiple tasks (Chen et al., 2018; Chen et al., 2017a; Chen et al., 2016; Zhang et al., 2017). Specifically, we use Adam (Kingma and Ba, 2014) for optimization. The initial learning rate is 4e-4 for SNLI and MultiNLI, 2e-3 for Age dataset, 1e-3 for Yelp dataset. For SNLI and MultiNLI dataset, stacked BiLSTMs have 3 layers. For Age and Yelp dataset, stacked BiLSTMs have 1 layer. The hidden states of BiLSTMs for each direction and MLP are 300 dimension, except for SNLI whose dimensions are 600. We clip the norm of gradients to make it smaller than 10 for SNLI and MultiNLI, and 0.5 for Age and Yelp dataset. The character embeddi"
C18-1154,D16-1053,0,0.0156898,"ce embedding. For example, Socher et al. (2013) introduced Recursive Neural Tensor Network (RNTN) over parse trees to compute sentence embedding for sentiment analysis. Zhu et al. (2015) and Tai et al. (2015) proposed tree-LSTM. Yu and Munkhdalai (2017a) proposed a memory augmented neural networks, called Neural Semantic Encoder (NSE), as sentence embedding for natural language understanding tasks. Some recent research began to explore inner/self-sentence attention mechanism for sentence embedding, which can be classified into two categories: self-attention network and self-attention pooling. Cheng et al. (2016) proposed an intra-sentence level attention mechanism on the base of LSTM, called LSTMN. For each step in LSTMN, it calculated the attention between a certain word and its previous words. Vaswani et al. (2017) proposed a self-attention network for the neural machine translation task. The self-attention network uses multi-head scaled dot-product attention to represent each word by weighted summation of all word in the sentence. Shen et al. (2017) proposed DiSAN, which is composed of a directional self-attention with temporal order encoded. Shen et al. (2018) proposed reinforced selfattention ne"
C18-1154,D17-1070,0,0.1438,"e done to model larger spans of text such as sentences or documents. The approaches to computing sentence embedding generally fall into two categories. The first consists of learning sentence embedding with unsupervised learning, e.g., auto-encoder-based models (Socher et al., 2011), Paragraph Vector (Le and Mikolov, 2014), SkipThought vectors (Kiros et al., 2015), FastSent (Hill et al., 2016), among others. The second category consists of models trained with supervised learning, such as convolution neural networks (CNN) (Kim, 2014; Kalchbrenner et al., 2014), recurrent neural networks (RNN) (Conneau et al., 2017; Bowman et al., 2015), and tree-structure recursive networks (Socher et al., 2013; Zhu et al., 2015; Tai et al., 2015), just to name a few. Pooling is an essential component of a wide variety of sentence representation and embedding models. For example, in recurrent-neural-network-based models, pooling is often used to aggregate hidden states at different time steps (i.e., words in a sentence) to obtain sentence embedding. Convolutional neural networks (CNN) also often uses max or mean pooling to obtain a fixed-size sentence embedding. In this paper we explore generalized pooling methods to e"
C18-1154,N16-1162,0,0.0221078,"in modeling natural language at different granularities. Learning representation for words (Bengio et al., 2000; Mikolov et al., 2013; Pennington et al., 2014), for example, has achieved notable success. Much remains to be done to model larger spans of text such as sentences or documents. The approaches to computing sentence embedding generally fall into two categories. The first consists of learning sentence embedding with unsupervised learning, e.g., auto-encoder-based models (Socher et al., 2011), Paragraph Vector (Le and Mikolov, 2014), SkipThought vectors (Kiros et al., 2015), FastSent (Hill et al., 2016), among others. The second category consists of models trained with supervised learning, such as convolution neural networks (CNN) (Kim, 2014; Kalchbrenner et al., 2014), recurrent neural networks (RNN) (Conneau et al., 2017; Bowman et al., 2015), and tree-structure recursive networks (Socher et al., 2013; Zhu et al., 2015; Tai et al., 2015), just to name a few. Pooling is an essential component of a wide variety of sentence representation and embedding models. For example, in recurrent-neural-network-based models, pooling is often used to aggregate hidden states at different time steps (i.e.,"
C18-1154,P14-1062,0,0.0315928,"for example, has achieved notable success. Much remains to be done to model larger spans of text such as sentences or documents. The approaches to computing sentence embedding generally fall into two categories. The first consists of learning sentence embedding with unsupervised learning, e.g., auto-encoder-based models (Socher et al., 2011), Paragraph Vector (Le and Mikolov, 2014), SkipThought vectors (Kiros et al., 2015), FastSent (Hill et al., 2016), among others. The second category consists of models trained with supervised learning, such as convolution neural networks (CNN) (Kim, 2014; Kalchbrenner et al., 2014), recurrent neural networks (RNN) (Conneau et al., 2017; Bowman et al., 2015), and tree-structure recursive networks (Socher et al., 2013; Zhu et al., 2015; Tai et al., 2015), just to name a few. Pooling is an essential component of a wide variety of sentence representation and embedding models. For example, in recurrent-neural-network-based models, pooling is often used to aggregate hidden states at different time steps (i.e., words in a sentence) to obtain sentence embedding. Convolutional neural networks (CNN) also often uses max or mean pooling to obtain a fixed-size sentence embedding. In"
C18-1154,D14-1181,0,0.221662,"al., 2014), for example, has achieved notable success. Much remains to be done to model larger spans of text such as sentences or documents. The approaches to computing sentence embedding generally fall into two categories. The first consists of learning sentence embedding with unsupervised learning, e.g., auto-encoder-based models (Socher et al., 2011), Paragraph Vector (Le and Mikolov, 2014), SkipThought vectors (Kiros et al., 2015), FastSent (Hill et al., 2016), among others. The second category consists of models trained with supervised learning, such as convolution neural networks (CNN) (Kim, 2014; Kalchbrenner et al., 2014), recurrent neural networks (RNN) (Conneau et al., 2017; Bowman et al., 2015), and tree-structure recursive networks (Socher et al., 2013; Zhu et al., 2015; Tai et al., 2015), just to name a few. Pooling is an essential component of a wide variety of sentence representation and embedding models. For example, in recurrent-neural-network-based models, pooling is often used to aggregate hidden states at different time steps (i.e., words in a sentence) to obtain sentence embedding. Convolutional neural networks (CNN) also often uses max or mean pooling to obtain a fixed"
C18-1154,P16-2022,0,0.0874509,"layer classifier to solve different problems. In this paper we evaluate our sentence embedding models on three different tasks: natural language inference (NLI), author profiling, and sentiment classification, on four datasets. The evaluation covers two typical types of problems. The author profiling and sentiment tasks classify individual sentences into different categories and the two NLI tasks classify sentence pairs. For the NLI tasks, to enhance the relationship between sentence pairs, we concatenate the embeddings of two sentences with their absolute difference and element-wise product (Mou et al., 2016) as the input to the multilayer perceptron (MLP) classifier: v = [va ; vb ; |va − vb |; va vb ] , (10) where is the element-wise product. The MLP has two hidden layers with ReLU activation with shortcut connections and a softmax output layer. The entire model is trained end-to-end through minimizing the cross-entropy loss. Note that for the two classification tasks on individual sentences (i.e., the author profiling and sentiment classification task), we use the same MLP classifiers described above for sentence pair classification. But instead of concatenating two sentences, we directly feed a"
C18-1154,W17-5308,0,0.0540595,"Missing"
C18-1154,D16-1244,0,0.103518,"Missing"
C18-1154,D14-1162,0,0.0909607,"sks: natural language inference (NLI), author profiling, and sentiment classification. The experiments show that the proposed model achieves significant improvement over strong sentence-encoding-based methods, resulting in state-of-the-art performances on four datasets. The proposed approach can be easily implemented for more problems than we discuss in this paper. 1 Introduction Distributed representation learned with neural networks has shown to be effective in modeling natural language at different granularities. Learning representation for words (Bengio et al., 2000; Mikolov et al., 2013; Pennington et al., 2014), for example, has achieved notable success. Much remains to be done to model larger spans of text such as sentences or documents. The approaches to computing sentence embedding generally fall into two categories. The first consists of learning sentence embedding with unsupervised learning, e.g., auto-encoder-based models (Socher et al., 2011), Paragraph Vector (Le and Mikolov, 2014), SkipThought vectors (Kiros et al., 2015), FastSent (Hill et al., 2016), among others. The second category consists of models trained with supervised learning, such as convolution neural networks (CNN) (Kim, 2014;"
C18-1154,D11-1014,0,0.0614308,"uss in this paper. 1 Introduction Distributed representation learned with neural networks has shown to be effective in modeling natural language at different granularities. Learning representation for words (Bengio et al., 2000; Mikolov et al., 2013; Pennington et al., 2014), for example, has achieved notable success. Much remains to be done to model larger spans of text such as sentences or documents. The approaches to computing sentence embedding generally fall into two categories. The first consists of learning sentence embedding with unsupervised learning, e.g., auto-encoder-based models (Socher et al., 2011), Paragraph Vector (Le and Mikolov, 2014), SkipThought vectors (Kiros et al., 2015), FastSent (Hill et al., 2016), among others. The second category consists of models trained with supervised learning, such as convolution neural networks (CNN) (Kim, 2014; Kalchbrenner et al., 2014), recurrent neural networks (RNN) (Conneau et al., 2017; Bowman et al., 2015), and tree-structure recursive networks (Socher et al., 2013; Zhu et al., 2015; Tai et al., 2015), just to name a few. Pooling is an essential component of a wide variety of sentence representation and embedding models. For example, in recur"
C18-1154,D13-1170,0,0.0554996,"to computing sentence embedding generally fall into two categories. The first consists of learning sentence embedding with unsupervised learning, e.g., auto-encoder-based models (Socher et al., 2011), Paragraph Vector (Le and Mikolov, 2014), SkipThought vectors (Kiros et al., 2015), FastSent (Hill et al., 2016), among others. The second category consists of models trained with supervised learning, such as convolution neural networks (CNN) (Kim, 2014; Kalchbrenner et al., 2014), recurrent neural networks (RNN) (Conneau et al., 2017; Bowman et al., 2015), and tree-structure recursive networks (Socher et al., 2013; Zhu et al., 2015; Tai et al., 2015), just to name a few. Pooling is an essential component of a wide variety of sentence representation and embedding models. For example, in recurrent-neural-network-based models, pooling is often used to aggregate hidden states at different time steps (i.e., words in a sentence) to obtain sentence embedding. Convolutional neural networks (CNN) also often uses max or mean pooling to obtain a fixed-size sentence embedding. In this paper we explore generalized pooling methods to enhance sentence embedding. Specifically, by extending scalar self-attention models"
C18-1154,P15-1150,0,0.261804,"lly fall into two categories. The first consists of learning sentence embedding with unsupervised learning, e.g., auto-encoder-based models (Socher et al., 2011), Paragraph Vector (Le and Mikolov, 2014), SkipThought vectors (Kiros et al., 2015), FastSent (Hill et al., 2016), among others. The second category consists of models trained with supervised learning, such as convolution neural networks (CNN) (Kim, 2014; Kalchbrenner et al., 2014), recurrent neural networks (RNN) (Conneau et al., 2017; Bowman et al., 2015), and tree-structure recursive networks (Socher et al., 2013; Zhu et al., 2015; Tai et al., 2015), just to name a few. Pooling is an essential component of a wide variety of sentence representation and embedding models. For example, in recurrent-neural-network-based models, pooling is often used to aggregate hidden states at different time steps (i.e., words in a sentence) to obtain sentence embedding. Convolutional neural networks (CNN) also often uses max or mean pooling to obtain a fixed-size sentence embedding. In this paper we explore generalized pooling methods to enhance sentence embedding. Specifically, by extending scalar self-attention models such as those proposed in Lin et al."
C18-1154,E17-1038,0,0.0148004,"t in the literature much previous work for sentence embedding with supervised learning, which mostly use RNN and CNN as building blocks. For example, Bowman et al. (2015) used BiLSTMs as sentence embedding for natural language inference task. Kim (2014) used CNN with max pooling for sentence classification. More complicated neural networks were also proposed for sentence embedding. For example, Socher et al. (2013) introduced Recursive Neural Tensor Network (RNTN) over parse trees to compute sentence embedding for sentiment analysis. Zhu et al. (2015) and Tai et al. (2015) proposed tree-LSTM. Yu and Munkhdalai (2017a) proposed a memory augmented neural networks, called Neural Semantic Encoder (NSE), as sentence embedding for natural language understanding tasks. Some recent research began to explore inner/self-sentence attention mechanism for sentence embedding, which can be classified into two categories: self-attention network and self-attention pooling. Cheng et al. (2016) proposed an intra-sentence level attention mechanism on the base of LSTM, called LSTMN. For each step in LSTMN, it calculated the attention between a certain word and its previous words. Vaswani et al. (2017) proposed a self-attenti"
C18-1154,E17-1002,0,0.0669919,"t in the literature much previous work for sentence embedding with supervised learning, which mostly use RNN and CNN as building blocks. For example, Bowman et al. (2015) used BiLSTMs as sentence embedding for natural language inference task. Kim (2014) used CNN with max pooling for sentence classification. More complicated neural networks were also proposed for sentence embedding. For example, Socher et al. (2013) introduced Recursive Neural Tensor Network (RNTN) over parse trees to compute sentence embedding for sentiment analysis. Zhu et al. (2015) and Tai et al. (2015) proposed tree-LSTM. Yu and Munkhdalai (2017a) proposed a memory augmented neural networks, called Neural Semantic Encoder (NSE), as sentence embedding for natural language understanding tasks. Some recent research began to explore inner/self-sentence attention mechanism for sentence embedding, which can be classified into two categories: self-attention network and self-attention pooling. Cheng et al. (2016) proposed an intra-sentence level attention mechanism on the base of LSTM, called LSTMN. For each step in LSTMN, it calculated the attention between a certain word and its previous words. Vaswani et al. (2017) proposed a self-attenti"
D14-2001,S13-2053,1,\N,Missing
D14-2001,W11-0705,0,\N,Missing
D14-2001,H05-1044,0,\N,Missing
D14-2001,J11-2001,0,\N,Missing
D14-2001,S14-2077,1,\N,Missing
D14-2001,P02-1053,0,\N,Missing
D14-2001,D08-1083,0,\N,Missing
D14-2001,W10-0204,1,\N,Missing
D14-2001,P97-1023,0,\N,Missing
D14-2001,S14-2009,0,\N,Missing
D14-2001,P14-1029,1,\N,Missing
D14-2001,P14-1146,0,\N,Missing
D14-2001,S14-2004,0,\N,Missing
D14-2001,D13-1170,0,\N,Missing
D14-2001,S14-2076,1,\N,Missing
D14-2001,S12-1033,1,\N,Missing
D14-2001,W10-3111,0,\N,Missing
D14-2001,S13-2052,0,\N,Missing
D19-1193,D19-1193,1,0.0512826,"ion by a simple concatenation or addition operation. Finally, the enhanced context representation is used to rank response candidates. This method has two main deficiencies. First, the context is treated as a whole for calculating its attention towards profile sentences. However, each context is composed of multiple utterances and these utterances may play different roles when matching different profile sentences. Second, the interactions between the persona and each response candidate are ignored when deriving the persona representation. In this paper, the interactive matching network (IMN) (Gu et al., 2019) is adopted as the fundamental architecture to build our baseline and improved models for personalized response selection. The baseline model follows the persona fusion method proposed by Zhang et al. (2018) and two improved models are then proposed. First, an IMN-based persona fusion model with finegrained context-persona interaction is designed. In this model, each utterance in a context, instead of the whole context, is used to calculate its similarity with each profile sentence in a persona. Second, a dually interactive matching network (DIM) is proposed by formulating the task of personal"
D19-1193,P82-1020,0,0.83818,"Missing"
D19-1193,P16-1094,0,0.361187,"response from a set of candidates given the context of a conversation, is an important technique to build retrieval-based chatbots (Zhou et al., 2018). Many previous studies on singleturn (Wang et al., 2013) or multi-turn response selection (Lowe et al., 2015; Zhou et al., 2018; Gu et al., 2019) rank response candidates according to their semantic relevance with the given context. With the emergence and popular use of personal assistants such as Apple Siri, Google Now and Microsoft Cortana, the techniques of making personalized dialogues has attracted much research attention in recent years (Li et al., 2016; Zhang et al., 2018; Mazar´e et al., 2018). Zhang et al. (2018) constructed a PERSONA-CHAT dataset for building personalized dialogue agents, where each persona was represented as multiple sentences of profile description. An example dialogue conditioned on given profiles from this dataset is given in Table 1 for illustration. A persona fusion method for personalized response selection was also proposed by Zhang et al. (2018). In this method, given a context and a persona composed of several profile sentences, the similarities between the context representation and all profile sentences are c"
D19-1193,W15-4640,0,0.403757,"l is designed in order to consider the utterance-level interactions between contexts and personas. (2) A dually interactive matching network (DIM) is proposed by formulating the task of personalized response selection as a dual matching problem, aiming to find a response that can properly match the given context and persona simultaneously. (3) Experimental results 2 Related Work Response Selection Response selection is an important problem in building retrieval-based chatbots. Existing work on response selection can be categorized into single-turn (Wang et al., 2013) and multi-turn dialogues (Lowe et al., 2015; Zhou et al., 2018; Gu et al., 2019). Early studies have been more on single-turn dialogues, considering only the last utterance of a context for response matching. More recently, the research focus has been shifted to multi-turn conversations, a more practical setup for real applications. Wu et al. (2017) proposed the sequential matching network (SMN) which first matched the response with each context utterance and then accumulated the matching information by a recurrent neural network (RNN). Zhou et al. (2018) proposed the deep attention matching network (DAM) to construct representations a"
D19-1193,D18-1298,0,0.364847,"Missing"
D19-1193,D16-1147,0,0.0347263,"ontext and response in order to derive the matching feature vector. 2.2 Persona Profile Encoder Embeddings Context Context Encoder Response Persona for Chatbots Chit-chat models suffer from a lack of a consistent personality as they are typically trained over many dialogues, each with different speakers, and a lack of explicit long-term memory as they are typically trained to produce an utterance given only a very recent dialogue history. Li et al. (2016) proposed a persona-based neural conversation model to capture individual characteristics such as background information and speaking style. Miller et al. (2016) proposed the key-value memory network, where the keys were dialogue histories, i.e., contexts, and the values were next dialogue utterances. Zhang et al. (2018) proposed the profile memory network by considering the dialogue history as input and then performing attention over the persona to be combined with the dialogue history. Mazar´e et al. (2018) proposed the fine-tuned persona-chat (FT-PC) model which first pretrained a model using a large-scale corpus with external knowledge and then fine-tuned it on the PERSONA-CHAT dataset. In general, all these methods adopted a contextlevel persona"
D19-1193,P18-1103,0,0.167679,"der to consider the utterance-level interactions between contexts and personas. (2) A dually interactive matching network (DIM) is proposed by formulating the task of personalized response selection as a dual matching problem, aiming to find a response that can properly match the given context and persona simultaneously. (3) Experimental results 2 Related Work Response Selection Response selection is an important problem in building retrieval-based chatbots. Existing work on response selection can be categorized into single-turn (Wang et al., 2013) and multi-turn dialogues (Lowe et al., 2015; Zhou et al., 2018; Gu et al., 2019). Early studies have been more on single-turn dialogues, considering only the last utterance of a context for response matching. More recently, the research focus has been shifted to multi-turn conversations, a more practical setup for real applications. Wu et al. (2017) proposed the sequential matching network (SMN) which first matched the response with each context utterance and then accumulated the matching information by a recurrent neural network (RNN). Zhou et al. (2018) proposed the deep attention matching network (DAM) to construct representations at different granula"
D19-1193,D14-1162,0,0.0823684,"mean reciprocal rank (MRR) (Voorhees, 1999) metric was also adopted to take the rank of the correct response over all candidates into consideration. 6.3 Training Details For building the IMN, IMNctx , IMNutr and DIM models, the Adam method (Kingma and Ba, 2015) was employed for optimization with a batch size of 16. The initial learning rate was 0.001 and was exponentially decayed by 0.96 every 5000 steps. Dropout (Srivastava et al., 2014) with a rate of 0.2 was applied to the word embeddings and all hidden layers. A word representation is a concatenation of a 300-dimensional GloVe embedding (Pennington et al., 2014), a 100-dimensional embedding estimated on the training set using the Word2Vec algorithm (Mikolov et al., 2013), and 150-dimensional character-level embeddings with window sizes {3, 4, 5}, each consisting of 50 filters. The word embeddings were not updated during training. All hidden states of the LSTM have 200 dimensions. The MLP at the prediction layer have 256 hidden units with ReLU (Nair and Hinton, 2010) activation. The maximum number of characters in a word, that of words in a context utterance, of utterances in a context, and of words in a response were set to be 18, 20, 15, and 1850 IR"
D19-1193,D13-1096,0,0.306593,"An IMN-based fine-grained persona fusion model is designed in order to consider the utterance-level interactions between contexts and personas. (2) A dually interactive matching network (DIM) is proposed by formulating the task of personalized response selection as a dual matching problem, aiming to find a response that can properly match the given context and persona simultaneously. (3) Experimental results 2 Related Work Response Selection Response selection is an important problem in building retrieval-based chatbots. Existing work on response selection can be categorized into single-turn (Wang et al., 2013) and multi-turn dialogues (Lowe et al., 2015; Zhou et al., 2018; Gu et al., 2019). Early studies have been more on single-turn dialogues, considering only the last utterance of a context for response matching. More recently, the research focus has been shifted to multi-turn conversations, a more practical setup for real applications. Wu et al. (2017) proposed the sequential matching network (SMN) which first matched the response with each context utterance and then accumulated the matching information by a recurrent neural network (RNN). Zhou et al. (2018) proposed the deep attention matching"
D19-1193,P17-1046,0,0.188991,"text and persona simultaneously. (3) Experimental results 2 Related Work Response Selection Response selection is an important problem in building retrieval-based chatbots. Existing work on response selection can be categorized into single-turn (Wang et al., 2013) and multi-turn dialogues (Lowe et al., 2015; Zhou et al., 2018; Gu et al., 2019). Early studies have been more on single-turn dialogues, considering only the last utterance of a context for response matching. More recently, the research focus has been shifted to multi-turn conversations, a more practical setup for real applications. Wu et al. (2017) proposed the sequential matching network (SMN) which first matched the response with each context utterance and then accumulated the matching information by a recurrent neural network (RNN). Zhou et al. (2018) proposed the deep attention matching network (DAM) to construct representations at different granularities with stacked self-attention. Gu et al. (2019) proposed the interactive matching network (IMN) 1846 to enhance the representations of the context and response at both the word-level and sentencelevel, and to perform the bidirectional and global interactions between the context and r"
D19-1193,P18-1205,0,0.122779,"set of candidates given the context of a conversation, is an important technique to build retrieval-based chatbots (Zhou et al., 2018). Many previous studies on singleturn (Wang et al., 2013) or multi-turn response selection (Lowe et al., 2015; Zhou et al., 2018; Gu et al., 2019) rank response candidates according to their semantic relevance with the given context. With the emergence and popular use of personal assistants such as Apple Siri, Google Now and Microsoft Cortana, the techniques of making personalized dialogues has attracted much research attention in recent years (Li et al., 2016; Zhang et al., 2018; Mazar´e et al., 2018). Zhang et al. (2018) constructed a PERSONA-CHAT dataset for building personalized dialogue agents, where each persona was represented as multiple sentences of profile description. An example dialogue conditioned on given profiles from this dataset is given in Table 1 for illustration. A persona fusion method for personalized response selection was also proposed by Zhang et al. (2018). In this method, given a context and a persona composed of several profile sentences, the similarities between the context representation and all profile sentences are computed first using"
D19-1403,P07-1056,0,0.0778859,"ne models in our experiments are introduced as follows. Experiments We evaluate our model by conducting experiments on two few-shot text classification datasets. All the experiments are implemented with Tensorflow. • Matching Networks: a few-shot learning model using a metric-based attention method (Vinyals et al., 2016). 5.1 • Prototypical Networks: a deep metric-based method using sample average as class prototypes (Snell et al., 2017). Datasets Amazon Review Sentiment Classification (ARSC) Following Yu et al. (2018), we use the multiple tasks with the multi-domain sentiment classification (Blitzer et al., 2007) dataset. The dataset comprises English reviews for 23 types of products on Amazon. For each product domain, there are three different binary classification tasks. These buckets then form 23 × 3 = 69 tasks in total. Following Yu et al. (2018), we select 12(4 × 3) tasks from 4 domains (Books, DVD, Electronics and Kitchen) as the test set, and there are only five examples as support set for each label in the test set. We create 5-shot learning models on this dataset. 3909 • Graph Network: a graph-based few-shot learning model that implements a task-driven message passing algorithm on the samplew"
D19-1403,P18-1164,0,0.0171606,"this way, we find the model is able to induce and generalize better. We evaluate the proposed model on a well-studied sentiment classification dataset (English) and a real-world dialogue intent classification dataset (Chinese). Experiment results show that on both datasets, the proposed model significantly outperforms the existing state-of-the-art approaches, proving the effectiveness of class-wise generalization in few-shot text classification. 1 Introduction Deep learning has achieved a great success in many fields such as computer vision, speech recognition and natural language processing (Kuang et al., 2018). However, supervised deep learning is notoriously greedy for large labeled datasets, which limits the generalizability of deep models to new classes due to annotation cost. Humans on the other hand are readily capable of rapidly learning new classes of concepts with few examples or stimuli. This notable gap provides a fertile ground for further research. ∗ Corresponding authors: Y.Li and P.Jian. Few-shot learning is devoted to resolving the data deficiency problem by recognizing novel classes from very few labeled examples. The limitation of only one or very few examples challenges the standa"
D19-1403,P18-2023,0,0.0216042,"Missing"
D19-1403,D14-1162,0,0.0806466,"Missing"
D19-1403,D18-1352,0,0.0499909,"ormed by computing squared Euclidean distances to prototype representations of each class. Different from fixed metric measures, the Relation Network learnt a deep distance metric to compare the query with given examples (Sung et al., 2018). Recently, some studies have been presented focusing specifically on few-shot text classification problems. Xu et al. (2018) studied lifelong domain word embeddings via meta-learning. Yu et al. (2018) argued that the optimal meta-model may vary across tasks, and they employed the multimetric model by clustering the meta-tasks into several defined clusters. Rios and Kavuluru (2018) developed a few-shot text classification model for multi-label text classification where there was a known structure over the label space. Xu et al. (2019) proposed a open-world learning model to deal with the unseen classes in the product classification problem. We solve the few-shot learning problem from a different perspective and propose a dynamic routing induction method to encapsulate the abstract class representation from samples, achieving state-of-the-art performances on two datasets. 2.2 Capsule Network The Capsule Network was first proposed by Sabour et al. (2017), which allowed th"
D19-1403,D18-1348,0,0.0359917,"dynamic routing induction method to encapsulate the abstract class representation from samples, achieving state-of-the-art performances on two datasets. 2.2 Capsule Network The Capsule Network was first proposed by Sabour et al. (2017), which allowed the network to learn robustly the invariants in part-whole relationships. Lately, Capsule Network has been explored in the natural language processing field. Yang et al. (2018) successfully applied Capsule Network to fully supervised text classification problem with large labeled datasets. Unlike their work, we study few-shot text classification. Xia et al. (2018) reused the supervised model similar to that of Yang et al. (2018) for intent classification, in which a capsule-based architecture is extended to compute similarity between the target intents and source intents. Unlike their work, we propose Induction Networks for few-shot learning, in which we propose to use capsules and dynamic routing to learn generalized class-level representation from samples based. The dynamic routing method makes our model generalize better in the few-shot text classification task. 3 3.1 Problem Definition Few-Shot Classification Few-shot classification (Vinyals et al."
D19-1403,D18-1350,0,0.0230996,"proposed a open-world learning model to deal with the unseen classes in the product classification problem. We solve the few-shot learning problem from a different perspective and propose a dynamic routing induction method to encapsulate the abstract class representation from samples, achieving state-of-the-art performances on two datasets. 2.2 Capsule Network The Capsule Network was first proposed by Sabour et al. (2017), which allowed the network to learn robustly the invariants in part-whole relationships. Lately, Capsule Network has been explored in the natural language processing field. Yang et al. (2018) successfully applied Capsule Network to fully supervised text classification problem with large labeled datasets. Unlike their work, we study few-shot text classification. Xia et al. (2018) reused the supervised model similar to that of Yang et al. (2018) for intent classification, in which a capsule-based architecture is extended to compute similarity between the target intents and source intents. Unlike their work, we propose Induction Networks for few-shot learning, in which we propose to use capsules and dynamic routing to learn generalized class-level representation from samples based. T"
D19-1403,N18-1109,0,0.67582,"the cosine distance, which was called Matching Networks. Snell et al. (2017) proposed the Prototypical Networks which learnt a metric space where classification could be performed by computing squared Euclidean distances to prototype representations of each class. Different from fixed metric measures, the Relation Network learnt a deep distance metric to compare the query with given examples (Sung et al., 2018). Recently, some studies have been presented focusing specifically on few-shot text classification problems. Xu et al. (2018) studied lifelong domain word embeddings via meta-learning. Yu et al. (2018) argued that the optimal meta-model may vary across tasks, and they employed the multimetric model by clustering the meta-tasks into several defined clusters. Rios and Kavuluru (2018) developed a few-shot text classification model for multi-label text classification where there was a known structure over the label space. Xu et al. (2019) proposed a open-world learning model to deal with the unseen classes in the product classification problem. We solve the few-shot learning problem from a different perspective and propose a dynamic routing induction method to encapsulate the abstract class rep"
E14-1064,C10-1081,0,0.0159755,"cern here is different ― instead of utilizing translation for sentiment analysis; we are interested in the SMT quality itself, by modeling bilingual sentiment in translation. As mentioned above, while we expect that statistics learned from parallel corpora have implicitly captured sentiment in some degree, we are curious if better modeling is possible. Considering semantic similarity in translation The literature has included interesting ideas of incorporating different types of semantic knowledge for SMT. A main stream of recent efforts have been leveraging semantic roles (Wu and Fung, 2009; Liu and Gildea, 2010; Li et al., 2013) to improve translation, e.g., through improving reordering. Also, Chen et al. (2010) have leveraged sense similarity between source and target side as additional features. In this work, we view a different dimension, i.e., semantic orientation, and show that incorporating such knowledge improves the translation performance. We hope this work would add more evidences to the existing literature of leveraging semantics for SMT, and shed some light on further exploration of semantic consistency, e.g., examining other semantic differential factors. 3 Problem & Approach 3.1 Consis"
E14-1064,D08-1014,0,0.0607651,"Missing"
E14-1064,P07-1123,0,0.407461,"bilingual sentiment-annotated data for our study. It suits our purpose here of exploring the basic role of sentiment for translation. Also, such a method has been reported to achieve a good cross-domain performance (Taboada et al., 2011) comparable with that of other state-of-the-art models. Translation for sentiment analysis A very interesting line of research has leveraged labeled data in a resource-rich language (e.g., English) to help sentiment analysis in a resource-poorer language. This includes the idea of constructing sentiment lexicons automatically by using a translation dictionary (Mihalcea et al., 2007), as well as the idea of utilizing parallel corpora or automatically translated documents to incorporate sentiment-labeled data from different languages (Wan, 2009; Mihalcea et al., 2007). Our concern here is different ― instead of utilizing translation for sentiment analysis; we are interested in the SMT quality itself, by modeling bilingual sentiment in translation. As mentioned above, while we expect that statistics learned from parallel corpora have implicitly captured sentiment in some degree, we are curious if better modeling is possible. Considering semantic similarity in translation Th"
E14-1064,P02-1038,0,0.157896,"ine the role of sentiment consistency in two ways: designing features for the translation model and using them for re-ranking. Before discussing the details of our features, we briefly recap phrase-based SMT for completeness. Given a source sentence f, the goal of statistical machine translation is to select a target language string e which maximizes the posterior probability P(e|f). In a phrase-based SMT system, the translation unit is the phrases, where a ""phrase"" is a sequence of words. Phrase-based statistical machine translation systems are usually modeled through a log-linear framework (Och and Ney, 2002) by introducing the hidden word alignment variable a (Brown et al., 1993). ~ M (2) e~*  arg max (m1 m H m (e~, f , a))  F2: if only one side contains sentiment units;  F3: if the source side contains sentiment units;  F4: if the target side contains sentiment units. Sentiment polarity The second group of features check the sentiment polarity. These features are still binary; they check if the polarities of the source and target side are the same.  F5: if the two sides of the pair (f, e) have the same polarity;  F6: if at least one side has a neutral sentiment;  F7: if the polarity is"
E14-1064,J93-2003,0,0.0888328,"r the translation model and using them for re-ranking. Before discussing the details of our features, we briefly recap phrase-based SMT for completeness. Given a source sentence f, the goal of statistical machine translation is to select a target language string e which maximizes the posterior probability P(e|f). In a phrase-based SMT system, the translation unit is the phrases, where a ""phrase"" is a sequence of words. Phrase-based statistical machine translation systems are usually modeled through a log-linear framework (Och and Ney, 2002) by introducing the hidden word alignment variable a (Brown et al., 1993). ~ M (2) e~*  arg max (m1 m H m (e~, f , a))  F2: if only one side contains sentiment units;  F3: if the source side contains sentiment units;  F4: if the target side contains sentiment units. Sentiment polarity The second group of features check the sentiment polarity. These features are still binary; they check if the polarities of the source and target side are the same.  F5: if the two sides of the pair (f, e) have the same polarity;  F6: if at least one side has a neutral sentiment;  F7: if the polarity is opposite on the two sides, i.e., one is positive and one is negative. e,"
E14-1064,P03-1021,0,0.0266612,"ent polarity The second group of features check the sentiment polarity. These features are still binary; they check if the polarities of the source and target side are the same.  F5: if the two sides of the pair (f, e) have the same polarity;  F6: if at least one side has a neutral sentiment;  F7: if the polarity is opposite on the two sides, i.e., one is positive and one is negative. e,a where e~ is a string of phrases in the target language, ~ f is the source language string, ~ H m (e~, f , a) are feature functions, and weights m are typically optimized to maximize the scoring function (Och, 2003). 3.4 Feature design In Section 3.2 above, we have discussed our lexicon-based approach, which leverages lexiconbased sentiment consistency. Below, we describe the specific features we designed for our experi~ ments. For a phrase pair ( f , e~ ) or a sentence pair (f, e) 6 , we propose the following four groups of consistency features. Subjectivity The first group of features is designed to check the subjectivity of a phrase or a sentence pair (f, e). This set of features examines if the source or target side contains sentiment units. As the name suggests, these features only capture if subjec"
E14-1064,P05-1033,0,0.120749,"a et al., 2011) and (Zhang et al., 2012), for our bilingual task. We suspect that better sentiment modeling may further improve the general translation performance or the quality of sentiment in translation. We will discuss some directions we think interesting in the future work section. 3.3 Incorporating sentiment consistency into phrase-based SMT In this paper, we focus on exploring sentiment consistency for phrase-based SMT. However, the approach might be used in other translation framework. For example, consistency may be considered in the variables used in hierarchical translation rules (Chiang, 2005). 4 The expression “very not good” is ungrammatical in English. However, in Chinese, it is possible to have this kind of expression, such as “很不漂亮”, whose transliteration is “very not beautiful”, meaning “very ugly”. 5 Note that when sentiment-annotated training data are available, merg(.) can be trained, e.g., if assuming it to be the widely-used (log-) linear form. 609 We will examine the role of sentiment consistency in two ways: designing features for the translation model and using them for re-ranking. Before discussing the details of our features, we briefly recap phrase-based SMT for co"
E14-1064,W02-1011,0,0.0234612,"Missing"
E14-1064,P10-1086,1,0.855261,"e SMT quality itself, by modeling bilingual sentiment in translation. As mentioned above, while we expect that statistics learned from parallel corpora have implicitly captured sentiment in some degree, we are curious if better modeling is possible. Considering semantic similarity in translation The literature has included interesting ideas of incorporating different types of semantic knowledge for SMT. A main stream of recent efforts have been leveraging semantic roles (Wu and Fung, 2009; Liu and Gildea, 2010; Li et al., 2013) to improve translation, e.g., through improving reordering. Also, Chen et al. (2010) have leveraged sense similarity between source and target side as additional features. In this work, we view a different dimension, i.e., semantic orientation, and show that incorporating such knowledge improves the translation performance. We hope this work would add more evidences to the existing literature of leveraging semantics for SMT, and shed some light on further exploration of semantic consistency, e.g., examining other semantic differential factors. 3 Problem & Approach 3.1 Consistency of sentiment Ideally, sentiment should be properly preserved in high-quality translation. An inte"
E14-1064,P02-1040,0,0.0929004,"-toEnglish translation task. The training data are from NIST Open MT 2012. All allowed bilingual corpora were used to train the translation model and reordering models. There are about 283M target word tokens. The development (dev) set comprised mainly data from the NIST 2005 test set, and also some balanced-genre web-text from NIST training data. Evaluation was performed on NIST 2006 and 2008, which have 1,664 and 1,357 sentences, 39.7K and 33.7K source words respectively. Four references were provided for all dev and test sets. 4.2 Results Our evaluation metric is case-insensitive IBM BLEU (Papineni et al., 2002), which performs matching of n-grams up to n = 4; we report BLEU scores on two test sets NIST06 and NIST08. Following (Koehn, 2004), we use the bootstrap resampling test to do significance testing. In Table 4-6, the sign * and ** denote statistically significant gains over the baseline at the p &lt; 0.05 and p &lt; 0.01 level, respectively. 8 The Stanford POS tagger (Toutanova et al., 2003) was used to tag phrase and sentence pairs for this purpose. 611 NIST06 Baseline 35.1 +feat. group1 35.6** +feat. group2 35.3* +feat. group3 35.3 +feat. group4 35.5* +feat. group1+2 35.8** +feat. group1+2+3 36.1**"
E14-1064,2011.mtsummit-papers.30,1,0.743356,"r of them;  F14: if both sides have an odd number of negation words not appearing outside any sentiment units, or if both sides have an even number of such negation words; 4 Experiments 4.1 Translation experimental settings Experiments were carried out with an in-house phrase-based system similar to Moses (Koehn et al., 2007). Each corpus was word-aligned using IBM model 2, HMM, and IBM model 4, and the phrase table was the union of phrase pairs extracted from these separate alignments, with a length limit of 7. The translation model was smoothed in both directions with Kneser-Ney smoothing (Chen et al., 2011). We use the hierarchical lexicalized reordering model (Galley and Manning, 2008), with a distortion limit of 7. Other features include lexical weighting in both directions, word count, a distance-based RM, a 4-gram LM trained on the target side of the parallel data, and a 6-gram English Gigaword LM. The system was tuned with batch lattice MIRA (Cherry and Foster, 2012). We conducted experiments on NIST Chinese-toEnglish translation task. The training data are from NIST Open MT 2012. All allowed bilingual corpora were used to train the translation model and reordering models. There are about 2"
E14-1064,J11-2001,0,0.0726138,"r Computational Linguistics, pages 607–615, c Gothenburg, Sweden, April 26-30 2014. 2014 Association for Computational Linguistics The granularities of text have spanned from words and phrases to passages and documents. Sentiment analysis has been approached mainly as an unsupervised or supervised problem, although the middle ground, semi-supervised approaches, exists. In this paper, we take a lexiconbased, unsupervised approach to considering sentiment consistency for translation, although the translation system itself is supervised. The advantages of such an approach have been discussed in (Taboada et al., 2011). Briefly, it is good at capturing the basic sentiment expressions common to different domains, and certainly it requires no bilingual sentiment-annotated data for our study. It suits our purpose here of exploring the basic role of sentiment for translation. Also, such a method has been reported to achieve a good cross-domain performance (Taboada et al., 2011) comparable with that of other state-of-the-art models. Translation for sentiment analysis A very interesting line of research has leveraged labeled data in a resource-rich language (e.g., English) to help sentiment analysis in a resource"
E14-1064,N12-1047,0,0.0188096,"BM model 2, HMM, and IBM model 4, and the phrase table was the union of phrase pairs extracted from these separate alignments, with a length limit of 7. The translation model was smoothed in both directions with Kneser-Ney smoothing (Chen et al., 2011). We use the hierarchical lexicalized reordering model (Galley and Manning, 2008), with a distortion limit of 7. Other features include lexical weighting in both directions, word count, a distance-based RM, a 4-gram LM trained on the target side of the parallel data, and a 6-gram English Gigaword LM. The system was tuned with batch lattice MIRA (Cherry and Foster, 2012). We conducted experiments on NIST Chinese-toEnglish translation task. The training data are from NIST Open MT 2012. All allowed bilingual corpora were used to train the translation model and reordering models. There are about 283M target word tokens. The development (dev) set comprised mainly data from the NIST 2005 test set, and also some balanced-genre web-text from NIST training data. Evaluation was performed on NIST 2006 and 2008, which have 1,664 and 1,357 sentences, 39.7K and 33.7K source words respectively. Four references were provided for all dev and test sets. 4.2 Results Our evalua"
E14-1064,N03-1033,0,0.0671445,"Missing"
E14-1064,P02-1053,0,0.01648,"Missing"
E14-1064,D08-1089,0,0.0214746,"ppearing outside any sentiment units, or if both sides have an even number of such negation words; 4 Experiments 4.1 Translation experimental settings Experiments were carried out with an in-house phrase-based system similar to Moses (Koehn et al., 2007). Each corpus was word-aligned using IBM model 2, HMM, and IBM model 4, and the phrase table was the union of phrase pairs extracted from these separate alignments, with a length limit of 7. The translation model was smoothed in both directions with Kneser-Ney smoothing (Chen et al., 2011). We use the hierarchical lexicalized reordering model (Galley and Manning, 2008), with a distortion limit of 7. Other features include lexical weighting in both directions, word count, a distance-based RM, a 4-gram LM trained on the target side of the parallel data, and a 6-gram English Gigaword LM. The system was tuned with batch lattice MIRA (Cherry and Foster, 2012). We conducted experiments on NIST Chinese-toEnglish translation task. The training data are from NIST Open MT 2012. All allowed bilingual corpora were used to train the translation model and reordering models. There are about 283M target word tokens. The development (dev) set comprised mainly data from the"
E14-1064,C96-2141,0,0.499625,"Missing"
E14-1064,P97-1023,0,0.0462066,"Missing"
E14-1064,P09-1027,0,0.0493368,"chieve a good cross-domain performance (Taboada et al., 2011) comparable with that of other state-of-the-art models. Translation for sentiment analysis A very interesting line of research has leveraged labeled data in a resource-rich language (e.g., English) to help sentiment analysis in a resource-poorer language. This includes the idea of constructing sentiment lexicons automatically by using a translation dictionary (Mihalcea et al., 2007), as well as the idea of utilizing parallel corpora or automatically translated documents to incorporate sentiment-labeled data from different languages (Wan, 2009; Mihalcea et al., 2007). Our concern here is different ― instead of utilizing translation for sentiment analysis; we are interested in the SMT quality itself, by modeling bilingual sentiment in translation. As mentioned above, while we expect that statistics learned from parallel corpora have implicitly captured sentiment in some degree, we are curious if better modeling is possible. Considering semantic similarity in translation The literature has included interesting ideas of incorporating different types of semantic knowledge for SMT. A main stream of recent efforts have been leveraging se"
E14-1064,W04-3250,0,0.099985,"odel and reordering models. There are about 283M target word tokens. The development (dev) set comprised mainly data from the NIST 2005 test set, and also some balanced-genre web-text from NIST training data. Evaluation was performed on NIST 2006 and 2008, which have 1,664 and 1,357 sentences, 39.7K and 33.7K source words respectively. Four references were provided for all dev and test sets. 4.2 Results Our evaluation metric is case-insensitive IBM BLEU (Papineni et al., 2002), which performs matching of n-grams up to n = 4; we report BLEU scores on two test sets NIST06 and NIST08. Following (Koehn, 2004), we use the bootstrap resampling test to do significance testing. In Table 4-6, the sign * and ** denote statistically significant gains over the baseline at the p &lt; 0.05 and p &lt; 0.01 level, respectively. 8 The Stanford POS tagger (Toutanova et al., 2003) was used to tag phrase and sentence pairs for this purpose. 611 NIST06 Baseline 35.1 +feat. group1 35.6** +feat. group2 35.3* +feat. group3 35.3 +feat. group4 35.5* +feat. group1+2 35.8** +feat. group1+2+3 36.1** +feat. group1+2+3+4 36.2** NIST08 28.4 29.0** 28.7* 28.7* 28.8* 29.1** 29.3** 29.4** Avg. 31.7 32.3 32.0 32.0 32.1 32.5 32.7 32.8"
E14-1064,H05-1044,0,0.113648,"istency, we use a lexicon-based approach to sentiment analysis. Based on this, we design four groups of features to represent the consistency. The basic idea of the lexicon-based approach is first identifying the sentiment words, intensifiers, and negation words with lexicons, and then calculating the sentiment value using manually designed formulas. To this end, we adapted the approaches of (Taboada et al., 2011) and (Zhang et al., 2012) so as to use the same formulas to analyze the sentiment on both the source and the target side. The English and Chinese sentiment lexicons we used are from (Wilson et al. 2005) and (Xu and Lin, 2007), respectively. We further use 75 English in608 tensifiers listed in (Benzinger, 1971; page 171) and 81 Chinese intensifiers from (Zhang et al., 2012). We use 17 English and 13 Chinese negation words. Similar to (Taboada et al., 2011) and (Zhang et al., 2012), we assigned a numerical score to each sentiment word, intensifier, and negation word. More specifically, one of the five values: -0.8, -0.4, 0, 0.4, and 0.8, was assigned to each sentiment word in both the source and target sentiment lexicons, according to the strength information annotated in these lexicons. The s"
E14-1064,N09-2004,0,0.0233717,"al., 2007). Our concern here is different ― instead of utilizing translation for sentiment analysis; we are interested in the SMT quality itself, by modeling bilingual sentiment in translation. As mentioned above, while we expect that statistics learned from parallel corpora have implicitly captured sentiment in some degree, we are curious if better modeling is possible. Considering semantic similarity in translation The literature has included interesting ideas of incorporating different types of semantic knowledge for SMT. A main stream of recent efforts have been leveraging semantic roles (Wu and Fung, 2009; Liu and Gildea, 2010; Li et al., 2013) to improve translation, e.g., through improving reordering. Also, Chen et al. (2010) have leveraged sense similarity between source and target side as additional features. In this work, we view a different dimension, i.e., semantic orientation, and show that incorporating such knowledge improves the translation performance. We hope this work would add more evidences to the existing literature of leveraging semantics for SMT, and shed some light on further exploration of semantic consistency, e.g., examining other semantic differential factors. 3 Problem"
E14-1064,P07-2045,0,0.00417343,"ures here additionally check the counts of negation words. This group of features is binary and triggered by the following conditions.  F12: if neither side of the pair (f, e) contain negation words;  F13: if both sides have an odd number of negation words or both sides have an even number of them;  F14: if both sides have an odd number of negation words not appearing outside any sentiment units, or if both sides have an even number of such negation words; 4 Experiments 4.1 Translation experimental settings Experiments were carried out with an in-house phrase-based system similar to Moses (Koehn et al., 2007). Each corpus was word-aligned using IBM model 2, HMM, and IBM model 4, and the phrase table was the union of phrase pairs extracted from these separate alignments, with a length limit of 7. The translation model was smoothed in both directions with Kneser-Ney smoothing (Chen et al., 2011). We use the hierarchical lexicalized reordering model (Galley and Manning, 2008), with a distortion limit of 7. Other features include lexical weighting in both directions, word count, a distance-based RM, a 4-gram LM trained on the target side of the parallel data, and a 6-gram English Gigaword LM. The syst"
E14-1064,N13-1060,0,0.0147683,"― instead of utilizing translation for sentiment analysis; we are interested in the SMT quality itself, by modeling bilingual sentiment in translation. As mentioned above, while we expect that statistics learned from parallel corpora have implicitly captured sentiment in some degree, we are curious if better modeling is possible. Considering semantic similarity in translation The literature has included interesting ideas of incorporating different types of semantic knowledge for SMT. A main stream of recent efforts have been leveraging semantic roles (Wu and Fung, 2009; Liu and Gildea, 2010; Li et al., 2013) to improve translation, e.g., through improving reordering. Also, Chen et al. (2010) have leveraged sense similarity between source and target side as additional features. In this work, we view a different dimension, i.e., semantic orientation, and show that incorporating such knowledge improves the translation performance. We hope this work would add more evidences to the existing literature of leveraging semantics for SMT, and shed some light on further exploration of semantic consistency, e.g., examining other semantic differential factors. 3 Problem & Approach 3.1 Consistency of sentiment"
E14-1064,W12-3102,0,\N,Missing
E17-2088,W11-1701,0,0.0450617,"Macro-averaged F-scores of different models on the Multi-Target Stance dataset decoder deep neural model on our dataset. This model has both the advantages of windows-based and cascading classification, and it has the best performance compared to all other models and baselines. By applying paired t-test on these results, we concluded that the differences between sequence-to-sequence model and all other models are statistically significant. 5 Related Work Stance Detection Over the last decade, there has been active research in modeling stance (Thomas et al., 2006; Somasundaran and Wiebe, 2009; Anand et al., 2011; Sobhani et al., 2015; Walker et al., 2012a; Hasan and Ng, 2013; Sobhani et al., 2016). However, all of these previous works treat each target independently, ignoring the potential dependencies that could exist among related targets. Stance detection was one of the tasks in the SemEval-2016 shared task competition (Mohammad et al., 2016a). Out of 19 participant teams, most used standard text classification features such as n-grams and word embedding vectors, as well as standard sentiment analysis features, while others used deep neural models such as RNNs and convolutional neural nets. Most o"
E17-2088,I13-1191,0,0.0226907,"Stance dataset decoder deep neural model on our dataset. This model has both the advantages of windows-based and cascading classification, and it has the best performance compared to all other models and baselines. By applying paired t-test on these results, we concluded that the differences between sequence-to-sequence model and all other models are statistically significant. 5 Related Work Stance Detection Over the last decade, there has been active research in modeling stance (Thomas et al., 2006; Somasundaran and Wiebe, 2009; Anand et al., 2011; Sobhani et al., 2015; Walker et al., 2012a; Hasan and Ng, 2013; Sobhani et al., 2016). However, all of these previous works treat each target independently, ignoring the potential dependencies that could exist among related targets. Stance detection was one of the tasks in the SemEval-2016 shared task competition (Mohammad et al., 2016a). Out of 19 participant teams, most used standard text classification features such as n-grams and word embedding vectors, as well as standard sentiment analysis features, while others used deep neural models such as RNNs and convolutional neural nets. Most of the existing datasets for stance detection were created from o"
E17-2088,S14-2145,0,0.0368761,"Missing"
E17-2088,W14-4012,0,0.0207149,"Missing"
E17-2088,D14-1179,0,0.0080364,"Missing"
E17-2088,D14-1080,0,0.0238292,"As RNN unit, we used a Gated Recurrent Unit (Cho et al., 2014a) with 128 cells. The word vectors at the embedding layer have 100 554 Classifier Baselines i. random ii. majority One Classifier per Target a. Independent SVMs b. Window-based SVMs c. Cascading SVMs Single Model A. 9-Class SVM B. Seq2Seq F-macro positions towards them. Deep Recurrent Neural Models Different structures of deep RNNs have recently shown to be very effective in a wide range of sequence modeling problems, particularly for opinion mining and sentiment analysis (Zhu et al., 2015a; Socher et al., 2013; Zhu et al., 2015b; Irsoy and Cardie, 2014; Zhu et al., 2016). These neural models were extended for tasks with variable input and output sequence length including: end-toend neural machine translation (Sutskever et al., 2014; Cho et al., 2014b), image-to-text conversion (Vinyals et al., 2015b), syntactic constituency parsing (Vinyals et al., 2015a) and question answering (Hermann et al., 2015). Subsequently, the attention mechanism allowed the models to learn alignments between different parts of the source and the target such as between speech frames and the text in speech recognition (Chorowski et al., 2014) or between image frames"
E17-2088,D15-1166,0,0.00920779,"Missing"
E17-2088,D15-1018,0,0.0122887,"na Inkpen1 and Xiaodan Zhu2 1 EECS, University of Ottawa National Research Council Canada {psobh090,diana.inkpen}@uottawa.ca {xiaodan.zhu}@nrc-cnrc.gc.ca 2 Abstract sponding to related targets. Then, we investigate the problem of jointly predicting the stance expressed towards multiple targets (two at a time), in order to demonstrate the utility of the dataset. The closest work related to our work is Deng and Wiebe (2015a), where sentiment toward different entities and events is jointly modeled using a rule-based probabilistic soft logic approach. The authors also made their dataset MPQA 3.0 (Deng and Wiebe, 2015b) available, However, this dataset is relatively small (it contains 70 documents) and has a potentially infinite number of targets (the target sets depend on the context), which makes it hard to train a system. Instead, we provide a reasonably large dataset for training and evaluation. Our dataset contains 4,455 tweets manually annotated for stance towards more than one target simultaneously. We will refer to this data as the Multi-Target Stance Dataset. Moreover, we make available a much larger unlabeled dataset providing more choices for users to further investigate the multi-target stance"
E17-2088,N15-1146,0,0.0171662,"na Inkpen1 and Xiaodan Zhu2 1 EECS, University of Ottawa National Research Council Canada {psobh090,diana.inkpen}@uottawa.ca {xiaodan.zhu}@nrc-cnrc.gc.ca 2 Abstract sponding to related targets. Then, we investigate the problem of jointly predicting the stance expressed towards multiple targets (two at a time), in order to demonstrate the utility of the dataset. The closest work related to our work is Deng and Wiebe (2015a), where sentiment toward different entities and events is jointly modeled using a rule-based probabilistic soft logic approach. The authors also made their dataset MPQA 3.0 (Deng and Wiebe, 2015b) available, However, this dataset is relatively small (it contains 70 documents) and has a potentially infinite number of targets (the target sets depend on the context), which makes it hard to train a system. Instead, we provide a reasonably large dataset for training and evaluation. Our dataset contains 4,455 tweets manually annotated for stance towards more than one target simultaneously. We will refer to this data as the Multi-Target Stance Dataset. Moreover, we make available a much larger unlabeled dataset providing more choices for users to further investigate the multi-target stance"
E17-2088,S16-1003,1,0.596818,"Missing"
E17-2088,P11-2008,0,0.0247663,"Missing"
E17-2088,N12-1072,0,0.0322812,"s on the Multi-Target Stance dataset decoder deep neural model on our dataset. This model has both the advantages of windows-based and cascading classification, and it has the best performance compared to all other models and baselines. By applying paired t-test on these results, we concluded that the differences between sequence-to-sequence model and all other models are statistically significant. 5 Related Work Stance Detection Over the last decade, there has been active research in modeling stance (Thomas et al., 2006; Somasundaran and Wiebe, 2009; Anand et al., 2011; Sobhani et al., 2015; Walker et al., 2012a; Hasan and Ng, 2013; Sobhani et al., 2016). However, all of these previous works treat each target independently, ignoring the potential dependencies that could exist among related targets. Stance detection was one of the tasks in the SemEval-2016 shared task competition (Mohammad et al., 2016a). Out of 19 participant teams, most used standard text classification features such as n-grams and word embedding vectors, as well as standard sentiment analysis features, while others used deep neural models such as RNNs and convolutional neural nets. Most of the existing datasets for stance detectio"
E17-2088,W15-0509,1,0.56123,"res of different models on the Multi-Target Stance dataset decoder deep neural model on our dataset. This model has both the advantages of windows-based and cascading classification, and it has the best performance compared to all other models and baselines. By applying paired t-test on these results, we concluded that the differences between sequence-to-sequence model and all other models are statistically significant. 5 Related Work Stance Detection Over the last decade, there has been active research in modeling stance (Thomas et al., 2006; Somasundaran and Wiebe, 2009; Anand et al., 2011; Sobhani et al., 2015; Walker et al., 2012a; Hasan and Ng, 2013; Sobhani et al., 2016). However, all of these previous works treat each target independently, ignoring the potential dependencies that could exist among related targets. Stance detection was one of the tasks in the SemEval-2016 shared task competition (Mohammad et al., 2016a). Out of 19 participant teams, most used standard text classification features such as n-grams and word embedding vectors, as well as standard sentiment analysis features, while others used deep neural models such as RNNs and convolutional neural nets. Most of the existing dataset"
E17-2088,walker-etal-2012-annotated,0,0.011822,"s on the Multi-Target Stance dataset decoder deep neural model on our dataset. This model has both the advantages of windows-based and cascading classification, and it has the best performance compared to all other models and baselines. By applying paired t-test on these results, we concluded that the differences between sequence-to-sequence model and all other models are statistically significant. 5 Related Work Stance Detection Over the last decade, there has been active research in modeling stance (Thomas et al., 2006; Somasundaran and Wiebe, 2009; Anand et al., 2011; Sobhani et al., 2015; Walker et al., 2012a; Hasan and Ng, 2013; Sobhani et al., 2016). However, all of these previous works treat each target independently, ignoring the potential dependencies that could exist among related targets. Stance detection was one of the tasks in the SemEval-2016 shared task competition (Mohammad et al., 2016a). Out of 19 participant teams, most used standard text classification features such as n-grams and word embedding vectors, as well as standard sentiment analysis features, while others used deep neural models such as RNNs and convolutional neural nets. Most of the existing datasets for stance detectio"
E17-2088,S16-2021,1,0.614302,"er deep neural model on our dataset. This model has both the advantages of windows-based and cascading classification, and it has the best performance compared to all other models and baselines. By applying paired t-test on these results, we concluded that the differences between sequence-to-sequence model and all other models are statistically significant. 5 Related Work Stance Detection Over the last decade, there has been active research in modeling stance (Thomas et al., 2006; Somasundaran and Wiebe, 2009; Anand et al., 2011; Sobhani et al., 2015; Walker et al., 2012a; Hasan and Ng, 2013; Sobhani et al., 2016). However, all of these previous works treat each target independently, ignoring the potential dependencies that could exist among related targets. Stance detection was one of the tasks in the SemEval-2016 shared task competition (Mohammad et al., 2016a). Out of 19 participant teams, most used standard text classification features such as n-grams and word embedding vectors, as well as standard sentiment analysis features, while others used deep neural models such as RNNs and convolutional neural nets. Most of the existing datasets for stance detection were created from online debate forums lik"
E17-2088,D13-1170,0,0.00153005,"learning rate (Adadelta (Zeiler, 2012)). As RNN unit, we used a Gated Recurrent Unit (Cho et al., 2014a) with 128 cells. The word vectors at the embedding layer have 100 554 Classifier Baselines i. random ii. majority One Classifier per Target a. Independent SVMs b. Window-based SVMs c. Cascading SVMs Single Model A. 9-Class SVM B. Seq2Seq F-macro positions towards them. Deep Recurrent Neural Models Different structures of deep RNNs have recently shown to be very effective in a wide range of sequence modeling problems, particularly for opinion mining and sentiment analysis (Zhu et al., 2015a; Socher et al., 2013; Zhu et al., 2015b; Irsoy and Cardie, 2014; Zhu et al., 2016). These neural models were extended for tasks with variable input and output sequence length including: end-toend neural machine translation (Sutskever et al., 2014; Cho et al., 2014b), image-to-text conversion (Vinyals et al., 2015b), syntactic constituency parsing (Vinyals et al., 2015a) and question answering (Hermann et al., 2015). Subsequently, the attention mechanism allowed the models to learn alignments between different parts of the source and the target such as between speech frames and the text in speech recognition (Chor"
E17-2088,S15-1001,1,0.782275,"ithm with adaptive learning rate (Adadelta (Zeiler, 2012)). As RNN unit, we used a Gated Recurrent Unit (Cho et al., 2014a) with 128 cells. The word vectors at the embedding layer have 100 554 Classifier Baselines i. random ii. majority One Classifier per Target a. Independent SVMs b. Window-based SVMs c. Cascading SVMs Single Model A. 9-Class SVM B. Seq2Seq F-macro positions towards them. Deep Recurrent Neural Models Different structures of deep RNNs have recently shown to be very effective in a wide range of sequence modeling problems, particularly for opinion mining and sentiment analysis (Zhu et al., 2015a; Socher et al., 2013; Zhu et al., 2015b; Irsoy and Cardie, 2014; Zhu et al., 2016). These neural models were extended for tasks with variable input and output sequence length including: end-toend neural machine translation (Sutskever et al., 2014; Cho et al., 2014b), image-to-text conversion (Vinyals et al., 2015b), syntactic constituency parsing (Vinyals et al., 2015a) and question answering (Hermann et al., 2015). Subsequently, the attention mechanism allowed the models to learn alignments between different parts of the source and the target such as between speech frames and the text in sp"
E17-2088,P09-1026,0,0.0147156,"32 52.05 50.63 54.81 Table 5: Macro-averaged F-scores of different models on the Multi-Target Stance dataset decoder deep neural model on our dataset. This model has both the advantages of windows-based and cascading classification, and it has the best performance compared to all other models and baselines. By applying paired t-test on these results, we concluded that the differences between sequence-to-sequence model and all other models are statistically significant. 5 Related Work Stance Detection Over the last decade, there has been active research in modeling stance (Thomas et al., 2006; Somasundaran and Wiebe, 2009; Anand et al., 2011; Sobhani et al., 2015; Walker et al., 2012a; Hasan and Ng, 2013; Sobhani et al., 2016). However, all of these previous works treat each target independently, ignoring the potential dependencies that could exist among related targets. Stance detection was one of the tasks in the SemEval-2016 shared task competition (Mohammad et al., 2016a). Out of 19 participant teams, most used standard text classification features such as n-grams and word embedding vectors, as well as standard sentiment analysis features, while others used deep neural models such as RNNs and convolutional"
E17-2088,W10-0214,0,0.0322195,"s treat each target independently, ignoring the potential dependencies that could exist among related targets. Stance detection was one of the tasks in the SemEval-2016 shared task competition (Mohammad et al., 2016a). Out of 19 participant teams, most used standard text classification features such as n-grams and word embedding vectors, as well as standard sentiment analysis features, while others used deep neural models such as RNNs and convolutional neural nets. Most of the existing datasets for stance detection were created from online debate forums like 4forums.com and createdebates.com (Somasundaran and Wiebe, 2010; Walker et al., 2012b; Hasan and Ng, 2013). The majority of these debates are two-sided, and the data labels are often provided by the authors of the posts. Recently, Mohammad et al. (2016b) created a dataset of tweets labeled for both stance and sentiment. None of the prior work has created a dataset annotated for more than one target simultaneously, neither has explored the dependencies and relationships between targets when predicting overall 6 Conclusions and Future Work We presented the first multi-target stance dataset of a reasonable size from social media, to help further exploration"
E17-2088,N16-1106,1,0.62421,"Gated Recurrent Unit (Cho et al., 2014a) with 128 cells. The word vectors at the embedding layer have 100 554 Classifier Baselines i. random ii. majority One Classifier per Target a. Independent SVMs b. Window-based SVMs c. Cascading SVMs Single Model A. 9-Class SVM B. Seq2Seq F-macro positions towards them. Deep Recurrent Neural Models Different structures of deep RNNs have recently shown to be very effective in a wide range of sequence modeling problems, particularly for opinion mining and sentiment analysis (Zhu et al., 2015a; Socher et al., 2013; Zhu et al., 2015b; Irsoy and Cardie, 2014; Zhu et al., 2016). These neural models were extended for tasks with variable input and output sequence length including: end-toend neural machine translation (Sutskever et al., 2014; Cho et al., 2014b), image-to-text conversion (Vinyals et al., 2015b), syntactic constituency parsing (Vinyals et al., 2015a) and question answering (Hermann et al., 2015). Subsequently, the attention mechanism allowed the models to learn alignments between different parts of the source and the target such as between speech frames and the text in speech recognition (Chorowski et al., 2014) or between image frames and the agent’s ac"
E17-2088,W06-1639,0,0.0184189,"34.26 32.11 51.37 48.32 52.05 50.63 54.81 Table 5: Macro-averaged F-scores of different models on the Multi-Target Stance dataset decoder deep neural model on our dataset. This model has both the advantages of windows-based and cascading classification, and it has the best performance compared to all other models and baselines. By applying paired t-test on these results, we concluded that the differences between sequence-to-sequence model and all other models are statistically significant. 5 Related Work Stance Detection Over the last decade, there has been active research in modeling stance (Thomas et al., 2006; Somasundaran and Wiebe, 2009; Anand et al., 2011; Sobhani et al., 2015; Walker et al., 2012a; Hasan and Ng, 2013; Sobhani et al., 2016). However, all of these previous works treat each target independently, ignoring the potential dependencies that could exist among related targets. Stance detection was one of the tasks in the SemEval-2016 shared task competition (Mohammad et al., 2016a). Out of 19 participant teams, most used standard text classification features such as n-grams and word embedding vectors, as well as standard sentiment analysis features, while others used deep neural models"
E17-2088,L16-1623,1,\N,Missing
I11-1057,J02-1002,0,0.0308484,"es on transcripts for each slide bullet are compared against the corresponding gold-standard boundaries to calculate offsets measured in number of words, counted after stopwords having been removed, which are then averaged over all boundaries to evaluate model performance. Though one may consider that different bullets may be of different importance, in this paper we do not use any heuristics to judge this and we treat all bullets equally in our evaluation. Note that topic segmentation research often uses metrics such as Pk and WindowDiff (Malioutov and Barzilay, 2006; Beeferman et al., 1999; Pevsner and Hearst, 2002). Our problem here, as an alignment problem, has an exact 1-to-1 correspondence between a gold and automatic boundary, in which we can directly measure the exact offset of each boundary. 2006), which denotes the normalized partition cost of the segment from utterance uj+1 to uk , inclusively. For complexity, since the cohesion model is O(M N 2 ), linearly combining it would not increase the time complexities of the corresponding polynomial alignment models, which are at least O(M N 2 ) by themselves. 7 Experiment Set-up Corpus Our experiment uses a corpus of four 50-minute university lectures"
I11-1057,J97-1003,0,0.0482495,"ed the HieBase model in the remainder of this paper. One major benefit of the deterministic hierarchical alignment models is their time complexity: still quadratic, same as the sequential alignment model discussed above, though models like HieCut can achieve a very competitive perfor6 The Topic-segmentation Model Up to now, we have discussed a variety of alignment models with different model capabilities and time complexities, which, however, consider only similarities between bullets and utterances. Cohesion in text or speech, by itself, often evidenced by the change of lexical distribution (Hearst, 1997), can also indicate topic or subtopic transitions, even among subtle subtopics (Malioutov and Barzilay, 2006). In our problem here, when a lecturer discusses a bullet, the words used are likely to be different from those used in another bullet, suggesting that the spoken documents themselves, when ignoring the alignment model above for the time being, could potentially indicate the semantic boundaries that we are interested in here. Particularly, the cohesion conveyed by the repetition of the words that appear in transcripts but not in slides could be additionally helpful; this is very likely"
I11-1057,W06-1644,0,0.018891,"ame as in (Malioutov et al., 2007), for which we split each lecture into M chunks, the number of bullets. Finally, we obtained a M-by-N bullet-utterance similarity matrix and a N-by-N utterance-utterance matrix to optimize the alignment model and topic-segmentation 8 Experimental Results Alignment Models Table 1 presents the experimental results obtained on the automatic transcripts generated by the ASR models discussed above, with WERs of 0.43 and 0.48, respectively, which are typical for lectures and conference presentations in realistic and less controlled situations (Leeuwis et al., 2003; Hsu and Glass, 2006; Munteanu et al., 2007). The results show that among the four quadratic models, i.e., the first four models in the table, HieCut achieves the best performance. The results also suggest that the improvement of HieCut over SeqBase comes from two aspects. First, the normalized-cut objective used in the graph-partitioning based model seems to outperform that used in the baseline, indicated by the better performance of SeqCut over SeqBase, since both take as input the same, sequentialized bullet sequence and the corresponding transcribed utterances. The DTW-based objective used in SeqBase correspo"
I11-1057,J02-4006,0,0.0196473,"nicity between transcripts and slide trees, which violates some basic properties of the problem that we will discuss. More recently, the work of (Zhu, 2011) proposes a graphpartitioning based model (revisited in Section 4) and shows that the model outperforms a bulletsequentializing model. 2 Related Work Alignment of parallel texts In general, research on finding correspondences between parallel texts pervades both spoken and written language processing, e.g., in training statistical machine translation models, identifying relationship between human-written summaries and their original texts (Jing, 2002), force-aligning speech and transcripts in ASR, and grounding text with database facts (Snyder and Barzilay, 2007; Chen and Mooney, 2008; Liang et al., 2009). Our problem here, however, is distinguished in several major aspects, which need to be considered in our modeling. First, it involves segmentation—alignment is conducted together with the decision of the corresponding segment boundaries on transcripts; in other words, we are not finally concerned with the specific utterances that a bullet is aligned to, but the region of utterances. In such a sense, graph partitioning seems intuitively t"
I11-1057,P09-1011,0,0.036298,", 2011) proposes a graphpartitioning based model (revisited in Section 4) and shows that the model outperforms a bulletsequentializing model. 2 Related Work Alignment of parallel texts In general, research on finding correspondences between parallel texts pervades both spoken and written language processing, e.g., in training statistical machine translation models, identifying relationship between human-written summaries and their original texts (Jing, 2002), force-aligning speech and transcripts in ASR, and grounding text with database facts (Snyder and Barzilay, 2007; Chen and Mooney, 2008; Liang et al., 2009). Our problem here, however, is distinguished in several major aspects, which need to be considered in our modeling. First, it involves segmentation—alignment is conducted together with the decision of the corresponding segment boundaries on transcripts; in other words, we are not finally concerned with the specific utterances that a bullet is aligned to, but the region of utterances. In such a sense, graph partitioning seems intuitively to be more relevant than models optimizing a full-alignment score. Second, unlike a string-to-string alignment task, the problem involves hierarchical tree st"
I11-1057,P06-1004,0,0.239262,"ing of the semantic tree-to-string alignment task. First of all, a basic question is associated with different ways of exploiting the semantic trees when performing alignment, which, as will be studied comprehensively in this paper, results in models of different modeling capabilities and time complexities. Second, all the models discussed above consider only similarities between bullets and transcribed utterances, while similarities among utterances, which directly underline a cohesion model, are generally ignored. We will show in this paper that the stateof-the-art topic-segmentation model (Malioutov and Barzilay, 2006) can be inherently incorporated into the graph-partitioning-based alignment models. Third, the different alignment objectives, e.g., that of the graph-partitioning models versus that of basic DTW-based models, are entangled together with different ways of exploiting the bullet tree structures in (Zhu, 2011). In this paper, we discuss two more quadratic-time models to bridge the gap. Specifically, this paper studies nine different models, with the aim to provide a comprehensive 510 utterance uj and ends at the kth, inclusively. Constrained by the tree structure, the transcript region correspond"
I11-1057,P07-1064,0,0.101654,"elivering, and even automatic transcription were possible. Navigating audio documents is often inherently much more difficult than browsing text. An obvious solution, in relying on human beings’ ability of reading text, is to conduct a speech-to-text conversion through ASR, which in turn raises a new set of problems to be considered. First, the convenience and efficiency of reading transcripts Semantic Structures of Spoken Documents Much previous work, similar to its written-text counterpart, has attempted to find certain flat structures of spoken documents such as topic and slide boundaries (Malioutov et al., 2007; Zhu et al., 2008), which, however, involve no hierarchical structures of a spoken document, thought as will be shown in this paper, topic-segmentation models can be considered in our alignment task. Research has also resorted to other multimedia channels, e.g., video (Fan et al., 2006), to detect slide 509 Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 509–517, c Chiang Mai, Thailand, November 8 – 13, 2011. 2011 AFNLP transitions. This type of approaches, however, are unlikely to recover semantic structures more detailed than slide boundaries. und"
I11-1057,N10-1006,0,0.0128394,"ent Models Xiaodan Zhu & Colin Cherry Institute for Information Technology National Research Council Canada Gerald Penn Department of Computer Science University of Toronto {Xiaodan.Zhu,Colin.Cherry}@nrc-cnrc.gc.ca gpenn@cs.toronto.edu Abstract are affected by errors produced in transcription channels, though if the goal is only to browse the most salient parts, recognition errors in excerpts can be reduced by considering ASR confidence (Xie and Liu, 2010; Hori and Furui, 2003; Zechner and Waibel, 2000) and the quality of excerpts can be improved from various perspectives (Zhang et al., 2010; Xie and Liu, 2010; Zhu et al., 2009; Murray, 2008; Zhu and Penn, 2006; Maskey and Hirschberg, 2005). Even if transcription quality were not a problem, browsing lengthy transcripts is not straightforward, since, as mentioned above, indicative browsing structures are barely manually created for and aligned with spoken documents. Ideally, such semantic structures should be inferred directly from the spoken documents themselves, but this is known to be difficult even for written texts, which are often more linguistically well-formed and less noisy than automatically transcribed text. This paper studies a less ambi"
I11-1057,A00-2025,0,0.0218271,"Missing"
I11-1057,P09-1062,1,0.850006,"Zhu & Colin Cherry Institute for Information Technology National Research Council Canada Gerald Penn Department of Computer Science University of Toronto {Xiaodan.Zhu,Colin.Cherry}@nrc-cnrc.gc.ca gpenn@cs.toronto.edu Abstract are affected by errors produced in transcription channels, though if the goal is only to browse the most salient parts, recognition errors in excerpts can be reduced by considering ASR confidence (Xie and Liu, 2010; Hori and Furui, 2003; Zechner and Waibel, 2000) and the quality of excerpts can be improved from various perspectives (Zhang et al., 2010; Xie and Liu, 2010; Zhu et al., 2009; Murray, 2008; Zhu and Penn, 2006; Maskey and Hirschberg, 2005). Even if transcription quality were not a problem, browsing lengthy transcripts is not straightforward, since, as mentioned above, indicative browsing structures are barely manually created for and aligned with spoken documents. Ideally, such semantic structures should be inferred directly from the spoken documents themselves, but this is known to be difficult even for written texts, which are often more linguistically well-formed and less noisy than automatically transcribed text. This paper studies a less ambitious problem: we"
I11-1057,C10-2177,1,0.418791,"Thailand, November 8 – 13, 2011. 2011 AFNLP transitions. This type of approaches, however, are unlikely to recover semantic structures more detailed than slide boundaries. understanding of the questions discussed above. In the remainder of the paper, we will first review the related work (Section 2) and more formally describe our problem (Section 3). Then we revisit the graph-partitioning alignment model (Section 4), before present all the alignment models we will study (Section 5). We describe our experiment setup in Section 7 and results in Section 8, and draw our conclusions in Section 9. Zhu et al. (2010) investigate the problem of aligning electronic slides with lecture transcripts by first sequentializing bullet trees on slides with a pre-order walk before conducting alignment, through which the problem is reduced to a string-to-string alignment problem and conventional methods such as DTW (dynamic time warping) based alignment can then be directly applicable. A pre-order walk of bullet tree on slides is actually a natural choice, since speakers of presentations often follow such an order to develop their talks, i.e., they discuss a parent bullet first and then each of its children in sequen"
I11-1057,P07-1069,0,\N,Missing
L16-1623,W11-1701,0,0.0742288,"Missing"
L16-1623,W14-2107,0,0.042192,"Missing"
L16-1623,W12-3810,0,0.0175543,"understand how stance can be detected from tweets that do not explicitly mention the target of interest. Stance detection can be thought of as a textual inference or textual entailment task, where the goal is to determine whether the favorability of the target is entailed by the tweet. Thus the dataset can be used for developing textual inference engines and open domain reasoning. 6. Related Work Past work on stance detection includes that by Somasundaran and Wiebe (2010), Anand et al. (2011), Faulkner (2014), Rajadesingan and Liu (2014), Djemili et al. (2014), ˇ Boltuzic and Snajder (2014), Conrad et al. (2012), Hasan and Ng (2013a), Djemili et al. (2014), Sridhar et al. (2014), and Sobhani et al. (2015). In one of the few works on stance detection in tweets, Rajadesingan and Liu (2014) determine stance at user-level based on the assumption that if several users retweet one pair of tweets about a controversial topic, it is likely that they support the same side of a debate. Djemili et al. (2014) use a set of rules based on the syntax and discourse structure of the tweet to identify tweets that contain ideological stance. However, none of these works attempts to determine stance from a single tweet."
L16-1623,P13-2142,0,0.00626643,"can be detected from tweets that do not explicitly mention the target of interest. Stance detection can be thought of as a textual inference or textual entailment task, where the goal is to determine whether the favorability of the target is entailed by the tweet. Thus the dataset can be used for developing textual inference engines and open domain reasoning. 6. Related Work Past work on stance detection includes that by Somasundaran and Wiebe (2010), Anand et al. (2011), Faulkner (2014), Rajadesingan and Liu (2014), Djemili et al. (2014), ˇ Boltuzic and Snajder (2014), Conrad et al. (2012), Hasan and Ng (2013a), Djemili et al. (2014), Sridhar et al. (2014), and Sobhani et al. (2015). In one of the few works on stance detection in tweets, Rajadesingan and Liu (2014) determine stance at user-level based on the assumption that if several users retweet one pair of tweets about a controversial topic, it is likely that they support the same side of a debate. Djemili et al. (2014) use a set of rules based on the syntax and discourse structure of the tweet to identify tweets that contain ideological stance. However, none of these works attempts to determine stance from a single tweet. There is a vast amou"
L16-1623,I13-1191,0,0.0256128,"can be detected from tweets that do not explicitly mention the target of interest. Stance detection can be thought of as a textual inference or textual entailment task, where the goal is to determine whether the favorability of the target is entailed by the tweet. Thus the dataset can be used for developing textual inference engines and open domain reasoning. 6. Related Work Past work on stance detection includes that by Somasundaran and Wiebe (2010), Anand et al. (2011), Faulkner (2014), Rajadesingan and Liu (2014), Djemili et al. (2014), ˇ Boltuzic and Snajder (2014), Conrad et al. (2012), Hasan and Ng (2013a), Djemili et al. (2014), Sridhar et al. (2014), and Sobhani et al. (2015). In one of the few works on stance detection in tweets, Rajadesingan and Liu (2014) determine stance at user-level based on the assumption that if several users retweet one pair of tweets about a controversial topic, it is likely that they support the same side of a debate. Djemili et al. (2014) use a set of rules based on the syntax and discourse structure of the tweet to identify tweets that contain ideological stance. However, none of these works attempts to determine stance from a single tweet. There is a vast amou"
L16-1623,S14-2076,1,0.86144,"ce from a single tweet. There is a vast amount of work in sentiment analysis of tweets, and we refer the reader to surveys (Pang and Lee, 2008; Liu and Zhang, 2012; Mohammad, 2015) and proceedings of recent shared task competitions (Wilson et al., 2013; Mohammad et al., 2013; Rosenthal et al., 2015). Closely-related is the area of aspect based sentiment analysis (ABSA), where the goal is to determine sentiment towards aspects of a product such as speed of processor and screen resolution of a cell phone. We refer the reader to SemEval proceedings for related work on ABSA (Pontiki et al., 2015; Kiritchenko et al., 2014; Pontiki et al., 2014). 7. Summary We presented a new dataset of 4,870 tweet–target pairs annotated for stance of the tweeter towards the target. This dataset, which we refer to as the Stance Dataset, has instances corresponding to six pre-chosen targets of interest: ‘Atheism’, ‘Climate Change is a Real Concern’, ‘Feminist Movement’, ‘Hillary Clinton’, ‘Legalization of Abortion’, and ‘Donald Trump’. The annotations were performed by crowdsourcing. Several techniques were employed to encourage high-quality annotations and to identify and discard poor annotations. We analyzed the dataset to sho"
L16-1623,S13-2053,1,0.0870714,"tion that if several users retweet one pair of tweets about a controversial topic, it is likely that they support the same side of a debate. Djemili et al. (2014) use a set of rules based on the syntax and discourse structure of the tweet to identify tweets that contain ideological stance. However, none of these works attempts to determine stance from a single tweet. There is a vast amount of work in sentiment analysis of tweets, and we refer the reader to surveys (Pang and Lee, 2008; Liu and Zhang, 2012; Mohammad, 2015) and proceedings of recent shared task competitions (Wilson et al., 2013; Mohammad et al., 2013; Rosenthal et al., 2015). Closely-related is the area of aspect based sentiment analysis (ABSA), where the goal is to determine sentiment towards aspects of a product such as speed of processor and screen resolution of a cell phone. We refer the reader to SemEval proceedings for related work on ABSA (Pontiki et al., 2015; Kiritchenko et al., 2014; Pontiki et al., 2014). 7. Summary We presented a new dataset of 4,870 tweet–target pairs annotated for stance of the tweeter towards the target. This dataset, which we refer to as the Stance Dataset, has instances corresponding to six pre-chosen tar"
L16-1623,S16-1003,1,0.853828,"at one can be in favor of Jeb Bush and yet also be in favor of Donald Trump. However, the goal in stance detection, is to determine which is more probable: that the author is in favor of, against, or neutral towards the target. In this case, most annotators will agree that the tweeter is likely against Donald Trump. To aid further analysis, the tweets in the Stance Dataset are also annotated for whether target of interest is the target of opinion in the tweet. Partitions of the Stance Dataset were used to create training and test sets for the SemEval-2016 Task 6: Detecting Stance from Tweets (Mohammad et al., 2016a).1 Mohammad et al. (2016b) subsequently annotated the Stance Dataset for sentiment and quantitatively explored the relationship between stance and sentiment. The rest of the paper is structured as follows. In Section 2, we describe how we created the Stance Dataset. Section 3 presents a detailed analysis of the stance annotations. Section 4 presents an online interactive visualization of the Stance Dataset. Section 5 discusses how the dataset can be (and is being) used by the research community. Finally we present concluding remarks in Section 6. All of the data created as part of this proje"
L16-1623,C10-2100,0,0.0609499,"Missing"
L16-1623,S14-2004,0,0.174273,"Missing"
L16-1623,S15-2082,0,0.0208171,"mpts to determine stance from a single tweet. There is a vast amount of work in sentiment analysis of tweets, and we refer the reader to surveys (Pang and Lee, 2008; Liu and Zhang, 2012; Mohammad, 2015) and proceedings of recent shared task competitions (Wilson et al., 2013; Mohammad et al., 2013; Rosenthal et al., 2015). Closely-related is the area of aspect based sentiment analysis (ABSA), where the goal is to determine sentiment towards aspects of a product such as speed of processor and screen resolution of a cell phone. We refer the reader to SemEval proceedings for related work on ABSA (Pontiki et al., 2015; Kiritchenko et al., 2014; Pontiki et al., 2014). 7. Summary We presented a new dataset of 4,870 tweet–target pairs annotated for stance of the tweeter towards the target. This dataset, which we refer to as the Stance Dataset, has instances corresponding to six pre-chosen targets of interest: ‘Atheism’, ‘Climate Change is a Real Concern’, ‘Feminist Movement’, ‘Hillary Clinton’, ‘Legalization of Abortion’, and ‘Donald Trump’. The annotations were performed by crowdsourcing. Several techniques were employed to encourage high-quality annotations and to identify and discard poor annotations. We a"
L16-1623,S15-2078,1,0.0922389,"ers retweet one pair of tweets about a controversial topic, it is likely that they support the same side of a debate. Djemili et al. (2014) use a set of rules based on the syntax and discourse structure of the tweet to identify tweets that contain ideological stance. However, none of these works attempts to determine stance from a single tweet. There is a vast amount of work in sentiment analysis of tweets, and we refer the reader to surveys (Pang and Lee, 2008; Liu and Zhang, 2012; Mohammad, 2015) and proceedings of recent shared task competitions (Wilson et al., 2013; Mohammad et al., 2013; Rosenthal et al., 2015). Closely-related is the area of aspect based sentiment analysis (ABSA), where the goal is to determine sentiment towards aspects of a product such as speed of processor and screen resolution of a cell phone. We refer the reader to SemEval proceedings for related work on ABSA (Pontiki et al., 2015; Kiritchenko et al., 2014; Pontiki et al., 2014). 7. Summary We presented a new dataset of 4,870 tweet–target pairs annotated for stance of the tweeter towards the target. This dataset, which we refer to as the Stance Dataset, has instances corresponding to six pre-chosen targets of interest: ‘Atheis"
L16-1623,W15-0509,1,0.0660808,"f interest. Stance detection can be thought of as a textual inference or textual entailment task, where the goal is to determine whether the favorability of the target is entailed by the tweet. Thus the dataset can be used for developing textual inference engines and open domain reasoning. 6. Related Work Past work on stance detection includes that by Somasundaran and Wiebe (2010), Anand et al. (2011), Faulkner (2014), Rajadesingan and Liu (2014), Djemili et al. (2014), ˇ Boltuzic and Snajder (2014), Conrad et al. (2012), Hasan and Ng (2013a), Djemili et al. (2014), Sridhar et al. (2014), and Sobhani et al. (2015). In one of the few works on stance detection in tweets, Rajadesingan and Liu (2014) determine stance at user-level based on the assumption that if several users retweet one pair of tweets about a controversial topic, it is likely that they support the same side of a debate. Djemili et al. (2014) use a set of rules based on the syntax and discourse structure of the tweet to identify tweets that contain ideological stance. However, none of these works attempts to determine stance from a single tweet. There is a vast amount of work in sentiment analysis of tweets, and we refer the reader to surv"
L16-1623,P09-1026,0,0.0238553,"Missing"
L16-1623,W10-0214,0,0.0476215,"Missing"
L16-1623,W14-2715,0,0.0421924,"icitly mention the target of interest. Stance detection can be thought of as a textual inference or textual entailment task, where the goal is to determine whether the favorability of the target is entailed by the tweet. Thus the dataset can be used for developing textual inference engines and open domain reasoning. 6. Related Work Past work on stance detection includes that by Somasundaran and Wiebe (2010), Anand et al. (2011), Faulkner (2014), Rajadesingan and Liu (2014), Djemili et al. (2014), ˇ Boltuzic and Snajder (2014), Conrad et al. (2012), Hasan and Ng (2013a), Djemili et al. (2014), Sridhar et al. (2014), and Sobhani et al. (2015). In one of the few works on stance detection in tweets, Rajadesingan and Liu (2014) determine stance at user-level based on the assumption that if several users retweet one pair of tweets about a controversial topic, it is likely that they support the same side of a debate. Djemili et al. (2014) use a set of rules based on the syntax and discourse structure of the tweet to identify tweets that contain ideological stance. However, none of these works attempts to determine stance from a single tweet. There is a vast amount of work in sentiment analysis of tweets, and"
L16-1623,W06-1639,0,0.0852829,"Missing"
L16-1623,N12-1072,0,0.0225157,"Missing"
L16-1623,S13-2052,0,0.0239726,"l based on the assumption that if several users retweet one pair of tweets about a controversial topic, it is likely that they support the same side of a debate. Djemili et al. (2014) use a set of rules based on the syntax and discourse structure of the tweet to identify tweets that contain ideological stance. However, none of these works attempts to determine stance from a single tweet. There is a vast amount of work in sentiment analysis of tweets, and we refer the reader to surveys (Pang and Lee, 2008; Liu and Zhang, 2012; Mohammad, 2015) and proceedings of recent shared task competitions (Wilson et al., 2013; Mohammad et al., 2013; Rosenthal et al., 2015). Closely-related is the area of aspect based sentiment analysis (ABSA), where the goal is to determine sentiment towards aspects of a product such as speed of processor and screen resolution of a cell phone. We refer the reader to SemEval proceedings for related work on ABSA (Pontiki et al., 2015; Kiritchenko et al., 2014; Pontiki et al., 2014). 7. Summary We presented a new dataset of 4,870 tweet–target pairs annotated for stance of the tweeter towards the target. This dataset, which we refer to as the Stance Dataset, has instances correspondin"
N06-2050,C04-1110,0,0.160099,"t utterances in spontaneous conversations could be different from that in broadcast news, e.g., the beginning part of news often contains important information, but in conversations, information may be more evenly distributed; (3) conversations often contain discourse clues, e.g., question-answer pairs and speakers’ information, which can be utilized to keep the summary coherent; (4) word error rates (WERs) from speech recognition are usually much higher in spontaneous conversations. Previous work on spontaneous-conversation summarization has mainly focused on textual features (Zechner, 2001; Gurevych and Strube, 2004), while speech-related features have not been explored for this type of speech source. This paper explores and compares the effectiveness of both textual features and speech-related features. The experiments show that these features incrementally improve summarization performance. We also discuss problems (1) and (2) mentioned above. For (1), Zechner (2001) proposes to detect and remove false starts and speech disfluencies from transcripts, in order to make the text-format summary concise and more readable. Nevertheless, it is not always necessary to remove them. One reason is that original ut"
N06-2050,A00-2025,0,0.0582625,"terance position) is less effective for summarizing spontaneous conversations than it is in broadcast news. MMR 197 Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 197–200, c New York, June 2006. 2006 Association for Computational Linguistics and lexical features are the best. Speech-related features follow. The structural feature is least effective. We do not discuss problem (3) and (4) in this paper. For problem (3), a similar idea has been proposed to summarize online blogs and discussions. Problem (4) has been partially addressed by (Zechner & Waibel, 2000); but it has not been studied together with acoustic features. 2 Utterance-extraction-based summarization Still at its early stage, current research on speech summarization targets a less ambitious goal: conducting extractive, single-document, generic, and surface-level-feature-based summarization. The pieces to be extracted could correspond to words (Koumpis, 2002; Hori and Furui, 2003). The extracts could be utterances, too. Utterance selection is useful. First, it could be a preliminary stage applied before word extraction, as proposed by Kikuchi et al. (2003) in their two-stage summarizer."
N16-1106,P15-1011,1,0.825085,"er et al., 2011; van der Velde and de Kamps, 2006). In human languages, the recent years have seen extensive interests on distributional approaches. The research includes the influential pioneering work that examined a number of explicit forms of compositional functions (Mitchell and Lapata, 2008). More recent works explored neural networks, e.g., (Socher et al., 2013; Irsoy and Cardie, 2014; Kalchbrenner et al., 2014; Tai et al., 2015; Le and Zuidema, 2015; Zhu et al., 2015c) among many others, which extended the success of word-level embeddings (Collobert et al., 2011; Mikolov et al., 2013; Chen et al., 2015) and modeled sentences through semantic composition. In general, neural models can fit very complicated functions and can be a universal approximator (Cybenko, 1989; Hornik, 1991). In obtaining the distributed representation for longer spans of text from its subsequences, previous neural models assume full compositionality from the atomic components and disregard noncompositionality and in general prior semantics. Some very recent work (Zhu et al., 2015a) has started to address this problem in recursive neural networks with the assumption of the availability of parse information. In this work,"
N16-1106,D08-1083,0,0.0214838,"e often short, use informal languages, and are often not linguistically well-formed. Syntactic analysis such as parsing is much less reliable in such data than in news articles, and sequential models without depending on deep linguistic analysis (e.g., parsing) are adopted by most previous work. In obtaining the sentiment of a text span, e.g., a sentence, early work often factorized the problem to consider smaller pieces of component words or phrases with bag-of-words or bag-of-phrases models (Liu and Zhang, 2012; Pang and Lee, 2008). More recent work has started to model composition process (Choi and Cardie, 2008; Moilanen and Pulman, 2007; Socher et al., 2012; Socher et al., 2013; Irsoy and Cardie, 2014; Kalchbrenner et al., 2014; Tai et al., 2015; Zhu et al., 2015b; Le and Zuidema, 2015), more closely. In general, the composition process is critical in the formation of the sentiment of a text span, which has not been well modeled yet and more work would be desirable. 4.1 Figure 2: An example of a fork memory block. Both the hidden vectors ht and cell vectors ct are passed along multiple outgoing paths to the future blocks. ⊗ denotes a Hadamard product, and the ”s” shape sign is a squashing function"
N16-1106,P14-1062,0,0.10426,"al semantics including non-compositional or holistically learned semantics. Compositionality Semantic composition exists in multiple modalities, including images and vision (Lake, 2014; Hummel, 2001; Socher et al., 2011; van der Velde and de Kamps, 2006). In human languages, the recent years have seen extensive interests on distributional approaches. The research includes the influential pioneering work that examined a number of explicit forms of compositional functions (Mitchell and Lapata, 2008). More recent works explored neural networks, e.g., (Socher et al., 2013; Irsoy and Cardie, 2014; Kalchbrenner et al., 2014; Tai et al., 2015; Le and Zuidema, 2015; Zhu et al., 2015c) among many others, which extended the success of word-level embeddings (Collobert et al., 2011; Mikolov et al., 2013; Chen et al., 2015) and modeled sentences through semantic composition. In general, neural models can fit very complicated functions and can be a universal approximator (Cybenko, 1989; Hornik, 1991). In obtaining the distributed representation for longer spans of text from its subsequences, previous neural models assume full compositionality from the atomic components and disregard noncompositionality and in general pr"
N16-1106,S15-1002,0,0.0805935,"g the compositional strength of LSTM with external semantic knowledge. 2 Related Work Linear and Structured RNN Linear-chain RNN, particularly LSTM, has been applied to a wide range of problems as in (Graves et al., 2013; Sutskever et al., 2014; Cho et al., 2014; Vinyals et al., 2014), among many others. While the models take a linear encoding process to absorb input symbols, they are capable of implicitly capturing rather complicated structures embedded in the input sequences. Recent research has also moved beyond linearchain LSTM. For example, in (Tai et al., 2015; Zhu et al., 2015b; Le and Zuidema, 2015), LSTM was extended to tree structures. The results show that tree-structured LSTM achieves the-state-of-the-art performance on semantic tasks such as paraphrasing 918 detection and sentiment analysis, due to its abilities in capturing both local and long-distance interplay over the structures. In this work, we proposed DAG-structured LSTM for modeling sequences of text. Unlike the treestructured LSTM, where the structures are used for considering syntax, the proposed models leverage DAG structures to incorporate external semantics including non-compositional or holistically learned semantics."
N16-1106,P08-1028,0,0.0249331,"STM, where the structures are used for considering syntax, the proposed models leverage DAG structures to incorporate external semantics including non-compositional or holistically learned semantics. Compositionality Semantic composition exists in multiple modalities, including images and vision (Lake, 2014; Hummel, 2001; Socher et al., 2011; van der Velde and de Kamps, 2006). In human languages, the recent years have seen extensive interests on distributional approaches. The research includes the influential pioneering work that examined a number of explicit forms of compositional functions (Mitchell and Lapata, 2008). More recent works explored neural networks, e.g., (Socher et al., 2013; Irsoy and Cardie, 2014; Kalchbrenner et al., 2014; Tai et al., 2015; Le and Zuidema, 2015; Zhu et al., 2015c) among many others, which extended the success of word-level embeddings (Collobert et al., 2011; Mikolov et al., 2013; Chen et al., 2015) and modeled sentences through semantic composition. In general, neural models can fit very complicated functions and can be a universal approximator (Cybenko, 1989; Hornik, 1991). In obtaining the distributed representation for longer spans of text from its subsequences, previou"
N16-1106,S13-2053,1,0.934402,"ctors ct are passed along multiple outgoing paths to the future blocks. ⊗ denotes a Hadamard product, and the ”s” shape sign is a squashing function (in this paper the tanh function). 4 Experiment Set-Up In this paper, we study the proposed models on a semantic composition task that determine the sentiment of a piece of text. We use social-media messages from the official SemEval Sentiment Analysis in Twitter competition. Analyzing social-media text has attracted extensive attention (Nakov et al., 2016; Kiritchenko et al., 2014; Mohammad et al., 2014; Mohammad et al., 2015; Zhu et al., 2014b; Mohammad et al., 2013a) and have many applications. Sen921 Data and Evaluation Metric In our experiments, we use the official data from the SemEval-2013 (Wilson et al., 2013) and SemEval2014 (Rosenthal et al., 2014) Sentiment Analysis in Twitter challenges. The task attempts to determine the sentiment category of a tweet; that is, detecting whether an entire tweet message conveys a positive, negative, or neutral sentiment. To give a rough idea about the data, the SemEval2013 tweets were collected through the public streaming Twitter API during a period of one year: between January 2012 and January 2013. The datase"
N16-1106,W14-2607,1,0.356917,"le of a fork memory block. Both the hidden vectors ht and cell vectors ct are passed along multiple outgoing paths to the future blocks. ⊗ denotes a Hadamard product, and the ”s” shape sign is a squashing function (in this paper the tanh function). 4 Experiment Set-Up In this paper, we study the proposed models on a semantic composition task that determine the sentiment of a piece of text. We use social-media messages from the official SemEval Sentiment Analysis in Twitter competition. Analyzing social-media text has attracted extensive attention (Nakov et al., 2016; Kiritchenko et al., 2014; Mohammad et al., 2014; Mohammad et al., 2015; Zhu et al., 2014b; Mohammad et al., 2013a) and have many applications. Sen921 Data and Evaluation Metric In our experiments, we use the official data from the SemEval-2013 (Wilson et al., 2013) and SemEval2014 (Rosenthal et al., 2014) Sentiment Analysis in Twitter challenges. The task attempts to determine the sentiment category of a tweet; that is, detecting whether an entire tweet message conveys a positive, negative, or neutral sentiment. To give a rough idea about the data, the SemEval2013 tweets were collected through the public streaming Twitter API during a peri"
N16-1106,D12-1110,0,0.155858,"en not linguistically well-formed. Syntactic analysis such as parsing is much less reliable in such data than in news articles, and sequential models without depending on deep linguistic analysis (e.g., parsing) are adopted by most previous work. In obtaining the sentiment of a text span, e.g., a sentence, early work often factorized the problem to consider smaller pieces of component words or phrases with bag-of-words or bag-of-phrases models (Liu and Zhang, 2012; Pang and Lee, 2008). More recent work has started to model composition process (Choi and Cardie, 2008; Moilanen and Pulman, 2007; Socher et al., 2012; Socher et al., 2013; Irsoy and Cardie, 2014; Kalchbrenner et al., 2014; Tai et al., 2015; Zhu et al., 2015b; Le and Zuidema, 2015), more closely. In general, the composition process is critical in the formation of the sentiment of a text span, which has not been well modeled yet and more work would be desirable. 4.1 Figure 2: An example of a fork memory block. Both the hidden vectors ht and cell vectors ct are passed along multiple outgoing paths to the future blocks. ⊗ denotes a Hadamard product, and the ”s” shape sign is a squashing function (in this paper the tanh function). 4 Experiment"
N16-1106,D13-1170,0,0.229186,"leverage DAG structures to incorporate external semantics including non-compositional or holistically learned semantics. Compositionality Semantic composition exists in multiple modalities, including images and vision (Lake, 2014; Hummel, 2001; Socher et al., 2011; van der Velde and de Kamps, 2006). In human languages, the recent years have seen extensive interests on distributional approaches. The research includes the influential pioneering work that examined a number of explicit forms of compositional functions (Mitchell and Lapata, 2008). More recent works explored neural networks, e.g., (Socher et al., 2013; Irsoy and Cardie, 2014; Kalchbrenner et al., 2014; Tai et al., 2015; Le and Zuidema, 2015; Zhu et al., 2015c) among many others, which extended the success of word-level embeddings (Collobert et al., 2011; Mikolov et al., 2013; Chen et al., 2015) and modeled sentences through semantic composition. In general, neural models can fit very complicated functions and can be a universal approximator (Cybenko, 1989; Hornik, 1991). In obtaining the distributed representation for longer spans of text from its subsequences, previous neural models assume full compositionality from the atomic components"
N16-1106,P15-1150,0,0.119765,"Missing"
N16-1106,P14-1146,0,0.0109081,"2 We use in our experiments the unigrams, bigrams and trigrams learned from the dataset with the occurrences higher than 5. We assign these ngrams into one of the 5 bins according to their sentiment scores obtained with Formula 6: (−∞, −2], (−2, −1], (−1, 1), [1, 2), and [2, +∞). Each ngram is now given a one-hot vector, indicating the polarity and strength of its sentiment. For example, a bigram with a score of -1.5 will be assigned a 5dimensional vector [0, 1, 0, 0, 0], indicating a weak negative. Note that we can also take into other forms of sentiment embeddings, such as those learned in (Tang et al., 2014). Manually Encoded Semantics In addition, we also leveraged prior knowledge from human, i.e., manually encoded semantics, for the task here. This includes a widely used sentiment lexicon, the MPQA Subjectivity Lexicon (Wilson et al., 2005), which encodes the prior knowledge that the human annotators have about the sentiment of words. The MPQA, which draws from the General Inquirer and other sources, has sentiment labels for about 8,000 words. The contained words marked with their prior polarity (positive or negative) and a discrete strength of evaluative intensity (strong or weak). We convert"
N16-1106,H05-1044,0,0.0325568,"2], (−2, −1], (−1, 1), [1, 2), and [2, +∞). Each ngram is now given a one-hot vector, indicating the polarity and strength of its sentiment. For example, a bigram with a score of -1.5 will be assigned a 5dimensional vector [0, 1, 0, 0, 0], indicating a weak negative. Note that we can also take into other forms of sentiment embeddings, such as those learned in (Tang et al., 2014). Manually Encoded Semantics In addition, we also leveraged prior knowledge from human, i.e., manually encoded semantics, for the task here. This includes a widely used sentiment lexicon, the MPQA Subjectivity Lexicon (Wilson et al., 2005), which encodes the prior knowledge that the human annotators have about the sentiment of words. The MPQA, which draws from the General Inquirer and other sources, has sentiment labels for about 8,000 words. The contained words marked with their prior polarity (positive or negative) and a discrete strength of evaluative intensity (strong or weak). We convert them to value -1.0, -0.5, 0, 0.5, 1, corresponding to strong negative, weak negative, neutral, weak positive, strong positive, respectively. 4.3 Training Details Our networks aim to minimize the cross-entropy error (Socher et al., 2013). T"
N16-1106,S13-2052,0,0.0270282,"Missing"
N16-1106,P14-1029,1,0.918334,"as c-path) and non-compositional path (nc-path), to incorporate different knowledge sources. For example, the c-path in the figure connects nodes 3, 4, 6, 7, and 9, which model the regular sequential compositional procedure. The two nc-paths explore noncompositional knowledge. The path 4-5-9 considers the composition vector accumulated at node 4 so far with the non-compositional knowledge of the phrase must try. Similarly, the path 3-8-9 considers holistic representation for the negated phrase not must try. Note that negation by itself has shown to be a rather complicated non-linear function (Zhu et al., 2014a), if being modeled only compositionally. The model here provides the flexibility to consider both compositional and non-compositional representations. All knowledge from these three paths are then merged, to obtain the comprehensive representation so far, at node 9. Later in the experiment section, we will discuss how to obtain prior non-compositional knowledge, from both human heuristics/annotation and from automatically learned resources. 3.2 Compositional and Non-compositional Memory Blocks The conventional components of DAG-LSTM in Figure 1 are nodes 0, 1, 2, 6, and 7, which implement li"
N16-1106,S14-2077,1,0.837611,"as c-path) and non-compositional path (nc-path), to incorporate different knowledge sources. For example, the c-path in the figure connects nodes 3, 4, 6, 7, and 9, which model the regular sequential compositional procedure. The two nc-paths explore noncompositional knowledge. The path 4-5-9 considers the composition vector accumulated at node 4 so far with the non-compositional knowledge of the phrase must try. Similarly, the path 3-8-9 considers holistic representation for the negated phrase not must try. Note that negation by itself has shown to be a rather complicated non-linear function (Zhu et al., 2014a), if being modeled only compositionally. The model here provides the flexibility to consider both compositional and non-compositional representations. All knowledge from these three paths are then merged, to obtain the comprehensive representation so far, at node 9. Later in the experiment section, we will discuss how to obtain prior non-compositional knowledge, from both human heuristics/annotation and from automatically learned resources. 3.2 Compositional and Non-compositional Memory Blocks The conventional components of DAG-LSTM in Figure 1 are nodes 0, 1, 2, 6, and 7, which implement li"
N16-1106,S15-1001,1,0.872289,"re engineering, by unifying the compositional strength of LSTM with external semantic knowledge. 2 Related Work Linear and Structured RNN Linear-chain RNN, particularly LSTM, has been applied to a wide range of problems as in (Graves et al., 2013; Sutskever et al., 2014; Cho et al., 2014; Vinyals et al., 2014), among many others. While the models take a linear encoding process to absorb input symbols, they are capable of implicitly capturing rather complicated structures embedded in the input sequences. Recent research has also moved beyond linearchain LSTM. For example, in (Tai et al., 2015; Zhu et al., 2015b; Le and Zuidema, 2015), LSTM was extended to tree structures. The results show that tree-structured LSTM achieves the-state-of-the-art performance on semantic tasks such as paraphrasing 918 detection and sentiment analysis, due to its abilities in capturing both local and long-distance interplay over the structures. In this work, we proposed DAG-structured LSTM for modeling sequences of text. Unlike the treestructured LSTM, where the structures are used for considering syntax, the proposed models leverage DAG structures to incorporate external semantics including non-compositional or holisti"
N16-1106,J13-3004,0,\N,Missing
N16-1106,S14-2009,0,\N,Missing
N19-5002,D15-1075,1,0.734119,"et al., 2018). Finally we will summarize the tutorial and flesh out some discussions on future directions. The task of natural language inference (NLI; also known as recognizing textual entailment, or RTE) asks a system to evaluate the relationships between the truth-conditional meanings of two sentences or, in other words, decide whether one sentence follows from another. This task neatly isolates the core NLP problem of sentence understanding as a classification problem, and also offers promise as an intermediate step in the building of complex systems (Dagan et al., 2005; MacCartney, 2009; Bowman et al., 2015). The last few years have seen fast progress in NLI, with the introduction of a few large training datasets and many popular evaluation sets as well as an explosion of new model architectures and methods for using unlabeled data and outside knowledge. This tutorial will layout the motivations for work on NLI, survey the available resources for the task, and present highlights from recent research showing us what NLI can teach us about the capabilities and limits of deep learning models for language understanding and reasoning. The tutorial will start from a brief discussion on the motivations"
N19-5002,C18-1154,1,0.852207,"that utilize crosssentence statistics (Bowman et al., 2015; Chen et al., 2017a, 2018b; Radford et al., 2018; Devlin et al., 2018; Peters et al., 2018). We will cover typical deep-learning architectures in both paradigms. Based on this we will deepen our discussion from several perspectives. We first describe models that can further consider linguistic structures in the deep-learning NLI architectures (Chen et al., 2017a). We then advance to discuss models that utilize external knowledge, which include two typical types of approaches: those explicitly incorporating human-authorized knowledge (Chen et al., 2018b) and those based on unsupervised pretraining (Radford et al., 2018; Devlin et al., 2018; Peters et al., 2018). We will present how NLI models are sensitive or robust to different newly proposed tests (Glockner et al., 2018; Wang et al., 2018; Naik et al., 2018; Poliak et al., 2018). The tutorial will also cover the recent modeling on crosslingual NLI (Conneau et al., 2018). Finally we will summarize the tutorial and flesh out some discussions on future directions. The task of natural language inference (NLI; also known as recognizing textual entailment, or RTE) asks a system to evaluate the"
N19-5002,P18-1224,1,0.831291,"that utilize crosssentence statistics (Bowman et al., 2015; Chen et al., 2017a, 2018b; Radford et al., 2018; Devlin et al., 2018; Peters et al., 2018). We will cover typical deep-learning architectures in both paradigms. Based on this we will deepen our discussion from several perspectives. We first describe models that can further consider linguistic structures in the deep-learning NLI architectures (Chen et al., 2017a). We then advance to discuss models that utilize external knowledge, which include two typical types of approaches: those explicitly incorporating human-authorized knowledge (Chen et al., 2018b) and those based on unsupervised pretraining (Radford et al., 2018; Devlin et al., 2018; Peters et al., 2018). We will present how NLI models are sensitive or robust to different newly proposed tests (Glockner et al., 2018; Wang et al., 2018; Naik et al., 2018; Poliak et al., 2018). The tutorial will also cover the recent modeling on crosslingual NLI (Conneau et al., 2018). Finally we will summarize the tutorial and flesh out some discussions on future directions. The task of natural language inference (NLI; also known as recognizing textual entailment, or RTE) asks a system to evaluate the"
N19-5002,P17-1152,1,0.613604,"sentence-embeddingbased modeling (Bowman et al., 2015; Chen et al., 2017b, 2018a; Williams et al., 2018; Yoon et al., 2018; Kiela et al., 2018; Talman et al., 2018) and deep-learning approaches that utilize crosssentence statistics (Bowman et al., 2015; Chen et al., 2017a, 2018b; Radford et al., 2018; Devlin et al., 2018; Peters et al., 2018). We will cover typical deep-learning architectures in both paradigms. Based on this we will deepen our discussion from several perspectives. We first describe models that can further consider linguistic structures in the deep-learning NLI architectures (Chen et al., 2017a). We then advance to discuss models that utilize external knowledge, which include two typical types of approaches: those explicitly incorporating human-authorized knowledge (Chen et al., 2018b) and those based on unsupervised pretraining (Radford et al., 2018; Devlin et al., 2018; Peters et al., 2018). We will present how NLI models are sensitive or robust to different newly proposed tests (Glockner et al., 2018; Wang et al., 2018; Naik et al., 2018; Poliak et al., 2018). The tutorial will also cover the recent modeling on crosslingual NLI (Conneau et al., 2018). Finally we will summarize t"
N19-5002,D18-1176,0,0.0229797,"Missing"
N19-5002,C18-1198,0,0.0252854,"ion from several perspectives. We first describe models that can further consider linguistic structures in the deep-learning NLI architectures (Chen et al., 2017a). We then advance to discuss models that utilize external knowledge, which include two typical types of approaches: those explicitly incorporating human-authorized knowledge (Chen et al., 2018b) and those based on unsupervised pretraining (Radford et al., 2018; Devlin et al., 2018; Peters et al., 2018). We will present how NLI models are sensitive or robust to different newly proposed tests (Glockner et al., 2018; Wang et al., 2018; Naik et al., 2018; Poliak et al., 2018). The tutorial will also cover the recent modeling on crosslingual NLI (Conneau et al., 2018). Finally we will summarize the tutorial and flesh out some discussions on future directions. The task of natural language inference (NLI; also known as recognizing textual entailment, or RTE) asks a system to evaluate the relationships between the truth-conditional meanings of two sentences or, in other words, decide whether one sentence follows from another. This task neatly isolates the core NLP problem of sentence understanding as a classification problem, and also offers prom"
N19-5002,N18-1202,0,0.0446999,"; Devlin et al., 2018; Peters et al., 2018). We will cover typical deep-learning architectures in both paradigms. Based on this we will deepen our discussion from several perspectives. We first describe models that can further consider linguistic structures in the deep-learning NLI architectures (Chen et al., 2017a). We then advance to discuss models that utilize external knowledge, which include two typical types of approaches: those explicitly incorporating human-authorized knowledge (Chen et al., 2018b) and those based on unsupervised pretraining (Radford et al., 2018; Devlin et al., 2018; Peters et al., 2018). We will present how NLI models are sensitive or robust to different newly proposed tests (Glockner et al., 2018; Wang et al., 2018; Naik et al., 2018; Poliak et al., 2018). The tutorial will also cover the recent modeling on crosslingual NLI (Conneau et al., 2018). Finally we will summarize the tutorial and flesh out some discussions on future directions. The task of natural language inference (NLI; also known as recognizing textual entailment, or RTE) asks a system to evaluate the relationships between the truth-conditional meanings of two sentences or, in other words, decide whether one se"
N19-5002,S18-2023,0,0.0586066,"Missing"
N19-5002,N18-1101,1,0.81029,"wledge. This tutorial will layout the motivations for work on NLI, survey the available resources for the task, and present highlights from recent research showing us what NLI can teach us about the capabilities and limits of deep learning models for language understanding and reasoning. The tutorial will start from a brief discussion on the motivations for NLI, problem definitions, and typical conventional approaches (Dagan et al., 2013; MacCartney, 2009; Iftene and Balahur-Dobrescu, 2007). Critical to the recent advance on NLI, the creation of larger annotated datasets (Bowman et al., 2015; Williams et al., 2018; Conneau et al., 2018) has made it feasible to train complex models that need to estimate a large number of parameters. The tutorial will present detailed discussion on the available datasets as well as the motivations for and insights from developing these datasets. Then based on more recent research on annotation artifacts, we will extend the discussion to what we should or shouldn’t take away from the current datasets. We will then focus on the cutting-edge deep learning models for NLI. We start from two basic 2 Tutorial Outline • Introduction • Background ◦ Problem definition ◦ Motivation"
P08-1054,A00-2025,0,\N,Missing
P08-1054,W06-1643,0,\N,Missing
P08-1054,N06-1047,0,\N,Missing
P08-1054,J95-4004,0,\N,Missing
P09-1062,P99-1071,0,0.0195776,"documents by using reoccurrence statistics of acoustic patterns. 2.2 3 An acoustics-based approach Multiple-document summarization Multi-document summarization on written text has been studied for over a decade. Compared with the single-document task, it needs to remove more content, cope with prominent redundancy, and organize content from different sources properly. This field has been pioneered by early work such as the SUMMONS architecture (Mckeown and Radev, 1995; Radev and McKeown, 1998). Several well-known models have been proposed, i.e., MMR (Carbonell and Goldstein, 1998), multiGen (Barzilay et al., 1999), and MEAD (Radev et al., 2004). Multi-document summarization has received intensive study at DUC. 1 Unfortunately, no such efforts have been extended to summarize multiple spoken documents yet. Abstractive approaches have been studied since the beginning. A famous effort in this direction is the information fusion approach proposed in Barzilay et al. (1999). However, for error-prone transcripts of spoken documents, an abstractive method still seems to be too ambitious for the time being. As in single-spoken-document summarization, this paper focuses on the extractive approach. Among the extra"
P09-1062,P08-1054,1,0.852692,"their scoring to discourage this possibility. Our approach uses acoustic evidence from the untranscribed audio stream. Consider text summarization first: many well-known models such as MMR (Carbonell and Goldstein, 1998) and MEAD (Radev et al., 2004) rely on the reoccurrence statistics of words. That is, if we switch any word w1 with another word w2 across an entire corpus, the ranking of extracts (often sentences) will be unaffected, because no wordspecific knowledge is involved. These models have achieved state-of-the-art performance in transcript-based speech summarization (Zechner, 2001; Penn and Zhu, 2008). For spoken documents, such reoccurrence statistics are available directly from the speech signal. In recent years, a variant of dynamic time warping (DTW) has been proposed to find reoccurring patterns in the speech signal (Park and Glass, 2008). This method has been successfully applied to tasks such as word detection (Park and Glass, 2006) and topic boundary detection (Malioutov et al., 2007). 2 Related work 2.1 Speech summarization Although abstractive summarization is more desirable, the state-of-the-art research on speech summarization has been less ambitious, focusing primarily on extr"
P09-1062,W04-1013,0,0.0453026,"Missing"
P09-1062,J98-3005,0,0.0257777,"ork above has been conducted on single-document summarization. In this paper we are interested in summarizing multiple spoken documents by using reoccurrence statistics of acoustic patterns. 2.2 3 An acoustics-based approach Multiple-document summarization Multi-document summarization on written text has been studied for over a decade. Compared with the single-document task, it needs to remove more content, cope with prominent redundancy, and organize content from different sources properly. This field has been pioneered by early work such as the SUMMONS architecture (Mckeown and Radev, 1995; Radev and McKeown, 1998). Several well-known models have been proposed, i.e., MMR (Carbonell and Goldstein, 1998), multiGen (Barzilay et al., 1999), and MEAD (Radev et al., 2004). Multi-document summarization has received intensive study at DUC. 1 Unfortunately, no such efforts have been extended to summarize multiple spoken documents yet. Abstractive approaches have been studied since the beginning. A famous effort in this direction is the information fusion approach proposed in Barzilay et al. (1999). However, for error-prone transcripts of spoken documents, an abstractive method still seems to be too ambitious for"
P09-1062,P07-1064,0,0.235485,"entify subsequences within acoustic sequences that appear highly similar to regions within other sequences, where each sequence consists of a progression of overlapping 20ms vectors (frames). In order to find those shared patterns, we apply a modification of the segmental dynamic time warping (SDTW) algorithm to pairs of audio sequences. This method is similar to standard DTW, except that it computes multiple constrained alignments, each within predetermined bands of the similarity matrix (Park and Glass, 2008).2 SDTW has been successfully applied to problems such as topic boundary detection (Malioutov et al., 2007) and word detection (Park and Glass, 2006). An example application of SDTW is shown in Figure 1, which shows the results of two utterances from the TDT-4 English dataset: 2 Park and Glass (2008) used Euclidean distance. We used cosine distance instead, which was found to be better on our held-out dataset. I: II: the explosion in aden harbor killed seventeen u.s. sailors and injured other thirty nine last month. seventeen sailors were killed. between frames are then estimated using cosine distance. All similarity scores are then normalized to the range of [0, 1], which yields similarity matrice"
P09-1086,H92-1022,0,0.349724,"without the obligatory round of development-set parameter tuning required by their heuristics, and in a manner that is robust to perplexity. Less is more. Section 2 briefly introduces TransformationBased Learning (TBL), a method used in various Natural Language Processing tasks to correct the output of a stochastic model, and then introduces a TBL-based solution for improving ASR transcripts for lectures. Section 3 describes our experimental setup, and Section 4 analyses its results. 2 Transformation-Based Learning Brill’s tagger introduced the concept of Transformation-Based Learning (TBL) (Brill, 1992). The fundamental principle of TBL is to employ a set of rules to correct the output of a stochastic model. In contrast to traditional rule-based approaches where rules are manually developed, TBL rules are automatically learned from training data. The training data consist of sample output from the stochastic model, aligned with the correct instances. For example, in Brill’s tagger, the system assigns POSs to words in a text, which are later corrected by TBL rules. These rules are learned from manually-tagged sentences that are aligned with the same sentences tagged by the system. Typically,"
P09-1086,W06-1644,0,0.0705315,"Missing"
P09-1086,N01-1006,0,0.0349595,"ing utterances in the manual and ASR transcripts of training data, and then extracting the mismatched word sequences, anchored by matching words. The matching words serve as contexts for the rules’ application. The rule discovery algorithm is outlined in Figure 2; it is applied to every mismatching word sequence between the utterance-aligned manual and ASR transcripts. For every mismatching sequence of words, a set word-level transformations that correct n-gram sequences. A typical challenge for TBL is the heavy computational requirements of the rule scoring function (Roche and Schabes, 1995; Ngai and Florian, 2001). This is no less true in largevocabulary ASR correction, where large training corpora are often needed to learn good rules over a much larger space (larger than POS tagging, for example). The training and development sets are typically up to five times larger than the evaluation test set, and all three sets must be sampled from the same cohesive corpus. While the objective function for improving the ASR transcript is WER reduction, the use of this for scoring TBL rules can be computationally pro766 Utterance-align ASR output and correct transcripts: ⋄ for every sequence of words c0 w1 . . . w"
P09-1086,J95-2004,0,0.0853009,"gnment between corresponding utterances in the manual and ASR transcripts of training data, and then extracting the mismatched word sequences, anchored by matching words. The matching words serve as contexts for the rules’ application. The rule discovery algorithm is outlined in Figure 2; it is applied to every mismatching word sequence between the utterance-aligned manual and ASR transcripts. For every mismatching sequence of words, a set word-level transformations that correct n-gram sequences. A typical challenge for TBL is the heavy computational requirements of the rule scoring function (Roche and Schabes, 1995; Ngai and Florian, 2001). This is no less true in largevocabulary ASR correction, where large training corpora are often needed to learn good rules over a much larger space (larger than POS tagging, for example). The training and development sets are typically up to five times larger than the evaluation test set, and all three sets must be sampled from the same cohesive corpus. While the objective function for improving the ASR transcript is WER reduction, the use of this for scoring TBL rules can be computationally pro766 Utterance-align ASR output and correct transcripts: ⋄ for every sequen"
P14-1029,W12-3802,0,0.0345519,"and Mohammad et al. (2013). Negation modeling for sentiment An early yet influential reversing assumption conjectures that a negator reverses the sign of the sentiment value of the modified text (Polanyi and Zaenen, 2004; Kennedy and Inkpen, 2006), e.g., from +0.5 to 0.5, or vice versa. A different hypothesis, called the shifting hypothesis in this paper, assumes that negators change the sentiment values by a constant amount (Taboada et al., 2011; Liu and Seneff, 2009). Other approaches to negation modeling have been discussed in (Jia et al., 2009; Wiegand et al., 2010; Lapponi et al., 2012; Benamara et al., 2012). In the process of semantic composition, the effect of negators could depend on the syntax and semantics of the text spans they modify. The approaches of modeling this include bag-of-wordbased models. For example, in the work of (Kennedy and Inkpen, 2006), a feature not good will be created if the word good is encountered We can see that the reversing assumption (the red diagonal line) does capture some regularity of human perception, but rather roughly. Moreover, the figure shows that same or similar s(w) ~ scores (x-axis) can correspond to very different s(wn , w) ~ scores (y-axis), which,"
P14-1029,D08-1083,0,0.362631,"positive value corresponds to a negative or a positive sentiment respectively; zero means neutral. The negator list will be discussed later in the paper. 2 Similar distribution is observed in other data such as Tweets (Kiritchenko et al., 2014). 305 a negator simply reverses the sentiment score s(w) ~ to be −s(w); ~ i.e., f (s(w)) ~ = −s(w). ~ within a predefined range after a negator. There exist different ways of incorporating more complicated syntactic and semantic information. Much recent work considers sentiment analysis from a semantic-composition perspective (Moilanen and Pulman, 2007; Choi and Cardie, 2008; Socher et al., 2012; Socher et al., 2013), which achieved the state-of-the-art performance. Moilanen and Pulman (2007) used a collection of hand-written compositional rules to assign sentiment values to different granularities of text spans. Choi and Cardie (2008) proposed a learning-based framework. The more recent work of (Socher et al., 2012; Socher et al., 2013) proposed models based on recursive neural networks that do not rely on any heuristic rules. Such models work in a bottom-up fashion over the parse tree of a sentence to infer the sentiment label of the sentence as a composition o"
P14-1029,P97-1023,0,0.183368,"ional Linguistics on the Stanford Sentiment Treebank.1 Each dot in the figure corresponds to a negated phrase in the treebank. The x-axis is the sentiment score of its argument s(w) ~ and y-axis the sentiment score of the entire negated phrase s(wn , w). ~ 2 Related work Automatic sentiment analysis The expression of sentiment is an integral component of human language. In written text, sentiment is conveyed with word senses and their composition, and in speech also via prosody such as pitch (Mairesse et al., 2012). Early work on automatic sentiment analysis includes the widely cited work of (Hatzivassiloglou and McKeown, 1997; Pang et al., 2002; Turney, 2002), among others. Since then, there has been an explosion of research addressing various aspects of the problem, including detecting subjectivity, rating and classifying sentiment, labeling sentiment-related semantic roles (e.g., target of sentiment), and visualizing sentiment (see surveys by Pang and Lee (2008) and Liu and Zhang (2012)). Negation modeling Negation is a general grammatical category pertaining to the changing of the truth values of propositions; negation modeling is not limited to sentiment. For example, paraphrase and contradiction detection sys"
P14-1029,D09-1017,0,0.0334571,"dead, however, not tall does not always mean short. Some automatic methods to detect opposites were proposed by Hatzivassiloglou and McKeown (1997) and Mohammad et al. (2013). Negation modeling for sentiment An early yet influential reversing assumption conjectures that a negator reverses the sign of the sentiment value of the modified text (Polanyi and Zaenen, 2004; Kennedy and Inkpen, 2006), e.g., from +0.5 to 0.5, or vice versa. A different hypothesis, called the shifting hypothesis in this paper, assumes that negators change the sentiment values by a constant amount (Taboada et al., 2011; Liu and Seneff, 2009). Other approaches to negation modeling have been discussed in (Jia et al., 2009; Wiegand et al., 2010; Lapponi et al., 2012; Benamara et al., 2012). In the process of semantic composition, the effect of negators could depend on the syntax and semantics of the text spans they modify. The approaches of modeling this include bag-of-wordbased models. For example, in the work of (Kennedy and Inkpen, 2006), a feature not good will be created if the word good is encountered We can see that the reversing assumption (the red diagonal line) does capture some regularity of human perception, but rather r"
P14-1029,J13-3004,1,0.0615959,"Negation is a general grammatical category pertaining to the changing of the truth values of propositions; negation modeling is not limited to sentiment. For example, paraphrase and contradiction detection systems rely on detecting negated expressions and opposites (Harabagiu et al., 2006). In general, a negated expression and the opposite of the expression may or may not convey the same meaning. For example, not alive has the same meaning as dead, however, not tall does not always mean short. Some automatic methods to detect opposites were proposed by Hatzivassiloglou and McKeown (1997) and Mohammad et al. (2013). Negation modeling for sentiment An early yet influential reversing assumption conjectures that a negator reverses the sign of the sentiment value of the modified text (Polanyi and Zaenen, 2004; Kennedy and Inkpen, 2006), e.g., from +0.5 to 0.5, or vice versa. A different hypothesis, called the shifting hypothesis in this paper, assumes that negators change the sentiment values by a constant amount (Taboada et al., 2011; Liu and Seneff, 2009). Other approaches to negation modeling have been discussed in (Jia et al., 2009; Wiegand et al., 2010; Lapponi et al., 2012; Benamara et al., 2012). In"
P14-1029,J12-2001,0,0.0468002,"dified by and composed with a negator as the argument (e.g., very good), and entire phrase (e.g., isn’t very good) as the negated phrase. The recently available Stanford Sentiment Treebank (Socher et al., 2013) renders manually annotated, real-valued sentiment scores for all phrases in parse trees. This corpus provides us with the data to further understand the quantitative behavior of negators, as the effect of negators can now be studied with arguments of rich syntactic and semantic variety. Figure 1 illustrates the effect of a common list of negators on sentiment as observed 1 Introduction Morante and Sporleder (2012) define negation to be “a grammatical category that allows the changing of the truth value of a proposition”. Negation is often expressed through the use of negative signals or negators–words like isn’t and never, and it can significantly affect the sentiment of its scope. Understanding the impact of negation on sentiment is essential in automatic analysis of sentiment. The literature contains interesting research attempting to model and understand the behavior (reviewed in Section 2). For example, 304 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pag"
P14-1029,P05-1015,0,0.0210681,"ly δp2 ,down [1 : d] and δp2 ,down [d + 1 : 2d], respectively. Following this notation, we have the error message for the two children of p2 , provided that we have the δp2 ,down : Data As described earlier, the Stanford Sentiment Treebank (Socher et al., 2013) has manually annotated, real-valued sentiment values for all phrases in parse trees. This provides us with the training and evaluation data to study the effect of negators with syntax and semantics of different complexity in a natural setting. The data contain around 11,800 sentences from movie reviews that were originally collected by Pang and Lee (2005). The sentences were parsed with the Stanford parser (Klein and Manning, 2003). The phrases at all tree nodes were manually annotated with one of 25 sentiment values that uniformly span between the positive and negative poles. The values are normalized to the range of [0, 1]. In this paper, we use a list of most frequent negators that include the words not, no, never, and their combinations with auxiliaries (e.g., didn’t). We search these negators in the Stanford Sentiment Treebank and normalize the same negators to a single form; e.g., “is n’t”, “isn’t”, and “is not” are all normalized to “is"
P14-1029,W02-1011,0,0.0270894,"ntiment Treebank.1 Each dot in the figure corresponds to a negated phrase in the treebank. The x-axis is the sentiment score of its argument s(w) ~ and y-axis the sentiment score of the entire negated phrase s(wn , w). ~ 2 Related work Automatic sentiment analysis The expression of sentiment is an integral component of human language. In written text, sentiment is conveyed with word senses and their composition, and in speech also via prosody such as pitch (Mairesse et al., 2012). Early work on automatic sentiment analysis includes the widely cited work of (Hatzivassiloglou and McKeown, 1997; Pang et al., 2002; Turney, 2002), among others. Since then, there has been an explosion of research addressing various aspects of the problem, including detecting subjectivity, rating and classifying sentiment, labeling sentiment-related semantic roles (e.g., target of sentiment), and visualizing sentiment (see surveys by Pang and Lee (2008) and Liu and Zhang (2012)). Negation modeling Negation is a general grammatical category pertaining to the changing of the truth values of propositions; negation modeling is not limited to sentiment. For example, paraphrase and contradiction detection systems rely on detect"
P14-1029,D12-1110,0,0.11819,"nds to a negative or a positive sentiment respectively; zero means neutral. The negator list will be discussed later in the paper. 2 Similar distribution is observed in other data such as Tweets (Kiritchenko et al., 2014). 305 a negator simply reverses the sentiment score s(w) ~ to be −s(w); ~ i.e., f (s(w)) ~ = −s(w). ~ within a predefined range after a negator. There exist different ways of incorporating more complicated syntactic and semantic information. Much recent work considers sentiment analysis from a semantic-composition perspective (Moilanen and Pulman, 2007; Choi and Cardie, 2008; Socher et al., 2012; Socher et al., 2013), which achieved the state-of-the-art performance. Moilanen and Pulman (2007) used a collection of hand-written compositional rules to assign sentiment values to different granularities of text spans. Choi and Cardie (2008) proposed a learning-based framework. The more recent work of (Socher et al., 2012; Socher et al., 2013) proposed models based on recursive neural networks that do not rely on any heuristic rules. Such models work in a bottom-up fashion over the parse tree of a sentence to infer the sentiment label of the sentence as a composition of the sentiment expre"
P14-1029,D13-1170,0,0.206229,"mental role in modifying sentiment of textual expressions. We will refer to a negation word as the negator and the text span within the scope of the negator as the argument. Commonly used heuristics to estimate the sentiment of negated expressions rely simply on the sentiment of argument (and not on the negator or the argument itself). We use a sentiment treebank to show that these existing heuristics are poor estimators of sentiment. We then modify these heuristics to be dependent on the negators and show that this improves prediction. Next, we evaluate a recently proposed composition model (Socher et al., 2013) that relies on both the negator and the argument. This model learns the syntax and semantics of the negator’s argument with a recursive neural network. We show that this approach performs better than those mentioned above. In addition, we explicitly incorporate the prior sentiment of the argument and observe that this information can help reduce fitting errors. Figure 1: Effect of a list of common negators in modifying sentiment values in Stanford Sentiment Treebank. The x-axis is s(w), ~ and y-axis is s(wn , w). ~ Each dot in the figure corresponds to a text span being modified by (composed"
P14-1029,J11-2001,0,0.606524,"g errors. Figure 1: Effect of a list of common negators in modifying sentiment values in Stanford Sentiment Treebank. The x-axis is s(w), ~ and y-axis is s(wn , w). ~ Each dot in the figure corresponds to a text span being modified by (composed with) a negator in the treebank. The red diagonal line corresponds to the sentiment-reversing hypothesis that simply reverses the sign of sentiment values. a simple yet influential hypothesis posits that a negator reverses the sign of the sentiment value of the modified text (Polanyi and Zaenen, 2004; Kennedy and Inkpen, 2006). The shifting hypothesis (Taboada et al., 2011), however, assumes that negators change sentiment values by a constant amount. In this paper, we refer to a negation word as the negator (e.g., isn’t), a text span being modified by and composed with a negator as the argument (e.g., very good), and entire phrase (e.g., isn’t very good) as the negated phrase. The recently available Stanford Sentiment Treebank (Socher et al., 2013) renders manually annotated, real-valued sentiment scores for all phrases in parse trees. This corpus provides us with the data to further understand the quantitative behavior of negators, as the effect of negators can"
P14-1029,P02-1053,0,0.010742,"Each dot in the figure corresponds to a negated phrase in the treebank. The x-axis is the sentiment score of its argument s(w) ~ and y-axis the sentiment score of the entire negated phrase s(wn , w). ~ 2 Related work Automatic sentiment analysis The expression of sentiment is an integral component of human language. In written text, sentiment is conveyed with word senses and their composition, and in speech also via prosody such as pitch (Mairesse et al., 2012). Early work on automatic sentiment analysis includes the widely cited work of (Hatzivassiloglou and McKeown, 1997; Pang et al., 2002; Turney, 2002), among others. Since then, there has been an explosion of research addressing various aspects of the problem, including detecting subjectivity, rating and classifying sentiment, labeling sentiment-related semantic roles (e.g., target of sentiment), and visualizing sentiment (see surveys by Pang and Lee (2008) and Liu and Zhang (2012)). Negation modeling Negation is a general grammatical category pertaining to the changing of the truth values of propositions; negation modeling is not limited to sentiment. For example, paraphrase and contradiction detection systems rely on detecting negated exp"
P14-1029,W10-3111,0,0.190685,"posed by Hatzivassiloglou and McKeown (1997) and Mohammad et al. (2013). Negation modeling for sentiment An early yet influential reversing assumption conjectures that a negator reverses the sign of the sentiment value of the modified text (Polanyi and Zaenen, 2004; Kennedy and Inkpen, 2006), e.g., from +0.5 to 0.5, or vice versa. A different hypothesis, called the shifting hypothesis in this paper, assumes that negators change the sentiment values by a constant amount (Taboada et al., 2011; Liu and Seneff, 2009). Other approaches to negation modeling have been discussed in (Jia et al., 2009; Wiegand et al., 2010; Lapponi et al., 2012; Benamara et al., 2012). In the process of semantic composition, the effect of negators could depend on the syntax and semantics of the text spans they modify. The approaches of modeling this include bag-of-wordbased models. For example, in the work of (Kennedy and Inkpen, 2006), a feature not good will be created if the word good is encountered We can see that the reversing assumption (the red diagonal line) does capture some regularity of human perception, but rather roughly. Moreover, the figure shows that same or similar s(w) ~ scores (x-axis) can correspond to very"
P14-1029,H05-1044,0,0.131616,"cted distribution y i ∈ Rm×1 at node i and the target distribution ti ∈ Rm×1 at that node. That is, the error for a sentence is calculated as:      T a a [1:d] a (6) +W V p2 = tanh( p1 p1 p1  T     a a a sen[1:d] sen + sen V +W ) p1 psen psen 1 1 As shown in Equation 6, for the node vector p1 ∈ Rd×1 , we employ a matrix, namely W sen ∈ Rd×(d+m) and a tensor, V sen ∈ R(d+m)×(d+m)×d , aiming at explicitly capturing the interplays between the sentiment class of p1 , denoted as psen 1 (∈ Rm×1 ), and the negator a. Here, we assume the sentiment task has m classes. Following the idea of Wilson et al. (2005), we regard the sentiment of p1 as a prior sentiment as it has not been affected by the specific context (negators), so we denote our method as prior sentiment-enriched tensor network (PSTN). In Figure 2, the red portion shows the added components of PSTN. Note that depending on different purposes, psen 1 can take the value of the automatically predicted sentiment distribution obtained in forward propagation, the gold sentiment annotation of node p1 , or even other normalized prior sentiment value or confidence score from external sources (e.g., sentiment lexicons or external training data). T"
P14-1029,P03-1054,0,\N,Missing
P15-1011,P15-1145,1,0.0834702,"Missing"
P15-1011,D08-1103,0,0.0298405,"d in our experiment for modeling lexical relatedness in the CRM component was Google one billion word corpus (Chelba et al., 2013). Normalization and tokenization were performed using the scripts distributed from https://code.google.com/p/1-billionword-language-modeling-benchmark/, and sentences were shuffled randomly. We computed embedding for a word if its count in the corpus is equal to or larger than five, with the method described in Section 3.4. Words with counts lower than five were discarded. Experiment Set-Up Data Our experiment uses the “most contrasting word” questions collected by Mohammad et al. (2008) from Graduate Record Examination (GRE), which was originally created by Educational Testing Service (ETS). Each GRE question has a target word and five candidate choices; the task is to identify among the choices the most contrasting word with regard to the given target word. The dataset consists of a development set and a test set, with 162 and 950 questions, respectively. Evaluation Metric Same as in previous work, the evaluation metric is F-score, where precision is the percentage of the questions answered correctly over the questions the models attempt to answer, 110 5.2 and recall is the"
P15-1011,J13-3004,0,0.28012,"esentations have been leveraged 106 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 106–115, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics 2 Related Work contrasting meaning is not distinguished well in such representations. Better models for contrasting meaning is fundamentally interesting. The terms contrasting, opposite, and antonym have different definitions in the literature, while sometimes they are used interchangeably. Following (Mohammad et al., 2013), in this paper we refer to opposites as word pairs that “have a strong binary incompatibility relation with each other or that are saliently different across a dimension of meaning”, e.g., day and night. Antonyms are a subset of opposites that are also gradable adjectives, with same definition as in (Cruse, 1986) as well. Contrasting word pairs have the broadest meaning among them, referring to word pairs having “some non-zero degree of binary incompatibility and/or have some non-zero difference across a dimension of meaning.” Therefore by definition, opposites are a subset of contrasting wor"
P15-1011,P14-1146,0,0.0388559,"Missing"
P15-1011,S14-2076,1,0.897642,"Missing"
P15-1011,D12-1111,0,0.0235237,"Missing"
P15-1011,D14-1161,0,0.0116963,"Missing"
P15-1011,P14-1029,1,0.747372,"Missing"
P15-1011,S14-2077,1,0.880691,"Missing"
P17-1152,D15-1075,0,0.927613,"g for inflation. h: Some of the companies in the poll reported cost increases. Introduction Reasoning and inference are central to both human and artificial intelligence. Modeling inference in human language is notoriously challenging but is a basic problem towards true natural language understanding, as pointed out by MacCartney and Manning (2008), “a necessary (if not sufficient) The most recent years have seen advances in modeling natural language inference. An important contribution is the creation of a much larger annotated dataset, the Stanford Natural Language Inference (SNLI) dataset (Bowman et al., 2015). The corpus has 570,000 human-written English sentence pairs manually labeled by multiple human subjects. This makes it feasible to train more complex inference models. Neural network models, which often need relatively large annotated data to estimate their parameters, have shown to achieve the state of the art on SNLI (Bowman et al., 2015, 2016; Munkhdalai and Yu, 2016b; Parikh et al., 2016; Sha et al., 2016; Paria et al., 2016). While some previous top-performing models use rather complicated network architectures to achieve the state-of-the-art results (Munkhdalai and Yu, 2016b), we demon"
P17-1152,P16-1139,0,0.349126,". Rocktäschel et al. (2015) proposed neural attention-based models for NLI, which captured the attention information. In general, attention based models have been shown to be effective in a wide range of tasks, including machine translation (Bahdanau et al., 2014), speech recognition (Chorowski et al., 2015; Chan et al., 2016), image caption (Xu et al., 2015), and text summarization (Rush et al., 2015; Chen et al., 2016), among others. For NLI, the idea allows neural models to pay attention to specific areas of the sentences. A variety of more advanced networks have been developed since then (Bowman et al., 2016; Vendrov et al., 2015; Mou et al., 2016; Liu et al., 2016; Munkhdalai and Yu, 2016a; Rocktäschel et al., 2015; Wang and Jiang, 2016; Cheng et al., 2016; Parikh et al., 2016; Munkhdalai and Yu, 2016b; Sha et al., 2016; Paria et al., 2016). Among them, more relevant to ours are the approaches proposed by Parikh et al. (2016) and Munkhdalai and Yu (2016b), which are among the best performing models. Parikh et al. (2016) propose a relatively simple but very effective decomposable model. The model decomposes the NLI problem into subproblems that can be solved separately. On the other hand, Munkhda"
P17-1152,P15-1011,1,0.299019,"Missing"
P17-1152,D16-1053,0,0.0771153,"Missing"
P17-1152,W14-4012,0,0.0198449,"Missing"
P17-1152,C14-1068,0,0.022914,"Missing"
P17-1152,P03-1054,0,0.0491768,"(SNLI) corpus (Bowman et al., 2015) focuses on three basic relationships between a premise and a potential hypothesis: the premise entails the hypothesis (entailment), they contradict each other (contradiction), or they are not related (neutral). The original SNLI corpus contains also “the other” category, which includes the sentence pairs lacking consensus among multiple human annotators. As in the related work, we remove this category. We used the same split as in Bowman et al. (2015) and other previous work. The parse trees used in this paper are produced by the Stanford PCFG Parser 3.5.3 (Klein and Manning, 2003) and they are delivered as part of the SNLI corpus. We use classification accuracy as the evaluation metric, as in related work. Training We use the development set to select models for testing. To help replicate our results, we publish our code1 . Below, we list our training details. We use the Adam method (Kingma and Ba, 2014) for optimization. The first momentum is set to be 0.9 and the second 0.999. The initial learning rate is 0.0004 and the batch size is 32. All hidden states of LSTMs, tree-LSTMs, and word embeddings have 300 dimensions. We use dropout with a rate of 0.5, which is applie"
P17-1152,S15-1002,0,0.0646178,"present that time step and its context. Note that we used LSTM memory blocks in our models. We examined other recurrent memory blocks such as GRUs (Gated Recurrent Units) (Cho et al., 2014) and they are inferior to LSTMs on the heldout set for our NLI task. As discussed above, it is intriguing to explore the effectiveness of syntax for natural language inference; for example, whether it is useful even when incorporated into the best-performing models. To this end, we will also encode syntactic parse trees of a premise and hypothesis through treeLSTM (Zhu et al., 2015; Tai et al., 2015; Le and Zuidema, 2015), which extends the chain LSTM to a recursive network (Socher et al., 2011). Figure 1: A high-level view of our hybrid neural inference networks. to encode the input premise and hypothesis (Equation (1) and (2)). Here BiLSTM learns to represent a word (e.g., ai ) and its context. Later we will also use BiLSTM to perform inference composition to construct the final prediction, where BiLSTM encodes local inference information and its interaction. To bookkeep the notations for later use, we ¯i the hidden (output) state generated by write as a the BiLSTM at time i over the input sequence a. ¯j: Th"
P17-1152,C08-1066,0,0.0129228,"itional improvement. Particularly, incorporating syntactic parsing information contributes to our best result—it further improves the performance even when added to the already very strong model. 1 p: Several airlines polled saw costs grow more than expected, even after adjusting for inflation. h: Some of the companies in the poll reported cost increases. Introduction Reasoning and inference are central to both human and artificial intelligence. Modeling inference in human language is notoriously challenging but is a basic problem towards true natural language understanding, as pointed out by MacCartney and Manning (2008), “a necessary (if not sufficient) The most recent years have seen advances in modeling natural language inference. An important contribution is the creation of a much larger annotated dataset, the Stanford Natural Language Inference (SNLI) dataset (Bowman et al., 2015). The corpus has 570,000 human-written English sentence pairs manually labeled by multiple human subjects. This makes it feasible to train more complex inference models. Neural network models, which often need relatively large annotated data to estimate their parameters, have shown to achieve the state of the art on SNLI (Bowman"
P17-1152,N10-1146,0,0.0231776,"Missing"
P17-1152,P16-2022,0,0.4001,"attention-based models for NLI, which captured the attention information. In general, attention based models have been shown to be effective in a wide range of tasks, including machine translation (Bahdanau et al., 2014), speech recognition (Chorowski et al., 2015; Chan et al., 2016), image caption (Xu et al., 2015), and text summarization (Rush et al., 2015; Chen et al., 2016), among others. For NLI, the idea allows neural models to pay attention to specific areas of the sentences. A variety of more advanced networks have been developed since then (Bowman et al., 2016; Vendrov et al., 2015; Mou et al., 2016; Liu et al., 2016; Munkhdalai and Yu, 2016a; Rocktäschel et al., 2015; Wang and Jiang, 2016; Cheng et al., 2016; Parikh et al., 2016; Munkhdalai and Yu, 2016b; Sha et al., 2016; Paria et al., 2016). Among them, more relevant to ours are the approaches proposed by Parikh et al. (2016) and Munkhdalai and Yu (2016b), which are among the best performing models. Parikh et al. (2016) propose a relatively simple but very effective decomposable model. The model decomposes the NLI problem into subproblems that can be solved separately. On the other hand, Munkhdalai and Yu (2016b) propose much more com"
P17-1152,D16-1244,0,0.640143,"ent years have seen advances in modeling natural language inference. An important contribution is the creation of a much larger annotated dataset, the Stanford Natural Language Inference (SNLI) dataset (Bowman et al., 2015). The corpus has 570,000 human-written English sentence pairs manually labeled by multiple human subjects. This makes it feasible to train more complex inference models. Neural network models, which often need relatively large annotated data to estimate their parameters, have shown to achieve the state of the art on SNLI (Bowman et al., 2015, 2016; Munkhdalai and Yu, 2016b; Parikh et al., 2016; Sha et al., 2016; Paria et al., 2016). While some previous top-performing models use rather complicated network architectures to achieve the state-of-the-art results (Munkhdalai and Yu, 2016b), we demonstrate in this paper that enhancing sequential inference models based on chain 1657 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 1657–1668 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-1152 models can outperform all previous results, suggesting that the potentials o"
P17-1152,D14-1162,0,0.123262,"Missing"
P17-1152,D15-1044,0,0.0690079,"Missing"
P17-1152,C16-1270,0,0.0980079,"dvances in modeling natural language inference. An important contribution is the creation of a much larger annotated dataset, the Stanford Natural Language Inference (SNLI) dataset (Bowman et al., 2015). The corpus has 570,000 human-written English sentence pairs manually labeled by multiple human subjects. This makes it feasible to train more complex inference models. Neural network models, which often need relatively large annotated data to estimate their parameters, have shown to achieve the state of the art on SNLI (Bowman et al., 2015, 2016; Munkhdalai and Yu, 2016b; Parikh et al., 2016; Sha et al., 2016; Paria et al., 2016). While some previous top-performing models use rather complicated network architectures to achieve the state-of-the-art results (Munkhdalai and Yu, 2016b), we demonstrate in this paper that enhancing sequential inference models based on chain 1657 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 1657–1668 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-1152 models can outperform all previous results, suggesting that the potentials of such sequential"
P17-1152,D13-1170,0,0.0469839,"Missing"
P17-1152,P15-1150,0,0.20089,"ep are concatenated to represent that time step and its context. Note that we used LSTM memory blocks in our models. We examined other recurrent memory blocks such as GRUs (Gated Recurrent Units) (Cho et al., 2014) and they are inferior to LSTMs on the heldout set for our NLI task. As discussed above, it is intriguing to explore the effectiveness of syntax for natural language inference; for example, whether it is useful even when incorporated into the best-performing models. To this end, we will also encode syntactic parse trees of a premise and hypothesis through treeLSTM (Zhu et al., 2015; Tai et al., 2015; Le and Zuidema, 2015), which extends the chain LSTM to a recursive network (Socher et al., 2011). Figure 1: A high-level view of our hybrid neural inference networks. to encode the input premise and hypothesis (Equation (1) and (2)). Here BiLSTM learns to represent a word (e.g., ai ) and its context. Later we will also use BiLSTM to perform inference composition to construct the final prediction, where BiLSTM encodes local inference information and its interaction. To bookkeep the notations for later use, we ¯i the hidden (output) state generated by write as a the BiLSTM at time i over the i"
P17-1152,N16-1170,0,0.201624,"ttention based models have been shown to be effective in a wide range of tasks, including machine translation (Bahdanau et al., 2014), speech recognition (Chorowski et al., 2015; Chan et al., 2016), image caption (Xu et al., 2015), and text summarization (Rush et al., 2015; Chen et al., 2016), among others. For NLI, the idea allows neural models to pay attention to specific areas of the sentences. A variety of more advanced networks have been developed since then (Bowman et al., 2016; Vendrov et al., 2015; Mou et al., 2016; Liu et al., 2016; Munkhdalai and Yu, 2016a; Rocktäschel et al., 2015; Wang and Jiang, 2016; Cheng et al., 2016; Parikh et al., 2016; Munkhdalai and Yu, 2016b; Sha et al., 2016; Paria et al., 2016). Among them, more relevant to ours are the approaches proposed by Parikh et al. (2016) and Munkhdalai and Yu (2016b), which are among the best performing models. Parikh et al. (2016) propose a relatively simple but very effective decomposable model. The model decomposes the NLI problem into subproblems that can be solved separately. On the other hand, Munkhdalai and Yu (2016b) propose much more complicated networks that consider sequential LSTM-based encoding, recursive networks, and comp"
P17-1152,P14-1029,1,0.288524,"Missing"
P17-5003,P14-3006,0,0.0417096,"Missing"
P17-5003,S15-1001,1,0.84434,"last several years have seen extensive interests on distributional approaches, in which text spans of different granularities are encoded as continuous vectors. If properly learned, such representations have been shown to help achieve the state-of-the-art performances on a variety of NLP problems. In this tutorial, we will cover the fundamentals and selected research topics on neural networkbased modeling for semantic composition, which aims to learn distributed representations for larger spans of text, e.g., phrases (Yin and Sch¨utze, 2014) and sentences (Zhu et al., 2016; Chen et al., 2016; Zhu et al., 2015b,a; Tai et al., 2015; Kalchbrenner et al., 2014; Irsoy and Cardie, 2014; Socher et al., 2012), from the meaning representations of their parts, e.g., word embedding. We begin by briefly introducing traditional approaches to semantic composition, including logic-based formal semantic approaches and simple arithmetic operations over vectors based on corpus word counts (Mitchell and Lapata, 2008; Landauer and Dumais, 1997). Our main focus, however, will be on distributed representation-based modeling, whereby the representations of words and the operations composing them are jointly learned from"
P17-5003,P16-1139,0,0.0134347,"ntations of words and the operations composing them are jointly learned from a training objective. We cover the generic ideas behind neural network-based semantic composition and dive into the details of three typical composition architectures: the convolutional composition models (Kalchbrenner et al., 2014; Zhang et al., 2015), recurrent composition models (Zhu et al., 2016), and recursive composition models (Irsoy and Cardie, 2014; Socher et al., 2012; Zhu et al., 2015b; Tai et al., 2015). After that, we will discuss several unsupervised approaches (Le and Mikolov, 2014; Kiros et al., 2014; Bowman et al., 2016; Miao et al., 2016). 2 Tutorial Outline • Introduction ◦ Definition of semantic composition ◦ Conventional and basic approaches  Formal semantics  Bag of words with learned representations (additive, learned projection) • Parametrising Composition Functions ◦ Convolutional composition models ◦ Recurrent composition models ◦ Recursive composition models  TreeRNN/TreeLSTM  SPINN and RL-SPINN ◦ Unsupervised models  Skip-thought vectors and paragraph vectors  Variational auto-encoders for text • Selected Topics ◦ Incorporating compositional and noncompositional (e.g., holistically learned)"
P17-5003,N16-1106,1,0.839768,"al language understanding (NLP). The last several years have seen extensive interests on distributional approaches, in which text spans of different granularities are encoded as continuous vectors. If properly learned, such representations have been shown to help achieve the state-of-the-art performances on a variety of NLP problems. In this tutorial, we will cover the fundamentals and selected research topics on neural networkbased modeling for semantic composition, which aims to learn distributed representations for larger spans of text, e.g., phrases (Yin and Sch¨utze, 2014) and sentences (Zhu et al., 2016; Chen et al., 2016; Zhu et al., 2015b,a; Tai et al., 2015; Kalchbrenner et al., 2014; Irsoy and Cardie, 2014; Socher et al., 2012), from the meaning representations of their parts, e.g., word embedding. We begin by briefly introducing traditional approaches to semantic composition, including logic-based formal semantic approaches and simple arithmetic operations over vectors based on corpus word counts (Mitchell and Lapata, 2008; Landauer and Dumais, 1997). Our main focus, however, will be on distributed representation-based modeling, whereby the representations of words and the operations co"
P17-5003,P14-1062,1,0.623972,"interests on distributional approaches, in which text spans of different granularities are encoded as continuous vectors. If properly learned, such representations have been shown to help achieve the state-of-the-art performances on a variety of NLP problems. In this tutorial, we will cover the fundamentals and selected research topics on neural networkbased modeling for semantic composition, which aims to learn distributed representations for larger spans of text, e.g., phrases (Yin and Sch¨utze, 2014) and sentences (Zhu et al., 2016; Chen et al., 2016; Zhu et al., 2015b,a; Tai et al., 2015; Kalchbrenner et al., 2014; Irsoy and Cardie, 2014; Socher et al., 2012), from the meaning representations of their parts, e.g., word embedding. We begin by briefly introducing traditional approaches to semantic composition, including logic-based formal semantic approaches and simple arithmetic operations over vectors based on corpus word counts (Mitchell and Lapata, 2008; Landauer and Dumais, 1997). Our main focus, however, will be on distributed representation-based modeling, whereby the representations of words and the operations composing them are jointly learned from a training objective. We cover the generic idea"
P17-5003,D12-1110,0,\N,Missing
P17-5003,P15-1150,0,\N,Missing
P17-5003,P08-1028,0,\N,Missing
P17-5003,P16-1162,0,\N,Missing
P18-1224,D15-1075,0,0.473188,"an Zhu ECE, Queen’s University xiaodan.zhu@queensu.ca Zhen-Hua Ling University of Science and Technology of China zhling@ustc.edu.cn Diana Inkpen University of Ottawa diana@site.uottawa.ca Si Wei iFLYTEK Research siwei@iflytek.com Abstract In the last several years, larger annotated datasets were made available, e.g., the SNLI (Bowman et al., 2015) and MultiNLI datasets (Williams et al., 2017), which made it feasible to train rather complicated neuralnetwork-based models that fit a large set of parameters to better model NLI. Such models have shown to achieve the state-of-the-art performance (Bowman et al., 2015, 2016; Yu and Munkhdalai, 2017b; Parikh et al., 2016; Sha et al., 2016; Chen et al., 2017a,b; Tay et al., 2018). While neural networks have been shown to be very effective in modeling NLI with large training data, they have often focused on end-to-end training by assuming that all inference knowledge is learnable from the provided training data. In this paper, we relax this assumption and explore whether external knowledge can further help NLI. Consider an example: Modeling natural language inference is a very challenging task. With the availability of large annotated data, it has recently be"
P18-1224,P16-1139,0,0.0147052,"g-based models use Siamese architecture (Bromley et al., 1993). The parametertied neural networks are applied to encode both the premise and the hypothesis. Then a neural network classifier is applied to decide relationship between the two sentences. Different neural networks have been utilized for sentence encoding, such as LSTM (Bowman et al., 2015), GRU (Vendrov et al., 2015), CNN (Mou et al., 2016), BiLSTM and its variants (Liu et al., 2016c; Lin et al., 2017; Chen et al., 2017b; Nie and Bansal, 2017), self-attention network (Shen et al., 2017, 2018), and more complicated neural networks (Bowman et al., 2016; Yu and Munkhdalai, 2017a,b; Choi et al., 2017). Sentence-encoding-based models transform sentences into fixed-length vector representations, which may help a wide range of tasks (Conneau et al., 2017). The second set of models use inter-sentence attention (Rockt¨aschel et al., 2015; Wang and Jiang, 2016; Cheng et al., 2016; Parikh et al., 2016; Chen et al., 2017a). Among them, Rockt¨aschel et al. (2015) were among the first to propose neural attention-based models for NLI. Chen et al. (2017a) proposed an enhanced sequential inference model (ESIM), which is one of the best models so far and i"
P18-1224,P15-1011,1,0.45444,"oves the state-of-the-art neural NLI models to achieve better performances on the SNLI and MultiNLI datasets. The advantage of using external knowledge is more significant when the size of training data is restricted, suggesting that if more knowledge can be obtained, it may have more benefit. In addition to attaining the state-of-the-art performance, we are also interested in understanding how external knowledge affect major components of neural-network-based NLI models. In general, external knowledge has shown to be effective in neural networks for other NLP tasks, including word embedding (Chen et al., 2015; Faruqui et al., 2015; Liu et al., 2015; Wieting et al., 2015; Mrksic et al., 2017), machine translation (Shi et al., 2016; Zhang et al., 2017b), language modeling (Ahn et al., 2016), and dialogue systems (Chen et al., 2016b). 3 Neural-Network-Based NLI Models with External Knowledge In this section we propose neural-network-based NLI models to incorporate external inference knowledge, which, as we will show later in Section 5, achieve the state-of-the-art performance. In addition to attaining the leading performance we are also interested in investigating the effects of external knowledge on"
P18-1224,D16-1053,0,0.00874405,"Missing"
P18-1224,D17-1070,0,0.0157156,"to decide relationship between the two sentences. Different neural networks have been utilized for sentence encoding, such as LSTM (Bowman et al., 2015), GRU (Vendrov et al., 2015), CNN (Mou et al., 2016), BiLSTM and its variants (Liu et al., 2016c; Lin et al., 2017; Chen et al., 2017b; Nie and Bansal, 2017), self-attention network (Shen et al., 2017, 2018), and more complicated neural networks (Bowman et al., 2016; Yu and Munkhdalai, 2017a,b; Choi et al., 2017). Sentence-encoding-based models transform sentences into fixed-length vector representations, which may help a wide range of tasks (Conneau et al., 2017). The second set of models use inter-sentence attention (Rockt¨aschel et al., 2015; Wang and Jiang, 2016; Cheng et al., 2016; Parikh et al., 2016; Chen et al., 2017a). Among them, Rockt¨aschel et al. (2015) were among the first to propose neural attention-based models for NLI. Chen et al. (2017a) proposed an enhanced sequential inference model (ESIM), which is one of the best models so far and is used as one of our baselines in this paper. In this paper we enrich neural-network-based NLI models with external knowledge. Unlike early work on NLI (Jijkoun and de Rijke, 2005; MacCartney et al., 20"
P18-1224,P17-1152,1,0.37788,"d Technology of China zhling@ustc.edu.cn Diana Inkpen University of Ottawa diana@site.uottawa.ca Si Wei iFLYTEK Research siwei@iflytek.com Abstract In the last several years, larger annotated datasets were made available, e.g., the SNLI (Bowman et al., 2015) and MultiNLI datasets (Williams et al., 2017), which made it feasible to train rather complicated neuralnetwork-based models that fit a large set of parameters to better model NLI. Such models have shown to achieve the state-of-the-art performance (Bowman et al., 2015, 2016; Yu and Munkhdalai, 2017b; Parikh et al., 2016; Sha et al., 2016; Chen et al., 2017a,b; Tay et al., 2018). While neural networks have been shown to be very effective in modeling NLI with large training data, they have often focused on end-to-end training by assuming that all inference knowledge is learnable from the provided training data. In this paper, we relax this assumption and explore whether external knowledge can further help NLI. Consider an example: Modeling natural language inference is a very challenging task. With the availability of large annotated data, it has recently become feasible to train complex models such as neural-network-based inference models, which"
P18-1224,N15-1184,0,0.00489591,"he-art neural NLI models to achieve better performances on the SNLI and MultiNLI datasets. The advantage of using external knowledge is more significant when the size of training data is restricted, suggesting that if more knowledge can be obtained, it may have more benefit. In addition to attaining the state-of-the-art performance, we are also interested in understanding how external knowledge affect major components of neural-network-based NLI models. In general, external knowledge has shown to be effective in neural networks for other NLP tasks, including word embedding (Chen et al., 2015; Faruqui et al., 2015; Liu et al., 2015; Wieting et al., 2015; Mrksic et al., 2017), machine translation (Shi et al., 2016; Zhang et al., 2017b), language modeling (Ahn et al., 2016), and dialogue systems (Chen et al., 2016b). 3 Neural-Network-Based NLI Models with External Knowledge In this section we propose neural-network-based NLI models to incorporate external inference knowledge, which, as we will show later in Section 5, achieve the state-of-the-art performance. In addition to attaining the leading performance we are also interested in investigating the effects of external knowledge on major components of n"
P18-1224,W17-5307,1,0.712456,"d Technology of China zhling@ustc.edu.cn Diana Inkpen University of Ottawa diana@site.uottawa.ca Si Wei iFLYTEK Research siwei@iflytek.com Abstract In the last several years, larger annotated datasets were made available, e.g., the SNLI (Bowman et al., 2015) and MultiNLI datasets (Williams et al., 2017), which made it feasible to train rather complicated neuralnetwork-based models that fit a large set of parameters to better model NLI. Such models have shown to achieve the state-of-the-art performance (Bowman et al., 2015, 2016; Yu and Munkhdalai, 2017b; Parikh et al., 2016; Sha et al., 2016; Chen et al., 2017a,b; Tay et al., 2018). While neural networks have been shown to be very effective in modeling NLI with large training data, they have often focused on end-to-end training by assuming that all inference knowledge is learnable from the provided training data. In this paper, we relax this assumption and explore whether external knowledge can further help NLI. Consider an example: Modeling natural language inference is a very challenging task. With the availability of large annotated data, it has recently become feasible to train complex models such as neural-network-based inference models, which"
P18-1224,P82-1020,0,0.845753,"Missing"
P18-1224,O97-1002,0,0.0637707,"H( j=1 αij rij )) w Pm Pn a = avi , exp(H( α r )) ij ij i=1 j=1 i=1 Pm n X exp(H( i=1 βij rji )) Pn Pm bvj . bw = exp(H( β r )) j=1 i=1 ij ji j=1 m X Representation of External Knowledge Lexical Semantic Relations As described in Section 3.1, to incorporate external knowledge (as a knowledge vector rij ) to the state-of-theart neural network-based NLI models, we first explore semantic relations in WordNet (Miller, 1995), motivated by MacCartney (2009). Specifically, the relations of lexical pairs are derived as described in (1)-(4) below. Instead of using JiangConrath WordNet distance metric (Jiang and Conrath, 1997), which does not improve the performance of our models on the development sets, we add a new feature, i.e., co-hyponyms, which consistently benefit our models. (1) Synonymy: It takes the value 1 if the words in the pair are synonyms in WordNet (i.e., belong to the same synset), and 0 otherwise. For example, [felicitous, good] = 1, [dog, wolf] = 0. (2) Antonymy: It takes the value 1 if the words in the pair are antonyms in WordNet, and 0 otherwise. For example, [wet, dry] = 1. (3) Hypernymy: It takes the value 1 − n/8 if one word is a (direct or indirect) hypernym of the other word in WordNet,"
P18-1224,P16-1098,0,0.0126473,"e complex models. These models mainly fall into two types of approaches: sentence-encoding-based models and models using also inter-sentence attention. Sentence-encoding-based models use Siamese architecture (Bromley et al., 1993). The parametertied neural networks are applied to encode both the premise and the hypothesis. Then a neural network classifier is applied to decide relationship between the two sentences. Different neural networks have been utilized for sentence encoding, such as LSTM (Bowman et al., 2015), GRU (Vendrov et al., 2015), CNN (Mou et al., 2016), BiLSTM and its variants (Liu et al., 2016c; Lin et al., 2017; Chen et al., 2017b; Nie and Bansal, 2017), self-attention network (Shen et al., 2017, 2018), and more complicated neural networks (Bowman et al., 2016; Yu and Munkhdalai, 2017a,b; Choi et al., 2017). Sentence-encoding-based models transform sentences into fixed-length vector representations, which may help a wide range of tasks (Conneau et al., 2017). The second set of models use inter-sentence attention (Rockt¨aschel et al., 2015; Wang and Jiang, 2016; Cheng et al., 2016; Parikh et al., 2016; Chen et al., 2017a). Among them, Rockt¨aschel et al. (2015) were among the first"
P18-1224,D16-1176,0,0.014176,"e complex models. These models mainly fall into two types of approaches: sentence-encoding-based models and models using also inter-sentence attention. Sentence-encoding-based models use Siamese architecture (Bromley et al., 1993). The parametertied neural networks are applied to encode both the premise and the hypothesis. Then a neural network classifier is applied to decide relationship between the two sentences. Different neural networks have been utilized for sentence encoding, such as LSTM (Bowman et al., 2015), GRU (Vendrov et al., 2015), CNN (Mou et al., 2016), BiLSTM and its variants (Liu et al., 2016c; Lin et al., 2017; Chen et al., 2017b; Nie and Bansal, 2017), self-attention network (Shen et al., 2017, 2018), and more complicated neural networks (Bowman et al., 2016; Yu and Munkhdalai, 2017a,b; Choi et al., 2017). Sentence-encoding-based models transform sentences into fixed-length vector representations, which may help a wide range of tasks (Conneau et al., 2017). The second set of models use inter-sentence attention (Rockt¨aschel et al., 2015; Wang and Jiang, 2016; Cheng et al., 2016; Parikh et al., 2016; Chen et al., 2017a). Among them, Rockt¨aschel et al. (2015) were among the first"
P18-1224,P15-1145,1,0.706034,"ls to achieve better performances on the SNLI and MultiNLI datasets. The advantage of using external knowledge is more significant when the size of training data is restricted, suggesting that if more knowledge can be obtained, it may have more benefit. In addition to attaining the state-of-the-art performance, we are also interested in understanding how external knowledge affect major components of neural-network-based NLI models. In general, external knowledge has shown to be effective in neural networks for other NLP tasks, including word embedding (Chen et al., 2015; Faruqui et al., 2015; Liu et al., 2015; Wieting et al., 2015; Mrksic et al., 2017), machine translation (Shi et al., 2016; Zhang et al., 2017b), language modeling (Ahn et al., 2016), and dialogue systems (Chen et al., 2016b). 3 Neural-Network-Based NLI Models with External Knowledge In this section we propose neural-network-based NLI models to incorporate external inference knowledge, which, as we will show later in Section 5, achieve the state-of-the-art performance. In addition to attaining the leading performance we are also interested in investigating the effects of external knowledge on major components of neural-network-base"
P18-1224,D08-1084,0,0.0476054,"Missing"
P18-1224,P14-5010,0,0.00165768,"the hypothesis (entailment), they contradict each other (contradiction), or they are not related (neutral). We use the same data split as in previous work (Bowman et al., 2015; Williams et al., 2017) and classification accuracy as the evaluation metric. In addition, we test our models (trained on the SNLI training set) on a new test set (Glockner et al., 2018), which assesses the lexical inference abilities of NLI systems and consists of 8,193 samples. WordNet 3.0 (Miller, 1995) is used to extract semantic relation features between words. The words are lemmatized using Stanford CoreNLP 3.7.0 (Manning et al., 2014). The premise and the hypothesis sentences fed into the input encoding layer are tokenized. 4.3 Training Details For duplicability, we release our code1 . All our models were strictly selected on the development set of the SNLI data and the in-domain development set of MultiNLI and were then tested on the corresponding test set. The main training details are as follows: the dimension of the hidden states of LSTMs and word embeddings are 300. The word embeddings are initialized by 300D GloVe 840B (Pennington et al., 2014), and out-of-vocabulary words among them are initialized randomly. All wor"
P18-1224,P16-2022,0,0.146841,"l., 2017), has made it possible to train more complex models. These models mainly fall into two types of approaches: sentence-encoding-based models and models using also inter-sentence attention. Sentence-encoding-based models use Siamese architecture (Bromley et al., 1993). The parametertied neural networks are applied to encode both the premise and the hypothesis. Then a neural network classifier is applied to decide relationship between the two sentences. Different neural networks have been utilized for sentence encoding, such as LSTM (Bowman et al., 2015), GRU (Vendrov et al., 2015), CNN (Mou et al., 2016), BiLSTM and its variants (Liu et al., 2016c; Lin et al., 2017; Chen et al., 2017b; Nie and Bansal, 2017), self-attention network (Shen et al., 2017, 2018), and more complicated neural networks (Bowman et al., 2016; Yu and Munkhdalai, 2017a,b; Choi et al., 2017). Sentence-encoding-based models transform sentences into fixed-length vector representations, which may help a wide range of tasks (Conneau et al., 2017). The second set of models use inter-sentence attention (Rockt¨aschel et al., 2015; Wang and Jiang, 2016; Cheng et al., 2016; Parikh et al., 2016; Chen et al., 2017a). Among them, Rock"
P18-1224,Q17-1022,0,0.010362,"Missing"
P18-1224,W17-5308,0,0.106918,"Missing"
P18-1224,D16-1244,0,0.0911062,"Missing"
P18-1224,D14-1162,0,0.114445,"atures between words. The words are lemmatized using Stanford CoreNLP 3.7.0 (Manning et al., 2014). The premise and the hypothesis sentences fed into the input encoding layer are tokenized. 4.3 Training Details For duplicability, we release our code1 . All our models were strictly selected on the development set of the SNLI data and the in-domain development set of MultiNLI and were then tested on the corresponding test set. The main training details are as follows: the dimension of the hidden states of LSTMs and word embeddings are 300. The word embeddings are initialized by 300D GloVe 840B (Pennington et al., 2014), and out-of-vocabulary words among them are initialized randomly. All word embeddings are updated during training. Adam (Kingma and Ba, 2014) is used for optimization with an initial learning rate of 0.0004. The mini-batch size is set to 32. Note that the above hyperparameter settings are same as those used in the baseline ESIM (Chen et al., 2017a) model. ESIM is a strong NLI baseline framework with the source code made available at https://github.com/lukecq1231/nli (the ESIM core code has also been adapted to summarization (Chen et al., 2016a) and questionanswering tasks (Zhang et al., 2017a"
P18-1224,C16-1270,0,0.0789147,"sity of Science and Technology of China zhling@ustc.edu.cn Diana Inkpen University of Ottawa diana@site.uottawa.ca Si Wei iFLYTEK Research siwei@iflytek.com Abstract In the last several years, larger annotated datasets were made available, e.g., the SNLI (Bowman et al., 2015) and MultiNLI datasets (Williams et al., 2017), which made it feasible to train rather complicated neuralnetwork-based models that fit a large set of parameters to better model NLI. Such models have shown to achieve the state-of-the-art performance (Bowman et al., 2015, 2016; Yu and Munkhdalai, 2017b; Parikh et al., 2016; Sha et al., 2016; Chen et al., 2017a,b; Tay et al., 2018). While neural networks have been shown to be very effective in modeling NLI with large training data, they have often focused on end-to-end training by assuming that all inference knowledge is learnable from the provided training data. In this paper, we relax this assumption and explore whether external knowledge can further help NLI. Consider an example: Modeling natural language inference is a very challenging task. With the availability of large annotated data, it has recently become feasible to train complex models such as neural-network-based infe"
P18-1224,P16-1212,0,0.0135296,"Missing"
P18-1224,E17-1038,0,0.27173,"ity xiaodan.zhu@queensu.ca Zhen-Hua Ling University of Science and Technology of China zhling@ustc.edu.cn Diana Inkpen University of Ottawa diana@site.uottawa.ca Si Wei iFLYTEK Research siwei@iflytek.com Abstract In the last several years, larger annotated datasets were made available, e.g., the SNLI (Bowman et al., 2015) and MultiNLI datasets (Williams et al., 2017), which made it feasible to train rather complicated neuralnetwork-based models that fit a large set of parameters to better model NLI. Such models have shown to achieve the state-of-the-art performance (Bowman et al., 2015, 2016; Yu and Munkhdalai, 2017b; Parikh et al., 2016; Sha et al., 2016; Chen et al., 2017a,b; Tay et al., 2018). While neural networks have been shown to be very effective in modeling NLI with large training data, they have often focused on end-to-end training by assuming that all inference knowledge is learnable from the provided training data. In this paper, we relax this assumption and explore whether external knowledge can further help NLI. Consider an example: Modeling natural language inference is a very challenging task. With the availability of large annotated data, it has recently become feasible to train complex"
P18-1224,E17-1002,0,0.246186,"ity xiaodan.zhu@queensu.ca Zhen-Hua Ling University of Science and Technology of China zhling@ustc.edu.cn Diana Inkpen University of Ottawa diana@site.uottawa.ca Si Wei iFLYTEK Research siwei@iflytek.com Abstract In the last several years, larger annotated datasets were made available, e.g., the SNLI (Bowman et al., 2015) and MultiNLI datasets (Williams et al., 2017), which made it feasible to train rather complicated neuralnetwork-based models that fit a large set of parameters to better model NLI. Such models have shown to achieve the state-of-the-art performance (Bowman et al., 2015, 2016; Yu and Munkhdalai, 2017b; Parikh et al., 2016; Sha et al., 2016; Chen et al., 2017a,b; Tay et al., 2018). While neural networks have been shown to be very effective in modeling NLI with large training data, they have often focused on end-to-end training by assuming that all inference knowledge is learnable from the provided training data. In this paper, we relax this assumption and explore whether external knowledge can further help NLI. Consider an example: Modeling natural language inference is a very challenging task. With the availability of large annotated data, it has recently become feasible to train complex"
P18-1224,D17-1146,0,0.0419347,"Missing"
P18-1224,N16-1170,0,0.0335271,"nce encoding, such as LSTM (Bowman et al., 2015), GRU (Vendrov et al., 2015), CNN (Mou et al., 2016), BiLSTM and its variants (Liu et al., 2016c; Lin et al., 2017; Chen et al., 2017b; Nie and Bansal, 2017), self-attention network (Shen et al., 2017, 2018), and more complicated neural networks (Bowman et al., 2016; Yu and Munkhdalai, 2017a,b; Choi et al., 2017). Sentence-encoding-based models transform sentences into fixed-length vector representations, which may help a wide range of tasks (Conneau et al., 2017). The second set of models use inter-sentence attention (Rockt¨aschel et al., 2015; Wang and Jiang, 2016; Cheng et al., 2016; Parikh et al., 2016; Chen et al., 2017a). Among them, Rockt¨aschel et al. (2015) were among the first to propose neural attention-based models for NLI. Chen et al. (2017a) proposed an enhanced sequential inference model (ESIM), which is one of the best models so far and is used as one of our baselines in this paper. In this paper we enrich neural-network-based NLI models with external knowledge. Unlike early work on NLI (Jijkoun and de Rijke, 2005; MacCartney et al., 2008; MacCartney, 2009) that explores external knowledge in conventional NLI models on relatively small NL"
P18-1224,Q15-1025,0,\N,Missing
P18-1224,N18-1101,0,\N,Missing
S13-2053,P11-2008,0,0.0483453,"Missing"
S13-2053,W12-2104,0,0.0657813,"ntiment-word hashtags, and one from tweets with emoticons. In the message-level task, the lexicon-based features provided a gain of 5 F-score points over all others. Both of our systems can be replicated using freely available resources.1 1 Introduction Hundreds of millions of people around the world actively use microblogging websites such as Twitter. Thus there is tremendous interest in sentiment analysis of tweets across a variety of domains such as commerce (Jansen et al., 2009), health (Chew and Eysenbach, 2010; Salath´e and Khandelwal, 2011), and disaster management (Verma et al., 2011; Mandel et al., 2012). 1 The three authors contributed equally to this paper. Svetlana Kiritchenko developed the system for the message-level task, Xiaodan Zhu developed the system for the term-level task, and Saif Mohammad led the overall effort, co-ordinated both tasks, and contributed to feature development. In this paper, we describe how we created two state-of-the-art SVM classifiers, one to detect the sentiment of messages such as tweets and SMS (message-level task) and one to detect the sentiment of a term within a message (term-level task). The sentiment can be one out of three possibilities: positive, neg"
S13-2053,W10-0204,1,0.796442,"Missing"
S13-2053,W11-1709,1,0.242544,"from tweets with sentiment-word hashtags, and one from tweets with emoticons. The automatically generated lexicons were particularly useful. In the message-level task for tweets, they alone provided a gain of more than 5 F-score points over and above that obtained using all other features. The lexicons are made freely available.3 2 Sentiment Lexicons Sentiment lexicons are lists of words with associations to positive and negative sentiments. 2.1 Existing, Automatically Created Sentiment Lexicons The manually created lexicons we used include the NRC Emotion Lexicon (Mohammad and Turney, 2010; Mohammad and Yang, 2011) (about 14,000 words), the MPQA Lexicon (Wilson et al., 2005) (about 8,000 words), and the Bing Liu Lexicon (Hu and Liu, 2004) (about 6,800 words). 2.2 New, Tweet-Specific, Automatically Generated Sentiment Lexicons 2.2.1 NRC Hashtag Sentiment Lexicon Certain words in tweets are specially marked with a hashtag (#) to indicate the topic or sentiment. Mo3 www.purl.com/net/sentimentoftweets 322 hammad (2012) showed that hashtagged emotion words such as joy, sadness, angry, and surprised are good indicators that the tweet as a whole (even without the hashtagged emotion word) is expressing the same"
S13-2053,S12-1033,1,0.415659,"Missing"
S13-2053,W02-1011,0,0.0227834,"emoticons at any position in the tweet; – whether the last token is a positive or negative emoticon; • elongated words: the number of words with one character repeated more than two times, for example, ‘soooo’; • clusters: The CMU pos-tagging tool provides the token clusters produced with the Brown clustering algorithm on 56 million Englishlanguage tweets. These 1,000 clusters serve as alternative representation of tweet content, reducing the sparcity of the token space. – the presence or absence of tokens from each of the 1000 clusters; • negation: the number of negated contexts. Following (Pang et al., 2002), we defined a negated context as a segment of a tweet that starts with a negation word (e.g., no, shouldn’t) and ends with one of the punctuation marks: ‘,’, ‘.’, ‘:’, ‘;’, ‘!’, ‘?’. A negated context affects the ngram and lexicon features: we add ‘ NEG’ suffix to each word following the negation word (‘perfect’ becomes ‘perfect NEG’). The ‘ NEG’ suffix is also added to polarity and emotion features (‘POLARITY positive’ becomes ‘POLARITY positive NEG’). The list of negation words was adopted from Christopher Potts’ sentiment tutorial.5 4 5 http://sentiment.christopherpotts.net/tokenizing.html"
S13-2053,H05-1044,0,0.893982,"th emoticons. The automatically generated lexicons were particularly useful. In the message-level task for tweets, they alone provided a gain of more than 5 F-score points over and above that obtained using all other features. The lexicons are made freely available.3 2 Sentiment Lexicons Sentiment lexicons are lists of words with associations to positive and negative sentiments. 2.1 Existing, Automatically Created Sentiment Lexicons The manually created lexicons we used include the NRC Emotion Lexicon (Mohammad and Turney, 2010; Mohammad and Yang, 2011) (about 14,000 words), the MPQA Lexicon (Wilson et al., 2005) (about 8,000 words), and the Bing Liu Lexicon (Hu and Liu, 2004) (about 6,800 words). 2.2 New, Tweet-Specific, Automatically Generated Sentiment Lexicons 2.2.1 NRC Hashtag Sentiment Lexicon Certain words in tweets are specially marked with a hashtag (#) to indicate the topic or sentiment. Mo3 www.purl.com/net/sentimentoftweets 322 hammad (2012) showed that hashtagged emotion words such as joy, sadness, angry, and surprised are good indicators that the tweet as a whole (even without the hashtagged emotion word) is expressing the same emotion. We adapted that idea to create a large corpus of po"
S13-2053,S13-2052,0,0.604218,"Missing"
S14-2076,J92-4003,0,0.121213,"quency of term w in the corpus, freq (pos) is the total number of tokens in positive reviews, and N is the total number of tokens in the corpus. PMI (w , neg) was calculated in a similar way. Since PMI is known to be a poor estimator of association for low-frequency events, we ignored terms that occurred less than five times in each (positive and negative) groups of reviews. 1 score (w , c) = PMI (w , c) − PMI (w , ¬c) (3) 2.3 Word Clusters Word clusters can provide an alternative representation of text, significantly reducing the sparsity of the token space. Using Brown clustering algorithm (Brown et al., 1992), we generated 1,000 word clusters from the Yelp restaurant reviews corpus. Additionally, we used publicly available http://www.yelp.com/dataset_challenge 438 word clusters generated from 56 million Englishlanguage tweets (Owoputi et al., 2013). 3 Subtask 1: Aspect Term Extraction The objective of this subtask is to detect aspect terms in sentences. We approached this problem using in-house entity-recognition software, very similar to the system used by de Bruijn et al. (2011) to detect medical concepts. First, sentences were tokenized to split away punctuation, and then the token sequence was"
S14-2076,W10-0204,1,0.338337,"ts has a sentiment score of 1.2 whereas the same term in negated contexts has a score of -1.4. We built two lexicons, Yelp Restaurant Sentiment AffLex and Yelp Restaurant Sentiment NegLex, as described in (Kiritchenko et al., 2014). Similarly, we generated in-domain sentiment lexicons from the Amazon laptop reviews corpus. In addition, we employed existing out-ofdomain sentiment lexicons: (1) large-coverage automatic tweet sentiment lexicons, Hashtag Sentiment lexicons and Sentiment140 lexicons (Kiritchenko et al., 2014), and (2) three manually created sentiment lexicons, NRC Emotion Lexicon (Mohammad and Turney, 2010), Bing Liu’s Lexicon (Hu and Liu, 2004), and the MPQA Subjectivity Lexicon (Wilson et al., 2005). Yelp Restaurant Word–Aspect Association Lexicon: The Yelp restaurant reviews corpus was also used to generate a lexicon of terms associated with the aspect categories of food, price, service, ambiance, and anecdotes. Each sentence of the corpus was labeled with zero, one, or more of the five aspect categories by our aspect category classification system (described in Section 5). Then, for each term w and each category c an association score was calculated as follows: Lexicons Sentiment Lexicons: F"
S14-2076,W02-1001,0,0.046524,"i+2 )), and prefixes and suffixes of wi (up to 3 characters in length). There are only two phrase-level emission feature templates: the cased and uncased identity of the entire phrase covered by a tag, which allow the system to memorize complete terms such as, “getting a table” or “fish and chips.” Transition features couple tags with tags. Let the current tag be yj . Its transition feature templates are short n-grams of tag identities: yj ; yj , yj−1 ; and yj , yj−1 , yj−2 . During development, we experimented with the training algorithm, trying both PA and the simpler structured perceptron (Collins, 2002). We also added the lowercased back-off features. In Table 2, we re-test these design decisions on the test set, revealing that lower-cased back-off features made a strong contribution, while PA training was perhaps not as important. Our complete system achieved an F1-score of 80.19 on the restaurant domain and 68.57 on the laptop domain, ranking third among 24 teams in both. System NRC-Canada (All) All − lower-casing All − PA + percep Restaurants P R F1 84.41 76.37 80.19 83.68 75.49 79.37 83.37 76.45 79.76 System NRC-Canada (All) All − lower-casing All − PA + percep Laptops P R F1 78.77 60.70"
S14-2076,S13-2053,1,0.543874,"elp restaurant reviews corpus was also used to generate a lexicon of terms associated with the aspect categories of food, price, service, ambiance, and anecdotes. Each sentence of the corpus was labeled with zero, one, or more of the five aspect categories by our aspect category classification system (described in Section 5). Then, for each term w and each category c an association score was calculated as follows: Lexicons Sentiment Lexicons: From the Yelp restaurant reviews corpus, we automatically created an indomain sentiment lexicon for restaurants. Following Turney and Littman (2003) and Mohammad et al. (2013), we calculated a sentiment score for each term w in the corpus: score (w ) = PMI (w , pos) − PMI (w , neg) (1) where pos denotes positive reviews and neg denotes negative reviews. PMI stands for pointwise mutual information: freq (w , pos) ∗ N PMI (w , pos) = log2 (2) freq (w ) ∗ freq (pos) where freq (w, pos) is the number of times a term w occurs in positive reviews, freq (w) is the total frequency of term w in the corpus, freq (pos) is the total number of tokens in positive reviews, and N is the total number of tokens in the corpus. PMI (w , neg) was calculated in a similar way. Since PMI"
S14-2076,N13-1039,0,0.0130894,"iation for low-frequency events, we ignored terms that occurred less than five times in each (positive and negative) groups of reviews. 1 score (w , c) = PMI (w , c) − PMI (w , ¬c) (3) 2.3 Word Clusters Word clusters can provide an alternative representation of text, significantly reducing the sparsity of the token space. Using Brown clustering algorithm (Brown et al., 1992), we generated 1,000 word clusters from the Yelp restaurant reviews corpus. Additionally, we used publicly available http://www.yelp.com/dataset_challenge 438 word clusters generated from 56 million Englishlanguage tweets (Owoputi et al., 2013). 3 Subtask 1: Aspect Term Extraction The objective of this subtask is to detect aspect terms in sentences. We approached this problem using in-house entity-recognition software, very similar to the system used by de Bruijn et al. (2011) to detect medical concepts. First, sentences were tokenized to split away punctuation, and then the token sequence was tagged using a semi-Markov tagger (Sarawagi and Cohen, 2004). The tagger had two possible tags: O for outside, and T for aspect term, where an aspect term could tag a phrase of up to 5 consecutive tokens. The tagger was trained using the struc"
S14-2076,S14-2004,0,0.53975,"ect category of ‘food’. In Task 4, customer reviews are provided for two domains: restaurants and laptops. A fixed set of five aspect categories is defined for the restaurant domain: food, service, price, ambiance, and anecdotes. Automatic systems are to determine if any of those aspect categories are described in a review. The example sentence above describes the aspect categories of food (positive sentiment) and service (negative sentiment). For the laptop reviews, there is no aspect category detection subtask. Further details of the task and data can be found in the task description paper (Pontiki et al., 2014). Introduction Automatically identifying sentiment expressed in text has a number of applications, including tracking sentiment towards products, movies, politicians, etc.; improving customer relation models; and detecting happiness and well-being. In many applications, it is important to associate sentiment with a particular entity or an aspect of an entity. For example, in reviews, customers might express different sentiment towards various aspects of a product or service they have availed. Consider: The lasagna was great, but the service was a bit slow. The review is for a restaurant, and w"
S14-2076,P07-1033,0,0.0159279,"Missing"
S14-2076,de-marneffe-etal-2006-generating,0,0.0335632,"Missing"
S14-2076,H05-1044,0,0.0642609,"t two lexicons, Yelp Restaurant Sentiment AffLex and Yelp Restaurant Sentiment NegLex, as described in (Kiritchenko et al., 2014). Similarly, we generated in-domain sentiment lexicons from the Amazon laptop reviews corpus. In addition, we employed existing out-ofdomain sentiment lexicons: (1) large-coverage automatic tweet sentiment lexicons, Hashtag Sentiment lexicons and Sentiment140 lexicons (Kiritchenko et al., 2014), and (2) three manually created sentiment lexicons, NRC Emotion Lexicon (Mohammad and Turney, 2010), Bing Liu’s Lexicon (Hu and Liu, 2004), and the MPQA Subjectivity Lexicon (Wilson et al., 2005). Yelp Restaurant Word–Aspect Association Lexicon: The Yelp restaurant reviews corpus was also used to generate a lexicon of terms associated with the aspect categories of food, price, service, ambiance, and anecdotes. Each sentence of the corpus was labeled with zero, one, or more of the five aspect categories by our aspect category classification system (described in Section 5). Then, for each term w and each category c an association score was calculated as follows: Lexicons Sentiment Lexicons: From the Yelp restaurant reviews corpus, we automatically created an indomain sentiment lexicon f"
S14-2076,P11-2008,0,0.0134045,"Missing"
S14-2076,P14-1029,1,0.61803,"n of 124,712 reviews as the Amazon laptop reviews corpus. Both the Yelp and the Amazon reviews have one to five star ratings associated with each review. We treated the one- and two-star reviews as negative reviews, and the four- and five-star reviews as positive reviews. 2.2 A positive sentiment score indicates a greater overall association with positive sentiment, whereas a negative score indicates a greater association with negative sentiment. The magnitude is indicative of the degree of association. Negation words (e.g., not, never) can significantly affect the sentiment of an expression (Zhu et al., 2014). Therefore, when generating the sentiment lexicons we distinguished terms appearing in negated contexts (defined as text spans between a negation word and a punctuation mark) and affirmative (non-negated) contexts. The sentiment scores were then calculated separately for the two types of contexts. For example, the term good in affirmative contexts has a sentiment score of 1.2 whereas the same term in negated contexts has a score of -1.4. We built two lexicons, Yelp Restaurant Sentiment AffLex and Yelp Restaurant Sentiment NegLex, as described in (Kiritchenko et al., 2014). Similarly, we gener"
S14-2077,S13-2053,1,0.8504,"c-cnrc.gc.ca Abstract Evaluation Set Twt14 Twt13 Sarc14 LvJn14 SMS13 This paper describes state-of-the-art statistical systems for automatic sentiment analysis of tweets. In a Semeval-2014 shared task (Task 9), our submissions obtained highest scores in the term-level sentiment classification subtask on both the 2013 and 2014 tweets test sets. In the message-level sentiment classification task, our submissions obtained highest scores on the LiveJournal blog posts test set, sarcastic tweets test set, and the 2013 SMS test set. These systems build on our SemEval-2013 sentiment analysis systems (Mohammad et al., 2013) which ranked first in both the termand message-level subtasks in 2013. Key improvements over the 2013 systems are in the handling of negation. We create separate tweet-specific sentiment lexicons for terms in affirmative contexts and in negated contexts. 1 Term-level Task 1 1 3 2 2 Message-level Task 4 2 1 1 1 Table 1: Overall rank of NRC-Canada sentiment analysis models in Semeval-2014 Task 9 under the constrained condition. The rows are five evaluation datasets and the columns are the two subtasks. tem and the subsequent submissions to the 2014 shared task (Rosenthal et al., 2014). The trai"
S14-2077,J12-2001,0,0.0124923,"e sparsity of the token space. Encodings The encoding features are derived from hashtags, punctuation marks, emoticons, elongated words, and uppercased words. For the term-level task, all the above features are extracted for target terms and their context, where a context is a window of words surrounding a target term. For the message-level task, the features are extracted from the whole tweet. Our systems are based on supervised SVMs and a number of surface-form, semantic, and sentiment features. The major improvement in our 2014 system over the 2013 system is in the way it handles negation. Morante and Sporleder (2012) define negation to be “a grammatical category that allows the changing of the truth value of a proposition”. Negation is often expressed through the use of negative signals or negators, words such as isnt and never, and it can significantly affect the sentiment of its scope. We create separate tweetspecific sentiment lexicons for terms in affirmative contexts and in negated contexts. That is, we automatically determine the average sentiment of a term when occurring in an affirmative context, and separately the average sentiment of a term when occurring in a negated context. 2 Our Systems Our"
S14-2077,W02-1011,0,0.0177185,"iment scores of the text span; (5) the sentiment score of the last token in the text span. Note that all these features are generated, when applicable, by using each of the sentiment lexicons mentioned above. Ngrams We employed two types of ngram features: word ngrams and character ngrams. The former reflect the presence or absence of contiguous or non-contiguous sequences of words, and the latter are sequences of prefix/suffix characters in each word. These features are same as in our last year’s submission. Negation The number of negated contexts. Our definition of a negated context follows Pang et al. (2002), which will be described in more details below in Section 2.1. POS The number of occurrences of each partof-speech tag. We tokenized and part-of-speech tagged the tweets with the Carnegie Mellon University (CMU) Twitter NLP tool (Gimpel et al., 2011). Cluster features The CMU POS-tagging tool provides the token clusters produced with the Brown clustering algorithm from 56 million Englishlanguage tweets. These 1,000 clusters serve as an alternative representation of tweet content, reducing the sparsity of the token space. Encodings The encoding features are derived from hashtags, punctuation m"
S14-2077,P11-2008,0,0.0187367,"Missing"
S14-2077,S14-2009,0,0.0724322,"s systems (Mohammad et al., 2013) which ranked first in both the termand message-level subtasks in 2013. Key improvements over the 2013 systems are in the handling of negation. We create separate tweet-specific sentiment lexicons for terms in affirmative contexts and in negated contexts. 1 Term-level Task 1 1 3 2 2 Message-level Task 4 2 1 1 1 Table 1: Overall rank of NRC-Canada sentiment analysis models in Semeval-2014 Task 9 under the constrained condition. The rows are five evaluation datasets and the columns are the two subtasks. tem and the subsequent submissions to the 2014 shared task (Rosenthal et al., 2014). The training data for the SemEval-2014 shared task is same as that of SemEval-2013 (about 10,000 tweets). The 2014 test set has five subcategories: a tweet set provided newly in 2014 (Twt14), the tweet set used for testing in the 2013 shared task (Twt13), a set of tweets that are sarcastic (Sarc14), a set of sentences from the blogging website LiveJournal (LvJn14), and the set of SMS messages used for testing in the 2013 shared task (SMS13). Instances from these categories were interspersed in the provided test set. The participants were not told about the source of the individual messages."
S14-2077,J11-2001,0,0.0477166,"negated context. We reconstructed the Hashtag Sentiment Lexicon and the Sentiment140 Lexicon with this approach and used them in our SemEval-2014 systems. Improving Lexicons and Negation Models An important advantage of our SemEval-2013 systems comes from the use of the two highcoverage tweet-specific sentiment lexicons. In the SemEval-2014 submissions, we improve these lexicons by incorporating negation modeling into the lexicon generation process. 2.1.2 Discriminating Negation Words Different negation words, e.g., never and didn’t, can have different effects on sentiment (Zhu et al., 2014; Taboada et al., 2011). In our SemEval-2014 submission, we discriminate negation words in the term-level models. For example, the word acceptable appearing in a sentence this is never acceptable is marked as acceptable beNever, while in the sentence this is not acceptable, it is marked as acceptable beNot. In this way, different negators (e.g., be not and be never) are treated differently. Note that we do not differentiate the tense and person of auxiliaries in order to reduce sparseness (e.g., was not and am not are treated in the same way). This new representation is used to extract ngrams and lexicon-based featu"
S14-2077,H05-1044,0,0.310616,"a term when occurring in a negated context. 2 Our Systems Our SemEval-2014 systems are based on our SemEval-2013 systems (Mohammad et al., 2013). For completeness, we briefly revisit our previous approach, which uses support vector machine (SVM) as the classification algorithm and leverages the following features. Lexicon features These features are generated by using three manually constructed sentiment lexicons and two automatically constructed lexicons. The manually constructed lexicons include the NRC Emotion Lexicon (Mohammad and Turney, 2010; Mohammad and Yang, 2011), the MPQA Lexicon (Wilson et al., 2005), and the Bing Liu Lexicon (Hu and Liu, 2004). The two automatically constructed lexicons, the Hashtag Sentiment Lexicon and the Sentiment140 Lexicon, were created specifically for tweets (Mohammad et al., 2013). The sentiment score of each term (e.g., a word or bigram) in the automatically constructed lexicons is computed by measuring the PMI (pointwise mutual information) between the term and the positive or negative category of tweets using the formula: SenScore (w) = P M I(w, pos) − P M I(w, neg) (1) where w is a term in the lexicons. P M I(w, pos) is the PMI score between w and the positi"
S14-2077,W10-0204,1,0.801641,"ring in an affirmative context, and separately the average sentiment of a term when occurring in a negated context. 2 Our Systems Our SemEval-2014 systems are based on our SemEval-2013 systems (Mohammad et al., 2013). For completeness, we briefly revisit our previous approach, which uses support vector machine (SVM) as the classification algorithm and leverages the following features. Lexicon features These features are generated by using three manually constructed sentiment lexicons and two automatically constructed lexicons. The manually constructed lexicons include the NRC Emotion Lexicon (Mohammad and Turney, 2010; Mohammad and Yang, 2011), the MPQA Lexicon (Wilson et al., 2005), and the Bing Liu Lexicon (Hu and Liu, 2004). The two automatically constructed lexicons, the Hashtag Sentiment Lexicon and the Sentiment140 Lexicon, were created specifically for tweets (Mohammad et al., 2013). The sentiment score of each term (e.g., a word or bigram) in the automatically constructed lexicons is computed by measuring the PMI (pointwise mutual information) between the term and the positive or negative category of tweets using the formula: SenScore (w) = P M I(w, pos) − P M I(w, neg) (1) where w is a term in the"
S14-2077,P14-1029,1,0.444802,"nd one for distant negated context. We reconstructed the Hashtag Sentiment Lexicon and the Sentiment140 Lexicon with this approach and used them in our SemEval-2014 systems. Improving Lexicons and Negation Models An important advantage of our SemEval-2013 systems comes from the use of the two highcoverage tweet-specific sentiment lexicons. In the SemEval-2014 submissions, we improve these lexicons by incorporating negation modeling into the lexicon generation process. 2.1.2 Discriminating Negation Words Different negation words, e.g., never and didn’t, can have different effects on sentiment (Zhu et al., 2014; Taboada et al., 2011). In our SemEval-2014 submission, we discriminate negation words in the term-level models. For example, the word acceptable appearing in a sentence this is never acceptable is marked as acceptable beNever, while in the sentence this is not acceptable, it is marked as acceptable beNot. In this way, different negators (e.g., be not and be never) are treated differently. Note that we do not differentiate the tense and person of auxiliaries in order to reduce sparseness (e.g., was not and am not are treated in the same way). This new representation is used to extract ngrams"
S14-2077,W11-1709,1,\N,Missing
S15-1001,D08-1083,0,0.693922,"bank and show that the proposed models achieve better results over the model that lacks this ability. 1 Introduction Automatically determining the sentiment of a phrase, a sentence, or even a longer piece of text is still a challenging problem. Data sparseness encountered in such tasks often requires to factorize the problem to consider smaller pieces of component words or phrases, for which much research has been performed on bag-of-words or bag-of-phrases models (Pang and Lee, 2008; Liu and Zhang, 2012). More recent work has started to model sentiment composition (Moilanen and Pulman, 2007; Choi and Cardie, 2008; Socher et al., 2012; Socher et al., 2013), a type of semantic composition that optimizes a sentiment objective. In general, the composition process is critical in the formation of the 1 sentiment of a span of text, which has not been well modeled yet and there is still scope for future work. Compositionality, or non-compositionality, of the senses of text spans is important for language understanding. Sentiment, as one of the major semantic differential categories (Osgood et al., 1957), faces the problem as well. For example, the phrase must see or must try in a movie or restaurant review of"
S15-1001,C10-2028,0,0.0323695,"n be viewed as introducing some prior sentiment knowledge, as in general the sentiment of a word or a phrase perceived independent of its context is often referred to as prior sentiment. Wordlevel prior sentiment is typically annotated in manual sentiment lexicons (Wilson et al., 2005; Hu and Liu, 2004; Mohammad and Turney, 2010), or learned in an unsupervised or semisupervised way (Hatzivassiloglou and McKeown, 1997; Esuli and Sebastiani, 2006; Turney and Littman, 2003; Mohammad et al., 2009). More recently, sentiment indicators, such as emoticons and hashtags, are utilized (Go et al., 2009; Davidov et al., 2010; Kouloumpis et al., 2011; Mohammad, 2012; Mohammad et al., 2013a). With enough data, such freely available (but noisy) annotation can be used to learn the sentiment of ngrams. In our study, we will investigate in the proposed composition models the effect of automatically learned sentimental ngrams. 3 Prior-enriched semantic networks In this paper, we propose several neural networks that enable each composition operation to possess the ability of choosing and merging sentiment from lower-level composition and that from non-compositional sources. We call the networks Prior-Enriched Semantic Ne"
S15-1001,esuli-sebastiani-2006-sentiwordnet,0,0.0301544,"rate compositional and non-compositional sentiment in sentiment composition. Prior knowledge of sentiment Integrating noncompositional sentiment into the composition pro2 cess can be viewed as introducing some prior sentiment knowledge, as in general the sentiment of a word or a phrase perceived independent of its context is often referred to as prior sentiment. Wordlevel prior sentiment is typically annotated in manual sentiment lexicons (Wilson et al., 2005; Hu and Liu, 2004; Mohammad and Turney, 2010), or learned in an unsupervised or semisupervised way (Hatzivassiloglou and McKeown, 1997; Esuli and Sebastiani, 2006; Turney and Littman, 2003; Mohammad et al., 2009). More recently, sentiment indicators, such as emoticons and hashtags, are utilized (Go et al., 2009; Davidov et al., 2010; Kouloumpis et al., 2011; Mohammad, 2012; Mohammad et al., 2013a). With enough data, such freely available (but noisy) annotation can be used to learn the sentiment of ngrams. In our study, we will investigate in the proposed composition models the effect of automatically learned sentimental ngrams. 3 Prior-enriched semantic networks In this paper, we propose several neural networks that enable each composition operation to"
S15-1001,P97-1023,0,0.0567846,"the neural network approach to integrate compositional and non-compositional sentiment in sentiment composition. Prior knowledge of sentiment Integrating noncompositional sentiment into the composition pro2 cess can be viewed as introducing some prior sentiment knowledge, as in general the sentiment of a word or a phrase perceived independent of its context is often referred to as prior sentiment. Wordlevel prior sentiment is typically annotated in manual sentiment lexicons (Wilson et al., 2005; Hu and Liu, 2004; Mohammad and Turney, 2010), or learned in an unsupervised or semisupervised way (Hatzivassiloglou and McKeown, 1997; Esuli and Sebastiani, 2006; Turney and Littman, 2003; Mohammad et al., 2009). More recently, sentiment indicators, such as emoticons and hashtags, are utilized (Go et al., 2009; Davidov et al., 2010; Kouloumpis et al., 2011; Mohammad, 2012; Mohammad et al., 2013a). With enough data, such freely available (but noisy) annotation can be used to learn the sentiment of ngrams. In our study, we will investigate in the proposed composition models the effect of automatically learned sentimental ngrams. 3 Prior-enriched semantic networks In this paper, we propose several neural networks that enable e"
S15-1001,P03-1054,0,0.00437392,"fter the error message passing from m3 to i3 is obtained, it can be summed up with the local error message from the sentiment prediction errors at the node i3 itself to obtain the total error message for i3 , which is in turn used to calculate the error messages passed down as well as the derivative in the lower-level tree. 4 4.1 Experiments Data We use the Stanford Sentiment Treebank (Socher et al., 2013) in our experiments. The data contain about 11,800 sentences from the movie reviews that were originally collected by Pang and Lee (2005). The sentences were parsed with the Stanford parser (Klein and Manning, 2003). Phrases at all the tree nodes were manually annotated with sentiment values. We use the same split of the training and test data as in (Socher et al., 2013) to predict the sentiment categories of the roots (sentences) and the phrases, and use the same evaluation metric, classification accuracy, to measure the performances. 4.2 Obtaining non-compositional sentiment In our experiments, we explore in sentiment composition the effect of two different types of noncompositional sentiment: (1) sentiment of ngrams automatically learned from an external, much larger corpus, and (2) sentiment of ngram"
S15-1001,W10-0204,0,0.0172445,"ckward propagation, to optimize the system performance. In this paper, we follow the neural network approach to integrate compositional and non-compositional sentiment in sentiment composition. Prior knowledge of sentiment Integrating noncompositional sentiment into the composition pro2 cess can be viewed as introducing some prior sentiment knowledge, as in general the sentiment of a word or a phrase perceived independent of its context is often referred to as prior sentiment. Wordlevel prior sentiment is typically annotated in manual sentiment lexicons (Wilson et al., 2005; Hu and Liu, 2004; Mohammad and Turney, 2010), or learned in an unsupervised or semisupervised way (Hatzivassiloglou and McKeown, 1997; Esuli and Sebastiani, 2006; Turney and Littman, 2003; Mohammad et al., 2009). More recently, sentiment indicators, such as emoticons and hashtags, are utilized (Go et al., 2009; Davidov et al., 2010; Kouloumpis et al., 2011; Mohammad, 2012; Mohammad et al., 2013a). With enough data, such freely available (but noisy) annotation can be used to learn the sentiment of ngrams. In our study, we will investigate in the proposed composition models the effect of automatically learned sentimental ngrams. 3 Prior-e"
S15-1001,D09-1063,0,0.0161694,"sentiment composition. Prior knowledge of sentiment Integrating noncompositional sentiment into the composition pro2 cess can be viewed as introducing some prior sentiment knowledge, as in general the sentiment of a word or a phrase perceived independent of its context is often referred to as prior sentiment. Wordlevel prior sentiment is typically annotated in manual sentiment lexicons (Wilson et al., 2005; Hu and Liu, 2004; Mohammad and Turney, 2010), or learned in an unsupervised or semisupervised way (Hatzivassiloglou and McKeown, 1997; Esuli and Sebastiani, 2006; Turney and Littman, 2003; Mohammad et al., 2009). More recently, sentiment indicators, such as emoticons and hashtags, are utilized (Go et al., 2009; Davidov et al., 2010; Kouloumpis et al., 2011; Mohammad, 2012; Mohammad et al., 2013a). With enough data, such freely available (but noisy) annotation can be used to learn the sentiment of ngrams. In our study, we will investigate in the proposed composition models the effect of automatically learned sentimental ngrams. 3 Prior-enriched semantic networks In this paper, we propose several neural networks that enable each composition operation to possess the ability of choosing and merging senti"
S15-1001,S13-2053,1,0.937035,"n general the sentiment of a word or a phrase perceived independent of its context is often referred to as prior sentiment. Wordlevel prior sentiment is typically annotated in manual sentiment lexicons (Wilson et al., 2005; Hu and Liu, 2004; Mohammad and Turney, 2010), or learned in an unsupervised or semisupervised way (Hatzivassiloglou and McKeown, 1997; Esuli and Sebastiani, 2006; Turney and Littman, 2003; Mohammad et al., 2009). More recently, sentiment indicators, such as emoticons and hashtags, are utilized (Go et al., 2009; Davidov et al., 2010; Kouloumpis et al., 2011; Mohammad, 2012; Mohammad et al., 2013a). With enough data, such freely available (but noisy) annotation can be used to learn the sentiment of ngrams. In our study, we will investigate in the proposed composition models the effect of automatically learned sentimental ngrams. 3 Prior-enriched semantic networks In this paper, we propose several neural networks that enable each composition operation to possess the ability of choosing and merging sentiment from lower-level composition and that from non-compositional sources. We call the networks Prior-Enriched Semantic Networks (PESN). We present several specific implementations based"
S15-1001,S12-1033,0,0.0233029,"knowledge, as in general the sentiment of a word or a phrase perceived independent of its context is often referred to as prior sentiment. Wordlevel prior sentiment is typically annotated in manual sentiment lexicons (Wilson et al., 2005; Hu and Liu, 2004; Mohammad and Turney, 2010), or learned in an unsupervised or semisupervised way (Hatzivassiloglou and McKeown, 1997; Esuli and Sebastiani, 2006; Turney and Littman, 2003; Mohammad et al., 2009). More recently, sentiment indicators, such as emoticons and hashtags, are utilized (Go et al., 2009; Davidov et al., 2010; Kouloumpis et al., 2011; Mohammad, 2012; Mohammad et al., 2013a). With enough data, such freely available (but noisy) annotation can be used to learn the sentiment of ngrams. In our study, we will investigate in the proposed composition models the effect of automatically learned sentimental ngrams. 3 Prior-enriched semantic networks In this paper, we propose several neural networks that enable each composition operation to possess the ability of choosing and merging sentiment from lower-level composition and that from non-compositional sources. We call the networks Prior-Enriched Semantic Networks (PESN). We present several specifi"
S15-1001,P05-1015,0,0.0606398,"erivative for Vm and Wm is the sum of their derivatives over the trees. After the error message passing from m3 to i3 is obtained, it can be summed up with the local error message from the sentiment prediction errors at the node i3 itself to obtain the total error message for i3 , which is in turn used to calculate the error messages passed down as well as the derivative in the lower-level tree. 4 4.1 Experiments Data We use the Stanford Sentiment Treebank (Socher et al., 2013) in our experiments. The data contain about 11,800 sentences from the movie reviews that were originally collected by Pang and Lee (2005). The sentences were parsed with the Stanford parser (Klein and Manning, 2003). Phrases at all the tree nodes were manually annotated with sentiment values. We use the same split of the training and test data as in (Socher et al., 2013) to predict the sentiment categories of the roots (sentences) and the phrases, and use the same evaluation metric, classification accuracy, to measure the performances. 4.2 Obtaining non-compositional sentiment In our experiments, we explore in sentiment composition the effect of two different types of noncompositional sentiment: (1) sentiment of ngrams automati"
S15-1001,D11-1014,0,0.0819757,"would be desirable that a composition model has the ability to decide the sources of knowledge it trusts more: the composition from the component words, the noncompositional source, or a soft combination of them. In such a situation, whether the text span is actually composable may be blur or may not be a concern. In general, the composition of sentiment is a rather complicated process. As a glimpse of evidence, the effect of negation words on changing sentiment of their scopes appears to be a complicated function (Zhu et al., 2014). The recently proposed neural networks (Socher et al., 2013; Socher et al., 2011) are promising, for their capability of modeling complicated functions (Mitchell, 1997) in Proceedings of the Fourth Joint Conference on Lexical and Computational Semantics (*SEM 2015), pages 1–9, Denver, Colorado, June 4–5, 2015. general, handling data sparseness by learning lowdimensional embeddings at each layer of composition, and providing a framework to optimize the composition process in principled way. This paper proposes neural networks for integrating compositional and non-compositional sentiment in the process of sentiment composition. To achieve this, we enable individual compositi"
S15-1001,D12-1110,0,0.64423,"proposed models achieve better results over the model that lacks this ability. 1 Introduction Automatically determining the sentiment of a phrase, a sentence, or even a longer piece of text is still a challenging problem. Data sparseness encountered in such tasks often requires to factorize the problem to consider smaller pieces of component words or phrases, for which much research has been performed on bag-of-words or bag-of-phrases models (Pang and Lee, 2008; Liu and Zhang, 2012). More recent work has started to model sentiment composition (Moilanen and Pulman, 2007; Choi and Cardie, 2008; Socher et al., 2012; Socher et al., 2013), a type of semantic composition that optimizes a sentiment objective. In general, the composition process is critical in the formation of the 1 sentiment of a span of text, which has not been well modeled yet and there is still scope for future work. Compositionality, or non-compositionality, of the senses of text spans is important for language understanding. Sentiment, as one of the major semantic differential categories (Osgood et al., 1957), faces the problem as well. For example, the phrase must see or must try in a movie or restaurant review often indicates a posit"
S15-1001,D13-1170,0,0.206764,"ve better results over the model that lacks this ability. 1 Introduction Automatically determining the sentiment of a phrase, a sentence, or even a longer piece of text is still a challenging problem. Data sparseness encountered in such tasks often requires to factorize the problem to consider smaller pieces of component words or phrases, for which much research has been performed on bag-of-words or bag-of-phrases models (Pang and Lee, 2008; Liu and Zhang, 2012). More recent work has started to model sentiment composition (Moilanen and Pulman, 2007; Choi and Cardie, 2008; Socher et al., 2012; Socher et al., 2013), a type of semantic composition that optimizes a sentiment objective. In general, the composition process is critical in the formation of the 1 sentiment of a span of text, which has not been well modeled yet and there is still scope for future work. Compositionality, or non-compositionality, of the senses of text spans is important for language understanding. Sentiment, as one of the major semantic differential categories (Osgood et al., 1957), faces the problem as well. For example, the phrase must see or must try in a movie or restaurant review often indicates a positive sentiment, which,"
S15-1001,P14-1146,0,0.0384024,"timent. We use in our experiments the bigrams and trigrams learned from the dataset with the occurrences higher than 5. We assign these ngrams into one of the 5 bins according to their sentiment scores obtained with Formula 15: (−∞, −2], (−2, −1], (−1, 1), [1, 2), and [2, +∞). Each ngram is now given a one-hot vector, indicating the polarity and strength of its sentiment. For example, a bigram with a score of -1.5 will be assigned a 5-dimensional vector [0, 1, 0, 0, 0], indicating a weak negative. Note that PESN can also take into other forms of sentiment embeddings, such as those learned in (Tang et al., 2014). In addition, the Stanford Sentiment Treebank contains manually annotated sentiment for each individual phrase in a parse tree, so we use such annotation but not other manual lexicons, by assuming such annotation fits the corpus itself the best. Specifically, we use bigram and trigram annotation in the treebank. Note that even longer ngrams are much sparser and probably less useful in general, one may learn sentiment for multi-word expressions of a larger length, which we will leave as future work. 4.3 Results Overall prediction performance Table 1 shows the accuracies of different models on"
S15-1001,H05-1044,0,0.164593,"a principled method, the forward and backward propagation, to optimize the system performance. In this paper, we follow the neural network approach to integrate compositional and non-compositional sentiment in sentiment composition. Prior knowledge of sentiment Integrating noncompositional sentiment into the composition pro2 cess can be viewed as introducing some prior sentiment knowledge, as in general the sentiment of a word or a phrase perceived independent of its context is often referred to as prior sentiment. Wordlevel prior sentiment is typically annotated in manual sentiment lexicons (Wilson et al., 2005; Hu and Liu, 2004; Mohammad and Turney, 2010), or learned in an unsupervised or semisupervised way (Hatzivassiloglou and McKeown, 1997; Esuli and Sebastiani, 2006; Turney and Littman, 2003; Mohammad et al., 2009). More recently, sentiment indicators, such as emoticons and hashtags, are utilized (Go et al., 2009; Davidov et al., 2010; Kouloumpis et al., 2011; Mohammad, 2012; Mohammad et al., 2013a). With enough data, such freely available (but noisy) annotation can be used to learn the sentiment of ngrams. In our study, we will investigate in the proposed composition models the effect of autom"
S15-1001,P14-1029,1,0.817739,"reliably learn the sentiment of a text span (e.g., an ngram) holistically, it would be desirable that a composition model has the ability to decide the sources of knowledge it trusts more: the composition from the component words, the noncompositional source, or a soft combination of them. In such a situation, whether the text span is actually composable may be blur or may not be a concern. In general, the composition of sentiment is a rather complicated process. As a glimpse of evidence, the effect of negation words on changing sentiment of their scopes appears to be a complicated function (Zhu et al., 2014). The recently proposed neural networks (Socher et al., 2013; Socher et al., 2011) are promising, for their capability of modeling complicated functions (Mitchell, 1997) in Proceedings of the Fourth Joint Conference on Lexical and Computational Semantics (*SEM 2015), pages 1–9, Denver, Colorado, June 4–5, 2015. general, handling data sparseness by learning lowdimensional embeddings at each layer of composition, and providing a framework to optimize the composition process in principled way. This paper proposes neural networks for integrating compositional and non-compositional sentiment in the"
S15-1001,J13-3004,0,\N,Missing
S16-1003,W11-1701,0,0.686791,"labels, with pkudblab using keyword rules, LitisMind using hashtag rules on external data, and INF-UFRGS using a combination of rules and third-party sentiment classifiers. However, we were pleased to see other teams attempting to generalize the supervised data from Task A in interesting ways, either using rules or multi-stage classifiers to bridge the target gap. We are optimistic that there is much interesting follow-up work yet to come on this task. Further details on the submissions can be found 39 Related Work Past work on stance detection includes that by Somasundaran and Wiebe (2010), Anand et al. (2011), Faulkner (2014), Rajadesingan and Liu ˇ (2014), Djemili et al. (2014), Boltuzic and Snajder (2014), Conrad et al. (2012), Sridhar et al. (2014), Rajadesingan and Liu (2014), and Sobhani et al. (2015). There is a vast amount of work in sentiment analysis of tweets, and we refer the reader to surveys (Pang and Lee, 2008; Liu and Zhang, 2012; Mohammad, 2015) and proceedings of recent shared task competitions (Wilson et al., 2013; Mohammad et al., 2013; Rosenthal et al., 2015). See Pontiki et al. (2014), Pontiki et al. (2015), and Kiritchenko et al. (2014a) for tasks and systems on aspect based"
S16-1003,S16-1063,0,0.18439,"25.52 20.13 29.72 28.43 67.19 51.60 50.04 50.62 40.66 38.87 38.06 34.16 28.86 25.77 29.50 22.66 25.02 19.14 22.80 22.60 4.69 18.35 56.28 44.66 42.32 42.02 34.08 32.70 32.39 26.14 25.73 in the system description papers published in the SemEval-2016 proceedings, including papers by Elfardy and Diab (2016) for CU-GWU, Dias and Becker (2016) for INF-URGS, Patra et al. (2016) for JU NLP, Wojatzki and Zesch (2016) for ltl.uni-due, Zarrella and Marsh (2016) for MITRE, Misra et al. (2016) for nldsucsc, Wei et al. (2016) for pkudblab, Tutek et al. (2016) for TakeLab, Yuki et al. (2016) for Tohoku, and Augenstein et al. (2016) for USFD. 7 Table 5: Results for Task B (the official competition metric Favg ) on different subsets of the test data. The highest score in each column is shown in bold. Table 5 shows results for Task B on subsets of the test set where opinion is expressed towards the target of interest and towards some other entity. Observe that here too results are markedly lower when stance is to be inferred from a tweet expressing opinion about some other entity (and not the target of interest). 6.3 Discussion Some teams did very well detecting tweets in favor of Trump (ltl.uni-due), with most of the othe"
S16-1003,W14-2107,0,0.0579715,"and INF-UFRGS using a combination of rules and third-party sentiment classifiers. However, we were pleased to see other teams attempting to generalize the supervised data from Task A in interesting ways, either using rules or multi-stage classifiers to bridge the target gap. We are optimistic that there is much interesting follow-up work yet to come on this task. Further details on the submissions can be found 39 Related Work Past work on stance detection includes that by Somasundaran and Wiebe (2010), Anand et al. (2011), Faulkner (2014), Rajadesingan and Liu ˇ (2014), Djemili et al. (2014), Boltuzic and Snajder (2014), Conrad et al. (2012), Sridhar et al. (2014), Rajadesingan and Liu (2014), and Sobhani et al. (2015). There is a vast amount of work in sentiment analysis of tweets, and we refer the reader to surveys (Pang and Lee, 2008; Liu and Zhang, 2012; Mohammad, 2015) and proceedings of recent shared task competitions (Wilson et al., 2013; Mohammad et al., 2013; Rosenthal et al., 2015). See Pontiki et al. (2014), Pontiki et al. (2015), and Kiritchenko et al. (2014a) for tasks and systems on aspect based sentiment analysis, where the goal is to determine sentiment towards aspects of a product such as sp"
S16-1003,W12-3810,0,0.00926055,"tion of rules and third-party sentiment classifiers. However, we were pleased to see other teams attempting to generalize the supervised data from Task A in interesting ways, either using rules or multi-stage classifiers to bridge the target gap. We are optimistic that there is much interesting follow-up work yet to come on this task. Further details on the submissions can be found 39 Related Work Past work on stance detection includes that by Somasundaran and Wiebe (2010), Anand et al. (2011), Faulkner (2014), Rajadesingan and Liu ˇ (2014), Djemili et al. (2014), Boltuzic and Snajder (2014), Conrad et al. (2012), Sridhar et al. (2014), Rajadesingan and Liu (2014), and Sobhani et al. (2015). There is a vast amount of work in sentiment analysis of tweets, and we refer the reader to surveys (Pang and Lee, 2008; Liu and Zhang, 2012; Mohammad, 2015) and proceedings of recent shared task competitions (Wilson et al., 2013; Mohammad et al., 2013; Rosenthal et al., 2015). See Pontiki et al. (2014), Pontiki et al. (2015), and Kiritchenko et al. (2014a) for tasks and systems on aspect based sentiment analysis, where the goal is to determine sentiment towards aspects of a product such as speed of processor and s"
S16-1003,S16-1061,0,0.0301153,"lt of 56.28 actually beating the best result for the supervised ‘Climate Change’ task (54.86). Team Baselines Majority class SVM-ngrams-comb Opinion Towards Target Other Participating Teams pkudblab LitisMind INF-UFRGS UWB ECNU USFD Thomson Reuters ltl.uni-due NEUSA All 35.20 31.39 25.52 20.13 29.72 28.43 67.19 51.60 50.04 50.62 40.66 38.87 38.06 34.16 28.86 25.77 29.50 22.66 25.02 19.14 22.80 22.60 4.69 18.35 56.28 44.66 42.32 42.02 34.08 32.70 32.39 26.14 25.73 in the system description papers published in the SemEval-2016 proceedings, including papers by Elfardy and Diab (2016) for CU-GWU, Dias and Becker (2016) for INF-URGS, Patra et al. (2016) for JU NLP, Wojatzki and Zesch (2016) for ltl.uni-due, Zarrella and Marsh (2016) for MITRE, Misra et al. (2016) for nldsucsc, Wei et al. (2016) for pkudblab, Tutek et al. (2016) for TakeLab, Yuki et al. (2016) for Tohoku, and Augenstein et al. (2016) for USFD. 7 Table 5: Results for Task B (the official competition metric Favg ) on different subsets of the test data. The highest score in each column is shown in bold. Table 5 shows results for Task B on subsets of the test set where opinion is expressed towards the target of interest and towards some other ent"
S16-1003,S16-1070,0,0.0161479,"ht have expected, with the best result of 56.28 actually beating the best result for the supervised ‘Climate Change’ task (54.86). Team Baselines Majority class SVM-ngrams-comb Opinion Towards Target Other Participating Teams pkudblab LitisMind INF-UFRGS UWB ECNU USFD Thomson Reuters ltl.uni-due NEUSA All 35.20 31.39 25.52 20.13 29.72 28.43 67.19 51.60 50.04 50.62 40.66 38.87 38.06 34.16 28.86 25.77 29.50 22.66 25.02 19.14 22.80 22.60 4.69 18.35 56.28 44.66 42.32 42.02 34.08 32.70 32.39 26.14 25.73 in the system description papers published in the SemEval-2016 proceedings, including papers by Elfardy and Diab (2016) for CU-GWU, Dias and Becker (2016) for INF-URGS, Patra et al. (2016) for JU NLP, Wojatzki and Zesch (2016) for ltl.uni-due, Zarrella and Marsh (2016) for MITRE, Misra et al. (2016) for nldsucsc, Wei et al. (2016) for pkudblab, Tutek et al. (2016) for TakeLab, Yuki et al. (2016) for Tohoku, and Augenstein et al. (2016) for USFD. 7 Table 5: Results for Task B (the official competition metric Favg ) on different subsets of the test data. The highest score in each column is shown in bold. Table 5 shows results for Task B on subsets of the test set where opinion is expressed towards the target of"
S16-1003,S14-2076,1,0.512441,"s these results. It also shows results on the complete test set (All), for easy reference. Observe that the stance task is markedly more difficult when stance is to be inferred from a tweet expressing opinion about some other entity (and not the target of interest). This is not surprising because it is a more challenging task, and because there has been very little work on this in the past. 5.3 Discussion Most teams used standard text classification features such as n-grams and word embedding vectors, as well as standard sentiment analysis features such as those drawn from sentiment lexicons (Kiritchenko et al., 2014b). Some teams polled Twitter for stancebearing hashtags, creating additional noisy stance data. Three teams tried variants of this strategy: MITRE, DeepStance and nldsucsc. These teams are distributed somewhat evenly throughout the standings, and although MITRE did use extra data in its top-placing entry, pkudblab achieved nearly the same score with only the provided data. Another possible differentiator would be the use of continuous word representations, derived either from extremely large sources such as Google News, directly from Twitter corpora, or as a by-product of training a neural ne"
S16-1003,S16-1068,0,0.0138727,"Towards Target Other Participating Teams pkudblab LitisMind INF-UFRGS UWB ECNU USFD Thomson Reuters ltl.uni-due NEUSA All 35.20 31.39 25.52 20.13 29.72 28.43 67.19 51.60 50.04 50.62 40.66 38.87 38.06 34.16 28.86 25.77 29.50 22.66 25.02 19.14 22.80 22.60 4.69 18.35 56.28 44.66 42.32 42.02 34.08 32.70 32.39 26.14 25.73 in the system description papers published in the SemEval-2016 proceedings, including papers by Elfardy and Diab (2016) for CU-GWU, Dias and Becker (2016) for INF-URGS, Patra et al. (2016) for JU NLP, Wojatzki and Zesch (2016) for ltl.uni-due, Zarrella and Marsh (2016) for MITRE, Misra et al. (2016) for nldsucsc, Wei et al. (2016) for pkudblab, Tutek et al. (2016) for TakeLab, Yuki et al. (2016) for Tohoku, and Augenstein et al. (2016) for USFD. 7 Table 5: Results for Task B (the official competition metric Favg ) on different subsets of the test data. The highest score in each column is shown in bold. Table 5 shows results for Task B on subsets of the test set where opinion is expressed towards the target of interest and towards some other entity. Observe that here too results are markedly lower when stance is to be inferred from a tweet expressing opinion about some other entity (and n"
S16-1003,W10-0204,1,0.110514,"ier. Nine of the nineteen entries used some form of word embedding, including the top three entries, but PKULCWM’s fourth place result shows that it is possible to do well with a more traditional approach that relies instead on Twitter-specific linguistic pre-processing. Along these lines, it is worth noting that both MITRE and pkudblab reflect knowledge-light approaches to the problem, each relying minimally on linguistic processing and external lexicons. Seven of the nineteen submissions made extensive use of publicly-available sentiment and emotion lexicons such as the NRC Emotion Lexicon (Mohammad and Turney, 2010), Hu and Liu Lexicon (Hu and Liu, 2004), MPQA Subjectivity Lexicon (Wilson et al., 2005), and NRC Hashtag Lexicons (Kiritchenko et al., 2014b). Recall that the SVM-ngrams baseline also performed very well, using only word and character ngrams in its classifiers. This helps emphasize the fact that for this young task, the community is still a long way from an established set of best practices. 6 Systems and Results for Task B The sub-sections below discuss baselines and official submissions to Task B. Recall, that the test data for Task B is for the target ‘Donald Trump’, and no training data f"
S16-1003,S13-2053,1,0.777604,"task. Further details on the submissions can be found 39 Related Work Past work on stance detection includes that by Somasundaran and Wiebe (2010), Anand et al. (2011), Faulkner (2014), Rajadesingan and Liu ˇ (2014), Djemili et al. (2014), Boltuzic and Snajder (2014), Conrad et al. (2012), Sridhar et al. (2014), Rajadesingan and Liu (2014), and Sobhani et al. (2015). There is a vast amount of work in sentiment analysis of tweets, and we refer the reader to surveys (Pang and Lee, 2008; Liu and Zhang, 2012; Mohammad, 2015) and proceedings of recent shared task competitions (Wilson et al., 2013; Mohammad et al., 2013; Rosenthal et al., 2015). See Pontiki et al. (2014), Pontiki et al. (2015), and Kiritchenko et al. (2014a) for tasks and systems on aspect based sentiment analysis, where the goal is to determine sentiment towards aspects of a product such as speed of processor and screen resolution of a cell phone. 8 Conclusions and Future Work We described a new shared task on detecting stance towards pre-chosen targets of interest from tweets. We formulated two tasks: a traditional supervised task where labeled training data for the test data targets is made available (Task A) and a more challenging formul"
S16-1003,L16-1623,1,0.884636,"d submissions from 9 teams wherein the highest classification F-score obtained was 56.28. The best performing systems used standard text classification features such as those drawn from n-grams, word vectors, and sentiment lexicons. Some teams drew additional gains from noisy stance-labeled data created using distant supervision techniques. A large number of teams used word embeddings and some used deep neural networks such as RNNs and convolutional neural nets. Nonetheless, for Task A, none of these systems surpassed a baseline SVM classifier that uses word and character n-grams as features (Mohammad et al., 2016b). Further, results are markedly worse for instances where the target of interest is not the target of opinion. More gains can be expected in the future on both tasks, as researchers better understand this new task and data. All of the data, an interactive visualization of the data, and the evaluation scripts are available on the task website as well as the homepage for this Stance project.2 2 Subtleties of Stance Detection In the sub-sections below we discuss some of the nuances of stance detection, including a discussion on neutral stance and the relationship between stance and sentiment. 2"
S16-1003,S16-1071,0,0.0205908,"result for the supervised ‘Climate Change’ task (54.86). Team Baselines Majority class SVM-ngrams-comb Opinion Towards Target Other Participating Teams pkudblab LitisMind INF-UFRGS UWB ECNU USFD Thomson Reuters ltl.uni-due NEUSA All 35.20 31.39 25.52 20.13 29.72 28.43 67.19 51.60 50.04 50.62 40.66 38.87 38.06 34.16 28.86 25.77 29.50 22.66 25.02 19.14 22.80 22.60 4.69 18.35 56.28 44.66 42.32 42.02 34.08 32.70 32.39 26.14 25.73 in the system description papers published in the SemEval-2016 proceedings, including papers by Elfardy and Diab (2016) for CU-GWU, Dias and Becker (2016) for INF-URGS, Patra et al. (2016) for JU NLP, Wojatzki and Zesch (2016) for ltl.uni-due, Zarrella and Marsh (2016) for MITRE, Misra et al. (2016) for nldsucsc, Wei et al. (2016) for pkudblab, Tutek et al. (2016) for TakeLab, Yuki et al. (2016) for Tohoku, and Augenstein et al. (2016) for USFD. 7 Table 5: Results for Task B (the official competition metric Favg ) on different subsets of the test data. The highest score in each column is shown in bold. Table 5 shows results for Task B on subsets of the test set where opinion is expressed towards the target of interest and towards some other entity. Observe that here too results"
S16-1003,S14-2004,0,0.0916108,"d 39 Related Work Past work on stance detection includes that by Somasundaran and Wiebe (2010), Anand et al. (2011), Faulkner (2014), Rajadesingan and Liu ˇ (2014), Djemili et al. (2014), Boltuzic and Snajder (2014), Conrad et al. (2012), Sridhar et al. (2014), Rajadesingan and Liu (2014), and Sobhani et al. (2015). There is a vast amount of work in sentiment analysis of tweets, and we refer the reader to surveys (Pang and Lee, 2008; Liu and Zhang, 2012; Mohammad, 2015) and proceedings of recent shared task competitions (Wilson et al., 2013; Mohammad et al., 2013; Rosenthal et al., 2015). See Pontiki et al. (2014), Pontiki et al. (2015), and Kiritchenko et al. (2014a) for tasks and systems on aspect based sentiment analysis, where the goal is to determine sentiment towards aspects of a product such as speed of processor and screen resolution of a cell phone. 8 Conclusions and Future Work We described a new shared task on detecting stance towards pre-chosen targets of interest from tweets. We formulated two tasks: a traditional supervised task where labeled training data for the test data targets is made available (Task A) and a more challenging formulation where no labeled data pertaining to the test d"
S16-1003,S15-2082,0,0.099003,"Missing"
S16-1003,S15-2078,1,0.755235,"n the submissions can be found 39 Related Work Past work on stance detection includes that by Somasundaran and Wiebe (2010), Anand et al. (2011), Faulkner (2014), Rajadesingan and Liu ˇ (2014), Djemili et al. (2014), Boltuzic and Snajder (2014), Conrad et al. (2012), Sridhar et al. (2014), Rajadesingan and Liu (2014), and Sobhani et al. (2015). There is a vast amount of work in sentiment analysis of tweets, and we refer the reader to surveys (Pang and Lee, 2008; Liu and Zhang, 2012; Mohammad, 2015) and proceedings of recent shared task competitions (Wilson et al., 2013; Mohammad et al., 2013; Rosenthal et al., 2015). See Pontiki et al. (2014), Pontiki et al. (2015), and Kiritchenko et al. (2014a) for tasks and systems on aspect based sentiment analysis, where the goal is to determine sentiment towards aspects of a product such as speed of processor and screen resolution of a cell phone. 8 Conclusions and Future Work We described a new shared task on detecting stance towards pre-chosen targets of interest from tweets. We formulated two tasks: a traditional supervised task where labeled training data for the test data targets is made available (Task A) and a more challenging formulation where no labeled da"
S16-1003,W15-0509,1,0.694399,"to see other teams attempting to generalize the supervised data from Task A in interesting ways, either using rules or multi-stage classifiers to bridge the target gap. We are optimistic that there is much interesting follow-up work yet to come on this task. Further details on the submissions can be found 39 Related Work Past work on stance detection includes that by Somasundaran and Wiebe (2010), Anand et al. (2011), Faulkner (2014), Rajadesingan and Liu ˇ (2014), Djemili et al. (2014), Boltuzic and Snajder (2014), Conrad et al. (2012), Sridhar et al. (2014), Rajadesingan and Liu (2014), and Sobhani et al. (2015). There is a vast amount of work in sentiment analysis of tweets, and we refer the reader to surveys (Pang and Lee, 2008; Liu and Zhang, 2012; Mohammad, 2015) and proceedings of recent shared task competitions (Wilson et al., 2013; Mohammad et al., 2013; Rosenthal et al., 2015). See Pontiki et al. (2014), Pontiki et al. (2015), and Kiritchenko et al. (2014a) for tasks and systems on aspect based sentiment analysis, where the goal is to determine sentiment towards aspects of a product such as speed of processor and screen resolution of a cell phone. 8 Conclusions and Future Work We described a"
S16-1003,W10-0214,0,0.524808,"the approach of producing noisy labels, with pkudblab using keyword rules, LitisMind using hashtag rules on external data, and INF-UFRGS using a combination of rules and third-party sentiment classifiers. However, we were pleased to see other teams attempting to generalize the supervised data from Task A in interesting ways, either using rules or multi-stage classifiers to bridge the target gap. We are optimistic that there is much interesting follow-up work yet to come on this task. Further details on the submissions can be found 39 Related Work Past work on stance detection includes that by Somasundaran and Wiebe (2010), Anand et al. (2011), Faulkner (2014), Rajadesingan and Liu ˇ (2014), Djemili et al. (2014), Boltuzic and Snajder (2014), Conrad et al. (2012), Sridhar et al. (2014), Rajadesingan and Liu (2014), and Sobhani et al. (2015). There is a vast amount of work in sentiment analysis of tweets, and we refer the reader to surveys (Pang and Lee, 2008; Liu and Zhang, 2012; Mohammad, 2015) and proceedings of recent shared task competitions (Wilson et al., 2013; Mohammad et al., 2013; Rosenthal et al., 2015). See Pontiki et al. (2014), Pontiki et al. (2015), and Kiritchenko et al. (2014a) for tasks and sys"
S16-1003,W14-2715,0,0.0663582,"d-party sentiment classifiers. However, we were pleased to see other teams attempting to generalize the supervised data from Task A in interesting ways, either using rules or multi-stage classifiers to bridge the target gap. We are optimistic that there is much interesting follow-up work yet to come on this task. Further details on the submissions can be found 39 Related Work Past work on stance detection includes that by Somasundaran and Wiebe (2010), Anand et al. (2011), Faulkner (2014), Rajadesingan and Liu ˇ (2014), Djemili et al. (2014), Boltuzic and Snajder (2014), Conrad et al. (2012), Sridhar et al. (2014), Rajadesingan and Liu (2014), and Sobhani et al. (2015). There is a vast amount of work in sentiment analysis of tweets, and we refer the reader to surveys (Pang and Lee, 2008; Liu and Zhang, 2012; Mohammad, 2015) and proceedings of recent shared task competitions (Wilson et al., 2013; Mohammad et al., 2013; Rosenthal et al., 2015). See Pontiki et al. (2014), Pontiki et al. (2015), and Kiritchenko et al. (2014a) for tasks and systems on aspect based sentiment analysis, where the goal is to determine sentiment towards aspects of a product such as speed of processor and screen resolution of a c"
S16-1003,S16-1075,0,0.119664,"Missing"
S16-1003,S16-1062,0,0.246544,"Teams pkudblab LitisMind INF-UFRGS UWB ECNU USFD Thomson Reuters ltl.uni-due NEUSA All 35.20 31.39 25.52 20.13 29.72 28.43 67.19 51.60 50.04 50.62 40.66 38.87 38.06 34.16 28.86 25.77 29.50 22.66 25.02 19.14 22.80 22.60 4.69 18.35 56.28 44.66 42.32 42.02 34.08 32.70 32.39 26.14 25.73 in the system description papers published in the SemEval-2016 proceedings, including papers by Elfardy and Diab (2016) for CU-GWU, Dias and Becker (2016) for INF-URGS, Patra et al. (2016) for JU NLP, Wojatzki and Zesch (2016) for ltl.uni-due, Zarrella and Marsh (2016) for MITRE, Misra et al. (2016) for nldsucsc, Wei et al. (2016) for pkudblab, Tutek et al. (2016) for TakeLab, Yuki et al. (2016) for Tohoku, and Augenstein et al. (2016) for USFD. 7 Table 5: Results for Task B (the official competition metric Favg ) on different subsets of the test data. The highest score in each column is shown in bold. Table 5 shows results for Task B on subsets of the test set where opinion is expressed towards the target of interest and towards some other entity. Observe that here too results are markedly lower when stance is to be inferred from a tweet expressing opinion about some other entity (and not the target of interest). 6.3"
S16-1003,H05-1044,0,0.231407,"tries, but PKULCWM’s fourth place result shows that it is possible to do well with a more traditional approach that relies instead on Twitter-specific linguistic pre-processing. Along these lines, it is worth noting that both MITRE and pkudblab reflect knowledge-light approaches to the problem, each relying minimally on linguistic processing and external lexicons. Seven of the nineteen submissions made extensive use of publicly-available sentiment and emotion lexicons such as the NRC Emotion Lexicon (Mohammad and Turney, 2010), Hu and Liu Lexicon (Hu and Liu, 2004), MPQA Subjectivity Lexicon (Wilson et al., 2005), and NRC Hashtag Lexicons (Kiritchenko et al., 2014b). Recall that the SVM-ngrams baseline also performed very well, using only word and character ngrams in its classifiers. This helps emphasize the fact that for this young task, the community is still a long way from an established set of best practices. 6 Systems and Results for Task B The sub-sections below discuss baselines and official submissions to Task B. Recall, that the test data for Task B is for the target ‘Donald Trump’, and no training data for this target was provided. 6.1 Task B Baselines We calculated two baselines listed bel"
S16-1003,S13-2052,0,0.0609198,"Missing"
S16-1003,S16-1069,0,0.0184379,"ate Change’ task (54.86). Team Baselines Majority class SVM-ngrams-comb Opinion Towards Target Other Participating Teams pkudblab LitisMind INF-UFRGS UWB ECNU USFD Thomson Reuters ltl.uni-due NEUSA All 35.20 31.39 25.52 20.13 29.72 28.43 67.19 51.60 50.04 50.62 40.66 38.87 38.06 34.16 28.86 25.77 29.50 22.66 25.02 19.14 22.80 22.60 4.69 18.35 56.28 44.66 42.32 42.02 34.08 32.70 32.39 26.14 25.73 in the system description papers published in the SemEval-2016 proceedings, including papers by Elfardy and Diab (2016) for CU-GWU, Dias and Becker (2016) for INF-URGS, Patra et al. (2016) for JU NLP, Wojatzki and Zesch (2016) for ltl.uni-due, Zarrella and Marsh (2016) for MITRE, Misra et al. (2016) for nldsucsc, Wei et al. (2016) for pkudblab, Tutek et al. (2016) for TakeLab, Yuki et al. (2016) for Tohoku, and Augenstein et al. (2016) for USFD. 7 Table 5: Results for Task B (the official competition metric Favg ) on different subsets of the test data. The highest score in each column is shown in bold. Table 5 shows results for Task B on subsets of the test set where opinion is expressed towards the target of interest and towards some other entity. Observe that here too results are markedly lower when stance is to"
S16-1003,S16-1065,0,0.0288745,"ltl.uni-due NEUSA All 35.20 31.39 25.52 20.13 29.72 28.43 67.19 51.60 50.04 50.62 40.66 38.87 38.06 34.16 28.86 25.77 29.50 22.66 25.02 19.14 22.80 22.60 4.69 18.35 56.28 44.66 42.32 42.02 34.08 32.70 32.39 26.14 25.73 in the system description papers published in the SemEval-2016 proceedings, including papers by Elfardy and Diab (2016) for CU-GWU, Dias and Becker (2016) for INF-URGS, Patra et al. (2016) for JU NLP, Wojatzki and Zesch (2016) for ltl.uni-due, Zarrella and Marsh (2016) for MITRE, Misra et al. (2016) for nldsucsc, Wei et al. (2016) for pkudblab, Tutek et al. (2016) for TakeLab, Yuki et al. (2016) for Tohoku, and Augenstein et al. (2016) for USFD. 7 Table 5: Results for Task B (the official competition metric Favg ) on different subsets of the test data. The highest score in each column is shown in bold. Table 5 shows results for Task B on subsets of the test set where opinion is expressed towards the target of interest and towards some other entity. Observe that here too results are markedly lower when stance is to be inferred from a tweet expressing opinion about some other entity (and not the target of interest). 6.3 Discussion Some teams did very well detecting tweets in favor of T"
S16-1003,S16-1074,0,0.155861,"jority class SVM-ngrams-comb Opinion Towards Target Other Participating Teams pkudblab LitisMind INF-UFRGS UWB ECNU USFD Thomson Reuters ltl.uni-due NEUSA All 35.20 31.39 25.52 20.13 29.72 28.43 67.19 51.60 50.04 50.62 40.66 38.87 38.06 34.16 28.86 25.77 29.50 22.66 25.02 19.14 22.80 22.60 4.69 18.35 56.28 44.66 42.32 42.02 34.08 32.70 32.39 26.14 25.73 in the system description papers published in the SemEval-2016 proceedings, including papers by Elfardy and Diab (2016) for CU-GWU, Dias and Becker (2016) for INF-URGS, Patra et al. (2016) for JU NLP, Wojatzki and Zesch (2016) for ltl.uni-due, Zarrella and Marsh (2016) for MITRE, Misra et al. (2016) for nldsucsc, Wei et al. (2016) for pkudblab, Tutek et al. (2016) for TakeLab, Yuki et al. (2016) for Tohoku, and Augenstein et al. (2016) for USFD. 7 Table 5: Results for Task B (the official competition metric Favg ) on different subsets of the test data. The highest score in each column is shown in bold. Table 5 shows results for Task B on subsets of the test set where opinion is expressed towards the target of interest and towards some other entity. Observe that here too results are markedly lower when stance is to be inferred from a tweet expressing opinion"
W03-1718,J96-1002,0,0.00689166,"ork to integrate various features from different knowledge sources. Each feature is typically represented as a binary constraint f. All features are then combined using a log-linear model shown in Eq. 5. Pλ ( y |x ) = 1 exp( Z λ ( x) λ i f i ( x , y )) i (5) where i is a weight of the feature fi , and Z(x) is a normalization factor. Weights ( ) are estimated using the maximum entropy principle: to satisfy constraints on observed data and assume a uniform distribution (with the maximum entropy) on unseen data. The training algorithm we used is the improved iterative scaling (IIS) described in (Berger et al, 1996)3. The context features include six characters: three on the left of the SCNE, and three on the right. Given the context features, the ME classifier would estimate the probability of the candidate being a SCNE. In our example, we treat candidates with the probability larger than 0.5 as SCNEs. To get the precision-recall curve, we can vary the probability threshold from 0.1 to 0.9. 4.2 Vector Space Model VSM is another model we used to detect SCNE. Similar to ME, we use six surrounding characters as the features, as shown in Figure 2. Figure 2. Context window In this approach, we apply the stan"
W03-1718,P03-1035,1,0.899904,"Missing"
W03-1718,C02-1054,0,0.0644216,"Missing"
W03-1718,C02-1012,1,0.900718,"Missing"
W03-1718,M98-1018,0,\N,Missing
W03-1718,P02-1060,0,\N,Missing
W03-1718,M98-1017,0,\N,Missing
W03-1718,P02-1062,0,\N,Missing
W08-0403,J07-2003,0,0.270841,"dels that utilize structures defined over linguistic theory and annotations (e.g., Penn Treebank), and SCFG rules are derived from parallel corpus that is guided by explicitly parsing on at least one side of the parallel corpus. Examples among others are (Yamada and Knight, 2001) and (Galley et al., 2004). • Formally syntax-based: models are based on hierarchical structures of natural language but synchronous grammars are automatically extracted from parallel corpus without any usage of linguistic knowledge or annotations. Examples include Wu’s (Wu, 1997) ITG and Chiang’s hierarchical models (Chiang, 2007). 1 Introduction In recent years, syntax-based translation models (Chiang, 2007; Galley et al., 2004; Liu et al., 2006) have shown promising progress in improving translation quality. There are two major elements accounting for such an improvement: namely the incorporation of phrasal translation structures adopted from widely applied phrase-based models (Och and Ney, 2004) to handle local fluency, and the engagement of synchronous context-free grammars (SCFG), which enhances the generative capacity of the underlying model that is limited by finite-state machinery. Approaches to syntax-based tr"
W08-0403,P05-1066,0,0.0614724,"Missing"
W08-0403,N04-1035,0,0.201986,"improved models based on tree kernel methods. Experiments on an English-to-Chinese task demonstrate that our proposed models outperformed the baseline formally syntaxbased models, while both of them achieved significant improvements over a state-of-theart phrase-based SMT system. • Linguistically syntax-based: models that utilize structures defined over linguistic theory and annotations (e.g., Penn Treebank), and SCFG rules are derived from parallel corpus that is guided by explicitly parsing on at least one side of the parallel corpus. Examples among others are (Yamada and Knight, 2001) and (Galley et al., 2004). • Formally syntax-based: models are based on hierarchical structures of natural language but synchronous grammars are automatically extracted from parallel corpus without any usage of linguistic knowledge or annotations. Examples include Wu’s (Wu, 1997) ITG and Chiang’s hierarchical models (Chiang, 2007). 1 Introduction In recent years, syntax-based translation models (Chiang, 2007; Galley et al., 2004; Liu et al., 2006) have shown promising progress in improving translation quality. There are two major elements accounting for such an improvement: namely the incorporation of phrasal translat"
W08-0403,P06-1121,0,0.210961,"oding. Recent work by (Zhang et al., 2006) shows a practically efficient approach that binarizes linguistically SCFG rules when possible. 19 Proceedings of the Second ACL Workshop on Syntax and Structure in Statistical Translation (SSST-2), pages 19–27, c ACL-08: HLT, Columbus, Ohio, USA, June 2008. 2008 Association for Computational Linguistics language model, which is a key element to ensure translation output quality. On the other hand, available linguistic theory and annotations could provide invaluable benefits in grammar induction and scoring, as shown by recent progress on such models (Galley et al., 2006). In contrast, formally syntax-based grammars often lack explicit linguistic constraints. In this paper, we propose a scheme to enrich formally syntax-based models with linguistically syntactic knowledge. In other words, we maintain our grammar to be based on formal syntax on surface, but incorporate linguistic knowledge into our models to leverage syntax theory and annotations. Our goal is two-fold. First, how to score SCFG rules whose general abstraction forms are unseen in the training data is an important question to answer. In hierarchical models, Chiang (Chiang, 2007) utilizes heuristics"
W08-0403,P03-1011,0,0.0217745,"tion. The remainder of the paper is organized as follows. We start with a brief review of some related work in Sec. 2. In Sec. 3, we describe our formally syntax-based models and decoder implementation, that is established as our baseline system. Sec. 4 presents the approach to score formal SCFG rules using kernel methods. Experimental results are provided in Sec. 5. Finally, Sec. 6 summarized our contributions with discussions and future work. 2 Related Work Syntax-based translation models engaged with SCFG have been actively investigated in the literature (Wu, 1997; Yamada and Knight, 2001; Gildea, 2003; Galley et al., 2004; Satta and Peserico, 2005). Recent work by (Chiang, 2007; Galley et al., 2006) shows promising improvements compared to phrase-based models for large-scale tasks. However, few previous work directly applied linguistically syntactic information into a formally syntaxbased models, which is explored in this paper. Kernel methods leverage the fact that the only operation in a procedure is the evaluation of inner dot products between pairs of observations, where the inner product is thus replaced with a Mercer kernel that provides an efficient way to carry out computation when"
W08-0403,N03-1017,0,0.0318403,"n source and target sides are identical). By “optimal”, it indicates that the derivation D maximizes following log-linear models over all possible derivations: P (D) ∝ PLM (e)λLM × Q Q λi i X→<γ,α>∈D φi (X →< γ, α >) , (4) where the set of φi (X →< γ, α >) are features defined over given production rule, and P LM (e) is the language model score on hypothesized output, the λi is the feature weight. Our baseline model follows Chiang’s hierarchical model (Chiang, 2007) in conjunction with additional features: • conditional probabilities in both directions: P (γ|α) and P (α|γ); • lexical weights (Koehn et al., 2003) in both directions: Pw (γ|α) and Pw (α|γ); • word counts |e|; • rule counts |D|; • target n-gram language model PLM (e); • glue rule penalty to learn preference of nonterminal rewriting over serial combination through Eq. 3; Moreover, we propose an additional feature, namely the abstraction penalty, to account for the accumulated number of nonterminals applied in D: • abstraction penalty exp(−Na ), where Na = P X→<γ,α>∈D n(γ) where n(γ) is the number of nonterminals in γ. This feature aims to learn the preference among phrasal rules, and abstract rules with one or two nonterminals. This makes"
W08-0403,P06-1077,0,0.0443672,"re derived from parallel corpus that is guided by explicitly parsing on at least one side of the parallel corpus. Examples among others are (Yamada and Knight, 2001) and (Galley et al., 2004). • Formally syntax-based: models are based on hierarchical structures of natural language but synchronous grammars are automatically extracted from parallel corpus without any usage of linguistic knowledge or annotations. Examples include Wu’s (Wu, 1997) ITG and Chiang’s hierarchical models (Chiang, 2007). 1 Introduction In recent years, syntax-based translation models (Chiang, 2007; Galley et al., 2004; Liu et al., 2006) have shown promising progress in improving translation quality. There are two major elements accounting for such an improvement: namely the incorporation of phrasal translation structures adopted from widely applied phrase-based models (Och and Ney, 2004) to handle local fluency, and the engagement of synchronous context-free grammars (SCFG), which enhances the generative capacity of the underlying model that is limited by finite-state machinery. Approaches to syntax-based translation models using SCFG can be further categorized into two classes, based on their dependency on annotated corWhil"
W08-0403,E06-1015,0,0.201305,"y, we propose a linguisticallymotivated method to train prior derivation models for formally syntax-based translation. In this framework, prior derivation models can be viewed as a smoothing of rule translation models, addressing the weakness of the baseline model estimation that relies on relative counts obtained from heuristics. First, we apply automatic parsers to obtain syntax annotations on the English side of the parallel corpus. Next, we extract tree fragments associated with phrase pairs, and measure similarity between such tree fragments using kernel methods (Collins and Duffy, 2002; Moschitti, 2006). Finally, we score 20 and rank rules based on their minimal cluster similarity of their nonterminals, which is used to compute the prior distribution of hypothesis derivations during decoding for improved translation. The remainder of the paper is organized as follows. We start with a brief review of some related work in Sec. 2. In Sec. 3, we describe our formally syntax-based models and decoder implementation, that is established as our baseline system. Sec. 4 presents the approach to score formal SCFG rules using kernel methods. Experimental results are provided in Sec. 5. Finally, Sec. 6 s"
W08-0403,P00-1056,0,0.208906,"rminal symbol X in the grammar, X → hγ, α, ∼i, (1) where ∼ is the one-to-one correspondence between X’s in γ and α, which is indicated by underscripted co-indices on both sides. For example, some English-to-Chinese production rules can be represented as follows: X → hX1 enjoy readingX2 , (2) X1 xihuan(enjoy) yuedu(reading)X2 i X → hX1 enjoy readingX2 , X1 xihuan(enjoy)X2 yuedu(reading)i The set of rules, denoted as R, are automatically extracted from sentence-aligned parallel corpus (Chiang, 2007). First, bidirectional word-level alignment is carried out on the parallel corpus running GIZA++ (Och and Ney, 2000). Based on the resulting Viterbi alignments Ae2f and Af 2e , the union, AU = Ae2f ∪ Af 2e , is taken as the symmetrized word-level alignment. Next, bilingual phrase pairs consistent with word alignments are extracted from AU (Och and Ney, 2004). Specifically, any pair of consecutive sequences of words below a maximum length M is considered to be a phrase pair if its component words are aligned only within the phrase pair and not to any words outside. The resulting bilingual phrase pair inventory is denoted as BP. Each phrase pair PP ∈ BP is represented as a production rule X → hfij , elk i, wh"
W08-0403,J04-4002,0,0.55546,"ctures of natural language but synchronous grammars are automatically extracted from parallel corpus without any usage of linguistic knowledge or annotations. Examples include Wu’s (Wu, 1997) ITG and Chiang’s hierarchical models (Chiang, 2007). 1 Introduction In recent years, syntax-based translation models (Chiang, 2007; Galley et al., 2004; Liu et al., 2006) have shown promising progress in improving translation quality. There are two major elements accounting for such an improvement: namely the incorporation of phrasal translation structures adopted from widely applied phrase-based models (Och and Ney, 2004) to handle local fluency, and the engagement of synchronous context-free grammars (SCFG), which enhances the generative capacity of the underlying model that is limited by finite-state machinery. Approaches to syntax-based translation models using SCFG can be further categorized into two classes, based on their dependency on annotated corWhile these two often resemble in appearance, from practical viewpoints, there are some distinctions in training and decoding procedures differentiating formally syntax-based models from linguistically syntax-based models. First, the former has no dependency o"
W08-0403,P03-1021,0,0.0136676,"from transcription and human translation of conversations. The vocabulary size is 37K for English and 44K for Chinese after segmentation. Our evaluation data is a held out data set of 2755 sentences pairs. We extracted every one out of two sentence pairs into the dev-set, and left the remainder as the test-set. We thereby obtained a dev-set of 1378 sentence pairs, and a test-set with 1377 sentence pairs. In both cases, there are about 15K running words on English side. All Chinese sentences in training, dev and test sets are all automatically segmented into words. Minimum-error-rate training (Och, 2003) are conducted on dev-set to optimize feature weights maximizing the BLEU score up to 4grams, and the obtained feature weights are blindly applied on the test-set. To compare performances excluding tokenization effects, all BLEU scores are optimized (on dev-set) and reported (on test-set) at Chinese character-level. From training data, we extracted an initial phrase pair set with 3.7M entries for phrases up to 8 words 25 on Chinese side. We trained a 4-gram language model for Chinese at word level, which is shared by all translation systems reported in this paper, using the Chinese side of the"
W08-0403,H05-1101,0,0.0123004,"organized as follows. We start with a brief review of some related work in Sec. 2. In Sec. 3, we describe our formally syntax-based models and decoder implementation, that is established as our baseline system. Sec. 4 presents the approach to score formal SCFG rules using kernel methods. Experimental results are provided in Sec. 5. Finally, Sec. 6 summarized our contributions with discussions and future work. 2 Related Work Syntax-based translation models engaged with SCFG have been actively investigated in the literature (Wu, 1997; Yamada and Knight, 2001; Gildea, 2003; Galley et al., 2004; Satta and Peserico, 2005). Recent work by (Chiang, 2007; Galley et al., 2006) shows promising improvements compared to phrase-based models for large-scale tasks. However, few previous work directly applied linguistically syntactic information into a formally syntaxbased models, which is explored in this paper. Kernel methods leverage the fact that the only operation in a procedure is the evaluation of inner dot products between pairs of observations, where the inner product is thus replaced with a Mercer kernel that provides an efficient way to carry out computation when original feature dimension is large or even inf"
W08-0403,J97-3002,0,0.344192,"ed SMT system. • Linguistically syntax-based: models that utilize structures defined over linguistic theory and annotations (e.g., Penn Treebank), and SCFG rules are derived from parallel corpus that is guided by explicitly parsing on at least one side of the parallel corpus. Examples among others are (Yamada and Knight, 2001) and (Galley et al., 2004). • Formally syntax-based: models are based on hierarchical structures of natural language but synchronous grammars are automatically extracted from parallel corpus without any usage of linguistic knowledge or annotations. Examples include Wu’s (Wu, 1997) ITG and Chiang’s hierarchical models (Chiang, 2007). 1 Introduction In recent years, syntax-based translation models (Chiang, 2007; Galley et al., 2004; Liu et al., 2006) have shown promising progress in improving translation quality. There are two major elements accounting for such an improvement: namely the incorporation of phrasal translation structures adopted from widely applied phrase-based models (Och and Ney, 2004) to handle local fluency, and the engagement of synchronous context-free grammars (SCFG), which enhances the generative capacity of the underlying model that is limited by f"
W08-0403,P01-1067,0,0.321147,"ing algorithm to achieve such improved models based on tree kernel methods. Experiments on an English-to-Chinese task demonstrate that our proposed models outperformed the baseline formally syntaxbased models, while both of them achieved significant improvements over a state-of-theart phrase-based SMT system. • Linguistically syntax-based: models that utilize structures defined over linguistic theory and annotations (e.g., Penn Treebank), and SCFG rules are derived from parallel corpus that is guided by explicitly parsing on at least one side of the parallel corpus. Examples among others are (Yamada and Knight, 2001) and (Galley et al., 2004). • Formally syntax-based: models are based on hierarchical structures of natural language but synchronous grammars are automatically extracted from parallel corpus without any usage of linguistic knowledge or annotations. Examples include Wu’s (Wu, 1997) ITG and Chiang’s hierarchical models (Chiang, 2007). 1 Introduction In recent years, syntax-based translation models (Chiang, 2007; Galley et al., 2004; Liu et al., 2006) have shown promising progress in improving translation quality. There are two major elements accounting for such an improvement: namely the incorpo"
W08-0403,N06-1033,0,0.0196192,"s in training and decoding procedures differentiating formally syntax-based models from linguistically syntax-based models. First, the former has no dependency on available linguistic theory and annotations for targeting language pairs, and thus the training and rule extraction are more efficient. Secondly, the decoding complexity of the former is lower 1 , especially when integrating a n-gram based 1 The complexity is dominated by synchronous parsing and boundary words keeping. Thus binary SCFG employed in formally syntax-based systems help to maintain efficient CKY decoding. Recent work by (Zhang et al., 2006) shows a practically efficient approach that binarizes linguistically SCFG rules when possible. 19 Proceedings of the Second ACL Workshop on Syntax and Structure in Statistical Translation (SSST-2), pages 19–27, c ACL-08: HLT, Columbus, Ohio, USA, June 2008. 2008 Association for Computational Linguistics language model, which is a key element to ensure translation output quality. On the other hand, available linguistic theory and annotations could provide invaluable benefits in grammar induction and scoring, as shown by recent progress on such models (Galley et al., 2006). In contrast, formall"
W11-0324,W05-0602,0,0.0234305,"cordings, with the sequential transcripts of the corresponding spoken documents, with the aim to help index and access the latter. This model optimizes a normalizedcut graph-partitioning criterion and considers local tree constraints at the same time. The experimental results show the advantage of this model over Viterbi-like, sequential alignment, under typical speech recognition errors. 1 Introduction Learning semantic structures of written text has been studied in a number of specific tasks, which include, but not limited to, those finding semantic representations for individual sentences (Ge and Mooney, 2005; Zettlemoyer and Collins, 2005; Lu et al., 2008), and those constructing hierarchical structures among sentences or larger text blocks (Marcu, 2000; Branavan et al., 2007). The inverse problem of the latter kind, e.g., aligning certain form of alreadyexisting semantic hierarchies with the corresponding text sequence, is not so much a prominent problem for written text as it is for spoken documents. In this paper, we study a specific type of such a problem, in which a hierarchical browsing structure, i.e., electronic slides of oral presentations, have already existed, the goal being to impose"
W11-0324,W06-1644,0,0.0409124,"Missing"
W11-0324,D08-1082,0,0.0228692,"responding spoken documents, with the aim to help index and access the latter. This model optimizes a normalizedcut graph-partitioning criterion and considers local tree constraints at the same time. The experimental results show the advantage of this model over Viterbi-like, sequential alignment, under typical speech recognition errors. 1 Introduction Learning semantic structures of written text has been studied in a number of specific tasks, which include, but not limited to, those finding semantic representations for individual sentences (Ge and Mooney, 2005; Zettlemoyer and Collins, 2005; Lu et al., 2008), and those constructing hierarchical structures among sentences or larger text blocks (Marcu, 2000; Branavan et al., 2007). The inverse problem of the latter kind, e.g., aligning certain form of alreadyexisting semantic hierarchies with the corresponding text sequence, is not so much a prominent problem for written text as it is for spoken documents. In this paper, we study a specific type of such a problem, in which a hierarchical browsing structure, i.e., electronic slides of oral presentations, have already existed, the goal being to impose such a structure onto the transcripts of the corr"
W11-0324,P06-1004,0,0.310212,"arent bullet’s information onto its children (Zhu et al., 2010). On the other hand, we should also note that the benefit of formulating the problem as a sequential alignment problem is its computational efficiency: the solution can be calculated with conventional Viterbi-like algorithms. This property is also important for the task, since the length of a spoken document, such as a lecture, is often long enough to make inefficient algorithms practically intractable. An important question is therefore how to, in principle, model the problem better. The second is how time efficient the model is. Malioutov and Barzilay (2006) describe a dynamic-programming version of a normalized-cut-based model in solving a topic segmentation problem for spoken documents. Inspired by their work, we will propose a model based on graph partitioning in finding the correspondence between bullets and the regions of transcripts that discuss them; the proposed model runs in polynomial time. We will empirically show its benefit on both improving the alignment performance over a sequential alignment and its robustness to speech recognition errors. 3 Problem We are given a speech sequence U = u1 , u2 , ..., uN , where ui is an utterance, a"
W11-0324,P07-1064,0,0.179961,"malized-cut criterion globally, in traversing the given hierarchical semantic structures. The experimental results show the advantage of this model over Viterbi-like, sequential alignment, under typical speech recognition errors. 2 Related work Flat structures of spoken documents Much previous work, similar to its written-text counterpart, has attempted to find certain flat structures of spoken documents, such as topic and slide boundaries. For example, the work of (Chen and Heng, 2003; Ruddarraju, 2006; Zhu et al., 2008) aims to find slide boundaries in the corresponding lecture transcripts. Malioutov et al. (2007) developed an approach to detecting topic boundaries of lecture recordings by finding repeated acoustic patterns. None of this work, however, has involved hierarchical structures of a spoken document. Research has also resorted to other multimedia channels, e.g., video (Liu et al., 2002; Wang et al., 2003; Fan et al., 2006), to detect slide transitions. This type of research, however, is unlikely to recover semantic structures in more details than slide boundaries. Hierarchical structures of spoken documents Recently, research has started to align hierarchical browsing structures with spoken d"
W11-0324,J02-1002,0,0.0292666,"aightforward—automatically acquired boundaries on transcripts for each slide bullet are compared against the corresponding gold-standard boundaries to calculate offsets measured in number of words. The offset scores are averaged over all boundaries to evaluate model performance. Though one may consider that different bullets may be of different importance, in this paper we do not use any heuristics to judge this and we treat all bullets equally in our evaluation. Note that topic segmentation research often uses metrics such as Pk and WindowDiff (Malioutov et al., 2007; Beeferman et al., 1999; Pevsner and Hearst, 2002). Our problem here, as an alignment problem, has an exact 1-to-1 correspondence between a gold and automatic boundary, in which we can directly measure the exact offset of each boundary. 6 Experimental results Table 1 presents the experimental results obtained on the automatic transcripts generated by the ASR models discussed above, with WERs at 0.43 and 0.48, respectively, which are typical WERs for lectures and conference presentations in realistic and less controlled situations. SEQ-ALN in the table stands for the Viterbi-like, sequential alignment discussed above in section 2, while G-CUT"
W11-0324,N10-1006,0,0.0126035,"tomatic speech recognition (ASR). Implicitly, solutions as such change the conventional speaking-for-hearing construals: now speech can be read through its transcripts, though, in most cases, it was not intended for this purpose, which in turn raises a new set of problems. The convenience and efficiency of reading transcripts (Stark et al., 2000; Munteanu et al., 2006) are first affected by errors produced in transcription channels for various reasons, though if the goal is only to browse salient excerpts, recognition errors on the extracts can be reduced by considering ASR confidence scores (Xie and Liu, 2010; Hori and Furui, 2003; Zechner and Waibel, 2000): trading off the expected salience of excerpts with their recognition-error rate could actually result in the improvement of excerpt quality in terms of the amount of important content being correctly presented (Zechner and Waibel, 2000). Even if transcription quality were not a problem, browsing transcripts is not straightforward. When intended to be read, written documents are almost always presented as more than uninterrupted strings of text. Consider that for many written documents, e.g., books, indicative structures such as section/subsect"
W11-0324,A00-2025,0,0.0151242,"tly, solutions as such change the conventional speaking-for-hearing construals: now speech can be read through its transcripts, though, in most cases, it was not intended for this purpose, which in turn raises a new set of problems. The convenience and efficiency of reading transcripts (Stark et al., 2000; Munteanu et al., 2006) are first affected by errors produced in transcription channels for various reasons, though if the goal is only to browse salient excerpts, recognition errors on the extracts can be reduced by considering ASR confidence scores (Xie and Liu, 2010; Hori and Furui, 2003; Zechner and Waibel, 2000): trading off the expected salience of excerpts with their recognition-error rate could actually result in the improvement of excerpt quality in terms of the amount of important content being correctly presented (Zechner and Waibel, 2000). Even if transcription quality were not a problem, browsing transcripts is not straightforward. When intended to be read, written documents are almost always presented as more than uninterrupted strings of text. Consider that for many written documents, e.g., books, indicative structures such as section/subsection headings and tables-of-contents are standard"
W11-0324,C10-2177,1,0.752245,"patterns. None of this work, however, has involved hierarchical structures of a spoken document. Research has also resorted to other multimedia channels, e.g., video (Liu et al., 2002; Wang et al., 2003; Fan et al., 2006), to detect slide transitions. This type of research, however, is unlikely to recover semantic structures in more details than slide boundaries. Hierarchical structures of spoken documents Recently, research has started to align hierarchical browsing structures with spoken documents, given that inferring such structures directly from spoken documents is still too challenging. Zhu et al. (2010) investigates bullet-slide alignment by first sequentializing bullet trees with a pre-order walk before conducting alignment, through which the problem is reduced to a string-to-string alignment problem and an efficient Viterbi-like method can be naturally applied. In this paper, we use such a sequential alignment as our baseline, which takes a standard dynamic-programming process to find the optimal path on an M-by-N similarity matrix, where M and N denote the number of bullets and utterances in a lecture, respectively. Specifically, we chose the path that maps each bullet to an utterance to"
W11-0324,P07-1069,0,\N,Missing
W12-2604,W04-1013,0,0.0324092,"s relative to using no summaries very favourably. 1 Background and Motivation Summarization maintains a representation of an entire spoken document, focusing on those utterances (sentence-like units) that are most important and therefore does not require the user to process everything that has been said. Our work focuses on extractive summarization where a selection of utterances is chosen from the original spoken document in order to make up a summary. Current speech summarization research has made extensive use of intrinsic evaluation measures such as F-measure, Relative Utility, and ROUGE (Lin, 2004), which score summaries against subjectively selected gold standard summaries obtained using human annotators. These annotators are asked to arbitrarily select (in or out) or rank utterances, and in doing so commit to relative salience judgements with no attention to goal orientation and no requirement to synthesize the meanings of larger units of structure into a coherent message. 28 Proceedings of the Workshop on Evaluation Metrics and System Comparison for Automatic Summarization, pages 28–35, c Montr´eal, Canada, June 3-8, 2012. 2012 Association for Computational Linguistics Given this sub"
W12-2604,W05-0905,0,0.0408008,"Missing"
W12-2604,P08-1054,1,0.728635,"on Evaluation Metrics and System Comparison for Automatic Summarization, pages 28–35, c Montr´eal, Canada, June 3-8, 2012. 2012 Association for Computational Linguistics Given this subjectivity, current intrinsic evaluation measures are unable to properly judge which summaries are useful for real-world applications. For example, intrinsic evaluations have failed to show that summaries created by algorithms based on complex linguistic and acoustic features are better than baseline summaries created by simply choosing the positionally first utterances or longest utterances in a spoken document (Penn and Zhu, 2008). What is needed is an ecologically valid evaluation that determines how valuable a summary is when embedded in a task, rather than how closely a summary matches the subjective utterance level scores assigned by annotators. Ecological validity is ""the ability of experiments to tell us how real people operate in the real world"" (Cohen, 1995). This is often obtained by using human judges, but it is important to realize that the mere use of human subjects provides no guarantee as to the ecological validity of their judgements. When utterances are merely ranked with numerical scores out of context"
W14-2607,P98-1013,0,0.207863,"compile a large dataset of tweets pertaining to the 2012 US presidential elections, and annotate it not only for emotion but also for the experiencer and the stimulus. We then develop a classifier for detecting emotion that obtains an accuracy of 56.84 on an eight-way classification task. Finally, we show how the stimulus identification task can also be framed as a classification task, obtaining an F-score of 58.30. 1 Detecting the who, what, and towards whom of emotions is essentially a semantic role-labeling problem (Gildea and Jurafsky, 2002). The semantic frame for ‘emotions’ in FrameNet (Baker et al., 1998) is shown in Table 1. In this work, we focus on the roles of Experiencer, State, and Stimulus. Note, however, that the state or emotion is often not explicitly present in text. Other roles such as Reason, Degree, and Event are also of significance, and remain suitable avenues for future work. Introduction Detecting emotions in text has a number of applications including tracking sentiment towards politicians, movies, and products (Pang and Lee, 2008), identifying what emotion a newspaper headline is trying to evoke (Bellegarda, 2010), developing more natural text-to-speech systems (Francisco a"
W14-2607,J02-3001,0,0.0626632,"d ‘towards whom is the emotion directed (the stimulus)?’. We automatically compile a large dataset of tweets pertaining to the 2012 US presidential elections, and annotate it not only for emotion but also for the experiencer and the stimulus. We then develop a classifier for detecting emotion that obtains an accuracy of 56.84 on an eight-way classification task. Finally, we show how the stimulus identification task can also be framed as a classification task, obtaining an F-score of 58.30. 1 Detecting the who, what, and towards whom of emotions is essentially a semantic role-labeling problem (Gildea and Jurafsky, 2002). The semantic frame for ‘emotions’ in FrameNet (Baker et al., 1998) is shown in Table 1. In this work, we focus on the roles of Experiencer, State, and Stimulus. Note, however, that the state or emotion is often not explicitly present in text. Other roles such as Reason, Degree, and Event are also of significance, and remain suitable avenues for future work. Introduction Detecting emotions in text has a number of applications including tracking sentiment towards politicians, movies, and products (Pang and Lee, 2008), identifying what emotion a newspaper headline is trying to evoke (Bellegarda"
W14-2607,W10-0201,0,0.00979264,"sky, 2002). The semantic frame for ‘emotions’ in FrameNet (Baker et al., 1998) is shown in Table 1. In this work, we focus on the roles of Experiencer, State, and Stimulus. Note, however, that the state or emotion is often not explicitly present in text. Other roles such as Reason, Degree, and Event are also of significance, and remain suitable avenues for future work. Introduction Detecting emotions in text has a number of applications including tracking sentiment towards politicians, movies, and products (Pang and Lee, 2008), identifying what emotion a newspaper headline is trying to evoke (Bellegarda, 2010), developing more natural text-to-speech systems (Francisco and Gerv´as, 2006), detecting how people use emotion-bearing-words and metaphors to persuade and coerce others (for example, in propaganda) (Kˇovecses, 2003), tracking response to natural disasters (Mandel et al., 2012), and so on. With the rapid proliferation of microblogging, there is growing amount of emotion analysis research on newly available datasets of Twitter posts (Mandel et al., 2012; Purver and Battersby, 2012; Mohammad, 2012b). However, past work has focused solely on detecting emotional state. It has ignored questions su"
W14-2607,D11-1052,0,0.0233667,"results that are greater than random and major5.1 Detecting emotional state 5.1.1 Features We included the following features for detecting emotional state in tweets. Word n-grams: We included unigrams (single words) and bigrams (two-word sequences) into our feature set. All words were stemmed with Porter’s stemmer (Porter, 1980). Punctuations: number of contiguous sequences of exclamation marks, question marks, or a combination of them. Elongated words: the number of words with the final character repeated 3 or more times (soooo, mannnnnn, etc). (Elongated words have been used similarly in (Brody and Diakopoulos, 2011).) Emoticons: presence/absence of positive and negative emoticons. The emoticon and its polarity were determined through a regular expression adopted from Christopher Potts’ tokenizing script.4 Emotion Lexicons: We used the NRC word– emotion association lexicon (Mohammad and Turney, 2010) to check if a tweet contains emotional words. The lexicon contains human annotations of emotion associations for about 14,200 word types. The annotation includes whether a word is positive or negative (sentiments), and whether it is associated with the eight basic emotions (joy, sadness, anger, fear, surprise"
W14-2607,W06-0301,0,0.0126049,"s of emotion. Semantic role labeling (SRL) identifies semantic arguments and roles with regard to a predicate 3 Challenges of Semantic Role Labeling of Emotions in Tweets Semantic role labeling of emotions in tweets poses certain unique challenges. Firstly, there are many differences between tweets and linguistically wellformed texts, such as written news (Liu et al., 2012; Ritter et al., 2011). Tweets are often less well-formed—they tend to be colloquial, have misspellings, and have non-standard tokens. Thus, methods depending heavily on deep language understanding such as syntactic parsing (Kim and Hovy, 2006) are less reliable. 2 http://www.purl.org/net/NRCEmotionLexicon http://csea.phhp.ufl.edu/media/anewmessage.html 33 Secondly, in a traditional SRL system, an argument frame is a cohesive structure with strong dependencies between the arguments. Thus it is often beneficial to develop joint models to identify the various elements of a frame (Toutanova et al., 2005). However, these assumptions are less viable when dealing with emotions in tweets. For example, there is no reason to believe that people with a certain name will have the same emotions towards the same entities. On the other hand, if w"
W14-2607,H05-1043,0,0.0274172,"is not covered by more specific frame elements. The Parameter is a domain in which the Experiencer experiences the Stimulus. The Reason is the explanation for why the Stimulus evokes a certain emotional response. Related Work in a sentence (Gildea and Jurafsky, 2002; M`arquez et al., 2008; Palmer et al., 2010). More recently, there has also been some work on semantic role labeling of tweets for verb and nominal predicates (Liu et al., 2012; Liu et al., 2011). There exists work on extracting opinions and the topics of opinions, however most of it if focused on opinions about product features (Popescu and Etzioni, 2005; Zhang et al., 2010; Kessler and Nicolov, 2009). For example, (Kessler and Nicolov, 2009) identifies semantic relations between sentiment expressions and their targets for car and digital-camera reviews. However, there is no work on semantic role labeling of emotions in tweets. We use many of the ideas developed in the sentiment analysis work and apply them to detect the stimulus of emotions in the electoral tweets data. Our work here is also related to template filling in information extraction (IE), for example as defined in MUC (Grishman, 1997), which extracts information (entities) from a"
W14-2607,E12-1049,0,0.0410302,"s politicians, movies, and products (Pang and Lee, 2008), identifying what emotion a newspaper headline is trying to evoke (Bellegarda, 2010), developing more natural text-to-speech systems (Francisco and Gerv´as, 2006), detecting how people use emotion-bearing-words and metaphors to persuade and coerce others (for example, in propaganda) (Kˇovecses, 2003), tracking response to natural disasters (Mandel et al., 2012), and so on. With the rapid proliferation of microblogging, there is growing amount of emotion analysis research on newly available datasets of Twitter posts (Mandel et al., 2012; Purver and Battersby, 2012; Mohammad, 2012b). However, past work has focused solely on detecting emotional state. It has ignored questions such as ‘who is feeling the emotion (the experiencer)?’ and ‘towards whom is the emotion directed (the stimulus)?’. In this paper, we present a system that analyzes tweets to determine who is feeling what emotion, We automatically compile a large dataset of 2012 US presidential elections using a small number of hand-chosen hashtags. Next we annotate the tweets for Experiencer, State, and Stimulus by crowdsourcing to Amazon’s Mechanical Turk.1 We analyze the annotations to determine"
W14-2607,D11-1141,0,0.0843212,"Missing"
W14-2607,strapparava-valitutti-2004-wordnet,0,0.108711,"ion, target, and other information about an event. Our work here is related to emotion analysis, semantic role labeling (SRL), and information extraction (IE). Much of the past work on emotion detection focuses on emotions argued to be the most basic. For example, Ekman (1992) proposed six basic emotions—joy, sadness, anger, fear, disgust, and surprise. Plutchik (1980) argued in favor of eight—Ekman’s six, surprise, and anticipation. Many of the automatic systems use affect lexicons pertaining to these basic emotions such as the NRC Emotion Lexicon (Mohammad and Turney, 2010), WordNet Affect (Strapparava and Valitutti, 2004), and the Affective Norms for English Words.2 Affect lexicons are lists of words and associated emotions. Emotion analysis techniques have been applied to many different kinds of text (Mihalcea and Liu, 2006; Genereux and Evans, 2006; Neviarouskaya et al., 2009; Mohammad, 2012a). More recently there has been work on tweets as well (Bollen et al., 2011; Tumasjan et al., 2010; Mohammad, 2012b). Bollen et al. (2011) measured tension, depression, anger, vigor, fatigue, and confusion in tweets. Tumasjan et al. (2010) study Twitter as a forum for political deliberation. Mohammad (2012b) developed a"
W14-2607,C08-1111,0,0.0446536,"Missing"
W14-2607,P05-1073,0,0.0565337,"Missing"
W14-2607,C10-2167,0,0.0477533,"Missing"
W14-2607,W12-2104,0,0.0388205,"eason, Degree, and Event are also of significance, and remain suitable avenues for future work. Introduction Detecting emotions in text has a number of applications including tracking sentiment towards politicians, movies, and products (Pang and Lee, 2008), identifying what emotion a newspaper headline is trying to evoke (Bellegarda, 2010), developing more natural text-to-speech systems (Francisco and Gerv´as, 2006), detecting how people use emotion-bearing-words and metaphors to persuade and coerce others (for example, in propaganda) (Kˇovecses, 2003), tracking response to natural disasters (Mandel et al., 2012), and so on. With the rapid proliferation of microblogging, there is growing amount of emotion analysis research on newly available datasets of Twitter posts (Mandel et al., 2012; Purver and Battersby, 2012; Mohammad, 2012b). However, past work has focused solely on detecting emotional state. It has ignored questions such as ‘who is feeling the emotion (the experiencer)?’ and ‘towards whom is the emotion directed (the stimulus)?’. In this paper, we present a system that analyzes tweets to determine who is feeling what emotion, We automatically compile a large dataset of 2012 US presidential el"
W14-2607,J08-2001,0,0.0260615,"Missing"
W14-2607,W10-0204,1,0.807595,"re-defined template, such as the date, location, target, and other information about an event. Our work here is related to emotion analysis, semantic role labeling (SRL), and information extraction (IE). Much of the past work on emotion detection focuses on emotions argued to be the most basic. For example, Ekman (1992) proposed six basic emotions—joy, sadness, anger, fear, disgust, and surprise. Plutchik (1980) argued in favor of eight—Ekman’s six, surprise, and anticipation. Many of the automatic systems use affect lexicons pertaining to these basic emotions such as the NRC Emotion Lexicon (Mohammad and Turney, 2010), WordNet Affect (Strapparava and Valitutti, 2004), and the Affective Norms for English Words.2 Affect lexicons are lists of words and associated emotions. Emotion analysis techniques have been applied to many different kinds of text (Mihalcea and Liu, 2006; Genereux and Evans, 2006; Neviarouskaya et al., 2009; Mohammad, 2012a). More recently there has been work on tweets as well (Bollen et al., 2011; Tumasjan et al., 2010; Mohammad, 2012b). Bollen et al. (2011) measured tension, depression, anger, vigor, fatigue, and confusion in tweets. Tumasjan et al. (2010) study Twitter as a forum for pol"
W14-2607,S13-2053,1,0.30783,"Missing"
W14-2607,N12-1071,1,0.948151,"roducts (Pang and Lee, 2008), identifying what emotion a newspaper headline is trying to evoke (Bellegarda, 2010), developing more natural text-to-speech systems (Francisco and Gerv´as, 2006), detecting how people use emotion-bearing-words and metaphors to persuade and coerce others (for example, in propaganda) (Kˇovecses, 2003), tracking response to natural disasters (Mandel et al., 2012), and so on. With the rapid proliferation of microblogging, there is growing amount of emotion analysis research on newly available datasets of Twitter posts (Mandel et al., 2012; Purver and Battersby, 2012; Mohammad, 2012b). However, past work has focused solely on detecting emotional state. It has ignored questions such as ‘who is feeling the emotion (the experiencer)?’ and ‘towards whom is the emotion directed (the stimulus)?’. In this paper, we present a system that analyzes tweets to determine who is feeling what emotion, We automatically compile a large dataset of 2012 US presidential elections using a small number of hand-chosen hashtags. Next we annotate the tweets for Experiencer, State, and Stimulus by crowdsourcing to Amazon’s Mechanical Turk.1 We analyze the annotations to determine the distribution"
W14-2607,S12-1033,1,0.757839,"Missing"
W14-2607,H05-2017,0,\N,Missing
W14-2607,C98-1013,0,\N,Missing
W17-5307,D15-1075,0,0.0228619,"Pn t 2 hpt ki k j 2 j=1 (6) k1 − ft k2 Pn hpt k1 − f k j 2 j=1 (7) ko k Pn t 2 hpt ko k j 2 j=1 (8) 4 Data RepEval 2017 use Multi-Genre NLI corpus (MultiNLI) (Williams et al., 2017), which focuses on three basic relationships between a premise and a potential hypothesis: the premise entails the hypothesis (entailment), they contradict each other (contradiction), or they are not related (neutral). The corpus has ten genres, such as fiction, letters, telephone speech and so on. Training set only has five genres of them, therefore there are in-domain and cross-domain development/test sets. SNLI (Bowman et al., 2015) corpus can be used as an additional training/development set, which includes content from the single genre of image captions. However, we don’t use SNLI as an additional training/development data in our experiments. where it , ft , ot are the input gate, forget gate, and output gate in the BiLSTM of the top layer. Note that the gates are concatenated by forward → − ← − and backward LSTM, i.e., it = [ it ; it ], ft = → − ← − − [ ft ; ft ], ot = [→ ot ; ← o−t ]. k∗k2 indicates l2 -norm, which converts vectors to scalars. The idea of gated-attention is inspired by the fact that human only rememb"
W17-5307,P16-2022,0,0.208419,"Eval 2017 Shared Task aims to evaluate language understanding models for sentence representation with natural language inference (NLI) tasks, where a sentence is represented as a fixedlength vector. Modeling inference in human language is very 36 Proceedings of the 2nd Workshop on Evaluating Vector-Space Representations for NLP, pages 36–40, c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics sentence encoder-based model. Many researchers have studied sentence encoder-based model for natural language inference (Bowman et al., 2015; Vendrov et al., 2015; Mou et al., 2016; Bowman et al., 2016; Munkhdalai and Yu, 2016a,b; Liu et al., 2016; Lin et al., 2017). It is, however, not very clear if the potential of the sentence encoderbased model has been well exploited. In this paper, we demonstrate that proposed models based on gated-attention can achieve a new state-of-theart performance for natural language inference. 3 character-composition vector and word-level embedding e = ([c1 ; w1 ], . . . , [cl ; wl ]). This is performed on both the premise and hypothesis, resulting into two matrices: the ep ∈ Rn×dw for a premise and the eh ∈ Rm×dw for a hypothesis, where n"
W17-5307,P16-1139,0,0.0714274,"ask aims to evaluate language understanding models for sentence representation with natural language inference (NLI) tasks, where a sentence is represented as a fixedlength vector. Modeling inference in human language is very 36 Proceedings of the 2nd Workshop on Evaluating Vector-Space Representations for NLP, pages 36–40, c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics sentence encoder-based model. Many researchers have studied sentence encoder-based model for natural language inference (Bowman et al., 2015; Vendrov et al., 2015; Mou et al., 2016; Bowman et al., 2016; Munkhdalai and Yu, 2016a,b; Liu et al., 2016; Lin et al., 2017). It is, however, not very clear if the potential of the sentence encoderbased model has been well exploited. In this paper, we demonstrate that proposed models based on gated-attention can achieve a new state-of-theart performance for natural language inference. 3 character-composition vector and word-level embedding e = ([c1 ; w1 ], . . . , [cl ; wl ]). This is performed on both the premise and hypothesis, resulting into two matrices: the ep ∈ Rn×dw for a premise and the eh ∈ Rm×dw for a hypothesis, where n and m are the length"
W17-5307,W17-5301,0,0.100549,"Missing"
W17-5307,P15-1011,1,0.817366,"Missing"
W17-5307,D14-1162,0,0.0893348,"Missing"
W17-5307,D14-1181,0,0.0210378,"Missing"
W17-5307,N18-1101,0,\N,Missing
