2021.iwpt-1.4,Semi-Automatic Construction of Text-to-{SQL} Data for Domain Transfer,2021,-1,-1,2,1,5813,tianyi li,Proceedings of the 17th International Conference on Parsing Technologies and the IWPT 2021 Shared Task on Parsing into Enhanced Universal Dependencies (IWPT 2021),0,"Strong and affordable in-domain data is a desirable asset when transferring trained semantic parsers to novel domains. As previous methods for semi-automatically constructing such data cannot handle the complexity of realistic SQL queries, we propose to construct SQL queries via context-dependent sampling, and introduce the concept of topic. Along with our SQL query construction method, we propose a novel pipeline of semi-automatic Text-to-SQL dataset construction that covers the broad space of SQL queries. We show that the created dataset is comparable with expert annotation along multiple dimensions, and is capable of improving domain transfer performance for SOTA semantic parsers."
2021.findings-emnlp.227,Cross-Lingual Leveled Reading Based on Language-Invariant Features,2021,-1,-1,3,0,6980,simin rao,Findings of the Association for Computational Linguistics: EMNLP 2021,0,"Leveled reading (LR) aims to automatically classify texts by the cognitive levels of readers, which is fundamental in providing appropriate reading materials regarding different reading capabilities. However, most state-of-the-art LR methods rely on the availability of copious annotated resources, which prevents their adaptation to low-resource languages like Chinese. In our work, to tackle LR in Chinese, we explore how different language transfer methods perform on English-Chinese LR. Specifically, we focus on adversarial training and cross-lingual pre-training method to transfer the LR knowledge learned from annotated data in the resource-rich English language to Chinese. For evaluation, we first introduce the age-based standard to align datasets with different leveling standards. Then we conduct experiments in both zero-shot and few-shot settings. Comparing these two methods, quantitative and qualitative evaluations show that the cross-lingual pre-training method effectively captures the language-invariant features between English and Chinese. We conduct analysis to propose further improvement in cross-lingual LR."
2021.findings-acl.430,Do It Once: An Embarrassingly Simple Joint Matching Approach to Response Selection,2021,-1,-1,3,0,8497,linhao zhang,Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021,0,None
2021.ccl-1.62,"é\
è¯»åçº§ç¸å\
³ç ç©¶ç»¼è¿°(A Survey of Leveled Reading)",2021,-1,-1,3,0,6980,simin rao,Proceedings of the 20th Chinese National Conference on Computational Linguistics,0,"{``}é
è¯»åçº§çæ¦å¿µå¨äºåä¸çºªæ©æå°±è¢«æè²å·¥ä½è
æåº,éçäººä»¬å¯¹é
è¯»åå¾è¶æ¥è¶éè§,é
è¯»åçº§å¼èµ·äºè¶æ¥è¶å¤çå
³æ³¨,èªå¨é
è¯»åçº§ææ¯ä¹å¾å°äºä¸å®ç¨åº¦çåå±ãæ¬ææ»ç»äºè¿å¹´æ¥çé
è¯»åçº§é¢åçç ç©¶è¿å±,é¦å
ä»ç»äºé
è¯»åçº§ç°æçæ ååéä¹èäº§ççåç§ä½ç³»åè¯­æèµæºãå¨æ­¤åºç¡ä¹ä¸æ´çäºå¨èªå¨é
è¯»åçº§å·¥ä½å·²ç»å¹¿æ³åºç¨çä¸ç±»æ¹æ³:å
¬å¼æ³ãä¼ ç»çæºå¨å­¦ä¹ æ¹æ³åæè¿ç­é¨çæ·±åº¦å­¦ä¹ æ¹æ³,å¹¶ç»åå®éªç»ææ¢³çäºä¸ç±»æ¹æ³å­å¨çå¼å©,ä»¥åå¯ä»¥æ¹è¿çæ¹åãæåæ¬æè¿å¯¹é
è¯»åçº§çæªæ¥åå±æ¹åä»¥åå¯ä»¥åºç¨çé¢åè¿è¡äºæ»ç»åå±æã{''}"
2021.acl-long.465,Guiding the Growth: Difficulty-Controllable Question Generation through Step-by-Step Rewriting,2021,-1,-1,5,0,13363,yi cheng,Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),1,"This paper explores the task of Difficulty-Controllable Question Generation (DCQG), which aims at generating questions with required difficulty levels. Previous research on this task mainly defines the difficulty of a question as whether it can be correctly answered by a Question Answering (QA) system, lacking interpretability and controllability. In our work, we redefine question difficulty as the number of inference steps required to answer it and argue that Question Generation (QG) systems should have stronger control over the logic of generated questions. To this end, we propose a novel framework that progressively increases question difficulty through step-by-step rewriting under the guidance of an extracted reasoning chain. A dataset is automatically constructed to facilitate the research, on which extensive experiments are conducted to test the performance of our method."
2021.acl-long.472,{BASS}: Boosting Abstractive Summarization with Unified Semantic Graph,2021,-1,-1,6,0,13374,wenhao wu,Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),1,"Abstractive summarization for long-document or multi-document remains challenging for the Seq2Seq architecture, as Seq2Seq is not good at analyzing long-distance relations in text. In this paper, we present BASS, a novel framework for Boosting Abstractive Summarization based on a unified Semantic graph, which aggregates co-referent phrases distributing across a long range of context and conveys rich relations between phrases. Further, a graph-based encoder-decoder model is proposed to improve both the document representation and summary generation process by leveraging the graph structure. Specifically, several graph augmentation methods are designed to encode both the explicit and implicit relations in the text while the graph-propagation attention mechanism is developed in the decoder to select salient content into the summary. Empirical results show that the proposed architecture brings substantial improvements for both long-document and multi-document summarization tasks."
2020.lrec-1.210,Evaluating Text Coherence at Sentence and Paragraph Levels,2020,-1,-1,3,0,17046,sennan liu,Proceedings of the 12th Language Resources and Evaluation Conference,0,"In this paper, to evaluate text coherence, we propose the paragraph ordering task as well as conducting sentence ordering. We collected four distinct corpora from different domains on which we investigate the adaptation of existing sentence ordering methods to a paragraph ordering task. We also compare the learnability and robustness of existing models by artificially creating mini datasets and noisy datasets respectively and verifying the efficiency of established models under these circumstances. Furthermore, we carry out human evaluation on the rearranged passages from two competitive models and confirm that WLCS-l is a better metric performing significantly higher correlations with human rating than Ï , the most prevalent metric used before. Results from these evaluations show that except for certain extreme conditions, the recurrent graph neural network-based model is an optimal choice for coherence modeling."
2020.iwdp-1.1,Research on Discourse Parsing: from the Dependency View,2020,-1,-1,1,1,5814,sujian li,Proceedings of the Second International Workshop of Discourse Processing,0,"Discourse parsing aims to comprehensively acquire the logical structure of the whole text which may be helpful to some downstream applications such as summarization, reading comprehension, QA and so on. One important issue behind discourse parsing is the representation of discourse structure. Up to now, many discourse structures have been proposed, and the correponding parsing methods are designed, promoting the development of discourse research. In this paper, we mainly introduce our recent discourse research and its preliminary application from the dependency view."
2020.coling-main.69,Syntax-Aware Graph Attention Network for Aspect-Level Sentiment Classification,2020,-1,-1,3,0,21126,lianzhe huang,Proceedings of the 28th International Conference on Computational Linguistics,0,"Aspect-level sentiment classification aims to distinguish the sentiment polarities over aspect terms in a sentence. Existing approaches mostly focus on modeling the relationship between the given aspect words and their contexts with attention, and ignore the use of more elaborate knowledge implicit in the context. In this paper, we exploit syntactic awareness to the model by the graph attention network on the dependency tree structure and external pre-training knowledge by BERT language model, which helps to model the interaction between the context and aspect words better. And the subwords of BERT are integrated into the dependency tree graphs, which can obtain more accurate representations of words by graph attention. Experiments demonstrate the effectiveness of our model."
2020.ccl-1.82,Refining Data for Text Generation,2020,-1,-1,4,0,22127,wenyu guan,Proceedings of the 19th Chinese National Conference on Computational Linguistics,0,"Recent work on data-to-text generation has made progress under the neural encoder-decoder architectures. However, the data input size is often enormous, while not all data records are important for text generation and inappropriate input may bring noise into the final output. To solve this problem, we propose a two-step approach which first selects and orders the important data records and then generates text from the noise-reduced data. Here we propose a learning to rank model to rank the importance of each record which is supervised by a relation extractor. With the noise-reduced data as input, we implement a text generator which sequentially models the input data records and emits a summary. Experiments on the ROTOWIRE dataset verifies the effectiveness of our proposed method in both performance and efficiency."
2020.acl-main.551,Composing Elementary Discourse Units in Abstractive Summarization,2020,-1,-1,3,0,20165,zhenwen li,Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,1,"In this paper, we argue that elementary discourse unit (EDU) is a more appropriate textual unit of content selection than the sentence unit in abstractive summarization. To well handle the problem of composing EDUs into an informative and fluent summary, we propose a novel summarization method that first designs an EDU selection model to extract and group informative EDUs and then an EDU fusion model to fuse the EDUs in each group into one sentence. We also design the reinforcement learning mechanism to use EDU fusion results to reward the EDU selection action, boosting the final summarization performance. Experiments on CNN/Daily Mail have demonstrated the effectiveness of our model."
W19-8102,Incorporating Textual Evidence in Visual Storytelling,2019,20,0,2,1,5813,tianyi li,Proceedings of the 1st Workshop on Discourse Structure in Neural NLG,0,"Previous work on visual storytelling mainly focused on exploring image sequence as evidence for storytelling and neglected textual evidence for guiding story generation. Motivated by human storytelling process which recalls stories for familiar images, we exploit textual evidence from similar images to help generate coherent and meaningful stories. To pick the images which may provide textual experience, we propose a two-step ranking method based on image object recognition techniques. To utilize textual information, we design an extended Seq2Seq model with two-channel encoder and attention. Experiments on the VIST dataset show that our method outperforms state-of-the-art baseline models without heavy engineering."
W19-8104,Zero-shot {C}hinese Discourse Dependency Parsing via Cross-lingual Mapping,2019,16,0,2,0,13363,yi cheng,Proceedings of the 1st Workshop on Discourse Structure in Neural NLG,0,"Due to the absence of labeled data, discourse parsing still remains challenging in some languages. In this paper, we present a simple and efficient method to conduct zero-shot Chinese text-level dependency parsing by leveraging English discourse labeled data and parsing techniques. We first construct the Chinese-English mapping from the level of sentence and elementary discourse unit (EDU), and then exploit the parsing results of the corresponding English translations to obtain the discourse trees for the Chinese text. This method can automatically conduct Chinese discourse parsing, with no need of a large scale of Chinese labeled data."
U19-1024,An Improved Coarse-to-Fine Method for Solving Generation Tasks,2019,0,0,5,0,24934,wenyv guan,Proceedings of the The 17th Annual Workshop of the Australasian Language Technology Association,0,"The coarse-to-fine (coarse2fine) methods have recently been widely used in the generation tasks. The methods first generate a rough sketch in the coarse stage and then use the sketch to get the final result in the fine stage. However, they usually lack the correction ability when getting a wrong sketch. To solve this problem, in this paper, we propose an improved coarse2fine model with a control mechanism, with which our method can control the influence of the sketch on the final results in the fine stage. Even if the sketch is wrong, our model still has the opportunity to get a correct result. We have experimented our model on the tasks of semantic parsing and math word problem solving. The results have shown the effectiveness of our proposed model."
P19-1226,Enhancing Pre-Trained Language Representations with Rich Knowledge for Machine Reading Comprehension,2019,0,18,8,1,8488,an yang,Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,1,"Machine reading comprehension (MRC) is a crucial and challenging task in NLP. Recently, pre-trained language models (LMs), especially BERT, have achieved remarkable success, presenting new state-of-the-art results in MRC. In this work, we investigate the potential of leveraging external knowledge bases (KBs) to further improve BERT for MRC. We introduce KT-NET, which employs an attention mechanism to adaptively select desired knowledge from KBs, and then fuses selected knowledge with BERT to enable context- and knowledge-aware predictions. We believe this would combine the merits of both deep LMs and curated KBs towards better MRC. Experimental results indicate that KT-NET offers significant and consistent improvements over BERT, outperforming competitive baselines on ReCoRD and SQuAD1.1 benchmarks. Notably, it ranks the 1st place on the ReCoRD leaderboard, and is also the best single model on the SQuAD1.1 leaderboard at the time of submission (March 4th, 2019)."
P19-1344,Exploring Sequence-to-Sequence Learning in Aspect Term Extraction,2019,0,3,2,1,8498,dehong ma,Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,1,"Aspect term extraction (ATE) aims at identifying all aspect terms in a sentence and is usually modeled as a sequence labeling problem. However, sequence labeling based methods cannot make full use of the overall meaning of the whole sentence and have the limitation in processing dependencies between labels. To tackle these problems, we first explore to formalize ATE as a sequence-to-sequence (Seq2Seq) learning task where the source sequence and target sequence are composed of words and labels respectively. At the same time, to make Seq2Seq learning suit to ATE where labels correspond to words one by one, we design the gated unit networks to incorporate corresponding word representation into the decoder, and position-aware attention to pay more attention to the adjacent words of a target word. The experimental results on two datasets show that Seq2Seq learning is effective in ATE accompanied with our proposed gated unit networks and position-aware attention mechanism."
D19-1241,Tree-structured Decoding for Solving Math Word Problems,2019,0,0,3,1,16253,qianying liu,Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),0,"Automatically solving math word problems is an interesting research topic that needs to bridge natural language descriptions and formal math equations. Previous studies introduced end-to-end neural network methods, but these approaches did not efficiently consider an important characteristic of the equation, i.e., an abstract syntax tree. To address this problem, we propose a tree-structured decoding method that generates the abstract syntax tree of the equation in a top-down manner. In addition, our approach can automatically stop during decoding without a redundant stop token. The experimental results show that our method achieves single model state-of-the-art performance on Math23K, which is the largest dataset on this task."
D19-1345,Text Level Graph Neural Network for Text Classification,2019,0,2,3,0,21126,lianzhe huang,Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),0,"Recently, researches have explored the graph neural network (GNN) techniques on text classification, since GNN does well in handling complex structures and preserving global information. However, previous methods based on GNN are mainly faced with the practical problems of fixed corpus level graph structure which don{'}t support online testing and high memory consumption. To tackle the problems, we propose a new GNN based model that builds graphs for each input text with global parameters sharing instead of a single graph for the whole corpus. This method removes the burden of dependence between an individual text and entire corpus which support online testing, but still preserve global information. Besides, we build graphs by much smaller windows in the text, which not only extract more local features but also significantly reduce the edge numbers as well as memory consumption. Experiments show that our model outperforms existing models on several text classification datasets even with consuming less memory."
D19-1412,Denoising based Sequence-to-Sequence Pre-training for Text Generation,2019,0,1,4,1,9340,liang wang,Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),0,"This paper presents a new sequence-to-sequence (seq2seq) pre-training method PoDA (Pre-training of Denoising Autoencoders), which learns representations suitable for text generation tasks. Unlike encoder-only (e.g., BERT) or decoder-only (e.g., OpenAI GPT) pre-training approaches, PoDA jointly pre-trains both the encoder and decoder by denoising the noise-corrupted text, and it also has the advantage of keeping the network architecture unchanged in the subsequent fine-tuning stage. Meanwhile, we design a hybrid model of Transformer and pointer-generator networks as the backbone architecture for PoDA. We conduct experiments on two text generation tasks: abstractive summarization, and grammatical error correction. Results on four datasets show that PoDA can improve model performance over strong baselines without using any task-specific techniques and significantly speed up convergence."
D19-1534,Do {NLP} Models Know Numbers? Probing Numeracy in Embeddings,2019,28,8,3,0,3249,eric wallace,Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),0,"The ability to understand and work with numbers (numeracy) is critical for many complex reasoning tasks. Currently, most NLP models treat numbers in text in the same way as other tokens{---}they embed them as distributed vectors. Is this enough to capture numeracy? We begin by investigating the numerical reasoning capabilities of a state-of-the-art question answering model on the DROP dataset. We find this model excels on questions that require numerical reasoning, i.e., it already captures numeracy. To understand how this capability emerges, we probe token embedding methods (e.g., BERT, GloVe) on synthetic list maximum, number decoding, and addition tasks. A surprising degree of numeracy is naturally present in standard embeddings. For example, GloVe and word2vec accurately encode magnitude for numbers up to 1,000. Furthermore, character-level embeddings are even more precise{---}ELMo captures numeracy the best for all pre-trained methods{---}but BERT, which uses sub-word units, is less exact."
W18-2611,Adaptations of {ROUGE} and {BLEU} to Better Evaluate Machine Reading Comprehension Task,2018,15,0,5,1,8488,an yang,Proceedings of the Workshop on Machine Reading for Question Answering,0,"Current evaluation metrics to question answering based machine reading comprehension (MRC) systems generally focus on the lexical overlap between candidate and reference answers, such as ROUGE and BLEU. However, bias may appear when these metrics are used for specific question types, especially questions inquiring yes-no opinions and entity lists. In this paper, we make adaptations on the metrics to better correlate $n$-gram overlap with the human judgment for answers to these two question types. Statistical analysis proves the effectiveness of our approach. Our adaptations may provide positive guidance for the development of real-scene MRC systems."
P18-2071,{S}ci{DTB}: Discourse Dependency {T}ree{B}ank for Scientific Abstracts,2018,12,1,2,1,8488,an yang,Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"Annotation corpus for discourse relations benefits NLP tasks such as machine translation and question answering. In this paper, we present SciDTB, a domain-specific discourse treebank annotated on scientific articles. Different from widely-used RST-DT and PDTB, SciDTB uses dependency trees to represent discourse structure, which is flexible and simplified to some extent but do not sacrifice structural integrity. We discuss the labeling framework, annotation workflow and some statistics about SciDTB. Furthermore, our treebank is made as a benchmark for evaluating discourse dependency parsers, on which we provide several baselines as fundamental work."
P18-1015,"Retrieve, Rerank and Rewrite: Soft Template Based Neural Summarization",2018,0,45,3,1,13375,ziqiang cao,Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"Most previous seq2seq summarization systems purely depend on the source text to generate summaries, which tends to work unstably. Inspired by the traditional template-based summarization approaches, this paper proposes to use existing summaries as soft templates to guide the seq2seq model. To this end, we use a popular IR platform to Retrieve proper summaries as candidate templates. Then, we extend the seq2seq framework to jointly conduct template Reranking and template-aware summary generation (Rewriting). Experiments show that, in terms of informativeness, our model significantly outperforms the state-of-the-art methods, and even soft templates themselves demonstrate high competitiveness. In addition, the import of high-quality external summaries improves the stability and readability of generated summaries."
P18-1178,Multi-Passage Machine Reading Comprehension with Cross-Passage Answer Verification,2018,26,4,7,1,6573,yizhong wang,Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"Machine reading comprehension (MRC) on real web data usually requires the machine to answer a question by analyzing multiple passages retrieved by search engine. Compared with MRC on a single passage, multi-passage MRC is more challenging, since we are likely to get multiple confusing answer candidates from different passages. To address this problem, we propose an end-to-end neural model that enables those answer candidates from different passages to verify each other based on their content representations. Specifically, we jointly train three modules that can predict the final answer based on three factors: the answer boundary, the answer content and the cross-passage answer verification. The experimental results show that our method outperforms the baseline by a large margin and achieves the state-of-the-art performance on the English MS-MARCO dataset and the Chinese DuReader dataset, both of which are designed for MRC in real-world settings."
N18-1018,Query and Output: Generating Words by Querying Distributed Word Representations for Paraphrase Generation,2018,0,0,4,0,4180,shuming ma,"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)",0,"Most recent approaches use the sequence-to-sequence model for paraphrase generation. The existing sequence-to-sequence model tends to memorize the words and the patterns in the training dataset instead of learning the meaning of the words. Therefore, the generated sentences are often grammatically correct but semantically improper. In this work, we introduce a novel model based on the encoder-decoder framework, called Word Embedding Attention Network (WEAN). Our proposed model generates the words by querying distributed word representations (i.e. neural word embeddings), hoping to capturing the meaning of the according words. Following previous work, we evaluate our model on two paraphrase-oriented tasks, namely text simplification and short text abstractive summarization. Experimental results show that our model outperforms the sequence-to-sequence baseline by the BLEU score of 6.3 and 5.5 on two English text simplification datasets, and the ROUGE-2 F1 score of 5.7 on a Chinese summarization dataset. Moreover, our model achieves state-of-the-art performances on these three benchmark datasets."
D18-1072,Auto-Dialabel: Labeling Dialogue Data with Unsupervised Learning,2018,0,3,4,0,30460,chen shi,Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing,0,"The lack of labeled data is one of the main challenges when building a task-oriented dialogue system. Existing dialogue datasets usually rely on human labeling, which is expensive, limited in size, and in low coverage. In this paper, we instead propose our framework auto-dialabel to automatically cluster the dialogue intents and slots. In this framework, we collect a set of context features, leverage an autoencoder for feature assembly, and adapt a dynamic hierarchical clustering method for intent and slot labeling. Experimental results show that our framework can promote human labeling cost to a great extent, achieve good intent clustering accuracy (84.1{\%}), and provide reasonable and instructive slot labeling results."
D18-1116,Toward Fast and Accurate Neural Discourse Segmentation,2018,0,7,2,1,6573,yizhong wang,Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing,0,"Discourse segmentation, which segments texts into Elementary Discourse Units, is a fundamental step in discourse analysis. Previous discourse segmenters rely on complicated hand-crafted features and are not practical in actual use. In this paper, we propose an end-to-end neural segmenter based on BiLSTM-CRF framework. To improve its accuracy, we address the problem of data insufficiency by transferring a word representation model that is trained on a large corpus. We also propose a restricted self-attention mechanism in order to capture useful information within a neighborhood. Experiments on the RST-DT corpus show that our model is significantly faster than previous methods, while achieving new state-of-the-art performance."
D18-1504,Joint Learning for Targeted Sentiment Analysis,2018,0,8,2,1,8498,dehong ma,Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing,0,"Targeted sentiment analysis (TSA) aims at extracting targets and classifying their sentiment classes. Previous works only exploit word embeddings as features and do not explore more potentials of neural networks when jointly learning the two tasks. In this paper, we carefully design the hierarchical stack bidirectional gated recurrent units (HSBi-GRU) model to learn abstract features for both tasks, and we propose a HSBi-GRU based joint model which allows the target label to have influence on their sentiment label. Experimental results on two datasets show that our joint learning model can outperform other baselines and demonstrate the effectiveness of HSBi-GRU in learning abstract features."
C18-1073,Multi-Perspective Context Aggregation for Semi-supervised Cloze-style Reading Comprehension,2018,0,0,2,1,9340,liang wang,Proceedings of the 27th International Conference on Computational Linguistics,0,"Cloze-style reading comprehension has been a popular task for measuring the progress of natural language understanding in recent years. In this paper, we design a novel multi-perspective framework, which can be seen as the joint training of heterogeneous experts and aggregate context information from different perspectives. Each perspective is modeled by a simple aggregation module. The outputs of multiple aggregation modules are fed into a one-timestep pointer network to get the final answer. At the same time, to tackle the problem of insufficient labeled data, we propose an efficient sampling mechanism to automatically generate more training examples by matching the distribution of candidates between labeled and unlabeled data. We conduct our experiments on a recently released cloze-test dataset CLOTH (Xie et al., 2017), which consists of nearly 100k questions designed by professional teachers. Results show that our method achieves new state-of-the-art performance over previous strong baselines."
S17-2161,{PKU}{\\_}{ICL} at {S}em{E}val-2017 Task 10: Keyphrase Extraction with Model Ensemble and External Knowledge,2017,0,7,2,1,9340,liang wang,Proceedings of the 11th International Workshop on Semantic Evaluation ({S}em{E}val-2017),0,"This paper presents a system that participated in SemEval 2017 Task 10 (subtask A and subtask B): Extracting Keyphrases and Relations from Scientific Publications (Augenstein et al., 2017). Our proposed approach utilizes external knowledge to enrich feature representation of candidate keyphrase, including Wikipedia, IEEE taxonomy and pre-trained word embeddings etc. Ensemble of unsupervised models, random forest and linear models are used for candidate keyphrase ranking and keyphrase type classification. Our system achieves the 3rd place in subtask A and 4th place in subtask B."
P17-2029,A Two-Stage Parsing Method for Text-Level Discourse Analysis,2017,6,12,2,1,6573,yizhong wang,Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"Previous work introduced transition-based algorithms to form a unified architecture of parsing rhetorical structures (including span, nuclearity and relation), but did not achieve satisfactory performance. In this paper, we propose that transition-based model is more appropriate for parsing the naked discourse tree (i.e., identifying span and nuclearity) due to data sparsity. At the same time, we argue that relation labeling can benefit from naked tree structure and should be treated elaborately with consideration of three kinds of relations including within-sentence, across-sentence and across-paragraph relations. Thus, we design a pipelined two-stage parsing method for generating an RST tree from text. Experimental results show that our method achieves state-of-the-art performance, especially on span and nuclearity identification."
I17-1050,Tag-Enhanced Tree-Structured Neural Networks for Implicit Discourse Relation Classification,2017,0,3,2,1,6573,yizhong wang,Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 1: Long Papers),0,"Identifying implicit discourse relations between text spans is a challenging task because it requires understanding the meaning of the text. To tackle this task, recent studies have tried several deep learning methods but few of them exploited the syntactic information. In this work, we explore the idea of incorporating syntactic parse tree into neural networks. Specifically, we employ the Tree-LSTM model and Tree-GRU model, which is based on the tree structure, to encode the arguments in a relation. And we further leverage the constituent tags to control the semantic composition process in these tree-structured neural networks. Experimental results show that our method achieves state-of-the-art performance on PDTB corpus."
I17-1064,Cascading Multiway Attentions for Document-level Sentiment Classification,2017,0,3,2,1,8498,dehong ma,Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 1: Long Papers),0,"Document-level sentiment classification aims to assign the user reviews a sentiment polarity. Previous methods either just utilized the document content without consideration of user and product information, or did not comprehensively consider what roles the three kinds of information play in text modeling. In this paper, to reasonably use all the information, we present the idea that user, product and their combination can all influence the generation of attentions to words and sentences, when judging the sentiment of a document. With this idea, we propose a cascading multiway attention (CMA) model, where multiple ways of using user and product information are cascaded to influence the generation of attentions on the word and sentence layers. Then, sentences and documents are well modeled by multiple representation vectors, which provide rich information for sentiment classification. Experiments on IMDB and Yelp datasets demonstrate the effectiveness of our model."
D17-1139,Learning to Rank Semantic Coherence for Topic Segmentation,2017,17,4,2,1,9340,liang wang,Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing,0,"Topic segmentation plays an important role for discourse parsing and information retrieval. Due to the absence of training data, previous work mainly adopts unsupervised methods to rank semantic coherence between paragraphs for topic segmentation. In this paper, we present an intuitive and simple idea to automatically create a {``}quasi{''} training dataset, which includes a large amount of text pairs from the same or different documents with different semantic coherence. With the training corpus, we design a symmetric CNN neural network to model text pairs and rank the semantic coherence within the learning to rank framework. Experiments show that our algorithm is able to achieve competitive performance over strong baselines on several real-world datasets."
P16-1116,{RBPB}: Regularization-Based Pattern Balancing Method for Event Extraction,2016,32,5,4,1,7754,lei sha,Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"Event extraction is a particularly challenging information extraction task, which intends to identify and classify event triggers and arguments from raw text. In recent works, when determining event types (trigger classification), most of the works are either pattern-only or feature-only. However, although patterns cannot cover all representations of an event, it is still a very important feature. In addition, when identifying and classifying arguments, previous works consider each candidate argument separately while ignoring the relationship between arguments. This paper proposes a Regularization-Based Pattern Balancing Method (RBPB). Inspired by the progress in representation learning, we use trigger embedding, sentence-level embedding and pattern features together as our features for trigger classification so that the effect of patterns and other useful features can be balanced. In addition, RBPB uses a regularization method to take advantage of the relationship between arguments. Experiments show that we achieve results better than current state-of-art equivalents."
N16-1049,Joint Learning Templates and Slots for Event Schema Induction,2016,1,0,2,1,7754,lei sha,Proceedings of the 2016 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"Automatic event schema induction (AESI) means to extract meta-event from raw text, in other words, to find out what types (templates) of event may exist in the raw text and what roles (slots) may exist in each event type. In this paper, we propose a joint entity-driven model to learn templates and slots simultaneously based on the constraints of templates and slots in the same sentence. In addition, the entities' semantic information is also considered for the inner connectivity of the entities. We borrow the normalized cut criteria in image segmentation to divide the entities into more accurate template clusters and slot clusters. The experiment shows that our model gains a relatively higher result than previous work."
D16-1075,News Stream Summarization using Burst Information Networks,2016,33,5,4,0.814815,3790,tao ge,Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing,0,"This paper studies summarizing key information from news streams. We propose simple yet effective models to solve the problem based on a novel and promising representation of text streams xe2x80x90 Burst Information Networks (BINets). A BINet can be aware of redundant information, allows global analysis of a text stream, and can be efficiently built and dynamically updated, which perfectly fits the demands of text stream summarization. Extensive experiments show that the BINet-based approaches are not only efficient and can be used in a real-time online summarization setting, but also can generate high-quality summaries, outperforming the state-of-the-art approach."
D16-1130,Recognizing Implicit Discourse Relations via Repeated Reading: Neural Networks with Multi-Level Attention,2016,28,1,2,0,1457,yang liu,Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing,0,"Recognizing implicit discourse relations is a challenging but important task in the field of Natural Language Processing. For such a complex text processing task, different from previous studies, we argue that it is necessary to repeatedly read the arguments and dynamically exploit the efficient features useful for recognizing discourse relations. To mimic the repeated reading strategy, we propose the neural networks with multi-level attention (NNMA), combining the attention mechanism and external memories to gradually fix the attention on some specific words helpful to judging the discourse relations. Experiments on the PDTB dataset show that our proposed method achieves the state-of-art results. The visualization of the attention weights also illustrates the progress that our model observes the arguments on each level and progressively locates the important words."
D16-1212,Capturing Argument Relationship for {C}hinese Semantic Role Labeling,2016,8,5,2,1,7754,lei sha,Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing,0,None
D16-1260,Encoding Temporal Information for Time-Aware Link Prediction,2016,20,18,5,0,30123,tingsong jiang,Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing,0,None
C16-1053,{A}tt{S}um: Joint Learning of Focusing and Summarization with Neural Attention,2016,26,44,3,1,13375,ziqiang cao,"Proceedings of {COLING} 2016, the 26th International Conference on Computational Linguistics: Technical Papers",0,"Query relevance ranking and sentence saliency ranking are the two main tasks in extractive query-focused summarization. Previous supervised summarization systems often perform the two tasks in isolation. However, since reference summaries are the trade-off between relevance and saliency, using them as supervision, neither of the two rankers could be trained well. This paper proposes a novel summarization system called AttSum, which tackles the two tasks jointly. It automatically learns distributed representations for sentences as well as the document cluster. Meanwhile, it applies the attention mechanism to simulate the attentive reading of human behavior when a query is given. Extensive experiments are conducted on DUC query-focused summarization benchmark datasets. Without using any hand-crafted features, AttSum achieves competitive performance. We also observe that the sentences recognized to focus on the query indeed meet the query need."
C16-1161,Towards Time-Aware Knowledge Graph Completion,2016,27,15,6,0,30123,tingsong jiang,"Proceedings of {COLING} 2016, the 26th International Conference on Computational Linguistics: Technical Papers",0,"Knowledge graph (KG) completion adds new facts to a KG by making inferences from existing facts. Most existing methods ignore the time information and only learn from time-unknown fact triples. In dynamic environments that evolve over time, it is important and challenging for knowledge graph completion models to take into account the temporal aspects of facts. In this paper, we present a novel time-aware knowledge graph completion model that is able to predict links in a KG using both the existing facts and the temporal information of the facts. To incorporate the happening time of facts, we propose a time-aware KG embedding model using temporal order information among facts. To incorporate the valid time of facts, we propose a joint time-aware inference model based on Integer Linear Programming (ILP) using temporal consistencyinformationasconstraints. Wefurtherintegratetwomodelstomakefulluseofglobal temporal information. We empirically evaluate our models on time-aware KG completion task. Experimental results show that our time-aware models achieve the state-of-the-art on temporal facts consistently."
C16-1270,Reading and Thinking: Re-read {LSTM} Unit for Textual Entailment Recognition,2016,16,32,4,1,7754,lei sha,"Proceedings of {COLING} 2016, the 26th International Conference on Computational Linguistics: Technical Papers",0,"Recognizing Textual Entailment (RTE) is a fundamentally important task in natural language processing that has many applications. The recently released Stanford Natural Language Inference (SNLI) corpus has made it possible to develop and evaluate deep neural network methods for the RTE task. Previous neural network based methods usually try to encode the two sentences (premise and hypothesis) and send them together into a multi-layer perceptron to get their entailment type, or use LSTM-RNN to link two sentences together while using attention mechanic to enhance the model{'}s ability. In this paper, we propose to use the re-read mechanic, which means to read the premise again and again while reading the hypothesis. After read the premise again, the model can get a better understanding of the premise, which can also affect the understanding of the hypothesis. On the contrary, a better understanding of the hypothesis can also affect the understanding of the premise. With the alternative re-read process, the model can {``}think{''} of a better decision of entailment type. We designed a new LSTM unit called re-read LSTM (rLSTM) to implement this {``}thinking{''} process. Experiments show that we achieve results better than current state-of-the-art equivalents."
P15-2047,A Dependency-Based Neural Network for Relation Classification,2015,15,17,3,0,1457,yang liu,Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 2: Short Papers),0,"Previous research on relation classification has verified the effectiveness of using dependency shortest paths or subtrees. In this paper, we further explore how to make full use of the combination of these dependency information. We first propose a new structure, termed augmented dependency path (ADP), which is composed of the shortest dependency path between two entities and the subtrees attached to the shortest path. To exploit the semantic representation behind the ADP structure, we develop dependency-based neural networks (DepNN): a recursive neural network designed to model the subtrees, and a convolutional neural network to capture the most important features on the shortest path. Experiments on the SemEval-2010 dataset show that our proposed method achieves state-of-art results."
P15-2102,A Hierarchical Knowledge Representation for Expert Finding on Social Media,2015,54,1,3,1,19627,yanran li,Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 2: Short Papers),0,"Expert finding on social media benefits both individuals and commercial services. In this paper, we exploit a 5-level tree representation to model the posts on social media and cast the expert finding problem to the matching problem between the learned user tree and domain tree. We enhance the traditional approximate tree matching algorithm and incorporate word embeddings to improve the matching result. The experiments conducted on Sina Microblog demonstrate the effectiveness of our work."
P15-2136,Learning Summary Prior Representation for Extractive Summarization,2015,17,57,3,1,13375,ziqiang cao,Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 2: Short Papers),0,"In this paper, we propose the concept of summary prior to define how much a sentence is appropriate to be selected into summary without consideration of its context. Different from previous work using manually compiled documentindependent features, we develop a novel summary system called PriorSum, which applies the enhanced convolutional neural networks to capture the summary prior features derived from length-variable phrases. Under a regression framework, the learned prior features are concatenated with document-dependent features for sentence ranking. Experiments on the DUC generic summarization benchmarks show that PriorSum can discover different aspects supporting the summary prior and outperform state-of-the-art baselines."
P15-1056,Bring you to the past: Automatic Generation of Topically Relevant Event Chronicles,2015,32,13,4,0.814815,3790,tao ge,Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),1,"An event chronicle provides people with an easy and fast access to learn the past. In this paper, we propose the first novel approach to automatically generate a topically relevant event chronicle during a certain period given a reference chronicle during another period. Our approach consists of two core components xe2x80x93 a timeaware hierarchical Bayesian model for event detection, and a learning-to-rank model to select the salient events to construct the final chronicle. Experimental results demonstrate our approach is promising to tackle this new problem."
P15-1057,Context-aware Entity Morph Decoding,2015,36,4,4,0,10367,boliang zhang,Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),1,"People create morphs, a special type of fake alternative names, to achieve certain communication goals such as expressing strong sentiment or evading censors. For example, xe2x80x9cBlack Mambaxe2x80x9d, the name for a highly venomous snake, is a morph that Kobe Bryant created for himself due to his agility and aggressiveness in playing basketball games. This paper presents the first end-to-end context-aware entity morph decoding system that can automatically identify, disambiguate, verify morph mentions based on specific contexts, and resolve them to target entities. Our approach is based on an absolute xe2x80x9ccold-startxe2x80x9d it does not require any candidate morph or target entity lists as input, nor any manually constructed morph-target pairs for training. We design a semi-supervised collective inference framework for morph mention extraction, and compare various deep learning based approaches for morph resolution. Our approach achieved significant improvement over the state-of-the-art method (Huang et al., 2013), which used a large amount of training data. 1"
N15-1126,Why Read if You Can Scan? Trigger Scoping Strategy for Biographical Fact Extraction,2015,22,1,3,0,3415,dian yu,Proceedings of the 2015 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"The rapid growth of information sources brings a unique challenge to biographical information extraction: how to find specific facts without having to read all the words. An effective solution is to follow the human scanning strategy which keeps a specific keyword in mind and searches within a specific scope. In this paper, we mimic a scanning process to extract biographical facts. We use event and relation triggers as"
D15-1098,Component-Enhanced {C}hinese Character Embeddings,2015,28,25,4,1,19627,yanran li,Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,0,"Distributed word representations are very useful for capturing semantic information and have been successfully applied in a variety of NLP tasks, especially on English. In this work, we innovatively develop two component-enhanced Chinese character embedding models and their bigram extensions. Distinguished from English word embeddings, our models explore the compositions of Chinese characters, which often serve as semantic indictors inherently. The evaluations on both word similarity and text classification demonstrate the effectiveness of our models."
D15-1185,Recognizing Textual Entailment Using Probabilistic Inference,2015,18,4,2,1,7754,lei sha,Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,0,"Recognizing Text Entailment (RTE) plays an important role in NLP applications including question answering, information retrieval, etc. In recent work, some research explore xe2x80x9cdeepxe2x80x9d expressions such as discourse commitments or strict logic for representing the text. However, these expressions suffer from the limitation of inference inconvenience or translation loss. To overcome the limitations, in this paper, we propose to use the predicate-argument structures to represent the discourse commitments extracted from text. At the same time, with the help of the YAGO knowledge, we borrow the distant supervision technique to mine the implicit facts from the text. We also construct a probabilistic network for all the facts and conduct inference to judge the confidence of each fact for RTE. The experimental results show that our proposed method achieves a competitive result compared to the previous work."
P14-1003,Text-level Discourse Dependency Parsing,2014,25,35,1,1,5814,sujian li,Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"Previous researches on Text-level discourse parsing mainly made use of constituency structure to parse the whole document into one discourse tree. In this paper, we present the limitations of constituency based discourse parsing and first propose to use dependency structure to directly represent the relations between elementary discourse units (EDUs). The state-of-the-art dependency parsing techniques, the Eisner algorithm and maximum spanning tree (MST) algorithm, are adopted to parse an optimal discourse dependency tree based on the arcfactored model and the large-margin learning techniques. Experiments show that our discourse dependency parsers achieve a competitive performance on text-level discourse parsing."
D14-1186,"Joint Learning of {C}hinese Words, Terms and Keywords",2014,13,2,2,1,13375,ziqiang cao,Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP}),0,"Previous work often used a pipelined framework where Chinese word segmentation is followed by term extraction and keyword extraction. Such framework suffers from error propagation and is unable to leverage information in later modules for prior components. In this paper, we propose a four-level Dirichlet Process based model (DP-4) to jointly learn the word distributions from the corpus, domain and document levels simultaneously. Based on the DP-4 model, a sentence-wise Gibbs sampler is adopted to obtain proper segmentation results. Meanwhile, terms and keywords are acquired in the sampling process. Experimental results have shown the effectiveness of our method."
D14-1198,Constructing Information Networks Using One Single Model,2014,15,43,4,0,3420,qi li,Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP}),0,"In this paper, we propose a new framework that unifies the output of three information extraction (IE) tasks - entity mentions, relations and events as an information network representation, and extracts all of them using one single joint model based on structured prediction. This novel formulation allows different parts of the information network fully interact with each other. For example, many relations can now be considered as the resultant states of events. Our approach achieves substantial improvements over traditional pipelined approaches, and significantly advances state-of-the-art end-toend event argument extraction."
C14-1113,Query-focused Multi-Document Summarization: Combining a Topic Model with Graph-based Semi-supervised Learning,2014,32,18,2,1,19627,yanran li,"Proceedings of {COLING} 2014, the 25th International Conference on Computational Linguistics: Technical Papers",0,"Graph-based learning algorithms have been shown to be an effective approach for query-focused multi-document summarization (MDS). In this paper, we extend the standard graph ranking algorithm by proposing a two-layer (i.e. sentence layer and topic layer) graph-based semi-supervised learning approach based on topic modeling techniques. Experimental results on TAC datasets show that by considering topic information, we can effectively improve the summary performance."
Q13-1008,A Novel Feature-based {B}ayesian Model for Query Focused Multi-document Summarization,2013,28,12,2,1,6713,jiwei li,Transactions of the Association for Computational Linguistics,0,"Supervised learning methods and LDA based topic model have been successfully applied in the field of multi-document summarization. In this paper, we propose a novel supervised approach that can incorporate rich sentence features into Bayesian topic models in a principled way, thus taking advantages of both topic model and feature based supervised learning methods. Experimental results on DUC2007, TAC2008 and TAC2009 demonstrate the effectiveness of our approach."
P13-2039,{T}opic{S}pam: a Topic-Model based approach for spam detection,2013,21,38,3,1,6713,jiwei li,Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"Product reviews are now widely used by individuals and organizations for decision making (Litvin et al., 2008; Jansen, 2010). And because of the profits at stake, people have been known to try to game the system by writing fake reviews to promote target products. As a result, the task of deceptive review detection has been gaining increasing attention. In this paper, we propose a generative LDA-based topic modeling approach for fake review detection. Our model can aptly detect the subtle differences between deceptive reviews and truthful ones and achieves about 95% accuracy on review spam datasets, outperforming existing baselines by a large margin."
P13-2099,Evolutionary Hierarchical {D}irichlet Process for Timeline Summarization,2013,13,23,2,1,6713,jiwei li,Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"Timeline summarization aims at generating concise summaries and giving readers a faster and better access to understand the evolution of news. It is a new challenge which combines salience ranking problem with novelty detection. Previous researches in this field seldom explore the evolutionary pattern of topics such as birth, splitting, merging, developing and death. In this paper, we develop a novel model called Evolutionary Hierarchical Dirichlet Process(EHDP) to capture the topic evolution pattern in timeline summarization. In EHDP, time varying information is formulated as a series of HDPs by considering time-dependent information. Experiments on 6 different datasets which contain 3156 documents demonstrates the good performance of our system with regard to ROUGE scores."
D13-1001,Event-Based Time Label Propagation for Automatic Dating of News Articles,2013,12,9,3,0.814815,3790,tao ge,Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,0,"Since many applications such as timeline summaries and temporal IR involving temporal analysis rely on document timestamps, the task of automatic dating of documents has been increasingly important. Instead of using feature-based methods as conventional models, our method attempts to date documents in a year level by exploiting relative temporal relations between documents and events, which are very effective for dating documents. Based on this intuition, we proposed an eventbased time label propagation model called confidence boosting in which time label information can be propagated between documents and events on a bipartite graph. The experiments show that our event-based propagation model can predict document timestamps in high accuracy and the model combined with a MaxEnt classifier outperforms the state-ofthe-art method for this task especially when the size of the training set is small."
W12-6321,The Task 2 of {CIPS}-{SIGHAN} 2012 Named Entity Recognition and Disambiguation in {C}hinese Bakeoff,2012,12,6,3,0,41389,zhengyan he,Proceedings of the Second {CIPS}-{SIGHAN} Joint Conference on {C}hinese Language Processing,0,"The CIPS-SIGHAN 2012 Chinese Named Entity Recognition and Disambiguation (NERD) bake-off was held in the summer of 2012. Named entity recognition and disambiguation is an important task in natural language processing and knowledge base construction. It aims at detecting entity mentions in raw text, followed by pointing the detected mentions to real world entities. Often, real world entities can be found on online encyclopedia like Wikipedia and Baike. This task focuses on NERD in Chinese Language, and presents some challenges unique to Chinese, namely the confusion of named entity with common words, and lack of capital clues as in English. We manually construct query names and a knowledge base from Baike. Evaluation results show promising future of this field."
D12-1114,Joint Learning for Coreference Resolution with {M}arkov {L}ogic,2012,30,16,4,0,7312,yang song,Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,0,"Pairwise coreference resolution models must merge pairwise coreference decisions to generate final outputs. Traditional merging methods adopt different strategies such as the best-first method and enforcing the transitivity constraint, but most of these methods are used independently of the pairwise learning methods as an isolated inference procedure at the end. We propose a joint learning model which combines pairwise classification and mention clustering with Markov logic. Experimental results show that our joint learning system outperforms independent learning systems. Our system gives a better performance than all the learning-based systems from the CoNLL-2011 shared task on the same dataset. Compared with the best system from CoNLL-2011, which employs a rule-based method, our system shows competitive performance."
C12-1098,Update Summarization using a Multi-level Hierarchical {D}irichlet Process Model,2012,31,14,2,1,6713,jiwei li,Proceedings of {COLING} 2012,0,"Update summarization is a new challenge which combines salience ranking with novelty detection. Previous researches usually convert novelty detection to the problem of redundancy removal or salience re-ranking, and seldom explore the birth, splitting, merging and death of aspects for a given topic. In this paper, we borrow the idea of evolutionary clustering and propose a three-level HDP model named h-uHDP, which reveals the diversity and commonality between aspects discovered from two different epochs (i.e. epoch history and epoch update). Specifically, we strengthen modeling the sentence level in the h-uHDP model to adapt to the sentence extraction based framework. Automatic and manual evaluations on TAC data demonstrate the effectiveness of our update summarization algorithm, especially from the novelty criterion."
C12-1168,Implicit Discourse Relation Recognition by Selecting Typical Training Examples,2012,16,23,2,0,35692,xun wang,Proceedings of {COLING} 2012,0,"Implicit discourse relation recognition is a challenging task in the natural language processing field, but important to many applications such as question answering, summarizat ion and so on. Previous research used either art ificially created implicit discourse relat ions with connectives removed from explicit relations or annotated implicit relat ions as training data to detect the possible implicit relations, and do not further discern which examples are fit to be training data. This paper is the first time to apply a d ifferent typical/atypical perspective to select the most suitable discourse relation examples as training data. To differentiate typical and atypical examples for each discourse relation, a novel single centroid clustering algorithm is proposed. With this typical/atypical distinction, we aim to recognize those easily identified discourse relations more precisely so as to promote the performance of the implicit relation recognition. The experimental results verify that the proposed new method outperforms the state -of-the-art methods."
C12-1187,Constructing {C}hinese Abbreviation Dictionary: A Stacked Approach,2012,21,3,2,0,38751,longkai zhang,Proceedings of {COLING} 2012,0,"Abbreviation is a common linguistic phenomenon with wide popularity and high rate of growth. Correctly linking full forms to their abbreviations will be helpful in many applications. For example, it can improve the recall of information retrieval systems. An intuition to solve this is to build an abbreviation dictionary in advance. This paper investigates an automatic abbreviation generation method, which uses a stacked approach for Chinese abbreviation generation. We tackle this problem in two stages. First we use a sequence labeling method to generate a list of candidate abbreviations. Then, we try to use search engine to incorporate web data to re-rank the candidates, and finally get the best candidate. We use a Chinese abbreviation corpus which contains 8015 abbreviation pairs to evaluate the performance. Experiments revealed that our method gave better performance than the baseline methods."
P10-2055,A Semi-Supervised Key Phrase Extraction Approach: Learning from Title Phrases through a Document Semantic Network,2010,8,14,2,0,45671,decong li,Proceedings of the {ACL} 2010 Conference Short Papers,0,"It is a fundamental and important task to extract key phrases from documents. Generally, phrases in a document are not independent in delivering the content of the document. In order to capture and make better use of their relationships in key phrase extraction, we suggest exploring the Wikipedia knowledge to model a document as a semantic network, where both n-ary and binary relationships among phrases are formulated. Based on a commonly accepted assumption that the title of a document is always elaborated to reflect the content of a document and consequently key phrases tend to have close semantics to the title, we propose a novel semi-supervised key phrase extraction approach in this paper by computing the phrase importance in the semantic network, through which the influence of title phrases is propagated to the other phrases iteratively. Experimental results demonstrate the remarkable performance of this approach."
li-etal-2006-interaction,Interaction between Lexical Base and Ontology with Formal Concept Analysis,2006,8,0,1,1,5814,sujian li,Proceedings of the Fifth International Conference on Language Resources and Evaluation ({LREC}{'}06),0,"An ontology describes conceptual knowledge in a specific domain. A lexical base collects a repository of words and gives independent definition of concepts. In this paper, we propose to use FCA as a tool to help constructing an ontology through an existing lexical base. We mainly address two issues. The first issue is how to select attributes to visualize the relations between lexical terms. The second issue is how to revise lexical definitions through analysing the relations in the ontology. Thus the focus is on the effect of interaction between a lexical base and an ontology for the purpose of good ontology construction. Finally, experiments have been conducted to verify our ideas."
xu-etal-2006-design,The Design and Construction of A {C}hinese Collocation Bank,2006,0,3,3,0,1816,ruifeng xu,Proceedings of the Fifth International Conference on Language Resources and Evaluation ({LREC}{'}06),0,"This paper presents an annotated Chinese collocation bank developed at the Hong Kong Polytechnic University. The definition of collocation with good linguistic consistency and good computational operability is first discussed and the properties of collocations are then presented. Secondly, based on the combination of different properties, collocations are classified into four types. Thirdly, the annotation guideline is presented. Fourthly, the implementation issues for collocation bank construction are addressed including the annotation with categorization, dependency and contextual information. Currently, the collocation bank is completed for 3,643 headwords in a 5-million-word corpus."
O05-5011,ååèå¯åé©è­ï¼å¹¶åæåä¸­å¿èªçèªç¾©éä¿å{CCD}çåè©èªç¾©åé¡ä½ç³» (Bidirectional Investigation: The Semantic Relations between the Conjuncts and the Noun Taxonomy in {CCD}) [In {C}hinese],2005,0,0,2,0,9059,yunfang wu,"International Journal of Computational Linguistics {\\&} {C}hinese Language Processing, Volume 10, Number 4, {D}ecember 2005: Special Issue on Selected Papers from {CLSW}-5",0,None
O05-5013,é±å»æ§æèªçèªç¾©æ å° (Semantic Mapping in {C}hinese Metaphorical Idioms) [In {C}hinese],2005,0,0,2,0,6430,yun li,"International Journal of Computational Linguistics {\\&} {C}hinese Language Processing, Volume 10, Number 4, {D}ecember 2005: Special Issue on Selected Papers from {CLSW}-5",0,None
I05-7010,Experiments of Ontology Construction with Formal Concept Analysis,2005,0,10,1,1,5814,sujian li,Proceedings of {O}nto{L}ex 2005 - Ontologies and Lexical Resources,0,"Introduction Ontologies are constructs of domain-specific concepts, and their relationships are used to reason about or define that domain. While an ontology may be constructed either manually or semi-automatically, it is never a trivial task. Manual methods usually require that the concept architecture be constructed by experts who consult dictionaries and other text sources. For example, the Upper Cyc Ontology built by Cycorp was manually constructed with approximately 3,000 terms (Lenat, 1998). Automatic and semi-automatic methods require two separate steps in which the first step acquires domain-specific terms followed by the second step of identifying relations among them from available lexicons or corpora. As lexicons are a good resource and are helpful for ontology construction, Chapters 5 and 15 discuss the problems involving ontology construction and lexicons. To use the available corpus resource, a common approach for automatic acquisition employs heuristic rules (Hearst, 1992; Maedche and Staab, 2000). However, such a method can only acquire limited relations. One new approach in the automatic construction of ontologies (Cimiano et al ., 2004) is FCA (Formal Concept Analysis), a mathematical data analysis approach based on the lattice theory. Because formal concept lattices are a natural representation of hierarchies and classifications, FCA has evolved from a pure mathematical tool to an effective method in computer science (Stumme, 2002), such as in the automatic construction of an ontology (Cimiano et al ., 2004). The focus of this work is on how to use FCA to construct a domainspecific ontology based on different Chinese data sources."
Y03-1031,News-Oriented Keyword Indexing with Maximum Entropy Principle,2003,8,3,1,1,5814,sujian li,"Proceedings of the 17th Pacific Asia Conference on Language, Information and Computation",0,None
W03-1713,News-Oriented Automatic {C}hinese Keyword Indexing,2003,6,7,1,1,5814,sujian li,Proceedings of the Second {SIGHAN} Workshop on {C}hinese Language Processing,0,"In our information era, keywords are very useful to information retrieval, text clustering and so on. News is always a domain attracting a large amount of attention. However, the majority of news articles come without keywords, and indexing them manually costs highly. Aiming at news articles' characteristics and the resources available, this paper introduces a simple procedure to index keywords based on the scoring system. In the process of indexing, we make use of some relatively mature linguistic techniques and tools to filter those meaningless candidate items. Furthermore, according to the hierarchical relations of content words, keywords are not restricted to extracting from text. These methods have improved our system a lot. At last experimental results are given and analyzed, showing that the quality of extracted keywords are satisfying."
O02-2003,åºæ¼ãç¥ç¶²ãçè¾­å½èªç¾©ç¸ä¼¼åº¦è¨ç® (Word Similarity Computing Based on How-net) [In {C}hinese],2002,5,163,2,0,5775,qun liu,"International Journal of Computational Linguistics {\\&} {C}hinese Language Processing, Volume 7, Number 2, August 2002: Special Issue on Computational {C}hinese Lexical Semantics",0,None
