2021.blackboxnlp-1.37,Controlled tasks for model analysis: Retrieving discrete information from sequences,2021,-1,-1,3,0,12128,ionutteodor sorodoc,Proceedings of the Fourth BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP,0,"In recent years, the NLP community has shown increasing interest in analysing how deep learning models work. Given that large models trained on complex tasks are difficult to inspect, some of this work has focused on controlled tasks that emulate specific aspects of language. We propose a new set of such controlled tasks to explore a crucial aspect of natural language processing that has not received enough attention: the need to retrieve discrete information from sequences. We also study model behavior on the tasks with simple instantiations of Transformers and LSTMs. Our results highlight the beneficial role of decoder attention and its sometimes unexpected interaction with other components. Moreover, we show that, for most of the tasks, these simple models still show significant difficulties. We hope that the community will take up the analysis possibilities that our tasks afford, and that a clearer understanding of model behavior on the tasks will lead to better and more transparent models."
2020.blackboxnlp-1.2,Emergent Language Generalization and Acquisition Speed are not tied to Compositionality,2020,21,1,2,1,12087,eugene kharitonov,Proceedings of the Third BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP,0,"Studies of discrete languages emerging when neural agents communicate to solve a joint task often look for evidence of compositional structure. This stems for the expectation that such a structure would allow languages to be acquired faster by the agents and enable them to generalize better. We argue that these beneficial properties are only loosely connected to compositionality. In two experiments, we demonstrate that, depending on the task, non-compositional languages might show equal, or better, generalization performance and acquisition speed than compositional ones. Further research in the area should be clearer about what benefits are expected from compositionality, and how the latter would lead to them."
2020.acl-main.407,Compositionality and Generalization In Emergent Languages,2020,45,1,5,1,12085,rahma chaabouni,Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,1,"Natural language allows us to refer to novel composite concepts by combining expressions denoting their parts according to systematic rules, a property known as compositionality. In this paper, we study whether the language emerging in deep multi-agent simulations possesses a similar ability to refer to novel primitive combinations, and whether it accomplishes this feat by strategies akin to human-language compositionality. Equipped with new ways to measure compositionality in emergent languages inspired by disentanglement in representation learning, we establish three main results: First, given sufficiently large input spaces, the emergent language will naturally develop the ability to refer to novel composite concepts. Second, there is no correlation between the degree of compositionality of an emergent language and its ability to generalize. Third, while compositionality is not necessary for generalization, it provides an advantage in terms of language transmission: The more compositional a language is, the more easily it will be picked up by new learners, even when the latter differ in architecture from the original agents. We conclude that compositionality does not arise from simple generalization pressure, but if an emergent language does chance upon it, it will be more likely to survive and thrive."
Q19-1033,Tabula Nearly Rasa: Probing the Linguistic Knowledge of Character-level Neural Language Models Trained on Unsegmented Text,2019,65,0,2,0,10213,michael hahn,Transactions of the Association for Computational Linguistics,0,"Recurrent neural networks (RNNs) have reached striking performance in many natural language processing tasks. This has renewed interest in whether these generic sequence processing devices are inducing genuine linguistic knowledge. Nearly all current analytical studies, however, initialize the RNNs with a vocabulary of known words, and feed them tokenized input during training. We present a multi-lingual study of the linguistic knowledge encoded in RNNs trained as character-level language models, on input data with word boundaries removed. These networks face a tougher and more cognitively realistic task, having to discover any useful linguistic unit from scratch based on input statistics. The results show that our {``}near tabula rasa{''} RNNs are mostly able to solve morphological, syntactic and semantic tasks that intuitively presuppose word-level knowledge, and indeed they learned, to some extent, to track word boundaries. Our study opens the door to speculations about the necessity of an explicit, rigid word lexicon in language learning and usage."
P19-1380,Miss Tools and Mr Fruit: Emergent Communication in Agents Learning about Object Affordances,2019,22,3,2,1,22897,diane bouchacourt,Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,1,"Recent research studies communication emergence in communities of deep network agents assigned a joint task, hoping to gain insights on human language evolution. We propose here a new task capturing crucial aspects of the human environment, such as natural object affordances, and of human conversation, such as full symmetry among the participants. By conducting a thorough pragmatic and semantic analysis of the emergent protocol, we show that the agents solve the shared task through genuine bilateral, referential communication. However, the agents develop multiple idiolects, which makes us conclude that full symmetry is not a sufficient condition for a common language to emerge."
P19-1381,{CNN}s found to jump around more skillfully than {RNN}s: Compositional Generalization in Seq2seq Convolutional Networks,2019,14,2,2,0,12086,roberto dessi,Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,1,"Lake and Baroni (2018) introduced the SCAN dataset probing the ability of seq2seq models to capture compositional generalizations, such as inferring the meaning of {``}jump around{''} 0-shot from the component words. Recurrent networks (RNNs) were found to completely fail the most challenging generalization cases. We test here a convolutional network (CNN) on these tasks, reporting hugely improved performance with respect to RNNs. Despite the big improvement, the CNN has however not induced systematic rules, suggesting that the difference between compositional and non-compositional behaviour is not clear-cut."
P19-1384,On the Distribution of Deep Clausal Embeddings: A Large Cross-linguistic Study,2019,0,2,6,0,4295,damian blasi,Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,1,"Embedding a clause inside another ({``}the girl [who likes cars [that run fast]] has arrived{''}) is a fundamental resource that has been argued to be a key driver of linguistic expressiveness. As such, it plays a central role in fundamental debates on what makes human language unique, and how they might have evolved. Empirical evidence on the prevalence and the limits of embeddings has however been based on either laboratory setups or corpus data of relatively limited size. We introduce here a collection of large, dependency-parsed written corpora in 17 languages, that allow us, for the first time, to capture clausal embedding through dependency graphs and assess their distribution. Our results indicate that there is no evidence for hard constraints on embedding depth: the tail of depth distributions is heavy. Moreover, although deeply embedded clauses tend to be shorter, suggesting processing load issues, complex sentences with many embeddings do not display a bias towards less deep embeddings. Taken together, the results suggest that deep embeddings are not disfavoured in written language. More generally, our study illustrates how resources and methods from latest-generation big-data NLP can provide new perspectives on fundamental questions in theoretical linguistics."
P19-1509,Word-order Biases in Deep-agent Emergent Communication,2019,44,1,5,1,12085,rahma chaabouni,Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,1,"Sequence-processing neural networks led to remarkable progress on many NLP tasks. As a consequence, there has been increasing interest in understanding to what extent they process language as humans do. We aim here to uncover which biases such models display with respect to {``}natural{''} word-order constraints. We train models to communicate about paths in a simple gridworld, using miniature languages that reflect or violate various natural language trends, such as the tendency to avoid redundancy or to minimize long-distance dependencies. We study how the controlled characteristics of our miniature languages affect individual learning and their stability across multiple network generations. The results draw a mixed picture. On the one hand, neural networks show a strong tendency to avoid long-distance dependencies. On the other hand, there is no clear preference for the efficient, non-redundant encoding of information that is widely attested in natural language. We thus suggest inoculating a notion of {``}effort{''} into neural networks, as a possible way to make their linguistic behavior more human-like."
N19-1002,The emergence of number and syntax units in {LSTM} language models,2019,0,9,6,0,26049,yair lakretz,"Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",0,"Recent work has shown that LSTMs trained on a generic language modeling objective capture syntax-sensitive generalizations such as long-distance number agreement. We have however no mechanistic understanding of how they accomplish this remarkable feat. Some have conjectured it depends on heuristics that do not truly take hierarchical structure into account. We present here a detailed study of the inner mechanics of number tracking in LSTMs at the single neuron level. We discover that long-distance number information is largely managed by two {``}number units{''}. Importantly, the behaviour of these units is partially controlled by other units independently shown to track syntactic structure. We conclude that LSTMs are, to some extent, implementing genuinely syntactic processing mechanisms, paving the way to a more general understanding of grammatical encoding in LSTMs."
D19-3010,{EGG}: a toolkit for research on Emergence of lan{G}uage in Games,2019,0,6,4,1,12087,eugene kharitonov,Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP): System Demonstrations,0,"There is renewed interest in simulating language emergence among deep neural agents that communicate to jointly solve a task, spurred by the practical aim to develop language-enabled interactive AIs, as well as by theoretical questions about the evolution of human language. However, optimizing deep architectures connected by a discrete communication channel (such as that in which language emerges) is technically challenging. We introduce EGG, a toolkit that greatly simplifies the implementation of emergent-language communication games. EGG{'}s modular design provides a set of building blocks that the user can combine to create new games, easily navigating the optimization and architecture space. We hope that the tool will lower the technical barrier, and encourage researchers from various backgrounds to do original work in this exciting area."
W18-5407,Jump to better conclusions: {SCAN} both left and right,2018,16,1,2,0,10758,jasmijn bastings,Proceedings of the 2018 {EMNLP} Workshop {B}lackbox{NLP}: Analyzing and Interpreting Neural Networks for {NLP},0,"Lake and Baroni (2018) recently introduced the SCAN data set, which consists of simple commands paired with action sequences and is intended to test the strong generalization abilities of recurrent sequence-to-sequence models. Their initial experiments suggested that such models may fail because they lack the ability to extract systematic rules. Here, we take a closer look at SCAN and show that it does not always capture the kind of generalization that it was designed for. To mitigate this we propose a complementary dataset, which requires mapping actions back to the original commands, called NACS. We show that models that do well on SCAN do not necessarily do well on NACS, and that NACS exhibits properties more closely aligned with realistic use-cases for sequence-to-sequence models."
W18-5413,Rearranging the Familiar: Testing Compositional Generalization in Recurrent Networks,2018,0,10,2,0,27954,joao loula,Proceedings of the 2018 {EMNLP} Workshop {B}lackbox{NLP}: Analyzing and Interpreting Neural Networks for {NLP},0,"Systematic compositionality is the ability to recombine meaningful units with regular and predictable outcomes, and it{'}s seen as key to the human capacity for generalization in language. Recent work (Lake and Baroni, 2018) has studied systematic compositionality in modern seq2seq models using generalization to novel navigation instructions in a grounded environment as a probing tool. Lake and Baroni{'}s main experiment required the models to quickly bootstrap the meaning of new words. We extend this framework here to settings where the model needs only to recombine well-trained functional words (such as {``}\textit{around}{''} and {``}\textit{right}{''}) in novel contexts. Our findings confirm and strengthen the earlier ones: seq2seq models can be impressively good at generalizing to novel combinations of previously-seen input, but only when they receive extensive training on the specific pattern to be generalized (e.g., generalizing from many examples of {``}X \textit{around right}{''} to {``}\textit{jump around right}{''}), while failing when generalization requires novel application of compositional rules (e.g., inferring the meaning of {``}\textit{around right}{''} from those of {``}\textit{right}{''} and {``}\textit{around}{''})."
P18-1198,What you can cram into a single {\\$}{\\&}!{\\#}* vector: Probing sentence embeddings for linguistic properties,2018,41,71,5,0,2456,alexis conneau,Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"Although much effort has recently been devoted to training high-quality sentence embeddings, we still have a poor understanding of what they are capturing. {``}Downstream{''} tasks, often based on sentence classification, are commonly used to evaluate the quality of sentence representations. The complexity of the tasks makes it however difficult to infer what kind of information is present in the representations. We introduce here 10 probing tasks designed to capture simple linguistic features of sentences, and we use them to study embeddings generated by three different encoders trained in eight distinct ways, uncovering intriguing properties of both encoders and training methods."
N18-1108,Colorless Green Recurrent Networks Dream Hierarchically,2018,31,10,5,0.444444,22877,kristina gulordava,"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)",0,"Recurrent neural networks (RNNs) achieved impressive results in a variety of linguistic processing tasks, suggesting that they can induce non-trivial properties of language. We investigate to what extent RNNs learn to track abstract hierarchical syntactic structure. We test whether RNNs trained with a generic language modeling objective in four languages (Italian, English, Hebrew, Russian) can predict long-distance number agreement in various constructions. We include in our evaluation nonsensical sentences where RNNs cannot rely on semantic or lexical cues ({``}The colorless green ideas I ate with the chair sleep furiously{''}), and, for Italian, we compare model performance to human intuitions. Our language-model-trained RNNs make reliable predictions about long-distance agreement, and do not lag much behind human performance. We thus bring support to the hypothesis that RNNs are not just shallow-pattern extractors, but they also acquire deeper grammatical competence."
D18-1119,How agents see things: On visual representations in an emergent language game,2018,0,16,2,1,22897,diane bouchacourt,Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing,0,"There is growing interest in the language developed by agents interacting in emergent-communication settings. Earlier studies have focused on the agents{'} symbol usage, rather than on their representation of visual input. In this paper, we consider the referential games of Lazaridou et al. (2017), and investigate the representations the agents develop during their evolving interaction. We find that the agents establish successful communication by inducing visual representations that almost perfectly align with each other, but, surprisingly, do not capture the conceptual properties of the objects depicted in the input images. We conclude that, if we care about developing language-like communication systems, we must pay more attention to the visual semantics agents associate to the symbols they use."
W17-6904,Living a discrete life in a continuous world: Reference in cross-modal entity tracking,2017,22,0,4,0,11383,gemma boleda,{IWCS} 2017 {---} 12th International Conference on Computational Semantics {---} Short papers,0,None
D17-1030,High-risk learning: acquiring new word vectors from tiny data,2017,18,6,2,0,2275,aurelie herbelot,Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing,0,"Distributional semantics models are known to struggle with small data. It is generally accepted that in order to learn {`}a good vector{'} for a word, a model must have sufficient examples of its usage. This contradicts the fact that humans can guess the meaning of a word from a few occurrences only. In this paper, we show that a neural language model such as Word2Vec only necessitates minor modifications to its standard architecture to learn new terms from tiny data, using background knowledge from a previously learnt semantic space. We test our model on word definitions and on a nonce task involving 2-6 sentences{'} worth of context, showing a large increase in performance over state-of-the-art models on the definitional task."
P16-2035,The red one!: On learning to refer to things based on discriminative properties,2016,18,1,3,1,20683,angeliki lazaridou,Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"As a first step towards agents learning to communicate about their visual environment, we propose a system that, given visual representations of a referent (cat) and a context (sofa), identifies their discriminative attributes, i.e., properties that distinguish them (has_tail). Moreover, despite the lack of direct supervision at the attribute level, the model learns to assign plausible attributes to objects (sofa-has_cushion). Finally, we present a preliminary experiment confirming the referential success of the predicted discriminative attributes."
P16-1144,The {LAMBADA} dataset: Word prediction requiring a broad discourse context,2016,15,8,7,1,15539,denis paperno,Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"We introduce LAMBADA, a dataset to evaluate the capabilities of computational models for text understanding by means of a word prediction task. LAMBADA is a collection of narrative passages sharing the characteristic that human subjects are able to guess their last word if they are exposed to the whole passage, but not if they only see the last sentence preceding the target word. To succeed on LAMBADA, computational models cannot simply rely on local context, but must be able to keep track of information in the broader discourse. We show that LAMBADA exemplifies a wide range of linguistic phenomena, and that none of several state-of-the-art language models reaches accuracy above 1% on this novel benchmark. We thus propose LAMBADA as a challenging test set, meant to encourage the development of new models capable of genuine understanding of broad context in natural language text."
N16-1043,Multimodal Semantic Learning from Child-Directed Input,2016,22,7,4,1,20683,angeliki lazaridou,Proceedings of the 2016 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"Children learn the meaning of words by being exposed to perceptually rich situations (linguistic discourse, visual scenes, etc). Current computational learning models typically simulate these rich situations through impoverished symbolic approximations. In this work, we present a distributed word learning model that operates on child-directed speech paired with realistic visual scenes. The model integrates linguistic and extra-linguistic information (visual and social cues), handles referential uncertainty, and correctly learns to associate words with objects, even in cases of limited linguistic exposure."
J16-4003,"There Is No Logical Negation Here, But There Are Alternatives: Modeling Conversational Negation with Distributional Semantics",2016,55,5,4,1,24892,german kruszewski,Computational Linguistics,0,"Logical negation is a challenge for distributional semantics, because predicates and their negations tend to occur in very similar contexts, and consequently their distributional vectors are very similar. Indeed, it is not even clear what properties a negated distributional vector should possess. However, when linguistic negation is considered in its actual discourse usage, it often performs a role that is quite different from straightforward logical negation. If someone states, in the middle of a conversation, that This is not a dog, the negation strongly suggests a restricted set of alternative predicates that might hold true of the object being talked about. In particular, other canids and middle-sized mammals are plausible alternatives, birds are less likely, skyscrapers and other large buildings virtually impossible. Conversational negation acts like a graded similarity function, of the sort that distributional semantics might be good at capturing. In this article, we introduce a large data set of alternative plausibility ratings for conversationally negated nominal predicates, and we show that simple similarity in distributional semantic space provides an excellent fit to subject data. On the one hand, this fills a gap in the literature on conversational negation, proposing distributional semantics as the right tool to make explicit predictions about potential alternatives of negated predicates. On the other hand, the results suggest that negation, when addressed from a broader pragmatic perspective, far from being a nuisance, is an ideal application domain for distributional semantic methods."
J16-2006,{S}quibs: When the Whole Is Less Than the Sum of Its Parts: How Composition Affects {PMI} Values in Distributional Semantic Vectors,2016,5,5,2,1,15539,denis paperno,Computational Linguistics,0,"Distributional semantic models, deriving vector-based word representations from patterns of word usage in corpora, have many useful applications Turney and Pantel 2010. Recently, there has been interest in compositional distributional models, which derive vectors for phrases from representations of their constituent words Mitchell and Lapata 2010. Often, the values of distributional vectors are pointwise mutual information PMI scores obtained from raw co-occurrence counts. In this article we study the relation between the PMI dimensions of a phrase vector and its components in order to gain insights into which operations an adequate composition model should perform. We show mathematically that the difference between the PMI dimension of a phrase vector and the sum of PMIs in the corresponding dimensions of the phrase's parts is an independently interpretable value, namely, a quantification of the impact of the context associated with the relevant dimension on the phrase's internal cohesion, as also measured by PMI. We then explore this quantity empirically, through an analysis of adjective-noun composition."
W15-2813,Do Distributed Semantic Models Dream of Electric Sheep? Visualizing Word Representations through Image Synthesis,2015,29,1,3,1,20683,angeliki lazaridou,Proceedings of the Fourth Workshop on Vision and Language,0,"We introduce the task of visualizing distributed semantic representations by generating images from word vectors. Given the corpus-based vector encoding the word broccoli, we convert it to a visual representation by means of a cross-modal mapping function, and then use the mapped representation to generate an image of broccoli as xe2x80x9cdreamedxe2x80x9d by the distributed model. We propose a baseline dream synthesis method based on averaging pictures whose visual representations are topologically close to the mapped vector. Two experiments show that we generate dreams that generally belong to the the right semantic category, and are sometimes accurate enough for subjects to distinguish the intended concept from a related one."
S15-1023,Leveraging Preposition Ambiguity to Assess Compositional Distributional Models of Semantics,2015,27,7,4,0,37319,samuel ritter,Proceedings of the Fourth Joint Conference on Lexical and Computational Semantics,0,"Complex interactions among the meanings of words are important factors in the function that maps word meanings to phrase meanings. Recently, compositional distributional semantics models (CDSM) have been designed with the goal of emulating these complex interactions; however, experimental results on the effectiveness of CDSM have been difficult to interpret because the current metrics for assessing them do not control for the confound of lexical information. We present a new method for assessing the degree to which CDSM capture semantic interactions that dissociates the influences of lexical and compositional information. We then provide a dataset for performing this type of assessment and use it to evaluate six compositional models using both co-occurrence based and neural language model input vectors. Results show that neural language input vectors are consistently superior to co-occurrence based vectors, that several CDSM capture substantial compositional information, and that, surprisingly, vector addition matches and is in many cases superior to purpose-built paramaterized models."
Q15-1014,From Visual Attributes to Adjectives through Decompositional Distributional Semantics,2015,36,1,4,1,20683,angeliki lazaridou,Transactions of the Association for Computational Linguistics,0,"As automated image analysis progresses, there is increasing interest in richer linguistic annotation of pictures, with attributes of objects (e.g., furry, brown{\ldots}) attracting most attention. By building on the recent {``}zero-shot learning{''} approach, and paying attention to the linguistic nature of attributes as noun modifiers, and specifically adjectives, we show that it is possible to tag images with attribute-denoting adjectives even when no training data containing the relevant annotation are available. Our approach relies on two key observations. First, objects can be seen as bundles of attributes, typically expressed as adjectival modifiers (a dog is something furry, brown, etc.), and thus a function trained to map visual representations of objects to nominal labels can implicitly learn to map attributes to adjectives. Second, objects and attributes come together in pictures (the same thing is a dog and it is brown). We can thus achieve better attribute (and object) label retrieval by treating images as {``}visual phrases{''}, and decomposing their linguistic representation into an attribute-denoting adjective and an object-denoting noun. Our approach performs comparably to a method exploiting manual attribute annotation, it out-performs various competitive alternatives in both attribute and object annotation, and it automatically constructs attribute-centric representations that significantly improve performance in supervised object recognition."
Q15-1027,Deriving {B}oolean structures from distributional vectors,2015,51,26,3,1,24892,german kruszewski,Transactions of the Association for Computational Linguistics,0,"Corpus-based distributional semantic models capture degrees of semantic relatedness among the words of very large vocabularies, but have problems with logical phenomena such as entailment, that are instead elegantly handled by model-theoretic approaches, which, in turn, do not scale up. We combine the advantages of the two views by inducing a mapping from distributional vectors of words (or sentences) into a Boolean structure of the kind in which natural language terms are assumed to denote. We evaluate this Boolean Distributional Semantic Model (BDSM) on recognizing entailment between words and sentences. The method achieves results comparable to a state-of-the-art SVM, degrades more gracefully when less training data are available and displays interesting qualitative properties."
P15-2004,A Multitask Objective to Inject Lexical Contrast into Distributional Semantics,2015,25,23,3,1,22647,nghia pham,Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 2: Short Papers),0,"Distributional semantic models have trouble distinguishing strongly contrasting words (such as antonyms) from highly compatible ones (such as synonyms), because both kinds tend to occur in similar contexts in corpora. We introduce the multitask Lexical Contrast Model (mLCM), an extension of the effective Skip-gram method that optimizes semantic vectors on the joint tasks of predicting corpus contexts and making the representations of WordNet synonyms closer than that of matching WordNet antonyms. mLCM outperforms Skip-gram both on general semantic tasks and on synonym/antonym discrimination, even when no direct lexical contrast information about the test words is provided during training. mLCM also shows promising results on the task of learning a compositional negation operator mapping adjectives to their antonyms."
P15-1027,Hubness and Pollution: Delving into Cross-Space Mapping for Zero-Shot Learning,2015,34,86,3,1,20683,angeliki lazaridou,Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),1,"Zero-shot methods in language, vision and other domains rely on a cross-space mapping function that projects vectors from the relevant feature space (e.g., visualfeature-based image representations) to a large semantic word space (induced in an unsupervised way from corpus data), where the entities of interest (e.g., objects images depict) are labeled with the words associated to the nearest neighbours of the mapped vectors. Zero-shot cross-space mapping methods hold great promise as a way to scale up annotation tasks well beyond the labels in the training data (e.g., recognizing objects that were never seen in training). However, the current performance of cross-space mapping functions is still quite low, so that the strategy is not yet usable in practical applications. In this paper, we explore some general properties, both theoretical and empirical, of the cross-space mapping function, and we build on them to propose better methods to estimate it. In this way, we attain large improvements over the state of the art, both in cross-linguistic (word translation) and cross-modal (image labeling) zero-shot experiments."
P15-1094,Jointly optimizing word representations for lexical and sentential tasks with the {C}-{PHRASE} model,2015,46,36,4,1,22647,nghia pham,Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),1,"We introduce C-PHRASE, a distributional semantic model that learns word representations by optimizing context prediction for phrases at all levels in a syntactic tree, from single words to full sentences. C-PHRASE outperforms the state-of-theart C-BOW model on a variety of lexical tasks. Moreover, since C-PHRASE word vectors are induced through a compositional learning objective (modeling the contexts of words combined into phrases), when they are summed, they produce sentence representations that rival those generated by ad-hoc compositional models."
N15-1016,Combining Language and Vision with a Multimodal Skip-gram Model,2015,35,72,3,1,20683,angeliki lazaridou,Proceedings of the 2015 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"We extend the SKIP-GRAM model of Mikolov et al. (2013a) by taking visual information into account. Like SKIP-GRAM, our multimodal models (MMSKIP-GRAM) build vector-based word representations by learning to predict linguistic contexts in text corpora. However, for a restricted set of words, the models are also exposed to visual representations of the objects they denote (extracted from natural images), and must predict linguistic and visual features jointly. The MMSKIP-GRAM models achieve good performance on a variety of semantic benchmarks. Moreover, since they propagate visual information to all words, we use them to improve image labeling and retrieval in the zero-shot setup, where the test concepts are never seen during model training. Finally, the MMSKIP-GRAM models discover intriguing visual properties of abstract words, paving the way to realistic implementations of embodied theories of meaning."
N15-1097,So similar and yet incompatible: Toward the automated identification of semantically compatible words,2015,13,7,2,1,24892,german kruszewski,Proceedings of the 2015 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"We introduce the challenge of detecting semantically compatible words, that is, words that can potentially refer to the same thing (cat and hindrance are compatible, cat and dog are not), arguing for its central role in many semantic tasks. We present a publicly available data-set of human compatibility ratings, and a neural-network model that takes distributional embeddings of words as input and learns alternative embeddings that perform the compatibility detection task quite well."
J15-1010,{S}quibs: When the Whole Is Not Greater Than the Combination of Its Parts: A {``}Decompositional{''} Look at Compositional Distributional Semantics,2015,11,5,3,0,20081,fabio zanzotto,Computational Linguistics,0,"Distributional semantics has been extended to phrases and sentences by means of composition operations. We look at how these operations affect similarity measurements, showing that similarity equations of an important class of composition methods can be decomposed into operations performed on the subparts of the input phrases. This establishes a strong link between these models and convolution kernels."
D15-1002,Distributional vectors encode referential attributes,2015,30,20,3,0,5573,abhijeet gupta,Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,0,"Distributional methods have proven to excel at capturing fuzzy, graded aspects of meaning (Italy is more similar to Spain than to Germany). In contrast, it is difficult to extract the values of more specific attributes of word referents from distributional representations, attributes of the kind typically found in structured knowledge bases (Italy has 60 million inhabitants). In this paper, we pursue the hypothesis that distributional vectors also implicitly encode referential attributes. We show that a standard supervised regression model is in fact sufficient to retrieve such attributes to a reasonable degree of accuracy: When evaluated on the prediction of both categorical and numeric attributes of countries and cities, the model consistently reduces baseline error by 30%, and is not far from the upper bound. Further analysis suggests that our model is able to xe2x80x9cobjectifyxe2x80x9d distributional representations for entities, anchoring them more firmly in the external world in measurable ways."
S14-2001,{S}em{E}val-2014 Task 1: Evaluation of Compositional Distributional Semantic Models on Full Sentences through Semantic Relatedness and Textual Entailment,2014,23,179,3,1,10198,marco marelli,Proceedings of the 8th International Workshop on Semantic Evaluation ({S}em{E}val 2014),0,"This paper presents the task on the evaluation of Compositional Distributional Semantics Models on full sentences organized for the first time within SemEval2014. Participation was open to systems based on any approach. Systems were presented with pairs of sentences and were evaluated on their ability to predict human judgments on (i) semantic relatedness and (ii) entailment. The task attracted 21 teams, most of which participated in both subtasks. We received 17 submissions in the relatedness subtask (for a total of 66 runs) and 18 in the entailment subtask (65 runs)."
S14-1021,Dead parrots make bad pets: Exploring modifier effects in noun phrases,2014,33,5,2,1,24892,german kruszewski,Proceedings of the Third Joint Conference on Lexical and Computational Semantics (*{SEM} 2014),0,"Sometimes modifiers have a strong effect on core aspects of the meaning of the nouns they are attached to: A parrot is a desirable pet, but a dead parrot is, at the very least, a rather unusual household companion. In order to stimulate computational research into the impact of modification on phrase meaning, we collected and made available a large dataset containing subject ratings for a variety of noun phrases and the categories they might belong to. We propose to use compositional distributional semantics to model these data, experimenting with numerous distributional semantic spaces, phrase composition methods and asymmetric similarity measures. Our models capture a statistically significant portion of the data, although much work is still needed before we achieve a full computational account of modification effects."
P14-1009,A practical and linguistically-motivated approach to compositional distributional semantics,2014,27,43,3,1,15539,denis paperno,Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"Distributional semantic methods to approximate word meaning with context vectors have been very successful empirically, and the last years have seen a surge of interest in their compositional extension to phrases and sentences. We present here a new model that, like those of Coecke et al. (2010) and Baroni and Zamparelli (2010), closely mimics the standard Montagovian semantic treatment of composition in distributional terms. However, our approach avoids a number of issues that have prevented the application of the earlier linguistically-motivated models to full-fledged, real-life sentences. We test the model on a variety of empirical tasks, showing that it consistently outperforms a set of competitive rivals. 1 Compositional distributional semantics The research of the last two decades has established empirically that distributional vectors for words obtained from corpus statistics can be used"
P14-1023,"Don{'}t count, predict! A systematic comparison of context-counting vs. context-predicting semantic vectors",2014,46,659,1,1,12129,marco baroni,Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"Context-predicting models (more commonly known as embeddings or neural language models) are the new kids on the distributional semantics block. Despite the buzz surrounding these models, the literature is still lacking a systematic comparison of the predictive models with classic, count-vector-based distributional semantic approaches. In this paper, we perform such an extensive evaluation, on a wide range of lexical semantics tasks and across many parameter settings. The results, to our own surprise, show that the buzz is fully justified, as the context-predicting models obtain a thorough and resounding victory against their count-based counterparts."
P14-1059,How to make words with vectors: Phrase generation in distributional semantics,2014,33,23,2,1,8881,georgiana dinu,Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"We introduce the problem of generation in distributional semantics: Given a distributional vector representing some meaning, how can we generate the phrase that best expresses that meaning? We motivate this novel challenge on theoretical and practical grounds and propose a simple data-driven approach to the estimation of generation functions. We test this in a monolingual scenario (paraphrase generation) as well as in a cross-lingual setting (translation by synthesizing adjectivenoun phrase vectors in English and generating the equivalent expressions in Italian)."
P14-1132,Is this a wampimuk? Cross-modal mapping between distributional semantics and the visual world,2014,55,71,3,1,20683,angeliki lazaridou,Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"Following up on recent work on establishing a mapping between vector-based semantic embeddings of words and the visual representations of the corresponding objects from natural images, we first present a simple approach to cross-modal vector-based semantics for the task of zero-shot learning, in which an image of a previously unseen object is mapped to a linguistic representation denoting its word. We then introduce fast mapping, a challenging and more cognitively plausible variant of the zero-shot task, in which the learner is exposed to new objects and the corresponding words in very limited linguistic contexts. By combining prior linguistic and visual knowledge acquired about words and their objects, as well as exploiting the limited new evidence available, the learner must learn to associate new objects with words. Our results on this task pave the way to realistic simulations of how children or robots could use existing knowledge to bootstrap grounded semantic knowledge about new concepts."
marelli-etal-2014-sick,A {SICK} cure for the evaluation of compositional distributional semantic models,2014,14,202,3,1,10198,marco marelli,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"Shared and internationally recognized benchmarks are fundamental for the development of any computational system. We aim to help the research community working on compositional distributional semantic models (CDSMs) by providing SICK (Sentences Involving Compositional Knowldedge), a large size English benchmark tailored for them. SICK consists of about 10,000 English sentence pairs that include many examples of the lexical, syntactic and semantic phenomena that CDSMs are expected to account for, but do not require dealing with other aspects of existing sentential data sets (idiomatic multiword expressions, named entities, telegraphic language) that are not within the scope of CDSMs. By means of crowdsourcing techniques, each pair was annotated for two crucial semantic tasks: relatedness in meaning (with a 5-point rating scale as gold score) and entailment relation between the two elements (with three possible gold labels: entailment, contradiction, and neutral). The SICK data set was used in SemEval-2014 Task 1, and it freely available for research purposes."
E14-1046,Improving the Lexical Function Composition Model with Pathwise Optimized Elastic-Net Regression,2014,25,3,2,0,40082,jiming li,Proceedings of the 14th Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,"In this paper, we show that the lexical function model for composition of distributional semantic vectors can be improved by adopting a more advanced regression technique. We use the pathwise coordinate-descent optimized elastic-net regression method to estimate the composition parameters, and compare the resulting model with several recent alternative approaches in the task of composing simple intransitive sentences, adjective-noun phrases and determiner phrases. Experimental results demonstrate that the lexical function model estimated by elastic-net regression achieves better performance, and it provides good qualitative interpretability through sparsity constraints on model parameters."
2014.lilt-9.5,Frege in Space: A Program for Composition Distributional Semantics,2014,152,125,1,1,12129,marco baroni,"Linguistic Issues in Language Technology, Volume 9, 2014 - Perspectives on Semantic Representations for Textual Inference",0,"The lexicon of any natural language encodes a huge number of distinct word meanings. Just to understand this article, you will need to know what thousands of words mean. The space of possible sentential meanings is infinite: In this article alone, you will encounter many sentences that express ideas you have never heard before, we hope. Statistical semantics has addressed the issue of the vastness of word meaning by proposing methods to harvest meaning automatically from large collections of text (corpora). Formal semantics in the Fregean tradition has developed methods to account for the infinity of sentential meaning based on the crucial insight of compositionality, the idea that meaning of sentences is built incrementally by combining the meanings of their constituents. This article sketches a new approach to semantics that brings together ideas from statistical and formal semantics to account, in parallel, for the richness of lexical meaning and the combinatorial power of sentential semantics. We adopt, in particular, the idea that word meaning can be approximated by the patterns of co-occurrence of words in corpora from statistical semantics, and the idea that compositionality can be captured in terms of a syntax-driven calculus of function application from formal semantics."
W13-3206,General estimation and evaluation of compositional distributional semantic models,2013,24,34,3,1,8881,georgiana dinu,Proceedings of the Workshop on Continuous Vector Space Models and their Compositionality,0,"In recent years, there has been widespread interest in compositional distributional semantic models (cDSMs), that derive meaning representations for phrases from their parts. We present an evaluation of alternative cDSMs under truly comparable conditions. In particular, we extend the idea of Baroni and Zamparelli (2010) and Guevara (2010) to use corpus-extracted examples of the target phrases for parameter estimation to the other models proposed in the literature, so that all models can be tested under the same training conditions. The linguistically motivated functional model of Baroni and Zamparelli (2010) and Coecke et al. (2010) emerges as the winner in all our tests."
W13-0603,Sentence paraphrase detection: When determiners and word order make the difference,2013,23,11,4,1,22647,nghia pham,Proceedings of the {IWCS} 2013 Workshop Towards a Formal Distributional Semantics,0,"Researchers working on distributional semantics have recently taken up the challenge of going beyond lexical meaning and tackle the issue of compositionality. Several Compositional Distributional Semantics Models (CDSMs) have been developed and promising results have been obtained in evaluations carried out against data sets of small phrases and as well as data sets of sentences. However, we believe there is the need to further develop good evaluation tasks that show whether CDSM truly capture compositionality. To this end, we present an evaluation task that highlights some differences among the CDSMs currently available by challenging them in detecting semantic differences caused by word order switch and by determiner replacements. We take as starting point simple intransitive and transitive sentences describing similar events, that we consider to be paraphrases of each other but not of the foil paraphrases we generate from them. Only the models sensitive to word order and determiner phrase meaning and their role in the sentence composition will not be captured into the foilsxe2x80x99 trap."
W13-0104,Intensionality was only alleged: On adjective-noun composition in distributional semantics,2013,-1,-1,2,0.223052,11383,gemma boleda,Proceedings of the 10th International Conference on Computational Semantics ({IWCS} 2013) {--} Long Papers,0,None
P13-5001,Visual Features for Linguists: Basic image analysis techniques for multimodally-curious {NLP}ers,2013,0,0,2,1,10786,elia bruni,Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Tutorials),0,"Features automatically extracted from images constitute a new and rich source of semantic knowledge that can complement information extracted from text. The convergence between visionand text-based information can be exploited in scenarios where the two modalities must be combined to solve a target task (e.g., generating verbal descriptions of images, or finding the right images to illustrate a story). However, the potential applications for integrated visual features go beyond mixed-media scenarios: Because of their complementary nature with respect to language, visual features might provide perceptually grounded semantic information that can be exploited in purely linguistic domains."
P13-4006,{DISSECT} - {DIS}tributional {SE}mantics Composition Toolkit,2013,18,52,3,1,8881,georgiana dinu,Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics: System Demonstrations,0,"We introduce DISSECT, a toolkit to build and explore computational models of word, phrase and sentence meaning based on the principles of distributional semantics. The toolkit focuses in particular on compositional meaning, and implements a number of composition methods that have been proposed in the literature. Furthermore, DISSECT can be useful to researchers and practitioners who need models of word meaning (without composition) as well, as it supports various methods to construct distributional semantic spaces, assessing similarity and even evaluating against benchmarks, that are independent of the composition infrastructure."
P13-2010,A relatedness benchmark to test the role of determiners in compositional distributional semantics,2013,15,13,4,0,1053,raffaella bernardi,Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"Distributional models of semantics capture word meaning very effectively, and they have been recently extended to account for compositionally-obtained representations of phrases made of content words. We explore whether compositional distributional semantic models can also handle a construction in which grammatical terms play a crucial role, namely determiner phrases (DPs). We introduce a new publicly available dataset to test distributional representations of DPs, and we evaluate state-of-the-art models on this set."
P13-1149,Compositional-ly Derived Representations of Morphologically Complex Words in Distributional Semantics,2013,35,59,4,1,20683,angeliki lazaridou,Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"Speakers of a language can construct an unlimited number of new words through morphological derivation. This is a major cause of data sparseness for corpus-based approaches to lexical semantics, such as distributional semantic models of word meaning. We adapt compositional methods originally developed for phrases to the task of deriving the distributional meaning of morphologically complex words from their parts. Semantic representations constructed in this way beat a strong baseline and can be of higher quality than representations directly constructed from corpus data. Our results constitute a novel evaluation of the proposed composition methods, in which the full additive model achieves the best performance, and demonstrate the usefulness of a compositional morphology component in distributional semantics."
D13-1015,Studying the Recursive Behaviour of Adjectival Modification with Compositional Distributional Semantics,2013,29,5,3,1,12299,eva vecchi,Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,0,"In this study, we use compositional distributional semantic methods to investigate restrictions in adjective ordering. Specifically, we focus on properties distinguishing AdjectiveAdjective-Noun phrases in which there is flexibility in the adjective ordering from those bound to a rigid order. We explore a number of measures extracted from the distributional representation of AAN phrases which may indicate a word order restriction. We find that we are able to distinguish the relevant classes and the correct order based primarily on the degree of modification of the adjectives. Our results offer fresh insight into the semantic properties that determine adjective ordering, building a bridge between syntax and distributional semantics."
D13-1196,Fish Transporters and Miracle Homes: How Compositional Distributional Semantics can Help {NP} Parsing,2013,21,27,3,1,20683,angeliki lazaridou,Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,0,"In this work, we argue that measures that have been shown to quantify the degree of semantic plausibility of phrases, as obtained from their compositionally-derived distributional semantic representations, can resolve syntactic ambiguities. We exploit this idea to choose the correct parsing of NPs (e.g., (live fish) transporter rather than live (fish transporter)). We show that our plausibility cues outperform a strong baseline and significantly improve performance when used in combination with state-of-the-art features."
D13-1202,"Of Words, Eyes and Brains: Correlating Image-Based Distributional Semantic Models with Neural Representations of Concepts",2013,44,23,5,0,32488,andrew anderson,Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,0,"Traditional distributional semantic models extract word meaning representations from cooccurrence patterns of words in text corpora. Recently, the distributional approach has been extended to models that record the cooccurrence of words with visual features in image collections. These image-based models should be complementary to text-based ones, providing a more cognitively plausible view of meaning grounded in visual perception. In this study, we test whether image-based models capture the semantic patterns that emerge from fMRI recordings of the neural signal. Our results indicate that, indeed, there is a significant correlation between image-based and brain-based semantic similarities, and that image-based models complement text-based ones, so that the best correlations are achieved when the two modalities are combined. Despite some unsatisfactory, but explained outcomes (in particular, failure to detect differential association of models with brain areas), the results show, on the one hand, that imagebased distributional semantic models can be a precious new tool to explore semantic representation in the brain, and, on the other, that neural data can be used as the ultimate test set to validate artificial semantic models in terms of their cognitive plausibility."
W12-0908,Unseen features. Collecting semantic data from congenital blind subjects,2012,0,0,2,0,928,alessandro lenci,Proceedings of the Workshop on Computational Models of Language Acquisition and Loss,0,"Congenital blind subjects are able to learn how to use color terms and other types of vision-related words in a way that is de facto undistinguishable from sighted people. It has actually been proposed that language provides a rich source of information that blind subjects can exploit to acquire aspects of word meaning that are related to visual experience, such as the color of fruits or animals. Despite this, whether and how sensory deprivation affects the structure of semantic representations is still an open question. In this talk, we present a new, freely available collection of feature norms produced by congenital blind subjects and normal sighted people. Subjects were asked to produce semantic features describing the meaning of concrete and abstract nouns and verbs. Data were collected from Italian subjects, translated into English, and categorized with respect to their semantic type (e.g. hypernym, meronym, physical property, etc.). First analyses of the feature norms highlight important differences between blind and sighted subjects, for instance for the role of color and other visual features in the produced semantic descriptions. This resource can provide new evidence on the role of perceptual experience in shaping concepts, as well as on its interplay with information extracted from linguistic data. The norms will also be used to carry out computational experiments with distributional semantic models to simulate blind and sighted semantic spaces."
P12-1015,Distributional Semantics in Technicolor,2012,36,209,3,1,10786,elia bruni,Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"Our research aims at building computational models of word meaning that are perceptually grounded. Using computer vision techniques, we build visual and multimodal distributional models and compare them to standard textual models. Our results show that, while visual models with state-of-the-art computer vision techniques perform worse than textual models in general tasks (accounting for semantic relatedness), they are as good or better models of the meaning of words with visual correlates such as color terms, even in a nontrivial task that involves nonliteral uses of such words. Moreover, we show that visual and textual information are tapping on different aspects of meaning, and indeed combining them in multimodal models often improves performance."
E12-1004,Entailment above the word level in distributional semantics,2012,33,136,1,1,12129,marco baroni,Proceedings of the 13th Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,"We introduce two ways to detect entailment using distributional semantic representations of phrases. Our first experiment shows that the entailment relation between adjective-noun constructions and their head nouns (big cat|= cat), once represented as semantic vector pairs, generalizes to lexical entailment among nouns (dog|= animal). Our second experiment shows that a classifier fed semantic vector pairs can similarly generalize the entailment relation among quantifier phrases (many dogs|= some dogs) to entailment involving unseen quantifiers (all cats|= several cats). Moreover, nominal and quantifier phrase entailment appears to be cued by different distributional correlates, as predicted by the type-based view of entailment in formal semantics."
W11-2501,How we {BLESS}ed distributional semantic evaluation,2011,18,153,1,1,12129,marco baroni,Proceedings of the {GEMS} 2011 Workshop on {GE}ometrical Models of Natural Language Semantics,0,"We introduce BLESS, a data set specifically designed for the evaluation of distributional semantic models. BLESS contains a set of tuples instantiating different, explicitly typed semantic relations, plus a number of controlled random tuples. It is thus possible to assess the ability of a model to detect truly related word pairs, as well as to perform in-depth analyses of the types of semantic relations that a model favors. We discuss the motivations for BLESS, describe its construction and structure, and present examples of its usage in the evaluation of distributional semantic models."
W11-2503,Distributional semantics from text and images,2011,31,45,3,1,10786,elia bruni,Proceedings of the {GEMS} 2011 Workshop on {GE}ometrical Models of Natural Language Semantics,0,"We present a distributional semantic model combining text- and image-based features. We evaluate this multimodal semantic model on simulating similarity judgments, concept clustering and the BLESS benchmark. When integrated with the same core text-based model, image-based features are at least as good as further text-based features, and they capture different qualitative aspects of the tasks, suggesting that the two sources of information are complementary."
W11-2508,A distributional similarity approach to the detection of semantic change in the {G}oogle {B}ooks Ngram corpus.,2011,11,92,2,0.444444,22877,kristina gulordava,Proceedings of the {GEMS} 2011 Workshop on {GE}ometrical Models of Natural Language Semantics,0,"This paper presents a novel approach for automatic detection of semantic change of words based on distributional similarity models. We show that the method obtains good results with respect to a reference ranking produced by human raters. The evaluation also analyzes the performance of frequency-based methods, comparing them to the similarity method proposed."
W11-1301,(Linear) Maps of the Impossible: Capturing Semantic Anomalies in Distributional Space,2011,18,33,2,1,12299,eva vecchi,Proceedings of the Workshop on Distributional Semantics and Compositionality,0,"In this paper, we present a first attempt to characterize the semantic deviance of composite expressions in distributional semantics. Specifically, we look for properties of adjective-noun combinations within a vector-based semantic space that might cue their lack of meaning. We evaluate four different compositionality models shown to have various levels of success in representing the meaning of AN pairs: the simple additive and multiplicative models of Mitchell and Lapata (2008), and the linear-map-based models of Guevara (2010) and Baroni and Zamparelli (2010). For each model, we generate composite vectors for a set of AN combinations unattested in the source corpus and which have been deemed either acceptable or semantically deviant. We then compute measures that might cue semantic anomaly, and compare each model's results for the two classes of ANs. Our study shows that simple, unsupervised cues can indeed significantly tell unattested but acceptable ANs apart from impossible, or deviant, ANs, and that the simple additive and multiplicative models are the most effective in this task."
W10-2007,Predicting Cognitively Salient Modifiers of the Constitutive Parts of Concepts,2010,15,4,2,1,40088,gerhard kremer,Proceedings of the 2010 Workshop on Cognitive Modeling and Computational Linguistics,0,"When subjects describe concepts in terms of their characteristic properties, they often produce composite properties, e. g., rabbits are said to have long ears, not just ears. We present a set of simple methods to extract the modifiers of composite properties (in particular: parts) from corpora. We achieve our best performance by combining evidence about the association between the modifier and the part both within the context of the target concept and independently of it. We show that this performance is relatively stable across languages (Italian and German) and for production vs. perception of properties."
poesio-etal-2010-babyexp,{B}aby{E}xp: Constructing a Huge Multimodal Resource to Acquire Commonsense Knowledge Like Children Do,2010,15,1,2,0,1743,massimo poesio,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"There is by now widespread agreement that the most realistic way to construct the large-scale commonsense knowledge repositories required by natural language and artificial intelligence applications is by letting machines learn such knowledge from large quantities of data, like humans do. A lot of attention has consequently been paid to the development of increasingly sophisticated machine learning algorithms for knowledge extraction. However, the nature of the input that humans are exposed to while learning commonsense knowledge has received much less attention. The BabyExp project is collecting very dense audio and video recordings of the first 3 years of life of a baby. The corpus constructed in this way will be transcribed with automated techniques and made available to the research community. Moreover, techniques to extract commonsense conceptual knowledge incrementally from these multimodal data are also being explored within the project. The current paper describes BabyExp in general, and presents pilot studies on the feasibility of the automated audio and video transcriptions."
J10-4006,Distributional Memory: A General Framework for Corpus-Based Semantics,2010,115,402,1,1,12129,marco baroni,Computational Linguistics,0,"Research into corpus-based semantics has focused on the development of ad hoc models that treat single tasks, or sets of closely related tasks, as unrelated challenges to be tackled by extracting different kinds of distributional information from the corpus. As an alternative to this one task, one model approach, the Distributional Memory framework extracts distributional information once and for all from the corpus, in the form of a set of weighted word-link-word tuples arranged into a third-order tensor. Different matrices are then generated from the tensor, and their rows and columns constitute natural spaces to deal with different semantic problems. In this way, the same distributional information can be shared across tasks such as modeling word similarity judgments, discovering synonyms, concept categorization, predicting selectional preferences of verbs, solving analogy problems, classifying relations between word pairs, harvesting qualia structures with patterns or example pairs, predicting the typical properties of concepts, and classifying verbs into alternation classes. Extensive empirical testing in all these domains shows that a Distributional Memory implementation performs competitively against task-specific algorithms recently reported in the literature for the same tasks, and against our implementations of several state-of-the-art methods. The Distributional Memory approach is thus shown to be tenable despite the constraints imposed by its multi-purpose nature."
D10-1115,"Nouns are Vectors, Adjectives are Matrices: Representing Adjective-Noun Constructions in Semantic Space",2010,28,361,1,1,12129,marco baroni,Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,0,"We propose an approach to adjective-noun composition (AN) for corpus-based distributional semantics that, building on insights from theoretical linguistics, represents nouns as vectors and adjectives as data-induced (linear) functions (encoded as matrices) over nominal vectors. Our model significantly outperforms the rivals on the task of reconstructing AN vectors not seen in training. A small post-hoc analysis further suggests that, when the model-generated AN vector is not similar to the corpus-observed AN vector, this is due to anomalies in the latter. We show moreover that our approach provides two novel ways to represent adjective meanings, alternative to its representation via corpus-based co-occurrence vectors, both outperforming the latter in an adjective clustering task."
W09-3207,Measuring semantic relatedness with vector space models and random walks,2009,-1,-1,3,0,46912,amacc herdaugdelen,Proceedings of the 2009 Workshop on Graph-based Methods for Natural Language Processing ({T}ext{G}raphs-4),0,None
W09-0201,"One Distributional Memory, Many Semantic Spaces",2009,20,25,1,1,12129,marco baroni,Proceedings of the Workshop on Geometrical Models of Natural Language Semantics,0,"We propose an approach to corpus-based semantics, inspired by cognitive science, in which different semantic tasks are tackled using the same underlying repository of distributional information, collected once and for all from the source corpus. Task-specific semantic spaces are then built on demand from the repository. A straightforward implementation of our proposal achieves state-of-the-art performance on a number of unrelated tasks."
W09-0205,{B}ag{P}ack: A General Framework to Represent Semantic Relations,2009,15,21,2,0,46912,amacc herdaugdelen,Proceedings of the Workshop on Geometrical Models of Natural Language Semantics,0,"We introduce a way to represent word pairs instantiating arbitrary semantic relations that keeps track of the contexts in which the words in the pair occur both together and independently. The resulting features are of sufficient generality to allow us, with the help of a standard supervised machine learning algorithm, to tackle a variety of unrelated semantic tasks with good results and almost no task-specific tailoring."
D09-1065,{EEG} responds to conceptual stimuli and corpus semantics,2009,21,29,2,0,18472,brian murphy,Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,0,"Mitchell et al. (2008) demonstrated that corpus-extracted models of semantic knowledge can predict neural activation patterns recorded using fMRI. This could be a very powerful technique for evaluating conceptual models extracted from corpora; however, fMRI is expensive and imposes strong constraints on data collection. Following on experiments that demonstrated that EEG activation patterns encode enough information to discriminate broad conceptual categories, we show that corpus-based semantic representations can predict EEG activation patterns with significant accuracy, and we evaluate the relative performance of different corpus-models on this task."
W08-1913,Cognitively Salient Relations for Multilingual Lexicography,2008,16,5,3,1,40088,gerhard kremer,Coling 2008: Proceedings of the Workshop on Cognitive Aspects of the Lexicon ({COGALEX} 2008),0,"Providing sets of semantically related words in the lexical entries of an electronic dictionary should help language learners quickly understand the meaning of the target words. Relational information might also improve memorisation, by allowing the generation of structured vocabulary study lists. However, an open issue is which semantic relations are cognitively most salient, and should therefore be used for dictionary construction. In this paper, we present a concept description elicitation experiment conducted with German and Italian speakers. The analysis of the experimental data suggests that there is a small set of concept-class--dependent relation types that are stable across languages and robust enough to allow discrimination across broad concept domains. Our further research will focus on harvesting instantiations of these classes from corpora."
baroni-etal-2008-cleaneval,{C}leaneval: a Competition for Cleaning Web Pages,2008,5,65,1,1,12129,marco baroni,Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08),0,"Cleaneval is a shared task and competitive evaluation on the topic of cleaning arbitrary web pages, with the goal of preparing web data for use as a corpus for linguistic and language technology research and development. The first exercise took place in 2007. We describe how it was set up, results, and lessons learnt"
W07-0607,{ISA} meets {L}ara: An incremental word space model for cognitively plausible simulations of semantic learning,2007,13,19,1,1,12129,marco baroni,Proceedings of the Workshop on Cognitive Aspects of Computational Language Acquisition,0,"We introduce Incremental Semantic Analysis, a fully incremental word space model, and we test it on longitudinal child-directed speech data. On this task, ISA outperforms the related Random Indexing algorithm, as well as a SVD-based technique. In addition, the model has interesting properties that might also be characteristic of the semantic space of children."
P07-2008,zipf{R}: Word Frequency Modeling in {R},2007,0,5,2,0,11404,stefan evert,Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions,0,None
P07-1114,Words and Echoes: Assessing and Mitigating the Non-Randomness Problem in Word Frequency Distribution Modeling,2007,5,7,1,1,12129,marco baroni,Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,1,"Frequency distribution models tuned to words and other linguistic events can predict the number of distinct types and their frequency distribution in samples of arbitrary sizes. We conduct, for the first time, a rigorous evaluation of these models based on cross-validation and separation of training and test data. Our experiments reveal that the prediction accuracy of the models is marred by serious overfitting problems, due to violations of the random sampling assumption in corpus data. We then propose a simple pre-processing method to alleviate such non-randomness problems. Further evaluation confirms the effectiveness of the method, which compares favourably to more complex correction techniques."
E06-2001,Large Linguistically-Processed Web Corpora for Multiple Languages,2006,5,96,1,1,12129,marco baroni,Demonstrations,0,"The Web contains vast amounts of linguistic data. One key issue for linguists and language technologists is how to access it. Commercial search engines give highly compromised access. An alternative is to crawl the Web ourselves, which also allows us to remove duplicates and near-duplicates, navigational material, and a range of other kinds of non-linguistic matter. We can also tokenize, lemmatise and part-of-speech tag the corpus, and load the data into a corpus query tool which supports sophisticated linguistic queries. We have now done this for German and Italian, with corpus sizes of over 1 billion words in each case. We provide Web access to the corpora in our query tool, the Sketch Engine."
E06-1028,A Figure of Merit for the Evaluation of Web-Corpus Randomness,2006,14,9,2,0,37169,massimiliano ciaramita,11th Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,"In this paper, we present an automated, quantitative, knowledge-poor method to evaluate the randomness of a collection of documents (corpus), with respect to a number of biased partitions. The method is based on the comparison of the word frequency distribution of the target corpus to word frequency distributions from corpora built in deliberately biased ways. We apply the method to the task of building a corpus via queries to Google. Our results indicate that this approach can be used, reliably, to discriminate biased and unbiased document collections and to choose the most appropriate query terms."
2006.eamt-1.31,{W}eb{B}oot{C}a{T}. Instant Domain-Specific Corpora to Support Human Translators,2006,3,34,1,1,12129,marco baroni,Proceedings of the 11th Annual conference of the European Association for Machine Translation,0,"We present a web service to aid translators by quicklyn producing corpora for specialist areas, in any of a range ofn languages, from the web. The underlying BootCaT tools haven already been extensively used: here, we present a version whichn is easy for non-technical people to use as all they need do isn fill in a web form. The corpus, once produced, can be eithern downloaded or loaded into the Sketch Engine, a corpus queryn tool, for further exploration. Reference corpora are used ton identify the key terms in the specialist domain."
baroni-etal-2004-introducing,"Introducing the La Repubblica Corpus: A Large, Annotated, {TEI}({XML})-compliant Corpus of Newspaper {I}talian",2004,13,97,1,1,12129,marco baroni,Proceedings of the Fourth International Conference on Language Resources and Evaluation ({LREC}{'}04),0,"This paper describes the La Repubblica corpus, currently being developed at the SSLMIT of the University of Bologna. The corpus is a very large collection of newspaper text, currently amounting to 175 million words, but expected to grow to 400 million before the end of 2004. When completed, it will contain all the articles published between 1985 and 2000 by the national daily La Repubblica. The paper discusses the techniques used to extract the text, tokenize it and annotate it (basic TEI annotation, POS tagging, genre/topic categorization), it presents examples of how it can be used, and gives details of the ways in which interested users can access it. The paper concludes with a discussion of current and future developments, and of weak and strong points of this resource."
baroni-bisi-2004-using,Using Cooccurrence Statistics and the Web to Discover Synonyms in a Technical Language,2004,10,60,1,1,12129,marco baroni,Proceedings of the Fourth International Conference on Language Resources and Evaluation ({LREC}{'}04),0,"Turney (2001) has shown that computing the mutual information of a pair of words by using cooccurrence counts obtained via queries to the AltaVista search engine performs very effectively in a synonym detection task. Since manual synonym detection is a challenging task for terminologists, we investigate whether the AltaVista-based Mutual Information (AVMI) method can be applied to the task of finding pairs of synonyms in the lexicon of a specialized sub-language. In particular, we experiment with synonyms in the field of nautical terminology. Our results indicate that AVMI is very good at spotting synonym couples among pairs of unrelated terms (with precision close to 90% at 62.5% recall) and that it outperforms more standard methods based on contextual cosine similarity. However, AVMI is not able to distinguish between synonyms and other semantically related terms. Thus, AVMI can be used for synonym mining only if it is combined with techniques to filter out other semantic relations."
baroni-bernardini-2004-bootcat,{B}oot{C}a{T}: Bootstrapping Corpora and Terms from the Web,2004,9,231,1,1,12129,marco baroni,Proceedings of the Fourth International Conference on Language Resources and Evaluation ({LREC}{'}04),0,None
W03-2501,Exploiting Long Distance Collocational Relations in Predictive Typing,2003,11,34,2,0,45465,johannes matiasek,Proceedings of the 2003 {EACL} Workshop on Language Modeling for Text Entry Methods,0,"In this paper, we report about some preliminary experiments in which we tried to improve the performance of a state-of-the-art Predictive Typing system for the German language by adding a collocation-based prediction component. This component tries to exploit the fact that texts have a topic and are semantically coherent. Thus, the appearance in a text of a certain word can be a cue that other, semantically related words are likely to appear soon. The collocation-based module exploits this kind of topical/semantic relatedness by relying on statistics about the co-occurrence of words within a large window of text in the training corpus. Our current experimental results indicate that using the collocation-based prediction module has a small but consistent positive effect on the performance of the system."
W02-0606,Unsupervised discovery of morphologically related words based on orthographic and semantic similarity,2002,16,87,1,1,12129,marco baroni,Proceedings of the {ACL}-02 Workshop on Morphological and Phonological Learning,0,"We present an algorithm that takes an unannotated corpus as its input, and returns a ranked list of probable morphologically related pairs as its output. The algorithm tries to discover morphologically related pairs by looking for pairs that are both orthographically and semantically similar, where orthographic similarity is measured in terms of minimum edit distance, and semantic similarity is measured in terms of mutual information. The procedure does not rely on a morpheme concatenation model, nor on distributional properties of word substrings (such as affix frequency). Experiments with German and English input give encouraging results, both in terms of precision (proportion of good pairs found at various cutoff points of the ranked list), and in terms of a qualitative analysis of the types of morphological patterns discovered by the algorithm."
C02-1096,Wordform- and Class-based Prediction of the Components of {G}erman Nominal Compounds in an {AAC} System,2002,7,17,1,1,12129,marco baroni,{COLING} 2002: The 19th International Conference on Computational Linguistics,0,"In word prediction systems for augmentative and alternative communication (AAC), productive word-formation processes such as compounding pose a serious problem. We present a model that predicts German nominal compounds by splitting them into their modifier and head components, instead of trying to predict them as a whole. The model is improved further by the use of class-based modifier-head bigrams constructed using semantic classes automatically extracted from a corpus. The evaluation shows that the split compound model with class bigrams leads to an improvement in keystroke savings of more than 15% over a no split compound baseline model. We also present preliminary results obtained with a word prediction model integrating compound and simple word prediction."
