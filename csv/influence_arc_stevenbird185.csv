2020.acl-main.594,D19-1091,0,0.0462906,"Missing"
2020.acl-main.594,W18-4804,0,0.0686007,"Missing"
2020.acl-main.594,W18-5815,0,0.0884399,"Missing"
2020.acl-main.594,L18-1416,0,0.114866,"d Work Finite state transducers (FSTs) are a popular choice for modelling the morphology of polysynthetic languages. Several toolkits exist, including XFST, Foma, and HFST (Beesley and Karttunen, 2003; Hulden, 2009; Lind´en et al., 2013). Each one is an optimized implementation of the finite state calculus (Kaplan and Kay, 1994), providing additional support for morphosyntactic and morphophonological processes. Most recent work on computational modelling of morphologically rich languages is built on the foundation of these tools (Arppe et al., 2017; Littell, 2018; Andriyanets and Tyers, 2018; Chen and Schwartz, 2018; Cardenas and Zeman, 2018). As a case in point, we applied Foma in the analysis of the morphology of Kunwinjku verbs, but ran into difficulties accounting for outof-vocabulary (OOV) items in open morph classes. We also stopped short of addressing complex features like reduplication and verbal compounding, for technical reasons related to the expressiveness of FSTs (cf. Lane and Bird, 2019). Recently, neural models have gained popularity for morphological processing because they address some of the weakness of FSTs: subword modeling shows an ability to remain robust in the face of out-of-vocab"
2020.acl-main.594,E09-2008,0,0.0706611,"Missing"
2020.acl-main.594,J94-3001,0,0.530031,"Missing"
2020.acl-main.594,U19-1001,1,0.744346,"slot −3 and will be adjacent to the verb root whenever slots −2 and −1 are empty, as is common. With adjacent open class slots, Kunwinjku opens up the possibility of there being contiguous OOV morphs. In Kunwinjku there is no template to help distinguish members of these adjacent classes, thus creating a novel challenge for predicting morph boundaries. While transitivity of the verb is lexically defined, there are three morph classes which signal valency change: the benefactive (BEN), comitative (COM), and reflexive (RR). More details about the respective function of these morphs is given in Lane and Bird (2019), but here it suffices to say their presence in a verb makes resolving valency impossible without wider sentential context. This impacts the FST modelling, as we are unable to restrict possible illegal analyses on this basis, which results in overgeneration. Morphological fusion can lead to a proliferation of morphs and analyses. In Kunwinjku, there are no fewer than 157 possibilities for the first slot of the verb, fusing person and number (for both subject and object) along with tense. We find that this fusion affects decisions around tokenization of the data in preparation for training the"
2020.acl-main.594,W18-4803,0,0.0868185,"Missing"
2020.acl-main.594,C18-1222,0,0.0143656,"d applications for the communities that speak these languages. In this work we investigate Kunwinjku, spoken by about 2,000 people in West Arnhem in the far north of Australia. Members of the community have expressed interest in using technology to support language learning and literacy development. Thus, we face the challenge of developing useful language technologies on top of robust models, with few resources and in a short space of time. We envisage morphologically-aware technologies including dictionary interfaces, spell checkers, text autocompletion, and tools for language learning (cf. Littell et al., 2018). This paper is organized as follows. We begin by reviewing previous work in finite state morphology, 6652 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 6652–6661 c July 5 - 10, 2020. 2020 Association for Computational Linguistics low resource morph analysis, neural approaches to morph analysis, and data augmentation for morphological reinflection (Sec. 2). Next, we describe our existing finite state model for Kunwinjku verbs (Sec. 3). In Section 4 we present a neural approach which addresses gaps in the previous model, including the ability to"
2020.acl-main.594,W17-0114,0,0.0696689,"Missing"
2020.acl-main.594,W18-4802,0,0.0596459,"Missing"
2020.acl-main.594,W19-6012,0,0.0754398,"glossed in consultation with language experts. We define coverage as number of analysed forms, and accuracy as the number of correctly analyzed forms, both as a fraction of n. We define precision 6654 FST Accuracy Coverage Precision 4.1 84.4 88.5 95.4 Given our low resource setting, training a neural encoder-decoder model like those used in neural machine translation (NMT) is not possible without augmenting what resources we do have. Following the established template of recent work on neural morphological analysis for low resource polysynthetic languages (Micher, 2017; Moeller et al., 2018; Schwartz et al., 2019) we use the FST model to generate morphotactically valid pairs of surface and analyzed verbs. For the purpose of training the base neural model, we adapted the Foma tool to randomly generate 3,000,000 surface/analysis pairs from the FST (see Fig. 6 for an example of a tokenized pair). An automatic process removed duplicates, leaving us with 2,666,243 unique pairs which we partitioned into an .8/.1/.1 train/dev/test split. In Schwartz et al. (2019)’s work on modelling complex nouns in Yupik, they generate a training set which exhaustively pairs every Yupik noun root with every inflectional suff"
2020.acl-main.594,W16-2323,0,0.0325272,"erb is captured and copied. In the case where we’ve used the form X k Y, we mean that pattern X is the reduplicated segment if found in the context of Y. Figure adapted from (Evans, 2003). combine that hallucinated data to the base training data set (the Base+halluc[...] models). The model setup is similar to the one described in (Schwartz et al., 2019). We use MarianNMT: a fast, open-source toolkit which implements neural models for machine translation (Junczys-Dowmunt et al., 2018). We used a shallow attentional encoderdecoder model (Bahdanau et al., 2014) using the parameters described in (Sennrich et al., 2016): the encoder and decoder each have 1 hidden layer of size 1024. We use cross-validation as the validation metric, set dropout to .2 on all RNN inputs, and enable early stopping to avoid overfitting. We use the same setup and parameters for all NMT models mentioned in this paper. A full accounting of the MarianNMT settings used can be seen in the Appendix. 5 Evaluation of the Neural Models We begin by reporting the performance of the neural models in terms of coverage, accuracy, and precision, so that they can be compared with the evaluation of the FST model, described in Section 3.2. Addition"
2020.acl-main.594,K17-2010,0,0.069514,"Missing"
2020.acl-main.594,P18-4020,0,\N,Missing
2020.acl-main.594,L18-1368,0,\N,Missing
2020.cl-4.1,P10-1010,1,0.856613,", an indexed list of morphemes with glosses and morphological classifications; (3) Transcribed Corpus: transcriptions prepared as soon as possible after the recording to reduce the frustrations of cold notes; (4) Translated Corpus: not transcribed but translated into some familiar language, with indications of the social contexts; (5) Raw Corpus: unprocessed recordings. Figure 3 The tapered corpus. The quantity of data at each level follows a power law based on the amount of curation required (after Samarin 1967, page 70; Twaddell 1954, page 108). A similar situation has been observed in NLP (Abney and Bird 2010). that far more material would be translated than we could ever hope to transcribe (Figure 3). The same was true fifty years earlier, when Boas prioritized translation over transcription (Sanjek 1990, pages 198f). Fifty years later it was still considered best practice to prioritize translation over transcription: At least a rough word-for-word translation must be done immediately along with a free translation. . . Putting off the transcription may spare the narrator’s patience. . . (Bouquiaux and Thomas 1992). It remains likely for endangered languages that more materials will be translated t"
2020.cl-4.1,L18-1530,1,0.827986,"t NLP tasks can be performed at scale, we have a problem known as the transcription bottleneck, illustrated in Figure 1. Meanwhile, linguists have wondered for years whether methods from speech recognition could be applied to automatically transcribe speech in unwritten languages. In such cases there will not be a pronunciation lexicon or a language model, but it is becoming popular to automate at least the phone recognition stage on its own. For example, Michaud et al. report phone error rates in the 0.12–16 range, after training on 5 hours of transcribed audio from the Na language of China (Adams et al. 2018; Michaud et al. 2018). The idea is for humans to post-edit the output, in this case, correcting one out of every 6–8 characters, and then to insert word boundaries, drawing on their knowledge of the lexicon and of likely word sequences, to produce a word-level transcription. The belief is that by manually cleaning up an errorful phone transcription, and converting it into a word-level transcription, we will save time compared with entering a word-level transcription from scratch. To date, this position has not been substantiated. Although such phone transcription methods are intended to suppo"
2020.cl-4.1,U17-1006,0,0.0258108,"us with gold transcriptions (or with transcriptions that are simulated using grapheme-tophoneme transliteration), along with translations. We simulate the low-resource scenario by giving the system access to a subset of the data with the possible addition of noise (e.g., Besacier, Zhou, and Gao 2006; Stahlberg et al. 2016). Some have compiled small corpora in the course of their work, for example, Griko (Boito et al. 2018) or Mboshi (Godard et al. 2018a; Rialland et al. 2018). Some collaborations have tapped a long-standing collection activity by one of the partners, for example, Yongning Na (Adams et al. 2017). Others have found small collections of transcribed and translated audio on the Web, for example, Arapaho and Ainu (Anastasopoulos et al. 2017). Phone error rate is the accepted measure for phone transcriptions, and it is easy to imagine how this could be refined for phonetic or phonemic similarity. However, optimizing phone error rate comes at a high cost for linguists, because it requires that they “aim for exhaustive transcriptions that are faithful to the audio” (Michaud et al. 2018, page 12). Reducing phone error rate is not necessarily the most effective way to improve word-level recogn"
2020.cl-4.1,2015.iwslt-papers.18,1,0.882634,"ata, the reliance on spontaneous oral interpretations, and working across language families. Godard et al. extended their work using a corpus of 5k Mboshi utterances, using phone recognizer output instead of gold transcriptions (Godard et al. 2018b). Although promising, their results demonstrate the difficulty of segmentation in the presence of noise, and sensitivity to the method chosen for acoustic unit discovery. Adams et al. performed joint word segmentation and alignment between phone sequences and translations using pialign, for German text transliterated into canonical phone sequences (Adams et al. 2015). They extracted a bilingual lexicon and showed that it was possible to generate hundreds of bilingual lexical entries on the basis of just 10k translated sentences. They extended this approach to speech input, training a phone recognizer on Japanese orthographic transcriptions transliterated to gold phoneme transcriptions, and segmented and aligned the phone lattice output (Adams et al. 2016b). They evaluated this in terms of improvement in phone error rate. This shift to speech input opens the way to a more radical possibility: bypassing transcription altogether. 3.3 Bypassing Transcription"
2020.cl-4.1,C12-2013,1,0.81121,"rthography of the language, even though that orthography might not be in widespread use. Literate speakers have some advantages over linguists when it comes to transcription. They have a comprehensive lexicon and language model. They can hold conversations in the language with other speakers to clarify nuances of meaning. Their professional work may have equipped them with editorial and keyboarding skills. In many places, employing locals is inexpensive and delivers local economic benefits. There are many initiatives to train these people up into “community linguists” (Dobrin 2008; Rice 2009; Bird and Chiang 2012; Yamada 2014; Sapién 2018). This is suggested as a solution to the transcription bottleneck: Ideally we should be getting transcriptions of all the recordings, but that is not always feasible or affordable... Most funders will pay for transcription work in the heritage 720 Bird Sparse Transcription Figure 4 Transcription and glossing performed by a speaker (Bird and Chiang 2012). language, so add that line item to your budget to get more recordings transcribed by someone else (King 2015, page 10). When speakers are not literate—in the narrow western sense—they can still identify words in conn"
2020.cl-4.1,J93-2003,0,0.19735,"Missing"
2020.cl-4.1,L18-1531,0,0.173373,"Missing"
2020.cl-4.1,P06-1085,0,0.148056,"Missing"
2020.cl-4.1,I13-1161,1,0.871651,"Missing"
2020.cl-4.1,P12-1018,0,0.0452976,"Missing"
2020.cl-4.1,P02-1040,0,0.109325,"n vs sun). This reliance on inadequate word identifiers and an incomplete lexicon will cause us to underestimate word error rate. Conversely, a lack of understanding about dialect variation, or lack of an agreed standard spelling, or noise in word segmentation, all serve to multiply the spellings for a word. This variability will cause us to overestimate word error rate. These problems carry forward to type-based measures of lexicon quality such as precision at k entries (Adams et al. 2015). Translation quality is another popular measure, and many of the above-cited papers report BLEU scores (Papineni et al. 2002). However, the data requirements for measuring translation quality amplify our sparse data problems. Data sparseness becomes more acute when we encounter mismatches between which concepts are lexicalized across source and target languages: Parallel texts only address standardized, universal stories, and fail to explore what is culture-specific, either in terms of stories or in terms of lexical items. Parallel bible or other corpora may tell us how to say ‘arise!’ or ‘Cain fought with Abel.’ But we will not encounter the whole subworld of lexical particularities that make a language unique, suc"
2020.cl-4.1,winkelmann-raess-2014-introducing,0,0.0274585,"Missing"
2020.cl-4.1,J97-3002,0,0.0585611,"Missing"
2020.cl-4.1,N07-1057,0,0.0950828,"on task (Section 3.5). The section concludes with a summary of approaches to computation and some high-level remarks (Section 3.6), and a discussion of how well these methods address the requirements for learning to transcribe (Section 3.7). 3.1 Segmenting and Aligning Phone Sequences This approach involves segmenting phone sequences into pseudowords then aligning pseudowords with a translation. We construe the aligned words as glosses. At this point, we have produced interlinear glossed text of the kind we saw in (3). Now we can use alignments to infer structure in the source utterances (cf. Xia and Lewis 2007). Phone sequences can be segmented into word-sized units using methods developed for segmentation in non-roman orthographies and in first language acquisition (Cartwright and Brent 1994; Goldwater, Griffiths, and Johnson 2006, 2009; Johnson and Goldwater 2009; Elsner et al. 2013). Besacier et al. took a corpus of Iraqi Arabic text with sentence-aligned English translations, converted the Arabic text to phone transcriptions by dictionary-lookup, then performed unsupervised segmentation of the transcriptions (Besacier, Zhou, and Gao 2006). In a second experiment, Besacier et al. replaced the can"
2020.coling-main.303,W17-0121,1,0.835002,"uery False Positive query translation hit translation nahne nemekke balanda bininj nemekke mahni mahne namekke balanda-ken bininj-beh yekke mahne it (pronoun) that one white man man that one this (demonstrative) this (demonstrative) that one (other spelling) of the white man (genitive) from the man dry season this (other spelling) Table 2: Top false positive generated by the spoken term detection system in Kunwinjku 3425 (a) screenshot of the app (b) deployment of the app Figure 3: Lexical confirmation app 6 Deployment Building on prior work developing mobile tools for language documentation (Bettinson and Bird, 2017), we have begun to explore methods for deploying the pipeline in a remote community. While the first step of identification of new words is straightforward, the task of lexical confirmation might be much more complex to apply. The members of an Aboriginal community might not be familiar with technologies and they are not necessarily literate (in the narrow western sense). Taking into account these constraints, we built a lexical confirmation app and trialled it on a small lexicon (Fig. 3). The idea would be to load the output of the spoken term detection system into the app. Then a speaker can"
2020.coling-main.303,2020.cl-4.1,1,0.893829,"stems, but the amount of data available in Indigenous language contexts is usually too limited for such methods to be effective. Recent research has shown the efficacy of spoken term detection methods when data are scarce (Menon et al., 2018a; Menon et al., 2018b). Taking advantage of the transcription of a few words would allow us to propagate it through the speech collection and thus assist language workers in their regular transcription work. So-called “sparse transcription” would be also a way to navigate a speech collection and allow us to be selective about what needs to be transcribed (Bird, 2020). Several tools exist for manual transcription, such as Elan and Praat (Wittenburg et al., 2006; Boersma and Weenink, 1996). However such transcriptions are often made in isolation from the speech community (First Languages Australia, 2014), and so we miss out on the opportunity to take advantage of the interests and skills of local people to shape and carry out the transcription work. We present a fieldwork pipeline which combines automatic speech processing and human expertise to support speech transcription in almost-zero resource settings. After giving the background, we detail the workflo"
2020.coling-main.303,L18-1531,1,0.863919,"Missing"
2020.coling-main.303,2020.lrec-1.307,0,0.0356795,"ts and skills of local people to shape and carry out the transcription work. We present a fieldwork pipeline which combines automatic speech processing and human expertise to support speech transcription in almost-zero resource settings. After giving the background, we detail the workflow and propose a pilot experiment on two very low-resource corpora. 2 Background Existing approaches to automatic transcription of endangered languages involve methods that have been developed for automatic speech recognition. While a few hours of transcribed speech can be enough to train single-speaker models (Gupta and Boulianne, 2020b), speaker-independent models require a large amount of training data to produce useful transcriptions (Gupta and Boulianne, 2020a; Foley et al., 2018). Moreover, they draw language workers and speakers into the time-consuming task of exhaustive transcription, forcing them to transcribe densely, including passages that may be difficult or impossible given the early state of our knowledge of the language. A more suitable approach, we believe, involves beginning with stretches of speech where we have the greatest confidence, and only later tackling the more difficult parts. This work is license"
2020.coling-main.303,2020.sltu-1.51,0,0.0345672,"ts and skills of local people to shape and carry out the transcription work. We present a fieldwork pipeline which combines automatic speech processing and human expertise to support speech transcription in almost-zero resource settings. After giving the background, we detail the workflow and propose a pilot experiment on two very low-resource corpora. 2 Background Existing approaches to automatic transcription of endangered languages involve methods that have been developed for automatic speech recognition. While a few hours of transcribed speech can be enough to train single-speaker models (Gupta and Boulianne, 2020b), speaker-independent models require a large amount of training data to produce useful transcriptions (Gupta and Boulianne, 2020a; Foley et al., 2018). Moreover, they draw language workers and speakers into the time-consuming task of exhaustive transcription, forcing them to transcribe densely, including passages that may be difficult or impossible given the early state of our knowledge of the language. A more suitable approach, we believe, involves beginning with stretches of speech where we have the greatest confidence, and only later tackling the more difficult parts. This work is license"
2020.coling-main.303,wittenburg-etal-2006-elan,0,0.0671219,"too limited for such methods to be effective. Recent research has shown the efficacy of spoken term detection methods when data are scarce (Menon et al., 2018a; Menon et al., 2018b). Taking advantage of the transcription of a few words would allow us to propagate it through the speech collection and thus assist language workers in their regular transcription work. So-called “sparse transcription” would be also a way to navigate a speech collection and allow us to be selective about what needs to be transcribed (Bird, 2020). Several tools exist for manual transcription, such as Elan and Praat (Wittenburg et al., 2006; Boersma and Weenink, 1996). However such transcriptions are often made in isolation from the speech community (First Languages Australia, 2014), and so we miss out on the opportunity to take advantage of the interests and skills of local people to shape and carry out the transcription work. We present a fieldwork pipeline which combines automatic speech processing and human expertise to support speech transcription in almost-zero resource settings. After giving the background, we detail the workflow and propose a pilot experiment on two very low-resource corpora. 2 Background Existing approa"
2020.coling-main.313,C12-2013,1,0.719996,", Spain (Online), December 8-13, 2020 Figure 1: The Language Documentation Mechanism: linguists (present and past) create language resources that support the construction of pedagogical resources to be used in language revitalisation For 25 years, many linguists have presented “indigenous language endangerment as a crisis that requires intervention” (Nevins, 2013, 21). Like many, I have taken this crisis as an golden opportunity for technology, prioritising data capture over local self-determination, in such areas as digital preservation (Bird and Simons, 2003), capacity building (Bird, 2010; Bird and Chiang, 2012; Bird et al., 2013), and app development (Bird et al., 2014a; Bird et al., 2014b). Five years ago I entered the Indigenous country of Arnhem Land in northern Australia imagining language capture on an even larger scale. In the course of living and working in an Aboriginal community, I began to participate in the language activities of a school, a ranger program, and an arts centre. I came to see how commodifying Indigenous languages as data alienates speakers (Dobrin et al., 2009), and how technology does not address the social injustices that underly language endangerment (Srinivasan, 2017)."
2020.coling-main.313,L18-1657,0,0.0450456,"Missing"
2020.coling-main.313,N09-1036,0,0.0153519,"his “zero resource scenario” disenfranchises local knowledge authorities (Verran and Christie, 2007, 215). It is a version of the clich´e of “new technologies saving ancient languages,” perpetuating colonial dichotomies of advanced vs. primitive, of domesticated vs. wild (Goody, 1977; Nevins, 2013). When we are unable to hear what linguists and speakers would tell us, we must fall back on unsupervised methods, like discovering a phone inventory from speech alone (Kempton and Moore, 2014; Vetter et al., 2016; Adda et al., 2016; M¨uller et al., 2017), or segmenting phone sequences into “words” (Johnson and Goldwater, 2009; Elsner et al., 2013). Yet after centuries of colonisation, missionary endeavours, and linguistic fieldwork, all languages have been identified and classified. There is always a wordlist. We know the language family. Related languages have been studied. There are texts and translations. There may be linguists and speakers. In short, we do not need to “discover” the language ex nihilo (L1 acquisition) but to leverage the available resources (L2 acquisition). Indigenous languages do not deserve this zero resource framing. 2.2 “Gold Data” From this position of language as lexico-grammatical code"
2020.coling-main.313,W17-0304,0,0.0276094,"). This demonstrates the value of digital preservation to people, and may stimulate discussion, as when people want to talk about a person or activity that is captured in a photo or recording. Support oral language learning. In the realm of speech and language technology, the focus of learning has been the machine, in order to use human labour efficiently. In the realm of language learning technologies, the focus has often been on overcoming the low-resource situation to create pedagogical content (Ward and Genabith, 2003; Ward, 2018), resulting in text-based solutions (Hermes and King, 2013; Katinskaia et al., 2017). There is an opportunity to support learners in creating their own oral language materials on the fly and leveraging such materials in their learning. 4.6 Not giving up It would be a counterproductive if members of the speech and language technology community decided to drop their interest in Indigenous languages, because of the inherent difficulties touched on here. To be sure, the risks are significant. No community speaks with a single voice, and in building relationships we can unwittingly align ourselves with agendas, clans, and gate-keepers. We may invest in building relationships over"
2020.coling-main.313,L18-1653,0,0.0382,"writing – its presence or its absence – is what makes all the difference... this discourse about language(s) establishes a horizon, a line, on the far side of which is oblivion... The discourse of language endangerment situates the act of writing precisely astraddle that line, endowing writing with the power to move (a) language, word by painstakingly transcribed word, from one side of that metaphysical divide to the other (Moore, 2006, 313). To frame an oral vernacular as an “unwritten language” is to set an agenda: it needs to have text-based resources and applications (Arppe et al., 2016; Maheshwari et al., 2018). The absence of the technology of writing aligns with other negatives: to be under-resourced, a language lacks what is required for creating speech and language technologies (Krauwer, 2003). In this one-dimensional view, Indigenous and endangered languages are acutely under-resourced (Jimerson and Prud’hommeaux, 2018). With the best of intentions we can imagine “developing speech recognition (and other technologies like machine translation) systems for literally all languages in the world” (Besacier et al., 2014, 86). This automatic translation connects with another colonising frame: universa"
2020.coling-main.405,W18-4804,0,0.163163,"Pyini (Gorman, 2016). For this work we use Foma, as we are extending previous work on the Kunwinjku FST (Lane and Bird, 2019). The conventional approach for building an FST morph analyzer is to divide the task into two stages: (a) accounting for the lexicon and morphotactics, and (b) defining the morphophonological rules which use the context at morphotactic boundaries to produce valid surface forms (Beesley and Karttunen, 2003). Many researchers have followed this pattern to build FST analyzers for morphologically complex languages (Çöltekin, 2010; Harrigan et al., 2017; Bowers et al., 2017; Andriyanets and Tyers, 2018; Moeller et al., 2018; Cardenas and Zeman, 2018; Lane and Bird, 2019). Others deviate slightly, e.g., Chen and Schwartz implement a mechanism for stage 2 which applies morphophonemic rules at morpheme boundaries working from left to right (Chen and Schwartz, 2018). In any case, the practice of defining a lexicon of morphs to be concatenated to form the intermediate representation of a word is ubiquitous. Accordingly, the ability to mark morph boundaries at this intermediate level serves as one of the basic assumptions of the work reported here. 2.2 Language modelling for morphologically compl"
2020.coling-main.405,W17-0108,0,0.017698,"who speak morphologically complex languages. 2 2.1 Background and Related Work Finite state analysis for morphologically complex languages The proposed approach builds on the established practice of applying finite state transducers for modelling the phonological and morphological systems of natural languages (Beesley and Karttunen, 2003). In recent years, researchers have demonstrated the suitability of finite state models for a variety of morphologically complex, low-resource languages including Cree, Haida, Kunwinjku, Odawa, Tsuut’ina, and Yupik (Snoek et al., 2014; Harrigan et al., 2017; Arppe et al., 2017a; Arppe et al., 2017b; Bowers et al., 2017; Chen and Schwartz, 2018; Lachler et al., 2018; Lane and Bird, 2019). The FST formalism is implemented by a number of frameworks including HFST (Lindén et al., 2013), Foma (Hulden, 2009), OpenFST (Allauzen et al., 2007), and Pyini (Gorman, 2016). For this work we use Foma, as we are extending previous work on the Kunwinjku FST (Lane and Bird, 2019). The conventional approach for building an FST morph analyzer is to divide the task into two stages: (a) accounting for the lexicon and morphotactics, and (b) defining the morphophonological rules which us"
2020.coling-main.405,W17-0101,0,0.0843928,"es. 2 2.1 Background and Related Work Finite state analysis for morphologically complex languages The proposed approach builds on the established practice of applying finite state transducers for modelling the phonological and morphological systems of natural languages (Beesley and Karttunen, 2003). In recent years, researchers have demonstrated the suitability of finite state models for a variety of morphologically complex, low-resource languages including Cree, Haida, Kunwinjku, Odawa, Tsuut’ina, and Yupik (Snoek et al., 2014; Harrigan et al., 2017; Arppe et al., 2017a; Arppe et al., 2017b; Bowers et al., 2017; Chen and Schwartz, 2018; Lachler et al., 2018; Lane and Bird, 2019). The FST formalism is implemented by a number of frameworks including HFST (Lindén et al., 2013), Foma (Hulden, 2009), OpenFST (Allauzen et al., 2007), and Pyini (Gorman, 2016). For this work we use Foma, as we are extending previous work on the Kunwinjku FST (Lane and Bird, 2019). The conventional approach for building an FST morph analyzer is to divide the task into two stages: (a) accounting for the lexicon and morphotactics, and (b) defining the morphophonological rules which use the context at morphotactic boundaries to"
2020.coling-main.405,W18-5815,0,0.0138334,"we are extending previous work on the Kunwinjku FST (Lane and Bird, 2019). The conventional approach for building an FST morph analyzer is to divide the task into two stages: (a) accounting for the lexicon and morphotactics, and (b) defining the morphophonological rules which use the context at morphotactic boundaries to produce valid surface forms (Beesley and Karttunen, 2003). Many researchers have followed this pattern to build FST analyzers for morphologically complex languages (Çöltekin, 2010; Harrigan et al., 2017; Bowers et al., 2017; Andriyanets and Tyers, 2018; Moeller et al., 2018; Cardenas and Zeman, 2018; Lane and Bird, 2019). Others deviate slightly, e.g., Chen and Schwartz implement a mechanism for stage 2 which applies morphophonemic rules at morpheme boundaries working from left to right (Chen and Schwartz, 2018). In any case, the practice of defining a lexicon of morphs to be concatenated to form the intermediate representation of a word is ubiquitous. Accordingly, the ability to mark morph boundaries at this intermediate level serves as one of the basic assumptions of the work reported here. 2.2 Language modelling for morphologically complex languages Some researchers have attempted to"
2020.coling-main.405,L18-1416,0,0.107952,"and Related Work Finite state analysis for morphologically complex languages The proposed approach builds on the established practice of applying finite state transducers for modelling the phonological and morphological systems of natural languages (Beesley and Karttunen, 2003). In recent years, researchers have demonstrated the suitability of finite state models for a variety of morphologically complex, low-resource languages including Cree, Haida, Kunwinjku, Odawa, Tsuut’ina, and Yupik (Snoek et al., 2014; Harrigan et al., 2017; Arppe et al., 2017a; Arppe et al., 2017b; Bowers et al., 2017; Chen and Schwartz, 2018; Lachler et al., 2018; Lane and Bird, 2019). The FST formalism is implemented by a number of frameworks including HFST (Lindén et al., 2013), Foma (Hulden, 2009), OpenFST (Allauzen et al., 2007), and Pyini (Gorman, 2016). For this work we use Foma, as we are extending previous work on the Kunwinjku FST (Lane and Bird, 2019). The conventional approach for building an FST morph analyzer is to divide the task into two stages: (a) accounting for the lexicon and morphotactics, and (b) defining the morphophonological rules which use the context at morphotactic boundaries to produce valid surface fo"
2020.coling-main.405,coltekin-2010-freely,0,0.335132,", Foma (Hulden, 2009), OpenFST (Allauzen et al., 2007), and Pyini (Gorman, 2016). For this work we use Foma, as we are extending previous work on the Kunwinjku FST (Lane and Bird, 2019). The conventional approach for building an FST morph analyzer is to divide the task into two stages: (a) accounting for the lexicon and morphotactics, and (b) defining the morphophonological rules which use the context at morphotactic boundaries to produce valid surface forms (Beesley and Karttunen, 2003). Many researchers have followed this pattern to build FST analyzers for morphologically complex languages (Çöltekin, 2010; Harrigan et al., 2017; Bowers et al., 2017; Andriyanets and Tyers, 2018; Moeller et al., 2018; Cardenas and Zeman, 2018; Lane and Bird, 2019). Others deviate slightly, e.g., Chen and Schwartz implement a mechanism for stage 2 which applies morphophonemic rules at morpheme boundaries working from left to right (Chen and Schwartz, 2018). In any case, the practice of defining a lexicon of morphs to be concatenated to form the intermediate representation of a word is ubiquitous. Accordingly, the ability to mark morph boundaries at this intermediate level serves as one of the basic assumptions of"
2020.coling-main.405,W16-2409,0,0.0169887,"natural languages (Beesley and Karttunen, 2003). In recent years, researchers have demonstrated the suitability of finite state models for a variety of morphologically complex, low-resource languages including Cree, Haida, Kunwinjku, Odawa, Tsuut’ina, and Yupik (Snoek et al., 2014; Harrigan et al., 2017; Arppe et al., 2017a; Arppe et al., 2017b; Bowers et al., 2017; Chen and Schwartz, 2018; Lachler et al., 2018; Lane and Bird, 2019). The FST formalism is implemented by a number of frameworks including HFST (Lindén et al., 2013), Foma (Hulden, 2009), OpenFST (Allauzen et al., 2007), and Pyini (Gorman, 2016). For this work we use Foma, as we are extending previous work on the Kunwinjku FST (Lane and Bird, 2019). The conventional approach for building an FST morph analyzer is to divide the task into two stages: (a) accounting for the lexicon and morphotactics, and (b) defining the morphophonological rules which use the context at morphotactic boundaries to produce valid surface forms (Beesley and Karttunen, 2003). Many researchers have followed this pattern to build FST analyzers for morphologically complex languages (Çöltekin, 2010; Harrigan et al., 2017; Bowers et al., 2017; Andriyanets and Tyer"
2020.coling-main.405,E09-2008,0,0.0479537,"or modelling the phonological and morphological systems of natural languages (Beesley and Karttunen, 2003). In recent years, researchers have demonstrated the suitability of finite state models for a variety of morphologically complex, low-resource languages including Cree, Haida, Kunwinjku, Odawa, Tsuut’ina, and Yupik (Snoek et al., 2014; Harrigan et al., 2017; Arppe et al., 2017a; Arppe et al., 2017b; Bowers et al., 2017; Chen and Schwartz, 2018; Lachler et al., 2018; Lane and Bird, 2019). The FST formalism is implemented by a number of frameworks including HFST (Lindén et al., 2013), Foma (Hulden, 2009), OpenFST (Allauzen et al., 2007), and Pyini (Gorman, 2016). For this work we use Foma, as we are extending previous work on the Kunwinjku FST (Lane and Bird, 2019). The conventional approach for building an FST morph analyzer is to divide the task into two stages: (a) accounting for the lexicon and morphotactics, and (b) defining the morphophonological rules which use the context at morphotactic boundaries to produce valid surface forms (Beesley and Karttunen, 2003). Many researchers have followed this pattern to build FST analyzers for morphologically complex languages (Çöltekin, 2010; Harri"
2020.coling-main.405,N19-4021,0,0.0202115,"rface which helps users enter morphologically complex words and which retrieves corresponding entries from the lexicon. 1 Introduction Indigenous communities in Australia’s far north are engaged in a variety of efforts to maintain their languages. In the Kunwinjku-speaking communities of West Arnhem, the challenges are to maintain orality and develop literacy. Technological solutions can support community efforts to create language resources and make them accessible. These solutions include dictionaries, digital archives, and collaborative applications for documentation and language learning (Hunt et al., 2019; Bow et al., 2014; Fillmore et al., 2019). All of these technologies require text input, posing a challenge for communities whose languages are primarily oral, and where writing—especially writing that conforms to an orthography standard—is not an established practice. In the context of morphologically complex languages, the term “intelligent dictionary” has been applied to systems which morphologically analyze the query in the course of retrieving relevant lexical entries (Johnson et al., 2013; Arppe et al., 2016). This is an important feature for speakers of polysynthetic languages like Kun"
2020.coling-main.405,W13-5610,0,0.0232918,"ictionaries, digital archives, and collaborative applications for documentation and language learning (Hunt et al., 2019; Bow et al., 2014; Fillmore et al., 2019). All of these technologies require text input, posing a challenge for communities whose languages are primarily oral, and where writing—especially writing that conforms to an orthography standard—is not an established practice. In the context of morphologically complex languages, the term “intelligent dictionary” has been applied to systems which morphologically analyze the query in the course of retrieving relevant lexical entries (Johnson et al., 2013; Arppe et al., 2016). This is an important feature for speakers of polysynthetic languages like Kunwinjku, which have multiple morph slots with sometimes large or unbounded sets of fillers. However, effective use of an intelligent dictionary still requires the ability to enter the fully-inflected surface form. For speakers and learners of morphologically complex languages, producing well-formed queries may pose a significant challenge. Littell et al. make a similar observation in their work on dictionary interfaces for North American languages, where few speakers are “both fluent in the langu"
2020.coling-main.405,U19-1001,1,0.728114,"hologically complex languages The proposed approach builds on the established practice of applying finite state transducers for modelling the phonological and morphological systems of natural languages (Beesley and Karttunen, 2003). In recent years, researchers have demonstrated the suitability of finite state models for a variety of morphologically complex, low-resource languages including Cree, Haida, Kunwinjku, Odawa, Tsuut’ina, and Yupik (Snoek et al., 2014; Harrigan et al., 2017; Arppe et al., 2017a; Arppe et al., 2017b; Bowers et al., 2017; Chen and Schwartz, 2018; Lachler et al., 2018; Lane and Bird, 2019). The FST formalism is implemented by a number of frameworks including HFST (Lindén et al., 2013), Foma (Hulden, 2009), OpenFST (Allauzen et al., 2007), and Pyini (Gorman, 2016). For this work we use Foma, as we are extending previous work on the Kunwinjku FST (Lane and Bird, 2019). The conventional approach for building an FST morph analyzer is to divide the task into two stages: (a) accounting for the lexicon and morphotactics, and (b) defining the morphophonological rules which use the context at morphotactic boundaries to produce valid surface forms (Beesley and Karttunen, 2003). Many rese"
2020.coling-main.405,W17-0119,0,0.368261,"ature for speakers of polysynthetic languages like Kunwinjku, which have multiple morph slots with sometimes large or unbounded sets of fillers. However, effective use of an intelligent dictionary still requires the ability to enter the fully-inflected surface form. For speakers and learners of morphologically complex languages, producing well-formed queries may pose a significant challenge. Littell et al. make a similar observation in their work on dictionary interfaces for North American languages, where few speakers are “both fluent in the language and trained in a systematic orthography” (Littell et al., 2017). While they address the problem of retrieving dictionary entries given a noisy query, we go further, guiding users as they type by suggesting completions up to the next morph boundary. The typical definition of “autocomplete” implies word-level completion (Cai et al., 2016; Dunlop and Crossan, 2000), but this is impractical for polysynthetic languages where single words are capable of expressing the meaning of an entire sentence. For low-resource polysynthetic languages, we propose a version of autocomplete which serves a different purpose: guiding writers who are building confidence, This wo"
2020.coling-main.405,L18-1294,0,0.0160584,"along with an extension which allows for imprecise input. The main resource assumed in this work is an FST model of morphology in the target language. The starting point is an FST which maps a surface form to its morphological analysis. For example, the analysis of wokmadbuni, “I was waiting for you to call,” is 1sg.2.past-wokmadbu-PP. This approach assumes that the FST model marks morpheme boundaries. A survey of recent work on FST morphological analyzers for morphologically rich languages shows that marking morpheme boundaries as part of an intermediate representation is a common practice (Lovick et al., 2018; Bowers et al., 2017; Chen and Schwartz, 2018; Andriyanets and Tyers, 2018; Snoek et al., 2014; Çöltekin, 2010). Thus, we anticipate that this is not an onerous requirement. Figure 2 shows morphotactic rules of the language, producing intermediate forms that are processed by the alternation rules. This minimal working example represents the foundation on which we build in the next section. 3.1 Morph-based autocomplete We start with a morph analyzer, e.g., Grammar in Figure 2(b). The process is described in Algorithm 1 using the XFST syntax (Beesley and Karttunen, 2003). We implement the algor"
2020.coling-main.405,L18-1653,0,0.0104648,"implement a mechanism for stage 2 which applies morphophonemic rules at morpheme boundaries working from left to right (Chen and Schwartz, 2018). In any case, the practice of defining a lexicon of morphs to be concatenated to form the intermediate representation of a word is ubiquitous. Accordingly, the ability to mark morph boundaries at this intermediate level serves as one of the basic assumptions of the work reported here. 2.2 Language modelling for morphologically complex languages Some researchers have attempted to build language models for morphologically complex Indigenous languages. Maheshwari et al (2018) collected a corpus for Mik’maq—a polysynthetic North American language—and developed n-gram and neural character-based language models. Their results highlight the difficulty of learning predictive models—with statistical or neural methods—from small corpora. Neural polysynthetic language modelling was also the theme of a workshop whose activities are reported by Schwartz et al (2020). They find that morphological segmentation of the training data resulted in better language models. They deployed their Tensorflow models to a modified Divvun backend (Moshagen et al., 2013) to create a prototyp"
2020.coling-main.405,W18-4802,0,0.0382868,"s work we use Foma, as we are extending previous work on the Kunwinjku FST (Lane and Bird, 2019). The conventional approach for building an FST morph analyzer is to divide the task into two stages: (a) accounting for the lexicon and morphotactics, and (b) defining the morphophonological rules which use the context at morphotactic boundaries to produce valid surface forms (Beesley and Karttunen, 2003). Many researchers have followed this pattern to build FST analyzers for morphologically complex languages (Çöltekin, 2010; Harrigan et al., 2017; Bowers et al., 2017; Andriyanets and Tyers, 2018; Moeller et al., 2018; Cardenas and Zeman, 2018; Lane and Bird, 2019). Others deviate slightly, e.g., Chen and Schwartz implement a mechanism for stage 2 which applies morphophonemic rules at morpheme boundaries working from left to right (Chen and Schwartz, 2018). In any case, the practice of defining a lexicon of morphs to be concatenated to form the intermediate representation of a word is ubiquitous. Accordingly, the ability to mark morph boundaries at this intermediate level serves as one of the basic assumptions of the work reported here. 2.2 Language modelling for morphologically complex languages Some rese"
2020.coling-main.405,W13-5631,0,0.0182357,"digenous languages. Maheshwari et al (2018) collected a corpus for Mik’maq—a polysynthetic North American language—and developed n-gram and neural character-based language models. Their results highlight the difficulty of learning predictive models—with statistical or neural methods—from small corpora. Neural polysynthetic language modelling was also the theme of a workshop whose activities are reported by Schwartz et al (2020). They find that morphological segmentation of the training data resulted in better language models. They deployed their Tensorflow models to a modified Divvun backend (Moshagen et al., 2013) to create a prototype mobile keyboard with predictive text, and propose an algorithm for generating predictions up to the next morph boundary from a neural language model. 4601 TSO 157 DIR x 3 ASP x 2 MSC1^ x 24 BEN x 2 MSC2 x 4 GIN x 78 BPIN* x 32 COM x 2 Root* x 541 RR x 2 TAM x 5 Total = 4.9x1012 Figure 1: An estimate for all morphotactically valid verb morph sequences covered by the Kunwinjku FST implemented in Lane and Bird (2019). Slots marked with an asterisk (*) are open class. 2.3 Flexible text input for morphologically complex languages Kunwinjku is primarily an oral language, and t"
2020.coling-main.405,W14-2205,0,0.285092,"age technology for orallanguage communities who speak morphologically complex languages. 2 2.1 Background and Related Work Finite state analysis for morphologically complex languages The proposed approach builds on the established practice of applying finite state transducers for modelling the phonological and morphological systems of natural languages (Beesley and Karttunen, 2003). In recent years, researchers have demonstrated the suitability of finite state models for a variety of morphologically complex, low-resource languages including Cree, Haida, Kunwinjku, Odawa, Tsuut’ina, and Yupik (Snoek et al., 2014; Harrigan et al., 2017; Arppe et al., 2017a; Arppe et al., 2017b; Bowers et al., 2017; Chen and Schwartz, 2018; Lachler et al., 2018; Lane and Bird, 2019). The FST formalism is implemented by a number of frameworks including HFST (Lindén et al., 2013), Foma (Hulden, 2009), OpenFST (Allauzen et al., 2007), and Pyini (Gorman, 2016). For this work we use Foma, as we are extending previous work on the Kunwinjku FST (Lane and Bird, 2019). The conventional approach for building an FST morph analyzer is to divide the task into two stages: (a) accounting for the lexicon and morphotactics, and (b) def"
2021.dash-1.16,2020.coling-main.313,1,0.788894,"on leverages the kind of work ful in low resource contexts, including spoken document retrieval and language learning. speakers are motivated to do. For example, when it comes to recordings, speakers tend to engage with 1 Introduction the content more than the particular form of expression (Maddieson, 2001, page 215). Identifying key Understanding the “transcription challenge” is a words and clarifying their meanings is often more prerequisite to designing effective solutions, miniengaging than puzzling over the transcription of mizing bottlenecks (Himmelmann, 2018). We must unclear passages (Bird, 2020b). An indexed corpus face realities such as the lack of a good lexicon, the can be searched to identify additional high-value short supply of transcribers, and the difficulty of engaging people in arduous work. Sparse tran- recordings for transcription. We report on a computational model for interscription is an approach to transcribing speech in active transcription in low-resource situations. We these low-resource situations, an approach which is well suited to places where there is limited capac- discuss the kinds of interactivity which the sparse transcription model enables, and propose a"
2021.dash-1.16,U19-1001,1,0.740399,"t from word spotting, which locates more tokens of existing glossary entries. Local word discovery attempts to fill in untranscribed regions between existing tokens. This agent provides transcription hints via a smaller feedback loop, the third kind of interactivity discussed in Section 3. The system retrieves the potentially large set of suggested words, and filters it down interactively as the transcriber types. The model is free to favor recall, because the raw suggestions do not need to be immediately revealed. We implement local word discovery using a finite state analyzer for Kunwinjku (Lane and Bird, 2019), modified to recognize possible word-forms given a stream of phones and the offsets of known lexemes. We use PanPhon to estimate articulatory distances between lexemes and phone subsequences to obtain rough alignments (Mortensen et al., 2016). 5 User Interface The user interface (Fig. 5) is inspired by minimalist design, motivated by the need for an inclusive agenda in language work (cf. Hatton, 2013). In the left column is a waveform which has been automatically segmented into breath groups. Below the waveform is a map of waveform peaks, to facilitate navigation across long audio files. Usef"
2021.dash-1.16,2020.coling-main.303,1,0.72061,"Missing"
2021.dash-1.16,C16-1328,0,0.0226643,"hird kind of interactivity discussed in Section 3. The system retrieves the potentially large set of suggested words, and filters it down interactively as the transcriber types. The model is free to favor recall, because the raw suggestions do not need to be immediately revealed. We implement local word discovery using a finite state analyzer for Kunwinjku (Lane and Bird, 2019), modified to recognize possible word-forms given a stream of phones and the offsets of known lexemes. We use PanPhon to estimate articulatory distances between lexemes and phone subsequences to obtain rough alignments (Mortensen et al., 2016). 5 User Interface The user interface (Fig. 5) is inspired by minimalist design, motivated by the need for an inclusive agenda in language work (cf. Hatton, 2013). In the left column is a waveform which has been automatically segmented into breath groups. Below the waveform is a map of waveform peaks, to facilitate navigation across long audio files. Useful context is also displayed, including the transcript of the preceding breath group, followed by the sequence of phones produced from the audio, with user transcriptions aligned roughly to the phone sequence. Below this is the input box, scop"
2021.emnlp-main.157,L18-1530,1,0.848164,"serves a number of realworld use cases aside from contiguous transcription, e.g. spotted words serve as an index into the audio, facilitating keyword-based retrieval across large corpora; and lexical entries and associated metadata can be used in language learning. This research takes place in the context of a se- 3.1 Task definition ries of engagements with the Bininj community of The starting point for local word discovery is an West Arnhem, in the far north of Australia. The audio file, preprocessed using a phone recognizer community is centered in the town of Gunbalanya (Li et al., 2020; Adams et al., 2018). We view the and a network of outstations, and predominantly output as a noisy, low-dimensional representations speaks Kunwinjku. Schools, ranger programs and of the signal (Figure 3, line Q). arts centres employ local people in cultural work We assume an early transcription scenario, where literacy in Kunwinjku is considered desirwhere non-speaker transcribers are learning to tranable, though not yet well established. scribe the language. The audio is manually anKunwinjku has limited electronic texts and lexicons, but there is a comprehensive grammar (Evans, notated with lexemes that non-spe"
2021.emnlp-main.157,C16-1086,0,0.0357783,"Missing"
2021.emnlp-main.157,2015.iwslt-papers.18,1,0.8297,"Missing"
2021.emnlp-main.157,N19-1009,0,0.0199552,"Missing"
2021.emnlp-main.157,D19-1572,0,0.0200972,"Missing"
2021.emnlp-main.157,N18-1032,0,0.0205265,"Missing"
2021.emnlp-main.157,L18-1657,0,0.0607632,"Missing"
2021.emnlp-main.157,N09-1036,0,0.066303,"Missing"
2021.emnlp-main.157,U19-1001,1,0.709948,"sity of sparse transcriptions. We integrate the new task of local word discovery with existing Task S (word spotting) and Task G (growing the glossary) (Bird, 2020), to form a new interactive workflow. Speaker Time (hh:mm:ss) PER GN TG DY RN SG MM 00:12:21 00:09:21 00:23:28 00:15:35 00:10:27 00:07:44 .289 .338 .417 .289 .256 .321 Total: 01:18:56 AVG: .318 Figure 6: Allosaurus phone error rate (PER) for each speaker held out as validation, with the model finetuned on the remaining speakers. have a detailed linguistic description (Evans, 2003), and an implementation of the morphology as an FST (Lane and Bird, 2019). This FST recognizes valid morphotactic sequences in Kunwinjku, and transduces to a morphological analysis. We can take the lower side of this transducer to obtain an FSA which recognizes the language of licensed surface forms of full words. From here, we build up the regular expression around the surface form to allow for the skipping of arbitrary characters on either side of the word. We have opted to output a padding character when we encounter characters which do not belong to the recognized word, for the purpose of retaining offset information. See Algorithm 1 for the implementation of t"
2021.emnlp-main.157,2020.coling-main.303,1,0.821002,"Missing"
2021.emnlp-main.157,C18-1222,0,0.0353205,"Missing"
2021.emnlp-main.157,Q13-1021,0,0.0328343,"Missing"
2021.emnlp-main.157,C16-1328,0,0.0232158,"e for that grapheme (Figure 7). 4 Experiment Setup We explore the concept of local word discovery on sparsely-transcribed audio by measuring the change in transcription densities before and after applying local word discovery implemented with the FST. The first step is to define an initial lexicon which we use to sparsely transcribe a collection of audio. We used the collection of transcribed utterances from speaker SG as the test set, and the automatic phone recognition model which was fined tuned Accounting for phone noise. We used Pan- on all but SG’s speech. From the transcriptions Phone (Mortensen et al., 2016) to acquire vec- of SG’s speech, we identify the 10 most frequent tor representations of each phone for both the Al- morphs and locate them in the speech. This is the losaurus representation and the orthographic-to- input for word discovery (Figure 3, line I). This proIPA mapping. We computed the cosine distance duced 126 annotated utterances: 126 breath groups between each phone in both representations, form- represented by their phone stream, with individing a matrix of distance calculations. In the FST, ual tokens of the lexemes from the initial lexicon 2062 Algorithm 1 Finite State Word Di"
2021.emnlp-main.157,P12-1018,0,0.0257552,"Missing"
bird-etal-2000-atlas,A97-1051,1,\N,Missing
bird-etal-2000-atlas,bird-etal-2000-towards,1,\N,Missing
bird-etal-2000-atlas,cunningham-etal-2000-software,0,\N,Missing
bird-etal-2000-towards,W99-0302,0,\N,Missing
bird-etal-2002-tabletrans,ma-etal-2002-models,1,\N,Missing
bird-etal-2008-acl,D07-1089,0,\N,Missing
bird-etal-2008-acl,W06-1613,0,\N,Missing
bird-etal-2008-acl,N04-1042,0,\N,Missing
bird-etal-2008-acl,radev-etal-2004-mead,1,\N,Missing
C12-2013,P10-1010,1,0.908867,"Missing"
C12-2013,W11-1216,1,0.836094,"ata that would be more useful for machine translation experiments, the following steps would be required. First, the primary textual sources should be audio recordings, and transcribed using a tool that preserves the audio alignment (for later verification) and which links wordforms to lexemes (for consistency in spelling, word breaks, and glosses). Second, the transcription and glossing software should operate in tandem with curating a shared n-language lexicon to speed up the process and encourage consistency across speakers, possibly using the structures described in (Baldwin et al., 2010; Abney and Bird, 2011). 5 Conclusion Most of the world’s languages will fall out of use before the world’s linguists and computational linguists are able to collect sufficient data. However, we have been investigating simple methodologies and supporting software that are helping speakers of endangered languages in Papua New Guinea to produce usable documentation on their own. The primary data type is bilingual text – or interlinear glossed text – which serves the dual purpose of documenting a language and developing translation models. Once the translation models reach an adequate level, they could be usable as the"
C12-2013,C10-3010,0,0.0306534,"nerate a quantity of data that would be more useful for machine translation experiments, the following steps would be required. First, the primary textual sources should be audio recordings, and transcribed using a tool that preserves the audio alignment (for later verification) and which links wordforms to lexemes (for consistency in spelling, word breaks, and glosses). Second, the transcription and glossing software should operate in tandem with curating a shared n-language lexicon to speed up the process and encourage consistency across speakers, possibly using the structures described in (Baldwin et al., 2010; Abney and Bird, 2011). 5 Conclusion Most of the world’s languages will fall out of use before the world’s linguists and computational linguists are able to collect sufficient data. However, we have been investigating simple methodologies and supporting software that are helping speakers of endangered languages in Papua New Guinea to produce usable documentation on their own. The primary data type is bilingual text – or interlinear glossed text – which serves the dual purpose of documenting a language and developing translation models. Once the translation models reach an adequate level, they"
C12-2013,I11-1059,1,0.840007,"recording device. The first step is to create a text, either by recording then transcribing, or by composing directly onto paper. Chances are that the speaker will have no experience at IPA transcription and that no standardised orthography for the language exists. Thus, transcription needs to use whatever orthography people know. This practice has some documentary value, for it shows meaningful sound contrasts and word boundaries, and serves as a rough finding aid. In cases where more than one speaker transcribes content in a language, we can try to clean up the transcriptions automatically (Foda and Bird, 2011). The second step is to translate the text, providing word by word glosses plus a phrasal translation. The correspondence between this literal and “free” translation amounts to training data for an alignment model, and does not require a separate translation model. The final step is to prepare a lexicon, in order to help fix the inconsistencies in spelling and glossing between. SIL’s Fieldworks Language Explorer (FLEx) software is ideal for this purpose, though it currently lacks support for synchronisation and conflict resolution between databases. An important refinement is to conduct the ab"
C12-2013,P12-2059,0,0.0266688,"epare a lexicon, in order to help fix the inconsistencies in spelling and glossing between. SIL’s Fieldworks Language Explorer (FLEx) software is ideal for this purpose, though it currently lacks support for synchronisation and conflict resolution between databases. An important refinement is to conduct the above workflow within a cluster of closely related languages. Speakers often produce a wealth of information about lexical correspondences with neighboring languages, as illustrated in Figure 1. Armed with these correspondences, we can pool knowledge about all the languages in the cluster (Nakov and Tiedemann, 2012). We can also try to guess word translations by leveraging regular sound correspondences. eng sun water fire earth tree mountain house food pig man woman father mother aso ho noso olo misumbo ya golo numuno nosonite ije we vene meneho’we ijeneho bef yege nagami logo mei yafa kosa nohi nosena yaga bo amo afonifu itonifu gah ho nagami lo mikasi za agoka numuni nosa’neta iza ve vena ahono izo’no ino yake tina ata mopa yosa akoya nona neya afu ve a’ne afo nimo’e ita kbq zge tina teve mo’pa zafa agona nona ne’zane afu ve’nene a’re nenfa anta’nimo snp fo no soo mika yaa obura numuna aáwa’a savu wee"
C12-2013,N07-1057,0,0.188144,"aller data files. To facilitate access, the raw data is usually transcribed and translated. It should be clear that language documentation is not the same as linguistic description, which calls for linguistic expertise and which produces systematic presentations of the phonology, morphology, syntax, and semantics of the language. Nevertheless, the descriptive work cannot proceed without the language documentation. This documentation – the bilingual text collection – is the same as what is needed for statistical MT and we can expect to apply MT algorithms to the data from linguistic fieldwork (Xia and Lewis, 2007; Palmer et al., 2010). The workflow for language documentation and description has never been standardised, but there is general agreement that it involves at least the following activities: (a) recording communicative events; (b) transcribing and translating the recordings; (c) performing basic morphosyntactic analysis leading to a lexicon and to a collection of morphologically-glossed text; (d) eliciting paradigms, i.e. systematic tabulations of linguistic forms designed to reveal underlying patterns; (e) preparing descriptive reports to show how the language is structured. These activities"
C12-3022,heid-etal-2004-querying,0,0.0350596,"nd correcting a parse error, it is desirable to quickly locate other instances of the same error, regardless of where they appear in the corpus. Syntactic research depends on manual exploration of conditioning factors that allow us to identify constructions of interest, and these constructions will usually be rare. Such activities require efficient query over very large treebanks. The last decade has seen the development of several corpus query tools, including TGrep2, TIGERSearch, Emu, Nite NXT Search, Netgraph, fsq, and Emdros (Rohde, 2001; Brants et al., 2002; Cassidy and Harrington, 2001; Heid et al., 2004; Mírovský, 2006; Kepser, 2003; Petersen, 2004). These tools are effective when the corpus fits in main memory, or when work can be done in batch mode (requiring a linear pass through the entire corpus on disk, typically taking tens of seconds). When the corpus is large, and when fast query processing is required, an entirely different approach is needed. Fangorn is designed to fill this gap. It is the outcome of an interdisciplinary research project combining the scaling properties of general purpose semi-structured databases and information retrieval engines with the features commonly found"
C12-3022,E03-1074,0,0.815521,"desirable to quickly locate other instances of the same error, regardless of where they appear in the corpus. Syntactic research depends on manual exploration of conditioning factors that allow us to identify constructions of interest, and these constructions will usually be rare. Such activities require efficient query over very large treebanks. The last decade has seen the development of several corpus query tools, including TGrep2, TIGERSearch, Emu, Nite NXT Search, Netgraph, fsq, and Emdros (Rohde, 2001; Brants et al., 2002; Cassidy and Harrington, 2001; Heid et al., 2004; Mírovský, 2006; Kepser, 2003; Petersen, 2004). These tools are effective when the corpus fits in main memory, or when work can be done in batch mode (requiring a linear pass through the entire corpus on disk, typically taking tens of seconds). When the corpus is large, and when fast query processing is required, an entirely different approach is needed. Fangorn is designed to fill this gap. It is the outcome of an interdisciplinary research project combining the scaling properties of general purpose semi-structured databases and information retrieval engines with the features commonly found in corpus query tools (Ghodke"
C12-3022,U04-1019,1,0.885222,"arch), while others are designed for dependency trees (e.g. Netgraph). Some are intended for corpora with multiple annotation types (e.g. Nite NXT Search, Emu). Some permit phrase structure trees with crossing branches (e.g. TIGERSearch) while others require strict trees (e.g. TGrep2). Some support query of extra properties on tree nodes or edges. Despite this diversity, there are still some abstract requirements that are common across all corpus query tools. All tools support expressive query languages, although the great variety of syntax obscures the expressive similarity of the languages. Lai and Bird (2004) compare several corpus query languages and present a few generic requirements for treebank query languages. They state that the query language should include more than just simple navigational operators and include features such as: subtree matching, non-tree navigation (e.g. the “immediate following” operator explained later), secondary edges, and closure operators. They should also handle Boolean operators: conjunction, disjunction, and negation. The language should be able to specify the granularity of results. While query language operators depend largely on the structure of the data, som"
C12-3022,J93-2004,0,0.0419501,"njunction, disjunction, and negation. The language should be able to specify the granularity of results. While query language operators depend largely on the structure of the data, some features such as finding secondary edges in a tree are specific to annotation style. Query languages usually support regular expressions over node labels, allowing search for label prefixes and suffixes (e.g. N.* for any noun-like syntactic category). Support for popular file formats is another requirement. Most corpus query tools accept one or more of the following annotation formats as inputs: Penn Treebank (Marcus et al., 1993), NEGRA (Skut et al., 1997), TIGER corpus (Brants et al., 2002), or custom XML formats. 176 Most tree query tools are not designed for very large data since they perform a linear pass over the entire collection, e.g. TGrep. More advanced tools store and index tree data using relational databases. However, relational databases are not efficient for storing and accessing trees (Zhang et al., 2001). On the other hand, semistructured databases have indexes that are specialised for efficient operations on large tree collections. Unfortunately, their query languages lack the required expressiveness."
C12-3022,C04-1172,0,0.391159,"uickly locate other instances of the same error, regardless of where they appear in the corpus. Syntactic research depends on manual exploration of conditioning factors that allow us to identify constructions of interest, and these constructions will usually be rare. Such activities require efficient query over very large treebanks. The last decade has seen the development of several corpus query tools, including TGrep2, TIGERSearch, Emu, Nite NXT Search, Netgraph, fsq, and Emdros (Rohde, 2001; Brants et al., 2002; Cassidy and Harrington, 2001; Heid et al., 2004; Mírovský, 2006; Kepser, 2003; Petersen, 2004). These tools are effective when the corpus fits in main memory, or when work can be done in batch mode (requiring a linear pass through the entire corpus on disk, typically taking tens of seconds). When the corpus is large, and when fast query processing is required, an entirely different approach is needed. Fangorn is designed to fill this gap. It is the outcome of an interdisciplinary research project combining the scaling properties of general purpose semi-structured databases and information retrieval engines with the features commonly found in corpus query tools (Ghodke and Bird, 2008, 2"
C12-3022,A97-1014,0,0.0422061,"egation. The language should be able to specify the granularity of results. While query language operators depend largely on the structure of the data, some features such as finding secondary edges in a tree are specific to annotation style. Query languages usually support regular expressions over node labels, allowing search for label prefixes and suffixes (e.g. N.* for any noun-like syntactic category). Support for popular file formats is another requirement. Most corpus query tools accept one or more of the following annotation formats as inputs: Penn Treebank (Marcus et al., 1993), NEGRA (Skut et al., 1997), TIGER corpus (Brants et al., 2002), or custom XML formats. 176 Most tree query tools are not designed for very large data since they perform a linear pass over the entire collection, e.g. TGrep. More advanced tools store and index tree data using relational databases. However, relational databases are not efficient for storing and accessing trees (Zhang et al., 2001). On the other hand, semistructured databases have indexes that are specialised for efficient operations on large tree collections. Unfortunately, their query languages lack the required expressiveness. An ideal system for large"
C12-3022,N10-1034,1,\N,Missing
C14-1096,P10-1010,1,0.285211,"indigenous communities. 1 Introduction Past the top one to three hundred economically significant languages, there are few prospects for resourcing the production of annotated corpora. Advances in natural language processing have relied on such corpora – including treebanks and wordnets – though they are expensive to produce and depend on substantial prior scholarship on the language. An alternative is to collect bilingual aligned text, relating a low-resource language to a high-resource language, and then infer lexical and syntactic information from the high-resource language via alignments (Abney and Bird, 2010; Baldwin et al., 2010; Palmer et al., 2010; Das and Petrov, 2011). This approach only works for written languages. Over half the world’s languages lack a literary tradition. In some cases they have a writing system, but it is not in regular use and so these languages remain effectively unwritten. Collecting data for unwritten languages necessarily involves speech. f1f2 f3f4 f5f6 f7f8 f9f10f11 f12f13f14 f15f16 f17f18f19 f20f21 f22f23 f24f25 f26f27 f1f2f3f4f5f6f7f8 f9f10f11f12f13f14 w1 w2 w3 w4 w5 w6 f15f16f17f18f19f20f21 f22f23f24f25f26f27 w7 w8 w9 w10 w11w12 w3 w4 w2 w1 w6 w5 w8 w7 w9 w10 w12"
C14-1096,C10-3010,0,0.058025,". 1 Introduction Past the top one to three hundred economically significant languages, there are few prospects for resourcing the production of annotated corpora. Advances in natural language processing have relied on such corpora – including treebanks and wordnets – though they are expensive to produce and depend on substantial prior scholarship on the language. An alternative is to collect bilingual aligned text, relating a low-resource language to a high-resource language, and then infer lexical and syntactic information from the high-resource language via alignments (Abney and Bird, 2010; Baldwin et al., 2010; Palmer et al., 2010; Das and Petrov, 2011). This approach only works for written languages. Over half the world’s languages lack a literary tradition. In some cases they have a writing system, but it is not in regular use and so these languages remain effectively unwritten. Collecting data for unwritten languages necessarily involves speech. f1f2 f3f4 f5f6 f7f8 f9f10f11 f12f13f14 f15f16 f17f18f19 f20f21 f22f23 f24f25 f26f27 f1f2f3f4f5f6f7f8 f9f10f11f12f13f14 w1 w2 w3 w4 w5 w6 f15f16f17f18f19f20f21 f22f23f24f25f26f27 w7 w8 w9 w10 w11w12 w3 w4 w2 w1 w6 w5 w8 w7 w9 w10 w12 w11 w&apos;1 w&apos;2 w&apos;3 w&apos;4 w"
C14-1096,C12-1016,0,0.0134929,", but few of its speakers know how to write it. Such languages are collectively spoken by billions, yet remain seriously under-resourced. Thus, while our focus is on endangered languages, the approach applies to under-resourced languages in general. Several other promising approaches to the problems raised by endangered languages are being actively pursued in computational linguistics, however they typically focus on written language with annotations, often with the goal of making optimal use of human expertise (Probst et al., 2002; Levin et al., 2006; Clark et al., 2008; Palmer et al., 2010; Bender et al., 2012; Beale, 2012; Bender et al., 2013). The research reported here is unique in its focus on securing spoken language data in a form and on a scale that will be usable even once the languages in question are no longer spoken. This paper explores ways that networked smartphones can be used for collecting bilingual aligned audio. We have used a prototype Android application for collecting audio and phrase-aligned translations (or consecutive interpretations). We took a set of phones to villages in Brazil and Nepal, and worked with languages Temb´e, Nhengatu and Kagate. We visited at the invitation"
C14-1096,W13-2710,0,0.0125866,"to write it. Such languages are collectively spoken by billions, yet remain seriously under-resourced. Thus, while our focus is on endangered languages, the approach applies to under-resourced languages in general. Several other promising approaches to the problems raised by endangered languages are being actively pursued in computational linguistics, however they typically focus on written language with annotations, often with the goal of making optimal use of human expertise (Probst et al., 2002; Levin et al., 2006; Clark et al., 2008; Palmer et al., 2010; Bender et al., 2012; Beale, 2012; Bender et al., 2013). The research reported here is unique in its focus on securing spoken language data in a form and on a scale that will be usable even once the languages in question are no longer spoken. This paper explores ways that networked smartphones can be used for collecting bilingual aligned audio. We have used a prototype Android application for collecting audio and phrase-aligned translations (or consecutive interpretations). We took a set of phones to villages in Brazil and Nepal, and worked with languages Temb´e, Nhengatu and Kagate. We visited at the invitation of the local communities and collab"
C14-1096,W14-2201,1,0.751114,". Similarly, the best translators may not be the best transcribers; they may be illiterate. Thus, for reasons of skill, not just scale, we need to involve a whole team of people in the data collection activity. In the medium term, we assume that this work would take place under the supervision of a linguist who provides hardware and training, and who monitors the balance of the collection, including coverage of various discourse types, getting everything translated, and so forth). Aikuma is open source software that supports recording of speech directly to phone storage (Hanke and Bird, 2013; Bird et al., 2014). Recordings are synchronized with other phones that are connected to the same WiFi LAN, so that any user can listen to recordings made on any phone in the same local 1016 network. A user can “like” a recording to improve its overall ranking. A user can also provide a phraseby-phrase spoken translation of the recording, using the interface shown in Figure 2. This functionality is based on the protocol of “Basic Oral Language Documentation” (Reiman, 2010; Bird, 2010). (a) Recording a Temb´e narrative (b) Translating Temb´e into Portuguese Figure 2: Recording and translating using the Aikuma And"
C14-1096,clark-etal-2008-toward,0,0.0116011,"s spoken by 17 million people in Ethiopia, but few of its speakers know how to write it. Such languages are collectively spoken by billions, yet remain seriously under-resourced. Thus, while our focus is on endangered languages, the approach applies to under-resourced languages in general. Several other promising approaches to the problems raised by endangered languages are being actively pursued in computational linguistics, however they typically focus on written language with annotations, often with the goal of making optimal use of human expertise (Probst et al., 2002; Levin et al., 2006; Clark et al., 2008; Palmer et al., 2010; Bender et al., 2012; Beale, 2012; Bender et al., 2013). The research reported here is unique in its focus on securing spoken language data in a form and on a scale that will be usable even once the languages in question are no longer spoken. This paper explores ways that networked smartphones can be used for collecting bilingual aligned audio. We have used a prototype Android application for collecting audio and phrase-aligned translations (or consecutive interpretations). We took a set of phones to villages in Brazil and Nepal, and worked with languages Temb´e, Nhengatu"
C14-1096,P11-1061,0,0.0137558,"hundred economically significant languages, there are few prospects for resourcing the production of annotated corpora. Advances in natural language processing have relied on such corpora – including treebanks and wordnets – though they are expensive to produce and depend on substantial prior scholarship on the language. An alternative is to collect bilingual aligned text, relating a low-resource language to a high-resource language, and then infer lexical and syntactic information from the high-resource language via alignments (Abney and Bird, 2010; Baldwin et al., 2010; Palmer et al., 2010; Das and Petrov, 2011). This approach only works for written languages. Over half the world’s languages lack a literary tradition. In some cases they have a writing system, but it is not in regular use and so these languages remain effectively unwritten. Collecting data for unwritten languages necessarily involves speech. f1f2 f3f4 f5f6 f7f8 f9f10f11 f12f13f14 f15f16 f17f18f19 f20f21 f22f23 f24f25 f26f27 f1f2f3f4f5f6f7f8 f9f10f11f12f13f14 w1 w2 w3 w4 w5 w6 f15f16f17f18f19f20f21 f22f23f24f25f26f27 w7 w8 w9 w10 w11w12 w3 w4 w2 w1 w6 w5 w8 w7 w9 w10 w12 w11 w&apos;1 w&apos;2 w&apos;3 w&apos;4 w&apos;5 w&apos;6 w&apos;7 w&apos;8 w&apos;8 w&apos;10 w&apos;11 w&apos;12 Figure 1:"
C14-1096,I13-1161,1,0.790037,"hey may be monolingual. Similarly, the best translators may not be the best transcribers; they may be illiterate. Thus, for reasons of skill, not just scale, we need to involve a whole team of people in the data collection activity. In the medium term, we assume that this work would take place under the supervision of a linguist who provides hardware and training, and who monitors the balance of the collection, including coverage of various discourse types, getting everything translated, and so forth). Aikuma is open source software that supports recording of speech directly to phone storage (Hanke and Bird, 2013; Bird et al., 2014). Recordings are synchronized with other phones that are connected to the same WiFi LAN, so that any user can listen to recordings made on any phone in the same local 1016 network. A user can “like” a recording to improve its overall ranking. A user can also provide a phraseby-phrase spoken translation of the recording, using the interface shown in Figure 2. This functionality is based on the protocol of “Basic Oral Language Documentation” (Reiman, 2010; Bird, 2010). (a) Recording a Temb´e narrative (b) Translating Temb´e into Portuguese Figure 2: Recording and translating"
C14-1096,P12-1005,0,0.00613861,"tion 4.0 International Licence. 1015 Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers, pages 1015–1024, Dublin, Ireland, August 23-29 2014. While the physical isolation of these languages presents a logistical challenge, it is still possible to collect hundreds of hours of speech using mobile devices (de Vries et al., 2014). Furthermore, there are promising signs that natural language processing methods and speech processing methods can be integrated (Zhang et al., 2004; Dredze et al., 2010; Vu et al., 2011; Siniscalchi et al., 2012; Lee and Glass, 2012). Thus, the challenge is to collect substantial quantities of bilingual aligned audio, transcribe the translations, extract phonetic features from the source language and, ultimately, produce bilingual aligned text (see Figure 1). We have chosen to focus on endangered languages because of the interesting and difficult challenges that are faced in collecting data. However, the resource problem exists even for vital languages having large speaker populations. For example, Shanghainese (Wu) is spoken by 77 million people in China, but is almost never written down because written Chinese is based"
C14-1096,C04-1168,0,0.042497,"Missing"
C14-1096,D10-1045,0,\N,Missing
C92-1015,E87-1002,0,0.258212,"s have independently come to embrace. The ultimate implications of this perspective for phonology is the removal of the role-representation distinction in favour of the description-object distinction4. Grammar formalisms like IIPSG also lack the rule-representation distinction. For exanlple, (1 a) is described by Pollard & Sag (1987, 149) as a role. However, they also provide an equivalent statement of the rule in the more conventional 'rewrite' notation (lb). (1) There have been some notable recent attempts to rescue the FST model from its linearity in order to encompass nonlinear phonology (Kay, 1987; Koroai, 1991). llowever, if a fundamental shift in perspective in phonology has indeed occurred, then these refinements to the FST model do not go far enough. We require a further restriction that a transducer can only add information: the set of symbols accepted on a particular cell of the 'surface' tape must be a subset of the set accepted on the corresponding cell of the 'lexical' tape. An I;ST so constrained is actually nothing more than a finite-state automaton (FSA). a. [ i t ~ t &gt; 1 n RIsYNlUx:ItJaX - mrs [COMe-DTRS ([l) b. [SUBCAT 0 ] ~ IIILFX ' l , (2 In this paper a constraint-base"
cotton-bird-2002-integrated,mengel-lezius-2000-xml,0,\N,Missing
cotton-bird-2002-integrated,J93-2004,0,\N,Missing
cotton-bird-2002-integrated,bosco-etal-2000-building,0,\N,Missing
cotton-bird-2002-integrated,bird-etal-2002-tabletrans,1,\N,Missing
cotton-bird-2002-integrated,moreno-etal-2000-treebank,0,\N,Missing
cotton-bird-2002-integrated,H01-1010,1,\N,Missing
D14-1096,N10-1083,0,0.0759904,"Missing"
D14-1096,J96-1002,0,0.0233732,"e DPM model while correcting for its inadequacies using direct supervision. We select only 1,000 annotated tokens to reflect a low resource scenario. A small supervised training sample is a more realistic form of supervision than a tag dictionary (noisy or otherwise). Although used in most prior work, a tag dictionary for a new language requires significant manual effort to construct. Garrette and Baldridge (2013) showed that a 1,000 token dataset could be collected very cheaply, requiring less than 2 hours of non-expert time. Our correction model makes use of a minimum divergence (MD) model (Berger et al., 1996), a variant of the maximum entropy model which biases the target distribution to be similar to a static reference distribution. The method has been used in several language applications including machine translation (Foster, 2000) and parsing (Plank and van Noord, 2008, Johnson and Riezler, 2000). These previous approaches have used various sources of reference distribution, e.g., incorporating information from a simpler model (Johnson and Riezler, 2000) or combining in- and outof-domain models (Plank and van Noord, 2008). Plank and van Noord (2008) concluded that this method for adding prior"
D14-1096,A00-1031,0,0.101349,"Missing"
D14-1096,W06-2920,0,0.500674,"than using a dictionary. We argue 1 http://www.wiktionary.org/ Das and Petrov (2011) Duong et al. (2013b) Li et al. (2012) T¨ackstr¨om et al. (2013) da 83.2 85.6 83.3 88.2 nl 79.5 84.0 86.3 85.9 de 82.8 85.4 85.4 90.5 el 82.5 80.4 79.2 89.5 it 86.8 81.4 86.5 89.3 pt 87.9 86.3 84.5 91.0 es 84.2 83.3 86.4 87.1 sv 80.5 81.0 86.1 88.9 Average 83.4 83.4 84.8 88.8 Table 1: Previously published token-level POS tagging accuracy for various models across 8 languages — Danish (da), Dutch (nl), German (de), Greek (el), Italian (it), Portuguese (pt), Spanish (es), Swedish (sv) — evaluated on CoNLL data (Buchholz and Marsi, 2006). that with a proper “guide”, we can take advantage of very limited annotated data. 2.1 Annotated data Our annotated data mainly comes from CoNLL shared tasks on dependency parsing (Buchholz and Marsi, 2006). The language specific tagsets are mapped into the universal tagset. We will use this annotated data mainly for evaluation. Table 2 shows the size of annotated data for each language. The 8 languages we are considering in this experiment are not actually resource-poor languages. However, running on these 8 languages makes our system comparable with previously proposed methods. Nevertheless"
D14-1096,D10-1056,0,0.024443,"Missing"
D14-1096,P11-1061,0,0.272017,"Missing"
D14-1096,I13-1177,1,0.896928,"Missing"
D14-1096,P13-2112,1,0.89119,"Missing"
D14-1096,P00-1006,0,0.0400144,"ictionary (noisy or otherwise). Although used in most prior work, a tag dictionary for a new language requires significant manual effort to construct. Garrette and Baldridge (2013) showed that a 1,000 token dataset could be collected very cheaply, requiring less than 2 hours of non-expert time. Our correction model makes use of a minimum divergence (MD) model (Berger et al., 1996), a variant of the maximum entropy model which biases the target distribution to be similar to a static reference distribution. The method has been used in several language applications including machine translation (Foster, 2000) and parsing (Plank and van Noord, 2008, Johnson and Riezler, 2000). These previous approaches have used various sources of reference distribution, e.g., incorporating information from a simpler model (Johnson and Riezler, 2000) or combining in- and outof-domain models (Plank and van Noord, 2008). Plank and van Noord (2008) concluded that this method for adding prior knowledge only works with high quality reference distributions, otherwise performance suffers. In contrast to these previous approaches, we consider the specific setting where both the learned model and the reference model so = P"
D14-1096,P08-1085,0,0.180683,"Missing"
D14-1096,W04-3229,0,0.406957,"Missing"
D14-1096,A00-2021,0,0.022186,"prior work, a tag dictionary for a new language requires significant manual effort to construct. Garrette and Baldridge (2013) showed that a 1,000 token dataset could be collected very cheaply, requiring less than 2 hours of non-expert time. Our correction model makes use of a minimum divergence (MD) model (Berger et al., 1996), a variant of the maximum entropy model which biases the target distribution to be similar to a static reference distribution. The method has been used in several language applications including machine translation (Foster, 2000) and parsing (Plank and van Noord, 2008, Johnson and Riezler, 2000). These previous approaches have used various sources of reference distribution, e.g., incorporating information from a simpler model (Johnson and Riezler, 2000) or combining in- and outof-domain models (Plank and van Noord, 2008). Plank and van Noord (2008) concluded that this method for adding prior knowledge only works with high quality reference distributions, otherwise performance suffers. In contrast to these previous approaches, we consider the specific setting where both the learned model and the reference model so = P (t|w) are both maximum entropy models. In this case we show that th"
D14-1096,2005.mtsummit-papers.11,0,0.110128,"Missing"
D14-1096,D12-1127,0,0.158782,"Missing"
D14-1096,J03-1002,0,0.00706494,"Missing"
D14-1096,E99-1010,0,0.0863202,"Missing"
D14-1096,petrov-etal-2012-universal,0,0.0846555,"the universal tagset. We will use this annotated data mainly for evaluation. Table 2 shows the size of annotated data for each language. The 8 languages we are considering in this experiment are not actually resource-poor languages. However, running on these 8 languages makes our system comparable with previously proposed methods. Nevertheless, we try to use as few resources as possible, in order to simulate the situation for resource-poor languages. Later in Section 6 we adapt the approach for Malagasy, a truly resource-poor language. 2.2 Universal tagset We employ the universal tagset from (Petrov et al., 2012) for our experiment. It consists of 12 common tags: NOUN, VERB, ADJ (adjective), ADV (adverb), PRON (pronoun), DET (determiner and article), ADP (preposition and postposition), CONJ (conjunctions), NUM (numerical), PRT (particle), PUNC (punctuation) and X (all other categories including foreign words and abbreviations). Petrov et al. (2012) provide the mapping from each language-specific tagset to the universal tagset. The idea of using the universal tagset is of great use in multilingual applications, enabling comparison across languages. However, the mapping is not always straightforward. Ta"
D14-1096,N13-1014,0,0.0990194,"cussed it makes many errors, due to invalid or inconsistent tag mappings, noisy alignments, and cross-linguistic syntactic divergence. However, our aim is to see how effectively we can exploit the strengths of the DPM model while correcting for its inadequacies using direct supervision. We select only 1,000 annotated tokens to reflect a low resource scenario. A small supervised training sample is a more realistic form of supervision than a tag dictionary (noisy or otherwise). Although used in most prior work, a tag dictionary for a new language requires significant manual effort to construct. Garrette and Baldridge (2013) showed that a 1,000 token dataset could be collected very cheaply, requiring less than 2 hours of non-expert time. Our correction model makes use of a minimum divergence (MD) model (Berger et al., 1996), a variant of the maximum entropy model which biases the target distribution to be similar to a static reference distribution. The method has been used in several language applications including machine translation (Foster, 2000) and parsing (Plank and van Noord, 2008, Johnson and Riezler, 2000). These previous approaches have used various sources of reference distribution, e.g., incorporating"
D14-1096,W08-1302,0,0.056461,"Missing"
D14-1096,P13-1057,0,0.203159,"Missing"
D14-1096,E14-1078,0,0.023072,"s. However, the mapping is not always straightforward. Table 2 shows the size of the annotated data for each language, the number of tags presented in the data, and the list of tags that are not matched. We can see that only 8 tags are presented in the annotated data for Danish, i.e, 4 tags (DET, PRT, PUNC, and NUM) are missing.2 Thus, a classifier using all 12 tags will be heavily penalized in the evaluation. Li et al. (2012) considered this problem and tried to manually modify the Danish mappings. Moreover, PRT is not really a universal tag since it only appears in 3 out of the 8 languages. Plank et al. (2014) pointed out that PRT often gets confused with ADP even in English. We will later show that the mapping problem causes substantial degradation in the performance of a POS tagger exploiting parallel data. The method we present here is more target-language oriented: our model is trained on the target language, in this way, only relevant information from the source language is retained. Thus, we automatically correct the mapping, and other incompatibilities arising from incorrect alignments and syntactic divergence between the source and target languages. Lang Size(k) # Tags da 94 8 nl 203 11 de"
D14-1096,W11-3603,0,0.0222103,"Missing"
D14-1096,Q13-1001,0,0.132253,"Missing"
D14-1096,N03-1033,0,0.0842473,"Missing"
D14-1096,P08-1086,0,0.101901,"Missing"
D14-1096,N01-1026,0,0.12215,"Missing"
D15-1040,D12-1001,0,0.060049,"s are highly informative of dependency relations, and that there exists shared dependency structures across languages. Building a dependency parser for a resourcepoor language usually starts with the delexicalized parser and then uses other resources to refine the model. McDonald et al. (2011) and Ma and Xia (2014) exploited parallel data as the bridge to transfer constraints from the source resourcerich language to the target resource-poor languages. T¨ackstr¨om et al. (2012) also used parallel data to induce cross-lingual word clusters which added as features for their delexicalized parser. Durrett et al. (2012) constructed the set of language-independent features and used a bilingual dictionary as the bridge to transfer these features from source to target language. T¨ackstr¨om et al. (2013) additionally used high-level linguistic features extracted from the World Atlas of Language Structures (WALS) (Dryer and Haspelmath, 2013). For low-resource languages, no large parallel corpus is available. Some linguists are dependency-annotating small amounts of field data, e.g. for Karuk, a nearly-extinct language of Northwest California (Garrett et al., 2013). Accordingly, we adopt a different resource requi"
D15-1040,D14-1034,0,0.007514,"Missing"
D15-1040,kamholz-etal-2014-panlex,0,0.0227054,"Missing"
D15-1040,H05-1091,0,0.019228,"Missing"
D15-1040,D14-1082,0,0.127244,"es from source to target language. T¨ackstr¨om et al. (2013) additionally used high-level linguistic features extracted from the World Atlas of Language Structures (WALS) (Dryer and Haspelmath, 2013). For low-resource languages, no large parallel corpus is available. Some linguists are dependency-annotating small amounts of field data, e.g. for Karuk, a nearly-extinct language of Northwest California (Garrett et al., 2013). Accordingly, we adopt a different resource require2.1 Supervised Neural Network Parser This section describes the monolingual neural network dependency parser structure of Chen and Manning (2014). This parser achieves excellent performance, and has a highly flexible formulation allowing auxilliary inputs. The model is based on a transition-based dependency parser (Nivre, 2006) formulated as a neural-network classifier to decide which transition to apply to each parsing state configuration.2 That is, for each configuration, the selected list of words, POS tags and labels from the Stack, Queue and Arcs are extracted. Each word, POS and label is mapped into a lowdimension vector representation using an embedding matrix, which is then fed into a two-layer neural network classifier to pred"
D15-1040,2005.mtsummit-papers.11,0,0.0112439,"Missing"
D15-1040,P14-1126,0,0.00966165,"parser is built without any lexical features and trained on a treebank for a resource-rich source language (Zeman et al., 2008). It is then applied directly to parse sentences in the target resource-poor languages. Delexicalized parsing relies on the fact that identical part-ofspeech (POS) inventories are highly informative of dependency relations, and that there exists shared dependency structures across languages. Building a dependency parser for a resourcepoor language usually starts with the delexicalized parser and then uses other resources to refine the model. McDonald et al. (2011) and Ma and Xia (2014) exploited parallel data as the bridge to transfer constraints from the source resourcerich language to the target resource-poor languages. T¨ackstr¨om et al. (2012) also used parallel data to induce cross-lingual word clusters which added as features for their delexicalized parser. Durrett et al. (2012) constructed the set of language-independent features and used a bilingual dictionary as the bridge to transfer these features from source to target language. T¨ackstr¨om et al. (2013) additionally used high-level linguistic features extracted from the World Atlas of Language Structures (WALS)"
D15-1040,D11-1006,0,0.168279,"alized parsing, in which a parser is built without any lexical features and trained on a treebank for a resource-rich source language (Zeman et al., 2008). It is then applied directly to parse sentences in the target resource-poor languages. Delexicalized parsing relies on the fact that identical part-ofspeech (POS) inventories are highly informative of dependency relations, and that there exists shared dependency structures across languages. Building a dependency parser for a resourcepoor language usually starts with the delexicalized parser and then uses other resources to refine the model. McDonald et al. (2011) and Ma and Xia (2014) exploited parallel data as the bridge to transfer constraints from the source resourcerich language to the target resource-poor languages. T¨ackstr¨om et al. (2012) also used parallel data to induce cross-lingual word clusters which added as features for their delexicalized parser. Durrett et al. (2012) constructed the set of language-independent features and used a bilingual dictionary as the bridge to transfer these features from source to target language. T¨ackstr¨om et al. (2013) additionally used high-level linguistic features extracted from the World Atlas of Langu"
D15-1040,P15-2139,1,0.534832,"e treebank covers 10 languages,6 with some languages very highly resourced—Czech, French and Spanish have 400k tokens—and only modest amounts of data for other languages—Hungarian and Irish have only around 25k tokens. Cross-lingual models assume English as the source language, for which we have a large treebank, and only a small treebank of 3k tokens exists in each target language, simulated by subsampling the corpus. 4.2 Baseline Cascade Model We compare our approach to a baseline interlingual model based on the same parsing algorithm as presented in section 2.1, but with cascaded training (Duong et al., 2015). This works by first learning the source language parser, and then training the target language parser using a regularization term to minimise the distance between the parameters of the target parser and the source parser (which is fixed). In this way, some structural information from the source parser can be used in the target parser, however it is likely that the representation will be overly biased towards the source language and consequently may not prove as useful for modelling the target. 4.3 Monolingual Word Embeddings While the E pos and E arc are randomly initialized, we initialize b"
D15-1040,N09-1028,0,0.0541894,"Missing"
D15-1040,skadins-etal-2014-billions,0,0.0228432,"Missing"
D15-1040,D13-1170,0,0.00392188,"Missing"
D15-1040,N12-1052,0,0.160554,"Missing"
D15-1040,N13-1126,0,0.0678031,"Missing"
D15-1040,I08-3008,0,\N,Missing
D16-1136,W13-3520,0,0.0658583,"Missing"
D16-1136,P11-1061,0,0.0122427,"l corpora, and competitive results on the monolingual word similarity and cross-lingual document classification task. 1 Introduction Monolingual word embeddings have had widespread success in many NLP tasks including sentiment analysis (Socher et al., 2013), dependency parsing (Dyer et al., 2015), machine translation (Bahdanau et al., 2014). Crosslingual word embeddings are a natural extension facilitating various crosslingual tasks, e.g. through transfer learning. A model built in a source resource-rich language can then applied to the target resource poor languages (Yarowsky and Ngai, 2001; Das and Petrov, 2011; T¨ackstr¨om et al., 2012; Duong et al., 2015). A key barrier for crosslingual transfer is lexical matching between the source and the target language. Crosslingual word embeddings are a natural remedy where both source and target language lexicon are presented as dense vectors in the same vector space (Klementiev et al., 2012). Most previous work has focused on down-stream crosslingual applications such as document classification and dependency parsing. We argue that good crosslingual embeddings should preserve both monolingual and crosslingual quality which we will use as the main evaluatio"
D16-1136,P15-2139,1,0.561844,"ngual word similarity and cross-lingual document classification task. 1 Introduction Monolingual word embeddings have had widespread success in many NLP tasks including sentiment analysis (Socher et al., 2013), dependency parsing (Dyer et al., 2015), machine translation (Bahdanau et al., 2014). Crosslingual word embeddings are a natural extension facilitating various crosslingual tasks, e.g. through transfer learning. A model built in a source resource-rich language can then applied to the target resource poor languages (Yarowsky and Ngai, 2001; Das and Petrov, 2011; T¨ackstr¨om et al., 2012; Duong et al., 2015). A key barrier for crosslingual transfer is lexical matching between the source and the target language. Crosslingual word embeddings are a natural remedy where both source and target language lexicon are presented as dense vectors in the same vector space (Klementiev et al., 2012). Most previous work has focused on down-stream crosslingual applications such as document classification and dependency parsing. We argue that good crosslingual embeddings should preserve both monolingual and crosslingual quality which we will use as the main evaluation criterion through monolingual word similarity"
D16-1136,P15-1033,0,0.017942,"ere unable to handle polysemy. We address these drawbacks in our method which takes advantage of a high coverage dictionary in an EM style training algorithm over monolingual corpora in two languages. Our model achieves state-of-theart performance on bilingual lexicon induction task exceeding models using large bilingual corpora, and competitive results on the monolingual word similarity and cross-lingual document classification task. 1 Introduction Monolingual word embeddings have had widespread success in many NLP tasks including sentiment analysis (Socher et al., 2013), dependency parsing (Dyer et al., 2015), machine translation (Bahdanau et al., 2014). Crosslingual word embeddings are a natural extension facilitating various crosslingual tasks, e.g. through transfer learning. A model built in a source resource-rich language can then applied to the target resource poor languages (Yarowsky and Ngai, 2001; Das and Petrov, 2011; T¨ackstr¨om et al., 2012; Duong et al., 2015). A key barrier for crosslingual transfer is lexical matching between the source and the target language. Crosslingual word embeddings are a natural remedy where both source and target language lexicon are presented as dense vecto"
D16-1136,E14-1049,0,0.506581,"comparable part of Wikipedia. They represent word using Wikipedia entries which are shared for many languages. A bilingual dictionary is an alternative source of bilingual information. Gouws and Søgaard (2015) randomly replace the text in a monolingual corpus with a random translation, using this corpus for learning word embeddings. Their approach doesn’t handle polysemy, as very few of the translations for each word will be valid in context. For this reason a high coverage or noisy dictionary with many translations might lead to poor outcomes. Mikolov et al. (2013a), Xiao and Guo (2014) and Faruqui and Dyer (2014) filter a bilingual dictionary for one-to-one translations, thus side-stepping the problem, however discarding much of the information in the dictionary. Our approach also uses a dictionary, however we use all the translations and explicitly disambiguate translations during training. Another distinguishing feature on the above-cited research is the method for training embeddings. Mikolov et al. (2013a) and Faruqui and Dyer (2014) use a cascade style of training where the word embeddings in both source and target language are trained separately and then combined later using the dictionary. Most"
D16-1136,N15-1157,0,0.254278,"pseudo-documents are then used for learning vector representations using Word2Vec. Their system, despite its simplicity, performed surprisingly well on a bilingual lexicon induction task (we compare our method with theirs on this task.) Their approach is compelling due to its lesser resource requirements, although comparable bilingual data is scarce for many languages. Related, Søgaard et al. (2015) exploit the comparable part of Wikipedia. They represent word using Wikipedia entries which are shared for many languages. A bilingual dictionary is an alternative source of bilingual information. Gouws and Søgaard (2015) randomly replace the text in a monolingual corpus with a random translation, using this corpus for learning word embeddings. Their approach doesn’t handle polysemy, as very few of the translations for each word will be valid in context. For this reason a high coverage or noisy dictionary with many translations might lead to poor outcomes. Mikolov et al. (2013a), Xiao and Guo (2014) and Faruqui and Dyer (2014) filter a bilingual dictionary for one-to-one translations, thus side-stepping the problem, however discarding much of the information in the dictionary. Our approach also uses a dictiona"
D16-1136,P14-1006,0,0.0206465,"olingual, bilingual and crosslingual transfer settings. 2 Related work There is a wealth of prior work on crosslingual word embeddings, which all exploit some kind of bilingual resource. This is often in the form of a parallel bilingual text, using word alignments as a bridge between tokens in the source and target languages, such that translations are assigned similar embedding vectors (Luong et al., 2015; Klementiev et al., 2012). These approaches are affected by errors from automatic word alignments, motivating other approaches which operate at the sentence level (Chandar A P et al., 2014; Hermann and Blunsom, 2014; Gouws et al., 2015) through learning compositional vector representations of sentences, 1286 in order that sentences and their translations representations closely match. The word embeddings learned this way capture translational equivalence, despite not using explicit word alignments. Nevertheless, these approaches demand large parallel corpora, which are not available for many language pairs. Vuli´c and Moens (2015) use bilingual comparable text, sourced from Wikipedia. Their approach creates a psuedo-document by forming a bag-ofwords from the lemmatized nouns in each comparable document c"
D16-1136,kamholz-etal-2014-panlex,0,0.0470178,"entries across several languages, however this approach tends to underperform other methods. To capture the monolingual distributional properties of words it is crucial to train on large monolingual corpora (Luong et al., 2015). However, many previous approaches are not capable of scaling up either because of the complicated objective functions or the nature of the algorithm. Other methods use a dictionary as the bridge between languages (Mikolov et al., 2013a; Xiao and Guo, 2014), however they do not adequately handle translation ambiguity. Our model uses a bilingual dictionary from Panlex (Kamholz et al., 2014) as the source of bilingual signal. Panlex covers more than a thousand languages and therefore our approach applies to many languages, including low-resource languages. Our method selects the translation based on the context in an Expectation-Maximization style training algorithm which explicitly handles polysemy through incorporating multiple dictionary translations (word sense and translation are closely linked (Resnik and Yarowsky, 1999)). In addition to the dictionary, 1285 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1285–1295, c Austin, Te"
D16-1136,C12-1089,0,0.0985088,"au et al., 2014). Crosslingual word embeddings are a natural extension facilitating various crosslingual tasks, e.g. through transfer learning. A model built in a source resource-rich language can then applied to the target resource poor languages (Yarowsky and Ngai, 2001; Das and Petrov, 2011; T¨ackstr¨om et al., 2012; Duong et al., 2015). A key barrier for crosslingual transfer is lexical matching between the source and the target language. Crosslingual word embeddings are a natural remedy where both source and target language lexicon are presented as dense vectors in the same vector space (Klementiev et al., 2012). Most previous work has focused on down-stream crosslingual applications such as document classification and dependency parsing. We argue that good crosslingual embeddings should preserve both monolingual and crosslingual quality which we will use as the main evaluation criterion through monolingual word similarity and bilingual lexicon induction tasks. Moreover, many prior work (Chandar A P et al., 2014; Koˇcisk´y et al., 2014) used bilingual or comparable corpus which is also expensive for many low-resource languages. Søgaard et al. (2015) impose a less onerous data condition in the form of"
D16-1136,P14-2037,0,0.0305298,"Missing"
D16-1136,W13-3512,0,0.0424945,"1995) to lemmatize the output of our combined model before evaluation. Table 3 (+lemmatization) shows some improvements but minor. It demonstrates that our model is already good at disambiguating morphology. For example, the top 2 translations for es word lenguas in en are languages and language which correctly prefer the plural translation. 7 Monolingual Word Similarity Now we consider the efficacy of our CLWE on monolingual word similarity. We evaluate on English monolingual similarity on WordSim353 (WSen), RareWord (RW-en) and German version of WordSim353 (WS-de) (Finkelstein et al., 2001; Luong et al., 2013; Luong et al., 2015). Each of those datasets contain many tuples (w1 , w2 , s) where s is a scalar denoting the semantic similarity between w1 and w2 given by human annotators. Good system should produce the score correlated with human judgement. We train the model as described in §4, which is the combine embeddings setting from Table 3. Since the evaluation involves de and en word similarity, we train the CLWE for en-de pair. Table 4 shows the performance of our combined model compared with several baselines. Our combined model out-performed both Luong et al. (2015) and Gouws and Søgaard (20"
D16-1136,W15-1521,0,0.243658,"Missing"
D16-1136,N13-1090,0,0.651605,"e corpus which is also expensive for many low-resource languages. Søgaard et al. (2015) impose a less onerous data condition in the form of linked Wikipedia entries across several languages, however this approach tends to underperform other methods. To capture the monolingual distributional properties of words it is crucial to train on large monolingual corpora (Luong et al., 2015). However, many previous approaches are not capable of scaling up either because of the complicated objective functions or the nature of the algorithm. Other methods use a dictionary as the bridge between languages (Mikolov et al., 2013a; Xiao and Guo, 2014), however they do not adequately handle translation ambiguity. Our model uses a bilingual dictionary from Panlex (Kamholz et al., 2014) as the source of bilingual signal. Panlex covers more than a thousand languages and therefore our approach applies to many languages, including low-resource languages. Our method selects the translation based on the context in an Expectation-Maximization style training algorithm which explicitly handles polysemy through incorporating multiple dictionary translations (word sense and translation are closely linked (Resnik and Yarowsky, 1999"
D16-1136,D14-1162,0,0.0902224,"Missing"
D16-1136,D13-1170,0,0.00406887,"iculty incorporating monolingual data or were unable to handle polysemy. We address these drawbacks in our method which takes advantage of a high coverage dictionary in an EM style training algorithm over monolingual corpora in two languages. Our model achieves state-of-theart performance on bilingual lexicon induction task exceeding models using large bilingual corpora, and competitive results on the monolingual word similarity and cross-lingual document classification task. 1 Introduction Monolingual word embeddings have had widespread success in many NLP tasks including sentiment analysis (Socher et al., 2013), dependency parsing (Dyer et al., 2015), machine translation (Bahdanau et al., 2014). Crosslingual word embeddings are a natural extension facilitating various crosslingual tasks, e.g. through transfer learning. A model built in a source resource-rich language can then applied to the target resource poor languages (Yarowsky and Ngai, 2001; Das and Petrov, 2011; T¨ackstr¨om et al., 2012; Duong et al., 2015). A key barrier for crosslingual transfer is lexical matching between the source and the target language. Crosslingual word embeddings are a natural remedy where both source and target langu"
D16-1136,P15-1165,0,0.0410086,"Missing"
D16-1136,N12-1052,0,0.0329814,"Missing"
D16-1136,P15-2118,0,0.153094,"Missing"
D16-1136,N01-1026,0,0.0254739,"dels using large bilingual corpora, and competitive results on the monolingual word similarity and cross-lingual document classification task. 1 Introduction Monolingual word embeddings have had widespread success in many NLP tasks including sentiment analysis (Socher et al., 2013), dependency parsing (Dyer et al., 2015), machine translation (Bahdanau et al., 2014). Crosslingual word embeddings are a natural extension facilitating various crosslingual tasks, e.g. through transfer learning. A model built in a source resource-rich language can then applied to the target resource poor languages (Yarowsky and Ngai, 2001; Das and Petrov, 2011; T¨ackstr¨om et al., 2012; Duong et al., 2015). A key barrier for crosslingual transfer is lexical matching between the source and the target language. Crosslingual word embeddings are a natural remedy where both source and target language lexicon are presented as dense vectors in the same vector space (Klementiev et al., 2012). Most previous work has focused on down-stream crosslingual applications such as document classification and dependency parsing. We argue that good crosslingual embeddings should preserve both monolingual and crosslingual quality which we will use"
D16-1136,N12-1077,0,0.0113378,".0 90.5 91.3 44.3 66.3 69.1 78.2 78.7 64.3 81.4 82.6 88.8 89.5 Ours Mono Baselines Table 3: Bilingual Lexicon Induction performance from es, it, nl to en. Gouws and Søgaard (2015) + Panlex/Wikt is our reimplementation using Panlex/Wiktionary dictionary. All our models use Panlex as the dictionary. We reported the recall at 1 and 5. The best performance is bold. Model WS-de WS-en RW-en Klementiev et al. (2012) Chandar A P et al. (2014) Hermann and Blunsom (2014) Luong et al. (2015) Gouws and Søgaard (2015) 23.8 34.6 28.3 47.4 67.4 13.2 39.8 19.8 49.3 71.8 7.3 20.5 13.6 25.3 31.0 CBOW + combine Yih and Qazvinian (2012) Shazeer et al. (2016) 62.2 65.8 - 70.3 74.1 81.0 74.8 42.7 43.1 48.3 Our joint-model + combine 59.3 71.1 68.6 76.2 38.1 44.0 Table 4: Spearman’s rank correlation for monolingual similarity measurement on 3 datasets WS-de (353 pairs), WS-en (353 pairs) and RW-en (2034 pairs). We compare against 5 baseline crosslingual word embeddings. The best CLWE performance is bold. For reference, we add the monolingual CBOW with and without embeddings combination, Yih and Qazvinian (2012) and Shazeer et al. (2016) which represents the monolingual state-of-the-art results for WS-en and RW-en. monolingual co"
D16-1263,W08-0336,0,0.0470765,"Missing"
D16-1263,N16-1109,1,0.622951,"pressed with a weighted finite-state transducer (WFST) framework represents the joint distribution of source acoustic features, phonemes and latent source words given the target words. Sampling of alignments is used to learn source words and their target translations, which are then used to improve transcription of the source audio they were learnt from. Importantly, the model assumes no prior lexicon or translation model. This method builds on work on phoneme translation modeling (Besacier et al., 2006; St¨uker et al., 2009; Stahlberg et al., 2012; Stahlberg et al., 2014; Adams et al., 2015; Duong et al., 2016), speech translation (Casacuberta et al., 2004; Matusov et al., 2005), computer-aided translation, (Brown et al., 1994; Vidal et al., 2006; Khadivi and Ney, 2008; Reddy and Rose, 2010; Pelemans et al., 2015), translation modeling from automatically transcribed 1 Code is available at https://github.com/oadams/latticetm. 2377 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2377–2382, c Austin, Texas, November 1-5, 2016. 2016 Association for Computational Linguistics speech (Paulik and Waibel, 2013), word segmentation and translation modeling (Chang e"
D16-1263,N09-1046,0,0.0684154,"Missing"
D16-1263,W08-0704,0,0.0500543,"Missing"
D16-1263,P11-2093,1,0.626336,"y their ability to improve phoneme recognition, measuring phoneme error rate (PER). Experimental setup We used less than 10 hours of English–Japanese data from the BTEC corpus (Takezawa et al., 2002), comprised of spoken utterances paired with textual translations. This allows us to assess the approach assuming quality acoustic models. We used acoustic models similar to Heck et al. (2015) to obtain source phoneme lattices. Gold phoneme transcriptions were obtained by transforming the text with pronunciation lexicons and, in the Japanese case, first segmenting the text into tokens using KyTea (Neubig et al., 2011). We run experiments in both directions: English– Japanese and Japanese–English (en–ja and ja–en), while comparing against three settings: the ASR 1best path uninformed by the model (ASR); a monolingual version of our model that is identical except without conditioning on the target side (Mono); and the model applied using the source language sentence as the target (Oracle). We tuned on the first 1,000 utterences (about 1 hour) of speech and trained on up to 9 hours of the English (en) Mono –ja Oracle ASR 22.1 Vague 17.7 18.5 17.2 Shifted 17.4 16.9 16.6 Poisson 17.3 17.2 16.8 Japanese (ja) Mon"
D16-1263,C10-1092,0,0.0680955,"Missing"
D16-1263,takezawa-etal-2002-toward,0,0.0482154,"ce constituting one block. To sample from WFSTs, we use forwardfiltering/backward-sampling (Scott, 2002; Neubig et al., 2012), creating forward probabilities using the forward algorithm for hidden Markov models before backward-sampling edges proportionally to the product of the forward probability and the edge weight.3 3 No Metropolis-Hastings rejection step was used. 2379 We evaluate the lexicon and translation model by their ability to improve phoneme recognition, measuring phoneme error rate (PER). Experimental setup We used less than 10 hours of English–Japanese data from the BTEC corpus (Takezawa et al., 2002), comprised of spoken utterances paired with textual translations. This allows us to assess the approach assuming quality acoustic models. We used acoustic models similar to Heck et al. (2015) to obtain source phoneme lattices. Gold phoneme transcriptions were obtained by transforming the text with pronunciation lexicons and, in the Japanese case, first segmenting the text into tokens using KyTea (Neubig et al., 2011). We run experiments in both directions: English– Japanese and Japanese–English (en–ja and ja–en), while comparing against three settings: the ASR 1best path uninformed by the mod"
D16-1263,1983.tc-1.13,0,0.746681,"Missing"
D16-1263,D15-1142,0,\N,Missing
D16-1263,W14-2201,1,\N,Missing
E17-1084,D15-1131,0,0.109174,"ly on data from Wikipedia, and it is non-trivial to extend them to languages that are not covered by Wikipedia. Lexicons are another source of bilingual signal, with the advantage of high coverage. Multilingual lexical resources such as PanLex (Kamholz et al., 2014) and Wiktionary2 cover thousands of languages, and have been used to construct high performance crosslingual word embeddings (Mikolov et al., 2013a; Xiao and Guo, 2014; Faruqui and Dyer, 2014). Previous work mainly focuses on building word embeddings for a pair of languages, typically with English on one side, with the exception of Coulmance et al. (2015), Søgaard et al. (2015) and Ammar et al. (2016). Coulmance et al. (2015) extend the bilingual skipgram model from Luong et al. (2015), training jointly over many languages using the Europarl corpora. We also compare our models with an extension of Huang et al. (2015) adapted for multiple languages also using bilingual corpora. However, parallel data is an expensive resource and using parallel data seems to under-perform on the bilingual lexicon induction task (Vuli´c and Moens, 2015). While Coulmance et al. (2015) use English as the pivot language, Søgaard et al. (2015) learn multilingual word"
E17-1084,P15-2139,1,0.0635551,"dings. Our multilingual word embeddings, on the other hand, map both Italian and Spanish to the same space without using any direct bilingual signal between them. In addition, multilingual word embeddings allow multiple source language transfer learning, producing a more general model and overcoming data sparseness (McDonald et al., 2011; Guo et al., 2016; Agi´c et al., 2016). Moreover, multilingual word embeddings are also crucial for multilingual applications such as multi-source machine translation (Zoph and Knight, 2016), and multisource transfer dependency parsing (McDonald et al., 2011; Duong et al., 2015a). We propose several algorithms to map bilingual word embeddings to the same vector space, either during training or during post-processing. We apply a linear transformation to map the English side of each pretrained crosslingual word embedding to the same space. We also extend Duong et al. (2016), which used a lexicon to learn bilingual word embeddings. We modify the objective function to jointly build multilingual word embeddings during training. Unlike most prior work which focuses on downstream applications, we measure the quality of our multilingual word embeddings in three ways: biling"
E17-1084,K15-1012,1,0.928755,"dings. Our multilingual word embeddings, on the other hand, map both Italian and Spanish to the same space without using any direct bilingual signal between them. In addition, multilingual word embeddings allow multiple source language transfer learning, producing a more general model and overcoming data sparseness (McDonald et al., 2011; Guo et al., 2016; Agi´c et al., 2016). Moreover, multilingual word embeddings are also crucial for multilingual applications such as multi-source machine translation (Zoph and Knight, 2016), and multisource transfer dependency parsing (McDonald et al., 2011; Duong et al., 2015a). We propose several algorithms to map bilingual word embeddings to the same vector space, either during training or during post-processing. We apply a linear transformation to map the English side of each pretrained crosslingual word embedding to the same space. We also extend Duong et al. (2016), which used a lexicon to learn bilingual word embeddings. We modify the objective function to jointly build multilingual word embeddings during training. Unlike most prior work which focuses on downstream applications, we measure the quality of our multilingual word embeddings in three ways: biling"
E17-1084,Q16-1022,0,0.0457063,"Missing"
E17-1084,W13-3520,0,0.0116751,"71.7 80.7 76.6 86.7 Model Table 1: Bilingual lexicon induction performance for four pairs. Bilingual word embeddings (BiWE) is the state-of-the-art result from Duong et al. (2016) where each pair is trained separately. Our proposed methods including linear transformation (Linear), joint prediction as in Equation (3) (Joint) and joint prediction with explicit mapping as in Equation (4) (+mapping). We report recall at 1 and 5 with respect to four baseline multilingual word embeddings. The best scores for are shown in bold. tences from the tokenized monolingual data from the Wikipedia dump from Al-Rfou et al. (2013).5 The dictionary is from PanLex which covers more than 1,000 language varieties. We build multilingual word embeddings for 5 languages (en, it, es, nl, de) jointly using the same parameters as Duong et al. (2016).6 During training, for a fairer comparison, we only use lexicons between English and each target language. However, it is straightforward to incorporate a lexicon between any pair of languages into our model. The pretrained bilingual word embeddings for the postprocessing experiment in §3 are also from Duong et al. (2016). In the following sections, we evaluate the performance of our"
E17-1084,P15-1033,0,0.00533046,"glish on one side. We investigate methods for building high quality crosslingual word embeddings for many languages in a unified vector space. In this way, we can exploit and combine information from many languages. We report competitive performance on bilingual lexicon induction, monolingual similarity and crosslingual document classification tasks. 1 Introduction Monolingual word embeddings have facilitated advances in many natural language processing tasks, such as natural language understanding (Collobert and Weston, 2008), sentiment analysis (Socher et al., 2013), and dependency parsing (Dyer et al., 2015). Crosslingual word embeddings represent words from several languages in the same low dimensional space. They are helpful for multilingual tasks such as machine translation (Brown et al., 1993) and bilingual named entity recognition (Wang et al., 2013). Crosslingual word embeddings can also be used in transfer learning, where the source model is trained on one language and applied directly to another language; this is suitable for the low-resource scenario (Yarowsky and Ngai, 2001; Duong et al., 2015b; Das and Petrov, 2011; T¨ackstr¨om et al., 2012). Most prior work on building crosslingual wo"
E17-1084,J93-2003,0,0.109062,"on from many languages. We report competitive performance on bilingual lexicon induction, monolingual similarity and crosslingual document classification tasks. 1 Introduction Monolingual word embeddings have facilitated advances in many natural language processing tasks, such as natural language understanding (Collobert and Weston, 2008), sentiment analysis (Socher et al., 2013), and dependency parsing (Dyer et al., 2015). Crosslingual word embeddings represent words from several languages in the same low dimensional space. They are helpful for multilingual tasks such as machine translation (Brown et al., 1993) and bilingual named entity recognition (Wang et al., 2013). Crosslingual word embeddings can also be used in transfer learning, where the source model is trained on one language and applied directly to another language; this is suitable for the low-resource scenario (Yarowsky and Ngai, 2001; Duong et al., 2015b; Das and Petrov, 2011; T¨ackstr¨om et al., 2012). Most prior work on building crosslingual word embeddings focuses on a pair of languages. English is usually on one side, thanks to the wealth 1 From here on we refer to crosslingual word embeddings for a pair of languages and multiple l"
E17-1084,E14-1049,0,0.585796,"monolingual and crosslingual transfer settings. 2 beddings for many languages using Wikipedia entries which are the same for many languages. However, their approach is limited to languages covered in Wikipedia and seems to under-perform other methods. Ammar et al. (2016) propose two algorithms, MultiCluster and MultiCCA, for multilingual word embeddings using set of bilingual lexicons. MultiCluster first builds the graph where nodes are lexical items and edges are translations. Each cluster in this graph is an anchor point for building multilingual word embeddings. MultiCCA is an extension of Faruqui and Dyer (2014), performing canonical correlation analysis (CCA) for multiple languages using English as the pivot. A shortcoming of MultiCCA is that it ignores polysemous translations by retaining only one-to-one dictionary pairs (Gouws et al., 2015), disregarding much information. As a simple solution, we propose a simple post hoc method by mapping the English parts of each bilingual word embedding to each other. In this way, the mapping is always exact and one-to-one. Duong et al. (2016) constructed bilingual word embeddings based on monolingual data and PanLex. In this way, their approach can be applied"
E17-1084,D11-1006,0,0.0296647,"gs for many languages so that different relations can be exploited.1 For example, since Italian and Spanish are similar, they are excellent candidates for transfer learning. However, few parallel resources exist between Italian and Spanish for directly building bilingual word embeddings. Our multilingual word embeddings, on the other hand, map both Italian and Spanish to the same space without using any direct bilingual signal between them. In addition, multilingual word embeddings allow multiple source language transfer learning, producing a more general model and overcoming data sparseness (McDonald et al., 2011; Guo et al., 2016; Agi´c et al., 2016). Moreover, multilingual word embeddings are also crucial for multilingual applications such as multi-source machine translation (Zoph and Knight, 2016), and multisource transfer dependency parsing (McDonald et al., 2011; Duong et al., 2015a). We propose several algorithms to map bilingual word embeddings to the same vector space, either during training or during post-processing. We apply a linear transformation to map the English side of each pretrained crosslingual word embedding to the same space. We also extend Duong et al. (2016), which used a lexico"
E17-1084,N13-1090,0,0.747701,"., 2014; Huang et al., 2015). Other work uses more widely available resources such as comparable data (Vuli´c and Moens, 2015) and shared Wikipedia entries (Søgaard et al., 2015). However, those approaches rely on data from Wikipedia, and it is non-trivial to extend them to languages that are not covered by Wikipedia. Lexicons are another source of bilingual signal, with the advantage of high coverage. Multilingual lexical resources such as PanLex (Kamholz et al., 2014) and Wiktionary2 cover thousands of languages, and have been used to construct high performance crosslingual word embeddings (Mikolov et al., 2013a; Xiao and Guo, 2014; Faruqui and Dyer, 2014). Previous work mainly focuses on building word embeddings for a pair of languages, typically with English on one side, with the exception of Coulmance et al. (2015), Søgaard et al. (2015) and Ammar et al. (2016). Coulmance et al. (2015) extend the bilingual skipgram model from Luong et al. (2015), training jointly over many languages using the Europarl corpora. We also compare our models with an extension of Huang et al. (2015) adapted for multiple languages also using bilingual corpora. However, parallel data is an expensive resource and using pa"
E17-1084,D15-1127,0,0.0415669,"Missing"
E17-1084,D13-1170,0,0.00191467,"ts embeddings for a pair of languages, with English on one side. We investigate methods for building high quality crosslingual word embeddings for many languages in a unified vector space. In this way, we can exploit and combine information from many languages. We report competitive performance on bilingual lexicon induction, monolingual similarity and crosslingual document classification tasks. 1 Introduction Monolingual word embeddings have facilitated advances in many natural language processing tasks, such as natural language understanding (Collobert and Weston, 2008), sentiment analysis (Socher et al., 2013), and dependency parsing (Dyer et al., 2015). Crosslingual word embeddings represent words from several languages in the same low dimensional space. They are helpful for multilingual tasks such as machine translation (Brown et al., 1993) and bilingual named entity recognition (Wang et al., 2013). Crosslingual word embeddings can also be used in transfer learning, where the source model is trained on one language and applied directly to another language; this is suitable for the low-resource scenario (Yarowsky and Ngai, 2001; Duong et al., 2015b; Das and Petrov, 2011; T¨ackstr¨om et al., 2012)."
E17-1084,kamholz-etal-2014-panlex,0,0.0332869,"ccurrence statistics from parallel text (Luong et al., 2015; Gouws et al., 2015; Chandar A P et al., 2014; Klementiev et al., 2012; Koˇcisk´y et al., 2014; Huang et al., 2015). Other work uses more widely available resources such as comparable data (Vuli´c and Moens, 2015) and shared Wikipedia entries (Søgaard et al., 2015). However, those approaches rely on data from Wikipedia, and it is non-trivial to extend them to languages that are not covered by Wikipedia. Lexicons are another source of bilingual signal, with the advantage of high coverage. Multilingual lexical resources such as PanLex (Kamholz et al., 2014) and Wiktionary2 cover thousands of languages, and have been used to construct high performance crosslingual word embeddings (Mikolov et al., 2013a; Xiao and Guo, 2014; Faruqui and Dyer, 2014). Previous work mainly focuses on building word embeddings for a pair of languages, typically with English on one side, with the exception of Coulmance et al. (2015), Søgaard et al. (2015) and Ammar et al. (2016). Coulmance et al. (2015) extend the bilingual skipgram model from Luong et al. (2015), training jointly over many languages using the Europarl corpora. We also compare our models with an extensio"
E17-1084,P15-1165,0,0.0362412,"Missing"
E17-1084,C12-1089,0,0.235651,"ing an EM algorithm for selecting a lexicon. Relative to many previous crosslingual word embeddings, their joint training algorithm achieved state-of-the-art performance for the bilingual lexicon induction task, performing significantly better on monolingual similarity and achieving a competitive result on cross lingual document classification. Here we also adopt their approach, and extend it to multilingual embeddings. Related work Crosslingual word embeddings are typically based on co-occurrence statistics from parallel text (Luong et al., 2015; Gouws et al., 2015; Chandar A P et al., 2014; Klementiev et al., 2012; Koˇcisk´y et al., 2014; Huang et al., 2015). Other work uses more widely available resources such as comparable data (Vuli´c and Moens, 2015) and shared Wikipedia entries (Søgaard et al., 2015). However, those approaches rely on data from Wikipedia, and it is non-trivial to extend them to languages that are not covered by Wikipedia. Lexicons are another source of bilingual signal, with the advantage of high coverage. Multilingual lexical resources such as PanLex (Kamholz et al., 2014) and Wiktionary2 cover thousands of languages, and have been used to construct high performance crosslingual"
E17-1084,N12-1052,0,0.0672846,"Missing"
E17-1084,P14-2037,0,0.0227298,"Missing"
E17-1084,P15-2118,0,0.130755,"Missing"
E17-1084,W13-3512,0,0.10193,"Missing"
E17-1084,P13-1106,0,0.0685321,"Missing"
E17-1084,P12-1000,0,0.201693,"Missing"
E17-1084,W15-1521,0,0.194796,"Missing"
E17-1084,N01-1026,0,0.030795,"language understanding (Collobert and Weston, 2008), sentiment analysis (Socher et al., 2013), and dependency parsing (Dyer et al., 2015). Crosslingual word embeddings represent words from several languages in the same low dimensional space. They are helpful for multilingual tasks such as machine translation (Brown et al., 1993) and bilingual named entity recognition (Wang et al., 2013). Crosslingual word embeddings can also be used in transfer learning, where the source model is trained on one language and applied directly to another language; this is suitable for the low-resource scenario (Yarowsky and Ngai, 2001; Duong et al., 2015b; Das and Petrov, 2011; T¨ackstr¨om et al., 2012). Most prior work on building crosslingual word embeddings focuses on a pair of languages. English is usually on one side, thanks to the wealth 1 From here on we refer to crosslingual word embeddings for a pair of languages and multiple languages as bilingual word embeddings and multilingual word embeddings respectively. 894 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers, pages 894–904, c Valencia, Spain, April 3-7, 2017. 2017 Association for"
E17-1084,N16-1004,0,0.0163154,"allel resources exist between Italian and Spanish for directly building bilingual word embeddings. Our multilingual word embeddings, on the other hand, map both Italian and Spanish to the same space without using any direct bilingual signal between them. In addition, multilingual word embeddings allow multiple source language transfer learning, producing a more general model and overcoming data sparseness (McDonald et al., 2011; Guo et al., 2016; Agi´c et al., 2016). Moreover, multilingual word embeddings are also crucial for multilingual applications such as multi-source machine translation (Zoph and Knight, 2016), and multisource transfer dependency parsing (McDonald et al., 2011; Duong et al., 2015a). We propose several algorithms to map bilingual word embeddings to the same vector space, either during training or during post-processing. We apply a linear transformation to map the English side of each pretrained crosslingual word embedding to the same space. We also extend Duong et al. (2016), which used a lexicon to learn bilingual word embeddings. We modify the objective function to jointly build multilingual word embeddings during training. Unlike most prior work which focuses on downstream applic"
E17-1084,P11-1061,0,\N,Missing
E17-1084,D16-1136,1,\N,Missing
E17-1088,D16-1047,0,0.0201262,"recognition of these languages, a difficult challenge. One of the touted advantages of neural network language models (NNLMs) is their ability to model sparse data (Bengio et al., 2003; Gandhe et al., 2014). However, despite the success of NNLMs 937 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers, pages 937–947, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics al., 2013a), leading to many further investigations (Chen et al., 2013; Pennington et al., 2014; Shazeer et al., 2016; Bhatia et al., 2016). A key application of word embeddings has been in the initializing of neural network architectures for a wide variety of NLP tasks with limited annotated data (Frome et al., 2013; Zhang et al., 2014; Zoph et al., 2016; Lau and Baldwin, 2016). guage inform embeddings trained with little target language data? Secondly, can such CLWEs improve language modeling in low-resource contexts by initializing the parameters of an NNLM? To answer these questions, we scale down the available monolingual data of the target language to as few as 1k sentences, while maintaining a large source language dataset"
E17-1088,P16-1186,0,0.0258344,"Missing"
E17-1088,D15-1131,0,0.060784,"e (Graves, 2013; Zaremba et al., 2014). Word embeddings have became more popular through the application of shallow neural network architectures that allow for training on large quantities of data (Mnih et al., 2009; Bengio et al., 2009; Collobert and Weston, 2008; Mikolov et Cross-lingual word embeddings Cross-lingual word embeddings have also been the subject of significant investigation. Many methods require parallel corpora or comparable corpora to connect the languages (Klementiev et al., 2012; Zou et al., 2013; Hermann and Blunsom, 2013; Chandar A P et al., 2014; Koˇcisk´y et al., 2014; Coulmance et al., 2015; Wang et al., 2016), while others use bilingual dictionaries (Mikolov et al., 2013b; Xiao and Guo, 2014; Faruqui and Dyer, 2014; Gouws and Søgaard, 2015; Duong et al., 2016; Ammar et al., 2016), or neither (Miceli Barone, 2016). In particular, we build on the work of Duong et al. (2016). Their method harnesses monolingual corpora in two languages along with a bilingual 938 equates to between 1 and 128 hours of speech. For the training data, we randomly chose sentences that include words in the WordSim353 task proportionally to their frequency in the set. As monolingual baselines, we use the s"
E17-1088,W13-3520,0,0.016042,"judgements of word similarity. Here we follow the same evaluation procedure, except where we simulate a lowresource language by reducing the availability of target English monolingual text while preserving a large quantity of source language text from other languages. This allows us to evaluate the CLWEs intrinsically using the WordSim353 task (Finkelstein et al., 2001) before progressing to downstream language modeling where we additionally consider other target languages. We trained a variety of embeddings on English Wikipedia data of between 1k and 128k sentences from the training data of Al-Rfou et al. (2013). In terms of transcribed speech data, this roughly 1 Hyperparameters for both mono and cross-lingual word embeddings: iters=15, negative=25, size=200, window=48, otherwise default. Smaller window sizes led to similar results for monolingual methods. 2 We also tried Italian, Dutch, German and Serbian, yielding similar results but omitted for presentation. 939 1,000 0.6 800 Perplexity Spearman’s ρ 0.8 0.4 0.2 600 400 0.0 1,000 10,000 200 100,000 1,000 Sentences GNC –de CBOW –ru SG –fi –ja –es MKN3 50 Figure 1: Performance of different embeddings on the WordSim353 task with different amounts of"
E17-1088,D16-1136,1,0.594676,"so been aided by initialization with word embeddings trained on large amounts of unannotated text (Frome et al., 2013; Zhang et al., 2014; Lau and Baldwin, 2016). However, in the case of extremely low-resource languages we do not have the luxury of this unannotated text. As a remedy to this problem we focus on crosslingual word embeddings (CLWEs), which learn word embeddings using information from multiple languages. Recent advances in CLWEs have shown that high quality embeddings can be learnt even in the absence of bilingual corpora by harnessing bilingual lexicons (Gouws and Søgaard, 2015; Duong et al., 2016). This is useful as some threatened and endangered languages have been subject to significant linguistic investigation, leading to the creation of high-quality lexicons, despite the dearth of transcriptions. For example, the training of a quality speech recognition system for Yongning Na, a Sino-Tibetan language spoken by approximately 40k people, is hindered by this lack of data (Do et al., 2014) despite significant linguistic investigation of the language (Michaud, 2008; Michaud, 2016). In this paper we address two research questions. First, is the quality of CLWEs dependent on having large"
E17-1088,E14-1049,0,0.06185,"rk architectures that allow for training on large quantities of data (Mnih et al., 2009; Bengio et al., 2009; Collobert and Weston, 2008; Mikolov et Cross-lingual word embeddings Cross-lingual word embeddings have also been the subject of significant investigation. Many methods require parallel corpora or comparable corpora to connect the languages (Klementiev et al., 2012; Zou et al., 2013; Hermann and Blunsom, 2013; Chandar A P et al., 2014; Koˇcisk´y et al., 2014; Coulmance et al., 2015; Wang et al., 2016), while others use bilingual dictionaries (Mikolov et al., 2013b; Xiao and Guo, 2014; Faruqui and Dyer, 2014; Gouws and Søgaard, 2015; Duong et al., 2016; Ammar et al., 2016), or neither (Miceli Barone, 2016). In particular, we build on the work of Duong et al. (2016). Their method harnesses monolingual corpora in two languages along with a bilingual 938 equates to between 1 and 128 hours of speech. For the training data, we randomly chose sentences that include words in the WordSim353 task proportionally to their frequency in the set. As monolingual baselines, we use the skip-gram (SG) and CBOW methods of Mikolov et al. (2013a) as imˇ uˇrek and plemented in the Gensim package (Reh˚ Sojka, 2010). We"
E17-1088,kamholz-etal-2014-panlex,0,0.0859548,"News Corpus embeddings with 300 dimensions, trained on 100 billion words. The CLWEs were trained using the method of Duong et al. (2016) since their method addresses polysemy which is rampant in dictionaries. The same 1k-128k sentence English Wikipedia data was used but with an additional 5 million sentences of Wikipedia data in a source language. The source languages include Japanese, German, Russian, Finnish, and Spanish, which represent languages of varying similarity with English, some with great morphological and syntactic differences. To relate the languages, we used the PanLex lexicon (Kamholz et al., 2014). Following Duong et al. (2016), we used the default window size of 48 so that the whole sentence’s context is almost always taken into account. This mitigates the effect of word re-ordering between languages. We trained with an embedding dimension of 200 for all data sizes as a larger dimension turned out to be helpful in capturing information from the source side.1 lexicon to connect the languages and represent the words in a common vector space. The model builds on the continuous bag-of-words (CBOW) model (Mikolov et al., 2013a) which learns embeddings by predicting words given their contex"
E17-1088,C12-1089,0,0.037298,"odels (Hochreiter and Schmidhuber, 1997) for modeling long-ranging statistical influences have been shown to be effective (Graves, 2013; Zaremba et al., 2014). Word embeddings have became more popular through the application of shallow neural network architectures that allow for training on large quantities of data (Mnih et al., 2009; Bengio et al., 2009; Collobert and Weston, 2008; Mikolov et Cross-lingual word embeddings Cross-lingual word embeddings have also been the subject of significant investigation. Many methods require parallel corpora or comparable corpora to connect the languages (Klementiev et al., 2012; Zou et al., 2013; Hermann and Blunsom, 2013; Chandar A P et al., 2014; Koˇcisk´y et al., 2014; Coulmance et al., 2015; Wang et al., 2016), while others use bilingual dictionaries (Mikolov et al., 2013b; Xiao and Guo, 2014; Faruqui and Dyer, 2014; Gouws and Søgaard, 2015; Duong et al., 2016; Ammar et al., 2016), or neither (Miceli Barone, 2016). In particular, we build on the work of Duong et al. (2016). Their method harnesses monolingual corpora in two languages along with a bilingual 938 equates to between 1 and 128 hours of speech. For the training data, we randomly chose sentences that in"
E17-1088,P14-2037,0,0.00965011,"Missing"
E17-1088,N15-1157,0,0.0789589,"e of NLP problems have also been aided by initialization with word embeddings trained on large amounts of unannotated text (Frome et al., 2013; Zhang et al., 2014; Lau and Baldwin, 2016). However, in the case of extremely low-resource languages we do not have the luxury of this unannotated text. As a remedy to this problem we focus on crosslingual word embeddings (CLWEs), which learn word embeddings using information from multiple languages. Recent advances in CLWEs have shown that high quality embeddings can be learnt even in the absence of bilingual corpora by harnessing bilingual lexicons (Gouws and Søgaard, 2015; Duong et al., 2016). This is useful as some threatened and endangered languages have been subject to significant linguistic investigation, leading to the creation of high-quality lexicons, despite the dearth of transcriptions. For example, the training of a quality speech recognition system for Yongning Na, a Sino-Tibetan language spoken by approximately 40k people, is hindered by this lack of data (Do et al., 2014) despite significant linguistic investigation of the language (Michaud, 2008; Michaud, 2016). In this paper we address two research questions. First, is the quality of CLWEs depen"
E17-1088,W16-1609,0,0.0426659,"emains unclear whether their advantages transfer to scenarios with extremely limited amounts of data. Appropriate initialization of parameters in neural network frameworks has been shown to be beneficial across a wide variety of domains, including speech recognition, where unsupervised pretraining of deep belief networks was instrumental in attaining breakthrough performance (Hinton et al., 2012). Neural network approaches to a range of NLP problems have also been aided by initialization with word embeddings trained on large amounts of unannotated text (Frome et al., 2013; Zhang et al., 2014; Lau and Baldwin, 2016). However, in the case of extremely low-resource languages we do not have the luxury of this unannotated text. As a remedy to this problem we focus on crosslingual word embeddings (CLWEs), which learn word embeddings using information from multiple languages. Recent advances in CLWEs have shown that high quality embeddings can be learnt even in the absence of bilingual corpora by harnessing bilingual lexicons (Gouws and Søgaard, 2015; Duong et al., 2016). This is useful as some threatened and endangered languages have been subject to significant linguistic investigation, leading to the creatio"
E17-1088,W11-2123,0,0.0461598,"Missing"
E17-1088,W16-1614,0,0.00570031,"Collobert and Weston, 2008; Mikolov et Cross-lingual word embeddings Cross-lingual word embeddings have also been the subject of significant investigation. Many methods require parallel corpora or comparable corpora to connect the languages (Klementiev et al., 2012; Zou et al., 2013; Hermann and Blunsom, 2013; Chandar A P et al., 2014; Koˇcisk´y et al., 2014; Coulmance et al., 2015; Wang et al., 2016), while others use bilingual dictionaries (Mikolov et al., 2013b; Xiao and Guo, 2014; Faruqui and Dyer, 2014; Gouws and Søgaard, 2015; Duong et al., 2016; Ammar et al., 2016), or neither (Miceli Barone, 2016). In particular, we build on the work of Duong et al. (2016). Their method harnesses monolingual corpora in two languages along with a bilingual 938 equates to between 1 and 128 hours of speech. For the training data, we randomly chose sentences that include words in the WordSim353 task proportionally to their frequency in the set. As monolingual baselines, we use the skip-gram (SG) and CBOW methods of Mikolov et al. (2013a) as imˇ uˇrek and plemented in the Gensim package (Reh˚ Sojka, 2010). We additionally used off-the-shelf CBOW Google News Corpus embeddings with 300 dimensions, trained on"
E17-1088,P14-1011,0,0.0293781,"Graves, 2013), it remains unclear whether their advantages transfer to scenarios with extremely limited amounts of data. Appropriate initialization of parameters in neural network frameworks has been shown to be beneficial across a wide variety of domains, including speech recognition, where unsupervised pretraining of deep belief networks was instrumental in attaining breakthrough performance (Hinton et al., 2012). Neural network approaches to a range of NLP problems have also been aided by initialization with word embeddings trained on large amounts of unannotated text (Frome et al., 2013; Zhang et al., 2014; Lau and Baldwin, 2016). However, in the case of extremely low-resource languages we do not have the luxury of this unannotated text. As a remedy to this problem we focus on crosslingual word embeddings (CLWEs), which learn word embeddings using information from multiple languages. Recent advances in CLWEs have shown that high quality embeddings can be learnt even in the absence of bilingual corpora by harnessing bilingual lexicons (Gouws and Søgaard, 2015; Duong et al., 2016). This is useful as some threatened and endangered languages have been subject to significant linguistic investigation"
E17-1088,D16-1163,0,0.0176967,"pite the success of NNLMs 937 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers, pages 937–947, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics al., 2013a), leading to many further investigations (Chen et al., 2013; Pennington et al., 2014; Shazeer et al., 2016; Bhatia et al., 2016). A key application of word embeddings has been in the initializing of neural network architectures for a wide variety of NLP tasks with limited annotated data (Frome et al., 2013; Zhang et al., 2014; Zoph et al., 2016; Lau and Baldwin, 2016). guage inform embeddings trained with little target language data? Secondly, can such CLWEs improve language modeling in low-resource contexts by initializing the parameters of an NNLM? To answer these questions, we scale down the available monolingual data of the target language to as few as 1k sentences, while maintaining a large source language dataset. We assess intrinsic embedding quality by considering correlation with human judgment on the WordSim353 test set (Finkelstein et al., 2001). We then perform language modeling experiments where we initialize the parame"
E17-1088,D13-1141,0,0.0113705,"midhuber, 1997) for modeling long-ranging statistical influences have been shown to be effective (Graves, 2013; Zaremba et al., 2014). Word embeddings have became more popular through the application of shallow neural network architectures that allow for training on large quantities of data (Mnih et al., 2009; Bengio et al., 2009; Collobert and Weston, 2008; Mikolov et Cross-lingual word embeddings Cross-lingual word embeddings have also been the subject of significant investigation. Many methods require parallel corpora or comparable corpora to connect the languages (Klementiev et al., 2012; Zou et al., 2013; Hermann and Blunsom, 2013; Chandar A P et al., 2014; Koˇcisk´y et al., 2014; Coulmance et al., 2015; Wang et al., 2016), while others use bilingual dictionaries (Mikolov et al., 2013b; Xiao and Guo, 2014; Faruqui and Dyer, 2014; Gouws and Søgaard, 2015; Duong et al., 2016; Ammar et al., 2016), or neither (Miceli Barone, 2016). In particular, we build on the work of Duong et al. (2016). Their method harnesses monolingual corpora in two languages along with a bilingual 938 equates to between 1 and 128 hours of speech. For the training data, we randomly chose sentences that include words in the"
E17-1088,D16-1124,1,0.822625,"esults of tuning the dimensions of the hidden layer in the LSTM with respect to perplexity on the validation set,3 as well as tuning the order of n-grams used by the MKN language model. A dimension of 100 yielded a good compromise between the smaller and larger training data sizes, while an order 5 MKN model performed slightly better than its lower-order brethren.4 Interestingly, MKN strongly outperforms the LSTM on low quantities of data, with the LSTM language model not reaching parity until between 16k and 32k sentences of data. This is consistent with the results of Chen et al. (2015) and Neubig and Dyer (2016) that show that n-gram models are typically better for rare words, and here our vocabulary is large but training data small since the data are random Wikipedia sentences. However these findings are inconsistent with the belief that NNLMs have the ability to cope well with sparse data conditions because of the smooth distributions that arise from using dense vector representations of words (Bengio et al., 2003). Traditional smoothing stands strong. 4.2 1,000 Perplexity 800 600 400 200 1,000 10,000 100,000 Sentences MKN –nl LSTM –el mono –ja GNC –fi Figure 3: Perplexity of LSTMs when pre-trained"
E17-1088,D14-1162,0,0.112314,"ng, which is a key tool for facilitating speech recognition of these languages, a difficult challenge. One of the touted advantages of neural network language models (NNLMs) is their ability to model sparse data (Bengio et al., 2003; Gandhe et al., 2014). However, despite the success of NNLMs 937 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers, pages 937–947, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics al., 2013a), leading to many further investigations (Chen et al., 2013; Pennington et al., 2014; Shazeer et al., 2016; Bhatia et al., 2016). A key application of word embeddings has been in the initializing of neural network architectures for a wide variety of NLP tasks with limited annotated data (Frome et al., 2013; Zhang et al., 2014; Zoph et al., 2016; Lau and Baldwin, 2016). guage inform embeddings trained with little target language data? Secondly, can such CLWEs improve language modeling in low-resource contexts by initializing the parameters of an NNLM? To answer these questions, we scale down the available monolingual data of the target language to as few as 1k sentences, while"
E17-1088,W14-1613,0,0.0121171,"shallow neural network architectures that allow for training on large quantities of data (Mnih et al., 2009; Bengio et al., 2009; Collobert and Weston, 2008; Mikolov et Cross-lingual word embeddings Cross-lingual word embeddings have also been the subject of significant investigation. Many methods require parallel corpora or comparable corpora to connect the languages (Klementiev et al., 2012; Zou et al., 2013; Hermann and Blunsom, 2013; Chandar A P et al., 2014; Koˇcisk´y et al., 2014; Coulmance et al., 2015; Wang et al., 2016), while others use bilingual dictionaries (Mikolov et al., 2013b; Xiao and Guo, 2014; Faruqui and Dyer, 2014; Gouws and Søgaard, 2015; Duong et al., 2016; Ammar et al., 2016), or neither (Miceli Barone, 2016). In particular, we build on the work of Duong et al. (2016). Their method harnesses monolingual corpora in two languages along with a bilingual 938 equates to between 1 and 128 hours of speech. For the training data, we randomly chose sentences that include words in the WordSim353 task proportionally to their frequency in the set. As monolingual baselines, we use the skip-gram (SG) and CBOW methods of Mikolov et al. (2013a) as imˇ uˇrek and plemented in the Gensim packag"
E91-1016,E87-1002,0,0.206773,"orphemes are often not manifested as contigl!ous strings of segments. A morphologically complex form must be expressed as the • intercalation of its component morphemes. An example of this phenomena is illustrated in Figure 1 for the perfective active r. Consider the form kattab in the second row. Its particular arrangement of four consonants and two There is a maximum o f one vowel per node. (AT) L0,~ ^ (~)~ -, [~]~). There is a maximum o f one segment per coda. (AS) ~((~, ,~v)q, -, b,, ,,-v],/,). There is a maximum o f one vowel per syllable, SThe approadaes to Arabic phonology presented by Kay (1987) and Gibbon (1990)---while addressing important computational issues--fail to represent the hierarchical organization of phonological structures. That is, O-1 is to O as P is to F. 7 Note that these are uninflected forms. Some forms are actually non-existent (for semantic reasons); these are i~dicated by a dash. However, this is unimportant since the present interest is in phonological structure and in potential forms. e - 92 - (Tll) (iii) il ~ i2 ~ i, i3 ~ j or ix ~ i, i2 ~ i3 ~ 3. It follows from the above default stipulations that two of the four consonants of (It) must be identical. By a s"
E91-1016,P87-1013,0,0.0275876,"t is straightforward using techniques discussed in (Gargov et al. 1987, 1989, Blackburn 1990) to provide a proof theory and obtain decidability results. At present we are investigating efficient proof methods for this logic and hope to implement a theorem prover. EXPRESSING P H O N O L O G I C A L C O N S T R A I N T S Feature Matrices. L can be used for describing feature matrices. For example, consider the matrix below. [ PHON (Kay, pats, Blackle) ] A possible description of this matrix is: (PHON)(Kay A Fi) A (PHON)(pats A i A F j) A (PHON)(Blackie A j). This representationof sequences (cf. Rounds & Manaster-Ramer 1987) enables the expression of partialordering constraints which are widely requiredin phonological descriptions.Note thatallinstancesof the following variantof the N O M schema are valid. E and E&apos; are stringsof modal operators from {&lt;>,F, P,O}. M b , "" iff t e V(a) M b , --,~biff M ~ : ~b M ~ , ¢ V &apos;C, iff M ~ t ¢ o r &apos; M ~ , ¢2 : M ~ , O¢ iff qt&apos; : tSt&apos; and M ~ , , ¢ M~, O~biffgt&apos;:tot&apos; andM~,,~b M ~ , F ¢ i f f S t &apos; : t &lt; t&apos; and M ~ , , 4~ M~,P~biffgt&apos;:t&apos;&lt;tandM~,,~b (NOME) Ei A E&apos;(i ^ ¢) .--,E(i A ¢) A E&apos;i. Formulas may be transferredbetween differentp~hs to the same interval. If 9*( ~ , ¢ then"
geoffrois-etal-2000-transcribing,bird-etal-2000-atlas,1,\N,Missing
gibbon-etal-2004-securing,hughes-etal-2004-functional,1,\N,Missing
gibbon-etal-2004-securing,trippel-etal-2004-consistent,1,\N,Missing
graff-bird-2000-many,W99-0301,1,\N,Missing
graff-bird-2000-many,wayne-2000-multilingual,0,\N,Missing
hughes-etal-2004-functional,bird-etal-2002-tabletrans,1,\N,Missing
hughes-etal-2004-functional,U03-1008,1,\N,Missing
hughes-etal-2006-reconsidering,W98-1111,0,\N,Missing
hughes-etal-2006-reconsidering,C96-2110,0,\N,Missing
hughes-etal-2006-reconsidering,A94-1003,0,\N,Missing
I11-1059,D09-1031,0,0.0507496,"Missing"
I11-1059,J93-1004,0,0.821823,"anguages. With enough resources, we could arrange for each source to be transcribed by more than one speaker. What would it take to automatically combine and normalise these transcriptions to produce a single transcription per source, of sufficient quality to be useful for downstream language technologies? These normalised transcriptions could then be aligned with manually supplied translations, leading to a bitext collection. 3 3.1 Existing methods The Gale-Church Algorithm The Gale-Church Algorithm (GCA) aligns the sentences of a document with those of its translation in a foreign language (Gale and Church, 1993). The algorithm exploits the fact that longer sentences in one language tend to correspond to longer sentences in the other. A pair of documents is aligned into cliques of zero, one or two consecutive sentences from each language. Model Description. The model assumes that for a sentence of length L1 , the length of the corresponding clique of sentences in the foreign language is distributed as: L2 ∼ N (cL1 , s2 L1 ) (3.1) where c represents the mean number of characters emitted in the foreign language for each character in the source language, and s2 is the variance per translated character. E"
I11-1059,J10-4005,0,0.0206436,"s of alignment types, based on European languages. In order to apply the algorithm to word fragment alignments, these prior probabilities should be reestimated. Since the GCA is only applied to synthetic data in this work, we will retain the original priors and generate data according to them. 3.2 Moses In order to perform a system-level evaluation of the GCA as a transcription pre-processor for character-based aligners, we require an established alignment system. For this purpose we use Moses, which is an SMT system designed to extend the IBM Models for unsupervised phrase-based translation (Koehn, 2010). Model description. Moses uses a mathematical model to determine a probability distribution over possible translations of a sentence. Given a source language sentence e, a foreign language sentence f , and a division of the sentences into I phrases (blocks of consecutive words), the probability of the foreign sentence given the source sentence is defined as follows (Koehn, 2010): p(f I |eI ) = I Y i=1 φ(fi |ei )d(starti − endi−1 − 1) (3.3) where ei and fi are the ith phrases in the source and foreign languages, φ(f |e) is the probability of translating the phrase e into f , starti is the posi"
I11-1059,J03-1002,0,0.00392471,"the ith phrases in the source and foreign languages, φ(f |e) is the probability of translating the phrase e into f , starti is the position of the first word in the ith phrase, endi is the position of the last word in the ith phrase, and d is a function that penalises re-ordering (when starti − endi−1 − 1 6= 0). Application to transcription problem. To apply Moses to the transcription normalisation problem, we adopt the basic unit of characters instead of words. In this context, a “phrase” corresponds to a word fragment, or sequence of characters. The first step of Moses training uses GIZA++ (Och and Ney, 2003) to establish likely word alignments. We use the HMM model for word alignment (Vogel et al., 1996), since the IBM models encompass reorderings which are not relevant for transcription alignment. An example of training Moses to detect regular sound correspondences between Portuguese and Spanish from a comparative wordlist (Wagner, 2010) is shown in Table 2. 4 HMM method In this section a new method for transcription normalisation based on Hidden Markov Models is in529 ES ie rse rs on ´ PT e r se r se ao ˜ φ(ES|PT) 0.72 0.93 0.81 0.71 N 50 28 32 24 Table 2: Sample of results from training Moses"
I11-1059,C96-2141,0,0.193042,"e phrase e into f , starti is the position of the first word in the ith phrase, endi is the position of the last word in the ith phrase, and d is a function that penalises re-ordering (when starti − endi−1 − 1 6= 0). Application to transcription problem. To apply Moses to the transcription normalisation problem, we adopt the basic unit of characters instead of words. In this context, a “phrase” corresponds to a word fragment, or sequence of characters. The first step of Moses training uses GIZA++ (Och and Ney, 2003) to establish likely word alignments. We use the HMM model for word alignment (Vogel et al., 1996), since the IBM models encompass reorderings which are not relevant for transcription alignment. An example of training Moses to detect regular sound correspondences between Portuguese and Spanish from a comparative wordlist (Wagner, 2010) is shown in Table 2. 4 HMM method In this section a new method for transcription normalisation based on Hidden Markov Models is in529 ES ie rse rs on ´ PT e r se r se ao ˜ φ(ES|PT) 0.72 0.93 0.81 0.71 N 50 28 32 24 Table 2: Sample of results from training Moses on a Spanish-Portuguese comparative wordlist. φ is the proportion of instances of the ES fragment"
I11-1059,N07-1057,0,0.270159,"Missing"
I11-1059,P10-1010,1,\N,Missing
I11-1059,J98-4003,0,\N,Missing
I11-1088,D10-1027,0,0.0182218,"pplications, the head of a hyperedge is usually a single node. This allows a single hyperedge to be semantically equivalent to a tree node with its children. Once a source language sentence is parsed into a packed forest and pruned, the next step is to find trees in the forest that have matching rules in the translation rule dictionary. Matching rules are used to produce a translation forest for decoding into a target language string. The forest is a hypergraph made up of hyperedges where each hyperedge has a single source node, while the rules in the rule dictionary are trees. Recent work by Huang and Mi (2010) has shown that forest expansion can be done in decoders using beam search (Koehn, 2004). However, to the best of our knowledge, no index-based search structures for incrementally finding trees have been proposed to date. 786 forest is a hypergraph made up of hyperedges, the left-hand-sides of rules in the rule dictionary are actually trees. Each node in a forest can have zero or more outgoing hyperedges. A hyperedge is be treated as a tree node with its children. Source sentence trees are constructed by recursively expanding nodes in the forest. Figure 2a shows a tree constructed with three h"
I11-1088,W06-3601,0,0.0176102,"st collections are extracted from NIST (2010a; 2010b) data. 2 Figure 1: A translation rule reproduced from (Mi and Huang, 2008) 2.2 Forest Based Translation Forest-based SMT overcomes the limitations of parse errors in tree-based translation and has been shown to be faster than k-best tree-based translation (Mi et al., 2008). For translating a sentence using the forest-based translation technique, we require a parser that can process a source language sentence and produce a packed forest and a translation rule dictionary, or database, which is a collection of tree-to-string translation rules (Huang et al., 2006; Liu et al., 2006). Background A translation rule is a mapping from a source language tree to a string in the target language. An example translation rule from Chinese to English is shown in Figure 1. The left-hand-side is the source language tree. The Chinese word yˇu is translated to with in the target side on the right. The xi variables on the right-hand-side are placeholders for the corresponding elements in the tree. Other numerical parameters associated with each translation rule are not shown here. Note that there may be several rules in the rule dictionary that have identical source l"
I11-1088,W01-1812,0,0.0425496,"are placeholders for the corresponding elements in the tree. Other numerical parameters associated with each translation rule are not shown here. Note that there may be several rules in the rule dictionary that have identical source language trees but different translations. In this section we present the fundamentals of packed forests and forest-based translation before briefly describing the B-tree index structure. 2.1 Packed Forest Packed forests or forests are directed hypergraphs and have been used to model and represent several applications in computer science and discrete mathematics (Klein and Manning, 2001). Directed hypergraphs can be defined as a pair: H = (V, E) where V = {v1 , v2 , · · · , vn } is the set of nodes and E = {E1 , E2 , · · · , Em } is the set of hyperedges. Each hyperedge can be defined as a pair: Ei = (Xi , Yi ) |Xi , Yi ⊆ V, i = 1, 2, · · · , m. Packed forests have been used in NLP in the area of sentence parsing (Gallo et al., 1993) where the propositions of a parse analysis correspond to the nodes in the hypergraph and rules are represented as hyperedges. A similar model is used in forest based machine translation, where multiple parses of the input sentence are modelled as"
I11-1088,koen-2004-pharaoh,0,0.0318684,"o be semantically equivalent to a tree node with its children. Once a source language sentence is parsed into a packed forest and pruned, the next step is to find trees in the forest that have matching rules in the translation rule dictionary. Matching rules are used to produce a translation forest for decoding into a target language string. The forest is a hypergraph made up of hyperedges where each hyperedge has a single source node, while the rules in the rule dictionary are trees. Recent work by Huang and Mi (2010) has shown that forest expansion can be done in decoders using beam search (Koehn, 2004). However, to the best of our knowledge, no index-based search structures for incrementally finding trees have been proposed to date. 786 forest is a hypergraph made up of hyperedges, the left-hand-sides of rules in the rule dictionary are actually trees. Each node in a forest can have zero or more outgoing hyperedges. A hyperedge is be treated as a tree node with its children. Source sentence trees are constructed by recursively expanding nodes in the forest. Figure 2a shows a tree constructed with three hyperedges, having head nodes A, B and F. Figure 2b shows the typical string representati"
I11-1088,P06-1077,0,0.0364607,"xtracted from NIST (2010a; 2010b) data. 2 Figure 1: A translation rule reproduced from (Mi and Huang, 2008) 2.2 Forest Based Translation Forest-based SMT overcomes the limitations of parse errors in tree-based translation and has been shown to be faster than k-best tree-based translation (Mi et al., 2008). For translating a sentence using the forest-based translation technique, we require a parser that can process a source language sentence and produce a packed forest and a translation rule dictionary, or database, which is a collection of tree-to-string translation rules (Huang et al., 2006; Liu et al., 2006). Background A translation rule is a mapping from a source language tree to a string in the target language. An example translation rule from Chinese to English is shown in Figure 1. The left-hand-side is the source language tree. The Chinese word yˇu is translated to with in the target side on the right. The xi variables on the right-hand-side are placeholders for the corresponding elements in the tree. Other numerical parameters associated with each translation rule are not shown here. Note that there may be several rules in the rule dictionary that have identical source language trees but d"
I11-1088,D08-1022,0,0.0473526,"Missing"
I11-1088,P08-1023,0,0.106786,"describe an algorithm that allows incremental search for trees in a forest and show that its performance is orders of magnitude faster than iterative search. A B-tree index is used to store the rule dictionaries. Prefix-compressed indexes with a large page size are found to provide a balance of fast search and disk space utilisation. 1 Introduction Statistical machine translation (SMT) uses machine learning and parallel corpora to perform translations automatically. Syntax based SMT systems can be broadly classified into two types based on the input to the system: tree-based and string-based (Mi et al., 2008). In a tree-based system, the input is a parse tree of the source language, whereas in the latter, the input is a sequence of words that is simultaneously parsed and translated. Forest-based translation employs multiple parse trees for each source language sentence. Forest-based translation can be performed in three main steps. First, the input sentence is parsed into a packed forest, which is then pruned. Next, for each tree in the packed forest, all matching translation rules are found. These translation rules are then combined to form a translation forest. The translation forest is finally"
I13-1161,P10-1010,1,0.797416,"Missing"
I13-1161,2001.mtsummit-papers.68,0,0.0398651,"Missing"
I13-1161,2006.amta-papers.25,0,0.0338296,"Missing"
I13-1161,P11-1122,0,0.0468569,"Missing"
I13-1161,P02-1040,0,\N,Missing
I13-1177,A00-1031,0,0.153464,"seed model tagger T0 is built on just the high scoring sentences. By applying self-training with revision, a series of new models T1 , T2 , . . . , Tm is constructed where Ti is the tagger after i iterations. The target language tagger, T agger(t), is then the last model, Tm . T agger(s) is trained from manually annotated data Data(s) which is mainly derived from the CoNLL 2006 and CoNLL 2007 Shared Tasks. Using the matching provided by Petrov et al., we map the individual tagsets to the Universal Tagset. We train a supervised POS tagger T agger(s) on the annotated data using the TNT tagger (Brants, 2000). Table 3 shows the source and size of annotated data, and the 5 fold cross-validation accuracy of T agger(s), for each language. We evaluate each T agger(t) using Data(t); results are shown in Table 4. The average tagger per3 NOUN, VERB, ADJ, ADV, PRON (pronouns), DET (determiners and articles), ADP (prepositions and postpositions), NUM (numerals), CONJ (conjunctions), PRT (particles), “.” (punctuation), and X (all other categories, e.g., foreign words, abbreviations). 1245 Source language en da nl pt sv el it es de Baseline en 55.73 75.70 72.40 66.56 47.67 74.50 68.76 72.24 30.28 da 76.17 76"
I13-1177,P11-1061,0,0.31602,"ta is not available for our target language, and we need to choose which data to collect – although further improvements can be obtained using features based on parallel corpora. We then show that if multiple source languages are available, even better accuracy can be obtained by combining information from them. 2 Related work One approach to build an unsupervised POS tagger is to project tag information from a resourcerich source language to a resource-poor target lan1243 International Joint Conference on Natural Language Processing, pages 1243–1249, Nagoya, Japan, 14-18 October 2013. guage. Das and Petrov (2011) and Duong et al. (2013) both achieve state-of-the-art performance on eight European languages using this crosslingual approach. The two approaches are similar in the following respects. First, both project tag information from source to target language, applying some kind of noise reduction along the way: Das and Petrov use high confidence alignments, while Duong et al. use high confidence sentences. Second, both use a semi-supervised method to obtain more labeled data: Das and Petrov use graph based label propagation, while Duong et al. use self-training. Finally, both apply noise reduction/"
I13-1177,P13-2112,1,0.741841,"r target language, and we need to choose which data to collect – although further improvements can be obtained using features based on parallel corpora. We then show that if multiple source languages are available, even better accuracy can be obtained by combining information from them. 2 Related work One approach to build an unsupervised POS tagger is to project tag information from a resourcerich source language to a resource-poor target lan1243 International Joint Conference on Natural Language Processing, pages 1243–1249, Nagoya, Japan, 14-18 October 2013. guage. Das and Petrov (2011) and Duong et al. (2013) both achieve state-of-the-art performance on eight European languages using this crosslingual approach. The two approaches are similar in the following respects. First, both project tag information from source to target language, applying some kind of noise reduction along the way: Das and Petrov use high confidence alignments, while Duong et al. use high confidence sentences. Second, both use a semi-supervised method to obtain more labeled data: Das and Petrov use graph based label propagation, while Duong et al. use self-training. Finally, both apply noise reduction/filtering on the (automa"
I13-1177,W11-3603,0,0.403144,"Missing"
I13-1177,D08-1109,0,0.072329,"Missing"
I13-1177,steinberger-etal-2006-jrc,0,0.0711628,"Missing"
I13-1177,W04-3229,0,0.801596,"Missing"
I13-1177,2005.mtsummit-papers.11,0,0.0294774,"the source or target language role. From Table 4, it seems that taggers perform better if the source and target language are in the same language family. For example, the top four source languages for Danish are Dutch, English, Swedish, and German, and the top two source languages for Portuguese are Italian and Spanish. This confirms the intuition in adding language relatedness features in section 4. Duong et al. (2013) used English as the source language to build taggers for the same eight other languages. The only difference between these two experiments is that Duong et al. used Europarl (Koehn, 2005) data instead of JRC-Acquis. Table 2 also compares the size of parallel data with JRC-Acquis 76.2 73.0 79.6 73.8 50.4 72.2 75.4 74.0 71.8 Europarl 85.6 84.0 86.3 81.0 80.0 81.4 83.3 85.4 83.4 Table 5: Accuracy on JRC-Acquis and Europarl using English as the source language. English as the source language for JRC-Acquis and Europarl. Given that Europarl is larger, higher performance is expected. Table 5 compares the tagger accuracy for each target language using English as the source language, for the two datasets. As expected, the accuracies are higher for Europarl. However, there is a strong"
I13-1177,2009.mtsummit-papers.7,0,0.0300127,"Missing"
I13-1177,petrov-etal-2012-universal,0,0.0786767,"he entropy for each lexical entry is calculated as X H(s) = − p(t|s) × log2 p(t|s) t∈T where T is the set of possible translations of word s, and t is a translation. For each language, we pick a fixed amount of text (1 million words) and calculate the average entropy for all words. 5 Build taggers In this section we construct 72 taggers, using parallel data for 72 language pairs, and then evaluate the performance of each pair. We use an open source unsupervised cross-lingual POS tagger (UMPOS) from Duong et al. (2013), a stateof-the-art system. UMPOS employs the consensus 12 Universal Tagset (Petrov et al., 2012),3 to avoid the problem of transliterating between different tagsets for different languages, and to enable comparison across languages. The input for UMPOS is a tagger for the source language, T agger(s), along with parallel data (s–t). The source language s is tagged using T agger(s), and then the tagged labels are projected to the target language t. Sentences are then ranked, and a seed model tagger T0 is built on just the high scoring sentences. By applying self-training with revision, a series of new models T1 , T2 , . . . , Tm is constructed where Ti is the tagger after i iterations. The"
J09-3007,W09-1905,0,0.0145162,"ly saving months of manual effort in the process. Once the data has some basic level of organization, the next challenge is one of simultaneously downscaling and upscaling. First, we need new techniques that work on small data sets (downscaling), with the consequence that fewer resources are spent on data collection, while permitting many more languages to be analyzed in the same timeframe (upscaling). What methods do we have that can detect structure in small, noisy data sets, while being directly applicable to a wide variety of languages? This represents uncharted territory for NLP.9 9 See (Palmer et al. 2009) for a promising pilot study. 472 Bird Natural Language Processing and Linguistic Fieldwork This dual perspective applies to the data collection work itself. If we have just one week in a location where a language is spoken, to collect all the data we will ever have for this language, what will we do? I write this on the eve of a one-week visit to the Usarufa language area in the Eastern Highlands of Papua New Guinea, under the auspices of SIL. The language has about 1,000 speakers, and is no longer being learned by children. We will give out digital voice recorders to have people record lingu"
J94-1003,C92-1015,1,0.866527,"of intersection, which corresponds to logical conjunction. Furthermore, this move of intensionalizing phonology enabled us to provide a straightforward formal basis for adding logical negation and disjunction to our representations. One important consequence of this work is that there are now good prospects for the incorporation of nonlinear phonology into constraint-based grammar formalisms such as HPSG (Pollard and Sag 1987). Such a move gives rise to a novel view of the relationship between phonology and the other modules of grammar, as some initial investigation has already demonstrated (Bird 1992; Bird and Klein in press). Making surface generalizations the only goal of analysis makes the machine learning of analyses simpler (Ellison forthcoming). The automaton semantics for autosegmental representations and rules gives us a mechanical way of comparing the empirical claims made by a range of autosegmental and segmental accounts of natural language phenomena. Finally, to the extent that phonologists are becoming increasingly committed to a declarative, constraint-based view of their domain, we believe that the model proposed here is well suited to their computational needs. Acknowledgm"
J94-1003,J92-1003,0,0.074084,"need to integrate it more closely with computational * University of Edinburgh, Centre for Cognitive Science, 2 Buccleuch Place, Edinburgh EH8 9LW, Scotland, U.K. E-mail: {steven,marke}@cogsci.ed.ac.uk 1 (Antworth 1990; Ritchie, Russell, Black, and Pulman 1992; Sproat 1992) 2 (Bear 1986; Antworth 1990; Schiller and Steffens 1991; Pulman and Hepple 1993) 3 Two caveats are n e c e s s a r y here. SPE rules must be restricted so as not to apply to their own output (Johnson 1972) and there is no guarantee that the transducer encoding an SPE rule can be expressed using the two-level rule notation (Ritchie 1992). (~) 1994 Association for Computational Linguistics Computational Linguistics Volume 20, Number 1 grammar frameworks on the one hand and modern nonlinear phonology on the other. The primary goal of this article is to show how the central tenets of autosegmental phonology translate into an implemented finite state model. The model is named ONE-LEVEL PHONOLOGY for two reasons. First, the model is monostratal, in that there is only one level of linguistic description. Second, the name is intended to contrast with models employing two levels (such as the FST model mentioned above) or three levels"
J94-1003,E93-1035,0,0.0717867,"responding consonant, but also any of the vowels. So the f autosegment on the consonant-tier denotes any sequence of the segments f, a, i, or u. The two uses of f are disambiguated by reference to the tiers on which they occur. Under this definition, autosegments on the consonant tier can spread over vowels to the next consonantal position. The root SFA generalizes over all stems constructed from the same root. The encoding on the consonant tier describing the root fql, to do, appears in (45). 23 The reader interested in other finite-state models of Arabic phonology is directed to the work of Narayanan and Hashem (1993), Beesley, Buckwater, and Newton (1989), and Beesley (1990). These have not been discussed at length here, because they do not seek to implement autosegmental phonology. 24 Biliteral roots in forms I, III, IV, V, VI, VII, VIII, X, XI, XII, and XIV, triliteral roots in forms IX and XI, and quadriliteral roots in form QIV reorder the consonant-vowel sequence in the final syllable of the root if followed by a vowel-initial inflection. Where possible, the Arabic verb stem will metathesize or delete short vowels to create a geminate with root consonants (McCarthy 1981, pp. 197f). 86 Steven Bird and"
J94-1003,J94-3010,1,\N,Missing
J94-1003,E87-1002,0,\N,Missing
J94-1003,J95-4011,0,\N,Missing
J94-3010,C92-1015,1,0.943791,"clear theoretical statement can be found, it is usually expressed in procedural terms, which clouds the empirical ramifications making a theory difficult to falsify. Finally, even when explicit and nonprocedural generalizations are found, they are commonly stated in a nonlinear model, which clearly goes beyond the assumptions about phonology made in HPSG as it currently stands. We approach these challenges by adopting a formal, nonprocedural, nonlinear model of phonology and showing how it can be integrated into HPSG, following on the heels of recent work by the authors (Bird and Klein 1990; Bird 1992; Klein 1992). One of the starting assumptions of this work is that phonological representations are intensional, i.e. each representation is actually a description of a class of utterances. Derivations progress by refining descriptions, further constraining the class of denoted objects. Lexical representations are likewise partial, and phonological constraints are cast as generalizations in a lexical inheritance hierarchy or in a prosodic inheritance hierarchy. When set against the background of constraint-based grammar, this intensional approach is quite natural (cf. Johnson [1988]). Moreove"
J94-3010,J94-1003,1,0.903474,"n some notable recent attempts to rescue the FST model from its linearity in order to encompass nonlinear phonology (Kay 1987; Kornai 1991; Wiebe 1992). However, from our perspective, these refinements to the FST model still admit unwarranted operations on phonological representations, as well as rule conspiracies and the like. Rather, we believe a more constrained and linguistically appealing approach is to employ finite-state automata (FSAs) in preference to FSTS, since it has been shown how FSAS can encode autosegmental representations and a variety of constraints on those representations (Bird and Ellison 1994). The leading idea in this work is that each tier is a partial description of a string, and tiers are put together using the intersection operation defined on FSAs. Apart from being truer to current phonological theorizing, this one-level model has a second important advantage over the two-level model. Since the set of FSAs forms a Boolean lattice under intersection, union, and complement (a direct consequence of the standard closure properties for regular languages), we can safely conjoin (&apos;unify&apos;), disjoin, and negate phonological descriptions. Such a framework is obviously compatible with c"
J94-3010,C90-3009,0,0.025442,"tion of a class of utterances. Derivations progress by refining descriptions, further constraining the class of denoted objects. Lexical representations are likewise partial, and phonological constraints are cast as generalizations in a lexical inheritance hierarchy or in a prosodic inheritance hierarchy. When set against the background of constraint-based grammar, this intensional approach is quite natural (cf. Johnson [1988]). Moreover, some recent thinking on the phonology-phonetics interface supports this view (Pierrehumbert 1990; Coleman 2 (Bach and Wheeler 1981; Wheeler 1981; Bird 1990; Cahill 1990; Coleman 1991; Scobbie 1991; Bird 1992; Walther 1992; Mastroianni 1993; Russell 1993) 456 Steven Bird and Ewan Klein Phonological Analysis in Typed Feature Systems 1992). However, it represents a fundamental split with the generative tradition, where rules do not so much refine descriptions as alter the objects themselves (Keating 1984). While it is clearly possible to integrate an essentially generative model into the mold of constraint-based grammar (Krieger, Pirker, and Nerbonne 1993), it is less clear that this is the approach most phonologists would wish to take nowadays. It is becoming"
J94-3010,C90-3052,0,0.0262286,"to be more an artifact of the grammar architecture than an independently motivated requirement. Equally, it has been argued that the phenomenon of heavy N P shift is a kind of syntaxphonology interaction that is simply stated in a constraint-based approach, where the linear precedence constraints of syntax are sensitive to the phonological category of weight (Bird 1992). 1.3 Theoretical Framework Typed feature structures (Carpenter 1992) impose a type discipline on constraint-based grammar formalisms. A partial ordering over the types gives rise to an inheritance hierarchy of constraints. As Emele and Zajac (1990) point out, this object-oriented approach brings a number of advantages to grammar writing, such as a high level of abstraction, inferential capacity and modularity. On the face of it, such benefits should extend beyond syntax--to phonology for example. Although there have been some valuable efforts to exploit inheritance and type hierarchies within phonology (e.g. Reinhard and Gibbon 1991), the potential of typed feature structures for this area has barely been scratched so far. In this section, we present a brief overview of HPSG (Pollard and Sag 1987), a constraint-based grammar formalism b"
J94-3010,P86-1038,0,0.0349874,"e declared as the appropriate values for certain attributes in a typed feature system, string types are only declared in terms of the basic alphabet and other string types. It is not possible to employ non-string types in the definition of string types. This is a severe restriction, since list types (say, in HPSG) allow arbitrary feature structures as elements, and we would like to be able to do the same for string types. Work on overcoming this limitation is currently in progress, and builds on the well-known similarity between feature structures and automata, when viewed as directed graphs (Kasper and Rounds 1986). 7. Conclusion In this paper, we have tried to give the reader an impression of how two rather different phonological phenomena can be given a declarative encoding in a constraint-based grammar. Although we have focused on phonology, we have also placed our analyses within a morphological context as befits the multi-dimensional perspective of HPSG. The formal framework of HPSG is rather powerful; certainly powerful enough to capture many analyses in the style of classical generative phonology in which arbitrary mappings are allowed between underlying and surface representations. We have limit"
J94-3010,E87-1002,0,0.13676,"ve comprehensive introductions to the field. The formalism is an attractive computational model for 1960s generative phonology. However, as has already been noted, phonologists have since moved away from complex string rewriting systems to a range of so-called nonlinear models of phonology. The central innovation of this more recent work is the idea that phonological representations are not strings but collections of strings, synchronized like an orchestral score. There have been some notable recent attempts to rescue the FST model from its linearity in order to encompass nonlinear phonology (Kay 1987; Kornai 1991; Wiebe 1992). However, from our perspective, these refinements to the FST model still admit unwarranted operations on phonological representations, as well as rule conspiracies and the like. Rather, we believe a more constrained and linguistically appealing approach is to employ finite-state automata (FSAs) in preference to FSTS, since it has been shown how FSAS can encode autosegmental representations and a variety of constraints on those representations (Bird and Ellison 1994). The leading idea in this work is that each tier is a partial description of a string, and tiers are p"
J94-3010,C92-1026,1,0.830396,"retical statement can be found, it is usually expressed in procedural terms, which clouds the empirical ramifications making a theory difficult to falsify. Finally, even when explicit and nonprocedural generalizations are found, they are commonly stated in a nonlinear model, which clearly goes beyond the assumptions about phonology made in HPSG as it currently stands. We approach these challenges by adopting a formal, nonprocedural, nonlinear model of phonology and showing how it can be integrated into HPSG, following on the heels of recent work by the authors (Bird and Klein 1990; Bird 1992; Klein 1992). One of the starting assumptions of this work is that phonological representations are intensional, i.e. each representation is actually a description of a class of utterances. Derivations progress by refining descriptions, further constraining the class of denoted objects. Lexical representations are likewise partial, and phonological constraints are cast as generalizations in a lexical inheritance hierarchy or in a prosodic inheritance hierarchy. When set against the background of constraint-based grammar, this intensional approach is quite natural (cf. Johnson [1988]). Moreover, some recen"
J94-3010,P93-1019,0,0.023584,"Missing"
J94-3010,E91-1023,0,0.0133915,"Framework Typed feature structures (Carpenter 1992) impose a type discipline on constraint-based grammar formalisms. A partial ordering over the types gives rise to an inheritance hierarchy of constraints. As Emele and Zajac (1990) point out, this object-oriented approach brings a number of advantages to grammar writing, such as a high level of abstraction, inferential capacity and modularity. On the face of it, such benefits should extend beyond syntax--to phonology for example. Although there have been some valuable efforts to exploit inheritance and type hierarchies within phonology (e.g. Reinhard and Gibbon 1991), the potential of typed feature structures for this area has barely been scratched so far. In this section, we present a brief overview of HPSG (Pollard and Sag 1987), a constraint-based grammar formalism built around a type system that suits our purposes in phonology. In order to formulate the type system of our grammar, we need to make two kinds of TYPEDECLARATION.The first kind contains information about the subsumption ordering over types. For example, the basic grammar object in HPSG is the feature structure of type sign. The type sign has some SUBTYPES.If a is a subtype of T, then ~r pr"
J94-3010,P87-1010,0,0.127302,"indicate the extra constraints: 8 Example 12 a. phrase =_foot + A C1 A . . . A Ck b. foot = syl + A C t A . . . A Cn This concludes our discussion of string-based phonology. We have tried to show h o w a phonological model based on FSAs is compatible with the list notation and type regime of HPSG. Next we m o v e onto a consideration of m o r p h o l o g y and the lexicon. 7 These techniques employ the following equivalences: ~[A: ~] = -~[A: q~] = -~[A: ~]V~[B: @] [-~(A: T)]V[A: ~*] Here ~(A:T) indicates that the attribute A is not appropriate for this feature structure. 8 Sproat and Brunson (1987) have also proposed a model in which prosodic constituents are defined as conjunctions of constraints. 463 Computational Linguistics Volume 20, Number 3 3. Morphology and the Lexicon 3.1 Linguistic Hierarchy The subsumption ordering over types can be used to induce a hierarchy of grammatically well-formed feature structures. This possibility has been exploited in the HPSG analysis of the lexicon: lexical entries consist of the idiosyncratic information particular to the entry, together with an indication of the minimal lexical types from which it inherits. To take an example from Pollard and S"
J94-3010,J92-2002,0,0.0222537,"as we shall see in Section 3.2, it is subject to some further appropriateness conditions that are not imposed on any of its supertypes. Continuing in the same vein, we can assign appropriateness conditions to the types synsem and phon that occurred as values in (2), (simplifying substantially from standard HPSG). Here we give the constraints for synsem. The type phon will be discussed in Section 2. Example 3 synsem CAT : AGR : SUBCAT : SEM : cat agr list semantics To conclude this section, we shall look very briefly at matters of interpretation and inference. As shown by Carpenter (1992) and Zajac (1992, in press), we can use constraint resolution to carry out type inference for feature terms. Following Zajac, let us say that a GROUND feature term is a term all of whose type symbols are minimal (i.e., the most specific types in the hierarchy immediately above _L). A WELL-TYPED feature term is one that obeys all the type definitions. Then the meaning of a feature term F is given by the set of all well-typed ground feature terms that are subsumed by F. Evaluating F, construed as a query, involves describing F&apos;s denotation; for example, enumerating all the well-typed ground feature terms it sub"
K15-1012,I13-1177,1,0.9016,"ngual transfer (Zeman et al., 2008; Søgaard, 2011; McDonald et al., 2011; Ma and Xia, 2014). In this setting, a delexicalized parser is trained on a resource-rich source language, and is then applied directly to a resource-poor target language. The only requirement here is that the source and target languages are POS tagged must use the same tagset. This assumption is pertinent for resourcepoor languages since it is relatively quick to manually POS tag the data. Moreover, there are many reports of high accuracy POS tagging for resourcepoor languages (Duong et al., 2014; Garrette et al., 2013; Duong et al., 2013b). The cross-lingual delexicalized approach has been shown to significantly outperform unsupervised approaches (McDonald et al., 2011; Ma and Xia, 2014). Parallel data can be used to boost the performance of a cross-lingual parser (McDonald et al., 2011; Ma and Xia, 2014). However, parallel data may be hard to acquire for truly resource-poor languages.1 Accordingly, we propose a method to improve the performance of a cross-lingual delexicalized parser using only monolingual data. Our approach is based on augmenting the delexicalized parser using syntactic word embeddings. Words from both sour"
K15-1012,P14-2133,0,0.0176224,"2014). However, parallel data may be hard to acquire for truly resource-poor languages.1 Accordingly, we propose a method to improve the performance of a cross-lingual delexicalized parser using only monolingual data. Our approach is based on augmenting the delexicalized parser using syntactic word embeddings. Words from both source and target language are mapped to a shared low-dimensional space based on their syntactic context, without recourse to parallel data. While prior work has struggled to efficiently incorporate word embedding information into the parsing model (Bansal et al., 2014; Andreas and Klein, 2014; Chen et al., 2014), we present a method for doing so using a neural netCross-lingual transfer has been shown to produce good results for dependency parsing of resource-poor languages. Although this avoids the need for a target language treebank, most approaches have still used large parallel corpora. However, parallel data is scarce for low-resource languages, and we report a new method that does not need parallel data. Our method learns syntactic word embeddings that generalise over the syntactic contexts of a bilingual vocabulary, and incorporates these into a neural network parser. We sho"
K15-1012,P13-2112,1,0.9306,"ngual transfer (Zeman et al., 2008; Søgaard, 2011; McDonald et al., 2011; Ma and Xia, 2014). In this setting, a delexicalized parser is trained on a resource-rich source language, and is then applied directly to a resource-poor target language. The only requirement here is that the source and target languages are POS tagged must use the same tagset. This assumption is pertinent for resourcepoor languages since it is relatively quick to manually POS tag the data. Moreover, there are many reports of high accuracy POS tagging for resourcepoor languages (Duong et al., 2014; Garrette et al., 2013; Duong et al., 2013b). The cross-lingual delexicalized approach has been shown to significantly outperform unsupervised approaches (McDonald et al., 2011; Ma and Xia, 2014). Parallel data can be used to boost the performance of a cross-lingual parser (McDonald et al., 2011; Ma and Xia, 2014). However, parallel data may be hard to acquire for truly resource-poor languages.1 Accordingly, we propose a method to improve the performance of a cross-lingual delexicalized parser using only monolingual data. Our approach is based on augmenting the delexicalized parser using syntactic word embeddings. Words from both sour"
K15-1012,P14-2131,0,0.131008,"l., 2011; Ma and Xia, 2014). However, parallel data may be hard to acquire for truly resource-poor languages.1 Accordingly, we propose a method to improve the performance of a cross-lingual delexicalized parser using only monolingual data. Our approach is based on augmenting the delexicalized parser using syntactic word embeddings. Words from both source and target language are mapped to a shared low-dimensional space based on their syntactic context, without recourse to parallel data. While prior work has struggled to efficiently incorporate word embedding information into the parsing model (Bansal et al., 2014; Andreas and Klein, 2014; Chen et al., 2014), we present a method for doing so using a neural netCross-lingual transfer has been shown to produce good results for dependency parsing of resource-poor languages. Although this avoids the need for a target language treebank, most approaches have still used large parallel corpora. However, parallel data is scarce for low-resource languages, and we report a new method that does not need parallel data. Our method learns syntactic word embeddings that generalise over the syntactic contexts of a bilingual vocabulary, and incorporates these into a neur"
K15-1012,D14-1096,1,0.840123,"idea of delexicalized parsing and cross-lingual transfer (Zeman et al., 2008; Søgaard, 2011; McDonald et al., 2011; Ma and Xia, 2014). In this setting, a delexicalized parser is trained on a resource-rich source language, and is then applied directly to a resource-poor target language. The only requirement here is that the source and target languages are POS tagged must use the same tagset. This assumption is pertinent for resourcepoor languages since it is relatively quick to manually POS tag the data. Moreover, there are many reports of high accuracy POS tagging for resourcepoor languages (Duong et al., 2014; Garrette et al., 2013; Duong et al., 2013b). The cross-lingual delexicalized approach has been shown to significantly outperform unsupervised approaches (McDonald et al., 2011; Ma and Xia, 2014). Parallel data can be used to boost the performance of a cross-lingual parser (McDonald et al., 2011; Ma and Xia, 2014). However, parallel data may be hard to acquire for truly resource-poor languages.1 Accordingly, we propose a method to improve the performance of a cross-lingual delexicalized parser using only monolingual data. Our approach is based on augmenting the delexicalized parser using synt"
K15-1012,P13-1057,0,0.0152204,"ed parsing and cross-lingual transfer (Zeman et al., 2008; Søgaard, 2011; McDonald et al., 2011; Ma and Xia, 2014). In this setting, a delexicalized parser is trained on a resource-rich source language, and is then applied directly to a resource-poor target language. The only requirement here is that the source and target languages are POS tagged must use the same tagset. This assumption is pertinent for resourcepoor languages since it is relatively quick to manually POS tag the data. Moreover, there are many reports of high accuracy POS tagging for resourcepoor languages (Duong et al., 2014; Garrette et al., 2013; Duong et al., 2013b). The cross-lingual delexicalized approach has been shown to significantly outperform unsupervised approaches (McDonald et al., 2011; Ma and Xia, 2014). Parallel data can be used to boost the performance of a cross-lingual parser (McDonald et al., 2011; Ma and Xia, 2014). However, parallel data may be hard to acquire for truly resource-poor languages.1 Accordingly, we propose a method to improve the performance of a cross-lingual delexicalized parser using only monolingual data. Our approach is based on augmenting the delexicalized parser using syntactic word embeddings."
K15-1012,J92-4003,0,0.0677996,"next subsection, we review some cross-lingual word embedding methods and propose our syntactic word embeddings. Section 4 empirically compares these word embeddings when incorporated into a dependency parser. 3.1 Tu mascota Pronoun Noun The weather Det Noun parece adorable Verb Adj is horrible today Verb Adj Noun Figure 1: Examples of the syntactic word embeddings for Spanish and English. In each case, the highlighted tags are predicted by the highlighted word. The Spanish sentence means “your pet looks lovely”. build cross-lingual word representations using a variant of the Brown clusterer (Brown et al., 1992) applied to parallel data. Bansal et al. (2014) and Turian et al. (2010) showed that for monolingual dependency parsing, the simple Brown clustering based algorithm outperformed many word embedding techniques. In this paper we compare our approach to forming cross-lingual word embeddings with those of both Hermann and Blunsom (2014a) and T¨ackstr¨om et al. (2012). Cross-lingual word embeddings We review methods that can represent words in both source and target languages in a lowdimensional space. There are many benefits of using a low-dimensional space. Instead of the traditional “one-hot” re"
K15-1012,W06-2920,0,0.0231945,"e start by building syntactic word embeddings between source and target languages as shown in algorithm 1. Next we incorporate syntactic word embeddings using the algorithm proposed in Section 3.3. The third step is to substitute source- with target-language syntactic word embeddings. Finally, we parse the target language using this substituted model. In this way, the model will recognize lexical items for the target language. 4 Experiments on CoNLL Data Experiments We test our method of incorporating syntactic word embeddings into a neural network parser, for both the existing CoNLL dataset (Buchholz and Marsi, 2006; Nivre et al., 2007) and the newlyreleased Universal Dependency Treebank (Nivre et al., 2015). We employed the Unlabeled Attachment Score (UAS) without punctuation for comparison with prior work on the CoNLL dataset. Where possible we also report Labeled Attachment Score (LAS) without punctuation. We use English as the source language for this experiment. 5 This is a consequence of only training the parser on the source language. If we were to update embeddings during parser training this would mean they no longer align with the target language embeddings. 6 117 All performance comparisons in"
K15-1012,W12-1909,1,0.714748,"9), text classification (Ozg¨ and G¨ung¨or, 2010), and question answering (Cui et al., 2005). Supervised approaches to dependency parsing have been very successful for many resource-rich languages, where relatively large treebanks are available (McDonald et al., 2005a). However, for many languages, annotated treebanks are not available, and are very costly to create (B¨ohmov´a et al., 2001). This motivates the development of unsupervised approaches that can make use of unannotated, monolingual data. However, purely unsupervised approaches have relatively low accuracy (Klein and Manning, 2004; Gelling et al., 2012). 1 Note that most research in this area (as do we) evaluates on simulated low-resource languages, through selective use of data in high-resource languages. Consequently parallel data is plentiful, however this is often not the case in the real setting, e.g., for Tagalog, where only scant parallel data exists (e.g., dictionaries, Wikipedia and the Bible). 113 Proceedings of the 19th Conference on Computational Language Learning, pages 113–122, c Beijing, China, July 30-31, 2015. 2015 Association for Computational Linguistics language. Delexicalized parsing relies on the fact that parts-of-spee"
K15-1012,H05-1091,0,0.0444913,"eed parallel data. Our method learns syntactic word embeddings that generalise over the syntactic contexts of a bilingual vocabulary, and incorporates these into a neural network parser. We show empirical improvements over a baseline delexicalised parser on both the CoNLL and Universal Dependency Treebank datasets. We analyse the importance of the source languages, and show that combining multiple source-languages leads to a substantial improvement. 1 Introduction Dependency parsing is a crucial component of many natural language processing (NLP) systems for tasks such as relation extraction (Bunescu and Mooney, 2005), statistical machine transla¨ ur tion (Xu et al., 2009), text classification (Ozg¨ and G¨ung¨or, 2010), and question answering (Cui et al., 2005). Supervised approaches to dependency parsing have been very successful for many resource-rich languages, where relatively large treebanks are available (McDonald et al., 2005a). However, for many languages, annotated treebanks are not available, and are very costly to create (B¨ohmov´a et al., 2001). This motivates the development of unsupervised approaches that can make use of unannotated, monolingual data. However, purely unsupervised approaches h"
K15-1012,P14-1006,0,0.0667528,"xamples of the syntactic word embeddings for Spanish and English. In each case, the highlighted tags are predicted by the highlighted word. The Spanish sentence means “your pet looks lovely”. build cross-lingual word representations using a variant of the Brown clusterer (Brown et al., 1992) applied to parallel data. Bansal et al. (2014) and Turian et al. (2010) showed that for monolingual dependency parsing, the simple Brown clustering based algorithm outperformed many word embedding techniques. In this paper we compare our approach to forming cross-lingual word embeddings with those of both Hermann and Blunsom (2014a) and T¨ackstr¨om et al. (2012). Cross-lingual word embeddings We review methods that can represent words in both source and target languages in a lowdimensional space. There are many benefits of using a low-dimensional space. Instead of the traditional “one-hot” representation with the number of dimensions equal to vocabulary size, words are represented using much fewer dimensions. This confers the benefit of generalising over the vocabulary to alleviate issues of data sparsity, through learning representations encoding lexical relations such as synonymy. Several approaches have sought to le"
K15-1012,D14-1082,0,0.0364157,"terising dependency relations. 115 Algorithm 1 Syntactic word embedding 1: Match the source and target tagsets to the Universal Tagset. 2: Extract word n-gram sequences for both the source and target language. 3: For each n-gram, keep the middle word, and replace the other words by their POS. 4: Train a skip-gram word embedding model on the resulting list of word and POS sequences from both the source and target language SOFT-MAX LAYER W2 HIDDEN LAYER W1 WORDS POS TAGS Eword ARC LABELS Epos Earc MAPPING LAYER CONFIGURATION (STACK, QUEUE, ARCS) Figure 2: Neural Network Parser Architecture from Chen and Manning (2014) We assume the same POS tagset is used for both the source and target language,2 and learn word embeddings for each word type in both languages into the same syntactic space of nearby POS contexts. In particular, we develop a predictive model of the tags to the left and right of a word, as illustrated in Figure 1 and outlined in Algorithm 1. Figure 1 illustrates two training contexts extracted from our English source and Spanish target language, where the highlighted fragments reflect the tags being predicted around each focus word. Note that for this example, the POS contexts for the English"
K15-1012,C14-1078,0,0.0261532,"Missing"
K15-1012,P04-1061,0,0.111529,"¨ ur tion (Xu et al., 2009), text classification (Ozg¨ and G¨ung¨or, 2010), and question answering (Cui et al., 2005). Supervised approaches to dependency parsing have been very successful for many resource-rich languages, where relatively large treebanks are available (McDonald et al., 2005a). However, for many languages, annotated treebanks are not available, and are very costly to create (B¨ohmov´a et al., 2001). This motivates the development of unsupervised approaches that can make use of unannotated, monolingual data. However, purely unsupervised approaches have relatively low accuracy (Klein and Manning, 2004; Gelling et al., 2012). 1 Note that most research in this area (as do we) evaluates on simulated low-resource languages, through selective use of data in high-resource languages. Consequently parallel data is plentiful, however this is often not the case in the real setting, e.g., for Tagalog, where only scant parallel data exists (e.g., dictionaries, Wikipedia and the Bible). 113 Proceedings of the 19th Conference on Computational Language Learning, pages 113–122, c Beijing, China, July 30-31, 2015. 2015 Association for Computational Linguistics language. Delexicalized parsing relies on the"
K15-1012,2005.mtsummit-papers.11,0,0.0892953,"Missing"
K15-1012,petrov-etal-2012-universal,0,0.0247689,"Missing"
K15-1012,P14-1126,0,0.13511,"r for Unsupervised Dependency Parsing Without Parallel Data Long Duong,12 Trevor Cohn,1 Steven Bird,1 and Paul Cook3 1 Department of Computing and Information Systems, University of Melbourne 2 National ICT Australia, Victoria Research Laboratory 3 Faculty of Computer Science, University of New Brunswick lduong@student.unimelb.edu.au {t.cohn,sbird}@unimelb.edu.au paul.cook@unb.ca Abstract Most recent work on unsupervised dependency parsing for low-resource languages has used the idea of delexicalized parsing and cross-lingual transfer (Zeman et al., 2008; Søgaard, 2011; McDonald et al., 2011; Ma and Xia, 2014). In this setting, a delexicalized parser is trained on a resource-rich source language, and is then applied directly to a resource-poor target language. The only requirement here is that the source and target languages are POS tagged must use the same tagset. This assumption is pertinent for resourcepoor languages since it is relatively quick to manually POS tag the data. Moreover, there are many reports of high accuracy POS tagging for resourcepoor languages (Duong et al., 2014; Garrette et al., 2013; Duong et al., 2013b). The cross-lingual delexicalized approach has been shown to significan"
K15-1012,P11-2120,0,0.0252529,"Missing"
K15-1012,P05-1012,0,0.0170725,"se the importance of the source languages, and show that combining multiple source-languages leads to a substantial improvement. 1 Introduction Dependency parsing is a crucial component of many natural language processing (NLP) systems for tasks such as relation extraction (Bunescu and Mooney, 2005), statistical machine transla¨ ur tion (Xu et al., 2009), text classification (Ozg¨ and G¨ung¨or, 2010), and question answering (Cui et al., 2005). Supervised approaches to dependency parsing have been very successful for many resource-rich languages, where relatively large treebanks are available (McDonald et al., 2005a). However, for many languages, annotated treebanks are not available, and are very costly to create (B¨ohmov´a et al., 2001). This motivates the development of unsupervised approaches that can make use of unannotated, monolingual data. However, purely unsupervised approaches have relatively low accuracy (Klein and Manning, 2004; Gelling et al., 2012). 1 Note that most research in this area (as do we) evaluates on simulated low-resource languages, through selective use of data in high-resource languages. Consequently parallel data is plentiful, however this is often not the case in the real s"
K15-1012,N12-1052,0,0.17988,"Missing"
K15-1012,H05-1066,0,0.0270963,"Missing"
K15-1012,D11-1006,0,0.194472,"r is trained on a resource-rich source language, and is then applied directly to a resource-poor target language. The only requirement here is that the source and target languages are POS tagged must use the same tagset. This assumption is pertinent for resourcepoor languages since it is relatively quick to manually POS tag the data. Moreover, there are many reports of high accuracy POS tagging for resourcepoor languages (Duong et al., 2014; Garrette et al., 2013; Duong et al., 2013b). The cross-lingual delexicalized approach has been shown to significantly outperform unsupervised approaches (McDonald et al., 2011; Ma and Xia, 2014). Parallel data can be used to boost the performance of a cross-lingual parser (McDonald et al., 2011; Ma and Xia, 2014). However, parallel data may be hard to acquire for truly resource-poor languages.1 Accordingly, we propose a method to improve the performance of a cross-lingual delexicalized parser using only monolingual data. Our approach is based on augmenting the delexicalized parser using syntactic word embeddings. Words from both source and target language are mapped to a shared low-dimensional space based on their syntactic context, without recourse to parallel dat"
K15-1012,N13-1126,0,0.143921,"Missing"
K15-1012,P10-1040,0,0.0215477,"nd propose our syntactic word embeddings. Section 4 empirically compares these word embeddings when incorporated into a dependency parser. 3.1 Tu mascota Pronoun Noun The weather Det Noun parece adorable Verb Adj is horrible today Verb Adj Noun Figure 1: Examples of the syntactic word embeddings for Spanish and English. In each case, the highlighted tags are predicted by the highlighted word. The Spanish sentence means “your pet looks lovely”. build cross-lingual word representations using a variant of the Brown clusterer (Brown et al., 1992) applied to parallel data. Bansal et al. (2014) and Turian et al. (2010) showed that for monolingual dependency parsing, the simple Brown clustering based algorithm outperformed many word embedding techniques. In this paper we compare our approach to forming cross-lingual word embeddings with those of both Hermann and Blunsom (2014a) and T¨ackstr¨om et al. (2012). Cross-lingual word embeddings We review methods that can represent words in both source and target languages in a lowdimensional space. There are many benefits of using a low-dimensional space. Instead of the traditional “one-hot” representation with the number of dimensions equal to vocabulary size, wor"
K15-1012,P12-1066,0,0.143232,"Missing"
K15-1012,N09-1028,0,0.00887976,"at generalise over the syntactic contexts of a bilingual vocabulary, and incorporates these into a neural network parser. We show empirical improvements over a baseline delexicalised parser on both the CoNLL and Universal Dependency Treebank datasets. We analyse the importance of the source languages, and show that combining multiple source-languages leads to a substantial improvement. 1 Introduction Dependency parsing is a crucial component of many natural language processing (NLP) systems for tasks such as relation extraction (Bunescu and Mooney, 2005), statistical machine transla¨ ur tion (Xu et al., 2009), text classification (Ozg¨ and G¨ung¨or, 2010), and question answering (Cui et al., 2005). Supervised approaches to dependency parsing have been very successful for many resource-rich languages, where relatively large treebanks are available (McDonald et al., 2005a). However, for many languages, annotated treebanks are not available, and are very costly to create (B¨ohmov´a et al., 2001). This motivates the development of unsupervised approaches that can make use of unannotated, monolingual data. However, purely unsupervised approaches have relatively low accuracy (Klein and Manning, 2004; Ge"
K15-1012,I08-3008,0,0.0208359,"ds for improving the delexicalized parser using syntactic word embeddings. Section 4 describes experiments on the CoNLL dataset and Universal Dependency Treebank. Section 5 presents methods for selecting the best source language given a target language. 2 Unsupervised Cross-lingual Dependency Parsing There are two main approaches for building dependency parsers for resource-poor languages without using target-language treebanks: delexicalized parsing and projection (Hwa et al., 2005; Ma and Xia, 2014; T¨ackstr¨om et al., 2013; McDonald et al., 2011). The delexicalized approach was proposed by Zeman et al. (2008). They built a delexicalized parser from a treebank in a resource-rich source language. This parser can be trained using any standard supervised approach, but without including any lexical features, then applied directly to parse sentences from the resource-poor 3 Improving Delexicalized Parsing We propose a novel method to improve the performance of a delexicalized cross-lingual parser without recourse to parallel data. Our method uses no additional resources and is designed to com114 plement other methods. The approach is based on syntactic word embeddings where a word is represented as a lo"
K15-1012,D13-1141,0,\N,Missing
K15-1012,D07-1096,0,\N,Missing
L18-1530,L16-1632,1,0.828001,"because they constitute phonological units (syllable rhymes; Na syllables are composed of an onset, a rhyme, and a tone). Concerning improvements to the original transcriptions, we addressed cases where the same phoneme had inconsistent representation in the corpus, such as /wæ / and /w æ/, as well as an instance where the unicode representation of a single phoneme was sometimes v+nasality+syllabic diacritic and sometimes v+syllabic diacritic+nasality. We computed the Na results of Tables 1-3 using the larger suite of 224 minutes and these preprocessing changes. For Chatino, we used data of Ćavar et al. (2016) from the GORILLA language archive for Eastern Chatino of San Juan Quiahije, Oaxaca, Mexico (Cavar et al., 2016) for the purposes of comparing phoneme and tone prediction with Na when data restriction is in place. We used up to 50 minutes of data for training, 6 minutes for validation and 6 minutes for testing. The phoneme inventory we used consists of 31 labels along with 14 tone labels. For both languages, preprocessing involved removing punctuation and any other symbols that are not phonemes, tones or the tone 3358 Chatino Na Input Output PER ↓ TER ↓ PER ↓ TER ↓ TGB-F1 ↑ fbank fbank+pitch f"
L18-1530,C16-1328,0,0.0709237,"nemic transcription; French, English and Chinese translations; target label sequences: (1) phonemes only, (2) tones only, (3) phonemes and tones together, and (4) phonemes and tones with tone group boundary markers, “|”. labels are collapsed. The use of an underlying recurrent neural network allows the model to implicitly model context via the parameters of the LSTM, despite the independent frame-wise label predictions of the CTC network. It is this feature of the architecture that makes it a promising tool for tonal prediction, since tonal information is suprasegmental, spanning many frames (Mortensen et al., 2016). Context beyond the immediate local signal is indispensable for tonal prediction, and longranging context is especially important in the case of morphotonologically rich languages such as Na and Chatino. Past work distinguishes between embedded tonal modelling, where phoneme and tone labels are jointly predicted, and explicit tonal modelling, where they are predicted separately (Lee et al., 2002). We compare several training objectives for the purposes of phoneme and tone prediction. This includes separate prediction of 1. phonemes and 2. tones, as well as 3. jointly predict phonemes and tone"
ma-etal-2002-models,W01-1514,1,\N,Missing
ma-etal-2002-models,W01-1506,1,\N,Missing
ma-etal-2002-models,bird-etal-2000-towards,1,\N,Missing
maeda-etal-2002-creating,ma-etal-2002-models,1,\N,Missing
N10-1034,cassidy-2002-xquery,0,0.0349429,"y revised, in which case indexes need to be easily updatable? Do we expect logically identical queries to have the same performance, so that users do not have to rewrite their queries for efficiency? Key performance measures are index size and search times. Architecture: Is the query system standalone, or does it exist in a client-server architecture? Is there a separate user-interface layer that interacts with a data server using a well-defined API, or is it a monolithic system? Should it translate queries into another language, such as SQL (Bird et al., 2006; Nakov et al., 2005), or XQuery (Cassidy, 2002; Mayo et al., 2006), or to automata (Maryns and Kepser, 2009), in order to benefit from the performance optimizations they provide Indexing. The indexing methods used in individual systems are usually not reported. Many systems display nearly constant time for querying a database, regardless of the selectivity of a query, a strong indicator that no indexes are being used. For example, Emu performs all queries in memory with no indexes, and several others are likely to be the same (Cassidy and Harrington, 2001; K¨onig and Lezius, 2001; Heid et al., 2004). TGrep2 (Rohde, 2005) uses a custom cor"
N10-1034,W03-1006,0,0.0110312,"iguated with the help of a POS tag (e.g. wind/N, park/V). (Existing IR engines already support query with part-of-speech tags (Chowdhury and McCabe, 1998)). More complex queries could stipulate the syntactic category of apple is in subject position. A second benefit of large scale tree query is for natural language processing. For example, we might compute the likelihood that a given noun appears as the agent or patient of a verb, as a measure of animacy. We can use features derived from syntactic trees in order to support semantic role labeling, language modeling, and information extraction (Chen and Rambow, 2003; Collins et al., 2005; Hakenberg et al., 2009). A further benefit for natural language processing, though not yet realized, is for a treebank and query engine to provide the underlying storage and retrieval for a variety of linguistic applications. Just as a relational database is present in most business applications, providing reliable and efficient access to relational data, such a system would act as a repository of annotated texts, and expose an expressive API to client applications. A third benefit of large scale tree query is to support syntactic investigations, e.g. for develop267 Hum"
N10-1034,J07-4004,0,0.0574329,"c and engineering tasks. 1 Introduction The problem of representing and querying linguistic annotations has been an active area of research for several years. Much of the work has grown from efforts to curate large databases of annotated text such as treebanks, for use in developing and testing language technologies (Marcus et al., 1993; Abeill´e, 2003; Hockenmaier and Steedman, 2007). At least a dozen linguistic tree query languages have been developed for interrogating treebanks (see §2). While high quality syntactic parsers are able to efficiently annotate large quantities of English text (Clark and Curran, 2007), existing approaches to query do not work on the same scale. Many existing systems load the entire corpus into memory and check a user-supplied query against every tree. Others avoid the memory limitation, and use relational or XML database systems. Although these have built-in support for indexes, they do not scale up either (Ghodke and Bird, 2008; Zhang et al., 2001)). The ability to interrogate large collections of parsed text has important practical applications. First, it opens the way to a new kind of information retrieval (IR) that is sensitive to syntactic information, permitting user"
N10-1034,P05-1063,0,0.0125267,"f a POS tag (e.g. wind/N, park/V). (Existing IR engines already support query with part-of-speech tags (Chowdhury and McCabe, 1998)). More complex queries could stipulate the syntactic category of apple is in subject position. A second benefit of large scale tree query is for natural language processing. For example, we might compute the likelihood that a given noun appears as the agent or patient of a verb, as a measure of animacy. We can use features derived from syntactic trees in order to support semantic role labeling, language modeling, and information extraction (Chen and Rambow, 2003; Collins et al., 2005; Hakenberg et al., 2009). A further benefit for natural language processing, though not yet realized, is for a treebank and query engine to provide the underlying storage and retrieval for a variety of linguistic applications. Just as a relational database is present in most business applications, providing reliable and efficient access to relational data, such a system would act as a repository of annotated texts, and expose an expressive API to client applications. A third benefit of large scale tree query is to support syntactic investigations, e.g. for develop267 Human Language Technologi"
N10-1034,heid-etal-2004-querying,0,0.417927,"ted with syntax for matching tree structure. However, tree query is a more 268 complex and interesting task, due to several factors which we list below. Structure of the data: There are many varieties of treebank. Some extend the nested bracketing syntax to store morphological information. Others store complex attribute-value matrices in tree nodes or have tree-valued attributes (Oepen et al., ˇ 2002), or store dependency structures (Cmejrek et al., 2004), or categorial grammar derivations (Hockenmaier and Steedman, 2007). Others store multiple overlapping trees (Cassidy and Harrington, 2001; Heid et al., 2004; Volk et al., 2007). Form of results: Do we want entire trees, or matching subtrees, or just a count of the number of results? Do we need some indication of why the query matched a particular tree, perhaps by showing how query terms relate to a hit, cf. document snippets and highlighted words in web search results? Do we want to see multiple hits when a query matches a particular tree in more than one place? Do we want to see tree diagrams, or some machinereadable tree representation that can be used in external analysis? Can a query serve to update the treebank, cf. SQL update queries? Numbe"
N10-1034,I05-2019,0,0.0810398,"ank alleviates these problems. To improve recall performance, multiple parses for a given sentence could be stored (possibly derived from different parsers). A fourth benefit for large scale tree query is to support the curation of treebanks, a major enterprise in its own right (Abeill´e, 2003). Manual selection and correction of automatically generated parse trees is a substantial part of the task of preparing a treebank. At the point of making such decisions, it is often helpful for an annotator to view existing annotations of a given construction which have already been manually validated (Hiroshi et al., 2005). Occasionally, an earlier annotation decision may need to be reconsidered in the light of new examples, leading to further queries and to corrections that are spread across the whole corpus (Wallis, 2003; Xue et al., 2005). This paper explores a new methods for scaling up tree query using an IR engine. In §2 we describe existing tree query systems, elaborating on the design decisions, and on key aspects of their implementation and performance. In §3 we describe a method for indexing trees using an IR engine, and discuss the details of our open source implementation. In §4 we report results fr"
N10-1034,J07-3004,0,0.21333,"eral experiments with a large treebank demonstrate excellent scaling characteristics for a wide range of query types. This work facilitates the curation of much larger treebanks, and enables them to be used effectively in a variety of scientific and engineering tasks. 1 Introduction The problem of representing and querying linguistic annotations has been an active area of research for several years. Much of the work has grown from efforts to curate large databases of annotated text such as treebanks, for use in developing and testing language technologies (Marcus et al., 1993; Abeill´e, 2003; Hockenmaier and Steedman, 2007). At least a dozen linguistic tree query languages have been developed for interrogating treebanks (see §2). While high quality syntactic parsers are able to efficiently annotate large quantities of English text (Clark and Curran, 2007), existing approaches to query do not work on the same scale. Many existing systems load the entire corpus into memory and check a user-supplied query against every tree. Others avoid the memory limitation, and use relational or XML database systems. Although these have built-in support for indexes, they do not scale up either (Ghodke and Bird, 2008; Zhang et al"
N10-1034,P06-2052,0,0.0146991,"its when a query matches a particular tree in more than one place? Do we want to see tree diagrams, or some machinereadable tree representation that can be used in external analysis? Can a query serve to update the treebank, cf. SQL update queries? Number of results: Do we want all results, or the first n results in document order, or the “best” n results, where our notion of best might be based on representativeness or distinctiveness. Description language: Do we prefer to describe trees by giving examples of tree fragments, replacing some nodes replaced with wildcards (Hiroshi et al., 2005; Ichikawa et al., 2006; M´ırovsk´y, 2006)? Or do we prefer a path language (Rohde, 2005; Lai and Bird, 2010)? Or perhaps we prefer a language involving variables, quantifiers, boolean operators, and negation (K¨onig and Lezius, 2001; Kepser, ˇ ep´anek, 2009)? What built-in 2003; Pajas and Stˇ tree relations are required, beyond the typical parent/child, ancestor/descendent, sibling and temporal relations? (E.g. last child, leftmost descendent, parent’s following sibling, pronoun’s antecedent.) Do we need to describe tree nodes using regular expressions, or attributes and values? Do we need a type system, a pattern"
N10-1034,E03-1074,0,0.273594,"Missing"
N10-1034,U04-1019,1,0.811196,"boolean operators, and negation (K¨onig and Lezius, 2001; Kepser, ˇ ep´anek, 2009)? What built-in 2003; Pajas and Stˇ tree relations are required, beyond the typical parent/child, ancestor/descendent, sibling and temporal relations? (E.g. last child, leftmost descendent, parent’s following sibling, pronoun’s antecedent.) Do we need to describe tree nodes using regular expressions, or attributes and values? Do we need a type system, a pattern language, or boolean logic for talking about attribute values? The expressive requirements of the query language have been discussed at length elsewhere (Lai and Bird, 2004; M´ırovsk´y, 2008), and we will not consider them further here. Performance: What performance is acceptable, especially as the data size grows? Do we want to optimize multiple reformulations of a query, for users who iteratively refine a query based on query results? Do we want to optimize certain query types? Are queries performed interactively or in batch mode? Is the treebank stable, or being actively revised, in which case indexes need to be easily updatable? Do we expect logically identical queries to have the same performance, so that users do not have to rewrite their queries for effic"
N10-1034,J93-2004,0,0.034069,"an information retrieval engine. Several experiments with a large treebank demonstrate excellent scaling characteristics for a wide range of query types. This work facilitates the curation of much larger treebanks, and enables them to be used effectively in a variety of scientific and engineering tasks. 1 Introduction The problem of representing and querying linguistic annotations has been an active area of research for several years. Much of the work has grown from efforts to curate large databases of annotated text such as treebanks, for use in developing and testing language technologies (Marcus et al., 1993; Abeill´e, 2003; Hockenmaier and Steedman, 2007). At least a dozen linguistic tree query languages have been developed for interrogating treebanks (see §2). While high quality syntactic parsers are able to efficiently annotate large quantities of English text (Clark and Curran, 2007), existing approaches to query do not work on the same scale. Many existing systems load the entire corpus into memory and check a user-supplied query against every tree. Others avoid the memory limitation, and use relational or XML database systems. Although these have built-in support for indexes, they do not sc"
N10-1034,W06-2704,0,0.0226009,"hich case indexes need to be easily updatable? Do we expect logically identical queries to have the same performance, so that users do not have to rewrite their queries for efficiency? Key performance measures are index size and search times. Architecture: Is the query system standalone, or does it exist in a client-server architecture? Is there a separate user-interface layer that interacts with a data server using a well-defined API, or is it a monolithic system? Should it translate queries into another language, such as SQL (Bird et al., 2006; Nakov et al., 2005), or XQuery (Cassidy, 2002; Mayo et al., 2006), or to automata (Maryns and Kepser, 2009), in order to benefit from the performance optimizations they provide Indexing. The indexing methods used in individual systems are usually not reported. Many systems display nearly constant time for querying a database, regardless of the selectivity of a query, a strong indicator that no indexes are being used. For example, Emu performs all queries in memory with no indexes, and several others are likely to be the same (Cassidy and Harrington, 2001; K¨onig and Lezius, 2001; Heid et al., 2004). TGrep2 (Rohde, 2005) uses a custom corpus file and process"
N10-1034,P08-1005,0,0.0393414,"Missing"
N10-1034,P05-3017,0,0.0241975,"reebank stable, or being actively revised, in which case indexes need to be easily updatable? Do we expect logically identical queries to have the same performance, so that users do not have to rewrite their queries for efficiency? Key performance measures are index size and search times. Architecture: Is the query system standalone, or does it exist in a client-server architecture? Is there a separate user-interface layer that interacts with a data server using a well-defined API, or is it a monolithic system? Should it translate queries into another language, such as SQL (Bird et al., 2006; Nakov et al., 2005), or XQuery (Cassidy, 2002; Mayo et al., 2006), or to automata (Maryns and Kepser, 2009), in order to benefit from the performance optimizations they provide Indexing. The indexing methods used in individual systems are usually not reported. Many systems display nearly constant time for querying a database, regardless of the selectivity of a query, a strong indicator that no indexes are being used. For example, Emu performs all queries in memory with no indexes, and several others are likely to be the same (Cassidy and Harrington, 2001; K¨onig and Lezius, 2001; Heid et al., 2004). TGrep2 (Rohd"
N10-1034,C02-2025,0,0.0935906,"Missing"
N10-1034,P09-4009,0,0.0313974,"Missing"
N10-1034,W04-2708,0,0.0530606,"Missing"
N10-1034,W07-1514,0,0.064914,"Missing"
N10-1034,W09-1411,0,\N,Missing
N16-1109,P10-1010,1,0.84534,"Missing"
N16-1109,C12-2013,1,0.844149,"Missing"
N16-1109,C14-1096,1,0.809603,"Missing"
N16-1109,J93-2003,0,0.0596236,"ement in measured alignment quality. We now give a brief overview of these components. Previous attention. In the basic attentional model, the alignment is calculated based on the source encoding HS and the previous hidden state HTi−1 of the target, αi = Attend(HTi−1 , HS ), where Attend is a function that outputs m attention coefficients. This attention mechanism is overly simplistic, in that it is incapable of capturing patterns in the attention over different positions i. Recognising and exploiting these kinds of patterns has proven critical in traditional word based models of translation (Brown et al., 1993; Vogel et al., 1996; Dyer et al., 2013). For this reason Cohn et al. (2016) include explicit features encoding structural biases from word based models, namely absolute and relative position, Markov conditioning and fertility: 1. previous alignment, αi−1 P 2. sum of previous alignments, i−1 j=1 α j 3. source index vector, (1, 2, 3, . . . , m); and 4. target index vector (i, i, i, . . . , i). These features are concatenated to form a feature matrix β ∈ R4×m , which are added to the alignment calculation, i.e., αi = Attend(HTi−1 , HS , β) . Coverage penalty. The sum over previous alignments fea"
N16-1109,2012.eamt-1.60,0,0.00647996,"that do not require pronunciation lexicons, but train only on speech with text transcriptions (Lee et al., 2013; Maas et al., 2015; Graves et al., 2006). Here, we bypass phonetic transcriptions completely, and rely only on translations. Such data can be found, for example, in subtitled or dubbed movies. Some specific examples of corpora of parallel speech are the European Parliament Plenary Sessions Corpus (Van den Heuvel et al., 2006), which includes parliamentary speeches in the 21 official EU languages, as well as their interpretation into all the other languages; and the TED Talks Corpus (Cettolo et al., 2012), which provides speech in one language (usually English) together with translations into other languages. As mentioned in the introduction, a steppingstone to model parallel speech is to assume a recognizer that can produce a phonetic transcription of the source language, then to model the transformation from transcription to translation. We compare against three previous models that can operate on sequences of phones. The first is simply to run GIZA++ (IBM Model 4) on a phonetic transcription (without word boundaries) of the source side. Stahlberg et al. (2012) present a modification of IBM"
N16-1109,N13-1073,0,0.0289172,"now give a brief overview of these components. Previous attention. In the basic attentional model, the alignment is calculated based on the source encoding HS and the previous hidden state HTi−1 of the target, αi = Attend(HTi−1 , HS ), where Attend is a function that outputs m attention coefficients. This attention mechanism is overly simplistic, in that it is incapable of capturing patterns in the attention over different positions i. Recognising and exploiting these kinds of patterns has proven critical in traditional word based models of translation (Brown et al., 1993; Vogel et al., 1996; Dyer et al., 2013). For this reason Cohn et al. (2016) include explicit features encoding structural biases from word based models, namely absolute and relative position, Markov conditioning and fertility: 1. previous alignment, αi−1 P 2. sum of previous alignments, i−1 j=1 α j 3. source index vector, (1, 2, 3, . . . , m); and 4. target index vector (i, i, i, . . . , i). These features are concatenated to form a feature matrix β ∈ R4×m , which are added to the alignment calculation, i.e., αi = Attend(HTi−1 , HS , β) . Coverage penalty. The sum over previous alignments feature, described above provides a basic f"
N16-1109,P07-2045,0,0.00650321,"Missing"
N16-1109,D13-1019,0,0.00895223,"ity of alignment directly on source-language speech. <s> w1 wi-1 wn Decoder HT Ci Attention 2 wi Background To our knowledge, there has been relatively little research on models that operate directly on parallel speech. Typically, speech is transcribed into a word sequence or lattice using ASR, or at least a phone sequence or lattice using a phone recognizer. This normally requires manually transcribed data and a pronunciation lexicon, which can be costly to create. Recent work has introduced models that do not require pronunciation lexicons, but train only on speech with text transcriptions (Lee et al., 2013; Maas et al., 2015; Graves et al., 2006). Here, we bypass phonetic transcriptions completely, and rely only on translations. Such data can be found, for example, in subtitled or dubbed movies. Some specific examples of corpora of parallel speech are the European Parliament Plenary Sessions Corpus (Van den Heuvel et al., 2006), which includes parliamentary speeches in the 21 official EU languages, as well as their interpretation into all the other languages; and the TED Talks Corpus (Cettolo et al., 2012), which provides speech in one language (usually English) together with translations into"
N16-1109,D15-1166,0,0.176882,"us audio, represented as PLP vectors at 10ms intervals 3 Model We base our approach on the attentional translation model of Cohn et al. (2016), an extension of Bahdanau et al. (2015) which incorporates more fine grained components of the attention mechanism to mimic the structural biases in standard word based translation models. The attentional model encodes a source as a sequence of vectors, then decodes it to generate the output. At each step, it “attends” to different parts of the encoded sequence. This model has been used for translation, image caption generation, and speech recognition (Luong et al., 2015; Xu et al., 2015; Chorowski et al., 2014; Chorowski et al., 2015). Here, we briefly describe the basic attentional model, following Bahdanau et al. (2015), review the extensions for encoding structural biases (Cohn et al., 2016), and then present our novel means for adapting the approach handle parallel speech. 3.1 Base attentional model The model is shown in Figure 1. The speech signal is represented as a sequence of vectors S 1 , S 2 , . . . , S m . For the first set of experiments, each S i is a 128dimensional vector-space embedding of a phone. For the second set of experiments, each S i i"
N16-1109,N15-1038,0,0.0279326,"irectly on source-language speech. <s> w1 wi-1 wn Decoder HT Ci Attention 2 wi Background To our knowledge, there has been relatively little research on models that operate directly on parallel speech. Typically, speech is transcribed into a word sequence or lattice using ASR, or at least a phone sequence or lattice using a phone recognizer. This normally requires manually transcribed data and a pronunciation lexicon, which can be costly to create. Recent work has introduced models that do not require pronunciation lexicons, but train only on speech with text transcriptions (Lee et al., 2013; Maas et al., 2015; Graves et al., 2006). Here, we bypass phonetic transcriptions completely, and rely only on translations. Such data can be found, for example, in subtitled or dubbed movies. Some specific examples of corpora of parallel speech are the European Parliament Plenary Sessions Corpus (Van den Heuvel et al., 2006), which includes parliamentary speeches in the 21 official EU languages, as well as their interpretation into all the other languages; and the TED Talks Corpus (Cettolo et al., 2012), which provides speech in one language (usually English) together with translations into other languages. As"
N16-1109,P11-1064,0,0.00736763,"r languages. As mentioned in the introduction, a steppingstone to model parallel speech is to assume a recognizer that can produce a phonetic transcription of the source language, then to model the transformation from transcription to translation. We compare against three previous models that can operate on sequences of phones. The first is simply to run GIZA++ (IBM Model 4) on a phonetic transcription (without word boundaries) of the source side. Stahlberg et al. (2012) present a modification of IBM Model 3, named Model 3P, designed specifically for phone-to-word alignment. Finally, pialign (Neubig et al., 2011), an unsupervised model for joint phrase alignment and extraction, has been shown to work well at the character level (Neubig et al., 2012) and extends naturally to work on phones. 950 Encoder HS Representation S1 S2 S3 Sm Speech Signal Figure 1: The attentional model as applied to our tasks. We consider two types of input: discrete phone input, or continuous audio, represented as PLP vectors at 10ms intervals 3 Model We base our approach on the attentional translation model of Cohn et al. (2016), an extension of Bahdanau et al. (2015) which incorporates more fine grained components of the att"
N16-1109,P12-1018,0,0.0352532,"ic transcription of the source language, then to model the transformation from transcription to translation. We compare against three previous models that can operate on sequences of phones. The first is simply to run GIZA++ (IBM Model 4) on a phonetic transcription (without word boundaries) of the source side. Stahlberg et al. (2012) present a modification of IBM Model 3, named Model 3P, designed specifically for phone-to-word alignment. Finally, pialign (Neubig et al., 2011), an unsupervised model for joint phrase alignment and extraction, has been shown to work well at the character level (Neubig et al., 2012) and extends naturally to work on phones. 950 Encoder HS Representation S1 S2 S3 Sm Speech Signal Figure 1: The attentional model as applied to our tasks. We consider two types of input: discrete phone input, or continuous audio, represented as PLP vectors at 10ms intervals 3 Model We base our approach on the attentional translation model of Cohn et al. (2016), an extension of Bahdanau et al. (2015) which incorporates more fine grained components of the attention mechanism to mimic the structural biases in standard word based translation models. The attentional model encodes a source as a sequ"
N16-1109,P00-1056,0,0.0780057,"truction of the “silver” standard for evaluation, described below). We also use the English translations produced by Post et al. (2013). We treat the Spanish speech as a sequence of 39dimensional PLP vectors (order 12 with energy and first and second order delta) encoding the power spectrum of the speech signal. We do not have gold standard alignments between the Spanish speech and English words for evaluation, so we produced “silver” standard alignments. We used a forced aligner (Gorman et al., 2011) to align the speech to its transcription, and GIZA++ with the gdfa symmetrization heuristic (Och and Ney, 2000) to align the Spanish transcription to the English translation. We then combined the two alignments to produce “silver” standard alignments between the Spanish speech and the English words. Cleaning and splitting the data based on dialogue turns, resulted in a set of 17,532 Spanish utterances from which we selected 250 for development and 500 testing. For each utterance we have the corresponding English translation, and for each word in the translation we have the corresponding span of Spanish speech. The forced aligner produces the phonetic sequences that correspond to each utterance, which w"
N16-1109,2013.iwslt-papers.14,0,0.326579,"nment smoothing Note that when T = 1 we recover the standard softmax function; we set T = 10 in both experiments. 5 Experimental Setup We work on the Spanish CALLHOME Corpus (LDC96S35), which consists of telephone conversations between Spanish native speakers based in the US and their relatives abroad. While Spanish is not a low-resource language, we pretend that it is by not using any Spanish ASR or resources like transcribed speech or pronunciation lexicons (except in the construction of the “silver” standard for evaluation, described below). We also use the English translations produced by Post et al. (2013). We treat the Spanish speech as a sequence of 39dimensional PLP vectors (order 12 with energy and first and second order delta) encoding the power spectrum of the speech signal. We do not have gold standard alignments between the Spanish speech and English words for evaluation, so we produced “silver” standard alignments. We used a forced aligner (Gorman et al., 2011) to align the speech to its transcription, and GIZA++ with the gdfa symmetrization heuristic (Och and Ney, 2000) to align the Spanish transcription to the English translation. We then combined the two alignments to produce “silve"
N16-1109,van-den-heuvel-etal-2006-tc,0,0.0903295,"Missing"
N16-1109,C96-2141,0,0.222851,"ignment quality. We now give a brief overview of these components. Previous attention. In the basic attentional model, the alignment is calculated based on the source encoding HS and the previous hidden state HTi−1 of the target, αi = Attend(HTi−1 , HS ), where Attend is a function that outputs m attention coefficients. This attention mechanism is overly simplistic, in that it is incapable of capturing patterns in the attention over different positions i. Recognising and exploiting these kinds of patterns has proven critical in traditional word based models of translation (Brown et al., 1993; Vogel et al., 1996; Dyer et al., 2013). For this reason Cohn et al. (2016) include explicit features encoding structural biases from word based models, namely absolute and relative position, Markov conditioning and fertility: 1. previous alignment, αi−1 P 2. sum of previous alignments, i−1 j=1 α j 3. source index vector, (1, 2, 3, . . . , m); and 4. target index vector (i, i, i, . . . , i). These features are concatenated to form a feature matrix β ∈ R4×m , which are added to the alignment calculation, i.e., αi = Attend(HTi−1 , HS , β) . Coverage penalty. The sum over previous alignments feature, described abov"
P06-4018,W05-0101,0,0.0845092,"ng is done first, we lose information required for tagging. If tagging is done first, the stemming must be able to skip over the tags. If both are done independently, we need to be able to align the results. As task combinations multiply, managing the data becomes extremely difficult. To address this problem, NLTK 1.4 introduced a blackboard architecture for tokens, unifying many data types, and permitting distinct tasks to be run independently. Unfortunately this architecture also came with a significant overhead for programmers, who were often forced to use “rather awkward code structures” (Hearst, 2005). It was clear that the re-engineering done in NLTK 1.4 unduly complicated the programmer’s task. This paper presents a brief overview and tutorial on a new, simplified toolkit, and describes how it is used in teaching. The Natural Language Toolkit is a suite of program modules, data sets and tutorials supporting research and teaching in computational linguistics and natural language processing. NLTK is written in Python and distributed under the GPL open source license. Over the past year the toolkit has been rewritten, simplifying many linguistic data structures and taking advantage of recen"
P06-4018,baldridge-etal-2002-leo,0,\N,Missing
P06-4018,W05-0111,0,\N,Missing
P06-4018,W02-0108,0,\N,Missing
P06-4018,W02-0109,1,\N,Missing
P10-1010,D07-1005,0,0.010531,"to identify key resources where negotiation with the original data provider, and where payment of all preparation costs plus compensation for lost revenue, leads to new material for the Corpus. This is a new publication model and a new business model, but it can co-exist with the existing models. CL Research. All manual annotation steps need to be automated. Each step presents a challenging semi-supervised learning and cross-linguistic bootstrapping problem. In addition, the overall measure of success—induction of machine translation systems from limited resources—pushes the state of the art (Kumar et al., 2007). Numerous other CL problems arise: active learning to improve the quality of alignments and bilingual lexicons; automatic language identification for lowdensity languages; and morphology learning. Language archives. Language archives have a special role to play as holders of unique materials. They could contribute existing data in its native format, for other participants to process. They could give bilingual texts a distinct status within their collections, to facilitate discovery. Tool builders. We need tools for annotation, format conversion, spidering and language identification, search,"
P10-1010,W06-0605,0,0.134369,"Missing"
P10-1010,cieri-etal-2010-road,0,0.0178643,"a total of 85,000 items—are already documented in the combined catalog of the Open Language Archives Community,3 so there is no need to recreate this information. Other resources can be logged by community members using a public access wiki, with a metadata template to ensure key fields are elicited such as resource owner, license, ISO 639 language code(s), and data type. This information can itself be curated and stored in the form of an OLAC archive, to permit search over the union of the existing and newly documented items. Work along these lines has already been initiated by LDC and ELRA (Cieri et al., 2010). Corpus readers. Software developers will inspect the file formats and identify high priority formats based on information about resource priorities and sizes. They will code a corpus reader, an open source reference implementation for converting between corpus formats and the storage model presented in section 3. Resource classification. Editors with knowledge of particular language families will categorize documented resources relative to the needs of the project, using controlled vocabularies. This involves examining a resource, determining the granularity and provenance of the segmentatio"
P10-1010,P02-1022,0,0.0412089,"sting infrastructure, and the voluntary effort of interested members of the language resources community. One possibility is to found a “Language Commons,” an open access repository of language resources hosted in the Internet Archive, with a lightweight method for community members to contribute data sets. A fully processed and indexed version of selected data can be made accessible via a web services interface to a major cloud storage facility, such as Amazon Web Services. A common query interface could be supported via APIs in multiple NLP toolkits such as NLTK and GATE (Bird et al., 2009; Cunningham et al., 2002), and also in generic frameworks such as UIMA and SOAP, leaving developers to work within their preferred environment. 4.2 Roles The enterprise requires collaboration of many individuals and groups, in a variety of roles. Editors. A critical group are people with sufficient engagement to serve as editors for particular language families, who have access to data or are able to negotiate redistribution rights, and oversee the workflow of transcription, translation, and annotation. 93 Data agencies. The LDC and ELRA have a central role to play, given their track record in obtaining, curating, and"
P10-1010,varadi-etal-2008-clarin,0,0.0634689,"Missing"
P10-1010,N07-1057,0,0.416963,"rg/ 89 allomorphs, reducing the amount of data required for training an MT system. This most-refined target annotation corresponds to the interlinear glossed texts that are the de facto standard of annotation in the documentary linguistics community. We postulate that interlinear glossed text is sufficiently fine-grained to serve our purposes. It invites efforts to enrich it by automatic means: for example, there has been work on parsing the English translations and using the word-by-word glosses to transfer the parse tree to the object language, effectively creating a treebank automatically (Xia and Lewis, 2007). At the same time, we believe that interlinear glossed text is sufficiently simple and well-understood to allow rapid construction of resources, and to make cross-linguistic consistency a realistic goal. Each of these layers—primary text, translations, alignments, and morphological glosses—seems to be an unavoidable piece of the overall solution. The fact that these layers will exist in diminishing quantity is also unavoidable. However, there is an important consequence: the primary texts will be permanently subject to new translation initiatives, which themselves will be subject to new align"
P10-1010,J10-4005,0,0.00766892,"s, the usual focus is on “vertical” processing. Our particular concern, by contrast, is “horizontal” processing that cuts indiscriminately across languages. Hence we require an unusual degree of consistency across languages. The kind of processing we wish to enable is much like the large-scale systematic research that motivated the Human Genome Project. Moreover, we do not aim to collect data merely in the vague hope that it will prove useful. Although we strive for maximum generality, we also propose a specific driving “use case,” namely, machine translation (MT), (Hutchins and Somers, 1992; Koehn, 2010). The corpus provides a testing ground for the development of MT system-construction methods that are dramatically “leaner” in their resource requirements, and which take advantage of cross-linguistic bootstrapping. The large engineering question is how one can turn the size of the task—constructing MT systems for all the world’s languages simultaneously—to one’s advantage, and thereby consume dramatically less data per language. One of the greatest impacts of having the sequence may well be in enabling an entirely new approach to biological research. In the past, researchers studied one or a"
P11-1151,P08-1034,0,0.0118503,"positives by the SVM. Use of ψi as defined above would lead to these being erroneously assigned the maximum score because of their low variance. The minimum-cut approach places instances in either the positive or negative class depending on which side of the cut they fall on. This means that no measure of classification confidence is available. This extra information is useful at the very least to give a human user an idea of how much to trust the classification. A measure of classification 1509 confidence may also be necessary for incorporation into a broader system, e.g., a meta-classifier (Andreevskaia and Bergler, 2008; Li and Zong, 2008). Tuning the α and θ parameters is likely to become a source of inaccuracy in cases where the tuning and test debates have dissimilar link structures. For example, if the tuning debates tend to have fewer, more accurate links the α parameter will be higher. This will not produce good results if the test debates have more frequent, less accurate links. 3.2 Heuristics for Improving Minimum-cut Bansal et al. (2008) offer preliminary work describing additions to the Thomas et al. minimum-cut approach to incorporate “different class” citation classifications. They use post hoc a"
P11-1151,C08-2004,0,0.315566,"st the classification. A measure of classification 1509 confidence may also be necessary for incorporation into a broader system, e.g., a meta-classifier (Andreevskaia and Bergler, 2008; Li and Zong, 2008). Tuning the α and θ parameters is likely to become a source of inaccuracy in cases where the tuning and test debates have dissimilar link structures. For example, if the tuning debates tend to have fewer, more accurate links the α parameter will be higher. This will not produce good results if the test debates have more frequent, less accurate links. 3.2 Heuristics for Improving Minimum-cut Bansal et al. (2008) offer preliminary work describing additions to the Thomas et al. minimum-cut approach to incorporate “different class” citation classifications. They use post hoc adjustments of graph capacities based on simple heuristics. Two of the three approaches they trial appear to offer performance improvements: The SetTo heuristic: This heuristic works through E in order and tries to force Vi and Vj into different classes for every “different class” (dij < 0) citation classifier output where i < j. It does this by altering the four relevant content-only preferences, φi (y), φi (n), φj (y), and φj (n)."
P11-1151,P09-2041,1,0.435288,"r incorporating the outputs of machine learners into collective classification algorithms. Our experimental evaluation shows that the mean-field algorithm obtains the best results for the task, significantly outperforming the benchmark technique. 1 Introduction Supervised document classification is a well-studied task. Research has been performed across many document types with a variety of classification tasks. Examples are topic classification of newswire articles (Yang and Liu, 1999), sentiment classification of movie reviews (Pang et al., 2002), and satire classification of news articles (Burfoot and Baldwin, 2009). This and other work has established the usefulness of document classifiers as stand-alone systems and as components of broader NLP systems. This paper deals with methods relevant to supervised document classification in domains with network structures, where collective classification can yield better performance than approaches that consider documents in isolation. Simply put, a network structure is any set of relationships between documents that can be used to assist the document classification process. Web encyclopedias and scholarly 1506 publications are two examples of document domains w"
P11-1151,U08-1003,0,0.0416228,"only and citation scores. Links are constructed between test instances and a set of k nearest neighbors drawn only from the training set. Restricting the links in this way means the optimization problem is simple. A similarity metric is used to find nearest neighbors. The Pang and Lee method is an instance of implicit link construction, an approach which is beyond the scope of this paper but nevertheless an important area for future research. A similar technique is used in a variation on the Thomas et al. experiment where additional links between speeches are inferred via a similarity metric (Burfoot, 2008). In cases where both citation and similarity links are present, the overall link score is taken as the sum of the two scores. This seems counter-intuitive, given that the two links are unlikely to be independent. In the framework of this research, the approach would be to train a link meta-classifier to take scores from both link classifiers and output an overall link probability. Within NLP, the use of LBP has not been restricted to document classification. Examples of other applications are dependency parsing (Smith and Eisner, 2008) and alignment (Cromires and Kurohashi, 2009). Conditional"
P11-1151,E09-1020,0,0.015401,"ed via a similarity metric (Burfoot, 2008). In cases where both citation and similarity links are present, the overall link score is taken as the sum of the two scores. This seems counter-intuitive, given that the two links are unlikely to be independent. In the framework of this research, the approach would be to train a link meta-classifier to take scores from both link classifiers and output an overall link probability. Within NLP, the use of LBP has not been restricted to document classification. Examples of other applications are dependency parsing (Smith and Eisner, 2008) and alignment (Cromires and Kurohashi, 2009). Conditional random fields (CRFs) are an approach based on Markov random fields that have been popular for segmenting and labeling sequence data (Lafferty et al., 2001). We rejected linear-chain CRFs as a candidate approach for our evaluation on the grounds that the arbitrarily connected graphs used in collective classification can not be fully represented in graphical format, i.e. Majority Content only Minimum-cut Minimum-cut (SetTo(.6)) Minimum-cut (SetTo(.8)) Minimum-cut (SetTo(1)) Minimum-cut (IncBy(.05)) Minimum-cut (IncBy(.15)) Minimum-cut (IncBy(.25)) Iterative-classifier (citation cou"
P11-1151,W09-3305,0,0.0251591,"classifiers as stand-alone systems and as components of broader NLP systems. This paper deals with methods relevant to supervised document classification in domains with network structures, where collective classification can yield better performance than approaches that consider documents in isolation. Simply put, a network structure is any set of relationships between documents that can be used to assist the document classification process. Web encyclopedias and scholarly 1506 publications are two examples of document domains where network structures have been used to assist classification (Gantner and Schmidt-Thieme, 2009; Cao and Gao, 2005). The contribution of this research is in four parts: (1) we introduce an approach that gives better than state of the art performance for collective classification on the ConVote corpus of congressional debate transcripts (Thomas et al., 2006); (2) we provide a comparative overview of collective document classification techniques to assist researchers in choosing an algorithm for collective document classification tasks; (3) we demonstrate effective novel approaches for incorporating the outputs of SVM classifiers into collective classifiers; and (4) we demonstrate effecti"
P11-1151,P08-2065,0,0.0155245,"as defined above would lead to these being erroneously assigned the maximum score because of their low variance. The minimum-cut approach places instances in either the positive or negative class depending on which side of the cut they fall on. This means that no measure of classification confidence is available. This extra information is useful at the very least to give a human user an idea of how much to trust the classification. A measure of classification 1509 confidence may also be necessary for incorporation into a broader system, e.g., a meta-classifier (Andreevskaia and Bergler, 2008; Li and Zong, 2008). Tuning the α and θ parameters is likely to become a source of inaccuracy in cases where the tuning and test debates have dissimilar link structures. For example, if the tuning debates tend to have fewer, more accurate links the α parameter will be higher. This will not produce good results if the test debates have more frequent, less accurate links. 3.2 Heuristics for Improving Minimum-cut Bansal et al. (2008) offer preliminary work describing additions to the Thomas et al. minimum-cut approach to incorporate “different class” citation classifications. They use post hoc adjustments of graph"
P11-1151,P05-1015,0,0.0877281,"expressed. Somasundaran et al. provides another argument for the usefulness of collective classification 1513 (specifically ICA), in this case as applied at a dialogue act level and relying on a complex system of annotations for link information. Somasundaran and Wiebe (2009) propose an unsupervised method for classifying the stance of each contribution to an online debate concerning the merits of competing products. Concessions to other stances are modeled, but there are no overt citations in the data that could be used to induce the network structure required for collective classification. Pang and Lee (2005) use metric labeling to perform multi-class collective classification of movie reviews. Metric labeling is a multi-class equivalent of the minimum-cut technique in which optimization is done over a cost function incorporating content-only and citation scores. Links are constructed between test instances and a set of k nearest neighbors drawn only from the training set. Restricting the links in this way means the optimization problem is simple. A similarity metric is used to find nearest neighbors. The Pang and Lee method is an instance of implicit link construction, an approach which is beyond"
P11-1151,W02-1011,0,0.0212671,"local and global formulations and introduce novel approaches for incorporating the outputs of machine learners into collective classification algorithms. Our experimental evaluation shows that the mean-field algorithm obtains the best results for the task, significantly outperforming the benchmark technique. 1 Introduction Supervised document classification is a well-studied task. Research has been performed across many document types with a variety of classification tasks. Examples are topic classification of newswire articles (Yang and Liu, 1999), sentiment classification of movie reviews (Pang et al., 2002), and satire classification of news articles (Burfoot and Baldwin, 2009). This and other work has established the usefulness of document classifiers as stand-alone systems and as components of broader NLP systems. This paper deals with methods relevant to supervised document classification in domains with network structures, where collective classification can yield better performance than approaches that consider documents in isolation. Simply put, a network structure is any set of relationships between documents that can be used to assist the document classification process. Web encyclopedia"
P11-1151,D08-1016,0,0.016352,"ional links between speeches are inferred via a similarity metric (Burfoot, 2008). In cases where both citation and similarity links are present, the overall link score is taken as the sum of the two scores. This seems counter-intuitive, given that the two links are unlikely to be independent. In the framework of this research, the approach would be to train a link meta-classifier to take scores from both link classifiers and output an overall link probability. Within NLP, the use of LBP has not been restricted to document classification. Examples of other applications are dependency parsing (Smith and Eisner, 2008) and alignment (Cromires and Kurohashi, 2009). Conditional random fields (CRFs) are an approach based on Markov random fields that have been popular for segmenting and labeling sequence data (Lafferty et al., 2001). We rejected linear-chain CRFs as a candidate approach for our evaluation on the grounds that the arbitrarily connected graphs used in collective classification can not be fully represented in graphical format, i.e. Majority Content only Minimum-cut Minimum-cut (SetTo(.6)) Minimum-cut (SetTo(.8)) Minimum-cut (SetTo(1)) Minimum-cut (IncBy(.05)) Minimum-cut (IncBy(.15)) Minimum-cut (I"
P11-1151,P09-1026,0,0.0325159,"Somasundaran et al. (2009) use ICA to improve sentiment polarity classification of dialogue acts in a corpus of multi-party meeting transcripts. Link features are derived from annotations giving frame relations and target relations. Respectively, these relate dialogue acts based on the sentiment expressed and the object towards which the sentiment is expressed. Somasundaran et al. provides another argument for the usefulness of collective classification 1513 (specifically ICA), in this case as applied at a dialogue act level and relying on a complex system of annotations for link information. Somasundaran and Wiebe (2009) propose an unsupervised method for classifying the stance of each contribution to an online debate concerning the merits of competing products. Concessions to other stances are modeled, but there are no overt citations in the data that could be used to induce the network structure required for collective classification. Pang and Lee (2005) use metric labeling to perform multi-class collective classification of movie reviews. Metric labeling is a multi-class equivalent of the minimum-cut technique in which optimization is done over a cost function incorporating content-only and citation scores"
P11-1151,D09-1018,0,0.0128398,"this. They note that the cost of incorrectly classifying a given instance can be magnified in collective classification, because errors are propagated throughout the network. The extent to which this happens may depend on the random interaction between base classification accuracy and network structure. There is scope for further work to more fully explain this phenomenon. From these statistical and theoretical factors we infer that more reliable conclusions can be drawn from collective classification experiments that use cross-validation instead of a single, fixed data split. 5 Related work Somasundaran et al. (2009) use ICA to improve sentiment polarity classification of dialogue acts in a corpus of multi-party meeting transcripts. Link features are derived from annotations giving frame relations and target relations. Respectively, these relate dialogue acts based on the sentiment expressed and the object towards which the sentiment is expressed. Somasundaran et al. provides another argument for the usefulness of collective classification 1513 (specifically ICA), in this case as applied at a dialogue act level and relying on a complex system of annotations for link information. Somasundaran and Wiebe (20"
P11-1151,W06-1639,0,0.72925,"ider documents in isolation. Simply put, a network structure is any set of relationships between documents that can be used to assist the document classification process. Web encyclopedias and scholarly 1506 publications are two examples of document domains where network structures have been used to assist classification (Gantner and Schmidt-Thieme, 2009; Cao and Gao, 2005). The contribution of this research is in four parts: (1) we introduce an approach that gives better than state of the art performance for collective classification on the ConVote corpus of congressional debate transcripts (Thomas et al., 2006); (2) we provide a comparative overview of collective document classification techniques to assist researchers in choosing an algorithm for collective document classification tasks; (3) we demonstrate effective novel approaches for incorporating the outputs of SVM classifiers into collective classifiers; and (4) we demonstrate effective novel feature models for iterative local classification of debate transcript data. In the next section (Section 2) we provide a formal definition of collective classification and describe the ConVote corpus that is the basis for our experimental evaluation. Sub"
P13-2112,P11-2000,0,0.650374,"her language (e.g., Hana et al., 2004; Feldman et al., 2006; Reddy and Sharoff, 2011). Other approaches have simultaneously tagged two languages based on alignments in a parallel corpus (e.g., Snyder et al., 2008). A number of studies have used tag projection to copy tag information from a resource-rich to a resource-poor language, based on word alignments in a parallel corpus. After alignment, the resource-rich language is tagged, and tags are projected from the source language to the target language based on the alignment (e.g., Yarowsky and Ngai, 2001; Das and Petrov, 2011). Das and Petrov (2011) achieved the current state-of-the-art for unsupervised tagging by exploiting high conﬁdence alignments to copy tags from the source language to the target language. Graph-based label propagation was used to automatically produce more labelled training data. First, a graph was constructed in which each vertex corresponds to a unique trigram, and edge weights represent the syntactic similarity between vertices. Labels were then propagated by optimizing a convex function to favor the same tags for closely related nodes Unsupervised part-of-speech tagging Currently, part-of-speech (POS) taggers a"
P13-2112,2005.mtsummit-papers.11,0,0.351921,"it achieves comparable results. 3 We eliminate many-to-one alignments (Step 2). Keeping these would give more POS-tagged tokens for the target side, but also introduce noise. For example, suppose English and French were the source and target language, respectively. In this case alignments such as English laws (NNS) to French les (DT) lois (NNS) would be expected (Yarowsky and Ngai, 2001). However, in Step 3, where tags are projected from the source to target language, this would incorrectly tag French les as NN. We build a French tagger based on English– French data from the Europarl Corpus (Koehn, 2005). We also compare the accuracy and coverage of the tags obtained through direct projection using the French Melt POS tagger (Denis and Sagot, 2009). Table 1 conﬁrms that the one-to-one alignments indeed give higher accuracy but lower coverage than the many-to-one alignments. At this stage of the model we hypothesize that highconﬁdence tags are important, and hence eliminate the many-to-one alignments. In Step 4, in an effort to again obtain higher quality target language tags from direct projection, we eliminate all but the top n sentences based on their alignment scores, as provided by the al"
P13-2112,A00-1031,0,0.840096,"Missing"
P13-2112,N06-1020,0,0.0488021,"stimated emission and transition probabilities into the TNT tagger (Brants, 2000), an implementation of a trigram HMM tagger. 4.2 language word wis is aligned with target language word wjt with probability p(wjt |wis ), Tis is the tag for wis using the tagger available for the source language, and Tjt is the tag for wjt using the tagger learned for the target language. If p(wjt |wis ) &gt; S, where S is a threshold which we heuristically set to 0.7, we replace Tjt by Tis . Self-training can suffer from over-ﬁtting, in which errors in the original model are repeated and ampliﬁed in the new model (McClosky et al., 2006). To avoid this, we remove the tag of any token that the model is uncertain of, i.e., if p(wjt |wis ) &lt; S and Tjt �= Tis then Tjt = Null. So, on the target side, aligned words have a tag from direct projection or no tag, and unaligned words have a tag assigned by our model. Step 4 estimates the emission and transition probabilities as in Algorithm 1. In Step 5, emission probabilities for lexical items in the previous model, but missing from the current model, are added to the current model. Later models therefore take advantage of information from earlier models, and have wider coverage. Self"
P13-2112,P11-1061,0,0.687686,"ne language based on a corpus for another language (e.g., Hana et al., 2004; Feldman et al., 2006; Reddy and Sharoff, 2011). Other approaches have simultaneously tagged two languages based on alignments in a parallel corpus (e.g., Snyder et al., 2008). A number of studies have used tag projection to copy tag information from a resource-rich to a resource-poor language, based on word alignments in a parallel corpus. After alignment, the resource-rich language is tagged, and tags are projected from the source language to the target language based on the alignment (e.g., Yarowsky and Ngai, 2001; Das and Petrov, 2011). Das and Petrov (2011) achieved the current state-of-the-art for unsupervised tagging by exploiting high conﬁdence alignments to copy tags from the source language to the target language. Graph-based label propagation was used to automatically produce more labelled training data. First, a graph was constructed in which each vertex corresponds to a unique trigram, and edge weights represent the syntactic similarity between vertices. Labels were then propagated by optimizing a convex function to favor the same tags for closely related nodes Unsupervised part-of-speech tagging Currently, part-of"
P13-2112,D08-1109,0,0.0462258,"(Das and Petrov, 2011), but is substantially less-sophisticated (speciﬁcally not requiring convex optimization or a feature-based HMM). The complexity of our algorithm is O(nlogn) compared to O(n2 ) for that of Das and Petrov 637 (2011) where n is the size of training data.3 We made our code are available for download.4 In future work we intend to consider using a larger training corpus to reduce the proportion of unknown tokens and improve accuracy. Given the improvements of our model over that of Das and Petrov on languages from the same family as our source language, and the observation of Snyder et al. (2008) that a better tagger can be learned from a more-closely related language, we also plan to consider strategies for selecting an appropriate source language for a given target language. Using our ﬁnal model with unsupervised HMM methods might improve the ﬁnal performance too, i.e. use our ﬁnal model as the initial state for HMM, then experiment with different inference algorithms such as Expectation Maximization (EM), Variational Bayers (VB) or Gibbs sampling (GS).5 Gao and Johnson (2008) compare EM, VB and GS for unsupervised English POS tagging. In many cases, GS outperformed other methods, t"
P13-2112,N03-1033,0,0.239791,"Missing"
P13-2112,N01-1026,0,0.781294,"s for an HMM tagger for one language based on a corpus for another language (e.g., Hana et al., 2004; Feldman et al., 2006; Reddy and Sharoff, 2011). Other approaches have simultaneously tagged two languages based on alignments in a parallel corpus (e.g., Snyder et al., 2008). A number of studies have used tag projection to copy tag information from a resource-rich to a resource-poor language, based on word alignments in a parallel corpus. After alignment, the resource-rich language is tagged, and tags are projected from the source language to the target language based on the alignment (e.g., Yarowsky and Ngai, 2001; Das and Petrov, 2011). Das and Petrov (2011) achieved the current state-of-the-art for unsupervised tagging by exploiting high conﬁdence alignments to copy tags from the source language to the target language. Graph-based label propagation was used to automatically produce more labelled training data. First, a graph was constructed in which each vertex corresponds to a unique trigram, and edge weights represent the syntactic similarity between vertices. Labels were then propagated by optimizing a convex function to favor the same tags for closely related nodes Unsupervised part-of-speech tag"
P13-2112,Y09-1013,0,\N,Missing
P13-2112,petrov-etal-2012-universal,0,\N,Missing
P13-2112,feldman-etal-2006-cross,0,\N,Missing
P13-2112,W04-3229,0,\N,Missing
P13-2112,D08-1036,0,\N,Missing
P15-2139,D14-1096,1,0.401152,"Epos , Earc , W1 and W2 can be shared as indicated with dashed lines. In particular we expect this to be the case when languages use the same POS tagset and arc label sets, as we presume herein. This assumption is motivated by the development of unified annotation for many languages (Nivre et al., 2015; Petrov et al., 2012; McDonald et al., 2013). To allow parameter sharing between languages we could jointly train the parser on the source and target language simultaneously. However, we leave this for future work. Here we take an alternative approach, namely regularization in a similar vein to Duong et al. (2014). First we train a lexicalized neural network parser on the source resource-rich language (English), as described in Section 2. The learned parameters are en , E en , E en , W en , W en . Second, we incorEword pos arc 1 2 porate English parameters as a prior for the target language training. This is straightforward when we use the same architecture, such as a neural network parser, for the target language. All we need to do is modify the learning objective function so that it includes the regularization part. However, we don’t want to regularize the en part related to Eword since it will be ve"
P15-2139,2005.mtsummit-papers.11,0,0.0269162,"h language in the collection. Some languages have over 400k tokens such as cs, fr and es, meanwhile, hu and ga have only around 25k tokens. 4.2 Table 1: Number of tokens (× 1,000) for each language in the Universal Dependency Treebank collection. We initialize the target language word embeddings Eword of our neural network cross-lingual model with pre-trained embeddings. This is an advantage since we can incorporate monolingual data which is usually available for resource-poor languages. We collect monolingual data for each language from the Machine Translation Workshop (WMT) data,5 Europarl (Koehn, 2005) and EU Bookshop Corpus (Skadin¸sˇ et al., 2014). The size of monolingual data also varies significantly. There are languages such as English and German with more than 400 million words, whereas, Irish only has 4 million. We use the skip-gram model from word2vec to induce 50-dimension word embeddings (Mikolov et al., 2013). tagset and arc label annotation for the source and target language. The same POS tagset is required so that the source language parser has similar structure with the target language parser. The requirement of same arc label annotation is mainly needed for evaluation using t"
P15-2139,P14-1126,0,0.0386164,"Missing"
P15-2139,P05-1012,0,0.018181,"uide the learning process. Our model saves at least half of the annotation effort to reach the same accuracy compared with using the purely supervised method. 1 Introduction Dependency parsing is a crucial component of many natural language processing systems, for ¨ ur and tasks such as text classification (Ozg¨ G¨ung¨or, 2010), statistical machine translation (Xu et al., 2009), relation extraction (Bunescu and Mooney, 2005), and question answering (Cui et al., 2005). Supervised approaches to dependency parsing have been successful for languages where relatively large treebanks are available (McDonald et al., 2005). However, for many languages, annotated treebanks are not available. They are costly to create, requiring careful design, testing and subsequent refinement of annotation guidelines, along with assessment and management of annotator quality (B¨ohmov´a et al., 2001). The Universal Treebank Annotation Guidelines aim at providing unified annotation for many languages enabling cross-lingual comparison (Nivre et al., 2015). This project provides a starting point for developing a treebank for resource-poor languages. However, a mature parser requires a large treebank for training, and this is still"
P15-2139,N09-1028,0,0.0172191,"costly and time-consuming to build. We propose a learning method that needs less data, based on the observation that there are underlying shared structures across languages. We exploit cues from a different source language in order to guide the learning process. Our model saves at least half of the annotation effort to reach the same accuracy compared with using the purely supervised method. 1 Introduction Dependency parsing is a crucial component of many natural language processing systems, for ¨ ur and tasks such as text classification (Ozg¨ G¨ung¨or, 2010), statistical machine translation (Xu et al., 2009), relation extraction (Bunescu and Mooney, 2005), and question answering (Cui et al., 2005). Supervised approaches to dependency parsing have been successful for languages where relatively large treebanks are available (McDonald et al., 2005). However, for many languages, annotated treebanks are not available. They are costly to create, requiring careful design, testing and subsequent refinement of annotation guidelines, along with assessment and management of annotator quality (B¨ohmov´a et al., 2001). The Universal Treebank Annotation Guidelines aim at providing unified annotation for many l"
P15-2139,D11-1006,0,0.0235477,"Missing"
P15-2139,P13-2017,0,0.0373468,"Missing"
P15-2139,I08-3008,0,0.0233446,"Missing"
P15-2139,petrov-etal-2012-universal,0,0.0150699,". 3 Cross-lingual parser Our model takes advantage of underlying structure shared between languages. Given the source language parsing structure as in Figure 1 (left), the set of parameters Eword will be different for the target language parser shown in Figure 1 (right) but we hypothesize that Epos , Earc , W1 and W2 can be shared as indicated with dashed lines. In particular we expect this to be the case when languages use the same POS tagset and arc label sets, as we presume herein. This assumption is motivated by the development of unified annotation for many languages (Nivre et al., 2015; Petrov et al., 2012; McDonald et al., 2013). To allow parameter sharing between languages we could jointly train the parser on the source and target language simultaneously. However, we leave this for future work. Here we take an alternative approach, namely regularization in a similar vein to Duong et al. (2014). First we train a lexicalized neural network parser on the source resource-rich language (English), as described in Section 2. The learned parameters are en , E en , E en , W en , W en . Second, we incorEword pos arc 1 2 porate English parameters as a prior for the target language training. This is stra"
P15-2139,skadins-etal-2014-billions,0,0.024592,"Missing"
P15-2139,N13-1126,0,0.0257058,"Missing"
P15-2139,D14-1082,0,\N,Missing
P15-2139,H05-1091,0,\N,Missing
S15-1012,P10-2047,0,0.0135158,"t into a collective classification algorithm. Dual Classifier Approach The dual classifier approach is made up of three steps, as depicted in Figure 1: 1. Base classification: Produce base classifications using (1) a content-only classifier; and (2) a relationship classifier. The contentonly classifier makes a binary prediction: F OR 109 5.1.1 Base classification For our content-only base classifier, we use the same bag-of-words SVM with binary (existencebased) unigram features as (Thomas et al., 2006). This classifier has been shown to be the best bagof-words model for B ITTERLEMONS (Beigman Klebanov et al., 2010). As our relationship base classifier, we use the cosine similarity scores described above, calculated using n-grams of several different lengths. 5.1.2 Normalisation We use probabilistic SVM normalisation to convert the signed decision-plane distance output by the content-only classifier into the probability that the instance is in the positive class (Platt, 1999). For the relationship classifier, the technique used to convert the cosine similarity score into a classification preference needs to fit complex criteria. Preliminary experiments suggested that while the very highest similarity sco"
S15-1012,P11-1151,1,0.936546,"within joint models for document-level semantic classification, focusing specifically on tasks which are not associated with any explicit graph structure. That is, we examine whether implicit semantic document links can improve the results of a point-wise (content-based) classification approach. Explicit inter-document links have been variously shown to improve document classifier performance, based on information sources including hyperlinks in web documents (Slattery and Craven, 1998; Oh et al., 2000; Yang et al., 2002), direct name-references in congressional debates (Thomas et al., 2006; Burfoot et al., 2011; Stoyanov and Eisner, 2012), citations in scientific papers (Giles et al., 1998; Lu and Getoor, 2003; McDowell et al., 2007), and user mentions or retweets in social media (Jiang et al., 2011; Tan et al., 2011). However, document collections often don’t contain explicit inter-document links, limiting the practical usefulness of such methods. In this paper, we seek to expand the reach of research which incorporates linking information, in inducing implicit linking information between documents, and demonstrating that the resultant (noisy) network structure improves document classification accu"
S15-1012,W03-1022,0,0.0604172,"en proposed, although in natural language processing at least, these have focused largely on conditional dependence, in the form of models such as 1 In some tasks, it can also indicate heterophily, i.e. the tendency for connected instances to have contrasting properties, as we shall see for one of our two dataset. hidden Markov models (Rabiner and Juang, 1986) and conditional random fields (Lafferty et al., 2001), where independent properties of words, e.g., are combined with conditional dependencies based on their context of use to jointly predict the senses of all words in a given sentence (Ciaramita and Johnson, 2003; Johannsen et al., 2014). This paper explores the utility of homophily within joint models for document-level semantic classification, focusing specifically on tasks which are not associated with any explicit graph structure. That is, we examine whether implicit semantic document links can improve the results of a point-wise (content-based) classification approach. Explicit inter-document links have been variously shown to improve document classifier performance, based on information sources including hyperlinks in web documents (Slattery and Craven, 1998; Oh et al., 2000; Yang et al., 2002),"
S15-1012,P11-1016,0,0.0417604,"emantic document links can improve the results of a point-wise (content-based) classification approach. Explicit inter-document links have been variously shown to improve document classifier performance, based on information sources including hyperlinks in web documents (Slattery and Craven, 1998; Oh et al., 2000; Yang et al., 2002), direct name-references in congressional debates (Thomas et al., 2006; Burfoot et al., 2011; Stoyanov and Eisner, 2012), citations in scientific papers (Giles et al., 1998; Lu and Getoor, 2003; McDowell et al., 2007), and user mentions or retweets in social media (Jiang et al., 2011; Tan et al., 2011). However, document collections often don’t contain explicit inter-document links, limiting the practical usefulness of such methods. In this paper, we seek to expand the reach of research which incorporates linking information, in inducing implicit linking information between documents, and demonstrating that the resultant (noisy) network structure improves document classification accuracy. The intuition underlying this work is that some types of documents have features which are either absent or ambiguous in training data, but which have 106 Proceedings of the Fourth Joint"
S15-1012,S14-1001,0,0.0313871,"ral language processing at least, these have focused largely on conditional dependence, in the form of models such as 1 In some tasks, it can also indicate heterophily, i.e. the tendency for connected instances to have contrasting properties, as we shall see for one of our two dataset. hidden Markov models (Rabiner and Juang, 1986) and conditional random fields (Lafferty et al., 2001), where independent properties of words, e.g., are combined with conditional dependencies based on their context of use to jointly predict the senses of all words in a given sentence (Ciaramita and Johnson, 2003; Johannsen et al., 2014). This paper explores the utility of homophily within joint models for document-level semantic classification, focusing specifically on tasks which are not associated with any explicit graph structure. That is, we examine whether implicit semantic document links can improve the results of a point-wise (content-based) classification approach. Explicit inter-document links have been variously shown to improve document classifier performance, based on information sources including hyperlinks in web documents (Slattery and Craven, 1998; Oh et al., 2000; Yang et al., 2002), direct name-references i"
S15-1012,W06-2915,0,0.039672,"to allow for a more statistically robust evaluation; and (3) we discard the manually annotated inter-document relationships based on references to speaker names, because implicit relationships are the focus of this work. Table 1 gives statistics for our rendering of C ON VOTE. The identical figures for the average number of speeches and speakers per debate reflect the fact that each speaker now contributes only one unified speech. cused on the content of the contributions rather than stylistic or biographical features that may identify one editor or the other. 3.2 B ITTERLEMONS B ITTERLEMONS (Lin et al., 2006) is a collection of articles on the Israeli–Arab conflict harvested from the Bitterlemons website.2 In each weekly issue, the editors contribute an article giving their perspectives on some aspect of the conflict, and two guest authors contribute articles, one from an Israeli perspective and the other from a Palestinian perspective. Sometimes these guest contributions take the form of an interview, in which case we remove the questions (from the editors) and retain only the answers. The statistics in Table 2 give a picture of the size and structure of B ITTERLEMONS. In accordance with Lin et a"
S15-1012,P05-1015,0,0.214269,"Missing"
S15-1012,W09-3210,0,0.0221556,"ifier approach has generally been found to perform best (Thomas et al., 2006; Burfoot et al., 2011). While the work presented here is conceptually quite simple, the findings are significant and potentially open the door to accuracy improvements on a range of document-level semantic tasks. 2 Related Work Previous work has dealt with the question of collective document classification using implicit interdocument relationships in two basic ways: 1. proximity: use a spatial or temporal dimension of the domain to relate documents (Agrawal et al., 2003; Goldberg et al., 2007; McDowell et al., 2009; Somasundaran et al., 2009). 2. similarity: relate documents via some notion of their content-based similarity (Blum and Chawla, 2001; Joachims, 2003; Takamura et al., 2007; Sindhwani and Melville, 2008; Jurgens, 2013) The work using similarity-based links is the closest to ours but is also strongly differentiated because 107 it focuses on transductive semi-supervised classification. That task begins with the premise that only a small amount of labelled training data is available, so content-only classification is likely to be inaccurate. By contrast, the supervised techniques in this paper deal with large amounts of la"
S15-1012,N12-1013,0,0.0935086,"or document-level semantic classification, focusing specifically on tasks which are not associated with any explicit graph structure. That is, we examine whether implicit semantic document links can improve the results of a point-wise (content-based) classification approach. Explicit inter-document links have been variously shown to improve document classifier performance, based on information sources including hyperlinks in web documents (Slattery and Craven, 1998; Oh et al., 2000; Yang et al., 2002), direct name-references in congressional debates (Thomas et al., 2006; Burfoot et al., 2011; Stoyanov and Eisner, 2012), citations in scientific papers (Giles et al., 1998; Lu and Getoor, 2003; McDowell et al., 2007), and user mentions or retweets in social media (Jiang et al., 2011; Tan et al., 2011). However, document collections often don’t contain explicit inter-document links, limiting the practical usefulness of such methods. In this paper, we seek to expand the reach of research which incorporates linking information, in inducing implicit linking information between documents, and demonstrating that the resultant (noisy) network structure improves document classification accuracy. The intuition underlyi"
S15-1012,N07-1037,0,0.0826646,"Missing"
S15-1012,W06-1639,0,0.903224,"utility of homophily within joint models for document-level semantic classification, focusing specifically on tasks which are not associated with any explicit graph structure. That is, we examine whether implicit semantic document links can improve the results of a point-wise (content-based) classification approach. Explicit inter-document links have been variously shown to improve document classifier performance, based on information sources including hyperlinks in web documents (Slattery and Craven, 1998; Oh et al., 2000; Yang et al., 2002), direct name-references in congressional debates (Thomas et al., 2006; Burfoot et al., 2011; Stoyanov and Eisner, 2012), citations in scientific papers (Giles et al., 1998; Lu and Getoor, 2003; McDowell et al., 2007), and user mentions or retweets in social media (Jiang et al., 2011; Tan et al., 2011). However, document collections often don’t contain explicit inter-document links, limiting the practical usefulness of such methods. In this paper, we seek to expand the reach of research which incorporates linking information, in inducing implicit linking information between documents, and demonstrating that the resultant (noisy) network structure improves docume"
U04-1017,W97-1105,1,0.879106,"Missing"
U04-1017,J01-1003,0,0.0267785,"gms, nor do they permit paradigms to be saved in a format which permits reuse. Outside purely linguistic description, work on computational morphology usually requires paradigms to be set up. For instance, Finnish and Romanian have such a large number of productive morphological processes it is impractical to list every form in the lexicon. Instead, regular derivational and inflectional processes are described using a formal system (such as a finite-state transducer). Groups of processes which apply to the same class of lexical items are sometimes referred to as a paradigm (e.g. (Tufis, 1989; Oflazer et al., 2001)). Unlike the descriptive viewpoint, in which a paradigm is a tabulation, here a paradigm is effectively treated as executable code which might be used to generate such tabulations. However, we are neutral on this issue since both viewpoints can be reconciled by treating a paradigm as a relation, as we do in §3. 3 Data Model Linguistic paradigms associate linguistic forms with linguistic categories. For instance, the German definite article paradigm in Figure 1 categorises the form den as either masculine singular accusative or as dative plural. Systematic changes in layout, such as interchang"
U04-1017,E89-1020,0,0.0956463,"itrary paradigms, nor do they permit paradigms to be saved in a format which permits reuse. Outside purely linguistic description, work on computational morphology usually requires paradigms to be set up. For instance, Finnish and Romanian have such a large number of productive morphological processes it is impractical to list every form in the lexicon. Instead, regular derivational and inflectional processes are described using a formal system (such as a finite-state transducer). Groups of processes which apply to the same class of lexical items are sometimes referred to as a paradigm (e.g. (Tufis, 1989; Oflazer et al., 2001)). Unlike the descriptive viewpoint, in which a paradigm is a tabulation, here a paradigm is effectively treated as executable code which might be used to generate such tabulations. However, we are neutral on this issue since both viewpoints can be reconciled by treating a paradigm as a relation, as we do in §3. 3 Data Model Linguistic paradigms associate linguistic forms with linguistic categories. For instance, the German definite article paradigm in Figure 1 categorises the form den as either masculine singular accusative or as dative plural. Systematic changes in lay"
U04-1019,cotton-bird-2002-integrated,1,0.79996,"tree editing operations. However, linguistic trees are more constrained than general trees. Freedom of movement of constituents almost always depends on preserving the base text. Subtree deletion is not allowed (except zero-width elements) nor is re-ordering of leaves. Any subtree can only legally move to a limited number of locations without perturbing the text. Subtree movement can be described in terms of node insertion and deletion. However, this will be extremely tedious for the user to specify as subtrees may be extremely large. Thus subtree movement should appear as a basic operation. (Cotton and Bird, 2002) present a tree edit operations all in terms of node movement of a distinguished node. The direction and surrounding structure determines where the node is reattached. Further operations are required to deal correctly with empty constituents. All update operations should have inverses so edits can be reversed. Syntactically annotated corpora are often annotated with respect to a particular grammar. These grammars may be updated and annotations need to be changed to reflect this. However, it is inefficient to reannotate the entire corpus every time this happens. A useful update mechanism should"
U04-1019,heid-etal-2004-querying,0,0.0610143,"yntax) ($v word) ($np syntax) ($pp syntax): ($v@pos==""V"") && ($np@cat==""NP"") && ($pp@cat==""PP"") && ($vp@cat==""VP"") && ($v <>1 $np) && ($np <>1 $pp) && ($vp ˆ $v) && ($vp ˆ $np) && ($vp ˆ $pp) Q5.*($vp syntax) ($np syntax) ($x syntax): ($vp@cat==""VP"") && ($np@cat==""NP"") && ($x ˆ $vp) && ($x ˆ $np) && ($np <> $vp) Q6. ($s syntax) ($i intermediate) ($w word): ($s@cat==""NP"") && ($i@tone==""L-"") && ($w@orth==""dark"") && ($s ˆ $w) && ($i ˆ $w) Q7. (exists $vp syntax) ($np syntax): ($vp@cat==""VP"") && ($np@cat==""NP"") && ($vp ˆ $np) Figure 7: NiteQL Queries been released as part of the NITE XML Toolkit (Heid et al., 2004). Queries consist of weakly typed variable declarations followed by match conditions. Matches are evaluated over attribute, structure and time constraints. Precedence can be defined by the application designer depending on the data model. The queries in Figure 7 assume the model used by Tgrep2. If crossing branches are permitted, and the left corners precedence definition is used, then NiteQL will behave like TIGERSearch instead. Dominance and precedence relations can take modifiers that provide more positional constraints. In Q3, ˆ1 indicates immediate dominance and [-1] indicates rightmost d"
U04-1019,H94-1020,0,0.0128036,"c fields. For example, Q6 requires both syntactic and phonological data. This can be represented as two trees that intersect at the word level (Cassidy and Harrington, 2001). Finally, trees can be large, and it is often undesirable to return whole trees for which the query tree matches a tiny fragment. Thus, we need a way to specify what part of a query simply constrains context, and what part should be returned. Q7 is a simple test of a query language’s ability to control output. 2.1 Penn Treebank and Tgrep2 Penn Treebank contains approximately 50,000 parse trees of Wall Street Journal text (Marcus et al., 1994). Each parse is represented as an ordered tree, and syntactic dependencies are indicated using Q1. Q2. Q3. Q4 Q5. S << saw S !<< saw NP <- N VP=vp << (V . (N >> =vp . PP >> =vp)) *=p << (NP=n .. (VP=v >> =p !>> (* << =n >> =p))) Q6.* Not expressible Q7. VP << ‘NP Figure 3: Tgrep2 Queries; Q5 is taken from (Rohde, 2001) zero-width trace elements co-indexed with a full noun phrase (cf. Figure 2). Tgrep2 is a grep-like tool for this database (Rohde, 2001), and all example queries can be specified correctly except Q6 (cf. Figure 3). Queries are nested expressions involving nodes and relationships"
U04-1019,W04-0212,0,0.0225196,"grammar (Chomsky, 1963) or syntactic charts, as shown in Figure 11. 3.2 Closures The surveyed languages include closures of basic relations such as dominance, precedence and sibling precedence. These are necessary as distance between nodes of interest can be arbitrarily large. These are generally represented as separate relations in tree querying languages. However, closures are required of more complicated structures that are not handled currently. 3.3 Beyond ordered trees Queries may need to extend beyond sentence boundaries. For example, anaphoric arguments may occur in previous sentences (Prasad et al., 2004). If trees represent sentences and querying is restricted to subtree matching this is a problem. One solution is to include multiple sentences in trees. However, this drastically increases the size of trees. Query trees are generally very small (if spread widely) so massive trees decrease filter effectiveness during query processing and have a bad effect on matching algorithms. This presents a good case for querying over ordered forests. In fact this is necessary when querying the Verbmobil treebanks of spontaneous speech (Hinrichs et al., 2000). Here discourse turns are modelled to include re"
U04-1019,steiner-kallmeyer-2002-viqtorya,0,0.0784965,"s are generally very small (if spread widely) so massive trees decrease filter effectiveness during query processing and have a bad effect on matching algorithms. This presents a good case for querying over ordered forests. In fact this is necessary when querying the Verbmobil treebanks of spontaneous speech (Hinrichs et al., 2000). Here discourse turns are modelled to include repetitions, interjections, disfluencies and sentence fragments. These are represented as trees disconnected from surrounding well-formed sentences. Trees can occur wrapped in other trees as seen in Figure 12. VIQTORYA (Steiner and Kallmeyer, 2002) is a query language developed for these treebanks. However, this can be considered a subset of the TIGERSearch language so was not discussed in the survey. Figure 12: Forest representation of the Verbmobil corpus (Steiner and Kallmeyer, 2002) There is a general need to move beyond single tree searches and integrate different types of linguistic data. Querying intersecting hierarchies has been well motivated by the workbenches such as Emu and the NITE project. There is also a need to query over relational and structural data. (e.g. Switchboard Treebank). We may want to match subtrees depending"
U05-1018,C00-2093,0,\N,Missing
U19-1001,L18-1416,0,0.339428,"which we used to produce a data set to support the development of the FST. Additionally, we glossed a small set of 114 verbs randomly sampled from the Kunwinjku translation of the Bible, for the purpose of judging how well the FST generalizes to another domain. The Bible translation was recently completed in 2018, and targets the modern vernacular. We use accuracy and coverage to measure the effectiveness of the model on the development data set as well as the test set. 4 Implementation Finite state transducers are viewed as an ideal framework to model morphology (Beesley and Karttunen, 2003; Chen and Schwartz, 2018; Lachler et al., 2018). Our FST was implemented using the Foma toolkit (Hulden, 2009) which is a popular framework for building morphological analyzers for polysynthetic languages (Chen and Schwartz, 2018; Moeller et al., 2018; Littell, 2018). The definition of an FST in Foma is comprised of a lexicon implemented in the .lexc format, and a .foma file for defining rules covering regular morphophonemic changes. The final FST is produced by composing the FSTs defined in both files. 4.1 The .lexc file The .lexc file contains definitions of lexicon groups corresponding to morphological units of th"
U19-1001,W18-5807,0,0.0174017,"cation can have more than one consonant (C) and vowel (V) reduplicative pattern, depending on which of the 11 verb form paradigms the verb belongs to. See Figure 2 for details. Computational modeling of partial reduplication in human language using finite state transducers (FSTs) has been addressed in the past (Culy, 1985; Roark et al., 2007; Dras et al., 2012), with the general consensus being that these kinds of partially reduplicative processes explode the state space of the model, and are therefore highly burdensome to develop. More recent work addresses these challenges using 2-way FSTs (Dolatian and Heinz, 2018, 2019), and offers a promising future avenue of exploration for our work with Kunwinjku. We include reduplication in this paper for the sake of completeness (see Figure 2), but acknowledge that a solution lies beyond the scope of this work. 3 Data and Metrics As mentioned previously, the grammar implementation is based on (Evans, 2003). The lexicon was subsequently expanded using the resources curated at kunwok.org, a website dedicated to open sharing of content and teaching the Kunwinjku language (Bird and Marley, 2019), as well as the verbs from the online Kunwinjku dictionary at njamed.com"
U19-1001,E09-2008,0,0.286265,"sed a small set of 114 verbs randomly sampled from the Kunwinjku translation of the Bible, for the purpose of judging how well the FST generalizes to another domain. The Bible translation was recently completed in 2018, and targets the modern vernacular. We use accuracy and coverage to measure the effectiveness of the model on the development data set as well as the test set. 4 Implementation Finite state transducers are viewed as an ideal framework to model morphology (Beesley and Karttunen, 2003; Chen and Schwartz, 2018; Lachler et al., 2018). Our FST was implemented using the Foma toolkit (Hulden, 2009) which is a popular framework for building morphological analyzers for polysynthetic languages (Chen and Schwartz, 2018; Moeller et al., 2018; Littell, 2018). The definition of an FST in Foma is comprised of a lexicon implemented in the .lexc format, and a .foma file for defining rules covering regular morphophonemic changes. The final FST is produced by composing the FSTs defined in both files. 4.1 The .lexc file The .lexc file contains definitions of lexicon groups corresponding to morphological units of the language. Lexical entries of the group are listed below the group definition. Each e"
U19-1001,N19-4021,0,0.0798068,"s. 1 Introduction Kunwinjku is an Aboriginal language of the Gunwinyguan language family (ISO gup), spoken by about 2000 speakers in the West Arnhem region of northern Australia. Several Kunwinjku communities have shown interest in leveraging technology to support the production of literacy materials and language learning applications (Bird, 2018). A major focus of our research group is to implement language technologies that have positive social impact, such as a morphologically-aware dictionary which lowers the barrier to entry for users who cannot reliably identify or spell citation forms (Hunt et al., 2019; Arppe et al., 2016), or a tool that generates linguistic structures which could help language learners master conjugation and verb structure (Kazantseva et al., 2018). One thing that these applications have in common is the need to decompose and manipulate text at the level of morphology. In order to accomplish this, we must address polysynthesis, morphophonemic alternations, incorporation, reduplication, and long-distance dependencies. Which aspects of morphosyntax can we model? What are the limitations of computational approaches for modeling polysynthetic languages more generally? In the"
U19-1001,W18-4806,0,0.0189359,"rthern Australia. Several Kunwinjku communities have shown interest in leveraging technology to support the production of literacy materials and language learning applications (Bird, 2018). A major focus of our research group is to implement language technologies that have positive social impact, such as a morphologically-aware dictionary which lowers the barrier to entry for users who cannot reliably identify or spell citation forms (Hunt et al., 2019; Arppe et al., 2016), or a tool that generates linguistic structures which could help language learners master conjugation and verb structure (Kazantseva et al., 2018). One thing that these applications have in common is the need to decompose and manipulate text at the level of morphology. In order to accomplish this, we must address polysynthesis, morphophonemic alternations, incorporation, reduplication, and long-distance dependencies. Which aspects of morphosyntax can we model? What are the limitations of computational approaches for modeling polysynthetic languages more generally? In the sections that follow, we will first give an overview of those features of the language which affect how we approach the modelling task (sec 2). Next, we introduce our d"
U19-1001,W18-4803,0,0.323713,"r domain. The Bible translation was recently completed in 2018, and targets the modern vernacular. We use accuracy and coverage to measure the effectiveness of the model on the development data set as well as the test set. 4 Implementation Finite state transducers are viewed as an ideal framework to model morphology (Beesley and Karttunen, 2003; Chen and Schwartz, 2018; Lachler et al., 2018). Our FST was implemented using the Foma toolkit (Hulden, 2009) which is a popular framework for building morphological analyzers for polysynthetic languages (Chen and Schwartz, 2018; Moeller et al., 2018; Littell, 2018). The definition of an FST in Foma is comprised of a lexicon implemented in the .lexc format, and a .foma file for defining rules covering regular morphophonemic changes. The final FST is produced by composing the FSTs defined in both files. 4.1 The .lexc file The .lexc file contains definitions of lexicon groups corresponding to morphological units of the language. Lexical entries of the group are listed below the group definition. Each entry in the lexicon is paired with its continuation class which defines legal paths through the FST, enforcing valid sequences of morphs. Figure 3 gives a LE"
U19-1001,W17-0114,0,0.0595246,"elative importance of the other much smaller error classes. It could also give us more insight into the distribution of other constructions in Kunwinjku, which may inform the pedagogical aspect of designing language learning applications in a lowresource setting. In future work we hope to expand the lexicon of this tool in parallel with developing other approaches to morphosyntactic analysis. Specifically, recent work in bootstrapping recurrent neural models using an FST to generate training examples has showed significant increase in coverage and accuracy in other polysynthetic environments (Micher, 2017; Moeller et al., 2018; Schwartz et al., 2019). Acknowledgments We are grateful for the support of the Warddeken Rangers of West Arnhem. We thank Ma¨ıa Ponsonnet for her valuable insights into Gunwinyguan morphology, syntax and semantics. We also thank Alexandra Marley for contributing her expertise on Kunwinjku morphosyntax. This research has been supported by grants from the Australian Research Council and the Indigenous Languages and Arts Program of the Federal Department of Communication and the Arts. Finally, we would like to thank the three anonymous reviewers for their constructive feed"
U19-1001,W18-4802,0,0.294879,"generalizes to another domain. The Bible translation was recently completed in 2018, and targets the modern vernacular. We use accuracy and coverage to measure the effectiveness of the model on the development data set as well as the test set. 4 Implementation Finite state transducers are viewed as an ideal framework to model morphology (Beesley and Karttunen, 2003; Chen and Schwartz, 2018; Lachler et al., 2018). Our FST was implemented using the Foma toolkit (Hulden, 2009) which is a popular framework for building morphological analyzers for polysynthetic languages (Chen and Schwartz, 2018; Moeller et al., 2018; Littell, 2018). The definition of an FST in Foma is comprised of a lexicon implemented in the .lexc format, and a .foma file for defining rules covering regular morphophonemic changes. The final FST is produced by composing the FSTs defined in both files. 4.1 The .lexc file The .lexc file contains definitions of lexicon groups corresponding to morphological units of the language. Lexical entries of the group are listed below the group definition. Each entry in the lexicon is paired with its continuation class which defines legal paths through the FST, enforcing valid sequences of morphs. Fig"
U19-1001,W19-6012,0,0.130669,"Missing"
W01-1514,graff-bird-2000-many,1,0.889461,"was based on phrase structures introduced by the Penn dysfluency annotation, which in turn was based on the Penn/NIST corrections, which in turn were based on the original TI transcriptions of the underlying (and largely unchanging) audio files. Switchboard and its derivatives remain in active use worldwide, and new derivatives continue to be produced, along with (published and unpublished) corrections of old ones. This worsens the already acute problem of establishing and maintaining coherent relations among the derivatives in common use today. The Switchboard-1 case is by no means isolated (Graff & Bird 2000). The Topic Detection and Tracking Corpus, TDT-2 (ISBN: 1-58563-157-4) was created in 1998 by LDC and contains newswire and more than 600 hours of transcribed broadcast news from 8 English and 3 Chinese sources sampled daily over six months with annotations to indicate story boundaries and relevance of those stories to 100 randomly selected topics. Since its release, TDT-2 has been used as training, developmenttest and evaluation data in the TDT evaluations; the audio has been used in TREC SDR evaluations (Garofalo, Auzanne and Voorhees 2000), TDT text has been partially re-annotated for entit"
W01-1514,W01-1515,1,0.658363,"Missing"
W01-1514,W00-0504,0,0.0215901,"Missing"
W01-1515,bird-etal-2000-atlas,1,0.609739,"erests of sharing and reuse are better served by agreeing on the data models and interfaces. Annotation graphs (AGs) provide an efficient and expressive data model for linguistic annotations of time-series data (Bird and Liberman, Figure 1: Architecture for Annotation Systems 2001). Recently, the LDC has been developing a complete software infrastructure supporting the rapid development of tools for transcribing and annotating time-series data, in cooperation with NIST and MITRE as part of the ATLAS project, and with the developers of other widely used annotation systems, Transcriber and Emu (Bird et al., 2000; Barras et al., 2001; Cassidy and Harrington, 2001). The infrastructure is being used in the development of a series of annotation tools at the Linguistic Data Consortium. Two tools are shown in the paper: one for dialogue annotation and one for interlinear transcription. In both cases, the transcriptions are time-aligned to a digital audio signal. This paper will cover the following points: the application programming interfaces for manipulating annotation graph data and importing data from other formats; the model of inter-component communication which permits easy reuse of software compone"
W02-0109,W02-0108,0,0.0206545,"n, 1980) to HPSG (Pollard and Sag, 1994). Recent work includes (Copestake, 2000; Baldridge et al., 2002a). A concurrent development has been the finite state toolkits, such as the Xerox toolkit (Beesley and Karttunen, 2002). This work has found widespread pedagogical application. Other Researchers and Developers. A variety of toolkits have been created for research or R&D purposes. Examples include the CMU-Cambridge Statistical Language Modeling Toolkit (Clarkson and Rosenfeld, 1997), the EMU Speech Database System (Harrington and Cassidy, 1999), the General Architecture for Text Engineering (Bontcheva et al., 2002), the Maxent Package for Maximum Entropy Models (Baldridge et al., 2002b), and the Annotation Graph Toolkit (Maeda et al., 2002). Although not originally motivated by pedagogical needs, all of these toolkits have pedagogical applications and many have already been used in teaching. 9 Conclusions and Future Work NLTK provides a simple, extensible, uniform framework for assignments, projects, and class demonstrations. It is well documented, easy to learn, and simple to use. We hope that NLTK will allow computational linguistics classes to include more hands-on experience with using and building"
W02-0109,baldridge-etal-2002-leo,0,0.0157845,"ogramming or computing to linguists. These are elementary on the computational side, providing a gentle introduction to students having no prior experience in computer science. Examples of such books are: Using Computers in Linguistics (Lawler and Dry, 1998), and Programming for Linguistics: Java Technology for Language Researchers (Hammond, 2002). Grammar Developers. Infrastructure for grammar development has a long history in unification-based (or constraint-based) grammar frameworks, from DCG (Pereira and Warren, 1980) to HPSG (Pollard and Sag, 1994). Recent work includes (Copestake, 2000; Baldridge et al., 2002a). A concurrent development has been the finite state toolkits, such as the Xerox toolkit (Beesley and Karttunen, 2002). This work has found widespread pedagogical application. Other Researchers and Developers. A variety of toolkits have been created for research or R&D purposes. Examples include the CMU-Cambridge Statistical Language Modeling Toolkit (Clarkson and Rosenfeld, 1997), the EMU Speech Database System (Harrington and Cassidy, 1999), the General Architecture for Text Engineering (Bontcheva et al., 2002), the Maxent Package for Maximum Entropy Models (Baldridge et al., 2002b), and t"
W02-0109,maeda-etal-2002-creating,1,0.16235,"has been the finite state toolkits, such as the Xerox toolkit (Beesley and Karttunen, 2002). This work has found widespread pedagogical application. Other Researchers and Developers. A variety of toolkits have been created for research or R&D purposes. Examples include the CMU-Cambridge Statistical Language Modeling Toolkit (Clarkson and Rosenfeld, 1997), the EMU Speech Database System (Harrington and Cassidy, 1999), the General Architecture for Text Engineering (Bontcheva et al., 2002), the Maxent Package for Maximum Entropy Models (Baldridge et al., 2002b), and the Annotation Graph Toolkit (Maeda et al., 2002). Although not originally motivated by pedagogical needs, all of these toolkits have pedagogical applications and many have already been used in teaching. 9 Conclusions and Future Work NLTK provides a simple, extensible, uniform framework for assignments, projects, and class demonstrations. It is well documented, easy to learn, and simple to use. We hope that NLTK will allow computational linguistics classes to include more hands-on experience with using and building NLP components and systems. NLTK is unique in its combination of three factors. First, it was deliberately designed as coursewar"
W03-0805,W01-1514,1,\N,Missing
W03-0805,A97-2017,0,\N,Missing
W07-0907,J98-1006,0,0.0220878,"exhibits in the path. For each word in the keyword set of each exhibit, the WordNet (Fellbaum, 1998) similarity is calculated against each word in another exhibit. The similarity is the sum of the WordNet similarities between all attribute keywords in the two exhibits (K1 , K2 ), normalised over the length of both keyword sets: P k1 ∈K1 P W N sim(k1 , k2 ) |K1 ||K2 | k2 ∈K2 For the purposes of this experiment we have chosen to use three WordNet similarity/relatedness measures to simulate the conceptual connections that visitors make between exhibits. The Lin (Lin, 1998) and Leacock-Chodorow (Leacock et al., 1998) similarity measures and the BanerjeePedersen (Patwardhan and Pedersen, 2003) relatedness measures were used. The similarities were normalised and transformed into probability P matrices such that j PW N sim (e|cj ) = 1 for each next exhibit ci . The use of WordNet measures is intended to simulate the mental connections that visitors make between exhibit content, given that each visit can interpret content in a number of different ways. The history of the visitor at any given time is essential in keeping the visitor’s conceptual model of the exhibit space current. The recency of a given exhibi"
W07-0907,P98-2127,0,0.00507288,"based on the meanings of previous exhibits in the path. For each word in the keyword set of each exhibit, the WordNet (Fellbaum, 1998) similarity is calculated against each word in another exhibit. The similarity is the sum of the WordNet similarities between all attribute keywords in the two exhibits (K1 , K2 ), normalised over the length of both keyword sets: P k1 ∈K1 P W N sim(k1 , k2 ) |K1 ||K2 | k2 ∈K2 For the purposes of this experiment we have chosen to use three WordNet similarity/relatedness measures to simulate the conceptual connections that visitors make between exhibits. The Lin (Lin, 1998) and Leacock-Chodorow (Leacock et al., 1998) similarity measures and the BanerjeePedersen (Patwardhan and Pedersen, 2003) relatedness measures were used. The similarities were normalised and transformed into probability P matrices such that j PW N sim (e|cj ) = 1 for each next exhibit ci . The use of WordNet measures is intended to simulate the mental connections that visitors make between exhibit content, given that each visit can interpret content in a number of different ways. The history of the visitor at any given time is essential in keeping the visitor’s conceptual model of the exhibit"
W07-0907,C98-2122,0,\N,Missing
W08-0204,W08-0208,1,0.736808,"f programming skills, and we found that a 1:5 staff-student ratio was insufficient. In the Computer Science department, the first approach was to introduce linguistics for 2-3 weeks before looking at algorithms for linguistic processing. This was unpopular with many students, who 29 did not see the motivation for learning about such topics as morphology and verb subcategorization in isolation from practical applications. A revised version of the course opened with topics in text processing, including tokenization, extracting text from the web, and moving on to topics in language engineering. (Bird et al. (2008b) provide a more extended discussion of opening topics.) A third option is to teach computational linguistic topics in the context of a specialised course in an allied field. Thus a course on morphology could include a module on finite-state morphology, and a course on machine learning could include a module on text mining. In the former case, a linguistic domain is presupposed and the instructor needs to teach the linguist audience about a particular corpus to be processed or an algorithm to be implemented or tested. In the latter case, a family of algorithms and data structures is presuppos"
W08-0208,W08-0201,1,0.832948,"has grown up into a comprehensive online book (Bird et al., 2008). The book has been designed to stay in lock-step with the NLTK library, and is intended to facilitate “active learning” (Bonwell and Eison, 1991). This paper describes the main features of NLTK , and reports on how it has been used effectively in classes that involve a combination of linguists and computer scientists. First we discuss aspects of the design of the toolkit that 1 (Bird and Loper, 2004; Loper, 2004; Bird, 2005; Hearst, 2005; Bird, 2006; Klein, 2006; Liddy and McCracken, 2005; Madnani, 2007; Madnani and Dorr, 2008; Baldridge and Erk, 2008) 62 Proceedings of the Third Workshop on Issues in Teaching Computational Linguistics (TeachCL-08), pages 62–70, c Columbus, Ohio, USA, June 2008. 2008 Association for Computational Linguistics arose from our need to teach computational linguistics to a multidisciplinary audience (§2). The following sections cover three distinct challenges: getting started with a course (§3); interactive demonstrations (§4); and organizing assignments and projects (§5). Other Python libraries are useful in the NLP context: NumPy provides optimized support for linear algebra and sparse arrays (NumPy, 2008) and"
W08-0208,P04-3031,1,0.74082,"es at several universities, and based on feedback from many teachers and students.1 Over this period, a series of practical online tutorials about NLTK has grown up into a comprehensive online book (Bird et al., 2008). The book has been designed to stay in lock-step with the NLTK library, and is intended to facilitate “active learning” (Bonwell and Eison, 1991). This paper describes the main features of NLTK , and reports on how it has been used effectively in classes that involve a combination of linguists and computer scientists. First we discuss aspects of the design of the toolkit that 1 (Bird and Loper, 2004; Loper, 2004; Bird, 2005; Hearst, 2005; Bird, 2006; Klein, 2006; Liddy and McCracken, 2005; Madnani, 2007; Madnani and Dorr, 2008; Baldridge and Erk, 2008) 62 Proceedings of the Third Workshop on Issues in Teaching Computational Linguistics (TeachCL-08), pages 62–70, c Columbus, Ohio, USA, June 2008. 2008 Association for Computational Linguistics arose from our need to teach computational linguistics to a multidisciplinary audience (§2). The following sections cover three distinct challenges: getting started with a course (§3); interactive demonstrations (§4); and organizing assignments and p"
W08-0208,P06-4018,1,0.46147,"eachers and students.1 Over this period, a series of practical online tutorials about NLTK has grown up into a comprehensive online book (Bird et al., 2008). The book has been designed to stay in lock-step with the NLTK library, and is intended to facilitate “active learning” (Bonwell and Eison, 1991). This paper describes the main features of NLTK , and reports on how it has been used effectively in classes that involve a combination of linguists and computer scientists. First we discuss aspects of the design of the toolkit that 1 (Bird and Loper, 2004; Loper, 2004; Bird, 2005; Hearst, 2005; Bird, 2006; Klein, 2006; Liddy and McCracken, 2005; Madnani, 2007; Madnani and Dorr, 2008; Baldridge and Erk, 2008) 62 Proceedings of the Third Workshop on Issues in Teaching Computational Linguistics (TeachCL-08), pages 62–70, c Columbus, Ohio, USA, June 2008. 2008 Association for Computational Linguistics arose from our need to teach computational linguistics to a multidisciplinary audience (§2). The following sections cover three distinct challenges: getting started with a course (§3); interactive demonstrations (§4); and organizing assignments and projects (§5). Other Python libraries are useful in"
W08-0208,W08-0204,1,0.723675,"tics students to write their first ever program, leads to stressed students who complain that they don’t know what is expected of them. Nevertheless, students need to confront the challenge of becoming bilingual, of working hard to learn the basics of another discipline. In parallel, instructors need to confront the challenge of synthesizing material from linguistics and computer science into a coherent whole, and devising effective methods for teaching, learning, and assessment. 3.3 Entry Points It is possible to identify several distinct pathways into the field of Computational Linguistics. Bird (2008) identifies four; each of these are supported by NLTK, as detailed below: Text Processing First: NLTK supports variety of approaches to tokenization, tagging, evaluation, and language engineering more generally. Programming First: NLTK is based on Python and the documentation teaches the language and provides many examples and exercises to test and reinforce student learning. Linguistics First: Here, students come with a grounding in one or more areas of linguistics, and focus on computational approaches to that area by working with the relevant chapter of the NLTK book in conjunction with lea"
W08-0208,W05-0101,0,0.0862997,"ck from many teachers and students.1 Over this period, a series of practical online tutorials about NLTK has grown up into a comprehensive online book (Bird et al., 2008). The book has been designed to stay in lock-step with the NLTK library, and is intended to facilitate “active learning” (Bonwell and Eison, 1991). This paper describes the main features of NLTK , and reports on how it has been used effectively in classes that involve a combination of linguists and computer scientists. First we discuss aspects of the design of the toolkit that 1 (Bird and Loper, 2004; Loper, 2004; Bird, 2005; Hearst, 2005; Bird, 2006; Klein, 2006; Liddy and McCracken, 2005; Madnani, 2007; Madnani and Dorr, 2008; Baldridge and Erk, 2008) 62 Proceedings of the Third Workshop on Issues in Teaching Computational Linguistics (TeachCL-08), pages 62–70, c Columbus, Ohio, USA, June 2008. 2008 Association for Computational Linguistics arose from our need to teach computational linguistics to a multidisciplinary audience (§2). The following sections cover three distinct challenges: getting started with a course (§3); interactive demonstrations (§4); and organizing assignments and projects (§5). Other Python libraries ar"
W08-0208,U06-1006,1,0.695584,"students.1 Over this period, a series of practical online tutorials about NLTK has grown up into a comprehensive online book (Bird et al., 2008). The book has been designed to stay in lock-step with the NLTK library, and is intended to facilitate “active learning” (Bonwell and Eison, 1991). This paper describes the main features of NLTK , and reports on how it has been used effectively in classes that involve a combination of linguists and computer scientists. First we discuss aspects of the design of the toolkit that 1 (Bird and Loper, 2004; Loper, 2004; Bird, 2005; Hearst, 2005; Bird, 2006; Klein, 2006; Liddy and McCracken, 2005; Madnani, 2007; Madnani and Dorr, 2008; Baldridge and Erk, 2008) 62 Proceedings of the Third Workshop on Issues in Teaching Computational Linguistics (TeachCL-08), pages 62–70, c Columbus, Ohio, USA, June 2008. 2008 Association for Computational Linguistics arose from our need to teach computational linguistics to a multidisciplinary audience (§2). The following sections cover three distinct challenges: getting started with a course (§3); interactive demonstrations (§4); and organizing assignments and projects (§5). Other Python libraries are useful in the NLP conte"
W08-0208,W05-0111,0,0.118105,"er this period, a series of practical online tutorials about NLTK has grown up into a comprehensive online book (Bird et al., 2008). The book has been designed to stay in lock-step with the NLTK library, and is intended to facilitate “active learning” (Bonwell and Eison, 1991). This paper describes the main features of NLTK , and reports on how it has been used effectively in classes that involve a combination of linguists and computer scientists. First we discuss aspects of the design of the toolkit that 1 (Bird and Loper, 2004; Loper, 2004; Bird, 2005; Hearst, 2005; Bird, 2006; Klein, 2006; Liddy and McCracken, 2005; Madnani, 2007; Madnani and Dorr, 2008; Baldridge and Erk, 2008) 62 Proceedings of the Third Workshop on Issues in Teaching Computational Linguistics (TeachCL-08), pages 62–70, c Columbus, Ohio, USA, June 2008. 2008 Association for Computational Linguistics arose from our need to teach computational linguistics to a multidisciplinary audience (§2). The following sections cover three distinct challenges: getting started with a course (§3); interactive demonstrations (§4); and organizing assignments and projects (§5). Other Python libraries are useful in the NLP context: NumPy provides optimize"
W08-0208,W02-0109,1,0.589789,"ode to a uniform cohort of students. Linguists can be taught to program, leading to projects where students manipulate their own linguistic data. Computer scientists can be taught methods for automatic text processing, leading to projects on text mining and chatbots. Yet these approaches have almost nothing in common, and it is a stretch to call either of these NLP: more apt titles for such courses might be “linguistic data management” and “text technologies.” The Natural Language Toolkit, or NLTK, was developed to give a broad range of students access to the core knowledge and skills of NLP (Loper and Bird, 2002). In particular, NLTK makes it feasible to run a course that covers a substantial amount of theory and practice with an audience consisting of both linguists and computer scientists. NLTK is a suite of Python modules distributed under the GPL open source license via nltk.org. NLTK comes with a large collection of corpora, extensive documentation, and hundreds of exercises, making NLTK unique in providing a comprehensive framework for students to develop a computational understanding of language. NLTK’s code base of 100,000 lines of Python code includes support for corpus access, tokenizing, st"
W08-0208,W08-0209,0,0.315004,"ne tutorials about NLTK has grown up into a comprehensive online book (Bird et al., 2008). The book has been designed to stay in lock-step with the NLTK library, and is intended to facilitate “active learning” (Bonwell and Eison, 1991). This paper describes the main features of NLTK , and reports on how it has been used effectively in classes that involve a combination of linguists and computer scientists. First we discuss aspects of the design of the toolkit that 1 (Bird and Loper, 2004; Loper, 2004; Bird, 2005; Hearst, 2005; Bird, 2006; Klein, 2006; Liddy and McCracken, 2005; Madnani, 2007; Madnani and Dorr, 2008; Baldridge and Erk, 2008) 62 Proceedings of the Third Workshop on Issues in Teaching Computational Linguistics (TeachCL-08), pages 62–70, c Columbus, Ohio, USA, June 2008. 2008 Association for Computational Linguistics arose from our need to teach computational linguistics to a multidisciplinary audience (§2). The following sections cover three distinct challenges: getting started with a course (§3); interactive demonstrations (§4); and organizing assignments and projects (§5). Other Python libraries are useful in the NLP context: NumPy provides optimized support for linear algebra and sparse"
W11-1216,P10-1010,1,0.755527,"in languages outside of the group of 30 or so for which there already exist non-trivial electronic resources. Optimistically, we aim for a universal corpus, in the sense of one that covers a widely representative set of the world’s languages and supports inquiry into universal linguistics and development of language technologies with universal applicability. Introduction We have previously proposed a community dataset of annotated text spanning a very large number of languages, with consistent annotation and format that enables automatic cross-linguistic processing on an unprecedented scale (Abney and Bird, 2010). Here we set out the data model in detail, and invite members of the computational linguistics community to begin work on the first version of the dataset. The targeted annotation generalizes over three widely-used kinds of data: (1) simple bitexts, that is, tokenized texts and their translations, which are We emphasize, however, that even if completely successful, it will be a universal corpus and not the universal corpus. The term “universal” should emphatically not be understood in the sense of encompassing all language annotation efforts. We are not proposing a standard or a philosophy of"
W11-1216,W06-2920,0,\N,Missing
W14-2201,P10-1010,1,0.375722,"said, and which can be used as the basis for phonetic analysis. This means we can postpone the transcription task – by years or even decades – until such time as the required linguistic expertise is available to work with archived recordings. By interpretation, we mean listening to a recording and producing a spoken translation of what was heard. Translation into another language obviates the need for the usual resource-intensive approaches to linguistic analysis that require syntactic treebanks along with semantic annotations, at the cost of a future decipherment effort (Xia and Lewis, 2007; Abney and Bird, 2010). Thought Experiment: The Future Philologist A typical language documentation project is resource-bound. So much documentation could be collected, yet the required human resources to process it all adequately are often not available. For instance, some have argued that it is not effective to collect large quantities of primary recordings because there is not the time to transcribe it.1 Estimates differ about the pace of language loss. Yet it is uncontroversial that – for hundreds of languages – only the oldest living speakers are wellversed in traditional folklore. While a given language may s"
W14-2201,D10-1045,0,0.0257568,"evel, we want to know what words were spoken, and what they meant. Recordings made in the wild suffer from the expected range of problems: far-field recording, significant ambient noise, audience participation, and so forth. We address these problems via the “respeaking” task (Woodbury, 2003). Recordings made in an endangered language may not be interpretable once the language falls out of use. We address this problem via the “oral translation” task. The result is relatively clean source audio recordings with phrase-aligned translations (see Figure 1). NLP methods are applicable to such data (Dredze et al., 2010), and we can hope that ultimately, researchers working on archived bilingual audio sources will be able to automatically extract word-glossed interlinear text. We describe Aikuma, an open source Android app that supports recording along with respeaking Proliferating smartphones and mobile software offer linguists a scalable, networked recording device. This paper describes Aikuma, a mobile app that is designed to put the key language documentation tasks of recording, respeaking, and translating in the hands of a speech community. After motivating the approach we describe the system and briefly"
W14-2201,I13-1161,1,0.883727,"Missing"
W14-2201,N07-1057,0,0.0657988,"understand what was said, and which can be used as the basis for phonetic analysis. This means we can postpone the transcription task – by years or even decades – until such time as the required linguistic expertise is available to work with archived recordings. By interpretation, we mean listening to a recording and producing a spoken translation of what was heard. Translation into another language obviates the need for the usual resource-intensive approaches to linguistic analysis that require syntactic treebanks along with semantic annotations, at the cost of a future decipherment effort (Xia and Lewis, 2007; Abney and Bird, 2010). Thought Experiment: The Future Philologist A typical language documentation project is resource-bound. So much documentation could be collected, yet the required human resources to process it all adequately are often not available. For instance, some have argued that it is not effective to collect large quantities of primary recordings because there is not the time to transcribe it.1 Estimates differ about the pace of language loss. Yet it is uncontroversial that – for hundreds of languages – only the oldest living speakers are wellversed in traditional folklore. While"
W17-0121,W14-2201,1,0.693118,"hone in Canada, and the ‘Ma’ series of dictionary apps in Australia and the Pacific including Ma Iwaidja, Ma Gamilaraay and Ma Bena Bena (Carew et al., 2015). Taiwan’s Council of Indigenous People’s e-dictionary includes 16 Formosan languages, via a mobile-accessible website (Taiwan Indigenous Council, 2016). Apps have been used to conduct experiments on dialect variation, capturing linguistic judgements together with the location (Goldman et al., 2014; Leemann et al., 2016). The Android app Aikuma, a precursor of the work reported here, allows users to collect and translate spoken narrative (Bird et al., 2014). Alongside these individual web apps there are web application suites for language documentation. LDC webann is an online annotation framework supporting remote collaboration (Wright et al., 2012). LingSync supports collaborative language documentation, and has been popular in North American linguistic fieldwork training (Cathcart et al., 2012). CAMOMILE is a framework for multilingual annotation, not specifically for linguistic research (Poignant et al., 2016). These apps fall into two categories according to their audience and purpose: research apps for language documentation and ‘community"
W17-0121,goldman-etal-2014-crowdsourcing,0,0.0265222,"12; Birch et al., 2013). To date, dictionary and flashcard apps for language learning have been the most popular. For example, the suite of First Voices apps for iPhone in Canada, and the ‘Ma’ series of dictionary apps in Australia and the Pacific including Ma Iwaidja, Ma Gamilaraay and Ma Bena Bena (Carew et al., 2015). Taiwan’s Council of Indigenous People’s e-dictionary includes 16 Formosan languages, via a mobile-accessible website (Taiwan Indigenous Council, 2016). Apps have been used to conduct experiments on dialect variation, capturing linguistic judgements together with the location (Goldman et al., 2014; Leemann et al., 2016). The Android app Aikuma, a precursor of the work reported here, allows users to collect and translate spoken narrative (Bird et al., 2014). Alongside these individual web apps there are web application suites for language documentation. LDC webann is an online annotation framework supporting remote collaboration (Wright et al., 2012). LingSync supports collaborative language documentation, and has been popular in North American linguistic fieldwork training (Cathcart et al., 2012). CAMOMILE is a framework for multilingual annotation, not specifically for linguistic rese"
W17-0121,L16-1226,0,0.030509,"ann et al., 2016). The Android app Aikuma, a precursor of the work reported here, allows users to collect and translate spoken narrative (Bird et al., 2014). Alongside these individual web apps there are web application suites for language documentation. LDC webann is an online annotation framework supporting remote collaboration (Wright et al., 2012). LingSync supports collaborative language documentation, and has been popular in North American linguistic fieldwork training (Cathcart et al., 2012). CAMOMILE is a framework for multilingual annotation, not specifically for linguistic research (Poignant et al., 2016). These apps fall into two categories according to their audience and purpose: research apps for language documentation and ‘community’ apps for language development. The developer profile and 3 Prototype Apps This section reports on the development of three apps over the course of 2016. These apps represent an evolving understanding of methods to achieve modularisation through reusable components. One component in particular, for language selection, is required for all apps. We discuss this component in the context of each app to shed light on some options concerning web technologies and data"
W17-0121,wright-etal-2012-annotation,0,0.0249844,"e’s e-dictionary includes 16 Formosan languages, via a mobile-accessible website (Taiwan Indigenous Council, 2016). Apps have been used to conduct experiments on dialect variation, capturing linguistic judgements together with the location (Goldman et al., 2014; Leemann et al., 2016). The Android app Aikuma, a precursor of the work reported here, allows users to collect and translate spoken narrative (Bird et al., 2014). Alongside these individual web apps there are web application suites for language documentation. LDC webann is an online annotation framework supporting remote collaboration (Wright et al., 2012). LingSync supports collaborative language documentation, and has been popular in North American linguistic fieldwork training (Cathcart et al., 2012). CAMOMILE is a framework for multilingual annotation, not specifically for linguistic research (Poignant et al., 2016). These apps fall into two categories according to their audience and purpose: research apps for language documentation and ‘community’ apps for language development. The developer profile and 3 Prototype Apps This section reports on the development of three apps over the course of 2016. These apps represent an evolving understan"
W97-1105,J94-1003,1,0.894346,"Missing"
W97-1105,J94-3004,0,0.0611171,"Missing"
W97-1105,J94-3001,0,\N,Missing
W99-0301,J93-2004,0,0.0476757,"]. ToBI is an acronym for &quot;Tones and Break Indices&quot;, and correspondingly provides two types of information: Tones, which are taken from a fixed vocabulary of categories of (stress-linked) &quot;pitch accents&quot; and (juncture-linked) &quot;boundary tones&quot;; and Break Indices, which are integers characterizing the strength and nature of interword disjunctures. We have added four additional annotations: coreference annotation and named entity annotation in the style of MUC-7 [wWW.muc. saic. com/proceedings/muc_7_toc. html] provided by Lynette Hirschman; syntactic structures in the style of the Penn TreeBank (Marcus et al., 1993) provided by Ann Taylor; and an alternative annotation for the F0 aspects of prosody, known as Tilt (Taylor, 1998) and provided by its inventor, Paul Taylor. Taylor has done Tilt annotations for much of the BU corpus, and will soon b e publishing them as a point of comparison with the ToBI tonal annotation. Tilt differs from ToBI in providing a quantitative rather than qualitative characterization of F0 obtrusions: where ToBI might say &quot;this is a L+H* pitch accent,&quot; Tilt would say &quot;This is an Fo obtrusion that starts at time to, lasts for duration d seconds, involves a Hz total F0 change, and"
Y05-1001,cassidy-2002-xquery,0,0.0301883,"the simple grammar fragment, NP → Adj NP; NP → N as: //NP[({/ˆAdj=&gt;NP$})*/N] The addition of the immediate following and immediate following sibling axes completes the set of X axes for navigating trees. In L+ , each axis has a corresponding one-step axis. The L+ axis set accounts for both hierarchical, sequential and sibling orderings on unranked ordered trees. As such, there do not appear to be any such (unconditional) relations lacking in the L+ axis set. Thus, L+ appears to have the complete set of axes necessary for linguistic tree query. 1 This is slightly harder version of the query in Cassidy (2002). Proceedings of PACLIC 19, the 19th Asia-Pacific Conference on Language, Information and Computation 5. Conclusion LPath was proposed as a new query language which augmented the navigational axes of XPath with three additional tree operators. The analysis of LPath operators shows that they are more than just syntactic sugar. In fact, LPath takes up a new rung on the expressiveness hierarchy strictly between Core and Conditional XPath. Conditional LPath, LPath extended with the conditional axis, has the same expressiveness as Conditional XPath. This provides evidence that the closures required"
Y05-1001,E03-1074,0,0.430927,"uivalent to //B(-&gt;A)+ imf BA+ (x, y) ≡ (?B/f ollowing?A) ∩ φ/f ollowing. Along with the proof, Marx (2005, Theorem 2) provides a method for finding the complement of any X + path set. Thus, we now have a concrete method for translating L+ expressions into X + . 4.2. Conditional LPath as Linguistic Tree Query Language L+ is capable of expressing a large range of linguistic tree queries, including all the basic subtree matching queries identified in our requirements analysis (Lai and Bird, 2004). The only other current linguistic treebank query language with this level of expressiveness is fsq (Kepser, 2003). However, fsq only allows boolean queries. Moreover, L+ ’s path-based syntax is much more intuitive and more closely aligned to actual descriptions of structure in the linguistics literature (Palm, 1999). However, there is still a price to pay for choosing this path-based variable-free approach over the variables and predicates of classical first-order logic. The major advantage of the classical approach of fsq is that variables can be used to identify specific nodes throughout a query. The scoping operator accounts for cases where there is a need to identify the root of a particular subtree,"
Y05-1001,U04-1019,1,0.694718,"t the nested structure of syntactic and prosodic constituents. Recently, the LPath language has been proposed as a convenient path-based language for querying linguistic trees. We establish the formal expressiveness of LPath relative to the XPath family of languages. We also extend LPath to permit simple closures, resulting in a first-order complete language which we believe is sufficiently expressive for the majority of linguistic tree query needs. 1. Introduction In recent years, a great variety of linguistic query languages have been proposed, most of them specialised for linguistic trees (Lai and Bird, 2004), and applied to corpora such as the Penn Treebank (Marcus et al., 1993). Despite this considerable effort, relatively little is known about the formal expressiveness of these languages, or the computational resources required to process them as the size of the data grows. One reason for this is that much of the work in this area has taken place in isolation from well-understood database query languages such as SQL and XPath (Clark and DeRose, 1999). Recently, the LPath language has been proposed as a convenient path-based language for querying linguistic trees (Bird et al., 2006). It augments"
Y05-1001,J93-2004,0,0.0338433,", the LPath language has been proposed as a convenient path-based language for querying linguistic trees. We establish the formal expressiveness of LPath relative to the XPath family of languages. We also extend LPath to permit simple closures, resulting in a first-order complete language which we believe is sufficiently expressive for the majority of linguistic tree query needs. 1. Introduction In recent years, a great variety of linguistic query languages have been proposed, most of them specialised for linguistic trees (Lai and Bird, 2004), and applied to corpora such as the Penn Treebank (Marcus et al., 1993). Despite this considerable effort, relatively little is known about the formal expressiveness of these languages, or the computational resources required to process them as the size of the data grows. One reason for this is that much of the work in this area has taken place in isolation from well-understood database query languages such as SQL and XPath (Clark and DeRose, 1999). Recently, the LPath language has been proposed as a convenient path-based language for querying linguistic trees (Bird et al., 2006). It augments the navigational axes of XPath with three additional tree operators, an"
