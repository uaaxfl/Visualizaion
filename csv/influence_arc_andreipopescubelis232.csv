2001.mtsummit-eval.5,1999.mtsummit-1.85,0,0.0311044,"Missing"
2003.mtsummit-papers.30,1999.mtsummit-1.85,0,0.0275928,"Missing"
2003.mtsummit-papers.30,W99-0604,0,0.0616643,"Missing"
2003.mtsummit-papers.30,rajman-hartley-2002-automatic,0,0.053801,"Missing"
2003.mtsummit-papers.30,1994.amta-1.25,0,0.106339,"Missing"
2003.mtsummit-papers.41,2001.mtsummit-eval.6,0,0.0286236,"ion, the results of automated metrics must correlate well with (some aspect of) human-based metrics. The participants were given a broad list of MT evaluation metrics, and were asked to apply an automatic and a human one. Below is a synopsis of the metrics, with the code names that will be used further on, and important references (Table 1). (A1) IBM&apos;s BLEU (Papineni 2002, Papineni, Roukos, Ward and Zhu 2001) and the NIST version (Doddington 2002) (A2) EvalTrans (Niessen, Och, Leusch and Ney 2000) (A3) Named entity translation (Reeder, Miller, Doyon and White 2001) (A4) X-Score / parsability (Hartley and Rajman 2001, Rajman and Hartley 2002) (A5) Dictionary update / number of untranslated words (Vanni and Miller 2002) (A6) Evaluating syntactic correctness from the implementation of transfer rules (H1) Reading time (Van Slype 1979) (H2) Correction / post-editing time (Van Slype 1979) (H3) Cloze test (Van Slype 1979) (H4a) Intelligibility / fluency (Van Slype 1979, p.70) (H4b) Clarity (Vanni and Miller 2002) (H5) Correctness / adequacy / fidelity (Doyon, Taylor and White 1998) (H6) Informativeness: comprehension task (Somers and Prieto-Alvarez 2000) Table 1. MT evaluation measures proposed for the present"
2003.mtsummit-papers.41,niessen-etal-2000-evaluation,0,0.200454,"Missing"
2003.mtsummit-papers.41,rajman-hartley-2002-automatic,0,0.0110242,"ated metrics must correlate well with (some aspect of) human-based metrics. The participants were given a broad list of MT evaluation metrics, and were asked to apply an automatic and a human one. Below is a synopsis of the metrics, with the code names that will be used further on, and important references (Table 1). (A1) IBM&apos;s BLEU (Papineni 2002, Papineni, Roukos, Ward and Zhu 2001) and the NIST version (Doddington 2002) (A2) EvalTrans (Niessen, Och, Leusch and Ney 2000) (A3) Named entity translation (Reeder, Miller, Doyon and White 2001) (A4) X-Score / parsability (Hartley and Rajman 2001, Rajman and Hartley 2002) (A5) Dictionary update / number of untranslated words (Vanni and Miller 2002) (A6) Evaluating syntactic correctness from the implementation of transfer rules (H1) Reading time (Van Slype 1979) (H2) Correction / post-editing time (Van Slype 1979) (H3) Cloze test (Van Slype 1979) (H4a) Intelligibility / fluency (Van Slype 1979, p.70) (H4b) Clarity (Vanni and Miller 2002) (H5) Correctness / adequacy / fidelity (Doyon, Taylor and White 1998) (H6) Informativeness: comprehension task (Somers and Prieto-Alvarez 2000) Table 1. MT evaluation measures proposed for the present experiment (A: automated;"
2003.mtsummit-papers.41,2001.mtsummit-eval.8,0,0.0953207,"Missing"
2003.mtsummit-papers.41,vanni-miller-2002-scaling,0,0.0286974,"participants were given a broad list of MT evaluation metrics, and were asked to apply an automatic and a human one. Below is a synopsis of the metrics, with the code names that will be used further on, and important references (Table 1). (A1) IBM&apos;s BLEU (Papineni 2002, Papineni, Roukos, Ward and Zhu 2001) and the NIST version (Doddington 2002) (A2) EvalTrans (Niessen, Och, Leusch and Ney 2000) (A3) Named entity translation (Reeder, Miller, Doyon and White 2001) (A4) X-Score / parsability (Hartley and Rajman 2001, Rajman and Hartley 2002) (A5) Dictionary update / number of untranslated words (Vanni and Miller 2002) (A6) Evaluating syntactic correctness from the implementation of transfer rules (H1) Reading time (Van Slype 1979) (H2) Correction / post-editing time (Van Slype 1979) (H3) Cloze test (Van Slype 1979) (H4a) Intelligibility / fluency (Van Slype 1979, p.70) (H4b) Clarity (Vanni and Miller 2002) (H5) Correctness / adequacy / fidelity (Doyon, Taylor and White 1998) (H6) Informativeness: comprehension task (Somers and Prieto-Alvarez 2000) Table 1. MT evaluation measures proposed for the present experiment (A: automated; H: human). 3 Test data 3.1 Presentation The human-translated texts were extrac"
2005.mtsummit-papers.16,babych-etal-2004-calibrating,0,0.0278993,"Missing"
2005.mtsummit-papers.16,babych-hartley-2004-modelling,0,0.0609235,"th a positive weighting, NIST computes the arithmetic mean of n-grams precision taking into account a comparison of the length of segments. BLEU/NIST is in the most widespread use nowadays in the MT community. BLEU scores 3 4 Part of the MLCC corpus, available at ELDA. “Arabic Data Set” corpus, available at ELDA. 119 proved to correlate with human judgements about the fluency (Thompson and Brew, 1994) of the evaluated translation (Zhang et al., 2004). 2.5.2 WNM The Weighted N-gram Model, or WNM (Babych, 2004), is a combination of BLEU and the Legitimate Translation Variation, or LTV, metrics (Babych and Hartley, 2004a, 2004b). For a given source text, more than one correct translation is possible. BLEU tries to cope with this by multiplying the number of reference translations to be compared to the evaluated one. But still, the fact that some n-gram does not occur in any reference does not mean that it is an erroneous translation, providing the meaning is the same. Babych and Hartley’s proposal is to extend BLEU and the computation of proximity scores (i.e. the distance measure between the evaluated translation and the references) by introducing weights coming from the statistical relevance of the words i"
2005.mtsummit-papers.16,P04-1079,0,0.532761,"th a positive weighting, NIST computes the arithmetic mean of n-grams precision taking into account a comparison of the length of segments. BLEU/NIST is in the most widespread use nowadays in the MT community. BLEU scores 3 4 Part of the MLCC corpus, available at ELDA. “Arabic Data Set” corpus, available at ELDA. 119 proved to correlate with human judgements about the fluency (Thompson and Brew, 1994) of the evaluated translation (Zhang et al., 2004). 2.5.2 WNM The Weighted N-gram Model, or WNM (Babych, 2004), is a combination of BLEU and the Legitimate Translation Variation, or LTV, metrics (Babych and Hartley, 2004a, 2004b). For a given source text, more than one correct translation is possible. BLEU tries to cope with this by multiplying the number of reference translations to be compared to the evaluated one. But still, the fact that some n-gram does not occur in any reference does not mean that it is an erroneous translation, providing the meaning is the same. Babych and Hartley’s proposal is to extend BLEU and the computation of proximity scores (i.e. the distance measure between the evaluated translation and the references) by introducing weights coming from the statistical relevance of the words i"
2005.mtsummit-papers.16,2001.mtsummit-eval.4,1,0.835476,"Missing"
2005.mtsummit-papers.16,dabbadie-etal-2002-terminological,1,0.893742,"Missing"
2005.mtsummit-papers.16,P02-1040,0,0.0734272,"Missing"
2005.mtsummit-papers.16,2001.mtsummit-eval.6,1,0.926205,"sually used in Information Retrieval, plus a normalisation according to the words’ relative frequency (Babych et al., 2003). Typically, words such as names, events, terminological lexemes, are statistically more salient. They can be translated in a unique way only, whereas function words or expressions can have several possible correct translations. A preliminary experiment (Babych et al., 2004) proved that WNM results for recall were well correlated (even better than BLEU) to human judgements about adequacy. This was confirmed by (Babych and Hartley, 2004b). 2.5.3 X-Score The X-Score metric (Rajman and Hartley, 2001) is based on the distribution of elementary linguistic information within a text, such as morphosyntactic categories, or syntactic relationships. The authors’ hypothesis is that this distribution of linguistic information is similar from one text to another within a given language. Depending on the nature of the linguistic information selected to work with, the metric’s precision will vary. For instance, working with syntactic dependencies will be much more precise that working with morphosyntactic categories only. Whichever type of information is selected, the XScore measures the grammaticali"
2005.mtsummit-papers.16,zhang-etal-2004-interpreting,0,0.0536417,"ntly from that of the reference translations. The NIST metric is an alternative to BLEU. Whereas BLEU computes the geometric mean of n-grams precision (1  n  N) with a positive weighting, NIST computes the arithmetic mean of n-grams precision taking into account a comparison of the length of segments. BLEU/NIST is in the most widespread use nowadays in the MT community. BLEU scores 3 4 Part of the MLCC corpus, available at ELDA. “Arabic Data Set” corpus, available at ELDA. 119 proved to correlate with human judgements about the fluency (Thompson and Brew, 1994) of the evaluated translation (Zhang et al., 2004). 2.5.2 WNM The Weighted N-gram Model, or WNM (Babych, 2004), is a combination of BLEU and the Legitimate Translation Variation, or LTV, metrics (Babych and Hartley, 2004a, 2004b). For a given source text, more than one correct translation is possible. BLEU tries to cope with this by multiplying the number of reference translations to be compared to the evaluated one. But still, the fact that some n-gram does not occur in any reference does not mean that it is an erroneous translation, providing the meaning is the same. Babych and Hartley’s proposal is to extend BLEU and the computation of pro"
2005.tc-1.3,P04-1079,0,0.0110471,"is the intrinsic difficulty of choosing appropriate metrics for MT evaluation. For example since there is rarely, if ever, a single “correct” translation, it is not possible to create a unique “gold standard” translation for a given text, to which the output of the MT system could be automatically compared. This is partly the reason why so many metrics targeting “translation quality” have been proposed – either to be applied by human judges, such as fidelity and fluency (White and O&apos; Connell 1994), or derived automatically through a statistical comparison with one or more human translations (Babych and Hartley 2004, Doddington 2002, Papineni et al. 2001). The importance of the intended context of use of an MT system for its evaluation is not always taken into enough consideration in evaluation design. Some popular evaluation campaigns1 do not consider the type of user of the system, or other requirements related to speed of translation or integration into existent software solutions. For example, a system with acceptable output quality that runs only under Windows may not be suitable for an organisation where the only operating system used is UNIX. The implementation of the Framework for the Evaluation"
2005.tc-1.3,1994.amta-1.25,0,0.119134,"Missing"
2006.jeptalnrecital-long.23,J95-2003,0,0.0951465,"Missing"
2006.jeptalnrecital-long.23,P92-1004,0,0.169081,"Missing"
2006.jeptalnrecital-long.23,W04-0710,1,0.801076,"Missing"
2006.jeptalnrecital-long.23,P98-2172,1,0.863497,"Missing"
2006.jeptalnrecital-long.23,salmon-alt-romary-2004-towards,0,0.029874,"Missing"
2006.jeptalnrecital-long.23,simov-etal-2004-clark,0,0.0397366,"Missing"
2006.jeptalnrecital-long.23,P03-1022,0,0.0700438,"Missing"
2006.jeptalnrecital-long.23,J00-4005,0,0.0204007,"Missing"
2007.mtsummit-papers.23,2005.iwslt-1.7,0,0.0146129,"in reliable scores. Finally, we propose a method to empirically determine the minimum number of documents needed to obtain acceptably reliable results. The results presented here are also a valuable resource, which could complement the guidelines for users of the CESTA corpus—made public by ELDA—along with reference translations and scores for automatic and human metrics. At this moment, we use the corpus in a black box evaluation but if it is intended to be used in glass box evaluation, other methods could be used to reduce the amount of text to evaluate. For example, the method proposed in (Eck et al. 2005), which consists in extracting from the corpus only the unduplicated n-grams, i.e. it eliminates redundancy. We plan to apply our method to other experimental setup, such as different corpora or language pairs. Special cases of study are the human based metrics, since the average of two human judgments was used for the adequacy and fluency metrics in the EN/FR first CESTA campaign. These metrics are limited by the loss of information about the difference between judgments. So far, it was not so easy to find a method of human weighted scores. Indeed, having (at least) three evaluations by segme"
2007.mtsummit-papers.23,2005.tc-1.3,1,0.823668,"Missing"
2007.mtsummit-papers.23,W01-1409,0,0.0381321,"Missing"
2007.mtsummit-papers.23,W04-3250,0,0.0794985,"), i = 1..N, constitute the N bootstrapped samples. If the original estimator of a given population parameter was θ(X), with the bootstrapped samples we can calculate the same estimator as θ(X*). An important parameter for bootstrapping is N, the number of bootstrapped samples, or the number of times the process is repeated. This number should be large enough to build a representative number of samples. It appears that, for instance, N = 200 leads to slightly biased estimations (Efron and Gong 1983; Zhang and Vogel 2004), so a larger N is preferred, for example N = 1,000 (Efron and Gong 1983; Koehn 2004) or even N = 10,000 (Bisani and Ney 2004). Based on these examples, we decided to use N = 1,500. Another source of error in inference statistics is the error induced by using a particular sample to represent a whole (unknown) population. In the present case, this amounts to considering that the scores on the 15 documents (or 790 segments) are fully representative of a system’s performance on this type of text. Application to MT Evaluation Scores In the MT field, bootstrapping has been mainly used to estimate confidence intervals for automatic metrics and to compute the statistical significance"
2007.mtsummit-papers.23,N04-1022,0,0.0385839,"N = 1,500. Another source of error in inference statistics is the error induced by using a particular sample to represent a whole (unknown) population. In the present case, this amounts to considering that the scores on the 15 documents (or 790 segments) are fully representative of a system’s performance on this type of text. Application to MT Evaluation Scores In the MT field, bootstrapping has been mainly used to estimate confidence intervals for automatic metrics and to compute the statistical significance of comparative performance of different MT systems, e.g. using the BLEU (Koehn 2004; Kumar and Byrne 2004; Zhang and Vogel 2004) or WER metric (Bisani and Ney 2004). Here, bootstrapping will be used to compute reliable estimators for different automatic metrics for MT, namely mean, standard deviation (often expressed as a percentage of the mean) and confidence intervals (based on standard deviations) for the mean of the bootstrapped sample. For the application of bootstrapping in MT, the original sample X is the set of text segments arranged in documents, each segment being accompanied by a list of scores obtained by each MT system, according to the metrics mentioned in the previous section. Desc"
2007.mtsummit-papers.23,niessen-etal-2000-evaluation,0,0.340416,"Missing"
2007.mtsummit-papers.23,2001.mtsummit-papers.68,0,0.131344,"Missing"
2007.mtsummit-papers.23,2003.mtsummit-papers.51,0,0.455381,"Missing"
2007.mtsummit-papers.23,2004.tmi-1.9,0,0.274599,"ty 1/N). The resulting population P*, noted X* = (X1*, …, XN*), with Xi* = (Xi1*, Xi2*, …, Xin*), i = 1..N, constitute the N bootstrapped samples. If the original estimator of a given population parameter was θ(X), with the bootstrapped samples we can calculate the same estimator as θ(X*). An important parameter for bootstrapping is N, the number of bootstrapped samples, or the number of times the process is repeated. This number should be large enough to build a representative number of samples. It appears that, for instance, N = 200 leads to slightly biased estimations (Efron and Gong 1983; Zhang and Vogel 2004), so a larger N is preferred, for example N = 1,000 (Efron and Gong 1983; Koehn 2004) or even N = 10,000 (Bisani and Ney 2004). Based on these examples, we decided to use N = 1,500. Another source of error in inference statistics is the error induced by using a particular sample to represent a whole (unknown) population. In the present case, this amounts to considering that the scores on the 15 documents (or 790 segments) are fully representative of a system’s performance on this type of text. Application to MT Evaluation Scores In the MT field, bootstrapping has been mainly used to estimate c"
2007.mtsummit-papers.23,E06-1032,0,\N,Missing
2007.mtsummit-papers.23,P02-1040,0,\N,Missing
2007.mtsummit-papers.23,hamon-etal-2006-cesta,0,\N,Missing
2007.mtsummit-papers.31,2005.mtsummit-posters.13,1,0.855467,"Missing"
2007.mtsummit-papers.31,C04-1016,1,0.902404,"Missing"
2007.mtsummit-papers.31,P04-1079,0,0.030327,"Missing"
2007.mtsummit-papers.31,hamon-etal-2006-cesta,0,0.0721614,"Missing"
2007.mtsummit-papers.31,hamon-rajman-2006-x,0,0.0228193,"Missing"
2007.mtsummit-papers.31,2001.mtsummit-papers.68,0,0.074963,"Missing"
2007.mtsummit-papers.31,2005.mtsummit-papers.16,1,0.863637,"Missing"
2007.mtsummit-papers.31,2001.mtsummit-eval.10,0,0.108512,"Missing"
2007.mtsummit-papers.31,1994.amta-1.25,0,0.240679,"Missing"
2007.mtsummit-papers.31,zhang-etal-2004-interpreting,0,0.0229871,"Missing"
2007.mtsummit-papers.31,P02-1040,0,\N,Missing
2007.mtsummit-ucnlg.10,W04-1013,0,0.00404045,"g either the performance of a human using the system’s output to accomplish a given task, or the performance of another NLP system using the NLG output, provided a simple quality metric exists for this second system. 3 Task-based Evaluation: Combining NLG with Reference Resolution Evaluating NLG like MT and Summarization Evaluation metrics that compute a distance between a candidate output, such as a generated sentence, and the samples of desired outputs have been applied with some success to MT evaluation (e.g. B LEU (Papineni et al., 2001)), and also to summarization evaluation (e.g. ROUGE (Lin, 2004)), although their accuracy has been challenged (Callison-Burch et al., 1 ‘Accuracy’ often means that the computed distance reflects well the “absolute” quality assessments done by human judges. 67 the resolution system to measure NLG performance. Which of the two tasks, co-reference or reference resolution, would be more appropriate? It is likely that co-reference would be less appropriate, as this would encourage the NLG system (or rather its authors) to generate “proper names” for each referent, and to repeat them identically throughout the generated text, which is neither natural nor usable"
2007.mtsummit-ucnlg.10,2001.mtsummit-papers.68,0,0.128055,"k evaluation campaigns (STECs) is often the key to making progress in a particular domain, thanks to the convergence of several research teams. However, the definition of STECs requires an acceptable agreement, among a community of researchers, on the relevance of the selected problem to the domain, as well as on common evaluation metrics that indicate progress on this task. In the domain of Natural Language Generation (NLG), recent proposals have started meeting the challenge of STEC definition (Belz and Kilgarriff, 2006), few years after a new metric for Machine Translation (MT) evaluation (Papineni et al., 2001) had revived the interest for common evaluations, thanks to its low application costs, which in turn led to significant improvement of MT systems, and especially statistical ones. So, an important question is: how could NLG benefit from a similarly innovative metric, and how could such a metric be found? This short paper offers an explanation of the difficulty to evaluate NLG systems based on a typology 66 2006)1 . The distance between generated sentences or expressions can be computed using n-gram similarity, word error rate, or other techniques. Depending however on the type of input data se"
2007.mtsummit-ucnlg.10,passonneau-2004-computing,0,0.0170518,"g REs”, which can mean two things. Coreference resolution deals with the grouping of the REs from a text which refer to the same entities (Hirschman, 1997), while reference resolution aims at constructing links between each RE and the (computer representation of the) entity that it refers to (Popescu-Belis and Lalanne, 2004). Reliable evaluation metrics exist for both tasks (Vilain et al., 1995; Popescu-Belis and Robba, 1998; Popescu-Belis et al., 2004), and they are expressed as a distance to the correct distribution of REs that can be easily annotated by human judges, with high reliability (Passonneau, 2004). The proposal is thus to couple an NLG module to a resolution system, and use the scores obtained by 1. distance-based metrics, by extrapolating the “quality” of a system’s output from its distance to the samples of the desired output; 2. task-based metrics, by measuring either the performance of a human using the system’s output to accomplish a given task, or the performance of another NLP system using the NLG output, provided a simple quality metric exists for this second system. 3 Task-based Evaluation: Combining NLG with Reference Resolution Evaluating NLG like MT and Summarization Evalua"
2007.mtsummit-ucnlg.10,W04-0710,1,0.812703,"d include one or more samples of the desired output for each input. From here, it is possible to use: 4 The design of an NLG STEC based on referring expressions (REs) need not however be limited to distance-based evaluation metrics. An idea is to observe that generating REs is the converse task of “solving REs”, which can mean two things. Coreference resolution deals with the grouping of the REs from a text which refer to the same entities (Hirschman, 1997), while reference resolution aims at constructing links between each RE and the (computer representation of the) entity that it refers to (Popescu-Belis and Lalanne, 2004). Reliable evaluation metrics exist for both tasks (Vilain et al., 1995; Popescu-Belis and Robba, 1998; Popescu-Belis et al., 2004), and they are expressed as a distance to the correct distribution of REs that can be easily annotated by human judges, with high reliability (Passonneau, 2004). The proposal is thus to couple an NLG module to a resolution system, and use the scores obtained by 1. distance-based metrics, by extrapolating the “quality” of a system’s output from its distance to the samples of the desired output; 2. task-based metrics, by measuring either the performance of a human us"
2007.mtsummit-ucnlg.10,popescu-belis-etal-2004-online,1,0.827945,"on referring expressions (REs) need not however be limited to distance-based evaluation metrics. An idea is to observe that generating REs is the converse task of “solving REs”, which can mean two things. Coreference resolution deals with the grouping of the REs from a text which refer to the same entities (Hirschman, 1997), while reference resolution aims at constructing links between each RE and the (computer representation of the) entity that it refers to (Popescu-Belis and Lalanne, 2004). Reliable evaluation metrics exist for both tasks (Vilain et al., 1995; Popescu-Belis and Robba, 1998; Popescu-Belis et al., 2004), and they are expressed as a distance to the correct distribution of REs that can be easily annotated by human judges, with high reliability (Passonneau, 2004). The proposal is thus to couple an NLG module to a resolution system, and use the scores obtained by 1. distance-based metrics, by extrapolating the “quality” of a system’s output from its distance to the samples of the desired output; 2. task-based metrics, by measuring either the performance of a human using the system’s output to accomplish a given task, or the performance of another NLP system using the NLG output, provided a simpl"
2007.mtsummit-ucnlg.10,W06-1421,0,0.0188938,"AGI, with I for ‘interactive’). Type A systems typically produce some form of Defining shared-task evaluation campaigns (STECs) is often the key to making progress in a particular domain, thanks to the convergence of several research teams. However, the definition of STECs requires an acceptable agreement, among a community of researchers, on the relevance of the selected problem to the domain, as well as on common evaluation metrics that indicate progress on this task. In the domain of Natural Language Generation (NLG), recent proposals have started meeting the challenge of STEC definition (Belz and Kilgarriff, 2006), few years after a new metric for Machine Translation (MT) evaluation (Papineni et al., 2001) had revived the interest for common evaluations, thanks to its low application costs, which in turn led to significant improvement of MT systems, and especially statistical ones. So, an important question is: how could NLG benefit from a similarly innovative metric, and how could such a metric be found? This short paper offers an explanation of the difficulty to evaluate NLG systems based on a typology 66 2006)1 . The distance between generated sentences or expressions can be computed using n-gram si"
2007.mtsummit-ucnlg.10,E06-1040,0,0.0238519,"sed on a typology 66 2006)1 . The distance between generated sentences or expressions can be computed using n-gram similarity, word error rate, or other techniques. Depending however on the type of input data selected for a STEC in NLG, it is quite likely that distance-based evaluation metrics are not finegrained enough to capture significant differences between the outputs of two NLG systems, especially at the sentence or sub-sentence level—in particular because distance-based metrics need a large amount of data to stabilize their scores. The GRE task proposed for the 2007 UCNLG+MT Workshop (Belz and Reiter, 2006; Gatt, 2007) focussed on the generation of referring expressions, or rather on the optimal selection of descriptive attributes from the logic-based description of a set of referents. Each candidate solution was compared to a set of solutions elicited from human judges—such a comparison follows the distance-based metrics mentioned above. This potentially successful STEC is nevertheless limited by the specificity of the input data, and by the cost of eliciting reference responses from human judges. annotation of linguistic input data. Even if the correct annotation of some reference data set ca"
2007.mtsummit-ucnlg.10,J98-2013,0,0.0638425,"Missing"
2007.mtsummit-ucnlg.10,E06-1032,0,0.0865685,"Missing"
2007.mtsummit-ucnlg.10,M95-1005,0,0.0483111,"possible to use: 4 The design of an NLG STEC based on referring expressions (REs) need not however be limited to distance-based evaluation metrics. An idea is to observe that generating REs is the converse task of “solving REs”, which can mean two things. Coreference resolution deals with the grouping of the REs from a text which refer to the same entities (Hirschman, 1997), while reference resolution aims at constructing links between each RE and the (computer representation of the) entity that it refers to (Popescu-Belis and Lalanne, 2004). Reliable evaluation metrics exist for both tasks (Vilain et al., 1995; Popescu-Belis and Robba, 1998; Popescu-Belis et al., 2004), and they are expressed as a distance to the correct distribution of REs that can be easily annotated by human judges, with high reliability (Passonneau, 2004). The proposal is thus to couple an NLG module to a resolution system, and use the scores obtained by 1. distance-based metrics, by extrapolating the “quality” of a system’s output from its distance to the samples of the desired output; 2. task-based metrics, by measuring either the performance of a human using the system’s output to accomplish a given task, or the performance"
2007.sigdial-1.3,W98-0301,0,0.0537027,"Missing"
2007.sigdial-1.3,J97-1005,0,0.0922888,"Missing"
2007.sigdial-1.3,J96-2004,0,0.0601535,"een as the retrieval of the DMs among all occurrences of a lexical item, then recall (r), precision (p) and their f-measure (f ) can be used to assess performance in a more detailed manner. However, given that the distribution of DM vs. non-DM occurrences of a lexical item is seldom uniform, the above metrics should be corrected for chance agreement. To our knowledge, there are no widely used chance-corrected versions of recall and precision—the Kullback-Leibler divergence is seldom used for classification tasks—but a wellknown agreement metric that is chance-corrected is the kappa (κ) score (Carletta, 1996). Although designed to measure inter-annotator agreement, κ quantifies the resemblance of two classifications by factoring out agreement by chance. The κ score measures classification performance between −1 and 1, with random classification scoring 0. The κ measure is quite strict as it was designed to be sensitive to even small differences between coders. Therefore, a κ value above 0.67 is often considered a sign of acceptable agreement, while a value above 0.8 is considered very significant. According to Landis and Koch (1977), strength of agreement is fair for 0.2 &lt; κ ≤ 0.4, moderate for 0."
2007.sigdial-1.3,J86-3001,0,0.491416,"Missing"
2007.sigdial-1.3,J99-4003,0,0.0743889,"Missing"
2007.sigdial-1.3,J93-3003,0,0.216016,"Missing"
2007.sigdial-1.3,P04-1087,0,0.0239208,"Missing"
2007.sigdial-1.3,W04-2319,0,0.02669,"Missing"
2007.sigdial-1.3,W04-2313,1,0.810253,"Missing"
2007.tmi-papers.8,P04-1079,0,0.0219652,"corpus size and of the selection procedure for bootstrapping (low vs. high scores) are also examined. 1 Introduction One of the design principles of automatic MT evaluation metrics is that their scores must “correlate” with a reliable measure of translation quality, generally estimated by human judges. Indeed, the claim that an automatic scoring procedure applied to MT output can provide an accurate view of translation quality must be substantiated by a proof that the scores do reflect genuine quality, as perceived by human users of a translation. For instance, the proponents of BLEU or WNM (Babych and Hartley, 2004; Papineni et al., 2001) have compared the scores produced by their metrics – which compare n-grams of MT-generated sentences with one or more reference translations produced by humans – with adequacy and fluency scores assigned by human judges. It is not, of course, that all metrics of translation quality must be correlated. Although adequacy (i.e. fidelity or “semantic correctness”) and fluency (acceptability as a valid sample of the target language) do seem correlated to some extent (White, 2001), one can easily imagine MT output with high fluency but low adequacy. However, an automatic MT"
2007.tmi-papers.8,E06-1032,0,0.0449959,"Missing"
2007.tmi-papers.8,papineni-2002-machine,0,0.0302442,"d for automatic vs. human metrics. The experiments also show that correlation varies with sample size, as well as with the subset of sentences that is considered (low vs. high quality). Samples from the two CESTA runs indicate however that correlations do not vary significantly with a different translation domain. 2 Correlation between MT Evaluation Metrics in Previous Experiments Many authors report on the correlation between human and automated metrics: some working at the sentence level (Kulesza and Shieber, 2004; Russo-Lassner et al., 2005), and some at the corpus level (Doddington, 2002; Papineni, 2002), in a variety of approaches and setups. Recent experiments, for instance, report that the correlation of the well-known BLEU metric with metrics applied by humans is not always as high as previously reported (Callison-Burch et al., 2006). In this section, we analyze three recent contributions that illustrate clearly the variety of methodologies used to compute correlations between metrics. 2.1 An Experiment with the Europarl Corpus Koehn and Monz (2006) describe the competition organized during the Statistical MT Workshop at NAACL 2006. Its main goal was to establish baseline performance of M"
2007.tmi-papers.8,1993.eamt-1.1,0,0.310349,"t of one system and that the results obtained are specific to that system. The disadvantage is of course, that direct comparison with standard cross-system correlation is not possible, since we only consider one system at a time. Therefore, this method can be used to estimate the correlation of metrics as the result of evaluating one system only, and can include of course any kind of metrics, human and automatic, in the analysis. 3.1 Bootstrapping Samples of Scores Bootstrapping is a statistical technique that is used to study the distribution of a variable based on an existing set of values (Efron and Tibshirani, 1993). This is done by randomly resampling with replacement (i.e. allowing repetition of the values) from the full existing sample and computing the desired parameters of the distribution of the samples. The method has the practical advantage of being easy to implement and the theoretical advantage of not presupposing anything about the underlying distribution of the variable. A simple programming routine can thus calculate the estimators of the mean, variance, etc., of any random variable distribution. Moreover, when the original sample is resampled a large number of times, the law of large number"
2007.tmi-papers.8,2007.mtsummit-papers.23,1,0.682639,"fluency bootstrapped scores using 5 documents Regarding the change in the size of the test data, the correlations (excluding adequacy vs. fluency) for S2 systematically increase when using 15 documents with respect to 5. However, this is less clear for S5: the correlation of NIST 59 with all other metrics increases, BLEU vs. WER/PER remains stable, but the correlations between automatic metrics and the human ones decrease, quite considerably in some cases, e.g. BLEU vs. fluency. This is probably due to the particular documents selected, since scores vary more on small test sets, as shown in (Estrella et al., 2007). A graphical representation of the scores appears in Figures 2 to 5, which plot two scores for each of the 1,500 bootstrapped samples, for systems S2 (light/green) and S5 (dark/blue). Figure 2 illustrates two metrics that are highly correlated, BLEU and WER: the clouds of dots are organized along a line, which has negative slope as S5 WER PER BLEU NIST ADE FLU PREC REC WER 1 PER 0.93 1 BLEU -0.90 -0.89 1 NIST -0.69 -0.76 0.83 1 lower WER corresponds to higher BLEU (and to better performance, in principle). The correlation coefficients for the samples in Figure 2 are respectively -0.83 and -0."
2007.tmi-papers.8,W04-3250,0,0.310123,"bability 1/N). 2. The resulting population P*, noted X* = (X1*, …, XN*), constitutes the N bootstrapped samples. 3. If the original estimator of a given population parameter was θ(X), with the bootstrapped samples we can calculate the same estimator as θ(X*). An important parameter for bootstrapping is N, the number of bootstrapped samples, i.e. the number of times the process is repeated. This number should be large enough to build a representative number of samples. It appears that, for instance, N = 200 leads to slightly biased estimations (Efron and Gong, 1983; Efron and Tibshirani, 1993; Koehn, 2004; Zhang et al., 2004, so N ~ 1,000 is preferred, for example N = 1,000 ) or even N = 10,000 (Bisani and Ney, 2004). Based on these examples, we decided to use N = 1,500 bootstrapped samples. correlations will be studied for each system, i.e. they are calculated on a per system basis as opposed to the common cross-system correlation. Since correlation concerns two sets of scores, we need to apply the metrics simultaneously to the same bootstrapped samples to keep consistency in the scores. Put in simpler words, we apply two (or more) different metrics to the same random sample per iteration of"
2007.tmi-papers.8,2001.mtsummit-papers.68,0,0.0305954,"lection procedure for bootstrapping (low vs. high scores) are also examined. 1 Introduction One of the design principles of automatic MT evaluation metrics is that their scores must “correlate” with a reliable measure of translation quality, generally estimated by human judges. Indeed, the claim that an automatic scoring procedure applied to MT output can provide an accurate view of translation quality must be substantiated by a proof that the scores do reflect genuine quality, as perceived by human users of a translation. For instance, the proponents of BLEU or WNM (Babych and Hartley, 2004; Papineni et al., 2001) have compared the scores produced by their metrics – which compare n-grams of MT-generated sentences with one or more reference translations produced by humans – with adequacy and fluency scores assigned by human judges. It is not, of course, that all metrics of translation quality must be correlated. Although adequacy (i.e. fidelity or “semantic correctness”) and fluency (acceptability as a valid sample of the target language) do seem correlated to some extent (White, 2001), one can easily imagine MT output with high fluency but low adequacy. However, an automatic MT evaluation metric should"
2007.tmi-papers.8,2003.mtsummit-papers.51,0,0.0361955,"participated in the ENFR first run, both commercial and research ones. For the second run, the goal was to improve the evaluation protocols used in the first run and to observe the impact of system adaptation to a particular domain. Therefore, the medical domain was chosen, using data collected from the Santé Canada website, with a total of 288 segments and an average of 22 words per segment. Almost the same systems participated in the second run. In addition to the automatic metrics used in the CESTA campaign, we included in our experiment precision and recall from the General Text Matcher (Turian et al., 2003). 5 Experimental Study of Correlation Although we performed the study using all the systems participating in the CESTA campaign, we will only present here the results of two systems, namely S2 and S5, chosen among the best. In Section 5.1, we compute correlations between metrics on two test sets of dissimilar size, in Section 5.2 we study the correlations for segments of very high and very low adequacy scores and, finally, in Section 5.3 we present the results of the correlations for a test set of a different domain. 5.1 As we expected, there is a relatively high correlation between metrics of"
2007.tmi-papers.8,zhang-etal-2004-interpreting,0,0.0445845,"Missing"
2007.tmi-papers.8,2005.mtsummit-papers.11,0,0.00621393,"ell-known BLEU metric with metrics applied by humans is not always as high as previously reported (Callison-Burch et al., 2006). In this section, we analyze three recent contributions that illustrate clearly the variety of methodologies used to compute correlations between metrics. 2.1 An Experiment with the Europarl Corpus Koehn and Monz (2006) describe the competition organized during the Statistical MT Workshop at NAACL 2006. Its main goal was to establish baseline performance of MT evaluation for specific training scenarios. The test corpus consisted of sentences from the Europarl corpus (Koehn, 2005) and from editorials of the Project Syndicate website, and contained a total of 3,064 sentences. The translation directions were SP↔EN, FR↔EN, DE↔EN and there were 14 participating systems. The BLEU metric was used for automatic evaluation, as the most commonly used metric in the MT community. To provide human quality judgments, the workshop participants had to assess 300–400 sentences each, in terms of adequacy and fluency, on a 5-point scale. Each evaluator was in fact simultaneously given 5 machine translations, one reference translation, and one source sentence, and was asked to perform a"
2007.tmi-papers.8,2004.tmi-1.8,0,0.0131124,"ents (Section 5) analyze the correlation between metrics and show that correlation is lower than expected for automatic vs. human metrics. The experiments also show that correlation varies with sample size, as well as with the subset of sentences that is considered (low vs. high quality). Samples from the two CESTA runs indicate however that correlations do not vary significantly with a different translation domain. 2 Correlation between MT Evaluation Metrics in Previous Experiments Many authors report on the correlation between human and automated metrics: some working at the sentence level (Kulesza and Shieber, 2004; Russo-Lassner et al., 2005), and some at the corpus level (Doddington, 2002; Papineni, 2002), in a variety of approaches and setups. Recent experiments, for instance, report that the correlation of the well-known BLEU metric with metrics applied by humans is not always as high as previously reported (Callison-Burch et al., 2006). In this section, we analyze three recent contributions that illustrate clearly the variety of methodologies used to compute correlations between metrics. 2.1 An Experiment with the Europarl Corpus Koehn and Monz (2006) describe the competition organized during the S"
2007.tmi-papers.8,N04-1022,0,0.0138048,"otstrapped sample: for(n=0; n&lt;N; n++){ for(m=0; m&lt;M; m++){ sample[m] = selectRandSeg(); } scoresA[n]=calcMetricA(sample*); scoresB[n]=calcMetricB(sample*); } calcCorrelation(scoresA, scoresB); 4 Figure 1. Example of histogram for the WER scores obtained with 1,500 bootstrapped samples (CESTA scores, first run, system S2) 3.2 Application to MT Evaluation Scores In the MT field, bootstrapping has been mainly used to estimate confidence intervals for automatic metrics and to compute the statistical significance of comparative performance of different MT systems, e.g. using the BLEU (Koehn, 2004; Kumar and Byrne, 2004; Zhang et al., 2004) or WER metric (Bisani and Ney, 2004). Here, bootstrapping will be used to compute the correlation between metrics for MT. These Evaluation Resources: Data, Systems and Metrics For the experiments presented here, we used the resources of the ENFR translation task in the CESTA MT evaluation campaign (Hamon et al., 2006). In all cases, the results of the participating systems are anonymized, therefore the systems will simply be referred to by the codes S1 to S5 in no particular order. One of the goals of the first run was to validate the use of automatic evaluation metrics"
2007.tmi-papers.8,niessen-etal-2000-evaluation,0,0.243505,"Missing"
2007.tmi-papers.8,P02-1040,0,\N,Missing
2007.tmi-papers.8,hamon-etal-2006-cesta,0,\N,Missing
2012.amta-caas14.1,2005.mtsummit-papers.11,0,0.015937,"Missing"
2012.amta-caas14.1,W12-0117,1,0.849868,"d, depending on the target language, these senses can be translated by different connectives. In other cases, a connective may be translated by a different construction (reformulation) or even be skipped in translation. We consider here seven frequent English discourse connectives: although, even though, meanwhile, since, though, while, and yet. Previous studies have shown that it is possible to disambiguate their main senses automatically with acceptable accuracy (Pitler and Nenkova 2009), and that the sense labels can be used by machine translation (MT) systems to improve their translation (Meyer and Popescu-Belis 2012). For instance, when translating from English to French, a statistical MT (SMT) system can use parallel corpora with labelled connectives to learn correct translations based on labels. One issue with such experiments is the capacity to measure the translation improvement due to the correct translation of connectives, for instance by focussing only on these lexical items. In this paper, we explore the translation of the seven above-mentioned English discourse connectives into Arabic. We study to what extent the ambiguities of these connectives are reduced (or not) by translation into Arabic, i."
2012.amta-caas14.1,W11-2022,1,0.904761,"Missing"
2012.amta-caas14.1,P00-1056,0,0.0711825,"Missing"
2012.amta-caas14.1,al-saif-markert-2010-leeds,0,0.0510196,"Missing"
2012.amta-caas14.1,W05-0909,0,0.222115,"Missing"
2012.amta-caas14.1,P05-1071,0,0.0359807,"ives, we used an automatic method based on alignment between sentences at the word level using GIZA++ (Och and Ney, 2000). We experimented with the large UN parallel corpus to find out the Arabic connectives that are aligned to English ones, a corpus of journal articles and news: • English: 1.2 GB of data, with 7.1 million 3 sentences and 182 million words. Arabic: 1.7 GB of data, with 7.1 million of sentences and 154 million words. For the alignment task, the data was pre-processed as follows: • English: tokenisation and lowercase. • Arabic: word transliteration, and segmentation using MADA (Habash and Rambow, 2005). • 2.3 Statistics for Connective Dictionaries Using the automatic alignment method described above, we extracted the word alignment on the Arabic side given the English one. The following tables (Table 1 to Table 7) show the correspondences between each English connective and Arabic translations detected automatically using the projection from English sentences to Arabic ones. Because word alignment is not perfect, we observe that the result is not always an Arabic connective, though it generally includes one. The main observation is that the obtained vocabulary is limited around more or less"
2012.amta-caas14.1,P09-2004,0,0.102522,", if the target connective conveys an unintended discourse relation. For instance, since can have a causal or a temporal sense, and, depending on the target language, these senses can be translated by different connectives. In other cases, a connective may be translated by a different construction (reformulation) or even be skipped in translation. We consider here seven frequent English discourse connectives: although, even though, meanwhile, since, though, while, and yet. Previous studies have shown that it is possible to disambiguate their main senses automatically with acceptable accuracy (Pitler and Nenkova 2009), and that the sense labels can be used by machine translation (MT) systems to improve their translation (Meyer and Popescu-Belis 2012). For instance, when translating from English to French, a statistical MT (SMT) system can use parallel corpora with labelled connectives to learn correct translations based on labels. One issue with such experiments is the capacity to measure the translation improvement due to the correct translation of connectives, for instance by focussing only on these lexical items. In this paper, we explore the translation of the seven above-mentioned English discourse co"
2012.amta-caas14.1,prasad-etal-2008-penn,0,0.0289658,"aning which can be translated in Arabic by “Alrgm” ”الظرغظم, or “rgm ”رغظم. As the translation of an English connective to Arabic varies depending on the intended discourse relation, an MT system that is capable to modulate the translation accordingly should avoid mistakes observed with current systems. Consequently, the MT evaluation should also take into account the acceptable senses of the connectives. 2 2.2 2.1 Translations of English Connectives into Arabic Discourse Ambiguity of Discourse Connectives The manual annotation of discourse relations in the Penn Discourse Treebank (PDTB) (Prasad et al., 2008) has provided a discourse-layer annotation over the Wall Street Journal Corpus. The annotation targeted either explicit discourse relations (18,459 connectives) or implicit ones (16,053 relations). The sense labels started from top-level senses (temporal, contingency, comparison, and expansion), with 16 subtypes on the second level and 23 subsubtypes on the third level. In (Al-Saif and Markert, 2010) a manual annotation of Arabic discourse connective has been performed and should be soon available. However, the published material is not explicit about the observed level of ambiguity of Arabic"
2012.amta-caas14.1,2012.amta-papers.20,1,\N,Missing
2012.amta-caas14.1,P02-1040,0,\N,Missing
2012.amta-caas14.1,W10-1737,0,\N,Missing
2012.amta-papers.20,D07-1007,0,0.0184342,"ting WSD with MT raises decoding problems (due to the larger search space) which do not apply to discourse connectives. Moreover, the criteria used to perform WSD vs. connective labeling are quite different: some WSD methods rely on local criteria that could be learned by phrase-based SMT models, or on global text-level topics (Eidelman et al., 2012), while connective labeling requires more structured and longer-range information. Insights from linguistics also indicate that the modeling of content word senses differs considerably from the modeling of the procedural meaning of function words. Carpuat and Wu (2007) have used the translation candidates output by a baseline SMT system as word sense labels. Then, the output of several classifiers based on linguistic features was weighed against the translation candidates output by the baseline SMT system. Therefore, integration of MT and WSD amounted to postprocessing of MT, while in the present proposal, connective labeling amounts to preprocessing. The WSD+SMT system of Carpuat and Wu (2007) improved BLEU scores by 0.4–0.5 for English/Chinese translation. As for attempts to couple function word disambiguation with SMT, as intended here, these are still i"
2012.amta-papers.20,W09-0436,0,0.0515253,"the translation candidates output by a baseline SMT system as word sense labels. Then, the output of several classifiers based on linguistic features was weighed against the translation candidates output by the baseline SMT system. Therefore, integration of MT and WSD amounted to postprocessing of MT, while in the present proposal, connective labeling amounts to preprocessing. The WSD+SMT system of Carpuat and Wu (2007) improved BLEU scores by 0.4–0.5 for English/Chinese translation. As for attempts to couple function word disambiguation with SMT, as intended here, these are still infrequent. Chang et al. (2009) disambiguated the Chinese particle ‘DE’ which has five different context-dependent usages (modifier, preposition, relative clause, etc.). When the linguisticallyinformed LogLinear classifier was used to label the particle prior to SMT, the translation quality was improved by up to 1.49 BLEU score for phrase-based Chinese/English translation. Similarly, Ma et al. (2011) proposed a Maximum Entropy model to annotate English collocational particles (e.g. come down/by, turn against, inform of ) with more specific labels than a standard POS tagger would output, i.e. only one label for all such part"
2012.amta-papers.20,P10-4002,0,0.0110904,"Missing"
2012.amta-papers.20,P12-2023,0,0.0132336,"se disambiguation (WSD) for functional words, for which several solutions have been studied. The main difference is that WSD concerns potentially all content words from a sentence, while connective labels are sparse, rarely more than one per sentence. Therefore, integrating WSD with MT raises decoding problems (due to the larger search space) which do not apply to discourse connectives. Moreover, the criteria used to perform WSD vs. connective labeling are quite different: some WSD methods rely on local criteria that could be learned by phrase-based SMT models, or on global text-level topics (Eidelman et al., 2012), while connective labeling requires more structured and longer-range information. Insights from linguistics also indicate that the modeling of content word senses differs considerably from the modeling of the procedural meaning of function words. Carpuat and Wu (2007) have used the translation candidates output by a baseline SMT system as word sense labels. Then, the output of several classifiers based on linguistic features was weighed against the translation candidates output by the baseline SMT system. Therefore, integration of MT and WSD amounted to postprocessing of MT, while in the pres"
2012.amta-papers.20,E12-3001,0,0.14856,"were only applied to the source side, the factored models did not conclusively improve German/English translation. Recently, Wang et al. (2012) have shown improvements for BLEU and manual evaluation for Bulgarian/English translation when using as factors POS, lemmas, dependency parsing, and minimal recursion semantics supertags. 3.2 Text-level Models Several ad-hoc solutions for adding text-level information to SMT models have been designed for various discourse phenomena. Several methods have been proposed to constrain pronoun choice (Hardmeier and Federico, 2010; Le Nagard and Koehn, 2010; Guillou, 2012), relying on knowledge of their antecedent, which was quite imperfect due to anaphora resolution errors. We presented two elementary methods for integrating labeled discourse connectives into MT in an earlier paper (Meyer and Popescu-Belis, 2012): phrase table modification with discourse labels, and concatenation of labels to the tokens at training and testing time. A text-level decoder for SMT was recently introduced by Hardmeier et al. (2012). Few evaluation metrics assess the coherence of translations across sentences of a text, and as a result the precise quantification of this type of pro"
2012.amta-papers.20,2010.iwslt-papers.10,0,0.490248,"for Dutch/English translation. However, when the factors were only applied to the source side, the factored models did not conclusively improve German/English translation. Recently, Wang et al. (2012) have shown improvements for BLEU and manual evaluation for Bulgarian/English translation when using as factors POS, lemmas, dependency parsing, and minimal recursion semantics supertags. 3.2 Text-level Models Several ad-hoc solutions for adding text-level information to SMT models have been designed for various discourse phenomena. Several methods have been proposed to constrain pronoun choice (Hardmeier and Federico, 2010; Le Nagard and Koehn, 2010; Guillou, 2012), relying on knowledge of their antecedent, which was quite imperfect due to anaphora resolution errors. We presented two elementary methods for integrating labeled discourse connectives into MT in an earlier paper (Meyer and Popescu-Belis, 2012): phrase table modification with discourse labels, and concatenation of labels to the tokens at training and testing time. A text-level decoder for SMT was recently introduced by Hardmeier et al. (2012). Few evaluation metrics assess the coherence of translations across sentences of a text, and as a result the"
2012.amta-papers.20,D12-1108,0,0.041495,"signed for various discourse phenomena. Several methods have been proposed to constrain pronoun choice (Hardmeier and Federico, 2010; Le Nagard and Koehn, 2010; Guillou, 2012), relying on knowledge of their antecedent, which was quite imperfect due to anaphora resolution errors. We presented two elementary methods for integrating labeled discourse connectives into MT in an earlier paper (Meyer and Popescu-Belis, 2012): phrase table modification with discourse labels, and concatenation of labels to the tokens at training and testing time. A text-level decoder for SMT was recently introduced by Hardmeier et al. (2012). Few evaluation metrics assess the coherence of translations across sentences of a text, and as a result the precise quantification of this type of problems is still missing. The FEMTI guidelines for MT evaluation (Hovy et al., 2003) highlight two attributes related to text-level relations: coherence (“the degree to which the reader can describe the role of each individual sentence or group of sentences with respect to the text as a whole”) and cohesion (“lexical chains and other elements, e.g. anaphora or ellipsis, that link individual units across sentences”). Recently, a proposal to incorp"
2012.amta-papers.20,D07-1091,0,0.0688196,"vey an erroneous argumentation. 3 State of the Art and Related Work The main statistical MT paradigm, based on decoding of source sentences, i.e. maximizing the observation probability given a translation model and a language model, is essentially phrase-based or sentence-based, and cannot model longer-range dependencies, in particular across sentences. To overcome this limitation, factored translation models have been proposed as a general way to make use of external knowledge, along with various other specific solutions for text-level MT. 3.1 Factored SMT Models Factored translation models (Koehn and Hoang, 2007), implemented in the Moses and cdec SMT toolkits (Koehn et al., 2007; Dyer et al., 2010), allow one to factor in arbitrary linguistic labels – such as morphological, syntactic, or even semantic or discourse ones – while building translation models. These models combine features in a log-linear way, and are most often used to integrate morphological information, for instance when translating to a morphologically rich language. Augmenting current hierarchical, syntax-based translation models by using semantic labels adjoined to syntactic ones has recently been studied by Baker et al. (2012). The"
2012.amta-papers.20,P07-2045,0,0.00714783,"Missing"
2012.amta-papers.20,2005.mtsummit-papers.11,0,0.0315468,"Missing"
2012.amta-papers.20,W10-1737,0,0.395622,"Missing"
2012.amta-papers.20,2011.mtsummit-papers.4,0,0.0431665,"ing. The WSD+SMT system of Carpuat and Wu (2007) improved BLEU scores by 0.4–0.5 for English/Chinese translation. As for attempts to couple function word disambiguation with SMT, as intended here, these are still infrequent. Chang et al. (2009) disambiguated the Chinese particle ‘DE’ which has five different context-dependent usages (modifier, preposition, relative clause, etc.). When the linguisticallyinformed LogLinear classifier was used to label the particle prior to SMT, the translation quality was improved by up to 1.49 BLEU score for phrase-based Chinese/English translation. Similarly, Ma et al. (2011) proposed a Maximum Entropy model to annotate English collocational particles (e.g. come down/by, turn against, inform of ) with more specific labels than a standard POS tagger would output, i.e. only one label for all such particles. Such a tagger could, as the authors suggest, be useful for English/Chinese translation, but there are no experiments so far on coupling it with an actual SMT system. 3.4 Classifiers for Discourse Connectives In an early proposal, Marcu (2000) suggested to couple a discourse parser with an MT system to improve Japanese/English translation. However, the paper only"
2012.amta-papers.20,N03-5008,0,0.0332142,"Missing"
2012.amta-papers.20,A00-2002,0,0.766319,"Missing"
2012.amta-papers.20,W12-0117,1,0.442071,"ion when using as factors POS, lemmas, dependency parsing, and minimal recursion semantics supertags. 3.2 Text-level Models Several ad-hoc solutions for adding text-level information to SMT models have been designed for various discourse phenomena. Several methods have been proposed to constrain pronoun choice (Hardmeier and Federico, 2010; Le Nagard and Koehn, 2010; Guillou, 2012), relying on knowledge of their antecedent, which was quite imperfect due to anaphora resolution errors. We presented two elementary methods for integrating labeled discourse connectives into MT in an earlier paper (Meyer and Popescu-Belis, 2012): phrase table modification with discourse labels, and concatenation of labels to the tokens at training and testing time. A text-level decoder for SMT was recently introduced by Hardmeier et al. (2012). Few evaluation metrics assess the coherence of translations across sentences of a text, and as a result the precise quantification of this type of problems is still missing. The FEMTI guidelines for MT evaluation (Hovy et al., 2003) highlight two attributes related to text-level relations: coherence (“the degree to which the reader can describe the role of each individual sentence or group of"
2012.amta-papers.20,J03-1002,0,0.00296922,"nective Translation). For each occurrence of a connective in a source sentence, ACT examines how it is translated in a reference vs. a candidate (SMT) translation. If the two translations are identical or equivalent, ACT counts one point, and zero otherwise. The ACT score is the total number of points divided by the number of source connectives (see Eq. 1–3). Given an English connective in a source sentence, its translation is spotted using a dictionary of possible translations, plus information from the automatic alignment of the source and target sentences using a pre-trained GIZA++ system (Och and Ney, 2003) – which, even when not perfect, allows us to discriminate between possible candidates. But the procedure does sometimes fail, when a translation is not included in the dictionary, or when a connective is not explicitly translated. A key point of the ACT metric is the use of a dictionary of equivalents to spot acceptable variations of connectives in translation. For each sense of each connective, the dictionary contains a list of acceptable translations, from a conservative point of view, i.e. limited to the closest possible ones. The dictionary was built using linguistic knowledge about conne"
2012.amta-papers.20,P03-1021,0,0.0955722,"Missing"
2012.amta-papers.20,P02-1040,0,0.104119,"Missing"
2012.amta-papers.20,P09-2004,0,0.0162633,"Missing"
2012.amta-papers.20,prasad-etal-2008-penn,0,0.00936211,"Missing"
2012.amta-papers.20,N03-1033,0,0.00576148,"bp to|to cooperate|vb ,|, while|in-contrast those|dt who|wp are|vbp not|rb willing|jj can|md stand|vb off|rp .|. 4. for|null the|null first|null time|null it|null was|null said|null that|null the|null countries|null who|null want|null are|null to|null cooperate|null ,|null while|contrast those|null who|null are|null not|null willing|null can|null stand|null off|null .|null Figure 1: Example sentence for factored translation models: (1) plain text, (2) POS tags as factors, (3) POS tags combined with discourse labels (DL), and (4) DL only. The POS tags were generated by the Stanford POS tagger (Toutanova et al., 2003), with the bidirectional-distsim-WSJ model. The target language text could be factored as well. However, as our annotation of discourse relations is monolingual only, we focus on source-side factors. For building the translation models and for MERT tuning, both the English source word and the factor information – either POS, POS+DL, or DL, thus corresponding to three different MT systems – is used to generate the surface French target word forms. As a consequence, all data (training, tuning and test) has to be factored in the same way. We built factored translation models using labels output b"
2012.amta-papers.20,C08-3012,0,0.0513117,"Missing"
2012.amta-papers.20,1983.tc-1.13,0,0.725018,"Missing"
2012.amta-papers.20,D12-1097,0,0.0276786,"a text, and as a result the precise quantification of this type of problems is still missing. The FEMTI guidelines for MT evaluation (Hovy et al., 2003) highlight two attributes related to text-level relations: coherence (“the degree to which the reader can describe the role of each individual sentence or group of sentences with respect to the text as a whole”) and cohesion (“lexical chains and other elements, e.g. anaphora or ellipsis, that link individual units across sentences”). Recently, a proposal to incorporate lexical cohesion into automated MT evaluation metrics has been put forward (Wong and Kit, 2012). 3.3 Discourse Connectives vs. Word Sense Disambiguation The disambiguation of senses signaled by discourse connectives might seem to be a specific case of word sense disambiguation (WSD) for functional words, for which several solutions have been studied. The main difference is that WSD concerns potentially all content words from a sentence, while connective labels are sparse, rarely more than one per sentence. Therefore, integrating WSD with MT raises decoding problems (due to the larger search space) which do not apply to discourse connectives. Moreover, the criteria used to perform WSD vs"
2012.amta-papers.20,W11-2022,1,\N,Missing
2012.amta-papers.20,W12-0116,0,\N,Missing
2012.amta-papers.20,J12-2006,0,\N,Missing
2012.amta-papers.20,W07-0702,0,\N,Missing
2020.isa-1.7,C18-1178,0,0.0230618,"hed The Dishwasher: Dead Samurai?”, which is a forward one. However, from the triple (‘Rampage’, ‘publisher’, ‘Midway Games’), someone wrote the question “What game is published by Midway Games?”, which is a backward one. One reason to consider this distinction is that predicates do not appear in both active and passive forms in the triple store, so questions are allowed to bear on the subject or or object of a triple. Due to the termination of Freebase, the triples of a subset of questions from SimpleQuestions have been converted to DBpedia triples, resulting in the SimpleDBpediaQA data set (Azmy et al., 2018).5 Two formatted questions from this data set are presented in Table 1. ‘Query’ is the original question formulated over a Freebase triple, whose former predicate URL is given under ‘Freebase Predicate’. ‘Subject’ points to the URL of the concept on DBpedia. There are three subfields under ‘predicate list’: the DBpedia URL of the predicate, the direction of the question (forward or backward), and a constraint on the expected answer type for backward questions. A subset of SimpleQuestions different from the one above has been converted to Wikidata triples, resulting in the SimpleQuestionsWikida"
2020.isa-1.7,J15-1001,0,0.025555,"ocardial infarction? ‘father’ and ‘mother’ where DBpedia has only ‘parent’). Another qualitative observation is that SimpleDBpediaQA contains a somewhat larger proportion of triples that are not useful for question generation, as they correspond to various numeric identifiers of entities in 3rd party repositories. 3. 3.1. Motivation for ForwardQuestions DBpedia or Wikidata triples represent only small subsets of the knowledge embodied in Wikipedia, which is why it may seem that generating questions directly from Wikipedia sentences could lead to more varied questions (Heilman and Smith, 2010; Chali and Hasan, 2015). However, our pilot experiments in this direction pointed to strong limitations. For instance, we considered identifying patterns such as verb + named entity in sentences from Wikipedia, and then reversing them to build a question, e.g. from “World War II ended in 1945” we aimed to derive “When did World War II end?” However, several difficulties appeared: (1) the VB+NE pattern also applies to relative clauses (e.g. “Billie Joe Armstrong took two years to write American Idiot”) from which questions cannot be easily generated; (2) the interrogative word is hard to predict; (3) pronouns lead to"
2020.isa-1.7,P19-1567,0,0.0494493,"Missing"
2020.isa-1.7,D17-1090,0,0.0396153,"Missing"
2020.isa-1.7,P13-2121,0,0.00980567,"endering of the predicate ‘genre’ in the initial question is too specific and incompatible with the sense of the new triple. Alternatively, the reference to the subject in the template can also be too specific, e.g. if we use the template “What genre is the tv program &lt;placeholder&gt;?” with the triple above, we obtain the incorrect question “What genre is the tv program Claude Monet?” 4.1. Ranking with a language model We observed that some ungrammatical questions obtained high semantic compatibility scores. Our second goal is thus to filter them out, using the KenLM language modeling software (Heafield et al., 2013)12 with a language model for English provided by Zamia.13 The perplexity score of the language model for the full question provides an estimate of the well-formedness of the question, i.e. a syntactic fluency score. Therefore, for a given triple, we combine the semantic and fluency scores, giving more weight to the first one, and select the question which has the highest average score. 5. Use of Questions for a Quiz Chatbot The method for generating questions from arbitrary triples, using ForwardQuestions, can be used to build a quiz chatbot which prompts the user to select a topic. This topic"
2020.isa-1.7,N10-1086,0,0.0511706,"or’s cause of death is myocardial infarction? ‘father’ and ‘mother’ where DBpedia has only ‘parent’). Another qualitative observation is that SimpleDBpediaQA contains a somewhat larger proportion of triples that are not useful for question generation, as they correspond to various numeric identifiers of entities in 3rd party repositories. 3. 3.1. Motivation for ForwardQuestions DBpedia or Wikidata triples represent only small subsets of the knowledge embodied in Wikipedia, which is why it may seem that generating questions directly from Wikipedia sentences could lead to more varied questions (Heilman and Smith, 2010; Chali and Hasan, 2015). However, our pilot experiments in this direction pointed to strong limitations. For instance, we considered identifying patterns such as verb + named entity in sentences from Wikipedia, and then reversing them to build a question, e.g. from “World War II ended in 1945” we aimed to derive “When did World War II end?” However, several difficulties appeared: (1) the VB+NE pattern also applies to relative clauses (e.g. “Billie Joe Armstrong took two years to write American Idiot”) from which questions cannot be easily generated; (2) the interrogative word is hard to predi"
2020.isa-1.7,P18-3022,0,0.0436705,"Missing"
2020.isa-1.7,D16-1264,0,0.126803,"Missing"
2020.isa-1.7,P16-1056,0,0.0629316,"Missing"
2020.isa-1.7,D18-1427,0,0.0230783,"Missing"
2020.isa-1.7,D18-1424,0,0.029816,"Missing"
2020.isa-1.7,D19-1622,0,0.0230073,"Missing"
2020.lrec-1.672,bunt-etal-2012-iso,1,0.897688,"s trained over a small subset (the first 25 MB of a total of 25 GB) of the English part of the OpenSubtitles corpus,8 with the set of subtitles for each movie counting as one dialogue. 3.3. Dialogue Controller Our proposal for switching between the chatbot and the QA components relies on a dialogue act recognizer implemented as an utterance classifier. Its goal is to distinguish the requests for information, which are sent to the QA system, from all other utterances including greetings and chitchat, which are sent to the chatbot. Therefore, instead of a using a complex dialogue act hierarchy (Bunt et al., 2012), our controller only needs to make a binary decision, which can be carried out using context-free classifiers (Clark and Popescu-Belis, 2004). For this, it uses an utterance classifier that is trained on a labeled data set constructed by joining SQuAD questions and movie dialogues (about 10,000 lines of each type), which all come directly from the original training data of each component. 3.4. Software and Hardware Platforms We use the Google Home smart speaker as a speech-based front-end to our conversational agent. Agents called ‘Actions’ can be created on a platform dedicated to developers"
2020.lrec-1.672,2020.lrec-1.677,0,0.0513335,"Missing"
2020.lrec-1.672,P17-1171,0,0.0168621,"et al., 2003). First, they identify the document or paragraph that potentially contains the answer to the given question, typically using methods pertaining to information retrieval. Second, they extract the specific answer using NLP techniques, relying in particular on the estimation of the type of the expected answer. Interactive QA systems are more challenging to design than non-interactive (single turn) ones, as dialogue states and policies are difficult to define in the case of open questions (Rieser and Lemon, 2009). Neural systems have recently demonstrated impressive QA capabilities (Chen et al., 2017), more specifically on the second part of the QA task, i.e. extracting answers given a question and a paragraph. Recent progress has been made using BERT, i.e. Bidirectional Encoder Representations from Transformers (Devlin et al., 2018). For QA, or more exactly for answer extraction, the BERT model is post-trained with a final layer dedicated to this task, and various systems using it with various strategies, including ensemble models, have recently reached nearly human-like levels on the SQuAD benchmark (Rajpurkar et al., 2016; Rajpurkar et al., 2018).2 SQuAD 1.1 contains 100,000 questions d"
2020.lrec-1.672,W04-2328,1,0.603784,"of subtitles for each movie counting as one dialogue. 3.3. Dialogue Controller Our proposal for switching between the chatbot and the QA components relies on a dialogue act recognizer implemented as an utterance classifier. Its goal is to distinguish the requests for information, which are sent to the QA system, from all other utterances including greetings and chitchat, which are sent to the chatbot. Therefore, instead of a using a complex dialogue act hierarchy (Bunt et al., 2012), our controller only needs to make a binary decision, which can be carried out using context-free classifiers (Clark and Popescu-Belis, 2004). For this, it uses an utterance classifier that is trained on a labeled data set constructed by joining SQuAD questions and movie dialogues (about 10,000 lines of each type), which all come directly from the original training data of each component. 3.4. Software and Hardware Platforms We use the Google Home smart speaker as a speech-based front-end to our conversational agent. Agents called ‘Actions’ can be created on a platform dedicated to developers,9 which is wired to the DialogFlow platform offering several out-of-the-box NLP tools. It is also possible to connect an Action to a remote s"
2020.lrec-1.672,N19-1423,0,0.051701,"Missing"
2020.lrec-1.672,P17-1045,0,0.0234197,"es based on Partially Observable Markov Decision Processes (POMDP); such models must be trained with reinforcement learning, possibly using simulations of human users (Rieser and Lemon, 2011, Chapter 3). The advent of deep learning has opened new possibilities for end-to-end training of conversational agents, typically with neural sequence-to-sequence models (Li et al., 2016). This is mainly successful for generating fluent and plausible responses when chatting, and has been essentially applied to chatbots for non-task-oriented dialogues, such as Cleverbot.com or TockTock (Zhou et al., 2016). Dhingra et al. (2017) have proposed a solution for end-toend training of a recurrent neural network intended for taskoriented dialogue, by converting probabilities in the final 5474 layer of the network into discrete entities, and thus allowing a movie database to be queried. However, the system did not generate utterances and its task was limited to attributebased search in a data table. Several large technology companies have designed robust smart assistants and put them in production. Such assistants rely on cloud-based solutions for speech recognition and synthesis, and on undisclosed dialogue management archi"
2020.lrec-1.672,N18-5020,0,0.0570637,"Missing"
2020.lrec-1.672,D16-1127,0,0.0319739,"icated modules for language processing, dialogue modeling, and task management. This is the case, for instance, when the dialogue management component relies on explicit finitestate representations of dialogue states, with dialogue policies based on Partially Observable Markov Decision Processes (POMDP); such models must be trained with reinforcement learning, possibly using simulations of human users (Rieser and Lemon, 2011, Chapter 3). The advent of deep learning has opened new possibilities for end-to-end training of conversational agents, typically with neural sequence-to-sequence models (Li et al., 2016). This is mainly successful for generating fluent and plausible responses when chatting, and has been essentially applied to chatbots for non-task-oriented dialogues, such as Cleverbot.com or TockTock (Zhou et al., 2016). Dhingra et al. (2017) have proposed a solution for end-toend training of a recurrent neural network intended for taskoriented dialogue, by converting probabilities in the final 5474 layer of the network into discrete entities, and thus allowing a movie database to be queried. However, the system did not generate utterances and its task was limited to attributebased search in"
2020.lrec-1.672,P11-4014,1,0.796317,"Missing"
2020.lrec-1.672,D16-1264,0,0.105509,"Missing"
2020.lrec-1.672,P18-2124,0,0.0294145,"ntly demonstrated impressive QA capabilities (Chen et al., 2017), more specifically on the second part of the QA task, i.e. extracting answers given a question and a paragraph. Recent progress has been made using BERT, i.e. Bidirectional Encoder Representations from Transformers (Devlin et al., 2018). For QA, or more exactly for answer extraction, the BERT model is post-trained with a final layer dedicated to this task, and various systems using it with various strategies, including ensemble models, have recently reached nearly human-like levels on the SQuAD benchmark (Rajpurkar et al., 2016; Rajpurkar et al., 2018).2 SQuAD 1.1 contains 100,000 questions derived by human workers from about 500 paragraphs from Wikipedia. Each question is accompanied by the correct answer, and by the paragraph on which it is based. It is thus possible to test answer extraction, but also end-to-end QA by hiding the paragraphs, though this method has several limitations (discussed in Section 4.1 below). SQuAD 2.0 adds 50,000 “unanswerable” questions, which systems should tag as such. Their role is to penalize systems that would aggressively search for answers in paragraphs. Our system augments with paragraph retrieval a syst"
2020.lrec-1.672,J00-3003,0,0.381482,"as such. Their role is to penalize systems that would aggressively search for answers in paragraphs. Our system augments with paragraph retrieval a system based on BERT, and uses SQuAD scores as a criterion to tune the system’s parameters. 1 developer.amazon.com/alexa-skills-kit, wit.ai, dialogflow.com, developers.google.com/ actions. 2 rajpurkar.github.io/SQuAD-explorer/ Finally, in our system, the controller that conveys utterances either to the chatbot or to the QA component uses dialogue act recognition, which we envisage here as a simple classification task. More complex sequence models (Stolcke et al., 2000) as well as neural models (Zhao and Kawahara, 2017) have been proposed and could be used in the future to improve performance. 3. Design and Implementation The main components of our conversational agent are shown in Figure 1. The input utterance obtained from a smart speaker with speech recognition or from a text-based interface is passed to the dialogue act recognizer. This component routes task-related inputs (i.e. genuine informationseeking questions) to the QA component, and other inputs (including greetings, politeness or non-goal-directed chat) to the chatbot. The answers from the respe"
2020.lrec-1.672,I17-1071,0,0.0246191,"would aggressively search for answers in paragraphs. Our system augments with paragraph retrieval a system based on BERT, and uses SQuAD scores as a criterion to tune the system’s parameters. 1 developer.amazon.com/alexa-skills-kit, wit.ai, dialogflow.com, developers.google.com/ actions. 2 rajpurkar.github.io/SQuAD-explorer/ Finally, in our system, the controller that conveys utterances either to the chatbot or to the QA component uses dialogue act recognition, which we envisage here as a simple classification task. More complex sequence models (Stolcke et al., 2000) as well as neural models (Zhao and Kawahara, 2017) have been proposed and could be used in the future to improve performance. 3. Design and Implementation The main components of our conversational agent are shown in Figure 1. The input utterance obtained from a smart speaker with speech recognition or from a text-based interface is passed to the dialogue act recognizer. This component routes task-related inputs (i.e. genuine informationseeking questions) to the QA component, and other inputs (including greetings, politeness or non-goal-directed chat) to the chatbot. The answers from the respective component are then sent back to the smart spe"
2021.findings-emnlp.224,D18-1269,0,0.0186629,"b). We update the weights of the whole model during this phase, since updating only the embeddings would not significantly reduce computation time (due to the need to calculate all activations for backpropagation) and has actually a negative impact on performance, as we observed in our initial experiments. At this stage, the transferred model could be used for any cross-lingual natural language understanding task (Hu et al., 2020) or for unsupervised machine translation (Conneau and Lample, 2019; Chronopoulou et al., 2020; Liu et al., 2020). In a second stage, we fine-tune the model for XNLI (Conneau et al., 2018) on labeled data in L1 (English), using L1 embeddings and freezing the embedding layer. Finally, we zero-shot transfer the model to L2 data by simply changing the languagespecific embedding layer. 5 5.1 Experiments with XNLI Models We compare several models in our experiments on cross-lingual natural language inference (textual 4 Language Model Transfer with SMALA entailment) with the XNLI dataset (Conneau et al., 2018). We note that all models, with the exception For the first set of experiments, we attempt to trans- of mBERT, follow the pipeline from the previous fer a pretrained Language Mo"
2021.findings-emnlp.224,2020.findings-emnlp.147,0,0.0214917,"nd then use FastAlign (Dyer et al., 2013) to estimate the alignment, similar to Tran (2020). Implementation details can be found in Appendix A.1. 3.2 Anchoring of Similar Subwords After the mapping step, we apply cosine similarity3 to compute a similarity matrix S: each of its coefficients Si,j is the cosine similarity between the embeddings of the ith subword of language L1 and of the j th subword of language L2 . We use the similarity matrix S to identify alignments between subwords in a completely unsupervised way. We extract the aligned subword alignments using the Argmax method of Jalili Sabet et al. (2020), as follows. A subword wiL1 from the L1 vocabulary is aligned to a subword wjL2 from the L2 vocabulary, if and only if wjL2 is the most similar subword to wiL1 and vice versa: Our motivation is to create cross-lingual vocabularies that are parameter-efficient and exploit the similarity of concepts between different languages. We propose a method for Subword Mapping and Anchoring across Languages (SMALA), which comi = arg max(Sl,j ) and j = arg max(Si,l ) (1) bines the powerful initialization of mapping methl l ods with the anchoring properties of joint training, 2 The use of subword co-occurr"
2021.findings-emnlp.224,2020.acl-main.536,0,0.215226,"lse posiity levels, based on character n-gram frequencies. tives (identical subwords with different meanTherefore, the embeddings of subwords that apings across languages) and false negatives (difpear in several languages act as anchors between ferent subwords with similar meanings). To these languages and, thus, provide implicit crossaddress these issues, we propose Subword lingual information that leads to improved perforMapping and Anchoring across Languages (SMALA), a method to construct bilingual mance (Conneau and Lample, 2019; Pires et al., subword vocabularies. SMALA extracts sub2019; Conneau et al., 2020b). word alignments using an unsupervised stateCross-lingual transfer in joint subword models of-the-art mapping technique and uses them to may be limited by false positives, i.e. identical subcreate cross-lingual anchors based on subword words with different meanings in two languages, a similarities. We demonstrate the benefits of phenomenon also known as ‘oversharing’ (Wang SMALA for cross-lingual natural language inference (XNLI), where it improves zero-shot et al., 2020b; Dhar and Bisazza, 2021). Moreover, transfer to an unseen language without taskthey do not benefit from false negatives,"
2021.findings-emnlp.224,D18-1330,0,0.122655,"d to build a shared vocabulary for MT, and bring experimental evidence of its benefits (Section 6). We release our code online1 . 2 Related Work Cross-lingual representations. A large body of work has attempted to harness the similarities of languages via cross-lingual word embeddings, i.e. continuous word vectors that can represent multiple languages in a shared vector space. A first approach to obtain these embeddings is offline mapping of pre-trained monolingual embeddings, where the mapping can be learned using supervision in the form of lexicons (Mikolov et al., 2013b; Xing et al., 2015; Joulin et al., 2018), or by leveraging weak supervision in the form of identical seed words (Artetxe et al., 2017; Søgaard et al., 2018), or in an unsupervised way (Artetxe et al., 2018; Lample et al., 2018a). A second approach to obtain crosslingual embeddings is joint training from scratch, by combining monolingual language modeling objectives with a cross-lingual objective – with either strong, or weak, or no supervision (see respectively Luong et al., 2015; Duong et al., 2016; Lample et al., 2018b). Despite their success, both approaches have certain limitations. On the one hand, alignment methods assume that"
2021.findings-emnlp.224,P17-4012,0,0.0223034,"ntencePiece11 . We choose the size of the shared subword vocabulary based on the size of the data, following Kudo (2018): 32k for high-resource pairs (En-Ru and En-De) and 16k for medium and low-resource pairs (En-Ro and En-Ar). We tokenize data using the Moses Tokenizer (Koehn et al., 2007). We report BLEU scores (Papineni et al., 2002) obtained with Sacre8 http://statmt.org/wmt17/translation-task.html http://statmt.org/wmt16/translation-task.html 10 TED talks from: https://wit3.fbk.eu/ 11 https://github.com/google/sentencepiece 9 BLEU (Post, 2018) on detokenized text.12 We train OpenNMT-py (Klein et al., 2017) for a maximum of 100k steps on high-resource pairs and 40k steps on medium or low-resource ones. Our base model is Transformer-Base (L=6, H=512) (Vaswani et al., 2017) with the same regularization and optimization procedures. We use a batch size of 4k tokens and evaluate every 5k steps. We select the best model based on validation loss. Final translations are generated with a beam width of five. 6.3 Results We present the results for our method and the baseline in Table 3. Our method yields comparable results to the baseline across all conditions of data availability and language relatedness."
2021.findings-emnlp.224,P02-1040,0,0.109388,"nd run experiments in both directions: Russian, German, Romanian and Arabic, to and from English. Training and test data comes from WMT178 for En-Ru and En-De, WMT169 for EnRo, and IWSLT1710 for En-Ar. We tokenize the data using the Unigram LM model (Kudo, 2018) as implemented in SentencePiece11 . We choose the size of the shared subword vocabulary based on the size of the data, following Kudo (2018): 32k for high-resource pairs (En-Ru and En-De) and 16k for medium and low-resource pairs (En-Ro and En-Ar). We tokenize data using the Moses Tokenizer (Koehn et al., 2007). We report BLEU scores (Papineni et al., 2002) obtained with Sacre8 http://statmt.org/wmt17/translation-task.html http://statmt.org/wmt16/translation-task.html 10 TED talks from: https://wit3.fbk.eu/ 11 https://github.com/google/sentencepiece 9 BLEU (Post, 2018) on detokenized text.12 We train OpenNMT-py (Klein et al., 2017) for a maximum of 100k steps on high-resource pairs and 40k steps on medium or low-resource ones. Our base model is Transformer-Base (L=6, H=512) (Vaswani et al., 2017) with the same regularization and optimization procedures. We use a batch size of 4k tokens and evaluate every 5k steps. We select the best model based"
2021.findings-emnlp.224,P19-1018,0,0.0166296,"al language modeling objectives with a cross-lingual objective – with either strong, or weak, or no supervision (see respectively Luong et al., 2015; Duong et al., 2016; Lample et al., 2018b). Despite their success, both approaches have certain limitations. On the one hand, alignment methods assume that the monolingual embedding spaces 1 https://github.com/GeorgeVern/smala have comparable structures, i.e., that they are isomorphic to a certain extent. However, this assumption has been challenged, especially for etymologically distant languages, but also for related ones (Søgaard et al., 2018; Patra et al., 2019; Ormazabal et al., 2019). Unsupervised joint training, on the other hand, relies on the assumption that identical tokens carry the same information across languages, which is not always true. To address the limitations of alignment and joint training (the isomorphism assumption and requirement for common script), combinations of the two methods have been proposed. Wang et al. (2020b) jointly train embeddings on concatenated monolingual corpora and then “unshare” identical words across languages, reallocating the overshared word embeddings and subsequently aligning them. Ormazabal et al. (2021"
2021.findings-emnlp.224,P19-1493,0,0.14855,"uperseded cross-lingual word embeddings, not only because they produce contextualized representations, but also because they can handle the open vocabulary problem through the use of subwords as tokens (Sennrich et al., 2016; Schuster and Nakajima, 2012; Kudo, 2018). Multilingual subword vocabularies are simply obtained by learning the subwords on the concatenation of all used languages. Since each subword is assigned to a unique embedding, identical subwords that appear in several languages serve as anchors between languages, providing implicit cross-lingual information (Wu and Dredze, 2019; Pires et al., 2019; Conneau et al., 2020b). Parameter sharing across languages make subword models particularly suitable for multilingual NLP and machine translation. The number of shared tokens in multilingual vocabularies highly depends on the similarities of script between languages. When this is not the case, transliteration can be applied (Nguyen and Chiang, 2017; Müller et al., 2020; Amrhein and Sennrich, 2020). In addition, shared subword vocabularies often produce inconsistent segmentations across languages that can hurt cross-lingual transfer. Regular2634 ization techniques that introduce randomness in"
2021.findings-emnlp.224,W18-6319,0,0.0207002,"Missing"
2021.findings-emnlp.224,2020.acl-main.170,0,0.0267799,"ring across languages make subword models particularly suitable for multilingual NLP and machine translation. The number of shared tokens in multilingual vocabularies highly depends on the similarities of script between languages. When this is not the case, transliteration can be applied (Nguyen and Chiang, 2017; Müller et al., 2020; Amrhein and Sennrich, 2020). In addition, shared subword vocabularies often produce inconsistent segmentations across languages that can hurt cross-lingual transfer. Regular2634 ization techniques that introduce randomness in the tokenization process (Kudo, 2018; Provilkov et al., 2020) can partially address this problem, or consistency between the different segmentations can be otherwise enforced (Wang et al., 2021). Still, there is no guarantee that shared (sub)words have identical meanings (false positives are not excluded) and, conversely, subwords with identical meanings but different spellings (false negatives) are missed. Cross-lingual LM transfer. The success of pretrained monolingual and multilingual language models raises the question of whether these models can be transferred to unseen languages. To transfer such a model, it is mostly necessary to add language-spe"
2021.findings-emnlp.224,Q18-1044,1,0.902144,"Missing"
2021.findings-emnlp.224,W17-4702,0,0.0487096,"Missing"
2021.findings-emnlp.224,2021.acl-long.243,0,0.0801344,"Missing"
2021.findings-emnlp.224,P16-1162,0,0.058398,"to create cross-lingual representations with a modified version of Skip-gram (Mikolov et al., 2013a). Our approach shares a similar motivation, but instead of directly creating cross-lingual representations, we shape the input space (i.e. the vocabulary) of multilingual systems in a way that facilitates cross-lingual transfer. Subword vocabularies. Recently, multilingual language models have superseded cross-lingual word embeddings, not only because they produce contextualized representations, but also because they can handle the open vocabulary problem through the use of subwords as tokens (Sennrich et al., 2016; Schuster and Nakajima, 2012; Kudo, 2018). Multilingual subword vocabularies are simply obtained by learning the subwords on the concatenation of all used languages. Since each subword is assigned to a unique embedding, identical subwords that appear in several languages serve as anchors between languages, providing implicit cross-lingual information (Wu and Dredze, 2019; Pires et al., 2019; Conneau et al., 2020b). Parameter sharing across languages make subword models particularly suitable for multilingual NLP and machine translation. The number of shared tokens in multilingual vocabularies"
2021.findings-emnlp.224,P18-1072,0,0.0521712,"Missing"
2021.findings-emnlp.224,2020.emnlp-main.586,0,0.0801002,"Missing"
2021.findings-emnlp.224,2021.naacl-main.40,0,0.0339643,"in multilingual vocabularies highly depends on the similarities of script between languages. When this is not the case, transliteration can be applied (Nguyen and Chiang, 2017; Müller et al., 2020; Amrhein and Sennrich, 2020). In addition, shared subword vocabularies often produce inconsistent segmentations across languages that can hurt cross-lingual transfer. Regular2634 ization techniques that introduce randomness in the tokenization process (Kudo, 2018; Provilkov et al., 2020) can partially address this problem, or consistency between the different segmentations can be otherwise enforced (Wang et al., 2021). Still, there is no guarantee that shared (sub)words have identical meanings (false positives are not excluded) and, conversely, subwords with identical meanings but different spellings (false negatives) are missed. Cross-lingual LM transfer. The success of pretrained monolingual and multilingual language models raises the question of whether these models can be transferred to unseen languages. To transfer such a model, it is mostly necessary to add language-specific parameters in the form of a subword embedding layer, which can be learned from scratch (Artetxe et al., 2020; de Vries and Niss"
2021.findings-emnlp.224,2020.findings-emnlp.240,0,0.256913,"rable structures, i.e., that they are isomorphic to a certain extent. However, this assumption has been challenged, especially for etymologically distant languages, but also for related ones (Søgaard et al., 2018; Patra et al., 2019; Ormazabal et al., 2019). Unsupervised joint training, on the other hand, relies on the assumption that identical tokens carry the same information across languages, which is not always true. To address the limitations of alignment and joint training (the isomorphism assumption and requirement for common script), combinations of the two methods have been proposed. Wang et al. (2020b) jointly train embeddings on concatenated monolingual corpora and then “unshare” identical words across languages, reallocating the overshared word embeddings and subsequently aligning them. Ormazabal et al. (2021) find word alignments that are used as anchors to create cross-lingual representations with a modified version of Skip-gram (Mikolov et al., 2013a). Our approach shares a similar motivation, but instead of directly creating cross-lingual representations, we shape the input space (i.e. the vocabulary) of multilingual systems in a way that facilitates cross-lingual transfer. Subword"
2021.findings-emnlp.224,N18-1101,0,0.0184316,"Missing"
2021.findings-emnlp.224,D19-1077,0,0.112087,"tasks such as not constrain the models to always represent or cross-lingual natural language understanding and translate them in the same way, as representations machine translation (Devlin et al., 2019; Conneau et al., 2020a; Aharoni et al., 2019). The perfor- are highly contextualized. mance of such systems is strongly connected to In this paper, we address the problem of false their use of an input space that can sufficiently positives and negatives by employing subword simrepresent all the considered languages (Sennrich ilarity to create cross-lingual anchors. Specifically, et al., 2016; Wu and Dredze, 2019; Conneau et al., using cross-lingual mapping, we determine sub2020a). Conceptually, an effective cross-lingual in- word alignments for a set of subwords, and then put space should exploit latent similarities between share their representations. In this way, we relax languages. the requirements for isomorphism and common State-of-the-art multilingual systems take ad- scripts between languages on which previous studvantage of cross-lingual similarities in their input ies rely. We demonstrate that this can improve both 2633 Findings of the Association for Computational Linguistics: EMNLP 2021, p"
2021.findings-emnlp.224,2020.emnlp-main.362,0,0.0791411,"Missing"
2021.findings-emnlp.224,N15-1104,0,0.0850494,"Missing"
2021.findings-emnlp.224,L16-1561,0,0.0182256,"T in our experiments as a reference for highperforming multilingual models. 5.2 Data and Settings For XNLI experiments, we select five target languages that vary in terms of language family, typology and script: Spanish (Es), German (De), Greek (El), Russian (Ru) and Arabic (Ar). We obtain monolingual corpora from the Wikipedia of each language using WikiExtractor5 . We use these corpora for MLM training, similar to Devlin et al. (2019), and to extract subword alignments using SMALA. When parallel data is used, we either use Europarl (Koehn et al., 2007) or the United Nations Parallel Corpus (Ziemski et al., 2016). We use the same amount of parallel data for each pair and we subsample the data, if needed. Both monolingual 4 In our experiments, even a random alignment produced better results than random initialization. 5 https://github.com/attardi/wikiextractor and parallel data are lowercased and tokenized with the Moses tokenizer (Koehn et al., 2007). For our implementation we use Hugging Face’s Transformers library (Wolf et al., 2019) and for RAMEN we use the public implementation from the author. We choose BERT- BASE (110M parameters) as our pretrained LM. We further train all bilingual models on ML"
bunt-etal-2010-towards,bunt-2006-dimensions,1,\N,Missing
bunt-etal-2010-towards,N09-2050,1,\N,Missing
bunt-etal-2010-towards,W03-0804,1,\N,Missing
bunt-etal-2012-iso,tonelli-etal-2010-annotation,0,\N,Missing
bunt-etal-2012-iso,W11-0125,1,\N,Missing
bunt-etal-2012-iso,W11-0101,1,\N,Missing
bunt-etal-2012-iso,bunt-etal-2010-towards,1,\N,Missing
bunt-etal-2012-iso,prasad-etal-2008-penn,0,\N,Missing
bunt-etal-2012-iso,kipp-2008-spatiotemporal,0,\N,Missing
C14-1056,P13-2115,1,0.787224,"or short conversations spans. Several topically-separated queries are constructed from keywords, and generate several lists of documents. The goal of the method proposed here is to generate a unique and concise list of documents that can be recommended in real time to the conversation participants. The list should cover the maximum number of implicit queries and therefore topics. To merge the lists of documents according to these criteria, we use inspiration from extractive text summarization (Lin and Bilmes, 2011; Li et al., 2012) and from our own previous work on diverse keyword extraction (Habibi and Popescu-Belis, 2013). The method proposed here rewards at the same time topic similarity – to select the most relevant documents to the conversation fragment – and topic diversity – to cover the maximum number of implicit queries and therefore topics in a concise and relevant list of recommendations, if more than one topic is discussed in the conversation fragment. Several studies have been previously carried out on merging lists of results in information retrieval. Despite the superficial similarity, the problem here is in fact different from distributed information retrieval, where several lists of results from"
C14-1056,P11-1052,0,0.230385,"resent a method for merging lists of documents retrieved through multiple implicit queries prepared for short conversations spans. Several topically-separated queries are constructed from keywords, and generate several lists of documents. The goal of the method proposed here is to generate a unique and concise list of documents that can be recommended in real time to the conversation participants. The list should cover the maximum number of implicit queries and therefore topics. To merge the lists of documents according to these criteria, we use inspiration from extractive text summarization (Lin and Bilmes, 2011; Li et al., 2012) and from our own previous work on diverse keyword extraction (Habibi and Popescu-Belis, 2013). The method proposed here rewards at the same time topic similarity – to select the most relevant documents to the conversation fragment – and topic diversity – to cover the maximum number of implicit queries and therefore topics in a concise and relevant list of recommendations, if more than one topic is discussed in the conversation fragment. Several studies have been previously carried out on merging lists of results in information retrieval. Despite the superficial similarity, t"
C14-1056,P11-4014,1,0.842264,"predefined taxonomy. In our case, the recommender system for conversational environments requires diversity in the results of multiple topically-separated queries, rather than of a single ambiguous query. Therefore, a new approach will be proposed, and will be compared in particular to a version of the explicit diversification approach (Santos et al., 2010) adapted to our problem. 3 Framework of our Document Recommender System We have designed the Automatic Content Linking Device (ACLD), a speech-based just-in-time document recommender system for business meetings (Popescu-Belis et al., 2008; Popescu-Belis et al., 2011). 589 Transcript of conversation fragment Extract the best k keywords that cover all the main topics with high probability (1) C {c1 ,..., ck } (2) Topical clustering of keywords to prepare M multiple topic-aware queries Q {q1 ,..., qM }, W (3) {w1 ,..., wM } Retrieval system Retrieval system L {l1 ,..., lM } (4) Similarity merging (SimM) S {d1 ,..., d N } list of relevant documents, Round-robin merging Diverse merging (DivM) S {d1 ,..., d N } S {d1 ,..., d N } l Diverse ranking (DivS) S {d1 ,..., d N } Figure 1: The four stages of our document recommendation approach (shown vertically: 1–4) a"
C98-2167,J94-4002,0,0.35792,"n&apos;t seem to use activation cues. Another system (Luperfoy 1992) uses &quot;discourse pegs&quot; to model referents and was applied successfully to a man-machine dialogue task. From a theoretical point of view, the model presented by Appelt and Kronfeld (1987) is in its background close to ours. Being further developed according to the speech acts theory, it relies however on models of intentions and beliefs of communicating agents which seem uneasy to implement for discourse understanding. 2.2 The activation of an MR is computed according to salience factors (this technique is described for instance by Lappin and Leass (1994)). Our salience factors are: de-activation in time, re-activation by various types of RE, re-activation according to the function of the RE. Among the MRs which pass the selection, activation is used to decide whether the current RE is added to an MR (the most active) or if a new MR is created. Activation is thus a dynamic factor, which changes for each MR according to the position in the text and the previous reference resolution decisions. 2 Comparison with other works Robust, lower-level systems 2.3 Advantages of the MR paradigm Theoretical studies of discourse processing have long been adv"
C98-2167,P92-1004,0,0.0200812,"&quot; for each of them has already been proposed by Kartunnen (1976). Evans (1985) and Recanati (1993) are both close to our proposals, however they neither give a computational implementation nor an evaluation on real texts. Sidner&apos;s work (1979) on focus led to salience factors and activations, but proved too demanding for an unrestricted use. A more operational system using semantic representation of referents is for instance LaSIE (Gaizauskas et al. 1995), presented at MUC-6, which relies however a lot on task-dependent knowledge. The system doesn&apos;t seem to use activation cues. Another system (Luperfoy 1992) uses &quot;discourse pegs&quot; to model referents and was applied successfully to a man-machine dialogue task. From a theoretical point of view, the model presented by Appelt and Kronfeld (1987) is in its background close to ours. Being further developed according to the speech acts theory, it relies however on models of intentions and beliefs of communicating agents which seem uneasy to implement for discourse understanding. 2.2 The activation of an MR is computed according to salience factors (this technique is described for instance by Lappin and Leass (1994)). Our salience factors are: de-activati"
C98-2167,W97-1314,1,0.869502,"Missing"
C98-2167,W97-1514,1,0.889124,"Missing"
C98-2167,M95-1005,0,\N,Missing
C98-2167,M95-1010,0,\N,Missing
C98-2167,M95-1017,0,\N,Missing
C98-2167,C69-7001,0,\N,Missing
C98-2167,C69-6902,0,\N,Missing
D14-1052,P05-1015,0,0.319852,"ntrast, we do not make any assumption about the nature or dimensionality of the feature space. The fine-grained analysis of opinions regarding specific aspects or features of items is known as multi-aspect sentiment analysis. This task usually requires aspect-related text segmentation, followed by prediction or summarization (Hu and Liu, 2004; Zhuang et al., 2006). Most attempts to perform this task have engineered various feature sets, augmenting words with topic or content models (Mei et al., 2007; Titov and McDonald, 2008; Sauper et al., 2010; Lu et al., 2011), or with linguistic features (Pang and Lee, 2005; Baccianella et al., 2009; Qu et al., 2010; Zhu et al., 2012). Other studies have advocated joint modeling of multiple aspects (Snyder and Barzilay, 2007) or multiple reviews for the same product (Li et al., 2011). McAuley et al. (2012) introduced new corpora of multi-aspect reviews, which we also partly use here, and proposed models for aspect detection, sentiment summarization and rating prediction. Lastly, joint aspect identification and sentiment classification have been used for aggregating product review snippets by Sauper at al. (2013). None of the above studies considers the multiplei"
D14-1052,W02-1011,0,0.0337943,"MIR and rating prediction. Section 3 formulates the problem while Section 4 describes previous MIR models. Section 5 presents our MIR model and learning procedure. Section 6 presents the datasets and evaluation methods. Section 7 reports our results on rating prediction tasks, and provides examples of rating explanation. 2.2 Rating Prediction from Text Sentiment analysis aims at analyzing the polarity of a given text, either with classification (for discrete labels) or regression (for real-valued labels). Early studies introduced machine learning techniques for sentiment classification, e.g. Pang et al. (2002), including unsupervised techniques based on the notion of semantic orientation of phrases, e.g. Turney et al. (2002). Other studies focused on subjectivity detection, i.e. whether a 456 assuming that X is the best bag representation for our task, we look for the optimal regression hyperplane Φ which minimizes a loss function L plus a regularization term Ω as follows:   Φ = arg min L(Y, X, Φ) + Ω(Φ) (1) |{z } |{z } Φ text span expresses opinions or not (Wiebe et al., 2004). Rating inference was defined by Pang et al. (2005) as multi-class classification or regression with respect to rating s"
D14-1052,C10-1103,0,0.120995,"e nature or dimensionality of the feature space. The fine-grained analysis of opinions regarding specific aspects or features of items is known as multi-aspect sentiment analysis. This task usually requires aspect-related text segmentation, followed by prediction or summarization (Hu and Liu, 2004; Zhuang et al., 2006). Most attempts to perform this task have engineered various feature sets, augmenting words with topic or content models (Mei et al., 2007; Titov and McDonald, 2008; Sauper et al., 2010; Lu et al., 2011), or with linguistic features (Pang and Lee, 2005; Baccianella et al., 2009; Qu et al., 2010; Zhu et al., 2012). Other studies have advocated joint modeling of multiple aspects (Snyder and Barzilay, 2007) or multiple reviews for the same product (Li et al., 2011). McAuley et al. (2012) introduced new corpora of multi-aspect reviews, which we also partly use here, and proposed models for aspect detection, sentiment summarization and rating prediction. Lastly, joint aspect identification and sentiment classification have been used for aggregating product review snippets by Sauper at al. (2013). None of the above studies considers the multipleinstance property of text in their modeling."
D14-1052,P11-1015,0,0.0103311,"ectivity detection, i.e. whether a 456 assuming that X is the best bag representation for our task, we look for the optimal regression hyperplane Φ which minimizes a loss function L plus a regularization term Ω as follows:   Φ = arg min L(Y, X, Φ) + Ω(Φ) (1) |{z } |{z } Φ text span expresses opinions or not (Wiebe et al., 2004). Rating inference was defined by Pang et al. (2005) as multi-class classification or regression with respect to rating scales. Pang and Lee (2008) discusses the large range of features engineered for this task, though several recent studies focus on feature learning (Maas et al., 2011; Socher et al., 2011), including the use of a deep neural network (Socher et al., 2013). In contrast, we do not make any assumption about the nature or dimensionality of the feature space. The fine-grained analysis of opinions regarding specific aspects or features of items is known as multi-aspect sentiment analysis. This task usually requires aspect-related text segmentation, followed by prediction or summarization (Hu and Liu, 2004; Zhuang et al., 2006). Most attempts to perform this task have engineered various feature sets, augmenting words with topic or content models (Mei et al., 2007;"
D14-1052,D10-1037,0,0.0659582,"luding the use of a deep neural network (Socher et al., 2013). In contrast, we do not make any assumption about the nature or dimensionality of the feature space. The fine-grained analysis of opinions regarding specific aspects or features of items is known as multi-aspect sentiment analysis. This task usually requires aspect-related text segmentation, followed by prediction or summarization (Hu and Liu, 2004; Zhuang et al., 2006). Most attempts to perform this task have engineered various feature sets, augmenting words with topic or content models (Mei et al., 2007; Titov and McDonald, 2008; Sauper et al., 2010; Lu et al., 2011), or with linguistic features (Pang and Lee, 2005; Baccianella et al., 2009; Qu et al., 2010; Zhu et al., 2012). Other studies have advocated joint modeling of multiple aspects (Snyder and Barzilay, 2007) or multiple reviews for the same product (Li et al., 2011). McAuley et al. (2012) introduced new corpora of multi-aspect reviews, which we also partly use here, and proposed models for aspect detection, sentiment summarization and rating prediction. Lastly, joint aspect identification and sentiment classification have been used for aggregating product review snippets by Saup"
D14-1052,N07-1038,0,0.0340633,"ific aspects or features of items is known as multi-aspect sentiment analysis. This task usually requires aspect-related text segmentation, followed by prediction or summarization (Hu and Liu, 2004; Zhuang et al., 2006). Most attempts to perform this task have engineered various feature sets, augmenting words with topic or content models (Mei et al., 2007; Titov and McDonald, 2008; Sauper et al., 2010; Lu et al., 2011), or with linguistic features (Pang and Lee, 2005; Baccianella et al., 2009; Qu et al., 2010; Zhu et al., 2012). Other studies have advocated joint modeling of multiple aspects (Snyder and Barzilay, 2007) or multiple reviews for the same product (Li et al., 2011). McAuley et al. (2012) introduced new corpora of multi-aspect reviews, which we also partly use here, and proposed models for aspect detection, sentiment summarization and rating prediction. Lastly, joint aspect identification and sentiment classification have been used for aggregating product review snippets by Sauper at al. (2013). None of the above studies considers the multipleinstance property of text in their modeling. 3 loss reg. Since the best set of representations X for a task is generally unknown, one has to make assumption"
D14-1052,D11-1014,0,0.0574779,"i.e. whether a 456 assuming that X is the best bag representation for our task, we look for the optimal regression hyperplane Φ which minimizes a loss function L plus a regularization term Ω as follows:   Φ = arg min L(Y, X, Φ) + Ω(Φ) (1) |{z } |{z } Φ text span expresses opinions or not (Wiebe et al., 2004). Rating inference was defined by Pang et al. (2005) as multi-class classification or regression with respect to rating scales. Pang and Lee (2008) discusses the large range of features engineered for this task, though several recent studies focus on feature learning (Maas et al., 2011; Socher et al., 2011), including the use of a deep neural network (Socher et al., 2013). In contrast, we do not make any assumption about the nature or dimensionality of the feature space. The fine-grained analysis of opinions regarding specific aspects or features of items is known as multi-aspect sentiment analysis. This task usually requires aspect-related text segmentation, followed by prediction or summarization (Hu and Liu, 2004; Zhuang et al., 2006). Most attempts to perform this task have engineered various feature sets, augmenting words with topic or content models (Mei et al., 2007; Titov and McDonald, 2"
D14-1052,J04-3002,0,0.0188286,"ression (for real-valued labels). Early studies introduced machine learning techniques for sentiment classification, e.g. Pang et al. (2002), including unsupervised techniques based on the notion of semantic orientation of phrases, e.g. Turney et al. (2002). Other studies focused on subjectivity detection, i.e. whether a 456 assuming that X is the best bag representation for our task, we look for the optimal regression hyperplane Φ which minimizes a loss function L plus a regularization term Ω as follows:   Φ = arg min L(Y, X, Φ) + Ω(Φ) (1) |{z } |{z } Φ text span expresses opinions or not (Wiebe et al., 2004). Rating inference was defined by Pang et al. (2005) as multi-class classification or regression with respect to rating scales. Pang and Lee (2008) discusses the large range of features engineered for this task, though several recent studies focus on feature learning (Maas et al., 2011; Socher et al., 2011), including the use of a deep neural network (Socher et al., 2013). In contrast, we do not make any assumption about the nature or dimensionality of the feature space. The fine-grained analysis of opinions regarding specific aspects or features of items is known as multi-aspect sentiment ana"
D14-1052,D13-1170,0,0.00324514,"for our task, we look for the optimal regression hyperplane Φ which minimizes a loss function L plus a regularization term Ω as follows:   Φ = arg min L(Y, X, Φ) + Ω(Φ) (1) |{z } |{z } Φ text span expresses opinions or not (Wiebe et al., 2004). Rating inference was defined by Pang et al. (2005) as multi-class classification or regression with respect to rating scales. Pang and Lee (2008) discusses the large range of features engineered for this task, though several recent studies focus on feature learning (Maas et al., 2011; Socher et al., 2011), including the use of a deep neural network (Socher et al., 2013). In contrast, we do not make any assumption about the nature or dimensionality of the feature space. The fine-grained analysis of opinions regarding specific aspects or features of items is known as multi-aspect sentiment analysis. This task usually requires aspect-related text segmentation, followed by prediction or summarization (Hu and Liu, 2004; Zhuang et al., 2006). Most attempts to perform this task have engineered various feature sets, augmenting words with topic or content models (Mei et al., 2007; Titov and McDonald, 2008; Sauper et al., 2010; Lu et al., 2011), or with linguistic fea"
D14-1052,P02-1053,0,0.0460551,"Missing"
E17-1089,W09-2404,0,0.436001,"Missing"
E17-1089,2012.eamt-1.60,0,0.0224607,"ce and target nouns and a large set of features (presented in Section 4) to decide whether one of the translations should be edited, and how. This decision will serve to post-edit and/or re-rank the baseline MT’s output (Section 4.4). To design the classifier, we train machine-learning classifiers over examples that are extracted from parallel data and from a baseline MT system, as described in Section 3.3. A separate subset of unseen examples will be used to test classifiers, first intrinsically and then in combination with MT. 3.2 Corpora and Pre-processing Our data comes from WIT3 Corpus1 (Cettolo et al., 2012), a collection of transcripts of TED talks, and the UN Corpora,2 a collection of documents from the United Nations. The experiments are on Chinese-to-English and German-to-English. We first build a phrase-based SMT system for each language pair with Moses (Koehn et al., 2007), with its default settings. Both MT systems are trained on the WIT3 data, and are used to generate candidate translations of the UN Corpora. Then, the ML classifiers are trained on noun pairs extracted from the UN Corpora, using semantic and syntactic features extracted from both source and target sides. The test sets als"
E17-1089,H92-1045,0,0.460548,"Missing"
E17-1089,D11-1084,0,0.600044,"Missing"
E17-1089,D12-1108,0,0.0385723,"Missing"
E17-1089,D15-1036,0,0.012909,"rom the baseline MT system, and the distance between the sentences that contain the two nouns. The second subset includes features that capture the size of the siblings in the parse trees of each of the two nouns. The third subset includes the size of sub-tree for the latest noun phrase ancestor for each analyzed noun, and also the depth distances to the next noun phrase ancestor. 4.3 discourse features, we use the word2vec word vector representations generated from a large corpus (Mikolov et al., 2013), which have been successfully used in the recent past to compute similarity between words (Schnabel et al., 2015). Specifically, we employ the model trained on the English Google News corpus 7 with about 100 billion words. For each pair of inconsistent translations (T1 , T2 ) of a source noun N , we compute the cosine similarities c1 and c2 between the vector representation of each translation and the mean vector of their contexts. These mean vectors, noted ~v1 and ~v2 , are computed by averaging all vectors of the words in the respective contexts of T1 and T2 . Here, the contexts consist of 20 words to the left and 20 words to the right of each Ti , possibly crossing sentence boundaries. The cosine simi"
E17-1089,W15-2501,0,0.0288105,"Missing"
E17-1089,W10-2602,0,0.393204,"Missing"
E17-1089,N12-1046,0,0.0441319,"Missing"
E17-1089,N03-5008,0,0.0180563,"ences Words 217K 4.4M 4.8M 800M Classifier testing Sentences Words Noun 7,771 225K 695 3,000 121K 647 Table 1: WIT3 data for building the SMT systems and UN data to train/test the classifiers. We use the WEKA environment4 to train and test several different learning algorithms: SVMs (Cortes and Vapnik, 1995), C4.5 Decision Trees (noted J48 in Weka) (Quinlan, 1993), and Random Forests (Breiman, 2001). We use 10-fold cross validation on the training set, and then test them once on the test set, and later on in combination with MT. For performance reasons, we used the Maximum Entropy classifier (Manning and Klein, 2003) from Stanford5 instead of WEKA’s Logistic Regression. The hyper-parameters of the above classifiers were set as follows, mostly following the default settings from WEKA, and setting others on the cross-validation sets (not the unseen test sets). For SVMs, the round-off error is  = 10−12 . For Decision Trees, we set the minimal number of instances per leaf (‘minNumObj’) at 2 and the confidence factor used for pruning to 0.25. For Random Forests, we defined the number of trees to be generated (‘numTree’) as 100 and set their maximal depth (‘maxDepth’) as unlimited. Finally, we set the MaxEnt s"
E17-1089,P02-1040,0,0.100135,"s to be generated (‘numTree’) as 100 and set their maximal depth (‘maxDepth’) as unlimited. Finally, we set the MaxEnt smoothing (σ) to 1.0, and the tolerance used for convergence in parameter optimization to 10−5 . We evaluate our proposal in two ways. First, we measure the classification accuracy in terms of accuracy and kappa (κ) agreement (Cohen, 1960) with the correct class, either in 10-fold crossvalidation experiments, or on the test set. Second, we compare the updated translations with the reference, to check if we obtain a result that is closer to it, using the popular B LEU measure (Papineni et al., 2002). used to set the ground-truth class (or decision) for training the classifiers, as follows. With the notations above (baseline translations of N noted T1 and T2 , with T1 6= T2 ), if the reference translations differ (RT1 6= RT2 ), then we label the pair as ‘none’, i.e. none of T1 and T2 should be post-edited and changed into the other, because this would not help to reach the reference translation anyway (recall that the only possible actions knowing the SMT baseline are replacing T1 by T2 or vice-versa). If the reference translations are the same (RT1 = RT2 ), then we examine this word, not"
E17-1089,P15-3002,1,0.893928,"Missing"
E17-1089,W12-3156,0,\N,Missing
E17-1089,P07-2045,0,\N,Missing
E17-2100,P81-1022,0,0.0756695,"Missing"
E17-2100,E12-3001,0,0.0668115,"r su can be translated by his, her, its or their depending on the gender, number and humanness of the possessor. In this paper, we provide a fully probabilistic integration of a Spanish anaphora resolution system into a phrase-based machine translation (MT) one, building upon a coreference-aware decoding model that we proposed earlier (Luong and Related Work Recent years have witnessed an increasing interest in improving machine translation of pronouns. Several studies have attempted to integrate anaphora resolution with statistical MT (Le Nagard and Koehn, 2010; Hardmeier and Federico, 2010; Guillou, 2012), but have often been limited by the accuracy of anaphora resolutions systems, even on the best-resourced language, English. For instance, Le Nagard and Koehn (2010) trained an English-French translation model on an annotated corpus in which each occurrence of the English pronouns it and they was annotated with the gender of its antecedent on the target side, but failed to improve over the baseline due to anaphora resolution errors. Hardmeier and Federico (2010) in631 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Pa"
E17-2100,2010.iwslt-papers.10,0,0.374559,"while the possessive determiner su can be translated by his, her, its or their depending on the gender, number and humanness of the possessor. In this paper, we provide a fully probabilistic integration of a Spanish anaphora resolution system into a phrase-based machine translation (MT) one, building upon a coreference-aware decoding model that we proposed earlier (Luong and Related Work Recent years have witnessed an increasing interest in improving machine translation of pronouns. Several studies have attempted to integrate anaphora resolution with statistical MT (Le Nagard and Koehn, 2010; Hardmeier and Federico, 2010; Guillou, 2012), but have often been limited by the accuracy of anaphora resolutions systems, even on the best-resourced language, English. For instance, Le Nagard and Koehn (2010) trained an English-French translation model on an annotated corpus in which each occurrence of the English pronouns it and they was annotated with the gender of its antecedent on the target side, but failed to improve over the baseline due to anaphora resolution errors. Hardmeier and Federico (2010) in631 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Vo"
E17-2100,J03-1002,0,0.0153657,"in Figure 1 shows other translation options for ellafeminine-singular-person: although there are several wrong triples as a consequence of alignment errors, they have small scores compared to that of the likely correct translation. Assignment of the Coreference Score To build the coreference model, for each of the anaphoric links found by CorZu, we append to each Spanish pronoun (noted P) the feature values of the respective antecedent (noted G, N, H). Moreover, we consider the English side of the parallel corpus (available with AnCora-ES), and using word-level alignments generated by GIZA++ (Och and Ney, 2003) we identify the translation of the Spanish pronoun. This results in a set of weighted triples of the form (P-G-NH, pron EN, probability) – e.g., (ella-femininesingular-person, she, 0.686453) – where probability results from the normalization of the current candidate score with respect to the total of the whole list. We gather all possible triples over the training data. If the candidates do not fully cover all possible P-G-N-H combinations, the remaining combinations will be generated, but with zero probability, and appended to the list in the 5 score(PGNH, pEN ) score(PGNH) P 4 Using the Cor"
E17-2100,W15-2501,0,0.0984227,"ar possessive pronouns suyo and suya. The possessive determiners agree in number with the possessed entity (which they determine) and refer to a possessor with unspecified gender and number, hence each of them can be translated by his, her, its or their. The possessive pronouns refer both to a possessed entity (with which they agree in gender and number) and a possessor of unspecified gender and number. Hence, they can be translated into English as his own (one), her own, its own or their own – but not with plural, e.g. not his own ones. The recent shared tasks on pronoun-focused translation (Hardmeier et al., 2015; Guillou et al., 2016) have promoted a pronoun correction task, which relies on information about the reference translation of the words surrounding the pronoun to be corrected, thus allowing automatic evaluation. Several systems developed for this task avoid direct use of anaphora resolution, but still reach competitive performance. Callin et al. (2015) designed a classifier based on a feed-forward neural network, which considered as features the preceding nouns and determiners along with their partsof-speech. Stymne (2016) combined the local context surrounding the source and target pronoun"
E17-2100,P03-1021,0,0.0203191,"its 463 28 4 87 its 4 0 0 0 putes several scores: the number of identical pronouns (noted C1) and of different ones (C3), the number of untranslated pronouns in the candidate (C4), in the reference (C5) or in both (C6).8 The goal is to increase C1 and decrease all other scores. APT was found to correlate well with human evaluation, but is stricter than it. Experimental Settings The MT training set for Moses is a part of the News Commentary (NC) 2011 set from WMT, combined with part of NC 2010, with a total of 250,000 ES-EN sentence pairs (see Section 3.1). The parameters are tuned using MERT (Och, 2003) on an NC 2011 development subset of 2,713 pairs. Another subset of NC 2011 with 13,000 sentences is used for testing. The language model is trained on an NC 2011 monolingual set with ca. 1.1M sentences. The test data contains 6,134 occurrences of the Spanish pronouns we study here, but CorZu found an antecedent only for 2,286 occurrences. For all other pronouns, our method will not translate them differently from the baseline system, therefore we do not count them below. We measure the Accuracy of Pronoun Translation (APT) by comparing the translated pronouns with those in the reference trans"
E17-2100,padro-stanilovsky-2012-freeling,0,0.0117239,", when decoding. We do not deal, however, with null pronouns, which raise different challenges, addressed e.g. by Wang et al. (2016) for Chinese-to-English MT and by Rios Gonzales and Tuggener (2017) for Spanishto-English MT. 3.1 Antecedent Identification using CorZu The goal of the first stage is to identify candidate antecedents of each source pronoun in the training data with their probabilities. The Spanish data is processed as follows. More detailed descriptions of the annotations are given by Rios (2016) and Rios Gonzales and Tuggener (2017) who also make them public.1 We use FreeLing2 (Padro and Stanilovsky, 2012) for morphological analysis and named entity recognition and classification, Wapiti3 (Lavergne et al., 2010) for PoS tagging, and the MaltParser4 (Nivre et al., 2006) for parsing. The models for tagging, parsing and co-reference resolution are all trained on the AnCora-ES Spanish 1 https://github.com/a-rios/CorefMT http://nlp.cs.upc.edu/freeling/ 3 https://wapiti.limsi.fr/ 4 http://www.maltparser.org/ 2 632 treebank (Taul´e et al., 2008).5 The CorZu coreference resolution system (Klenner and Tuggener, 2011; Tuggener, 2016) annotates the dependency trees with referential entities. CorZu impleme"
E17-2100,R11-1025,1,0.889641,"Missing"
E17-2100,E17-2104,1,0.833661,"antecedent candidates. In addition, instead of training and testing an SMT system on the gender-marked datasets (as did Le Nagard and Koehn (2010)), and use antecedents with absolute confidence, we model the probabilistic connection between a given pronoun and a given gender/number on the training set, and use the probabilistic scores of the antecedent within a coreference model, along with the translation and language models, when decoding. We do not deal, however, with null pronouns, which raise different challenges, addressed e.g. by Wang et al. (2016) for Chinese-to-English MT and by Rios Gonzales and Tuggener (2017) for Spanishto-English MT. 3.1 Antecedent Identification using CorZu The goal of the first stage is to identify candidate antecedents of each source pronoun in the training data with their probabilities. The Spanish data is processed as follows. More detailed descriptions of the annotations are given by Rios (2016) and Rios Gonzales and Tuggener (2017) who also make them public.1 We use FreeLing2 (Padro and Stanilovsky, 2012) for morphological analysis and named entity recognition and classification, Wapiti3 (Lavergne et al., 2010) for PoS tagging, and the MaltParser4 (Nivre et al., 2006) for"
E17-2100,W16-2355,0,0.0244583,"es. The recent shared tasks on pronoun-focused translation (Hardmeier et al., 2015; Guillou et al., 2016) have promoted a pronoun correction task, which relies on information about the reference translation of the words surrounding the pronoun to be corrected, thus allowing automatic evaluation. Several systems developed for this task avoid direct use of anaphora resolution, but still reach competitive performance. Callin et al. (2015) designed a classifier based on a feed-forward neural network, which considered as features the preceding nouns and determiners along with their partsof-speech. Stymne (2016) combined the local context surrounding the source and target pronouns (lemmas and POS tags) together with source-side dependency heads. The winning systems of the WMT 2016 pronoun task used neural networks: Luotolahti et al. (2016) and Dabre et al. (2016) summarized the backward and forward local contexts and passed them to a deep Recurrent Neural Network to predict pronoun translation. In this paper, we exploit anaphora resolution as the main knowledge source, building upon the model we have proposed earlier (Luong and Popescu-Belis, 2016), in which coreference features are directly used dur"
E17-2100,P10-1052,0,0.0248159,"Missing"
E17-2100,taule-etal-2008-ancora,0,0.0432464,"Missing"
E17-2100,W10-1737,0,0.35264,"Missing"
E17-2100,W16-2202,1,0.813906,"31 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers, pages 631–636, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics 3 tegrated a word dependency model into the SMT decoder as an additional feature, to keep track of pairs of source words acting respectively as antecedent and anaphor in a coreference link, and improved English-German MT over the baseline. Learning the Coreference Model The coreference model is the essential component of the general framework we proposed earlier (Luong and Popescu-Belis, 2016). The goal of the coreference model is to learn the probabilities of translating a given source pronoun, represented by the features of its antecedent, into a target pronoun. Due to anaphora resolution errors and variability in translation, the coreference model is not deterministic, but contains probabilities of translations, which are later combined with those from the translation and language models. We build a fully probabilistic coreference model, unlike our previous attempt, which relied only on the best candidate antecedent. Building the model requires two stages, presented in 3.1 and 3"
E17-2100,N16-1113,0,0.0540256,"source word, and the gender, number and humanness of the antecedent candidates. In addition, instead of training and testing an SMT system on the gender-marked datasets (as did Le Nagard and Koehn (2010)), and use antecedents with absolute confidence, we model the probabilistic connection between a given pronoun and a given gender/number on the training set, and use the probabilistic scores of the antecedent within a coreference model, along with the translation and language models, when decoding. We do not deal, however, with null pronouns, which raise different challenges, addressed e.g. by Wang et al. (2016) for Chinese-to-English MT and by Rios Gonzales and Tuggener (2017) for Spanishto-English MT. 3.1 Antecedent Identification using CorZu The goal of the first stage is to identify candidate antecedents of each source pronoun in the training data with their probabilities. The Spanish data is processed as follows. More detailed descriptions of the annotations are given by Rios (2016) and Rios Gonzales and Tuggener (2017) who also make them public.1 We use FreeLing2 (Padro and Stanilovsky, 2012) for morphological analysis and named entity recognition and classification, Wapiti3 (Lavergne et al., 2"
E17-2100,W16-2353,0,0.0869107,"urrounding the pronoun to be corrected, thus allowing automatic evaluation. Several systems developed for this task avoid direct use of anaphora resolution, but still reach competitive performance. Callin et al. (2015) designed a classifier based on a feed-forward neural network, which considered as features the preceding nouns and determiners along with their partsof-speech. Stymne (2016) combined the local context surrounding the source and target pronouns (lemmas and POS tags) together with source-side dependency heads. The winning systems of the WMT 2016 pronoun task used neural networks: Luotolahti et al. (2016) and Dabre et al. (2016) summarized the backward and forward local contexts and passed them to a deep Recurrent Neural Network to predict pronoun translation. In this paper, we exploit anaphora resolution as the main knowledge source, building upon the model we have proposed earlier (Luong and Popescu-Belis, 2016), in which coreference features are directly used during the decoding process through an additional translation table. However, we extend our previous model and use additional features, including the source word, and the gender, number and humanness of the antecedent candidates. In ad"
E17-3029,P09-1039,0,0.0167833,"Representations (Banarescu et al., 2013) with the aim to incorporate them into the storyline generation eventually. The parser was developed by Damonte et al. (2017). It is an incremental left-to-right parser that builds an AMR graph structure using a neural network controller. It also includes adaptations to German, Spanish, Italian and Chinese. 2.5 Entity Tagging and Linking Depending on the source language, Entity Tagging and Linking is performed either natively, or on the English translation. Entities are detected with TurboEntityRecognizer, a named entity recognizer within TurboParser4 (Martins et al., 2009). Then, we link the detected mentions to the knowledge base with a system based on our submission to TAC-KBP 2016 (Paikens et al., 2016). 4 2.8 Knowledge Base Construction This component provides a knowledge base of factual relations between entities, built with a model based on Universal Schemas (Riedel et al., 2013), a low-rank matrix factorization approach.The entity relations are extracted jointly across multiple languages, with entities pairs as rows and a set of structured relations and textual patterns as columns. The relations provide information about how various entities present in n"
E17-3029,N13-1008,1,0.77865,"Spanish, Italian and Chinese. 2.5 Entity Tagging and Linking Depending on the source language, Entity Tagging and Linking is performed either natively, or on the English translation. Entities are detected with TurboEntityRecognizer, a named entity recognizer within TurboParser4 (Martins et al., 2009). Then, we link the detected mentions to the knowledge base with a system based on our submission to TAC-KBP 2016 (Paikens et al., 2016). 4 2.8 Knowledge Base Construction This component provides a knowledge base of factual relations between entities, built with a model based on Universal Schemas (Riedel et al., 2013), a low-rank matrix factorization approach.The entity relations are extracted jointly across multiple languages, with entities pairs as rows and a set of structured relations and textual patterns as columns. The relations provide information about how various entities present in news https://github.com/andre-martins/ TurboParser 118 Marcin Junczys-Dowmunt, Tomasz Dwojak, and Hieu Hoang. 2016. Is neural machine translation ready for deployment? A case study on 30 translation directions. CoRR, abs/1610.01108. documents are connected. 2.9 Storyline Construction and Summarization Storylines are co"
E17-3029,E17-3017,1,0.751559,"m a multilingual corpus of nearly 600k documents in 8 of the 9 SUMMA languages (all except Latvian), which were manually annotated by journalists at Deutsche Welle. The document model is a hierarchical attention network with attention at each level of the hierarchy, inspired by Yang et al. (2016), followed by a sigmoid classification layer. 2.4 Machine Translation 2.7 Deep Semantic Tagging The lingua franca within SUMMA is English. Machine translation based on neural networks is used to translate content into English automatically. The back-end MT systems are trained with the Nematus Toolkit (Sennrich et al., 2017); translation is performed with AmuNMT (Junczys-Dowmunt et al., 2016). The system also has a component that performs semantic parsing into Abstract Meaning Representations (Banarescu et al., 2013) with the aim to incorporate them into the storyline generation eventually. The parser was developed by Damonte et al. (2017). It is an incremental left-to-right parser that builds an AMR graph structure using a neural network controller. It also includes adaptations to German, Spanish, Italian and Chinese. 2.5 Entity Tagging and Linking Depending on the source language, Entity Tagging and Linking is"
E17-3029,P13-1020,0,0.025126,"Missing"
E17-3029,E17-1051,1,0.815701,"lassification layer. 2.4 Machine Translation 2.7 Deep Semantic Tagging The lingua franca within SUMMA is English. Machine translation based on neural networks is used to translate content into English automatically. The back-end MT systems are trained with the Nematus Toolkit (Sennrich et al., 2017); translation is performed with AmuNMT (Junczys-Dowmunt et al., 2016). The system also has a component that performs semantic parsing into Abstract Meaning Representations (Banarescu et al., 2013) with the aim to incorporate them into the storyline generation eventually. The parser was developed by Damonte et al. (2017). It is an incremental left-to-right parser that builds an AMR graph structure using a neural network controller. It also includes adaptations to German, Spanish, Italian and Chinese. 2.5 Entity Tagging and Linking Depending on the source language, Entity Tagging and Linking is performed either natively, or on the English translation. Entities are detected with TurboEntityRecognizer, a named entity recognizer within TurboParser4 (Martins et al., 2009). Then, we link the detected mentions to the knowledge base with a system based on our submission to TAC-KBP 2016 (Paikens et al., 2016). 4 2.8 K"
I17-1102,W13-3520,0,0.0117531,"ecture. To encourage further research in multilingual representation learning our code and dataset are made available at https://github.com/idiap/mhan. 2 Related Work Research on learning multilingual word representations is based on early work on word embeddings (Turian et al., 2010; Mikolov et al., 2013; Pennington et al., 2014). The goal is to learn an aligned word embedding space for multiple languages by leveraging bilingual dictionaries (Faruqui and Dyer, 2014; Ammar et al., 2016), parallel sentences (Gouws et al., 2015) or comparable documents such as Wikipedia pages (Yih et al., 2011; Al-Rfou et al., 2013). Bilingual embeddings were learned from word alignments using neural language models (Klementiev et al., 2012; Zou et al., 2013), including auto-encoders (Chandar et al., 2014). Despite progress at the word level, the document level remains comparatively less explored. The approaches proposed by Hermann and Blunsom (2014) or Ferreira et al. 1 Germany’s news broadcaster: http://dw.com. (2016) are based on shallow modeling and are applicable only to classification tasks with label sets shared across languages, which are costly to produce and are often unavailable. Here, we remove this constrain"
I17-1102,E14-1049,0,0.0180828,"erform monolingual ones in both scenarios, thus confirming the utility of crosslanguage transfer and the computational efficiency of the proposed architecture. To encourage further research in multilingual representation learning our code and dataset are made available at https://github.com/idiap/mhan. 2 Related Work Research on learning multilingual word representations is based on early work on word embeddings (Turian et al., 2010; Mikolov et al., 2013; Pennington et al., 2014). The goal is to learn an aligned word embedding space for multiple languages by leveraging bilingual dictionaries (Faruqui and Dyer, 2014; Ammar et al., 2016), parallel sentences (Gouws et al., 2015) or comparable documents such as Wikipedia pages (Yih et al., 2011; Al-Rfou et al., 2013). Bilingual embeddings were learned from word alignments using neural language models (Klementiev et al., 2012; Zou et al., 2013), including auto-encoders (Chandar et al., 2014). Despite progress at the word level, the document level remains comparatively less explored. The approaches proposed by Hermann and Blunsom (2014) or Ferreira et al. 1 Germany’s news broadcaster: http://dw.com. (2016) are based on shallow modeling and are applicable only"
I17-1102,P16-1190,0,0.0153147,"adratically in the case of multi-way multilingual NMT (Firat et al., 2016a). Secondly, the models should be capable of cross-language transfer, which is an important component in human language learning (Ringbom, 2007). For instance, Johnson et al. (2016) attempted to use a single sequence-to-sequence neural network model for NMT across multiple language pairs. Previous studies in document classification attempted to address these issues by employing multilingual word embeddings, which allow direct comparisons and groupings across languages (Klementiev et al., 2012; Hermann and Blunsom, 2014; Ferreira et al., 2016). However, they are only applicable when common label sets are available across languages which is often not the case (e.g. Wikipedia or news). Moreover, despite recent advances in monolingual document modeling (Tang et al., 2015; Yang et al., 2016), multilingual models are still based on shallow approaches. In this paper, we propose Multilingual Hierarchical Attention Networks to learn shared doc1015 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 1015–1025, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP ument structures across lang"
I17-1102,N16-1101,0,0.136543,"has become increasingly useful for a variety of NLP tasks such as document classification (Tang et al., 2015; Yang et al., 2016), neural machine translation (NMT) (Cho et al., 2014; Luong et al., 2015), question answering (Chen et al., 2015; Kumar et al., 2015) and summarization (Rush et al., 2015). However, when data are available in multiple languages, representation learning must address two main challenges. Firstly, the computational cost of training separate models for each language, which grows linearly with their number, or even quadratically in the case of multi-way multilingual NMT (Firat et al., 2016a). Secondly, the models should be capable of cross-language transfer, which is an important component in human language learning (Ringbom, 2007). For instance, Johnson et al. (2016) attempted to use a single sequence-to-sequence neural network model for NMT across multiple language pairs. Previous studies in document classification attempted to address these issues by employing multilingual word embeddings, which allow direct comparisons and groupings across languages (Klementiev et al., 2012; Hermann and Blunsom, 2014; Ferreira et al., 2016). However, they are only applicable when common lab"
I17-1102,D16-1026,0,0.148871,"has become increasingly useful for a variety of NLP tasks such as document classification (Tang et al., 2015; Yang et al., 2016), neural machine translation (NMT) (Cho et al., 2014; Luong et al., 2015), question answering (Chen et al., 2015; Kumar et al., 2015) and summarization (Rush et al., 2015). However, when data are available in multiple languages, representation learning must address two main challenges. Firstly, the computational cost of training separate models for each language, which grows linearly with their number, or even quadratically in the case of multi-way multilingual NMT (Firat et al., 2016a). Secondly, the models should be capable of cross-language transfer, which is an important component in human language learning (Ringbom, 2007). For instance, Johnson et al. (2016) attempted to use a single sequence-to-sequence neural network model for NMT across multiple language pairs. Previous studies in document classification attempted to address these issues by employing multilingual word embeddings, which allow direct comparisons and groupings across languages (Klementiev et al., 2012; Hermann and Blunsom, 2014; Ferreira et al., 2016). However, they are only applicable when common lab"
I17-1102,P14-1006,0,0.178797,"th their number, or even quadratically in the case of multi-way multilingual NMT (Firat et al., 2016a). Secondly, the models should be capable of cross-language transfer, which is an important component in human language learning (Ringbom, 2007). For instance, Johnson et al. (2016) attempted to use a single sequence-to-sequence neural network model for NMT across multiple language pairs. Previous studies in document classification attempted to address these issues by employing multilingual word embeddings, which allow direct comparisons and groupings across languages (Klementiev et al., 2012; Hermann and Blunsom, 2014; Ferreira et al., 2016). However, they are only applicable when common label sets are available across languages which is often not the case (e.g. Wikipedia or news). Moreover, despite recent advances in monolingual document modeling (Tang et al., 2015; Yang et al., 2016), multilingual models are still based on shallow approaches. In this paper, we propose Multilingual Hierarchical Attention Networks to learn shared doc1015 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 1015–1025, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP umen"
I17-1102,P17-1092,0,0.0049612,"ues by proposing a new multilingual model based on HANs, which learns shared document structures and to transfer knowledge across languages. Early examples of attention mechanisms appeared in computer vision, e.g. for optical character recognition (Larochelle and Hinton, 2010), image tracking (Denil et al., 2012), or image classification (Mnih et al., 2014). For text classification, studies which aimed to learn the importance of sentences included those by Yessenalina et al. (2010); Pappas and Popescu-Belis (2014); Yang et al. (2016) and more recently those by Pappas and Popescu-Belis (2017); Ji and Smith (2017). For NMT, Bahdanau et al. (2015) proposed an attention-based encoder-decoder network, while Luong et al. (2015) proposed a local and ensemble attention model. Firat et al. (2016a) proposed a single encoder-decoder model with shared attention across language pairs for multi-way, multilingual NMT. Hermann et al. (2015) developed attention-based document readers for question an1016 swering. Chen et al. (2015) proposed a recurrent attention model over an external memory. Similarly, Kumar et al. (2015) introduced a dynamic memory network for question answering and other tasks. We propose here to s"
I17-1102,Q17-1024,0,0.0229782,"Missing"
I17-1102,N15-1011,0,0.0300905,"nd are applicable only to classification tasks with label sets shared across languages, which are costly to produce and are often unavailable. Here, we remove this constraint, and develop deeper multilingual document models with hierarchical structure based on prior art at the word level. Early work on neural document classification was based on shallow feed-forward networks, which required unsupervised pre-training (Le and Mikolov, 2014). Later studies focused on neural networks with hierarchical structure. Kim (2014) proposed a convolutional neural network (CNN) for sentence classification. Johnson and Zhang (2015) proposed a CNN for high-dimensional data classification, while Zhang et al. (2015) adopted a character-level CNN for text classification. Lai et al. (2015) proposed a recurrent CNN to capture sequential information, which outperformed simpler CNNs. Lin et al. (2015) and Tang et al. (2015) proposed hierarchical recurrent NNs and showed that they were superior to CNN-based models. Recently, Yang et al. (2016) proposed a hierarchical attention network (HAN) with bi-directional gated encoders which outperforms traditional and neural baselines. Using such networks in multilingual settings has two"
I17-1102,D14-1181,0,0.0046658,"l. 1 Germany’s news broadcaster: http://dw.com. (2016) are based on shallow modeling and are applicable only to classification tasks with label sets shared across languages, which are costly to produce and are often unavailable. Here, we remove this constraint, and develop deeper multilingual document models with hierarchical structure based on prior art at the word level. Early work on neural document classification was based on shallow feed-forward networks, which required unsupervised pre-training (Le and Mikolov, 2014). Later studies focused on neural networks with hierarchical structure. Kim (2014) proposed a convolutional neural network (CNN) for sentence classification. Johnson and Zhang (2015) proposed a CNN for high-dimensional data classification, while Zhang et al. (2015) adopted a character-level CNN for text classification. Lai et al. (2015) proposed a recurrent CNN to capture sequential information, which outperformed simpler CNNs. Lin et al. (2015) and Tang et al. (2015) proposed hierarchical recurrent NNs and showed that they were superior to CNN-based models. Recently, Yang et al. (2016) proposed a hierarchical attention network (HAN) with bi-directional gated encoders which"
I17-1102,C12-1089,0,0.631458,", which grows linearly with their number, or even quadratically in the case of multi-way multilingual NMT (Firat et al., 2016a). Secondly, the models should be capable of cross-language transfer, which is an important component in human language learning (Ringbom, 2007). For instance, Johnson et al. (2016) attempted to use a single sequence-to-sequence neural network model for NMT across multiple language pairs. Previous studies in document classification attempted to address these issues by employing multilingual word embeddings, which allow direct comparisons and groupings across languages (Klementiev et al., 2012; Hermann and Blunsom, 2014; Ferreira et al., 2016). However, they are only applicable when common label sets are available across languages which is often not the case (e.g. Wikipedia or news). Moreover, despite recent advances in monolingual document modeling (Tang et al., 2015; Yang et al., 2016), multilingual models are still based on shallow approaches. In this paper, we propose Multilingual Hierarchical Attention Networks to learn shared doc1015 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 1015–1025, c Taipei, Taiwan, November 27 – Decem"
I17-1102,D16-1076,0,0.0450122,"Missing"
I17-1102,D15-1106,0,0.0317814,"the word level. Early work on neural document classification was based on shallow feed-forward networks, which required unsupervised pre-training (Le and Mikolov, 2014). Later studies focused on neural networks with hierarchical structure. Kim (2014) proposed a convolutional neural network (CNN) for sentence classification. Johnson and Zhang (2015) proposed a CNN for high-dimensional data classification, while Zhang et al. (2015) adopted a character-level CNN for text classification. Lai et al. (2015) proposed a recurrent CNN to capture sequential information, which outperformed simpler CNNs. Lin et al. (2015) and Tang et al. (2015) proposed hierarchical recurrent NNs and showed that they were superior to CNN-based models. Recently, Yang et al. (2016) proposed a hierarchical attention network (HAN) with bi-directional gated encoders which outperforms traditional and neural baselines. Using such networks in multilingual settings has two drawbacks: the computational complexity increases linearly with the number of languages, and knowledge is acquired separately for each language. We address these issues by proposing a new multilingual model based on HANs, which learns shared document structures and t"
I17-1102,D15-1044,0,0.0392933,"their computational efficiency and the utility of cross-language transfer. 1 Figure 1: Vectors of documents labeled with ‘Europe’, ‘Culture’ and their Arabic counterparts. The multilingual hierarchical attention network separates topics better than monolingual ones. Introduction Learning word sequence representations has become increasingly useful for a variety of NLP tasks such as document classification (Tang et al., 2015; Yang et al., 2016), neural machine translation (NMT) (Cho et al., 2014; Luong et al., 2015), question answering (Chen et al., 2015; Kumar et al., 2015) and summarization (Rush et al., 2015). However, when data are available in multiple languages, representation learning must address two main challenges. Firstly, the computational cost of training separate models for each language, which grows linearly with their number, or even quadratically in the case of multi-way multilingual NMT (Firat et al., 2016a). Secondly, the models should be capable of cross-language transfer, which is an important component in human language learning (Ringbom, 2007). For instance, Johnson et al. (2016) attempted to use a single sequence-to-sequence neural network model for NMT across multiple languag"
I17-1102,D15-1166,0,0.0109748,"ones in low-resource as well as full-resource settings, and use fewer parameters, thus confirming their computational efficiency and the utility of cross-language transfer. 1 Figure 1: Vectors of documents labeled with ‘Europe’, ‘Culture’ and their Arabic counterparts. The multilingual hierarchical attention network separates topics better than monolingual ones. Introduction Learning word sequence representations has become increasingly useful for a variety of NLP tasks such as document classification (Tang et al., 2015; Yang et al., 2016), neural machine translation (NMT) (Cho et al., 2014; Luong et al., 2015), question answering (Chen et al., 2015; Kumar et al., 2015) and summarization (Rush et al., 2015). However, when data are available in multiple languages, representation learning must address two main challenges. Firstly, the computational cost of training separate models for each language, which grows linearly with their number, or even quadratically in the case of multi-way multilingual NMT (Firat et al., 2016a). Secondly, the models should be capable of cross-language transfer, which is an important component in human language learning (Ringbom, 2007). For instance, Johnson et al. (2016) a"
I17-1102,D15-1167,0,0.357769,"ews documents in 8 languages, and 5k labels. The multilingual models outperform monolingual ones in low-resource as well as full-resource settings, and use fewer parameters, thus confirming their computational efficiency and the utility of cross-language transfer. 1 Figure 1: Vectors of documents labeled with ‘Europe’, ‘Culture’ and their Arabic counterparts. The multilingual hierarchical attention network separates topics better than monolingual ones. Introduction Learning word sequence representations has become increasingly useful for a variety of NLP tasks such as document classification (Tang et al., 2015; Yang et al., 2016), neural machine translation (NMT) (Cho et al., 2014; Luong et al., 2015), question answering (Chen et al., 2015; Kumar et al., 2015) and summarization (Rush et al., 2015). However, when data are available in multiple languages, representation learning must address two main challenges. Firstly, the computational cost of training separate models for each language, which grows linearly with their number, or even quadratically in the case of multi-way multilingual NMT (Firat et al., 2016a). Secondly, the models should be capable of cross-language transfer, which is an importan"
I17-1102,P10-1040,0,0.0185262,"ngual document collection with 600k documents, labeled with general (1.2k) and specific topics (4.4k), in 8 languages from Deutsche Welle’s news website.1 Our multilingual models outperform monolingual ones in both scenarios, thus confirming the utility of crosslanguage transfer and the computational efficiency of the proposed architecture. To encourage further research in multilingual representation learning our code and dataset are made available at https://github.com/idiap/mhan. 2 Related Work Research on learning multilingual word representations is based on early work on word embeddings (Turian et al., 2010; Mikolov et al., 2013; Pennington et al., 2014). The goal is to learn an aligned word embedding space for multiple languages by leveraging bilingual dictionaries (Faruqui and Dyer, 2014; Ammar et al., 2016), parallel sentences (Gouws et al., 2015) or comparable documents such as Wikipedia pages (Yih et al., 2011; Al-Rfou et al., 2013). Bilingual embeddings were learned from word alignments using neural language models (Klementiev et al., 2012; Zou et al., 2013), including auto-encoders (Chandar et al., 2014). Despite progress at the word level, the document level remains comparatively less ex"
I17-1102,N16-1174,0,0.579034,"languages, and 5k labels. The multilingual models outperform monolingual ones in low-resource as well as full-resource settings, and use fewer parameters, thus confirming their computational efficiency and the utility of cross-language transfer. 1 Figure 1: Vectors of documents labeled with ‘Europe’, ‘Culture’ and their Arabic counterparts. The multilingual hierarchical attention network separates topics better than monolingual ones. Introduction Learning word sequence representations has become increasingly useful for a variety of NLP tasks such as document classification (Tang et al., 2015; Yang et al., 2016), neural machine translation (NMT) (Cho et al., 2014; Luong et al., 2015), question answering (Chen et al., 2015; Kumar et al., 2015) and summarization (Rush et al., 2015). However, when data are available in multiple languages, representation learning must address two main challenges. Firstly, the computational cost of training separate models for each language, which grows linearly with their number, or even quadratically in the case of multi-way multilingual NMT (Firat et al., 2016a). Secondly, the models should be capable of cross-language transfer, which is an important component in human"
I17-1102,D10-1102,0,0.0306707,"al complexity increases linearly with the number of languages, and knowledge is acquired separately for each language. We address these issues by proposing a new multilingual model based on HANs, which learns shared document structures and to transfer knowledge across languages. Early examples of attention mechanisms appeared in computer vision, e.g. for optical character recognition (Larochelle and Hinton, 2010), image tracking (Denil et al., 2012), or image classification (Mnih et al., 2014). For text classification, studies which aimed to learn the importance of sentences included those by Yessenalina et al. (2010); Pappas and Popescu-Belis (2014); Yang et al. (2016) and more recently those by Pappas and Popescu-Belis (2017); Ji and Smith (2017). For NMT, Bahdanau et al. (2015) proposed an attention-based encoder-decoder network, while Luong et al. (2015) proposed a local and ensemble attention model. Firat et al. (2016a) proposed a single encoder-decoder model with shared attention across language pairs for multi-way, multilingual NMT. Hermann et al. (2015) developed attention-based document readers for question an1016 swering. Chen et al. (2015) proposed a recurrent attention model over an external me"
I17-1102,W11-0329,0,0.0240159,"he proposed architecture. To encourage further research in multilingual representation learning our code and dataset are made available at https://github.com/idiap/mhan. 2 Related Work Research on learning multilingual word representations is based on early work on word embeddings (Turian et al., 2010; Mikolov et al., 2013; Pennington et al., 2014). The goal is to learn an aligned word embedding space for multiple languages by leveraging bilingual dictionaries (Faruqui and Dyer, 2014; Ammar et al., 2016), parallel sentences (Gouws et al., 2015) or comparable documents such as Wikipedia pages (Yih et al., 2011; Al-Rfou et al., 2013). Bilingual embeddings were learned from word alignments using neural language models (Klementiev et al., 2012; Zou et al., 2013), including auto-encoders (Chandar et al., 2014). Despite progress at the word level, the document level remains comparatively less explored. The approaches proposed by Hermann and Blunsom (2014) or Ferreira et al. 1 Germany’s news broadcaster: http://dw.com. (2016) are based on shallow modeling and are applicable only to classification tasks with label sets shared across languages, which are costly to produce and are often unavailable. Here, w"
I17-1102,D14-1052,1,0.368504,"early with the number of languages, and knowledge is acquired separately for each language. We address these issues by proposing a new multilingual model based on HANs, which learns shared document structures and to transfer knowledge across languages. Early examples of attention mechanisms appeared in computer vision, e.g. for optical character recognition (Larochelle and Hinton, 2010), image tracking (Denil et al., 2012), or image classification (Mnih et al., 2014). For text classification, studies which aimed to learn the importance of sentences included those by Yessenalina et al. (2010); Pappas and Popescu-Belis (2014); Yang et al. (2016) and more recently those by Pappas and Popescu-Belis (2017); Ji and Smith (2017). For NMT, Bahdanau et al. (2015) proposed an attention-based encoder-decoder network, while Luong et al. (2015) proposed a local and ensemble attention model. Firat et al. (2016a) proposed a single encoder-decoder model with shared attention across language pairs for multi-way, multilingual NMT. Hermann et al. (2015) developed attention-based document readers for question an1016 swering. Chen et al. (2015) proposed a recurrent attention model over an external memory. Similarly, Kumar et al. (20"
I17-1102,D14-1162,0,0.1096,"ts, labeled with general (1.2k) and specific topics (4.4k), in 8 languages from Deutsche Welle’s news website.1 Our multilingual models outperform monolingual ones in both scenarios, thus confirming the utility of crosslanguage transfer and the computational efficiency of the proposed architecture. To encourage further research in multilingual representation learning our code and dataset are made available at https://github.com/idiap/mhan. 2 Related Work Research on learning multilingual word representations is based on early work on word embeddings (Turian et al., 2010; Mikolov et al., 2013; Pennington et al., 2014). The goal is to learn an aligned word embedding space for multiple languages by leveraging bilingual dictionaries (Faruqui and Dyer, 2014; Ammar et al., 2016), parallel sentences (Gouws et al., 2015) or comparable documents such as Wikipedia pages (Yih et al., 2011; Al-Rfou et al., 2013). Bilingual embeddings were learned from word alignments using neural language models (Klementiev et al., 2012; Zou et al., 2013), including auto-encoders (Chandar et al., 2014). Despite progress at the word level, the document level remains comparatively less explored. The approaches proposed by Hermann and B"
I17-1102,D13-1141,0,0.0137427,"ub.com/idiap/mhan. 2 Related Work Research on learning multilingual word representations is based on early work on word embeddings (Turian et al., 2010; Mikolov et al., 2013; Pennington et al., 2014). The goal is to learn an aligned word embedding space for multiple languages by leveraging bilingual dictionaries (Faruqui and Dyer, 2014; Ammar et al., 2016), parallel sentences (Gouws et al., 2015) or comparable documents such as Wikipedia pages (Yih et al., 2011; Al-Rfou et al., 2013). Bilingual embeddings were learned from word alignments using neural language models (Klementiev et al., 2012; Zou et al., 2013), including auto-encoders (Chandar et al., 2014). Despite progress at the word level, the document level remains comparatively less explored. The approaches proposed by Hermann and Blunsom (2014) or Ferreira et al. 1 Germany’s news broadcaster: http://dw.com. (2016) are based on shallow modeling and are applicable only to classification tasks with label sets shared across languages, which are costly to produce and are often unavailable. Here, we remove this constraint, and develop deeper multilingual document models with hierarchical structure based on prior art at the word level. Early work o"
L16-1355,2012.eamt-1.60,0,0.0360852,"Missing"
L16-1355,2012.iwslt-papers.13,0,0.0190992,"he errors of the system’s initial output. Then, the system proposes a new solution, and the process can be repeated until the output meets the desired quality. Alabau et al. (2011) used keyboard, mouse and speech as input modalities and reported a significant performance boost in speed and quality. Vidal et al. (2006) followed a similar approach: the human translator utters part of a prefix of the final target sentence, and then either amends or validates it until the end of the sentence is reached. Using an interactive predictive process to correct the system generated errors was explored by Khadivi and Vakil (2012): an ASR n-best list is rescored by using translation models, thereby achieving better results. Ortiz-Mart´ınez et al. (2012) presented a translator’s workbench which takes into account the cognitive processes involved in human translation. The workbench determines what type of assistance is offered to the translator, and takes input from user by means of keyboard, mouse or e-pen. Our work differs from the previously conducted work in two ways. On the one hand, we aim mainly at disseminating translated information when a keyboard is not available, mainly aiming at scenarios like mobile applica"
L16-1355,P06-2061,0,0.0251886,"ed by rescoring ASR lattices from an initial recognition pass with a language model trained from the SMT output for the given document and speaker. A different approach was taken by Reddy and Rose (2010), where translation probabilities derived from SMT, along with named entity tags derived from named entity recognition, were used with acoustic phonetic information obtained from an ASR system. Reddy and Rose (2008) addressed issues related to taskindependent ASR, by including domain information from the document in the form of named entity labels. Instead of using n-best rescoring approaches, Khadivi et al. (2006) unified MT models and ASR models using finite state automata, which they claim is more suitable for a realtime prediction engine. Also, in a different manner, Matusov et al. (2005) exploited word lattices of ASR hypotheses as input to the translation system based on weighted finite-state transducers. Interactive machine translation (IMT), where a human expert is integrated in the process of automatic translation, has been considered by a number of previous studies. In IMT, a human expert interacts with a system by partially correcting the errors of the system’s initial output. Then, the syste"
L16-1355,P07-2045,0,0.0153802,"Missing"
L16-1355,2005.mtsummit-papers.11,0,0.0537938,"Missing"
L16-1355,W14-0315,0,0.0244421,"ather than the assimilation ones, as defined by Hovy et al. (2002), because it helps producing a highquality translation. The applications are intended for users who are proficient in both source and target languages, so that spoken post-editing is accurate, but who prefer using voice rather than a keyboard – e.g. on a mobile device such as a smartphone or a smartwatch. For instance, they could use spoken post-editing of MT to translate and disseminate to their colleagues an email originally written in a foreign language, or to re-tweet to their followers a message from a foreign contributor. Mesa-Lao (2014) surveyed the post-editors’ views and attitudes before and after the introduction of speech technology as a front-end to a computer-aided translation workbench. The survey shows that people tend to respond positively towards ASR used in post-editing, and they seem willing to adopt it as an input method for future post-editing tasks. In another user-oriented study, Dragsted et al. (2011) investigated the efficiency that can be achieved by using speech recognition software for translation tasks. With sufficient training and practice, the speech recognition’s time consumption appears to approach"
L16-1355,P03-1021,0,0.0187122,"Missing"
L16-1355,P02-1040,0,0.0952349,"Missing"
L18-1597,D14-1179,0,0.00914482,"Missing"
L18-1597,P16-1160,0,0.015329,"MT systems are typically trained using recurrent neural networks (RNNs) to encode a source sentence, and then decode its representation into the target language (Cho et al., 2014). The RNNs are often augmented with an attention mechanism to the source sentence (Bahdanau et al., 2014). However, training an NMT is not feasible for GSW/DE, as the size of our resources is several orders of magnitude below NMT requirements. However, several recent approaches have explored a new strategy: the translation system is trained at the character level (Ling et al., 2015; Costa-juss`a and Fonollosa, 2016; Chung et al., 2016; Bradbury et al., 2017; Lee et al., 2017), or at least character-level techniques such as byte-pair encoding are used to translate OOV words (Sennrich et al., 2016). As the available data is limited, one possible approach is to combine a PBSMT and a CBNMT system: the former translates known words, while the latter translates OOV ones. The CBNMT system has two main advantages: it can translate unseen words based on the spelling regularities observed in the training data, and it can be trained with smaller amounts of data compared to the requirements of standard NMT methods. Among the CBNMT app"
L18-1597,P16-2058,0,0.024165,"Missing"
L18-1597,J82-2005,0,0.573049,"Missing"
L18-1597,W13-2233,0,0.0241726,"died on many other important cases, in particular for Arabic dialects which are also predominantly used for spoken communication (Zbib et al., 2012). The lack of a normalized spelling of dialects has for instance an impact on training and evaluation of automatic speech recognition: a solution is to address spelling variation by mining text from social networks (Ali et al., 2017). Other strategies are the crowdsourcing of additional parallel data, or the use of large monolingual and comparable corpora to perform bilingual lexicon induction before training an MT system (Klementiev et al., 2012; Irvine and Callison-Burch, 2013; Irvine and Callison-Burch, 2016). The METIS-II EU project replaced the need for parallel corpora by using linguistic pre-processing and statistics from target-language corpora only (Carl et al., 2008). In a recent study applied to Afrikaans-to-Dutch translation, the authors use a characterbased “cipher model” and a word-based language model to design a decoder for the low-resourced input language (Pourdamghani and Knight, 2017). The Workshops on Statistical MT have proposed translation tasks for low-resourced languages to/from English, such as Hindi in 2014 (Bojar et al., 2014), Finnish in 2"
L18-1597,E12-1014,0,0.0254993,"or dialects has been studied on many other important cases, in particular for Arabic dialects which are also predominantly used for spoken communication (Zbib et al., 2012). The lack of a normalized spelling of dialects has for instance an impact on training and evaluation of automatic speech recognition: a solution is to address spelling variation by mining text from social networks (Ali et al., 2017). Other strategies are the crowdsourcing of additional parallel data, or the use of large monolingual and comparable corpora to perform bilingual lexicon induction before training an MT system (Klementiev et al., 2012; Irvine and Callison-Burch, 2013; Irvine and Callison-Burch, 2016). The METIS-II EU project replaced the need for parallel corpora by using linguistic pre-processing and statistics from target-language corpora only (Carl et al., 2008). In a recent study applied to Afrikaans-to-Dutch translation, the authors use a characterbased “cipher model” and a word-based language model to design a decoder for the low-resourced input language (Pourdamghani and Knight, 2017). The Workshops on Statistical MT have proposed translation tasks for low-resourced languages to/from English, such as Hindi in 2014 ("
L18-1597,N03-1017,0,0.0480925,"om the Archimob corpus (see 2.2 above), i.e. a Swiss German word and its normalized version, with a training set of 40,789 word pairs and a development set of 2,780 word pairs. 5. Integration with Machine Translation We use phrase-based statistical MT for the core of our system, as data was not sufficient to train neural MT. We experimented indeed with two NMT systems16 , which are typically trained on at least one million sentences, and tuned on 100k. In our case, the available data did not allow NMT to outperform PBSMT, which is used below. Using the Moses toolkit17 to build a PBSMT system (Koehn et al., 2003), we used various subsets of the parallel GSW/DE data presented in Section 2.2 above to learn 14 https://github.com/Kyubyong/quasi-rnn Originally, if the size of the minibatch is n, and the number of sentences modulo n is y (i.e. there are n ∗ x + y sentences), then only the n ∗ x first sentences were translated by the system, which ignored the y last ones. 16 The DL4MT toolkit https://github.com/nyu-dl/ dl4mt-tutorial and the OpenNMT-py one https:// github.com/OpenNMT/OpenNMT-py. 17 http://www.statmt.org/moses/ 3785 15 translation models. As for the target language model, we trained a tri-gra"
L18-1597,Q17-1026,0,0.0126855,"urrent neural networks (RNNs) to encode a source sentence, and then decode its representation into the target language (Cho et al., 2014). The RNNs are often augmented with an attention mechanism to the source sentence (Bahdanau et al., 2014). However, training an NMT is not feasible for GSW/DE, as the size of our resources is several orders of magnitude below NMT requirements. However, several recent approaches have explored a new strategy: the translation system is trained at the character level (Ling et al., 2015; Costa-juss`a and Fonollosa, 2016; Chung et al., 2016; Bradbury et al., 2017; Lee et al., 2017), or at least character-level techniques such as byte-pair encoding are used to translate OOV words (Sennrich et al., 2016). As the available data is limited, one possible approach is to combine a PBSMT and a CBNMT system: the former translates known words, while the latter translates OOV ones. The CBNMT system has two main advantages: it can translate unseen words based on the spelling regularities observed in the training data, and it can be trained with smaller amounts of data compared to the requirements of standard NMT methods. Among the CBNMT approaches, we use here Quasi Recurrent Neura"
L18-1597,W11-2164,0,0.0633329,"Missing"
L18-1597,2010.eamt-1.37,0,0.0818271,"Missing"
L18-1597,P02-1040,0,0.110888,"Missing"
L18-1597,D17-1266,0,0.0276153,"l parallel data, or the use of large monolingual and comparable corpora to perform bilingual lexicon induction before training an MT system (Klementiev et al., 2012; Irvine and Callison-Burch, 2013; Irvine and Callison-Burch, 2016). The METIS-II EU project replaced the need for parallel corpora by using linguistic pre-processing and statistics from target-language corpora only (Carl et al., 2008). In a recent study applied to Afrikaans-to-Dutch translation, the authors use a characterbased “cipher model” and a word-based language model to design a decoder for the low-resourced input language (Pourdamghani and Knight, 2017). The Workshops on Statistical MT have proposed translation tasks for low-resourced languages to/from English, such as Hindi in 2014 (Bojar et al., 2014), Finnish in 2015, or Latvian in 2017. However, these languages are clearly not as low-resourced as Swiss German, and possess at least a normalized version with a unified spelling. In 2011, the featured translation task aimed at translating text messages from Haitian Creole into English, with a parallel corpus of similar size as ours (ca. 35k words on each side, plus a Bible 12 http://www.statmt.org/wmt17/ translation-task.html Normalizing Swi"
L18-1597,L16-1641,0,0.0454152,"Missing"
L18-1597,P16-1162,0,0.031547,"(Cho et al., 2014). The RNNs are often augmented with an attention mechanism to the source sentence (Bahdanau et al., 2014). However, training an NMT is not feasible for GSW/DE, as the size of our resources is several orders of magnitude below NMT requirements. However, several recent approaches have explored a new strategy: the translation system is trained at the character level (Ling et al., 2015; Costa-juss`a and Fonollosa, 2016; Chung et al., 2016; Bradbury et al., 2017; Lee et al., 2017), or at least character-level techniques such as byte-pair encoding are used to translate OOV words (Sennrich et al., 2016). As the available data is limited, one possible approach is to combine a PBSMT and a CBNMT system: the former translates known words, while the latter translates OOV ones. The CBNMT system has two main advantages: it can translate unseen words based on the spelling regularities observed in the training data, and it can be trained with smaller amounts of data compared to the requirements of standard NMT methods. Among the CBNMT approaches, we use here Quasi Recurrent Neural Networks (QRNNs) (Bradbury et al., 2017), which take advantage of both convolutional and recurrent layers. The increased"
L18-1597,N12-1006,0,0.0812409,"d automatically using character-based statistical MT (Scherrer and Ljubeˇsi´c, 2016). Initial attempts for MT of GSW include the abovementioned system for generating GSW texts from DE (Scherrer, 2012a), and a system combining ASR and MT of Swiss German from Valais (Garner et al., 2014). A normalization attempt for MT, on a different Germanic dialect, has been proposed for Viennese (Hildenbrandt et al., 2013). The MT of low-resource languages or dialects has been studied on many other important cases, in particular for Arabic dialects which are also predominantly used for spoken communication (Zbib et al., 2012). The lack of a normalized spelling of dialects has for instance an impact on training and evaluation of automatic speech recognition: a solution is to address spelling variation by mining text from social networks (Ali et al., 2017). Other strategies are the crowdsourcing of additional parallel data, or the use of large monolingual and comparable corpora to perform bilingual lexicon induction before training an MT system (Klementiev et al., 2012; Irvine and Callison-Burch, 2013; Irvine and Callison-Burch, 2016). The METIS-II EU project replaced the need for parallel corpora by using linguisti"
lisowska-etal-2004-user,popescu-belis-2004-abstracting,1,\N,Missing
loaiciga-etal-2014-english,C08-3012,0,\N,Missing
loaiciga-etal-2014-english,chrupala-etal-2008-learning,0,\N,Missing
loaiciga-etal-2014-english,E12-1074,0,\N,Missing
loaiciga-etal-2014-english,N10-3011,0,\N,Missing
loaiciga-etal-2014-english,N03-5008,0,\N,Missing
loaiciga-etal-2014-english,P02-1040,0,\N,Missing
loaiciga-etal-2014-english,D07-1091,0,\N,Missing
loaiciga-etal-2014-english,W08-2122,0,\N,Missing
loaiciga-etal-2014-english,D12-1026,0,\N,Missing
loaiciga-etal-2014-english,J03-1002,0,\N,Missing
loaiciga-etal-2014-english,W13-3305,1,\N,Missing
loaiciga-etal-2014-english,2005.mtsummit-papers.11,0,\N,Missing
loaiciga-etal-2014-english,vilar-etal-2006-error,0,\N,Missing
loaiciga-etal-2014-english,W11-2107,0,\N,Missing
N18-1124,N16-1036,0,0.147894,"the de-facto baseline. This architecture is composed of two recurrent neural networks (RNNs), an encoder and a decoder, and an attention mechanism between them for modeling a As pointed out by Cheng et al. (2016), sequential models present two main problems for natural language processing. First, the memory of the encoder is shared across multiple words and is prone to bias towards the recent past. Second, such models do not fully capture the structural composition of language. To address these limitations, several recent models have been proposed, namely memory networks (Cheng et al., 2016; Tran et al., 2016; Wang et al., 2016) and self-attention networks (Daniluk et al., 2016; Liu and Lapata, 2018). We experimented with these methods, applying them to NMT: memory RNN (Cheng et al., 2016) and self-attentive RNN (Daniluk et al., 2016). How1366 Proceedings of NAACL-HLT 2018, pages 1366–1379 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics ever, we observed no significant gains in performance over the baseline architecture. In this paper, we propose a self-attentive residual recurrent decoder, presented in Figure 1b, which, if unfolded over time, represents"
N18-1124,D16-1027,0,0.0793476,"ine. This architecture is composed of two recurrent neural networks (RNNs), an encoder and a decoder, and an attention mechanism between them for modeling a As pointed out by Cheng et al. (2016), sequential models present two main problems for natural language processing. First, the memory of the encoder is shared across multiple words and is prone to bias towards the recent past. Second, such models do not fully capture the structural composition of language. To address these limitations, several recent models have been proposed, namely memory networks (Cheng et al., 2016; Tran et al., 2016; Wang et al., 2016) and self-attention networks (Daniluk et al., 2016; Liu and Lapata, 2018). We experimented with these methods, applying them to NMT: memory RNN (Cheng et al., 2016) and self-attentive RNN (Daniluk et al., 2016). How1366 Proceedings of NAACL-HLT 2018, pages 1366–1379 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics ever, we observed no significant gains in performance over the baseline architecture. In this paper, we propose a self-attentive residual recurrent decoder, presented in Figure 1b, which, if unfolded over time, represents a denselyconnected r"
N18-1124,D16-1093,0,0.0873741,"e underlying semantics of source sentences. In contrast to these studies, we focus instead on the contextual information on the target side. Our model relies on residual connections, which have been shown to improve the learning process of deep neural networks by addressing the vanishing gradient problem (He et al., 2016). These connections create a direct path from previous layers, helping the transmission of information. Recently, several architectures using residual connections with LSTMs have been proposed for sequence prediction (Zhang et al., 2016b; Kim et al., 2017; Zilly et al., 2017; Wang and Tian, 2016). To our knowledge, our study is the first one to use self-attentive residual connections within residual RNNs for NMT. In parallel to our study, a similar method was recently proposed for sentiment analysis (Wang, 2017). 1367 3 Background: Neural Machine Translation Neural machine translation aims to compute the conditional distribution of emitting a sentence in a target language given a sentence in a source language, denoted by pΘ (y|x), where Θ is the set of parameters of the neural model, and y = {y1 , ..., yn } and x = {x1 , ..., xm } are respectively the representations of source and tar"
N18-1124,W14-3324,0,0.0272305,"r English-to-German on Newstest (NT) 2014, 2015, 2016. Significance test: * p &lt; 0.05, ** p &lt; 0.01. The Winning WMT systems are listed in the text below. Table 2 shows BLEU values calculated by multibleu, and includes the NMT system proposed by Luong et al. (2015) which replaces unknown predicted words with the most strongly aligned word on the source sentence. Also, the table includes other systems described in Section 2. Additionally, Table 3 shows values calculated by the NIST BLEU scorer, as well as results reported by the “Winning WMT” systems for each test set respectively: UEDIN-SYNTAX (Williams et al., 2014), UEDIN-SYNTAX (Williams et al., 2015), and UEDIN-NMT (Sennrich et al., 2016a). Also, we include the results reported by Sennrich et al. (2016b) for a baseline encoder-decoder NMT with BPE for unknown words similar to our configuration, and finally the system proposed by Nadejde et al. (2017), an explicit syntax-aware NMT that introduces combinatory categorial grammar (CCG) supertags on the target side by predicting words and tags alternately. The comparison with this work is relevant for the analysis described later in Section 8.2. The results confirm that the self-attentive residual connecti"
N18-1124,W15-3024,0,0.0367638,"014, 2015, 2016. Significance test: * p &lt; 0.05, ** p &lt; 0.01. The Winning WMT systems are listed in the text below. Table 2 shows BLEU values calculated by multibleu, and includes the NMT system proposed by Luong et al. (2015) which replaces unknown predicted words with the most strongly aligned word on the source sentence. Also, the table includes other systems described in Section 2. Additionally, Table 3 shows values calculated by the NIST BLEU scorer, as well as results reported by the “Winning WMT” systems for each test set respectively: UEDIN-SYNTAX (Williams et al., 2014), UEDIN-SYNTAX (Williams et al., 2015), and UEDIN-NMT (Sennrich et al., 2016a). Also, we include the results reported by Sennrich et al. (2016b) for a baseline encoder-decoder NMT with BPE for unknown words similar to our configuration, and finally the system proposed by Nadejde et al. (2017), an explicit syntax-aware NMT that introduces combinatory categorial grammar (CCG) supertags on the target side by predicting words and tags alternately. The comparison with this work is relevant for the analysis described later in Section 8.2. The results confirm that the self-attentive residual connections improve significantly the translat"
N18-1124,E17-2061,0,0.0721138,"e the memory capacity of LSTMs by using memory networks (Weston et al., 2015; Sukhbaatar et al., 2015). For instance, Cheng et al. (2016) incorporate different memory cells for each previous output representation, which are later accessed by an attention mechanism. Tran et al. (2016) include a memory block to access recent input words in a selective manner. Both methods show improvements on language Other studies aim to improve the modeling of source-side contextual information, for example through a context-aware encoder using selfattention (Zhang et al., 2017), or a recurrent attention NMT (Yang et al., 2017) that is aware of previously attended words on the source-side in order to better predict which words will be attended in future. Additionally, variational NMT (Zhang et al., 2016a) introduces a latent variable to model the underlying semantics of source sentences. In contrast to these studies, we focus instead on the contextual information on the target side. Our model relies on residual connections, which have been shown to improve the learning process of deep neural networks by addressing the vanishing gradient problem (He et al., 2016). These connections create a direct path from previous"
N18-1124,D16-1050,0,0.216131,"ch previous output representation, which are later accessed by an attention mechanism. Tran et al. (2016) include a memory block to access recent input words in a selective manner. Both methods show improvements on language Other studies aim to improve the modeling of source-side contextual information, for example through a context-aware encoder using selfattention (Zhang et al., 2017), or a recurrent attention NMT (Yang et al., 2017) that is aware of previously attended words on the source-side in order to better predict which words will be attended in future. Additionally, variational NMT (Zhang et al., 2016a) introduces a latent variable to model the underlying semantics of source sentences. In contrast to these studies, we focus instead on the contextual information on the target side. Our model relies on residual connections, which have been shown to improve the learning process of deep neural networks by addressing the vanishing gradient problem (He et al., 2016). These connections create a direct path from previous layers, helping the transmission of information. Recently, several architectures using residual connections with LSTMs have been proposed for sequence prediction (Zhang et al., 20"
P07-2024,broeder-etal-2004-large,0,0.0621062,"Missing"
P07-2024,broeder-etal-2004-using,0,0.030123,"since the media resources are not listed explicitly in the annotation files. This implies using different strategies to extract the metadata: for example, stylesheets are the best option to deal with the above-mentioned XML files, while a crawler script is used for HTTP access to the distribution site. However, the solution adopted for annotations in Section 3 can be reused with one major extension and applied to the construction of the metadata database. The standard chosen for the explicit metadata files is the IMDI format, proposed by the ISLE Meta Data Initiative (Wittenburg et al., 2002; Broeder et al., 2004a) (see http://www.mpi.nl/IMDI/tools), which is precisely intended to describe multimedia recordings of dialogues. This standard provides a flexible and extensive schema to store the defined metadata either in specific IMDI elements or as additional key/value pairs. The metadata generated for the AMI Corpus can be explored with the IMDI BC-Browser (Broeder et al., 2004b), a tool that is freely available and has useful features such as search or metadata editing. The process of extracting, structuring and storing 95 the metadata is as follows: 1. Crawl the AMI Corpus website and store resulting"
P07-2024,wittenburg-etal-2002-metadata,0,0.0675323,"Missing"
P07-2024,W04-0602,0,0.0519768,"Missing"
P11-4014,W09-3206,0,0.0129644,"(or its inverse, persistence) can be tuned by the user, but in any case, all past results are saved by the user interface and can be consulted at any time. 4.4 Semantic Search over Wikipedia The goal of our method for semantic search is to improve the relevance of the retrieved documents, and to make the mechanism more robust to noise from the ASR. We have applied to document retrieval the graph-based model of semantic relatedness that we recently developed (Yazdani and Popescu-Belis, 2010), which is also related to other proposals (Strube and Ponzetto, 2006; Gabrilovich and Markovitch, 2007; Yeh et al., 2009). The model is grounded in a measure of semantic relatedness between text fragments, which is computed using random walk over the network of Wikipedia articles – about 1.2 million articles from the WEX data set (Metaweb Technologies, 2010). The articles are linked through hyperlinks, and also through lexical similarity links that are constructed upon initialization. The random walk model allows the computation of a visiting probability (VP) from one article to another, and then a VP between sets of articles, which has been shown to function as a measure of semantic relatedness, and has been ap"
P13-2115,P11-1052,0,0.0637334,"us learning algorithms (Turney, 1999; Frank et al., 1999; Hulth, 2003). These approaches, however, rely on the availability of in-domain training data, and the objective functions they use for learning do not consider yet the diversity of keywords. 3 et al. (1978) showed that a greedy algorithm can find an approximate solution guaranteed to be within (1 − 1e ) &apos; 0.63 of the optimal solution if the coverage function is submodular and monotone nondecreasing1 . To find a monotone submodular function for keyword extraction, we used inspiration from recent work on extractive summarization methods (Lin and Bilmes, 2011; Li et al., 2012), which proposed a square root function for diverse selection of sentences to cover the maximum number of key concepts of a given document. The function rewards diversity by increasing the gain of selecting a sentence including a concept that was not yet covered by a previously selected sentence. This must be adapted for keyword extraction by defining an appropriate reward function. We first introduce rS,z , the topical similarity with respect to topic z of the keyword set S selected from the fragment t, defined as follows: X rS,z = p(z|w) · p(z|t). Diverse Keyword Extraction"
P13-2115,N09-1070,0,0.0211413,"rd extraction that rewards both word similarity, to extract the most representative words, and word diversity, to cover several topics if necessary. The paper is organized as follows. In Section 2 we review existing methods for keyword extraction. In Section 3 we describe our proposal, which relies on topic modeling and a novel topic-aware diverse keyword extraction algorithm. Section 4 presents 651 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 651–657, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics topic. Z. Liu et al. (2009b) used part-of-speech information and word clustering techniques, while F. Liu et al. (2009a) added this information to the TFIDF method so as to consider both word dependency and semantic information. However, although they considered topical similarity, the above methods did not explicitly reward diversity and might miss secondary topics. Supervised methods have been used to learn a model for extracting keywords with various learning algorithms (Turney, 1999; Frank et al., 1999; Hulth, 2003). These approaches, however, rely on the availability of in-domain training data, and the objective f"
P13-2115,cieri-etal-2004-fisher,0,0.129396,"Missing"
P13-2115,D09-1027,0,0.0576319,"rd extraction that rewards both word similarity, to extract the most representative words, and word diversity, to cover several topics if necessary. The paper is organized as follows. In Section 2 we review existing methods for keyword extraction. In Section 3 we describe our proposal, which relies on topic modeling and a novel topic-aware diverse keyword extraction algorithm. Section 4 presents 651 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 651–657, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics topic. Z. Liu et al. (2009b) used part-of-speech information and word clustering techniques, while F. Liu et al. (2009a) added this information to the TFIDF method so as to consider both word dependency and semantic information. However, although they considered topical similarity, the above methods did not explicitly reward diversity and might miss secondary topics. Supervised methods have been used to learn a model for extracting keywords with various learning algorithms (Turney, 1999; Frank et al., 1999; Hulth, 2003). These approaches, however, rely on the availability of in-domain training data, and the objective f"
P13-2115,D10-1036,0,0.0541515,"(2007) used the frequency of all words belonging to the same WordNet concept set, while the Wikifier system (Csomai and Mihalcea, 2007) relied on Wikipedia links to compute a substitute to word frequency. Harwath and Hazen (2012) used topic modeling with PLSA to build a thesaurus, which they used to rank words based on topical similarity to the topics of a transcribed conversation. To consider dependencies among selected words, word co-occurrence has been combined with PageRank by Mihalcea and Tarau (2004), and additionally with WordNet by Wang et al. (2007), or with topical information by Z. Liu et al. (2010). However, as shown empirically by Mihalcea and Tarau (2004) and by Z. Liu et al. (2010) with various co-occurrence windows, such approaches have difficulties modeling long-range dependencies between words related to the same Introduction The goal of keyword extraction from texts is to provide a set of words that are representative of the semantic content of the texts. In the application intended here, keywords are automatically extracted from transcripts of conversation fragments, and are used to formulate queries to a just-in-time document recommender system. It is thus important that the ke"
P13-2115,W03-1028,0,0.02967,"57, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics topic. Z. Liu et al. (2009b) used part-of-speech information and word clustering techniques, while F. Liu et al. (2009a) added this information to the TFIDF method so as to consider both word dependency and semantic information. However, although they considered topical similarity, the above methods did not explicitly reward diversity and might miss secondary topics. Supervised methods have been used to learn a model for extracting keywords with various learning algorithms (Turney, 1999; Frank et al., 1999; Hulth, 2003). These approaches, however, rely on the availability of in-domain training data, and the objective functions they use for learning do not consider yet the diversity of keywords. 3 et al. (1978) showed that a greedy algorithm can find an approximate solution guaranteed to be within (1 − 1e ) &apos; 0.63 of the optimal solution if the coverage function is submodular and monotone nondecreasing1 . To find a monotone submodular function for keyword extraction, we used inspiration from recent work on extractive summarization methods (Lin and Bilmes, 2011; Li et al., 2012), which proposed a square root f"
P13-2115,W04-3252,0,\N,Missing
P15-3002,N09-1014,0,0.0144851,"the model propagates the error to the following sentences. Gong et al. (2011) later extended Tiedemann’s proposal by initializing the cache with phrase pairs from similar documents at the beginning of the translation and by also applying a topic cache, which was introduced to deal with the error propagation issue. Xiao et al. (2011) defined a three step procedure that enforces the consistent translation of ambiguous words, achieving improvements for EN/ZH. Ture et al. (2012) encouraged consistency for AR/EN MT by introducing cross-sentence consistency features to the translation model, while Alexandrescu and Kirchhoff (2009) enforced similar translations to sentences having a similar graph representation. Our work is an instance of a recent trend aiming to go beyond sentence-by-sentence MT, by using semantic information from previous sentences to constrain or correct the decoding of the current one. In this paper, we compared caching and post-editing as ways of achieving this goal, but a document-level decoder such as Docent (Hardmeier et al., 2012) could be used as well. In other studies, factored translation models (Koehn and Hoang, 2007) have been used with the same purpose, by incorporating contextual informa"
P15-3002,W09-2404,0,0.492585,"ask. Finally, our work and its perspectives contribute to the general objective of using discourse-level information to improve MT (Hardmeier, 2014; Meyer, 2014). system translates only about half of them with a correct noun (9 out of 20), while the post-edited system translates correctly 19 out of 20. 5 Related Work We briefly review in this section several previous studies from which the present one has benefited. Our idea is built upon the one-sense-per-discourse hypothesis (Gale et al., 1992) and its application to machine translation is based on the premise that consistency in discourse (Carpuat, 2009) is desirable. The initial compound idea was first published by Mascarell et al. (2014), in which the coreference of compound noun phrases in German (e.g. Nordwand/Wand) was studied and used to improve DE/FR translation by assuming that the last constituent of the compound Y should share the same translation as that of Y in XY . Several other approaches focused on enforcing consistent lexical choice. Tiedemann (2010) proposed a cache-model to enforce consistent translation of phrases across the document. However, caching is sensitive to error propagation, that is, when a phrase is incorrectly"
P15-3002,W10-1737,0,0.0683154,"Missing"
P15-3002,2012.eamt-1.60,0,0.0984312,"ignment. We verify if the translations of Y in both noun phrases are identical or different. Both elements comprising the compound structure XY /Y are identified, for the standard cases, with only one possible XY referring to one Y . The translation of both words are provided by the baseline SMT system, and our system subsequently verifies if the translations of Y in both noun phrases are identical or different. We keep them intact in the first case, while in the second 3 Experimental Settings The experiments are carried out on two different parallel corpora: the WIT3 Chinese-English dataset (Cettolo et al., 2012) with transcripts of TED lectures and their translations, and the Text+Berg German-French corpus (Bubenhofer et al., 2013), a collection of articles from the year4 Upon manual examination, we found that using the most recent XY was not a reliable candidate for the antecedent. 5 In fact, we can use the translation of Y as a translation candidate for XY . Our observations show that this helps to improve BLEU scores, but does not affect the specific scoring of Y in Section 4. 1 Using the Stanford Word Segmenter available from http://nlp.stanford.edu/software/segmenter.shtml. 2 Using the Stanford"
P15-3002,loaiciga-etal-2014-english,1,0.84855,"idates for Y in the phrase table (Mascarell et al., 2014). 4.2 Manual Evaluation of Undecided Cases When both the baseline and one of our systems generate translations of Y which differ from the reference, it is not possible to compare the translations without having them examined by human subjects. This was done for the 73 such cases of the ZH/EN post-editing system. Three of the authors, working independently, considered each 6 12 Average of 1 3 P3 i=1 |scorei − mean |over all ratings . discourse connectives (Meyer and Popescu-Belis, 2012) or the expected tenses of verb phrase translations (Loaiciga et al., 2014). Quite naturally, there are analogies between our work and studies of pronoun translation (Le Nagard and Koehn, 2010; Hardmeier and Federico, 2010; Guillou, 2012), with the notable difference that pronominal anaphora resolution remains a challenging task. Finally, our work and its perspectives contribute to the general objective of using discourse-level information to improve MT (Hardmeier, 2014; Meyer, 2014). system translates only about half of them with a correct noun (9 out of 20), while the post-edited system translates correctly 19 out of 20. 5 Related Work We briefly review in this sec"
P15-3002,H92-1045,0,0.725085,"derico, 2010; Guillou, 2012), with the notable difference that pronominal anaphora resolution remains a challenging task. Finally, our work and its perspectives contribute to the general objective of using discourse-level information to improve MT (Hardmeier, 2014; Meyer, 2014). system translates only about half of them with a correct noun (9 out of 20), while the post-edited system translates correctly 19 out of 20. 5 Related Work We briefly review in this section several previous studies from which the present one has benefited. Our idea is built upon the one-sense-per-discourse hypothesis (Gale et al., 1992) and its application to machine translation is based on the premise that consistency in discourse (Carpuat, 2009) is desirable. The initial compound idea was first published by Mascarell et al. (2014), in which the coreference of compound noun phrases in German (e.g. Nordwand/Wand) was studied and used to improve DE/FR translation by assuming that the last constituent of the compound Y should share the same translation as that of Y in XY . Several other approaches focused on enforcing consistent lexical choice. Tiedemann (2010) proposed a cache-model to enforce consistent translation of phrase"
P15-3002,D11-1084,0,0.315107,"2014), in which the coreference of compound noun phrases in German (e.g. Nordwand/Wand) was studied and used to improve DE/FR translation by assuming that the last constituent of the compound Y should share the same translation as that of Y in XY . Several other approaches focused on enforcing consistent lexical choice. Tiedemann (2010) proposed a cache-model to enforce consistent translation of phrases across the document. However, caching is sensitive to error propagation, that is, when a phrase is incorrectly translated and cached, the model propagates the error to the following sentences. Gong et al. (2011) later extended Tiedemann’s proposal by initializing the cache with phrase pairs from similar documents at the beginning of the translation and by also applying a topic cache, which was introduced to deal with the error propagation issue. Xiao et al. (2011) defined a three step procedure that enforces the consistent translation of ambiguous words, achieving improvements for EN/ZH. Ture et al. (2012) encouraged consistency for AR/EN MT by introducing cross-sentence consistency features to the translation model, while Alexandrescu and Kirchhoff (2009) enforced similar translations to sentences h"
P15-3002,E12-3001,0,0.0176957,"Y which differ from the reference, it is not possible to compare the translations without having them examined by human subjects. This was done for the 73 such cases of the ZH/EN post-editing system. Three of the authors, working independently, considered each 6 12 Average of 1 3 P3 i=1 |scorei − mean |over all ratings . discourse connectives (Meyer and Popescu-Belis, 2012) or the expected tenses of verb phrase translations (Loaiciga et al., 2014). Quite naturally, there are analogies between our work and studies of pronoun translation (Le Nagard and Koehn, 2010; Hardmeier and Federico, 2010; Guillou, 2012), with the notable difference that pronominal anaphora resolution remains a challenging task. Finally, our work and its perspectives contribute to the general objective of using discourse-level information to improve MT (Hardmeier, 2014; Meyer, 2014). system translates only about half of them with a correct noun (9 out of 20), while the post-edited system translates correctly 19 out of 20. 5 Related Work We briefly review in this section several previous studies from which the present one has benefited. Our idea is built upon the one-sense-per-discourse hypothesis (Gale et al., 1992) and its a"
P15-3002,W12-0117,1,0.841939,"since it only enforces a translation when it appears as one of the translation candidates for Y in the phrase table (Mascarell et al., 2014). 4.2 Manual Evaluation of Undecided Cases When both the baseline and one of our systems generate translations of Y which differ from the reference, it is not possible to compare the translations without having them examined by human subjects. This was done for the 73 such cases of the ZH/EN post-editing system. Three of the authors, working independently, considered each 6 12 Average of 1 3 P3 i=1 |scorei − mean |over all ratings . discourse connectives (Meyer and Popescu-Belis, 2012) or the expected tenses of verb phrase translations (Loaiciga et al., 2014). Quite naturally, there are analogies between our work and studies of pronoun translation (Le Nagard and Koehn, 2010; Hardmeier and Federico, 2010; Guillou, 2012), with the notable difference that pronominal anaphora resolution remains a challenging task. Finally, our work and its perspectives contribute to the general objective of using discourse-level information to improve MT (Hardmeier, 2014; Meyer, 2014). system translates only about half of them with a correct noun (9 out of 20), while the post-edited system tran"
P15-3002,2010.iwslt-papers.10,0,0.0165377,"tems generate translations of Y which differ from the reference, it is not possible to compare the translations without having them examined by human subjects. This was done for the 73 such cases of the ZH/EN post-editing system. Three of the authors, working independently, considered each 6 12 Average of 1 3 P3 i=1 |scorei − mean |over all ratings . discourse connectives (Meyer and Popescu-Belis, 2012) or the expected tenses of verb phrase translations (Loaiciga et al., 2014). Quite naturally, there are analogies between our work and studies of pronoun translation (Le Nagard and Koehn, 2010; Hardmeier and Federico, 2010; Guillou, 2012), with the notable difference that pronominal anaphora resolution remains a challenging task. Finally, our work and its perspectives contribute to the general objective of using discourse-level information to improve MT (Hardmeier, 2014; Meyer, 2014). system translates only about half of them with a correct noun (9 out of 20), while the post-edited system translates correctly 19 out of 20. 5 Related Work We briefly review in this section several previous studies from which the present one has benefited. Our idea is built upon the one-sense-per-discourse hypothesis (Gale et al.,"
P15-3002,P03-1021,0,0.0144953,"imize the likelihood of coreference. In German, less restrictive rules selected 7,365 XY /Y pairs (a rate of one every 40 sentences). Still, in what follows, we randomly selected 261 XY /Y pairs for the DE/FR test data, to match their number in the ZH/EN test data. Our baseline SMT system is the Moses phrasebased decoder (Koehn et al., 2007), trained over tokenized and true-cased data. The language models were built using SRILM (Stolcke et al., 2011) at order 3 (i.e. up to trigrams) using the default smoothing method (i.e. Good-Turing). Optimization was done using Minimum Error Rate Training (Och, 2003) as provided with Moses. The effectiveness of proposed systems is measured in two ways. First, we use BLEU (Papineni et al., 2002) for overall evaluation, to verify whether our systems provide better translation for entire texts. Then, we focus on the XY /Y pairs and count the number of cases in which the translations of Y match the reference or not, which can be computed automatically using the alignments. However, the automatic comparison of a system’s translation with the reference is not entirely informative, because even if the two differ, the system’s translation can still be acceptable."
P15-3002,D12-1108,0,0.0604372,"vements for EN/ZH. Ture et al. (2012) encouraged consistency for AR/EN MT by introducing cross-sentence consistency features to the translation model, while Alexandrescu and Kirchhoff (2009) enforced similar translations to sentences having a similar graph representation. Our work is an instance of a recent trend aiming to go beyond sentence-by-sentence MT, by using semantic information from previous sentences to constrain or correct the decoding of the current one. In this paper, we compared caching and post-editing as ways of achieving this goal, but a document-level decoder such as Docent (Hardmeier et al., 2012) could be used as well. In other studies, factored translation models (Koehn and Hoang, 2007) have been used with the same purpose, by incorporating contextual information into labels used to indicate the meaning of ambiguous 6 Conclusion and Perspectives We presented a method to enforce the consistent translation of coreferences to a compound, when the coreference matches the head noun of the compound. Experimental results showed that baseline SMT systems often translate coreferences to compounds consistently for DE/FR, but much less so for ZH/EN. For a significant number of cases in which th"
P15-3002,P02-1040,0,0.0921765,"40 sentences). Still, in what follows, we randomly selected 261 XY /Y pairs for the DE/FR test data, to match their number in the ZH/EN test data. Our baseline SMT system is the Moses phrasebased decoder (Koehn et al., 2007), trained over tokenized and true-cased data. The language models were built using SRILM (Stolcke et al., 2011) at order 3 (i.e. up to trigrams) using the default smoothing method (i.e. Good-Turing). Optimization was done using Minimum Error Rate Training (Och, 2003) as provided with Moses. The effectiveness of proposed systems is measured in two ways. First, we use BLEU (Papineni et al., 2002) for overall evaluation, to verify whether our systems provide better translation for entire texts. Then, we focus on the XY /Y pairs and count the number of cases in which the translations of Y match the reference or not, which can be computed automatically using the alignments. However, the automatic comparison of a system’s translation with the reference is not entirely informative, because even if the two differ, the system’s translation can still be acceptable. Therefore, we analyzed these “undecided” situations BASELINE C ACHING P OST- EDITING O RACLE ZH/EN 11.18 11.23 11.27 11.30 DE/FR"
P15-3002,D07-1091,0,0.018175,"-sentence consistency features to the translation model, while Alexandrescu and Kirchhoff (2009) enforced similar translations to sentences having a similar graph representation. Our work is an instance of a recent trend aiming to go beyond sentence-by-sentence MT, by using semantic information from previous sentences to constrain or correct the decoding of the current one. In this paper, we compared caching and post-editing as ways of achieving this goal, but a document-level decoder such as Docent (Hardmeier et al., 2012) could be used as well. In other studies, factored translation models (Koehn and Hoang, 2007) have been used with the same purpose, by incorporating contextual information into labels used to indicate the meaning of ambiguous 6 Conclusion and Perspectives We presented a method to enforce the consistent translation of coreferences to a compound, when the coreference matches the head noun of the compound. Experimental results showed that baseline SMT systems often translate coreferences to compounds consistently for DE/FR, but much less so for ZH/EN. For a significant number of cases in which the noun phrase Y had multiple meanings, our system reduced the frequency of mistranslations in"
P15-3002,W10-2602,0,0.770009,"anslated word), in which case we apply post-editing as above on the word preceding Y , if it is aligned. In the caching method (Mascarell et al., 2014), once an XY compound is identified, we obtain the translation of the Y part of the compound through the word alignment given by the SMT decoder. Next, we check that this translation appears as a translation of Y in the phrase table, and if so, we cache both Y and the obtained translation. We then enforce the cached translation every time a coreference Y to XY is identified. Note that this is different from the probabilistic caching proposed by Tiedemann (2010), because in our case the cached translation is deterministically enforced as the translation of Y . Enforcing the Translation of Y Two language-independent methods have been designed to ensure that the translations of XY and Y are a consistent: post-editing and caching. The second one builds upon an earlier proposal tested only on DE/FR with subjective evaluations (Mascarell et al., 2014). In the post-editing method, for each XY /Y pair, the translations of XY and Y by a baseline SMT system (see Section 3) are first identified through word alignment. We verify if the translations of Y in both"
P15-3002,2011.mtsummit-papers.13,0,0.364573,"other approaches focused on enforcing consistent lexical choice. Tiedemann (2010) proposed a cache-model to enforce consistent translation of phrases across the document. However, caching is sensitive to error propagation, that is, when a phrase is incorrectly translated and cached, the model propagates the error to the following sentences. Gong et al. (2011) later extended Tiedemann’s proposal by initializing the cache with phrase pairs from similar documents at the beginning of the translation and by also applying a topic cache, which was introduced to deal with the error propagation issue. Xiao et al. (2011) defined a three step procedure that enforces the consistent translation of ambiguous words, achieving improvements for EN/ZH. Ture et al. (2012) encouraged consistency for AR/EN MT by introducing cross-sentence consistency features to the translation model, while Alexandrescu and Kirchhoff (2009) enforced similar translations to sentences having a similar graph representation. Our work is an instance of a recent trend aiming to go beyond sentence-by-sentence MT, by using semantic information from previous sentences to constrain or correct the decoding of the current one. In this paper, we com"
P15-3002,N12-1046,0,\N,Missing
P15-3002,P07-2045,0,\N,Missing
P98-2172,J94-4002,0,0.357882,"n&apos;t seem to use activation cues. Another system (Luperfoy 1992) uses ""discourse pegs"" to model referents and was applied successfully to a man-machine dialogue task. From a theoretical point of view, the model presented by Appelt and Kronfeld (1987) is in its background close to ours. Being further developed according to the speech acts theory, it relies however on models of intentions and beliefs of communicating agents which seem uneasy to implement for discourse understanding. 2.2 The activation of an MR is computed according to salience factors (this technique is described for instance by Lappin and Leass (1994)). Our salience factors are: de-activation in time, re-activation by various types of RE, re-activation according to the function of the RE. Among the MRs which pass the selection, activation is used to decide whether the current RE is added to an MR (the most active) or if a new MR is created. Activation is thus a dynamic factor, which changes for each MR according to the position i n the text and the previous reference resolution decisions. 2 models Comparison with other works Robust, lower-level systems 2.3 Advantages of the MR paradigm Theoretical studies of discourse processing have long"
P98-2172,P92-1004,0,0.329079,"Missing"
P98-2172,W97-1314,1,0.827386,"adigm, as it privileges links between individual REs, from which the MRs could even be built a posteriori, using the coreference chains. But here MRs are also characterized by an intrinsic activation factor, evolving along the text, which cannot be managed in the coreference paradigm. resentations for discourse referents. However, implementations of running systems have rather focused on anaphora or coreference. Our purpose here is to show how a simplified computational model of discourse reference can be implemented and give significant results for reference resolution; we showed previously (Popescu-Belis and Robba 1997) that it was also relevant for pronoun resolution. 1.5 Activation Some of the robust approaches derive from anaphora resolution (e.g., Boguraev and Kennedy (1996)) because the antecedent / anaphoric links are a particular sort of coreference links, which disambiguate pronouns. Most of these systems however remain within the coreference paradigm, as defined by the MUC-6 coreference task. Numerous low-level techniques have been developed, using generally pattern-matching between potentially coreferent strings (e.g., McCarthy and Lehnert 1995). An interesting solution has been proposed by Lin (19"
P98-2172,W97-1514,1,0.888073,"Missing"
P98-2172,M95-1005,0,\N,Missing
P98-2172,M95-1010,0,\N,Missing
P98-2172,M95-1017,0,\N,Missing
P98-2172,C69-7001,0,\N,Missing
P98-2172,C69-6902,0,\N,Missing
popescu-belis-etal-2002-electronic,C00-2173,1,\N,Missing
popescu-belis-etal-2002-electronic,2001.mtsummit-papers.16,0,\N,Missing
popescu-belis-etal-2004-building,W04-2319,0,\N,Missing
popescu-belis-etal-2004-building,J97-1005,0,\N,Missing
popescu-belis-etal-2004-building,J00-4005,0,\N,Missing
popescu-belis-etal-2004-building,P03-1071,0,\N,Missing
popescu-belis-etal-2004-building,J00-3003,0,\N,Missing
popescu-belis-etal-2004-building,W04-2328,1,\N,Missing
popescu-belis-etal-2004-online,M95-1005,0,\N,Missing
popescu-belis-etal-2004-online,J00-4005,0,\N,Missing
popescu-belis-etal-2004-online,W01-1612,0,\N,Missing
popescu-belis-etal-2004-online,J96-2004,0,\N,Missing
popescu-belis-etal-2004-online,W99-0309,0,\N,Missing
popescu-belis-etal-2004-online,salmon-alt-romary-2004-towards,1,\N,Missing
popescu-belis-etal-2008-task,traum-etal-2004-evaluation,0,\N,Missing
popescu-belis-etal-2008-task,P97-1035,0,\N,Missing
popescu-belis-etal-2008-task,W02-0219,0,\N,Missing
popescu-belis-etal-2008-task,popescu-belis-georgescul-2006-tqb,1,\N,Missing
popescu-belis-etal-2012-discourse,cartoni-meyer-2012-extracting,1,\N,Missing
popescu-belis-etal-2012-discourse,W12-0117,1,\N,Missing
popescu-belis-etal-2012-discourse,W11-1211,1,\N,Missing
popescu-belis-etal-2012-discourse,P02-1040,0,\N,Missing
popescu-belis-etal-2012-discourse,W11-2022,1,\N,Missing
popescu-belis-etal-2012-discourse,P07-2045,0,\N,Missing
popescu-belis-etal-2012-discourse,W10-1737,0,\N,Missing
popescu-belis-etal-2012-discourse,P10-1144,0,\N,Missing
popescu-belis-etal-2012-discourse,2010.iwslt-papers.10,0,\N,Missing
popescu-belis-etal-2012-discourse,prasad-etal-2008-penn,0,\N,Missing
popescu-belis-etal-2012-discourse,P09-2004,0,\N,Missing
popescu-belis-etal-2012-discourse,2005.mtsummit-papers.11,0,\N,Missing
Q18-1044,P05-1048,0,0.0615184,"rman pair shown in Table 8 demonstrate that our baseline is strong and that our model is competitive with respect to recent models that leverage contextual information in different ways. 7 Related Work Word sense disambiguation aims to identify the sense of a word appearing in a given context (Agirre and Edmonds, 2007). Resolving word sense ambiguities should be useful, in particular, for lexical choice in MT. An initial investigation found that a statistical MT system that makes use of off-the-shelf WSD does not yield significantly better quality translations than an SMT system not using it (Carpuat and Wu, 2005). However, several studies (Cabezas and Resnik, 2005; Vickrey et al., 2005; Carpuat and Wu, 2007; Chan et al., 2007) reformulated the task of WSD for SMT and showed that integrating the ambiguity information generated from modified WSD improved 645 8 used in these studies. In previous work (Pu et al., 2017), we used this information to perform sense induction on source-side data using k-means and demonstrated improvement with factored phrasebased SMT but not NMT. Neural MT became the state of the art (Sutskever et al., 2014; Bahdanau et al., 2015). Instead of working directly at the discrete s"
Q18-1044,E09-1005,0,0.0401028,"luster all occurrences wi of a given word type Wt , represented as word vectors ui , according to the similarity of their senses, as inferred from the similarity of the context vectors. We compare the algorithms empirically in §5. K-means Clustering. The original k-means algorithm (MacQueen, 1967) aims to partition a set of items, which are here tokens w1 , w2 , . . . , wn of the same word type Wt , represented through their 5 k X X code.google.com/archive/p/word2vec/. 637 Random Walks. Finally, we also consider for comparison a WSD method based on random walks on the WordNet knowledge graph (Agirre and Soroa, 2009; Agirre et al., 2014) available from the UKB toolkit.6 In the graph, senses correspond to nodes and the relationships or dependencies between pairs of senses correspond to the edges between those nodes. From each input sentence, we extract its content words (nouns, verbs, adjectives, and adverbs) that have an entry in the WordNet weighted graph. The method calculates the probability of a random walk over the graph from a target word’s sense ending on any other sense in the graph, and determines the sense with the highest probability for each analyzed word. In this case, the random walk algori"
Q18-1044,D07-1007,0,0.0606955,"ive with respect to recent models that leverage contextual information in different ways. 7 Related Work Word sense disambiguation aims to identify the sense of a word appearing in a given context (Agirre and Edmonds, 2007). Resolving word sense ambiguities should be useful, in particular, for lexical choice in MT. An initial investigation found that a statistical MT system that makes use of off-the-shelf WSD does not yield significantly better quality translations than an SMT system not using it (Carpuat and Wu, 2005). However, several studies (Cabezas and Resnik, 2005; Vickrey et al., 2005; Carpuat and Wu, 2007; Chan et al., 2007) reformulated the task of WSD for SMT and showed that integrating the ambiguity information generated from modified WSD improved 645 8 used in these studies. In previous work (Pu et al., 2017), we used this information to perform sense induction on source-side data using k-means and demonstrated improvement with factored phrasebased SMT but not NMT. Neural MT became the state of the art (Sutskever et al., 2014; Bahdanau et al., 2015). Instead of working directly at the discrete symbol level as SMT, it projects and manipulates the source sequence of discrete symbols in a con"
Q18-1044,2012.eamt-1.60,0,0.0106597,"98 3,844 1,475 3,915 1,647 2015. Second, for EN/FR and EN/ES, we use data from WMT 2014 (Bojar et al., 2014)9 with 5.3M sentences for EN/FR and 3.8M sentences for EN/ES. Here, the development sets are NewsTest 2008 and 2009, and the testing sets are NewsTest 2012 and 2013 for both language pairs. The source sides of these larger additional sets contain around 3,500 unique English word forms with more than one sense in WordNet, and our system generates ca. 8K different noun labels and 2.5K verb labels for each set. Finally, for comparison purposes and model selection, we use the WIT3 Corpus10 (Cettolo et al., 2012), a collection of transcripts of TED talks. We use 150K sentence pairs for training, 5K for development and 50K for testing. Pre-processing. Before assigning sense labels, we tokenize all the texts and identify the parts of speech using the Stanford POS tagger.11 Then, we filter out the stopwords and the nouns that are proper names according to the Stanford Name Entity Recognizer.11 Furthermore, we convert the plural forms of nouns to their singular forms and the verb forms to infinitives using the stemmer and lemmatizer from NLTK,12 which is essential because WordNet has description entries o"
Q18-1044,P07-1005,0,0.0522818,"cent models that leverage contextual information in different ways. 7 Related Work Word sense disambiguation aims to identify the sense of a word appearing in a given context (Agirre and Edmonds, 2007). Resolving word sense ambiguities should be useful, in particular, for lexical choice in MT. An initial investigation found that a statistical MT system that makes use of off-the-shelf WSD does not yield significantly better quality translations than an SMT system not using it (Carpuat and Wu, 2005). However, several studies (Cabezas and Resnik, 2005; Vickrey et al., 2005; Carpuat and Wu, 2007; Chan et al., 2007) reformulated the task of WSD for SMT and showed that integrating the ambiguity information generated from modified WSD improved 645 8 used in these studies. In previous work (Pu et al., 2017), we used this information to perform sense induction on source-side data using k-means and demonstrated improvement with factored phrasebased SMT but not NMT. Neural MT became the state of the art (Sutskever et al., 2014; Bahdanau et al., 2015). Instead of working directly at the discrete symbol level as SMT, it projects and manipulates the source sequence of discrete symbols in a continuous vector space"
Q18-1044,S10-1078,0,0.0838147,"Missing"
Q18-1044,P17-4012,0,0.0474407,"s Settings. Unless otherwise stated, we adopt the following settings in the k-means algorithm, with the implementation provided in Scikit-learn (Pedregosa et al., 2011). We use the definition of each sense for initializing the centroids, and later compare this choice with the use of examples. We set kt , the initial number of clusters, to the number of WordNet senses of each ambiguous word type Wt , and set the window size for the context surrounding each occurrence to c = 8. Neural MT. We build upon the attentionbased neural translation model (Bahdanau et al., 2015) from the OpenNMT toolkit (Klein et al., 2017).13 We use LSTM and not GRU. For the proposed ATT and ATT ini models, we add an Words 2,006 3,876 1,976 3,194 1,987 3,558 1,915 2,210 Table 1: Size of data sets used for machine translation from English to five different target languages (TL). FR = French; DE = German; ES = Spanish; ZH = Chinese; NL = Dutch. 4 Data, Metrics, and Implementation Data Sets. We train and test our sense-aware MT systems on the data shown in Table 1: the UN Corpus7 (Rafalovitch and Dale, 2009) and the Europarl Corpus8 (Koehn, 2005). We first experiment with our models using the same data set and protocol as in our p"
Q18-1044,S15-2049,0,0.125091,"is PageRank (Grin and Page, 1998), which computes a relative structural importance or “rank” for each node. method considered as a practical interpretation of a Dirichlet process (Ferguson, 1973) for nonparametric clustering. In the original analogy, each token is compared to a customer in a restaurant, and each cluster is a table where customers can be seated. A new customer can choose to sit at a table with other customers, with a probability proportional to the numbers of customers at that table, or sit at a new, empty table. In an application to multisense word embeddings, Li and Jurafsky (2015) proposed that the probability to “sit at a table” should also depend on the contextual similarity between the token and the sense modeled by the table. We build upon this idea and bring several modifications that allow for an instantiation with sense-related knowledge from WordNet, as follows. For each word type Wt appearing in the data, we start by fixing the maximal number kt of senses or clusters as the number of senses of Wt in WordNet. This avoids an unbounded number of clusters (as in the original CRP algorithm) and the risk of cluster sparsity by setting a non-arbitrary limit based on"
Q18-1044,2005.mtsummit-papers.11,0,0.0602538,"Missing"
Q18-1044,S10-1079,0,0.0420461,"Missing"
Q18-1044,D15-1200,0,0.161846,"walk algorithm is PageRank (Grin and Page, 1998), which computes a relative structural importance or “rank” for each node. method considered as a practical interpretation of a Dirichlet process (Ferguson, 1973) for nonparametric clustering. In the original analogy, each token is compared to a customer in a restaurant, and each cluster is a table where customers can be seated. A new customer can choose to sit at a table with other customers, with a probability proportional to the numbers of customers at that table, or sit at a new, empty table. In an application to multisense word embeddings, Li and Jurafsky (2015) proposed that the probability to “sit at a table” should also depend on the contextual similarity between the token and the sense modeled by the table. We build upon this idea and bring several modifications that allow for an instantiation with sense-related knowledge from WordNet, as follows. For each word type Wt appearing in the data, we start by fixing the maximal number kt of senses or clusters as the number of senses of Wt in WordNet. This avoids an unbounded number of clusters (as in the original CRP algorithm) and the risk of cluster sparsity by setting a non-arbitrary limit based on"
Q18-1044,N18-1121,0,0.1812,"Missing"
Q18-1044,D14-1113,0,0.0930834,"sistently better than the baselines, with the largest improvements achieved by NMT on EN/FR and EN/ES. The neural systems outperform the phrasebased statistical ones (Pu et al., 2017), which are shown for comparison in the upper part of the table. We compare our proposal to the recent system proposed by Yang et al. (2017), on the 500K-line EN/FR Europarl data set (the differences between their system and ours are listed in §7). We carefully implemented their model by following their paper, since their code is not available. Using the sense embeddings of the multi-sense skip-gram model (MSSG) (Neelakantan et al., 2014) as they 643 0.8 Baseline Our 0.6method Proportion C. I. C. I. 0.6 0.4 0.2 0.0 Table 6: Confusion matrix for our WSD+NMT (ATT ini ) system and our WSD+SMT system against their respective baselines (NMT and SMT), over the Europarl test data, for two language pairs. O>B O=B O&lt;B 2 - Good 1 - Acceptable 0 - Wrong 0.8 Proportion WSD+ NMT WSD+ SMT 1.0 1.0 Baselines EN/FR EN/ES Correct Incorrect Correct Incorrect 134,552 17,145 146,806 16,523 10,551 101,228 8,183 58,387 124,759 13,408 139,800 11,194 9,676 115,633 7,559 71,346 0.4 0.2 deal face mark subject Candidate (a) System ratings. 0.0 deal face"
Q18-1044,D15-1166,0,0.0527114,"t vector ui the algorithm decides whether the token is assigned to one of the sense clusters Sj to which previous tokens have been assigned, or whether it is assigned to a new empty cluster, by selecting the option that has the highest probability, which is computed as follows:   Nj (λ1 s(ui , dtj ) + λ2 s(ui , µj ))     if N 6= 0 (non-empty sense) j P ∝ (3)  γs(ui , dtj )     if N = 0 (empty sense) j 3 Integration with Neural MT 3.1 Baseline Neural MT Model We now present several models integrating WSD into NMT, starting from an attention-based NMT baseline (Bahdanau et al., 2015; Luong et al., 2015). Given a source sentence X with words wx , X = (w1x , w2x , ..., wTx ), the model computes a conditional distribution over translations, expressed as p(Y = (w1y , w2y , ..., wTy 0 )|X). The neural network model consists of an encoder, a decoder, and an attention mechanism. First, each source word wtx ∈ V is projected from a one-hot word vector into a continuous vector space representation xt . Then, the resulting sequence of word vectors is read by the bidirectional encoder, which consists of forward and backward recurrent networks (RNNs). The forward RNN reads the sequence in → − → − → − lef"
Q18-1044,P02-1040,0,0.104018,"Missing"
Q18-1044,S10-1081,0,0.0441185,"Missing"
Q18-1044,W17-4701,1,0.33706,"aging. 2.2 S = arg min S ||u − µi ||2 (1) i=1 u∈Si At the first iteration, when there are no clusters yet, the algorithm selects k random points as centroids of the k clusters. Then, at each subsequent iteration t, the algorithm calculates for each candidate cluster a new centroid of the observations, defined as their average vector, as follows: µit+1 = 1 X uj |Sit | t (2) uj ∈Si In an earlier application of k-means to phrasebased statistical MT, but not neural MT, we made several modifications to the original k-means algorithm to make it adaptive to the word senses observed in training data (Pu et al., 2017). We maintain these changes and summarize them briefly here. The initial number of clusters kt for each ambiguous word type Wt is set to the number of its senses in WordNet, either considering only the senses that have a definition or those that have an example. The centroids of the clusters are initialized to the vectors representing the senses from WordNet, either using their definition vectors dtj or their example vectors etj . These initializations are thus a form of weak supervision of the clustering process. Finally, and most importantly, after running the k-means algorithm, the number o"
Q18-1044,H05-1097,0,0.0554247,"our model is competitive with respect to recent models that leverage contextual information in different ways. 7 Related Work Word sense disambiguation aims to identify the sense of a word appearing in a given context (Agirre and Edmonds, 2007). Resolving word sense ambiguities should be useful, in particular, for lexical choice in MT. An initial investigation found that a statistical MT system that makes use of off-the-shelf WSD does not yield significantly better quality translations than an SMT system not using it (Carpuat and Wu, 2005). However, several studies (Cabezas and Resnik, 2005; Vickrey et al., 2005; Carpuat and Wu, 2007; Chan et al., 2007) reformulated the task of WSD for SMT and showed that integrating the ambiguity information generated from modified WSD improved 645 8 used in these studies. In previous work (Pu et al., 2017), we used this information to perform sense induction on source-side data using k-means and demonstrated improvement with factored phrasebased SMT but not NMT. Neural MT became the state of the art (Sutskever et al., 2014; Bahdanau et al., 2015). Instead of working directly at the discrete symbol level as SMT, it projects and manipulates the source sequence of dis"
Q18-1044,2009.mtsummit-posters.15,0,0.0250179,"Missing"
Q18-1044,N18-1124,1,0.825847,"., 2016) or generated from both sides of word dependencies (Su et al., 2015). However, apart from the sense graph, WordNet also provides textual information such as sense definitions and examples, which should be useful for WSD, but were not Choi et al. (2017) attempts to improve NMT by integrating context vectors associated to source words into the generation process during decoding. The model proposed by Zhang et al. (2017) is aware of previous attended words on the source side in order to better predict which words will be attended in future. The self-attentive residual decoder designed by Werlen et al. (2018) leverages the contextual information from previously translated words on the target side. BLEU scores on the English–German pair shown in Table 8 demonstrate that our baseline is strong and that our model is competitive with respect to recent models that leverage contextual information in different ways. 7 Related Work Word sense disambiguation aims to identify the sense of a word appearing in a given context (Agirre and Edmonds, 2007). Resolving word sense ambiguities should be useful, in particular, for lexical choice in MT. An initial investigation found that a statistical MT system that m"
Q18-1044,W17-4702,0,0.041133,"generates only one embedding for each word type, regardless of its possibly different senses, as analyzed, for example, by Hill et al. (2017). Several studies proposed efficient nonparametric models for monolingual word sense representation (Neelakantan et al., 2014; Li and Jurafsky, 2015; Bartunov et al., 2016; Liu et al., 2017), but left open the question whether sense representations can help neural MT by reducing word ambiguity. Recent studies integrate the additional sense assignment with neural MT based on these approaches, either by adding such sense assignments as additional features (Rios et al., 2017) or by merging the context information on both sides of parallel data for encoding and decoding (Choi et al., 2017). Yang et al. (2017) recently proposed to add sense information by using weighted sense embeddings as input to neural MT. The sense labels were generated by a MSSG model (Neelakantan et al., 2014), and the context vector used for sense weight generation was computed from the output of a bidirectional RNN. Finally, the weighted average sense embeddings were used in place of the word embedding for the NMT encoder. The numerical results given in §6 show that our options for using sen"
Q18-1044,D15-1145,0,0.0168514,"rming self-learned word sense induction instead of using pre-specified word senses as traditional WSD does. However, they created the risk of discovering sense clusters that do not correspond to the senses of words actually needed for MT. Hence, they left open an important question, namely, whether WSD based on semantic resources such as WordNet (Fellbaum, 1998) can be successfully integrated with SMT. Several studies integrated sense information as features to SMT, either obtained from the sense graph provided by WordNet (Neale et al., 2016) or generated from both sides of word dependencies (Su et al., 2015). However, apart from the sense graph, WordNet also provides textual information such as sense definitions and examples, which should be useful for WSD, but were not Choi et al. (2017) attempts to improve NMT by integrating context vectors associated to source words into the generation process during decoding. The model proposed by Zhang et al. (2017) is aware of previous attended words on the source side in order to better predict which words will be attended in future. The self-attentive residual decoder designed by Werlen et al. (2018) leverages the contextual information from previously tr"
Q18-1044,P14-1137,0,0.0175742,"–0.57 BLEU points compared with baselines. Recently, Tang et al. (2016) used only the supersenses from WordNet (coarse-grained semantic labels) for automatic WSD, using maximum entropy classification or sense embeddings learned using word2vec. When combining WSD with SMT using a factored model, Tang et al. improved BLEU scores by 0.7 points on average, though with large differences between their three test subsets (IT Q&A pairs). Although these reformulations of the WSD task proved helpful for SMT, they did not determine whether actual source-side senses are helpful or not for end-to-end SMT. Xiong and Zhang (2014) attempted to answer this question by performing self-learned word sense induction instead of using pre-specified word senses as traditional WSD does. However, they created the risk of discovering sense clusters that do not correspond to the senses of words actually needed for MT. Hence, they left open an important question, namely, whether WSD based on semantic resources such as WordNet (Fellbaum, 1998) can be successfully integrated with SMT. Several studies integrated sense information as features to SMT, either obtained from the sense graph provided by WordNet (Neale et al., 2016) or gener"
W04-0710,J95-2003,0,0.479336,"Section 6, with conclusions about their relevance. Section 7 outlines the applications of the ref2doc algorithm to the exploitation of documents in meeting processing and retrieval applications. 2 Challenges of Reference Resolution over a Restricted Domain From a cognitive point of view, the role of referring expressions in discourse is to specify the entities about which the speaker talks. It has long been observed that a more accurate view is that REs rather specify representations of entities in the speaker’s or hearer’s mind, an abstraction called discourse entities or DEs (Sidner, 1983; Grosz et al., 1995). Reference resolution can be defined as the construction of the discourse entities specified by referring expressions, or rather, the construction of computational representations of DEs. This difficult but important task in discourse understanding by computers appears to be more tractable when enough knowledge about a domain is available to a system (Gaizauskas and Humphreys, 1997), or when the representations are considerably simplified (Popescu-Belis et al., 1998). The coreference and anaphoric links, that is, links between REs only, are somewhat different aspects of the phenomenon of refe"
W04-0710,J95-1003,0,0.0290235,"nt content, such as the persons mentioned in an article. Reference resolution in a restricted domain presents similarities with problems in natural language generation (NLG) and in command dialogs, that is, when the sets of referents are known a priori to the system. In NLG, the problem is to generate REs from existing computational descriptions of entities—see Paraboni and van Deemter (2002) for an application to intra-document references. In command dialogs, the problem is to match the REs produced by the user against the objects managed by the interface, again known formally to the system (Huls et al., 1995; Skantze, 2002). 4. Connect or match each RE to the document element it refers to. Each of these components can be further subdivided. Our main focus here is task (4). For this task, an evaluation procedure, an algorithm, and its evaluation are provided respectively in Sections 4.3, 5, and 6. Task (3) is discussed below in Section 3.2.1. Task (1), which amounts more or less to automated speech recognition, is of course a standard one, for which the performance level, as measured by the word error rate (WER), depends on the microphone used, the environment, the type of the meeting, etc. To fac"
W04-0710,lisowska-etal-2004-user,1,0.766439,"ech alignment. 7.2 Overall Application: Meeting Processing and Retrieval A promising use of human dialog understanding is for the processing and retrieval of staff or business meetings (Armstrong et al., 2003). When meetings deal with one or several documents, it is important to link in a precise manner each episode or even utterance of the meeting to the sections of the documents that they refer to. Considering users who have missed a meeting or want to review a meeting that they attended, this alignment is required for two types of queries that appear in recent studies of user requirements (Lisowska et al., 2004). First, the users could look for episodes of a meeting in which a particular section of a given document was discussed, so that they can learn what was said about that section. Second, the relevant documents could automatically be displayed when the users browse a given episode of a meeting—so that a rich, multimodal context of the meeting episode is presented. 8 Conclusion This article described a framework and an algorithm for solving references made to documents in meeting recordings by linking referring expressions to the document elements they denote. The implementation of the algorithm,"
W04-0710,P98-2172,1,0.667734,"r specify representations of entities in the speaker’s or hearer’s mind, an abstraction called discourse entities or DEs (Sidner, 1983; Grosz et al., 1995). Reference resolution can be defined as the construction of the discourse entities specified by referring expressions, or rather, the construction of computational representations of DEs. This difficult but important task in discourse understanding by computers appears to be more tractable when enough knowledge about a domain is available to a system (Gaizauskas and Humphreys, 1997), or when the representations are considerably simplified (Popescu-Belis et al., 1998). The coreference and anaphoric links, that is, links between REs only, are somewhat different aspects of the phenomenon of reference (Devitt and Sterelny, 1999; Lycan, 2000). Coreference is the relation between two REs that specify the same DE. Anaphora is a relation between two REs, called antecedent RE and anaphoric RE, where the DE specified by the latter is determined by knowledge of the DE specified by the former. In other terms, the DE specified by the anaphoric RE cannot be fully determined without knowledge of the antecedent RE. Depending on how the referent of the second RE is determ"
W04-0710,popescu-belis-etal-2004-building,1,0.835441,", the objects contained in the PDF file (text, images, and graphics) are extracted and matched with the result of the layout analysis; for instance, text is associated to physical (graphical) blocks. Finally, the cleaned PDF is parsed into a unique tree, which can be transformed Two important elements for testing are the available data (4.2), which must be specifically annotated (4.1), and a scoring procedure (4.3), which is quite straightforward, and provides several scores. 4.1 Annotation Model The annotation model for the references to documents builds upon a shallow dialog analysis model (Popescu-Belis et al., 2004), implemented in XML. The main idea is to add external annotation blocks that do not alter the master resource—here the timed meeting transcription, divided into separate channels. However, REs are annotated on the dialog transcription itself. A more principled solution, but more complex to implement, would be to index the master transcriptions by the number of words, then externalize the annotation of REs as well (SalmonAlt and Romary, 2004). As shown in Figure 2, the ref pointers from the REs to the document elements are grouped in a ref2doc block at the end of the document, using as attribu"
W04-0710,salmon-alt-romary-2004-towards,0,0.137461,"Missing"
W04-0710,J00-4005,0,0.221986,"Missing"
W04-0710,C98-2167,1,\N,Missing
W04-2328,J97-1002,0,0.042003,"Missing"
W04-2328,W98-0319,0,0.0464834,"Missing"
W04-2328,N03-5008,0,0.0465289,"Missing"
W04-2328,W04-2319,0,0.0247533,"t al., 2000), or to locate “hot spots” in meetings (Wrede and Shriberg, 2003). 3 Available Data and Annotations: ICSI Meeting Recorder The volume of available annotated data suffers from the diversity of DA tagsets (Klein and Soria, 1998). One of the most significant resources is the Switchboard corpus mentioned above, but telephone conversations have many differences with multi-party meetings. Apart from the data recently available in the IM2 project, results reported in this paper make use of the ICSI Meeting Recording (MR) corpus of transcribed and annotated dialogues (Morgan et al., 2003; Shriberg et al., 2004)3 . 3.1 Overview of ICSI MR Corpus The ICSI-MR corpus consists of 75 one-hour recordings of staff meetings, each involving up to eight speakers on separate mike channels. Each channel was manually transcribed and timed, then annotated with dialogue act and adjacency pair information (Shriberg et al., 2004). Following a preliminary release in November 2003 (sound files, transcriptions, and annotations), the full corpus was released in February 2004 to IM2 partners. The dialogue act annotation makes use of the preexisting segmentation of each channel into (prosodic) utterances, sometimes segment"
W04-2328,J00-3003,0,0.068317,"Missing"
W11-1105,E09-1005,0,0.0132281,"visiting probability proposed here are hitting time and Personalized PageRank mentioned in Section 3.3. Hitting time has been used in various studies as a distance measure in graphs, e.g. for dimensionality reduction (Saerens et al., 2004) or for collaborative filtering in a recommender system (Brand, 2005). Hitting time was also used for link prediction in social networks along with other distances (Liben-Nowell and Kleinberg, 2003), or for semantic query suggestion using a query/URL bipartite graph (Mei et al., 2008). As for Personalized PageRank, it was used for word sense disambiguation (Agirre and Soroa, 2009), and for measuring lexical relatedness of words in a graph built from WordNet (Hughes and Ramage, 2007). 7 Conclusion We proposed a model for measuring text semantic relatedness based on knowledge embodied in Wikipedia, seen here as document network with two types of links – hyperlinks and lexical similarity ones. We have used visiting probability to measure proximity between weighted sets of nodes, and have proposed approximation algorithms to make computation efficient for large graphs (more than one million nodes and 40 million links) and large text clustering datasets (20,000 documents in"
W11-1105,D07-1061,0,0.0355648,". Hitting time has been used in various studies as a distance measure in graphs, e.g. for dimensionality reduction (Saerens et al., 2004) or for collaborative filtering in a recommender system (Brand, 2005). Hitting time was also used for link prediction in social networks along with other distances (Liben-Nowell and Kleinberg, 2003), or for semantic query suggestion using a query/URL bipartite graph (Mei et al., 2008). As for Personalized PageRank, it was used for word sense disambiguation (Agirre and Soroa, 2009), and for measuring lexical relatedness of words in a graph built from WordNet (Hughes and Ramage, 2007). 7 Conclusion We proposed a model for measuring text semantic relatedness based on knowledge embodied in Wikipedia, seen here as document network with two types of links – hyperlinks and lexical similarity ones. We have used visiting probability to measure proximity between weighted sets of nodes, and have proposed approximation algorithms to make computation efficient for large graphs (more than one million nodes and 40 million links) and large text clustering datasets (20,000 documents in 20 Newsgroups). Results on the document clustering task showed an improvement using both word cooccurre"
W11-1105,W09-3206,0,0.0204704,"trary lengths. Explicit Semantic Analysis (Gabrilovich and Markovitch, 2007), instead of mapping a text to a node or a small group of nodes in a taxonomy, maps the text to the entire collection of available concepts, by computing the degree of affinity of each concept to the input text. Similarity is measured in the new concept space. ESA does not use the link structure or other structured knowledge from Wikipedia. Moreover, by walking over a content similarity graph, our method benefits from a nonlinear distance measure according to the paths consisting of small neighborhoods. In the work of Yeh et al. (2009), a graph of documents and hyperlinks is computed from Wikipedia, then a Personalized PageRank (Haveliwala, 2003) is computed for each text fragment, with the teleport vector being the one resulting from the ESA algorithm cited above. To compute semantic relatedness between two texts, Yeh et al. (2009) simply compare their personalized page rank vectors. By comparison, in our method, we also consider in addition to 35 hyperlinks the effect of word co-occurrence between article contents. The use of visiting probability also gives different results over personalized page rank, as it measures dif"
W11-1211,2009.jeptalnrecital-position.1,0,0.189017,"Missing"
W11-1211,W97-0122,0,0.0876615,"Missing"
W11-1211,2005.mtsummit-papers.11,0,0.0156296,"Missing"
W11-2022,W11-1211,1,0.806552,"several possible “senses” in context. This paper studies the manual and automated disambiguation of three ambiguous connectives in two languages: alors que in French, since and while in English. We will show how the multilingual per2 2.1 Explicit Connectives and their Translation Three Multi-functional Connectives Discourse connectives form a functional category of lexical items that are used to mark coherence relations such as Cause or Contrast between units of discourse. Along with other function words, many connectives appear among the most frequent words, as shown for instance by counts (Cartoni et al., 2011) over the Europarl corpus (Koehn, 2005). The Penn Discourse Treebank (Prasad et al., 2008) (see Section 3.1 below) includes around 100 connective types, but the exact number varies across studies, 194 Proceedings of the SIGDIAL 2011: the 12th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 194–203, c Portland, Oregon, June 17-18, 2011. 2011 Association for Computational Linguistics depending on the discourse theory used to classify them. Among these types, Pitler et al.(2008) have shown that most of them are unambiguous and easy to identify, but others, especially"
W11-2022,P05-1022,0,0.0613806,"Missing"
W11-2022,Y09-1013,0,0.0135657,"ly). 5 Disambiguation Experiments The features for connective classification, the results obtained and a detailed feature analysis are discussed in this section. We show that an automated disambiguation system can be used to determine the most appropriate set of labels, and thus to corroborate the selection we made using translation spotting. 5.1 Features For feature extraction, all the datasets described in Section 4 were processed as follows. The English texts were parsed and POS-tagged by Charniak and Johnson’s (2005) reranking parser. The French texts were POS-tagged with the MElt tagger (Denis and Sagot, 2009) and parsed with MaltParser (Nivre, 2003). As the English parser provides constituency trees, and the parser for French generates dependency trees, the features are slightly different in the two languages. The other features below were extracted using elementary pre-processing of the sentences. For English sentences, we used the following features: the sentence-initial character of the connective (yes/no); the POS tag of the first verb in the sentence; the type of first auxiliary verb in the sentence (if any); the word preceding the connective; the word following the connective; the POS tag of"
W11-2022,P09-1075,0,0.0223973,"idered for the verb directly preceding and the one directly following the connective. Tense was believed to be potentially relevant because since and while can have temporal meanings. The occurrence of auxiliary verbs (be, have, do, or need) may give additional indications about temporal relations in the sentence. We therefore used the types of auxiliary verbs as features, including the elementary conjugations, represented for to be as: be present, be past, be part, be inf, be gerund – and similarly for the other auxiliary verbs, as in (Miltsakaki et al., 2005). As shown by Lin et al. (2010), duVerle and Prendinger (2009) or Wellner et al. (2006), the context of a connective is very important. We therefore extracted the words preceding and following each connective, the verbs and the first and the last word of the sentences. These may include numbers, sometimes indicating a numerical comparison, time expressions, or antonyms, which could indicate contrastive relations, such as rise vs. fall (e.g. It is interesting to see the fundamental stock pickers scream ”foul” on program trading when the markets decline, while hailing the great values still abounding 199 as the markets rise.). For French, we likewise extra"
W11-2022,2005.mtsummit-papers.11,0,0.0791423,"r studies the manual and automated disambiguation of three ambiguous connectives in two languages: alors que in French, since and while in English. We will show how the multilingual per2 2.1 Explicit Connectives and their Translation Three Multi-functional Connectives Discourse connectives form a functional category of lexical items that are used to mark coherence relations such as Cause or Contrast between units of discourse. Along with other function words, many connectives appear among the most frequent words, as shown for instance by counts (Cartoni et al., 2011) over the Europarl corpus (Koehn, 2005). The Penn Discourse Treebank (Prasad et al., 2008) (see Section 3.1 below) includes around 100 connective types, but the exact number varies across studies, 194 Proceedings of the SIGDIAL 2011: the 12th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 194–203, c Portland, Oregon, June 17-18, 2011. 2011 Association for Computational Linguistics depending on the discourse theory used to classify them. Among these types, Pitler et al.(2008) have shown that most of them are unambiguous and easy to identify, but others, especially temporal ones, often signal multiple s"
W11-2022,P11-3009,1,0.843968,"Missing"
W11-2022,W03-3017,0,0.00641546,"connective classification, the results obtained and a detailed feature analysis are discussed in this section. We show that an automated disambiguation system can be used to determine the most appropriate set of labels, and thus to corroborate the selection we made using translation spotting. 5.1 Features For feature extraction, all the datasets described in Section 4 were processed as follows. The English texts were parsed and POS-tagged by Charniak and Johnson’s (2005) reranking parser. The French texts were POS-tagged with the MElt tagger (Denis and Sagot, 2009) and parsed with MaltParser (Nivre, 2003). As the English parser provides constituency trees, and the parser for French generates dependency trees, the features are slightly different in the two languages. The other features below were extracted using elementary pre-processing of the sentences. For English sentences, we used the following features: the sentence-initial character of the connective (yes/no); the POS tag of the first verb in the sentence; the type of first auxiliary verb in the sentence (if any); the word preceding the connective; the word following the connective; the POS tag of the first verb following the connective;"
W11-2022,W09-3715,0,0.0241271,"n 3.1 below) includes around 100 connective types, but the exact number varies across studies, 194 Proceedings of the SIGDIAL 2011: the 12th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 194–203, c Portland, Oregon, June 17-18, 2011. 2011 Association for Computational Linguistics depending on the discourse theory used to classify them. Among these types, Pitler et al.(2008) have shown that most of them are unambiguous and easy to identify, but others, especially temporal ones, often signal multiple senses depending on their context. Following the terminology of Petukhova and Bunt (2009, Section 2), we are interested here in “sequential” multi-functionality, i.e. the fact that the same connective can signal different relations in different contexts. We do not deal with “simultaneous” multi-functionality, i.e. the possibility for a single occurrence to signal several relations, which has been less frequently studied for connectives (see Petukhova and Bunt (2009) for the discourse usage of and). We identified the two English connectives while and since, along with the French connective alors que, as being particularly problematic because they are highly multi-functional, i.e."
W11-2022,P09-2004,0,0.0527179,"Missing"
W11-2022,C08-2022,0,0.00862968,"Missing"
W11-2022,prasad-etal-2008-penn,0,0.112569,"uation of three ambiguous connectives in two languages: alors que in French, since and while in English. We will show how the multilingual per2 2.1 Explicit Connectives and their Translation Three Multi-functional Connectives Discourse connectives form a functional category of lexical items that are used to mark coherence relations such as Cause or Contrast between units of discourse. Along with other function words, many connectives appear among the most frequent words, as shown for instance by counts (Cartoni et al., 2011) over the Europarl corpus (Koehn, 2005). The Penn Discourse Treebank (Prasad et al., 2008) (see Section 3.1 below) includes around 100 connective types, but the exact number varies across studies, 194 Proceedings of the SIGDIAL 2011: the 12th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 194–203, c Portland, Oregon, June 17-18, 2011. 2011 Association for Computational Linguistics depending on the discourse theory used to classify them. Among these types, Pitler et al.(2008) have shown that most of them are unambiguous and easy to identify, but others, especially temporal ones, often signal multiple senses depending on their context. Following the ter"
W11-2022,P98-2202,0,0.588876,"Missing"
W11-2022,W06-1317,0,0.112333,"eding and the one directly following the connective. Tense was believed to be potentially relevant because since and while can have temporal meanings. The occurrence of auxiliary verbs (be, have, do, or need) may give additional indications about temporal relations in the sentence. We therefore used the types of auxiliary verbs as features, including the elementary conjugations, represented for to be as: be present, be past, be part, be inf, be gerund – and similarly for the other auxiliary verbs, as in (Miltsakaki et al., 2005). As shown by Lin et al. (2010), duVerle and Prendinger (2009) or Wellner et al. (2006), the context of a connective is very important. We therefore extracted the words preceding and following each connective, the verbs and the first and the last word of the sentences. These may include numbers, sometimes indicating a numerical comparison, time expressions, or antonyms, which could indicate contrastive relations, such as rise vs. fall (e.g. It is interesting to see the fundamental stock pickers scream ”foul” on program trading when the markets decline, while hailing the great values still abounding 199 as the markets rise.). For French, we likewise extracted the words immediatel"
W11-2022,zikanova-etal-2010-typical,0,0.0419525,"Missing"
W11-2022,P07-2045,0,\N,Missing
W11-2022,C98-2197,0,\N,Missing
W12-0117,D07-1007,0,0.0749582,"Missing"
W12-0117,W11-1211,1,0.920915,"xical items, which are often (though not always) discourse connectives, are among the 400 most frequent tokens over a total of 12,846,003 (in parentheses, rank and number of occurrences): after (244th/6485), although (375th/4062), however (110th/12,857), indeed (334th/4486), rather (316th/4688), 129 Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 129–138, c Avignon, France, April 23 - 27 2012. 2012 Association for Computational Linguistics since (190th/8263), still (168th/9195), while (390th/3938), yet (331st/4532) – see also (Cartoni et al., 2011). Discourse connectives can be difficult to translate, because many of them can signal different relations between clauses in different contexts. Moreover, if a wrong connective is used in translation, then a text becomes incoherent, as in the two examples below, taken from Europarl and translated (EN/FR) with Moses (Koehn et al., 2007) trained on the entire corpus: 1. EN: This tax, though [contrast], does not come without its problems. FR-SMT: *Cette taxe, mˆeme si [concession], ne se pr´esente pas sans ses probl`emes. 2. EN: Finally, and in conclusion, Mr President, with the expiry of the EC"
W12-0117,W09-0436,0,0.155045,"Missing"
W12-0117,P05-1022,0,0.153216,"Missing"
W12-0117,P07-2045,0,0.013332,"Missing"
W12-0117,2005.mtsummit-papers.11,0,0.453125,"y, with senses of connectives. We further show that a modified SMT system is best used when the confidence for a given label is high (Section 6). The paper concludes with a comparison to related work (Section 7) and an outline of future work (Section 8). 2 Discourse Connectives in Translation Discourse connectives such as although, however, since or while form a functional category of lexical items that are frequently used to mark coherence or discourse relations such as explanation, synchrony or contrast between units of text or discourse. For example, in the Europarl corpus from years 199x (Koehn, 2005), the following nine lexical items, which are often (though not always) discourse connectives, are among the 400 most frequent tokens over a total of 12,846,003 (in parentheses, rank and number of occurrences): after (244th/6485), although (375th/4062), however (110th/12,857), indeed (334th/4486), rather (316th/4688), 129 Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 129–138, c Avignon, France, April 23 - 27 2012. 2012 Association for Computational Linguistics since (190th/8263), still (168th/9195), while (390th/3938), yet (3"
W12-0117,2011.mtsummit-papers.4,0,0.420905,"Missing"
W12-0117,J03-1002,0,0.00251846,"of training set: total and per sense Test set: total and per sense 173 155 Cs, 18 Ct 10 5 Cs, 5 Ct 179 165 Cs, 14 Ct 10 5 Cs, 5 Ct 413 274 S, 131 Ca, 8 S/Ca 10 5 Ca, 3 S, 2 S/Ca 150 80 Cs, 70 Ct 10 5 Cs, 5 Ct 280 130 Cs, 41 Ct, 89 S/Ct, 13 S/Ca, 7 S 14 4 Cs, 2 Ct, 2 S/Ct, 2 S/Ca, 4 S 1,195 – 54 – F1 Score 0.67 1.00 0.80 1.00 0.64 0.82 Table 2: Performance of a MaxEnt connective sense classifier (Classifier EU) for 5 connectives in the Europarl corpus. The sense labels are coded as follows. Cs: Concession, Ct: Contrast, S: Synchrony, Ca: Cause. glish source with target French) by using GIZA++ (Och and Ney, 2003). In this alignment, we searched for the translation equivalents of the 13 PDTB connectives by using a hand-crafted dictionary of possible French translations. When the translation candidate is not ambiguous – e.g. bien que as a translation for while clearly signals a concession – its specific sense label was added as the value of an additional feature. In some cases, however, the values of the features are not determined (and are set to NONE): either when the SMT system or GIZA++ failed in translating or aligning a connective, or when the target connective was just as ambiguous as the source"
W12-0117,P03-1021,0,0.0484421,"e, we gather descriptions of the corresponding data. a: Modification of the phrase table: Europarl (346,803 sentences), labeling the translation model after training. b: Integration of manual annotation: Europarl (346,803 sentences), minus all 8,901 sentences containing one of the above 5 connective types, plus 1,147 sentences with manually sense-labeled connectives. c: Integration of automated annotation: Europarl – years 199x (58,673 sentences), all occurrences of the 13 PDTB subset connective types have been labeled by classifiers (in 6,961 sentences). For Minimum Error Rate tuning (MERT) (Och, 2003) of the SMT systems, we used the 2009 1 statmt.org/wmt10/translation-task.html News Commentary (NC) EN/FR development set with the following modifications: d: Phrase table: NC 2009 (2,051 sentences), no modifications. e: Manual annotation: NC 2009 (2,051 sentences), minus all 123 sentences containing one of the above 5 connective types, plus 102 sentences with manually sense-labeled connectives. f: Automated annotation: NC 2009 (2,051 sentences), all occurrences of the 13 PDTB subset connective types have been labeled by classifiers (in 340 sentences). For testing our modified SMT systems, thr"
W12-0117,P02-1040,0,0.0980258,"senses (Section 7). Classifier EU also compares favorably to PT and PT+, as seen for instance for since (0.80 vs. 0.78) or although (0.67 vs. 0.60–0.66). 5 Use of Labeled Connectives for SMT In this section, we report on experiments that study the effect of discourse connective labeling on SMT. The experiments differ with respect to the method used for taking advantage of the labels, but also with respect to the data sets and the sense classifiers that are used. 5.1 Evaluation Metrics for MT The variation in MT quality can be estimated in several ways. On the one hand, we use the BLEU metric (Papineni et al., 2002) with one reference translation as is most often done in current SMT research2 . To improve confidence in the BLEU scores, especially when test sets are small, we also compute BLEU scores using bootstrapping of data sets (Zhang and Vogel, 2010); the test sets are re-sampled a thousand times and the average BLEU score is computed from individual sample scores. The BLEU approach is not likely, however, to be sensitive enough to the small differences due to the correction of discourse connectives (less than one word per sentence). We therefore additionally resort to a manual evaluation metric, re"
W12-0117,P09-2004,0,0.23184,"Missing"
W12-0117,W06-1317,0,0.123899,"Missing"
W12-0117,C08-3012,0,\N,Missing
W12-0117,N03-5008,0,\N,Missing
W12-0117,prasad-etal-2008-penn,0,\N,Missing
W13-3305,J96-2004,0,0.138859,"Missing"
W13-3305,W11-1211,1,0.802153,"uilt a 5-gram language model with SRILM (Stolcke et al., 2011) over the entire FR part of Europarl. Tuning was performed by Minimum Error Rate Training (MERT) (Och, 2003). All translation models were phrase-based using either plain text (possibly with concatenated labels) or factored training as implemented in the Moses SMT toolkit (Koehn et al., 2007). 2 We only considered texts that were originally authored in English, not translated into it from French or a third-party language, to ensure only proper tenses uses are observed. The relevance of this constraint is discussed for connectives by Cartoni et al. (2011). 38 Criterion Labeling improved, as we will now show, the translation of all other words is not changed by our method, so only a small fraction of the words in the test data are changed. 4.4 Verb tense Results: Human Evaluation To assess the improvement specifically due to the narrativity labels, we manually evaluated the FR translations by the FACTORED model for the 207 first SP verbs in the test set against the translations from the BASELINE model. As the TAGGED model did not result in good scores, we did not further consider it for evaluation. Manual scoring was performed along the followi"
W13-3305,P05-1022,0,0.0691262,"Missing"
W13-3305,P11-2031,0,0.0199542,"ining and test data. 4.2 Results: Automatic Evaluation In order to obtain reliable automatic evaluation scores, we executed three runs of MERT tuning for each type of translation model. With MERT being a randomized, non-deterministic optimization process, each run leads to different feature weights and, as a consequence, to different BLEU scores when translating unseen data. Table 3 shows the average BLEU and TER scores on the ‘newstest 2010’ data for the three systems. The scores are averages over the three tuning runs, with resampling of the test set, both provided in the evaluation tool by Clark et al. (2011) (www.github.com/jhclark/ multeval). BLEU is computed using jBLEU V0.1.1 (an exact reimplementation of NIST’s ‘mteval-v13.pl’ script without tokenization). The Translation Error Rate (TER) is computed with version 0.8.0 of the software (Snover et al., 2006). A t-test was used to compute p values that indicate the significance of differences in scores. Data In all experiments, we made use of parallel English/French training, tuning and testing data from the translation task of the Workshop on Machine Translation (www.statmt.org/wmt12/). Translation model BASELINE • For training, we used Europar"
W13-3305,W12-0117,1,0.826327,"Missing"
W13-3305,2012.amta-papers.20,1,0.935591,"ibed in a text. Current statistical machine translation (SMT) systems may have difficulties in choosing the correct verb tense translations, in some language pairs, because these depend on a wider-range context than SMT systems consider. Indeed, decoding for SMT is still at the phrase or sentence level only, thus missing 33 Proceedings of the Workshop on Discourse in Machine Translation (DiscoMT), pages 33–42, c Sofia, Bulgaria, August 9, 2013. 2013 Association for Computational Linguistics combination of a classifier for discourse connectives with an SMT system (Meyer and PopescuBelis, 2012; Meyer et al., 2012). EN: (1) After a party, I offered [Narrative] to throw out a few glass and plastic bottles. (2) But, on Kounicova Ulice, there were [Non-narrative] no colored bins to be seen. (3) Luckily, on the way to the tram, I found [Narrative] the right place. (4) But it was [Non-narrative] overflowing with garbage. The paper is organized as follows. Section 2 exemplifies the hypothesized relation between narrativity and the translations of the English Simple Past into French, along with related work on modeling tense for MT. The automatic labeling experiments are presented in Section 3. Experiments wit"
W13-3305,E12-1074,0,0.0905239,"can be more correctly rendered. Tense is morphologically not marked in Chinese, unlike in English, where the verbs forms are modified according to tense (among other factors). With such a model, the authors improved translation by up to 0.8 BLEU points. Conversely, in view of English/Chinese translation but without implementing an actual translation system, Ye et al. (2007) used a classifier to generate and insert appropriate Chinese aspect markers that in certain contexts have to follow the Chinese verbs but are not present in the English source texts. For translation from English to German, Gojun and Fraser (2012) reordered verbs in the English source to positions where they normally occur in 3. The tense output by an MT system may be grammatically wrong. In Example 3, the PC a renouvel´e (has renewed) cannot replace the IMP renouvelaient (renewed) because of the conflict with the imperfective meaning conveyed by the adverbial sans cesse (again and again). 4. Finally, a wrong tense in the MT output can be misleading, if it does not convey the meaning of the source text but remains unnoticed by the reader. In Example 4, using the PC a e´ t´e leads to the interpretation that the person was no longer invo"
W13-3305,P03-1021,0,0.00949433,"be due to the scarcity of data (by a factor of 0.5) when verb word-forms are altered by concatenating them with the narrativity labels. The small improvement by the FACTORED model of overall scores (such as BLEU) is also related to the scarcity of SP verbs: although their translation is • For testing, we used the ‘newstest 2010’ data (2,489 sentences), with 1,156 automatically labeled SP verbs (621 narrative and 535 nonnarrative). We built a 5-gram language model with SRILM (Stolcke et al., 2011) over the entire FR part of Europarl. Tuning was performed by Minimum Error Rate Training (MERT) (Och, 2003). All translation models were phrase-based using either plain text (possibly with concatenated labels) or factored training as implemented in the Moses SMT toolkit (Koehn et al., 2007). 2 We only considered texts that were originally authored in English, not translated into it from French or a third-party language, to ensure only proper tenses uses are observed. The relevance of this constraint is discussed for connectives by Cartoni et al. (2011). 38 Criterion Labeling improved, as we will now show, the translation of all other words is not changed by our method, so only a small fraction of t"
W13-3305,D12-1026,0,0.186678,"glish SP by an MT system, differing from the reference translation: (1) unproblematic, (2) strange but acceptable, (3) grammatically wrong (*), and (4) misleading. 2. In other contexts, the tense proposed by the MT system can sound strange but remains acceptable. For instance, in Example 2, there is a focus on temporal length with the IMP translation (voyait, viewed) but this meaning is not preserved if a PC is used (a vu, has viewed) though it can be recovered by the reader. 2.3 Verb Tenses in SMT Modeling verb tenses for SMT has only recently been addressed. For Chinese/English translation, Gong et al. (2012) built an n-gram-like sequence model that passes information from previously translated main verbs onto the next verb so that its tense can be more correctly rendered. Tense is morphologically not marked in Chinese, unlike in English, where the verbs forms are modified according to tense (among other factors). With such a model, the authors improved translation by up to 0.8 BLEU points. Conversely, in view of English/Chinese translation but without implementing an actual translation system, Ye et al. (2007) used a classifier to generate and insert appropriate Chinese aspect markers that in cer"
W13-3305,2006.amta-papers.25,0,0.0247745,"each run leads to different feature weights and, as a consequence, to different BLEU scores when translating unseen data. Table 3 shows the average BLEU and TER scores on the ‘newstest 2010’ data for the three systems. The scores are averages over the three tuning runs, with resampling of the test set, both provided in the evaluation tool by Clark et al. (2011) (www.github.com/jhclark/ multeval). BLEU is computed using jBLEU V0.1.1 (an exact reimplementation of NIST’s ‘mteval-v13.pl’ script without tokenization). The Translation Error Rate (TER) is computed with version 0.8.0 of the software (Snover et al., 2006). A t-test was used to compute p values that indicate the significance of differences in scores. Data In all experiments, we made use of parallel English/French training, tuning and testing data from the translation task of the Workshop on Machine Translation (www.statmt.org/wmt12/). Translation model BASELINE • For training, we used Europarl v6 (Koehn, 2005), original EN2 to translated FR (321,577 sentences), with 66,143 instances of SP verbs labeled automatically: 30,452 are narrative and 35,691 are non-narrative. TAGGED FACTORED BLEU 21.4 21.3 21.6* TER 61.9 61.8 61.7* Table 3: Average valu"
W13-3305,D07-1091,0,0.377106,"Missing"
W13-3305,N03-1033,0,0.0374495,"ith the basic features of phrase-based SMT models (phrase translation, lexical and language model probabilities). To assess the performance gain of narrativityaugmented systems, we built three different SMT systems, with the following names and configurations: For further comparison we built a CRF model (Lafferty et al., 2001) in order to label narrativity in sequence of other tags, such as POS. The CRF uses as features the two preceding POS tags to label the next POS tag in a sequence of words. The same training set of 458 sentences as used above was POS-tagged using the Stanford POS tagger (Toutanova et al., 2003), with the left3words-distsim model. We replaced the instances of ‘VBD’ (the POS tag for SP verbs) with the narrativity labels from the manual annotation. The same procedure was then applied to the 118 sentences of the test set on which CRF was evaluated. Reference Narrative Non-narr. Total SMT with Narrativity Labels 2. TAGGED SMT: on wednesday the c˘ ssd declared-Narrative the approval of next year’s budget to be a success. the people’s party wasNon-narrative also satisfied. 3. FACTORED SMT: on wednesday the c˘ ssd declared|Narrative the approval of next year’s budget to be a success. the pe"
W13-3305,2005.mtsummit-papers.11,0,0.0444079,"Missing"
W13-3305,C08-3012,0,0.258752,"Missing"
W13-3305,N03-5008,0,0.261013,"Missing"
W13-3305,2007.mtsummit-papers.69,0,0.181144,"erb tenses for SMT has only recently been addressed. For Chinese/English translation, Gong et al. (2012) built an n-gram-like sequence model that passes information from previously translated main verbs onto the next verb so that its tense can be more correctly rendered. Tense is morphologically not marked in Chinese, unlike in English, where the verbs forms are modified according to tense (among other factors). With such a model, the authors improved translation by up to 0.8 BLEU points. Conversely, in view of English/Chinese translation but without implementing an actual translation system, Ye et al. (2007) used a classifier to generate and insert appropriate Chinese aspect markers that in certain contexts have to follow the Chinese verbs but are not present in the English source texts. For translation from English to German, Gojun and Fraser (2012) reordered verbs in the English source to positions where they normally occur in 3. The tense output by an MT system may be grammatically wrong. In Example 3, the PC a renouvel´e (has renewed) cannot replace the IMP renouvelaient (renewed) because of the conflict with the imperfective meaning conveyed by the adverbial sans cesse (again and again). 4."
W13-3305,P07-2045,0,\N,Missing
W13-3305,P05-3021,0,\N,Missing
W15-2513,W10-1737,0,0.236183,"Missing"
W15-2513,W11-1902,0,0.116078,"mmatical Gender and Number French pronouns always conform to the grammatical gender and number of their antecedent. Ignoring this contextual factor, as current phrasebased MT systems do, may generate inaccurate pronoun translations. Therefore, we consider the antecedent’s gender and number as the most important criterion for pronoun translation. We thus perform anaphora resolution on the source side, and using alignment we hypothesize the noun phrase antecedent on the target side (French), and determine its gender and number. More specifically, we first employ the Stanford Coreference system (Lee et al., 2011), which currently supports English and Chinese, for identifying the antecedents of the source pronouns (“it” or “they”). In cases where antecedent is a noun 3.2.1 Overview of our Approach Our post-editing process considers the baseline translation of each pronoun “it” and “they” from the output of Pass 1. If this is one of the “complex” pronouns (e.g. “celui” or “cela”, see Section 3.2.6), then we simply accept the results from 95 3.2.4 phrase with several nouns, then the head word is identified by the toolkit using syntactic features extracted from the sentence’s parse tree (Raghunathan et al"
W15-2513,D14-1082,0,0.0157547,"ions into French are generally plural subject pronouns2 . On the contrary, “it” can be used either as a subject or an object. Due to the fact that, unlike English, French singular subject and object pronouns are different, we propose post-editing rules to deal with this case. Generally, the object pronoun “it” refers to the “recipient” of an action caused by the subject, and generally follows the verb. However, its position might be either right after the verb (e.g. “I know it”) or several words away (e.g. “I talk about it.”). In order to detect the object pronouns, we employ Stanford parser (Chen and Manning, 2014). In the parse tree, an object pronoun is always a node of a subtree whose root is a verb phrase (VP) node, while a subject pronoun is under a noun phrase (NP) node. Therefore, we traverse up-ward from 1 All other singular pronouns are considered as special cases, see Section 3.2.6. 2 Except when they refer to English plural nouns which are singular in French, e.g. “trousers” − > “pantalon”. 96 the pronoun node to the root. If on the way we encounter “VP” node, then we consider the pronoun as an object one. The translation of “it” depends on the object type (direct or indirect), which we ident"
W15-2513,W12-0117,1,0.854919,"tecedent hypothesized by a coreference system, along with the decoder’s score for this lexical item calculated from the search graph during decoding. The selected pronoun is the one that maximizes the combined scores of these two criteria. In the latter case (object pronoun), we use a set of heuristics based on French grammar rules to seek the appropriate word. Finally, the post-edited word is substituted to the one from Pass 1 in order to generate the output of Pass 2. These steps are displayed in Figure 1. nouns. The approach was shown to be successful for translating discourse connectives (Meyer and Popescu-Belis, 2012). A large set of features was used within a deep neural network architecture by Hardmeier (2014, Chapters 7–9). In our system for the second sub-task, we extend the features sketched by Popescu-Belis et al. (2012). 3 Pronoun-Focused Translation Our system for this task works in two passes. First, the source text is pre-processed and translated by a baseline MT system to acquire pronoun candidates. Then, we apply several post-editing strategies over the translations of “it” and “they”, which help in correcting erroneous instances. 3.1 Pass 1: Baseline MT Outputs The test data is first tokenized"
W15-2513,chrupala-etal-2008-learning,0,0.432113,"Missing"
W15-2513,1995.tmi-1.7,0,0.496319,"s from the hypothesized coreference links, and candidate translations from an SMT decoder. The system for the second sub-task avoids hypothesizing a coreference link, and uses instead a large set of source-side and target-side features from the noun phrases surrounding the pronoun to train a pronoun predictor. 1 2 As rule-based anaphora resolution systems reached their maturity in the 1990s (Mitkov, 2002), several early attempts were made to use these methods for MT, especially in situations when pronominal issues must be addressed specifically such as EN/JP translation (Bond and Ogura, 1998; Nakaiwa and Ikehara, 1995). Following the development of statistical methods for anaphora resolution (Ng, 2010), several studies have attempted to integrate anaphora resolution with statistical MT, as reviewed by Hardmeier (2014, Section 2.3.1). Le Nagard and Koehn (2010) designed a two-pass system for EN/FR MT, first translating all possible antecedents, identifying the antecedents of pronouns using (imperfect) anaphora resolution, and constraining pronoun translation according to the features of the antecedent (with moderate improvements of MT). Other attempts along the same lines include those by Hardmeier and Feder"
W15-2513,E12-3001,0,0.289593,"elopment of statistical methods for anaphora resolution (Ng, 2010), several studies have attempted to integrate anaphora resolution with statistical MT, as reviewed by Hardmeier (2014, Section 2.3.1). Le Nagard and Koehn (2010) designed a two-pass system for EN/FR MT, first translating all possible antecedents, identifying the antecedents of pronouns using (imperfect) anaphora resolution, and constraining pronoun translation according to the features of the antecedent (with moderate improvements of MT). Other attempts along the same lines include those by Hardmeier and Federico (2010), and by Guillou (2012). Our system for the first sub-task (Section 3) enriches the approach with a probabilistic combination of constraints from anaphora resolution and pronoun candidates from the search graph generated by the MT decoder. Another line of research attempted to postedit pronouns in SMT output, possibly including as features the baseline translations of proIntroduction The NLP Group of the Idiap Research Institute participated in both sub-tasks of the DiscoMT 2015 Shared Task: pronoun-focused translation and pronoun prediction (Hardmeier et al., 2015). The first task aimed at evaluating the quality of"
W15-2513,P10-1142,0,0.0865459,"for the second sub-task avoids hypothesizing a coreference link, and uses instead a large set of source-side and target-side features from the noun phrases surrounding the pronoun to train a pronoun predictor. 1 2 As rule-based anaphora resolution systems reached their maturity in the 1990s (Mitkov, 2002), several early attempts were made to use these methods for MT, especially in situations when pronominal issues must be addressed specifically such as EN/JP translation (Bond and Ogura, 1998; Nakaiwa and Ikehara, 1995). Following the development of statistical methods for anaphora resolution (Ng, 2010), several studies have attempted to integrate anaphora resolution with statistical MT, as reviewed by Hardmeier (2014, Section 2.3.1). Le Nagard and Koehn (2010) designed a two-pass system for EN/FR MT, first translating all possible antecedents, identifying the antecedents of pronouns using (imperfect) anaphora resolution, and constraining pronoun translation according to the features of the antecedent (with moderate improvements of MT). Other attempts along the same lines include those by Hardmeier and Federico (2010), and by Guillou (2012). Our system for the first sub-task (Section 3) enri"
W15-2513,2010.iwslt-papers.10,0,0.442678,"and Ikehara, 1995). Following the development of statistical methods for anaphora resolution (Ng, 2010), several studies have attempted to integrate anaphora resolution with statistical MT, as reviewed by Hardmeier (2014, Section 2.3.1). Le Nagard and Koehn (2010) designed a two-pass system for EN/FR MT, first translating all possible antecedents, identifying the antecedents of pronouns using (imperfect) anaphora resolution, and constraining pronoun translation according to the features of the antecedent (with moderate improvements of MT). Other attempts along the same lines include those by Hardmeier and Federico (2010), and by Guillou (2012). Our system for the first sub-task (Section 3) enriches the approach with a probabilistic combination of constraints from anaphora resolution and pronoun candidates from the search graph generated by the MT decoder. Another line of research attempted to postedit pronouns in SMT output, possibly including as features the baseline translations of proIntroduction The NLP Group of the Idiap Research Institute participated in both sub-tasks of the DiscoMT 2015 Shared Task: pronoun-focused translation and pronoun prediction (Hardmeier et al., 2015). The first task aimed at ev"
W15-2513,popescu-belis-etal-2012-discourse,1,0.830555,"of these two criteria. In the latter case (object pronoun), we use a set of heuristics based on French grammar rules to seek the appropriate word. Finally, the post-edited word is substituted to the one from Pass 1 in order to generate the output of Pass 2. These steps are displayed in Figure 1. nouns. The approach was shown to be successful for translating discourse connectives (Meyer and Popescu-Belis, 2012). A large set of features was used within a deep neural network architecture by Hardmeier (2014, Chapters 7–9). In our system for the second sub-task, we extend the features sketched by Popescu-Belis et al. (2012). 3 Pronoun-Focused Translation Our system for this task works in two passes. First, the source text is pre-processed and translated by a baseline MT system to acquire pronoun candidates. Then, we apply several post-editing strategies over the translations of “it” and “they”, which help in correcting erroneous instances. 3.1 Pass 1: Baseline MT Outputs The test data is first tokenized using the tokenizer provided by the organizers. Then, we apply a baseline MT system to generate the candidate pronouns. This system is the Moses decoder (Koehn et al., 2007) with a translation and a language mode"
W15-2513,W15-2501,0,0.0849191,"ines include those by Hardmeier and Federico (2010), and by Guillou (2012). Our system for the first sub-task (Section 3) enriches the approach with a probabilistic combination of constraints from anaphora resolution and pronoun candidates from the search graph generated by the MT decoder. Another line of research attempted to postedit pronouns in SMT output, possibly including as features the baseline translations of proIntroduction The NLP Group of the Idiap Research Institute participated in both sub-tasks of the DiscoMT 2015 Shared Task: pronoun-focused translation and pronoun prediction (Hardmeier et al., 2015). The first task aimed at evaluating the quality of pronoun translation in the output of a full-fledged machine translation (MT) system, while the second task aimed at restoring hidden pronouns in a high-quality reference translation. In our view, both sub-tasks raise the same question: given the limitations of current anaphora resolution systems, to what extent is it possible to correctly translate pronouns with unreliable knowledge of their antecedents? Although the answer depends on the translation divergencies from the source language to the target one, we explore here two different approa"
W15-2513,D10-1048,0,0.0674251,"Missing"
W15-2513,P07-2045,0,0.00574414,"xtend the features sketched by Popescu-Belis et al. (2012). 3 Pronoun-Focused Translation Our system for this task works in two passes. First, the source text is pre-processed and translated by a baseline MT system to acquire pronoun candidates. Then, we apply several post-editing strategies over the translations of “it” and “they”, which help in correcting erroneous instances. 3.1 Pass 1: Baseline MT Outputs The test data is first tokenized using the tokenizer provided by the organizers. Then, we apply a baseline MT system to generate the candidate pronouns. This system is the Moses decoder (Koehn et al., 2007) with a translation and a language model trained with no additional resources other than the official data provided by the shared task organizers (including Europarl, News Commentary and Ted talks). Parameters are tuned on domain-specific Ted(dev) data set. We run the Moses decoder with the -print-alignment-info and -output-search-graph options to obtain the word alignments and the search graph plain-text representation, used for post-editing in the second pass. 3.2 Pass 2: Automatic Pronoun Post-editing Figure 1: Flowchart of post-editing process Since the pronoun-focused task concentrates on"
W16-2202,W15-2501,0,0.409017,"ation, possibly from one or more sentences before the one that is being translated. In this paper, we focus on the divergencies that occur when translating the English neutral pronouns it and they into French. Depending on their functions (referential or pleonastic) and on their actual antecedents, 12 Proceedings of the First Conference on Machine Translation, Volume 1: Research Papers, pages 12–20, c Berlin, Germany, August 11-12, 2016. 2016 Association for Computational Linguistics 2010; Le Nagard and Koehn, 2010). Still, at the DiscoMT 2015 shared task on pronoun-focused EN/FR translation (Hardmeier et al., 2015), none of the submitted systems was able to outperform a well-trained phrase-based statistical MT baseline. Apart from the need for considering first the functions of pronouns and then their antecedents, if any (Guillou, 2016), one of the reasons that limit performance is the large number of errors made by co-reference or anaphora resolution systems. In this paper, we attempt to model the uncertainty of an off-the-shelf coreference resolution system (Lee et al.’s (2011) Stanford system) with respect to its impact on MT. We propose to learn from parallel data the correlations between target sid"
W16-2202,2005.mtsummit-papers.11,0,0.0514915,"λCM of the added table is declared in the [weight] section, as shown in Figure 3. In our experiments, we assign a default weight of 0.8 to the CM model, which is identical to the sum of the four feature functions related to the default table. The optimization of this weight will be studied in future work. Before using the Coreference-Aware Decoder, the document to be translated is pre-processed by 5 5.1 Experiments and Results Data and Evaluation Metrics We built the phrase table on the following parallel datasets: aligned TED talks from the WIT3 corpus (Cettolo et al., 2012), Europarl v. 7 (Koehn, 2005), News Commentary v. 9 and other news data from WMT 2007–2013 (Bojar et al., 2014). The language model was trained on the target side (French) of all above datasets. Then, the system was tuned on a development set of 887 sentences from IWSLT 2010 provided for the shared task on pronoun translation of the DiscoMT 2015 workshop (Hardmeier et al., 2015). The test set was also the one from the DiscoMT 2015 shared task, with 2,093 English sentences along with French gold-standard translations, extracted from 12 recent TED talks. The test set contains 809 occurrences of it and 307 of they. We proces"
W16-2202,W15-2508,0,0.204601,"Missing"
W16-2202,W10-1737,0,0.38042,"Missing"
W16-2202,2012.eamt-1.60,0,0.0127702,"ff] and [mapping] sections. The weight λCM of the added table is declared in the [weight] section, as shown in Figure 3. In our experiments, we assign a default weight of 0.8 to the CM model, which is identical to the sum of the four feature functions related to the default table. The optimization of this weight will be studied in future work. Before using the Coreference-Aware Decoder, the document to be translated is pre-processed by 5 5.1 Experiments and Results Data and Evaluation Metrics We built the phrase table on the following parallel datasets: aligned TED talks from the WIT3 corpus (Cettolo et al., 2012), Europarl v. 7 (Koehn, 2005), News Commentary v. 9 and other news data from WMT 2007–2013 (Bojar et al., 2014). The language model was trained on the target side (French) of all above datasets. Then, the system was tuned on a development set of 887 sentences from IWSLT 2010 provided for the shared task on pronoun translation of the DiscoMT 2015 workshop (Hardmeier et al., 2015). The test set was also the one from the DiscoMT 2015 shared task, with 2,093 English sentences along with French gold-standard translations, extracted from 12 recent TED talks. The test set contains 809 occurrences of"
W16-2202,W11-1902,0,0.0533366,"dels, but also in hierarchical ones, the phrase table cannot constrain the generation of a target pronoun based on features of its antecedent. Moreover, such features cannot be reliably obtained from anaphora resolution systems, as they are quite error prone. We propose to model the uncertainty of anaphora resolution and the acceptable variability of pronoun EN/FR translation by estimating the likelihood of observing a target language pronoun depending on the gender and number of its antecedent (noted respectively as ‘G’ and ‘N’), as hypothesized by the Stanford coreference resolution system (Lee et al., 2011). The construction of the model is represented in Figure 2, and explained in detail in the remainder of this section. In a nutshell, we extract pairs of pronouns and their antecedents from the sourceside of a large bilingual corpus. Then, we obtain the gender and number of the translation of the antecedent through target-side POS tagging. Finally, we estimate the co-occurrence probability of each 1 For training, one could also, more directly, perform anaphora resolution on the target side of the parallel corpus. However, this cannot be done during decoding, since the correctness of the target"
W16-2202,chrupala-etal-2008-learning,0,0.0880239,"Missing"
W16-2202,W15-2513,1,0.728427,"t. This system was trained on the same datasets as CM, but was tuned on IWSLT 2010 development data and IWSLT 2011 test data (1,705 sentences). Results using Automatic Metrics We report the performance first by automatically computing the following four scores, inspired by the ACT metric for evaluating the translation of discourse connectives (Hajlaoui and PopescuBelis, 2013). These scores rely on the comparison of the system’s pronouns (candidates) with the ones in the reference translation. • PE: our post-editing system for the translations of it and they generated by a baseline SMT system (Luong et al., 2015), which was the highest scoring system at the DiscoMT 2015 shared task on pronoun-focused translation. It was trained on the DiscoMT 2015 data and tuned on the IWSLT 2010 development data. • C1 : Number of candidate pronouns which are identical to the reference ones. • C2 : Number of candidate pronouns which are “similar” to the reference ones. Similarity allows for two equivalence classes of French pronouns, accounting for the variants of “ce” and “c¸a” with or without apostrophe, and for two different symbols used for the apostrophe: {ce, c&apos;, c’} and {c¸a, ca, c¸&apos;, c¸’}. • C3 : Number of can"
W16-2202,E12-3001,0,0.34435,"Missing"
W16-2202,P10-1142,0,0.114363,"Missing"
W16-2202,P08-4003,0,0.0787872,"Missing"
W16-2202,2010.iwslt-papers.10,0,0.2683,"date translation. We translated the test set using the three systems, and computed the C1, . . . , C4 scores over the 563 pronouns. The results, shown in Table 1, reveal that CM outperforms both BL and PE, with gains in the numbers of exact translations (C1 ) of 16 and 25 pronouns respectively. In terms of the number of correct translations (C1 + C2 ), CM is also the best-performing one, with 21 instances above BL and 30 above PE. For the sake of completeness, we also compare the performance of three above mentioned systems in overall Precision, Recall and F-score for pronouns, as proposed by Hardmeier and Federico (2010) and used in DiscoMT 2015 among other metrics. We also compute the BLEU score to investigate the impact of pronoun improvement on the global translation quality. The results in Table 2 show that CM surpasses BL and PE by 0.022 and 0.025 in terms of F-score, which is very similar to the above C1 + C2 score. In terms of BLEU, CM outperforms BL and PE by respectively 0.35 and 0.06 BLEU points. The small magnitude of these differences is due to the sparseness of pronouns in the evaluated texts, but they tend to confirm the improvements brought by the CM. Although these scores, even taken together,"
W16-2202,P07-2045,0,\N,Missing
W16-2345,D12-1133,0,0.253709,"set of the provided training data that has well-defined document boundaries in order to allow for meaningful extraction of coreference chains. The MaxEnt classifiers consistently outperform the CRF models. Feature ablation shows that the antecedent feature is useful for English–German, and predicting NULL-translations is useful for English–French. It also reveals that the LM feature hurts performance. A number of tools and resources are used in the LIMSI system. Stanford CoreNLP is used for PoS tagging, syntactic dependencies, and coreference resolution over the English text. The Mate Parser (Bohnet and Nivre, 2012), retrained on SPMRL 2014 data (Seddah et al., 2014) (dependency trees), and the Lefff (Sagot, 2010), a morphological and syntactic lexicon (used for information on noun gender and impersonal adjectives and verbs), are both used for French. 5.4 TurkuNLP The architecture for the T URKU NLP system (Luotolahti et al., 2016) is based on token-level sequence classification around the target pronoun using stacked recurrent neural networks. The system learns token-level embeddings for the source-language lemmata, target-language tokens, PoS tags, combination of words and PoS tags and separate embeddi"
W16-2345,W16-2348,0,0.0249438,"urce word aligned to the target pronoun, and approximations of the coreference and dependency relations in the target language. Following the submission of the CUNI systems for English–German, an error was discovered in the merging of the classifier output into the test data file for submission. Fixing it yielded an improvement, with the contrastive system achieving recall of 51.74, and 54.37 for the primary system. Except for the English wordlist with gender distributions by Bergsma and Lin (2006), only the shared task data was used in the CUNI systems. 5.2 IDIAP 5.3 LIMSI The LIMSI systems (Bawden, 2016) for the English–French task are linguistically-driven statistical classification systems. The systems use random forests, with few, high-level features, relying on explicit coreference resolution and external linguistic resources and syntactic dependencies. The systems include several types of contextual features, including a single feature using context templates to target particularly discriminative contexts for the prediction of certain pronoun classes, in particular the OTHER class. The difference between the primary and contrastive systems is small. In the primary system, the feature val"
W16-2345,P06-1005,0,0.150956,"res based on the target-language model estimates provided by the baseline system, linguistic features concerning the source word aligned to the target pronoun, and approximations of the coreference and dependency relations in the target language. Following the submission of the CUNI systems for English–German, an error was discovered in the merging of the classifier output into the test data file for submission. Fixing it yielded an improvement, with the contrastive system achieving recall of 51.74, and 54.37 for the primary system. Except for the English wordlist with gender distributions by Bergsma and Lin (2006), only the shared task data was used in the CUNI systems. 5.2 IDIAP 5.3 LIMSI The LIMSI systems (Bawden, 2016) for the English–French task are linguistically-driven statistical classification systems. The systems use random forests, with few, high-level features, relying on explicit coreference resolution and external linguistic resources and syntactic dependencies. The systems include several types of contextual features, including a single feature using context templates to target particularly discriminative contexts for the prediction of certain pronoun classes, in particular the OTHER clas"
W16-2345,2012.eamt-1.60,1,0.85967,"predicting all of the other pronouns, the system relied solely on the scores coming from the proposed PLM model. This target-side PLM model uses a large target-language training dataset to learn a probabilistic relation between each target pronoun and the distribution of the gender-number of its preceding nouns and pronouns. For prediction, given each source pronoun “it” or “they”, the system uses the PLM to score all possible candidates and to select the one with the highest score. In addition to the PoS-tagged lemmatised data that was provided for the shared task, the WIT3 parallel corpus (Cettolo et al., 2012), provided as part of the training data at the DiscoMT 2015 workshop, was used to train the PLM model. Furthermore, a French PoS-tagger, Morfette (Chrupala et al., 2008), was employed for gendernumber extraction. Before extracting the examples as feature vectors, the data is linguistically preprocessed usˇ ing the Treex framework (Popel and Zabokrtsk´ y, 2010). The source-language texts undergo a thorough analysis and are enriched with PoS tags, dependency syntax, as well as semantic roles and coreference for English. On the other hand, only grammatical genders are assigned to nouns in the tar"
W16-2345,chrupala-etal-2008-learning,0,0.0898214,"Missing"
W16-2345,W16-2350,1,0.838182,"on. The combined system with the ‘it’-labels performed slightly worse than the system without it (57.03 vs. 59.84 macro-averaged recall). The same underlying translation model forms the contrastive system for English–French, and the primary system for all other subtasks. 5.9 The CRF model was trained on the IWSLT15 corpus and used the TED talks for development. The rule-based morphological Analyser SMOR (Schmid et al., 2004) as well as its English spinoff EMOR (not published) were used to derive the gender and number of the German and English words. 5.10 UU-Hardmeier The UU-H ARDMEIER system (Hardmeier, 2016) is a system combination of two different models. One of them, based on earlier work (Hardmeier et al., 2013), is a feed-forward neural network that takes as input the source pronoun and the source context words, target lemmata and target PoS tags in a window of 3 words to the left and to the right of the pronoun. In addition, the network receives a list of potential antecedent candidates identified by the preprocessing part of a coreference resolution system. Anaphora resolution is treated as a latent variable by the model. This system is combined by linear interpolation with a specially trai"
W16-2345,W11-2123,0,0.0192239,"e classifier is trained on a combination of semantic, based on lexical resources such as VerbNet (Schuler, 2005) and WordNet (Miller, 1995), and frequencies computed over the annotated Gigaword corpus (Napoles et al., 2012), syntactic, from the dependency parser in the Mate tools (Bohnet et al., 2013), and contextual features. The event classification results are modest, reaching only 54.2 F-score for the event class. The translation model, into which the classifier is integrated, is a 6-gram language model computed over target lemmata using modified KneserNey smoothing and the KenLM toolkit (Heafield, 2011). In addition to the pure target lemma context, it also has access to the identity of the sourcelanguage pronoun, used as a concatenated label to each REPLACE item. This provides information about the number marking of the pronouns in the source, and also allows for the incorporation of the output of the ‘it’-label classifier. To predict classes for an unseen test set, a uniform unannotated RE PLACE tag is used for all classes. The ‘disambig’ tool of the SRILM toolkit (Stolcke, 2002) is then used to recover the tag annotated with the correct solution. The combined system with the ‘it’-labels p"
W16-2345,W16-2349,0,0.0373816,"sing the given tokens and PoS labels as features. Coreference resolution is not used, but additional selected items in the prior context are extracted to enrich the model. In particular, a small number of the nearest determiners, nouns and proper nouns are taken as possible antecedent candidates. The contribution of these features is limited even with the lemmatised target-language context that makes it harder to disambiguate pronoun translation decisions. The model performs reasonably well especially for the prediction of pronoun translations into English. 5.7 UEDIN UKYOTO The UKYOTO system (Dabre et al., 2016) is a simple Recurrent Neural Network system with an attention mechanism which encodes both the source sentence and the context of the pronoun to be predicted and then predicts the pronoun. The interesting thing about the approach is that it uses a simple language-independent Neural Network (NN) mechanism that performs well in almost all cases. Another interesting aspect is that good performance is achieved, even though only the IWSLT data is used. The UEDIN systems (Wetzel, 2016) for English– French and English–German are Maximum Entropy (MaxEnt) classifiers with the following set of features"
W16-2345,W10-1737,0,0.434398,"Missing"
W16-2345,guillou-etal-2014-parcor,1,0.910739,"the fact that all talks are originally given in English, which means that French–English translation is in reality a back-translation. • she: feminine singular subject pronoun; 3 1 We explain below in Section 3.3.3 how non-subject pronouns are filtered out from the data. 528 TED talks address topics of general interest and are delivered to a live public audience whose responses are also audible on the recordings. The talks generally aim to be persuasive and to change the viewers’ behaviour or beliefs. The genre of the TED talks is transcribed planned speech. As shown in analysis presented by Guillou et al. (2014), TED talks differ from other text types with respect to pronoun usage. TED speakers frequently use first- and second-person pronouns (singular and plural): first-person to refer to themselves and their colleagues or to themselves and the audience, second-person to refer to the audience, the larger set of viewers, or people in general. TED speakers often use the pronoun “they” without a specific textual antecedent, in sentences such as “This is what they think.” They also use deictic and third-person pronouns to refer to things in the spatio-temporal context shared by the speaker and the audie"
W16-2345,W16-2351,1,0.900928,"Missing"
W16-2345,E12-3001,1,0.880326,"it is required by syntax to fill the subject position. An event reference pronoun may refer to a verb phrase (VP), a clause, an entire sentence, or a longer passage of text. Examples of each of these pronoun functions are provided in Figure 1. It is clear that instances of the English pronoun “it” belonging to each of these functions would have different translation requirements in French and German. Introduction Pronoun translation poses a problem for current state-of-the-art Statistical Machine Translation (SMT) systems (Le Nagard and Koehn, 2010; Hardmeier and Federico, 2010; Nov´ak, 2011; Guillou, 2012; Hardmeier, 2014). 525 Proceedings of the First Conference on Machine Translation, Volume 2: Shared Task Papers, pages 525–542, c Berlin, Germany, August 11-12, 2016. 2016 Association for Computational Linguistics 2 The problem of pronouns in machine translation has long been studied. In particular, for SMT systems, the recent previous studies cited above have focused on the translation of anaphoric pronouns. In this case, a well-known constraint of languages with grammatical gender is that agreement must hold between an anaphoric pronoun and the NP with which it corefers, called its antecede"
W16-2345,W16-2352,1,0.881771,"Missing"
W16-2345,W16-2353,0,0.0435664,"s useful for English–French. It also reveals that the LM feature hurts performance. A number of tools and resources are used in the LIMSI system. Stanford CoreNLP is used for PoS tagging, syntactic dependencies, and coreference resolution over the English text. The Mate Parser (Bohnet and Nivre, 2012), retrained on SPMRL 2014 data (Seddah et al., 2014) (dependency trees), and the Lefff (Sagot, 2010), a morphological and syntactic lexicon (used for information on noun gender and impersonal adjectives and verbs), are both used for French. 5.4 TurkuNLP The architecture for the T URKU NLP system (Luotolahti et al., 2016) is based on token-level sequence classification around the target pronoun using stacked recurrent neural networks. The system learns token-level embeddings for the source-language lemmata, target-language tokens, PoS tags, combination of words and PoS tags and separate embeddings for the sourcelanguage pronouns that are aligned with the target pronoun. The network is fed sequences of these embeddings within a certain window to the left and to the right of the target pronoun. The window size used by the system is 50 tokens or until the end of the sentence boundary. All of these inputs are read"
W16-2345,2010.iwslt-papers.10,1,0.888921,"Missing"
W16-2345,D13-1037,1,0.883273,"3 vs. 59.84 macro-averaged recall). The same underlying translation model forms the contrastive system for English–French, and the primary system for all other subtasks. 5.9 The CRF model was trained on the IWSLT15 corpus and used the TED talks for development. The rule-based morphological Analyser SMOR (Schmid et al., 2004) as well as its English spinoff EMOR (not published) were used to derive the gender and number of the German and English words. 5.10 UU-Hardmeier The UU-H ARDMEIER system (Hardmeier, 2016) is a system combination of two different models. One of them, based on earlier work (Hardmeier et al., 2013), is a feed-forward neural network that takes as input the source pronoun and the source context words, target lemmata and target PoS tags in a window of 3 words to the left and to the right of the pronoun. In addition, the network receives a list of potential antecedent candidates identified by the preprocessing part of a coreference resolution system. Anaphora resolution is treated as a latent variable by the model. This system is combined by linear interpolation with a specially trained 6gram language model identical to the contrastive system of the UUPPSALA submission described above. The"
W16-2345,S16-1001,1,0.795211,"d 69.76 in macro-averaged recall. This is very much above the performance of baseline0 and baseline-1.5, which are in the low-mid 40s. It is also well above the majority/random baseline (not shown) at 11.11, which is outperformed by far by all systems. Note that the top-3 systems in terms of macro-averaged recall are also the top-3 in terms of accuracy, but in different order. Evaluation While in 2015 we used macro-averaged F1 as an official evaluation measure, this year we adopted macro-averaged recall, which was also recently adopted by some other competitions, e.g., by SemEval-2016 Task 4 (Nakov et al., 2016). Moreover, as in 2015, we also report accuracy as a secondary evaluation measure. Macro-averaged recall ranges in [0, 1], where a value of 1 is achieved by the perfect classifier,8 and a value of 0 is achieved by the classifier that misclassifies all examples. The value of 1/C, where C is the number of classes, is achieved by a trivial classifier that assigns the same class to all examples (regardless of which class is chosen), and is also the expected value of a random classifier. 8 If the test data did not have any instances of some of the classes, we excluded these classes from the macro-a"
W16-2345,W15-2501,1,0.657407,"ould replace a placeholder value (represented by the token REPLACE) in the target-language text. It requires no specific Machine Translation (MT) expertise and is interesting as a machine learning task in its own right. Within the context of SMT, one could think of the task of cross-lingual pronoun prediction as a component of an SMT system. This component may take the form of a decoder feature or it may be used to provide “corrected” pronoun translations in a post-editing scenario. The design of the WMT 2016 shared task has been influenced by the design and the results of a 2015 shared task (Hardmeier et al., 2015) organised at the EMNLP workshop on Discourse in MT (DiscoMT). The first intuition about evaluating pronoun translation is to require participants to submit MT systems — possibly with specific strategies for pronoun translation — and to estimate the correctness of the pronouns they output. This estimation, however, cannot be performed with full reliability only by comparing pronouns across candidate and reference translations because this would miss the legitimate variation of certain pronouns, as well as variations in gender or number of the antecedent itself. Human judges are thus required f"
W16-2345,W16-2354,0,0.0469939,"Missing"
W16-2345,H05-1108,0,0.0601982,"Missing"
W16-2345,W14-3334,1,0.800608,"he OTHER class. For the DiscoMT 2015 shared task, we explored this issue for English–French and found that GIZA++ model 4 and HMM with grow-diag-final-and symmetrisation gave the best results. For pronoun– pronoun links, we had an F-score of 0.96, with perfect recall and precision of 0.93 (Hardmeier et al., 2015). This was slightly higher than for other links, which had an F-score of 0.92. For German–English, we explored this issue this year since it is a new language pair. We used an aligned gold standard of 987 sentences from (Pad´o and Lapata, 2005), which has been extensively evaluated by Stymne et al. (2014). We used the same methodology as in 2015, and performed an evaluation on the subset of links between the pronouns we are interested in. We report precision and recall of links both for the pronoun subset and for all links, shown in Table 4. The alignment quality is considerably worse than for French–English both for all links and for pronouns, but again the results for pronouns is better than for all links in both precision and recall. 6 https://github.com/slavpetrov/ universal-pos-tags 530 Alignment Symmetrisation Model 4 fast-align gdfa HMM gd gdf ∪ ∩ All links P R Pronouns P R .75 .69 .80"
W16-2345,W16-2355,1,0.832701,"the test dataset is imbalanced. Thus, one cannot interpret the absolute value of accuracy (e.g., is 0.7 a good or a bad value?) without comparing it to a baseline that must be computed for each specific test dataset. In contrast, for macro-averaged recall, it is clear that a value of, e.g., 0.7, is well above the majority-class and the random baselines, which are both always 1/C (e.g., 0.5 with two classes, 0.33 with three classes, etc.). Standard F1 and macro-averaged F1 are also sensitive to class imbalance for the same reason; see Sebastiani (2015) for more detail. The UU-S TYMNE systems (Stymne, 2016) use linear SVM classifiers for all language pairs. A number of different features were explored, but anaphora is not explicitly modelled. The features used can be grouped in the following way: source pronouns, local context words/lemmata, preceding nouns, target PoS n-grams with two different PoS tag-sets, dependency heads of pronouns, target LM scores, alignments, and pronoun position. A joint tagger and dependency parser on the source text is used for some of the features. The primary system is a 2-step classifier where a binary classifier is first used to distinguish between the OTHER clas"
W16-2345,petrov-etal-2012-universal,0,0.0937891,"Missing"
W16-2345,W16-2356,1,0.48149,"networks, except for the embedding for the aligned pronoun. All outputs of the recurrent layers are concatenated to a single vector along with the embedding of the aligned pronoun. This vector is then used to make the pronoun prediction by a dense neural network layer. The primary systems are trained to optimise macro-averaged recall and the contrastive systems are optimised without preference towards rare classes. The system is trained only on the shared task data and all parts of the data, in-domain and out-of-domain, are used for training the system. 5.5 5.6 UHELSINKI The UHELSINKI system (Tiedemann, 2016) implements a simple linear classifier based on LibSVM with its L2-loss SVC dual solver. The system applies local source-language and target-language context using the given tokens and PoS labels as features. Coreference resolution is not used, but additional selected items in the prior context are extracted to enrich the model. In particular, a small number of the nearest determiners, nouns and proper nouns are taken as possible antecedent candidates. The contribution of these features is limited even with the lemmatised target-language context that makes it harder to disambiguate pronoun tra"
W16-2345,W16-2357,0,0.0259257,"well especially for the prediction of pronoun translations into English. 5.7 UEDIN UKYOTO The UKYOTO system (Dabre et al., 2016) is a simple Recurrent Neural Network system with an attention mechanism which encodes both the source sentence and the context of the pronoun to be predicted and then predicts the pronoun. The interesting thing about the approach is that it uses a simple language-independent Neural Network (NN) mechanism that performs well in almost all cases. Another interesting aspect is that good performance is achieved, even though only the IWSLT data is used. The UEDIN systems (Wetzel, 2016) for English– French and English–German are Maximum Entropy (MaxEnt) classifiers with the following set of features: tokens and their PoS tags are extracted from a context window around source- and targetside pronouns. N -gram combinations of these features are included by concatenating adjacent tokens or PoS tags. Furthermore, the pleonastic use of a pronoun is detected with NADA (Bergsma and Yarowsky, 2011) on the source side. 534 This CRF approach has been applied only to German, but there are plans to extend it to other languages. This indicates that the NN mechanism is quite effective. Th"
W16-2345,sagot-2010-lefff,0,0.0184156,"traction of coreference chains. The MaxEnt classifiers consistently outperform the CRF models. Feature ablation shows that the antecedent feature is useful for English–German, and predicting NULL-translations is useful for English–French. It also reveals that the LM feature hurts performance. A number of tools and resources are used in the LIMSI system. Stanford CoreNLP is used for PoS tagging, syntactic dependencies, and coreference resolution over the English text. The Mate Parser (Bohnet and Nivre, 2012), retrained on SPMRL 2014 data (Seddah et al., 2014) (dependency trees), and the Lefff (Sagot, 2010), a morphological and syntactic lexicon (used for information on noun gender and impersonal adjectives and verbs), are both used for French. 5.4 TurkuNLP The architecture for the T URKU NLP system (Luotolahti et al., 2016) is based on token-level sequence classification around the target pronoun using stacked recurrent neural networks. The system learns token-level embeddings for the source-language lemmata, target-language tokens, PoS tags, combination of words and PoS tags and separate embeddings for the sourcelanguage pronouns that are aligned with the target pronoun. The network is fed seq"
W16-2345,schmid-etal-2004-smor,0,0.0349386,"test set, a uniform unannotated RE PLACE tag is used for all classes. The ‘disambig’ tool of the SRILM toolkit (Stolcke, 2002) is then used to recover the tag annotated with the correct solution. The combined system with the ‘it’-labels performed slightly worse than the system without it (57.03 vs. 59.84 macro-averaged recall). The same underlying translation model forms the contrastive system for English–French, and the primary system for all other subtasks. 5.9 The CRF model was trained on the IWSLT15 corpus and used the TED talks for development. The rule-based morphological Analyser SMOR (Schmid et al., 2004) as well as its English spinoff EMOR (not published) were used to derive the gender and number of the German and English words. 5.10 UU-Hardmeier The UU-H ARDMEIER system (Hardmeier, 2016) is a system combination of two different models. One of them, based on earlier work (Hardmeier et al., 2013), is a feed-forward neural network that takes as input the source pronoun and the source context words, target lemmata and target PoS tags in a window of 3 words to the left and to the right of the pronoun. In addition, the network receives a list of potential antecedent candidates identified by the pr"
W16-2345,W12-3018,0,\N,Missing
W16-2345,2015.iwslt-evaluation.1,1,\N,Missing
W16-2345,W14-6111,0,\N,Missing
W16-2352,W10-1737,0,0.234239,"Missing"
W16-2352,W16-3416,1,0.529924,"in (b), depending on the number of feminine singular (pro)nouns preceding elle. other exhibit strong independence from the gender and number of the previous nouns and pronouns, hence they are unable to benefit from the PLM as much as the remaining ones. To detect their presence in the target sentence, we apply specific rules, based on their grammar constraints with the neighboring words. Moreover, the log-probabilities for four combinations of features ({masculine, feminine} × {singular, plural}) and the twelve most frequent French pronouns which are translations of it and they are given in (Luong and Popescu-Belis, 2016). These results suggest that, for most thirdperson pronouns (il, elle, ils, elles, le, la) the average log-probability of the pronoun gradually increases when more and more nouns (or pronouns) of the same gender and number are found before it. By contrast, the log-probability decreases with the presence of more words of a different gender and number. However, such tendencies are not observed for the neuter indefinite pronoun on, the vowel-preceding object pronoun l’, or the indirect object pronoun lui. Another important observation, which holds for all four possible combinations of gender and"
W16-2352,W15-2513,1,0.868301,"Missing"
W16-2352,W15-2508,0,0.0278084,"Missing"
W16-2352,2012.eamt-1.60,0,0.0228804,"ed as a pronoun-aware language model (PLM), which is trained as explained in the next subsection, and is then used for selecting pronoun candidate as explained in Section 6. 590 4.2 Learning the PLM are always ending a sequence in the training data, but not necessarily in the n-grams generated by SRILM, as exemplified in Figure 1: the examples include n-grams that do not end with a pronoun, e.g. the fifth and the sixth ones. These will be needed for back-off search and are kept in the model used below. The data used for training the PLM is the target side (French) of the WIT3 parallel corpus (Cettolo et al., 2012) distributed by the IWSLT workshops. This corpus is made of transcripts of TED talks, i.e. lectures that typically last 18 minutes, on various topics from science and the humanities with high relevance to society. The TED talks are given in English, then transcribed and translated by volunteers and TED editors. The French side contains 179,404 sentences, with a total of 3,880,369 words. We process the data sequentially, word by word, from the beginning to the end. We keep track of the gender and number of the N most recent nouns and pronouns in a list, which is initialized as empty and is then"
W16-2352,W15-2515,0,0.0438574,"Missing"
W16-2352,chrupala-etal-2008-learning,0,0.380149,"Missing"
W16-2352,2010.iwslt-papers.10,0,0.265345,"Missing"
W16-2352,W15-2501,0,0.494565,"to the target-side texts provided in the subtask. Although the PLM helps to outperform a random baseline, it still scores far lower than system using both source and target texts. 1 Introduction The translation of pronouns has been recognized as a challenge since the early years of machine translation (MT), as pronoun systems do not map 1:1 across languages. Recently, specific strategies for translating pronouns have been proposed and evaluated, as reviewed by Hardmeier (2014, Section 2.3.1) and by Guillou (Guillou, 2016). Following the DiscoMT 2015 shared task on pronoun-focused translation (Hardmeier et al., 2015), the goal of the 2016 WMT pronoun shared task (Guillou et al., 2016) is to compare systems that are able to predict the translation of a source pronoun among a small, closed set of target candidates. The task was proposed for four language pairs: English/German and English/French, in both directions. Besides the original source documents (transcripts of TED talks), participants were given the formatted target documents, where 589 Proceedings of the First Conference on Machine Translation, Volume 2: Shared Task Papers, pages 589–595, c Berlin, Germany, August 11-12, 2016. 2016 Association for"
W16-3416,W14-3302,0,0.0610718,"Missing"
W16-3416,W15-2508,0,0.335913,"Missing"
W16-3416,2012.eamt-1.60,0,0.0445238,"Missing"
W16-3416,chrupala-etal-2008-learning,0,0.193397,"Missing"
W16-3416,E12-3001,0,0.017167,"le-based or statistical methods for anaphora resolution, several studies have attempted to improve pronoun translation by integrating anaphora resolution with statistical MT, as reviewed by Hardmeier (2014, Section 2.3.1). Le Nagard and Koehn (2010) trained an English-French translation model on an annotated corpus in which each occurrence of English pronouns it and they was annotated with the gender of its antecedent in the target side, but this solution could not outperform a baseline that was not aware of coreference links. Integrating anaphora resolution with English-Czech statistical MT, Guillou (2012) studied the role of imperfect coreference and alignment results. Hardmeier and Federico (2010) integrated a word dependency model into an SMT decoder as an additional feature function, which keeps track of pairs of source words acting as antecedent and anaphor in a coreference link. This model helped to improve slightly the EnglishGerman SMT performance (F-score customized for pronouns) on the WMT News Commentary 2008 and 2009 test sets. Following a similar strategy, Luong et al. (2015) linearly combined the score obtained from a coreference resolution system with the score from the search gr"
W16-3416,P14-6007,0,0.0740077,"noun systems do not strictly map across languages, and therefore translation divergencies of pronouns must often be addressed in machine translation (MT). For instance, depending on its function (referential or pleonastic) and on its actual referent, an occurrence of the English it could be translated into French by il, elle, ce/c’ or cela, to mention only the most frequent possibilities. While designers of MT systems have tried to address the problem since the early years of MT, it is only in recent years that specific strategies for translating pronouns have been proposed and evaluated (see Hardmeier, 2014, Section 2.3.1). However, in the culmination of these recent efforts at the DiscoMT 2015 shared task on pronounfocused translation (Hardmeier et al., 2015), none of the submitted systems was able to beat a well-trained phrase-based statistical MT baseline. A large proportion of previous studies have attempted to convey information from anaphora resolution systems, albeit Contextual Language Model for Pronouns 293 imperfect, to statistical MT ones (Hardmeier and Federico, 2010; Le Nagard and Koehn, 2010), or have advocated distinguishing first the functions of pronouns (Guillou, 2016). In this"
W16-3416,2010.iwslt-papers.10,0,0.071621,"tempted to improve pronoun translation by integrating anaphora resolution with statistical MT, as reviewed by Hardmeier (2014, Section 2.3.1). Le Nagard and Koehn (2010) trained an English-French translation model on an annotated corpus in which each occurrence of English pronouns it and they was annotated with the gender of its antecedent in the target side, but this solution could not outperform a baseline that was not aware of coreference links. Integrating anaphora resolution with English-Czech statistical MT, Guillou (2012) studied the role of imperfect coreference and alignment results. Hardmeier and Federico (2010) integrated a word dependency model into an SMT decoder as an additional feature function, which keeps track of pairs of source words acting as antecedent and anaphor in a coreference link. This model helped to improve slightly the EnglishGerman SMT performance (F-score customized for pronouns) on the WMT News Commentary 2008 and 2009 test sets. Following a similar strategy, Luong et al. (2015) linearly combined the score obtained from a coreference resolution system with the score from the search graph of the Moses decoder, to determine whether an English-French SMT pronoun translation should"
W16-3416,W15-2501,0,0.0749645,"Missing"
W16-3416,2005.mtsummit-papers.11,0,0.0319715,"of a translation hypothesis: we add the gender and number of the four nouns preceding the pronoun il, which is tagged as PRN by Morfette (wrong translation of the source it instead of elle). ‘SRC−1’ and ‘HYP−1’ denote the source and target sentences before the one being processed, and ‘F-HYP’ denotes the formatted sentence. 300 6 6.1 Luong and Popescu-Belis Experiments Settings and evaluation metrics We trained the Moses phrase-based SMT system (Koehn et al., 2007) on the following parallel and monolingual datasets: aligned TED talks from the WIT3 corpus (Cettolo et al., 2012), Europarl v. 7 (Koehn, 2005), News Commentary v. 9 and other news data from WMT 2007–2013 (Bojar et al., 2014). The system was tuned on a development set of 887 sentences from IWSLT 2010 provided for the shared task on pronoun translation of the DiscoMT 2015 workshop (Hardmeier et al., 2015). Our test set was also the one of the DiscoMT 2015 shared task, with 2,093 English sentences extracted from 12 recent TED talks (French gold-standard translations were made available after the task). The test set contains 809 occurrences of it and 307 of they, hence a total of 1,116 pronouns. We compare two systems: (1) the Moses phr"
W16-3416,P07-2045,0,0.00872378,", if all four preceding words are masculine singular, then the most likely pronoun is il (−0.891). Moreover, among the remaining pronouns, the PLM prioritizes the neuter ones (e.g. ce, c’ , or ca) over those of the opposite gender or number. This is indeed beneficial for pronoun selection by re-ranking hypotheses from an SMT decoder, since it is preferable to reward neutral or pleonastic pronouns rather than rewarding a pronoun with a gender and number which is not shared with any of the four nouns preceding it. 5 Re-ranking translation hypotheses with the PLM The Moses statistical MT system (Koehn et al., 2007) used in this study outputs on demand a list of N-best translation hypotheses, for every source sentence, together with their score. In production mode, only the 1-best hypothesis is output as the translation of the source. However, in this study, we will consider several translation hypotheses for the source sentences containing the pronouns it or they, and re-rank them based on additional information from the pronoun language model presented above. As a result, the 1-best hypothesis may change, and we will demonstrate in Section 6 that pronoun translation is on average improved. For every so"
W16-3416,W10-1737,0,0.180964,"Missing"
W16-3416,W15-2513,1,0.841617,"line that was not aware of coreference links. Integrating anaphora resolution with English-Czech statistical MT, Guillou (2012) studied the role of imperfect coreference and alignment results. Hardmeier and Federico (2010) integrated a word dependency model into an SMT decoder as an additional feature function, which keeps track of pairs of source words acting as antecedent and anaphor in a coreference link. This model helped to improve slightly the EnglishGerman SMT performance (F-score customized for pronouns) on the WMT News Commentary 2008 and 2009 test sets. Following a similar strategy, Luong et al. (2015) linearly combined the score obtained from a coreference resolution system with the score from the search graph of the Moses decoder, to determine whether an English-French SMT pronoun translation should be post-edited into the opposite gender (e.g. il → elle). Their system performed best among six participants on the pronoun-focused shared task at the 2015 DiscoMT workshop (Hardmeier et al., 2015), but still remained below the SMT baseline. A considerable set of coreference features, used in a deep neural network architecture, was presented by Hardmeier (2014, Chapters 7–9), who observed sign"
W16-6213,P05-1015,0,0.0823668,"e matched by the system, according to this metric. Still, these results provide evidence that the weights found by the system capture the explanatory value of sentences in a way that is similar to humans. 6 Related Work Multi-aspect sentiment analysis. This task usually requires aspect segmentation, followed by prediction or summarization (Hu and Liu, 2004; Zhuang et al., 2006). Most related studies have engineered various feature sets, augmenting words with topic or content models (Mei et al., 2007; Titov and McDonald, 2008; Sauper et al., 2010; Lu et al., 2011), or with linguistic features (Pang and Lee, 2005; Qu et al., 2010; Zhu et al., 2012). McAuley et al. (2012) proposed an interpretable probabilistic model for modeling aspect reviews. Kim et al. (2013) proposed an hierarchical model to discover the review structure from unlabeled corpora. Previous systems for rating prediction were trained on segmented texts (Zhu et al., 2012; McAuley et al., 2012), while our system (Pappas and Popescu-Belis, 2014) used weak supervision on unsegmented text. Here, we introduced a new evaluation of such models on sentiment summarization considering human attention. 98 Document classification. Recent studies ha"
W16-6213,D14-1052,1,0.867827,"Missing"
W16-6213,C10-1103,0,0.0145404,"tem, according to this metric. Still, these results provide evidence that the weights found by the system capture the explanatory value of sentences in a way that is similar to humans. 6 Related Work Multi-aspect sentiment analysis. This task usually requires aspect segmentation, followed by prediction or summarization (Hu and Liu, 2004; Zhuang et al., 2006). Most related studies have engineered various feature sets, augmenting words with topic or content models (Mei et al., 2007; Titov and McDonald, 2008; Sauper et al., 2010; Lu et al., 2011), or with linguistic features (Pang and Lee, 2005; Qu et al., 2010; Zhu et al., 2012). McAuley et al. (2012) proposed an interpretable probabilistic model for modeling aspect reviews. Kim et al. (2013) proposed an hierarchical model to discover the review structure from unlabeled corpora. Previous systems for rating prediction were trained on segmented texts (Zhu et al., 2012; McAuley et al., 2012), while our system (Pappas and Popescu-Belis, 2014) used weak supervision on unsegmented text. Here, we introduced a new evaluation of such models on sentiment summarization considering human attention. 98 Document classification. Recent studies have shown that att"
W16-6213,D15-1044,0,0.0456439,"hierarchical model to discover the review structure from unlabeled corpora. Previous systems for rating prediction were trained on segmented texts (Zhu et al., 2012; McAuley et al., 2012), while our system (Pappas and Popescu-Belis, 2014) used weak supervision on unsegmented text. Here, we introduced a new evaluation of such models on sentiment summarization considering human attention. 98 Document classification. Recent studies have shown that attention mechanisms are beneficial to machine translation (Bahdanau et al., 2014), question answering (Sukhbaatar et al., 2015), text summarization (Rush et al., 2015), and document classification (Pappas and Popescu-Belis, 2014). Most recently, Yang et al. (2016) introduced hierarchical attention networks for document classification. Despite the improvements, it is yet unclear what exactly this attention mechanism captures for the task at hand. Our dataset enables the direct comparison of such mechanism and human attention scores for document classification, thus contributing to a better understanding of the document attention models. 7 Conclusion We presented a new dataset with human attention to sentences triggered when attributing aspect ratings to revi"
W16-6213,D10-1037,0,0.0123977,"’. For ‘performance’, however, the high agreement of judges cannot be matched by the system, according to this metric. Still, these results provide evidence that the weights found by the system capture the explanatory value of sentences in a way that is similar to humans. 6 Related Work Multi-aspect sentiment analysis. This task usually requires aspect segmentation, followed by prediction or summarization (Hu and Liu, 2004; Zhuang et al., 2006). Most related studies have engineered various feature sets, augmenting words with topic or content models (Mei et al., 2007; Titov and McDonald, 2008; Sauper et al., 2010; Lu et al., 2011), or with linguistic features (Pang and Lee, 2005; Qu et al., 2010; Zhu et al., 2012). McAuley et al. (2012) proposed an interpretable probabilistic model for modeling aspect reviews. Kim et al. (2013) proposed an hierarchical model to discover the review structure from unlabeled corpora. Previous systems for rating prediction were trained on segmented texts (Zhu et al., 2012; McAuley et al., 2012), while our system (Pappas and Popescu-Belis, 2014) used weak supervision on unsegmented text. Here, we introduced a new evaluation of such models on sentiment summarization conside"
W16-6213,D15-1166,0,0.044528,"set of bags (here, reviews), each of which contains a variable number of instances (here, sentences). The labels used for training (here, the aspect ratings) can be at the bag level (weak supervision), and not at the instance level. Our system learns to assign importance scores to individual instances, and to predict the labels of unseen bags. In past models, the influence of instance labels on bag labels has been modeled with simplifying assumptions (e.g. averaging), whereas our system learns to aggregate instances of a bag according to their importance, like attention-based neural networks (Luong et al., 2015). To jointly learn instance weights and target labels, the system minimizes a regularized least squares loss. While in our 2014 paper this was done using alternating projections (as in Wagstaff and Lane, 2007), we use here stochastic gradient descent (Bottou, 1998) with the efficient ADAGRAD implementation (Duchi et al., 2011). In particular, the attention is modeled by a normalized exponential function, namely a softmax and a linear activation between a contextual vector and the document matrix (sentence vectors). Essentially, this formulation enables learning with stochastic gradient descent"
W16-6213,N16-1174,0,0.166337,"input documents, as it is unknown which parts of the documents convey information about each category refer to. Using supervised learning to solve this task requires labeled data. Several previous studies have adopted a strongly-supervised approach using sentence-level labels (McAuley et al., 2012; Zhu et al., 2012), obtained with a significant human annotation effort. However, document-level labels are often available in social media, but learning from them requires a weakly-supervised approach. Recently, attention mechanisms for document modeling, either using hierarchical neural networks (Yang et al., 2016) or weighted multiple-instance learning (Pappas and Popescu-Belis, 2014), have proved superior in classification performance and are also able to quantify the contribution of each sentence to the documentlevel category. While explicit document models can be indirectly evaluated on aspect rating prediction or document segmentation, a more direct way to estimate their qualities is to compare the sentence-level weights or attention scores that they assign with those assigned by human judges. In this paper, we present a dataset1 containing human estimates of the contribution of each sentence of an"
W17-1505,D08-1031,0,0.0297383,"r to the same entity in a text. This task includes two stages: mention identification, and coreference resolution. The first stage is usually based on part-of-speech annotation and named-entity recognition. Candidate mentions are usually noun phrases, pronouns, and named entities (Lee et al., 2011). Coreference resolvers follow three main approaches: pairwise, re-ranking, and clustering. Pairwise resolvers perform a binary classification, predicting if two mentions refer to the same entity or not. This assumes strong independence of mentions and does not utilize features of the entire entity (Bengtson and Roth, 2008). The second approach lists a set of candidate antecedents for each mention that are simultaneously considered to find the best match. Interpolation between the best and worse candidate is considered (Wiseman et al., 2015; Bengtson and Roth, 2008). Finally, the clustering approach considers the features of a complete cluster of mentions to decide whether a mention belongs or not to a cluster (Clark and Manning, 2015; Fernandes et al., 2012). Coreference resolution is typically evaluated in comparison with a gold-standard annotation (Popescu-Belis, 1999; Recasens and Hovy, 2011). The main metri"
W17-1505,W15-2508,0,0.0157597,"in a probabilistic way, and demonstrated improvement on pronouns. Two shared tasks on pronoun-focused translation have been recently organized. The improvement of pronoun translation was only marginal with respect to a baseline SMT system in the 2015 shared task (Hardmeier et al., 2015), while the 2016 shared task was only aiming at pronoun prediction given source texts and lemmatized reference translations (Guillou et al., 2016). Some of the best systems developed for these tasks avoided, in fact, the direct use of anaphora resolution (with the exception of Luong et al. (2015)). For example, Callin et al. (2015) designed a classifier based on a feed-forward neural network, which considered as features the preceding nouns and determiners along with their part-of-speech tags. The winning systems of the 2016 task used deep neural networks: Luotolahti et al. (2016) and Dabre et al. (2016) summarized the preceding and following contexts of the pronoun to predict and passed them to a recurrent neural network. To the best of our knowledge, we present here the first proof-ofconcept that coreference links across noun phrases and pronouns can serve to improve statistical MT. Related Work Coreference Resolution"
W17-1505,P15-1136,0,0.0425437,"Missing"
W17-1505,P16-1061,0,0.019308,"0.26 (match, political party) = 0.08, (party, political party) = 0.53 (political party, who) = 0.12, (political party, which) = 0.27 j i6=j ∈c xm x The information about the entity in source side can indicate how well a particular hypothesis represents it. Thus, we define a simple representation of an entity by setting relevant features such as gender, number, and animation. The features are extracted and summarized from all mentions in the cluster. This is a naive representation, and more advanced work on entity-level representations has been performed in relation to coreference resolution (Clark and Manning, 2016; Wiseman et al., 2016), which could be applied here in the future. We illustrate this idea with an example. Here, we have a sentence in Spanish and its translation to English. We show one coreference cluster c1 formed by three mentions: Source (es): La alcaldesa de M´alaga y cabeza del [partido]c1 [que]c1 gan´o en esta ciudad, pidi´o a los militantes de [este partido pol´ıtico]c1 ... Target (en): The mayor of Malaga and head of the [m1 ]c1 [m2 ]c1 won in this city, asked the militants of this [m3 ]c1 to... 35 Having an entity representation, we define a simple scoring function which measures"
W17-1505,W16-2349,0,0.0127352,"., 2015), while the 2016 shared task was only aiming at pronoun prediction given source texts and lemmatized reference translations (Guillou et al., 2016). Some of the best systems developed for these tasks avoided, in fact, the direct use of anaphora resolution (with the exception of Luong et al. (2015)). For example, Callin et al. (2015) designed a classifier based on a feed-forward neural network, which considered as features the preceding nouns and determiners along with their part-of-speech tags. The winning systems of the 2016 task used deep neural networks: Luotolahti et al. (2016) and Dabre et al. (2016) summarized the preceding and following contexts of the pronoun to predict and passed them to a recurrent neural network. To the best of our knowledge, we present here the first proof-ofconcept that coreference links across noun phrases and pronouns can serve to improve statistical MT. Related Work Coreference Resolution and Evaluation Coreference resolution is the task of grouping together the expressions that refer to the same entity in a text. This task includes two stages: mention identification, and coreference resolution. The first stage is usually based on part-of-speech annotation and"
W17-1505,W10-1737,0,0.0309715,"Missing"
W17-1505,W12-4502,0,0.0257218,"Missing"
W17-1505,W11-1902,0,0.0381677,"rent neural network. To the best of our knowledge, we present here the first proof-ofconcept that coreference links across noun phrases and pronouns can serve to improve statistical MT. Related Work Coreference Resolution and Evaluation Coreference resolution is the task of grouping together the expressions that refer to the same entity in a text. This task includes two stages: mention identification, and coreference resolution. The first stage is usually based on part-of-speech annotation and named-entity recognition. Candidate mentions are usually noun phrases, pronouns, and named entities (Lee et al., 2011). Coreference resolvers follow three main approaches: pairwise, re-ranking, and clustering. Pairwise resolvers perform a binary classification, predicting if two mentions refer to the same entity or not. This assumes strong independence of mentions and does not utilize features of the entire entity (Bengtson and Roth, 2008). The second approach lists a set of candidate antecedents for each mention that are simultaneously considered to find the best match. Interpolation between the best and worse candidate is considered (Wiseman et al., 2015; Bengtson and Roth, 2008). Finally, the clustering ap"
W17-1505,W16-2345,1,0.864335,"the MT baseline. Recently, a model for MT decoding proposed by Luong (2016; 2017) combined several features of the antecedent candidates (gender, number and humanness) with an MT decoder, in a probabilistic way, and demonstrated improvement on pronouns. Two shared tasks on pronoun-focused translation have been recently organized. The improvement of pronoun translation was only marginal with respect to a baseline SMT system in the 2015 shared task (Hardmeier et al., 2015), while the 2016 shared task was only aiming at pronoun prediction given source texts and lemmatized reference translations (Guillou et al., 2016). Some of the best systems developed for these tasks avoided, in fact, the direct use of anaphora resolution (with the exception of Luong et al. (2015)). For example, Callin et al. (2015) designed a classifier based on a feed-forward neural network, which considered as features the preceding nouns and determiners along with their part-of-speech tags. The winning systems of the 2016 task used deep neural networks: Luotolahti et al. (2016) and Dabre et al. (2016) summarized the preceding and following contexts of the pronoun to predict and passed them to a recurrent neural network. To the best o"
W17-1505,H05-1004,0,0.18462,"lete cluster of mentions to decide whether a mention belongs or not to a cluster (Clark and Manning, 2015; Fernandes et al., 2012). Coreference resolution is typically evaluated in comparison with a gold-standard annotation (Popescu-Belis, 1999; Recasens and Hovy, 2011). The main metrics used for evaluation are MUC (Vilain et al., 1995), which counts the minimum number of links between mentions to be inserted or deleted in order to map the evaluated document to the gold-standard. The B3 measure (Bagga and Baldwin, 1998) computes precision and recall for all mentions of a document, while CEAF (Luo, 2005) computes them at the entity level. BLANC (Recasens and Hovy, 2011) makes use of the Rand Index, an algorithm for the evaluation of clustering. These metrics are implemented in the scorer for CoNLL 2012 (Pradhan et al., 2014) and the SemEval 2013 one (M`arquez et al., 2013). 3 Coreference Resolution for MT A principle of translation is that the information conveyed in a document should be preserved in 31 Source Human Translation Machine Translation La pel´ıcula narra la historia de [un joven parisiense]c1 que marcha a Ruman´ıa en busca de [una cantante z´ıngara]c2 , ya que [su]c1 fallecido pad"
W17-1505,W16-2202,1,0.838883,"Missing"
W17-1505,E12-3001,0,0.0182076,"ons, and postedits them using information from coreference chains in the source text. Finally, the results presented in Section 6 show that the second method increases the accuracy of pronoun translation from Spanish to English, while obtaining BLEU scores similar to those of the MT baseline. 2 2.1 Coreference-Aware Machine Translation Despite the numerous coreference and anaphora resolution systems designed in the past decades (Mitkov, 2002; Ng, 2010), the interest in using them to improve pronoun translation has only recently emerged (Le Nagard and Koehn, 2010; Hardmeier and Federico, 2010; Guillou, 2012). The still limited accuracy of coreference resolution may explain its restricted use in MT, although, it has long been known that some pronouns require knowledge of the antecedent for correct translation. For instance, Le Nagard and Koehn (2010) trained an English-French translation model on an annotated corpus in which each occurrence of the English pronouns it and they was annotated with the gender of its antecedent on the target side. Their system correctly translated 40 pronouns out of the 59 that they examined, but did not outperform the MT baseline. Recently, a model for MT decoding pro"
W17-1505,2010.iwslt-papers.10,0,0.101543,"nslation variants of the mentions, and postedits them using information from coreference chains in the source text. Finally, the results presented in Section 6 show that the second method increases the accuracy of pronoun translation from Spanish to English, while obtaining BLEU scores similar to those of the MT baseline. 2 2.1 Coreference-Aware Machine Translation Despite the numerous coreference and anaphora resolution systems designed in the past decades (Mitkov, 2002; Ng, 2010), the interest in using them to improve pronoun translation has only recently emerged (Le Nagard and Koehn, 2010; Hardmeier and Federico, 2010; Guillou, 2012). The still limited accuracy of coreference resolution may explain its restricted use in MT, although, it has long been known that some pronouns require knowledge of the antecedent for correct translation. For instance, Le Nagard and Koehn (2010) trained an English-French translation model on an annotated corpus in which each occurrence of the English pronouns it and they was annotated with the gender of its antecedent on the target side. Their system correctly translated 40 pronouns out of the 59 that they examined, but did not outperform the MT baseline. Recently, a model for"
W17-1505,E17-2100,1,0.893973,"Missing"
W17-1505,D12-1108,0,0.0382677,"Missing"
W17-1505,W15-2513,1,0.853581,"and humanness) with an MT decoder, in a probabilistic way, and demonstrated improvement on pronouns. Two shared tasks on pronoun-focused translation have been recently organized. The improvement of pronoun translation was only marginal with respect to a baseline SMT system in the 2015 shared task (Hardmeier et al., 2015), while the 2016 shared task was only aiming at pronoun prediction given source texts and lemmatized reference translations (Guillou et al., 2016). Some of the best systems developed for these tasks avoided, in fact, the direct use of anaphora resolution (with the exception of Luong et al. (2015)). For example, Callin et al. (2015) designed a classifier based on a feed-forward neural network, which considered as features the preceding nouns and determiners along with their part-of-speech tags. The winning systems of the 2016 task used deep neural networks: Luotolahti et al. (2016) and Dabre et al. (2016) summarized the preceding and following contexts of the pronoun to predict and passed them to a recurrent neural network. To the best of our knowledge, we present here the first proof-ofconcept that coreference links across noun phrases and pronouns can serve to improve statistical MT."
W17-1505,W15-2501,0,0.0514491,"gender of its antecedent on the target side. Their system correctly translated 40 pronouns out of the 59 that they examined, but did not outperform the MT baseline. Recently, a model for MT decoding proposed by Luong (2016; 2017) combined several features of the antecedent candidates (gender, number and humanness) with an MT decoder, in a probabilistic way, and demonstrated improvement on pronouns. Two shared tasks on pronoun-focused translation have been recently organized. The improvement of pronoun translation was only marginal with respect to a baseline SMT system in the 2015 shared task (Hardmeier et al., 2015), while the 2016 shared task was only aiming at pronoun prediction given source texts and lemmatized reference translations (Guillou et al., 2016). Some of the best systems developed for these tasks avoided, in fact, the direct use of anaphora resolution (with the exception of Luong et al. (2015)). For example, Callin et al. (2015) designed a classifier based on a feed-forward neural network, which considered as features the preceding nouns and determiners along with their part-of-speech tags. The winning systems of the 2016 task used deep neural networks: Luotolahti et al. (2016) and Dabre et"
W17-1505,W16-2353,0,0.0130003,"shared task (Hardmeier et al., 2015), while the 2016 shared task was only aiming at pronoun prediction given source texts and lemmatized reference translations (Guillou et al., 2016). Some of the best systems developed for these tasks avoided, in fact, the direct use of anaphora resolution (with the exception of Luong et al. (2015)). For example, Callin et al. (2015) designed a classifier based on a feed-forward neural network, which considered as features the preceding nouns and determiners along with their part-of-speech tags. The winning systems of the 2016 task used deep neural networks: Luotolahti et al. (2016) and Dabre et al. (2016) summarized the preceding and following contexts of the pronoun to predict and passed them to a recurrent neural network. To the best of our knowledge, we present here the first proof-ofconcept that coreference links across noun phrases and pronouns can serve to improve statistical MT. Related Work Coreference Resolution and Evaluation Coreference resolution is the task of grouping together the expressions that refer to the same entity in a text. This task includes two stages: mention identification, and coreference resolution. The first stage is usually based on part-o"
W17-1505,P14-5010,0,0.0138852,"2 songs. Pudiera considerarse un viaje fallido, porque [∅]c1 no encuentra [su]c1 objetivo, pero el azar [le]c1 conduce a una pequea comunidad... It could be considered a failed journey, because [he]c1 does not find [his]c1 objective, but the fate leads [him]c1 to a small community... It could be considered [a failed trip]c3 , because [it]c3 does not find [its]c3 objective, but the chance leads ∅ to a small community... Table 1: Comparison of coreference chains in the Spanish source vs. English human and machine translations. English chains were obtained with the Stanford coreference resolver (Manning et al., 2014). The chains are numbed c1 , c2 , . . . and are also color-coded. The void symbol ∅ indicates a correct null subject pronoun in Spanish, and an incorrect object pronoun dropped by the MT system. The third coreference chain (c3 ) in the MT output is erroneous. its translation. Here, we focus on the referential information, i.e. the coreference links between mentions. If we apply coreference resolution to a source text and to a faithful translation of it, then the grouping of mentions should be identical. We thus formulate the following criterion for MT: better translations should have coreferen"
W17-1505,P07-2045,0,0.00733282,"Missing"
W17-1505,M95-1005,0,0.42739,"t of candidate antecedents for each mention that are simultaneously considered to find the best match. Interpolation between the best and worse candidate is considered (Wiseman et al., 2015; Bengtson and Roth, 2008). Finally, the clustering approach considers the features of a complete cluster of mentions to decide whether a mention belongs or not to a cluster (Clark and Manning, 2015; Fernandes et al., 2012). Coreference resolution is typically evaluated in comparison with a gold-standard annotation (Popescu-Belis, 1999; Recasens and Hovy, 2011). The main metrics used for evaluation are MUC (Vilain et al., 1995), which counts the minimum number of links between mentions to be inserted or deleted in order to map the evaluated document to the gold-standard. The B3 measure (Bagga and Baldwin, 1998) computes precision and recall for all mentions of a document, while CEAF (Luo, 2005) computes them at the entity level. BLANC (Recasens and Hovy, 2011) makes use of the Rand Index, an algorithm for the evaluation of clustering. These metrics are implemented in the scorer for CoNLL 2012 (Pradhan et al., 2014) and the SemEval 2013 one (M`arquez et al., 2013). 3 Coreference Resolution for MT A principle of trans"
W17-1505,P15-1137,0,0.0180155,"are usually noun phrases, pronouns, and named entities (Lee et al., 2011). Coreference resolvers follow three main approaches: pairwise, re-ranking, and clustering. Pairwise resolvers perform a binary classification, predicting if two mentions refer to the same entity or not. This assumes strong independence of mentions and does not utilize features of the entire entity (Bengtson and Roth, 2008). The second approach lists a set of candidate antecedents for each mention that are simultaneously considered to find the best match. Interpolation between the best and worse candidate is considered (Wiseman et al., 2015; Bengtson and Roth, 2008). Finally, the clustering approach considers the features of a complete cluster of mentions to decide whether a mention belongs or not to a cluster (Clark and Manning, 2015; Fernandes et al., 2012). Coreference resolution is typically evaluated in comparison with a gold-standard annotation (Popescu-Belis, 1999; Recasens and Hovy, 2011). The main metrics used for evaluation are MUC (Vilain et al., 1995), which counts the minimum number of links between mentions to be inserted or deleted in order to map the evaluated document to the gold-standard. The B3 measure (Bagga"
W17-1505,P10-1142,0,0.0302705,"d with EACL 2017, pages 30–40, c Valencia, Spain, April 4, 2017. 2017 Association for Computational Linguistics 2.2 the translation variants of the mentions, and postedits them using information from coreference chains in the source text. Finally, the results presented in Section 6 show that the second method increases the accuracy of pronoun translation from Spanish to English, while obtaining BLEU scores similar to those of the MT baseline. 2 2.1 Coreference-Aware Machine Translation Despite the numerous coreference and anaphora resolution systems designed in the past decades (Mitkov, 2002; Ng, 2010), the interest in using them to improve pronoun translation has only recently emerged (Le Nagard and Koehn, 2010; Hardmeier and Federico, 2010; Guillou, 2012). The still limited accuracy of coreference resolution may explain its restricted use in MT, although, it has long been known that some pronouns require knowledge of the antecedent for correct translation. For instance, Le Nagard and Koehn (2010) trained an English-French translation model on an annotated corpus in which each occurrence of the English pronouns it and they was annotated with the gender of its antecedent on the target side."
W17-1505,P02-1040,0,0.0990371,"mely the CorZu system of these choices on global translation quality. We (Tuggener, 2016; Rios, 2015), for the post-editing translated 10 sample documents from the test set approach. to serve as reference translations for evaluation. Table 3 shows the results of the experiments. We first calculate BLEU, APT, and ANT values 6.1 Evaluation with Automatic Metrics at document-level, and show the values of the avThe evaluation of global MT quality is made with erage and standard deviation for the three evaluthe well-known BLEU n-gram precision metric ated systems: baseline, and our two proposed ap(Papineni et al., 2002), while the evaluation of proaches. Additionally, we show the significance mentions, being less standardized, is performed in levels (t-test) of the results in comparison to the several ways. We reuse previous insights on pro5 noun translation and therefore score them with a https://github.com/idiap/APT The final score is defined as follows: 36 Evaluation No. ‘0’ (wrong) No. ‘1’ (acceptable) No. ‘2’ (eq. to ref.) Sum of the scores Baseline 53 21 115 251 System Re-rank 55 19 115 249 Post-edit 21 28 140 308 Table 4: Manual evaluation of fourth randomly selected documents. The evaluation was done"
W17-1505,N16-1114,0,0.0597284,"Missing"
W17-1505,P14-2006,0,0.0688139,"nnotation (Popescu-Belis, 1999; Recasens and Hovy, 2011). The main metrics used for evaluation are MUC (Vilain et al., 1995), which counts the minimum number of links between mentions to be inserted or deleted in order to map the evaluated document to the gold-standard. The B3 measure (Bagga and Baldwin, 1998) computes precision and recall for all mentions of a document, while CEAF (Luo, 2005) computes them at the entity level. BLANC (Recasens and Hovy, 2011) makes use of the Rand Index, an algorithm for the evaluation of clustering. These metrics are implemented in the scorer for CoNLL 2012 (Pradhan et al., 2014) and the SemEval 2013 one (M`arquez et al., 2013). 3 Coreference Resolution for MT A principle of translation is that the information conveyed in a document should be preserved in 31 Source Human Translation Machine Translation La pel´ıcula narra la historia de [un joven parisiense]c1 que marcha a Ruman´ıa en busca de [una cantante z´ıngara]c2 , ya que [su]c1 fallecido padre escuchaba siempre [sus]c2 canciones. The film tells the story of [a young Parisian]c1 who goes to Romania in search of [a gypsy singer]c2 , as [his]c1 deceased father use to listen to [her]c2 songs. The film tells the stor"
W17-1505,W09-2411,0,0.0660938,"Missing"
W17-4701,2012.eamt-1.60,0,0.0260994,"dd a NULL label.6 The translation system will thus take the source-side sense labels into consideration during the training and the decoding processes. 4 • we use the definition of each sense for initializing the centroids in the adaptive k-means methods (and compare this later with using the examples); Datasets, Preparation and Settings We evaluate our sense-aware SMT on the UN Corpus7 (Rafalovitch and Dale, 2009) and on the Europarl Corpus8 (Koehn, 2005). We select 0.5 million parallel sentences for each language pair from Europarl, as shown in Table 1. We also use the smaller WIT3 Corpus9 (Cettolo et al., 2012), a collection of transcripts of TED talks, to evaluate the impact of costly model choices, namely the type of the resource (definition vs. examples), the length of the context window, and the k-means method (adaptive vs. original). Before assigning sense labels, we first tokenize all the texts and identify the parts of speech (POS) using the Stanford POS tagger10 . Then, we filter out the stopwords and the nouns which are proper names according to the Stanford Name Entity Recognizer10 . Furthermore, we convert the • we set kt equal to mt , i.e. the number of senses of an ambiguous word type W"
W17-4701,P07-1005,0,0.231816,"word sense ambiguities should be useful, in particular, for lexical choice in MT. An initial investigation found that an SMT system which makes use of off-the-shelf WSD does not yield significantly better quality translations than a SMT system not using it (Carpuat and Wu, 2005). However, another study (Vickrey et al., 2005) reformulated the task of WSD for SMT as predicting possible target translations rather than senses of ambiguous source words, and showed that WSD improved such a simplified word translation task. Subsequent studies which adopted this formulation (Cabezas and Resnik, 2005; Chan et al., 2007; Carpuat and Wu, 2007), successfully integrated WSD to hierarchical or phrase-based SMT. These systems yielded slightly better translations compared to SMT baselines in most cases (0.15–0.30 BLEU). Although the WSD reformulation above proved helpful for SMT, it did not determine whether actual source-side senses are helpful or not for endto-end SMT. Xiong and Zhang (2014) attempted to answer this question by performing word sense induction for large scale data. In particular, they proposed a topic model that automatically learned sense clusters for words in the source language. In this way, o"
W17-4701,S10-1082,0,0.0564266,"Missing"
W17-4701,E09-1005,0,0.114869,"If these numbers come from WordNet, the senses may be too fine-grained for the needs of translation, especially when a specific domain is targeted. In contrast, as we explain below, our WSD method initializes a contextdependent clustering algorithm with information from WordNet senses for each word (nouns and verbs), but then adapts the number of clusters to the observed training data for MT. semantic resources such as WordNet (Fellbaum, 1998) can be successfully integrated with SMT. Neale et al. (2016) attempted such an integration, by using a WSD system based on a sense graph from WordNet (Agirre and Soroa, 2009). This system detects the senses of words in context using a random walk algorithm over the sense graph. The authors used it to specify the senses of the source words and integrate them as contextual features with a MaxEnt-based translation model for English-Portuguese MT. Similarly, Su et al. (2015) built a large weighted graph model of both source and target word dependencies and integrated them as features to a SMT model. However, apart from the sense graph, WordNet provides also textual information such as sense definitions and examples, which should be useful for disambiguating senses, bu"
W17-4701,S10-1080,0,0.0346246,"Missing"
W17-4701,P08-1087,0,0.0218452,"on such as sense definitions and examples, which should be useful for disambiguating senses, but were not used in the above studies. Here, we aim to exploit this information to perform word sense induction from large scale monolingual data (in a first phase), thus combining the benefits of semantic ontologies and word sense induction for WSD. Several other studies integrated additional information from a larger context using factored-based MT models (Koehn and Hoang, 2007). Birch et al. (2007) used supertags from a Combinatorial Categorial Grammar as factors in phrase-based translation model. Avramidis and Koehn (2008) added source-side syntactic information for each word for translating from a morphologically poorer language to a richer one (English-Greek). The levels of improvement achieved with factored models such as the ones above range from 0.15 to 0.50 BLEU points. Here, we also observe improvements in the upper part of this range, and they are consistent across several language pairs. 3 3.1 Representing Definitions, Examples and Contexts of Word Occurrences For each noun or verb type Wt appearing in the training data, as identified by the Stanford POS tagger,2 we extract the senses associated to it"
W17-4701,W07-0702,0,0.0147693,"integrated them as features to a SMT model. However, apart from the sense graph, WordNet provides also textual information such as sense definitions and examples, which should be useful for disambiguating senses, but were not used in the above studies. Here, we aim to exploit this information to perform word sense induction from large scale monolingual data (in a first phase), thus combining the benefits of semantic ontologies and word sense induction for WSD. Several other studies integrated additional information from a larger context using factored-based MT models (Koehn and Hoang, 2007). Birch et al. (2007) used supertags from a Combinatorial Categorial Grammar as factors in phrase-based translation model. Avramidis and Koehn (2008) added source-side syntactic information for each word for translating from a morphologically poorer language to a richer one (English-Greek). The levels of improvement achieved with factored models such as the ones above range from 0.15 to 0.50 BLEU points. Here, we also observe improvements in the upper part of this range, and they are consistent across several language pairs. 3 3.1 Representing Definitions, Examples and Contexts of Word Occurrences For each noun or"
W17-4701,S10-1078,0,0.541242,"Missing"
W17-4701,2005.mtsummit-papers.11,0,0.0365423,"for a given source sentence. To each source noun or verb token, we add a sense label obtained from our adaptive WSD system. To all the other words, we add a NULL label.6 The translation system will thus take the source-side sense labels into consideration during the training and the decoding processes. 4 • we use the definition of each sense for initializing the centroids in the adaptive k-means methods (and compare this later with using the examples); Datasets, Preparation and Settings We evaluate our sense-aware SMT on the UN Corpus7 (Rafalovitch and Dale, 2009) and on the Europarl Corpus8 (Koehn, 2005). We select 0.5 million parallel sentences for each language pair from Europarl, as shown in Table 1. We also use the smaller WIT3 Corpus9 (Cettolo et al., 2012), a collection of transcripts of TED talks, to evaluate the impact of costly model choices, namely the type of the resource (definition vs. examples), the length of the context window, and the k-means method (adaptive vs. original). Before assigning sense labels, we first tokenize all the texts and identify the parts of speech (POS) using the Stanford POS tagger10 . Then, we filter out the stopwords and the nouns which are proper names"
W17-4701,P05-1048,0,0.300874,"rman, French, Spanish or Dutch, in comparison to an SMT baseline that is not aware of word senses. With respect to previous work that used WSD for MT, discussed in Section 2, we innovate on the following points: Word sense disambiguation aims to identify the sense of a word appearing in a given context (Agirre and Edmonds, 2007). Resolving word sense ambiguities should be useful, in particular, for lexical choice in MT. An initial investigation found that an SMT system which makes use of off-the-shelf WSD does not yield significantly better quality translations than a SMT system not using it (Carpuat and Wu, 2005). However, another study (Vickrey et al., 2005) reformulated the task of WSD for SMT as predicting possible target translations rather than senses of ambiguous source words, and showed that WSD improved such a simplified word translation task. Subsequent studies which adopted this formulation (Cabezas and Resnik, 2005; Chan et al., 2007; Carpuat and Wu, 2007), successfully integrated WSD to hierarchical or phrase-based SMT. These systems yielded slightly better translations compared to SMT baselines in most cases (0.15–0.30 BLEU). Although the WSD reformulation above proved helpful for SMT, it"
W17-4701,D07-1091,0,0.363399,"et word dependencies and integrated them as features to a SMT model. However, apart from the sense graph, WordNet provides also textual information such as sense definitions and examples, which should be useful for disambiguating senses, but were not used in the above studies. Here, we aim to exploit this information to perform word sense induction from large scale monolingual data (in a first phase), thus combining the benefits of semantic ontologies and word sense induction for WSD. Several other studies integrated additional information from a larger context using factored-based MT models (Koehn and Hoang, 2007). Birch et al. (2007) used supertags from a Combinatorial Categorial Grammar as factors in phrase-based translation model. Avramidis and Koehn (2008) added source-side syntactic information for each word for translating from a morphologically poorer language to a richer one (English-Greek). The levels of improvement achieved with factored models such as the ones above range from 0.15 to 0.50 BLEU points. Here, we also observe improvements in the upper part of this range, and they are consistent across several language pairs. 3 3.1 Representing Definitions, Examples and Contexts of Word Occurre"
W17-4701,D07-1007,0,0.399862,"ies should be useful, in particular, for lexical choice in MT. An initial investigation found that an SMT system which makes use of off-the-shelf WSD does not yield significantly better quality translations than a SMT system not using it (Carpuat and Wu, 2005). However, another study (Vickrey et al., 2005) reformulated the task of WSD for SMT as predicting possible target translations rather than senses of ambiguous source words, and showed that WSD improved such a simplified word translation task. Subsequent studies which adopted this formulation (Cabezas and Resnik, 2005; Chan et al., 2007; Carpuat and Wu, 2007), successfully integrated WSD to hierarchical or phrase-based SMT. These systems yielded slightly better translations compared to SMT baselines in most cases (0.15–0.30 BLEU). Although the WSD reformulation above proved helpful for SMT, it did not determine whether actual source-side senses are helpful or not for endto-end SMT. Xiong and Zhang (2014) attempted to answer this question by performing word sense induction for large scale data. In particular, they proposed a topic model that automatically learned sense clusters for words in the source language. In this way, on the one hand, they av"
W17-4701,H05-1097,0,0.188447,"o an SMT baseline that is not aware of word senses. With respect to previous work that used WSD for MT, discussed in Section 2, we innovate on the following points: Word sense disambiguation aims to identify the sense of a word appearing in a given context (Agirre and Edmonds, 2007). Resolving word sense ambiguities should be useful, in particular, for lexical choice in MT. An initial investigation found that an SMT system which makes use of off-the-shelf WSD does not yield significantly better quality translations than a SMT system not using it (Carpuat and Wu, 2005). However, another study (Vickrey et al., 2005) reformulated the task of WSD for SMT as predicting possible target translations rather than senses of ambiguous source words, and showed that WSD improved such a simplified word translation task. Subsequent studies which adopted this formulation (Cabezas and Resnik, 2005; Chan et al., 2007; Carpuat and Wu, 2007), successfully integrated WSD to hierarchical or phrase-based SMT. These systems yielded slightly better translations compared to SMT baselines in most cases (0.15–0.30 BLEU). Although the WSD reformulation above proved helpful for SMT, it did not determine whether actual source-side s"
W17-4701,P14-1137,0,0.264714,"dicting possible target translations rather than senses of ambiguous source words, and showed that WSD improved such a simplified word translation task. Subsequent studies which adopted this formulation (Cabezas and Resnik, 2005; Chan et al., 2007; Carpuat and Wu, 2007), successfully integrated WSD to hierarchical or phrase-based SMT. These systems yielded slightly better translations compared to SMT baselines in most cases (0.15–0.30 BLEU). Although the WSD reformulation above proved helpful for SMT, it did not determine whether actual source-side senses are helpful or not for endto-end SMT. Xiong and Zhang (2014) attempted to answer this question by performing word sense induction for large scale data. In particular, they proposed a topic model that automatically learned sense clusters for words in the source language. In this way, on the one hand, they avoided using a pre-specified inventory of word senses as traditional WSD does, but on the other hand, they created the risk of discovering sense clusters which do not correspond to the common senses of words needed for MT. Hence, this study left open an important question, namely whether WSD based on • we design a sense clustering method with explicit"
W17-4701,S10-1011,0,0.21705,"per word to the ones observed in the training data rather than constraining them by the full list of senses from WordNet; • we use the abstract sense labels for each analyzed word as factors in an SMT system. 2 of each occurrence with a cluster number that will be used as a factor in statistical MT. Our method answers several limitations of previous supervised or unsupervised WSD methods. Supervised methods require data with manually sense-annotated labels and are therefore often limited to a small number of word types: for instance, only 50 nouns and 50 verbs were targeted in SemEval 20101 (Manandhar et al., 2010). On the contrary, our method does not require labeled texts for training, and applies to all word types appearing with multiple senses in WordNet. Unsupervised methods often pre-define the number of possible senses for each ambiguous word before clustering the various occurrences according to the senses. If these numbers come from WordNet, the senses may be too fine-grained for the needs of translation, especially when a specific domain is targeted. In contrast, as we explain below, our WSD method initializes a contextdependent clustering algorithm with information from WordNet senses for eac"
W17-4701,P02-1040,0,0.101194,"rence of a noun or verb which has more than one sense in WordNet. For translation, we train and tune baseline and factored phrase-based models with Moses12 (Koehn et al., 2007). We also carried out pilot experiments with neural machine translation (NMT). However, due to the large datasets NMT requires for training, its performance was below SMT on the datasets above, and sense labels did not improve it. We thus focus on SMT in what follows, and leave WSD for NMT for future studies. We select the optimal model configuration based on the MT performance, measured with the traditional BLEU score (Papineni et al., 2002), on the WIT3 corpus for EN/ZH and EN/DE. Unless otherwise stated, we use the following settings in the k-means algorithm, starting from the implementation provided in Scikit-learn (Pedregosa et al., 2011): Integration with Machine Translation Our adaptive WSD system assigns a sense number for each ambiguous word token in the source-side of a parallel corpus. To pass this information to an SMT system, we use a factored phrase-based translation model (Koehn and Hoang, 2007). The factored model offers a principled way to supplement words with additional information – such as, traditionally, part"
W17-4701,S10-1081,0,0.546955,"Missing"
W17-4701,2009.mtsummit-posters.15,0,0.278787,"e goal remains to find the most probable target sentence for a given source sentence. To each source noun or verb token, we add a sense label obtained from our adaptive WSD system. To all the other words, we add a NULL label.6 The translation system will thus take the source-side sense labels into consideration during the training and the decoding processes. 4 • we use the definition of each sense for initializing the centroids in the adaptive k-means methods (and compare this later with using the examples); Datasets, Preparation and Settings We evaluate our sense-aware SMT on the UN Corpus7 (Rafalovitch and Dale, 2009) and on the Europarl Corpus8 (Koehn, 2005). We select 0.5 million parallel sentences for each language pair from Europarl, as shown in Table 1. We also use the smaller WIT3 Corpus9 (Cettolo et al., 2012), a collection of transcripts of TED talks, to evaluate the impact of costly model choices, namely the type of the resource (definition vs. examples), the length of the context window, and the k-means method (adaptive vs. original). Before assigning sense labels, we first tokenize all the texts and identify the parts of speech (POS) using the Stanford POS tagger10 . Then, we filter out the stop"
W17-4701,P07-2045,0,\N,Missing
W17-4701,D15-1145,0,\N,Missing
W17-4802,W14-3348,0,0.0409188,"ticular pair (t, r). Finally, we selected all pairs where p(c = 1|t, r) > 0.5, which indicates that the two pronouns are more likely to be correct translation alternatives than not. The final lists found for French are shown in Table 2. Two examples of pronoun equivalence in English/French translation are: “it is difficult . . .” translated to “il / c’ est difficile . . .”, and “it would be nice . . .” to “ce / c¸a serait beau . . .”. 3 Identical 3.2 Other Metrics for Comparison We compare the results of APT with two wellknown automatic metrics for MT: BLEU (Papineni et al., 2002) and METEOR (Denkowski and Lavie, 2014). Additionally, we include the METEOR score restricted to the French pronouns present in the manual annotation. For this purpose, we set the function words list of METEOR to the list of French pronouns defined in DiscoMT (listed above), and its δ parameter to 0 to give preference to the evaluation of the function words (in our case, pronouns). Additionally, we include the AutoP, AutoR and AutoF metrics proposed by Hardmeier and Federico (2010) for automatic evaluation of pronoun translation. These metrics were inspired by BLEU score. First, they extracts a list C of all words aligned to the so"
W17-4802,W16-2345,1,0.838553,"n. 1 Introduction The machine translation of pronouns has long been known as a challenge, especially for pro-drop languages. The correct translation of pronouns often requires non-local information, which is one of the reasons it is quite challenging for statistical or neural MT systems. Still, the problem has attracted new interest in recent years (Hardmeier, 2014; Guillou, 2016), in particular through the organization of three shared tasks: at the EMNLP DiscoMT 2015 and 2017 workshops (Hardmeier et al., 2015; Lo´aiciga et al., 2017), and at the First Conference on Machine Translation (WMT) (Guillou et al., 2016). 17 Proceedings of the Third Workshop on Discourse in Machine Translation, pages 17–25, c Copenhagen, Denmark, September 8, 2017. 2017 Association for Computational Linguistics. counts are considered – the most straightforward one gives credit for identical matches and discards all other ones. As we will show, the APT scores correlate strongly with the human scores on the data from the DiscoMT 2015 shared task on pronounfocused translation (0.993–0.999 Pearson and 1.000 Spearman rank correlation). This is considerably higher than general purpose automatic metrics such as BLEU and METEOR, and"
W17-4802,2010.iwslt-papers.10,0,0.841062,"ntical 3.2 Other Metrics for Comparison We compare the results of APT with two wellknown automatic metrics for MT: BLEU (Papineni et al., 2002) and METEOR (Denkowski and Lavie, 2014). Additionally, we include the METEOR score restricted to the French pronouns present in the manual annotation. For this purpose, we set the function words list of METEOR to the list of French pronouns defined in DiscoMT (listed above), and its δ parameter to 0 to give preference to the evaluation of the function words (in our case, pronouns). Additionally, we include the AutoP, AutoR and AutoF metrics proposed by Hardmeier and Federico (2010) for automatic evaluation of pronoun translation. These metrics were inspired by BLEU score. First, they extracts a list C of all words aligned to the source pronouns from the candidate Experimental Settings DiscoMT Data Set and Metrics The data set we use for our experiments was generated during the shared task on pronoun-focused 20 Meteor Meteor o/Pronouns Pearson: 0.977 Spearman: 0.821 0.1 t$h_other Pearson: 0.98 Spearman: 0.821 t$h_other Pearson: 0.976 Spearman: 0.964 0.3 0.5 Bleu 0.15 0.25 0.35 0.30 0.35 0.40 0.50 0.55 0.20 AutoR t$meteor t$h_other Pearson: 0.972 Spearman: 0.821 0.25 0.30"
W17-4802,W15-2501,0,0.553645,"BLEU, METEOR, or those specific to pronouns used at DiscoMT 2015 reach only 0.972–0.986 Pearson correlation. 1 Introduction The machine translation of pronouns has long been known as a challenge, especially for pro-drop languages. The correct translation of pronouns often requires non-local information, which is one of the reasons it is quite challenging for statistical or neural MT systems. Still, the problem has attracted new interest in recent years (Hardmeier, 2014; Guillou, 2016), in particular through the organization of three shared tasks: at the EMNLP DiscoMT 2015 and 2017 workshops (Hardmeier et al., 2015; Lo´aiciga et al., 2017), and at the First Conference on Machine Translation (WMT) (Guillou et al., 2016). 17 Proceedings of the Third Workshop on Discourse in Machine Translation, pages 17–25, c Copenhagen, Denmark, September 8, 2017. 2017 Association for Computational Linguistics. counts are considered – the most straightforward one gives credit for identical matches and discards all other ones. As we will show, the APT scores correlate strongly with the human scores on the data from the DiscoMT 2015 shared task on pronounfocused translation (0.993–0.999 Pearson and 1.000 Spearman rank corr"
W17-4802,P07-2045,0,0.0155028,"nguage texts. In the case of the candidate translation, the alignment can be directly obtained from the MT system if such an option is available. However, in the case of the reference, it is necessary to perform automatic word alignment. We use here the GIZA++ system (Och and Ney, 2003), including the sentences to be scored in a larger corpus to ensure an acceptable accuracy, since GIZA++ has no separate training vs. testing stages. The alignment is made both in direct (source-target) and reverse (target-source) directions, which are then merged using the grow-diag-final heuristic from Moses (Koehn et al., 2007). Accurate pronoun alignment is essential to APT. To estimate its accuracy, we manually evaluated 100 randomly selected sentences from the WIT3 parallel corpus of English-French TED Talks (Cettolo et al., 2012), containing the pronouns it and they. We found that the alignments of 19 out of 100 pronoun were missing, and that 4 pronouns were incorrectly aligned. As expected, the majority of misalignments involved infrequently-used target pronouns. We defined several pronoun-specific heuristics to improve the alignment. Our four-step procedure is exemplified in Table 1 below, where the alignment"
W17-4802,W17-4801,0,0.151596,"Missing"
W17-4802,E17-2100,1,0.900272,"Missing"
W17-4802,W17-1505,1,0.897963,"Missing"
W17-4802,J03-1002,0,0.00992275,"ould consider the possibility of identical pronouns with different forms. For example, French has pronoun contractions such as c’ for ce, in the expletive construction c’est (meaning it is). 2.3 Given the list of source pronouns considered for evaluation, the first step is to obtain their corresponding alignments in the target language texts. In the case of the candidate translation, the alignment can be directly obtained from the MT system if such an option is available. However, in the case of the reference, it is necessary to perform automatic word alignment. We use here the GIZA++ system (Och and Ney, 2003), including the sentences to be scored in a larger corpus to ensure an acceptable accuracy, since GIZA++ has no separate training vs. testing stages. The alignment is made both in direct (source-target) and reverse (target-source) directions, which are then merged using the grow-diag-final heuristic from Moses (Koehn et al., 2007). Accurate pronoun alignment is essential to APT. To estimate its accuracy, we manually evaluated 100 randomly selected sentences from the WIT3 parallel corpus of English-French TED Talks (Cettolo et al., 2012), containing the pronouns it and they. We found that the a"
W17-4802,P02-1040,0,0.10744,"mber of correct samples given a particular pair (t, r). Finally, we selected all pairs where p(c = 1|t, r) > 0.5, which indicates that the two pronouns are more likely to be correct translation alternatives than not. The final lists found for French are shown in Table 2. Two examples of pronoun equivalence in English/French translation are: “it is difficult . . .” translated to “il / c’ est difficile . . .”, and “it would be nice . . .” to “ce / c¸a serait beau . . .”. 3 Identical 3.2 Other Metrics for Comparison We compare the results of APT with two wellknown automatic metrics for MT: BLEU (Papineni et al., 2002) and METEOR (Denkowski and Lavie, 2014). Additionally, we include the METEOR score restricted to the French pronouns present in the manual annotation. For this purpose, we set the function words list of METEOR to the list of French pronouns defined in DiscoMT (listed above), and its δ parameter to 0 to give preference to the evaluation of the function words (in our case, pronouns). Additionally, we include the AutoP, AutoR and AutoF metrics proposed by Hardmeier and Federico (2010) for automatic evaluation of pronoun translation. These metrics were inspired by BLEU score. First, they extracts"
W17-4802,E17-2104,0,0.0802537,"Missing"
W17-4802,W15-3031,0,0.06654,"Missing"
W17-4802,2012.eamt-1.60,0,\N,Missing
W97-1314,C90-3063,0,0.224724,"Missing"
W97-1314,J95-1003,0,0.0655631,"Missing"
W97-1314,C96-1021,0,0.0780308,"Missing"
W97-1314,J94-4002,0,0.606695,"Missing"
W97-1314,A88-1003,0,0.313654,"Missing"
W97-1314,J81-2001,0,\N,Missing
