2020.acl-main.269,D15-1075,0,0.0207773,"h the probability 1 − π. = sigmoid((Es + G0 − G00 )/τ ) NLP Benchmarks (5) where G0 and G00 are two independent Gumbel noises (Gumbel, 1954), and τ ∈ (0, ∞) is a temperature parameter. As τ diminishes to zero, a sample from the Gumbel-Sigmoid distribution becomes cold and resembles the one-hot samples. At training time, we can use Gumbel-Sigmoid to obtain Experimental Setup Natural Language Inference aims to classify semantic relationship between a pair of sentences, i.e., a premise and corresponding hypothesis. We conduct experiments on the Stanford Natural Language Inference (SNLI) dataset (Bowman et al., 2015), which has three classes: Entailment, Contradiction and Neutral. We followed Shen et al. (2018b) to use a token2token SAN layer followed by a source2token SAN layer to generate a compressed vector representation of input sentence. The selector is integrated into the token2token SAN layer. Taking the premise representation sp and the hypothesis vector sh as input, their semantic relationship is represented by the concatenation of sp , sh , sp −sh and sp · sh , which is passed to a classification module to generate a categorical distribution over the three classes. We initialize the word embedd"
2020.acl-main.269,P18-1008,0,0.115114,"word order encoding (Yang et al., 2019a) and syntactic structure modeling (Tang et al., 2018). In this work, we concentrate on these two commonly-cited issues. Word Order Encoding SANs merely rely on attention mechanism with neither recurrence nor convolution structures. In order to incorporate sequence order information, Vaswani et al. (2017) proposed to inject position information into the input word embedding with additional position embedding. Nevertheless, SANs are still weak at learning word order information (Yang et al., 2019a). Recent studies have shown that incorporating recurrence (Chen et al., 2018; Hao et al., 2019b,c), convolution (Song et al., 2018; Yang et al., 2019b), or advanced position encoding (Shaw et al., 2018; Wang et al., 2019a) into vanilla SANs can further boost their performance, confirming its shortcomings at modeling sequence order. Structure Modeling Due to lack of supervision signals of learning structural information, recent studies pay widespread attention on incorporating syntactic structure into SANs. For instance, Strubell et al. (2018) utilized one attention head to learn to attend to syntactic parents of each word. Towards generating better sentence representa"
2020.acl-main.269,P18-1198,0,0.0224993,"Missing"
2020.acl-main.269,N19-1423,0,0.0125418,"word order encoding and structure modeling. Specifically, the selective mechanism improves SANs by paying more attention to content words that contribute to the meaning of the sentence. The code and data are released at https://github.com/xwgeng/SSAN. 1 Introduction Self-attention networks (SANs) (Lin et al., 2017) have achieved promising progress in various natural language processing (NLP) tasks, including machine translation (Vaswani et al., 2017), natural language inference (Shen et al., 2018b), semantic role labeling (Tan et al., 2018; Strubell et al., 2018) and language representation (Devlin et al., 2019). The appealing strength of SANs derives from high parallelism as well as flexibility in modeling dependencies among all the input elements. Recently, there has been a growing interest in integrating selective mechanism into SANs, which has produced substantial improvements in a variety ∗ Work done when interning at Tencent AI Lab. of NLP tasks. For example, some researchers incorporated a hard constraint into SANs to select a subset of input words, on top of which self-attention is conducted (Shen et al., 2018c; Hou et al., 2019; Yang et al., 2019b). Yang et al. (2018) and Guo et al. (2019) p"
2020.acl-main.269,C16-1276,1,0.893209,"Missing"
2020.acl-main.269,D19-1082,1,0.814363,"Missing"
2020.acl-main.269,D19-1135,1,0.882131,"urther refinement. In this study, we bridge this gap by assessing the strengths of selective mechanism on capturing essentially linguistic properties via well-designed experiments. The starting point for our approach is recent findings: the standard SANs suffer from two representation limitation on modeling word order encoding (Shaw et al., 2018; Yang et al., 2019a) 2986 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 2986–2995 c July 5 - 10, 2020. 2020 Association for Computational Linguistics and syntactic structure modeling (Tang et al., 2018; Hao et al., 2019a), which are essential for natural language understanding and generation. Experimental results on targeted linguistic evaluation lead to the following observations: • SSANs can identify the improper word orders in both local (§4.1) and global (§4.2) ranges by learning to attend to the expected words. • SSANs produce more syntactic representations (§5.1) with a better modeling of structure by selective attention (§5.2). • The selective mechanism improves SANs by paying more attention to content words that posses semantic content and contribute to the meaning of the sentence (§5.3). 2 2.1 Metho"
2020.acl-main.269,N19-1122,1,0.877134,"Missing"
2020.acl-main.269,D19-1088,1,0.908786,"Missing"
2020.acl-main.269,D18-1317,1,0.881654,"are pretrained on Wikipedia and Gigaword, to initialize our networks, but they are not fixed during training. We choose the better feed-forward networks (FFN) variants of DEEPATT as our standard settings. Machine Translation is a conditional generation task, which aims to translate a sentence from a source language to its counterpart in a target language. We carry out experiments on several widelyused datasets, including small English⇒Japanese (En⇒Ja) and English⇒Romanian (En⇒Ro) corpora, as well as a relatively large English⇒German (En⇒De) corpus. For En⇒De and En⇒Ro, we respectively follow Li et al. (2018) and He et al. (2018) to prepare WMT20142 and IWSLT20143 corpora. For En⇒Ja, we use KFTT4 dataset provided by Neubig (2011). All the data are tokenized and then segmented into subword symbols using BPE (Sennrich et al., 2016) with 32K operations. We implemented the approach on top of advanced T RANSFORMER model (Vaswani et al., 2017). On the large-scale En⇒De dataset, we followed the base configurations to train the NMT model, which consists of 6 stacked encoder and decoder layers with the layer size being 512 and the number of attention heads being 8. On the small-scale En⇒Ro and En⇒Ja datase"
2020.acl-main.269,N19-1359,1,0.870621,"Missing"
2020.acl-main.269,D15-1166,0,0.0438377,"uistic evaluation lead to the following observations: • SSANs can identify the improper word orders in both local (§4.1) and global (§4.2) ranges by learning to attend to the expected words. • SSANs produce more syntactic representations (§5.1) with a better modeling of structure by selective attention (§5.2). • The selective mechanism improves SANs by paying more attention to content words that posses semantic content and contribute to the meaning of the sentence (§5.3). 2 2.1 Methodology Self-Attention Networks SANs (Lin et al., 2017), as a variant of attention model (Bahdanau et al., 2015; Luong et al., 2015), compute attention weights between each pair of elements in a single sequence. Given the input layer H = {h1 , · · · , hN } ∈ N ×d , SANs first transform the layer H into the queries Q ∈ N ×d , the keys K ∈ N ×d , and the values V ∈ N ×d with three separate weight matrices. The output layer O is calculated as: R R O = ATT(Q, K)V R R (1) where the alternatives to ATT(·) can be additive attention (Bahdanau et al., 2015) or dot-product attention (Luong et al., 2015). Due to time and space efficiency, we used the dot-product attention in this study, which is computed as: QKT ATT(Q, K) = sof tmax("
2020.acl-main.269,D18-1458,0,0.0466347,"Missing"
2020.acl-main.269,W18-5444,0,0.0310899,"Missing"
2020.acl-main.269,D14-1162,0,0.0828181,"Missing"
2020.acl-main.269,D19-1145,1,0.865883,"Missing"
2020.acl-main.269,N18-1202,0,0.0976303,"Missing"
2020.acl-main.269,D19-1098,0,0.0165419,"y-cited issues. Word Order Encoding SANs merely rely on attention mechanism with neither recurrence nor convolution structures. In order to incorporate sequence order information, Vaswani et al. (2017) proposed to inject position information into the input word embedding with additional position embedding. Nevertheless, SANs are still weak at learning word order information (Yang et al., 2019a). Recent studies have shown that incorporating recurrence (Chen et al., 2018; Hao et al., 2019b,c), convolution (Song et al., 2018; Yang et al., 2019b), or advanced position encoding (Shaw et al., 2018; Wang et al., 2019a) into vanilla SANs can further boost their performance, confirming its shortcomings at modeling sequence order. Structure Modeling Due to lack of supervision signals of learning structural information, recent studies pay widespread attention on incorporating syntactic structure into SANs. For instance, Strubell et al. (2018) utilized one attention head to learn to attend to syntactic parents of each word. Towards generating better sentence representations, several researchers propose phrase-level SANs by performing self-attention across words inside a ngram phrase or syntactic constituent (W"
2020.acl-main.269,P16-1162,0,0.016109,"lation is a conditional generation task, which aims to translate a sentence from a source language to its counterpart in a target language. We carry out experiments on several widelyused datasets, including small English⇒Japanese (En⇒Ja) and English⇒Romanian (En⇒Ro) corpora, as well as a relatively large English⇒German (En⇒De) corpus. For En⇒De and En⇒Ro, we respectively follow Li et al. (2018) and He et al. (2018) to prepare WMT20142 and IWSLT20143 corpora. For En⇒Ja, we use KFTT4 dataset provided by Neubig (2011). All the data are tokenized and then segmented into subword symbols using BPE (Sennrich et al., 2016) with 32K operations. We implemented the approach on top of advanced T RANSFORMER model (Vaswani et al., 2017). On the large-scale En⇒De dataset, we followed the base configurations to train the NMT model, which consists of 6 stacked encoder and decoder layers with the layer size being 512 and the number of attention heads being 8. On the small-scale En⇒Ro and En⇒Ja datasets, we followed He et al. (2018) to decrease the layer size to 256 and the number of attention heads to 4. For all the tasks, we applied the selector to the first layer of encoder to better capture lexical and syntactic infor"
2020.acl-main.269,N18-2074,0,0.0355278,"ion (i.e., sequence generation), demonstrate that SSANs consistently outperform the standard SANs (§3). Despite demonstrating the effectiveness of SSANs, the underlying reasons for their strong performance have not been well explained, which poses great challenges for further refinement. In this study, we bridge this gap by assessing the strengths of selective mechanism on capturing essentially linguistic properties via well-designed experiments. The starting point for our approach is recent findings: the standard SANs suffer from two representation limitation on modeling word order encoding (Shaw et al., 2018; Yang et al., 2019a) 2986 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 2986–2995 c July 5 - 10, 2020. 2020 Association for Computational Linguistics and syntactic structure modeling (Tang et al., 2018; Hao et al., 2019a), which are essential for natural language understanding and generation. Experimental results on targeted linguistic evaluation lead to the following observations: • SSANs can identify the improper word orders in both local (§4.1) and global (§4.2) ranges by learning to attend to the expected words. • SSANs produce more syntact"
2020.acl-main.269,D18-1408,0,0.0225505,"9a) into vanilla SANs can further boost their performance, confirming its shortcomings at modeling sequence order. Structure Modeling Due to lack of supervision signals of learning structural information, recent studies pay widespread attention on incorporating syntactic structure into SANs. For instance, Strubell et al. (2018) utilized one attention head to learn to attend to syntactic parents of each word. Towards generating better sentence representations, several researchers propose phrase-level SANs by performing self-attention across words inside a ngram phrase or syntactic constituent (Wu et al., 2018; Hao et al., 2019a; Wang et al., 2019b). These studies show that the introduction of syntactic information can achieve further improvement over SANs, demonstrating its potential weakness on structure modeling. 2.3 Selective Self-Attention Networks In this study, we implement the selective mechanism on SANs by introducing an additional selector, namely SSANs, as illustrated in Figure 1. The selector aims to select a subset of elements from the input sequence, on top of which the standard self-attention (Equation 1) is conducted. We implement the selector with Gumbel-Softmax, which has proven e"
2020.acl-main.269,D18-1475,1,0.809343,"guage representation (Devlin et al., 2019). The appealing strength of SANs derives from high parallelism as well as flexibility in modeling dependencies among all the input elements. Recently, there has been a growing interest in integrating selective mechanism into SANs, which has produced substantial improvements in a variety ∗ Work done when interning at Tencent AI Lab. of NLP tasks. For example, some researchers incorporated a hard constraint into SANs to select a subset of input words, on top of which self-attention is conducted (Shen et al., 2018c; Hou et al., 2019; Yang et al., 2019b). Yang et al. (2018) and Guo et al. (2019) proposed a soft mechanism by imposing a learned Gaussian bias over the original attention distribution to enhance its ability of capturing local contexts. Shen et al. (2018c) incorporated reinforced sampling to dynamically choose a subset of input elements, which are fed to SANs. Although the general idea of selective mechanism works well across NLP tasks, previous studies only validate their own implementations in a few tasks, either on only classification tasks (Shen et al., 2018c; Guo et al., 2019) or sequence generation tasks (Yang et al., 2018, 2019b). This poses a"
2020.acl-main.269,P19-1354,1,0.897977,"et al., 2018) and language representation (Devlin et al., 2019). The appealing strength of SANs derives from high parallelism as well as flexibility in modeling dependencies among all the input elements. Recently, there has been a growing interest in integrating selective mechanism into SANs, which has produced substantial improvements in a variety ∗ Work done when interning at Tencent AI Lab. of NLP tasks. For example, some researchers incorporated a hard constraint into SANs to select a subset of input words, on top of which self-attention is conducted (Shen et al., 2018c; Hou et al., 2019; Yang et al., 2019b). Yang et al. (2018) and Guo et al. (2019) proposed a soft mechanism by imposing a learned Gaussian bias over the original attention distribution to enhance its ability of capturing local contexts. Shen et al. (2018c) incorporated reinforced sampling to dynamically choose a subset of input elements, which are fed to SANs. Although the general idea of selective mechanism works well across NLP tasks, previous studies only validate their own implementations in a few tasks, either on only classification tasks (Shen et al., 2018c; Guo et al., 2019) or sequence generation tasks (Yang et al., 2018,"
2020.acl-main.269,N19-1407,1,0.818121,"et al., 2018) and language representation (Devlin et al., 2019). The appealing strength of SANs derives from high parallelism as well as flexibility in modeling dependencies among all the input elements. Recently, there has been a growing interest in integrating selective mechanism into SANs, which has produced substantial improvements in a variety ∗ Work done when interning at Tencent AI Lab. of NLP tasks. For example, some researchers incorporated a hard constraint into SANs to select a subset of input words, on top of which self-attention is conducted (Shen et al., 2018c; Hou et al., 2019; Yang et al., 2019b). Yang et al. (2018) and Guo et al. (2019) proposed a soft mechanism by imposing a learned Gaussian bias over the original attention distribution to enhance its ability of capturing local contexts. Shen et al. (2018c) incorporated reinforced sampling to dynamically choose a subset of input elements, which are fed to SANs. Although the general idea of selective mechanism works well across NLP tasks, previous studies only validate their own implementations in a few tasks, either on only classification tasks (Shen et al., 2018c; Guo et al., 2019) or sequence generation tasks (Yang et al., 2018,"
2020.acl-main.269,C18-1259,0,0.0489127,"Missing"
2020.acl-main.269,D18-1548,0,0.12373,"to mitigating two commonly-cited weaknesses of SANs: word order encoding and structure modeling. Specifically, the selective mechanism improves SANs by paying more attention to content words that contribute to the meaning of the sentence. The code and data are released at https://github.com/xwgeng/SSAN. 1 Introduction Self-attention networks (SANs) (Lin et al., 2017) have achieved promising progress in various natural language processing (NLP) tasks, including machine translation (Vaswani et al., 2017), natural language inference (Shen et al., 2018b), semantic role labeling (Tan et al., 2018; Strubell et al., 2018) and language representation (Devlin et al., 2019). The appealing strength of SANs derives from high parallelism as well as flexibility in modeling dependencies among all the input elements. Recently, there has been a growing interest in integrating selective mechanism into SANs, which has produced substantial improvements in a variety ∗ Work done when interning at Tencent AI Lab. of NLP tasks. For example, some researchers incorporated a hard constraint into SANs to select a subset of input words, on top of which self-attention is conducted (Shen et al., 2018c; Hou et al., 2019; Yang et al.,"
2020.acl-main.269,W13-3516,0,\N,Missing
2020.acl-main.269,D16-1159,0,\N,Missing
2020.coling-main.179,2020.acl-main.708,0,0.195787,"ure and text’s fidelity. In detail, the table structure reconstruction task is proposed for GPT-2 which force it to embed table structure into its representation when modeling structured table. Besides, we utilize content matching task that help model correctly describe important information from table via Optimal-Transport (Chen et al., 2019a) technique, which measures the distance between the information in generated text and information in table and use the distance as penalty for text with incorrect information. We conducted experiments on three data-to-text datasets on different domains (Chen et al., 2020b): Humans, Books, Songs in various settings. Both automatic evaluation and human evaluation results show that our model can achieve new state-of-the-art performance for table-to-text generation in terms of generating fluent and high-fidelity text in most few-shot settings. 2 2.1 Background Task Definition For the table-to-text task discussed in this paper, we can formulate each training instance as pair of table and summary E = (S, T ). Given a table, which can be formulated as sets of records S = {ri }N i=1 , the model is expected to generate descriptive text T = w1 , w2 , ..., wL . N is the"
2020.coling-main.179,2020.acl-main.18,0,0.372609,"ure and text’s fidelity. In detail, the table structure reconstruction task is proposed for GPT-2 which force it to embed table structure into its representation when modeling structured table. Besides, we utilize content matching task that help model correctly describe important information from table via Optimal-Transport (Chen et al., 2019a) technique, which measures the distance between the information in generated text and information in table and use the distance as penalty for text with incorrect information. We conducted experiments on three data-to-text datasets on different domains (Chen et al., 2020b): Humans, Books, Songs in various settings. Both automatic evaluation and human evaluation results show that our model can achieve new state-of-the-art performance for table-to-text generation in terms of generating fluent and high-fidelity text in most few-shot settings. 2 2.1 Background Task Definition For the table-to-text task discussed in this paper, we can formulate each training instance as pair of table and summary E = (S, T ). Given a table, which can be formulated as sets of records S = {ri }N i=1 , the model is expected to generate descriptive text T = w1 , w2 , ..., wL . N is the"
2020.coling-main.179,N19-1423,0,0.179043,"h as WIKIBIO (Lebret et al., 2016) and E2E (Duˇsek et al., 2020). However, it is not always feasible to collect large-scale labeled dataset for various domains in the real world, resulting in unsatisfying performance due to the insufficient training. Such few-shot learning setting for table-to-text generation is not well-explored, and in this paper, we focus on exploring how to efficiently model for few-shot table-to-text generation with limited training pairs. Recently, pre-trained language models have shown promising progress in various natural language processing tasks (Yang et al., 2019b; Devlin et al., 2019; Radford et al., 2019). They can capture linguistic knowledge by pretraining on large-scale unlabeled dataset and generalize to downstream tasks with little labeled data in target domain, effectively modeling for few-shot setting (Peng et al., 2020). However, efforts to benefit table-to-text generation from the powerful pre-trained language model, especially in few-shot setting, are non-trivial due to three challenges. (1) There is a gap between the structured data input for table-to-text generation and natural language input that is used for pretraining GPT-2. (2) Also, it lacks modeling of"
2020.coling-main.179,D19-1310,1,0.799494,"s , England .” and the expression of wrong birth date, which shows the imperfect switch policy on deciding when to copy from table can sometimes hurt model’s ability to generate high-fidelity text. On the contrary, TableGPT, enhanced to generate high-fidelity text with two auxiliary tasks without breaking one unified GPT-2 model for generating text, performs better in terms of fidelity and fluency of text in this example. 5 Related Work In recent years, neural models for generating texts directly from preprocessed data (Wiseman et al., 2017; Puduppully et al., 2019a; Puduppully et al., 2019b; Gong et al., 2019; Feng et al., 2020), have become mainstream for table-to-text generation and achieved impressive performance with the help of largescale dataset. Mei et al. (2016) proposes a pre-selector on encoder-aligner-decoder model for generation, which strengthens model’s content selection ability and obtains considerable improvement over standard Seq2Seq model. Sha et al. (2018) proposes a hybrid attention mechanism for modeling the order of content when generating texts. Liu et al. (2018) presents a field-gating encoder focusing on modeling table structure and dual attention mechanism to utilize the"
2020.coling-main.179,D16-1128,0,0.0498271,"le-to-text datasets in different domains, our model outperforms existing systems on most few-shot settings. 1 Introduction Table-to-text generation, aiming at generating descriptive text about important information in structured data, has well application prospect in communicating with human in a comprehensible and natural way, such as financial report (Murakami et al., 2017), medical report (Hasan and Farri, 2019) generation, etc. In recent years, data-driven models have shown impressive capability to produce informative and fluent text with the help of large-scale datasets, such as WIKIBIO (Lebret et al., 2016) and E2E (Duˇsek et al., 2020). However, it is not always feasible to collect large-scale labeled dataset for various domains in the real world, resulting in unsatisfying performance due to the insufficient training. Such few-shot learning setting for table-to-text generation is not well-explored, and in this paper, we focus on exploring how to efficiently model for few-shot table-to-text generation with limited training pairs. Recently, pre-trained language models have shown promising progress in various natural language processing tasks (Yang et al., 2019b; Devlin et al., 2019; Radford et al"
2020.coling-main.179,W04-1013,0,0.015591,"n this paper, we propose TableGPT that exploits GPT-2’s learnt knowledge from pretraining on vast corpus for few-shot learning while enhance it for generating high fidelity text with two auxiliary tasks. Also, we perform ablation studies for evaluating each auxiliary task’s contribution. -sr represents the variant without table structure reconstruction, -cm represents the variant without content matching and -sr&cm shows the performance of GPT-2 without auxilary tasks. 4.3 Automatic Evaluation Following the previous work Chen et al. (2020b), we adopt BLEU-4 (Papineni et al., 2002) and ROUGE4 (Lin, 2004) to conduct automatic evaluations. Table 2 and 3 show corresponding results of comparing methods on different datasets. Although the Base achieves competitive results when training on largescale dataset (Liu et al., 2018), the performance drops drastically in few-shot setting. While utilizing a switch policy to combine copying words from table and generating words from GPT-2 (Base + switch + LM) can achieve impressive performance in all few-shot setting, a standard GPT-2 model (TableGPT - sr&cm) that takes a serialized table as input and generate text afterwards without copying can actually pe"
2020.coling-main.179,N16-1086,0,0.0221814,"y to generate high-fidelity text. On the contrary, TableGPT, enhanced to generate high-fidelity text with two auxiliary tasks without breaking one unified GPT-2 model for generating text, performs better in terms of fidelity and fluency of text in this example. 5 Related Work In recent years, neural models for generating texts directly from preprocessed data (Wiseman et al., 2017; Puduppully et al., 2019a; Puduppully et al., 2019b; Gong et al., 2019; Feng et al., 2020), have become mainstream for table-to-text generation and achieved impressive performance with the help of largescale dataset. Mei et al. (2016) proposes a pre-selector on encoder-aligner-decoder model for generation, which strengthens model’s content selection ability and obtains considerable improvement over standard Seq2Seq model. Sha et al. (2018) proposes a hybrid attention mechanism for modeling the order of content when generating texts. Liu et al. (2018) presents a field-gating encoder focusing on modeling table structure and dual attention mechanism to utilize the structure information when decoding. In addition, Bao et al. (2018) develops a table-aware sequence-to-sequence model on this task. However, Chen et al. (2020b) sho"
2020.coling-main.179,P02-1040,0,0.108992,"ase + switch + LM(R). • TableGPT: In this paper, we propose TableGPT that exploits GPT-2’s learnt knowledge from pretraining on vast corpus for few-shot learning while enhance it for generating high fidelity text with two auxiliary tasks. Also, we perform ablation studies for evaluating each auxiliary task’s contribution. -sr represents the variant without table structure reconstruction, -cm represents the variant without content matching and -sr&cm shows the performance of GPT-2 without auxilary tasks. 4.3 Automatic Evaluation Following the previous work Chen et al. (2020b), we adopt BLEU-4 (Papineni et al., 2002) and ROUGE4 (Lin, 2004) to conduct automatic evaluations. Table 2 and 3 show corresponding results of comparing methods on different datasets. Although the Base achieves competitive results when training on largescale dataset (Liu et al., 2018), the performance drops drastically in few-shot setting. While utilizing a switch policy to combine copying words from table and generating words from GPT-2 (Base + switch + LM) can achieve impressive performance in all few-shot setting, a standard GPT-2 model (TableGPT - sr&cm) that takes a serialized table as input and generate text afterwards without"
2020.coling-main.179,2020.findings-emnlp.17,0,0.0500674,"Missing"
2020.coling-main.179,P19-1195,0,0.0841739,"he inconsistent expression “played for , among others , England .” and the expression of wrong birth date, which shows the imperfect switch policy on deciding when to copy from table can sometimes hurt model’s ability to generate high-fidelity text. On the contrary, TableGPT, enhanced to generate high-fidelity text with two auxiliary tasks without breaking one unified GPT-2 model for generating text, performs better in terms of fidelity and fluency of text in this example. 5 Related Work In recent years, neural models for generating texts directly from preprocessed data (Wiseman et al., 2017; Puduppully et al., 2019a; Puduppully et al., 2019b; Gong et al., 2019; Feng et al., 2020), have become mainstream for table-to-text generation and achieved impressive performance with the help of largescale dataset. Mei et al. (2016) proposes a pre-selector on encoder-aligner-decoder model for generation, which strengthens model’s content selection ability and obtains considerable improvement over standard Seq2Seq model. Sha et al. (2018) proposes a hybrid attention mechanism for modeling the order of content when generating texts. Liu et al. (2018) presents a field-gating encoder focusing on modeling table structur"
2020.coling-main.179,2020.acl-main.101,0,0.0701278,"able. In order to overcome this 2 2 problem, we use the recently proposed Inexact Proximal point method for Optimal Transport (IPOT) (Chen et al., 2019a) as an approximation. For natural language generation tasks such as neural machine translation, OT distance is often applied by matching source sequence with whole target sequence, since almost every word in both sequences are supposed to be matched. However, when it comes to table-to-text task in a realistic way, there are some redundant information or words in both table and text. In order to apply the OT distance, unlike previous adoption (Wang et al., 2020) based on the assumption that all information in the table should be described in text, we propose to only match the record words which appear in both table and reference text. In this way, the OT distance is able to avoid wrongly penalizing text that doesn’t mention redundant information in table. 3.4 Learning Objective For table structure reconstruction and content matching, both auxiliary tasks are trained with the main GPT-2’s language model loss together, which can be regarded as multi-task learning. The loss function of multi-task learning consists of language model loss LLM , table stru"
2020.coling-main.179,D17-1239,0,0.0673118,"-2 may attributes to the inconsistent expression “played for , among others , England .” and the expression of wrong birth date, which shows the imperfect switch policy on deciding when to copy from table can sometimes hurt model’s ability to generate high-fidelity text. On the contrary, TableGPT, enhanced to generate high-fidelity text with two auxiliary tasks without breaking one unified GPT-2 model for generating text, performs better in terms of fidelity and fluency of text in this example. 5 Related Work In recent years, neural models for generating texts directly from preprocessed data (Wiseman et al., 2017; Puduppully et al., 2019a; Puduppully et al., 2019b; Gong et al., 2019; Feng et al., 2020), have become mainstream for table-to-text generation and achieved impressive performance with the help of largescale dataset. Mei et al. (2016) proposes a pre-selector on encoder-aligner-decoder model for generation, which strengthens model’s content selection ability and obtains considerable improvement over standard Seq2Seq model. Sha et al. (2018) proposes a hybrid attention mechanism for modeling the order of content when generating texts. Liu et al. (2018) presents a field-gating encoder focusing o"
2020.coling-main.179,P19-1296,0,0.029996,"Missing"
2020.coling-main.179,2020.acl-demos.30,0,0.0171698,"s a hybrid attention mechanism for modeling the order of content when generating texts. Liu et al. (2018) presents a field-gating encoder focusing on modeling table structure and dual attention mechanism to utilize the structure information when decoding. In addition, Bao et al. (2018) develops a table-aware sequence-to-sequence model on this task. However, Chen et al. (2020b) shows that the well-performed Seq2Seq model trained on large-scale dataset suffer from limited training data in few-shot setting. Recently, GPT-2 has been successfully adapted to dialogue generation in few-shot setting (Zhang et al., 2020; Peng et al., 2020), showing potential to address insufficient training data problem for few-shot learning with the help of learnt knowledge from pretraining on vast corpus. As for table-to-text generation, Chen et al. (2020b) propose a switch model that use GPT-2 to generate template-like functional words while generating factual expressions via copying records’ values from table in few-shot scenario. Different from this work, we model the table and generate text within a GPT-2 model in a unified way and we show that our proposed TableGPT can perform well in the few-shot scenario. In additio"
2020.coling-main.238,D15-1109,0,0.0417366,"Missing"
2020.coling-main.238,L16-1432,0,0.0312328,"Missing"
2020.coling-main.238,D18-1241,0,0.0475096,"Missing"
2020.coling-main.238,N19-1423,0,0.0529108,"prehension for multiparty dialogs Methods. SQuAD 2.0 is an MRC dataset that adopts a passage as the input and the answer is a span from input passage (Rajpurkar et al., 2018). We adopt the following existing methods for SQuAD 2.0 on our dataset. In this paper, we use three different kinds of settings of BERT: BERT-base, BERT-large, and BERT-whole word masking (BERT-wwm). We concatenate all utterances from input dialog as a passage, and each utterance includes speaker and text. We used the open-source code of BERT to perform our experiments3 . BERT is a bidirectional encoder from transformers (Devlin et al., 2019). To learn better representations for text, BERT adopts two objectives: masked language modeling and the next sentence prediction during pretraining. In the BERT-wwm, if a part of a complete word WordPiece is replaced by [mask], the other parts of the same word will also be replaced by mask, which is the whole word mask. 3 https://github.com/google-research/bert 2648 Table 7: Results of machine reading comprehension for multiparty dialogs. Method BERT-base BERT-large BERT-wwm Human performance Human-machine gap EM Squad 2.0 73.1 80.0 86.7 86.8 0.1 Our 45.3 51.8 54.7 64.3 9.6 F1 Squad 2.0 76.2"
2020.coling-main.238,P17-1147,0,0.0218738,"in–lose game. We introduce the senses of discourse relation in STAC in Section 3.2. The STAC corpus contains 1,091 dialogs with 10,677 utterances and 11,348 discourse relations. Compared with STAC, our Molweni dataset contains 10,000 dialogs comprising 88,303 utterances and 78,245 discourse relations. Machine reading comprehension. There are several types of datasets for machine comprehension, including multiple-choice datasets (Richardson et al., 2013; Lai et al., 2017), answer sentence selection datasets (Wang et al., 2007; Yang et al., 2015) and extractive datasets (Rajpurkar et al., 2016; Joshi et al., 2017; Trischler et al., 2017; Rajpurkar et al., 2018) . To extend existing corpora, our Molweni dataset is constructed to be an extractive MRC dataset for multiparty dialog, which includes both answerable questions and unanswerable questions. Similar to Squad 2.0 (Rajpurkar et al., 2018), we also annotate plausible answers for unanswerable questions. Three closely related datasets are the extended crowdsourced Friends corpus (Ma et al., 2018), DREAM (Sun et al., 2019) and FriendsQA (Yang and Choi, 2019). Different from these three MRC datasets for dialog, Molweni contributes the discourse structur"
2020.coling-main.238,Q18-1023,0,0.0494099,"Missing"
2020.coling-main.238,D17-1082,0,0.168593,"y chat dialogs (Asher et al., 2016). The corpus derives from the online version of the game The Settlers of Catan. The game is a multiparty, win–lose game. We introduce the senses of discourse relation in STAC in Section 3.2. The STAC corpus contains 1,091 dialogs with 10,677 utterances and 11,348 discourse relations. Compared with STAC, our Molweni dataset contains 10,000 dialogs comprising 88,303 utterances and 78,245 discourse relations. Machine reading comprehension. There are several types of datasets for machine comprehension, including multiple-choice datasets (Richardson et al., 2013; Lai et al., 2017), answer sentence selection datasets (Wang et al., 2007; Yang et al., 2015) and extractive datasets (Rajpurkar et al., 2016; Joshi et al., 2017; Trischler et al., 2017; Rajpurkar et al., 2018) . To extend existing corpora, our Molweni dataset is constructed to be an extractive MRC dataset for multiparty dialog, which includes both answerable questions and unanswerable questions. Similar to Squad 2.0 (Rajpurkar et al., 2018), we also annotate plausible answers for unanswerable questions. Three closely related datasets are the extended crowdsourced Friends corpus (Ma et al., 2018), DREAM (Sun et"
2020.coling-main.238,P19-1210,0,0.0607923,"Missing"
2020.coling-main.238,W15-4640,0,0.0772636,"r speakers converse over seven utterances. We additionally employ annotators to read the passage and contribute questions: in the example, the annotators propose three questions: two answerable and one unanswerable. We observe that adjacent utterance pairs can be incoherent, illustrating the key challenge. It is non-trivial to detect discourse relations between non-adjacent utterances; and crucially, difficult to correctly interpret a multiparty dialog without a proper understanding of the input’s complex structure. We derived Molweni from the large-scale multiparty dialog Ubuntu Chat Corpus (Lowe et al., 2015). We chose the name Molweni, as it is the plural form of “Hello” in the Xhosa language, representing multiparty dialog in the same language as Ubuntu. Our dataset contains 10,000 dialogs with 88,303 utterances and 30,066 questions including answerable and unanswerable questions. All answerable questions are extractive questions whose answer is a span in the source dialog. For unanswerable questions, we annotate their plausible answers from dialog. Most questions in Molweni are 5W1H questions – Why, What, Who, Where, When, and How. For each dialog in the corpus, annotators propose three questio"
2020.coling-main.238,N18-1185,0,0.141448,"et al., 2013; Lai et al., 2017), answer sentence selection datasets (Wang et al., 2007; Yang et al., 2015) and extractive datasets (Rajpurkar et al., 2016; Joshi et al., 2017; Trischler et al., 2017; Rajpurkar et al., 2018) . To extend existing corpora, our Molweni dataset is constructed to be an extractive MRC dataset for multiparty dialog, which includes both answerable questions and unanswerable questions. Similar to Squad 2.0 (Rajpurkar et al., 2018), we also annotate plausible answers for unanswerable questions. Three closely related datasets are the extended crowdsourced Friends corpus (Ma et al., 2018), DREAM (Sun et al., 2019) and FriendsQA (Yang and Choi, 2019). Different from these three MRC datasets for dialog, Molweni contributes the discourse structure of dialogs, and additional instances of multiparty dialogs and unanswerable questions. 3 The Molweni corpus Our dataset derives from the large scale multiparty dialogs dataset — the Ubuntu Chat Corpus (Lowe et al., 2015). We list our three reasons in choosing the Ubuntu Chat Corpus as the base corpus for annotation. • First, the Ubuntu dataset is a large multiparty dataset. After filtering the dataset by only retaining all utterances wi"
2020.coling-main.238,N16-1013,0,0.0594168,"Missing"
2020.coling-main.238,prasad-etal-2008-penn,0,0.129569,"Missing"
2020.coling-main.238,D16-1264,0,0.0434744,"game is a multiparty, win–lose game. We introduce the senses of discourse relation in STAC in Section 3.2. The STAC corpus contains 1,091 dialogs with 10,677 utterances and 11,348 discourse relations. Compared with STAC, our Molweni dataset contains 10,000 dialogs comprising 88,303 utterances and 78,245 discourse relations. Machine reading comprehension. There are several types of datasets for machine comprehension, including multiple-choice datasets (Richardson et al., 2013; Lai et al., 2017), answer sentence selection datasets (Wang et al., 2007; Yang et al., 2015) and extractive datasets (Rajpurkar et al., 2016; Joshi et al., 2017; Trischler et al., 2017; Rajpurkar et al., 2018) . To extend existing corpora, our Molweni dataset is constructed to be an extractive MRC dataset for multiparty dialog, which includes both answerable questions and unanswerable questions. Similar to Squad 2.0 (Rajpurkar et al., 2018), we also annotate plausible answers for unanswerable questions. Three closely related datasets are the extended crowdsourced Friends corpus (Ma et al., 2018), DREAM (Sun et al., 2019) and FriendsQA (Yang and Choi, 2019). Different from these three MRC datasets for dialog, Molweni contributes th"
2020.coling-main.238,P18-2124,0,0.204274,"course relation in STAC in Section 3.2. The STAC corpus contains 1,091 dialogs with 10,677 utterances and 11,348 discourse relations. Compared with STAC, our Molweni dataset contains 10,000 dialogs comprising 88,303 utterances and 78,245 discourse relations. Machine reading comprehension. There are several types of datasets for machine comprehension, including multiple-choice datasets (Richardson et al., 2013; Lai et al., 2017), answer sentence selection datasets (Wang et al., 2007; Yang et al., 2015) and extractive datasets (Rajpurkar et al., 2016; Joshi et al., 2017; Trischler et al., 2017; Rajpurkar et al., 2018) . To extend existing corpora, our Molweni dataset is constructed to be an extractive MRC dataset for multiparty dialog, which includes both answerable questions and unanswerable questions. Similar to Squad 2.0 (Rajpurkar et al., 2018), we also annotate plausible answers for unanswerable questions. Three closely related datasets are the extended crowdsourced Friends corpus (Ma et al., 2018), DREAM (Sun et al., 2019) and FriendsQA (Yang and Choi, 2019). Different from these three MRC datasets for dialog, Molweni contributes the discourse structure of dialogs, and additional instances of multipa"
2020.coling-main.238,Q19-1016,0,0.0616554,"Missing"
2020.coling-main.238,D13-1020,0,0.0464644,"urse parsing on multiparty chat dialogs (Asher et al., 2016). The corpus derives from the online version of the game The Settlers of Catan. The game is a multiparty, win–lose game. We introduce the senses of discourse relation in STAC in Section 3.2. The STAC corpus contains 1,091 dialogs with 10,677 utterances and 11,348 discourse relations. Compared with STAC, our Molweni dataset contains 10,000 dialogs comprising 88,303 utterances and 78,245 discourse relations. Machine reading comprehension. There are several types of datasets for machine comprehension, including multiple-choice datasets (Richardson et al., 2013; Lai et al., 2017), answer sentence selection datasets (Wang et al., 2007; Yang et al., 2015) and extractive datasets (Rajpurkar et al., 2016; Joshi et al., 2017; Trischler et al., 2017; Rajpurkar et al., 2018) . To extend existing corpora, our Molweni dataset is constructed to be an extractive MRC dataset for multiparty dialog, which includes both answerable questions and unanswerable questions. Similar to Squad 2.0 (Rajpurkar et al., 2018), we also annotate plausible answers for unanswerable questions. Three closely related datasets are the extended crowdsourced Friends corpus (Ma et al., 2"
2020.coling-main.238,Q19-1014,0,0.222945,"2017), answer sentence selection datasets (Wang et al., 2007; Yang et al., 2015) and extractive datasets (Rajpurkar et al., 2016; Joshi et al., 2017; Trischler et al., 2017; Rajpurkar et al., 2018) . To extend existing corpora, our Molweni dataset is constructed to be an extractive MRC dataset for multiparty dialog, which includes both answerable questions and unanswerable questions. Similar to Squad 2.0 (Rajpurkar et al., 2018), we also annotate plausible answers for unanswerable questions. Three closely related datasets are the extended crowdsourced Friends corpus (Ma et al., 2018), DREAM (Sun et al., 2019) and FriendsQA (Yang and Choi, 2019). Different from these three MRC datasets for dialog, Molweni contributes the discourse structure of dialogs, and additional instances of multiparty dialogs and unanswerable questions. 3 The Molweni corpus Our dataset derives from the large scale multiparty dialogs dataset — the Ubuntu Chat Corpus (Lowe et al., 2015). We list our three reasons in choosing the Ubuntu Chat Corpus as the base corpus for annotation. • First, the Ubuntu dataset is a large multiparty dataset. After filtering the dataset by only retaining all utterances with response relations, the"
2020.coling-main.238,W17-2623,0,0.022661,"roduce the senses of discourse relation in STAC in Section 3.2. The STAC corpus contains 1,091 dialogs with 10,677 utterances and 11,348 discourse relations. Compared with STAC, our Molweni dataset contains 10,000 dialogs comprising 88,303 utterances and 78,245 discourse relations. Machine reading comprehension. There are several types of datasets for machine comprehension, including multiple-choice datasets (Richardson et al., 2013; Lai et al., 2017), answer sentence selection datasets (Wang et al., 2007; Yang et al., 2015) and extractive datasets (Rajpurkar et al., 2016; Joshi et al., 2017; Trischler et al., 2017; Rajpurkar et al., 2018) . To extend existing corpora, our Molweni dataset is constructed to be an extractive MRC dataset for multiparty dialog, which includes both answerable questions and unanswerable questions. Similar to Squad 2.0 (Rajpurkar et al., 2018), we also annotate plausible answers for unanswerable questions. Three closely related datasets are the extended crowdsourced Friends corpus (Ma et al., 2018), DREAM (Sun et al., 2019) and FriendsQA (Yang and Choi, 2019). Different from these three MRC datasets for dialog, Molweni contributes the discourse structure of dialogs, and additi"
2020.coling-main.238,D07-1003,0,0.0332586,"from the online version of the game The Settlers of Catan. The game is a multiparty, win–lose game. We introduce the senses of discourse relation in STAC in Section 3.2. The STAC corpus contains 1,091 dialogs with 10,677 utterances and 11,348 discourse relations. Compared with STAC, our Molweni dataset contains 10,000 dialogs comprising 88,303 utterances and 78,245 discourse relations. Machine reading comprehension. There are several types of datasets for machine comprehension, including multiple-choice datasets (Richardson et al., 2013; Lai et al., 2017), answer sentence selection datasets (Wang et al., 2007; Yang et al., 2015) and extractive datasets (Rajpurkar et al., 2016; Joshi et al., 2017; Trischler et al., 2017; Rajpurkar et al., 2018) . To extend existing corpora, our Molweni dataset is constructed to be an extractive MRC dataset for multiparty dialog, which includes both answerable questions and unanswerable questions. Similar to Squad 2.0 (Rajpurkar et al., 2018), we also annotate plausible answers for unanswerable questions. Three closely related datasets are the extended crowdsourced Friends corpus (Ma et al., 2018), DREAM (Sun et al., 2019) and FriendsQA (Yang and Choi, 2019). Differ"
2020.coling-main.238,W19-5923,0,0.0645038,"datasets (Wang et al., 2007; Yang et al., 2015) and extractive datasets (Rajpurkar et al., 2016; Joshi et al., 2017; Trischler et al., 2017; Rajpurkar et al., 2018) . To extend existing corpora, our Molweni dataset is constructed to be an extractive MRC dataset for multiparty dialog, which includes both answerable questions and unanswerable questions. Similar to Squad 2.0 (Rajpurkar et al., 2018), we also annotate plausible answers for unanswerable questions. Three closely related datasets are the extended crowdsourced Friends corpus (Ma et al., 2018), DREAM (Sun et al., 2019) and FriendsQA (Yang and Choi, 2019). Different from these three MRC datasets for dialog, Molweni contributes the discourse structure of dialogs, and additional instances of multiparty dialogs and unanswerable questions. 3 The Molweni corpus Our dataset derives from the large scale multiparty dialogs dataset — the Ubuntu Chat Corpus (Lowe et al., 2015). We list our three reasons in choosing the Ubuntu Chat Corpus as the base corpus for annotation. • First, the Ubuntu dataset is a large multiparty dataset. After filtering the dataset by only retaining all utterances with response relations, there are still over 380K sessions and"
2020.coling-main.238,D15-1237,0,0.0235004,"rsion of the game The Settlers of Catan. The game is a multiparty, win–lose game. We introduce the senses of discourse relation in STAC in Section 3.2. The STAC corpus contains 1,091 dialogs with 10,677 utterances and 11,348 discourse relations. Compared with STAC, our Molweni dataset contains 10,000 dialogs comprising 88,303 utterances and 78,245 discourse relations. Machine reading comprehension. There are several types of datasets for machine comprehension, including multiple-choice datasets (Richardson et al., 2013; Lai et al., 2017), answer sentence selection datasets (Wang et al., 2007; Yang et al., 2015) and extractive datasets (Rajpurkar et al., 2016; Joshi et al., 2017; Trischler et al., 2017; Rajpurkar et al., 2018) . To extend existing corpora, our Molweni dataset is constructed to be an extractive MRC dataset for multiparty dialog, which includes both answerable questions and unanswerable questions. Similar to Squad 2.0 (Rajpurkar et al., 2018), we also annotate plausible answers for unanswerable questions. Three closely related datasets are the extended crowdsourced Friends corpus (Ma et al., 2018), DREAM (Sun et al., 2019) and FriendsQA (Yang and Choi, 2019). Different from these three"
2020.coling-main.238,P19-1374,0,\N,Missing
2020.coling-main.360,D19-1015,0,0.659462,"lysis of comments in social media (Chatterjee et al., 2019), and so on. Different from the common sentence-level emotion recognition task, ERC is special due to some characteristics. The first one is that the utterances are context dependent, and modeling context can provide more information for emotion recognition (Poria et al., 2017; Jiao et al., 2019). The second characteristic of ERC is that the utterances are speaker-sensitive, thus many researchers modeled the state of speakers and the inter-speaker dependency relations (Hazarika et al., 2018b; Majumder et al., 2019; Zhang et al., 2019; Ghosal et al., 2019). In this paper, we observe another characteristic that is the emotions of the utterances are interactive. For example, in Figure 1, the emotion of the utterance from Speaker A can directly influence Speaker B. Thus, modeling the emotion interaction between utterances is helpful for the ERC task. Previous works usually implicitly model the emotion interaction by modeling dialogue context (Poria et al., 2017; Jiao et al., 2019). However, because of the arbitrariness of the dialogue, the context utterances often convey misleading emotion information when recognizing the emotion of the target utt"
2020.coling-main.360,D18-1280,0,0.684109,"customers in artificial services (Song et al., 2019), sentiment analysis of comments in social media (Chatterjee et al., 2019), and so on. Different from the common sentence-level emotion recognition task, ERC is special due to some characteristics. The first one is that the utterances are context dependent, and modeling context can provide more information for emotion recognition (Poria et al., 2017; Jiao et al., 2019). The second characteristic of ERC is that the utterances are speaker-sensitive, thus many researchers modeled the state of speakers and the inter-speaker dependency relations (Hazarika et al., 2018b; Majumder et al., 2019; Zhang et al., 2019; Ghosal et al., 2019). In this paper, we observe another characteristic that is the emotions of the utterances are interactive. For example, in Figure 1, the emotion of the utterance from Speaker A can directly influence Speaker B. Thus, modeling the emotion interaction between utterances is helpful for the ERC task. Previous works usually implicitly model the emotion interaction by modeling dialogue context (Poria et al., 2017; Jiao et al., 2019). However, because of the arbitrariness of the dialogue, the context utterances often convey misleading"
2020.coling-main.360,N18-1193,0,0.560718,"customers in artificial services (Song et al., 2019), sentiment analysis of comments in social media (Chatterjee et al., 2019), and so on. Different from the common sentence-level emotion recognition task, ERC is special due to some characteristics. The first one is that the utterances are context dependent, and modeling context can provide more information for emotion recognition (Poria et al., 2017; Jiao et al., 2019). The second characteristic of ERC is that the utterances are speaker-sensitive, thus many researchers modeled the state of speakers and the inter-speaker dependency relations (Hazarika et al., 2018b; Majumder et al., 2019; Zhang et al., 2019; Ghosal et al., 2019). In this paper, we observe another characteristic that is the emotions of the utterances are interactive. For example, in Figure 1, the emotion of the utterance from Speaker A can directly influence Speaker B. Thus, modeling the emotion interaction between utterances is helpful for the ERC task. Previous works usually implicitly model the emotion interaction by modeling dialogue context (Poria et al., 2017; Jiao et al., 2019). However, because of the arbitrariness of the dialogue, the context utterances often convey misleading"
2020.coling-main.360,L18-1252,0,0.136026,"Missing"
2020.coling-main.360,N19-1037,0,0.0610291,"h utterance in conversations. Recently it has received much attention due to its applications in various conversation scenes, such as emotional chatbots (Zhou et al., 2018), emotion detection of customers in artificial services (Song et al., 2019), sentiment analysis of comments in social media (Chatterjee et al., 2019), and so on. Different from the common sentence-level emotion recognition task, ERC is special due to some characteristics. The first one is that the utterances are context dependent, and modeling context can provide more information for emotion recognition (Poria et al., 2017; Jiao et al., 2019). The second characteristic of ERC is that the utterances are speaker-sensitive, thus many researchers modeled the state of speakers and the inter-speaker dependency relations (Hazarika et al., 2018b; Majumder et al., 2019; Zhang et al., 2019; Ghosal et al., 2019). In this paper, we observe another characteristic that is the emotions of the utterances are interactive. For example, in Figure 1, the emotion of the utterance from Speaker A can directly influence Speaker B. Thus, modeling the emotion interaction between utterances is helpful for the ERC task. Previous works usually implicitly mode"
2020.coling-main.360,D14-1181,0,0.00591326,"Missing"
2020.coling-main.360,D14-1162,0,0.0844867,"could set separate states for each speaker and associate states with the speaker’s utterance. In our experiment, we use the open-source codes3 of DialogueRNN provided by the authors. DialogueGCN (Ghosal et al., 2019) This is a GCN-based model. This baseline uses a GCN to model the conversation, the nodes in the graph represent utterances, and the types of edges are determined based on the speaker information. In our experiment, we use the open-source codes4 of DialogueGCN provided by the authors. 3.3 Experimental Settings In our experiment setting, we use the pretrained 840B GloVe embedding (Pennington et al., 2014) to initialize the 300 dimensional word embedding layer, and we set the emotion embedding dimension to 32. In utterance encoder, the hidden size of GRU is 50 for IEMOCAP and 100 for MELD. In emotion interaction based context encoder, the hidden size of GRU for two datasets is 132 and 232 respectively. We use Adam (Kingma and Ba, 2015) to optimize the parameters in our models, and use a minibatch size of 32. To regulate our models, we set the weight decay to 0.0001, and apply dropout with a dropout rate at 0.1. Based on validation performance on IEMOCAP, the learning rate is set to 0.0002, the"
2020.coling-main.360,P17-1081,0,0.539856,"e the emotion of each utterance in conversations. Recently it has received much attention due to its applications in various conversation scenes, such as emotional chatbots (Zhou et al., 2018), emotion detection of customers in artificial services (Song et al., 2019), sentiment analysis of comments in social media (Chatterjee et al., 2019), and so on. Different from the common sentence-level emotion recognition task, ERC is special due to some characteristics. The first one is that the utterances are context dependent, and modeling context can provide more information for emotion recognition (Poria et al., 2017; Jiao et al., 2019). The second characteristic of ERC is that the utterances are speaker-sensitive, thus many researchers modeled the state of speakers and the inter-speaker dependency relations (Hazarika et al., 2018b; Majumder et al., 2019; Zhang et al., 2019; Ghosal et al., 2019). In this paper, we observe another characteristic that is the emotions of the utterances are interactive. For example, in Figure 1, the emotion of the utterance from Speaker A can directly influence Speaker B. Thus, modeling the emotion interaction between utterances is helpful for the ERC task. Previous works usu"
2020.coling-main.360,P19-1050,0,0.393101,"Missing"
2020.coling-main.360,D19-1019,0,0.026833,"gold emotion labels to explicitly model the emotion interaction. This approach solves the above problem, and can effectively retain the performance advantages of explicit modeling. We conduct experiments on two datasets, and our approach achieves state-of-the-art performance. 1 Introduction Emotion recognition in conversations (ERC) aims to recognize the emotion of each utterance in conversations. Recently it has received much attention due to its applications in various conversation scenes, such as emotional chatbots (Zhou et al., 2018), emotion detection of customers in artificial services (Song et al., 2019), sentiment analysis of comments in social media (Chatterjee et al., 2019), and so on. Different from the common sentence-level emotion recognition task, ERC is special due to some characteristics. The first one is that the utterances are context dependent, and modeling context can provide more information for emotion recognition (Poria et al., 2017; Jiao et al., 2019). The second characteristic of ERC is that the utterances are speaker-sensitive, thus many researchers modeled the state of speakers and the inter-speaker dependency relations (Hazarika et al., 2018b; Majumder et al., 2019; Zhang"
2020.coling-main.360,D19-1016,0,0.291032,"Missing"
2020.findings-emnlp.139,D14-1181,0,0.0124414,"Missing"
2020.findings-emnlp.139,P07-2045,0,0.00976133,"Missing"
2020.findings-emnlp.139,2020.acl-main.703,0,0.111381,"Missing"
2020.findings-emnlp.139,P16-1195,0,0.0204779,"eration, evaluated with smoothed BLEU-4 score. language could improve code-to-NL generation. Besides, results in the Table 4 show that CodeBERT pre-trained with RTD and MLM objectives brings a gain of 1.3 BLEU score over RoBERTa overall and achieve the state-of-the-art performance8 . 4.4 Generalization to Programming Languages NOT in Pre-training We would like to evaluate CodeBERT on the programming language which is never seen in the pretraining step. To this end, we study the task of generating a natural language summary of a C# code snippet. We conduct experiments on the dataset of CodeNN (Iyer et al., 2016)9 , which consists of 66,015 pairs of questions and answers automatically collected from StackOverflow. This dataset is challenging since the scale of dataset is orders of magnitude smaller than CodeSearchNet Corpus. We evaluate models using smoothed BLEU-4 score and use the same evaluation scripts as Iyer et al. (2016). M ODEL BLEU MOSES (KOEHN ET AL ., 2007) IR SUM-NN (RUSH ET AL ., 2015) 2- LAYER B I LSTM T RANSFORMER (VASWANI ET AL ., 2017) T REE LSTM (TAI ET AL ., 2015) C ODE NN (I YER ET AL ., 2016) CODE 2 SEQ (A LON ET AL ., 2019) RO BERTA PRE - TRAIN W / CODE ONLY C ODE BERT (RTD) C OD"
2020.findings-emnlp.139,C04-1072,0,0.0190066,"ly applied. Predicted probabilities of RoBERTa and CodeBERT are given. Code Documentation Generation Although the pre-training objective of CodeBERT does not include generation-based objectives (Lewis et al., 2019), we would like to investigate to what extent does CodeBERT perform on generation tasks. Specifically, we study code-to-NL generation, and report results for the documentation generation task on CodeSearchNet Corpus in six programming languages. Since the generated documentations are short and higher order n-grams may not overlap, we remedy this problem by using smoothed BLEU score (Lin and Och, 2004). 7 The example comes from https:// github.com/peri-source/peri/blob/ 61beed5deaaf978ab31ed716e8470d86ba639867/ peri/comp/psfcalc.py#L994-L1002 Model Comparisons We compare our model with several baselines, including a RNN-based model with attention mechanism (Sutskever et al., 2014), the Transformer (Vaswani et al., 2017), RoBERTa and the model pre-trained on code only. To demonstrate the effectiveness of CodeBERT on code-to-NL generation tasks, we adopt various pre-trained models as encoders and keep the hyperparameters consistent. Detailed hyper-parameters are given in Appendix B.3. Table 4"
2020.findings-emnlp.139,2021.ccl-1.108,0,0.209986,"Missing"
2020.findings-emnlp.139,N18-1202,0,0.370575,"lps to learn better generators. We evaluate CodeBERT on two NL-PL applications by fine-tuning model parameters. Results show that CodeBERT achieves state-of-the-art performance on both natural language code search and code documentation generation. Furthermore, to investigate what type of knowledge is learned in CodeBERT, we construct a dataset for NL-PL probing, and evaluate in a zero-shot setting where parameters of pre-trained models are fixed. Results show that CodeBERT performs better than previous pre-trained models on NLPL probing.1 1 Introduction Large pre-trained models such as ELMo (Peters et al., 2018), GPT (Radford et al., 2018), BERT (Devlin et al., 2018), XLNet (Yang et al., 2019) ∗ Work done while this author was an intern at Microsoft Research Asia. 1 All the codes and data are available at https:// github.com/microsoft/CodeBERT and RoBERTa (Liu et al., 2019) have dramatically improved the state-of-the-art on a variety of natural language processing (NLP) tasks. These pre-trained models learn effective contextual representations from massive unlabeled text optimized by self-supervised objectives, such as masked language modeling, which predicts the original masked word from an artifici"
2020.findings-emnlp.139,D19-1250,0,0.0585014,"Missing"
2020.findings-emnlp.139,P19-1493,0,0.0463596,"Missing"
2020.findings-emnlp.139,D15-1044,0,0.118151,"Missing"
2020.findings-emnlp.139,P15-1150,0,0.0976114,"Missing"
2020.findings-emnlp.139,2020.tacl-1.48,0,0.0794978,"Missing"
2020.findings-emnlp.139,P02-1040,0,\N,Missing
2020.findings-emnlp.262,P00-1037,0,0.592852,"Missing"
2020.findings-emnlp.262,D19-1310,1,0.926342,", which entangle aforementioned stages and generate text directly from structured data through a neural encoder-decoder framework (Wiseman et al., 2017; Nie et al., 2018). As in Fig. 1, this task provides tables with redundant records. Each record has three elements: table row header (entity, e.g. Conley), table column header (type, e.g. points) and table cell (value, e.g. 32). Models are expected to generate descriptive text reflecting salient records. Many neural end-to-end models have achieved remarkable progress of generating fluent and natural text on this task (Puduppully et al., 2019b; Gong et al., 2019). However, previous work notices that the content planning stage is the key factor in table-to-text generation (Gkatzia, 2016), but end-to-end models are difficult to explicitly improve their content planning ability. Recently, Puduppully et al. (2019a) proposed Neural Content Planning (NCP), a twostage model that explicitly selects and orders salient records whilst keeping the ability to generate fluent text of end-to-end models. They show that content planning (referring to both “content selection and planning” in Puduppully et al. (2019a)) indeed correlates with the quality of final output."
2020.findings-emnlp.262,P16-1014,0,0.0301339,"ut text, denoted as r∗ = {r1∗ , . . . , rT∗ } (T is the number of records mentioned in y). Here, we follow Puduppully et al. (2019a) to extract content plans using an information extraction (IE) approach as oracles. In each time step, the decoder takes previously selected record’s representation as input and use the attention weights to select the next important one. In Stage 2 (surface realization), a standard encoder-decoder model is applied, taking the output content plan from Stage 1 as input and generating text with attention mechanism (Luong et al., 2015) and conditional copy mechanism (Gulcehre et al., 2016). From results in Puduppully et al. (2019a), it is observed that performance bottleneck lies in Stage 1. That is, if we feed gold content plans into Stage 2, final results are much better, but if inferred content plans are fed instead, performance decreases drastically. Therefore, we focus on improving NCP’s Stage 1 for better final outputs. 2906 Grizzlies Player PTS AST s1 r2cs r3cs … … … … Mean Pooling … r3cs rjcs rjcs cs cs cs y3 rics Attention +Copy p 112 Value Score r3cs r1cs rics … h1 h2 h3 h4 … rs r3cs r1cs rics Entity Importance … r2 r3 rj Reference Record Sequences 1. Contextual Numer"
2020.findings-emnlp.262,P83-1022,0,0.484569,"xample with NCP’s result and gold text. Important/unimportant entities and records are in red/blue. Text that accurately/incorrectly report statistics in table is in bold/italic. Introduction Table-to-text generation refers to the task of generating text from structured data. Models for this task can be mainly categorized into two types: pipeline-style models, which decompose the generation process into sequential stages, including content planning (Stage 1, selecting and ordering salient content from the input) and surface realization (Stage 2, converting the content plan to surface string) (Kukich, 1983; McKeown, 1985); and end-to-end models, which entangle aforementioned stages and generate text directly from structured data through a neural encoder-decoder framework (Wiseman et al., 2017; Nie et al., 2018). As in Fig. 1, this task provides tables with redundant records. Each record has three elements: table row header (entity, e.g. Conley), table column header (type, e.g. points) and table cell (value, e.g. 32). Models are expected to generate descriptive text reflecting salient records. Many neural end-to-end models have achieved remarkable progress of generating fluent and natural text o"
2020.findings-emnlp.262,N16-1014,0,0.031493,"r training contextual numerical value representations, we use the hinge loss (Eq.5). ξ is the margin and T (·) gives +1 if · is true and −1 otherwise. `pre = • Entity Importance (EI) evaluates if a predicted record rt contains an important entity by comparing whether the entity is mentioned in the gold content plan {ri∗ }. R(·) function gives +1 reward when · is true and -1 otherwise. Content Planning Verification The original NCP uses the pointer network to explicitly infer a content plan by optimizing the MLE of gold content plans. As noticed in other generation tasks (Sordoni et al., 2015; Li et al., 2016a; Dai et al., 2017), generation models with the MLE as the objective function tend to generate universal output sequences observed in the training data and it is desirable to integrate developer-defined rewards that better mimic the true goal of an ideal output sequence (Li et al., 2016b), which is the sequence of the content plan in our task. In order to explicitly reflect the quality of content plans, we explore rewards that measure the following five criteria, and optimized the model according to them via policy gradient (Sutton and Barto, 1998). (6) • Entity Recall (ER) measures how many"
2020.findings-emnlp.262,D16-1127,0,0.0183659,"r training contextual numerical value representations, we use the hinge loss (Eq.5). ξ is the margin and T (·) gives +1 if · is true and −1 otherwise. `pre = • Entity Importance (EI) evaluates if a predicted record rt contains an important entity by comparing whether the entity is mentioned in the gold content plan {ri∗ }. R(·) function gives +1 reward when · is true and -1 otherwise. Content Planning Verification The original NCP uses the pointer network to explicitly infer a content plan by optimizing the MLE of gold content plans. As noticed in other generation tasks (Sordoni et al., 2015; Li et al., 2016a; Dai et al., 2017), generation models with the MLE as the objective function tend to generate universal output sequences observed in the training data and it is desirable to integrate developer-defined rewards that better mimic the true goal of an ideal output sequence (Li et al., 2016b), which is the sequence of the content plan in our task. In order to explicitly reflect the quality of content plans, we explore rewards that measure the following five criteria, and optimized the model according to them via policy gradient (Sutton and Barto, 1998). (6) • Entity Recall (ER) measures how many"
2020.findings-emnlp.262,C18-1089,0,0.161244,"Missing"
2020.findings-emnlp.262,D15-1166,0,0.0227895,"equence of important records extracted from the output text, denoted as r∗ = {r1∗ , . . . , rT∗ } (T is the number of records mentioned in y). Here, we follow Puduppully et al. (2019a) to extract content plans using an information extraction (IE) approach as oracles. In each time step, the decoder takes previously selected record’s representation as input and use the attention weights to select the next important one. In Stage 2 (surface realization), a standard encoder-decoder model is applied, taking the output content plan from Stage 1 as input and generating text with attention mechanism (Luong et al., 2015) and conditional copy mechanism (Gulcehre et al., 2016). From results in Puduppully et al. (2019a), it is observed that performance bottleneck lies in Stage 1. That is, if we feed gold content plans into Stage 2, final results are much better, but if inferred content plans are fed instead, performance decreases drastically. Therefore, we focus on improving NCP’s Stage 1 for better final outputs. 2906 Grizzlies Player PTS AST s1 r2cs r3cs … … … … Mean Pooling … r3cs rjcs rjcs cs cs cs y3 rics Attention +Copy p 112 Value Score r3cs r1cs rics … h1 h2 h3 h4 … rs r3cs r1cs rics Entity Importance …"
2020.findings-emnlp.262,P19-1329,0,0.0628395,"es when decoding texts. Different from them, we model numerical values during encoding. Iso et al. (2019) incorporate writers’ information to generate text step-by-step. Our work can also consider such information in surface realization (Stage 2). For a fair comparison of all methods, we do not include the use of this model here. Gong et al. (2019) utilize hierarchical encoders with dual attention to consider both the table structure and history information. In terms of building numerical value representations, Spithourakis and Riedel (2018) explore number prediction for language models while Naik et al. (2019) explore numerical embeddings to capture the numeration and magnitude properties of numbers. In our task, generation models rely heavily on copy mechanism to cover numerical values in text and achieve good results. Thus, how to understand numerical values to select records becomes important and we propose to understand them through their context. 6 Conclusion In order to enhance neural content planning for table-to-text generation, we proposed (1) contextual numerical value representations to help model understand data values and (2) effective rewards 2912 to verify a model’s inferred importan"
2020.findings-emnlp.262,D18-1422,0,0.082854,"t generation refers to the task of generating text from structured data. Models for this task can be mainly categorized into two types: pipeline-style models, which decompose the generation process into sequential stages, including content planning (Stage 1, selecting and ordering salient content from the input) and surface realization (Stage 2, converting the content plan to surface string) (Kukich, 1983; McKeown, 1985); and end-to-end models, which entangle aforementioned stages and generate text directly from structured data through a neural encoder-decoder framework (Wiseman et al., 2017; Nie et al., 2018). As in Fig. 1, this task provides tables with redundant records. Each record has three elements: table row header (entity, e.g. Conley), table column header (type, e.g. points) and table cell (value, e.g. 32). Models are expected to generate descriptive text reflecting salient records. Many neural end-to-end models have achieved remarkable progress of generating fluent and natural text on this task (Puduppully et al., 2019b; Gong et al., 2019). However, previous work notices that the content planning stage is the key factor in table-to-text generation (Gkatzia, 2016), but end-to-end models ar"
2020.findings-emnlp.262,P02-1040,0,0.107318,"valuation Metrics We conducted experiments on both ROTOWIRE1 and MLB (Puduppully et al., 2019b) dataset. The former provides pairs of NBA game statistics and summary. Differently, the latter provides summary and heterogeneous input, consisting of MLB game statistics and event data (including event type, actors, etc.) in chronological order. For ROTOWIRE, we follow official training, development and test splits of 3398/727/728 instances. For MLB, as the contents are not released, we are able to retrieve a split of 22820/1739/1744 instances via official scripts 2 . For evaluations, we use BLEU (Papineni et al., 2002) and three extractive metrics, which evaluate the generated results from the following aspects: (1) Relation Generation (RG), measuring the text fidelity about whether to describe information from table truthfully. (2) Content Selection (CS) to measure whether important information is selected from redundant game statistics. (3) Content Ordering (CO) evaluates a model’s ability to plan and order data records naturally in text. More details can be found in Wiseman et al. (2017). Implementation Details We follow Puduppully et al. (2019a)’s and Puduppully et al. (2019b)’s training configurations"
2020.findings-emnlp.262,P19-1195,0,0.422684,"85); and end-to-end models, which entangle aforementioned stages and generate text directly from structured data through a neural encoder-decoder framework (Wiseman et al., 2017; Nie et al., 2018). As in Fig. 1, this task provides tables with redundant records. Each record has three elements: table row header (entity, e.g. Conley), table column header (type, e.g. points) and table cell (value, e.g. 32). Models are expected to generate descriptive text reflecting salient records. Many neural end-to-end models have achieved remarkable progress of generating fluent and natural text on this task (Puduppully et al., 2019b; Gong et al., 2019). However, previous work notices that the content planning stage is the key factor in table-to-text generation (Gkatzia, 2016), but end-to-end models are difficult to explicitly improve their content planning ability. Recently, Puduppully et al. (2019a) proposed Neural Content Planning (NCP), a twostage model that explicitly selects and orders salient records whilst keeping the ability to generate fluent text of end-to-end models. They show that content planning (referring to both “content selection and planning” in Puduppully et al. (2019a)) indeed correlates with the qua"
2020.findings-emnlp.262,N15-1020,0,0.0233429,"her than f (˜ rj ). For training contextual numerical value representations, we use the hinge loss (Eq.5). ξ is the margin and T (·) gives +1 if · is true and −1 otherwise. `pre = • Entity Importance (EI) evaluates if a predicted record rt contains an important entity by comparing whether the entity is mentioned in the gold content plan {ri∗ }. R(·) function gives +1 reward when · is true and -1 otherwise. Content Planning Verification The original NCP uses the pointer network to explicitly infer a content plan by optimizing the MLE of gold content plans. As noticed in other generation tasks (Sordoni et al., 2015; Li et al., 2016a; Dai et al., 2017), generation models with the MLE as the objective function tend to generate universal output sequences observed in the training data and it is desirable to integrate developer-defined rewards that better mimic the true goal of an ideal output sequence (Li et al., 2016b), which is the sequence of the content plan in our task. In order to explicitly reflect the quality of content plans, we explore rewards that measure the following five criteria, and optimized the model according to them via policy gradient (Sutton and Barto, 1998). (6) • Entity Recall (ER) m"
2020.findings-emnlp.262,P18-1196,0,0.0796316,"he content planning. Puduppully et al. (2019b) propose to specifically model entities when decoding texts. Different from them, we model numerical values during encoding. Iso et al. (2019) incorporate writers’ information to generate text step-by-step. Our work can also consider such information in surface realization (Stage 2). For a fair comparison of all methods, we do not include the use of this model here. Gong et al. (2019) utilize hierarchical encoders with dual attention to consider both the table structure and history information. In terms of building numerical value representations, Spithourakis and Riedel (2018) explore number prediction for language models while Naik et al. (2019) explore numerical embeddings to capture the numeration and magnitude properties of numbers. In our task, generation models rely heavily on copy mechanism to cover numerical values in text and achieve good results. Thus, how to understand numerical values to select records becomes important and we propose to understand them through their context. 6 Conclusion In order to enhance neural content planning for table-to-text generation, we proposed (1) contextual numerical value representations to help model understand data valu"
2020.findings-emnlp.262,D17-1239,0,0.0378658,"Missing"
2020.findings-emnlp.58,C10-3004,1,0.668986,"f the whole word, which is easier for the model to predict. In Chinese condition, WordPiece tokenizer no longer split the word into small fragments, as Chinese characters are not formed by alphabet-like symbols. We use the traditional Chinese Word Segmentation (CWS) tool to split the text into several words. In this way, we could adopt whole word masking in Chinese to mask the word instead of individual Chinese characters. For implementation, we strictly followed the original whole word masking codes and did not change other components, such as the percentage of word masking, etc. We use LTP (Che et al., 2010) for Chinese word segmentation to identify the word boundaries. 659 Chinese English Original Sentence + CWS + BERT Tokenizer 使用语言模型来预测下一个词的概率。 使用 语言 模型 来 预测 下 一个 词 的 概率 。 使用语言模型来预测下一个词的概率。 we use a language model to predict the probability of the next word. we use a language model to pre ##di ##ct the pro ##ba ##bility of the next word . Original Masking + WWM ++ N-gram Masking +++ Mac Masking 使 用 语 言 [M] 型 来 [M] 测 下 一 个 词 的 概 率 。 使 用 语 言 [M] [M] 来 [M] [M] 下 一 个 词 的 概 率 。 使 用 [M] [M] [M] [M] 来 [M] [M] 下 一 个 词 的 概 率 。 使用语法建模来预见下一个词的几率。 we use a language [M] to [M] ##di ##ct the pro [M] ##bility"
2020.findings-emnlp.58,D18-1536,0,0.0401552,"on various natural language processing tasks, covering a wide spectrum of text length, i.e., from sentencelevel to document-level. Task details are shown in Table 2. Specifically, we choose the following eight popular Chinese datasets. 5 https://cloud.google.com/tpu/ • Machine Reading Comprehension (MRC): CMRC 2018 (Cui et al., 2019b), DRCD (Shao et al., 2018), CJRC (Duan et al., 2019). • Single Sentence Classification (SSC): ChnSentiCorp (Tan and Zhang, 2008), THUCNews (Li and Sun, 2007). • Sentence Pair Classification (SPC): XNLI (Conneau et al., 2018), LCQMC (Liu et al., 2018), BQ Corpus (Chen et al., 2018). In order to make a fair comparison, for each dataset, we keep the same hyper-parameters (such as maximum length, warm-up steps, etc.) and only tune the initial learning rate from 1e-5 to 5e-5 for each task. Note that the initial learning rates are tuned on original Chinese BERT, and it would be possible to achieve another gains by tuning the learning rate individually. We run the same experiment ten times to ensure the reliability of results. The best initial learning rate is determined by selecting the best average development set performance. We report the maximum and average scores to bot"
2020.findings-emnlp.58,D18-1241,0,0.0205389,"could achieve state-of-the-art performances on many NLP tasks, and we also ablate details with several findings that may help future research.1 1 Introduction Bidirectional Encoder Representations from Transformers (BERT) (Devlin et al., 2019) has become enormously popular and has proven to be effective in recent natural language processing studies, which utilizes large-scale unlabeled training data and generates enriched contextual representations. As we traverse several popular machine reading comprehension benchmarks, such as SQuAD (Rajpurkar et al., 2018), CoQA (Reddy et al., 2019), QuAC (Choi et al., 2018), NaturalQuestions (Kwiatkowski et al., 2019), RACE (Lai et al., 2017), we can see that most of the top-performing models are based on BERT and its variants (Dai et al., 2019; Zhang et al., 2019; Ran et al., 2019), demonstrating that the pre-trained language models have 1 https://github.com/ymcui/MacBERT become new fundamental components in natural language processing field. Starting from BERT, the community have made great and rapid progress on optimizing the pretrained language models, such as ERNIE (Sun et al., 2019a), XLNet (Yang et al., 2019), RoBERTa (Liu et al., 2019), SpanBERT (Joshi e"
2020.findings-emnlp.58,D18-1269,0,0.0475474,"rained language models, we carried out extensive experiments on various natural language processing tasks, covering a wide spectrum of text length, i.e., from sentencelevel to document-level. Task details are shown in Table 2. Specifically, we choose the following eight popular Chinese datasets. 5 https://cloud.google.com/tpu/ • Machine Reading Comprehension (MRC): CMRC 2018 (Cui et al., 2019b), DRCD (Shao et al., 2018), CJRC (Duan et al., 2019). • Single Sentence Classification (SSC): ChnSentiCorp (Tan and Zhang, 2008), THUCNews (Li and Sun, 2007). • Sentence Pair Classification (SPC): XNLI (Conneau et al., 2018), LCQMC (Liu et al., 2018), BQ Corpus (Chen et al., 2018). In order to make a fair comparison, for each dataset, we keep the same hyper-parameters (such as maximum length, warm-up steps, etc.) and only tune the initial learning rate from 1e-5 to 5e-5 for each task. Note that the initial learning rates are tuned on original Chinese BERT, and it would be possible to achieve another gains by tuning the learning rate individually. We run the same experiment ten times to ensure the reliability of results. The best initial learning rate is determined by selecting the best average development set per"
2020.findings-emnlp.58,D19-1600,1,0.93983,"on both left and right context in all Transformer layers. Primarily, BERT consists of two pre-training tasks: Masked Language Model (MLM) and Next Sentence Prediction (NSP). Later, they further proposed a technique called whole word masking (wwm) for optimizing the original masking in the MLM task. In this setting, instead of randomly selecting WordPiece (Wu et al., 2016) tokens to mask, we always mask all of the tokens corresponding to a whole word at once. This will explicitly force the model to recover the whole word in the MLM pre-training task instead of just recovering WordPiece tokens (Cui et al., 2019a), which is much more challenging. As the whole word masking only affects the masking strategy of the pre-training process, it would not bring additional burdens on down-stream tasks. Moreover, as training pre-trained language models are computationally expensive, they also release all the pretrained models as well as the source codes, which stimulates the community to have great interests in the research of pre-trained language models. 2.2 ERNIE ERNIE (Enhanced Representation through kNowledge IntEgration) (Sun et al., 2019a) is designed to optimize the masking process of BERT, which include"
2020.findings-emnlp.58,P19-1285,0,0.0713452,"Missing"
2020.findings-emnlp.58,N19-1423,0,0.480575,"he community. We also propose a simple but effective model called MacBERT, which improves upon RoBERTa in several ways, especially the masking strategy that adopts MLM as correction (Mac). We carried out extensive experiments on eight Chinese NLP tasks to revisit the existing pre-trained language models as well as the proposed MacBERT. Experimental results show that MacBERT could achieve state-of-the-art performances on many NLP tasks, and we also ablate details with several findings that may help future research.1 1 Introduction Bidirectional Encoder Representations from Transformers (BERT) (Devlin et al., 2019) has become enormously popular and has proven to be effective in recent natural language processing studies, which utilizes large-scale unlabeled training data and generates enriched contextual representations. As we traverse several popular machine reading comprehension benchmarks, such as SQuAD (Rajpurkar et al., 2018), CoQA (Reddy et al., 2019), QuAC (Choi et al., 2018), NaturalQuestions (Kwiatkowski et al., 2019), RACE (Lai et al., 2017), we can see that most of the top-performing models are based on BERT and its variants (Dai et al., 2019; Zhang et al., 2019; Ran et al., 2019), demonstrat"
2020.findings-emnlp.58,Q19-1026,0,0.0190409,"ances on many NLP tasks, and we also ablate details with several findings that may help future research.1 1 Introduction Bidirectional Encoder Representations from Transformers (BERT) (Devlin et al., 2019) has become enormously popular and has proven to be effective in recent natural language processing studies, which utilizes large-scale unlabeled training data and generates enriched contextual representations. As we traverse several popular machine reading comprehension benchmarks, such as SQuAD (Rajpurkar et al., 2018), CoQA (Reddy et al., 2019), QuAC (Choi et al., 2018), NaturalQuestions (Kwiatkowski et al., 2019), RACE (Lai et al., 2017), we can see that most of the top-performing models are based on BERT and its variants (Dai et al., 2019; Zhang et al., 2019; Ran et al., 2019), demonstrating that the pre-trained language models have 1 https://github.com/ymcui/MacBERT become new fundamental components in natural language processing field. Starting from BERT, the community have made great and rapid progress on optimizing the pretrained language models, such as ERNIE (Sun et al., 2019a), XLNet (Yang et al., 2019), RoBERTa (Liu et al., 2019), SpanBERT (Joshi et al., 2019), ALBERT (Lan et al., 2019), ELEC"
2020.findings-emnlp.58,D17-1082,0,0.0475345,"lso ablate details with several findings that may help future research.1 1 Introduction Bidirectional Encoder Representations from Transformers (BERT) (Devlin et al., 2019) has become enormously popular and has proven to be effective in recent natural language processing studies, which utilizes large-scale unlabeled training data and generates enriched contextual representations. As we traverse several popular machine reading comprehension benchmarks, such as SQuAD (Rajpurkar et al., 2018), CoQA (Reddy et al., 2019), QuAC (Choi et al., 2018), NaturalQuestions (Kwiatkowski et al., 2019), RACE (Lai et al., 2017), we can see that most of the top-performing models are based on BERT and its variants (Dai et al., 2019; Zhang et al., 2019; Ran et al., 2019), demonstrating that the pre-trained language models have 1 https://github.com/ymcui/MacBERT become new fundamental components in natural language processing field. Starting from BERT, the community have made great and rapid progress on optimizing the pretrained language models, such as ERNIE (Sun et al., 2019a), XLNet (Yang et al., 2019), RoBERTa (Liu et al., 2019), SpanBERT (Joshi et al., 2019), ALBERT (Lan et al., 2019), ELECTRA (Clark et al., 2020),"
2020.findings-emnlp.58,D07-1081,0,0.024255,"4.2 Setups for Fine-tuning Tasks To thoroughly test these pre-trained language models, we carried out extensive experiments on various natural language processing tasks, covering a wide spectrum of text length, i.e., from sentencelevel to document-level. Task details are shown in Table 2. Specifically, we choose the following eight popular Chinese datasets. 5 https://cloud.google.com/tpu/ • Machine Reading Comprehension (MRC): CMRC 2018 (Cui et al., 2019b), DRCD (Shao et al., 2018), CJRC (Duan et al., 2019). • Single Sentence Classification (SSC): ChnSentiCorp (Tan and Zhang, 2008), THUCNews (Li and Sun, 2007). • Sentence Pair Classification (SPC): XNLI (Conneau et al., 2018), LCQMC (Liu et al., 2018), BQ Corpus (Chen et al., 2018). In order to make a fair comparison, for each dataset, we keep the same hyper-parameters (such as maximum length, warm-up steps, etc.) and only tune the initial learning rate from 1e-5 to 5e-5 for each task. Note that the initial learning rates are tuned on original Chinese BERT, and it would be possible to achieve another gains by tuning the learning rate individually. We run the same experiment ten times to ensure the reliability of results. The best initial learning r"
2020.findings-emnlp.58,C18-1166,0,0.0996317,"Missing"
2020.findings-emnlp.58,2021.ccl-1.108,0,0.114968,"Missing"
2020.findings-emnlp.58,P18-2124,0,0.0291812,"proposed MacBERT. Experimental results show that MacBERT could achieve state-of-the-art performances on many NLP tasks, and we also ablate details with several findings that may help future research.1 1 Introduction Bidirectional Encoder Representations from Transformers (BERT) (Devlin et al., 2019) has become enormously popular and has proven to be effective in recent natural language processing studies, which utilizes large-scale unlabeled training data and generates enriched contextual representations. As we traverse several popular machine reading comprehension benchmarks, such as SQuAD (Rajpurkar et al., 2018), CoQA (Reddy et al., 2019), QuAC (Choi et al., 2018), NaturalQuestions (Kwiatkowski et al., 2019), RACE (Lai et al., 2017), we can see that most of the top-performing models are based on BERT and its variants (Dai et al., 2019; Zhang et al., 2019; Ran et al., 2019), demonstrating that the pre-trained language models have 1 https://github.com/ymcui/MacBERT become new fundamental components in natural language processing field. Starting from BERT, the community have made great and rapid progress on optimizing the pretrained language models, such as ERNIE (Sun et al., 2019a), XLNet (Yang et al.,"
2020.findings-emnlp.58,D16-1264,0,0.0704602,"019), we only compare BERT (Devlin et al., 2019), BERT-wwm, BERT-wwm-ext, RoBERTawwm-ext, RoBERTa-wwm-ext-large, ELECTRA, along with our MacBERT to ensure relatively fair comparisons among different models, where all models are trained by ourselves except for the original Chinese BERT by Devlin et al. (2019). We carried out experiments under TensorFlow framework (Abadi et al., 2016) with slight modifications to the fine-tuning scripts6 provided by Devlin et al. (2019) to better adapt to Chinese. 5 • CMRC 2018: A span-extraction machine reading comprehension dataset, which is similar to SQuAD (Rajpurkar et al., 2016) that extract a passage span for the given question. • DRCD: This is also a span-extraction MRC dataset but in Traditional Chinese. • CJRC: Similar to CoQA (Reddy et al., 2019), which has yes/no questions, no-answer questions, and span-extraction questions. The data is collected from Chinese law judgment documents. Note that we only use small-train-data.json for training. F1 EM F1 BERT BERT-wwm BERT-wwm-ext RoBERTa-wwm-ext ELECTRA-base MacBERT-base 83.1 (82.7) 84.3 (83.4) 85.0 (84.5) 86.6 (85.9) 87.5 (87.0) 89.4 (89.2) 89.9 (89.6) 90.5 (90.2) 91.2 (90.9) 92.5 (92.2) 92.5 (92.3) 94.3 (94.1) 82."
2020.findings-emnlp.58,Q19-1016,0,0.0917529,"results show that MacBERT could achieve state-of-the-art performances on many NLP tasks, and we also ablate details with several findings that may help future research.1 1 Introduction Bidirectional Encoder Representations from Transformers (BERT) (Devlin et al., 2019) has become enormously popular and has proven to be effective in recent natural language processing studies, which utilizes large-scale unlabeled training data and generates enriched contextual representations. As we traverse several popular machine reading comprehension benchmarks, such as SQuAD (Rajpurkar et al., 2018), CoQA (Reddy et al., 2019), QuAC (Choi et al., 2018), NaturalQuestions (Kwiatkowski et al., 2019), RACE (Lai et al., 2017), we can see that most of the top-performing models are based on BERT and its variants (Dai et al., 2019; Zhang et al., 2019; Ran et al., 2019), demonstrating that the pre-trained language models have 1 https://github.com/ymcui/MacBERT become new fundamental components in natural language processing field. Starting from BERT, the community have made great and rapid progress on optimizing the pretrained language models, such as ERNIE (Sun et al., 2019a), XLNet (Yang et al., 2019), RoBERTa (Liu et al."
2020.findings-emnlp.58,L18-1431,1,0.872933,"or clarity, we do not list ‘ext’ models, where the other parameters are the same with the one that is not trained on extended data. 4.2 Setups for Fine-tuning Tasks To thoroughly test these pre-trained language models, we carried out extensive experiments on various natural language processing tasks, covering a wide spectrum of text length, i.e., from sentencelevel to document-level. Task details are shown in Table 2. Specifically, we choose the following eight popular Chinese datasets. 5 https://cloud.google.com/tpu/ • Machine Reading Comprehension (MRC): CMRC 2018 (Cui et al., 2019b), DRCD (Shao et al., 2018), CJRC (Duan et al., 2019). • Single Sentence Classification (SSC): ChnSentiCorp (Tan and Zhang, 2008), THUCNews (Li and Sun, 2007). • Sentence Pair Classification (SPC): XNLI (Conneau et al., 2018), LCQMC (Liu et al., 2018), BQ Corpus (Chen et al., 2018). In order to make a fair comparison, for each dataset, we keep the same hyper-parameters (such as maximum length, warm-up steps, etc.) and only tune the initial learning rate from 1e-5 to 5e-5 for each task. Note that the initial learning rates are tuned on original Chinese BERT, and it would be possible to achieve another gains by tuning the"
2020.semeval-1.43,D14-1181,0,0.00240485,"pervised fashion. Basically, each pre-trained language model is trained in a supervised fashion with labeled data. For unlabeled data, Pseudo-Labels, created by just picking up the class which has more votes from pre-trained language models, are used as if they were true labels. 2 Related Work Subtask 1 is a text classification task. Traditional machine learning systems, such as Naive Bayes (Domingos and Pazzani, 1997) and Support Vector Machines (Cortes and Vapnik, 1995) perform well in this task. In recent years, text classification has made breakthroughs in deep learning. A few of studies (Kim, 2014; Liu et al., 2016; Lai et al., 2015) made a series of improvements to the structure of deep neural networks. BERT (Devlin et al., 2018), a dynamic word vector based on the language model, has achieved very competitive performance in multiple domains of text classification. In addition, pre-trained language models such as XLNet (Yang et al., 2019), RoBERTa (Liu et al., 2019), ALBERT (Lan et al., 2019) have also achieved high performances in various NLP tasks. Subtask 2 is a sequence labeling task. Traditional work uses statistical approaches such as Hidden Markov Models (HMM) and Conditional R"
2020.semeval-1.43,N16-1030,0,0.0301246,"dels such as XLNet (Yang et al., 2019), RoBERTa (Liu et al., 2019), ALBERT (Lan et al., 2019) have also achieved high performances in various NLP tasks. Subtask 2 is a sequence labeling task. Traditional work uses statistical approaches such as Hidden Markov Models (HMM) and Conditional Random Fields (CRF) (Ratinov and Roth, 2009; Passos et al., 2014; Luo et al., 2015). Deep learning sequence labeling models utilize word-level Long ShortTerm Memory (LSTM) structures to represent global sequence information and a CRF layer to capture dependencies between neighboring labels (Huang et al., 2015; Lample et al., 2016; Ma and Hovy, 2016; Peters et al., 2017). In similar manner, fine-tuning the pre-trained model to complete the task of sequence labeling such as BERT has achieved excellent performances. 3 Methodology For two subtasks (i.e., text classification task and sequence labeling task), we start by fine-tuning pretrained language models, including BERT, RoBERTa and XLNet. Then, we enlarge the training set with pseudo-labeled sentences, which are predicted on the test set by voting ensemble of several single pre-trained models. Finally, we use the ensemble model to classify counterfactual statements fo"
2020.semeval-1.43,D15-1104,0,0.0193744,"ts to the structure of deep neural networks. BERT (Devlin et al., 2018), a dynamic word vector based on the language model, has achieved very competitive performance in multiple domains of text classification. In addition, pre-trained language models such as XLNet (Yang et al., 2019), RoBERTa (Liu et al., 2019), ALBERT (Lan et al., 2019) have also achieved high performances in various NLP tasks. Subtask 2 is a sequence labeling task. Traditional work uses statistical approaches such as Hidden Markov Models (HMM) and Conditional Random Fields (CRF) (Ratinov and Roth, 2009; Passos et al., 2014; Luo et al., 2015). Deep learning sequence labeling models utilize word-level Long ShortTerm Memory (LSTM) structures to represent global sequence information and a CRF layer to capture dependencies between neighboring labels (Huang et al., 2015; Lample et al., 2016; Ma and Hovy, 2016; Peters et al., 2017). In similar manner, fine-tuning the pre-trained model to complete the task of sequence labeling such as BERT has achieved excellent performances. 3 Methodology For two subtasks (i.e., text classification task and sequence labeling task), we start by fine-tuning pretrained language models, including BERT, RoBE"
2020.semeval-1.43,P16-1101,0,0.0215048,"ang et al., 2019), RoBERTa (Liu et al., 2019), ALBERT (Lan et al., 2019) have also achieved high performances in various NLP tasks. Subtask 2 is a sequence labeling task. Traditional work uses statistical approaches such as Hidden Markov Models (HMM) and Conditional Random Fields (CRF) (Ratinov and Roth, 2009; Passos et al., 2014; Luo et al., 2015). Deep learning sequence labeling models utilize word-level Long ShortTerm Memory (LSTM) structures to represent global sequence information and a CRF layer to capture dependencies between neighboring labels (Huang et al., 2015; Lample et al., 2016; Ma and Hovy, 2016; Peters et al., 2017). In similar manner, fine-tuning the pre-trained model to complete the task of sequence labeling such as BERT has achieved excellent performances. 3 Methodology For two subtasks (i.e., text classification task and sequence labeling task), we start by fine-tuning pretrained language models, including BERT, RoBERTa and XLNet. Then, we enlarge the training set with pseudo-labeled sentences, which are predicted on the test set by voting ensemble of several single pre-trained models. Finally, we use the ensemble model to classify counterfactual statements for subtask 1 and uti"
2020.semeval-1.43,W14-1609,0,0.0147325,"series of improvements to the structure of deep neural networks. BERT (Devlin et al., 2018), a dynamic word vector based on the language model, has achieved very competitive performance in multiple domains of text classification. In addition, pre-trained language models such as XLNet (Yang et al., 2019), RoBERTa (Liu et al., 2019), ALBERT (Lan et al., 2019) have also achieved high performances in various NLP tasks. Subtask 2 is a sequence labeling task. Traditional work uses statistical approaches such as Hidden Markov Models (HMM) and Conditional Random Fields (CRF) (Ratinov and Roth, 2009; Passos et al., 2014; Luo et al., 2015). Deep learning sequence labeling models utilize word-level Long ShortTerm Memory (LSTM) structures to represent global sequence information and a CRF layer to capture dependencies between neighboring labels (Huang et al., 2015; Lample et al., 2016; Ma and Hovy, 2016; Peters et al., 2017). In similar manner, fine-tuning the pre-trained model to complete the task of sequence labeling such as BERT has achieved excellent performances. 3 Methodology For two subtasks (i.e., text classification task and sequence labeling task), we start by fine-tuning pretrained language models, i"
2020.semeval-1.43,P17-1161,0,0.0132424,"RoBERTa (Liu et al., 2019), ALBERT (Lan et al., 2019) have also achieved high performances in various NLP tasks. Subtask 2 is a sequence labeling task. Traditional work uses statistical approaches such as Hidden Markov Models (HMM) and Conditional Random Fields (CRF) (Ratinov and Roth, 2009; Passos et al., 2014; Luo et al., 2015). Deep learning sequence labeling models utilize word-level Long ShortTerm Memory (LSTM) structures to represent global sequence information and a CRF layer to capture dependencies between neighboring labels (Huang et al., 2015; Lample et al., 2016; Ma and Hovy, 2016; Peters et al., 2017). In similar manner, fine-tuning the pre-trained model to complete the task of sequence labeling such as BERT has achieved excellent performances. 3 Methodology For two subtasks (i.e., text classification task and sequence labeling task), we start by fine-tuning pretrained language models, including BERT, RoBERTa and XLNet. Then, we enlarge the training set with pseudo-labeled sentences, which are predicted on the test set by voting ensemble of several single pre-trained models. Finally, we use the ensemble model to classify counterfactual statements for subtask 1 and utilize a CRF model to pe"
2020.semeval-1.43,W09-1119,0,0.0177257,"Lai et al., 2015) made a series of improvements to the structure of deep neural networks. BERT (Devlin et al., 2018), a dynamic word vector based on the language model, has achieved very competitive performance in multiple domains of text classification. In addition, pre-trained language models such as XLNet (Yang et al., 2019), RoBERTa (Liu et al., 2019), ALBERT (Lan et al., 2019) have also achieved high performances in various NLP tasks. Subtask 2 is a sequence labeling task. Traditional work uses statistical approaches such as Hidden Markov Models (HMM) and Conditional Random Fields (CRF) (Ratinov and Roth, 2009; Passos et al., 2014; Luo et al., 2015). Deep learning sequence labeling models utilize word-level Long ShortTerm Memory (LSTM) structures to represent global sequence information and a CRF layer to capture dependencies between neighboring labels (Huang et al., 2015; Lample et al., 2016; Ma and Hovy, 2016; Peters et al., 2017). In similar manner, fine-tuning the pre-trained model to complete the task of sequence labeling such as BERT has achieved excellent performances. 3 Methodology For two subtasks (i.e., text classification task and sequence labeling task), we start by fine-tuning pretrain"
2020.semeval-1.43,2020.semeval-1.40,0,0.106702,"rning systems’ lack of understanding of causal relations is perhaps the biggest roadblock to giving them human-level intelligence (Pearl, 2019). Causation is a fundamental concept in human thinking and reasoning, which indicates a special semantic relation between the cause with the effect (Stukker et al., 2008). Pearl and Mackenzie (2018) propose that there are three levels of causation, and the top level is the counterfactual analysis. Counterfactual statements describe events that did not actually happen or cannot happen, as well as the possible consequence if the events have had happened (Yang et al., 2020). SemEval 2020 Task 5 consists of two subtasks which aim to detect counterfactual descriptions in sentences, this paper presents solutions to both of two subtasks. Subtask1 — Detecting counterfactual statements refers to determining whether a given statement is counterfactual or not in several domains. For example, given the sentence: “Her post-traumatic stress could have been avoided if a combination of paroxetine and exposure therapy had been prescribed two months earlier.”, the system is required to determine whether it is a counterfactual statement or not. This categorization task is evalu"
2021.acl-long.117,2020.tacl-1.28,0,0.0310816,"nowledge (Feng et al., 2020a). In this paper, we combine three types of auxiliary information to help better modeling dialogues, including keywords, redundant utterances and topics. Pre-trained Language Models Pre-trained models such as BERT (Devlin et al., 2019) and GPT-3 (Brown et al., 2020) have advanced various NLP tasks. On one hand, some works utilized the knowledge contained in pre-trained models by finetuning on supervised data of downstream tasks (Qin et al., 2019; Liu and Lapata, 2019; Qin et al., 2020). On the other hand, some works examined the knowledge in an unsupervised manner (Jiang et al., 2020; Xiao et al., 2020; Lin et al., 2020). Ku1486 Rob : Hey there , what's up ? Bob : Not much , watching the game . You ? [Topic 1] Rob : Same . Having a few people over . Rob : But the game is boring as fuck lol . That's why I'm writing Bob : Yeah , true that Rob : Any plans for the weekend ? Bob : Most likely the usual run some errands , cook some food , go out for a few beers . Nothing super interesting have appeared yet Rob : I've heard that Jim is planning to celebrate his birthday [Topic 2] Bob : Oh right , his birthday is like next Wednesday ? Rob : Yeah , normally that would make the nex"
2021.acl-long.117,2020.coling-main.499,0,0.10487,"erate a succinct summary while retaining essential information of the dialogue (Gurevych and Strube, 2004; Chen and Yang, 2020). Theoretically, Peyrard (2019) point out that a good summary is intuitively related to three aspects, including Informativeness, Redundancy and Relevance. To this end, previous works have taken the above three aspects into account by incorporating auxiliary annotations into the dialogue. To improve informativeness, some works annotated linguistically specific words (e.g., nouns and verbs), domain terminologies and topic words in the dialogue (Riedhammer et al., 2008; Koay et al., 2020; Zhao et al., 2020). To reduce redundancy, some works ∗ Corresponding author. Our codes are available at: https://github.com/ xcfcode/PLM_annotator 1 used sentence similarity-based methods to annotate redundant utterances. (Zechner, 2002; Murray et al., 2005). To improve relevance, some works annotated topics for the dialogue (Li et al., 2019; Liu et al., 2019; Chen and Yang, 2020). However, these annotations are usually obtained via open-domain toolkits, which are not suitable for dialogues, or require manual annotations, which are labor-consuming. To alleviate the above problem, we explore"
2021.acl-long.117,2020.lifelongnlp-1.3,0,0.0604519,"Missing"
2021.acl-long.117,2020.acl-main.703,0,0.336616,"Topic Segmentation aims to divide a dialogue into topically coherent segments (shown in Figure 1(c)). Our DialoGPT annotator inserts a topic segmentation point before one utterance if it is unpredictable. We assume that if an utterance is difficult to be inferred from the dialogue context based on DialoGPT, this utterance may belong to a new topic. We use our DialoGPT annotator to annotate the SAMSum (Gliwa et al., 2019) and AMI (Carletta et al., 2005) datasets. Each annotation is converted into a specific identifier and we insert them into the dialogue text. Then, we employ pre-traind BART (Lewis et al., 2020) and non pre-trained PGN (See et al., 2017) as our summarizers. Extensive experimental results show that our method can obtain consistent and remarkable improvements over strong baselines on both datasets and achieves new stateof-the-art performance on the SAMSum dataset. 2 Preliminaries In this section, we will describe the task definition as well as the background of DialoGPT. 2.1 Task Definition Given an input dialogue D, a dialogue summarizer aims to produce a condensed summary S, where D consists of |D |utterances [u1 , u2 , ...u|D |] and S consists of |S |words [s1 , s2 , ...s|S |]. Each"
2021.acl-long.117,P19-1210,0,0.224025,"nto account by incorporating auxiliary annotations into the dialogue. To improve informativeness, some works annotated linguistically specific words (e.g., nouns and verbs), domain terminologies and topic words in the dialogue (Riedhammer et al., 2008; Koay et al., 2020; Zhao et al., 2020). To reduce redundancy, some works ∗ Corresponding author. Our codes are available at: https://github.com/ xcfcode/PLM_annotator 1 used sentence similarity-based methods to annotate redundant utterances. (Zechner, 2002; Murray et al., 2005). To improve relevance, some works annotated topics for the dialogue (Li et al., 2019; Liu et al., 2019; Chen and Yang, 2020). However, these annotations are usually obtained via open-domain toolkits, which are not suitable for dialogues, or require manual annotations, which are labor-consuming. To alleviate the above problem, we explore the pre-trained language model as an unsupervised annotator to automatically provide annotations for the dialogue. Recently, some works have investigated the use of pre-trained language models in an unsupervised manner. For example, Sainz and Rigau (2021) exploited pre-trained models for assigning domain labels to WordNet synsets. The successf"
2021.acl-long.117,2020.emnlp-main.557,0,0.0207306,"paper, we combine three types of auxiliary information to help better modeling dialogues, including keywords, redundant utterances and topics. Pre-trained Language Models Pre-trained models such as BERT (Devlin et al., 2019) and GPT-3 (Brown et al., 2020) have advanced various NLP tasks. On one hand, some works utilized the knowledge contained in pre-trained models by finetuning on supervised data of downstream tasks (Qin et al., 2019; Liu and Lapata, 2019; Qin et al., 2020). On the other hand, some works examined the knowledge in an unsupervised manner (Jiang et al., 2020; Xiao et al., 2020; Lin et al., 2020). Ku1486 Rob : Hey there , what's up ? Bob : Not much , watching the game . You ? [Topic 1] Rob : Same . Having a few people over . Rob : But the game is boring as fuck lol . That's why I'm writing Bob : Yeah , true that Rob : Any plans for the weekend ? Bob : Most likely the usual run some errands , cook some food , go out for a few beers . Nothing super interesting have appeared yet Rob : I've heard that Jim is planning to celebrate his birthday [Topic 2] Bob : Oh right , his birthday is like next Wednesday ? Rob : Yeah , normally that would make the next weekend a good time but he is going"
2021.acl-long.117,D19-1387,0,0.018767,"). Other works also explored dialogue act (Goo and Chen, 2018), dialogue discourse (Feng et al., 2020b) and commonsense knowledge (Feng et al., 2020a). In this paper, we combine three types of auxiliary information to help better modeling dialogues, including keywords, redundant utterances and topics. Pre-trained Language Models Pre-trained models such as BERT (Devlin et al., 2019) and GPT-3 (Brown et al., 2020) have advanced various NLP tasks. On one hand, some works utilized the knowledge contained in pre-trained models by finetuning on supervised data of downstream tasks (Qin et al., 2019; Liu and Lapata, 2019; Qin et al., 2020). On the other hand, some works examined the knowledge in an unsupervised manner (Jiang et al., 2020; Xiao et al., 2020; Lin et al., 2020). Ku1486 Rob : Hey there , what's up ? Bob : Not much , watching the game . You ? [Topic 1] Rob : Same . Having a few people over . Rob : But the game is boring as fuck lol . That's why I'm writing Bob : Yeah , true that Rob : Any plans for the weekend ? Bob : Most likely the usual run some errands , cook some food , go out for a few beers . Nothing super interesting have appeared yet Rob : I've heard that Jim is planning to celebrate his"
2021.acl-long.117,W04-3252,0,0.0159113,"he first-ranked and second-ranked results respectively. 4.3 R-L 15.70 13.91 13.72 25.47† 24.00 23.49 24.11 24.27 24.05 24.59†† Table 3: Test set results on the AMI dataset. PGN(DK E ), PGN(DR D ) and PGN(DT S ) represent training PGN on the AMI with keywords, redundancy and topic annotation respectively. SAMSum Model BS BART 86.91 MV-BART 88.46 BART(DA LL ) 90.04 AMI Model BS PGN 80.51 HMNet 82.24 PGN(DA LL ) 82.76 Table 4: Test set results on the SAMSum and AMI. “BS” is short for BERTScore. Baselines and Metrics For SAMSum, LONGEST-3 views the first three utterances as the summary. TextRank (Mihalcea and Tarau, 2004) is a traditional graph-based method. Transformer (Vaswani et al., 2017) is a seq2seq method based on full self-attention operations. D-HGN (Feng et al., 2020a) incorporates commonsense knowledge to help understand dialogues. TGDGA (Zhao et al., 2020) uses topic words and models graph structures for dialogues. DialoGPT (Zhang et al., 2020b) means that finetuning DialoGPT on the SAMSum. MV-BART (Chen and Yang, 2020) is a BART-based method that incorporates topic and stage information. For AMI, SummaRunner (Nallapati et al., 2017) is an extractive method based on hierarchical RNN network. UNS (S"
2021.acl-long.117,K16-1028,0,0.0746752,"Missing"
2021.acl-long.117,D18-1206,0,0.0243061,"oGPT emb 53.14 27.25 49.42 Ours DialoGPTK E 53.43 28.03 49.93 Table 5: Human evaluation results. “Info.” is short for informativeness, “Conc.” for conciseness, “Cov.” for coverage. For SAMSum, the inter-annotator agreement (Fleiss’ kappa) scores for each metric are 0.46, 0.37 and 0.43 respectively. For AMI, Fleiss’ kappa scores are 0.48, 0.40 and 0.41 respectively. 4.5 Table 6: Test set results of fine-tuning BART on the SAMSum that is annotated with keywords using various methods. Entities, nouns and verbs are obtained by Qi et al. (2020). Topic words are obtained by a pre-trained LDA model (Narayan et al., 2018). KeyBERT (Grootendorst, 2020) leverages pre-trained language model embeddings to create keywords. Method TextRank Entities DialoGPTK E Human Evaluation We conduct a human evaluation of the dialogue summary to assess its informativeness, conciseness and coverage. Informativeness measures how well the summary includes key information. Conciseness measures how well the summary discards the redundant information. Coverage measures how well the summary covers each part of the dialogue. We randomly sample 100 dialogues (SAMSum) and 10 meetings (AMI) with corresponding generated summaries to conduct"
2021.acl-long.117,P19-1101,0,0.020159,"ge of dialogue background knowledge encoded in DialoGPT. We apply DialoGPT to label three types of features on two dialogue summarization datasets, SAMSum and AMI, and employ pre-trained and non pre-trained models as our summarizers. Experimental results show that our proposed method can obtain remarkable improvements on both datasets and achieves new state-of-theart performance on the SAMSum dataset1 . 1 Introduction Dialogue summarization aims to generate a succinct summary while retaining essential information of the dialogue (Gurevych and Strube, 2004; Chen and Yang, 2020). Theoretically, Peyrard (2019) point out that a good summary is intuitively related to three aspects, including Informativeness, Redundancy and Relevance. To this end, previous works have taken the above three aspects into account by incorporating auxiliary annotations into the dialogue. To improve informativeness, some works annotated linguistically specific words (e.g., nouns and verbs), domain terminologies and topic words in the dialogue (Riedhammer et al., 2008; Koay et al., 2020; Zhao et al., 2020). To reduce redundancy, some works ∗ Corresponding author. Our codes are available at: https://github.com/ xcfcode/PLM_an"
2021.acl-long.117,2020.acl-demos.14,0,0.0445304,"guage Model-Based Methods KeyBERT w/ BERT emb 52.39 27.14 48.52 w/ DialoGPT emb 53.14 27.25 49.42 Ours DialoGPTK E 53.43 28.03 49.93 Table 5: Human evaluation results. “Info.” is short for informativeness, “Conc.” for conciseness, “Cov.” for coverage. For SAMSum, the inter-annotator agreement (Fleiss’ kappa) scores for each metric are 0.46, 0.37 and 0.43 respectively. For AMI, Fleiss’ kappa scores are 0.48, 0.40 and 0.41 respectively. 4.5 Table 6: Test set results of fine-tuning BART on the SAMSum that is annotated with keywords using various methods. Entities, nouns and verbs are obtained by Qi et al. (2020). Topic words are obtained by a pre-trained LDA model (Narayan et al., 2018). KeyBERT (Grootendorst, 2020) leverages pre-trained language model embeddings to create keywords. Method TextRank Entities DialoGPTK E Human Evaluation We conduct a human evaluation of the dialogue summary to assess its informativeness, conciseness and coverage. Informativeness measures how well the summary includes key information. Conciseness measures how well the summary discards the redundant information. Coverage measures how well the summary covers each part of the dialogue. We randomly sample 100 dialogues (SAM"
2021.acl-long.117,P17-1099,0,0.43215,"into topically coherent segments (shown in Figure 1(c)). Our DialoGPT annotator inserts a topic segmentation point before one utterance if it is unpredictable. We assume that if an utterance is difficult to be inferred from the dialogue context based on DialoGPT, this utterance may belong to a new topic. We use our DialoGPT annotator to annotate the SAMSum (Gliwa et al., 2019) and AMI (Carletta et al., 2005) datasets. Each annotation is converted into a specific identifier and we insert them into the dialogue text. Then, we employ pre-traind BART (Lewis et al., 2020) and non pre-trained PGN (See et al., 2017) as our summarizers. Extensive experimental results show that our method can obtain consistent and remarkable improvements over strong baselines on both datasets and achieves new stateof-the-art performance on the SAMSum dataset. 2 Preliminaries In this section, we will describe the task definition as well as the background of DialoGPT. 2.1 Task Definition Given an input dialogue D, a dialogue summarizer aims to produce a condensed summary S, where D consists of |D |utterances [u1 , u2 , ...u|D |] and S consists of |S |words [s1 , s2 , ...s|S |]. Each utterance ui is compose of a sequence of w"
2021.acl-long.117,P18-1062,0,0.0509974,"Missing"
2021.acl-long.117,2020.emnlp-main.293,0,0.014839,"., 2020a). In this paper, we combine three types of auxiliary information to help better modeling dialogues, including keywords, redundant utterances and topics. Pre-trained Language Models Pre-trained models such as BERT (Devlin et al., 2019) and GPT-3 (Brown et al., 2020) have advanced various NLP tasks. On one hand, some works utilized the knowledge contained in pre-trained models by finetuning on supervised data of downstream tasks (Qin et al., 2019; Liu and Lapata, 2019; Qin et al., 2020). On the other hand, some works examined the knowledge in an unsupervised manner (Jiang et al., 2020; Xiao et al., 2020; Lin et al., 2020). Ku1486 Rob : Hey there , what's up ? Bob : Not much , watching the game . You ? [Topic 1] Rob : Same . Having a few people over . Rob : But the game is boring as fuck lol . That's why I'm writing Bob : Yeah , true that Rob : Any plans for the weekend ? Bob : Most likely the usual run some errands , cook some food , go out for a few beers . Nothing super interesting have appeared yet Rob : I've heard that Jim is planning to celebrate his birthday [Topic 2] Bob : Oh right , his birthday is like next Wednesday ? Rob : Yeah , normally that would make the next weekend a good ti"
2021.acl-long.117,J02-4003,0,0.535386,"formativeness, Redundancy and Relevance. To this end, previous works have taken the above three aspects into account by incorporating auxiliary annotations into the dialogue. To improve informativeness, some works annotated linguistically specific words (e.g., nouns and verbs), domain terminologies and topic words in the dialogue (Riedhammer et al., 2008; Koay et al., 2020; Zhao et al., 2020). To reduce redundancy, some works ∗ Corresponding author. Our codes are available at: https://github.com/ xcfcode/PLM_annotator 1 used sentence similarity-based methods to annotate redundant utterances. (Zechner, 2002; Murray et al., 2005). To improve relevance, some works annotated topics for the dialogue (Li et al., 2019; Liu et al., 2019; Chen and Yang, 2020). However, these annotations are usually obtained via open-domain toolkits, which are not suitable for dialogues, or require manual annotations, which are labor-consuming. To alleviate the above problem, we explore the pre-trained language model as an unsupervised annotator to automatically provide annotations for the dialogue. Recently, some works have investigated the use of pre-trained language models in an unsupervised manner. For example, Sainz"
2021.acl-long.117,2020.acl-demos.30,0,0.436249,"oring DialoGPT for Dialogue Summarization Xiachong Feng1 , Xiaocheng Feng1,2∗, Libo Qin1 , Bing Qin1,2 , Ting Liu1,2 1 Harbin Institute of Technology, China 2 Peng Cheng Laboratory, China {xiachongfeng,xcfeng,lbqin,bqin,tliu}@ir.hit.edu.cn Abstract Current dialogue summarization systems usually encode the text with a number of general semantic features (e.g., keywords and topics) to gain more powerful dialogue modeling capabilities. However, these features are obtained via open-domain toolkits that are dialogagnostic or heavily relied on human annotations. In this paper, we show how DialoGPT (Zhang et al., 2020b), a pre-trained model for conversational response generation, can be developed as an unsupervised dialogue annotator, which takes advantage of dialogue background knowledge encoded in DialoGPT. We apply DialoGPT to label three types of features on two dialogue summarization datasets, SAMSum and AMI, and employ pre-trained and non pre-trained models as our summarizers. Experimental results show that our proposed method can obtain remarkable improvements on both datasets and achieves new state-of-theart performance on the SAMSum dataset1 . 1 Introduction Dialogue summarization aims to generate"
2021.acl-long.117,2020.findings-emnlp.19,0,0.0899951,"ng et al., 2020a) incorporates commonsense knowledge to help understand dialogues. TGDGA (Zhao et al., 2020) uses topic words and models graph structures for dialogues. DialoGPT (Zhang et al., 2020b) means that finetuning DialoGPT on the SAMSum. MV-BART (Chen and Yang, 2020) is a BART-based method that incorporates topic and stage information. For AMI, SummaRunner (Nallapati et al., 2017) is an extractive method based on hierarchical RNN network. UNS (Shang et al., 2018) is a fully unsupervised and graph-based method. TopicSeg (Li et al., 2019) incorporates topics to model the meeting. HMNet (Zhu et al., 2020) is a transformer-based method that incorporates POS and entity information and is pre-trained on news summarization dataset. We adopt ROUGE (Lin, 2004) and BERTScore (Zhang et al., 2020a) for evaluating our models. 4.4 Automatic Evaluation The results on SAMSum and AMI are shown in Table 2 and 3 respectively. We can see that using our annotated datasets DK E , DR D and DT S , both BART and PGN can obtain improvements. Furthermore, our BART(DA LL ) achieves SOTA performance. For SAMSum, it’s worth noting that BART(DK E ) performs better compared with BART(DR D ) and BART(DT S ). We attribute t"
2021.acl-long.183,D18-1307,0,0.0453628,"Missing"
2021.acl-long.183,N19-1423,0,0.00508925,"ρj = {r1j ∧, · · · , ∧rljj } describes a serial of transitive causal logical rules starting from the cause event C and ending at the effect event E. Considering that each rule rkj ∈ ρj is composed by two events ejk−1 and ejk , a causal logic chain ρj with lj rules contains totally lj + 1 events {ej0 , · · · , ejlj } , where ej0 and ejlj are the cause event C and the effect event E, respectively. Taking C and E as the start and end point respectively, we can enumerate all distinct causal logic chains in the CLG using a Depth First Searching algorithm. 3.2.3 Event Encoding A BERT-based encoder (Devlin et al., 2019) is employed to encode all events within each causal logic chain into chain-specific distributed embeddings. Specifically, for a causal logic chain ρj containing lj+1 events {ej0 , · · · , ejlj }, we first process the event sequence into the form of: [CLS] ej0 · · · [CLS] ejk · · · [CLS] ejlj . After that, the processed event sequence is fed into BERT. We define the final hidden state of the [CLS] token before each event as the representation of the corresponding event. In this way, we obtain an event embedding set H = {hj0 , · · · , hjlj }, where hjk ∈ Rd is the embedding of the kth event wit"
2021.acl-long.183,N18-2017,0,0.021401,"commonly expressed by humans in the text of natural language, and is of great value for various Artificial Intelligence applications, such as question answering (Oh et al., 2013), event prediction (Li et al., 2018), and decision making (Sun et al., 2018). Previous work mainly learns causal knowledge from manually annotated causal event pairs, and achieves promising performances (Luo et al., 2016; ∗ Corresponding author Xie and Mu, 2019a; Li et al., 2019). However, recent works have questioned the seemingly superb performance for some of these studies (McCoy et al., 2019; Poliak et al., 2018; Gururangan et al., 2018). Specifically, training data may contain exploitable superficial cues that are correlative of the expected output. The main concern is that these works have not learned the underlying mechanism of causation so that their inference models are not stable enough and their results are not explainable. While we notice that there is plentiful evidence information outside the given corpus that can provide more clues for understanding the logical law of the causality. Figure 1 (a) exemplifies two clues I1 : Excess Liquidity and I2 : Invest Demand Increase for explaining how a: Quantitative Easing gra"
2021.acl-long.183,P19-1475,0,0.0272946,"Missing"
2021.acl-long.183,P19-1334,0,0.0388484,"Missing"
2021.acl-long.183,D19-1187,0,0.0591499,"Missing"
2021.acl-long.183,P13-1170,0,0.0838456,"Missing"
2021.acl-long.183,N18-1202,0,0.0101493,"and-crafted features; 2) MLN cannot model the influence of antecedents of rules. Different from MLN, in this paper, we propose a Conditional Markov Neural Logic Network, which works on the embedding space of logic rules to model the conditional causal strength of rules. 3 3.1 Given an event pair hC, Ei outside the causal event graph, to obtain the evidences from the CEG, we first locate the cause and effect in the CEG. Intuitively, semantically similar events would have similar causes and effects, and share similar locations in the CEG. To this end, we employ a pretrained language model ELMo (Peters et al., 2018) to derive the semantic representation for events in the CEG, as well as the cause and effect event. Then events in the CEG which are semantically similar to the input cause and effect event can be found using cosine similarity of the semantic representations. These events can serve as anchors for locating the cause and effect event. Then as Figure 2 shows, taking the anchors of the cause event as start points, and taking the anchors of the effect event as end points, the evidence events can be retrieved by a Breadth First Search (BFS) algorithm. After the retrieving process, the cause, effect"
2021.acl-long.183,W18-5441,0,0.0639778,"Missing"
2021.acl-long.183,C18-1069,0,0.0609692,"Missing"
2021.acl-long.183,N18-1089,0,0.055871,"Missing"
2021.acl-long.403,D17-1070,0,0.244557,"sample 300,000 positive and 300,000 negative instances from the auxiliary dataset. Then given an event pair (Vi , Vj ), the finetuned RoBERTa-large model would be able to predict the probability that Vj is the subsequent event of Vi . Event Graph Based Pseudo Instance Set for Pretraining ege-RoBERTa To effectively utilize the event graph knowledge, we induce a set of pseudo instances for pretraining the ege-RoBERTa model. Specifically, given a five-sentence-story within the auxiliary dataset, as Table 1 shows, we define the 1st and 5th sentence of the story as two 5185 Methods SVM Infersent (Conneau et al., 2017) GPT (Radford et al., 2018) BERT-base (Devlin et al., 2019) RoBERTa-base (Liu et al., 2019) BERT-large (Devlin et al., 2019) RoBERTa-large (Liu et al., 2019) Concurrent Methods L2 R (Zhu et al., 2020) RoBERTa-GPT-MHKA (Paul et al., 2020) This Work ege-RoBERTa-largeu ege-RoBERTa-largeλ=0 ege-RoBERTa-base ege-RoBERTa-large Human Performance observed events, the 3rd sentence as the hypothesis event, the 2nd and 4th sentence as intermediary events, respectively. In this way, the posterior event sequence X 0 and the event sequence X of a pseudo instance could be obtained. In addition, given X 0 , w"
2021.acl-long.403,N19-1423,0,0.565079,"f natural language in a formal logic system. To facilitate this, Bhagavatula et al. (2019) proposed a natural language based abductive reasoning task αNLI. As shown in Figure 1 (a), given two observed events O1 and O2 , the αNLI task requires the prediction model to choose a more reasonable explanation from two candidate hypothesis events H1 and H2 . Both observed events and hypothesis events are daily-life events, and are described in natural language. Together with the αNLI task, Bhagavatula et al. (2019) also explored conducting such reasoning using pretrained language models such as BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019). However, despite pretrained language models could capture rich linguistic knowledge benefit for understanding the semantics of events, additional commonsense knowledge is still necessary for the abductive reasoning. For example, as illustrated in Figure 1 (b), given observations O1 and O2 , to choose the more likely explanation H1 : A thief 5181 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 5181–5190 August 1–6, 2021. ©2021 Association for"
2021.acl-long.403,N16-1147,0,0.0380804,") A Pseudo Instance={X, X’, A}, where 0 X = (O1 , H1 , O2 ); X = (O1 , I1 , H1 , I2 , O2 ) A is initialized from the event graph. Optimizing Experiments 5.1 αNLI Dataset The αNLI dataset (Bhagavatula et al., 2019) consists of 169,654, 1,532 and 4,056 hO1 , O2 , H1 , H2 i Construction of Event Graph The event graph serves as an external knowledge base to provide information about the relationship between observation events and intermediary events. To this end, we build the event graph based on an auxiliary dataset, which are composed of two short story corpora independent to αNLI, i.e., VIST (Huang et al., 2016), and TimeTravel (Qin et al., 2019). Both VIST and TimeTravel are composed of five-sentences short stories. Totally there are 121,326 stories in the auxiliary dataset. To construct the event graph, we define each sentence in the auxiliary dataset as a node in the event graph. To get the edge weight Wij between two nodes Vi and Vj (i.e., the probability that Vj is the subsequent event of Vi ), we finetune a RoBERTa-large model through a next sentence prediction task. Specifically, we define adjacent sentence pairs in the story text (for example, [1st, 2nd] sentence, [4th, 5th] sentence of a sto"
2021.acl-long.403,2021.ccl-1.108,0,0.0667599,"Missing"
2021.acl-long.403,N16-1098,0,0.0213487,"he pre-training stage ege-RoBERTa is trained to predict the masked tokens in the event sequence X rather than the relatedness score. In addition, in order to balance the masked token prediction loss with the KL term, we introduce an additional hyperparameter λ. Hence, the objective function in the pretraining stage is defined as follows: LELBO (θ, φ) =Eq(z|X 0 ,A) logLM LM (X, z; θ) − λKL(qφ (z|X 0 , A)||pθ (z|X)), L(θ) = pθ (Y |z, X) = pθ (Y |z, X)pθ (z|X). quadruples in training, development and test set, respectively. The observation events are collected from a short story corpus ROCstory (Mostafazadeh et al., 2016), while all of hypothesis events are independently generated through crowdsourcing. 5.2 (15) Training Details We implement two different sizes of ege-RoBERTa model (i.e. ege-RoBERTa-base and ege-RoBERTalarge) based on RoBERTa-base framework and RoBERTa-large framework, respectively. For the ege-RoBERTa-base model, in the aggregator, the prior network, the recognition network and the merger, the dimension of the attention mechanism d is set as 768, and all multi-head attention layers contain 12 heads. While for the ege-RoBERTa-large model, d is equal to 1024 and all multi-head attention layers"
2021.acl-long.403,P13-1170,0,0.0684874,"Missing"
2021.acl-long.403,2020.findings-emnlp.267,0,0.0373436,"Missing"
2021.acl-long.403,D19-1509,0,0.0354924,"Missing"
2021.emnlp-main.265,D14-1179,0,0.0481335,"Missing"
2021.emnlp-main.265,J82-2005,0,0.621449,"Missing"
2021.emnlp-main.265,D18-1048,1,0.842716,"to improve Gehring et al., 2017; Vaswani et al., 2017). In- NAT by repeatedly refining previously generated stead of sequential decoding as in the autoregres- translation. Instead of enforcing NAT to generate sive translation (AT), non-autoregressive neural ma- accurate translation by one-pass decoding, these chine translation (NAT) (Gu et al., 2018; Guo et al., approaches are expected to revise incorrect transla2019; Ma et al., 2019; Wei et al., 2019; Sun et al., tion pieces through several refinements (Xia et al., 1 Our code is publicly available at https://github. 2017; Zhang et al., 2018; Geng et al., 2018). With com/xwgeng/RewriteNAT. the introduction of iterative decoding, NAT further 3297 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 3297–3308 c November 7–11, 2021. 2021 Association for Computational Linguistics boosts translation quality, bridging performance gap between NAT and AT models. However, existing iterative NAT models expose the weakness in distinguishing the erroneous words. The dominant approach to identify the mistakes is mask-predict algorithm (Ghazvininejad et al., 2019; Guo et al., 2020b), which employs inefficient heuristic rul"
2021.emnlp-main.265,D19-1633,0,0.179366,"t R EWRITE NAT can achieve better performance while significantly reducing decoding time, compared with previous iterative decoding strategies. In particular, R EWRITE NAT can obtain competitive results with autoregressive translation on WMT14 En↔De, En→Fr and WMT16 Ro→En translation benchmarks1 . translation translation y1y1 yy2 2 yy3 3 y4y4 y5y5 Revisor Revisor yy1 1 [MASK] [MASK][MASK] [MASK] yy 55 4 4 yy Locator Locator Abstract Encoder Encoder xx1 1 xx2 2 xx3 3 xx4 4 (b) RewriteNAT Figure 1: Illustration of the difference in masking words between (a) conventional masked LM-based NAT (Ghazvininejad et al., 2019) and (b) our proposed R EWRITE NAT. Instead of using inefficient heuristic rules which perhaps mask correct words in some case (e.g., y1 y4 ), R EWRITE NAT utilizes an additional locator module to learn to explicitly distinguish erroneous translation pieces (e.g., yˆ2 yˆ3 ), annotated as special symbol (i.e., [MASK]). 2019; Ghazvininejad et al., 2020a; Zhou et al., 2020; Ding et al., 2021a,b) generates the whole target sentence simultaneously. To enable parallel decoding, NAT imposes a conditional independence assumption among words in target sentences, yielding significantly faster inference"
2021.emnlp-main.265,2020.acl-main.36,0,0.349652,"insic dependencies within target sentence are omitted, NAT suffers from severe inconsistency problem (Wang et al., 2019), leading to inferior translation quality, especially 1 Introduction when capturing highly multimodal distribution of State-of-the-art neural machine translation (NMT) target translations (Gu et al., 2018). systems use autoregressive decoding where the deTowards tackling above fundamental problem, coder generates a target sentence word by word, iterative decoding (Lee et al., 2018; Ghazvinineand the generation of the latter words depends on jad et al., 2019; Gu et al., 2019; Guo et al., 2020b; previously generated ones (Bahdanau et al., 2015; Ghazvininejad et al., 2020b) is proposed to improve Gehring et al., 2017; Vaswani et al., 2017). In- NAT by repeatedly refining previously generated stead of sequential decoding as in the autoregres- translation. Instead of enforcing NAT to generate sive translation (AT), non-autoregressive neural ma- accurate translation by one-pass decoding, these chine translation (NAT) (Gu et al., 2018; Guo et al., approaches are expected to revise incorrect transla2019; Ma et al., 2019; Wei et al., 2019; Sun et al., tion pieces through several refinemen"
2021.emnlp-main.265,D16-1139,0,0.0259063,"(2018), where newsdev2016 and newstest2016 are 2 https://www.statmt.org/wmt14 https://www.statmt.org/wmt16 4 https://www.statmt.org/wmt17 3 taken as development and test sets. For WMT17 En→Zh translation, we pre-process the dataset following Hassan et al. (2018). We treat newsdev2017 as the development set and newstest2017 as the test set. The datasets are tokenized into subword units using BPE (Sennrich et al., 2016). We evaluate performance with BLEU (Papineni et al., 2002) for all language pairs, except for En→Zh, where we use SacreBLEU (Post, 2018) 5 . Distillation Knowledge distillation (Kim and Rush, 2016; Hinton et al., 2015) is utilized to train the NAT models due to its effectiveness of alleviating multimodality (Gu et al., 2018) using the generated translation by T RANS FORMER (T RANSFORMER -B IG for WMT14 En↔De and En→Fr as well as WMT17 En→Zh, T RANSFORMER -BASE for WMT16 En↔Ro) as a substitute for target-side ground-truth (Ghazvininejad et al., 2019). Hyperparameters We follow most of the standard hyperparameters for T RANSFORMER 5 SacreBLEU hash: BLEU+case.mixed+lang.en-zh +numrefs.1+smooth.exp+test.wmt17+tok.zh+version.1.4.14 3301 BASE (Vaswani et al., 2017): 6 layers per stack, 8 att"
2021.emnlp-main.265,D18-1149,0,0.353714,"ption among words in target sentences, yielding significantly faster inference speed than AT. However, since intrinsic dependencies within target sentence are omitted, NAT suffers from severe inconsistency problem (Wang et al., 2019), leading to inferior translation quality, especially 1 Introduction when capturing highly multimodal distribution of State-of-the-art neural machine translation (NMT) target translations (Gu et al., 2018). systems use autoregressive decoding where the deTowards tackling above fundamental problem, coder generates a target sentence word by word, iterative decoding (Lee et al., 2018; Ghazvinineand the generation of the latter words depends on jad et al., 2019; Gu et al., 2019; Guo et al., 2020b; previously generated ones (Bahdanau et al., 2015; Ghazvininejad et al., 2020b) is proposed to improve Gehring et al., 2017; Vaswani et al., 2017). In- NAT by repeatedly refining previously generated stead of sequential decoding as in the autoregres- translation. Instead of enforcing NAT to generate sive translation (AT), non-autoregressive neural ma- accurate translation by one-pass decoding, these chine translation (NAT) (Gu et al., 2018; Guo et al., approaches are expected to r"
2021.emnlp-main.265,2020.emnlp-main.73,0,0.0391941,"Missing"
2021.emnlp-main.265,D19-1573,0,0.0344656,"Missing"
2021.emnlp-main.265,D18-1336,0,0.101007,"(1), while the remaining are mapped into keep (0): ( 1, if Yˆtl 6= Yt zt (Yˆ l ) = (11) 0, otherwise 4.2 Inference During training R EWRITE NAT generates the translations with same length as the ground-truth, while in inference we apply R EWRITE NAT over a sequence of “[MASK]” with a length predicted by length classifier (Lee et al., 2018). When locator (9) module classifies entire sentence into keep or the classifications of two consecutive refinements keep the same, decoding stops (a.k.a dynamic halting). 3300 Model T RANSFORMER (Vaswani et al., 2017) NAT w/ Fertility (Gu et al., 2018) CTC (Libovický and Helcl, 2018) NAT-REG (Wang et al., 2019) Imitate-NAT (Wei et al., 2019) Flowseq (Ma et al., 2019) Hint-NAT (Li et al., 2019) NAT-DCRF (Sun et al., 2019) Bag-of-ngram (Shao et al., 2020) FCL-NAT (Guo et al., 2020a) TCL-NAT (Liu et al., 2020) EM+ODD (Sun and Yang, 2020) AXE (Ghazvininejad et al., 2020a) iNAT (Lee et al., 2018) InsT (Stern et al., 2019) CMLM (Ghazvininejad et al., 2019) LevT (Gu et al., 2019) LaNMT (Shu et al., 2020) SMART (Ghazvininejad et al., 2020b) JM-NAT (Guo et al., 2020b) DisCo (Kasai et al., 2020) O UR PROPOSED R EWRITE NAT En→De Iters. BLEU N 27.82 1 19.17 1 17.68 1 24.61 1 24.15 1"
2021.emnlp-main.265,D19-1437,0,0.188536,"n of the latter words depends on jad et al., 2019; Gu et al., 2019; Guo et al., 2020b; previously generated ones (Bahdanau et al., 2015; Ghazvininejad et al., 2020b) is proposed to improve Gehring et al., 2017; Vaswani et al., 2017). In- NAT by repeatedly refining previously generated stead of sequential decoding as in the autoregres- translation. Instead of enforcing NAT to generate sive translation (AT), non-autoregressive neural ma- accurate translation by one-pass decoding, these chine translation (NAT) (Gu et al., 2018; Guo et al., approaches are expected to revise incorrect transla2019; Ma et al., 2019; Wei et al., 2019; Sun et al., tion pieces through several refinements (Xia et al., 1 Our code is publicly available at https://github. 2017; Zhang et al., 2018; Geng et al., 2018). With com/xwgeng/RewriteNAT. the introduction of iterative decoding, NAT further 3297 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 3297–3308 c November 7–11, 2021. 2021 Association for Computational Linguistics boosts translation quality, bridging performance gap between NAT and AT models. However, existing iterative NAT models expose the weakness in distinguishing t"
2021.emnlp-main.265,P14-5010,0,0.00287773,"lation. the locator module, automatically. In addition, the As an early alternative, Lee et al. (2018) corrected erroneous translation pieces (e.g., “are children the original non-autoregressive output by passing children”) can be accurately distinguished. In conit multiple times through a denoising autoencoder. trast, strong CMLM baseline shows their weakInstead of generating in discrete space of sentences, ness at tackling the incorrect ones. Consequently, continuous latent variables were utilized to improve 6 iterative refinements (Shu et al., 2020; Lee et al., S TANFORD C ORE NLP TOOLKIT (Manning et al., 2014) is utilized to annotate translation output with Part-of-Speechs. 2020). Subsequently, Ghazvininejad et al. (2019) 7 PUNC-punctuation, NOUN-noun, PRT-particle, DETintroduced mask-predict, which first generate tardeterminer, CONJ-conjunction, ADJ-adjective, ADV-adverb, PREP-preposition, VERB-verb get words non-autoregressively, and then repeat3304 S OURCE Den Kindern stehen regionale Handwerker von 11 bis 17 Uhr helfend zur Seite . CMLM 1∼8 9 10 R EWRITE NAT 1 2 3 Regional craftsmen are at their children from 11 a.m. to 5 p.m . Regional craftsmen are assist their children from 11 a.m. to 5 p.m"
2021.emnlp-main.265,N19-4009,0,0.0477576,"Missing"
2021.emnlp-main.265,P02-1040,0,0.10913,"et al. (2017), validate on newstest2012+2013 and test on newstest2014. For WMT16 En↔Ro translation, we use the dataset released by Lee et al. (2018), where newsdev2016 and newstest2016 are 2 https://www.statmt.org/wmt14 https://www.statmt.org/wmt16 4 https://www.statmt.org/wmt17 3 taken as development and test sets. For WMT17 En→Zh translation, we pre-process the dataset following Hassan et al. (2018). We treat newsdev2017 as the development set and newstest2017 as the test set. The datasets are tokenized into subword units using BPE (Sennrich et al., 2016). We evaluate performance with BLEU (Papineni et al., 2002) for all language pairs, except for En→Zh, where we use SacreBLEU (Post, 2018) 5 . Distillation Knowledge distillation (Kim and Rush, 2016; Hinton et al., 2015) is utilized to train the NAT models due to its effectiveness of alleviating multimodality (Gu et al., 2018) using the generated translation by T RANS FORMER (T RANSFORMER -B IG for WMT14 En↔De and En→Fr as well as WMT17 En→Zh, T RANSFORMER -BASE for WMT16 En↔Ro) as a substitute for target-side ground-truth (Ghazvininejad et al., 2019). Hyperparameters We follow most of the standard hyperparameters for T RANSFORMER 5 SacreBLEU hash: BLE"
2021.emnlp-main.265,W18-6319,0,0.0116655,"anslation, we use the dataset released by Lee et al. (2018), where newsdev2016 and newstest2016 are 2 https://www.statmt.org/wmt14 https://www.statmt.org/wmt16 4 https://www.statmt.org/wmt17 3 taken as development and test sets. For WMT17 En→Zh translation, we pre-process the dataset following Hassan et al. (2018). We treat newsdev2017 as the development set and newstest2017 as the test set. The datasets are tokenized into subword units using BPE (Sennrich et al., 2016). We evaluate performance with BLEU (Papineni et al., 2002) for all language pairs, except for En→Zh, where we use SacreBLEU (Post, 2018) 5 . Distillation Knowledge distillation (Kim and Rush, 2016; Hinton et al., 2015) is utilized to train the NAT models due to its effectiveness of alleviating multimodality (Gu et al., 2018) using the generated translation by T RANS FORMER (T RANSFORMER -B IG for WMT14 En↔De and En→Fr as well as WMT17 En→Zh, T RANSFORMER -BASE for WMT16 En↔Ro) as a substitute for target-side ground-truth (Ghazvininejad et al., 2019). Hyperparameters We follow most of the standard hyperparameters for T RANSFORMER 5 SacreBLEU hash: BLEU+case.mixed+lang.en-zh +numrefs.1+smooth.exp+test.wmt17+tok.zh+version.1.4.14"
2021.emnlp-main.265,P16-1162,0,0.0533909,"pectively. For WMT14 En→Fr, we borrow the setup of Gehring et al. (2017), validate on newstest2012+2013 and test on newstest2014. For WMT16 En↔Ro translation, we use the dataset released by Lee et al. (2018), where newsdev2016 and newstest2016 are 2 https://www.statmt.org/wmt14 https://www.statmt.org/wmt16 4 https://www.statmt.org/wmt17 3 taken as development and test sets. For WMT17 En→Zh translation, we pre-process the dataset following Hassan et al. (2018). We treat newsdev2017 as the development set and newstest2017 as the test set. The datasets are tokenized into subword units using BPE (Sennrich et al., 2016). We evaluate performance with BLEU (Papineni et al., 2002) for all language pairs, except for En→Zh, where we use SacreBLEU (Post, 2018) 5 . Distillation Knowledge distillation (Kim and Rush, 2016; Hinton et al., 2015) is utilized to train the NAT models due to its effectiveness of alleviating multimodality (Gu et al., 2018) using the generated translation by T RANS FORMER (T RANSFORMER -B IG for WMT14 En↔De and En→Fr as well as WMT17 En→Zh, T RANSFORMER -BASE for WMT16 En↔Ro) as a substitute for target-side ground-truth (Ghazvininejad et al., 2019). Hyperparameters We follow most of the stan"
2021.emnlp-main.265,D18-1044,0,0.0348484,"Missing"
2021.emnlp-main.265,P19-1125,0,0.215779,"ords depends on jad et al., 2019; Gu et al., 2019; Guo et al., 2020b; previously generated ones (Bahdanau et al., 2015; Ghazvininejad et al., 2020b) is proposed to improve Gehring et al., 2017; Vaswani et al., 2017). In- NAT by repeatedly refining previously generated stead of sequential decoding as in the autoregres- translation. Instead of enforcing NAT to generate sive translation (AT), non-autoregressive neural ma- accurate translation by one-pass decoding, these chine translation (NAT) (Gu et al., 2018; Guo et al., approaches are expected to revise incorrect transla2019; Ma et al., 2019; Wei et al., 2019; Sun et al., tion pieces through several refinements (Xia et al., 1 Our code is publicly available at https://github. 2017; Zhang et al., 2018; Geng et al., 2018). With com/xwgeng/RewriteNAT. the introduction of iterative decoding, NAT further 3297 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 3297–3308 c November 7–11, 2021. 2021 Association for Computational Linguistics boosts translation quality, bridging performance gap between NAT and AT models. However, existing iterative NAT models expose the weakness in distinguishing the erroneous words"
2021.emnlp-main.298,D17-2011,0,0.0269186,"liu, qinb}@ir.hit.edu.cn Abstract Neural networks have recently become the mainstream models for QA (Lukovnikov et al., 2017; Jia Many open-domain question answering probet al., 2018; Yang et al., 2019). Most of the models, lems can be cast as a textual entailment task, however, are unable to give explainable inference where a question and candidate answers are results. Developing effective and yet explainable concatenated to form hypotheses. A QA system then determines if the supporting knowlquestion answering models has attracted more atedge bases, regarded as potential premises, entention (Abujabal et al., 2017; Yang et al., 2018; tail the hypotheses. In this paper, we investiZhou et al., 2018; Sydorova et al., 2019; Weber gate a neural-symbolic QA approach that inet al., 2019). tegrates natural logic reasoning within deep In this paper, we investigate a neural-symbolic learning architectures, towards developing efQA approach that integrates natural logic reasonfective and yet explainable question answering models. The proposed model gradually ing (Lakoff, 1970; Nairn et al., 2006; MacCartney bridges a hypothesis and candidate premises and Manning, 2009) within deep learning architecfollowing natura"
2021.emnlp-main.298,D14-1059,0,0.405853,"A, aiming to keep the backbone of inferproof paths. Entailment scores between the ence based on the natural logic formalism, while acquired intermediate hypotheses and candiintegrating neural networks to make the systems date premises are measured to determine if a powerful and robust. Conventional natural logic premise entails the hypothesis. As the natural has been designed for natural language inference logic reasoning process forms a tree-like, hierarchical structure, we embed hypotheses and and question answering (MacCartney and Manning, premises in a Hyperbolic space rather than Eu2009; Angeli and Manning, 2014). As opposed to clidean space to acquire more precise represenperforming deduction on an abstract logical form, tations. Empirically, our method outperforms e.g., first-order logic (FOL) or its fragments, in prior work on answering multiple-choice sciwhich obtaining representation for abstract logic ence questions, achieving the best results on forms is known to face many thorny challenges, two publicly available datasets. The natural natural logic provides a formal proof framework logic inference process inherently provides evbased on the monotonicity calculus or projectivity. idence to help"
2021.emnlp-main.298,P16-1042,0,0.289245,"plication but also a challenging task for as- candidate premises by following natural logic insessing how well AI systems understand human ference steps and incorporating neural models to language and perform reasoning to answer ques- help build the proof paths. NeuNLI first converts tions. A main challenge of QA is that the answers a question and candidate answers to form declaraoften do not explicitly exist in a supporting knowl- tive sentences, namely hypotheses. It then rewrites edge base but instead need to be inferred from it. these original hypotheses to obtain intermediate Prior work (Angeli et al., 2016) has viewed QA hypotheses and repeats this process to construct a as a textual entailment problem performed on a proof tree for each question-answer pair. large premise set, where a question and candidate Since the reasoning process forms a tree-like, answers are formulated as hypotheses that need to hierarchical structure (Angeli and Manning, 2014), be proved. it can lead to structural distortion when learning * Corresponding author. embeddings for hypotheses and premises in the 3673 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 3673–3684 c Nove"
2021.emnlp-main.298,W18-5308,0,0.023179,"del NeuNLI performs the state-of-the-art. For example, the hypothesis is “in order to survive, all animals need food, water and air”. By lexical mutation in NeuNLI, we get the sentence “in order to live, all animals need food, water and air”, which is closer to the premise “animals need air, water, and food in order to live and thrive”. E and NeuNLI-E w/o reasoning. It indicates that exploiting natural logic-based reasoning is very effective for QA. 6 Related Work Question answering systems that integrate deep learning methods have made great progress in recent years (Lukovnikov et al., 2017; Bhandwaldar and Zadrozny, 2018; Jia et al., 2018; Yang et al., 2019). Many works first adopt learnable encoders for sentence representation like convolutional encoders (Zhang et al., 2017), recurrent encoders (Tay et al., 2017) and transformers (Yang et al., 2019). Then an interaction layer is devised to calculate the semantic similarity, which is the main difference in many models. Severyn and Moschitti (2015) utilize a multi-layered perceptron to combine the CNN encoded representations. Yang et al. (2016) perform a soft-attention alignment to measure word similarity between the question and the answer. Though neural netw"
2021.emnlp-main.298,N19-1423,0,0.067616,"Missing"
2021.emnlp-main.298,W18-1708,0,0.071441,"e the reasoning process forms a tree-like, answers are formulated as hypotheses that need to hierarchical structure (Angeli and Manning, 2014), be proved. it can lead to structural distortion when learning * Corresponding author. embeddings for hypotheses and premises in the 3673 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 3673–3684 c November 7–11, 2021. 2021 Association for Computational Linguistics Euclidean space (Sarkar, 2011; Sala et al., 2018). Additionally, natural language text exhibits hierarchical structure in a variety of respects (Dhingra et al., 2018). NeuNLI projects the question and answer embeddings to the Hyperbolic space. For a proof tree, NeuNLI computes an entailment score between tree nodes and candidate premises in a Hyperbolic space and use that to help select the answer. We demonstrate modelling entailment score in the Hyperbolic space improves the performance. To train the above process in an end-to-end differentiable manner, we utilize the Gumbel-Softmax technique (Jang et al., 2017), which can effectively approximate the discrete variable, as an approximation of the non-differentiable selecting process of candidate mutations."
2021.emnlp-main.298,N19-1357,0,0.0209611,"he main difference in many models. Severyn and Moschitti (2015) utilize a multi-layered perceptron to combine the CNN encoded representations. Yang et al. (2016) perform a soft-attention alignment to measure word similarity between the question and the answer. Though neural networks-based models make great advances in QA, they are short of illustrating the step-by-step prediction derivation process, where the logic-based method is adept (Rocktäschel and Riedel, 2017; Weber et al., 2019; Minervini et al., 2020), which differs from the widely used attention mechanism (Doshi-Velez and Kim, 2017; Jain and Wallace, 2019). Angeli et al. (2016) proposed a Natural Logic Inference framework to utilize natural logic to conduct interpretable question answering and viewed the open-domain question answering as a textual entailment problem. Our NeuNLI is inspired by natural logic inference but can achieve better performance by modeling the contextual information during natural logic proving using two pre-trained language models and training the whole process in an end-to-end fashion. Ablation Study. We conduct the ablation study on the QA-S test set with the Barron’s knowledge base. The experimental results are shown"
2021.emnlp-main.298,2020.acl-main.282,0,0.0170551,"idean space grows polynomially, which would lead to structural distortion in the Euclidean space (Sarkar, 2011; Sala et al., 2018). Additionally, natural language text itself exhibits hierarchical structure. Thus, we calculate the entailment scores between them in Hyperbolic space as shown in the right part of Figure 2. Here, we choose the Poincaré ball model (Cannon et al., 1997) to project the candidate premise and intermediate hypothesis into the Hyperbolic space to acquire more precise representations. We exploit the re-parameterization technique (Dhingra et al., 2018; López et al., 2019; Cao et al., 2020) to implement it, which involves calculating a direction vector m and a norm magnitude µ. Take vpEj as an example to illustrate the procedure: ´ ¯ mp mpj “ ψdir v E mpj “ m j pj , } pj } ´ ¯ ` ˘ E µ pj “ σ µ ¯ pj µ ¯pj “ ψnorm v pj , (2) where ψdir : Rd Ñ RdH is a multi-layer perceptron. ψnorm : Rd Ñ R is a linear function. σ is the sigmoid function to ensure the resulting norm µpj P p0, 1q. The re-parameterized premise representation is defined as v H pj “ µpj mpj , which lies in Hyperbolic space B dH . The re-parameterization technique has the ability to avoid the need to adopt the stochasti"
2021.emnlp-main.298,2021.ccl-1.108,0,0.037808,"Missing"
2021.emnlp-main.298,W19-4319,0,0.0124268,"y. However, the Euclidean space grows polynomially, which would lead to structural distortion in the Euclidean space (Sarkar, 2011; Sala et al., 2018). Additionally, natural language text itself exhibits hierarchical structure. Thus, we calculate the entailment scores between them in Hyperbolic space as shown in the right part of Figure 2. Here, we choose the Poincaré ball model (Cannon et al., 1997) to project the candidate premise and intermediate hypothesis into the Hyperbolic space to acquire more precise representations. We exploit the re-parameterization technique (Dhingra et al., 2018; López et al., 2019; Cao et al., 2020) to implement it, which involves calculating a direction vector m and a norm magnitude µ. Take vpEj as an example to illustrate the procedure: ´ ¯ mp mpj “ ψdir v E mpj “ m j pj , } pj } ´ ¯ ` ˘ E µ pj “ σ µ ¯ pj µ ¯pj “ ψnorm v pj , (2) where ψdir : Rd Ñ RdH is a multi-layer perceptron. ψnorm : Rd Ñ R is a linear function. σ is the sigmoid function to ensure the resulting norm µpj P p0, 1q. The re-parameterized premise representation is defined as v H pj “ µpj mpj , which lies in Hyperbolic space B dH . The re-parameterization technique has the ability to avoid the need to"
2021.emnlp-main.298,W09-3714,0,0.0425453,"llenges. In this Example–1: research, we investigate developing neural natural Question: The main function of a fish’s fins is to logic models for QA, which provide insight into the help the fish _____. derivation process but also sidestep the difficulties (A) reproduce (B) see (C) breathe (D) move of translating sentences into FOL. Knowledge Base: . . . A fish has a flipper or fin that Natural logic proving is operated by inserting, helps them swim. The dorsal fin can help to keep deleting, or mutating words following monotonicthe fish stable in the water. . . . ity calculus or projectivity (MacCartney and Manning, 2009; Valencia, 1991). In their recent work Given a science question, four candidate an- MacCartney and Manning (2009) utilize seven logswers, and relevant knowledge, a model needs ical relations as shown in Table 1. For example, to choose the correct answer supported by the mutating animals to dogs corresponds to a reverse knowledge base. Following Clark et al. (2018), entailment relation, i.e., animals Ě dogs. Natural we explore to solve the multiple-choice question logic then projects the lexical relation based on answering as a textual entailment problem. Specif- the monotonicity or projectivi"
2021.emnlp-main.298,P14-5010,0,0.00253515,"where wl is assigned to segment 0 and wl1 is assigned to segment 1. The predicted result is negation relation (N), calculated by the representation of the [CLS] token. Then, we use the projection function φ to obtain the sentence-level semantic relation according to the predicted lexical relation and the lexical polarity of the word wl . If the lexical polarity is upward, the sentence-level relation will be identical to the lexical relation. Otherwise, the projection from the word-level relation to the sentence-level relation is performed as shown in Table 2. We employ Stanford natlog parser (Manning et al., 2014) to acquire the lexical polarity of words. For example, as the polarity of the mutated word “longest” is upward, and the logical relation between “longest” and “shortest” is N, the semantic relation of the hypothesis hi and the intermediate hypothesis h1i still maintains N. If the predicted polarity of “longest” is downward, the sentence-level relation will be ë. As we only conduct inference on the sentence-level relation of ” or Ě, this mutation would be filtered out. of intermediate hypotheses grows exponentially. However, the Euclidean space grows polynomially, which would lead to structura"
2021.emnlp-main.298,W06-3907,0,0.045731,"supporting knowlquestion answering models has attracted more atedge bases, regarded as potential premises, entention (Abujabal et al., 2017; Yang et al., 2018; tail the hypotheses. In this paper, we investiZhou et al., 2018; Sydorova et al., 2019; Weber gate a neural-symbolic QA approach that inet al., 2019). tegrates natural logic reasoning within deep In this paper, we investigate a neural-symbolic learning architectures, towards developing efQA approach that integrates natural logic reasonfective and yet explainable question answering models. The proposed model gradually ing (Lakoff, 1970; Nairn et al., 2006; MacCartney bridges a hypothesis and candidate premises and Manning, 2009) within deep learning architecfollowing natural logic inference steps to build tures for QA, aiming to keep the backbone of inferproof paths. Entailment scores between the ence based on the natural logic formalism, while acquired intermediate hypotheses and candiintegrating neural networks to make the systems date premises are measured to determine if a powerful and robust. Conventional natural logic premise entails the hypothesis. As the natural has been designed for natural language inference logic reasoning process f"
2021.emnlp-main.298,D14-1162,0,0.085042,"sentence hi , i.e., “In New York state, the longest period of daylight occurs during June”. 3.1 Candidate Premises Retrieval The knowledge base K consists of unstructured text. This makes available the great amount of text as knowledge source to help perform question answering. Given a hypothesis, as shown in the right part of Figure 2, NeuNLI first retrieves candidate premises. Specifically, a premise is one of the sentences in the knowledge base K “ tp1 , . . . , pn u. Given a hypothesis hi , we obtain the representation of hi and each pj in K by computing the average Glove word embeddings (Pennington et al., 2014) of it, respectively. Then we calculate the cosine similarity between hi and each pj in K, respectively, to find the top k relevant candidate premises (k is tuned on the development set). In this paper we propose the Neural Natural Logic Inference (NeuNLI) framework, aiming to combine the advantages of natural logic and deep neural networks for question answering, which builds explainability in the model and leverages the powerful 3.2 Contextualized Neural Natural Logic capacity and robustness of neural models. Figure Prover 2 depicts the overall architecture of NeuNLI; the pseudocode of NeuNL"
2021.emnlp-main.298,P19-1488,0,0.0174838,"ikov et al., 2017; Jia Many open-domain question answering probet al., 2018; Yang et al., 2019). Most of the models, lems can be cast as a textual entailment task, however, are unable to give explainable inference where a question and candidate answers are results. Developing effective and yet explainable concatenated to form hypotheses. A QA system then determines if the supporting knowlquestion answering models has attracted more atedge bases, regarded as potential premises, entention (Abujabal et al., 2017; Yang et al., 2018; tail the hypotheses. In this paper, we investiZhou et al., 2018; Sydorova et al., 2019; Weber gate a neural-symbolic QA approach that inet al., 2019). tegrates natural logic reasoning within deep In this paper, we investigate a neural-symbolic learning architectures, towards developing efQA approach that integrates natural logic reasonfective and yet explainable question answering models. The proposed model gradually ing (Lakoff, 1970; Nairn et al., 2006; MacCartney bridges a hypothesis and candidate premises and Manning, 2009) within deep learning architecfollowing natural logic inference steps to build tures for QA, aiming to keep the backbone of inferproof paths. Entailment"
2021.emnlp-main.298,P19-1618,0,0.026418,"017) and transformers (Yang et al., 2019). Then an interaction layer is devised to calculate the semantic similarity, which is the main difference in many models. Severyn and Moschitti (2015) utilize a multi-layered perceptron to combine the CNN encoded representations. Yang et al. (2016) perform a soft-attention alignment to measure word similarity between the question and the answer. Though neural networks-based models make great advances in QA, they are short of illustrating the step-by-step prediction derivation process, where the logic-based method is adept (Rocktäschel and Riedel, 2017; Weber et al., 2019; Minervini et al., 2020), which differs from the widely used attention mechanism (Doshi-Velez and Kim, 2017; Jain and Wallace, 2019). Angeli et al. (2016) proposed a Natural Logic Inference framework to utilize natural logic to conduct interpretable question answering and viewed the open-domain question answering as a textual entailment problem. Our NeuNLI is inspired by natural logic inference but can achieve better performance by modeling the contextual information during natural logic proving using two pre-trained language models and training the whole process in an end-to-end fashion. Abl"
2021.emnlp-main.298,N19-4013,0,0.0664828,"Missing"
2021.emnlp-main.298,D18-1259,0,0.0248612,"n Abstract Neural networks have recently become the mainstream models for QA (Lukovnikov et al., 2017; Jia Many open-domain question answering probet al., 2018; Yang et al., 2019). Most of the models, lems can be cast as a textual entailment task, however, are unable to give explainable inference where a question and candidate answers are results. Developing effective and yet explainable concatenated to form hypotheses. A QA system then determines if the supporting knowlquestion answering models has attracted more atedge bases, regarded as potential premises, entention (Abujabal et al., 2017; Yang et al., 2018; tail the hypotheses. In this paper, we investiZhou et al., 2018; Sydorova et al., 2019; Weber gate a neural-symbolic QA approach that inet al., 2019). tegrates natural logic reasoning within deep In this paper, we investigate a neural-symbolic learning architectures, towards developing efQA approach that integrates natural logic reasonfective and yet explainable question answering models. The proposed model gradually ing (Lakoff, 1970; Nairn et al., 2006; MacCartney bridges a hypothesis and candidate premises and Manning, 2009) within deep learning architecfollowing natural logic inference s"
2021.emnlp-main.298,C18-1171,0,0.0236837,"dels for QA (Lukovnikov et al., 2017; Jia Many open-domain question answering probet al., 2018; Yang et al., 2019). Most of the models, lems can be cast as a textual entailment task, however, are unable to give explainable inference where a question and candidate answers are results. Developing effective and yet explainable concatenated to form hypotheses. A QA system then determines if the supporting knowlquestion answering models has attracted more atedge bases, regarded as potential premises, entention (Abujabal et al., 2017; Yang et al., 2018; tail the hypotheses. In this paper, we investiZhou et al., 2018; Sydorova et al., 2019; Weber gate a neural-symbolic QA approach that inet al., 2019). tegrates natural logic reasoning within deep In this paper, we investigate a neural-symbolic learning architectures, towards developing efQA approach that integrates natural logic reasonfective and yet explainable question answering models. The proposed model gradually ing (Lakoff, 1970; Nairn et al., 2006; MacCartney bridges a hypothesis and candidate premises and Manning, 2009) within deep learning architecfollowing natural logic inference steps to build tures for QA, aiming to keep the backbone of inferp"
2021.findings-acl.417,P18-1209,0,0.0132361,"then average them to obtain the final features, which are called utterance-level features. The utterance-level textual features can be obtained by applying RNNs for words. The obtained utterance-level features are fed into the fusion model to get the multimodal representation. Some models have been proposed for effective multimodal feature fusion. Zadeh et al. (2017) proposed Tensor Fusion to explicitly capture unimodal, bimodal, and trimodal interactions. But this method uses the three-fold Cartesian product to fuse the multimodal features, which makes the time cost very high. To address it, Liu et al. (2018) presented the Efficient Low-rank Multimodal Fusion, which applies multimodal fusion using low-rank tensors to accelerate the fusion process. Mai et al. (2020) proposed a graph fusion network to model unimodal, bimodal, and trimodal interactions successively. The utterance-level features mainly contain global information, which may fail to capture local information. Therefore, recent works are mostly focusing on word-level multimodal feature fusion. 4731 And our work in this paper is also based on wordlevel features. To extract word-level features, the first step is applying force alignment to"
2021.findings-acl.417,D14-1162,0,0.0882711,"93 videos collected from the YouTube website. The length of the videos varies from 2-5 mins. These videos are split into 2,199 short video clips and labeled with sentiment scores from -3 (strongly negative) to 3 (strongly positive). CMU multimodal opinion sentiment and emotion intensity (CMU-MOSEI) consists of 23,453 annotated video utterances from 1,000 distinct speakers and 250 topics. Each utterance is annotated with sentiment scores from -3 (strongly negative) to 3 (strongly positive). The multimodal features used in our experiments are described as follows. We use glove word embeddings (Pennington et al., 2014) to represent the words. The dimension of each word embedding is 300. We extract the visual features using Facet, which can extract 35 facial action units (Ekman et al., 1980; Ekman, 1992) from each frame resulting in a 35-dimensional vector. The acoustic features are obtained by applying COVAREP (Degottex et al., 2014), which includes 12 Mel-frequency cepstral coefficients (MFCCs) and other low-level features. The dimension of the acoustic feature is 74. 4.2 Training Details Evaluation Metrics Following previous works, we take 2-class accuracy(Acc), f1 score(F1), mean absolute error (MAE), an"
2021.findings-acl.417,P19-1656,0,0.0769886,"ord-level features, lots of methods are proposed for performing word-level multimodal feature fusion. Zadeh et al. (2018a) proposed the Memory Fusion Network(MFN) to capture the interactions across both different modalities and timesteps. Inspired by the observation that the meaning of words often varies dynamically in different non-verbal contexts, Wang et al. (2019) proposed the Recurrent Attended Variation Embedding Network (RAVEN). This model applies the Attention Gating module to fuse the word-level features, which can dynamically use the non-verbal features to shift the word embeddings. Tsai et al. (2019) presented multimodal transformer (Mult), which uses the cross-modal attention to capture the bimodal interactions, motivated by the great success of transformer in NLP(Vaswani et al., 2017). Besides, there is a related work (Pham et al., 2019) need to be noticed, which proposed that translation from a source to a target modality provides a way to learn joint representations and proposed the Multimodal Cyclic Translation Network model (MCTN) to learn joint multimodal representations. Comparing to this work, our model has a significant difference. That is we use the crossmodal prediction task t"
2021.findings-acl.417,2020.emnlp-main.143,0,0.0873303,"odal features and concatenates three obtained representations to get the final representation. MFN (Zadeh et al., 2018a) captures the interactions across both the different modalities and time. RAVEN (Wang et al., 2019) first combines the nonverbal information with word representations and then feeds the modified word representations into an LSTM network to obtain the utterance representation. MCTN (Pham et al., 2019) learns joint multimodal representations by translating between modalities. MulT (Tsai et al., 2019) uses crossmodal transformers to fuse multimodal features. Multimodal Routing (Tsai et al., 2020) proposes a routing mechanism to capture the interactions between input modalities and outputs. TCSP(Base) is our base model. The model architecture is as same as our full model, but it doesn’t use shared and private masks. Comparing TCSP(Base) and TCSP(Full), we can judge whether distinguishing the shared and private features of non-textual modalities is useful. 4.5 Experimental Results We compare our model with several baselines and the experimental results are shown in Table 2. Comparing our base model with other baselines, our base model fails to obtain the best result and underperforms RA"
2021.findings-acl.417,P18-1208,0,0.0287803,"Missing"
2021.findings-emnlp.168,2020.findings-emnlp.58,1,0.804498,"Missing"
2021.findings-emnlp.168,P18-1104,0,0.153848,"❌ Judgement 2: Not satisfied & Need rewrite Rewriting 2: Everything will be better. It&apos;s necessary to find the right job, just hold on. (Score≈0.9, Positive) ✔ Figure 1: A short conversation example which shows the difference between two frameworks. Introduction Expressing affect is a key factor to build humanlike dialogue systems, which can significantly promote affective communication and enhance user satisfaction (Prendinger and Ishizuka, 2005; Partala and Surakka, 2004) during human-computer interactions. This problem has been studied a lot in generation-based chatbots (Zhou et al., 2018; Zhou and Wang, 2018; Song et al., 2019; Shen and Feng, 2020), which is usually defined as obtaining an affective response given an affect label and the context of a conversation (Yuan et al., 2020). ∗ Email corresponding. However, the related research in retrieval-based chatbots is still in the early stage (Qiu et al., 2020). Retrieval-based chatbots have an advantage over generation-based chatbots in obtaining diverse and informative responses, which is also widely used. Therefore, research on obtaining affective response in retrieval-based chatbots is meaningful. In existing studies, affect is regarded as the"
2021.findings-emnlp.95,2020.lrec-1.663,0,0.328915,"nd sparse subnetworks. Only the parameters (red arrow →) of the subnetworks are updated. The rest (grey arrow →) are frozen but used in inference. high-quality datasets is costly and time-consuming, especially for cases that require specific domain knowledge. It hinders us from applying the datadriven solutions directly to scenarios or domains without sufficient annotation data. In this case, domain adaptation (Golub et al., 2017; Wang et al., 2019; Shakeri et al., 2020) is used to obtain a reasonable target domain performance. Unsupervised domain adaptation (Wang et al., 1 Introduction 2019; Cao et al., 2020) exploits the unlabeled conReading comprehension (Rajpurkar et al., 2016, text passages for adaptation. However, these methods have difficulties in adapting to the desiderata 2018) obtains great attention from both research of questions and question-context reasonings in the and industry for its practical value. State-of-the-art systems based on pre-trained language models (De- target domain. In this paper, we focus on supervlin et al., 2019; Yang et al., 2019; Liu et al., 2019; vised domain adaptation for reading comprehension in the few-shot settings. We are devoted to Dong et al., 2019; Jos"
2021.findings-emnlp.95,W19-4828,0,0.134213,"nt of a Transformer block is multi-head self-attention. For the l-th layer, the previous layer’s output Hl−1 is linearly projected to a triple of queries Q, keys K and values V usl , Wl , Wl ∈ Rdk ×dk ing parameter matrices WQ K V respectively. Then the attention of the i-th head is computed via: Qi K&gt; Ai = softmax( √ i ) dk (1) where dk is the size of the hidden states. At last, the output of multi-head self-attention is l , MultiHead(Hl−1 ) = [A1 V1 , · · · , Ah Vh ]WO l d ×d k k where WO ∈ R , h is the number of heads, [·] means concatenation. 2.2 Self-Attention Head Importance Many works (Clark et al., 2019; Kovaleva et al., 2019) have tried to interpret Transformer models’ behaviors. Recently, Hao et al. (2020) propose a self-attention attribution (ATTATTR) method by running an integrated gradients (Sundararajan et al., 2017) procedure over all the attention links. A higher attribution score indicates greater contribution to the model prediction. Concretely, given input x of n tokens, the attribution score of each attention link within the i-th head is computed as: Z 1 Attr(Ai ) = Ai α=0 ∂F(x, αA) dα ∈ Rn×n ∂Ai where is element-wise multiplication, attention map Ai is computed as in Equation 1,"
2021.findings-emnlp.95,N19-1423,0,0.187022,"map of head importance on SQuAD v1.1 and the correlation of importance scores between each two of the three datasets are shown in Figure 2. Given the same BERT initialization, we can see that, despite the domain differences, the important heads are highly correlated. The preliminary results uncover the value of exploiting important heads for efficient domain adaptation. 3 Method In this section, we describe our few-shot domain adaptation method for machine reading comprehension in detail. In the source domain, we have a model trained on a large-scale annotated dataset. We fine-tune BERT-base (Devlin et al., 2019), a representative Transformer-based pre-trained language model with tremendous number of parameters, as our source domain model. In the target domain, only limited annotated data, 1k examples at most, can be used for domain adaptation. The mismatch between a small amount of data and a large number of parameters makes it challenging to adapt all source domain model parameters to the target domain. Thus, we exploit a small fraction of deliberately selected parameters for domain adaptation by first identifying and then fine-tuning the lottery subnetwork. In this work, we focus on extractive read"
2021.findings-emnlp.95,D19-5801,0,0.0135261,"d, A LTER (Adaptable Lottery), against the following baselines: SQuAD v1.1 (Rajpurkar et al., 2016): Crowdworkers are shown with Wikipedia paragraphs and ask questions with extractive answers. We use the default splits of training and development sets, containing 87, 599 and 10, 570 examples respectively. Fine-tuning We fine-tune the full source domain model on the target domain data. NewsQA (Trischler et al., 2017): NewsQA is crowdsourced based on CNN news articles. Questions are asked by only seeing the article’s headline and summary instead of the full article. We use the MRQA Shared Task (Fisch et al., 2019) version. TriviaQA (Joshi et al., 2017): Question and answer pairs are sourced from trivia and quiz-league websites. We employ MRQA Shared Task version where the contexts are web snippets and documents from the Bing search engine. TweetQA (Xiong et al., 2019): TweetQA is crowdsourced by gathering tweets used by journalists to write news articles as the context. We only keep the extractive questions and obtain 7, 108 training examples and 883 development examples. Zero-Shot We apply the source domain model to the target domain without adaptation. EWC Elastic Weight Consolidation (Kirkpatrick et"
2021.findings-emnlp.95,2020.findings-emnlp.171,0,0.0113536,"da et al., 2020) are mainly unsupervised and require plenty of unlabeled text. Most of them are devoted to generate synthetic questions (Golub et al., 2017). Adversarial training (Wang et al., 2019; Lee et al., 2019; Cao et al., 2020), self-training (Rennie et al., 2020) and several filtering methods (Shakeri et al., 2020; Rennie et al., 2020) are explored in this direction. But they have the inherent difficulty to accommodate the question and reasoning types desired in the target domain. Several works have explored the domain generalization in reading comprehension. Talmor and Berant (2019), Khashabi et al. (2020) and Lourie et al. (2021) improve the generalization by training on multiple datasets. Su et al. (2020) introduces Adapters (Houlsby et al., 2019) to accommodate each domain. Theses method requires a quite amount of annotated data to work. We focus on more efficient few-shot domain adaptation. Ram et al. (2021) explores few-shot question answering via pre-training, which is orthogonal to our work. Analyzing and Pruning Transformer Analyses (Clark et al., 2019; Mareˇcek and Rosa, 2019; Voita et al., 2019b; Brunner et al., 2020; Hao et al., 2020) on Transformer mainly focus on understanding the"
2021.findings-emnlp.95,D17-1087,0,0.15435,"ork Su bn Source Domain Model Adap etwo tat rk ion Target Domain Model Figure 1: Domain adaptation with subnetworks of the source domain model. Various pruning methods can be used to find sparse subnetworks. Only the parameters (red arrow →) of the subnetworks are updated. The rest (grey arrow →) are frozen but used in inference. high-quality datasets is costly and time-consuming, especially for cases that require specific domain knowledge. It hinders us from applying the datadriven solutions directly to scenarios or domains without sufficient annotation data. In this case, domain adaptation (Golub et al., 2017; Wang et al., 2019; Shakeri et al., 2020) is used to obtain a reasonable target domain performance. Unsupervised domain adaptation (Wang et al., 1 Introduction 2019; Cao et al., 2020) exploits the unlabeled conReading comprehension (Rajpurkar et al., 2016, text passages for adaptation. However, these methods have difficulties in adapting to the desiderata 2018) obtains great attention from both research of questions and question-context reasonings in the and industry for its practical value. State-of-the-art systems based on pre-trained language models (De- target domain. In this paper, we fo"
2021.findings-emnlp.95,2020.repl4nlp-1.18,0,0.0226415,"et al., 2019b; Brunner et al., 2020; Hao et al., 2020) on Transformer mainly focus on understanding the multi-head self-attention mechanism. Michel et al. (2019); Voita et al. (2019a,b) show that most self-attention heads can be pruned with marginal performance loss. Structured pruning on more components are also explored (McCarley et al., 2019; Fan et al., 2020). We are inspired to treat self-attention heads unequally for domain adaptation. Unstructured magnitude pruning (Han et al., 2015) with tricks (Zhu and Gupta, 2018; Frankle et al., 2020) can reduce more parameters (Sanh et al., 2020; Gordon et al., 2020). In this work, we exploit both structured and unstructured pruning to find sparse structures. ATTR H EAD prunes the whole attention head with structured pruning, and applies unstructured magnitude pruning in feed-forward layers. In Table 3, the sizes of subnetworks are identical. Methods in the second group work without structure importance priors. They perform similarly and outperform the full-model fine-tuning baseline surprisingly, which shows adapting all parameters to Lottery Ticket in NLP The Lottery Ticket Hythe target domain is not optimal when given few ex- pothesis (Frankle and Carb"
2021.findings-emnlp.95,2020.tacl-1.5,0,0.0153242,"20) exploits the unlabeled conReading comprehension (Rajpurkar et al., 2016, text passages for adaptation. However, these methods have difficulties in adapting to the desiderata 2018) obtains great attention from both research of questions and question-context reasonings in the and industry for its practical value. State-of-the-art systems based on pre-trained language models (De- target domain. In this paper, we focus on supervlin et al., 2019; Yang et al., 2019; Liu et al., 2019; vised domain adaptation for reading comprehension in the few-shot settings. We are devoted to Dong et al., 2019; Joshi et al., 2020) have achieved remarkable performance on the task. Despite pre- transfer a model trained on a large amount of source training, they still rely on large amounts of anno- domain data to the target domain with only limited tated data (Rajpurkar et al., 2018; Trischler et al., annotated data. It is generally feasible to annotate 2017; Kwiatkowski et al., 2019) to reach the de- a small amout of question answering pairs. sired task performance. Manually collecting such Typical reading comprehension models based on pre-trained language model contain at least hun∗ Corresponding author. 1 The code is p"
2021.findings-emnlp.95,P17-1147,0,0.154186,"a small amount of data and a large number of parameters makes it challenging to adapt all source domain model parameters to the target domain. Thus, we exploit a small fraction of deliberately selected parameters for domain adaptation by first identifying and then fine-tuning the lottery subnetwork. In this work, we focus on extractive reading comprehension, which aims to extract a continuous span from the text context c as the answer a to a question q. It has been a prevalent format since SQuAD v1.1 (Rajpurkar et al., 2016) and widely adopted by several other reading comprehension datasets (Joshi et al., 2017; Trischler et al., 2017; 3.1 Identifying the Lottery Network Yang et al., 2018; Kwiatkowski et al., 2019) in various domains. Neural networks are over-parameterized (AllenThe differences between the domains are mainly Zhu et al., 2019), a great fraction of the parameters derived from: a) the styles and the sources of the are redundant and can be pruned with minimal or 1104 Algorithm 1 Identifying the Lottery Subnetwork with Self-Attention Head Importance Require: 1: Source domain model F(x; M θ0 ) 2: Initial pruning mask M = 1|θ0 | 3: Target sparsity s, pruning frequency ∇t and steps N 4: Imp"
2021.findings-emnlp.95,D19-1445,0,0.0207762,"block is multi-head self-attention. For the l-th layer, the previous layer’s output Hl−1 is linearly projected to a triple of queries Q, keys K and values V usl , Wl , Wl ∈ Rdk ×dk ing parameter matrices WQ K V respectively. Then the attention of the i-th head is computed via: Qi K&gt; Ai = softmax( √ i ) dk (1) where dk is the size of the hidden states. At last, the output of multi-head self-attention is l , MultiHead(Hl−1 ) = [A1 V1 , · · · , Ah Vh ]WO l d ×d k k where WO ∈ R , h is the number of heads, [·] means concatenation. 2.2 Self-Attention Head Importance Many works (Clark et al., 2019; Kovaleva et al., 2019) have tried to interpret Transformer models’ behaviors. Recently, Hao et al. (2020) propose a self-attention attribution (ATTATTR) method by running an integrated gradients (Sundararajan et al., 2017) procedure over all the attention links. A higher attribution score indicates greater contribution to the model prediction. Concretely, given input x of n tokens, the attribution score of each attention link within the i-th head is computed as: Z 1 Attr(Ai ) = Ai α=0 ∂F(x, αA) dα ∈ Rn×n ∂Ai where is element-wise multiplication, attention map Ai is computed as in Equation 1, A = (x,αA) computes the"
2021.findings-emnlp.95,Q19-1026,0,0.166356,"re-trained language models (De- target domain. In this paper, we focus on supervlin et al., 2019; Yang et al., 2019; Liu et al., 2019; vised domain adaptation for reading comprehension in the few-shot settings. We are devoted to Dong et al., 2019; Joshi et al., 2020) have achieved remarkable performance on the task. Despite pre- transfer a model trained on a large amount of source training, they still rely on large amounts of anno- domain data to the target domain with only limited tated data (Rajpurkar et al., 2018; Trischler et al., annotated data. It is generally feasible to annotate 2017; Kwiatkowski et al., 2019) to reach the de- a small amout of question answering pairs. sired task performance. Manually collecting such Typical reading comprehension models based on pre-trained language model contain at least hun∗ Corresponding author. 1 The code is publicly available at https://github. dreds of millions parameters, e.g., size of BERTcom/haichao592/ALTER. base is 110M. Previous works (Voita et al., 2019a; 1102 Findings of the Association for Computational Linguistics: EMNLP 2021, pages 1102–1113 November 7–11, 2021. ©2021 Association for Computational Linguistics Michel et al., 2019; Sanh et al., 2020)"
2021.findings-emnlp.95,D19-5826,0,0.0184572,"tic methods to explore the choice of subnetwork structures for domain adaptation: R ANDOM chooses parameters to constitute subnetworks randomly. M AGNITUDE selects the highest magnitudes parameters in one-shot. S ALVAGE reuses the pruned redundant parameters, which operates conversely with our method. 6 Related Work Domain Adaptation and Generalization in MRC Previous domain adaptation works (Nishida et al., 2020) are mainly unsupervised and require plenty of unlabeled text. Most of them are devoted to generate synthetic questions (Golub et al., 2017). Adversarial training (Wang et al., 2019; Lee et al., 2019; Cao et al., 2020), self-training (Rennie et al., 2020) and several filtering methods (Shakeri et al., 2020; Rennie et al., 2020) are explored in this direction. But they have the inherent difficulty to accommodate the question and reasoning types desired in the target domain. Several works have explored the domain generalization in reading comprehension. Talmor and Berant (2019), Khashabi et al. (2020) and Lourie et al. (2021) improve the generalization by training on multiple datasets. Su et al. (2020) introduces Adapters (Houlsby et al., 2019) to accommodate each domain. Theses method requ"
2021.findings-emnlp.95,2021.ccl-1.108,0,0.0476378,"Missing"
2021.findings-emnlp.95,W19-4827,0,0.0381516,"Missing"
2021.findings-emnlp.95,2021.acl-long.239,0,0.0167618,", 2020; Rennie et al., 2020) are explored in this direction. But they have the inherent difficulty to accommodate the question and reasoning types desired in the target domain. Several works have explored the domain generalization in reading comprehension. Talmor and Berant (2019), Khashabi et al. (2020) and Lourie et al. (2021) improve the generalization by training on multiple datasets. Su et al. (2020) introduces Adapters (Houlsby et al., 2019) to accommodate each domain. Theses method requires a quite amount of annotated data to work. We focus on more efficient few-shot domain adaptation. Ram et al. (2021) explores few-shot question answering via pre-training, which is orthogonal to our work. Analyzing and Pruning Transformer Analyses (Clark et al., 2019; Mareˇcek and Rosa, 2019; Voita et al., 2019b; Brunner et al., 2020; Hao et al., 2020) on Transformer mainly focus on understanding the multi-head self-attention mechanism. Michel et al. (2019); Voita et al. (2019a,b) show that most self-attention heads can be pruned with marginal performance loss. Structured pruning on more components are also explored (McCarley et al., 2019; Fan et al., 2020). We are inspired to treat self-attention heads une"
2021.findings-emnlp.95,2020.emnlp-main.87,0,0.0238983,"ctures for domain adaptation: R ANDOM chooses parameters to constitute subnetworks randomly. M AGNITUDE selects the highest magnitudes parameters in one-shot. S ALVAGE reuses the pruned redundant parameters, which operates conversely with our method. 6 Related Work Domain Adaptation and Generalization in MRC Previous domain adaptation works (Nishida et al., 2020) are mainly unsupervised and require plenty of unlabeled text. Most of them are devoted to generate synthetic questions (Golub et al., 2017). Adversarial training (Wang et al., 2019; Lee et al., 2019; Cao et al., 2020), self-training (Rennie et al., 2020) and several filtering methods (Shakeri et al., 2020; Rennie et al., 2020) are explored in this direction. But they have the inherent difficulty to accommodate the question and reasoning types desired in the target domain. Several works have explored the domain generalization in reading comprehension. Talmor and Berant (2019), Khashabi et al. (2020) and Lourie et al. (2021) improve the generalization by training on multiple datasets. Su et al. (2020) introduces Adapters (Houlsby et al., 2019) to accommodate each domain. Theses method requires a quite amount of annotated data to work. We focus"
2021.findings-emnlp.95,2020.emnlp-main.439,0,0.144629,"tat rk ion Target Domain Model Figure 1: Domain adaptation with subnetworks of the source domain model. Various pruning methods can be used to find sparse subnetworks. Only the parameters (red arrow →) of the subnetworks are updated. The rest (grey arrow →) are frozen but used in inference. high-quality datasets is costly and time-consuming, especially for cases that require specific domain knowledge. It hinders us from applying the datadriven solutions directly to scenarios or domains without sufficient annotation data. In this case, domain adaptation (Golub et al., 2017; Wang et al., 2019; Shakeri et al., 2020) is used to obtain a reasonable target domain performance. Unsupervised domain adaptation (Wang et al., 1 Introduction 2019; Cao et al., 2020) exploits the unlabeled conReading comprehension (Rajpurkar et al., 2016, text passages for adaptation. However, these methods have difficulties in adapting to the desiderata 2018) obtains great attention from both research of questions and question-context reasonings in the and industry for its practical value. State-of-the-art systems based on pre-trained language models (De- target domain. In this paper, we focus on supervlin et al., 2019; Yang et al."
2021.findings-emnlp.95,2020.emnlp-main.259,0,0.015979,"ies unstructured magnitude pruning in feed-forward layers. In Table 3, the sizes of subnetworks are identical. Methods in the second group work without structure importance priors. They perform similarly and outperform the full-model fine-tuning baseline surprisingly, which shows adapting all parameters to Lottery Ticket in NLP The Lottery Ticket Hythe target domain is not optimal when given few ex- pothesis (Frankle and Carbin, 2019) is largely reamples. We put the structure-aware methods in the searched in Vision. Recent works (Yu et al., 2020; third group. Comparing S ALVAGE and A LTER, we Prasanna et al., 2020; Chen et al., 2020) in NLP find using important parameters instead of the re- explore the existence of lottery subnetworks at predundant parameters are more effective. Results on trained initialization and after training on downATTR H EAD show that high magnitude parameters stream tasks. In our work, we identify and fine-tune in less important heads are also useful. lottery subnetworks for domain adaptation. 1109 7 Conclusions In this work, we propose A LTER, a simple and effective domain adaptation paradigm for few-shot reading comprehension. We exploit a small fraction of parameters of the"
2021.findings-emnlp.95,P18-2124,0,0.0157537,"n-context reasonings in the and industry for its practical value. State-of-the-art systems based on pre-trained language models (De- target domain. In this paper, we focus on supervlin et al., 2019; Yang et al., 2019; Liu et al., 2019; vised domain adaptation for reading comprehension in the few-shot settings. We are devoted to Dong et al., 2019; Joshi et al., 2020) have achieved remarkable performance on the task. Despite pre- transfer a model trained on a large amount of source training, they still rely on large amounts of anno- domain data to the target domain with only limited tated data (Rajpurkar et al., 2018; Trischler et al., annotated data. It is generally feasible to annotate 2017; Kwiatkowski et al., 2019) to reach the de- a small amout of question answering pairs. sired task performance. Manually collecting such Typical reading comprehension models based on pre-trained language model contain at least hun∗ Corresponding author. 1 The code is publicly available at https://github. dreds of millions parameters, e.g., size of BERTcom/haichao592/ALTER. base is 110M. Previous works (Voita et al., 2019a; 1102 Findings of the Association for Computational Linguistics: EMNLP 2021, pages 1102–1113 Nove"
2021.findings-emnlp.95,D16-1264,0,0.40598,"networks are updated. The rest (grey arrow →) are frozen but used in inference. high-quality datasets is costly and time-consuming, especially for cases that require specific domain knowledge. It hinders us from applying the datadriven solutions directly to scenarios or domains without sufficient annotation data. In this case, domain adaptation (Golub et al., 2017; Wang et al., 2019; Shakeri et al., 2020) is used to obtain a reasonable target domain performance. Unsupervised domain adaptation (Wang et al., 1 Introduction 2019; Cao et al., 2020) exploits the unlabeled conReading comprehension (Rajpurkar et al., 2016, text passages for adaptation. However, these methods have difficulties in adapting to the desiderata 2018) obtains great attention from both research of questions and question-context reasonings in the and industry for its practical value. State-of-the-art systems based on pre-trained language models (De- target domain. In this paper, we focus on supervlin et al., 2019; Yang et al., 2019; Liu et al., 2019; vised domain adaptation for reading comprehension in the few-shot settings. We are devoted to Dong et al., 2019; Joshi et al., 2020) have achieved remarkable performance on the task. Despi"
2021.findings-emnlp.95,P19-1485,0,0.0185299,"in adaptation works (Nishida et al., 2020) are mainly unsupervised and require plenty of unlabeled text. Most of them are devoted to generate synthetic questions (Golub et al., 2017). Adversarial training (Wang et al., 2019; Lee et al., 2019; Cao et al., 2020), self-training (Rennie et al., 2020) and several filtering methods (Shakeri et al., 2020; Rennie et al., 2020) are explored in this direction. But they have the inherent difficulty to accommodate the question and reasoning types desired in the target domain. Several works have explored the domain generalization in reading comprehension. Talmor and Berant (2019), Khashabi et al. (2020) and Lourie et al. (2021) improve the generalization by training on multiple datasets. Su et al. (2020) introduces Adapters (Houlsby et al., 2019) to accommodate each domain. Theses method requires a quite amount of annotated data to work. We focus on more efficient few-shot domain adaptation. Ram et al. (2021) explores few-shot question answering via pre-training, which is orthogonal to our work. Analyzing and Pruning Transformer Analyses (Clark et al., 2019; Mareˇcek and Rosa, 2019; Voita et al., 2019b; Brunner et al., 2020; Hao et al., 2020) on Transformer mainly foc"
2021.findings-emnlp.95,W17-2623,0,0.413566,"c) the methodology under which the questions were collected, including manually written by crowdworkers, domain experts, and automatically mined from the web or search logs. Our preliminary experiments explore the dynamics of important self-attention heads across different domains. We fine-tune BERT-base on each domain dataset independently to obtain domainspecific models. Then we employ ATTATTR, in Section 2.2, to get the importance scores of attention heads using Equation 2. We take three representative datasets, SQuAD v1.1 (Rajpurkar et al., 2016), NQ (Kwiatkowski et al., 2019) and NewsQA (Trischler et al., 2017), that differ in the sources of the context passages and question types. The heatmap of head importance on SQuAD v1.1 and the correlation of importance scores between each two of the three datasets are shown in Figure 2. Given the same BERT initialization, we can see that, despite the domain differences, the important heads are highly correlated. The preliminary results uncover the value of exploiting important heads for efficient domain adaptation. 3 Method In this section, we describe our few-shot domain adaptation method for machine reading comprehension in detail. In the source domain, we"
2021.findings-emnlp.95,D19-1448,0,0.0659168,"rely on large amounts of anno- domain data to the target domain with only limited tated data (Rajpurkar et al., 2018; Trischler et al., annotated data. It is generally feasible to annotate 2017; Kwiatkowski et al., 2019) to reach the de- a small amout of question answering pairs. sired task performance. Manually collecting such Typical reading comprehension models based on pre-trained language model contain at least hun∗ Corresponding author. 1 The code is publicly available at https://github. dreds of millions parameters, e.g., size of BERTcom/haichao592/ALTER. base is 110M. Previous works (Voita et al., 2019a; 1102 Findings of the Association for Computational Linguistics: EMNLP 2021, pages 1102–1113 November 7–11, 2021. ©2021 Association for Computational Linguistics Michel et al., 2019; Sanh et al., 2020) show that dense neural networks are over-parameterized and considerable parameters of a trained model can be pruned with marginal or even no loss in performance. Meanwhile, “The Lottery Ticket Hypothesis” (Frankle and Carbin, 2019) argues that the initialization of over-parameterized neural networks contains sparse sub-network at initialization, which, when trained in isolation, rival the orig"
2021.findings-emnlp.95,P19-1580,0,0.132891,"rely on large amounts of anno- domain data to the target domain with only limited tated data (Rajpurkar et al., 2018; Trischler et al., annotated data. It is generally feasible to annotate 2017; Kwiatkowski et al., 2019) to reach the de- a small amout of question answering pairs. sired task performance. Manually collecting such Typical reading comprehension models based on pre-trained language model contain at least hun∗ Corresponding author. 1 The code is publicly available at https://github. dreds of millions parameters, e.g., size of BERTcom/haichao592/ALTER. base is 110M. Previous works (Voita et al., 2019a; 1102 Findings of the Association for Computational Linguistics: EMNLP 2021, pages 1102–1113 November 7–11, 2021. ©2021 Association for Computational Linguistics Michel et al., 2019; Sanh et al., 2020) show that dense neural networks are over-parameterized and considerable parameters of a trained model can be pruned with marginal or even no loss in performance. Meanwhile, “The Lottery Ticket Hypothesis” (Frankle and Carbin, 2019) argues that the initialization of over-parameterized neural networks contains sparse sub-network at initialization, which, when trained in isolation, rival the orig"
2021.findings-emnlp.95,D19-1254,0,0.0775787,"ain Model Adap etwo tat rk ion Target Domain Model Figure 1: Domain adaptation with subnetworks of the source domain model. Various pruning methods can be used to find sparse subnetworks. Only the parameters (red arrow →) of the subnetworks are updated. The rest (grey arrow →) are frozen but used in inference. high-quality datasets is costly and time-consuming, especially for cases that require specific domain knowledge. It hinders us from applying the datadriven solutions directly to scenarios or domains without sufficient annotation data. In this case, domain adaptation (Golub et al., 2017; Wang et al., 2019; Shakeri et al., 2020) is used to obtain a reasonable target domain performance. Unsupervised domain adaptation (Wang et al., 1 Introduction 2019; Cao et al., 2020) exploits the unlabeled conReading comprehension (Rajpurkar et al., 2016, text passages for adaptation. However, these methods have difficulties in adapting to the desiderata 2018) obtains great attention from both research of questions and question-context reasonings in the and industry for its practical value. State-of-the-art systems based on pre-trained language models (De- target domain. In this paper, we focus on supervlin et"
2021.findings-emnlp.95,P19-1496,0,0.0155811,"ning 87, 599 and 10, 570 examples respectively. Fine-tuning We fine-tune the full source domain model on the target domain data. NewsQA (Trischler et al., 2017): NewsQA is crowdsourced based on CNN news articles. Questions are asked by only seeing the article’s headline and summary instead of the full article. We use the MRQA Shared Task (Fisch et al., 2019) version. TriviaQA (Joshi et al., 2017): Question and answer pairs are sourced from trivia and quiz-league websites. We employ MRQA Shared Task version where the contexts are web snippets and documents from the Bing search engine. TweetQA (Xiong et al., 2019): TweetQA is crowdsourced by gathering tweets used by journalists to write news articles as the context. We only keep the extractive questions and obtain 7, 108 training examples and 883 development examples. Zero-Shot We apply the source domain model to the target domain without adaptation. EWC Elastic Weight Consolidation (Kirkpatrick et al., 2017) is a regularization algorithm that constrains parameters to stay close to their original values and prevents large deviations. Layer Freeze We only fine-tune the top layers of the source domain model on the target domain data and freeze the rest."
C14-1018,baccianella-etal-2010-sentiwordnet,0,0.227345,"or negative score reflecting its sentiment polarity and strength. Sentiment lexicon is crucial for sentiment analysis (or opining mining) as it provides rich sentiment information and forms the foundation of many sentiment analysis systems (Pang and Lee, 2008; Liu, 2012; Feldman, 2013). Existing sentiment lexicon learning algorithms mostly utilize propagation methods to estimate the sentiment score of each phrase. These methods typically employ parsing results, syntactic contexts or linguistic information from thesaurus (e.g. WordNet) to calculate the similarity between phrases. For example, Baccianella et al. (2010) use the glosses information from WordNet; Velikovich et al. (2010) represent each phrase with its context words from the web documents; Qiu et al. (2011) exploit the dependency relations between sentiment words and aspect words. However, parsing information and the linguistic information from WordNet are not suitable for constructing large-scale sentiment lexicon from Twitter. The reason lies in that WordNet cannot well cover the colloquial expressions in tweets, and it is hard to have reliable tweet parsers due to the informal language style. In this paper, we propose to build large-scale se"
C14-1018,J81-4005,0,0.756294,"Missing"
C14-1018,P07-1054,0,0.0130165,"component for sentiment analysis, which can be built manually (Das and Chen, 2007), through heuristics (Kim and Hovy, 2004) or using machine learning algorithms (Turney, 2002; Li et al., 2012; Xu et al., 2013). Existing studies typically employ machine learning methods 1 2 Word/unigram is also regarded as phrase in this paper. http://www.urbandictionary.com/ 173 and adopt the propagation method to build sentiment lexicon. In the first step, a graph is built by regarding each item (word or phrase) as a node and their similarity as the edge. Then, graph propagation algorithms, such as pagerank (Esuli and Sebastiani, 2007), label propagation (Rao and Ravichandran, 2009) or random walk (Baccianella et al., 2010), are utilized to iteratively calculate the sentiment score of each item. Under this direction, parsing results, syntactic contexts or linguistic clues in thesaurus are mostly explored to calculate the similarity between items. Wiebe (2000) utilize the dependency triples from an existing parser (Lin, 1994). Qiu et al. (2009; 2011) adopt dependency relations between sentiment words and aspect words. Esuli and Sebastiani (2005) exploit the glosses information from Wordnet. Hu and Liu (2004) use the synonym"
C14-1018,P12-1092,0,0.137724,"ons between phrases and sentiment seeds, we exploit the semantic and sentimental connections between phrases through phrase embedding and propose a representation learning approach to build sentiment lexicon. 2.2 Learning Continuous Representation of Word and Phrase Continuous representation of words and phrases are proven effective in many NLP tasks (Turian et al., 2010). Embedding learning algorithms have been extensively studied in recent years (Bengio et al., 2013), and are dominated by the syntactic context based algorithms (Bengio et al., 2003; Collobert et al., 2011; Dahl et al., 2012; Huang et al., 2012; Mikolov et al., 2013a; Lebret et al., 2013; Sun et al., 2014). To integrate the sentiment information of text into the word embedding, Maas et al. (2011) extend the probabilistic document model (Blei et al., 2003) and predict the sentiment of a sentence with the embedding of each word. Labutov and Lipson (2013) learn task-specific embedding from an existing embedding and sentences with gold sentiment polarity. Tang et al. (2014) propose to learn sentimentspecific word embedding from tweets collected by emoticons for Twitter sentiment classification. Unlike previous trails, we learn sentiment"
C14-1018,C04-1200,0,0.143042,"ng the sentiment-specific phrase embedding from massive tweets selected with positive and negative emoticons; • We report the results that our lexicon outperforms existing sentiment lexicons by applying them in a supervised learning framework for Twitter sentiment classification. 2 Related Work In this section, we give a brief review about building sentiment lexicon and learning continuous representation of words and phrases. 2.1 Sentiment Lexicon Learning Sentiment lexicon is a fundamental component for sentiment analysis, which can be built manually (Das and Chen, 2007), through heuristics (Kim and Hovy, 2004) or using machine learning algorithms (Turney, 2002; Li et al., 2012; Xu et al., 2013). Existing studies typically employ machine learning methods 1 2 Word/unigram is also regarded as phrase in this paper. http://www.urbandictionary.com/ 173 and adopt the propagation method to build sentiment lexicon. In the first step, a graph is built by regarding each item (word or phrase) as a node and their similarity as the edge. Then, graph propagation algorithms, such as pagerank (Esuli and Sebastiani, 2007), label propagation (Rao and Ravichandran, 2009) or random walk (Baccianella et al., 2010), are"
C14-1018,P13-2087,0,0.0738275,"hrases are proven effective in many NLP tasks (Turian et al., 2010). Embedding learning algorithms have been extensively studied in recent years (Bengio et al., 2013), and are dominated by the syntactic context based algorithms (Bengio et al., 2003; Collobert et al., 2011; Dahl et al., 2012; Huang et al., 2012; Mikolov et al., 2013a; Lebret et al., 2013; Sun et al., 2014). To integrate the sentiment information of text into the word embedding, Maas et al. (2011) extend the probabilistic document model (Blei et al., 2003) and predict the sentiment of a sentence with the embedding of each word. Labutov and Lipson (2013) learn task-specific embedding from an existing embedding and sentences with gold sentiment polarity. Tang et al. (2014) propose to learn sentimentspecific word embedding from tweets collected by emoticons for Twitter sentiment classification. Unlike previous trails, we learn sentiment-specific phrase embedding with a tailored neural network. Unlike Mikolov et al. (2013b) that only use the syntactic contexts of phrases to learn phrase embedding, we integrate the sentiment information of text into our method. It is worth noting that we focus on learning the continuous representation of words an"
C14-1018,P12-1043,0,0.0177108,"with positive and negative emoticons; • We report the results that our lexicon outperforms existing sentiment lexicons by applying them in a supervised learning framework for Twitter sentiment classification. 2 Related Work In this section, we give a brief review about building sentiment lexicon and learning continuous representation of words and phrases. 2.1 Sentiment Lexicon Learning Sentiment lexicon is a fundamental component for sentiment analysis, which can be built manually (Das and Chen, 2007), through heuristics (Kim and Hovy, 2004) or using machine learning algorithms (Turney, 2002; Li et al., 2012; Xu et al., 2013). Existing studies typically employ machine learning methods 1 2 Word/unigram is also regarded as phrase in this paper. http://www.urbandictionary.com/ 173 and adopt the propagation method to build sentiment lexicon. In the first step, a graph is built by regarding each item (word or phrase) as a node and their similarity as the edge. Then, graph propagation algorithms, such as pagerank (Esuli and Sebastiani, 2007), label propagation (Rao and Ravichandran, 2009) or random walk (Baccianella et al., 2010), are utilized to iteratively calculate the sentiment score of each item."
C14-1018,C94-1079,0,0.0290494,"nt lexicon. In the first step, a graph is built by regarding each item (word or phrase) as a node and their similarity as the edge. Then, graph propagation algorithms, such as pagerank (Esuli and Sebastiani, 2007), label propagation (Rao and Ravichandran, 2009) or random walk (Baccianella et al., 2010), are utilized to iteratively calculate the sentiment score of each item. Under this direction, parsing results, syntactic contexts or linguistic clues in thesaurus are mostly explored to calculate the similarity between items. Wiebe (2000) utilize the dependency triples from an existing parser (Lin, 1994). Qiu et al. (2009; 2011) adopt dependency relations between sentiment words and aspect words. Esuli and Sebastiani (2005) exploit the glosses information from Wordnet. Hu and Liu (2004) use the synonym and antonym relations within linguistic resources. Velikovich et al. (2010) represent words and phrases with their syntactic contexts within a window size from the web documents. Unlike the dominated propagation based methods, we explore the classification framework based on representation learning for building large-scale sentiment lexicon from Twitter. To construct the Twitter-specific sentim"
C14-1018,P11-1015,0,0.623804,"ntation learning approach to build sentiment lexicon. 2.2 Learning Continuous Representation of Word and Phrase Continuous representation of words and phrases are proven effective in many NLP tasks (Turian et al., 2010). Embedding learning algorithms have been extensively studied in recent years (Bengio et al., 2013), and are dominated by the syntactic context based algorithms (Bengio et al., 2003; Collobert et al., 2011; Dahl et al., 2012; Huang et al., 2012; Mikolov et al., 2013a; Lebret et al., 2013; Sun et al., 2014). To integrate the sentiment information of text into the word embedding, Maas et al. (2011) extend the probabilistic document model (Blei et al., 2003) and predict the sentiment of a sentence with the embedding of each word. Labutov and Lipson (2013) learn task-specific embedding from an existing embedding and sentences with gold sentiment polarity. Tang et al. (2014) propose to learn sentimentspecific word embedding from tweets collected by emoticons for Twitter sentiment classification. Unlike previous trails, we learn sentiment-specific phrase embedding with a tailored neural network. Unlike Mikolov et al. (2013b) that only use the syntactic contexts of phrases to learn phrase em"
C14-1018,S13-2053,0,0.722061,"(2009; 2011) adopt dependency relations between sentiment words and aspect words. Esuli and Sebastiani (2005) exploit the glosses information from Wordnet. Hu and Liu (2004) use the synonym and antonym relations within linguistic resources. Velikovich et al. (2010) represent words and phrases with their syntactic contexts within a window size from the web documents. Unlike the dominated propagation based methods, we explore the classification framework based on representation learning for building large-scale sentiment lexicon from Twitter. To construct the Twitter-specific sentiment lexicon, Mohammad et al. (2013) use pointwise mutual information (PMI) between each phrase and hashtag/emoticon seed words, such as #good, #bad, :) and :(. Chen et al. (2012) utilize the Urban Dictionary and extract the target-dependent sentiment expressions from Twitter. Unlike Mohammad et al. (2013) that only capture the relations between phrases and sentiment seeds, we exploit the semantic and sentimental connections between phrases through phrase embedding and propose a representation learning approach to build sentiment lexicon. 2.2 Learning Continuous Representation of Word and Phrase Continuous representation of word"
C14-1018,S13-2052,0,0.0305225,"Missing"
C14-1018,W02-1011,0,0.0230923,"m tweets, leveraging massive tweets containing positive and negative emoticons as training set without any manual annotation. To obtain more training data for building the phrase-level sentiment classifier, we exploit the similar words from Urban Dictionary 2 , which is a crowd-sourcing resource, to expand a small list of sentiment seeds. Finally, we utilize the classifier to predict the sentiment score of each phrase in the vocabulary of SSPE, resulting in the sentiment lexicon. We evaluate the effectiveness of our sentiment lexicon (TS-Lex) by applying it in a supervised learning framework (Pang et al., 2002) for Twitter sentiment classification. Experiment results on the benchmark dataset of SemEval 2013 show that, TS-Lex yields better performance than previously introduced lexicons, including two large-scale Twitter-specific sentiment lexicons, and further improves the topperformed system in SemEval 2013 by feature combination. The quality of SSPE is also evaluated by regarding SSPE as the feature for sentiment classification of the items in existing sentiment lexicons (Hu and Liu, 2004; Wilson et al., 2005). Experiment results show that SSPE outperforms existing embedding learning algorithms. T"
C14-1018,J11-1002,0,0.0117166,"timent information and forms the foundation of many sentiment analysis systems (Pang and Lee, 2008; Liu, 2012; Feldman, 2013). Existing sentiment lexicon learning algorithms mostly utilize propagation methods to estimate the sentiment score of each phrase. These methods typically employ parsing results, syntactic contexts or linguistic information from thesaurus (e.g. WordNet) to calculate the similarity between phrases. For example, Baccianella et al. (2010) use the glosses information from WordNet; Velikovich et al. (2010) represent each phrase with its context words from the web documents; Qiu et al. (2011) exploit the dependency relations between sentiment words and aspect words. However, parsing information and the linguistic information from WordNet are not suitable for constructing large-scale sentiment lexicon from Twitter. The reason lies in that WordNet cannot well cover the colloquial expressions in tweets, and it is hard to have reliable tweet parsers due to the informal language style. In this paper, we propose to build large-scale sentiment lexicon from Twitter with a representation learning approach, as illustrated in Figure 1. We cast sentiment lexicon learning as a phrase-level cla"
C14-1018,E09-1077,0,0.0168758,"uilt manually (Das and Chen, 2007), through heuristics (Kim and Hovy, 2004) or using machine learning algorithms (Turney, 2002; Li et al., 2012; Xu et al., 2013). Existing studies typically employ machine learning methods 1 2 Word/unigram is also regarded as phrase in this paper. http://www.urbandictionary.com/ 173 and adopt the propagation method to build sentiment lexicon. In the first step, a graph is built by regarding each item (word or phrase) as a node and their similarity as the edge. Then, graph propagation algorithms, such as pagerank (Esuli and Sebastiani, 2007), label propagation (Rao and Ravichandran, 2009) or random walk (Baccianella et al., 2010), are utilized to iteratively calculate the sentiment score of each item. Under this direction, parsing results, syntactic contexts or linguistic clues in thesaurus are mostly explored to calculate the similarity between items. Wiebe (2000) utilize the dependency triples from an existing parser (Lin, 1994). Qiu et al. (2009; 2011) adopt dependency relations between sentiment words and aspect words. Esuli and Sebastiani (2005) exploit the glosses information from Wordnet. Hu and Liu (2004) use the synonym and antonym relations within linguistic resource"
C14-1018,D11-1014,0,0.0867976,"an existing embedding and sentences with gold sentiment polarity. Tang et al. (2014) propose to learn sentimentspecific word embedding from tweets collected by emoticons for Twitter sentiment classification. Unlike previous trails, we learn sentiment-specific phrase embedding with a tailored neural network. Unlike Mikolov et al. (2013b) that only use the syntactic contexts of phrases to learn phrase embedding, we integrate the sentiment information of text into our method. It is worth noting that we focus on learning the continuous representation of words and phrases, which is orthogonal with Socher et al. (2011; 2013) that learn the compositionality of sentences. 3 Methodology In this section, we describe our method for building large-scale sentiment lexicon from Twitter within a classification framework, as illustrated in Figure 1. We leverage the continuous representation of phrases as features, without parsers or hand-crafted rules, and automatically obtain the training data by seed expansion from Urban Dictionary. After the classifier is built, we employ it to predict the sentiment distribution of each phrase in the embedding vocabulary, resulting in the sentiment lexicon. To encode the sentimen"
C14-1018,D13-1170,0,0.0317488,"Missing"
C14-1018,P14-1146,1,0.621391,"d in recent years (Bengio et al., 2013), and are dominated by the syntactic context based algorithms (Bengio et al., 2003; Collobert et al., 2011; Dahl et al., 2012; Huang et al., 2012; Mikolov et al., 2013a; Lebret et al., 2013; Sun et al., 2014). To integrate the sentiment information of text into the word embedding, Maas et al. (2011) extend the probabilistic document model (Blei et al., 2003) and predict the sentiment of a sentence with the embedding of each word. Labutov and Lipson (2013) learn task-specific embedding from an existing embedding and sentences with gold sentiment polarity. Tang et al. (2014) propose to learn sentimentspecific word embedding from tweets collected by emoticons for Twitter sentiment classification. Unlike previous trails, we learn sentiment-specific phrase embedding with a tailored neural network. Unlike Mikolov et al. (2013b) that only use the syntactic contexts of phrases to learn phrase embedding, we integrate the sentiment information of text into our method. It is worth noting that we focus on learning the continuous representation of words and phrases, which is orthogonal with Socher et al. (2011; 2013) that learn the compositionality of sentences. 3 Methodolo"
C14-1018,P02-1053,0,0.0288544,"eets selected with positive and negative emoticons; • We report the results that our lexicon outperforms existing sentiment lexicons by applying them in a supervised learning framework for Twitter sentiment classification. 2 Related Work In this section, we give a brief review about building sentiment lexicon and learning continuous representation of words and phrases. 2.1 Sentiment Lexicon Learning Sentiment lexicon is a fundamental component for sentiment analysis, which can be built manually (Das and Chen, 2007), through heuristics (Kim and Hovy, 2004) or using machine learning algorithms (Turney, 2002; Li et al., 2012; Xu et al., 2013). Existing studies typically employ machine learning methods 1 2 Word/unigram is also regarded as phrase in this paper. http://www.urbandictionary.com/ 173 and adopt the propagation method to build sentiment lexicon. In the first step, a graph is built by regarding each item (word or phrase) as a node and their similarity as the edge. Then, graph propagation algorithms, such as pagerank (Esuli and Sebastiani, 2007), label propagation (Rao and Ravichandran, 2009) or random walk (Baccianella et al., 2010), are utilized to iteratively calculate the sentiment sco"
C14-1018,N10-1119,0,0.0358758,"entiment lexicon is crucial for sentiment analysis (or opining mining) as it provides rich sentiment information and forms the foundation of many sentiment analysis systems (Pang and Lee, 2008; Liu, 2012; Feldman, 2013). Existing sentiment lexicon learning algorithms mostly utilize propagation methods to estimate the sentiment score of each phrase. These methods typically employ parsing results, syntactic contexts or linguistic information from thesaurus (e.g. WordNet) to calculate the similarity between phrases. For example, Baccianella et al. (2010) use the glosses information from WordNet; Velikovich et al. (2010) represent each phrase with its context words from the web documents; Qiu et al. (2011) exploit the dependency relations between sentiment words and aspect words. However, parsing information and the linguistic information from WordNet are not suitable for constructing large-scale sentiment lexicon from Twitter. The reason lies in that WordNet cannot well cover the colloquial expressions in tweets, and it is hard to have reliable tweet parsers due to the informal language style. In this paper, we propose to build large-scale sentiment lexicon from Twitter with a representation learning approac"
C14-1018,H05-1044,0,0.940027,"eness of our sentiment lexicon (TS-Lex) by applying it in a supervised learning framework (Pang et al., 2002) for Twitter sentiment classification. Experiment results on the benchmark dataset of SemEval 2013 show that, TS-Lex yields better performance than previously introduced lexicons, including two large-scale Twitter-specific sentiment lexicons, and further improves the topperformed system in SemEval 2013 by feature combination. The quality of SSPE is also evaluated by regarding SSPE as the feature for sentiment classification of the items in existing sentiment lexicons (Hu and Liu, 2004; Wilson et al., 2005). Experiment results show that SSPE outperforms existing embedding learning algorithms. The main contributions of this work are as follows: • To our best knowledge, this is the first work that leverages the continuous representation of phrases for building large-scale sentiment lexicon from Twitter; • We propose a tailored neural architecture for learning the sentiment-specific phrase embedding from massive tweets selected with positive and negative emoticons; • We report the results that our lexicon outperforms existing sentiment lexicons by applying them in a supervised learning framework fo"
C14-1018,P13-1173,0,0.0128539,"negative emoticons; • We report the results that our lexicon outperforms existing sentiment lexicons by applying them in a supervised learning framework for Twitter sentiment classification. 2 Related Work In this section, we give a brief review about building sentiment lexicon and learning continuous representation of words and phrases. 2.1 Sentiment Lexicon Learning Sentiment lexicon is a fundamental component for sentiment analysis, which can be built manually (Das and Chen, 2007), through heuristics (Kim and Hovy, 2004) or using machine learning algorithms (Turney, 2002; Li et al., 2012; Xu et al., 2013). Existing studies typically employ machine learning methods 1 2 Word/unigram is also regarded as phrase in this paper. http://www.urbandictionary.com/ 173 and adopt the propagation method to build sentiment lexicon. In the first step, a graph is built by regarding each item (word or phrase) as a node and their similarity as the edge. Then, graph propagation algorithms, such as pagerank (Esuli and Sebastiani, 2007), label propagation (Rao and Ravichandran, 2009) or random walk (Baccianella et al., 2010), are utilized to iteratively calculate the sentiment score of each item. Under this directi"
C14-1018,P10-1040,0,\N,Missing
C14-1087,E06-1002,0,0.241458,"tain of Iraqi” in the text of Figure 1 (a), which is well-known to readers. During reading process, these gaps will be automatically plugged effortlessly by the background knowledge in human brain. However, the situation is different for machine because it lacks the ability to acquire and select the proper background knowledge, which limits the performances of certain NLP applications. Document enrichment has been proved helpful in these tasks such as web search (Pantel and Fuxman, 2011), coreference resolution (Bryl et al., 2010), document cluster (Hu et al., 2009) and entity disambiguation (Bunescu and Pasca, 2006; Sen, 2012). In the past, there are mainly two kinds of document enrichment researches according to the resource they relying on. The first line of works make use of WikiPedia, the largest available on-line encyclopedia as resource and link the entity (e.g. Baghdad) of document to its corresponding Wiki page (e.g. Baghdad 1 in WikiPedia), so that to enrich the document with the context of Wiki page (Bunescu and Pasca, 2006; Cucerzan, 2007; Han et al., 2011; Kataria et al., 2011; Sen, 2012; He et al., 2013). Despite the great success of these methods, there remain a great challenge that not al"
C14-1087,D07-1074,0,0.663248,"as web search (Pantel and Fuxman, 2011), coreference resolution (Bryl et al., 2010), document cluster (Hu et al., 2009) and entity disambiguation (Bunescu and Pasca, 2006; Sen, 2012). In the past, there are mainly two kinds of document enrichment researches according to the resource they relying on. The first line of works make use of WikiPedia, the largest available on-line encyclopedia as resource and link the entity (e.g. Baghdad) of document to its corresponding Wiki page (e.g. Baghdad 1 in WikiPedia), so that to enrich the document with the context of Wiki page (Bunescu and Pasca, 2006; Cucerzan, 2007; Han et al., 2011; Kataria et al., 2011; Sen, 2012; He et al., 2013). Despite the great success of these methods, there remain a great challenge that not all information in the linked Wiki page is helpful to the understanding of corresponding document. For example, the Wiki page of Baghdad contains lots of information about city history and culture, which are not quite relevant to the semantic of context in Figure 1 (a). So treating the whole Wiki page as the enrichment to document may cause noise ∗ Corresponding author. This work is licensed under a Creative Commons Attribution 4.0 Internati"
C14-1087,doddington-etal-2004-automatic,0,0.0397574,"xtracted from source document automatically by open information extraction technology (Banko et al., 2007), especially Reverb, the famous Open IE system developed by University of Washington (Etzioni et al., 2011). The output of ReVerb is formed as “argument1 , predicate, argument2 ”, which is naturally presented as triple. In this study, we use ACE corpus as source documents and all sd-nodes are extracted by ReVerb. The setup of automatic extraction makes our method usable in many real applications. To evaluate the effect of automatic extraction, we also use the golden annotation within ACE (Doddington et al., 2004) corpus as source document information and compare the performance that with automatic extraction. Background knowledge node (bk-node) Bk-nodes consist of the background knowledge extracted from external corpus resources automatically by Reverb too. We do not rely on certain existed knowledge base and extract background knowledge from external corpus resources for corresponding source document. This setup makes our methods usable in many real applications. Although we do not rely on special knowledge base, we do adapt our method for the existed knowledge base such as YAGO (Hoffart et al., 2013"
C14-1087,P13-2006,0,0.275026,"ryl et al., 2010), document cluster (Hu et al., 2009) and entity disambiguation (Bunescu and Pasca, 2006; Sen, 2012). In the past, there are mainly two kinds of document enrichment researches according to the resource they relying on. The first line of works make use of WikiPedia, the largest available on-line encyclopedia as resource and link the entity (e.g. Baghdad) of document to its corresponding Wiki page (e.g. Baghdad 1 in WikiPedia), so that to enrich the document with the context of Wiki page (Bunescu and Pasca, 2006; Cucerzan, 2007; Han et al., 2011; Kataria et al., 2011; Sen, 2012; He et al., 2013). Despite the great success of these methods, there remain a great challenge that not all information in the linked Wiki page is helpful to the understanding of corresponding document. For example, the Wiki page of Baghdad contains lots of information about city history and culture, which are not quite relevant to the semantic of context in Figure 1 (a). So treating the whole Wiki page as the enrichment to document may cause noise ∗ Corresponding author. This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the or"
C14-1087,D11-1072,0,0.188078,"Missing"
C14-1087,D10-1123,0,0.0428279,"Missing"
C14-1087,nastase-etal-2010-wikinet,0,0.330162,"Missing"
C14-1087,P11-1009,0,0.181009,"thors usually omit basic but well-known information to make the document more concise. For example, author omits “Baghdad is the captain of Iraqi” in the text of Figure 1 (a), which is well-known to readers. During reading process, these gaps will be automatically plugged effortlessly by the background knowledge in human brain. However, the situation is different for machine because it lacks the ability to acquire and select the proper background knowledge, which limits the performances of certain NLP applications. Document enrichment has been proved helpful in these tasks such as web search (Pantel and Fuxman, 2011), coreference resolution (Bryl et al., 2010), document cluster (Hu et al., 2009) and entity disambiguation (Bunescu and Pasca, 2006; Sen, 2012). In the past, there are mainly two kinds of document enrichment researches according to the resource they relying on. The first line of works make use of WikiPedia, the largest available on-line encyclopedia as resource and link the entity (e.g. Baghdad) of document to its corresponding Wiki page (e.g. Baghdad 1 in WikiPedia), so that to enrich the document with the context of Wiki page (Bunescu and Pasca, 2006; Cucerzan, 2007; Han et al., 2011; Katari"
C14-1087,P10-1044,0,0.0604382,"Missing"
C14-1087,D10-1106,0,0.0214427,"cuses on extracting assertions from massive corpora without a pre-specied vocabulary (Banko et al., 2007). Open IE systems are unlexicalized-formed only in terms of syntactic tokens and closed-word classes, instead of specific nouns and verbs at all costs. There are existing Open IE systems such as TextRunner (Banko et al., 2007), WOE (Wu and Weld, 2010), and StatSnowball (Zhu et al., 2009). The output of these systems has been used to support many NLP tasks such as learning selectional preference (Ritter et al., 2010), acquiring sense knowledge (Lin et al., 2010), and recognizing entailment (Schoenmackers et al., 2010). In this work, we use the famous Open IE system Reverb (Etzioni et al., 2011), which is generated from TextRunner (Etzioni et al., 2008), to extract source document information and background knowledge automatically. We use the newest version of ReVerb (version 1.3) without modification, which is free download on-line 3 . Source document node (sd-node) Sd-nodes consists of the information extracted from source document automatically by open information extraction technology (Banko et al., 2007), especially Reverb, the famous Open IE system developed by University of Washington (Etzioni et al."
C14-1087,P10-1013,0,0.0125683,"., 2013). 2.2 Nodes in the Graph There are two kinds of nodes in the triple graph: source document nodes (sd-nodes) and background knowledge nodes (bk-nodes). Both of them are extracted automatically with Open Information Extraction (Open IE) technology which focuses on extracting assertions from massive corpora without a pre-specied vocabulary (Banko et al., 2007). Open IE systems are unlexicalized-formed only in terms of syntactic tokens and closed-word classes, instead of specific nouns and verbs at all costs. There are existing Open IE systems such as TextRunner (Banko et al., 2007), WOE (Wu and Weld, 2010), and StatSnowball (Zhu et al., 2009). The output of these systems has been used to support many NLP tasks such as learning selectional preference (Ritter et al., 2010), acquiring sense knowledge (Lin et al., 2010), and recognizing entailment (Schoenmackers et al., 2010). In this work, we use the famous Open IE system Reverb (Etzioni et al., 2011), which is generated from TextRunner (Etzioni et al., 2008), to extract source document information and background knowledge automatically. We use the newest version of ReVerb (version 1.3) without modification, which is free download on-line 3 . Sour"
C14-1087,N10-1072,0,0.158765,"ge into source document. There are mainly two kinds of works in this topic according to the resource they relying on. The first line of works make use of WikiPedia and enrich source document by linking the entity to its corresponding Wiki page (Bunescu and Pasca, 2006; Cucerzan, 2007). In early stage, most researches rely on the similarity between the context of the mention and the definition of candidate entities by proposing different measuring criteria such as dot product, cosine similarity, KL divergence, Jaccard distance and more complicated ones (Bunescu and Pasca, 2006; Cucerzan, 2007; Zheng et al., 2010; Hoffart et al., 2011; Zhang et al., 2011). However, these methods mainly rely on text similarity but neglect the internal structure between mentions. So another kind of works explore the structure information with collective disambiguation (Kulkarni et al., 2009; Kataria et al., 2011; Sen, 2012; He et al., 2013). These methods make use of structure information within context and resolve different mentions based on the coherence among decisions. Despite the success, the entity linking methods rely on WikiPedia which has great coverage but less accuracy. Another line of works try to improve th"
C14-1129,N07-1039,0,0.193334,"外形” (appearance). According, h外形, 新颖i (happearance, noveli) is the T-P collocation. Generally, T-P collocation is a basic and complete sentiment unit, thus is very useful for many sentiment analysis applications. Features derived from syntactic parse trees are particularly useful for T-P collocation extraction (AbATT basi et al., 2008; Duric and Song, 2012). For example, the syntactic relation “Adj x Noun”, where the ATT denotes an attributive syntactic relation, can be used as an important evidence to extract the T-P collocation h外形, 新颖i (happearance, noveli) in the above sentiment sentence (Bloom et al., 2007; Qiu et al., 2011; Xu et al., 2013). However, one major problem of these approaches is the “naturalness” of sentiment sentences, that is, such sentences are more natural or spontaneous compared with normal sentences, thus posing a challenge to syntactic parsers. Accordingly, many wrong syntactic features have been produced and these can further result in the poor performance of T-P collocation extraction. Taking the sentence in Figure 1(a) as an example, because the word “多亏” (fortunately) is so chatty,1 the parsing result is wrong. Thus, are unable to extract the T-P collocation h键盘, 好i (hke"
C14-1129,J92-4003,0,0.0373749,"ures (cluster(·)) as the other latent semantic feature to further improve the generalization over common words. Word clustering features contain some semantic information and have been successfully used in several natural language processing tasks, including NER (Miller et al., 2004; Che et al., 2013) and dependency parsing (Koo et al., 2008). For instance, the words 外观 and 样子 (appearance) belong to the same word cluster, although they have a different suffix or prefix. Both words are important for T-P collocation extraction and should be retained. We used the Brown word clustering algorithm (Brown et al., 1992) to obtain the word clusters (Liang, 2005). Raw texts were obtained from the fifth edition of Chinese Gigaword (LDC2011T13). Finally, similar to McDonald (2006), we also added the dependency relation between a word and its parent as the syntactic features. Intuitively, the dependency relations are helpful in carrying out sentence compression. For example, the ROOT relation typically indicates that the word should not be removed because it is the main verb of a sentence. 4 Experiments 4.1 Experimental Setup 4.1.1 Corpus We conducted the experiments on a Chinese corpus of four product domains, w"
C14-1129,C10-3004,1,0.802646,"3.1 Problem Analysis First, we conducted an error analysis for the results of current T-P collocation extraction, from which we observed that the “naturalness” of sentiment sentences is one of the main problems. For examples: • Chatty form: some sentiment sentences are so chatty, that they bring many difficulties to the parser. For example, in the sentence “多亏键盘好” (fortunately the keyboard is good) shown in Figure 1, the usage of the chatty word “多亏” (fortunately) affects the accuracy of the syntactic parser. 2 A Chinese natural language processing toolkit, Language Technology Platform (LTP) (Che et al., 2010), was used as our dependency parser. More information about the syntactic relations can be found in their paper. The state-of-the-art graph-based dependency parsing model, in the toolkit, was trained on Chinese Dependency Treebank 1.0 (LDC2012T05). 1362 ROOT ROOT ROOT ROOT SBV ADV comp POB 除了 照片 好 besides photo good ATT RAD POB SBV 照片 好 photo good SBV comp 屏幕 给 人 的 感觉 不错 screen for people feel good (a) parse tree 1 before and after compression SBV 屏幕 不错 screen good (b) parse tree 2 before and after compression Figure 3: “Naturalness” problem of sentiment sentences. • Conjunction word usage: co"
C14-1129,N13-1006,1,0.818055,"ehicle), which is also the suffix of the three words. Given that all of them may become targets, they tend to be retained in compressed sentences. The verbs, 感 觉 and 感 到, can be denoted by their prefix feel (感), and can be removed from original sentences because they are feeling words. We used word clustering features (cluster(·)) as the other latent semantic feature to further improve the generalization over common words. Word clustering features contain some semantic information and have been successfully used in several natural language processing tasks, including NER (Miller et al., 2004; Che et al., 2013) and dependency parsing (Koo et al., 2008). For instance, the words 外观 and 样子 (appearance) belong to the same word cluster, although they have a different suffix or prefix. Both words are important for T-P collocation extraction and should be retained. We used the Brown word clustering algorithm (Brown et al., 1992) to obtain the word clusters (Liang, 2005). Raw texts were obtained from the fifth edition of Chinese Gigaword (LDC2011T13). Finally, similar to McDonald (2006), we also added the dependency relation between a word and its parent as the syntactic features. Intuitively, the dependenc"
C14-1129,1993.eamt-1.1,0,0.165369,"Missing"
C14-1129,P08-1109,0,0.0239906,"ing the Chinese data. 1360 Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers, pages 1360–1369, Dublin, Ireland, August 23-29 2014. ROOT ROOT SBV SBV VOB 多亏 键盘 好 fortunately keyboard good 键盘 好 keyboard good (a) before compression (b) after compression Figure 1: Parse trees before and after compression. This idea is motivated by the observation that, current syntactic parsers usually perform accurately for short, simple and formal sentences, whereas error rates increase for longer, more complex or more natural and spontaneous sentences (Finkel et al., 2008). Hence, the improvement in syntactic parsing performance would have a ripple effect over T-P collocation extraction. For example, we can compress the sentence in Figure 1(a) into a shortened sentence in Figure 1(b) by removing the chatty part “多亏” (fortunately). We can see that the shortened sentence is now well-formed (in Chinese) and its parse tree is correct, making it easier to accurately extract T-P collocation. Traditional sentence compression aims to obtain a shorter grammatical sentence by retaining important information (usually important grammar structure) (Jing, 2000). For example,"
C14-1129,N10-1131,0,0.0556384,"model to improve this task. 5.2 Sentence Compression Sentence compression is a paraphrasing task aimed at generating sentences shorter than the given ones, while preserving the essential content (Jing, 2000). There are many applications that can benefit from a robust compression system, such as summarization systems (Li et al., 2013), semantic role labeling (Vickrey and Koller, 2008), relation extraction (Miwa et al., 2010) and so on. Commonly used to compress sentences, tree-based approaches (Knight and Marcu, 2002; Turner and Charniak, 2005; Galley and McKeown, 2007; Cohn and Lapata, 2009; Galanis and Androutsopoulos, 2010; Woodsend and Lapata, 2011; Thadani and McKeown, 2013) compress a sentence by editing the syntactic tree of the original sentence. However, the automatic parsing results may not be correct; thus, the compressed tree (after removing constituents from a bad parse) may not produce a good compressed sentence. McDonald (2006), Nomoto (2007), and Clarke and Lapata (2008) tried to solve the problem by using discriminative models. Aside from above extractive sentence compression approaches, there is another research line, namely, abstractive approach, which compresses an original sentence by reorderi"
C14-1129,N07-1023,0,0.019638,"is no previous work using a sentence compression model to improve this task. 5.2 Sentence Compression Sentence compression is a paraphrasing task aimed at generating sentences shorter than the given ones, while preserving the essential content (Jing, 2000). There are many applications that can benefit from a robust compression system, such as summarization systems (Li et al., 2013), semantic role labeling (Vickrey and Koller, 2008), relation extraction (Miwa et al., 2010) and so on. Commonly used to compress sentences, tree-based approaches (Knight and Marcu, 2002; Turner and Charniak, 2005; Galley and McKeown, 2007; Cohn and Lapata, 2009; Galanis and Androutsopoulos, 2010; Woodsend and Lapata, 2011; Thadani and McKeown, 2013) compress a sentence by editing the syntactic tree of the original sentence. However, the automatic parsing results may not be correct; thus, the compressed tree (after removing constituents from a bad parse) may not produce a good compressed sentence. McDonald (2006), Nomoto (2007), and Clarke and Lapata (2008) tried to solve the problem by using discriminative models. Aside from above extractive sentence compression approaches, there is another research line, namely, abstractive a"
C14-1129,A00-1043,0,0.0568815,"ces (Finkel et al., 2008). Hence, the improvement in syntactic parsing performance would have a ripple effect over T-P collocation extraction. For example, we can compress the sentence in Figure 1(a) into a shortened sentence in Figure 1(b) by removing the chatty part “多亏” (fortunately). We can see that the shortened sentence is now well-formed (in Chinese) and its parse tree is correct, making it easier to accurately extract T-P collocation. Traditional sentence compression aims to obtain a shorter grammatical sentence by retaining important information (usually important grammar structure) (Jing, 2000). For example, the sentence “Overall, this is a great camera.” can be compressed into “This is a camera.” by removing the adverbial “overall” and the modifier “great”. However, the modifier “great” is a polarity word and very important for sentiment analysis. Therefore, Sent Comp model for sentiment sentences is different from the traditional compression models, because it needs to retain the important sentiment information, such as the polarity word. Hence, using Sent Comp, the above sentence can be compressed into “This is a great camera.” We regard Sent Comp as a sequence labeling task, whi"
C14-1129,P08-1068,0,0.0190562,"hree words. Given that all of them may become targets, they tend to be retained in compressed sentences. The verbs, 感 觉 and 感 到, can be denoted by their prefix feel (感), and can be removed from original sentences because they are feeling words. We used word clustering features (cluster(·)) as the other latent semantic feature to further improve the generalization over common words. Word clustering features contain some semantic information and have been successfully used in several natural language processing tasks, including NER (Miller et al., 2004; Che et al., 2013) and dependency parsing (Koo et al., 2008). For instance, the words 外观 and 样子 (appearance) belong to the same word cluster, although they have a different suffix or prefix. Both words are important for T-P collocation extraction and should be retained. We used the Brown word clustering algorithm (Brown et al., 1992) to obtain the word clusters (Liang, 2005). Raw texts were obtained from the fifth edition of Chinese Gigaword (LDC2011T13). Finally, similar to McDonald (2006), we also added the dependency relation between a word and its parent as the syntactic features. Intuitively, the dependency relations are helpful in carrying out se"
C14-1129,D13-1047,0,0.0165015,"ly affect the performance of syntactic parser. Once our sentiment sentence compression method can improve the quality of parsing, the performance of T-P collocation extraction task can be improved as well. Note that, to date, there is no previous work using a sentence compression model to improve this task. 5.2 Sentence Compression Sentence compression is a paraphrasing task aimed at generating sentences shorter than the given ones, while preserving the essential content (Jing, 2000). There are many applications that can benefit from a robust compression system, such as summarization systems (Li et al., 2013), semantic role labeling (Vickrey and Koller, 2008), relation extraction (Miwa et al., 2010) and so on. Commonly used to compress sentences, tree-based approaches (Knight and Marcu, 2002; Turner and Charniak, 2005; Galley and McKeown, 2007; Cohn and Lapata, 2009; Galanis and Androutsopoulos, 2010; Woodsend and Lapata, 2011; Thadani and McKeown, 2013) compress a sentence by editing the syntactic tree of the original sentence. However, the automatic parsing results may not be correct; thus, the compressed tree (after removing constituents from a bad parse) may not produce a good compressed sente"
C14-1129,E06-1038,0,0.441306,"mation and have been successfully used in several natural language processing tasks, including NER (Miller et al., 2004; Che et al., 2013) and dependency parsing (Koo et al., 2008). For instance, the words 外观 and 样子 (appearance) belong to the same word cluster, although they have a different suffix or prefix. Both words are important for T-P collocation extraction and should be retained. We used the Brown word clustering algorithm (Brown et al., 1992) to obtain the word clusters (Liang, 2005). Raw texts were obtained from the fifth edition of Chinese Gigaword (LDC2011T13). Finally, similar to McDonald (2006), we also added the dependency relation between a word and its parent as the syntactic features. Intuitively, the dependency relations are helpful in carrying out sentence compression. For example, the ROOT relation typically indicates that the word should not be removed because it is the main verb of a sentence. 4 Experiments 4.1 Experimental Setup 4.1.1 Corpus We conducted the experiments on a Chinese corpus of four product domains, which came from the Task3 of the Chinese Opinion Analysis Evaluation (COAE) (Zhao et al., 2008).6 Table 2 describes the corpus, 5 6 www.keenage.com www.ir-china."
C14-1129,N04-1043,0,0.0112461,"various kinds of 车 (vehicle), which is also the suffix of the three words. Given that all of them may become targets, they tend to be retained in compressed sentences. The verbs, 感 觉 and 感 到, can be denoted by their prefix feel (感), and can be removed from original sentences because they are feeling words. We used word clustering features (cluster(·)) as the other latent semantic feature to further improve the generalization over common words. Word clustering features contain some semantic information and have been successfully used in several natural language processing tasks, including NER (Miller et al., 2004; Che et al., 2013) and dependency parsing (Koo et al., 2008). For instance, the words 外观 and 样子 (appearance) belong to the same word cluster, although they have a different suffix or prefix. Both words are important for T-P collocation extraction and should be retained. We used the Brown word clustering algorithm (Brown et al., 1992) to obtain the word clusters (Liang, 2005). Raw texts were obtained from the fifth edition of Chinese Gigaword (LDC2011T13). Finally, similar to McDonald (2006), we also added the dependency relation between a word and its parent as the syntactic features. Intuiti"
C14-1129,C10-1089,0,0.0122581,"hod can improve the quality of parsing, the performance of T-P collocation extraction task can be improved as well. Note that, to date, there is no previous work using a sentence compression model to improve this task. 5.2 Sentence Compression Sentence compression is a paraphrasing task aimed at generating sentences shorter than the given ones, while preserving the essential content (Jing, 2000). There are many applications that can benefit from a robust compression system, such as summarization systems (Li et al., 2013), semantic role labeling (Vickrey and Koller, 2008), relation extraction (Miwa et al., 2010) and so on. Commonly used to compress sentences, tree-based approaches (Knight and Marcu, 2002; Turner and Charniak, 2005; Galley and McKeown, 2007; Cohn and Lapata, 2009; Galanis and Androutsopoulos, 2010; Woodsend and Lapata, 2011; Thadani and McKeown, 2013) compress a sentence by editing the syntactic tree of the original sentence. However, the automatic parsing results may not be correct; thus, the compressed tree (after removing constituents from a bad parse) may not produce a good compressed sentence. McDonald (2006), Nomoto (2007), and Clarke and Lapata (2008) tried to solve the problem"
C14-1129,J11-1002,0,0.535699,"cording, h外形, 新颖i (happearance, noveli) is the T-P collocation. Generally, T-P collocation is a basic and complete sentiment unit, thus is very useful for many sentiment analysis applications. Features derived from syntactic parse trees are particularly useful for T-P collocation extraction (AbATT basi et al., 2008; Duric and Song, 2012). For example, the syntactic relation “Adj x Noun”, where the ATT denotes an attributive syntactic relation, can be used as an important evidence to extract the T-P collocation h外形, 新颖i (happearance, noveli) in the above sentiment sentence (Bloom et al., 2007; Qiu et al., 2011; Xu et al., 2013). However, one major problem of these approaches is the “naturalness” of sentiment sentences, that is, such sentences are more natural or spontaneous compared with normal sentences, thus posing a challenge to syntactic parsers. Accordingly, many wrong syntactic features have been produced and these can further result in the poor performance of T-P collocation extraction. Taking the sentence in Figure 1(a) as an example, because the word “多亏” (fortunately) is so chatty,1 the parsing result is wrong. Thus, are unable to extract the T-P collocation h键盘, 好i (hkeyboard, goodi). To"
C14-1129,W13-3508,0,0.0116971,"compression is a paraphrasing task aimed at generating sentences shorter than the given ones, while preserving the essential content (Jing, 2000). There are many applications that can benefit from a robust compression system, such as summarization systems (Li et al., 2013), semantic role labeling (Vickrey and Koller, 2008), relation extraction (Miwa et al., 2010) and so on. Commonly used to compress sentences, tree-based approaches (Knight and Marcu, 2002; Turner and Charniak, 2005; Galley and McKeown, 2007; Cohn and Lapata, 2009; Galanis and Androutsopoulos, 2010; Woodsend and Lapata, 2011; Thadani and McKeown, 2013) compress a sentence by editing the syntactic tree of the original sentence. However, the automatic parsing results may not be correct; thus, the compressed tree (after removing constituents from a bad parse) may not produce a good compressed sentence. McDonald (2006), Nomoto (2007), and Clarke and Lapata (2008) tried to solve the problem by using discriminative models. Aside from above extractive sentence compression approaches, there is another research line, namely, abstractive approach, which compresses an original sentence by reordering, substituting, and inserting, as well as removing (C"
C14-1129,P05-1036,0,0.0267549,". Note that, to date, there is no previous work using a sentence compression model to improve this task. 5.2 Sentence Compression Sentence compression is a paraphrasing task aimed at generating sentences shorter than the given ones, while preserving the essential content (Jing, 2000). There are many applications that can benefit from a robust compression system, such as summarization systems (Li et al., 2013), semantic role labeling (Vickrey and Koller, 2008), relation extraction (Miwa et al., 2010) and so on. Commonly used to compress sentences, tree-based approaches (Knight and Marcu, 2002; Turner and Charniak, 2005; Galley and McKeown, 2007; Cohn and Lapata, 2009; Galanis and Androutsopoulos, 2010; Woodsend and Lapata, 2011; Thadani and McKeown, 2013) compress a sentence by editing the syntactic tree of the original sentence. However, the automatic parsing results may not be correct; thus, the compressed tree (after removing constituents from a bad parse) may not produce a good compressed sentence. McDonald (2006), Nomoto (2007), and Clarke and Lapata (2008) tried to solve the problem by using discriminative models. Aside from above extractive sentence compression approaches, there is another research l"
C14-1129,P08-1040,0,0.0661594,"modifier “great”. However, the modifier “great” is a polarity word and very important for sentiment analysis. Therefore, Sent Comp model for sentiment sentences is different from the traditional compression models, because it needs to retain the important sentiment information, such as the polarity word. Hence, using Sent Comp, the above sentence can be compressed into “This is a great camera.” We regard Sent Comp as a sequence labeling task, which can be solved by a conditional random fields (CRF) model. Instead of seeking the manual rules on parse trees for compression, as in other studies (Vickrey and Koller, 2008), this method is an automatic procedure. In this work, we introduce some sentiment-related features to retain the sentiment information for Sent Comp. We apply Sent Comp as the first step in the T-P collocation extraction task. First, we compress the sentiment sentences into easier to parse ones using Sent Comp, after which we employ the state-of-theart T-P collocation extraction approach on the compressed sentences. Experimental results on a Chinese corpus of four product domains show the effectiveness of our approach. The main contributions of this paper are as follows: • We present a framew"
C14-1129,D11-1038,0,0.0140719,"ntence Compression Sentence compression is a paraphrasing task aimed at generating sentences shorter than the given ones, while preserving the essential content (Jing, 2000). There are many applications that can benefit from a robust compression system, such as summarization systems (Li et al., 2013), semantic role labeling (Vickrey and Koller, 2008), relation extraction (Miwa et al., 2010) and so on. Commonly used to compress sentences, tree-based approaches (Knight and Marcu, 2002; Turner and Charniak, 2005; Galley and McKeown, 2007; Cohn and Lapata, 2009; Galanis and Androutsopoulos, 2010; Woodsend and Lapata, 2011; Thadani and McKeown, 2013) compress a sentence by editing the syntactic tree of the original sentence. However, the automatic parsing results may not be correct; thus, the compressed tree (after removing constituents from a bad parse) may not produce a good compressed sentence. McDonald (2006), Nomoto (2007), and Clarke and Lapata (2008) tried to solve the problem by using discriminative models. Aside from above extractive sentence compression approaches, there is another research line, namely, abstractive approach, which compresses an original sentence by reordering, substituting, and inser"
C14-1129,P13-1173,0,0.156232,"(happearance, noveli) is the T-P collocation. Generally, T-P collocation is a basic and complete sentiment unit, thus is very useful for many sentiment analysis applications. Features derived from syntactic parse trees are particularly useful for T-P collocation extraction (AbATT basi et al., 2008; Duric and Song, 2012). For example, the syntactic relation “Adj x Noun”, where the ATT denotes an attributive syntactic relation, can be used as an important evidence to extract the T-P collocation h外形, 新颖i (happearance, noveli) in the above sentiment sentence (Bloom et al., 2007; Qiu et al., 2011; Xu et al., 2013). However, one major problem of these approaches is the “naturalness” of sentiment sentences, that is, such sentences are more natural or spontaneous compared with normal sentences, thus posing a challenge to syntactic parsers. Accordingly, many wrong syntactic features have been produced and these can further result in the poor performance of T-P collocation extraction. Taking the sentence in Figure 1(a) as an example, because the word “多亏” (fortunately) is so chatty,1 the parsing result is wrong. Thus, are unable to extract the T-P collocation h键盘, 好i (hkeyboard, goodi). To solve the “natura"
C16-1276,W09-1604,0,0.076015,"Missing"
C16-1276,P16-2011,1,0.871694,"Missing"
C16-1276,P16-1212,0,0.0613039,"Missing"
C16-1276,D11-1014,0,0.087629,"previously obtained entity representation and relational representation, both of which play important roles for representing the meaning of a triple. Furthermore, a better approach should benefit from both aspects, and integrate them in triple semantic with an automatic method. To this end, we introduce a gated neural network in this part. It takes entity and relational vectors of a triple as input, and adaptively produces the composed continuous representation of them. Given ve and vr as inputs, a traditional compositional function is to concatenate ve and vr and feed them to a linear layer (Socher et al., 2011), which is calculated as Equation 2. Despite its computational efficiency, tied parameters cannot easily capture the complex linguistic phenomena in natural language expressions. v˜ = tanh(We ve + Wr vr + b) (2) α = σ(Weg ve + Wrg vr + bg ) (3) v(t) = α · vr + (1 − α) · v˜ (4) Therefore, we add a neural gate to change parameter values for different input vectors ve and vr , which is partly inspired by the recent success of gated recurrent neural network (Cho et al., 2014; Chung et al., 2015) and Long Short-Term Memory (Hochreiter and Schmidhuber, 1997; Tai et al., 2015). And our gated neural n"
C16-1276,P13-1045,0,0.111394,"c calculating process based on different triples rather than a fixed model. In this way, source triple and target triple are naturally encoded in the same semantic vector space. We design a ranking-type hinge loss function to effectively train the parameters of neural networks. We evaluate the effectiveness of our method on a manually created corpus. We conduct experiments in two settings. Empirical results show that the proposed method consistently outperforms baseline methods. We also show that the use of gated neural network improves strong composition models such as neural tensor network (Socher et al., 2013b) in terms of translation accuracy. The main contributions of this work are as follows: • We introduce an approach based on representation learning for English-Chinese KB translation in this paper. • We present a gated neural network to adaptively integrate entity and relational level evidences in triple representation. • We build a dataset for English-Chinese KB translation, and report the superior performance of our method over baseline methods on it. 2 The Approach In this section, we present our neural network method for KB Translation in detail. Figure 2 displays a high-level overview of"
C16-1276,D13-1170,0,0.158804,"c calculating process based on different triples rather than a fixed model. In this way, source triple and target triple are naturally encoded in the same semantic vector space. We design a ranking-type hinge loss function to effectively train the parameters of neural networks. We evaluate the effectiveness of our method on a manually created corpus. We conduct experiments in two settings. Empirical results show that the proposed method consistently outperforms baseline methods. We also show that the use of gated neural network improves strong composition models such as neural tensor network (Socher et al., 2013b) in terms of translation accuracy. The main contributions of this work are as follows: • We introduce an approach based on representation learning for English-Chinese KB translation in this paper. • We present a gated neural network to adaptively integrate entity and relational level evidences in triple representation. • We build a dataset for English-Chinese KB translation, and report the superior performance of our method over baseline methods on it. 2 The Approach In this section, we present our neural network method for KB Translation in detail. Figure 2 displays a high-level overview of"
C16-1276,P15-1150,0,0.152104,"m to a linear layer (Socher et al., 2011), which is calculated as Equation 2. Despite its computational efficiency, tied parameters cannot easily capture the complex linguistic phenomena in natural language expressions. v˜ = tanh(We ve + Wr vr + b) (2) α = σ(Weg ve + Wrg vr + bg ) (3) v(t) = α · vr + (1 − α) · v˜ (4) Therefore, we add a neural gate to change parameter values for different input vectors ve and vr , which is partly inspired by the recent success of gated recurrent neural network (Cho et al., 2014; Chung et al., 2015) and Long Short-Term Memory (Hochreiter and Schmidhuber, 1997; Tai et al., 2015). And our gated neural network is inspired by highway network, which allow the model to suffer less from the vanishing gradient problem (Srivastava et al., 2015a; Srivastava et al., 2015b). The gate takes ve and vr as inputs, and outputs as a weight α ∈ [0, 1], which linearly weights the two parts. Specifically, the gate is calculated as Equation 3, where σ is standard sigmoid function, Weg , Wrg and bg are parameters. Triple representation v(t) is calculated as given in Equation 4, which linearly weights the candidate composed representation v˜ and relational representation vr . In this way,"
C16-1276,D13-1141,0,0.02453,"ing bilingual parallel corpus, these word vectors are mapped into different semantic spaces. This is not desirable for comparing the semantic relatedness between English triple and Chinese triple. We use linear layers to transform English and Chinese word vectors in a same semantic vector space. A simple linear layer is calculated as ve = W e + b, where W and b are the parameters. One could also learn bilingual 3 code.google.com/p/word2vec/ https://dumps.wikimedia.org/ 5 http://baike.baidu.com/ 4 2937 word vectors simultaneously from bilingual parallel corpus with tailored learning algorithm (Zou et al., 2013). We leave this as a future work and we believe our method could benefit from the bilingual word embeddings. 2.2.2 Relational Representation We model the semantic relatedness between entities in this part. The basic idea is that the semantic relatedness between entities is determined by the semantics of entities and their relations. Based on this, we utilize neural tensor network, which is one of state-of-the-art semantic composition approach for natural language processing tasks (Mitchell and Lapata, 2010; Socher et al., 2013a; Jenatton et al., 2012). A standard neural tensor with rank 3 is e"
C16-1311,D15-1263,0,0.00993495,"compute the vector of a sentence/document from the vectors of its constituents based on the principle of compositionality (Frege, 1892). Representative compositional approaches to learn sentence representation include recursive neural networks (Socher et al., 2013b; Irsoy and Cardie, 2014), convolutional neural network (Kalchbrenner et al., 2014; Kim, 2014), long short-term memory (Li et al., 2015a) and tree-structured LSTM (Tai et al., 2015; Zhu et al., 2015). There also exists some studies focusing on learning continuous representation of documents (Le and Mikolov, 2014; Tang et al., 2015; Bhatia et al., 2015; Yang et al., 2016). 5 Conclusion We develop target-specific long short term memory models for target-dependent sentiment classification. The approach captures the connection between target word and its contexts when generating the representation of a sentence. We train the model in an end-to-end way on a benchmark dataset, and show that 3305 incorporating target information could boost the performance of a long short-term memory model. The target-dependent LSTM model obtains state-of-the-art classification accuracy. Acknowledgements We greatly thank Yaming Sun for tremendously helpful discus"
C16-1311,P14-2009,1,0.714033,"ccuracy. The target-dependent LSTM models achieve state-of-the-art performances without using syntactic parser or external sentiment lexicons.1 1 Introduction Sentiment analysis, also known as opinion mining (Pang and Lee, 2008; Liu, 2012), is a fundamental task in natural language processing and computational linguistics. Sentiment analysis is crucial to understanding user generated text in social networks or product reviews, and has drawn a lot of attentions from both industry and academic communities. In this paper, we focus on target-dependent sentiment classification (Jiang et al., 2011; Dong et al., 2014; Vo and Zhang, 2015), which is a fundamental and extensively studied task in the field of sentiment analysis. Given a sentence and a target mention, the task calls for inferring the sentiment polarity (e.g. positive, negative, neutral) of the sentence towards the target. For example, let us consider the sentence: “I bought a new camera. The picture quality is amazing but the battery life is too short”. If the target string is picture quality, the expected sentiment polarity is “positive” as the sentence expresses a positive opinion towards picture quality. If we consider the target as battery"
C16-1311,P11-1016,0,0.0890004,"the classification accuracy. The target-dependent LSTM models achieve state-of-the-art performances without using syntactic parser or external sentiment lexicons.1 1 Introduction Sentiment analysis, also known as opinion mining (Pang and Lee, 2008; Liu, 2012), is a fundamental task in natural language processing and computational linguistics. Sentiment analysis is crucial to understanding user generated text in social networks or product reviews, and has drawn a lot of attentions from both industry and academic communities. In this paper, we focus on target-dependent sentiment classification (Jiang et al., 2011; Dong et al., 2014; Vo and Zhang, 2015), which is a fundamental and extensively studied task in the field of sentiment analysis. Given a sentence and a target mention, the task calls for inferring the sentiment polarity (e.g. positive, negative, neutral) of the sentence towards the target. For example, let us consider the sentence: “I bought a new camera. The picture quality is amazing but the battery life is too short”. If the target string is picture quality, the expected sentiment polarity is “positive” as the sentence expresses a positive opinion towards picture quality. If we consider th"
C16-1311,P14-1062,0,0.0547208,"entence/document level sentiment classification, previous studies mostly have two steps. They first learn continuous word vector embeddings from data (Bengio et al., 2003; Mikolov et al., 2013; Pennington et al., 2014). Afterwards, semantic compositional approaches are used to compute the vector of a sentence/document from the vectors of its constituents based on the principle of compositionality (Frege, 1892). Representative compositional approaches to learn sentence representation include recursive neural networks (Socher et al., 2013b; Irsoy and Cardie, 2014), convolutional neural network (Kalchbrenner et al., 2014; Kim, 2014), long short-term memory (Li et al., 2015a) and tree-structured LSTM (Tai et al., 2015; Zhu et al., 2015). There also exists some studies focusing on learning continuous representation of documents (Le and Mikolov, 2014; Tang et al., 2015; Bhatia et al., 2015; Yang et al., 2016). 5 Conclusion We develop target-specific long short term memory models for target-dependent sentiment classification. The approach captures the connection between target word and its contexts when generating the representation of a sentence. We train the model in an end-to-end way on a benchmark dataset, an"
C16-1311,D14-1181,0,0.00785348,"iment classification, previous studies mostly have two steps. They first learn continuous word vector embeddings from data (Bengio et al., 2003; Mikolov et al., 2013; Pennington et al., 2014). Afterwards, semantic compositional approaches are used to compute the vector of a sentence/document from the vectors of its constituents based on the principle of compositionality (Frege, 1892). Representative compositional approaches to learn sentence representation include recursive neural networks (Socher et al., 2013b; Irsoy and Cardie, 2014), convolutional neural network (Kalchbrenner et al., 2014; Kim, 2014), long short-term memory (Li et al., 2015a) and tree-structured LSTM (Tai et al., 2015; Zhu et al., 2015). There also exists some studies focusing on learning continuous representation of documents (Le and Mikolov, 2014; Tang et al., 2015; Bhatia et al., 2015; Yang et al., 2016). 5 Conclusion We develop target-specific long short term memory models for target-dependent sentiment classification. The approach captures the connection between target word and its contexts when generating the representation of a sentence. We train the model in an end-to-end way on a benchmark dataset, and show that"
C16-1311,D15-1278,0,0.108959,"r semantic composition in the area of sentiment 3299 Softmax Target-Connection Long Short-Term Memory ℎ1 ℎ? LSTML LSTML ?? ?1 …… ℎ?+1 LSTML LSTML ??+1 ℎ?−1 ℎ?+1 LSTMR LSTMR ??+1 ??−1 ℎ? ℎ?−1 …… ??−1 …… ℎ? LSTMR LSTMR ?? ?? …… ??????? Figure 2: The target-connection long short-term memory (TC-LSTM) model for target-dependent sentiment classification, where w stands for word in a sentence whose length is n, {wl+1 , wl+2 , ..., wr−1 } are target words, vtarget is target representation, {w1 , w2 , ..., wl } are preceding context words, {wr , ..., wn−1 , wn } are following context words. analysis (Li et al., 2015a; Tang et al., 2015). It is capable of computing the representation of a longer expression (e.g. a sentence) from the representation of its children with multi levels of abstraction. The sentence representation can be naturally considered as the feature to predict the sentiment polarity of sentence. Specifically, each word is represented as a low dimensional, continuous and real-valued vector, also known as word embedding (Bengio et al., 2003; Mikolov et al., 2013; Pennington et al., 2014; Tang et al., 2014). All the word vectors are stacked in a word embedding matrix Lw ∈ Rd×|V |, where d is"
C16-1311,P15-1107,0,0.565302,"r semantic composition in the area of sentiment 3299 Softmax Target-Connection Long Short-Term Memory ℎ1 ℎ? LSTML LSTML ?? ?1 …… ℎ?+1 LSTML LSTML ??+1 ℎ?−1 ℎ?+1 LSTMR LSTMR ??+1 ??−1 ℎ? ℎ?−1 …… ??−1 …… ℎ? LSTMR LSTMR ?? ?? …… ??????? Figure 2: The target-connection long short-term memory (TC-LSTM) model for target-dependent sentiment classification, where w stands for word in a sentence whose length is n, {wl+1 , wl+2 , ..., wr−1 } are target words, vtarget is target representation, {w1 , w2 , ..., wl } are preceding context words, {wr , ..., wn−1 , wn } are following context words. analysis (Li et al., 2015a; Tang et al., 2015). It is capable of computing the representation of a longer expression (e.g. a sentence) from the representation of its children with multi levels of abstraction. The sentence representation can be naturally considered as the feature to predict the sentiment polarity of sentence. Specifically, each word is represented as a low dimensional, continuous and real-valued vector, also known as word embedding (Bengio et al., 2003; Mikolov et al., 2013; Pennington et al., 2014; Tang et al., 2014). All the word vectors are stacked in a word embedding matrix Lw ∈ Rd×|V |, where d is"
C16-1311,W02-1011,0,0.0501051,"e model. A potential reason might be that the attention based LSTM has larger number of parameters, which cannot be easily optimized with the small number of corpus. 4 Related Work We briefly review existing studies on target-dependent sentiment classification and neural network approaches for sentiment classification in this section. 4.1 Target-Dependent Sentiment Classification Target-dependent sentiment classification is typically regarded as a kind of text classification problem in literature. Therefore, standard text classification approach such as feature-based Supported Vector Machine (Pang et al., 2002; Jiang et al., 2011) can be naturally employed to build a sentiment classifier. Despite the effectiveness of feature engineering, it is labor intensive and unable to discover the discriminative or explanatory factors of data. To handle this problem, some recent studies (Dong et al., 2014; Vo and Zhang, 2015) use neural network methods and encode each sentence in continuous and low-dimensional vector space without feature engineering. Dong et al. (2014) transfer a dependency tree of a sentence into a target-specific recursive structure, and get higher level representation based on that structu"
C16-1311,D14-1162,0,0.120323,", {w1 , w2 , ..., wl } are preceding context words, {wr , ..., wn−1 , wn } are following context words. analysis (Li et al., 2015a; Tang et al., 2015). It is capable of computing the representation of a longer expression (e.g. a sentence) from the representation of its children with multi levels of abstraction. The sentence representation can be naturally considered as the feature to predict the sentiment polarity of sentence. Specifically, each word is represented as a low dimensional, continuous and real-valued vector, also known as word embedding (Bengio et al., 2003; Mikolov et al., 2013; Pennington et al., 2014; Tang et al., 2014). All the word vectors are stacked in a word embedding matrix Lw ∈ Rd×|V |, where d is the dimension of word vector and |V |is vocabulary size. In this work, we pre-train the values of word vectors from text corpus with embedding learning algorithms (Pennington et al., 2014; Tang et al., 2014) to make better use of semantic and grammatical associations of words. We use LSTM to compute the vector of a sentence from the vectors of words it contains, an illustration of the model is shown in Figure 1. LSTM is a kind of recurrent neural network (RNN), which is capable of mapping"
C16-1311,D13-1170,0,0.0826717,"ew of TC-LSTM is illustrated in Figure 2. The input of TC-LSTM is a sentence consisting of n words {w1 , w2 , ...wn } and a target string t occurs in the sentence. We represent target t as {wl+1 , wl+2 ...wr−1 } because a target could be a word sequence of variable length, such as “google” or “harry potter”. When processing a sentence, we split it into three components: target words, preceding context words and following context words. We obtain target vector vtarget by averaging the vectors of words it contains, which has been proven to be simple and effective in representing named entities (Socher et al., 2013a; Sun et al., 2015). When compute the hidden vectors of preceding and following context words, we use two separate long short-term memory models, which are similar with the strategy used in TD-LSTM. The difference is that in TC-LSTM the input at each position is the concatenation of word embedding and target vector vtarget , while in TD-LSTM the input at each position only includes 3301 the embedding of current word. We believe that TC-LSTM could make better use of the connection between target and each context word when building the representation of a sentence. 2.4 Model Training We train L"
C16-1311,P15-1150,0,0.0526433,"ontinuous word vector embeddings from data (Bengio et al., 2003; Mikolov et al., 2013; Pennington et al., 2014). Afterwards, semantic compositional approaches are used to compute the vector of a sentence/document from the vectors of its constituents based on the principle of compositionality (Frege, 1892). Representative compositional approaches to learn sentence representation include recursive neural networks (Socher et al., 2013b; Irsoy and Cardie, 2014), convolutional neural network (Kalchbrenner et al., 2014; Kim, 2014), long short-term memory (Li et al., 2015a) and tree-structured LSTM (Tai et al., 2015; Zhu et al., 2015). There also exists some studies focusing on learning continuous representation of documents (Le and Mikolov, 2014; Tang et al., 2015; Bhatia et al., 2015; Yang et al., 2016). 5 Conclusion We develop target-specific long short term memory models for target-dependent sentiment classification. The approach captures the connection between target word and its contexts when generating the representation of a sentence. We train the model in an end-to-end way on a benchmark dataset, and show that 3305 incorporating target information could boost the performance of a long short-term"
C16-1311,P14-1146,1,0.913339,"e preceding context words, {wr , ..., wn−1 , wn } are following context words. analysis (Li et al., 2015a; Tang et al., 2015). It is capable of computing the representation of a longer expression (e.g. a sentence) from the representation of its children with multi levels of abstraction. The sentence representation can be naturally considered as the feature to predict the sentiment polarity of sentence. Specifically, each word is represented as a low dimensional, continuous and real-valued vector, also known as word embedding (Bengio et al., 2003; Mikolov et al., 2013; Pennington et al., 2014; Tang et al., 2014). All the word vectors are stacked in a word embedding matrix Lw ∈ Rd×|V |, where d is the dimension of word vector and |V |is vocabulary size. In this work, we pre-train the values of word vectors from text corpus with embedding learning algorithms (Pennington et al., 2014; Tang et al., 2014) to make better use of semantic and grammatical associations of words. We use LSTM to compute the vector of a sentence from the vectors of words it contains, an illustration of the model is shown in Figure 1. LSTM is a kind of recurrent neural network (RNN), which is capable of mapping vectors of words wi"
C16-1311,D15-1167,1,0.590323,"tion in the area of sentiment 3299 Softmax Target-Connection Long Short-Term Memory ℎ1 ℎ? LSTML LSTML ?? ?1 …… ℎ?+1 LSTML LSTML ??+1 ℎ?−1 ℎ?+1 LSTMR LSTMR ??+1 ??−1 ℎ? ℎ?−1 …… ??−1 …… ℎ? LSTMR LSTMR ?? ?? …… ??????? Figure 2: The target-connection long short-term memory (TC-LSTM) model for target-dependent sentiment classification, where w stands for word in a sentence whose length is n, {wl+1 , wl+2 , ..., wr−1 } are target words, vtarget is target representation, {w1 , w2 , ..., wl } are preceding context words, {wr , ..., wn−1 , wn } are following context words. analysis (Li et al., 2015a; Tang et al., 2015). It is capable of computing the representation of a longer expression (e.g. a sentence) from the representation of its children with multi levels of abstraction. The sentence representation can be naturally considered as the feature to predict the sentiment polarity of sentence. Specifically, each word is represented as a low dimensional, continuous and real-valued vector, also known as word embedding (Bengio et al., 2003; Mikolov et al., 2013; Pennington et al., 2014; Tang et al., 2014). All the word vectors are stacked in a word embedding matrix Lw ∈ Rd×|V |, where d is the dimension of wor"
C16-1311,N16-1174,0,0.0298263,"f a sentence/document from the vectors of its constituents based on the principle of compositionality (Frege, 1892). Representative compositional approaches to learn sentence representation include recursive neural networks (Socher et al., 2013b; Irsoy and Cardie, 2014), convolutional neural network (Kalchbrenner et al., 2014; Kim, 2014), long short-term memory (Li et al., 2015a) and tree-structured LSTM (Tai et al., 2015; Zhu et al., 2015). There also exists some studies focusing on learning continuous representation of documents (Le and Mikolov, 2014; Tang et al., 2015; Bhatia et al., 2015; Yang et al., 2016). 5 Conclusion We develop target-specific long short term memory models for target-dependent sentiment classification. The approach captures the connection between target word and its contexts when generating the representation of a sentence. We train the model in an end-to-end way on a benchmark dataset, and show that 3305 incorporating target information could boost the performance of a long short-term memory model. The target-dependent LSTM model obtains state-of-the-art classification accuracy. Acknowledgements We greatly thank Yaming Sun for tremendously helpful discussions. This work was"
D12-1015,N07-1039,0,0.0153534,"satisfy the “attribute-head” relation. The only difference is that the polarity of this kind of queries is opposite to that of the collocation. Similar to the queries from Strategy0, the queries generated by Strategy1∼3 are all searched with quotes. In addition, note that the modifier and the negation word are taken from Modifier Lexicon and Negation Lexicon introduced in Table 2. 3.2.2 Query Expansion Strategy 3.2.3 Pseudo Context Acquisition We first investigate the modifying relations between polarity words and the targets, and then construct effective queries. Observed from previous work (Bloom et al., 2007; Kobayashi et al., 2004; Popescu and Etzioni, 2005), there are two kinds of common relations between the polarity words and their targets. One is the “subject-copula-predicate” relation, such as the relationship between “long” and “battery life” in the sentence “The battery life of this camera is long”. The other is the “attribute-head” relation, such as the relationship between them in the sentence “This camera has long battery life”. As a result, three heuristic query expansion strategies are adopted to construct efficient queries for searching. Take the collocation ⟨长,电 池 寿 命⟩ (⟨long, batt"
D12-1015,P11-1014,0,0.0218584,"rity. Ding (Ding et al., 2008) proposed a holistic lexicon-based approach of using global information to solve this problem. However, the contexts or evidences from these two methods are limited and unreliable. Except for the above unsupervised methods, some researchers (Wilson et al., 2005; Wilson et al., 2009) proposed supervised methods for this task, which need large annotated corpora. In addition, many related works tried to learn word polarity in a specific domain, but ignored the problem that even the same word in the same domain may indicate different polarities (Jijkoun et al., 2010; Bollegala et al., 2011). And some work (Lu et al., 2011) combined difference sources of information, especially the lexicons and heuristic rules for this task, but ignored the important role of the context. Besides, there exists some research focusing on word sense subjectivity disambiguation, which aims to classify a word sense into subjective or objective (Wiebe and Mihalcea, 2006; Su and Markert, 2009). Obviously, this task is different from ours. 3 The Proposed Approach start A Chinese collocation in a review Original context acquisition Sentiment analysis Query expansion Combination end Pos/Neg Searching Web sn"
D12-1015,P97-1023,0,0.885827,"ion Using Web-based Pseudo Contexts Yanyan Zhao, Bing Qin and Ting Liu∗ Harbin Institute of Technology, Harbin, China {yyzhao, bqin, tliu}@ir.hit.edu.cn Abstract Wilson et al., 2009; He et al., 2011), opinion retrieval (Zhang et al., 2007; Zhang and Ye, 2008; Li et al., 2010) and so on. One fundamental task for sentiment analysis is to determine the semantic orientations of words. For example, the word “beautiful” is positive, while “ugly” is negative. Many researchers have developed several algorithms for this purpose and generated large static lexicons of words marked with prior polarities (Hatzivassiloglou and McKeown, 1997; Turney et al., 2003; Esuli, 2008; Mohammad et al., 2009; Velikovich et al., 2010). However, there exist some polarity-ambiguous words, which can dynamically reflect different polarities along with different contexts. A typical polarity-ambiguous word “长” (“long” in English) is shown with two example sentences as follows. This paper focuses on the task of collocation polarity disambiguation. The collocation refers to a binary tuple of a polarity word and a target (such as ⟨long, battery life⟩ or ⟨long, startup⟩), in which the sentiment orientation of the polarity word (“long”) changes along w"
D12-1015,P11-1013,0,0.115538,"Missing"
D12-1015,P10-1060,0,0.133343,"Missing"
D12-1015,D07-1115,0,0.0232739,"s. Section 4 and 5 presents the experiments and results. Finally we conclude this paper in Section 6. 2 Related Work The key of the collocation polarity disambiguation task is to recognize the polarity word’s sentiment orientation of a collocation. There are basically two types of approaches for word polarity recognition: corpus-based and dictionary-based approaches. Corpus-based approaches find cooccurrence patterns of words in the large corpora to determine the word sentiments, such as the work in (Hatzivassiloglou and McKeown, 1997; Wiebe, 2000; Riloff and Wiebe, 2003; Turney et al., 2003; Kaji and Kitsuregawa, 2007; Velikovich et al., 2010). On the other hand, dictionary-based approaches use synonyms and antonyms in WordNet to determine word sentiments based on a set of seed polarity words. Such approaches are studied in (Kim and Hovy, 2006; Esuli and Sebastiani, 2005; Kamps et al., 2004). Overall, most of the above approaches aim to generate a large static polarity word lexicon marked with prior polarities. However, it is not sensible to predict a word’s sentiment orientation without considering its context. In fact, even in the same domain, a word may indicate different polarities depending on what ta"
D12-1015,kamps-etal-2004-using,0,0.276264,"Missing"
D12-1015,W06-1642,0,0.0885996,"on what targets it is applied to, especially for the polarity-ambiguous words, such as “长” (“long” in English) shown in Section 1. Based on these, we need to consider both the polarity words and their modified targets, i.e., the collocations mentioned in this paper, rather than only the polarity words. To date, the task in this paper is similar with much previous work. Some researchers exploited the features of the sentences containing collocations to help disambiguate the polarity of the polarity-ambiguous word. For example, Hatzivassiloglou (Hatzivassiloglou and McKeown, 1997) and Kanayama (Kanayama and Nasukawa, 2006) used conjunction rules to solve this problem from large domain corpora. Suzuki (Suzuki et al., 2006) took into account many contextual information of the word within the sentence, such as exclamation words, emoticons and so on. However, the experimental results show that these in-sentence features are not rich enough. Instead of considering the current sentence alone, some researchers exploited external information and evidences in other sentences or other reviews to infer the collocation’s polarity. For a collocation, Hu (Hu and Liu, 2004) analyzed its surrounding sentences’ polarities to di"
D12-1015,I05-2011,0,0.0173915,"he [battery life]t of this camera is [long]p . (Positive) Introduction In recent years, more attention has been paid to sentiment analysis as it has been widely used in various natural language processing applications, such as question answering (Wiebe et al., 2003; Yu and Hatzivassiloglou, 2003), information extraction (Riloff et al., 2005) and opinion-oriented summarization (Hu and Liu, 2004; Liu et al., 2005). Meanwhile, it also brings us lots of interesting and challenging research topics, such as subjectivity analysis (Riloff and Wiebe, 2003), sentiment classification (Pang et al., 2002; Kim and Hovy, 2005; ∗ Correspondence author: tliu@ir.hit.edu.cn 2. 该相机的[启动时间]t 很[长]p 。(Negative) Translated as: This camera has [long]p [startup]t . (Negative) The phrases marked with p superscript are the polarity-ambiguous words, and the phrases marked with t superscript are targets modified by the polarity words. In the above two sentences, the sentiment orientation of the polarity word “长” (“long” in English) changes along with different targets. When modifying the target “电池寿命” (“battery life” in English), its polarity is positive; and when modifying “启动时间” (“startup” in English), its polarity is 160 Proce"
D12-1015,N06-1026,0,0.0190447,"llocation. There are basically two types of approaches for word polarity recognition: corpus-based and dictionary-based approaches. Corpus-based approaches find cooccurrence patterns of words in the large corpora to determine the word sentiments, such as the work in (Hatzivassiloglou and McKeown, 1997; Wiebe, 2000; Riloff and Wiebe, 2003; Turney et al., 2003; Kaji and Kitsuregawa, 2007; Velikovich et al., 2010). On the other hand, dictionary-based approaches use synonyms and antonyms in WordNet to determine word sentiments based on a set of seed polarity words. Such approaches are studied in (Kim and Hovy, 2006; Esuli and Sebastiani, 2005; Kamps et al., 2004). Overall, most of the above approaches aim to generate a large static polarity word lexicon marked with prior polarities. However, it is not sensible to predict a word’s sentiment orientation without considering its context. In fact, even in the same domain, a word may indicate different polarities depending on what targets it is applied to, especially for the polarity-ambiguous words, such as “长” (“long” in English) shown in Section 1. Based on these, we need to consider both the polarity words and their modified targets, i.e., the collocation"
D12-1015,P10-1139,0,0.0372585,"Missing"
D12-1015,D09-1063,0,0.0584889,"∗ Harbin Institute of Technology, Harbin, China {yyzhao, bqin, tliu}@ir.hit.edu.cn Abstract Wilson et al., 2009; He et al., 2011), opinion retrieval (Zhang et al., 2007; Zhang and Ye, 2008; Li et al., 2010) and so on. One fundamental task for sentiment analysis is to determine the semantic orientations of words. For example, the word “beautiful” is positive, while “ugly” is negative. Many researchers have developed several algorithms for this purpose and generated large static lexicons of words marked with prior polarities (Hatzivassiloglou and McKeown, 1997; Turney et al., 2003; Esuli, 2008; Mohammad et al., 2009; Velikovich et al., 2010). However, there exist some polarity-ambiguous words, which can dynamically reflect different polarities along with different contexts. A typical polarity-ambiguous word “长” (“long” in English) is shown with two example sentences as follows. This paper focuses on the task of collocation polarity disambiguation. The collocation refers to a binary tuple of a polarity word and a target (such as ⟨long, battery life⟩ or ⟨long, startup⟩), in which the sentiment orientation of the polarity word (“long”) changes along with different targets (“battery life” or “startup”). To d"
D12-1015,W02-1011,0,0.0134375,"e) Translated as: The [battery life]t of this camera is [long]p . (Positive) Introduction In recent years, more attention has been paid to sentiment analysis as it has been widely used in various natural language processing applications, such as question answering (Wiebe et al., 2003; Yu and Hatzivassiloglou, 2003), information extraction (Riloff et al., 2005) and opinion-oriented summarization (Hu and Liu, 2004; Liu et al., 2005). Meanwhile, it also brings us lots of interesting and challenging research topics, such as subjectivity analysis (Riloff and Wiebe, 2003), sentiment classification (Pang et al., 2002; Kim and Hovy, 2005; ∗ Correspondence author: tliu@ir.hit.edu.cn 2. 该相机的[启动时间]t 很[长]p 。(Negative) Translated as: This camera has [long]p [startup]t . (Negative) The phrases marked with p superscript are the polarity-ambiguous words, and the phrases marked with t superscript are targets modified by the polarity words. In the above two sentences, the sentiment orientation of the polarity word “长” (“long” in English) changes along with different targets. When modifying the target “电池寿命” (“battery life” in English), its polarity is positive; and when modifying “启动时间” (“startup” in English), its p"
D12-1015,H05-1043,0,0.330857,"nly difference is that the polarity of this kind of queries is opposite to that of the collocation. Similar to the queries from Strategy0, the queries generated by Strategy1∼3 are all searched with quotes. In addition, note that the modifier and the negation word are taken from Modifier Lexicon and Negation Lexicon introduced in Table 2. 3.2.2 Query Expansion Strategy 3.2.3 Pseudo Context Acquisition We first investigate the modifying relations between polarity words and the targets, and then construct effective queries. Observed from previous work (Bloom et al., 2007; Kobayashi et al., 2004; Popescu and Etzioni, 2005), there are two kinds of common relations between the polarity words and their targets. One is the “subject-copula-predicate” relation, such as the relationship between “long” and “battery life” in the sentence “The battery life of this camera is long”. The other is the “attribute-head” relation, such as the relationship between them in the sentence “This camera has long battery life”. As a result, three heuristic query expansion strategies are adopted to construct efficient queries for searching. Take the collocation ⟨长,电 池 寿 命⟩ (⟨long, battery life⟩ in English) as an example, the strategies"
D12-1015,W03-1014,0,0.348015,"thod is effective. 1 1. 该相机的[电池寿命]t 很[长]p 。(Positive) Translated as: The [battery life]t of this camera is [long]p . (Positive) Introduction In recent years, more attention has been paid to sentiment analysis as it has been widely used in various natural language processing applications, such as question answering (Wiebe et al., 2003; Yu and Hatzivassiloglou, 2003), information extraction (Riloff et al., 2005) and opinion-oriented summarization (Hu and Liu, 2004; Liu et al., 2005). Meanwhile, it also brings us lots of interesting and challenging research topics, such as subjectivity analysis (Riloff and Wiebe, 2003), sentiment classification (Pang et al., 2002; Kim and Hovy, 2005; ∗ Correspondence author: tliu@ir.hit.edu.cn 2. 该相机的[启动时间]t 很[长]p 。(Negative) Translated as: This camera has [long]p [startup]t . (Negative) The phrases marked with p superscript are the polarity-ambiguous words, and the phrases marked with t superscript are targets modified by the polarity words. In the above two sentences, the sentiment orientation of the polarity word “长” (“long” in English) changes along with different targets. When modifying the target “电池寿命” (“battery life” in English), its polarity is positive; and when m"
D12-1015,N09-1001,0,0.0167187,"pora. In addition, many related works tried to learn word polarity in a specific domain, but ignored the problem that even the same word in the same domain may indicate different polarities (Jijkoun et al., 2010; Bollegala et al., 2011). And some work (Lu et al., 2011) combined difference sources of information, especially the lexicons and heuristic rules for this task, but ignored the important role of the context. Besides, there exists some research focusing on word sense subjectivity disambiguation, which aims to classify a word sense into subjective or objective (Wiebe and Mihalcea, 2006; Su and Markert, 2009). Obviously, this task is different from ours. 3 The Proposed Approach start A Chinese collocation in a review Original context acquisition Sentiment analysis Query expansion Combination end Pos/Neg Searching Web snippets Pseudo context acquisition Sentiment analysis Figure 1: The framework of our approach. ity with this collocation. Searching these queries in the domain-related websites, lots of snippets can be acquired. Then we can extract the pseudo contexts from these snippets. 2. Sentiment Analysis: For both original contexts and the expanded pseudo contexts from web, a simple lexicon-bas"
D12-1015,N10-1119,0,0.21066,"echnology, Harbin, China {yyzhao, bqin, tliu}@ir.hit.edu.cn Abstract Wilson et al., 2009; He et al., 2011), opinion retrieval (Zhang et al., 2007; Zhang and Ye, 2008; Li et al., 2010) and so on. One fundamental task for sentiment analysis is to determine the semantic orientations of words. For example, the word “beautiful” is positive, while “ugly” is negative. Many researchers have developed several algorithms for this purpose and generated large static lexicons of words marked with prior polarities (Hatzivassiloglou and McKeown, 1997; Turney et al., 2003; Esuli, 2008; Mohammad et al., 2009; Velikovich et al., 2010). However, there exist some polarity-ambiguous words, which can dynamically reflect different polarities along with different contexts. A typical polarity-ambiguous word “长” (“long” in English) is shown with two example sentences as follows. This paper focuses on the task of collocation polarity disambiguation. The collocation refers to a binary tuple of a polarity word and a target (such as ⟨long, battery life⟩ or ⟨long, startup⟩), in which the sentiment orientation of the polarity word (“long”) changes along with different targets (“battery life” or “startup”). To disambiguate a collocation’"
D12-1015,P06-1134,0,0.0228362,"h need large annotated corpora. In addition, many related works tried to learn word polarity in a specific domain, but ignored the problem that even the same word in the same domain may indicate different polarities (Jijkoun et al., 2010; Bollegala et al., 2011). And some work (Lu et al., 2011) combined difference sources of information, especially the lexicons and heuristic rules for this task, but ignored the important role of the context. Besides, there exists some research focusing on word sense subjectivity disambiguation, which aims to classify a word sense into subjective or objective (Wiebe and Mihalcea, 2006; Su and Markert, 2009). Obviously, this task is different from ours. 3 The Proposed Approach start A Chinese collocation in a review Original context acquisition Sentiment analysis Query expansion Combination end Pos/Neg Searching Web snippets Pseudo context acquisition Sentiment analysis Figure 1: The framework of our approach. ity with this collocation. Searching these queries in the domain-related websites, lots of snippets can be acquired. Then we can extract the pseudo contexts from these snippets. 2. Sentiment Analysis: For both original contexts and the expanded pseudo contexts from we"
D12-1015,H05-1044,0,0.103651,"features are not rich enough. Instead of considering the current sentence alone, some researchers exploited external information and evidences in other sentences or other reviews to infer the collocation’s polarity. For a collocation, Hu (Hu and Liu, 2004) analyzed its surrounding sentences’ polarities to disambiguate its polarity. Ding (Ding et al., 2008) proposed a holistic lexicon-based approach of using global information to solve this problem. However, the contexts or evidences from these two methods are limited and unreliable. Except for the above unsupervised methods, some researchers (Wilson et al., 2005; Wilson et al., 2009) proposed supervised methods for this task, which need large annotated corpora. In addition, many related works tried to learn word polarity in a specific domain, but ignored the problem that even the same word in the same domain may indicate different polarities (Jijkoun et al., 2010; Bollegala et al., 2011). And some work (Lu et al., 2011) combined difference sources of information, especially the lexicons and heuristic rules for this task, but ignored the important role of the context. Besides, there exists some research focusing on word sense subjectivity disambiguati"
D12-1015,J09-3003,0,0.0248787,"enough. Instead of considering the current sentence alone, some researchers exploited external information and evidences in other sentences or other reviews to infer the collocation’s polarity. For a collocation, Hu (Hu and Liu, 2004) analyzed its surrounding sentences’ polarities to disambiguate its polarity. Ding (Ding et al., 2008) proposed a holistic lexicon-based approach of using global information to solve this problem. However, the contexts or evidences from these two methods are limited and unreliable. Except for the above unsupervised methods, some researchers (Wilson et al., 2005; Wilson et al., 2009) proposed supervised methods for this task, which need large annotated corpora. In addition, many related works tried to learn word polarity in a specific domain, but ignored the problem that even the same word in the same domain may indicate different polarities (Jijkoun et al., 2010; Bollegala et al., 2011). And some work (Lu et al., 2011) combined difference sources of information, especially the lexicons and heuristic rules for this task, but ignored the important role of the context. Besides, there exists some research focusing on word sense subjectivity disambiguation, which aims to clas"
D12-1015,W03-1017,0,0.0199994,"polarity is insufficient to be reliable. We therefore propose an unsupervised three-component framework to expand some pseudo contexts from web, to help disambiguate a collocation’s polarity.Without using any additional labeled data, experiments show that our method is effective. 1 1. 该相机的[电池寿命]t 很[长]p 。(Positive) Translated as: The [battery life]t of this camera is [long]p . (Positive) Introduction In recent years, more attention has been paid to sentiment analysis as it has been widely used in various natural language processing applications, such as question answering (Wiebe et al., 2003; Yu and Hatzivassiloglou, 2003), information extraction (Riloff et al., 2005) and opinion-oriented summarization (Hu and Liu, 2004; Liu et al., 2005). Meanwhile, it also brings us lots of interesting and challenging research topics, such as subjectivity analysis (Riloff and Wiebe, 2003), sentiment classification (Pang et al., 2002; Kim and Hovy, 2005; ∗ Correspondence author: tliu@ir.hit.edu.cn 2. 该相机的[启动时间]t 很[长]p 。(Negative) Translated as: This camera has [long]p [startup]t . (Negative) The phrases marked with p superscript are the polarity-ambiguous words, and the phrases marked with t superscript are targets modified by"
D12-1015,H05-2017,0,\N,Missing
D13-1045,P11-1011,0,0.0198444,"ries.” Comparing with previous work, the advantages of our approach are on the following aspects. First, we generate structured annotation of queries based on top search results, not some global knowledge base or query logs. Second, they mainly focus on the method of generating structured annotation of queries, rather than leverage the generated query structures to improve web search rankings. In this paper, we not only offer a novel solution for generating structured annotation of queries, but also propose a re-ranking approach to improve Web search based on structured annotation of queries. Bendersky et al., (2011) also used top search results to generate structured annotation of queries. However, the annotations in their definition are capitalization, POS tags, and segmentation indicators, which are different from ours. 2.2 Query Template Generation The concept of query template has been discussed in a few recent papers (Agarwal et al., 2010; Pasca 2011; Liu et al., 2011; Szpektor et al., 2011). A query template is a sequence of terms, where each term could be a word or an attribute. For example, <#artist_name lyrics #lyrics> is a query template, “#artist_name” and “#lyrics” are attributes, and “lyrics"
D13-1045,D07-1086,0,\N,Missing
D13-1085,D07-1074,0,0.14966,"ot, and his opinion on flood tax policy. To understand that this post mentions Tony Abbot is not trivial because the name Abbot can refer to many people and organizations. In the Wikipedia page of Abbott, there lists more than 20 Abbotts, such as baseball player Jim Abbott, actor Bud Abbott and company Abbott Laboratories, etc.. Given a knowledge base (KB) (e.g. Wikipedia), entity linking is the task to identify the referent KB entity of a target name mention in plain text. Most current entity linking techniques are designed for long text such as news/blog articles (Mihalcea and Csomai, 2007; Cucerzan, 2007; Milne and Witten, 2008; Han and Sun, 2011; Zhang et al., 2011; Shen et al., 2012; Kulkarni et al., 2009; Ratinov et al., 2011). Entity linking for microblog posts has not been well studied. Comparing with news/blog articles, microblog posts are: short each post contains no more than 140 characters; fresh the new entity-related content may have not been included in the knowledge base; informal acronyms and spoken language writing style are common. Due to these properties, few feature can be extracted from a post. Without enough features, previous entity linking methods may fail. In order to o"
D13-1085,N13-1122,0,0.171725,"ask is that, for a name mention in a microblog post, the system is to find the referent entity of the name in a knowledge base, or return a NIL mark if the entity is absence from the knowledge base. This definition is close to the entity linking task in the TAC-KBP evaluation (Ji and Grishman, 2011) except for the context of the target name is microblog post whereas in TAC-KBP the context is news article or web log. Several related tasks have been studied on microblog posts. In Meij et al. (2012)’s work, they link a post, rather than a name mention in the post, to relevant Wikipedia concepts. Guo et al. (2013a) and Liu et al. (2013) define entity linking as to first detect all the mentions in a post and then link the mentions to the knowledge base. In contrast, our definition (as well as the TAC-KBP definition) focuses on a concerned name mention across different posts/documents. 3 Method A typical entity linking system can be broken down into two steps: candidate generation This step narrows down the candidate entity range from any entity in the world to a limited set. candidate ranking This step ranks the candidates and output the top ranked entity as the result. Figure 1: An example of the GMEL"
D13-1085,P11-1095,0,0.0370182,"Missing"
D13-1085,P11-1115,0,0.0346173,"EL can significantly improve the performance • We annotate a microblog entity linking corpus which is comparable to an existing long text corpus. 864 • We show the inefficiency of previous method on the microblog corpus and our method can significantly improve the results. 2 Task defination The microblog entity linking task is that, for a name mention in a microblog post, the system is to find the referent entity of the name in a knowledge base, or return a NIL mark if the entity is absence from the knowledge base. This definition is close to the entity linking task in the TAC-KBP evaluation (Ji and Grishman, 2011) except for the context of the target name is microblog post whereas in TAC-KBP the context is news article or web log. Several related tasks have been studied on microblog posts. In Meij et al. (2012)’s work, they link a post, rather than a name mention in the post, to relevant Wikipedia concepts. Guo et al. (2013a) and Liu et al. (2013) define entity linking as to first detect all the mentions in a post and then link the mentions to the knowledge base. In contrast, our definition (as well as the TAC-KBP definition) focuses on a concerned name mention across different posts/documents. 3 Metho"
D13-1085,P13-1128,0,0.0311755,"mention in a microblog post, the system is to find the referent entity of the name in a knowledge base, or return a NIL mark if the entity is absence from the knowledge base. This definition is close to the entity linking task in the TAC-KBP evaluation (Ji and Grishman, 2011) except for the context of the target name is microblog post whereas in TAC-KBP the context is news article or web log. Several related tasks have been studied on microblog posts. In Meij et al. (2012)’s work, they link a post, rather than a name mention in the post, to relevant Wikipedia concepts. Guo et al. (2013a) and Liu et al. (2013) define entity linking as to first detect all the mentions in a post and then link the mentions to the knowledge base. In contrast, our definition (as well as the TAC-KBP definition) focuses on a concerned name mention across different posts/documents. 3 Method A typical entity linking system can be broken down into two steps: candidate generation This step narrows down the candidate entity range from any entity in the world to a limited set. candidate ranking This step ranks the candidates and output the top ranked entity as the result. Figure 1: An example of the GMEL graph. p1 . . . p4 are"
D13-1085,N13-1039,0,0.0970558,"Missing"
D13-1085,P11-1138,0,0.021886,"Abbot can refer to many people and organizations. In the Wikipedia page of Abbott, there lists more than 20 Abbotts, such as baseball player Jim Abbott, actor Bud Abbott and company Abbott Laboratories, etc.. Given a knowledge base (KB) (e.g. Wikipedia), entity linking is the task to identify the referent KB entity of a target name mention in plain text. Most current entity linking techniques are designed for long text such as news/blog articles (Mihalcea and Csomai, 2007; Cucerzan, 2007; Milne and Witten, 2008; Han and Sun, 2011; Zhang et al., 2011; Shen et al., 2012; Kulkarni et al., 2009; Ratinov et al., 2011). Entity linking for microblog posts has not been well studied. Comparing with news/blog articles, microblog posts are: short each post contains no more than 140 characters; fresh the new entity-related content may have not been included in the knowledge base; informal acronyms and spoken language writing style are common. Due to these properties, few feature can be extracted from a post. Without enough features, previous entity linking methods may fail. In order to overcome the feature sparseness, we turn to another property of microblog: 863 Proceedings of the 2013 Conference on Empirical Me"
D13-1085,P10-1149,0,0.0300152,"Missing"
D13-1085,N10-1072,0,0.0212542,"25 target names and manually link the target name mentions in the posts to the TAC-KBP knowledge base. In order to evaluate the assumption in CEMEL: similar posts tend to co-reference, we randomly select 10 posts for 5 target names respectively and search for the posts in the post collection. From the search result of each of the 50 posts, we select the top 20 posts and manually annotate if they coreference with the query post. 4.2 Settings We generate candidates with the method described in (Guo et al., 2013b) and use Vector Space Model (VSM) (Varma et al., 2009) and Learning to Rank (LTR) (Zheng et al., 2010) as the ranking model. We 866 Figure 4: Accuracy of GMEL with different rate of extra post nodes use Lucene and ListNet with default settings for the VSM and LTR implementation respectively. We use bigram feature for VSM and the feature set of (Chen et al., 2011) for LTR. LTR is evaluated with 10-fold cross validation. Given a target name, the GMEL graph includes all the evaluation posts as well as a set of extra post nodes searched from the post collection with the query of the target name. We filter out determiners, interjections, punctuations, emoticons, discourse markers and URLs in the po"
D13-1122,P99-1016,0,0.409111,"s for hypernym discovery can be summarized into two major categories, i.e., patternbased methods and encyclopedia-based methods. Pattern-based methods make use of manually or automatically constructed patterns to mine hypernym relations from text corpora. The pioneer work by Hearst (1992) finds that linking two noun phrases (NPs) via certain lexical constructions often implies hypernym relations. For example, NP1 is a hypernym of NP2 in the lexical pattern “such NP1 as NP2 ”. Similarly, succeeding researchers follow her work and use handcrafted patterns to extract hypernym pairs from corpora (Caraballo, 1999; Scott and Dominic, 2003; Ciaramita and Johnson, 2003; Turney et al., 2003; Pasca, 2004; Etzioni et al., 2005; Ritter et al., 2009; Zhang et al., 2011). Evans (2004) considers the web data as a large corpus and uses search engines to identify hypernyms based on lexical patterns. Given an arbitrary document, he takes each capitalized word sequence as an entity and aims to find its potential hypernyms through pattern-based web searching. Suppose X is a capitalized word sequence. Some pattern queries like “such as X” are threw into the search engine. Then, in the retrieved documents, the nouns t"
D13-1122,C10-3004,1,0.782048,"Missing"
D13-1122,W03-1022,0,0.0264129,"into two major categories, i.e., patternbased methods and encyclopedia-based methods. Pattern-based methods make use of manually or automatically constructed patterns to mine hypernym relations from text corpora. The pioneer work by Hearst (1992) finds that linking two noun phrases (NPs) via certain lexical constructions often implies hypernym relations. For example, NP1 is a hypernym of NP2 in the lexical pattern “such NP1 as NP2 ”. Similarly, succeeding researchers follow her work and use handcrafted patterns to extract hypernym pairs from corpora (Caraballo, 1999; Scott and Dominic, 2003; Ciaramita and Johnson, 2003; Turney et al., 2003; Pasca, 2004; Etzioni et al., 2005; Ritter et al., 2009; Zhang et al., 2011). Evans (2004) considers the web data as a large corpus and uses search engines to identify hypernyms based on lexical patterns. Given an arbitrary document, he takes each capitalized word sequence as an entity and aims to find its potential hypernyms through pattern-based web searching. Suppose X is a capitalized word sequence. Some pattern queries like “such as X” are threw into the search engine. Then, in the retrieved documents, the nouns that immediately precede the pattern are recognized as"
D13-1122,C92-2082,0,0.643931,"2006), question answering (McNamee et al., 2008), and so on. Some manually constructed thesauri such as WordNet can also provide some semantic relations such as hypernyms. However, these thesauri are limited in its scope and domain, and manual construction is knowledge-intensive and time-consuming. Therefore, many researchers try to automatically extract semantic relations or to construct taxonomies. Most previous methods on automatic hypernym discovery are based on lexical patterns and suffer from the problem that such patterns can only cover a small part of complex linguistic circumstances (Hearst, 1992; Turney et al., 2003; Zhang et al., 2011). Other work tries to extract hypernym relations from large-scale encyclopedias like Wikipedia and achieves high precision (Suchanek et al., 2008; Hoffart et al., 2012). However, the coverage is limited since there exist many infrequent and new entities that are missing in encyclopedias (Lin et al., 2012). We made similar observation that more than a half of entities in our data set have no entries in the encyclopedias. This paper proposes a simple yet effective distant supervision framework for Chinese open-domain hypernym discovery. Given an entity n"
D13-1122,D12-1082,0,0.0676915,"cally extract semantic relations or to construct taxonomies. Most previous methods on automatic hypernym discovery are based on lexical patterns and suffer from the problem that such patterns can only cover a small part of complex linguistic circumstances (Hearst, 1992; Turney et al., 2003; Zhang et al., 2011). Other work tries to extract hypernym relations from large-scale encyclopedias like Wikipedia and achieves high precision (Suchanek et al., 2008; Hoffart et al., 2012). However, the coverage is limited since there exist many infrequent and new entities that are missing in encyclopedias (Lin et al., 2012). We made similar observation that more than a half of entities in our data set have no entries in the encyclopedias. This paper proposes a simple yet effective distant supervision framework for Chinese open-domain hypernym discovery. Given an entity name, our goal is to discover its hypernyms by leveraging knowledge from multiple sources. Considering the case where a person wants to know the meaning of an unknown entity, he/she may search it in a search engine and then finds out the answer after going through the search results. Furthermore, if he/she finds an entry about the entity in an aut"
D13-1122,I08-2112,0,0.253715,"covery is a task to extract such noun pairs that one noun is a hypernym of the other (Snow et al., 2005). A noun H is a hypernym of another noun E if E is an instance or subclass of H. In other word, H is a semantic class of E. For instance, “actor” is a hypernym of “Mel Gibson”; “dog” is a hypernym of “Caucasian sheepdog”; “medicine” is a hypernym of “Aspirin”. Hypernym discovery is an important subtask of semantic relation extraction ∗ Email correspondence. and has many applications in ontology construction (Suchanek et al., 2008), machine reading (Etzioni et al., 2006), question answering (McNamee et al., 2008), and so on. Some manually constructed thesauri such as WordNet can also provide some semantic relations such as hypernyms. However, these thesauri are limited in its scope and domain, and manual construction is knowledge-intensive and time-consuming. Therefore, many researchers try to automatically extract semantic relations or to construct taxonomies. Most previous methods on automatic hypernym discovery are based on lexical patterns and suffer from the problem that such patterns can only cover a small part of complex linguistic circumstances (Hearst, 1992; Turney et al., 2003; Zhang et al.,"
D13-1122,W03-0415,0,0.0299393,"scovery can be summarized into two major categories, i.e., patternbased methods and encyclopedia-based methods. Pattern-based methods make use of manually or automatically constructed patterns to mine hypernym relations from text corpora. The pioneer work by Hearst (1992) finds that linking two noun phrases (NPs) via certain lexical constructions often implies hypernym relations. For example, NP1 is a hypernym of NP2 in the lexical pattern “such NP1 as NP2 ”. Similarly, succeeding researchers follow her work and use handcrafted patterns to extract hypernym pairs from corpora (Caraballo, 1999; Scott and Dominic, 2003; Ciaramita and Johnson, 2003; Turney et al., 2003; Pasca, 2004; Etzioni et al., 2005; Ritter et al., 2009; Zhang et al., 2011). Evans (2004) considers the web data as a large corpus and uses search engines to identify hypernyms based on lexical patterns. Given an arbitrary document, he takes each capitalized word sequence as an entity and aims to find its potential hypernyms through pattern-based web searching. Suppose X is a capitalized word sequence. Some pattern queries like “such as X” are threw into the search engine. Then, in the retrieved documents, the nouns that immediately precede t"
D13-1122,P11-1116,0,0.100561,"et al., 2008), and so on. Some manually constructed thesauri such as WordNet can also provide some semantic relations such as hypernyms. However, these thesauri are limited in its scope and domain, and manual construction is knowledge-intensive and time-consuming. Therefore, many researchers try to automatically extract semantic relations or to construct taxonomies. Most previous methods on automatic hypernym discovery are based on lexical patterns and suffer from the problem that such patterns can only cover a small part of complex linguistic circumstances (Hearst, 1992; Turney et al., 2003; Zhang et al., 2011). Other work tries to extract hypernym relations from large-scale encyclopedias like Wikipedia and achieves high precision (Suchanek et al., 2008; Hoffart et al., 2012). However, the coverage is limited since there exist many infrequent and new entities that are missing in encyclopedias (Lin et al., 2012). We made similar observation that more than a half of entities in our data set have no entries in the encyclopedias. This paper proposes a simple yet effective distant supervision framework for Chinese open-domain hypernym discovery. Given an entity name, our goal is to discover its hypernyms"
D14-1054,D08-1083,0,0.0209071,"Sentiment Analysis Duyu Tang∗, Furu Wei‡ , Bing Qin , Li Dong]∗ , Ting Liu , Ming Zhou‡  Research Center for Social Computing and Information Retrieval, Harbin Institute of Technology, China ‡ Microsoft Research, Beijing, China ] Beihang University, Beijing, China  {dytang, qinb, tliu}@ir.hit.edu.cn ‡ {fuwei, mingzhou}@microsoft.com ] donglixp@gmail.com Abstract timent classification as a special case of text categorization task. Under this perspective, previous studies typically use pipelined methods with two steps. They first produce sentence segmentations with separate text analyzers (Choi and Cardie, 2008; Nakagawa et al., 2010; Socher et al., 2013b) or bag-of-words (Paltoglou and Thelwall, 2010; Maas et al., 2011). Then, feature learning and sentiment classification algorithms take the segmentation results as inputs to build the sentiment classifier (Socher et al., 2011; Kalchbrenner et al., 2014; Dong et al., 2014). The major disadvantage of a pipelined method is the problem of error propagation, since sentence segmentation errors cannot be corrected by the sentiment classification model. A typical kind of error is caused by the polarity inconsistency between a phrase and the words it contai"
D14-1054,P14-2009,1,0.766966,"hou}@microsoft.com ] donglixp@gmail.com Abstract timent classification as a special case of text categorization task. Under this perspective, previous studies typically use pipelined methods with two steps. They first produce sentence segmentations with separate text analyzers (Choi and Cardie, 2008; Nakagawa et al., 2010; Socher et al., 2013b) or bag-of-words (Paltoglou and Thelwall, 2010; Maas et al., 2011). Then, feature learning and sentiment classification algorithms take the segmentation results as inputs to build the sentiment classifier (Socher et al., 2011; Kalchbrenner et al., 2014; Dong et al., 2014). The major disadvantage of a pipelined method is the problem of error propagation, since sentence segmentation errors cannot be corrected by the sentiment classification model. A typical kind of error is caused by the polarity inconsistency between a phrase and the words it contains, such as hnot bad, badi and ha great deal of, greati. The segmentations based on bag-of-words or syntactic chunkers are not effective enough to handle the polarity inconsistency phenomenons. The reason lies in that bag-of-words segmentations regard each word as a separate unit, which losses the word order and does"
D14-1054,P13-2087,0,0.0212728,"using iterated matrix multiplication. Socher et al. propose Recursive Autoencoder (RAE) (2011), Matrix-Vector Recursive Neural Network (MV-RNN) (2012) and Recursive Neural Tensor Network (RNTN) (2013b) to learn the composition of variable-length phrases based on the representation of its children. To learn the sentence representation, Kalchbrenner et al. (2014) exploit Dynamic Convolutional Neural Network and Le and Mikolov (2014) investigate Paragraph Vector. To learn word vectors for sentiment analysis, Maas et al. (2011) propose a probabilistic document model following Blei et al. (2003), Labutov and Lipson (2013) re-embed words from existing word embeddings and Tang et al. (2014b) develop three neural networks to learn word vectors from tweets containing positive/negative emoticons. Unlike most previous corpus-based algorithms that build sentiment classifier based on splitting a sentence as a word sequence, we produce sentence segmentations automatically within a joint framework, and conduct sentiment classification based on the segmentation results. 3 3.1 Task Definition The task of sentiment classification has been well formalized in previous studies (Pang and Lee, 2008; Liu, 2012). The objective is"
D14-1054,P11-2008,0,0.131527,"Missing"
D14-1054,P11-1015,0,0.3622,"cial Computing and Information Retrieval, Harbin Institute of Technology, China ‡ Microsoft Research, Beijing, China ] Beihang University, Beijing, China  {dytang, qinb, tliu}@ir.hit.edu.cn ‡ {fuwei, mingzhou}@microsoft.com ] donglixp@gmail.com Abstract timent classification as a special case of text categorization task. Under this perspective, previous studies typically use pipelined methods with two steps. They first produce sentence segmentations with separate text analyzers (Choi and Cardie, 2008; Nakagawa et al., 2010; Socher et al., 2013b) or bag-of-words (Paltoglou and Thelwall, 2010; Maas et al., 2011). Then, feature learning and sentiment classification algorithms take the segmentation results as inputs to build the sentiment classifier (Socher et al., 2011; Kalchbrenner et al., 2014; Dong et al., 2014). The major disadvantage of a pipelined method is the problem of error propagation, since sentence segmentation errors cannot be corrected by the sentiment classification model. A typical kind of error is caused by the polarity inconsistency between a phrase and the words it contains, such as hnot bad, badi and ha great deal of, greati. The segmentations based on bag-of-words or syntactic ch"
D14-1054,D13-1171,0,0.046832,"Missing"
D14-1054,J11-2001,0,0.0176143,"notated only with sentiment polarity, without any segmentation annotations. We evaluate the effectiveness of our joint model on a benchmark Twitter sentiment classification dataset in SemEval 2013. Results show that the joint model performs comparably with stateof-the-art methods, and consistently outperforms pipeline methods in various experiment settings. The main contributions of the work presented in this paper are as follows. Related Work Existing approaches for sentiment classification are dominated by two mainstream directions. Lexicon-based approaches (Turney, 2002; Ding et al., 2008; Taboada et al., 2011; Thelwall et al., 2012) typically utilize a lexicon of sentiment words, each of which is annotated with the sentiment polarity or sentiment strength. Linguistic rules such as intensifications and negations are usually incorporated to aggregate the sentiment polarity of sentences (or documents). Corpusbased methods treat sentiment classification as a special case of text categorization task (Pang et al., 2002). They mostly build the sentiment classifier from sentences (or documents) with manually annotated sentiment polarity or distantly-supervised corpora collected by sentiment signals like e"
D14-1054,S13-2053,0,0.183839,"such as hnot bad, badi and ha great deal of, greati. The segmentations based on bag-of-words or syntactic chunkers are not effective enough to handle the polarity inconsistency phenomenons. The reason lies in that bag-of-words segmentations regard each word as a separate unit, which losses the word order and does not capture the phrasal information. The segmentations based on syntactic chunkers typically aim to identify noun groups, verb groups or named entities from a sentence. However, many sentiment indicators are phrases constituted of adjectives, negations, adverbs or idioms (Liu, 2012; Mohammad et al., 2013a), which are splitted by syntactic chunkers. Besides, a better approach would be to utilize the sentiment information to improve the segmentor. Accordingly, the sentiment-specific segmentor will enhance the performance of sentiment classification in turn. In this paper, we propose a joint segmentation and classification framework (JSC) for sentiment analysis, which simultaneous conducts sentence segmentation and sentence-level sentiment classification. The framework is illustrated in FigIn this paper, we propose a joint segmentation and classification framework for sentiment analysis. Existin"
D14-1054,C14-1018,1,0.854768,"encoder (RAE) (2011), Matrix-Vector Recursive Neural Network (MV-RNN) (2012) and Recursive Neural Tensor Network (RNTN) (2013b) to learn the composition of variable-length phrases based on the representation of its children. To learn the sentence representation, Kalchbrenner et al. (2014) exploit Dynamic Convolutional Neural Network and Le and Mikolov (2014) investigate Paragraph Vector. To learn word vectors for sentiment analysis, Maas et al. (2011) propose a probabilistic document model following Blei et al. (2003), Labutov and Lipson (2013) re-embed words from existing word embeddings and Tang et al. (2014b) develop three neural networks to learn word vectors from tweets containing positive/negative emoticons. Unlike most previous corpus-based algorithms that build sentiment classifier based on splitting a sentence as a word sequence, we produce sentence segmentations automatically within a joint framework, and conduct sentiment classification based on the segmentation results. 3 3.1 Task Definition The task of sentiment classification has been well formalized in previous studies (Pang and Lee, 2008; Liu, 2012). The objective is to identify the sentiment polarity of a sentence (or document) as"
D14-1054,N10-1120,0,0.161612,"Tang∗, Furu Wei‡ , Bing Qin , Li Dong]∗ , Ting Liu , Ming Zhou‡  Research Center for Social Computing and Information Retrieval, Harbin Institute of Technology, China ‡ Microsoft Research, Beijing, China ] Beihang University, Beijing, China  {dytang, qinb, tliu}@ir.hit.edu.cn ‡ {fuwei, mingzhou}@microsoft.com ] donglixp@gmail.com Abstract timent classification as a special case of text categorization task. Under this perspective, previous studies typically use pipelined methods with two steps. They first produce sentence segmentations with separate text analyzers (Choi and Cardie, 2008; Nakagawa et al., 2010; Socher et al., 2013b) or bag-of-words (Paltoglou and Thelwall, 2010; Maas et al., 2011). Then, feature learning and sentiment classification algorithms take the segmentation results as inputs to build the sentiment classifier (Socher et al., 2011; Kalchbrenner et al., 2014; Dong et al., 2014). The major disadvantage of a pipelined method is the problem of error propagation, since sentence segmentation errors cannot be corrected by the sentiment classification model. A typical kind of error is caused by the polarity inconsistency between a phrase and the words it contains, such as hnot bad, b"
D14-1054,P14-1146,1,0.670131,"encoder (RAE) (2011), Matrix-Vector Recursive Neural Network (MV-RNN) (2012) and Recursive Neural Tensor Network (RNTN) (2013b) to learn the composition of variable-length phrases based on the representation of its children. To learn the sentence representation, Kalchbrenner et al. (2014) exploit Dynamic Convolutional Neural Network and Le and Mikolov (2014) investigate Paragraph Vector. To learn word vectors for sentiment analysis, Maas et al. (2011) propose a probabilistic document model following Blei et al. (2003), Labutov and Lipson (2013) re-embed words from existing word embeddings and Tang et al. (2014b) develop three neural networks to learn word vectors from tweets containing positive/negative emoticons. Unlike most previous corpus-based algorithms that build sentiment classifier based on splitting a sentence as a word sequence, we produce sentence segmentations automatically within a joint framework, and conduct sentiment classification based on the segmentation results. 3 3.1 Task Definition The task of sentiment classification has been well formalized in previous studies (Pang and Lee, 2008; Liu, 2012). The objective is to identify the sentiment polarity of a sentence (or document) as"
D14-1054,pak-paroubek-2010-twitter,0,0.0427636,"pically utilize a lexicon of sentiment words, each of which is annotated with the sentiment polarity or sentiment strength. Linguistic rules such as intensifications and negations are usually incorporated to aggregate the sentiment polarity of sentences (or documents). Corpusbased methods treat sentiment classification as a special case of text categorization task (Pang et al., 2002). They mostly build the sentiment classifier from sentences (or documents) with manually annotated sentiment polarity or distantly-supervised corpora collected by sentiment signals like emoticons (Go et al., 2009; Pak and Paroubek, 2010; Kouloumpis et al., 2011; Zhao et al., 2012). Majority of existing approaches follow Pang et al. (2002) and employ corpus-based method for sentiment classification. Pang et al. (2002) pioneer to treat the sentiment classification of reviews as a special case of text categorization problem and first investigate machine learning methods. They employ Naive Bayes, Maximum Entropy and Support Vector Machines (SVM) with a diverse set of features. In their experiments, the best performance is achieved by SVM with bagof-words feature. Under this perspective, many studies focus on designing or learnin"
D14-1054,P10-1141,0,0.148379,"Zhou‡  Research Center for Social Computing and Information Retrieval, Harbin Institute of Technology, China ‡ Microsoft Research, Beijing, China ] Beihang University, Beijing, China  {dytang, qinb, tliu}@ir.hit.edu.cn ‡ {fuwei, mingzhou}@microsoft.com ] donglixp@gmail.com Abstract timent classification as a special case of text categorization task. Under this perspective, previous studies typically use pipelined methods with two steps. They first produce sentence segmentations with separate text analyzers (Choi and Cardie, 2008; Nakagawa et al., 2010; Socher et al., 2013b) or bag-of-words (Paltoglou and Thelwall, 2010; Maas et al., 2011). Then, feature learning and sentiment classification algorithms take the segmentation results as inputs to build the sentiment classifier (Socher et al., 2011; Kalchbrenner et al., 2014; Dong et al., 2014). The major disadvantage of a pipelined method is the problem of error propagation, since sentence segmentation errors cannot be corrected by the sentiment classification model. A typical kind of error is caused by the polarity inconsistency between a phrase and the words it contains, such as hnot bad, badi and ha great deal of, greati. The segmentations based on bag-of-w"
D14-1054,P02-1053,0,0.0058165,"the joint model from sentences annotated only with sentiment polarity, without any segmentation annotations. We evaluate the effectiveness of our joint model on a benchmark Twitter sentiment classification dataset in SemEval 2013. Results show that the joint model performs comparably with stateof-the-art methods, and consistently outperforms pipeline methods in various experiment settings. The main contributions of the work presented in this paper are as follows. Related Work Existing approaches for sentiment classification are dominated by two mainstream directions. Lexicon-based approaches (Turney, 2002; Ding et al., 2008; Taboada et al., 2011; Thelwall et al., 2012) typically utilize a lexicon of sentiment words, each of which is annotated with the sentiment polarity or sentiment strength. Linguistic rules such as intensifications and negations are usually incorporated to aggregate the sentiment polarity of sentences (or documents). Corpusbased methods treat sentiment classification as a special case of text categorization task (Pang et al., 2002). They mostly build the sentiment classifier from sentences (or documents) with manually annotated sentiment polarity or distantly-supervised corp"
D14-1054,W02-1011,0,0.0553792,"fication performance. The joint model is trained only based on the annotated sentiment polarity of sentences, without any segmentation annotations. Experiments on a benchmark Twitter sentiment classification dataset in SemEval 2013 show that, our joint model performs comparably with the state-of-the-art methods. 1 Introduction Sentiment classification, which classifies the sentiment polarity of a sentence (or document) as positive or negative, is a major research direction in the field of sentiment analysis (Pang and Lee, 2008; Liu, 2012; Feldman, 2013). Majority of existing approaches follow Pang et al. (2002) and treat sen∗ This work was partly done when the first and fourth authors were visiting Microsoft Research. 477 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 477–487, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics CG SC SEG SEG SC that is not bad -1 <+1,-1> NO 0.6 0.6 that is not bad that is not bad -1 <+1,-1> NO 0.4 0.4 Polarity: +1 that is not bad +1 <+1,+1> YES 2.3 2.3 that is not bad +1 <+1,+1> YES 1.6 1.6 Segmentations Polarity Update Rank Update Input Top K Figure 1: The joint segmentation and c"
D14-1054,P12-2018,0,0.141338,"l. (2002) and employ corpus-based method for sentiment classification. Pang et al. (2002) pioneer to treat the sentiment classification of reviews as a special case of text categorization problem and first investigate machine learning methods. They employ Naive Bayes, Maximum Entropy and Support Vector Machines (SVM) with a diverse set of features. In their experiments, the best performance is achieved by SVM with bagof-words feature. Under this perspective, many studies focus on designing or learning effective features to obtain better classification performance. On movie or product reviews, Wang and Manning (2012) present NBSVM, which trades-off • To our knowledge, this is the first work that automatically produces sentence segmentation for sentiment classification within a joint framework. • We show that the joint model yields comparable performance with the state-of-the-art methods on the benchmark Twitter sentiment classification datasets in SemEval 2013. 478 overview of the proposed joint segmentation and classification model (JSC) for sentiment analysis. The segmentation candidate generation model and the segmentation ranking model are described in Section 4. The details of the sentiment classific"
D14-1054,H05-1044,0,0.0828802,"Missing"
D14-1054,D11-1014,0,0.256554,"ytang, qinb, tliu}@ir.hit.edu.cn ‡ {fuwei, mingzhou}@microsoft.com ] donglixp@gmail.com Abstract timent classification as a special case of text categorization task. Under this perspective, previous studies typically use pipelined methods with two steps. They first produce sentence segmentations with separate text analyzers (Choi and Cardie, 2008; Nakagawa et al., 2010; Socher et al., 2013b) or bag-of-words (Paltoglou and Thelwall, 2010; Maas et al., 2011). Then, feature learning and sentiment classification algorithms take the segmentation results as inputs to build the sentiment classifier (Socher et al., 2011; Kalchbrenner et al., 2014; Dong et al., 2014). The major disadvantage of a pipelined method is the problem of error propagation, since sentence segmentation errors cannot be corrected by the sentiment classification model. A typical kind of error is caused by the polarity inconsistency between a phrase and the words it contains, such as hnot bad, badi and ha great deal of, greati. The segmentations based on bag-of-words or syntactic chunkers are not effective enough to handle the polarity inconsistency phenomenons. The reason lies in that bag-of-words segmentations regard each word as a sepa"
D14-1054,D13-1016,0,0.0546336,"Missing"
D14-1054,D12-1110,0,0.0455462,"Missing"
D14-1054,D11-1016,0,0.0130106,"the document feature. On Twitter, Mohammad et al. (2013b) develop a state-of-the-art Twitter sentiment classifier in SemEval 2013, using a variety of sentiment lexicons and hand-crafted features. With the revival of deep learning (representation learning (Hinton and Salakhutdinov, 2006; Bengio et al., 2013; Jones, 2014)), more recent studies focus on learning the low-dimensional, dense and real-valued vector as text features for sentiment classification. Glorot et al. (2011) investigate Stacked Denoising Autoencoders to learn document vector for domain adaptation in sentiment classification. Yessenalina and Cardie (2011) represent each word as a matrix and compose words using iterated matrix multiplication. Socher et al. propose Recursive Autoencoder (RAE) (2011), Matrix-Vector Recursive Neural Network (MV-RNN) (2012) and Recursive Neural Tensor Network (RNTN) (2013b) to learn the composition of variable-length phrases based on the representation of its children. To learn the sentence representation, Kalchbrenner et al. (2014) exploit Dynamic Convolutional Neural Network and Le and Mikolov (2014) investigate Paragraph Vector. To learn word vectors for sentiment analysis, Maas et al. (2011) propose a probabili"
D14-1054,P14-1011,1,0.830375,"ords or phrases of variable length. Under this scenario, phrase embedding is highly suitable as it is capable to represent phrases with different length into a consistent distributed vector space (Mikolov et al., 2013). For each phrase, phrase embedding is a dense, real-valued and continuous vector. After the phrase embedding is trained, the nearest neighbors in the embedding space are favored to have similar grammatical usages and semantic meanings. The effectiveness of phrase embedding has been verified for building large-scale sentiment lexicon (Tang et al., 2014a) and machine translation (Zhang et al., 2014). We learn phrase embedding with Skip-Gram model (Mikolov et al., 2013), which is the state-of(2) k where φij is the segmentation score of Ωij ; sf eijk is the k-th segmentation feature of Ωij ; w and b are the parameters of the segmentation ranking model. During training, given a sentence si and its gold sentiment polarity polig , the optimization objec2 j∈Hi Segmentation-Specific Feature We empirically design four segmentation-specific features to reflect the information of each segmentation, as listed in Table 3. The objective of the segmentation ranking model is to assign a scalar to each"
D14-1054,D13-1170,0,0.203308,"ng Qin , Li Dong]∗ , Ting Liu , Ming Zhou‡  Research Center for Social Computing and Information Retrieval, Harbin Institute of Technology, China ‡ Microsoft Research, Beijing, China ] Beihang University, Beijing, China  {dytang, qinb, tliu}@ir.hit.edu.cn ‡ {fuwei, mingzhou}@microsoft.com ] donglixp@gmail.com Abstract timent classification as a special case of text categorization task. Under this perspective, previous studies typically use pipelined methods with two steps. They first produce sentence segmentations with separate text analyzers (Choi and Cardie, 2008; Nakagawa et al., 2010; Socher et al., 2013b) or bag-of-words (Paltoglou and Thelwall, 2010; Maas et al., 2011). Then, feature learning and sentiment classification algorithms take the segmentation results as inputs to build the sentiment classifier (Socher et al., 2011; Kalchbrenner et al., 2014; Dong et al., 2014). The major disadvantage of a pipelined method is the problem of error propagation, since sentence segmentation errors cannot be corrected by the sentiment classification model. A typical kind of error is caused by the polarity inconsistency between a phrase and the words it contains, such as hnot bad, badi and ha great deal"
D14-1054,J13-3004,0,\N,Missing
D14-1054,P11-1016,1,\N,Missing
D15-1167,P14-1023,0,0.00747045,"xt generation task. Despite the effectiveness of feature engineering, it is labor intensive and unable to extract and organize the discriminative information from data (Bengio et al., 2015). Recently, neural network emerges as an effective way to learn continuous text representation for sentiment classification. Existing studies in this direction can be divided into two groups. One line of research focuses on learning continuous word embedding. Traditional embedding learning algorithms typically leverage contexts of words in a context-prediction way (Bengio et al., 2003; Mikolov et al., 2013; Baroni et al., 2014). Since these methods typically map words with similar contexts but opposite polarity (e.g. “good” and “bad”) to neighboring vectors, several studies (Maas et al., 2011; Labutov and Lipson, 2013; Tang et al., 2014) learn sentiment-specific word embeddings by taking sentiment of texts into account. Another line of research concentrates on semantic composition (Mitchell and Lapata, 2010). Yessenalina and Cardie (2011) represent each word as a matrix and use iterated matrix multiplication as phrase-level composition function. Socher et al. (2013b) introduce a family of recursive neural networks f"
D15-1167,W06-3808,0,0.0107953,"edNN Avg and Bi GatedNN Avg obtains comparable performances with GatedNN. 4 Related Work Document level sentiment classification is a fundamental problem in sentiment analysis (Pang and Lee, 2008; Liu, 2012), which aims at identifying the sentiment label of a document (Pang et al., 2002; Turney, 2002). Pang and Lee (2002; 2005) 1428 cast this problem as a classification task, and use machine learning method in a supervised learning framework. Turney (2002) introduces an unsupervised approach by using sentiment words/phrases extracted from syntactic patterns to determine the document polarity. Goldberg and Zhu (2006) place this task in a semi-supervised setting, and use unlabelled reviews with graph-based method. Dominant studies in literature follow Pang et al. (2002) and work on designing effective features for building a powerful sentiment classifier. Representative features include word ngrams (Wang and Manning, 2012), text topic (Ganu et al., 2009), bag-of-opinions (Qu et al., 2010), syntactic relations (Xia and Zong, 2010), sentiment lexicon features (Kiritchenko et al., 2014). sition (Kim, 2014; Kalchbrenner et al., 2014; Denil et al., 2014; Johnson and Zhang, 2015) by automatically capturing local"
D15-1167,P13-1088,0,0.0400688,"er line of research concentrates on semantic composition (Mitchell and Lapata, 2010). Yessenalina and Cardie (2011) represent each word as a matrix and use iterated matrix multiplication as phrase-level composition function. Socher et al. (2013b) introduce a family of recursive neural networks for sentence-level semantic composition. Recursive neural network is extended with global feedbackward (Paulus et al., 2014), feature weight tuning (Li, 2014), deep recursive layer (Irsoy and Cardie, 2014), adaptive composition functions (Dong et al., 2014), combined with Combinatory Categorial Grammar (Hermann and Blunsom, 2013), and used for opinion relation detection (Xu et al., 2014). Glorot et al. (2011) use stacked denoising autoencoder. Convolutional neural networks are widely used for semantic compo5 Conclusion We introduce neural network models (ConvGRNN and LSTM-GRNN) for document level sentiment classification. The approach encodes semantics of sentences and their relations in document representation, and is effectively trained end-to-end with supervised sentiment classification objectives. We conduct extensive experiments on four review datasets with two evaluation metrics. Empirical results show that our"
D15-1167,N15-1011,0,0.619314,"can be randomly initialized from a uniform distribution (Socher et al., 2013b), or be pre-trained from text corpus with embedding learning algorithms (Mikolov et al., 2013; Pennington et al., 2014; Tang et al., 2014). We adopt the latter strategy to make better use of semantic and grammatical associations of words. We use convolutional neural network (CNN) and long short-term memory (LSTM) to compute continuous representations of sentences with semantic composition. CNN and LSTM are stateof-the-art semantic composition models for sentiment classification (Kim, 2014; Kalchbrenner et al., 2014; Johnson and Zhang, 2015; Li et al., 2015a). They learn fixed-length vectors for sentences of varying length, captures words order in a sentence and does not depend on external dependency or constituency parse results. One could also use tree-based composition method such as Recursive Neural Tensor Network (Socher et al., 2013b) or Tree-Structured LSTM (Tai et al., 2015; Zhu et al., 2015) as alternatives. Specifically, we try CNN with multiple convolutional filters of different widths (Tang et al., 2015) to produce sentence representation. Figure 2 displays the method. We use multiple convolutional filters in order t"
D15-1167,P14-1062,0,0.857869,"y size. These word vectors can be randomly initialized from a uniform distribution (Socher et al., 2013b), or be pre-trained from text corpus with embedding learning algorithms (Mikolov et al., 2013; Pennington et al., 2014; Tang et al., 2014). We adopt the latter strategy to make better use of semantic and grammatical associations of words. We use convolutional neural network (CNN) and long short-term memory (LSTM) to compute continuous representations of sentences with semantic composition. CNN and LSTM are stateof-the-art semantic composition models for sentiment classification (Kim, 2014; Kalchbrenner et al., 2014; Johnson and Zhang, 2015; Li et al., 2015a). They learn fixed-length vectors for sentences of varying length, captures words order in a sentence and does not depend on external dependency or constituency parse results. One could also use tree-based composition method such as Recursive Neural Tensor Network (Socher et al., 2013b) or Tree-Structured LSTM (Tai et al., 2015; Zhu et al., 2015) as alternatives. Specifically, we try CNN with multiple convolutional filters of different widths (Tang et al., 2015) to produce sentence representation. Figure 2 displays the method. We use multiple convolu"
D15-1167,W02-1011,0,0.120139,"gated recurrent neural network dramatically outperforms standard recurrent neural network in document modeling for sentiment classification.1 1 Introduction Document level sentiment classification is a fundamental task in sentiment analysis, and is crucial to understand user generated content in social networks or product reviews (Manning and Sch¨utze, 1999; Jurafsky and Martin, 2000; Pang and Lee, 2008; Liu, 2012). The task calls for identifying the overall sentiment polarity (e.g. thumbs up or thumbs down, 1-5 stars on review sites) of a document. In literature, dominant approaches follow (Pang et al., 2002) and exploit machine learn∗ Corresponding author. Codes and datasets are publicly available at http://ir.hit.edu.cn/˜dytang. 1 ing algorithm to build sentiment classifier. Many of them focus on designing hand-crafted features (Qu et al., 2010; Paltoglou and Thelwall, 2010) or learning discriminate features from data, since the performance of a machine learner is heavily dependent on the choice of data representation (Bengio et al., 2015). Document level sentiment classification remains a significant challenge: how to encode the intrinsic (semantic or syntactic) relations between sentences in t"
D15-1167,P13-2087,0,0.00447156,"Recently, neural network emerges as an effective way to learn continuous text representation for sentiment classification. Existing studies in this direction can be divided into two groups. One line of research focuses on learning continuous word embedding. Traditional embedding learning algorithms typically leverage contexts of words in a context-prediction way (Bengio et al., 2003; Mikolov et al., 2013; Baroni et al., 2014). Since these methods typically map words with similar contexts but opposite polarity (e.g. “good” and “bad”) to neighboring vectors, several studies (Maas et al., 2011; Labutov and Lipson, 2013; Tang et al., 2014) learn sentiment-specific word embeddings by taking sentiment of texts into account. Another line of research concentrates on semantic composition (Mitchell and Lapata, 2010). Yessenalina and Cardie (2011) represent each word as a matrix and use iterated matrix multiplication as phrase-level composition function. Socher et al. (2013b) introduce a family of recursive neural networks for sentence-level semantic composition. Recursive neural network is extended with global feedbackward (Paulus et al., 2014), feature weight tuning (Li, 2014), deep recursive layer (Irsoy and Car"
D15-1167,D14-1162,0,0.131068,"scribe word vector representation, before presenting a convolutional neural network with multiple filters for sentence composition. Each word is represented as a low dimensional, continuous and real-valued vector, also known as word embedding (Bengio et al., 2003). All the word vectors are stacked in a word embedding matrix Lw ∈ Rd×|V |, where d is the dimension of word vector and |V |is vocabulary size. These word vectors can be randomly initialized from a uniform distribution (Socher et al., 2013b), or be pre-trained from text corpus with embedding learning algorithms (Mikolov et al., 2013; Pennington et al., 2014; Tang et al., 2014). We adopt the latter strategy to make better use of semantic and grammatical associations of words. We use convolutional neural network (CNN) and long short-term memory (LSTM) to compute continuous representations of sentences with semantic composition. CNN and LSTM are stateof-the-art semantic composition models for sentiment classification (Kim, 2014; Kalchbrenner et al., 2014; Johnson and Zhang, 2015; Li et al., 2015a). They learn fixed-length vectors for sentences of varying length, captures words order in a sentence and does not depend on external dependency or consti"
D15-1167,D15-1278,0,0.409141,"Missing"
D15-1167,C10-1103,0,0.0117788,"d is crucial to understand user generated content in social networks or product reviews (Manning and Sch¨utze, 1999; Jurafsky and Martin, 2000; Pang and Lee, 2008; Liu, 2012). The task calls for identifying the overall sentiment polarity (e.g. thumbs up or thumbs down, 1-5 stars on review sites) of a document. In literature, dominant approaches follow (Pang et al., 2002) and exploit machine learn∗ Corresponding author. Codes and datasets are publicly available at http://ir.hit.edu.cn/˜dytang. 1 ing algorithm to build sentiment classifier. Many of them focus on designing hand-crafted features (Qu et al., 2010; Paltoglou and Thelwall, 2010) or learning discriminate features from data, since the performance of a machine learner is heavily dependent on the choice of data representation (Bengio et al., 2015). Document level sentiment classification remains a significant challenge: how to encode the intrinsic (semantic or syntactic) relations between sentences in the semantic meaning of document. This is crucial for sentiment classification because relations like “contrast” and “cause” have great influences on determining the meaning and the overall polarity of a document. However, existing studies typ"
D15-1167,P15-1107,0,0.501227,"Missing"
D15-1167,P13-1045,0,0.0387591,"ection 2.3). 2.1 Average Tanh Pooling Filter 1 Filter 2 Filter 3 Convolution Lookup Sentence Composition We first describe word vector representation, before presenting a convolutional neural network with multiple filters for sentence composition. Each word is represented as a low dimensional, continuous and real-valued vector, also known as word embedding (Bengio et al., 2003). All the word vectors are stacked in a word embedding matrix Lw ∈ Rd×|V |, where d is the dimension of word vector and |V |is vocabulary size. These word vectors can be randomly initialized from a uniform distribution (Socher et al., 2013b), or be pre-trained from text corpus with embedding learning algorithms (Mikolov et al., 2013; Pennington et al., 2014; Tang et al., 2014). We adopt the latter strategy to make better use of semantic and grammatical associations of words. We use convolutional neural network (CNN) and long short-term memory (LSTM) to compute continuous representations of sentences with semantic composition. CNN and LSTM are stateof-the-art semantic composition models for sentiment classification (Kim, 2014; Kalchbrenner et al., 2014; Johnson and Zhang, 2015; Li et al., 2015a). They learn fixed-length vectors"
D15-1167,P11-1015,0,0.552207,"ngio et al., 2015). Recently, neural network emerges as an effective way to learn continuous text representation for sentiment classification. Existing studies in this direction can be divided into two groups. One line of research focuses on learning continuous word embedding. Traditional embedding learning algorithms typically leverage contexts of words in a context-prediction way (Bengio et al., 2003; Mikolov et al., 2013; Baroni et al., 2014). Since these methods typically map words with similar contexts but opposite polarity (e.g. “good” and “bad”) to neighboring vectors, several studies (Maas et al., 2011; Labutov and Lipson, 2013; Tang et al., 2014) learn sentiment-specific word embeddings by taking sentiment of texts into account. Another line of research concentrates on semantic composition (Mitchell and Lapata, 2010). Yessenalina and Cardie (2011) represent each word as a matrix and use iterated matrix multiplication as phrase-level composition function. Socher et al. (2013b) introduce a family of recursive neural networks for sentence-level semantic composition. Recursive neural network is extended with global feedbackward (Paulus et al., 2014), feature weight tuning (Li, 2014), deep recu"
D15-1167,P14-5010,0,0.019412,"Missing"
D15-1167,P10-1141,0,0.0122869,"nderstand user generated content in social networks or product reviews (Manning and Sch¨utze, 1999; Jurafsky and Martin, 2000; Pang and Lee, 2008; Liu, 2012). The task calls for identifying the overall sentiment polarity (e.g. thumbs up or thumbs down, 1-5 stars on review sites) of a document. In literature, dominant approaches follow (Pang et al., 2002) and exploit machine learn∗ Corresponding author. Codes and datasets are publicly available at http://ir.hit.edu.cn/˜dytang. 1 ing algorithm to build sentiment classifier. Many of them focus on designing hand-crafted features (Qu et al., 2010; Paltoglou and Thelwall, 2010) or learning discriminate features from data, since the performance of a machine learner is heavily dependent on the choice of data representation (Bengio et al., 2015). Document level sentiment classification remains a significant challenge: how to encode the intrinsic (semantic or syntactic) relations between sentences in the semantic meaning of document. This is crucial for sentiment classification because relations like “contrast” and “cause” have great influences on determining the meaning and the overall polarity of a document. However, existing studies typically fail to effectively capt"
D15-1167,P05-1015,0,0.42979,"re our methods (Conv-GRNN and LSTM-GRNN) with the following baseline methods for document level sentiment classification. (1) Majority is a heuristic baseline, which assigns the majority sentiment label in training set to each document in test set. (2) In SVM+Ngrams, we use bag-of-unigrams and bag-of-bigrams as features and train SVM classifier with LibLinear (Fan et al., 2008)5 . (3) In TextFeatures, we implement sophisticated features (Kiritchenko et al., 2014) including word ngrams, character ngrams, sentiment lexicon features, cluster features, et al. 5 We also try discretized regression (Pang and Lee, 2005) with fixed decision thresholds (e.g. 0.5, 1.5, 2.5, ...). However, its performance is obviously worse than SVM classifier. 1426 Majority SVM + Unigrams SVM + Bigrams SVM + TextFeatures SVM + AverageSG SVM + SSWE JMARS Paragraph Vector Convolutional NN Conv-GRNN LSTM-GRNN Yelp 2013 Accuracy MSE 0.356 3.06 0.589 0.79 0.576 0.75 0.598 0.68 0.543 1.11 0.535 1.12 N/A – 0.577 0.86 0.597 0.76 0.637 0.56 0.651 0.50 Yelp 2014 Accuracy MSE 0.361 3.28 0.600 0.78 0.616 0.65 0.618 0.63 0.557 1.08 0.543 1.13 N/A – 0.592 0.70 0.610 0.68 0.655 0.51 0.671 0.48 Yelp 2015 Accuracy MSE 0.369 3.30 0.611 0.75 0.62"
D15-1167,D13-1170,0,0.490494,"ection 2.3). 2.1 Average Tanh Pooling Filter 1 Filter 2 Filter 3 Convolution Lookup Sentence Composition We first describe word vector representation, before presenting a convolutional neural network with multiple filters for sentence composition. Each word is represented as a low dimensional, continuous and real-valued vector, also known as word embedding (Bengio et al., 2003). All the word vectors are stacked in a word embedding matrix Lw ∈ Rd×|V |, where d is the dimension of word vector and |V |is vocabulary size. These word vectors can be randomly initialized from a uniform distribution (Socher et al., 2013b), or be pre-trained from text corpus with embedding learning algorithms (Mikolov et al., 2013; Pennington et al., 2014; Tang et al., 2014). We adopt the latter strategy to make better use of semantic and grammatical associations of words. We use convolutional neural network (CNN) and long short-term memory (LSTM) to compute continuous representations of sentences with semantic composition. CNN and LSTM are stateof-the-art semantic composition models for sentiment classification (Kim, 2014; Kalchbrenner et al., 2014; Johnson and Zhang, 2015; Li et al., 2015a). They learn fixed-length vectors"
D15-1167,P15-1150,0,0.772158,"NN) and long short-term memory (LSTM) to compute continuous representations of sentences with semantic composition. CNN and LSTM are stateof-the-art semantic composition models for sentiment classification (Kim, 2014; Kalchbrenner et al., 2014; Johnson and Zhang, 2015; Li et al., 2015a). They learn fixed-length vectors for sentences of varying length, captures words order in a sentence and does not depend on external dependency or constituency parse results. One could also use tree-based composition method such as Recursive Neural Tensor Network (Socher et al., 2013b) or Tree-Structured LSTM (Tai et al., 2015; Zhu et al., 2015) as alternatives. Specifically, we try CNN with multiple convolutional filters of different widths (Tang et al., 2015) to produce sentence representation. Figure 2 displays the method. We use multiple convolutional filters in order to capture local semantics of n-grams of various granularities, which have been proven effective for sentiment classification. For example, a convolutional filter with a width of 2 essentially captures the semantics of bigrams in a sentence. In this work, we use three convolutional filters whose widths are 1, 2 and 3 to encode the semantics of uni"
D15-1167,P14-1146,1,0.843593,"entation, before presenting a convolutional neural network with multiple filters for sentence composition. Each word is represented as a low dimensional, continuous and real-valued vector, also known as word embedding (Bengio et al., 2003). All the word vectors are stacked in a word embedding matrix Lw ∈ Rd×|V |, where d is the dimension of word vector and |V |is vocabulary size. These word vectors can be randomly initialized from a uniform distribution (Socher et al., 2013b), or be pre-trained from text corpus with embedding learning algorithms (Mikolov et al., 2013; Pennington et al., 2014; Tang et al., 2014). We adopt the latter strategy to make better use of semantic and grammatical associations of words. We use convolutional neural network (CNN) and long short-term memory (LSTM) to compute continuous representations of sentences with semantic composition. CNN and LSTM are stateof-the-art semantic composition models for sentiment classification (Kim, 2014; Kalchbrenner et al., 2014; Johnson and Zhang, 2015; Li et al., 2015a). They learn fixed-length vectors for sentences of varying length, captures words order in a sentence and does not depend on external dependency or constituency parse results"
D15-1167,P15-1098,1,0.437565,"ateof-the-art semantic composition models for sentiment classification (Kim, 2014; Kalchbrenner et al., 2014; Johnson and Zhang, 2015; Li et al., 2015a). They learn fixed-length vectors for sentences of varying length, captures words order in a sentence and does not depend on external dependency or constituency parse results. One could also use tree-based composition method such as Recursive Neural Tensor Network (Socher et al., 2013b) or Tree-Structured LSTM (Tai et al., 2015; Zhu et al., 2015) as alternatives. Specifically, we try CNN with multiple convolutional filters of different widths (Tang et al., 2015) to produce sentence representation. Figure 2 displays the method. We use multiple convolutional filters in order to capture local semantics of n-grams of various granularities, which have been proven effective for sentiment classification. For example, a convolutional filter with a width of 2 essentially captures the semantics of bigrams in a sentence. In this work, we use three convolutional filters whose widths are 1, 2 and 3 to encode the semantics of unigrams, bigramw1 w2 w3 w4 …… w?−1 w? Figure 2: Sentence composition with convolutional neural network. s and trigrams in a sentence. Each"
D15-1167,P02-1053,0,0.0607185,"es, GatedNN obtains dramatic accuracy improvements over Recurrent and significantly outperforms previous settings. The results indicate that Gated RNN is capable of handling the vanishing gradient problem to some extend, and it is practical to adaptively model sentence semantics in document representation. GatedNN Avg and Bi GatedNN Avg obtains comparable performances with GatedNN. 4 Related Work Document level sentiment classification is a fundamental problem in sentiment analysis (Pang and Lee, 2008; Liu, 2012), which aims at identifying the sentiment label of a document (Pang et al., 2002; Turney, 2002). Pang and Lee (2002; 2005) 1428 cast this problem as a classification task, and use machine learning method in a supervised learning framework. Turney (2002) introduces an unsupervised approach by using sentiment words/phrases extracted from syntactic patterns to determine the document polarity. Goldberg and Zhu (2006) place this task in a semi-supervised setting, and use unlabelled reviews with graph-based method. Dominant studies in literature follow Pang et al. (2002) and work on designing effective features for building a powerful sentiment classifier. Representative features include word"
D15-1167,P12-2018,0,0.86286,"rformance of a machine learner is heavily dependent on the choice of data representation (Bengio et al., 2015). Document level sentiment classification remains a significant challenge: how to encode the intrinsic (semantic or syntactic) relations between sentences in the semantic meaning of document. This is crucial for sentiment classification because relations like “contrast” and “cause” have great influences on determining the meaning and the overall polarity of a document. However, existing studies typically fail to effectively capture such information. For example, Pang et al. (2002) and Wang and Manning (2012) represent documents with bag-of-ngrams features and build SVM classifier upon that. Although such feature-driven SVM is an extremely strong performer and hardly to be transcended, its “sparse” and “discrete” characteristics make it clumsy in taking into account of side information like relations between sentences. Recently, Le and Mikolov (2014) exploit neural networks to learn continuous document representation from data. Essentially, they use local ngram information and do not capture semantic relations between sentences. Furthermore, a person asked to do this task will naturally carry it o"
D15-1167,C10-2153,0,0.0144539,"ed learning framework. Turney (2002) introduces an unsupervised approach by using sentiment words/phrases extracted from syntactic patterns to determine the document polarity. Goldberg and Zhu (2006) place this task in a semi-supervised setting, and use unlabelled reviews with graph-based method. Dominant studies in literature follow Pang et al. (2002) and work on designing effective features for building a powerful sentiment classifier. Representative features include word ngrams (Wang and Manning, 2012), text topic (Ganu et al., 2009), bag-of-opinions (Qu et al., 2010), syntactic relations (Xia and Zong, 2010), sentiment lexicon features (Kiritchenko et al., 2014). sition (Kim, 2014; Kalchbrenner et al., 2014; Denil et al., 2014; Johnson and Zhang, 2015) by automatically capturing local and global semantics. Le and Mikolov (2014) introduce Paragraph Vector to learn document representation from semantics of words. Sequential model like recurrent neural network or long short-term memory (LSTM) are also verified as strong approaches for semantic composition (Li et al., 2015a). In this work, we represent document with convolutional-gated recurrent neural network, which adaptively encodes semantics of s"
D15-1167,C14-1064,0,0.0121979,"d Lapata, 2010). Yessenalina and Cardie (2011) represent each word as a matrix and use iterated matrix multiplication as phrase-level composition function. Socher et al. (2013b) introduce a family of recursive neural networks for sentence-level semantic composition. Recursive neural network is extended with global feedbackward (Paulus et al., 2014), feature weight tuning (Li, 2014), deep recursive layer (Irsoy and Cardie, 2014), adaptive composition functions (Dong et al., 2014), combined with Combinatory Categorial Grammar (Hermann and Blunsom, 2013), and used for opinion relation detection (Xu et al., 2014). Glorot et al. (2011) use stacked denoising autoencoder. Convolutional neural networks are widely used for semantic compo5 Conclusion We introduce neural network models (ConvGRNN and LSTM-GRNN) for document level sentiment classification. The approach encodes semantics of sentences and their relations in document representation, and is effectively trained end-to-end with supervised sentiment classification objectives. We conduct extensive experiments on four review datasets with two evaluation metrics. Empirical results show that our approaches achieve state-of-the-art performances on all the"
D15-1167,D11-1016,0,0.00442251,"on learning continuous word embedding. Traditional embedding learning algorithms typically leverage contexts of words in a context-prediction way (Bengio et al., 2003; Mikolov et al., 2013; Baroni et al., 2014). Since these methods typically map words with similar contexts but opposite polarity (e.g. “good” and “bad”) to neighboring vectors, several studies (Maas et al., 2011; Labutov and Lipson, 2013; Tang et al., 2014) learn sentiment-specific word embeddings by taking sentiment of texts into account. Another line of research concentrates on semantic composition (Mitchell and Lapata, 2010). Yessenalina and Cardie (2011) represent each word as a matrix and use iterated matrix multiplication as phrase-level composition function. Socher et al. (2013b) introduce a family of recursive neural networks for sentence-level semantic composition. Recursive neural network is extended with global feedbackward (Paulus et al., 2014), feature weight tuning (Li, 2014), deep recursive layer (Irsoy and Cardie, 2014), adaptive composition functions (Dong et al., 2014), combined with Combinatory Categorial Grammar (Hermann and Blunsom, 2013), and used for opinion relation detection (Xu et al., 2014). Glorot et al. (2011) use sta"
D15-1167,D11-1015,0,0.00575452,"ument composition, while adding neural gates dramatically boosts the performance, (2) LSTM performs better than a multi-filtered CNN in modeling sentence representation. We briefly discuss some future plans. How to effectively compose sentence meanings to document meaning is a central problem in natural language processing. In this work, we develop neural models in a sequential way, and encode sentence semantics and their relations automatically without using external discourse analysis results. From one perspective, one could carefully define a set of sentiment-sensitive discourse relations (Zhou et al., 2011), such as “contrast”, “condition”, “cause”, etc. Afterwards, relation-specific gated RNN can be developed to explicitly model semantic composition rules for each relation (Socher et al., 2013a). However, defining such a relation scheme is linguistic driven and time consuming, which we leave as future work. From another perspective, one could compose document 1429 representation over discourse tree structures rather than in a sequential way. Accordingly, Recursive Neural Network (Socher et al., 2013b) and Structured LSTM (Tai et al., 2015; Zhu et al., 2015) can be used as composition algorithms"
D15-1167,D14-1181,0,\N,Missing
D15-1167,D14-1179,0,\N,Missing
D16-1021,D15-1263,0,0.00556126,"use matrix multiplication as compositional function to compute vectors for longer phrases. To compute sentence representation, researchers develop denoising autoencoder (Glorot et al., 2011), convolutional neural network (Kalchbrenner et al., 2014; Kim, 2014; Yin and Sch¨utze, 2015), sequence based recurrent neural models (Sutskever et al., 2014; Kiros et al., 2015; Li et al., 2015b) and tree-structured neural networks (Socher et al., 2013; Tai et al., 2015; Zhu et al., 2015). Several recent studies calculate continuous representation for documents with neural networks (Le and Mikolov, 2014; Bhatia et al., 2015; Li et al., 2015a; Tang et al., 2015b; Yang et al., 2016). 5.3 Attention and Memory Networks Recently, there is a resurgence in computational models with attention mechanism and explicit memory to learn representations of texts (Graves et al., 2014; Weston et al., 2014; Sukhbaatar et al., 2015; Bahdanau et al., 2015). In this line of research, memory is encoded as a continuous representation and operations on memory (e.g. reading and writing) are typically implemented with neural networks. Attention mechanism could be viewed as a compositional function, where lower level representations are r"
D16-1021,P10-2050,0,0.00966793,"n and Shirai, 2015; Tang et al., 2015a) are of growing attention for their capacity to learn powerful text representation from data. However, these neural models (e.g. LSTM) are computationally expensive, and could not explicitly reveal the importance of context evidences with regard to an aspect. Instead, we develop simple and fast approach that explicitly encodes the context importance towards a given aspect. It is worth noting that the task we focus on differs from finegrained opinion extraction, which assigns each word a tag (e.g. B,I,O) to indicate whether it is an aspect/sentiment word (Choi and Cardie, 2010; Irsoy and Cardie, 2014; Liu et al., 2015). The aspect word in this work is given as a part of the input. 5.2 Compositionality in Vector Space In NLP community, compositionality means that the meaning of a composed expression (e.g. a phrase/sentence/document) comes from the meanings of its constituents (Frege, 1892). Mitchell and Lapata (2010) exploits a variety of addition and multiplication functions to calculate phrase vector. Yessenalina and Cardie (2011) use matrix multiplication as compositional function to compute vectors for longer phrases. To compute sentence representation, research"
D16-1021,P14-2009,1,0.264452,"finegrained classification task in sentiment analysis, which aims at identifying the sentiment polarity of a sentence expressed towards an aspect (Pontiki et al., 2014). Most existing works use machine learning algorithms, and build sentiment classifier from sentences with manually annotated polarity labels. One of the most successful approaches in liter221 ature is feature based SVM. Experts could design effective feature templates and make use of external resources like parser and sentiment lexicons (Kiritchenko et al., 2014; Wagner et al., 2014). In recent years, neural network approaches (Dong et al., 2014; Lakkaraju et al., 2014; Nguyen and Shirai, 2015; Tang et al., 2015a) are of growing attention for their capacity to learn powerful text representation from data. However, these neural models (e.g. LSTM) are computationally expensive, and could not explicitly reveal the importance of context evidences with regard to an aspect. Instead, we develop simple and fast approach that explicitly encodes the context importance towards a given aspect. It is worth noting that the task we focus on differs from finegrained opinion extraction, which assigns each word a tag (e.g. B,I,O) to indicate whether i"
D16-1021,D14-1080,0,0.0185064,"g et al., 2015a) are of growing attention for their capacity to learn powerful text representation from data. However, these neural models (e.g. LSTM) are computationally expensive, and could not explicitly reveal the importance of context evidences with regard to an aspect. Instead, we develop simple and fast approach that explicitly encodes the context importance towards a given aspect. It is worth noting that the task we focus on differs from finegrained opinion extraction, which assigns each word a tag (e.g. B,I,O) to indicate whether it is an aspect/sentiment word (Choi and Cardie, 2010; Irsoy and Cardie, 2014; Liu et al., 2015). The aspect word in this work is given as a part of the input. 5.2 Compositionality in Vector Space In NLP community, compositionality means that the meaning of a composed expression (e.g. a phrase/sentence/document) comes from the meanings of its constituents (Frege, 1892). Mitchell and Lapata (2010) exploits a variety of addition and multiplication functions to calculate phrase vector. Yessenalina and Cardie (2011) use matrix multiplication as compositional function to compute vectors for longer phrases. To compute sentence representation, researchers develop denoising au"
D16-1021,P14-1062,0,0.00487534,"part of the input. 5.2 Compositionality in Vector Space In NLP community, compositionality means that the meaning of a composed expression (e.g. a phrase/sentence/document) comes from the meanings of its constituents (Frege, 1892). Mitchell and Lapata (2010) exploits a variety of addition and multiplication functions to calculate phrase vector. Yessenalina and Cardie (2011) use matrix multiplication as compositional function to compute vectors for longer phrases. To compute sentence representation, researchers develop denoising autoencoder (Glorot et al., 2011), convolutional neural network (Kalchbrenner et al., 2014; Kim, 2014; Yin and Sch¨utze, 2015), sequence based recurrent neural models (Sutskever et al., 2014; Kiros et al., 2015; Li et al., 2015b) and tree-structured neural networks (Socher et al., 2013; Tai et al., 2015; Zhu et al., 2015). Several recent studies calculate continuous representation for documents with neural networks (Le and Mikolov, 2014; Bhatia et al., 2015; Li et al., 2015a; Tang et al., 2015b; Yang et al., 2016). 5.3 Attention and Memory Networks Recently, there is a resurgence in computational models with attention mechanism and explicit memory to learn representations of texts"
D16-1021,D14-1181,0,0.00616185,"positionality in Vector Space In NLP community, compositionality means that the meaning of a composed expression (e.g. a phrase/sentence/document) comes from the meanings of its constituents (Frege, 1892). Mitchell and Lapata (2010) exploits a variety of addition and multiplication functions to calculate phrase vector. Yessenalina and Cardie (2011) use matrix multiplication as compositional function to compute vectors for longer phrases. To compute sentence representation, researchers develop denoising autoencoder (Glorot et al., 2011), convolutional neural network (Kalchbrenner et al., 2014; Kim, 2014; Yin and Sch¨utze, 2015), sequence based recurrent neural models (Sutskever et al., 2014; Kiros et al., 2015; Li et al., 2015b) and tree-structured neural networks (Socher et al., 2013; Tai et al., 2015; Zhu et al., 2015). Several recent studies calculate continuous representation for documents with neural networks (Le and Mikolov, 2014; Bhatia et al., 2015; Li et al., 2015a; Tang et al., 2015b; Yang et al., 2016). 5.3 Attention and Memory Networks Recently, there is a resurgence in computational models with attention mechanism and explicit memory to learn representations of texts (Graves et"
D16-1021,S14-2076,0,0.468058,"ontext word and then utilizes this information to calculate continuous text representation. The text representation in the last layer is regarded as the feature for sentiment classification. As every component is differentiable, the entire model could be efficiently trained end-toend with gradient descent, where the loss function is the cross-entropy error of sentiment classification. We apply the proposed approach to laptop and restaurant datasets from SemEval 2014 (Pontiki et al., 2014). Experimental results show that our approach performs comparable to a top system using feature-based SVM (Kiritchenko et al., 2014). On both datasets, our approach outperforms both LSTM and attention-based LSTM models (Tang et al., 2015a) in terms of classification accuracy and running speed. Lastly, we show that using multiple computational layers over external memory could achieve improved performance. put representation given a new input and the current memory state, R outputs a response based on the output representation. Let us take question answering as an example to explain the work flow of memory network. Given a list of sentences and a question, the task aims to find evidences from these sentences and generate an"
D16-1021,P15-1107,0,0.0116738,"phrase/sentence/document) comes from the meanings of its constituents (Frege, 1892). Mitchell and Lapata (2010) exploits a variety of addition and multiplication functions to calculate phrase vector. Yessenalina and Cardie (2011) use matrix multiplication as compositional function to compute vectors for longer phrases. To compute sentence representation, researchers develop denoising autoencoder (Glorot et al., 2011), convolutional neural network (Kalchbrenner et al., 2014; Kim, 2014; Yin and Sch¨utze, 2015), sequence based recurrent neural models (Sutskever et al., 2014; Kiros et al., 2015; Li et al., 2015b) and tree-structured neural networks (Socher et al., 2013; Tai et al., 2015; Zhu et al., 2015). Several recent studies calculate continuous representation for documents with neural networks (Le and Mikolov, 2014; Bhatia et al., 2015; Li et al., 2015a; Tang et al., 2015b; Yang et al., 2016). 5.3 Attention and Memory Networks Recently, there is a resurgence in computational models with attention mechanism and explicit memory to learn representations of texts (Graves et al., 2014; Weston et al., 2014; Sukhbaatar et al., 2015; Bahdanau et al., 2015). In this line of research, memory is encoded a"
D16-1021,D15-1278,0,0.007366,"phrase/sentence/document) comes from the meanings of its constituents (Frege, 1892). Mitchell and Lapata (2010) exploits a variety of addition and multiplication functions to calculate phrase vector. Yessenalina and Cardie (2011) use matrix multiplication as compositional function to compute vectors for longer phrases. To compute sentence representation, researchers develop denoising autoencoder (Glorot et al., 2011), convolutional neural network (Kalchbrenner et al., 2014; Kim, 2014; Yin and Sch¨utze, 2015), sequence based recurrent neural models (Sutskever et al., 2014; Kiros et al., 2015; Li et al., 2015b) and tree-structured neural networks (Socher et al., 2013; Tai et al., 2015; Zhu et al., 2015). Several recent studies calculate continuous representation for documents with neural networks (Le and Mikolov, 2014; Bhatia et al., 2015; Li et al., 2015a; Tang et al., 2015b; Yang et al., 2016). 5.3 Attention and Memory Networks Recently, there is a resurgence in computational models with attention mechanism and explicit memory to learn representations of texts (Graves et al., 2014; Weston et al., 2014; Sukhbaatar et al., 2015; Bahdanau et al., 2015). In this line of research, memory is encoded a"
D16-1021,D15-1168,0,0.0594919,"growing attention for their capacity to learn powerful text representation from data. However, these neural models (e.g. LSTM) are computationally expensive, and could not explicitly reveal the importance of context evidences with regard to an aspect. Instead, we develop simple and fast approach that explicitly encodes the context importance towards a given aspect. It is worth noting that the task we focus on differs from finegrained opinion extraction, which assigns each word a tag (e.g. B,I,O) to indicate whether it is an aspect/sentiment word (Choi and Cardie, 2010; Irsoy and Cardie, 2014; Liu et al., 2015). The aspect word in this work is given as a part of the input. 5.2 Compositionality in Vector Space In NLP community, compositionality means that the meaning of a composed expression (e.g. a phrase/sentence/document) comes from the meanings of its constituents (Frege, 1892). Mitchell and Lapata (2010) exploits a variety of addition and multiplication functions to calculate phrase vector. Yessenalina and Cardie (2011) use matrix multiplication as compositional function to compute vectors for longer phrases. To compute sentence representation, researchers develop denoising autoencoder (Glorot e"
D16-1021,D15-1166,0,0.0247409,"014; Sukhbaatar et al., 2015; Bahdanau et al., 2015). In this line of research, memory is encoded as a continuous representation and operations on memory (e.g. reading and writing) are typically implemented with neural networks. Attention mechanism could be viewed as a compositional function, where lower level representations are regarded as the memory, and the function is to choose “where to look” by assigning a weight/importance to each lower position when computing an upper level representation. Such attention based approaches have achieved promising performances on a variety of NLP tasks (Luong et al., 2015; Kumar et al., 2015; Rush et al., 2015). 6 Conclusion We develop deep memory networks that capture importances of context words for aspect level sentiment classification. Compared with recurrent neural models like LSTM, this approach is simpler and faster. Empirical results on two datasets verify that the proposed approach performs comparable to state-of-the-art feature based SVM system, and substantively better than LSTM architectures. We implement different attention strategies and show that leveraging both content and location information could learn better context weight and text represen"
D16-1021,D15-1298,0,0.234316,"t analysis, which aims at identifying the sentiment polarity of a sentence expressed towards an aspect (Pontiki et al., 2014). Most existing works use machine learning algorithms, and build sentiment classifier from sentences with manually annotated polarity labels. One of the most successful approaches in liter221 ature is feature based SVM. Experts could design effective feature templates and make use of external resources like parser and sentiment lexicons (Kiritchenko et al., 2014; Wagner et al., 2014). In recent years, neural network approaches (Dong et al., 2014; Lakkaraju et al., 2014; Nguyen and Shirai, 2015; Tang et al., 2015a) are of growing attention for their capacity to learn powerful text representation from data. However, these neural models (e.g. LSTM) are computationally expensive, and could not explicitly reveal the importance of context evidences with regard to an aspect. Instead, we develop simple and fast approach that explicitly encodes the context importance towards a given aspect. It is worth noting that the task we focus on differs from finegrained opinion extraction, which assigns each word a tag (e.g. B,I,O) to indicate whether it is an aspect/sentiment word (Choi and Cardie, 2"
D16-1021,D14-1162,0,0.11812,"classification aims at determining the sentiment polarity of 1 In practice, an aspect might be a multi word expression such as “battery life”. For simplicity we still consider aspect as a single word in this definition. sentence s towards the aspect wi . For example, the sentiment polarity of sentence “great food but the service was dreadful!” towards aspect “food” is positive, while the polarity towards aspect “service” is negative. When dealing with a text corpus, we map each word into a low dimensional, continuous and real-valued vector, also known as word embedding (Mikolov et al., 2013; Pennington et al., 2014). All the word vectors are stacked in a word embedding matrix L ∈ Rd×|V |, where d is the dimension of word vector and |V |is vocabulary size. The word embedding of wi is notated as ei ∈ Rd×1 , which is a column in the embedding matrix L. 3.2 An Overview of the Approach softmax hop 3 Attention ∑ Linear word embedding hop 2 Attention ∑ Linear Linear hop 1 context words context words Attention ∑ Tanh Linear Linear sentence: ?1 , ?2 … ??−1 , ?? , ??+1 … ??−1 , ?? aspect word ??1 wi Figure 1: An illustration of our deep memory network with We present an overview of the deep memory network for aspe"
D16-1021,S14-2004,0,0.648481,"er an external memory. Experiments on laptop and restaurant datasets demonstrate that our approach performs comparable to state-of-art feature based SVM system, and substantially better than LSTM and attention-based LSTM architectures. On both datasets we show that multiple computational layers could improve the performance. Moreover, our approach is also fast. The deep memory network with 9 layers is 15 times faster than LSTM with a CPU implementation. 1 Introduction Aspect level sentiment classification is a fundamental task in the field of sentiment analysis (Pang and Lee, 2008; Liu, 2012; Pontiki et al., 2014). Given a sentence and an aspect occurring in the sentence, this task aims at inferring the sentiment polarity (e.g. positive, negative, neutral) of the aspect. For example, in sentence “great food but the service was dreadful!”, the sentiment polarity of aspect “food” is positive while the polarity of aspect “service” is ∗ Corresponding author. Despite these advantages, conventional neural models like long short-term memory (LSTM) (Tang et al., 2015a) capture context information in an implicit way, and are incapable of explicitly exhibiting important context clues of an aspect. We believe tha"
D16-1021,D15-1044,0,0.037867,"t al., 2015). In this line of research, memory is encoded as a continuous representation and operations on memory (e.g. reading and writing) are typically implemented with neural networks. Attention mechanism could be viewed as a compositional function, where lower level representations are regarded as the memory, and the function is to choose “where to look” by assigning a weight/importance to each lower position when computing an upper level representation. Such attention based approaches have achieved promising performances on a variety of NLP tasks (Luong et al., 2015; Kumar et al., 2015; Rush et al., 2015). 6 Conclusion We develop deep memory networks that capture importances of context words for aspect level sentiment classification. Compared with recurrent neural models like LSTM, this approach is simpler and faster. Empirical results on two datasets verify that the proposed approach performs comparable to state-of-the-art feature based SVM system, and substantively better than LSTM architectures. We implement different attention strategies and show that leveraging both content and location information could learn better context weight and text representation. We also demonstrate that using m"
D16-1021,D13-1170,0,0.00729358,"ts constituents (Frege, 1892). Mitchell and Lapata (2010) exploits a variety of addition and multiplication functions to calculate phrase vector. Yessenalina and Cardie (2011) use matrix multiplication as compositional function to compute vectors for longer phrases. To compute sentence representation, researchers develop denoising autoencoder (Glorot et al., 2011), convolutional neural network (Kalchbrenner et al., 2014; Kim, 2014; Yin and Sch¨utze, 2015), sequence based recurrent neural models (Sutskever et al., 2014; Kiros et al., 2015; Li et al., 2015b) and tree-structured neural networks (Socher et al., 2013; Tai et al., 2015; Zhu et al., 2015). Several recent studies calculate continuous representation for documents with neural networks (Le and Mikolov, 2014; Bhatia et al., 2015; Li et al., 2015a; Tang et al., 2015b; Yang et al., 2016). 5.3 Attention and Memory Networks Recently, there is a resurgence in computational models with attention mechanism and explicit memory to learn representations of texts (Graves et al., 2014; Weston et al., 2014; Sukhbaatar et al., 2015; Bahdanau et al., 2015). In this line of research, memory is encoded as a continuous representation and operations on memory (e.g"
D16-1021,P15-1150,0,0.0288958,"e, 1892). Mitchell and Lapata (2010) exploits a variety of addition and multiplication functions to calculate phrase vector. Yessenalina and Cardie (2011) use matrix multiplication as compositional function to compute vectors for longer phrases. To compute sentence representation, researchers develop denoising autoencoder (Glorot et al., 2011), convolutional neural network (Kalchbrenner et al., 2014; Kim, 2014; Yin and Sch¨utze, 2015), sequence based recurrent neural models (Sutskever et al., 2014; Kiros et al., 2015; Li et al., 2015b) and tree-structured neural networks (Socher et al., 2013; Tai et al., 2015; Zhu et al., 2015). Several recent studies calculate continuous representation for documents with neural networks (Le and Mikolov, 2014; Bhatia et al., 2015; Li et al., 2015a; Tang et al., 2015b; Yang et al., 2016). 5.3 Attention and Memory Networks Recently, there is a resurgence in computational models with attention mechanism and explicit memory to learn representations of texts (Graves et al., 2014; Weston et al., 2014; Sukhbaatar et al., 2015; Bahdanau et al., 2015). In this line of research, memory is encoded as a continuous representation and operations on memory (e.g. reading and writ"
D16-1021,D15-1167,1,0.358709,"1 Introduction Aspect level sentiment classification is a fundamental task in the field of sentiment analysis (Pang and Lee, 2008; Liu, 2012; Pontiki et al., 2014). Given a sentence and an aspect occurring in the sentence, this task aims at inferring the sentiment polarity (e.g. positive, negative, neutral) of the aspect. For example, in sentence “great food but the service was dreadful!”, the sentiment polarity of aspect “food” is positive while the polarity of aspect “service” is ∗ Corresponding author. Despite these advantages, conventional neural models like long short-term memory (LSTM) (Tang et al., 2015a) capture context information in an implicit way, and are incapable of explicitly exhibiting important context clues of an aspect. We believe that only some subset of context words are needed to infer the sentiment towards an aspect. For example, in sentence “great food but the service was dreadful!”, “dreadful” is an important clue for the aspect “service” but “great” is not needed. Standard LSTM works in a sequential way and manipulates each context word with the same operation, so that it cannot explicitly reveal the importance of each context word. A desirable solution should be capable o"
D16-1021,S14-2036,0,0.0874602,"ntiment Classification Aspect level sentiment classification is a finegrained classification task in sentiment analysis, which aims at identifying the sentiment polarity of a sentence expressed towards an aspect (Pontiki et al., 2014). Most existing works use machine learning algorithms, and build sentiment classifier from sentences with manually annotated polarity labels. One of the most successful approaches in liter221 ature is feature based SVM. Experts could design effective feature templates and make use of external resources like parser and sentiment lexicons (Kiritchenko et al., 2014; Wagner et al., 2014). In recent years, neural network approaches (Dong et al., 2014; Lakkaraju et al., 2014; Nguyen and Shirai, 2015; Tang et al., 2015a) are of growing attention for their capacity to learn powerful text representation from data. However, these neural models (e.g. LSTM) are computationally expensive, and could not explicitly reveal the importance of context evidences with regard to an aspect. Instead, we develop simple and fast approach that explicitly encodes the context importance towards a given aspect. It is worth noting that the task we focus on differs from finegrained opinion extraction, w"
D16-1021,N16-1174,0,0.012777,"pute vectors for longer phrases. To compute sentence representation, researchers develop denoising autoencoder (Glorot et al., 2011), convolutional neural network (Kalchbrenner et al., 2014; Kim, 2014; Yin and Sch¨utze, 2015), sequence based recurrent neural models (Sutskever et al., 2014; Kiros et al., 2015; Li et al., 2015b) and tree-structured neural networks (Socher et al., 2013; Tai et al., 2015; Zhu et al., 2015). Several recent studies calculate continuous representation for documents with neural networks (Le and Mikolov, 2014; Bhatia et al., 2015; Li et al., 2015a; Tang et al., 2015b; Yang et al., 2016). 5.3 Attention and Memory Networks Recently, there is a resurgence in computational models with attention mechanism and explicit memory to learn representations of texts (Graves et al., 2014; Weston et al., 2014; Sukhbaatar et al., 2015; Bahdanau et al., 2015). In this line of research, memory is encoded as a continuous representation and operations on memory (e.g. reading and writing) are typically implemented with neural networks. Attention mechanism could be viewed as a compositional function, where lower level representations are regarded as the memory, and the function is to choose “wher"
D16-1021,D11-1016,0,0.0129842,"on differs from finegrained opinion extraction, which assigns each word a tag (e.g. B,I,O) to indicate whether it is an aspect/sentiment word (Choi and Cardie, 2010; Irsoy and Cardie, 2014; Liu et al., 2015). The aspect word in this work is given as a part of the input. 5.2 Compositionality in Vector Space In NLP community, compositionality means that the meaning of a composed expression (e.g. a phrase/sentence/document) comes from the meanings of its constituents (Frege, 1892). Mitchell and Lapata (2010) exploits a variety of addition and multiplication functions to calculate phrase vector. Yessenalina and Cardie (2011) use matrix multiplication as compositional function to compute vectors for longer phrases. To compute sentence representation, researchers develop denoising autoencoder (Glorot et al., 2011), convolutional neural network (Kalchbrenner et al., 2014; Kim, 2014; Yin and Sch¨utze, 2015), sequence based recurrent neural models (Sutskever et al., 2014; Kiros et al., 2015; Li et al., 2015b) and tree-structured neural networks (Socher et al., 2013; Tai et al., 2015; Zhu et al., 2015). Several recent studies calculate continuous representation for documents with neural networks (Le and Mikolov, 2014;"
D16-1021,K15-1021,0,0.00778988,"Missing"
D18-1048,D14-1179,0,0.0559999,"Missing"
D18-1048,I17-1013,0,0.146431,"cording to the complexity of source sentences and the quality of the generated translations. Extensive experiments on ChineseEnglish translation demonstrate the effectiveness of our proposed adaptive multi-pass decoder upon the conventional NMT with a significant improvement about 1.55 BLEU. 1 Reference 1st-pass 2nd-pass 3rd-pass 4th-pass Table 1: Translation examples of more decoding passes with the proposed multi-pass decoder. draft and then polish it based on global understanding of the whole draft (Niehues et al., 2016; Chatterjee et al., 2016; Zhou et al., 2017; Xia et al., 2017; Junczys Dowmunt and Grundkiewicz, 2017) . Moreover, Zhang et al. (2018) introduces a backward decoder to better exploit the right-toleft target-side contexts. Generally these methods employ two separate decoders to accomplish the polishing task. Introduction In the past several years, end-to-end neural machine translation (NMT) (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2014) has attracted increasing attention from both academic and industry communities. Compared with conventional statistical machine translation (SMT) (Brown et al., 1993; Koehn et al., 2003), which needs to explicitl"
D18-1048,D13-1176,0,0.0724593,"3rd-pass 4th-pass Table 1: Translation examples of more decoding passes with the proposed multi-pass decoder. draft and then polish it based on global understanding of the whole draft (Niehues et al., 2016; Chatterjee et al., 2016; Zhou et al., 2017; Xia et al., 2017; Junczys Dowmunt and Grundkiewicz, 2017) . Moreover, Zhang et al. (2018) introduces a backward decoder to better exploit the right-toleft target-side contexts. Generally these methods employ two separate decoders to accomplish the polishing task. Introduction In the past several years, end-to-end neural machine translation (NMT) (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2014) has attracted increasing attention from both academic and industry communities. Compared with conventional statistical machine translation (SMT) (Brown et al., 1993; Koehn et al., 2003), which needs to explicitly model latent structures, NMT adopts a unified encoder-decoder framework to directly transform a source sentence into a target sentence. Furthermore, the introduction of attention mechanism (Bahdanau et al., 2014) enhances the capability of NMT in capturing long-distance dependencies. Recently, a number of authors have"
D18-1048,P07-2045,0,0.0146892,"er or not the source sentence is easy to π(al |spolicy ; θp ) = sof tmax(Wp spolicy + bp ) l l (12) where Wp and bp are the parameters of the policy network. In this work we use REINFORCE algorithm (Williams, 1992), which is an instance of a broader class of algorithms called policy gradient methods (Sutton and Barto, 1998), to learn the parameter set θp such that the sequence of actions 526 1. Moses3 : an open source phrase-based translation system with default configuration and a 4-gram language model trained on the target portion of training data. Note that we used all data to train MOSES (Koehn et al., 2007). a = {a1 , . . . , al , . . . , aL(x,y) } maximizes the total expected reward. The expected reward for an instance is defined as: J policy (θp ) = Eπ(a|spolicy ;θp ) r(ˆ yL(x,y) ) (13) where r(ˆ yL(x,y) ) is the reward at the L(x,y) -th decoding pass. In this work, we use BLEU (Papineni ˆ L(x,y) generet al., 2002) of the final translation y ated by greedy search as input to compute our reward as follows: r(ˆ y 4 L(x,y) ) = BLEU(ˆ y L(x,y) , y) 2. RNNSearch: a variant of the attention-based NMT system (Bahdanau et al., 2014) with slight changes from dl4mt tutorial4 . 3. Deliberation Network5 :"
D18-1048,J93-2003,0,0.0704305,", 2017; Xia et al., 2017; Junczys Dowmunt and Grundkiewicz, 2017) . Moreover, Zhang et al. (2018) introduces a backward decoder to better exploit the right-toleft target-side contexts. Generally these methods employ two separate decoders to accomplish the polishing task. Introduction In the past several years, end-to-end neural machine translation (NMT) (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2014) has attracted increasing attention from both academic and industry communities. Compared with conventional statistical machine translation (SMT) (Brown et al., 1993; Koehn et al., 2003), which needs to explicitly model latent structures, NMT adopts a unified encoder-decoder framework to directly transform a source sentence into a target sentence. Furthermore, the introduction of attention mechanism (Bahdanau et al., 2014) enhances the capability of NMT in capturing long-distance dependencies. Recently, a number of authors have endeavored to adopt the polishing mechanism into NMT. Similar to human cognitive process for writing a good paper, their models first create a complete ∗ h´esh`ızh`u xi¯ansh¯eng d¯e wˇeir`enq¯ı w´ei y¯ı ni´an , yˇı p`eih´e q´ı w´ei"
D18-1048,W16-2377,0,0.0446959,"Missing"
D18-1048,N03-1017,0,0.0779442,"Missing"
D18-1048,D15-1166,0,0.328147,"odel. 2 cn = P (yn |x, y&lt;n ; θ) (1) n=1 where θ is a set of model parameters and y&lt;n denotes a partial translation. Prediction of n-th word is generally made in an encoder-decoder framework: P (yn |x, y&lt;n ; θ) = g(yn−1 , sn , cn ) (2) where g(·) is a non-linear function, yn−1 denotes the previously generated word, sn is n-th decoding hidden state, and cn is a context vector for generating n-th target word. The decoder state sn is computed by RNNs as follows: sn = f (sn−1 , yn−1 , cn ) (4) where αm,n measures how well xm and yn are aligned, calculated by attention model (Bahdanau et al., 2014; Luong et al., 2015), and hm is the encoder hidden state of the m-th source word. For the purpose of capturing both forward and backward contexts, bidirectional RNN (Schuster and Paliwal, 1997) is often employed as the encoder which converts the source sentence into an annotation sequence h = {h1 , . . . , hm , . . . , hM }, where → − ← − hm = [ h m , h m ] captures information about mth word with respect to the preceding and following words in the source sentence respectively. Although the introduction of RNNs as a decoder has resulted in substantial improvements in terms of translation quality, simultaneously i"
D18-1048,C16-1172,0,0.15093,"Missing"
D18-1048,P02-1040,0,0.105229,"NMT system with two independent left-to-right decoders (Xia et al., 2017). The first-pass decoder is identical to one of RNNSearch to generate a draft translation, while the second-pass decoder polishes it with an extra attention over the first pass decoder. The second-pass decoder is integrated with the first-pass decoder via reinforcement learning. (14) Experiments In this section, we describe experimental settings and report empirical results. 4.1 Setup We evaluated the proposed adaptive multipass decoder on Chinese-English translation task. The evaluation metric was case-insensitive BLEU (Papineni et al., 2002) calculated by the multi-bleu.perl1 script. The training corpus2 consisted of 1.25M bilingual sentences with 27.9M Chinese words and 34.5M English words. We used the NIST 2002 (MT02) as the validation set for hyper-parameter optimization and model selection, and NIST 2003 (MT03), 2004 (MT04), 2005 (MT05) and 2006 (MT06) as test sets. To effectively train the NMT model, we trained each model with sentences of length up to 50 words. Besides, we limited vocabulary size to 30K for both languages and map all the out-ofvocabulary words in the Chinese-English corpus to a special token UNK. We applied"
D18-1048,P17-2060,0,0.0744219,"e and effective number of decoding passes, according to the complexity of source sentences and the quality of the generated translations. Extensive experiments on ChineseEnglish translation demonstrate the effectiveness of our proposed adaptive multi-pass decoder upon the conventional NMT with a significant improvement about 1.55 BLEU. 1 Reference 1st-pass 2nd-pass 3rd-pass 4th-pass Table 1: Translation examples of more decoding passes with the proposed multi-pass decoder. draft and then polish it based on global understanding of the whole draft (Niehues et al., 2016; Chatterjee et al., 2016; Zhou et al., 2017; Xia et al., 2017; Junczys Dowmunt and Grundkiewicz, 2017) . Moreover, Zhang et al. (2018) introduces a backward decoder to better exploit the right-toleft target-side contexts. Generally these methods employ two separate decoders to accomplish the polishing task. Introduction In the past several years, end-to-end neural machine translation (NMT) (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2014) has attracted increasing attention from both academic and industry communities. Compared with conventional statistical machine translation (SMT) (Brown"
D18-1264,P17-1044,0,0.0254208,"d in our intrinsic evaluation. 8 e.g., “North Koreans” cannot be parsed into (name :op1 &quot;North&quot; :op2 &quot;Korea&quot;) 9 e.g., “Wi Sung - lac” cannot be parsed into (name :op1 &quot;Wi&quot; :op2 &quot;Sung-lac&quot;) 10 e.g. the first “nuclear” aligned to nucleus˜1 in Fig. 1 exp{ga · S TACK LSTM(s) + ba } , a0 exp{ga0 · S TACK LSTM(s) + ba0 } where S TACK LSTM(s) encodes the state s into a vector and ga is the embedding vector of action a. We encourage the reader to refer Ballesteros and Al-Onaizan (2017) for more details. Ensemble. Ensemble has been shown as an effective way of improving the neural model’s performance (He et al., 2017). Since the transitionbased parser directly parse a sentence into its AMR graph, ensemble of several parsers is easier compared to the two-staged AMR parsers. In this paper, we ensemble the parsers trained with different initialization by averaging their probability distribution over the actions. 5 5.1 Alignment Experiments Settings We evaluate our aligner on the LDC2014T12 dataset. Two kinds of evaluations are carried out including the intrinsic and extrinsic evaluations. For the intrinsic evaluation, we follow Flanigan et al. (2014) and evaluate the F1 score of the alignments produced by our"
D18-1264,P17-1014,0,0.380708,"scu et al., 2013) is a semantic representation which encodes the meaning of a sentence in a rooted and directed graph, whose nodes are abstract semantic concepts and edges are semantic relations between concepts (see Figure 1 for an example). Parsing a sentence into its AMR graph has drawn a lot of research attention in recent years with a number of parsers being developed (Flanigan et al., 2014; Wang et al., 2015b; Pust et al., 2015; Artzi et al., 2015; Peng et al., 2015; Zhou et al., 2016; Goodman et al., 2016; Damonte et al., 2017; Ballesteros and Al-Onaizan, 2017; Foland and Martin, 2017; Konstas et al., 2017). The nature of abstracting away the association between a concept and a span of words complicates the training of the AMR parser. A wordconcept aligner is required to derive such association from the sentence-AMR-graph pair and the * Email corresponding. recieve-01 ARG0 ARG1 Introduction ∗ freeze-01 ARG3 alignment output is then used as reference to train the AMR parser. In previous works, such alignment is extracted by either greedily applying a set of heuristic rules (Flanigan et al., 2014) or adopting the unsupervised word alignment technique from machine translation (Pourdamghani et al.,"
D18-1264,D15-1198,0,0.220272,"Figure 1: AMR graph for the sentence “North Korea froze its nuclear actions in exchange for two nuclear reactors.” Abstract Meaning Representation (AMR) (Banarescu et al., 2013) is a semantic representation which encodes the meaning of a sentence in a rooted and directed graph, whose nodes are abstract semantic concepts and edges are semantic relations between concepts (see Figure 1 for an example). Parsing a sentence into its AMR graph has drawn a lot of research attention in recent years with a number of parsers being developed (Flanigan et al., 2014; Wang et al., 2015b; Pust et al., 2015; Artzi et al., 2015; Peng et al., 2015; Zhou et al., 2016; Goodman et al., 2016; Damonte et al., 2017; Ballesteros and Al-Onaizan, 2017; Foland and Martin, 2017; Konstas et al., 2017). The nature of abstracting away the association between a concept and a span of words complicates the training of the AMR parser. A wordconcept aligner is required to derive such association from the sentence-AMR-graph pair and the * Email corresponding. recieve-01 ARG0 ARG1 Introduction ∗ freeze-01 ARG3 alignment output is then used as reference to train the AMR parser. In previous works, such alignment is extracted by either gree"
D18-1264,N15-1142,0,0.0374791,"uced by the semantic matching especially by the word embedding similarity component. Without filtering the noise by our oracle parser, just introducing more matching rules will harm the performance. 6 6.1 newswire 73.1 72.7 67.6 71.3 Parsing Experiments Settings We use the same settings in our aligner extrinsic evaluation for the experiments on our transitionbased parser. For the input to the parser, we tried two settings: 1) using only words as input, and 2) using words and POS tags as input. Automatic POS tags are assigned with Stanford POS tagger (Manning et al., 2014). Word embedding from Ling et al. (2015) is used in the same way with Ballesteros and Al-Onaizan (2017). To opt Results Table 6 shows the performance of our transitionbased parser along with comparison to the parsers in the previous works. When compared with our transition-based counterpart (Ballesteros and AlOnaizan, 2017), our word-only model outperforms theirs using the same JAMR alignment. The same trend is witnessed using words and POS tags as input. When replacing the JAMR alignments with ours, the parsing performances are improved in the same way as in Table 4, which further confirms the effectiveness of our aligner. The seco"
D18-1264,D17-1130,0,0.0550349,"ar reactors.” Abstract Meaning Representation (AMR) (Banarescu et al., 2013) is a semantic representation which encodes the meaning of a sentence in a rooted and directed graph, whose nodes are abstract semantic concepts and edges are semantic relations between concepts (see Figure 1 for an example). Parsing a sentence into its AMR graph has drawn a lot of research attention in recent years with a number of parsers being developed (Flanigan et al., 2014; Wang et al., 2015b; Pust et al., 2015; Artzi et al., 2015; Peng et al., 2015; Zhou et al., 2016; Goodman et al., 2016; Damonte et al., 2017; Ballesteros and Al-Onaizan, 2017; Foland and Martin, 2017; Konstas et al., 2017). The nature of abstracting away the association between a concept and a span of words complicates the training of the AMR parser. A wordconcept aligner is required to derive such association from the sentence-AMR-graph pair and the * Email corresponding. recieve-01 ARG0 ARG1 Introduction ∗ freeze-01 ARG3 alignment output is then used as reference to train the AMR parser. In previous works, such alignment is extracted by either greedily applying a set of heuristic rules (Flanigan et al., 2014) or adopting the unsupervised word alignment technique"
D18-1264,W13-2322,0,0.262658,"sistently improving two open-sourced AMR parsers. Based on our aligner and transition system, we develop a transition-based AMR parser that parses a sentence into its AMR graph directly. An ensemble of our parsers with only words and POS tags as input leads to 68.4 Smatch F1 score, which outperforms the parser of Wang and Xue (2017). 1 ARG1 ARG0 country act-02 name name op1 &quot;North&quot; ARG1 reactor mod nucleus~1 quant 2 mod nucleus~2 op2 &quot;Korea&quot; Figure 1: AMR graph for the sentence “North Korea froze its nuclear actions in exchange for two nuclear reactors.” Abstract Meaning Representation (AMR) (Banarescu et al., 2013) is a semantic representation which encodes the meaning of a sentence in a rooted and directed graph, whose nodes are abstract semantic concepts and edges are semantic relations between concepts (see Figure 1 for an example). Parsing a sentence into its AMR graph has drawn a lot of research attention in recent years with a number of parsers being developed (Flanigan et al., 2014; Wang et al., 2015b; Pust et al., 2015; Artzi et al., 2015; Peng et al., 2015; Zhou et al., 2016; Goodman et al., 2016; Damonte et al., 2017; Ballesteros and Al-Onaizan, 2017; Foland and Martin, 2017; Konstas et al., 2"
D18-1264,P13-2131,0,0.475771,"Missing"
D18-1264,P13-1104,0,0.266774,"rst element of the buffer (a word or span) into a concept c. a special form of C ONFIRM that derives the first element into an entity and builds the internal entity AMR fragment. generates a new concept c and pushes it to the front of the buffer. Table 2: The transition system. The letters in monospace font represent the concepts, the italic letters represent the word, and the letters in normal font are either concepts or words. Their work presented the possibility for the oracle parser, but their oracle parser was not touched explicitly. What’s more, in the non-projective dependency parsing, Choi and McCallum (2013)’s extension to the list-based system (Nivre, 2008) with caching mechanism achieves expected linear time complexity and requires fewer actions to parse a non-projective tree than the swap-based system. Their extension to transition-based AMR parsing is worth studying. In this paper, we propose to extend Choi and McCallum (2013)’s transition system to AMR parsing and present the corresponding oracle parser. The oracle parser is used for tuning our aligner and training our parser. We also present a comprehensive comparison of our system with that of Ballesteros and Al-Onaizan (2017) in Section 6"
D18-1264,E17-1051,0,0.619876,"exchange for two nuclear reactors.” Abstract Meaning Representation (AMR) (Banarescu et al., 2013) is a semantic representation which encodes the meaning of a sentence in a rooted and directed graph, whose nodes are abstract semantic concepts and edges are semantic relations between concepts (see Figure 1 for an example). Parsing a sentence into its AMR graph has drawn a lot of research attention in recent years with a number of parsers being developed (Flanigan et al., 2014; Wang et al., 2015b; Pust et al., 2015; Artzi et al., 2015; Peng et al., 2015; Zhou et al., 2016; Goodman et al., 2016; Damonte et al., 2017; Ballesteros and Al-Onaizan, 2017; Foland and Martin, 2017; Konstas et al., 2017). The nature of abstracting away the association between a concept and a span of words complicates the training of the AMR parser. A wordconcept aligner is required to derive such association from the sentence-AMR-graph pair and the * Email corresponding. recieve-01 ARG0 ARG1 Introduction ∗ freeze-01 ARG3 alignment output is then used as reference to train the AMR parser. In previous works, such alignment is extracted by either greedily applying a set of heuristic rules (Flanigan et al., 2014) or adopting the uns"
D18-1264,P15-1033,0,0.0258826,"ctions in Table 2 are our ex2426 4. If b0 is a word or span and it aligns to one or more concepts, perform C ONFIRM (c) where c is the concept b0 aligns and has the longest graph distance to the root. 5. If b0 is a concept and its head concept c has the same alignment as b0 , perform N EW (c). 6. If b0 is a concept and there is an unprocessed edge r between s0 and t0 , perform L EFT (r) or R IGHT (r) according to r’s direction. g1 a1 Training Data Aligner .. . an Oracle .. . raw sentence directly into its AMR graph. In this paper, we follow Ballesteros and Al-Onaizan (2017) and use StackLSTM (Dyer et al., 2015) to model the states. The score of a transition action a on state s is calculated as s1 Eval. gn .. . sn p(a|s) = P highest-scored, pick Figure 2: The workflow of tuning the aligner with the oracle parser. ai denotes the i-th alignment, gi denotes the i-th AMR graph, and si denotes the score of the i-th AMR graph. 7. If s0 has unprocessed edge, perform C ACHE. 8. If s0 doesn’t have unprocessed edge, perform R EDUCE. 9. perform S HIFT. We test our oracle parser on the hand-align data created by Flanigan et al. (2014) and it achieves 97.4 Smatch F1 score.7 Besides the errors resulted from incorr"
D18-1264,S16-1186,0,0.252255,"ieves 68.4 Smatch F1 score with only words and POS tags as input (§6) and outperforms the parser of Wang and Xue (2017). Our code and the alignments for LDC2014T12 dataset are publicly available at https:// github.com/Oneplus/tamr 2 Related Work AMR Parsers. AMR parsing maps a natural language sentence into its AMR graph. Most current parsers construct the AMR graph in a two-staged manner which first identifies concepts (nodes in the graph) from the input sentence, then identifies relations (edges in the graph) between the identified concepts. Flanigan et al. (2014) and their follow-up works (Flanigan et al., 2016; Zhou et al., 2016) model the parsing problem as finding the maximum spanning connected graph. Wang et al. (2015b) proposes to greedily transduce the dependency tree into AMR graph and a bunch of works (Wang et al., 2015a; Goodman et al., 2016; Wang and Xue, 2017) further improve the transducer’s performance with rich features and imitation learning.2 Transition-based methods 2 Wang et al. (2015b) and the follow-up works refer their transducing process as “transition-based”. However, to dis2423 that directly parse an input sentence into its AMR graph have also been studied (Ballesteros and Al"
D18-1264,P14-1134,0,0.0948162,"&quot; ARG1 reactor mod nucleus~1 quant 2 mod nucleus~2 op2 &quot;Korea&quot; Figure 1: AMR graph for the sentence “North Korea froze its nuclear actions in exchange for two nuclear reactors.” Abstract Meaning Representation (AMR) (Banarescu et al., 2013) is a semantic representation which encodes the meaning of a sentence in a rooted and directed graph, whose nodes are abstract semantic concepts and edges are semantic relations between concepts (see Figure 1 for an example). Parsing a sentence into its AMR graph has drawn a lot of research attention in recent years with a number of parsers being developed (Flanigan et al., 2014; Wang et al., 2015b; Pust et al., 2015; Artzi et al., 2015; Peng et al., 2015; Zhou et al., 2016; Goodman et al., 2016; Damonte et al., 2017; Ballesteros and Al-Onaizan, 2017; Foland and Martin, 2017; Konstas et al., 2017). The nature of abstracting away the association between a concept and a span of words complicates the training of the AMR parser. A wordconcept aligner is required to derive such association from the sentence-AMR-graph pair and the * Email corresponding. recieve-01 ARG0 ARG1 Introduction ∗ freeze-01 ARG3 alignment output is then used as reference to train the AMR parser. In"
D18-1264,P17-1043,0,0.136676,"resentation (AMR) (Banarescu et al., 2013) is a semantic representation which encodes the meaning of a sentence in a rooted and directed graph, whose nodes are abstract semantic concepts and edges are semantic relations between concepts (see Figure 1 for an example). Parsing a sentence into its AMR graph has drawn a lot of research attention in recent years with a number of parsers being developed (Flanigan et al., 2014; Wang et al., 2015b; Pust et al., 2015; Artzi et al., 2015; Peng et al., 2015; Zhou et al., 2016; Goodman et al., 2016; Damonte et al., 2017; Ballesteros and Al-Onaizan, 2017; Foland and Martin, 2017; Konstas et al., 2017). The nature of abstracting away the association between a concept and a span of words complicates the training of the AMR parser. A wordconcept aligner is required to derive such association from the sentence-AMR-graph pair and the * Email corresponding. recieve-01 ARG0 ARG1 Introduction ∗ freeze-01 ARG3 alignment output is then used as reference to train the AMR parser. In previous works, such alignment is extracted by either greedily applying a set of heuristic rules (Flanigan et al., 2014) or adopting the unsupervised word alignment technique from machine translation"
D18-1264,P16-1001,0,0.124026,"ts nuclear actions in exchange for two nuclear reactors.” Abstract Meaning Representation (AMR) (Banarescu et al., 2013) is a semantic representation which encodes the meaning of a sentence in a rooted and directed graph, whose nodes are abstract semantic concepts and edges are semantic relations between concepts (see Figure 1 for an example). Parsing a sentence into its AMR graph has drawn a lot of research attention in recent years with a number of parsers being developed (Flanigan et al., 2014; Wang et al., 2015b; Pust et al., 2015; Artzi et al., 2015; Peng et al., 2015; Zhou et al., 2016; Goodman et al., 2016; Damonte et al., 2017; Ballesteros and Al-Onaizan, 2017; Foland and Martin, 2017; Konstas et al., 2017). The nature of abstracting away the association between a concept and a span of words complicates the training of the AMR parser. A wordconcept aligner is required to derive such association from the sentence-AMR-graph pair and the * Email corresponding. recieve-01 ARG0 ARG1 Introduction ∗ freeze-01 ARG3 alignment output is then used as reference to train the AMR parser. In previous works, such alignment is extracted by either greedily applying a set of heuristic rules (Flanigan et al., 201"
D18-1264,P14-5010,0,0.0056679,"servation to that alignment noise is introduced by the semantic matching especially by the word embedding similarity component. Without filtering the noise by our oracle parser, just introducing more matching rules will harm the performance. 6 6.1 newswire 73.1 72.7 67.6 71.3 Parsing Experiments Settings We use the same settings in our aligner extrinsic evaluation for the experiments on our transitionbased parser. For the input to the parser, we tried two settings: 1) using only words as input, and 2) using words and POS tags as input. Automatic POS tags are assigned with Stanford POS tagger (Manning et al., 2014). Word embedding from Ling et al. (2015) is used in the same way with Ballesteros and Al-Onaizan (2017). To opt Results Table 6 shows the performance of our transitionbased parser along with comparison to the parsers in the previous works. When compared with our transition-based counterpart (Ballesteros and AlOnaizan, 2017), our word-only model outperforms theirs using the same JAMR alignment. The same trend is witnessed using words and POS tags as input. When replacing the JAMR alignments with ours, the parsing performances are improved in the same way as in Table 4, which further confirms th"
D18-1264,J08-4003,0,0.0603115,"special form of C ONFIRM that derives the first element into an entity and builds the internal entity AMR fragment. generates a new concept c and pushes it to the front of the buffer. Table 2: The transition system. The letters in monospace font represent the concepts, the italic letters represent the word, and the letters in normal font are either concepts or words. Their work presented the possibility for the oracle parser, but their oracle parser was not touched explicitly. What’s more, in the non-projective dependency parsing, Choi and McCallum (2013)’s extension to the list-based system (Nivre, 2008) with caching mechanism achieves expected linear time complexity and requires fewer actions to parse a non-projective tree than the swap-based system. Their extension to transition-based AMR parsing is worth studying. In this paper, we propose to extend Choi and McCallum (2013)’s transition system to AMR parsing and present the corresponding oracle parser. The oracle parser is used for tuning our aligner and training our parser. We also present a comprehensive comparison of our system with that of Ballesteros and Al-Onaizan (2017) in Section 6.3. 4.1 tended actions, and they are used to derivi"
D18-1264,K15-1004,0,0.283535,"for the sentence “North Korea froze its nuclear actions in exchange for two nuclear reactors.” Abstract Meaning Representation (AMR) (Banarescu et al., 2013) is a semantic representation which encodes the meaning of a sentence in a rooted and directed graph, whose nodes are abstract semantic concepts and edges are semantic relations between concepts (see Figure 1 for an example). Parsing a sentence into its AMR graph has drawn a lot of research attention in recent years with a number of parsers being developed (Flanigan et al., 2014; Wang et al., 2015b; Pust et al., 2015; Artzi et al., 2015; Peng et al., 2015; Zhou et al., 2016; Goodman et al., 2016; Damonte et al., 2017; Ballesteros and Al-Onaizan, 2017; Foland and Martin, 2017; Konstas et al., 2017). The nature of abstracting away the association between a concept and a span of words complicates the training of the AMR parser. A wordconcept aligner is required to derive such association from the sentence-AMR-graph pair and the * Email corresponding. recieve-01 ARG0 ARG1 Introduction ∗ freeze-01 ARG3 alignment output is then used as reference to train the AMR parser. In previous works, such alignment is extracted by either greedily applying a set"
D18-1264,E17-1035,0,0.292067,"; Damonte et al., 2017). In these works, the concept identification and relation identification are performed jointly. An aligner which maps a span of words into its concept serves to the generation of training data for the concept identifier, thus is important to the parser training. Missing or incorrect alignments lead to poor concept identification, which then hurt the overall AMR parsing performance. Besides the typical two-staged methods, the aligner also works in some other AMR parsing algorithms like that using syntax-based machine translation (Pust et al., 2015), sequence-to-sequence (Peng et al., 2017; Konstas et al., 2017), Hyperedge Replacement Grammar (Peng et al., 2015) and Combinatory Category Grammar (Artzi et al., 2015). Previous aligner works solve the alignment problem in two different ways. The rule-based aligner (Flanigan et al., 2014) defines a set of heuristic rules which align a span of words to the graph fragment and greedily applies these rules. The unsupervised aligner (Pourdamghani et al., 2014; Wang and Xue, 2017) uncovers the word-toconcept alignment from the linearized AMR graph through EM. All these approaches yield a single alignment for one sentence and its effect o"
D18-1264,D14-1162,0,0.081,"rs because of the dependencies and mutual exclusions between rules. In the JAMR aligner, rules that recall more alignments but introduce errors are carefully opted out and it influences the aligner’s performance. Our motivation is to use rich semantic resources to recall more alignments. Instead of resolving the resulted conflicts and errors by greedy search, we keep the multiple alignments produced by the aligner and let a parser decide the best alignment. In this paper, we use two kinds of semantic resources to recall more alignments, which include the similarity drawn from Glove embedding (Pennington et al., 2014)3 and the morphosemantic database (Fellbaum et al., 2009) in the WordNet project4 . Two additional matching schemes semantic match and morphological match are proposed as: Semantic Match. Glove embedding encodes a word into its vector representation. We define semantic match of a concept as a word in the sentence that has a cosine similarity greater than 0.7 in the embedding space with the concept striping off trailing number (e.g. run-01 → run). Morphological Match. Morphosemantic is a database that contains links among derivational 3 nlp.stanford.edu/projects/glove/ wordnet.princeton.edu/wor"
D18-1264,D14-1048,0,0.483311,"Konstas et al., 2017). The nature of abstracting away the association between a concept and a span of words complicates the training of the AMR parser. A wordconcept aligner is required to derive such association from the sentence-AMR-graph pair and the * Email corresponding. recieve-01 ARG0 ARG1 Introduction ∗ freeze-01 ARG3 alignment output is then used as reference to train the AMR parser. In previous works, such alignment is extracted by either greedily applying a set of heuristic rules (Flanigan et al., 2014) or adopting the unsupervised word alignment technique from machine translation (Pourdamghani et al., 2014; Wang and Xue, 2017). The rule-based aligner – JAMR aligner proposed by Flanigan et al. (2014) is widely used in previous works thanks to its flexibility of incorporating additional linguistic resources like WordNet. However, achieving good alignments with the JAMR aligner still faces some difficult challenges. The first challenge is deriving an optimal alignment in ambiguous situations. Taking the sentence-AMR-graph pair in Figure 1 for example, the JAMR aligner doesn’t distinguish between the two “nuclear”s in the sentence and can yield sub-optimal alignment in which the first “nuclear” is"
D18-1264,D15-1136,0,0.286353,"cleus~2 op2 &quot;Korea&quot; Figure 1: AMR graph for the sentence “North Korea froze its nuclear actions in exchange for two nuclear reactors.” Abstract Meaning Representation (AMR) (Banarescu et al., 2013) is a semantic representation which encodes the meaning of a sentence in a rooted and directed graph, whose nodes are abstract semantic concepts and edges are semantic relations between concepts (see Figure 1 for an example). Parsing a sentence into its AMR graph has drawn a lot of research attention in recent years with a number of parsers being developed (Flanigan et al., 2014; Wang et al., 2015b; Pust et al., 2015; Artzi et al., 2015; Peng et al., 2015; Zhou et al., 2016; Goodman et al., 2016; Damonte et al., 2017; Ballesteros and Al-Onaizan, 2017; Foland and Martin, 2017; Konstas et al., 2017). The nature of abstracting away the association between a concept and a span of words complicates the training of the AMR parser. A wordconcept aligner is required to derive such association from the sentence-AMR-graph pair and the * Email corresponding. recieve-01 ARG0 ARG1 Introduction ∗ freeze-01 ARG3 alignment output is then used as reference to train the AMR parser. In previous works, such alignment is extr"
D18-1264,D17-1035,0,0.0145197,"Aligner JAMR Our Alignment F1 (on hand-align) 90.6 95.2 Oracle’s Smatch (on dev. dataset) 91.7 94.7 model JAMR parser + Our aligner - Semantic matching - Oracle Parser Tuning JAMR parser + JAMR aligner Table 3: The intrinsic evaluation results. model newswire JAMR parser: Word, POS, NER, DEP + JAMR aligner 71.3 + Our aligner 73.1 CAMR parser: Word, POS, NER, DEP + JAMR aligner 68.4 + Our aligner 68.8 Table 5: The ablation test results. all out the effect of different initialization in training the neural network, we run 10 differently seeded runs and report their average performance following Reimers and Gurevych (2017). 65.9 67.6 64.6 65.1 6.2 Table 4: The parsing results. Extrinsic Evaluation. Table 4 shows the results. From this table, we can see that our alignment consistently improves all the parsers by a margin ranging from 0.5 to 1.7. Both the intrinsic and the extrinsic evaluations show the effectiveness our aligner. 5.3 Ablation To have a better understanding of our aligner, we conduct ablation test by removing the semantic matching and oracle parser tuning respectively and retrain the JAMR parser on the newswire proportion. The results are shown in Table 5. From this table, we can see that removing"
D18-1264,D17-1129,0,0.460076,"racle parser. Our aligner is further tuned by our oracle parser via picking the alignment that leads to the highestscored achievable AMR graph. Experimental results show that our aligner outperforms the rule-based aligner in previous work by achieving higher alignment F1 score and consistently improving two open-sourced AMR parsers. Based on our aligner and transition system, we develop a transition-based AMR parser that parses a sentence into its AMR graph directly. An ensemble of our parsers with only words and POS tags as input leads to 68.4 Smatch F1 score, which outperforms the parser of Wang and Xue (2017). 1 ARG1 ARG0 country act-02 name name op1 &quot;North&quot; ARG1 reactor mod nucleus~1 quant 2 mod nucleus~2 op2 &quot;Korea&quot; Figure 1: AMR graph for the sentence “North Korea froze its nuclear actions in exchange for two nuclear reactors.” Abstract Meaning Representation (AMR) (Banarescu et al., 2013) is a semantic representation which encodes the meaning of a sentence in a rooted and directed graph, whose nodes are abstract semantic concepts and edges are semantic relations between concepts (see Figure 1 for an example). Parsing a sentence into its AMR graph has drawn a lot of research attention in recent"
D18-1264,P15-2141,0,0.39251,"eus~1 quant 2 mod nucleus~2 op2 &quot;Korea&quot; Figure 1: AMR graph for the sentence “North Korea froze its nuclear actions in exchange for two nuclear reactors.” Abstract Meaning Representation (AMR) (Banarescu et al., 2013) is a semantic representation which encodes the meaning of a sentence in a rooted and directed graph, whose nodes are abstract semantic concepts and edges are semantic relations between concepts (see Figure 1 for an example). Parsing a sentence into its AMR graph has drawn a lot of research attention in recent years with a number of parsers being developed (Flanigan et al., 2014; Wang et al., 2015b; Pust et al., 2015; Artzi et al., 2015; Peng et al., 2015; Zhou et al., 2016; Goodman et al., 2016; Damonte et al., 2017; Ballesteros and Al-Onaizan, 2017; Foland and Martin, 2017; Konstas et al., 2017). The nature of abstracting away the association between a concept and a span of words complicates the training of the AMR parser. A wordconcept aligner is required to derive such association from the sentence-AMR-graph pair and the * Email corresponding. recieve-01 ARG0 ARG1 Introduction ∗ freeze-01 ARG3 alignment output is then used as reference to train the AMR parser. In previous works, su"
D18-1264,N15-1040,0,0.456906,"eus~1 quant 2 mod nucleus~2 op2 &quot;Korea&quot; Figure 1: AMR graph for the sentence “North Korea froze its nuclear actions in exchange for two nuclear reactors.” Abstract Meaning Representation (AMR) (Banarescu et al., 2013) is a semantic representation which encodes the meaning of a sentence in a rooted and directed graph, whose nodes are abstract semantic concepts and edges are semantic relations between concepts (see Figure 1 for an example). Parsing a sentence into its AMR graph has drawn a lot of research attention in recent years with a number of parsers being developed (Flanigan et al., 2014; Wang et al., 2015b; Pust et al., 2015; Artzi et al., 2015; Peng et al., 2015; Zhou et al., 2016; Goodman et al., 2016; Damonte et al., 2017; Ballesteros and Al-Onaizan, 2017; Foland and Martin, 2017; Konstas et al., 2017). The nature of abstracting away the association between a concept and a span of words complicates the training of the AMR parser. A wordconcept aligner is required to derive such association from the sentence-AMR-graph pair and the * Email corresponding. recieve-01 ARG0 ARG1 Introduction ∗ freeze-01 ARG3 alignment output is then used as reference to train the AMR parser. In previous works, su"
D18-1264,D16-1065,0,0.64792,"North Korea froze its nuclear actions in exchange for two nuclear reactors.” Abstract Meaning Representation (AMR) (Banarescu et al., 2013) is a semantic representation which encodes the meaning of a sentence in a rooted and directed graph, whose nodes are abstract semantic concepts and edges are semantic relations between concepts (see Figure 1 for an example). Parsing a sentence into its AMR graph has drawn a lot of research attention in recent years with a number of parsers being developed (Flanigan et al., 2014; Wang et al., 2015b; Pust et al., 2015; Artzi et al., 2015; Peng et al., 2015; Zhou et al., 2016; Goodman et al., 2016; Damonte et al., 2017; Ballesteros and Al-Onaizan, 2017; Foland and Martin, 2017; Konstas et al., 2017). The nature of abstracting away the association between a concept and a span of words complicates the training of the AMR parser. A wordconcept aligner is required to derive such association from the sentence-AMR-graph pair and the * Email corresponding. recieve-01 ARG0 ARG1 Introduction ∗ freeze-01 ARG3 alignment output is then used as reference to train the AMR parser. In previous works, such alignment is extracted by either greedily applying a set of heuristic rules"
D19-1029,P15-1034,0,0.0252062,", we use pair-wise comparison to match the extracted and groundtruth tuples. We evaluate the correctness on the tuple’s five slots using the same metrics. Baselines: We compare with statistical sequence labeling methods: Structured Support Vector Machine (SVM) (Tsochantaridis et al., 2005) and Conditional random field (CRF) (Lafferty et al., 2001). We compare with a neural sequence labeling method, BiLSTM-LSTMd (Zheng et al., 2017). We replace its encoder with BERT (Devlin et al., 2018) to make it a more competitive baseline. We also compare against two renown OpenIE systems, Stanford OpenIE (Angeli et al., 2015) and AllenNLP OpenIE (Stanovsky et al., 2018) followed by a condition/fact classification. We enhance statistical sequence labeling models with multi-input signals for fairness, and train them for fact tuple and condition tuple extrac4.2 Results on BioCFE In this section, we present overall performance, ablation study, error analysis, and efficiency. 4.2.1 Overall Performance Table 1 shows that the proposed multi-input multioutput sequence labeling model with a BERT encoder consistently performs the best over all the baselines on tag prediction and tuple extraction. Compared to BiLSTM-LSTMd, B"
D19-1029,D15-1104,0,0.081185,"Missing"
D19-1029,W13-2001,0,0.297343,"Missing"
D19-1029,W14-1609,0,0.0193652,"Missing"
D19-1029,D14-1162,0,0.0821539,"rain the RNT layer with the multi-input module till the relation name’s tag prediction achieves a higherthan-0.8 F1 score. Then we plug the TCT layer onto the RNT layer and train the entire framework to generate the multi-output tag sequences. 4 Experiments tion separately. In the neural baselines (BiLSTMLSTMd and BERT-LSTMd), fact extraction and condition extraction share the encoder-decoder model and use different, proper parameters in the linear-softmax layer. Hyperparameters: The multi-input module has a BiLSTM/BERT encoder and a LSTM decoder. The word embeddings were obtained from GloVe (Pennington et al., 2014) with the dimension size dW E = 50. The language model dimension size ns dLM = 200. The size of POS tag embedding is dP OS = 6. The size of CAP tag embedding is dCAP = 3. The number of LSTM units in the encoding layer is 300. The number of transformer units in the BERT encoding layer is 768. We evaluate the performance of condition/fact tag prediction and tuple extraction by the proposed MIMO model, its variants, and state-of-the-art models on the newly annotated BioCFE dataset and transferred to the BioNLP2013 dataset. 4.1 Experimental Setup Datasets: Statistics of BioCFE has been given in th"
D19-1029,D15-1025,0,0.0624245,"Missing"
D19-1029,C18-1194,0,0.0270153,"Missing"
D19-1029,N18-1081,0,0.151065,"entific literature (Miller, 1947). Existing ScienceIE methods, which extract (subject, relational phrase, object)-tuples from scientific text, do not distinguish the roles of fact and condition. Simply adding a tuple classification module has two weak points: (1) one tuple may have different roles in different sentences; (2) the tuples in one sentence have high dependencies with each other, for example, given a statement sentence in a biochemistry paper (Tomilin et al., 2016): “We observed that ... alkaline pH increases the activity of TRPV5/V6 channels in Jurkat T cells.” an existing system (Stanovsky et al., 2018) would return one tuple as below: Multi-Input Module Multi-Head Input-Gates Multi-Head Input-Gates Statement Sentence Figure 1: Our framework has two modules: (1) a multiinput module (bottom) based on a multi-head encoderdecoder model with multi-input gates; (2) a multioutput module (top) of a relation name tagging layer and a tuple completion tagging layer. (alkaline pH, increases, activity of TRPV5/V6 channels in Jurkat T cells). where (a) the object should just be the channel’s activity and (b) the condition tuple (TRPV5/V6 channels, in, Jurkat T cells) was not found. Note that the term “TR"
D19-1029,D18-1360,0,0.18642,"taset 3.1 We built a system with GUI (Figure 2) to collect a new dataset for the joint tuple extraction purpose, named Biomedical Conditional Fact Extraction (BioCFE). Three participants (experts in biomedical domain) manually annotated the fact and condition tuples from statement sentences from 31 paper abstracts in the MEDLINE database. The anThe Multi-Input Module Preprocessing for input sequences: Following fundamental NLP techniques have achieved high accuracy requiring no additional training with labeled data: Language Model (LM) (Howard and Ruder, 2018), POS (Labeau et al., 2015), CAP (Luan et al., 2018; Jiang et al., 2017; Shang 303 et al., 2018; Wang et al., 2018a). For any given input sentence, we tokenize it and represent each token by its word embedding (pre-trained GloVe vector in this paper). Then we get another three input sequences by the input sentence and the above three fundamental NLP techniques. (1) A pre-trained LSTM-based language model takes the sentence as input and returns semantic embedding sequence, where the dependencies between a token and its predecessors in distant contexts are preserved. (2) We employ NLTK tool to generate the POS tag sequence for the given sentence"
D19-1029,D17-1279,0,0.0259644,"the conditions of the factual claims. They describe the methodology of the observation (e.g., “using”, “in combination with”) or the context (e.g., “in” a specific disease or “from” specific animals). In some other cases, we find the temperature and pH values are detected as the conditions of observations. 5 5.1 Related Work Scientific Information Extraction Information extraction in scientific literature, e.g., computer science, biology and chemistry, has been receiving much attention in recent years. ScienceIE in computer science focus on concept recognition and factual relation extraction (Luan et al., 2017; G´abor et al., 2018; Luan et al., 2018). ScienceIE in biological literature aims at identifying the relationships between biological concepts tion and fact tuples from scientific text. We proposed a multi-input multi-output sequence labeling model to utilize results from well-established related tasks and extract an uncertain number of fact(s)/condition(s). Our model yields improvement over all the baselines on a newly annotated dataset BioCFE and a public dataset BioNLP2013. We argue that structured representations of knowledge, such as fact/condition tuple, for scientific statements will e"
D19-1029,P10-1013,0,0.151675,"Missing"
D19-1029,P17-1132,0,0.0442362,"Missing"
D19-1029,P17-1113,0,0.107708,"s of words. Dependencies between the POS tag and tuple tag (e.g., “VBD” and “B-f2p”) can be modeled. We also spot high dependencies between the CAP tag and tuple tag. For example, the tokens of “B/I-c” (concept) and “B/I-a” (attribute) tags have high probability of being labeled as “B/I-XYc” and “B/IXYa” in the output sequences, respectively. Multi-head Encoder-Decoder: We investigate two neural models as encoder: one is bidirectional LSTM (BiLSTM), the other is the renown, bidirectional encoder representations from Transformers (BERT). We adopt a LSTM structure as the decoding layer (LSTMd) (Zheng et al., 2017). We observe that the input sequences may have different tag predictability on different sentences. For short sentences, POS and CAP are more useful (modeling local dependencies); for long sentences, LM is more effective (modeling distant dependencies). In order to secure the model’s robustness on massive data, we apply a multi-head mechanism to the encoder-decoder model. Each 304 Finally, we apply the matching function in (Stanovsky et al., 2018) to complete and extract the tuples (i.e., the concepts and/or attributes in the subjects and objects) for each output sequence. and the softmax pred"
D19-1169,P18-1163,0,0.0509824,"Missing"
D19-1169,P17-1055,1,0.877462,"erimental results on two public Chinese reading comprehension datasets show that the proposed cross-lingual approaches yield significant improvements over various baseline systems and set new state-of-the-art performances. 2 Related Works Machine Reading Comprehension (MRC) has been a trending research topic in recent years. Among various types of MRC tasks, spanextraction reading comprehension has been enormously popular (such as SQuAD (Rajpurkar et al., 2016)), and we have seen a great progress on related neural network approaches (Wang and Jiang, 2016; Seo et al., 2016; Xiong et al., 2016; Cui et al., 2017; Hu et al., 2019), especially those were built on pre-trained language models, such as BERT (Devlin et al., 2019). While massive achievements have been made by the community, reading comprehension in other than English has not been well-studied mainly due to the lack of large-scale training data. Asai et al. (2018) proposed to use runtime machine translation for multilingual extractive reading comprehension. They first translate the data from the target language to English and then obtain an answer using an English reading comprehension model. Finally, they recover the corresponding answer in"
D19-1169,D19-1600,1,0.855968,"when there is no or partially available resources in the target language. Then we propose a novel model called Dual BERT to further improve the system performance when there is training data available in the target language. We first translate target language training data into English to form pseudo bilingual parallel data. Then we use multilingual BERT (Devlin et al., 2019) to simultaneously model the &lt;passage, question, answer&gt; in both languages, and fuse the representations of both to generate final predictions. Experimental results on two Chinese reading comprehension dataset CMRC 2018 (Cui et al., 2019) and DRCD (Shao et al., 2018) show that by utilizing English resources could substantially improve system performance and the proposed Dual BERT achieves state-of-the-art performances on both datasets, and even surpass human performance on some metrics. Also, we 1586 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 1586–1595, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics conduct experiments on the Japanese and French SQuAD (Asai et al.,"
D19-1169,N19-1423,0,0.681076,"tween &lt;passage, question, answer&gt;. In this paper, we propose the Cross-Lingual Machine Reading Comprehension (CLMRC) task that aims to help reading comprehension in lowresource languages. First, we present several back-translation approaches when there is no or partially available resources in the target language. Then we propose a novel model called Dual BERT to further improve the system performance when there is training data available in the target language. We first translate target language training data into English to form pseudo bilingual parallel data. Then we use multilingual BERT (Devlin et al., 2019) to simultaneously model the &lt;passage, question, answer&gt; in both languages, and fuse the representations of both to generate final predictions. Experimental results on two Chinese reading comprehension dataset CMRC 2018 (Cui et al., 2019) and DRCD (Shao et al., 2018) show that by utilizing English resources could substantially improve system performance and the proposed Dual BERT achieves state-of-the-art performances on both datasets, and even surpass human performance on some metrics. Also, we 1586 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the"
D19-1169,P17-1168,0,0.0185237,"emonstrate the potentials in CLMRC task. 1 1 Introduction Machine Reading Comprehension (MRC) has been a popular task to test the reading ability of the machine, which requires to read text material and answer the questions based on it. Starting from cloze-style reading comprehension, various neural network approaches have been proposed and massive progresses have been made in creating largescale datasets and neural models (Hermann et al., 2015; Hill et al., 2015; Kadlec et al., 2016; Cui 1 Resources available: https://github.com/ ymcui/Cross-Lingual-MRC. et al., 2017; Rajpurkar et al., 2016; Dhingra et al., 2017). Though various types of contributions had been made, most works are dealing with English reading comprehension. Reading comprehension in other than English has not been well-addressed mainly due to the lack of large-scale training data. To enrich the training data, there are two traditional approaches. Firstly, we can annotate data by human experts, which is ideal and high-quality, while it is time-consuming and rather expensive. One can also obtain large-scale automatically generated data (Hermann et al., 2015; Hill et al., 2015; Liu et al., 2017), but the quality is far beyond the usable t"
D19-1169,P16-1086,0,0.0304847,"and DRCD. The results show consistent and significant improvements over various stateof-the-art systems by a large margin, which demonstrate the potentials in CLMRC task. 1 1 Introduction Machine Reading Comprehension (MRC) has been a popular task to test the reading ability of the machine, which requires to read text material and answer the questions based on it. Starting from cloze-style reading comprehension, various neural network approaches have been proposed and massive progresses have been made in creating largescale datasets and neural models (Hermann et al., 2015; Hill et al., 2015; Kadlec et al., 2016; Cui 1 Resources available: https://github.com/ ymcui/Cross-Lingual-MRC. et al., 2017; Rajpurkar et al., 2016; Dhingra et al., 2017). Though various types of contributions had been made, most works are dealing with English reading comprehension. Reading comprehension in other than English has not been well-addressed mainly due to the lack of large-scale training data. To enrich the training data, there are two traditional approaches. Firstly, we can annotate data by human experts, which is ideal and high-quality, while it is time-consuming and rather expensive. One can also obtain large-scale"
D19-1169,P17-1010,1,0.839775,". et al., 2017; Rajpurkar et al., 2016; Dhingra et al., 2017). Though various types of contributions had been made, most works are dealing with English reading comprehension. Reading comprehension in other than English has not been well-addressed mainly due to the lack of large-scale training data. To enrich the training data, there are two traditional approaches. Firstly, we can annotate data by human experts, which is ideal and high-quality, while it is time-consuming and rather expensive. One can also obtain large-scale automatically generated data (Hermann et al., 2015; Hill et al., 2015; Liu et al., 2017), but the quality is far beyond the usable threshold. Another way is to exploit cross-lingual approaches to utilize the data in richresource language to implicitly learn the relations between &lt;passage, question, answer&gt;. In this paper, we propose the Cross-Lingual Machine Reading Comprehension (CLMRC) task that aims to help reading comprehension in lowresource languages. First, we present several back-translation approaches when there is no or partially available resources in the target language. Then we propose a novel model called Dual BERT to further improve the system performance when ther"
D19-1169,D16-1264,0,0.760855,"a large margin, which demonstrate the potentials in CLMRC task. 1 1 Introduction Machine Reading Comprehension (MRC) has been a popular task to test the reading ability of the machine, which requires to read text material and answer the questions based on it. Starting from cloze-style reading comprehension, various neural network approaches have been proposed and massive progresses have been made in creating largescale datasets and neural models (Hermann et al., 2015; Hill et al., 2015; Kadlec et al., 2016; Cui 1 Resources available: https://github.com/ ymcui/Cross-Lingual-MRC. et al., 2017; Rajpurkar et al., 2016; Dhingra et al., 2017). Though various types of contributions had been made, most works are dealing with English reading comprehension. Reading comprehension in other than English has not been well-addressed mainly due to the lack of large-scale training data. To enrich the training data, there are two traditional approaches. Firstly, we can annotate data by human experts, which is ideal and high-quality, while it is time-consuming and rather expensive. One can also obtain large-scale automatically generated data (Hermann et al., 2015; Hill et al., 2015; Liu et al., 2017), but the quality is"
D19-1169,L18-1431,1,0.946224,"ly available resources in the target language. Then we propose a novel model called Dual BERT to further improve the system performance when there is training data available in the target language. We first translate target language training data into English to form pseudo bilingual parallel data. Then we use multilingual BERT (Devlin et al., 2019) to simultaneously model the &lt;passage, question, answer&gt; in both languages, and fuse the representations of both to generate final predictions. Experimental results on two Chinese reading comprehension dataset CMRC 2018 (Cui et al., 2019) and DRCD (Shao et al., 2018) show that by utilizing English resources could substantially improve system performance and the proposed Dual BERT achieves state-of-the-art performances on both datasets, and even surpass human performance on some metrics. Also, we 1586 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 1586–1595, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics conduct experiments on the Japanese and French SQuAD (Asai et al., 2018) and achieves substanti"
D19-1169,1983.tc-1.13,0,0.231539,"Missing"
D19-1299,H05-1042,0,0.186391,"attern doesn’t hold for this case, because he is British. Hopefully, such cases are rare since the birth place conforms to the nationality in most of cases, so our methods can bring improvement as indicated by the overall better performance across almost all Wikipedia categories. 5 Related Work Data-to-text generation is an important task in natural language generation which has been studied for decades (Kukich, 1983; Holmes-Higgin, 1994; Reiter and Dale, 1997). This task is broadly divided into two subproblems: content selection (Kukich, 1983; Reiter and Dale, 1997; Duboue and McKeown, 2003; Barzilay and Lapata, 2005) and surface realization (Goldberg et al., 1994; Reiter et al., 2005). With the advent of neural text generation, the distinction between content selection and surface realization becomes blurred. For example, Mei et al. (2016) proposed an end-to-end encoder-aligner-decoder model to learn both content selection and surface realization jointly which shows good results on WeatherGov and RoboCub datasets. Wiseman et al. (2017) generate long descriptive game summaries from a database of basketball games where they show the current state-of-the-art neural models are quite good at generating fluent"
D19-1299,W03-1016,0,0.0900029,"However, this inference pattern doesn’t hold for this case, because he is British. Hopefully, such cases are rare since the birth place conforms to the nationality in most of cases, so our methods can bring improvement as indicated by the overall better performance across almost all Wikipedia categories. 5 Related Work Data-to-text generation is an important task in natural language generation which has been studied for decades (Kukich, 1983; Holmes-Higgin, 1994; Reiter and Dale, 1997). This task is broadly divided into two subproblems: content selection (Kukich, 1983; Reiter and Dale, 1997; Duboue and McKeown, 2003; Barzilay and Lapata, 2005) and surface realization (Goldberg et al., 1994; Reiter et al., 2005). With the advent of neural text generation, the distinction between content selection and surface realization becomes blurred. For example, Mei et al. (2016) proposed an end-to-end encoder-aligner-decoder model to learn both content selection and surface realization jointly which shows good results on WeatherGov and RoboCub datasets. Wiseman et al. (2017) generate long descriptive game summaries from a database of basketball games where they show the current state-of-the-art neural models are quit"
D19-1299,W18-6539,0,0.0402782,"Missing"
D19-1299,P83-1022,0,0.717921,"performance. 1 Entity Linking Object country (P17) Algeria (Q262) defender (Q336286) instance of (P31) association football position (Q4611891) … … … MC El Eulma (Q2742749) league (P118) Algerian Ligue Professionnelle 1 (Q647746) Figure 1: An example of generating description from a Wikipedia infobox. External background knowledge expanded from the infobox is helpful for generation. Automatic text generation from structured data (data-to-text) is a classic task in natural language generation which aims to automatically generate fluent, truthful and informative texts based on structured data (Kukich, 1983; Holmes-Higgin, 1994; Reiter and Dale, 1997). Data-to-text is often formulated into two subproblems: content selection which decides what contents should be included in the text and surface realization which Contribution during internship at Microsoft Research. Relation Guelma (Q609871) External Background Knowledge from Wikidata Introduction ∗ Subject determines how to generate the text based on selected contents. Traditionally, these two subproblems have been tackled separately. In recent years, neural generation models, especially the encoder-decoder model, solve these two subproblems join"
D19-1299,D16-1128,0,0.127722,"Missing"
D19-1299,P17-1099,0,0.44058,"Our model takes a data table D (e.g., a Wikipedia infobox) and a relevant external knowledge base (KB) containing a set of facts F as input and generates a natural language text y = y1 , ..., yT consisting of T words. To augment the infobox with external knowledge, we preserve the Wikipedia internal hyperlink information in the field values of infobox, and track these hyperlinks to get their corresponding entities from Wikidata2 where we retrieve only one-hop facts. The backbone of our model is an attention based sequence-to-sequence model (Bahdanau et al., 2014) equipped with copy mechanism (See et al., 2017). As shown in Fig. 2, the model consists of four main components: a table encoder, a KB encoder, the dual attention mechanism and a decoder. We describe each component in the following sections. 2.1 Table Encoder In Fig. 2, the input data table D consists of several field name and field value pairs. We follow (Sha et al., 2017; Liu et al., 2017) to tokenize the field values and transform the input table into a flattened sequence {(ni , vi )}N i=1 , where each element is a token vi from a field value paired with its corresponding field name ni . To encode the flattened table, we map each (ni ,"
D19-1299,P14-5010,0,0.00469779,"Missing"
D19-1299,N16-1086,0,0.262803,"en formulated into two subproblems: content selection which decides what contents should be included in the text and surface realization which Contribution during internship at Microsoft Research. Relation Guelma (Q609871) External Background Knowledge from Wikidata Introduction ∗ Subject determines how to generate the text based on selected contents. Traditionally, these two subproblems have been tackled separately. In recent years, neural generation models, especially the encoder-decoder model, solve these two subproblems jointly and have achieved remarkable successes in several benchmarks (Mei et al., 2016; Lebret et al., 2016; Wiseman et al., 2017; Duˇsek et al., 2018; Nie et al., 2018). Such end-to-end data-to-text models rely on massive parallel pairs of data and text to learn the writing knowledge. They often assume that all writing knowledge can be learned from the training data. However, when people are writing, they will not only rely on the data contents themselves but also consider related knowledge, which is neglected by previous methods. For example, as shown in Fig. 1, an infobox about a person called Nacer Hammami is paired with its corresponding biography description from the Wiki"
D19-1299,P18-1076,0,0.0245316,"k to multi-sentence text generation, where they focus on bootstrapping generators from loosely aligned data. However, most of the work mentioned above assume all the writing knowledge can be learned from massive parallel pairs of training data. Different from the previous work, we exploit incorporating external 3029 knowledge into this task to improve the fidelity of generated text. Our work is also relevant to recent works on integrating external knowledge into neural models for other NLP tasks. The motivations of incorporating external knowledge range from enriching the context information (Mihaylov and Frank, 2018) in reading comprehension, improving the inference ability of models (Chen et al., 2018) in natural language inference, to providing the model a knowledge source to copy from in language modelling (Ahn et al., 2016). Our model, KBAtt, is most relevant to Mihaylov and Frank (2018), where they focus on similarity calculation but we focus on generation in this paper. Moreover, in addition to demonstrating the positive effect of incorporating external knowledge as previous work, we also design a new metric to quantify the potential gains of external knowledge for a specific dataset which can expla"
D19-1299,N18-1139,0,0.150009,"zation jointly which shows good results on WeatherGov and RoboCub datasets. Wiseman et al. (2017) generate long descriptive game summaries from a database of basketball games where they show the current state-of-the-art neural models are quite good at generating fluent outputs, but perform poorly in content selection and capturing long-term structure. Our work falls into the task of single sentence generation from Wikipedia infoboxes. The model structure ranges from feed-forward networks work (Lebret et al., 2016) to encoderdecoder models (Sha et al., 2017; Liu et al., 2017; Bao et al., 2018; Nema et al., 2018). Recently, Perez-Beltrachini and Lapata (2018) generalize this task to multi-sentence text generation, where they focus on bootstrapping generators from loosely aligned data. However, most of the work mentioned above assume all the writing knowledge can be learned from massive parallel pairs of training data. Different from the previous work, we exploit incorporating external 3029 knowledge into this task to improve the fidelity of generated text. Our work is also relevant to recent works on integrating external knowledge into neural models for other NLP tasks. The motivations of incorporatin"
D19-1299,D18-1422,1,0.862745,"should be included in the text and surface realization which Contribution during internship at Microsoft Research. Relation Guelma (Q609871) External Background Knowledge from Wikidata Introduction ∗ Subject determines how to generate the text based on selected contents. Traditionally, these two subproblems have been tackled separately. In recent years, neural generation models, especially the encoder-decoder model, solve these two subproblems jointly and have achieved remarkable successes in several benchmarks (Mei et al., 2016; Lebret et al., 2016; Wiseman et al., 2017; Duˇsek et al., 2018; Nie et al., 2018). Such end-to-end data-to-text models rely on massive parallel pairs of data and text to learn the writing knowledge. They often assume that all writing knowledge can be learned from the training data. However, when people are writing, they will not only rely on the data contents themselves but also consider related knowledge, which is neglected by previous methods. For example, as shown in Fig. 1, an infobox about a person called Nacer Hammami is paired with its corresponding biography description from the Wikipedia. However, the information in the infobox is not enough to cover all the facts"
D19-1299,P02-1040,0,0.103893,"ethods. We also compare our results with other published results using the WikiBio dataset. The model structure of our baseline model is most similar to Sha et al. (2017) by removing their specialized design on order planning which is not the focus of this paper. Since our aim is to verify the effectiveness of external knowledge for datato-text task, we keep our baseline model as general as possible without other specialized design. Our primary model is KBAtt: a model which integrates the background knowledge into baseline model through a KB encoder and KB attention mechanism. We employ BLEU (Papineni et al., 2002) as the automatic evaluation metric. In addition to BLEU, we conduct human evaluation to assess the factual accuracy of generated sentences. 6 Although the English Wikipedia has about 5 million entities, we totally parsed 1,656,458 infoboxes and drop some of them due to data noise which indicates nearly 2/3 entities have no infobox. 3026 Training Details The dimensions of all trainable word embeddings are set to 512, and the GRU hidden states sizes are set to 512. To limit the memory of our model, we set the maximum number of facts per table to 500. We initialize all the model parameters rando"
D19-1299,D17-1239,0,0.153643,"tent selection which decides what contents should be included in the text and surface realization which Contribution during internship at Microsoft Research. Relation Guelma (Q609871) External Background Knowledge from Wikidata Introduction ∗ Subject determines how to generate the text based on selected contents. Traditionally, these two subproblems have been tackled separately. In recent years, neural generation models, especially the encoder-decoder model, solve these two subproblems jointly and have achieved remarkable successes in several benchmarks (Mei et al., 2016; Lebret et al., 2016; Wiseman et al., 2017; Duˇsek et al., 2018; Nie et al., 2018). Such end-to-end data-to-text models rely on massive parallel pairs of data and text to learn the writing knowledge. They often assume that all writing knowledge can be learned from the training data. However, when people are writing, they will not only rely on the data contents themselves but also consider related knowledge, which is neglected by previous methods. For example, as shown in Fig. 1, an infobox about a person called Nacer Hammami is paired with its corresponding biography description from the Wikipedia. However, the information in the info"
D19-1299,N18-1137,0,0.0169804,"d results on WeatherGov and RoboCub datasets. Wiseman et al. (2017) generate long descriptive game summaries from a database of basketball games where they show the current state-of-the-art neural models are quite good at generating fluent outputs, but perform poorly in content selection and capturing long-term structure. Our work falls into the task of single sentence generation from Wikipedia infoboxes. The model structure ranges from feed-forward networks work (Lebret et al., 2016) to encoderdecoder models (Sha et al., 2017; Liu et al., 2017; Bao et al., 2018; Nema et al., 2018). Recently, Perez-Beltrachini and Lapata (2018) generalize this task to multi-sentence text generation, where they focus on bootstrapping generators from loosely aligned data. However, most of the work mentioned above assume all the writing knowledge can be learned from massive parallel pairs of training data. Different from the previous work, we exploit incorporating external 3029 knowledge into this task to improve the fidelity of generated text. Our work is also relevant to recent works on integrating external knowledge into neural models for other NLP tasks. The motivations of incorporating external knowledge range from enriching the c"
D19-1310,N18-2097,0,0.0869617,"representation of the row which may reflect the row (player or team)’s overall performance. C denotes the number of columns. rowi = M eanP ooling(˜ ri,1 , r˜i,2 , ..., r˜i,C ) (8) Then, we adopt content selection gate gi , which is proposed by Puduppully et al. (2019) on rows’ representations rowi , and obtain a new representation row ˜ i = gi rowi to choose more important information based on each row’s context. 3.4 Decoder with Dual Attention Since record encoders with record fusion gate provide record-level representation and row-level encoder provides row-level representation. Inspired by Cohan et al. (2018), we can modify 4.1 Experiments Dataset and Evaluation Metrics We conducted experiments on ROTOWIRE (Wiseman et al., 2017). For each example, it provides three tables as described in Section 2.1 which consists of 628 records in total with a long game summary. The average length of game summary is 337.1. In this paper, we followed the data split introduced in Wiseman et al. (2017): 3398 examples in training set, 727 examples in development set and 728 examples in test set. We followed Wiseman et al. (2017)’s work and use BLEU (Papineni et al., 2002) and three extractive evaluation metrics RG, C"
D19-1310,P16-1014,0,0.207025,"son scored 18 points. Several related models have been proposed . They typically encode the table’s records separately or as a long sequence and generate a long descriptive summary 3143 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 3143–3152, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics by a standard Seq2Seq decoder with some modifications. Wiseman et al. (2017) explored two types of copy mechanism and found conditional copy model (Gulcehre et al., 2016) perform better . Puduppully et al. (2019) enhanced content selection ability by explicitly selecting and planning relevant records. Li and Wan (2018) improved the precision of describing data records in the generated texts by generating a template at first and filling in slots via copy mechanism. Nie et al. (2018) utilized results from pre-executed operations to improve the fidelity of generated texts. However, we claim that their encoding of tables as sets of records or a long sequence is not suitable. Because (1) the table consists of multiple players and different types of information as s"
D19-1310,N18-2098,0,0.0418078,"on” in this match as “double-double” which requires ability to capture dependency from different columns (different type of record) in the same row (player). In addition, it models “Al Jefferson” history performance and correctly states that “It was his second double-double over his last three games”, which is also mentioned in gold texts included in Figure 1 in a similar way. 5 Related Work In recent years, neural data-to-text systems make remarkable progress on generating texts directly from data. Mei et al. (2016) proposes an encoderaligner-decoder model to generate weather forecast, while Jain et al. (2018) propose a mixed hierarchical attention. Sha et al. (2018) proposes a hybrid content- and linkage-based attention mechanism to model the order of content. Liu et al. (2018) propose to integrate field information into table representation and enhance decoder with dual attention. Bao et al. (2018) develops a table-aware encoder-decoder model. Wiseman et al. (2017) introduced a document-scale data-totext dataset, consisting of long text with more redundant records, which requires the model to select important information to generate. We describe recent works in Section 1. Also, some studies in ab"
D19-1310,C18-1089,0,0.132127,"Missing"
D19-1310,W17-4505,0,0.0124538,"ose to integrate field information into table representation and enhance decoder with dual attention. Bao et al. (2018) develops a table-aware encoder-decoder model. Wiseman et al. (2017) introduced a document-scale data-totext dataset, consisting of long text with more redundant records, which requires the model to select important information to generate. We describe recent works in Section 1. Also, some studies in abstractive text summarization encode long texts in a hierarchical manner. Cohan et al. (2018) uses a hierarchical encoder to encode input, paired with a discourse-aware decoder. Ling and Rush (2017) encode document hierarchically and propose coarse-to-fine attention for decoder. Recently, Liu et al. (2019) propose a hierarchical encoder for data-to-text generation which uses LSTM as its cell. Murakami et al. (2017) propose to model stock market time-series data and generate comments. As for incorporating historical background in generation, Robin (1994) proposed to build a draft with essential new facts at first, then incorporate background facts when revising the draft based on functional unification grammars. Different from that, we encode the historical (time dimension) information in"
D19-1310,Q18-1005,0,0.0298412,"xts. We describe model’s details in following parts. 3.1 3.1.1 Layer 1: Record Encoders Row Dimension Encoder Based on our observation, when someone’s points is mentioned in texts, some related records such as “field goals made” (FGM) and “field goals attempted” (FGA) will also be included in texts. Taken gold texts in Figure 1 as example, when Al Jefferson’s point 18 is mentioned, his FGM 9 and FGA 19 are also mentioned. Thus, when modeling a record, other records in the same row can be useful. Since the record in the row is not sequential, we use a self-attention network which is similar to Liu and Lapata (2018) to model records in the conrow be text of other records in the same row. Let ri,j the row dimension representation of the record of ith row and j th column. Then, we obtain the context vector in row dimension crow i,j by attending to other records in the same row as follows. Please row ∝ exp(r T W r 0 ) is normalnote that αi,j,j 0 o i,j i,j ized across records in the same row i. Wo is a trainable parameter. X row = αi,j,j (2) crow 0 ri,j 0 i,j j 0 ,j 0 6=j Then, we combine record’s representation with ci,j and obtain the row dimension record reprerow = tanh(W [r ; crow ]). W is sentation ri,j"
D19-1310,D15-1166,0,0.161286,"rcution In this paper, we construct timelines tl = {tle,c }E,C e=1,c=1 for records. E denotes the number of distinct record entities and C denotes the number of record types. For each timeline tle,c , we first extract records with the same entity e and type c from dataset. Then we sort them into a sequence according to the record’s date from old to new. This sequence is considered as timeline tle,c . For example, in Figure 2, the “Timeline” part in the lower-left corner represents a timeline for entity Al Jefferson and type PTS (points). 2.3 Baseline Model We use Seq2Seq model with attention (Luong et al., 2015) and conditional copy (Gulcehre et al., 2016) as the base model. During training, given tables S and their corresponding reference texts y, the model Q maximized the conditional probability P (y|S) = Tt=1 P (yt |y<t , S) . t is the timestep of decoder. First, for each record of the ith row and j th column in the table, we utilize 1-layer MLP to encode the embeddings of each record’s four types of information into a dense vector ri,j , ri,j = ReLU (Wa [ri,j .e; ri,j .c; ri,j .v; ri,j .f ] + ba ). Wa and ba are trainable parameters. The word embeddings for each type of information are trainable"
D19-1310,D18-1422,0,0.0374875,"history windows size as 3 from {3,5,7} based on the results. Code of our model can be found at https://github.com/ernestgong/data2textthree-dimensions/. 4.3 4.3.1 Results Automatic Evaluation Table 1 displays the automatic evaluation results on both development and test set. We chose Conditional Copy (CC) model as our baseline, which is the best model in Wiseman et al. (2017). We included reported scores with updated IE model by Puduppully et al. (2019) and our implementation’s result on CC in this paper. Also, we compared our models with other existing works on this dataset including OpATT (Nie et al., 2018) and Neural Content Planning with conditional copy (NCP+CC) (Puduppully et al., 2019). In addition, we implemented three other hierarchical encoders that encoded tables’ row dimension information in both record-level and row-level to compare with the hierarchical structure of encoder in our model. The decoder was equipped with dual attention (Cohan et al., 2018). The one with LSTM cell is similar to the one in Cohan et al. (2018) with 1 layer from {1,2,3}. The one with CNN cell 3148 (Gehring et al., 2017) has kernel width 3 from {3, 5} and 10 layer from {5,10,15,20}. The one with transformer-s"
D19-1310,P02-1040,0,0.103869,"r provides row-level representation. Inspired by Cohan et al. (2018), we can modify 4.1 Experiments Dataset and Evaluation Metrics We conducted experiments on ROTOWIRE (Wiseman et al., 2017). For each example, it provides three tables as described in Section 2.1 which consists of 628 records in total with a long game summary. The average length of game summary is 337.1. In this paper, we followed the data split introduced in Wiseman et al. (2017): 3398 examples in training set, 727 examples in development set and 728 examples in test set. We followed Wiseman et al. (2017)’s work and use BLEU (Papineni et al., 2002) and three extractive evaluation metrics RG, CS and CO (Wiseman et al., 2017) for evaluation. The main idea of the extractive evaluation metrics is to use an Information Extraction (IE) model to identify records mentioned in texts. Then compare them with tables or records extracted from reference to evaluate the model. RG (Relation Generation) measures content fidelity of 3147 Model RG Gold Template CC (Wiseman et al., 2017) NCP+CC (Puduppully et al., 2019) Hierarchical LSTM Encoder Hierarchical CNN Encoder Hierarchical SA Encoder Hierarchical MHSA Encoder CC (Our implementation) Our Model -ro"
D19-1310,N16-1086,0,0.034777,"ta (column) between each rows (players). Also it correctly summarize performance of “Al Jefferson” in this match as “double-double” which requires ability to capture dependency from different columns (different type of record) in the same row (player). In addition, it models “Al Jefferson” history performance and correctly states that “It was his second double-double over his last three games”, which is also mentioned in gold texts included in Figure 1 in a similar way. 5 Related Work In recent years, neural data-to-text systems make remarkable progress on generating texts directly from data. Mei et al. (2016) proposes an encoderaligner-decoder model to generate weather forecast, while Jain et al. (2018) propose a mixed hierarchical attention. Sha et al. (2018) proposes a hybrid content- and linkage-based attention mechanism to model the order of content. Liu et al. (2018) propose to integrate field information into table representation and enhance decoder with dual attention. Bao et al. (2018) develops a table-aware encoder-decoder model. Wiseman et al. (2017) introduced a document-scale data-totext dataset, consisting of long text with more redundant records, which requires the model to select im"
I11-1030,P03-2031,0,0.167886,"and English is also more complex because of the tremendous difference between the two languages. Huang and Vogel (2002) presented an integrated approach to extract an NE translation dictionary from an English-Chinese parallel corpus while improving the monolingual NE annotation quality for both languages. They started with low-quality NE tagging for both languages and improved the annotation result using alignment information. But they did not filter the annotated data and evaluate its impact for NER as training data. Besides, some other resources have been used to generate NE tagged corpus. An et al. (2003) and Whitelaw et al. (2008) used seed sets of entities and search engines to collect NER training data from the web. However, constructing of a high-quality seed list is also a time-consuming work. Richman and Schone (2008) and Nothman et al. (2008) used similar methods to create NE training data. They transformed Wikipedia’s links into named entity annotations by classifying the target articles into common entity types. But the article classification seeds also had to be hand-labeled in advance. In the biomedical domain, Vlachos and Gasperin (2006) automatically created training material for"
I11-1030,C02-1025,0,0.0550395,"Missing"
I11-1030,W03-0425,0,0.116048,"Missing"
I11-1030,W03-0428,0,0.0929965,"Missing"
I11-1030,zhang-etal-2004-interpreting,0,0.0186626,"Missing"
I11-1030,U08-1016,0,0.0560751,"oving the monolingual NE annotation quality for both languages. They started with low-quality NE tagging for both languages and improved the annotation result using alignment information. But they did not filter the annotated data and evaluate its impact for NER as training data. Besides, some other resources have been used to generate NE tagged corpus. An et al. (2003) and Whitelaw et al. (2008) used seed sets of entities and search engines to collect NER training data from the web. However, constructing of a high-quality seed list is also a time-consuming work. Richman and Schone (2008) and Nothman et al. (2008) used similar methods to create NE training data. They transformed Wikipedia’s links into named entity annotations by classifying the target articles into common entity types. But the article classification seeds also had to be hand-labeled in advance. In the biomedical domain, Vlachos and Gasperin (2006) automatically created training material for the task of gene name recognition from the broader raw corpus using existing domain resources. In our work, we generate a large scale Chinese NER training data from a bilingual corpus without any NE seed lists and filter it by using effective strate"
I11-1030,P02-1060,0,0.0864602,"Missing"
I11-1030,W04-1221,0,0.0185845,"Missing"
I11-1030,W02-2029,0,0.0155673,"Missing"
I11-1030,W06-3328,0,0.0143561,"resources have been used to generate NE tagged corpus. An et al. (2003) and Whitelaw et al. (2008) used seed sets of entities and search engines to collect NER training data from the web. However, constructing of a high-quality seed list is also a time-consuming work. Richman and Schone (2008) and Nothman et al. (2008) used similar methods to create NE training data. They transformed Wikipedia’s links into named entity annotations by classifying the target articles into common entity types. But the article classification seeds also had to be hand-labeled in advance. In the biomedical domain, Vlachos and Gasperin (2006) automatically created training material for the task of gene name recognition from the broader raw corpus using existing domain resources. In our work, we generate a large scale Chinese NER training data from a bilingual corpus without any NE seed lists and filter it by using effective strategies. And we prove that it can improve the performance of Chinese NER as additional training data. 3 Our Approach In this section we describe our approach of generating NER training data from a parallel corpus. The framework of our system consists of four components as shown in Figure 1.  Alignment: Sent"
I11-1030,I08-5007,0,0.0502059,"Missing"
I11-1030,H01-1035,0,0.0327142,"ty training data, which are very effective and important as the experiments show. And third, we prove that our generated training data can be used as an additional corpus to improve the NER performance. This paper is organized as follows. Section 2 discusses the related work. Section 3 describes our approach in detail. Section 4 presents and discusses the results of our experiments. Finally, we present our conclusions and future work in section 5. 2 Related Work In this section, we introduce some previous work about NER training data generation. The most closed related work to our approach is Yarowsky et al. (2001). They used word alignment on parallel corpora to induce several text analysis tools from English to other languages for which such resources are scarce. An NE tagger was transferred from English to French and achieved good classification accuracy. However, Chinese NER is more difficult than French and word alignment between Chinese and English is also more complex because of the tremendous difference between the two languages. Huang and Vogel (2002) presented an integrated approach to extract an NE translation dictionary from an English-Chinese parallel corpus while improving the monolingual"
I11-1030,J03-3002,0,\N,Missing
I11-1030,P08-1001,0,\N,Missing
I11-1030,C10-3004,1,\N,Missing
I11-1030,W10-2906,0,\N,Missing
I11-1030,N03-1017,0,\N,Missing
I11-1030,ma-2006-champollion,0,\N,Missing
I11-1030,W04-1200,0,\N,Missing
I11-1030,P00-1056,0,\N,Missing
I11-1030,C96-1079,0,\N,Missing
I13-1036,P01-1008,0,0.0570982,"Missing"
I13-1036,P09-1068,0,0.0832705,"Missing"
I13-1036,P11-1098,0,0.0468116,"Missing"
I13-1036,W09-1207,1,0.835918,"t, which is an important feature for recognizing the event type. Kiyoshi Sudo (2003) summarized three classical models for representing events. All of these three models rely on the syntactic tree structure and the trigger is specified as a predicate in this structure. In order to accurately extract event triggers, we employ the predicateargument model (Yangarder et al., 2000) which is based on a direct syntactic relation between a predicate and its arguments. We extract the syntactic relation for predicate-argument model by means of the HIT (Harbin Institute of Technology) Dependency Parser (Che et al., 2009). Based on the predicate-argument model, we propose a trigger extraction algorithm (TE). The details are shown in Figure 2. Take the following sentence as an example: 毛泽东 1893 年 出生 于 湖南湘潭。 1 2 3 4 5 → Mao Zedong was born in Xiangtan, Hunan 1 2 3 4 5 Province in 1893. 6 7 The HIT Chinese dependencies are: SBV (出生-3, 毛泽东-1) Dependency → (born-3, Mao Zedong-1) VOB (出生-3, 湖南湘潭-5) → (born-3, Xiangtan, Hunan Province-5) 312 Parser ADV (出生-3, 1893 年-2) → (born-3, 1893-7) POB (湖南湘潭-5, 于-4) → (Hunan Province-5, in-4) where each atomic formula represents a binary dependence from the governor (the first"
I13-1036,P12-1088,0,0.0206591,"ing Ji’s work, Liao et al., (2010) used document level cross-event inference to improve event extraction. Chen and Ji, (2009) combined word-based classifier with character-based classifier; and explored effective features for the Chinese event extraction task. Liao and Grishman, (2010) ranked two semi-supervised learning methods for adapting the event extraction system to new event types. Hong et al, (2011) proposed a blind cross-entity inference method for event extraction, which well uses the consistency of entity mention to achieve sentence-level trigger and argument (role) classification. Lu and Roth, (2012) presented a novel model based on the semi-Markov conditional random ﬁelds for the challenging event extraction task. The model takes in coarse mention boundary and type information and predicts complete structures indicating the corresponding argument role for each mention. However, for all the above approaches, it is necessary to specify the target event type in advance. Defining and identifying those types heavily rely on expert knowledge, and reaching an agreement among the experts or annotators requires a lot of human labor. Li et al., (2010) proposed a domain-independent novel event disc"
I13-1036,N04-1043,0,0.0488039,"rule (1) did. This is due to the fact that rule (2) is more effective on the domain-specific corpus. 4 4.1 Related Work Word Cluster Discovery Our approach of automatically building an event type paradigm is related to some prior work on word cluster discovery (e.g. Barzilay and McKeown, 2001; Ibrahim et al., 2003; Pang et al., 2003). Most of these works are based on machine translation techniques to solve paraphrase extraction problem. However, several recent researches have stressed the benefits of using word clusters to improve the performance of information extraction tasks. For example, Miller et al., (2004) proved that word clusters could significantly improve English name tagging performance. In the same vein, some studies work on the problem of relation extraction (Chambers and Jurasky, 2011 and 2009; Poon and Domingos, 2009 and 2008; Yates and Etzioni, 2009). In these work, “relation words” were extracted and clustered. In this paper, our work confirmed that trigger clusters are also effective for event type paradigm building. The problem of event trigger 317 words extraction and clustering is also a challenge problem. 4.2 Traditional Event Extraction The commonly used approaches for most eve"
I13-1036,P11-1113,0,0.014349,"exploited a correlation between senses of verbs (that are the triggers for events) and topics of documents. They first proposed refining event extraction through unsupervised cross-document inference. Following Ji’s work, Liao et al., (2010) used document level cross-event inference to improve event extraction. Chen and Ji, (2009) combined word-based classifier with character-based classifier; and explored effective features for the Chinese event extraction task. Liao and Grishman, (2010) ranked two semi-supervised learning methods for adapting the event extraction system to new event types. Hong et al, (2011) proposed a blind cross-entity inference method for event extraction, which well uses the consistency of entity mention to achieve sentence-level trigger and argument (role) classification. Lu and Roth, (2012) presented a novel model based on the semi-Markov conditional random ﬁelds for the challenging event extraction task. The model takes in coarse mention boundary and type information and predicts complete structures indicating the corresponding argument role for each mention. However, for all the above approaches, it is necessary to specify the target event type in advance. Defining and id"
I13-1036,W03-1608,0,0.0350848,"Missing"
I13-1036,P08-1030,0,0.085081,"ction The commonly used approaches for most event extraction systems are the knowledge engineering approach and the machine learning approach. Grishman et al., (2005) used a combination of pattern matching and statistical modeling techniques. They extract two kinds of patterns: 1) the sequence of constituent heads separating anchor and its arguments; and 2) a predicate argument sub-graph of the sentence connecting anchor to all the event arguments. In conjunction, they used a set of Maximum Entropy based classiﬁers for 1) Trigger labeling, 2) Argument classiﬁcation and 3) Event classiﬁcation. Ji and Grishman, (2008) further exploited a correlation between senses of verbs (that are the triggers for events) and topics of documents. They first proposed refining event extraction through unsupervised cross-document inference. Following Ji’s work, Liao et al., (2010) used document level cross-event inference to improve event extraction. Chen and Ji, (2009) combined word-based classifier with character-based classifier; and explored effective features for the Chinese event extraction task. Liao and Grishman, (2010) ranked two semi-supervised learning methods for adapting the event extraction system to new event"
I13-1036,C10-1077,0,0.0264754,"opy based classiﬁers for 1) Trigger labeling, 2) Argument classiﬁcation and 3) Event classiﬁcation. Ji and Grishman, (2008) further exploited a correlation between senses of verbs (that are the triggers for events) and topics of documents. They first proposed refining event extraction through unsupervised cross-document inference. Following Ji’s work, Liao et al., (2010) used document level cross-event inference to improve event extraction. Chen and Ji, (2009) combined word-based classifier with character-based classifier; and explored effective features for the Chinese event extraction task. Liao and Grishman, (2010) ranked two semi-supervised learning methods for adapting the event extraction system to new event types. Hong et al, (2011) proposed a blind cross-entity inference method for event extraction, which well uses the consistency of entity mention to achieve sentence-level trigger and argument (role) classification. Lu and Roth, (2012) presented a novel model based on the semi-Markov conditional random ﬁelds for the challenging event extraction task. The model takes in coarse mention boundary and type information and predicts complete structures indicating the corresponding argument role for each"
I13-1036,N03-1024,0,0.0297658,"Missing"
I13-1036,D08-1068,0,0.0922965,"Missing"
I13-1036,D09-1001,0,0.126104,"to be very data dependent. As a result, it  may prevent the event extraction from being widely applicable. Since event types among domains are different, the event type paradigm of ACE, which does not define music related events, is useless for the music domain event extraction. So we have to build a totally different event type paradigm for the music domain from scratch. Recently, some researchers have been aware of the limitations of only considering pre-defined paradigm as well. In the same vein, some studies work on the problem of relation extraction (Chambers and Jurasky, 2011 and 2009; Poon and Domingos, 2009 and 2008; Yates and Etzioni, 2009). Rosenfeld and Feldman (2006) built a high-performance unsupervised relation extraction system without target relations in advance. Hasegawa et al. (2004) discovered relations among named entities from large corpora by clustering pairs of named entities. However, most of the above work focuses on relation extraction rather than event extraction. In contrast to the well-studied problem of relation extraction, only a few works focused on event extraction. For example, Li (2010) proposed a domain-independent novel event discovery approach. They exploited a cros"
I13-1036,P06-2086,0,0.0215375,"event extraction from being widely applicable. Since event types among domains are different, the event type paradigm of ACE, which does not define music related events, is useless for the music domain event extraction. So we have to build a totally different event type paradigm for the music domain from scratch. Recently, some researchers have been aware of the limitations of only considering pre-defined paradigm as well. In the same vein, some studies work on the problem of relation extraction (Chambers and Jurasky, 2011 and 2009; Poon and Domingos, 2009 and 2008; Yates and Etzioni, 2009). Rosenfeld and Feldman (2006) built a high-performance unsupervised relation extraction system without target relations in advance. Hasegawa et al. (2004) discovered relations among named entities from large corpora by clustering pairs of named entities. However, most of the above work focuses on relation extraction rather than event extraction. In contrast to the well-studied problem of relation extraction, only a few works focused on event extraction. For example, Li (2010) proposed a domain-independent novel event discovery approach. They exploited a cross-lingual clustering algorithm based on sentence-aligned bilingua"
I13-1036,H01-1009,0,0.0443004,"Missing"
I13-1036,P03-1029,0,0.0873923,"Missing"
I13-1036,W02-1028,0,0.106828,"Missing"
I13-1036,P04-1053,0,\N,Missing
I13-1036,J93-3001,0,\N,Missing
I13-1036,E03-3004,0,\N,Missing
I13-1036,Y10-1027,0,\N,Missing
I13-1055,W03-1028,0,0.0486897,"patterns are carefully chosen according to morphological structure or special format of corpus (Nakayama et al., 2008), either manually or via automatic bootstrapping (Hearst, 1992). However, the methods suffer from sparse coverage of patterns in a given corpus. Some researchers try to map the words to a thesaurus or an existed ontology (WordNet or Wikipedia) automatically so as to get key concepts (Angeletou et al., 2008). The coverage and openness of existed ontologies seriously limit the scope of these works. Simple statistical methods http://movie.douban.com 481 such as TF-IDF weighting (Hulth, 2003) are not feasible for folksonomy since short text snippets only. Graph-based ranking methods are the state of the art. They are superior to the statistic-based methods because of considering structure information between words. Mihalcea and Tara (2004) propose to use TextRank, a modified PageRank algorithm to extract key concepts from text. But TextRank only maintain a single importance score for each word. Hotho et al. (2006) propose a graph-based ranking algorithm for folksonomy, named FolkRank. They convert triadic hypergraph in folksonomy into an undirected tripartite graph. But we conside"
I13-1055,P11-1039,0,0.0734293,"xtracted by LDA are more conformed to the actual situation than the topics of ODP. They pay more attention to the effect of the transfer action probability on the importance score of tags which benefit us to know the propagation process. It seems that mixing together the random 2 walks of all the topics in one graph may cause noise compared to independent topic graphs. Liu et al. (2010) decompose a traditional random walk into multiple random walks specific to various topics, named Topical PageRank (TPR). The novel contribution is the study on topicspecific preference value setting. And then, Zhao et al. (2011) argue that context-free propagation may cause the importance scores to be off-topic. They model the score propagation with topic context when setting the link weights and then denote this context-sensitive topical PageRank as cTPR. Enlightened by TPR and cTPR, we further propose a new link weight function to express the semantic similarity between two tags of folksonomy. The novel link weight function combines the local similarity (defined as cooccurrence of tags in a same resource assigned to the given topic) with the global similarity (defined as cosine similarity of two tags over all the t"
I13-1055,D10-1036,0,0.110987,"ectory Project)2 extracted manually. Based on their works, Jin et al. (2011) implement a topicsensitive tag ranking (TSTR) approach in folksonomy automatically through LDA. TSTR performs better than TSPR and TLA because topics extracted by LDA are more conformed to the actual situation than the topics of ODP. They pay more attention to the effect of the transfer action probability on the importance score of tags which benefit us to know the propagation process. It seems that mixing together the random 2 walks of all the topics in one graph may cause noise compared to independent topic graphs. Liu et al. (2010) decompose a traditional random walk into multiple random walks specific to various topics, named Topical PageRank (TPR). The novel contribution is the study on topicspecific preference value setting. And then, Zhao et al. (2011) argue that context-free propagation may cause the importance scores to be off-topic. They model the score propagation with topic context when setting the link weights and then denote this context-sensitive topical PageRank as cTPR. Enlightened by TPR and cTPR, we further propose a new link weight function to express the semantic similarity between two tags of folksono"
I13-1055,W04-3252,0,\N,Missing
I13-1055,C92-2082,0,\N,Missing
N10-1059,H05-1043,0,0.0267018,"Missing"
N10-1059,H05-2017,0,\N,Missing
N15-1012,W11-2832,0,0.451539,"follow previous work and conduct experiments on the Penn Treebank (PTB), using Wall Street Jour118 nal sections 2–21 for training, 22 for development testing and 23 for final testing. Gold-standard dependency trees are derived from bracketed sentences in the treebank using Penn2Malt1 , and base noun phrases are treated as a single word (Wan et al., 2009; Zhang, 2013). The BLEU score (Papineni et al., 2002) is used to evaluate the performance of linearization, which has been adopted in former literals (Wan et al., 2009; White and Rajkumar, 2009; Zhang and Clark, 2011b) and recent shared-tasks (Belz et al., 2011). We use our implementation of the best-first system of Zhang (2013), which gives the state-of-the-art results, as the baseline. 4.1 Influence of Beam size We first study the influence of beam size by performing free word ordering on the development test data. BLEU score curves with different beam sizes are shown in Figure 7. From this figure, we can see that the systems with beam 64 and 128 achieve the best results. However, the 128-beam system does not improve the performance significantly (48.2 vs 47.5), but runs twice slower. As a result, we set the beam size to 64 in the remaining experim"
N15-1012,C10-1012,0,0.414105,"([σ|j i], β, A) ([σ|j], β, A ∪ {j → i}) Figure 1: The arc-standard parsing algorithm. Introduction Linearization is the task of ordering a bag of words into a grammatical and fluent sentence. Syntaxbased linearization algorithms generate a sentence along with its syntactic structure. Depending on how much syntactic information is available as inputs, recent work on syntactic linearization can be classified into free word ordering (Wan et al., 2009; Zhang et al., 2012; de Gispert et al., 2014), which orders a bag of words without syntactic constraints, full tree linearization (He et al., 2009; Bohnet et al., 2010; Song et al., 2014), which orders a bag of words given a full-spanning syntactic tree, and partial tree linearization (Zhang, 2013), which orders a bag of words given some syntactic relations between them as partial constraints. The search space for syntactic linearization is huge. Even with a full syntax tree being available as constraints, permutation of nodes on each level is an NP-hard problem. As a result, heuristic search has been adopted by most previous work, and the best results have been achieved by a time-constrained best-first search framework (White, 2004a; White and Rajkumar, 20"
N15-1012,P04-1015,0,0.171585,"and their modifiers. The original feature templates of Zhang and Nivre (2011) also contain information of the front words on the buffer. However, since the buffer is unordered for linearization, we do not include these features. The linearization feature templates are specific for linearization, and captures surface ngram information. Each search state represents a partially linearized sentence. We represents the last word in the partially linearized sentence as w0 and the second last as w−1 . Given a set of labeled training examples, the averaged perceptron (Collins, 2002) with early update (Collins and Roark, 2004; Zhang and Nivre, 2011) is used to train the parameters θ of the model. 3.2 Input Syntactic Constraints The use of syntactic constraints to achieve better linearization performance has been studied in previous work. Wan et al. (2009) employ POS constraints 116 NP . VBD . Dr. Talcott . .2 1 led NP . . NP . .IN a team . 3 of.4 Harvard University . 5 Figure 4: Example partial tree. Words in the same sub dependency trees are grouped by rounded boxes. Word indices do not specify their orders. Base phrases (e.g. Dr. Talcott) are treated as single words. in learning a dependency language model. Zha"
N15-1012,W02-1001,0,0.0628857,"e context information for S0 , S1 and their modifiers. The original feature templates of Zhang and Nivre (2011) also contain information of the front words on the buffer. However, since the buffer is unordered for linearization, we do not include these features. The linearization feature templates are specific for linearization, and captures surface ngram information. Each search state represents a partially linearized sentence. We represents the last word in the partially linearized sentence as w0 and the second last as w−1 . Given a set of labeled training examples, the averaged perceptron (Collins, 2002) with early update (Collins and Roark, 2004; Zhang and Nivre, 2011) is used to train the parameters θ of the model. 3.2 Input Syntactic Constraints The use of syntactic constraints to achieve better linearization performance has been studied in previous work. Wan et al. (2009) employ POS constraints 116 NP . VBD . Dr. Talcott . .2 1 led NP . . NP . .IN a team . 3 of.4 Harvard University . 5 Figure 4: Example partial tree. Words in the same sub dependency trees are grouped by rounded boxes. Word indices do not specify their orders. Base phrases (e.g. Dr. Talcott) are treated as single words. i"
N15-1012,E14-1028,0,0.228722,"Missing"
N15-1012,P09-1091,0,0.647686,"β, A ∪ {j ← i}) ([σ|j i], β, A) ([σ|j], β, A ∪ {j → i}) Figure 1: The arc-standard parsing algorithm. Introduction Linearization is the task of ordering a bag of words into a grammatical and fluent sentence. Syntaxbased linearization algorithms generate a sentence along with its syntactic structure. Depending on how much syntactic information is available as inputs, recent work on syntactic linearization can be classified into free word ordering (Wan et al., 2009; Zhang et al., 2012; de Gispert et al., 2014), which orders a bag of words without syntactic constraints, full tree linearization (He et al., 2009; Bohnet et al., 2010; Song et al., 2014), which orders a bag of words given a full-spanning syntactic tree, and partial tree linearization (Zhang, 2013), which orders a bag of words given some syntactic relations between them as partial constraints. The search space for syntactic linearization is huge. Even with a full syntax tree being available as constraints, permutation of nodes on each level is an NP-hard problem. As a result, heuristic search has been adopted by most previous work, and the best results have been achieved by a time-constrained best-first search framework (White, 2004a; W"
N15-1012,P10-1001,0,0.0355131,"connection between syntactic linearization and syntactic parsing: both build a syntactic tree over a sentence, with the former performing word ordering in addition to derivation construction. As a result, syntactic linearization can be treated as a generalized form of parsing, for which there is no input word order, and therefore extensions to parsing algorithms can be used to perform linearization. For syntactic parsing, the algorithm of Zhang and Nivre (2011) gives competitive accuracies under linear complexity. Compared with parsers that use dynamic programming (McDonald and Pereira, 2006; Koo and Collins, 2010), the efficient beam-search system is more suitable for the NP-hard linearization task. We extend the parser of Zhang and Nivre (2011), so that word ordering is performed in addition to syntactic tree construction. Experimental results show that the transition-based linearization system runs an order of magnitude faster than a state-ofthe-art best-first baseline, with improved accuracies in standard evaluation. Our linearizer is publicly available under GPL at http://sourceforge. net/projects/zgen/. 2 Transition-Based Parsing The task of dependency parsing is to find a dependency tree given an"
N15-1012,E06-1011,0,0.044596,"r method is inspired by the connection between syntactic linearization and syntactic parsing: both build a syntactic tree over a sentence, with the former performing word ordering in addition to derivation construction. As a result, syntactic linearization can be treated as a generalized form of parsing, for which there is no input word order, and therefore extensions to parsing algorithms can be used to perform linearization. For syntactic parsing, the algorithm of Zhang and Nivre (2011) gives competitive accuracies under linear complexity. Compared with parsers that use dynamic programming (McDonald and Pereira, 2006; Koo and Collins, 2010), the efficient beam-search system is more suitable for the NP-hard linearization task. We extend the parser of Zhang and Nivre (2011), so that word ordering is performed in addition to syntactic tree construction. Experimental results show that the transition-based linearization system runs an order of magnitude faster than a state-ofthe-art best-first baseline, with improved accuracies in standard evaluation. Our linearizer is publicly available under GPL at http://sourceforge. net/projects/zgen/. 2 Transition-Based Parsing The task of dependency parsing is to find a"
N15-1012,J08-4003,0,0.0934826,"onstruction. Experimental results show that the transition-based linearization system runs an order of magnitude faster than a state-ofthe-art best-first baseline, with improved accuracies in standard evaluation. Our linearizer is publicly available under GPL at http://sourceforge. net/projects/zgen/. 2 Transition-Based Parsing The task of dependency parsing is to find a dependency tree given an input sentence. Figure 2 shows an example dependency tree, which consists of dependency arcs that represent syntactic relations between pairs of words. A transition-based dependency parsing algorithm (Nivre, 2008) can be formalized as a transition system, S = (C, T, cs , Ct ), where C is the set of states, T is a set of transition actions, cs is the initial state and Ct is a set of terminal states. The parsing process is modeled as an application of a sequence of actions, transducing the initial state into a final state, while constructing de114 0 1 2 3 4 5 6 7 8 9 10 11 Transition S HIFT S HIFT S HIFT S HIFT S HIFT R IGHTA RC R IGHTA RC R IGHTA RC S HIFT R IGHTA RC L EFTA RC σ [] [1] [1 2] [1 2 3] [1 2 3 4] [1 2 3 4 5] [1 2 3 4] [1 2 3] [1 2] [1 2 6] [1 2] [2] β [1...6] [2...6] [3...6] [4...6] [5,6] ["
N15-1012,P02-1040,0,0.0922216,"build arcs between top two words i and j on the stack (line 10-13). If no arc exists between i and j, the next action should shift the parent word of i or a word in i’s sibling tree (line 14-16). 4 Experiments We follow previous work and conduct experiments on the Penn Treebank (PTB), using Wall Street Jour118 nal sections 2–21 for training, 22 for development testing and 23 for final testing. Gold-standard dependency trees are derived from bracketed sentences in the treebank using Penn2Malt1 , and base noun phrases are treated as a single word (Wan et al., 2009; Zhang, 2013). The BLEU score (Papineni et al., 2002) is used to evaluate the performance of linearization, which has been adopted in former literals (Wan et al., 2009; White and Rajkumar, 2009; Zhang and Clark, 2011b) and recent shared-tasks (Belz et al., 2011). We use our implementation of the best-first system of Zhang (2013), which gives the state-of-the-art results, as the baseline. 4.1 Influence of Beam size We first study the influence of beam size by performing free word ordering on the development test data. BLEU score curves with different beam sizes are shown in Figure 7. From this figure, we can see that the systems with beam 64 and"
N15-1012,P05-1025,0,0.015265,"nlabeled and labeled dependency trees (He et al., 2009; Zhang, 2013). These methods mostly use greedy or best-first algorithms to order each tree node. Our work is different by performing word ordering using a transition process. Besides dependency grammar, linearization with other syntactic grammars, such as CFG and CCG (White and Rajkumar, 2009; Zhang and Clark, 2011b), has also been studied. In this paper, we adopt the dependency grammar for transition-based linearization. However, since transition-based parsing algorithms has been successfully applied to different grammars, including CFG (Sagae et al., 2005) and CCG (Xu et al., 2014), our linearization method can be applied to these grammars. 6 Conclusion We studied transition-based syntactic linearization as an extension to transition-based parsing. Compared with best-first systems, the advantage of our transition-based algorithm includes bounded time complexity, and the guarantee to yield full sentences when given a bag of words. Experimental results show that our algorithm achieves improved accuracies, with significantly faster decoding speed compared with a state-of-the-art best-first baseline. We publicly release our code at http: //sourcefo"
N15-1012,E09-1097,0,0.510304,"State ([ ], [1...n], ∅) Final State ([ ], [ ], A) Induction Rules: S HIFT L EFTA RC R IGHTA RC (σ, [i|β], A) ([σ |i], β, A) ([σ|j i], β, A) ([σ|i], β, A ∪ {j ← i}) ([σ|j i], β, A) ([σ|j], β, A ∪ {j → i}) Figure 1: The arc-standard parsing algorithm. Introduction Linearization is the task of ordering a bag of words into a grammatical and fluent sentence. Syntaxbased linearization algorithms generate a sentence along with its syntactic structure. Depending on how much syntactic information is available as inputs, recent work on syntactic linearization can be classified into free word ordering (Wan et al., 2009; Zhang et al., 2012; de Gispert et al., 2014), which orders a bag of words without syntactic constraints, full tree linearization (He et al., 2009; Bohnet et al., 2010; Song et al., 2014), which orders a bag of words given a full-spanning syntactic tree, and partial tree linearization (Zhang, 2013), which orders a bag of words given some syntactic relations between them as partial constraints. The search space for syntactic linearization is huge. Even with a full syntax tree being available as constraints, permutation of nodes on each level is an NP-hard problem. As a result, heuristic search"
N15-1012,D09-1043,0,0.751283,"9; Bohnet et al., 2010; Song et al., 2014), which orders a bag of words given a full-spanning syntactic tree, and partial tree linearization (Zhang, 2013), which orders a bag of words given some syntactic relations between them as partial constraints. The search space for syntactic linearization is huge. Even with a full syntax tree being available as constraints, permutation of nodes on each level is an NP-hard problem. As a result, heuristic search has been adopted by most previous work, and the best results have been achieved by a time-constrained best-first search framework (White, 2004a; White and Rajkumar, 2009; Zhang and Clark, 2011b; Song et al., 2014). Though empirically highly accurate, one drawback of this approach is that there is no asymptotic upper bound on the time complexity of finding the first full sentence. As a result, it can take 5–10 seconds to process a sentence, and sometimes fail to yield a full sentence at timeout. This issue is more severe for larger bags of words, and makes the algorithms practically less useful. We study the effect of an alternative learning and search framework for the linearization prob113 Human Language Technologies: The 2015 Annual Conference of the North"
N15-1012,P14-1021,1,0.82756,"cy trees (He et al., 2009; Zhang, 2013). These methods mostly use greedy or best-first algorithms to order each tree node. Our work is different by performing word ordering using a transition process. Besides dependency grammar, linearization with other syntactic grammars, such as CFG and CCG (White and Rajkumar, 2009; Zhang and Clark, 2011b), has also been studied. In this paper, we adopt the dependency grammar for transition-based linearization. However, since transition-based parsing algorithms has been successfully applied to different grammars, including CFG (Sagae et al., 2005) and CCG (Xu et al., 2014), our linearization method can be applied to these grammars. 6 Conclusion We studied transition-based syntactic linearization as an extension to transition-based parsing. Compared with best-first systems, the advantage of our transition-based algorithm includes bounded time complexity, and the guarantee to yield full sentences when given a bag of words. Experimental results show that our algorithm achieves improved accuracies, with significantly faster decoding speed compared with a state-of-the-art best-first baseline. We publicly release our code at http: //sourceforge.net/projects/zgen/. Fo"
N15-1012,J11-1005,1,0.277872,"ng et al., 2014), which orders a bag of words given a full-spanning syntactic tree, and partial tree linearization (Zhang, 2013), which orders a bag of words given some syntactic relations between them as partial constraints. The search space for syntactic linearization is huge. Even with a full syntax tree being available as constraints, permutation of nodes on each level is an NP-hard problem. As a result, heuristic search has been adopted by most previous work, and the best results have been achieved by a time-constrained best-first search framework (White, 2004a; White and Rajkumar, 2009; Zhang and Clark, 2011b; Song et al., 2014). Though empirically highly accurate, one drawback of this approach is that there is no asymptotic upper bound on the time complexity of finding the first full sentence. As a result, it can take 5–10 seconds to process a sentence, and sometimes fail to yield a full sentence at timeout. This issue is more severe for larger bags of words, and makes the algorithms practically less useful. We study the effect of an alternative learning and search framework for the linearization prob113 Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the"
N15-1012,D11-1106,1,0.31725,"ng et al., 2014), which orders a bag of words given a full-spanning syntactic tree, and partial tree linearization (Zhang, 2013), which orders a bag of words given some syntactic relations between them as partial constraints. The search space for syntactic linearization is huge. Even with a full syntax tree being available as constraints, permutation of nodes on each level is an NP-hard problem. As a result, heuristic search has been adopted by most previous work, and the best results have been achieved by a time-constrained best-first search framework (White, 2004a; White and Rajkumar, 2009; Zhang and Clark, 2011b; Song et al., 2014). Though empirically highly accurate, one drawback of this approach is that there is no asymptotic upper bound on the time complexity of finding the first full sentence. As a result, it can take 5–10 seconds to process a sentence, and sometimes fail to yield a full sentence at timeout. This issue is more severe for larger bags of words, and makes the algorithms practically less useful. We study the effect of an alternative learning and search framework for the linearization prob113 Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the"
N15-1012,P11-2033,1,0.955294,"cy tree. lem, which has a theoretical upper bound on the time complexity, and always yields a full sentence in quadratic time. Our method is inspired by the connection between syntactic linearization and syntactic parsing: both build a syntactic tree over a sentence, with the former performing word ordering in addition to derivation construction. As a result, syntactic linearization can be treated as a generalized form of parsing, for which there is no input word order, and therefore extensions to parsing algorithms can be used to perform linearization. For syntactic parsing, the algorithm of Zhang and Nivre (2011) gives competitive accuracies under linear complexity. Compared with parsers that use dynamic programming (McDonald and Pereira, 2006; Koo and Collins, 2010), the efficient beam-search system is more suitable for the NP-hard linearization task. We extend the parser of Zhang and Nivre (2011), so that word ordering is performed in addition to syntactic tree construction. Experimental results show that the transition-based linearization system runs an order of magnitude faster than a state-ofthe-art best-first baseline, with improved accuracies in standard evaluation. Our linearizer is publicly a"
N15-1012,E12-1075,1,0.781136,"n], ∅) Final State ([ ], [ ], A) Induction Rules: S HIFT L EFTA RC R IGHTA RC (σ, [i|β], A) ([σ |i], β, A) ([σ|j i], β, A) ([σ|i], β, A ∪ {j ← i}) ([σ|j i], β, A) ([σ|j], β, A ∪ {j → i}) Figure 1: The arc-standard parsing algorithm. Introduction Linearization is the task of ordering a bag of words into a grammatical and fluent sentence. Syntaxbased linearization algorithms generate a sentence along with its syntactic structure. Depending on how much syntactic information is available as inputs, recent work on syntactic linearization can be classified into free word ordering (Wan et al., 2009; Zhang et al., 2012; de Gispert et al., 2014), which orders a bag of words without syntactic constraints, full tree linearization (He et al., 2009; Bohnet et al., 2010; Song et al., 2014), which orders a bag of words given a full-spanning syntactic tree, and partial tree linearization (Zhang, 2013), which orders a bag of words given some syntactic relations between them as partial constraints. The search space for syntactic linearization is huge. Even with a full syntax tree being available as constraints, permutation of nodes on each level is an NP-hard problem. As a result, heuristic search has been adopted by"
N15-1115,J08-1001,0,0.310993,"parts of the text; therefore, it misses the contribution from semantically related but not necessarily coreferential entities (e.g., Gates and Microsoft). In this paper, we capture such semantic relatedness by leveraging world knowledge (e.g., Gates is the person who created Microsoft), and use two existing evaluation frameworks. First, in the unsupervised framework, we introduce semantic relatedness as an enrichment to the original graph-based model of Guinaudeau and Strube (2013). In addition, we incorporate semantic relatedness as additional features into the popular entity-based model of Barzilay and Lapata (2008). Across both frameworks, our enriched model with semantic relatedness outperforms the original methods, especially on short documents. 1 Introduction In a well-written document, sentences are organized and presented in a logical and coherent form, which makes the text fluent and easily understood. Therefore, coherence is a fundamental aspect of high text quality, and the evaluation of coherence is a crucial component of many NLP applications, such as essay scoring (Miltsakaki and Kukich, 2004), story generation (McIntyre and Lapata, 2010), and document summarization (Barzilay et al., 2002). ∗"
N15-1115,P08-2011,0,0.0781101,"Missing"
N15-1115,P11-2022,0,0.66887,"kaki and Kukich, 2004), story generation (McIntyre and Lapata, 2010), and document summarization (Barzilay et al., 2002). ∗ This work was partly done while the first author was visiting University of Toronto. A particularly popular model for evaluating text coherence is the entity-based local coherence model of Barzilay and Lapata (2008) (B&L), which extracts mentions of entities in adjacent sentences, and captures local coherence in terms of the transitions in the grammatical role of each mention. Following this direction, a number of extensions have been proposed (Elsner and Charniak, 2008; Elsner and Charniak, 2011; Lin et al., 2011; Feng et al., 2014), the majority of which focus on enriching the original entity features. An exception is the unsupervised model of Guinaudeau and Strube (2013) (G&S), which converts the document into a graph of sentences, and evaluates the text coherence by computing the average out-degree over the entire graph. However, despite the apparent success of these methods, they rely merely on matching mentions of the same entity, but neglect the contribution from semantically related but not necessarily coreferential entities. For example, the text in Figure 1a1 has no common e"
N15-1115,D11-1142,0,0.0124592,"trieve all world knowledge related to d. There are two major issues for this process: (1) knowledge sources: where can we obtain this knowledge?, and (2) knowledge selection: how do we pinpoint the most relevant ones? Knowledge sources There are two main kinds of knowledge sources: (1) manually edited knowledge 1089 bases, such as YAGO (Hoffart et al., 2013), which consists of about 4 million human-edited instances from on-line encyclopedias such as WikiPedia (Denoyer and Gallinari, 2007) and FreeBase (Bollacker et al., 2008), and (2) automatically constructed knowledge bases, such as Reverb (Fader et al., 2011), which covers about 20 million instances extracted from raw texts. Generally speaking, manually edited knowledge bases have better accuracy but lower coverage, while automatically extracted knowledge bases are the opposite. To seek a good balance, we use both YAGO and Reverb as our knowledge sources. In addition, the automatically constructed knowledge bases can be extracted from raw texts of any domain, which makes our method adaptable. Both sources are presented in triples, argument1 -predicate-argument2 , (e.g., Gates-create-Microsoft), where the two arguments are usually entities and the"
N15-1115,C14-1089,1,0.939925,"ntyre and Lapata, 2010), and document summarization (Barzilay et al., 2002). ∗ This work was partly done while the first author was visiting University of Toronto. A particularly popular model for evaluating text coherence is the entity-based local coherence model of Barzilay and Lapata (2008) (B&L), which extracts mentions of entities in adjacent sentences, and captures local coherence in terms of the transitions in the grammatical role of each mention. Following this direction, a number of extensions have been proposed (Elsner and Charniak, 2008; Elsner and Charniak, 2011; Lin et al., 2011; Feng et al., 2014), the majority of which focus on enriching the original entity features. An exception is the unsupervised model of Guinaudeau and Strube (2013) (G&S), which converts the document into a graph of sentences, and evaluates the text coherence by computing the average out-degree over the entire graph. However, despite the apparent success of these methods, they rely merely on matching mentions of the same entity, but neglect the contribution from semantically related but not necessarily coreferential entities. For example, the text in Figure 1a1 has no common entity in s2 and s3 . However, the tran"
N15-1115,W07-2321,0,0.0327156,"(d)), where pt (d) is the normalized frequency of the transition t in the entity grid, and m is the number of predefined transitions. pt (d) is computed as the number of occurrences of transition t among all entities in the entity grid, divided by the total number of transitions of the same length. Using this feature encoding, the model is then trained as a preference ranking problem between documents of different degrees of coherence. 2.2 Graph-based local coherence modeling As mentioned previously, most extensions to the entity-based local coherence model focus on enriching the feature set (Filippova and Strube, 2007; Elsner and Charniak, 2011; Lin et al., 2011; Feng et al., 2014), all of which follow a supervised learning framework. To the best of our knowledge, the only exception is the unsupervised method proposed by s1 s2 s3 G&S, which transforms the entity grid into a sentence graph and measures text coherence by computing the average out-degree of the graph. For a document d, its entity grid is constructed first, following the method described in Section 2.1. Then, a bipartite graph G = (V s , Ve , L, W) is constructed, where V s is the set of nodes representing sentences in the text; Ve is the set"
N15-1115,P13-1010,0,0.839476,"gh}@cs.toronto.edu Abstract Previous work on text coherence was primarily based on matching multiple mentions of the same entity in different parts of the text; therefore, it misses the contribution from semantically related but not necessarily coreferential entities (e.g., Gates and Microsoft). In this paper, we capture such semantic relatedness by leveraging world knowledge (e.g., Gates is the person who created Microsoft), and use two existing evaluation frameworks. First, in the unsupervised framework, we introduce semantic relatedness as an enrichment to the original graph-based model of Guinaudeau and Strube (2013). In addition, we incorporate semantic relatedness as additional features into the popular entity-based model of Barzilay and Lapata (2008). Across both frameworks, our enriched model with semantic relatedness outperforms the original methods, especially on short documents. 1 Introduction In a well-written document, sentences are organized and presented in a logical and coherent form, which makes the text fluent and easily understood. Therefore, coherence is a fundamental aspect of high text quality, and the evaluation of coherence is a crucial component of many NLP applications, such as essay"
N15-1115,P11-1100,0,0.511546,"ry generation (McIntyre and Lapata, 2010), and document summarization (Barzilay et al., 2002). ∗ This work was partly done while the first author was visiting University of Toronto. A particularly popular model for evaluating text coherence is the entity-based local coherence model of Barzilay and Lapata (2008) (B&L), which extracts mentions of entities in adjacent sentences, and captures local coherence in terms of the transitions in the grammatical role of each mention. Following this direction, a number of extensions have been proposed (Elsner and Charniak, 2008; Elsner and Charniak, 2011; Lin et al., 2011; Feng et al., 2014), the majority of which focus on enriching the original entity features. An exception is the unsupervised model of Guinaudeau and Strube (2013) (G&S), which converts the document into a graph of sentences, and evaluates the text coherence by computing the average out-degree over the entire graph. However, despite the apparent success of these methods, they rely merely on matching mentions of the same entity, but neglect the contribution from semantically related but not necessarily coreferential entities. For example, the text in Figure 1a1 has no common entity in s2 and s3"
N15-1115,de-marneffe-etal-2006-generating,0,0.00708196,"train a ranker to prefer the(b)more coherent Sentence graph text over the less coherent one. Performance is therefore measured as the fraction of correct pairwise rankings as recognized by the ranker. We use SVMlight (Joachims, 2002) with the ranking configuration to train and evaluate our models, with all parameters set to default values. On both tasks, across two frameworks, supervised and unsupervised, we directly compare our modified model against the original one, i.e., B&L in the supervised framework and G&S in the unsupervised framework. In our experiments, we use the Stanford parser (Marneffe et al., 2006) to automatically extract the grammatical role for each entity mention. 5.1 Sentence ordering The task of sentence ordering attempts to simulate the situation where, given a predefined set of information-bearing items, we need to determine the best order to arrange those items. In this paper, we follow G&S and introduce CoNLL 20122 (Pradhan et al., 2012) as our dataset, which is composed of documents from multiple news sources. For each text, we randomly shuffle its sentences to generate 20 permutations with incorrect sentence order. For a fair comparison, we also evaluate our model on a filte"
N15-1115,P10-1158,0,0.026107,"Missing"
N15-1115,W12-4501,0,0.0383312,"across two frameworks, supervised and unsupervised, we directly compare our modified model against the original one, i.e., B&L in the supervised framework and G&S in the unsupervised framework. In our experiments, we use the Stanford parser (Marneffe et al., 2006) to automatically extract the grammatical role for each entity mention. 5.1 Sentence ordering The task of sentence ordering attempts to simulate the situation where, given a predefined set of information-bearing items, we need to determine the best order to arrange those items. In this paper, we follow G&S and introduce CoNLL 20122 (Pradhan et al., 2012) as our dataset, which is composed of documents from multiple news sources. For each text, we randomly shuffle its sentences to generate 20 permutations with incorrect sentence order. For a fair comparison, we also evaluate our model on a filtered subset of documents with an average length of 31.8 sentences. Therefore, our dataset contains 72 documents and 72 × 20 = 1440 permutations, among which the shortest one contains 25 sentences. For our enhanced graph-based model (introduced in Section 4.1), which is purely unsupervised, we evaluate our model over the entire dataset. For our enhanced en"
N15-1115,C14-1087,1,0.829171,"s extracted from raw texts. Generally speaking, manually edited knowledge bases have better accuracy but lower coverage, while automatically extracted knowledge bases are the opposite. To seek a good balance, we use both YAGO and Reverb as our knowledge sources. In addition, the automatically constructed knowledge bases can be extracted from raw texts of any domain, which makes our method adaptable. Both sources are presented in triples, argument1 -predicate-argument2 , (e.g., Gates-create-Microsoft), where the two arguments are usually entities and the predicate is the relation between them (Zhang et al., 2014). Knowledge selection For each document d, we then select the subset of relevant knowledge instances, in the sense that they represent relations between the entities in d. In particular, we extract all entities in d, and query the knowledge bases to obtain all the knowledge instances in which both of the two arguments, argument1 and argument2 , match some of the entities in d. One issue in knowledge selection is whether to retrieve knowledge instances using exact or partial matching. For a given pair of entities in the text, the chance is rather low to find instances in the knowledge bases whe"
N18-1088,D15-1041,1,0.919347,"overed by UD guidelines.1 Our annotation includes tokenization, part-of-speech (POS) tags, and (labeled) Universal Dependencies. We characterize the disagreements among our annotators and find that consistent annotation is still challenging to deliver even with the extended guidelines. Based on these annotations, we nonetheless designed a pipeline to parse raw tweets into Universal Dependencies. Our pipeline includes: a bidirectional LSTM (bi-LSTM) tokenizer, a word cluster–enhanced POS tagger (following Owoputi et al., 2013), and a stack LSTM parser with character-based word representations (Ballesteros et al., 2015), which we refer to as our “baseline” parser. To overcome the noise in our annotated We study the problem of analyzing tweets with Universal Dependencies (UD; Nivre et al., 2016). We extend the UD guidelines to cover special constructions in tweets that affect tokenization, part-ofspeech tagging, and labeled dependencies. Using the extended guidelines, we create a new tweet treebank for English (T WEEBANK V 2) that is four times larger than the (unlabeled) T WEEBANK V 1 introduced by Kong et al. (2014). We characterize the disagreements between our annotators and show that it is challenging to"
N18-1088,D16-1211,1,0.907114,"Missing"
N18-1088,J92-4003,0,0.519758,"Missing"
N18-1088,de-marneffe-etal-2014-universal,0,0.0901755,"Missing"
N18-1088,R13-1026,0,0.167981,"Missing"
N18-1088,D16-1180,1,0.877777,"e updated version of Twokenizer from Owoputi et al. (2013). 13 http://nlp.stanford.edu/data/glove.twitter. 27B.zip 971 System Kong et al. (2014) Dozat et al. (2017) Ballesteros et al. (2015) Ensemble (20) Distillation (α = 1.0) Distillation (α = 0.9) Distillation w/ exploration UAS 81.4 81.8 80.2 83.4 81.8 82.0 82.1 LAS 76.9 77.7 75.7 79.4 77.6 77.8 77.9 Kt/s 0.3 1.7 2.3 0.2 2.3 2.3 2.3 tems above (Table 6). Distillation. The shortcoming of the 20-parser ensemble is, of course, that it requires twenty times the runtime of a single greedy parser, making it the slowest system in our comparison. Kuncoro et al. (2016) proposed the distillation of 20 greedy transition-based parser into a single graphbased parser; they transformed the votes of the ensemble into a structured loss function. However, as Kuncoro et al. pointed out, it is not straightforward to use a structured loss in a transition-based parsing algorithm. Because fast runtime is so important for NLP on social media, we introduce a new way to distill our greedy ensemble into a single transition-based parser (the first such attempt, to our knowledge). Our approach applies techniques from Hinton et al. (2015) and Kim and Rush (2016) to parsing. Not"
N18-1088,K17-3002,0,0.103467,"ination of UD_English-EWT and T WEE BANK V 2 training sets. Gold-standard tokenization and automatic POS tags are used. Automatic POS tags are assigned with 5-fold jackknifing. Hyperparameters are tuned on the T WEEBANK V 2 development set. Unlabeled attachment score and labeled attachment score (including punctuation) are reported. All the experiments were run on a Xeon E5-2670 2.6 GHz machine. Reimers and Gurevych (2017) and others have 12 We use the updated version of Twokenizer from Owoputi et al. (2013). 13 http://nlp.stanford.edu/data/glove.twitter. 27B.zip 971 System Kong et al. (2014) Dozat et al. (2017) Ballesteros et al. (2015) Ensemble (20) Distillation (α = 1.0) Distillation (α = 0.9) Distillation w/ exploration UAS 81.4 81.8 80.2 83.4 81.8 82.0 82.1 LAS 76.9 77.7 75.7 79.4 77.6 77.8 77.9 Kt/s 0.3 1.7 2.3 0.2 2.3 2.3 2.3 tems above (Table 6). Distillation. The shortcoming of the 20-parser ensemble is, of course, that it requires twenty times the runtime of a single greedy parser, making it the slowest system in our comparison. Kuncoro et al. (2016) proposed the distillation of 20 greedy transition-based parser into a single graphbased parser; they transformed the votes of the ensemble int"
N18-1088,K17-3001,0,0.199799,"a tweet. We therefore treat the whole expression as non-syntactic, including assigning the other (X) part of speech to both RT and @coldplay, attaching the at-mention to RT with the discourse label and the colon to RT with the punct(uation) label, and attaching RT to the predicate of the following sentence. course while they used parataxis. In referential URLs, we use list (following the precedent of UD_English-EWT) while they used dep. Our choice of discourse for sentiment emoticons is inspired by the observation that emoticons are annotated as discourse by UD_English-EWT; Sanguinetti et al. (2017) used the same relation for the emoticons. Retweet constructions and truncated words were not explicitly touched by Sanguinetti et al. (2017). Judging from the released treebank8 , the RT marker, at-mention, and colon in the retweet construction are all attached to the predicate of the following sentence with dep, vocative:mention and punct. We expect that the official UD guidelines will eventually adopt standards for these constructions so the treebanks can be harmonized. Constructions handled by UD. A number of constructions that are especially common in tweets are handled by UD conventions:"
N18-1088,N13-1037,0,0.0477398,"Missing"
N18-1088,P16-1101,0,0.076965,"Missing"
N18-1088,P14-5010,0,0.00286153,"s the tokenization results, 11 Manual annotation was done with Arborator (Gerdes, 2013), a web platform for drawing dependency trees. 970 System Stanford CoreNLP Twokenizer UDPipe v1.2 our bi-LSTM tokenizer System Stanford CoreNLP Owoputi et al., 2013 (greedy) Owoputi et al., 2013 (CRF) Ma and Hovy, 2016 F1 97.3 94.6 97.4 98.3 Table 3: Tokenizer comparison on the T WEEBANK V 2 test set. Table 4: POS tagger comparison on gold-standard tokens in the T WEEBANK V 2 test set. Tokenization System Stanford CoreNLP our bi-LSTM tokenizer (§3.1) compared to other available tokenizers. Stanford CoreNLP (Manning et al., 2014) and Twokenizer (O’Connor et al., 2010)12 are rule-based systems and were not adapted to the UD tokenization scheme. The UDPipe v1.2 (Straka and Straková, 2017) model was re-trained on the same data as our system. Compared with UDPipe, we use an LSTM instead of a GRU in our model and we also use a larger size for hidden units (64 vs. 20), which has stronger representational power. Our bi-LSTM tokenizer achieves the best accuracy among all these tokenizers. These results speak to the value of statistical modeling in tokenization for informal texts. 3.2 Accuracy 90.6 93.7 94.6 92.5 F1 92.3 93.3"
N18-1088,J93-2004,0,0.062664,"to distill an ensemble of 20 transition-based parsers into a single one. Our parser achieves an improvement of 2.2 in LAS over the un-ensembled baseline and outperforms parsers that are state-ofthe-art on other treebanks in both accuracy and speed. 1 Noah A. Smith University of Washington Introduction NLP for social media messages is challenging, requiring domain adaptation and annotated datasets (e.g., treebanks) for training and evaluation. Pioneering work by Foster et al. (2011) annotated 7,630 tokens’ worth of tweets according to the phrase-structure conventions of the Penn Treebank (PTB; Marcus et al., 1993), enabling conversion to Stanford Dependencies. Kong et al. (2014) further studied the challenges in annotating tweets and 1 We developed our treebank independently of a similar effort for Italian tweets (Sanguinetti et al., 2017). See §2.5 for a comparison. 965 Proceedings of NAACL-HLT 2018, pages 965–975 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics Their aim was the rapid development of a dependency parser for tweets, and to that end they contributed a new annotated corpus, T WEEBANK, consisting of 12,149 tokens. Their annotations added unlabeled"
N18-1088,W13-3711,0,0.0175481,"rimental results. Our preliminary results showed that our model trained on the combination of UD_English-EWT and T WEEBANK V 2 outperformed the one trained only on the UD_EnglishEWT or T WEEBANK V 2, consistent with previous work on dialect treebank parsing (Wang et al., 2017). So we trained our tokenizer on the training portion of T WEEBANK V 2 combined with the UD_English-EWT training set and tested on the T WEEBANK V 2 test set. We report F1 scores, combining precision and recall for token identification. Table 3 shows the tokenization results, 11 Manual annotation was done with Arborator (Gerdes, 2013), a web platform for drawing dependency trees. 970 System Stanford CoreNLP Twokenizer UDPipe v1.2 our bi-LSTM tokenizer System Stanford CoreNLP Owoputi et al., 2013 (greedy) Owoputi et al., 2013 (CRF) Ma and Hovy, 2016 F1 97.3 94.6 97.4 98.3 Table 3: Tokenizer comparison on the T WEEBANK V 2 test set. Table 4: POS tagger comparison on gold-standard tokens in the T WEEBANK V 2 test set. Tokenization System Stanford CoreNLP our bi-LSTM tokenizer (§3.1) compared to other available tokenizers. Stanford CoreNLP (Manning et al., 2014) and Twokenizer (O’Connor et al., 2010)12 are rule-based systems a"
N18-1088,P11-2008,1,0.897627,"Missing"
N18-1088,C12-1059,0,0.0394777,"Missing"
N18-1088,N13-1039,1,0.950015,"Missing"
N18-1088,Q13-1033,0,0.0408296,"Missing"
N18-1088,D14-1162,0,0.0814147,"Missing"
N18-1088,D17-1256,0,0.437481,"Missing"
N18-1088,petrov-etal-2012-universal,0,0.0767282,"Missing"
N18-1088,P15-1119,1,0.752001,"ggested by Schneider et al. (2013), fairly close to Yamada and Matsumoto (2003) dependencies (without labels). Both annotation efforts were highly influenced by the PTB, whose guidelines have good grammatical coverage on newswire. However, when it comes to informal, unedited, user-generated text, the guidelines may leave many annotation decisions unspecified. Universal Dependencies (Nivre et al., 2016, UD) were introduced to enable consistent annotation across different languages. To allow such consistency, UD was designed to be adaptable to different genres (Wang et al., 2017) and languages (Guo et al., 2015; Ammar et al., 2016). We propose that analyzing the syntax of tweets can benefit from such adaptability. In this paper, we introduce a new English tweet treebank of 55,607 tokens that follows the UD guidelines, but also contends with social media-specific challenges that were not covered by UD guidelines.1 Our annotation includes tokenization, part-of-speech (POS) tags, and (labeled) Universal Dependencies. We characterize the disagreements among our annotators and find that consistent annotation is still challenging to deliver even with the extended guidelines. Based on these annotations, we"
N18-1088,D17-1035,0,0.0171475,"hich should help mitigate the challenge of spelling variation. We encourage the reader to refer their paper for more details about the model. In our initial experiments, we train our parser on the combination of UD_English-EWT and T WEE BANK V 2 training sets. Gold-standard tokenization and automatic POS tags are used. Automatic POS tags are assigned with 5-fold jackknifing. Hyperparameters are tuned on the T WEEBANK V 2 development set. Unlabeled attachment score and labeled attachment score (including punctuation) are reported. All the experiments were run on a Xeon E5-2670 2.6 GHz machine. Reimers and Gurevych (2017) and others have 12 We use the updated version of Twokenizer from Owoputi et al. (2013). 13 http://nlp.stanford.edu/data/glove.twitter. 27B.zip 971 System Kong et al. (2014) Dozat et al. (2017) Ballesteros et al. (2015) Ensemble (20) Distillation (α = 1.0) Distillation (α = 0.9) Distillation w/ exploration UAS 81.4 81.8 80.2 83.4 81.8 82.0 82.1 LAS 76.9 77.7 75.7 79.4 77.6 77.8 77.9 Kt/s 0.3 1.7 2.3 0.2 2.3 2.3 2.3 tems above (Table 6). Distillation. The shortcoming of the 20-parser ensemble is, of course, that it requires twenty times the runtime of a single greedy parser, making it the slowe"
N18-1088,D11-1141,0,0.495674,"Missing"
N18-1088,W17-6526,0,0.480518,"oth accuracy and speed. 1 Noah A. Smith University of Washington Introduction NLP for social media messages is challenging, requiring domain adaptation and annotated datasets (e.g., treebanks) for training and evaluation. Pioneering work by Foster et al. (2011) annotated 7,630 tokens’ worth of tweets according to the phrase-structure conventions of the Penn Treebank (PTB; Marcus et al., 1993), enabling conversion to Stanford Dependencies. Kong et al. (2014) further studied the challenges in annotating tweets and 1 We developed our treebank independently of a similar effort for Italian tweets (Sanguinetti et al., 2017). See §2.5 for a comparison. 965 Proceedings of NAACL-HLT 2018, pages 965–975 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics Their aim was the rapid development of a dependency parser for tweets, and to that end they contributed a new annotated corpus, T WEEBANK, consisting of 12,149 tokens. Their annotations added unlabeled dependencies to a portion of the data annotated with POS tags by Gimpel et al. (2011) and Owoputi et al. (2013) after rule-based tokenization (O’Connor et al., 2010). Kong et al. also contributed a system for parsing; we defer th"
N18-1088,D16-1139,0,0.0215692,"r comparison. Kuncoro et al. (2016) proposed the distillation of 20 greedy transition-based parser into a single graphbased parser; they transformed the votes of the ensemble into a structured loss function. However, as Kuncoro et al. pointed out, it is not straightforward to use a structured loss in a transition-based parsing algorithm. Because fast runtime is so important for NLP on social media, we introduce a new way to distill our greedy ensemble into a single transition-based parser (the first such attempt, to our knowledge). Our approach applies techniques from Hinton et al. (2015) and Kim and Rush (2016) to parsing. Note that training a transition-based parser typically involves the transformation of the training data into a sequence of “oracle” state-action pairs. Let q(a |s) denote the distilled model’s probability of an action a given parser state s; let p(a |s) be the probability under the ensemble (i.e., the average of the 20 separately-trained ensemble members). To train the distilled model, we minimize the interpolation between their distillation loss and the conventional log loss: XX argminq α −p(a |si ) · log q(a |si ) Table 6: Dependency parser comparison on T WEE BANK V 2 test set,"
N18-1088,Q16-1023,0,0.108829,"Missing"
N18-1088,W13-2307,1,0.929727,"Missing"
N18-1088,D14-1108,1,0.404607,"uti et al., 2013), and a stack LSTM parser with character-based word representations (Ballesteros et al., 2015), which we refer to as our “baseline” parser. To overcome the noise in our annotated We study the problem of analyzing tweets with Universal Dependencies (UD; Nivre et al., 2016). We extend the UD guidelines to cover special constructions in tweets that affect tokenization, part-ofspeech tagging, and labeled dependencies. Using the extended guidelines, we create a new tweet treebank for English (T WEEBANK V 2) that is four times larger than the (unlabeled) T WEEBANK V 1 introduced by Kong et al. (2014). We characterize the disagreements between our annotators and show that it is challenging to deliver consistent annotation due to ambiguity in understanding and explaining tweets. Nonetheless, using the new treebank, we build a pipeline system to parse raw tweets into UD. To overcome annotation noise without sacrificing computational efficiency, we propose a new method to distill an ensemble of 20 transition-based parsers into a single one. Our parser achieves an improvement of 2.2 in LAS over the un-ensembled baseline and outperforms parsers that are state-ofthe-art on other treebanks in bot"
N18-1088,K17-3009,0,0.0304172,"oreNLP Twokenizer UDPipe v1.2 our bi-LSTM tokenizer System Stanford CoreNLP Owoputi et al., 2013 (greedy) Owoputi et al., 2013 (CRF) Ma and Hovy, 2016 F1 97.3 94.6 97.4 98.3 Table 3: Tokenizer comparison on the T WEEBANK V 2 test set. Table 4: POS tagger comparison on gold-standard tokens in the T WEEBANK V 2 test set. Tokenization System Stanford CoreNLP our bi-LSTM tokenizer (§3.1) compared to other available tokenizers. Stanford CoreNLP (Manning et al., 2014) and Twokenizer (O’Connor et al., 2010)12 are rule-based systems and were not adapted to the UD tokenization scheme. The UDPipe v1.2 (Straka and Straková, 2017) model was re-trained on the same data as our system. Compared with UDPipe, we use an LSTM instead of a GRU in our model and we also use a larger size for hidden units (64 vs. 20), which has stronger representational power. Our bi-LSTM tokenizer achieves the best accuracy among all these tokenizers. These results speak to the value of statistical modeling in tokenization for informal texts. 3.2 Accuracy 90.6 93.7 94.6 92.5 F1 92.3 93.3 Table 5: Owoputi et al. (2013) POS tagging performance with automatic tokenization on the T WEEBANK V 2 test set. Experimental results. We tested the POS tagger"
N18-1088,P17-1159,0,0.100495,"d largely following conventions suggested by Schneider et al. (2013), fairly close to Yamada and Matsumoto (2003) dependencies (without labels). Both annotation efforts were highly influenced by the PTB, whose guidelines have good grammatical coverage on newswire. However, when it comes to informal, unedited, user-generated text, the guidelines may leave many annotation decisions unspecified. Universal Dependencies (Nivre et al., 2016, UD) were introduced to enable consistent annotation across different languages. To allow such consistency, UD was designed to be adaptable to different genres (Wang et al., 2017) and languages (Guo et al., 2015; Ammar et al., 2016). We propose that analyzing the syntax of tweets can benefit from such adaptability. In this paper, we introduce a new English tweet treebank of 55,607 tokens that follows the UD guidelines, but also contends with social media-specific challenges that were not covered by UD guidelines.1 Our annotation includes tokenization, part-of-speech (POS) tags, and (labeled) Universal Dependencies. We characterize the disagreements among our annotators and find that consistent annotation is still challenging to deliver even with the extended guidelines"
N18-1088,W03-3023,0,0.271111,"Missing"
N18-1088,L16-1262,0,\N,Missing
P14-1113,D07-1017,0,0.0489721,"of entities in order to improve the performance of a question answering system. Ritter et al. (2009) propose a method based on patterns to find hypernyms on arbitrary noun phrases. They use a support vector machine classifier to identify the correct hypernyms from the candidates that match the patterns. As our experiments show, patternbased methods suffer from low recall because of the low coverage of patterns. Besides Kotlerman et al. (2010) and Lenci and Benotto (2012), other researchers also propose directional distributional similarity methods (Weeds et al., 2004; Geffet and Dagan, 2005; Bhagat et al., 2007; Szpektor et al., 2007; Clarke, 2009). However, their basic assumption that a hyponym can only be used in contexts where its hypernyms can be used and that a hypernym might be used in all of the contexts where its hyponyms are used may not always rational. Snow et al. (2006) provides a global optimization scheme for extending WordNet, which is different from the above-mentioned pairwise relationships identification methods. Word embeddings have been successfully applied in many applications, such as in sentiment analysis (Socher et al., 2011b), paraphrase detection (Socher et al., 2011a), chu"
P14-1113,C10-3004,1,0.465681,"uned on a development dataset. 3.3.3 Training Data To learn the projection matrices, we extract training data from a Chinese semantic thesaurus, Tongyi Cilin (Extended) (CilinE for short) which 1202 δ Root 物 object B 昆虫 : 动物 (insect : animal) 蜻蜓 : 动物 (dragonfly : animal) 动物 animal … Sense Code: Bi 昆虫 insect 18 -- … 蜻蜓 : 昆虫 Sense Code: Bi18A 蜻蜓 dragonfly Sense Code: Bi18A06@ A … … Level 3 x Figure 4: In this example, Φk x is located in the circle with center y and radius δ. So y is considered as a hypernym of x. Conversely, y is not a hypernym of x0 . Level 5 06@ CilinE contains 100,093 words (Che et al., 2010).3 CilinE is organized as a hierarchy of five levels, in which the words are linked by hypernym–hyponym relations (right panel, Figure 3). Each word in CilinE has one or more sense codes (some words are polysemous) that indicate its position in the hierarchy. The senses of words in the first level, such as “物 (object)” and “时间 (time),” are very general. The fourth level only has sense codes without real words. Therefore, we extract words in the second, third and fifth levels to constitute hypernym– hyponym pairs (left panel, Figure 3). Note that mapping one hyponym to multiple hypernyms with t"
P14-1113,W09-0215,0,0.0162212,"ance of a question answering system. Ritter et al. (2009) propose a method based on patterns to find hypernyms on arbitrary noun phrases. They use a support vector machine classifier to identify the correct hypernyms from the candidates that match the patterns. As our experiments show, patternbased methods suffer from low recall because of the low coverage of patterns. Besides Kotlerman et al. (2010) and Lenci and Benotto (2012), other researchers also propose directional distributional similarity methods (Weeds et al., 2004; Geffet and Dagan, 2005; Bhagat et al., 2007; Szpektor et al., 2007; Clarke, 2009). However, their basic assumption that a hyponym can only be used in contexts where its hypernyms can be used and that a hypernym might be used in all of the contexts where its hyponyms are used may not always rational. Snow et al. (2006) provides a global optimization scheme for extending WordNet, which is different from the above-mentioned pairwise relationships identification methods. Word embeddings have been successfully applied in many applications, such as in sentiment analysis (Socher et al., 2011b), paraphrase detection (Socher et al., 2011a), chunking, and named entity recognition (T"
P14-1113,D13-1122,1,0.267748,"versely, “dog” is a hyponym of “canine.” As key sources of knowledge, semantic thesauri and ontologies can support many natural language processing applications. However, these semantic resources are limited in its scope and domain, and their manual construction is knowledge intensive and time consuming. Therefore, many researchers Email correspondence. 毛茛科 Ranunculaceae 乌头属 Aconitum Introduction ∗ 生物 organism 植物 plant have attempted to automatically extract semantic relations or to construct taxonomies. A major challenge for this task is the automatic discovery of hypernym-hyponym relations. Fu et al. (2013) propose a distant supervision method to extract hypernyms for entities from multiple sources. The output of their model is a list of hypernyms for a given enity (left panel, Figure 1). However, there usually also exists hypernym–hyponym relations among these hypernyms. For instance, “植 物 (plant)” and “毛 茛 科 (Ranunculaceae)” are both hypernyms of the entity “乌 头 (aconit),” and “植 物 (plant)” is also a hypernym of “毛 茛 科 (Ranunculaceae).” Given a list of hypernyms of an entity, our goal in the present work is to construct a semantic hierarchy of these hypernyms (right panel, Figure 1).1 Some pre"
P14-1113,P05-1014,0,0.450387,"hod to extract hypernyms of entities in order to improve the performance of a question answering system. Ritter et al. (2009) propose a method based on patterns to find hypernyms on arbitrary noun phrases. They use a support vector machine classifier to identify the correct hypernyms from the candidates that match the patterns. As our experiments show, patternbased methods suffer from low recall because of the low coverage of patterns. Besides Kotlerman et al. (2010) and Lenci and Benotto (2012), other researchers also propose directional distributional similarity methods (Weeds et al., 2004; Geffet and Dagan, 2005; Bhagat et al., 2007; Szpektor et al., 2007; Clarke, 2009). However, their basic assumption that a hyponym can only be used in contexts where its hypernyms can be used and that a hypernym might be used in all of the contexts where its hyponyms are used may not always rational. Snow et al. (2006) provides a global optimization scheme for extending WordNet, which is different from the above-mentioned pairwise relationships identification methods. Word embeddings have been successfully applied in many applications, such as in sentiment analysis (Socher et al., 2011b), paraphrase detection (Soche"
P14-1113,C92-2082,0,0.775886,"物 (plant)” is also a hypernym of “毛 茛 科 (Ranunculaceae).” Given a list of hypernyms of an entity, our goal in the present work is to construct a semantic hierarchy of these hypernyms (right panel, Figure 1).1 Some previous works extend and refine manually-built semantic hierarchies by using other resources (e.g., Wikipedia) (Suchanek et al., 2008). However, the coverage is limited by the scope of the resources. Several other works relied heavily on lexical patterns, which would suffer from deficiency because such patterns can only cover a small proportion of complex linguistic circumstances (Hearst, 1992; Snow et al., 2005). 1 In this study, we focus on Chinese semantic hierarchy construction. The proposed method can be easily adapted to other languages. 1199 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1199–1209, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics Besides, distributional similarity methods (Kotlerman et al., 2010; Lenci and Benotto, 2012) are based on the assumption that a term can only be used in contexts where its hypernyms can be used and that a term might be used in any contexts whe"
P14-1113,S12-1012,0,0.493654,"icult to guarantee. Generally speaking, these pattern-based methods often suffer from low recall or precision because of the coverage or the quality of the patterns. The distributional methods assume that the contexts of hypernyms are broader than the ones of their hyponyms. For distributional similarity computing, each word is represented as a semantic vector composed of the pointwise mutual information (PMI) with its contexts. Kotlerman et al. (2010) design a directional distributional measure to infer hypernym–hyponym relations based on the standard IR Average Precision evaluation measure. Lenci and Benotto (2012) propose another measure focusing on the contexts that hypernyms do not share with their hyponyms. However, broader semantics may not always infer broader contexts. For example, for terms “Obama’ and 1200 “American people”, it is hard to say whose contexts are broader. Our previous work (Fu et al., 2013) applies a web mining method to discover the hypernyms of Chinese entities from multiple sources. We assume that the hypernyms of an entity co-occur with it frequently. It works well for named entities. But for class names (e.g., singers in Hong Kong, tropical fruits) with wider range of meanin"
P14-1113,I08-2112,0,0.0347144,". One possible solution may be adding more data of this kind to the training set. 6 Related Work In addition to the works mentioned in Section 2, we introduce another set of related studies in this section. Evans (2004), Ortega-Mendoza et al. (2007), and Sang (2007) consider web data as a large corpus and use search engines to identify hypernyms based on the lexical patterns of Hearst (1992). However, the low quality of the sentences in the search results negatively influence the precision of hypernym extraction. Following the method for discovering patterns automatically (Snow et al., 2005), McNamee et al. (2008) apply the same method to extract hypernyms of entities in order to improve the performance of a question answering system. Ritter et al. (2009) propose a method based on patterns to find hypernyms on arbitrary noun phrases. They use a support vector machine classifier to identify the correct hypernyms from the candidates that match the patterns. As our experiments show, patternbased methods suffer from low recall because of the low coverage of patterns. Besides Kotlerman et al. (2010) and Lenci and Benotto (2012), other researchers also propose directional distributional similarity methods (W"
P14-1113,N13-1090,0,0.480991,"ks well for hypernym extraction of entity names, but it is unsuitable for semantic hierarchy construction which involves many words with broad semantics. Moreover, all of these methods do not use the word semantics effectively. This paper proposes a novel approach for semantic hierarchy construction based on word embeddings. Word embeddings, also known as distributed word representations, typically represent words with dense, low-dimensional and realvalued vectors. Word embeddings have been empirically shown to preserve linguistic regularities, such as the semantic relationship between words (Mikolov et al., 2013b). For example, v(king) − v(queen) ≈ v(man) − v(woman), where v(w) is the embedding of the word w. We observe that a similar property also applies to the hypernym–hyponym relationship (Section 3.3), which is the main inspiration of the present study. However, we further observe that hypernym– hyponym relations are more complicated than a single offset can represent. To address this challenge, we propose a more sophisticated and general method — learning a linear projection which maps words to their hypernyms (Section 3.3.1). Furthermore, we propose a piecewise linear projection method based o"
P14-1113,P07-2042,0,0.242856,"Missing"
P14-1113,P06-1101,0,0.241765,"tes that match the patterns. As our experiments show, patternbased methods suffer from low recall because of the low coverage of patterns. Besides Kotlerman et al. (2010) and Lenci and Benotto (2012), other researchers also propose directional distributional similarity methods (Weeds et al., 2004; Geffet and Dagan, 2005; Bhagat et al., 2007; Szpektor et al., 2007; Clarke, 2009). However, their basic assumption that a hyponym can only be used in contexts where its hypernyms can be used and that a hypernym might be used in all of the contexts where its hyponyms are used may not always rational. Snow et al. (2006) provides a global optimization scheme for extending WordNet, which is different from the above-mentioned pairwise relationships identification methods. Word embeddings have been successfully applied in many applications, such as in sentiment analysis (Socher et al., 2011b), paraphrase detection (Socher et al., 2011a), chunking, and named entity recognition (Turian et al., 2010; Collobert et al., 2011). These applications mainly utilize the representing power of word embeddings to alleviate the problem of data sparsity. Mikolov et al. (2013a) and Mikolov et al. (2013b) further observe that the"
P14-1113,D11-1014,0,0.0421,"ethods (Weeds et al., 2004; Geffet and Dagan, 2005; Bhagat et al., 2007; Szpektor et al., 2007; Clarke, 2009). However, their basic assumption that a hyponym can only be used in contexts where its hypernyms can be used and that a hypernym might be used in all of the contexts where its hyponyms are used may not always rational. Snow et al. (2006) provides a global optimization scheme for extending WordNet, which is different from the above-mentioned pairwise relationships identification methods. Word embeddings have been successfully applied in many applications, such as in sentiment analysis (Socher et al., 2011b), paraphrase detection (Socher et al., 2011a), chunking, and named entity recognition (Turian et al., 2010; Collobert et al., 2011). These applications mainly utilize the representing power of word embeddings to alleviate the problem of data sparsity. Mikolov et al. (2013a) and Mikolov et al. (2013b) further observe that the semantic relationship of words can be induced by performing simple algebraic operations with word vectors. Their work indicates that word embeddings preserve some interesting linguistic regularities, which might provide support for many applications. In this paper, we im"
P14-1113,P07-1058,0,0.026901,"to improve the performance of a question answering system. Ritter et al. (2009) propose a method based on patterns to find hypernyms on arbitrary noun phrases. They use a support vector machine classifier to identify the correct hypernyms from the candidates that match the patterns. As our experiments show, patternbased methods suffer from low recall because of the low coverage of patterns. Besides Kotlerman et al. (2010) and Lenci and Benotto (2012), other researchers also propose directional distributional similarity methods (Weeds et al., 2004; Geffet and Dagan, 2005; Bhagat et al., 2007; Szpektor et al., 2007; Clarke, 2009). However, their basic assumption that a hyponym can only be used in contexts where its hypernyms can be used and that a hypernym might be used in all of the contexts where its hyponyms are used may not always rational. Snow et al. (2006) provides a global optimization scheme for extending WordNet, which is different from the above-mentioned pairwise relationships identification methods. Word embeddings have been successfully applied in many applications, such as in sentiment analysis (Socher et al., 2011b), paraphrase detection (Socher et al., 2011a), chunking, and named entity"
P14-1113,P10-1040,0,0.084559,"). However, their basic assumption that a hyponym can only be used in contexts where its hypernyms can be used and that a hypernym might be used in all of the contexts where its hyponyms are used may not always rational. Snow et al. (2006) provides a global optimization scheme for extending WordNet, which is different from the above-mentioned pairwise relationships identification methods. Word embeddings have been successfully applied in many applications, such as in sentiment analysis (Socher et al., 2011b), paraphrase detection (Socher et al., 2011a), chunking, and named entity recognition (Turian et al., 2010; Collobert et al., 2011). These applications mainly utilize the representing power of word embeddings to alleviate the problem of data sparsity. Mikolov et al. (2013a) and Mikolov et al. (2013b) further observe that the semantic relationship of words can be induced by performing simple algebraic operations with word vectors. Their work indicates that word embeddings preserve some interesting linguistic regularities, which might provide support for many applications. In this paper, we improve on their work by learning multiple linear projections in the embedding space, to model hypernym–hypony"
P14-1113,C04-1146,0,0.0870413,"Missing"
P14-1146,C10-2005,0,0.00563043,"aches. The lexicon-based approaches (Turney, 2002; Ding et al., 2008; Taboada et al., 2011; Thelwall et al., 2012) mostly use a dictionary of sentiment words with their associated sentiment polarity, and incorporate negation and intensification to compute the sentiment polarity for each sentence (or document). The learning based methods for Twitter sentiment classification follow Pang et al. (2002)’s work, which treat sentiment classification of texts as a special case of text categorization issue. Many studies on Twitter sentiment classification (Pak and Paroubek, 2010; Davidov et al., 2010; Barbosa and Feng, 2010; Kouloumpis et al., 2011; Zhao et al., 2012) leverage massive noisy-labeled tweets selected by positive and negative emoticons as training set and build sentiment classifiers directly, which is called distant supervision (Go et al., 2009). Instead of directly using the distantsupervised data as training set, Liu et al. (2012) adopt the tweets with emoticons to smooth the language model and Hu et al. (2013) incorporate the emotional signals into an unsupervised learning framework for Twitter sentiment classification. Many existing learning based methods on Twitter sentiment classification focu"
P14-1146,P07-1056,0,0.114896,"plement this system because the codes are not publicly available 3 . NRC-ngram refers to the feature set of NRC leaving out ngram features. Except for DistSuper, other baseline methods are conducted in a supervised manner. We do not compare with RNTN (Socher et al., 2013b) because we cannot efficiently train the RNTN model. The reason lies in that the tweets in our dataset do not have accurately parsed results or fine grained sentiment labels for phrases. Another reason is that the RNTN model trained on movie reviews cannot be directly applied on tweets due to the differences between domains (Blitzer et al., 2007). Results and Analysis. Table 2 shows the macroF1 of the baseline systems as well as the SSWEbased methods on positive/negative sentiment classification of tweets. Distant supervision is relatively weak because the noisy-labeled tweets are treated as the gold standard, which affects the performance of classifier. The results of bagof-ngram (uni/bi/tri-gram) features are not satisfied because the one-hot word representation cannot capture the latent connections between words. NBSVM and RAE perform comparably and have 3 For 3-class sentiment classification in SemEval 2013, our re-implementation"
P14-1146,C10-2028,0,0.00814251,"t classification approaches. The lexicon-based approaches (Turney, 2002; Ding et al., 2008; Taboada et al., 2011; Thelwall et al., 2012) mostly use a dictionary of sentiment words with their associated sentiment polarity, and incorporate negation and intensification to compute the sentiment polarity for each sentence (or document). The learning based methods for Twitter sentiment classification follow Pang et al. (2002)’s work, which treat sentiment classification of texts as a special case of text categorization issue. Many studies on Twitter sentiment classification (Pak and Paroubek, 2010; Davidov et al., 2010; Barbosa and Feng, 2010; Kouloumpis et al., 2011; Zhao et al., 2012) leverage massive noisy-labeled tweets selected by positive and negative emoticons as training set and build sentiment classifiers directly, which is called distant supervision (Go et al., 2009). Instead of directly using the distantsupervised data as training set, Liu et al. (2012) adopt the tweets with emoticons to smooth the language model and Hu et al. (2013) incorporate the emotional signals into an unsupervised learning framework for Twitter sentiment classification. Many existing learning based methods on Twitter senti"
P14-1146,P11-2008,0,0.0123158,"Missing"
P14-1146,P13-1088,0,0.0113325,"Missing"
P14-1146,P11-1016,1,0.370167,"their loss functions. To obtain large scale training corpora, we learn the sentiment-specific word embedding from massive distant-supervised tweets collected by positive and negative emoticons. Experiments on applying SSWE to a benchmark Twitter sentiment classification dataset in SemEval 2013 show that (1) the SSWE feature performs comparably with hand-crafted features in the top-performed system; (2) the performance is further improved by concatenating SSWE with existing feature set. 1 Introduction Twitter sentiment classification has attracted increasing research interest in recent years (Jiang et al., 2011; Hu et al., 2013). The objective is to classify the sentiment polarity of a tweet as positive, ∗ This work was done when the first and third authors were visiting Microsoft Research Asia. negative or neutral. The majority of existing approaches follow Pang et al. (2002) and employ machine learning algorithms to build classifiers from tweets with manually annotated sentiment polarity. Under this direction, most studies focus on designing effective features to obtain better classification performance. For example, Mohammad et al. (2013) build the top-performed system in the Twitter sentiment cl"
P14-1146,P13-2087,0,0.792293,"utoencoders for domain adaptation in sentiment classification. Socher et al. propose Recursive Neural Network (RNN) (2011b), matrixvector RNN (2012) and Recursive Neural Tensor Network (RNTN) (2013b) to learn the compositionality of phrases of any length based on the representation of each pair of children recursively. Hermann et al. (2013) present Combinatory Categorial Autoencoders to learn the compositionality of sentence, which marries the Combinatory Categorial Grammar with Recursive Autoencoder. The representation of words heavily relies on the applications or tasks in which it is used (Labutov and Lipson, 2013). This paper focuses on learning sentiment-specific word embedding, which is tailored for sentiment analysis. Unlike Maas et al. (2011) that follow the probabilistic document model (Blei et al., 2003) and give an sentiment predictor function to each word, we develop neural networks and map each ngram to the sentiment polarity of sentence. Unlike Socher et al. (2011c) that utilize manually labeled texts to learn the meaning of phrase (or sentence) through compositionality, we focus on learning the meaning of word, namely word embedding, from massive distant-supervised tweets. Unlike Labutov and"
P14-1146,P11-1015,0,0.646787,"(2012) and Recursive Neural Tensor Network (RNTN) (2013b) to learn the compositionality of phrases of any length based on the representation of each pair of children recursively. Hermann et al. (2013) present Combinatory Categorial Autoencoders to learn the compositionality of sentence, which marries the Combinatory Categorial Grammar with Recursive Autoencoder. The representation of words heavily relies on the applications or tasks in which it is used (Labutov and Lipson, 2013). This paper focuses on learning sentiment-specific word embedding, which is tailored for sentiment analysis. Unlike Maas et al. (2011) that follow the probabilistic document model (Blei et al., 2003) and give an sentiment predictor function to each word, we develop neural networks and map each ngram to the sentiment polarity of sentence. Unlike Socher et al. (2011c) that utilize manually labeled texts to learn the meaning of phrase (or sentence) through compositionality, we focus on learning the meaning of word, namely word embedding, from massive distant-supervised tweets. Unlike Labutov and Lipson (2013) that produce task-specific embedding from an existing word embedding, we learn sentiment-specific word embedding from sc"
P14-1146,D12-1110,0,0.368522,"Missing"
P14-1146,P13-1045,0,0.149339,"p-performed system in the Twitter sentiment classification track of SemEval 2013 (Nakov et al., 2013), using diverse sentiment lexicons and a variety of hand-crafted features. Feature engineering is important but laborintensive. It is therefore desirable to discover explanatory factors from the data and make the learning algorithms less dependent on extensive feature engineering (Bengio, 2013). For the task of sentiment classification, an effective feature learning method is to compose the representation of a sentence (or document) from the representations of the words or phrases it contains (Socher et al., 2013b; Yessenalina and Cardie, 2011). Accordingly, it is a crucial step to learn the word representation (or word embedding), which is a dense, low-dimensional and real-valued vector for a word. Although existing word embedding learning algorithms (Collobert et al., 2011; Mikolov et al., 2013) are intuitive choices, they are not effective enough if directly used for sentiment classification. The most serious problem is that traditional methods typically model the syntactic context of words but ignore the sentiment information of text. As a result, words with opposite polarity, such as good and bad"
P14-1146,S13-2053,0,0.231338,"on has attracted increasing research interest in recent years (Jiang et al., 2011; Hu et al., 2013). The objective is to classify the sentiment polarity of a tweet as positive, ∗ This work was done when the first and third authors were visiting Microsoft Research Asia. negative or neutral. The majority of existing approaches follow Pang et al. (2002) and employ machine learning algorithms to build classifiers from tweets with manually annotated sentiment polarity. Under this direction, most studies focus on designing effective features to obtain better classification performance. For example, Mohammad et al. (2013) build the top-performed system in the Twitter sentiment classification track of SemEval 2013 (Nakov et al., 2013), using diverse sentiment lexicons and a variety of hand-crafted features. Feature engineering is important but laborintensive. It is therefore desirable to discover explanatory factors from the data and make the learning algorithms less dependent on extensive feature engineering (Bengio, 2013). For the task of sentiment classification, an effective feature learning method is to compose the representation of a sentence (or document) from the representations of the words or phrases"
P14-1146,S13-2052,0,0.0193943,"to classify the sentiment polarity of a tweet as positive, ∗ This work was done when the first and third authors were visiting Microsoft Research Asia. negative or neutral. The majority of existing approaches follow Pang et al. (2002) and employ machine learning algorithms to build classifiers from tweets with manually annotated sentiment polarity. Under this direction, most studies focus on designing effective features to obtain better classification performance. For example, Mohammad et al. (2013) build the top-performed system in the Twitter sentiment classification track of SemEval 2013 (Nakov et al., 2013), using diverse sentiment lexicons and a variety of hand-crafted features. Feature engineering is important but laborintensive. It is therefore desirable to discover explanatory factors from the data and make the learning algorithms less dependent on extensive feature engineering (Bengio, 2013). For the task of sentiment classification, an effective feature learning method is to compose the representation of a sentence (or document) from the representations of the words or phrases it contains (Socher et al., 2013b; Yessenalina and Cardie, 2011). Accordingly, it is a crucial step to learn the w"
P14-1146,pak-paroubek-2010-twitter,0,0.047259,"low traditional sentiment classification approaches. The lexicon-based approaches (Turney, 2002; Ding et al., 2008; Taboada et al., 2011; Thelwall et al., 2012) mostly use a dictionary of sentiment words with their associated sentiment polarity, and incorporate negation and intensification to compute the sentiment polarity for each sentence (or document). The learning based methods for Twitter sentiment classification follow Pang et al. (2002)’s work, which treat sentiment classification of texts as a special case of text categorization issue. Many studies on Twitter sentiment classification (Pak and Paroubek, 2010; Davidov et al., 2010; Barbosa and Feng, 2010; Kouloumpis et al., 2011; Zhao et al., 2012) leverage massive noisy-labeled tweets selected by positive and negative emoticons as training set and build sentiment classifiers directly, which is called distant supervision (Go et al., 2009). Instead of directly using the distantsupervised data as training set, Liu et al. (2012) adopt the tweets with emoticons to smooth the language model and Hu et al. (2013) incorporate the emotional signals into an unsupervised learning framework for Twitter sentiment classification. Many existing learning based me"
P14-1146,W02-1011,0,0.132502,"dataset in SemEval 2013 show that (1) the SSWE feature performs comparably with hand-crafted features in the top-performed system; (2) the performance is further improved by concatenating SSWE with existing feature set. 1 Introduction Twitter sentiment classification has attracted increasing research interest in recent years (Jiang et al., 2011; Hu et al., 2013). The objective is to classify the sentiment polarity of a tweet as positive, ∗ This work was done when the first and third authors were visiting Microsoft Research Asia. negative or neutral. The majority of existing approaches follow Pang et al. (2002) and employ machine learning algorithms to build classifiers from tweets with manually annotated sentiment polarity. Under this direction, most studies focus on designing effective features to obtain better classification performance. For example, Mohammad et al. (2013) build the top-performed system in the Twitter sentiment classification track of SemEval 2013 (Nakov et al., 2013), using diverse sentiment lexicons and a variety of hand-crafted features. Feature engineering is important but laborintensive. It is therefore desirable to discover explanatory factors from the data and make the lea"
P14-1146,D11-1014,0,0.926164,"Autoencoders to learn the compositionality of sentence, which marries the Combinatory Categorial Grammar with Recursive Autoencoder. The representation of words heavily relies on the applications or tasks in which it is used (Labutov and Lipson, 2013). This paper focuses on learning sentiment-specific word embedding, which is tailored for sentiment analysis. Unlike Maas et al. (2011) that follow the probabilistic document model (Blei et al., 2003) and give an sentiment predictor function to each word, we develop neural networks and map each ngram to the sentiment polarity of sentence. Unlike Socher et al. (2011c) that utilize manually labeled texts to learn the meaning of phrase (or sentence) through compositionality, we focus on learning the meaning of word, namely word embedding, from massive distant-supervised tweets. Unlike Labutov and Lipson (2013) that produce task-specific embedding from an existing word embedding, we learn sentiment-specific word embedding from scratch. 3 Sentiment-Specific Word Embedding for Twitter Sentiment Classification In this section, we present the details of learning sentiment-specific word embedding (SSWE) for Twitter sentiment classification. We propose incorporat"
P14-1146,D13-1170,0,0.34155,"p-performed system in the Twitter sentiment classification track of SemEval 2013 (Nakov et al., 2013), using diverse sentiment lexicons and a variety of hand-crafted features. Feature engineering is important but laborintensive. It is therefore desirable to discover explanatory factors from the data and make the learning algorithms less dependent on extensive feature engineering (Bengio, 2013). For the task of sentiment classification, an effective feature learning method is to compose the representation of a sentence (or document) from the representations of the words or phrases it contains (Socher et al., 2013b; Yessenalina and Cardie, 2011). Accordingly, it is a crucial step to learn the word representation (or word embedding), which is a dense, low-dimensional and real-valued vector for a word. Although existing word embedding learning algorithms (Collobert et al., 2011; Mikolov et al., 2013) are intuitive choices, they are not effective enough if directly used for sentiment classification. The most serious problem is that traditional methods typically model the syntactic context of words but ignore the sentiment information of text. As a result, words with opposite polarity, such as good and bad"
P14-1146,J11-2001,0,0.172252,"ion, we present a brief review of the related work from two perspectives, Twitter sentiment classification and learning continuous representations for sentiment classification. 2.1 Twitter Sentiment Classification Twitter sentiment classification, which identifies the sentiment polarity of short, informal tweets, has attracted increasing research interest (Jiang et al., 2011; Hu et al., 2013) in recent years. Generally, the methods employed in Twitter sentiment classification follow traditional sentiment classification approaches. The lexicon-based approaches (Turney, 2002; Ding et al., 2008; Taboada et al., 2011; Thelwall et al., 2012) mostly use a dictionary of sentiment words with their associated sentiment polarity, and incorporate negation and intensification to compute the sentiment polarity for each sentence (or document). The learning based methods for Twitter sentiment classification follow Pang et al. (2002)’s work, which treat sentiment classification of texts as a special case of text categorization issue. Many studies on Twitter sentiment classification (Pak and Paroubek, 2010; Davidov et al., 2010; Barbosa and Feng, 2010; Kouloumpis et al., 2011; Zhao et al., 2012) leverage massive noisy"
P14-1146,P02-1053,0,0.0615851,"asks. 2 Related Work In this section, we present a brief review of the related work from two perspectives, Twitter sentiment classification and learning continuous representations for sentiment classification. 2.1 Twitter Sentiment Classification Twitter sentiment classification, which identifies the sentiment polarity of short, informal tweets, has attracted increasing research interest (Jiang et al., 2011; Hu et al., 2013) in recent years. Generally, the methods employed in Twitter sentiment classification follow traditional sentiment classification approaches. The lexicon-based approaches (Turney, 2002; Ding et al., 2008; Taboada et al., 2011; Thelwall et al., 2012) mostly use a dictionary of sentiment words with their associated sentiment polarity, and incorporate negation and intensification to compute the sentiment polarity for each sentence (or document). The learning based methods for Twitter sentiment classification follow Pang et al. (2002)’s work, which treat sentiment classification of texts as a special case of text categorization issue. Many studies on Twitter sentiment classification (Pak and Paroubek, 2010; Davidov et al., 2010; Barbosa and Feng, 2010; Kouloumpis et al., 2011;"
P14-1146,P12-2018,0,0.321669,"ve/negative) instead of 3-class Twitter sentiment classification (positive/negative/neutral) in SemEval2013. Baseline Methods. We compare our method with the following sentiment classification algorithms: (1) DistSuper: We use the 10 million tweets selected by positive and negative emoticons as training data, and build sentiment classifier with LibLinear and ngram features (Go et al., 2009). (2) SVM: The ngram features and Support Vector Machine are widely used baseline methods to build sentiment classifiers (Pang et al., 2002). LibLinear is used to train the SVM classifier. (3) NBSVM: NBSVM (Wang and Manning, 2012) is a state-of-the-art performer on many sentiment classification datasets, which trades-off between Naive Bayes and NB-enhanced SVM. (4) RAE: Recursive Autoencoder (Socher et al., 2011c) has been proven effective in many sentiment analysis tasks by learning compositionality automatically. We run RAE with randomly initialized word embedding. (5) NRC: NRC builds the top-performed system in SemEval 2013 Twitter sentiment classification track which incorporates diverse sentiment lexicons and many manually designed features. We re-implement this system because the codes are not publicly available"
P14-1146,H05-1044,0,0.100893,"he sentiment lexicon, P#Lex PN j=1 β(wi , cij ) i=1 Accuracy = (10) #Lex × N where #Lex is the number of words in the sentiment lexicon, wi is the i-th word in the lexicon, cij is the j-th closest word to wi in the lexicon with cosine similarity, β(wi , cij ) is an indicator function that is equal to 1 if wi and cij have the same sentiment polarity and 0 for the opposite case. The higher accuracy refers to a better polarity consistency of words in the sentiment lexicon. We set N as 100 in our experiment. Experiment Setup and Datasets We utilize the widely-used sentiment lexicons, namely MPQA (Wilson et al., 2005) and HL (Hu and Liu, 2004), to evaluate the quality of word embedding. For each lexicon, we remove the words that do not appear in the lookup table of word embedding. We only use unigram embedding in this section because these sentiment lexicons do not contain phrases. The distribution of the lexicons used in this paper is listed in Table 4. Lexicon HL MPQA Joint Positive 1,331 1,932 1,051 Negative 2,647 2,817 2,024 Total 3,978 4,749 3,075 Table 4: Statistics of the sentiment lexicons. Joint stands for the words that occur in both HL and MPQA with the same sentiment polarity. Results. Table 5"
P14-1146,D11-1016,0,0.0249497,"the Twitter sentiment classification track of SemEval 2013 (Nakov et al., 2013), using diverse sentiment lexicons and a variety of hand-crafted features. Feature engineering is important but laborintensive. It is therefore desirable to discover explanatory factors from the data and make the learning algorithms less dependent on extensive feature engineering (Bengio, 2013). For the task of sentiment classification, an effective feature learning method is to compose the representation of a sentence (or document) from the representations of the words or phrases it contains (Socher et al., 2013b; Yessenalina and Cardie, 2011). Accordingly, it is a crucial step to learn the word representation (or word embedding), which is a dense, low-dimensional and real-valued vector for a word. Although existing word embedding learning algorithms (Collobert et al., 2011; Mikolov et al., 2013) are intuitive choices, they are not effective enough if directly used for sentiment classification. The most serious problem is that traditional methods typically model the syntactic context of words but ignore the sentiment information of text. As a result, words with opposite polarity, such as good and bad, are mapped into close vectors."
P14-1146,D13-1061,0,0.152584,"representation (or word embedding), which is a dense, low-dimensional and real-valued vector for a word. Although existing word embedding learning algorithms (Collobert et al., 2011; Mikolov et al., 2013) are intuitive choices, they are not effective enough if directly used for sentiment classification. The most serious problem is that traditional methods typically model the syntactic context of words but ignore the sentiment information of text. As a result, words with opposite polarity, such as good and bad, are mapped into close vectors. It is meaningful for some tasks such as pos-tagging (Zheng et al., 2013) as the two words have similar usages and grammatical roles, but it becomes a disaster for sentiment analysis as they have the opposite sentiment polarity. In this paper, we propose learning sentimentspecific word embedding (SSWE) for sentiment analysis. We encode the sentiment information in1555 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1555–1565, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics to the continuous representation of words, so that it is able to separate good and bad to opposite ends"
P14-1146,P10-1040,0,\N,Missing
P15-1051,C10-3004,1,0.77884,"Missing"
P15-1051,D07-1074,0,0.205382,"Missing"
P15-1051,P13-2006,0,0.0290674,"Missing"
P15-1051,D11-1072,0,0.076523,"Missing"
P15-1051,C14-1087,1,0.243274,"ghdad hasChild CapitalQusay Hussein k2: SaddamkHussein Saddam Hussein hasChild Qusay Hussein k(b)2: Two omitted relevant pieces of background knowledge Figure 1: An example of document enrichment: A source document about a U.S. air strike omitting two important pieces of background knowledge which are acquired by our framework. triples from multiple sources, we also get better coverage. Therefore, one can expect that this representation is helpful for better document enrichment by incorporating both accuracy and coverage. In fact, there is already evidence that this representation is helpful. Zhang et al. (2014) proposed a triple-based document enrichment framework which uses triples of SPO as background knowledge. They first proposed a search engine– based method to evaluate the relatedness between every pair of triples, and then an iterative propagation algorithm was introduced to select the most relevant triples to a given source document (see Section 2), which achieved a good performance. However, to evaluate the semantic relatedness between two triples, Zhang et al. (2014) primarily relied on the text of triples and used search engines, which makes their method difficult to re-implement and in t"
P15-1051,J00-3005,0,0.0121463,"Missing"
P15-1051,nastase-etal-2010-wikinet,0,0.0788196,"Missing"
P15-1051,N10-1072,0,0.0604509,"Missing"
P15-1051,P11-1009,0,0.0688529,"Missing"
P15-1051,P09-1077,0,0.0248825,"Missing"
P15-1098,D10-1115,0,0.00705842,"rder to model user-text consistency, we represent each user as a continuous matrix Uk ∈ RdU ×d , which acts as an operator to modify the semantic meaning of a word. This is on the basis of vector based semantic composition (Mitchell and Lapata, 2010). They regard compositional modifier as a matrix X1 to modify another component x2 , and use matrix-vector multiplication y = X1 × x2 as the composition function. Multiplicative semantic composition is suitable for our need of user modifying word meaning, and it has been successfully utilized to model adjectivenoun composition (Clark et al., 2008; Baroni and Zamparelli, 2010) and adverb-adjective composition (Socher et al., 2012). Similarly, we model product-text consistency by encoding each product as a matrix Pj ∈ RdP ×d , where d is the dimension of word vector, dP is the output length of product-word multiplicative composition. After conducting user-word multiplication and productword multiplication operations, we concatenate their outputs and feed them to CNN (detailed in Section 3.1) for producing user and product enhanced document representation. Sentiment Classification We apply UPNN to document level sentiment classification under a supervised learning fr"
P15-1098,I13-1156,0,0.266074,"lassifier and achieve better performance2 . In this paper, we propose a new model dubbed User Product Neural Network (UPNN) to capture user- and product-level information for sentiment classification of documents (e.g. reviews). UPNN takes as input a variable-sized document as well as the user who writes the review and the product which is evaluated. It outputs sentiment polarity label of a document. Users and products are encoded in continuous vector spaces, the representations of which capture important global clues such 2 One can manually design a small number of user and product features (Gao et al., 2013). However, we argue that they are not effective enough to capture sophisticated semantics of users and products. 1014 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 1014–1023, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics as user preferences and product qualities. These representations are further integrated with continuous text representation in a unified neural framework for sentiment classification. We apply UPNN to three datasets der"
P15-1098,W06-3808,0,0.0209607,"earns tailored representation for each user and product. We evaluate classification accuracy on the extracted OOV test set. Experimental results are given in Figure 5. We can find that these two strategies perform slightly better than UPNN (no UP), but still worse than the full model. 5 5.1 Related Work Sentiment Classification Sentiment classification is a fundamental problem in sentiment analysis, which targets at inferring the sentiment label of a document. Pang and Lee (2002; 2005) cast this problem a classification task, and use machine learning method in a supervised learning framework. Goldberg and Zhu (2006) use unlabelled reviews in a graphbased semi-supervised learning method. Many studies design effective features, such as text topic (Ganu et al., 2009), bag-of-opinion (Qu et al., 2010) and sentiment lexicon features (Kiritchenko et al., 2014). User information is also used for sentiment classification. Gao et al. (2013) design user-specific features to capture user leniency. Li et al. (2014) incorporate textual topic and user-word factors with supervised topic modeling. Tan et al. (2011) and Hu et al. (2013) utilize usertext and user-user relations for Twitter sentiment analysis. Unlike most"
P15-1098,P14-1062,0,0.13981,"rd this problem as a multi-class classification task. They usually ∗ Corresponding author. 1 The codes and datasets are available at http://ir. hit.edu.cn/˜dytang/ use machine learning algorithms, and build sentiment classifier from documents with accompanying sentiment labels. Since the performance of a machine learner is heavily dependent on the choice of data representations (Domingos, 2012), many works focus on designing effective features (Pang et al., 2002; Qu et al., 2010; Kiritchenko et al., 2014) or learning discriminative features from data with neural networks (Socher et al., 2013; Kalchbrenner et al., 2014; Le and Mikolov, 2014). Despite the apparent success of neural network methods, they typically only use text information while ignoring the important influences of users and products. Let us take reviews with respect to 1-5 rating scales as an example. A critical user might write a review “it works great” and mark 4 stars, while a lenient user might give 5 stars even if he posts an (almost) identical review. In this case, user preference affects the sentiment rating of a review. Product quality also has an impact on review sentiment rating. Reviews towards high-quality products (e.g. Macbook)"
P15-1098,D14-1181,0,0.0387917,"m distribution, regarded as a parameter and jointly trained with other parameters of neural networks. Alternatively, they can be pretrained from text corpus with embedding learning algorithms (Mikolov et al., 2013; Pennington et al., 2014; Tang et al., 2014b), and applied as initial values of word embedding matrix. We adopt the latter strategy which better exploits the semantic and grammatical associations of words. To model semantic representations of sentences, convolutional neural network (CNN) and recursive neural network (Socher et al., 2013) are two state-of-the-art methods. We use CNN (Kim, 2014; Kalchbrenner et al., 2014) in this work as it does not rely on external parse tree. Specifically, we use multiple convolutional filters with different widths to produce sentence representation. The reason is that they are capable of capturing local semantics of n-grams of various granularities, which are proven powerful for sentiment classification. The convolutional filter with a width of 3 essentially captures the semantics of trigrams in a sentence. Accordingly, multiple convolutional filters with widths of 1, 2 and 3 encode the semantics of unigrams, bigrams and trigrams in a sentence. A"
P15-1098,P13-2087,0,0.0199465,"n that we encode four kinds of consistencies and use neural network approach. User representation is also leveraged for recommendation (Weston et al., 2013), web search (Song et al., 2014) and social media analytics (Perozzi et al., 2014). 5.2 Neural Network for Sentiment Classification Neural networks have achieved promising results for sentiment classification. Existing neural network methods can be divided into two groups: word embedding and semantic composition. For learning word embeddings, (Mikolov et al., 2013; Pennington et al., 2014) use local and global contexts, (Maas et al., 2011; Labutov and Lipson, 2013; Tang et al., 2014b; Tang et al., 2014a; Zhou et al., 2015) further incorporate sentiment of texts. For learning semantic composition, Glorot et al. (2011) use stacked denoising autoencoder, Socher et al. (2013) introduce a family of recursive deep neural networks (RNN). RNN is extended with adaptive composition functions (Dong et al., 2014), global feedbackward (Paulus et al., 2014), feature weight tuning (Li, 2014), and also used for opinion relation detection (Xu et al., 2014). Li et al. (2015) compare the effectiveness of recursive neural network and recurrent neural network on five NLP t"
P15-1098,D14-1162,0,0.111996,"sentations of user k and product j for capturing user-text and product-text consistencies. ous and real-valued vector, also known as word embedding (Bengio et al., 2003). All the word vectors are stacked in a word embedding matrix Lw ∈ Rd×|V |, where d is the dimension of word vector and |V |is the size of word vocabulary. These word vectors can be randomly initialized from a uniform distribution, regarded as a parameter and jointly trained with other parameters of neural networks. Alternatively, they can be pretrained from text corpus with embedding learning algorithms (Mikolov et al., 2013; Pennington et al., 2014; Tang et al., 2014b), and applied as initial values of word embedding matrix. We adopt the latter strategy which better exploits the semantic and grammatical associations of words. To model semantic representations of sentences, convolutional neural network (CNN) and recursive neural network (Socher et al., 2013) are two state-of-the-art methods. We use CNN (Kim, 2014; Kalchbrenner et al., 2014) in this work as it does not rely on external parse tree. Specifically, we use multiple convolutional filters with different widths to produce sentence representation. The reason is that they are capab"
P15-1098,C10-1103,0,0.102162,"ity or intensity (e.g. 1-5 or 1-10 stars on review sites) of a document. Dominating studies follow Pang et al. (2002; 2005) and regard this problem as a multi-class classification task. They usually ∗ Corresponding author. 1 The codes and datasets are available at http://ir. hit.edu.cn/˜dytang/ use machine learning algorithms, and build sentiment classifier from documents with accompanying sentiment labels. Since the performance of a machine learner is heavily dependent on the choice of data representations (Domingos, 2012), many works focus on designing effective features (Pang et al., 2002; Qu et al., 2010; Kiritchenko et al., 2014) or learning discriminative features from data with neural networks (Socher et al., 2013; Kalchbrenner et al., 2014; Le and Mikolov, 2014). Despite the apparent success of neural network methods, they typically only use text information while ignoring the important influences of users and products. Let us take reviews with respect to 1-5 rating scales as an example. A critical user might write a review “it works great” and mark 4 stars, while a lenient user might give 5 stars even if he posts an (almost) identical review. In this case, user preference affects the sen"
P15-1098,D15-1278,0,0.0128866,"t al., 2013; Pennington et al., 2014) use local and global contexts, (Maas et al., 2011; Labutov and Lipson, 2013; Tang et al., 2014b; Tang et al., 2014a; Zhou et al., 2015) further incorporate sentiment of texts. For learning semantic composition, Glorot et al. (2011) use stacked denoising autoencoder, Socher et al. (2013) introduce a family of recursive deep neural networks (RNN). RNN is extended with adaptive composition functions (Dong et al., 2014), global feedbackward (Paulus et al., 2014), feature weight tuning (Li, 2014), and also used for opinion relation detection (Xu et al., 2014). Li et al. (2015) compare the effectiveness of recursive neural network and recurrent neural network on five NLP tasks including sentiment classification. (Kalchbrenner et al., 2014; Kim, 2014; Johnson and Zhang, 2014) use convolutional neural networks. Le and Mikolov (2014) introduce 1021 Paragraph Vector. Unlike existing neural network approaches that only use the semantics of texts, we take consideration of user and product representations and leverage their connections with text semantics for sentiment classification. This work is an extension of our previous work (Tang et al., 2015), which only takes cons"
P15-1098,D12-1110,0,0.0623837,"a continuous matrix Uk ∈ RdU ×d , which acts as an operator to modify the semantic meaning of a word. This is on the basis of vector based semantic composition (Mitchell and Lapata, 2010). They regard compositional modifier as a matrix X1 to modify another component x2 , and use matrix-vector multiplication y = X1 × x2 as the composition function. Multiplicative semantic composition is suitable for our need of user modifying word meaning, and it has been successfully utilized to model adjectivenoun composition (Clark et al., 2008; Baroni and Zamparelli, 2010) and adverb-adjective composition (Socher et al., 2012). Similarly, we model product-text consistency by encoding each product as a matrix Pj ∈ RdP ×d , where d is the dimension of word vector, dP is the output length of product-word multiplicative composition. After conducting user-word multiplication and productword multiplication operations, we concatenate their outputs and feed them to CNN (detailed in Section 3.1) for producing user and product enhanced document representation. Sentiment Classification We apply UPNN to document level sentiment classification under a supervised learning framework (Pang and Lee, 2005). Instead of using handcraf"
P15-1098,P11-1015,0,0.0913889,"Li et al. (2014) in that we encode four kinds of consistencies and use neural network approach. User representation is also leveraged for recommendation (Weston et al., 2013), web search (Song et al., 2014) and social media analytics (Perozzi et al., 2014). 5.2 Neural Network for Sentiment Classification Neural networks have achieved promising results for sentiment classification. Existing neural network methods can be divided into two groups: word embedding and semantic composition. For learning word embeddings, (Mikolov et al., 2013; Pennington et al., 2014) use local and global contexts, (Maas et al., 2011; Labutov and Lipson, 2013; Tang et al., 2014b; Tang et al., 2014a; Zhou et al., 2015) further incorporate sentiment of texts. For learning semantic composition, Glorot et al. (2011) use stacked denoising autoencoder, Socher et al. (2013) introduce a family of recursive deep neural networks (RNN). RNN is extended with adaptive composition functions (Dong et al., 2014), global feedbackward (Paulus et al., 2014), feature weight tuning (Li, 2014), and also used for opinion relation detection (Xu et al., 2014). Li et al. (2015) compare the effectiveness of recursive neural network and recurrent ne"
P15-1098,P14-5010,0,0.0030416,"atasets used for sentiment classification. The rating scale of IMDB dataset is 1-10. The rating scale of Yelp 2014 and Yelp 2013 datasets is 1-5. |V |is the vocabulary size of words in each dataset. #users is the number of users, #docs/user means the average number of documents per user posts in the corpus. et al., 2014) and Yelp Dataset Challenge4 in 2013 and 2014. Statistical information of the generated datasets are given in Table 1. We split each corpus into training, development and testing sets with a 80/10/10 split, and conduct tokenization and sentence splitting with Stanford CoreNLP (Manning et al., 2014). We use standard accuracy (Manning and Sch¨utze, 1999; Jurafsky and Martin, 2000) to measure the overall sentiment classification performance, and use M AE and RM SE to measure the divergences between predicted sentiment ratings (pr) and ground truth ratings (gd). P |gdi − pri | M AE = i (3) N rP 2 i (gdi − pri ) RM SE = (4) N 4.2 Baseline Methods We compare UPNN with the following baseline methods for document-level sentiment classification. (1) Majority is a heuristic baseline method, which assigns the majority sentiment category in training set to each review in the test dataset. (2) In Tr"
P15-1098,P05-1015,0,0.0327322,"rb-adjective composition (Socher et al., 2012). Similarly, we model product-text consistency by encoding each product as a matrix Pj ∈ RdP ×d , where d is the dimension of word vector, dP is the output length of product-word multiplicative composition. After conducting user-word multiplication and productword multiplication operations, we concatenate their outputs and feed them to CNN (detailed in Section 3.1) for producing user and product enhanced document representation. Sentiment Classification We apply UPNN to document level sentiment classification under a supervised learning framework (Pang and Lee, 2005). Instead of using handcrafted features, we use continuous representation of documents, users and products as discriminative features. The sentiment classifier is built from documents with gold standard sentiment labels. As is shown in Figure 2, the feature representation for building rating predictor is the concatenation of three parts: continuous user representation uk , continuous product representation pj and continuous document representation vd , where vd encodes user-text consistency, product-text consistency and document level semantic composition. We use sof tmax to build the classifi"
P15-1098,W02-1011,0,0.0331478,"l evidence in turn facilitates embedding learning procedure at document level, yielding better text representations. By combining evidence at user-, product- and documentlevel in a unified neural framework, the proposed model achieves state-of-the-art performances on IMDB and Yelp datasets1 . 1 Introduction Document-level sentiment classification is a fundamental problem in the field of sentiment analysis and opinion mining (Pang and Lee, 2008; Liu, 2012). The task is to infer the sentiment polarity or intensity (e.g. 1-5 or 1-10 stars on review sites) of a document. Dominating studies follow Pang et al. (2002; 2005) and regard this problem as a multi-class classification task. They usually ∗ Corresponding author. 1 The codes and datasets are available at http://ir. hit.edu.cn/˜dytang/ use machine learning algorithms, and build sentiment classifier from documents with accompanying sentiment labels. Since the performance of a machine learner is heavily dependent on the choice of data representations (Domingos, 2012), many works focus on designing effective features (Pang et al., 2002; Qu et al., 2010; Kiritchenko et al., 2014) or learning discriminative features from data with neural networks (Soche"
P15-1098,D13-1170,0,0.476576,"(2002; 2005) and regard this problem as a multi-class classification task. They usually ∗ Corresponding author. 1 The codes and datasets are available at http://ir. hit.edu.cn/˜dytang/ use machine learning algorithms, and build sentiment classifier from documents with accompanying sentiment labels. Since the performance of a machine learner is heavily dependent on the choice of data representations (Domingos, 2012), many works focus on designing effective features (Pang et al., 2002; Qu et al., 2010; Kiritchenko et al., 2014) or learning discriminative features from data with neural networks (Socher et al., 2013; Kalchbrenner et al., 2014; Le and Mikolov, 2014). Despite the apparent success of neural network methods, they typically only use text information while ignoring the important influences of users and products. Let us take reviews with respect to 1-5 rating scales as an example. A critical user might write a review “it works great” and mark 4 stars, while a lenient user might give 5 stars even if he posts an (almost) identical review. In this case, user preference affects the sentiment rating of a review. Product quality also has an impact on review sentiment rating. Reviews towards high-qual"
P15-1098,C14-1018,1,0.783389,"ernational Joint Conference on Natural Language Processing, pages 1014–1023, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics as user preferences and product qualities. These representations are further integrated with continuous text representation in a unified neural framework for sentiment classification. We apply UPNN to three datasets derived from IMDB and Yelp Dataset Challenge. We compare to several neural network models including recursive neural networks (Socher et al., 2013), paragraph vector (Le and Mikolov, 2014), sentimentspecific word embedding (Tang et al., 2014b), and a state-of-the-art recommendation algorithm JMARS (Diao et al., 2014). Experimental results show that: (1) UPNN outperforms baseline methods for sentiment classification of documents; (2) incorporating representations of users and products significantly improves classification accuracy. The main contributions of this work are as follows: • We present a new neural network method (UPNN) by leveraging users and products for document-level sentiment classification. • We validate the influences of users and products in terms of sentiment and text on massive IMDB and Yelp reviews. • We repor"
P15-1098,P14-1146,1,0.652646,"ernational Joint Conference on Natural Language Processing, pages 1014–1023, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics as user preferences and product qualities. These representations are further integrated with continuous text representation in a unified neural framework for sentiment classification. We apply UPNN to three datasets derived from IMDB and Yelp Dataset Challenge. We compare to several neural network models including recursive neural networks (Socher et al., 2013), paragraph vector (Le and Mikolov, 2014), sentimentspecific word embedding (Tang et al., 2014b), and a state-of-the-art recommendation algorithm JMARS (Diao et al., 2014). Experimental results show that: (1) UPNN outperforms baseline methods for sentiment classification of documents; (2) incorporating representations of users and products significantly improves classification accuracy. The main contributions of this work are as follows: • We present a new neural network method (UPNN) by leveraging users and products for document-level sentiment classification. • We validate the influences of users and products in terms of sentiment and text on massive IMDB and Yelp reviews. • We repor"
P15-1098,C14-1064,0,0.0191227,"ddings, (Mikolov et al., 2013; Pennington et al., 2014) use local and global contexts, (Maas et al., 2011; Labutov and Lipson, 2013; Tang et al., 2014b; Tang et al., 2014a; Zhou et al., 2015) further incorporate sentiment of texts. For learning semantic composition, Glorot et al. (2011) use stacked denoising autoencoder, Socher et al. (2013) introduce a family of recursive deep neural networks (RNN). RNN is extended with adaptive composition functions (Dong et al., 2014), global feedbackward (Paulus et al., 2014), feature weight tuning (Li, 2014), and also used for opinion relation detection (Xu et al., 2014). Li et al. (2015) compare the effectiveness of recursive neural network and recurrent neural network on five NLP tasks including sentiment classification. (Kalchbrenner et al., 2014; Kim, 2014; Johnson and Zhang, 2014) use convolutional neural networks. Le and Mikolov (2014) introduce 1021 Paragraph Vector. Unlike existing neural network approaches that only use the semantics of texts, we take consideration of user and product representations and leverage their connections with text semantics for sentiment classification. This work is an extension of our previous work (Tang et al., 2015), whi"
P16-2011,P15-2047,1,0.12748,"as alternatives. 2.2 Output 2.4 Training In our model, the loss function is the cross-entropy error of event trigger identification and trigger classification. We initialize all parameters to form a uniform distribution U (−0.01, 0.01). We set the widths of convolutional filters as 2 and 3. The number of feature maps is 300 and the dimension of the PF is 5. Table 1 illustrates the setting parameters used for three languages in our experiments (Zeiler, 2012). Convolution Neural Network 3 As the convolutional neural network (CNN) is good at capturing salient features from a sequence of objects (Liu et al., 2015), we design a CNN to capture some local chunks. This approach has been used for event detection in previous studies (Nguyen and Grishman, 2015; Chen et al., 2015). Specifically, we use multiple convolutional filters with different widths to produce local context representation. The reason is that they are capable of capturing local semantics of n-grams of various granularities, which are proven powerful for event detection. In our work, multiple convolutional filters with widths of 2 and 3 encode the semantics of bigrams and trigrams in a sentence. This local information can also help our mode"
P16-2011,R15-1010,0,0.0363436,"and syntactic features, thus the precision is lower than neural network based methods. (4) RNN and LSTM perform slightly worse than Bi-LSTM. An obvious reason is that RNN and LSTM only consider the preceding sequence information of the trigger, which may miss some important following clues. Considering S1 again, when extracting the trigger “releases”, both models will miss the following sequence “20 million euros to Iraq”. This may seriously hinder the performance of RNN and LSTM for event detection. Table 2: Comparison of different methods on English event detection. (5) Pattern Recognition (Miao and Grishman, 2015), using a pattern expansion technique to extract event triggers. (6) Convolutional Neural Network (Chen et al., 2015), which exploits a dynamic multi-pooling convolutional neural network for event trigger detection. 3.2 Comparison On English Table 2 shows the overall performance of all methods on the ACE2005 English corpus. We can see that our approach significantly outperforms all previous methods. The better performance of HNN can be further explained by the following reasons: (1) Compared with feature based methods, such as MaxEnt, Cross-Event, Cross-Entity, and Joint Model, neural network"
P16-2011,C12-1033,0,0.0193323,"Missing"
P16-2011,P15-1017,0,0.563445,"t trigger or not. Specifically, we first use a Bi-LSTM to encode semantics of each word with its preceding and following information. Then, we add a convolutional neural network to capture structure information from local contexts. 2.1 Bi-LSTM In this section we describe a Bidirectional LSTM model for event detection. Bi-LSTM is a type of bidirectional recurrent neural networks (RNN), which can simultaneously model word representation with its preceding and following information. Word representations can be naturally considered as features to detect triggers and their event types. As show in (Chen et al., 2015), we take all the words of the whole sentence as the input and each token is transformed by looking up word embeddings. Specifically, we use the SkipGram model to pre-train the word embeddings to represent each word (Mikolov et al., 2013; Bahdanau et al., 2014). We present the details of Bi-LSTM for event trigger extraction in Figure 2. We can see that Bi-LSTM is composed of two LSTM neural networks, a forward LSTMF to model the preced67 vector with fixed length. C3 Max-Pooling Feature Map 1 Feature Map 2 Lookup 2.3 Feature Map n ... Convolution ... At the end, we concatenate the bidirectional"
P16-2011,P15-2060,0,0.548592,"d trigger classification. We initialize all parameters to form a uniform distribution U (−0.01, 0.01). We set the widths of convolutional filters as 2 and 3. The number of feature maps is 300 and the dimension of the PF is 5. Table 1 illustrates the setting parameters used for three languages in our experiments (Zeiler, 2012). Convolution Neural Network 3 As the convolutional neural network (CNN) is good at capturing salient features from a sequence of objects (Liu et al., 2015), we design a CNN to capture some local chunks. This approach has been used for event detection in previous studies (Nguyen and Grishman, 2015; Chen et al., 2015). Specifically, we use multiple convolutional filters with different widths to produce local context representation. The reason is that they are capable of capturing local semantics of n-grams of various granularities, which are proven powerful for event detection. In our work, multiple convolutional filters with widths of 2 and 3 encode the semantics of bigrams and trigrams in a sentence. This local information can also help our model fix some errors due to lexical ambiguity. An illustration of CNN with three convolutional filters is given in Figure 3. Let us denote a sent"
P16-2011,P11-1113,0,0.595906,"ection aims to extract event triggers (most often a single verb or noun) and classify them into specific types precisely. It is a crucial and quite challenging sub-task of event extraction, because the same event might appear in the form of various trigger expressions and an expression might represent different event types in different contexts. Figure 1 shows two examples. In S1, “release” is a verb concept and a trigger for “Transfer-Money” event, while in S2, “release ” is a noun concept and a trigger for “Release-Parole” event. Most of previous methods (Ji et al., 2008; Liao et al., 2010; Hong et al., 2011; Li et al., 2013; Li et al., 2015b) considered event detection as a classi66 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 66–71, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics Event Trigger SoftMax Concatenate with CNN LSTMB LSTMF LSTM LSTM BV ...... ...... LSTM LSTM LSTM LSTM LSTM C2 FV ...... LSTM LSTM LSTM LSTM LSTM C3 LSTM LSTM LSTM LSTM Look up The European Unit will release 20 million euros … Figure 2: An illustration of our model for event trigger extraction (here the trigger candidate is “release”)"
P16-2011,P08-1030,1,0.898397,"Missing"
P16-2011,D15-1167,1,0.218601,"it will release 20 million euros … Figure 2: An illustration of our model for event trigger extraction (here the trigger candidate is “release”). Fv and Bv are the output of Bi-LSTM and C2 , C3 are the output of CNN with convolutional filters with widths of 2 and 3. use of word embeddings to induce a more general representation for trigger candidates. Recently, deep learning techniques have been widely used in modeling complex structures and proven effective for many NLP tasks, such as machine translation (Bahdanau et al., 2014), relation extraction (Zeng et al., 2014) and sentiment analysis (Tang et al., 2015a). Bi-directional long short-term memory (Bi-LSTM) model (Schuster et al., 1997) is a two-way recurrent neural network (RNN) (Mikolov et al., 2010) which can capture both the preceding and following context information of each word. Convolutional neural network (CNN) (LeCun et al., 1995) is another effective model for extracting semantic representations and capturing salient features in a flat structure (Liu et al., 2015), such as chunks. In this work, we develop a hybrid neural network incorporating two types of neural networks: Bi-LSTM and CNN, to model both sequence and chunk information f"
P16-2011,P14-1038,1,0.0783646,"e (F). Table 1 shows the detailed description of the data sets used in our experiments. We abbreviate our model as HNN (Hybrid Neural Networks). 3.1 Baseline Methods We compare our approach with the following baseline methods. (1) MaxEnt, a basesline feature-based method, which trains a Maximum Entropy classifier with some lexical and syntactic features (Ji et al., 2008). (2) Cross-Event (Liao et al., 2010), using document-level information to improve the performance of ACE event extraction. (3) Cross-Entity (Hong et al., 2011), extracting events using cross-entity inference. (4) Joint Model (Li and Ji, 2014), a joint structured perception approach, incorporating multilevel linguistic features to extract event triggers and arguments at the same time so that local predictions can be mutually improved. 68 Language English Chinese Spanish Word Embedding corpus dim NYT 300 Gigaword 300 Gigaword 300 Gradient Learning Method method parameters SGD learning rate r = 0.03 Adadelta p = 0.95, δ = 1e−6 Adadelta p = 0.95, δ = 1e−6 Corpus ACE2005 ACE2005 ERE Data Sets Train Dev 529 30 513 60 93 12 Test 40 60 12 Table 1: Hyperparameters and # of documents used in our experiments on three languages. Model MaxEnt"
P16-2011,P13-1008,1,0.945252,"act event triggers (most often a single verb or noun) and classify them into specific types precisely. It is a crucial and quite challenging sub-task of event extraction, because the same event might appear in the form of various trigger expressions and an expression might represent different event types in different contexts. Figure 1 shows two examples. In S1, “release” is a verb concept and a trigger for “Transfer-Money” event, while in S2, “release ” is a noun concept and a trigger for “Release-Parole” event. Most of previous methods (Ji et al., 2008; Liao et al., 2010; Hong et al., 2011; Li et al., 2013; Li et al., 2015b) considered event detection as a classi66 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 66–71, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics Event Trigger SoftMax Concatenate with CNN LSTMB LSTMF LSTM LSTM BV ...... ...... LSTM LSTM LSTM LSTM LSTM C2 FV ...... LSTM LSTM LSTM LSTM LSTM C3 LSTM LSTM LSTM LSTM Look up The European Unit will release 20 million euros … Figure 2: An illustration of our model for event trigger extraction (here the trigger candidate is “release”). Fv and Bv are t"
P16-2011,D15-1278,0,0.0128362,"Missing"
P16-2011,C14-1220,0,0.0106635,"Missing"
P16-2011,P15-1107,0,0.0217978,"s (most often a single verb or noun) and classify them into specific types precisely. It is a crucial and quite challenging sub-task of event extraction, because the same event might appear in the form of various trigger expressions and an expression might represent different event types in different contexts. Figure 1 shows two examples. In S1, “release” is a verb concept and a trigger for “Transfer-Money” event, while in S2, “release ” is a noun concept and a trigger for “Release-Parole” event. Most of previous methods (Ji et al., 2008; Liao et al., 2010; Hong et al., 2011; Li et al., 2013; Li et al., 2015b) considered event detection as a classi66 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 66–71, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics Event Trigger SoftMax Concatenate with CNN LSTMB LSTMF LSTM LSTM BV ...... ...... LSTM LSTM LSTM LSTM LSTM C2 FV ...... LSTM LSTM LSTM LSTM LSTM C3 LSTM LSTM LSTM LSTM Look up The European Unit will release 20 million euros … Figure 2: An illustration of our model for event trigger extraction (here the trigger candidate is “release”). Fv and Bv are the output of Bi-L"
P16-2011,P10-1081,0,0.353297,"Missing"
P17-4003,D09-1026,0,0.0138392,"4: The framework of the proposed LTS model for response generation. versation state tracking as inputs and estimates the triggered domain distribution using a convolutional neural network. In Benben, we proposed a topic augmented convolutional neural network to integrate the continuous word representations and the discrete topic information into a unified framework for domain selection. Figure 3 shows the framework of the proposed topic augmented convolutional neural network for domain selection. The word embedding matrix and the topic matrix are obtained using the word2vec6 and Labeled LDA (Ramage et al., 2009), respectively. The two representations of the input conversation utterance are combined in the full connection layer and output the domain triggered distribution. At last, the domains whose triggered probabilities are larger than a threshold are selected to execute the following domain processing step. Note that after the domain selection step, there may be one or more triggered domains. If there is no domain to be triggered, the conversation state is updated and then sent to the response generation module. tracker records the historical content, the current domain and the historically trigge"
P17-4003,C10-3004,1,0.744957,"e seen, the architecture of Benben can be corresponded to the classic architecture of spoken dialogue systems (Young et al., 2013). Concretely, the natural language understanding, dialogue management and natural language generation in spoken dialogue systems are corresponding to the 1), 2) and 3), 4) components of the Benben architecture, respectively. We will next detail each component in the following sections. 2.1 Language Understanding The user input can be either text or speech. Therefore, the first step is to understand both the speech transcription and text. In Benben, the LTP toolkit (Che et al., 2010) is utilized to the basic language processing, including Chinese word segmentation, part-of-speech tagging, word sense disambiguation, named entity recognition, dependency parsing, semantic role labelling and semantic 5 2.2 Conversation State Tracking After the language understanding step, an input sentence is transferred to several feature representations. These feature representations are then taken as the inputs of the conversation state tracking and domain selection. The conversation state http://www.ltp-cloud.com 14 Word Embedding Matrix yt yt-1 y2 y1 st st-1 s2 s1 Word2Vec y0 =g(c,E) Ini"
P17-4003,P17-1010,1,\N,Missing
P18-1034,Q13-1005,0,0.0484716,"udies differ from (1) the form of the knowledge base, e.g. facts from Freebase, a table (or relational database), an image (Suhr et al., 2017; Johnson et al., 2017; Hu et al., 2017; Goldman et al., 2017) or a world state (Long et al., 2016); (2) the programming language, e.g. first-order logic, lambda calculus, lambda DCS, SQL, parameterized neural programmer (Yin et al., 2015; Neelakantan et al., 2016), or coupled distributed and symbolic executors (Mou et al., 2017); (3) the supervision used for learning the semantic parser, e.g. question-denotation pairs, binary correct/incorrect feedback (Artzi and Zettlemoyer, 2013), or richer supervision of question-logical form pairs (Dong and Lapata, 2016). In this work, we study semantic parsing over tables, which is critical for users to access relational databases with natural language, and could serve users’ in362 ability distribution of which is calculated with an attention mechanism (Bahdanau et al., 2015). The probability of generating the i-th word xi in the input sequence at the t-th time step is calculated as Equation 1, where hdec t is the decoder hidden state at the t-th time step, henc is the encoder hidden i state of the word xi , Wa is the model paramet"
P18-1034,D13-1160,0,0.522838,"Missing"
P18-1034,D14-1179,0,0.0149909,"Missing"
P18-1034,H94-1010,0,0.425333,"he input, and outputs a SQL query y. We do not consider the join operation over multiple relational tables, which we leave in the future work. We use WikiSQL (Zhong et al., 2017), the largest hand-annotated semantic parsing dataset to date which consists of 87,726 questions and SQL queries distributed across 26,375 tables from Wikipedia. 3 formation need for structured data on the web. We use SQL as the programming language, which has a broad acceptance to programmers. Natural Language Interface for Databases. Our work relates to the area of accessing database with natural language interface (Dahl et al., 1994; Brad et al., 2017). Popescu et al. (2003) use a parser to parse the question, and then use lexicon matching between question and the table column names/cells. Giordani and Moschitti (2012) parse the question with dependency parser, compose candidate SQL queries with heuristic rules, and use kernel based SVM ranker to rank the results. Li and Jagadish (2014) translate natural language utterances into SQL queries based on dependency parsing results, and interact with users to ensure the correctness of the interpretation process. Yaghmazadeh et al. (2017) build a semantic parser on the top of S"
P18-1034,P17-1174,1,0.889119,"Missing"
P18-1034,N16-1024,0,0.0300555,"imum likelihood estimation (MLE) (Guu et al., 2017; Mou et al., 2017; Liang et al., 2017). However, the focus of this work is the design of the neural architecture, despite we also implement an RL strategy (refer to §4.4). Structure/Grammar Guided Neural Decoder Our approach could be viewed as an extension of the sequence-to-sequence learning (Sutskever et al., 2014; Bahdanau et al., 2015) with a tailored neural decoder driven by the characteristic of the target language (Yin and Neubig, 2017; Rabinovich et al., 2017). Methods with similar intuitions have been developed for language modeling (Dyer et al., 2016), neural machine translation (Wu et al., 2017) and lambda calculus based semantic parsing (Dong and Lapata, 2016; Krishnamurthy et al., 2017). The difference is that our model is developed for sequence-to-SQL generation, in which table structure and SQL syntax are considered. 4 enc p(yt = xi |y&lt;t , x) ∝ exp(Wa [hdec t ; hi ]) It is worth to note that if a column name consists of multiple words (such as “original artist” in Figure 1), these words are separated in the input sequence. The approach has no guarantee that a multi-word column name could be successively generated, which would affect t"
P18-1034,P17-1089,0,0.106833,"mn names/cells. Giordani and Moschitti (2012) parse the question with dependency parser, compose candidate SQL queries with heuristic rules, and use kernel based SVM ranker to rank the results. Li and Jagadish (2014) translate natural language utterances into SQL queries based on dependency parsing results, and interact with users to ensure the correctness of the interpretation process. Yaghmazadeh et al. (2017) build a semantic parser on the top of SEMPRE (Pasupat and Liang, 2015) to get a SQL sketch, which only has the SQL shape and will be subsequently completed based on the table content. Iyer et al. (2017) maps utterances to SQL queries through sequence-tosequence learning. User feedbacks are incorporated to reduce the number of queries to be labeled. Zhong et al. (2017) develop an augmented pointer network, which is further improved with reinforcement learning for SQL sequence prediction. Xu et al. (2017) adopts a sequence-to-set model to predict WHERE columns, and uses an attentional model to predict the slots in where clause. Different from (Iyer et al., 2017; Zhong et al., 2017), our approach leverages SQL syntax and table structure. Compared to (Popescu et al., 2003; Giordani and Moschitti"
P18-1034,C12-2040,0,0.0387214,", the largest hand-annotated semantic parsing dataset to date which consists of 87,726 questions and SQL queries distributed across 26,375 tables from Wikipedia. 3 formation need for structured data on the web. We use SQL as the programming language, which has a broad acceptance to programmers. Natural Language Interface for Databases. Our work relates to the area of accessing database with natural language interface (Dahl et al., 1994; Brad et al., 2017). Popescu et al. (2003) use a parser to parse the question, and then use lexicon matching between question and the table column names/cells. Giordani and Moschitti (2012) parse the question with dependency parser, compose candidate SQL queries with heuristic rules, and use kernel based SVM ranker to rank the results. Li and Jagadish (2014) translate natural language utterances into SQL queries based on dependency parsing results, and interact with users to ensure the correctness of the interpretation process. Yaghmazadeh et al. (2017) build a semantic parser on the top of SEMPRE (Pasupat and Liang, 2015) to get a SQL sketch, which only has the SQL shape and will be subsequently completed based on the table content. Iyer et al. (2017) maps utterances to SQL que"
P18-1034,P17-1167,0,0.0492084,"Missing"
P18-1034,P18-1168,0,0.060629,"Missing"
P18-1034,P16-1154,0,0.051477,"etworks, and then present our approach that considers the table structure and the SQL syntax. 4.1 (1) Background: Pointer Networks Pointer networks is originally introduced by (Vinyals et al., 2015), which takes a sequence of elements as the input and outputs a sequence of discrete tokens corresponding to positions in the input sequence. The approach has been successfully applied in reading comprehension (Kadlec et al., 2016) for pointing to the positions of answer span from the document, and also in sequenceto-sequence based machine translation (Gulcehre et al., 2016) and text summarization (Gu et al., 2016) for replicating rare words from the source sequence to the target sequence. The approach of Zhong et al. (2017) is based on pointer networks. The encoder is a recurrent neural network (RNN) with gated recurrent unit (GRU) (Cho et al., 2014), whose input is the concatenation of question words, words from column names and SQL keywords. The decoder is another GRU based RNN, which works in a sequential way and generates a word at each time step. The generation of a word is actually selectively replicating a word from the input sequence, the probzt (2) One advantage of this architecture is that it"
P18-1034,P16-1002,0,0.0341733,"al., 2017; Zhong et al., 2017), our approach leverages SQL syntax and table structure. Compared to (Popescu et al., 2003; Giordani and Moschitti, 2012; Yaghmazadeh et al., 2017), our approach is end-to-end learning and independent of a syntactic parser or manuRelated Work Semantic Parsing. Semantic parsing aims to map natural language utterances to programs (e.g., logical forms), which will be executed to obtain the answer (denotation) (Zettlemoyer and Collins, 2005; Liang et al., 2011; Berant et al., 2013; Poon, 2013; Krishnamurthy and Kollar, 2013; Pasupat and Liang, 2016; Sun et al., 2016; Jia and Liang, 2016; Koˇcisk´y et al., 2016; Lin et al., 2017). Existing studies differ from (1) the form of the knowledge base, e.g. facts from Freebase, a table (or relational database), an image (Suhr et al., 2017; Johnson et al., 2017; Hu et al., 2017; Goldman et al., 2017) or a world state (Long et al., 2016); (2) the programming language, e.g. first-order logic, lambda calculus, lambda DCS, SQL, parameterized neural programmer (Yin et al., 2015; Neelakantan et al., 2016), or coupled distributed and symbolic executors (Mou et al., 2017); (3) the supervision used for learning the semantic parser, e.g. questi"
P18-1034,P16-1014,0,0.0177017,"y We first describe the background on pointer networks, and then present our approach that considers the table structure and the SQL syntax. 4.1 (1) Background: Pointer Networks Pointer networks is originally introduced by (Vinyals et al., 2015), which takes a sequence of elements as the input and outputs a sequence of discrete tokens corresponding to positions in the input sequence. The approach has been successfully applied in reading comprehension (Kadlec et al., 2016) for pointing to the positions of answer span from the document, and also in sequenceto-sequence based machine translation (Gulcehre et al., 2016) and text summarization (Gu et al., 2016) for replicating rare words from the source sequence to the target sequence. The approach of Zhong et al. (2017) is based on pointer networks. The encoder is a recurrent neural network (RNN) with gated recurrent unit (GRU) (Cho et al., 2014), whose input is the concatenation of question words, words from column names and SQL keywords. The decoder is another GRU based RNN, which works in a sequential way and generates a word at each time step. The generation of a word is actually selectively replicating a word from the input sequence, the probzt (2) One"
P18-1034,P16-1086,0,0.028017,"s a probability distribution over the tokens from one of the three channels. X p(yt |y&lt;t , x) = pw (yt |zt , y&lt;t , x)pz (zt |y&lt;t , x) Methodology We first describe the background on pointer networks, and then present our approach that considers the table structure and the SQL syntax. 4.1 (1) Background: Pointer Networks Pointer networks is originally introduced by (Vinyals et al., 2015), which takes a sequence of elements as the input and outputs a sequence of discrete tokens corresponding to positions in the input sequence. The approach has been successfully applied in reading comprehension (Kadlec et al., 2016) for pointing to the positions of answer span from the document, and also in sequenceto-sequence based machine translation (Gulcehre et al., 2016) and text summarization (Gu et al., 2016) for replicating rare words from the source sequence to the target sequence. The approach of Zhong et al. (2017) is based on pointer networks. The encoder is a recurrent neural network (RNN) with gated recurrent unit (GRU) (Cho et al., 2014), whose input is the concatenation of question words, words from column names and SQL keywords. The decoder is another GRU based RNN, which works in a sequential way and ge"
P18-1034,P17-1097,0,0.143076,"is critical for users to access relational databases with natural language, and could serve users’ in362 ability distribution of which is calculated with an attention mechanism (Bahdanau et al., 2015). The probability of generating the i-th word xi in the input sequence at the t-th time step is calculated as Equation 1, where hdec t is the decoder hidden state at the t-th time step, henc is the encoder hidden i state of the word xi , Wa is the model parameter. ally designed templates. We are aware of existing studies that combine reinforcement learning and maximum likelihood estimation (MLE) (Guu et al., 2017; Mou et al., 2017; Liang et al., 2017). However, the focus of this work is the design of the neural architecture, despite we also implement an RL strategy (refer to §4.4). Structure/Grammar Guided Neural Decoder Our approach could be viewed as an extension of the sequence-to-sequence learning (Sutskever et al., 2014; Bahdanau et al., 2015) with a tailored neural decoder driven by the characteristic of the target language (Yin and Neubig, 2017; Rabinovich et al., 2017). Methods with similar intuitions have been developed for language modeling (Dyer et al., 2016), neural machine translation (Wu"
P18-1034,D16-1116,0,0.0360868,"Missing"
P18-1034,D17-1160,0,0.26447,"sign of the neural architecture, despite we also implement an RL strategy (refer to §4.4). Structure/Grammar Guided Neural Decoder Our approach could be viewed as an extension of the sequence-to-sequence learning (Sutskever et al., 2014; Bahdanau et al., 2015) with a tailored neural decoder driven by the characteristic of the target language (Yin and Neubig, 2017; Rabinovich et al., 2017). Methods with similar intuitions have been developed for language modeling (Dyer et al., 2016), neural machine translation (Wu et al., 2017) and lambda calculus based semantic parsing (Dong and Lapata, 2016; Krishnamurthy et al., 2017). The difference is that our model is developed for sequence-to-SQL generation, in which table structure and SQL syntax are considered. 4 enc p(yt = xi |y&lt;t , x) ∝ exp(Wa [hdec t ; hi ]) It is worth to note that if a column name consists of multiple words (such as “original artist” in Figure 1), these words are separated in the input sequence. The approach has no guarantee that a multi-word column name could be successively generated, which would affect the executability of the generated SQL query. 4.2 STAMP: Syntax- and Table- Aware seMantic Parser Figure 2 illustrates an overview of the prop"
P18-1034,Q13-1016,0,0.0505279,"tional model to predict the slots in where clause. Different from (Iyer et al., 2017; Zhong et al., 2017), our approach leverages SQL syntax and table structure. Compared to (Popescu et al., 2003; Giordani and Moschitti, 2012; Yaghmazadeh et al., 2017), our approach is end-to-end learning and independent of a syntactic parser or manuRelated Work Semantic Parsing. Semantic parsing aims to map natural language utterances to programs (e.g., logical forms), which will be executed to obtain the answer (denotation) (Zettlemoyer and Collins, 2005; Liang et al., 2011; Berant et al., 2013; Poon, 2013; Krishnamurthy and Kollar, 2013; Pasupat and Liang, 2016; Sun et al., 2016; Jia and Liang, 2016; Koˇcisk´y et al., 2016; Lin et al., 2017). Existing studies differ from (1) the form of the knowledge base, e.g. facts from Freebase, a table (or relational database), an image (Suhr et al., 2017; Johnson et al., 2017; Hu et al., 2017; Goldman et al., 2017) or a world state (Long et al., 2016); (2) the programming language, e.g. first-order logic, lambda calculus, lambda DCS, SQL, parameterized neural programmer (Yin et al., 2015; Neelakantan et al., 2016), or coupled distributed and symbolic executors (Mou et al., 2017); (3) th"
P18-1034,D11-1140,0,0.0801846,"Missing"
P18-1034,P15-1142,0,0.163748,"et al., 2017). Popescu et al. (2003) use a parser to parse the question, and then use lexicon matching between question and the table column names/cells. Giordani and Moschitti (2012) parse the question with dependency parser, compose candidate SQL queries with heuristic rules, and use kernel based SVM ranker to rank the results. Li and Jagadish (2014) translate natural language utterances into SQL queries based on dependency parsing results, and interact with users to ensure the correctness of the interpretation process. Yaghmazadeh et al. (2017) build a semantic parser on the top of SEMPRE (Pasupat and Liang, 2015) to get a SQL sketch, which only has the SQL shape and will be subsequently completed based on the table content. Iyer et al. (2017) maps utterances to SQL queries through sequence-tosequence learning. User feedbacks are incorporated to reduce the number of queries to be labeled. Zhong et al. (2017) develop an augmented pointer network, which is further improved with reinforcement learning for SQL sequence prediction. Xu et al. (2017) adopts a sequence-to-set model to predict WHERE columns, and uses an attentional model to predict the slots in where clause. Different from (Iyer et al., 2017; Z"
P18-1034,P16-1003,0,0.0875917,"s in where clause. Different from (Iyer et al., 2017; Zhong et al., 2017), our approach leverages SQL syntax and table structure. Compared to (Popescu et al., 2003; Giordani and Moschitti, 2012; Yaghmazadeh et al., 2017), our approach is end-to-end learning and independent of a syntactic parser or manuRelated Work Semantic Parsing. Semantic parsing aims to map natural language utterances to programs (e.g., logical forms), which will be executed to obtain the answer (denotation) (Zettlemoyer and Collins, 2005; Liang et al., 2011; Berant et al., 2013; Poon, 2013; Krishnamurthy and Kollar, 2013; Pasupat and Liang, 2016; Sun et al., 2016; Jia and Liang, 2016; Koˇcisk´y et al., 2016; Lin et al., 2017). Existing studies differ from (1) the form of the knowledge base, e.g. facts from Freebase, a table (or relational database), an image (Suhr et al., 2017; Johnson et al., 2017; Hu et al., 2017; Goldman et al., 2017) or a world state (Long et al., 2016); (2) the programming language, e.g. first-order logic, lambda calculus, lambda DCS, SQL, parameterized neural programmer (Yin et al., 2015; Neelakantan et al., 2016), or coupled distributed and symbolic executors (Mou et al., 2017); (3) the supervision used for le"
P18-1034,P17-1003,0,0.0189993,"tional databases with natural language, and could serve users’ in362 ability distribution of which is calculated with an attention mechanism (Bahdanau et al., 2015). The probability of generating the i-th word xi in the input sequence at the t-th time step is calculated as Equation 1, where hdec t is the decoder hidden state at the t-th time step, henc is the encoder hidden i state of the word xi , Wa is the model parameter. ally designed templates. We are aware of existing studies that combine reinforcement learning and maximum likelihood estimation (MLE) (Guu et al., 2017; Mou et al., 2017; Liang et al., 2017). However, the focus of this work is the design of the neural architecture, despite we also implement an RL strategy (refer to §4.4). Structure/Grammar Guided Neural Decoder Our approach could be viewed as an extension of the sequence-to-sequence learning (Sutskever et al., 2014; Bahdanau et al., 2015) with a tailored neural decoder driven by the characteristic of the target language (Yin and Neubig, 2017; Rabinovich et al., 2017). Methods with similar intuitions have been developed for language modeling (Dyer et al., 2016), neural machine translation (Wu et al., 2017) and lambda calculus base"
P18-1034,D14-1162,0,0.0809332,"ells, and weighted average two cell distributions, which is calculated as follows. cell pcell pcell w (j) = λˆ w (j) + (1 − λ)αj 4.5 As the WikiSQL data contains rich supervision of question-SQL pairs, we use them to train model parameters. The model has two cross-entropy loss functions, as given below. One is for the switching gate classifier (pz ) and another is for the attentional probability distribution of a channel (pw ). l=− X X logpw (yt |zt , y&lt;t , x) logpz (zt |y&lt;t , x)− t t (6) Our parameter setting strictly follows Zhong et al. (2017). We represent each word using word embedding2 (Pennington et al., 2014) and the mean of the sub-word embeddings of all the n-grams in the word (Hashimoto et al., 2016)3 . The dimension of the concatenated word embedding is 400. We clamp the embedding values to avoid over-fitting. We set the dimension of encoder and decoder hidden state as 200. During training, we randomize model parameters from a uniform distribution with fan-in and fan-out, set batch size as 64, set the learning rate of SGD as 0.5, and update the model with stochastic gradient descent. Greedy search is used in the inference process. We use the model trained from question-SQL pairs as initializat"
P18-1034,P13-1092,0,0.0220016,"ses an attentional model to predict the slots in where clause. Different from (Iyer et al., 2017; Zhong et al., 2017), our approach leverages SQL syntax and table structure. Compared to (Popescu et al., 2003; Giordani and Moschitti, 2012; Yaghmazadeh et al., 2017), our approach is end-to-end learning and independent of a syntactic parser or manuRelated Work Semantic Parsing. Semantic parsing aims to map natural language utterances to programs (e.g., logical forms), which will be executed to obtain the answer (denotation) (Zettlemoyer and Collins, 2005; Liang et al., 2011; Berant et al., 2013; Poon, 2013; Krishnamurthy and Kollar, 2013; Pasupat and Liang, 2016; Sun et al., 2016; Jia and Liang, 2016; Koˇcisk´y et al., 2016; Lin et al., 2017). Existing studies differ from (1) the form of the knowledge base, e.g. facts from Freebase, a table (or relational database), an image (Suhr et al., 2017; Johnson et al., 2017; Hu et al., 2017; Goldman et al., 2017) or a world state (Long et al., 2016); (2) the programming language, e.g. first-order logic, lambda calculus, lambda DCS, SQL, parameterized neural programmer (Yin et al., 2015; Neelakantan et al., 2016), or coupled distributed and symbolic exec"
P18-1034,P11-1060,0,0.149695,"set model to predict WHERE columns, and uses an attentional model to predict the slots in where clause. Different from (Iyer et al., 2017; Zhong et al., 2017), our approach leverages SQL syntax and table structure. Compared to (Popescu et al., 2003; Giordani and Moschitti, 2012; Yaghmazadeh et al., 2017), our approach is end-to-end learning and independent of a syntactic parser or manuRelated Work Semantic Parsing. Semantic parsing aims to map natural language utterances to programs (e.g., logical forms), which will be executed to obtain the answer (denotation) (Zettlemoyer and Collins, 2005; Liang et al., 2011; Berant et al., 2013; Poon, 2013; Krishnamurthy and Kollar, 2013; Pasupat and Liang, 2016; Sun et al., 2016; Jia and Liang, 2016; Koˇcisk´y et al., 2016; Lin et al., 2017). Existing studies differ from (1) the form of the knowledge base, e.g. facts from Freebase, a table (or relational database), an image (Suhr et al., 2017; Johnson et al., 2017; Hu et al., 2017; Goldman et al., 2017) or a world state (Long et al., 2016); (2) the programming language, e.g. first-order logic, lambda calculus, lambda DCS, SQL, parameterized neural programmer (Yin et al., 2015; Neelakantan et al., 2016), or coup"
P18-1034,P17-1105,0,0.048093,"ally designed templates. We are aware of existing studies that combine reinforcement learning and maximum likelihood estimation (MLE) (Guu et al., 2017; Mou et al., 2017; Liang et al., 2017). However, the focus of this work is the design of the neural architecture, despite we also implement an RL strategy (refer to §4.4). Structure/Grammar Guided Neural Decoder Our approach could be viewed as an extension of the sequence-to-sequence learning (Sutskever et al., 2014; Bahdanau et al., 2015) with a tailored neural decoder driven by the characteristic of the target language (Yin and Neubig, 2017; Rabinovich et al., 2017). Methods with similar intuitions have been developed for language modeling (Dyer et al., 2016), neural machine translation (Wu et al., 2017) and lambda calculus based semantic parsing (Dong and Lapata, 2016; Krishnamurthy et al., 2017). The difference is that our model is developed for sequence-to-SQL generation, in which table structure and SQL syntax are considered. 4 enc p(yt = xi |y&lt;t , x) ∝ exp(Wa [hdec t ; hi ]) It is worth to note that if a column name consists of multiple words (such as “original artist” in Figure 1), these words are separated in the input sequence. The approach has n"
P18-1034,P17-2034,0,0.0222559,"to-end learning and independent of a syntactic parser or manuRelated Work Semantic Parsing. Semantic parsing aims to map natural language utterances to programs (e.g., logical forms), which will be executed to obtain the answer (denotation) (Zettlemoyer and Collins, 2005; Liang et al., 2011; Berant et al., 2013; Poon, 2013; Krishnamurthy and Kollar, 2013; Pasupat and Liang, 2016; Sun et al., 2016; Jia and Liang, 2016; Koˇcisk´y et al., 2016; Lin et al., 2017). Existing studies differ from (1) the form of the knowledge base, e.g. facts from Freebase, a table (or relational database), an image (Suhr et al., 2017; Johnson et al., 2017; Hu et al., 2017; Goldman et al., 2017) or a world state (Long et al., 2016); (2) the programming language, e.g. first-order logic, lambda calculus, lambda DCS, SQL, parameterized neural programmer (Yin et al., 2015; Neelakantan et al., 2016), or coupled distributed and symbolic executors (Mou et al., 2017); (3) the supervision used for learning the semantic parser, e.g. question-denotation pairs, binary correct/incorrect feedback (Artzi and Zettlemoyer, 2013), or richer supervision of question-logical form pairs (Dong and Lapata, 2016). In this work, we study semantic p"
P18-1034,D07-1071,0,0.600848,"Missing"
P18-1034,P07-1121,0,0.306045,"Missing"
P18-1034,D17-1125,0,0.635741,"Missing"
P18-1034,P17-1065,1,0.837395,"17; Mou et al., 2017; Liang et al., 2017). However, the focus of this work is the design of the neural architecture, despite we also implement an RL strategy (refer to §4.4). Structure/Grammar Guided Neural Decoder Our approach could be viewed as an extension of the sequence-to-sequence learning (Sutskever et al., 2014; Bahdanau et al., 2015) with a tailored neural decoder driven by the characteristic of the target language (Yin and Neubig, 2017; Rabinovich et al., 2017). Methods with similar intuitions have been developed for language modeling (Dyer et al., 2016), neural machine translation (Wu et al., 2017) and lambda calculus based semantic parsing (Dong and Lapata, 2016; Krishnamurthy et al., 2017). The difference is that our model is developed for sequence-to-SQL generation, in which table structure and SQL syntax are considered. 4 enc p(yt = xi |y&lt;t , x) ∝ exp(Wa [hdec t ; hi ]) It is worth to note that if a column name consists of multiple words (such as “original artist” in Figure 1), these words are separated in the input sequence. The approach has no guarantee that a multi-word column name could be successively generated, which would affect the executability of the generated SQL query. 4"
P18-1034,P16-1127,0,0.111172,"Missing"
P18-1034,P17-1041,0,0.227409,"the model parameter. ally designed templates. We are aware of existing studies that combine reinforcement learning and maximum likelihood estimation (MLE) (Guu et al., 2017; Mou et al., 2017; Liang et al., 2017). However, the focus of this work is the design of the neural architecture, despite we also implement an RL strategy (refer to §4.4). Structure/Grammar Guided Neural Decoder Our approach could be viewed as an extension of the sequence-to-sequence learning (Sutskever et al., 2014; Bahdanau et al., 2015) with a tailored neural decoder driven by the characteristic of the target language (Yin and Neubig, 2017; Rabinovich et al., 2017). Methods with similar intuitions have been developed for language modeling (Dyer et al., 2016), neural machine translation (Wu et al., 2017) and lambda calculus based semantic parsing (Dong and Lapata, 2016; Krishnamurthy et al., 2017). The difference is that our model is developed for sequence-to-SQL generation, in which table structure and SQL syntax are considered. 4 enc p(yt = xi |y&lt;t , x) ∝ exp(Wa [hdec t ; hi ]) It is worth to note that if a column name consists of multiple words (such as “original artist” in Figure 1), these words are separated in the input se"
P18-1034,N18-2093,0,0.393921,"Missing"
P18-1034,P16-1004,0,\N,Missing
P18-1034,P16-1138,0,\N,Missing
P18-1129,D16-1211,0,0.0161719,"ation and the results are shown in Figure 3. Sharpen the distribution during the sampling process generally performs better on development set. Our distillation from exploration model gets almost the same performance as that from reference, but simply combing these two sets of data outperform both models by achieving an LAS of 92.14. We also compare our parser with the other parsers in Table 2. The second group shows the greedy transition-based parsers in previous literatures. Andor et al. (2016) presented an alternative state representation and explored both greedy and beam search decoding. (Ballesteros et al., 2016) explores training the greedy parser with dynamic oracle. Our distillation parser outperforms all these greedy counterparts. The third group shows 92.14 92 89 BLEU on dev. set 4.2 BLEU 22.79 26.26 24.76 24.64 25.44 20.73 22.53 23.83 Baseline Ensemble (10) Distill (reference, α=0.8) Distill (exploration, T =0.1) Distill (both) MIXER BSO (local, B=1) BSO (global, B=1) LAS on dev. set Baseline Ensemble (20) Distill (reference, α=1.0) Distill (exploration, T =1.0) Distill (both) Ballesteros et al. (2016) (dyn. oracle) Andor et al. (2016) (local, B=1) Buckman et al. (2016) (local, B=8) Andor et al."
P18-1129,D16-1254,0,0.0321506,"Missing"
P18-1129,P04-1015,0,0.387277,"ch-based structured prediction tasks – transition-based dependency parsing and neural machine translation show that distillation can effectively improve the single model’s performance and the final model achieves improvements of 1.32 in LAS and 2.65 in BLEU score on these two tasks respectively over strong baselines and it outperforms the greedy structured prediction models in previous literatures. 1 Reference states Search-based structured prediction models the generation of natural language structure (part-ofspeech tags, syntax tree, translations, semantic graphs, etc.) as a search problem (Collins and Roark, 2004; Liang et al., 2006; Zhang and Clark, 2008; Huang et al., 2012; Sutskever et al., 2014; Goodman et al., 2016). It has drawn a lot of research attention in recent years thanks to its competitive performance on both accuracy and running time. A stochastic policy that controls the whole search process is usually learned by imitating a reference policy. The imitation is usually addressed as training a classifier to predict the ref* Email corresponding. Distillation loss Exploration states Reference policy Training data Exploration policy model1 Ensemble model ... modelM Figure 1: Workflow of our"
P18-1129,P81-1022,0,0.63388,"Missing"
P18-1129,P15-1033,0,0.025449,"and exploration with the following manner: we use πR and πE to generate a set of training states. Then, we learn p(a |s) on the generated states. If one state was generated by the reference policy, we minimize the interpretation of distillation and NLL loss. Otherwise, we minimize the distillation loss only. 4 Experiments We perform experiments on two tasks: transitionbased dependency parsing and neural machine translation. Both these two tasks are converted to search-based structured prediction as Section 2.1. For the transition-based parsing, we use the stack-lstm parsing model proposed by Dyer et al. (2015) to parameterize the classifier.1 For the neural machine translation, we parameterize the classifier as an LSTM encoder-decoder model by following Luong et al. (2015).2 We encourage the reader of this paper to refer corresponding papers for more details. 4.1 Settings 4.1.1 Transition-based Dependency Parsing We perform experiments on Penn Treebank (PTB) dataset with standard data split (Section 2-21 for training, Section 22 for development, and Section 23 for testing). Stanford dependencies are converted from the original constituent trees using Stanford CoreNLP 3.3.03 by following Dyer et al."
P18-1129,P16-1231,0,0.0231506,".0, best performance on development set is achieved and the test LAS is 91.99. We tune the temperature T during exploration and the results are shown in Figure 3. Sharpen the distribution during the sampling process generally performs better on development set. Our distillation from exploration model gets almost the same performance as that from reference, but simply combing these two sets of data outperform both models by achieving an LAS of 92.14. We also compare our parser with the other parsers in Table 2. The second group shows the greedy transition-based parsers in previous literatures. Andor et al. (2016) presented an alternative state representation and explored both greedy and beam search decoding. (Ballesteros et al., 2016) explores training the greedy parser with dynamic oracle. Our distillation parser outperforms all these greedy counterparts. The third group shows 92.14 92 89 BLEU on dev. set 4.2 BLEU 22.79 26.26 24.76 24.64 25.44 20.73 22.53 23.83 Baseline Ensemble (10) Distill (reference, α=0.8) Distill (exploration, T =0.1) Distill (both) MIXER BSO (local, B=1) BSO (global, B=1) LAS on dev. set Baseline Ensemble (20) Distill (reference, α=1.0) Distill (exploration, T =1.0) Distill (bo"
P18-1129,C12-1059,0,0.258443,"ructured prediction. The yellow bracket represents the ensemble of multiple models trained with different initialization. The dashed red line shows our distillation from reference (§3.2). The solid blue line shows our distillation from exploration (§3.3). Introduction ∗ Distilled model erence policy’s search action on the encountered states when performing the reference policy. Such imitation process can sometimes be problematic. One problem is the ambiguities of the reference policy, in which multiple actions lead to the optimal structure but usually, only one is chosen as training instance (Goldberg and Nivre, 2012). Another problem is the discrepancy between training and testing, in which during the test phase, the learned policy enters non-optimal states whose search action is never learned (Ross and Bagnell, 2010; Ross et al., 2011). All these problems harm the generalization ability of search-based structured prediction and lead to poor performance. Previous works tackle these problems from two directions. To overcome the ambiguities in data, techniques like ensemble are often adopted (Di1393 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages"
P18-1129,Q14-1010,0,0.0138696,"imal 89.59 90.90 91.38 Table 4: The ranking performance of parsers’ output distributions evaluated in MAP on “problematic” states. 4.3.1 Ensemble on “Problematic” States As mentioned in previous sections, “problematic” states which is either ambiguous or non-optimal harm structured prediciton’s performance. Ensemble shows to improve the performance in Section 4.2, which indicates it does better on these states. To empirically testify this, we use dependency parsing as a testbed and study the ensemble’s output distribution using the dynamic oracle. The dynamic oracle (Goldberg and Nivre, 2012; Goldberg et al., 2014) can be used to efficiently determine, given any state s, which transition action leads to the best achievable parse from s; if some errors may have already made, what is the best the parser can do, going forward? This allows us to analyze the accuracy of each parser’s individual decisions, in the “problematic” states. In this paper, we evaluate the output distributions of the baseline and ensemble parser against the reference actions suggested by the dynamic oracle. Since dynamic oracle yields more than one reference actions due to ambiguities and previous mistakes and the output distribution"
P18-1129,P16-1001,0,0.145512,"hat distillation can effectively improve the single model’s performance and the final model achieves improvements of 1.32 in LAS and 2.65 in BLEU score on these two tasks respectively over strong baselines and it outperforms the greedy structured prediction models in previous literatures. 1 Reference states Search-based structured prediction models the generation of natural language structure (part-ofspeech tags, syntax tree, translations, semantic graphs, etc.) as a search problem (Collins and Roark, 2004; Liang et al., 2006; Zhang and Clark, 2008; Huang et al., 2012; Sutskever et al., 2014; Goodman et al., 2016). It has drawn a lot of research attention in recent years thanks to its competitive performance on both accuracy and running time. A stochastic policy that controls the whole search process is usually learned by imitating a reference policy. The imitation is usually addressed as training a classifier to predict the ref* Email corresponding. Distillation loss Exploration states Reference policy Training data Exploration policy model1 Ensemble model ... modelM Figure 1: Workflow of our knowledge distillation for search-based structured prediction. The yellow bracket represents the ensemble of m"
P18-1129,nilsson-nivre-2008-malteval,0,0.0401907,"Missing"
P18-1129,J08-4003,0,0.595658,"parsing (σ, β, A), where σ is a stack, β is a buffer, and A is the partially generated tree {S HIFT, L EFT, R IGHT} {([ ], [1, .., n], ∅)} {([ROOT], [ ], A)} • S HIFT: (σ, j|β) → (σ|j, β) • L EFT: (σ|i j, β) → (σ|j, β) A ← A ∪ {i ← j} • R IGHT: (σ|i j, β) → (σ|i, β) A ← A ∪ {i → j} Neural machine translation ($, y1 , y2 , ..., yt ), where $ is the start symbol. pick one word w from the target side vocabulary W. {($)} {($, y1 , y2 , ..., ym )} ($, y1 , y2 , ..., yt ) → ($, y1 , y2 , ..., yt , yt+1 = w) Table 1: The search-based structured prediction view of transition-based dependency parsing (Nivre, 2008) and neural machine translation (Sutskever et al., 2014). etterich, 2000). To mitigate the discrepancy, exploration is encouraged during the training process (Ross and Bagnell, 2010; Ross et al., 2011; Goldberg and Nivre, 2012; Bengio et al., 2015; Goodman et al., 2016). In this paper, we propose to consider these two problems in an integrated knowledge distillation manner (Hinton et al., 2015). We distill a single model from the ensemble of several baselines trained with different initialization by matching the ensemble’s output distribution on the reference states. We also let the ensemble r"
P18-1129,P02-1040,0,0.10334,"t n = 20. 4.1.2 Neural Machine Translation We conduct our experiments on a small machine translation dataset, which is the Germanto-English portion of the IWSLT 2014 machine translation evaluation campaign. The dataset contains around 153K training sentence pairs, 7K development sentence pairs, and 7K testing sentence pairs. We use the same preprocessing as Ranzato et al. (2015), which leads to a German vocabulary of about 30K entries and an English vocabulary of 25K entries. One-layer LSTM for both encoder and decoder with 256 hidden units are used by following Wiseman and Rush (2016). BLEU (Papineni et al., 2002) was used to evaluate the translator’s performance.4 Like in the dependency parsing experiments, we run M = 10 differentlyseeded runs and report the averaged score. Optimizing the distillation loss in Equation 1 requires enumerating over the action space. It is expensive for machine translation since the size of the action space (vocabulary) is considerably large (25K in our experiments). In this paper, we use the K-most probable actions (translations on target side) on one state to approximate the P whole probability distribution of q(a |s) as a q(a | P s) · log p(a |s) ≈ K ak |s) · log p(ˆ a"
P18-1129,N12-1015,0,0.0257324,"arsing and neural machine translation show that distillation can effectively improve the single model’s performance and the final model achieves improvements of 1.32 in LAS and 2.65 in BLEU score on these two tasks respectively over strong baselines and it outperforms the greedy structured prediction models in previous literatures. 1 Reference states Search-based structured prediction models the generation of natural language structure (part-ofspeech tags, syntax tree, translations, semantic graphs, etc.) as a search problem (Collins and Roark, 2004; Liang et al., 2006; Zhang and Clark, 2008; Huang et al., 2012; Sutskever et al., 2014; Goodman et al., 2016). It has drawn a lot of research attention in recent years thanks to its competitive performance on both accuracy and running time. A stochastic policy that controls the whole search process is usually learned by imitating a reference policy. The imitation is usually addressed as training a classifier to predict the ref* Email corresponding. Distillation loss Exploration states Reference policy Training data Exploration policy model1 Ensemble model ... modelM Figure 1: Workflow of our knowledge distillation for search-based structured prediction."
P18-1129,D17-1035,0,0.0240206,"tion and α is determined on the development set. 1 The code for parsing experiments is available at: https://github.com/Oneplus/twpipe. 2 We based our NMT experiments on OpenNMT (Klein et al., 2017). The code for NMT experiments is available at: https://github.com/Oneplus/OpenNMT-py. 3 stanfordnlp.github.io/CoreNLP/ history.html BLEU score on dev. set 3.4 27.25 27.13 27.06 27.00 26.934 26.926 10 20 26.86 26.99 26.99 50 100 26.75 26.50 26.25 1 2 5 Figure 2: The effect of using different Ks when approximating distillation loss with K-most probable actions in the machine translation experiments. Reimers and Gurevych (2017) and others have pointed out that neural network training is nondeterministic and depends on the seed for the random number generator. To control for this effect, they suggest to report the average of M differentlyseeded runs. In all our dependency parsing, we set n = 20. 4.1.2 Neural Machine Translation We conduct our experiments on a small machine translation dataset, which is the Germanto-English portion of the IWSLT 2014 machine translation evaluation campaign. The dataset contains around 153K training sentence pairs, 7K development sentence pairs, and 7K testing sentence pairs. We use the"
P18-1129,D16-1139,0,0.0931068,"ons are achieved by our distillation methods. As Keskar et al. (2016) pointed out that the generalmin max σ 90.45 92.00 91.14 92.37 0.17 0.09 21.63 24.22 23.67 25.65 0.55 0.12 Table 5: The minimal, maximum, and standard derivation values on differently-seeded runs. ization gap is not due to overfit, but due to the network converge to sharp minimizer which generalizes worse, we attribute the more stable training from our distillation model as the distillation loss presents less sharp minimizers. 5 Related Work Several works have been proposed to applying knowledge distillation to NLP problems. Kim and Rush (2016) presented a distillation model which focus on distilling the structured loss from a large model into a small one which works on sequencelevel. In contrast to their work, we pay more attention to action-level distillation and propose to do better action-level distillation by both from reference and exploration. Freitag et al. (2017) used an ensemble of 6translators to generate training reference. Exploration was tried in their work with beam-search. We differ their work by training the single model 1400 to match the distribution of the ensemble. Using ensemble in exploration was also studied i"
P18-1129,P17-4012,0,0.0724743,"Missing"
P18-1129,W04-3250,0,0.0171869,"a q(a | P s) · log p(a |s) ≈ K ak |s) · log p(ˆ ak |s), k q(ˆ where a ˆk is the k-th probable action. We fix α to 4 We use multi-bleu.perl to evaluate our model’s performance 1397 LAS 90.83 92.73 91.99 92.00 92.14 91.42 91.02 91.19 91.70 92.79 94.08 92.06 94.60 Table 2: The dependency parsing results. Significance test (Nilsson and Nivre, 2008) shows the improvement of our Distill (both) over Baseline is statistically significant with p < 0.01. Table 3: The machine translation results. MIXER denotes that of Ranzato et al. (2015), BSO denotes that of Wiseman and Rush (2016). Significance test (Koehn, 2004) shows the improvement of our Distill (both) over Baseline is statistically significant with p < 0.01. 92.11 1 and vary K and evaluate the distillation model’s performance. These results are shown in Figure 2 where there is no significant difference between different Ks and in speed consideration, we set K to 1 in the following experiments. 4.2.1 92.05 92.09 92.11 0.5 0.67 1 91.98 91.85 91 90 89.04 0.1 Results Transition-based Dependency Parsing Table 2 shows our PTB experimental results. From this result, we can see that the ensemble model outperforms the baseline model by 1.90 in LAS. For ou"
P18-1129,D17-1208,0,0.0231132,"transfer knowledge of cumbersome model into a simple one on the unlabeled data (Liang et al., 2008; Li et al., 2014). Their extensions to knowledge distillation call for further study. Kuncoro et al. (2016) proposed to compile the knowledge from an ensemble of 20 transitionbased parsers into a voting and distill the knowledge by introducing the voting results as a regularizer in learning a graph-based parser. Different from their work, we directly do the distillation on the classifier of the transition-based parser. Besides the attempts for directly using the knowledge distillation technique, Stahlberg and Byrne (2017) propose to first build the ensemble of several machine translators into one network by unfolding and then use SVD to shrink its parameters, which can be treated as another kind of knowledge distillation. 6 Conclusion In this paper, we study knowledge distillation for search-based structured prediction and propose to distill an ensemble into a single model both from reference and exploration states. Experiments on transition-based dependency parsing and machine translation show that our distillation method significantly improves the single model’s performance. Comparison analysis gives empiric"
P18-1129,E17-1117,0,0.0735766,"Missing"
P18-1129,D16-1180,0,0.140531,"outperforms all these greedy counterparts. The third group shows 92.14 92 89 BLEU on dev. set 4.2 BLEU 22.79 26.26 24.76 24.64 25.44 20.73 22.53 23.83 Baseline Ensemble (10) Distill (reference, α=0.8) Distill (exploration, T =0.1) Distill (both) MIXER BSO (local, B=1) BSO (global, B=1) LAS on dev. set Baseline Ensemble (20) Distill (reference, α=1.0) Distill (exploration, T =1.0) Distill (both) Ballesteros et al. (2016) (dyn. oracle) Andor et al. (2016) (local, B=1) Buckman et al. (2016) (local, B=8) Andor et al. (2016) (local, B=32) Andor et al. (2016) (global, B=32) Dozat and Manning (2016) Kuncoro et al. (2016) Kuncoro et al. (2017) 27.00 26.99 0.2 26.97 1.5 2 5 26.93 26.7 26.75 26.50 26.24 26.25 0.1 0.2 0.5 0.67 1 Figure 3: The effect of T on PTB (above) and IWSLT 2014 (below) development set. parsers trained on different techniques including decoding with beam search (Buckman et al., 2016; Andor et al., 2016), training transitionbased parser with beam search (Andor et al., 2016), graph-based parsing (Dozat and Manning, 2016), distilling a graph-based parser from the output of 20 parsers (Kuncoro et al., 2016), and converting constituent parsing results to dependencies (Kuncoro et al., 2017). Our d"
P18-1129,P14-1043,0,0.0301015,"and exploration. Freitag et al. (2017) used an ensemble of 6translators to generate training reference. Exploration was tried in their work with beam-search. We differ their work by training the single model 1400 to match the distribution of the ensemble. Using ensemble in exploration was also studied in reinforcement learning community (Osband et al., 2016). In addition to distilling the ensemble on the labeled training data, a line of semisupervised learning works show that it’s effective to transfer knowledge of cumbersome model into a simple one on the unlabeled data (Liang et al., 2008; Li et al., 2014). Their extensions to knowledge distillation call for further study. Kuncoro et al. (2016) proposed to compile the knowledge from an ensemble of 20 transitionbased parsers into a voting and distill the knowledge by introducing the voting results as a regularizer in learning a graph-based parser. Different from their work, we directly do the distillation on the classifier of the transition-based parser. Besides the attempts for directly using the knowledge distillation technique, Stahlberg and Byrne (2017) propose to first build the ensemble of several machine translators into one network by un"
P18-1129,Q14-1042,0,0.0137791,"d model significantly improves over strong baselines and outperforms other greedy structured prediction (§4.2). Comprehensive analysis empirically shows the feasibility of our distillation method (§4.3). 2 2.1 Background Search-based Structured Prediction Structured prediction maps an input x = (x1 , x2 , ..., xn ) to its structural output y = (y1 , y2 , ..., ym ), where each component of y has some internal dependencies. Search-based structured prediction (Collins and Roark, 2004; Daum´e III et al., 2005; Daum´e III et al., 2009; Ross and Bagnell, 2010; Ross et al., 2011; Doppa et al., 2014; Vlachos and Clark, 2014; Chang et al., 2015) models the generation of the structure as a search problem and it can be formalized as a tuple (S, A, T (s, a), S0 , ST ), in which S is a set of states, A is a set of actions, T is a function that maps S × A → S, S0 is a set of initial states, and ST is a set of terminal states. Starting from an initial state s0 ∈ S0 , the structured prediction model repeatably chooses an action at ∈ A by following a policy π(s) and applies at to st and enter a new state st+1 as st+1 ← T (st , at ), until a final state sT ∈ ST is achieved. Several natural language structured prediction p"
P18-1129,D16-1137,0,0.0952854,"l our dependency parsing, we set n = 20. 4.1.2 Neural Machine Translation We conduct our experiments on a small machine translation dataset, which is the Germanto-English portion of the IWSLT 2014 machine translation evaluation campaign. The dataset contains around 153K training sentence pairs, 7K development sentence pairs, and 7K testing sentence pairs. We use the same preprocessing as Ranzato et al. (2015), which leads to a German vocabulary of about 30K entries and an English vocabulary of 25K entries. One-layer LSTM for both encoder and decoder with 256 hidden units are used by following Wiseman and Rush (2016). BLEU (Papineni et al., 2002) was used to evaluate the translator’s performance.4 Like in the dependency parsing experiments, we run M = 10 differentlyseeded runs and report the averaged score. Optimizing the distillation loss in Equation 1 requires enumerating over the action space. It is expensive for machine translation since the size of the action space (vocabulary) is considerably large (25K in our experiments). In this paper, we use the K-most probable actions (translations on target side) on one state to approximate the P whole probability distribution of q(a |s) as a q(a | P s) · log"
P18-1129,D08-1059,0,0.0483245,"tion-based dependency parsing and neural machine translation show that distillation can effectively improve the single model’s performance and the final model achieves improvements of 1.32 in LAS and 2.65 in BLEU score on these two tasks respectively over strong baselines and it outperforms the greedy structured prediction models in previous literatures. 1 Reference states Search-based structured prediction models the generation of natural language structure (part-ofspeech tags, syntax tree, translations, semantic graphs, etc.) as a search problem (Collins and Roark, 2004; Liang et al., 2006; Zhang and Clark, 2008; Huang et al., 2012; Sutskever et al., 2014; Goodman et al., 2016). It has drawn a lot of research attention in recent years thanks to its competitive performance on both accuracy and running time. A stochastic policy that controls the whole search process is usually learned by imitating a reference policy. The imitation is usually addressed as training a classifier to predict the ref* Email corresponding. Distillation loss Exploration states Reference policy Training data Exploration policy model1 Ensemble model ... modelM Figure 1: Workflow of our knowledge distillation for search-based str"
P18-1129,P06-1096,0,0.162632,"Missing"
P18-1129,D15-1166,0,0.0503662,"Missing"
P19-1415,D14-1179,0,0.0545077,"Missing"
P19-1415,P18-1078,0,0.0736195,"eparate answer representations for further matching. Yao et al. (2018) introduce a latent variable for capturing variability and an observed variable for controlling question types. In summary, the above mentioned systems aim to generate answerable questions with certain context. On the contrary, our goal is to generate unanswerable questions. Adversarial Examples for MRC To evaluate the language understanding ability of pre-trained systems, Jia and Liang (2017) construct adversarial examples by adding distractor sentences that do not contradict question answering for humans to the paragraph. Clark and Gardner (2018) and Tan et al. (2018) use questions to retrieve paragraphs that do not contain the answer as adversarial examples. Rajpurkar et al. (2018) create unanswerable questions through rigid rules, which swap entities, numbers and antonyms of answerable questions. It has been shown that adversarial examples generated by rule-based systems are much easier to detect than ones in the SQuAD 2.0 dataset. Data Augmentation for MRC Several attempts have been made to augment training data for machine reading comprehension. We catego4239 SEQ2SEQ tween inputs. The decoder of two models generates unanswerable q"
P19-1415,N19-1423,0,0.183228,") = Interac+on |˜ q| Y P (˜ qt |˜ q&lt;t , q, p, a) (1) t=1 Encoder W W W C … C C P A P W Encoder W W W W C … C C … C C C … C C C Q P A P Q Q Q Paragraph + Ques+on W Encoder P Paragraph W P W W W … where q˜&lt;t = q˜1 . . . q˜t−1 . W C C Q Q 3.1 Ques+on Figure 2: Diagram of the proposed pair-to-sequence model and sequence-to-sequence model. The input embeddings is the sum of the word embeddings, the character embeddings and the token type embeddings. The input questions are all answerable. rize these work according to the type of the augmentation data: external data source, paragraphs or questions. Devlin et al. (2019) fine-tune BERT on the SQuAD dataset jointly with another dataset TriviaQA (Joshi et al., 2017). Yu et al. (2018) paraphrase paragraphs with backtranslation. Another line of work adheres to generate answerable questions. Yang et al. (2017) propose to generate questions based on the unlabeled text for semisupervised question answering. Sun et al. (2019) propose a rule-based system to generate multiplechoice questions with candidate options upon the paragraphs. We aim at generating unanswerable questions as a means of data augmentation. 3 Problem Formulation Given an answerable question q and it"
P19-1415,P18-1177,0,0.049005,"Missing"
P19-1415,P17-1123,0,0.0430977,"l., 2018; Yu et al., 2018; Wang et al., 2018) have outperformed humans on this task in terms of automatic metrics. The SQuAD 2.0 dataset (Rajpurkar et al., 2018) extends SQuAD with more than 50, 000 crowdsourced unanswerable questions. So far, neural reading comprehension models still fall behind humans on SQuAD 2.0. Abstaining from answering when no answer can be inferred from the given document does require more understanding than barely extracting an answer. Question Generation for MRC In recent years, there has been an increasing interest in generating questions for reading comprehension. Du et al. (2017) show that neural models based on the encoder-decoder framework can generate significantly better questions than rule-based systems (Heilman and Smith, 2010). To generate answer-focused questions, one can simply indicate the answer positions in the context with extra features (Yuan et al., 2017; Zhou et al., 2018; Du and Cardie, 2018; Sun et al., 2018; Dong et al., 2019). Song et al. (2018) and Kim et al. (2019) separate answer representations for further matching. Yao et al. (2018) introduce a latent variable for capturing variability and an observed variable for controlling question types. I"
P19-1415,P16-1154,0,0.0669924,"Missing"
P19-1415,N10-1086,0,0.0764351,"t al., 2018) extends SQuAD with more than 50, 000 crowdsourced unanswerable questions. So far, neural reading comprehension models still fall behind humans on SQuAD 2.0. Abstaining from answering when no answer can be inferred from the given document does require more understanding than barely extracting an answer. Question Generation for MRC In recent years, there has been an increasing interest in generating questions for reading comprehension. Du et al. (2017) show that neural models based on the encoder-decoder framework can generate significantly better questions than rule-based systems (Heilman and Smith, 2010). To generate answer-focused questions, one can simply indicate the answer positions in the context with extra features (Yuan et al., 2017; Zhou et al., 2018; Du and Cardie, 2018; Sun et al., 2018; Dong et al., 2019). Song et al. (2018) and Kim et al. (2019) separate answer representations for further matching. Yao et al. (2018) introduce a latent variable for capturing variability and an observed variable for controlling question types. In summary, the above mentioned systems aim to generate answerable questions with certain context. On the contrary, our goal is to generate unanswerable quest"
P19-1415,P82-1020,0,0.820425,"Missing"
P19-1415,D17-1215,0,0.184369,"xtra features (Yuan et al., 2017; Zhou et al., 2018; Du and Cardie, 2018; Sun et al., 2018; Dong et al., 2019). Song et al. (2018) and Kim et al. (2019) separate answer representations for further matching. Yao et al. (2018) introduce a latent variable for capturing variability and an observed variable for controlling question types. In summary, the above mentioned systems aim to generate answerable questions with certain context. On the contrary, our goal is to generate unanswerable questions. Adversarial Examples for MRC To evaluate the language understanding ability of pre-trained systems, Jia and Liang (2017) construct adversarial examples by adding distractor sentences that do not contradict question answering for humans to the paragraph. Clark and Gardner (2018) and Tan et al. (2018) use questions to retrieve paragraphs that do not contain the answer as adversarial examples. Rajpurkar et al. (2018) create unanswerable questions through rigid rules, which swap entities, numbers and antonyms of answerable questions. It has been shown that adversarial examples generated by rule-based systems are much easier to detect than ones in the SQuAD 2.0 dataset. Data Augmentation for MRC Several attempts hav"
P19-1415,P17-1147,0,0.0420112,"W W W C … C C … C C C … C C C Q P A P Q Q Q Paragraph + Ques+on W Encoder P Paragraph W P W W W … where q˜&lt;t = q˜1 . . . q˜t−1 . W C C Q Q 3.1 Ques+on Figure 2: Diagram of the proposed pair-to-sequence model and sequence-to-sequence model. The input embeddings is the sum of the word embeddings, the character embeddings and the token type embeddings. The input questions are all answerable. rize these work according to the type of the augmentation data: external data source, paragraphs or questions. Devlin et al. (2019) fine-tune BERT on the SQuAD dataset jointly with another dataset TriviaQA (Joshi et al., 2017). Yu et al. (2018) paraphrase paragraphs with backtranslation. Another line of work adheres to generate answerable questions. Yang et al. (2017) propose to generate questions based on the unlabeled text for semisupervised question answering. Sun et al. (2019) propose a rule-based system to generate multiplechoice questions with candidate options upon the paragraphs. We aim at generating unanswerable questions as a means of data augmentation. 3 Problem Formulation Given an answerable question q and its corresponding paragraph p that contains the answer a, we aim to generate unanswerable questio"
P19-1415,P17-4012,0,0.0800098,"Missing"
P19-1415,Q18-1023,0,0.0474809,"Missing"
P19-1415,K17-1034,0,0.0257042,"ast example shows that inserting negation words in different positions (“n’t public” versus “not in victoria”) can express different meanings. Such cases are critical for generated questions’ answerability, which is hard to handle in a rule-based system. 4.2 4.2.1 Data Augmentation for Machine Reading Comprehension Question Answering Models We apply our automatically generated unanswerable questions as augmentation data to the following reading comprehension models: BiDAF-No-Answer (BNA) BiDAF (Seo et al., 2017) is a benchmark model on extractive machine reading comprehension. Based on BiDAF, Levy et al. (2017) propose the BiDAF-No-Answer model to predict the distribution of answer candidates and the probability of a question being unanswerable at the same time. DocQA Clark and Gardner (2018) propose the DocQA model to address document-level reading comprehension. The no-answer probability is also predicted jointly. BERT Fine-Tuning It is the state-of-the-art model on unanswerable machine reading comprehension. We adopt the uncased version of BERT (Devlin et al., 2019) for fine-tuning. The batch sizes of BERT-base and BERT-large are set to 12 and 24 respectively. The rest hyperparameters are kept un"
P19-1415,W04-1013,0,0.0112197,"d embeddings. The hidden state size of LSTM network is 150. Dropout probability is set to 0.2. The data are shuffled and split into mini-batches of size 32 for training. The model is optimized with Adagrad (Duchi et al., 2011) with an initial learning rate of 0.15. During inference, the beam size is 5. We prohibit producing unknown words by setting the score of &lt;unk&gt; token to -inf. We filter the beam outputs that make no differences to the input question. 4.1.3 Evaluation Metrics The generation quality is evaluated using three automatic evaluation metrics: BLEU (Papineni et al., 2002), ROUGE (Lin, 2004) and GLEU (Napoles et al., 2015). BLEU1 is a commonly used metric in machine translation that computes ngram precisions over references. Recall-oriented ROUGE2 metric is widely adopted in summarization, and ROUGE-L measures longest common subsequence between system outputs and references. GLEU3 is a variant of BLEU with the modification that penalizes system output n-grams that present in input but absent from the reference. This makes GLEU a preferable metric for tasks with subtle but critical differences in a monolingual setting as in our unanswerable question generation task. We also conduc"
P19-1415,P18-1157,0,0.0277041,"Missing"
P19-1415,D15-1166,0,0.0405388,"rent neural networks with long short-term memory units (LSTM; Hochreiter and Schmidhuber, 1997) to produce encoder hidden states hi = fBiLSTM (hi−1 , ei ). On each decoding step t, the hidden states of decoder (a single-layer unidirectional LSTM network) are computed by st = fLSTM (st−1 , [yt−1 ; ct−1 ]), where yt−1 is the word embedding of previously predicted token and ct−1 is the encoder context vector of previous step. Besides, we use an attention mechanism to summarize the encoder-side information into ct for current step. The attention distribution γt over source words is computed as in Luong et al. (2015): score(hi , st ) = hT i Wγ s t γi,t = exp(score(hi , st ))/Zt ct = |x| X γi,t hi (2) (3) (4) i P|x| where Zt = k exp(score(hk , st )), Wγ in score function is a learnable parameter. Next, st is concatenated with ct to produce the vocabulary distribution Pv : 4240 Pv = softmax(Wv [st ; ct ] + bv ) (5) where Wv and bv are learnable parameters. Copy mechanism (See et al., 2017) is incorporated to directly copy words from inputs, because words in paragraphs or source questions are of great value for unanswerable question generation. Specifically, we use st and ct to produce a gating probability g"
P19-1415,P15-2097,0,0.0231868,"den state size of LSTM network is 150. Dropout probability is set to 0.2. The data are shuffled and split into mini-batches of size 32 for training. The model is optimized with Adagrad (Duchi et al., 2011) with an initial learning rate of 0.15. During inference, the beam size is 5. We prohibit producing unknown words by setting the score of &lt;unk&gt; token to -inf. We filter the beam outputs that make no differences to the input question. 4.1.3 Evaluation Metrics The generation quality is evaluated using three automatic evaluation metrics: BLEU (Papineni et al., 2002), ROUGE (Lin, 2004) and GLEU (Napoles et al., 2015). BLEU1 is a commonly used metric in machine translation that computes ngram precisions over references. Recall-oriented ROUGE2 metric is widely adopted in summarization, and ROUGE-L measures longest common subsequence between system outputs and references. GLEU3 is a variant of BLEU with the modification that penalizes system output n-grams that present in input but absent from the reference. This makes GLEU a preferable metric for tasks with subtle but critical differences in a monolingual setting as in our unanswerable question generation task. We also conduct human evaluation on 100 sample"
P19-1415,P02-1040,0,0.104033,"are the same vocabulary and word embeddings. The hidden state size of LSTM network is 150. Dropout probability is set to 0.2. The data are shuffled and split into mini-batches of size 32 for training. The model is optimized with Adagrad (Duchi et al., 2011) with an initial learning rate of 0.15. During inference, the beam size is 5. We prohibit producing unknown words by setting the score of &lt;unk&gt; token to -inf. We filter the beam outputs that make no differences to the input question. 4.1.3 Evaluation Metrics The generation quality is evaluated using three automatic evaluation metrics: BLEU (Papineni et al., 2002), ROUGE (Lin, 2004) and GLEU (Napoles et al., 2015). BLEU1 is a commonly used metric in machine translation that computes ngram precisions over references. Recall-oriented ROUGE2 metric is widely adopted in summarization, and ROUGE-L measures longest common subsequence between system outputs and references. GLEU3 is a variant of BLEU with the modification that penalizes system output n-grams that present in input but absent from the reference. This makes GLEU a preferable metric for tasks with subtle but critical differences in a monolingual setting as in our unanswerable question generation t"
P19-1415,D14-1162,0,0.0826805,"Missing"
P19-1415,P18-2124,0,0.555636,"aset. The annotated (plausible) answer span in the paragraph is used as a pivot to align the pair of answerable and unanswerable questions. Introduction Extractive reading comprehension (Hermann et al., 2015; Rajpurkar et al., 2016) obtains great attentions from both research and industry in recent years. End-to-end neural models (Seo et al., 2017; Wang et al., 2017; Yu et al., 2018) have achieved remarkable performance on the task if answers are assumed to be in the given paragraph. Nonetheless, the current systems are still not good at deciding whether no answer is presented in the context (Rajpurkar et al., 2018). For unanswerable questions, the systems are supposed to abstain from answering rather than making unreliable guesses, which is an embodiment of language understanding ability. ∗ Asia. Contribution during internship at Microsoft Research We attack the problem by automatically generating unanswerable questions for data augmentation to improve question answering models. The generated unanswerable questions should not be too easy for the question answering model so that data augmentation can better help the model. For example, a simple baseline method is randomly choosing a question asked for an"
P19-1415,D16-1264,0,0.0603667,"ion on the SQuAD 2.0 dataset, yielding 1.9 absolute F1 improvement with BERT-base model and 1.7 absolute F1 improvement with BERT-large model. 1 Ans. Question: What organization runs the public schools in Victoria? UnAns. Question: What organization runs the waste management in Victoria? (Plausible) Answer: Victoria Department of Education Figure 1: An example taken from the SQuAD 2.0 dataset. The annotated (plausible) answer span in the paragraph is used as a pivot to align the pair of answerable and unanswerable questions. Introduction Extractive reading comprehension (Hermann et al., 2015; Rajpurkar et al., 2016) obtains great attentions from both research and industry in recent years. End-to-end neural models (Seo et al., 2017; Wang et al., 2017; Yu et al., 2018) have achieved remarkable performance on the task if answers are assumed to be in the given paragraph. Nonetheless, the current systems are still not good at deciding whether no answer is presented in the context (Rajpurkar et al., 2018). For unanswerable questions, the systems are supposed to abstain from answering rather than making unreliable guesses, which is an embodiment of language understanding ability. ∗ Asia. Contribution during int"
P19-1415,P17-1099,0,0.0281374,"s the encoder context vector of previous step. Besides, we use an attention mechanism to summarize the encoder-side information into ct for current step. The attention distribution γt over source words is computed as in Luong et al. (2015): score(hi , st ) = hT i Wγ s t γi,t = exp(score(hi , st ))/Zt ct = |x| X γi,t hi (2) (3) (4) i P|x| where Zt = k exp(score(hk , st )), Wγ in score function is a learnable parameter. Next, st is concatenated with ct to produce the vocabulary distribution Pv : 4240 Pv = softmax(Wv [st ; ct ] + bv ) (5) where Wv and bv are learnable parameters. Copy mechanism (See et al., 2017) is incorporated to directly copy words from inputs, because words in paragraphs or source questions are of great value for unanswerable question generation. Specifically, we use st and ct to produce a gating probability gt : gt = sigmoid(Wg [st ; ct ] + bg ) (6) where Wg and bg are learnable parameters. The gate gt determines whether generating a word from the vocabulary or copying a word from inputs. Finally, we obtain the probability of generating q˜t by: X P (˜ qt |˜ q&lt;t , q, p, a) = gt Pv (˜ qt ) + (1 − gt ) γˆi,t i∈ζq˜t where ζq˜t denotes all the occurrence of q˜t in inputs, and the copy"
P19-1415,N18-2090,0,0.0319965,"document does require more understanding than barely extracting an answer. Question Generation for MRC In recent years, there has been an increasing interest in generating questions for reading comprehension. Du et al. (2017) show that neural models based on the encoder-decoder framework can generate significantly better questions than rule-based systems (Heilman and Smith, 2010). To generate answer-focused questions, one can simply indicate the answer positions in the context with extra features (Yuan et al., 2017; Zhou et al., 2018; Du and Cardie, 2018; Sun et al., 2018; Dong et al., 2019). Song et al. (2018) and Kim et al. (2019) separate answer representations for further matching. Yao et al. (2018) introduce a latent variable for capturing variability and an observed variable for controlling question types. In summary, the above mentioned systems aim to generate answerable questions with certain context. On the contrary, our goal is to generate unanswerable questions. Adversarial Examples for MRC To evaluate the language understanding ability of pre-trained systems, Jia and Liang (2017) construct adversarial examples by adding distractor sentences that do not contradict question answering for h"
P19-1415,N19-1270,0,0.0353075,"Missing"
P19-1415,D18-1427,0,0.0242532,"answer can be inferred from the given document does require more understanding than barely extracting an answer. Question Generation for MRC In recent years, there has been an increasing interest in generating questions for reading comprehension. Du et al. (2017) show that neural models based on the encoder-decoder framework can generate significantly better questions than rule-based systems (Heilman and Smith, 2010). To generate answer-focused questions, one can simply indicate the answer positions in the context with extra features (Yuan et al., 2017; Zhou et al., 2018; Du and Cardie, 2018; Sun et al., 2018; Dong et al., 2019). Song et al. (2018) and Kim et al. (2019) separate answer representations for further matching. Yao et al. (2018) introduce a latent variable for capturing variability and an observed variable for controlling question types. In summary, the above mentioned systems aim to generate answerable questions with certain context. On the contrary, our goal is to generate unanswerable questions. Adversarial Examples for MRC To evaluate the language understanding ability of pre-trained systems, Jia and Liang (2017) construct adversarial examples by adding distractor sentences that do"
P19-1415,P18-1158,0,0.0133866,"ith BERT-large model. 2 Related Work Machine Reading Comprehension (MRC) Various large-scale datasets (Hermann et al., 2015; Rajpurkar et al., 2016; Nguyen et al., 2016; Joshi et al., 2017; Rajpurkar et al., 2018; Kocisky et al., 2018) have spurred rapid progress on machine reading comprehension in recent years. SQuAD (Rajpurkar et al., 2016) is an extractive benchmark whose questions and answers spans are annotated by humans. Neural reading comprehension systems (Wang and Jiang, 2017; Seo et al., 2017; Wang et al., 2017; Hu et al., 2018; Huang et al., 2018; Liu et al., 2018; Yu et al., 2018; Wang et al., 2018) have outperformed humans on this task in terms of automatic metrics. The SQuAD 2.0 dataset (Rajpurkar et al., 2018) extends SQuAD with more than 50, 000 crowdsourced unanswerable questions. So far, neural reading comprehension models still fall behind humans on SQuAD 2.0. Abstaining from answering when no answer can be inferred from the given document does require more understanding than barely extracting an answer. Question Generation for MRC In recent years, there has been an increasing interest in generating questions for reading comprehension. Du et al. (2017) show that neural models base"
P19-1415,P17-1018,1,0.86445,"1 Ans. Question: What organization runs the public schools in Victoria? UnAns. Question: What organization runs the waste management in Victoria? (Plausible) Answer: Victoria Department of Education Figure 1: An example taken from the SQuAD 2.0 dataset. The annotated (plausible) answer span in the paragraph is used as a pivot to align the pair of answerable and unanswerable questions. Introduction Extractive reading comprehension (Hermann et al., 2015; Rajpurkar et al., 2016) obtains great attentions from both research and industry in recent years. End-to-end neural models (Seo et al., 2017; Wang et al., 2017; Yu et al., 2018) have achieved remarkable performance on the task if answers are assumed to be in the given paragraph. Nonetheless, the current systems are still not good at deciding whether no answer is presented in the context (Rajpurkar et al., 2018). For unanswerable questions, the systems are supposed to abstain from answering rather than making unreliable guesses, which is an embodiment of language understanding ability. ∗ Asia. Contribution during internship at Microsoft Research We attack the problem by automatically generating unanswerable questions for data augmentation to improve"
P19-1415,P17-1096,0,0.0389431,"ues+on Figure 2: Diagram of the proposed pair-to-sequence model and sequence-to-sequence model. The input embeddings is the sum of the word embeddings, the character embeddings and the token type embeddings. The input questions are all answerable. rize these work according to the type of the augmentation data: external data source, paragraphs or questions. Devlin et al. (2019) fine-tune BERT on the SQuAD dataset jointly with another dataset TriviaQA (Joshi et al., 2017). Yu et al. (2018) paraphrase paragraphs with backtranslation. Another line of work adheres to generate answerable questions. Yang et al. (2017) propose to generate questions based on the unlabeled text for semisupervised question answering. Sun et al. (2019) propose a rule-based system to generate multiplechoice questions with candidate options upon the paragraphs. We aim at generating unanswerable questions as a means of data augmentation. 3 Problem Formulation Given an answerable question q and its corresponding paragraph p that contains the answer a, we aim to generate unanswerable questions q˜ that fulfills certain requirements. First, it cannot be answered by paragraph p. Second, it must be relevant to both answerable question q"
P19-1415,P13-1171,0,0.0355318,"Research We attack the problem by automatically generating unanswerable questions for data augmentation to improve question answering models. The generated unanswerable questions should not be too easy for the question answering model so that data augmentation can better help the model. For example, a simple baseline method is randomly choosing a question asked for another paragraph, and using it as an unanswerable question. However, it would be trivial to determine whether the retrieved question is answerable by using wordoverlap heuristics, because the question is irrelevant to the context (Yih et al., 2013). In this work, we propose to generate unanswerable questions by editing an answerable question and conditioning on the corresponding paragraph that contains the answer. So the generated unanswerable questions are more lexically similar and relevant to the context. Moreover, by using the answerable question as a prototype and its answer span as a plausible answer, the generated examples can provide more discriminative training signal to the question answering model. 4238 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4238–4248 c Florence, Italy,"
S14-2033,S13-2053,0,0.595629,"utomatically feature learning, Tang et al. (2014) propose to learn sentiment-specific word embedding and Kalchbrenner et al. (2014) model sentence representation with Dynamic Convolutional Neural Network. In this paper, we develop a deep learning system for Twitter sentiment classification. Firstly, we learn sentiment-specific word embedding (SSWE) (Tang et al., 2014), which encodes the sentiment information of text into the continuous representation of words (Mikolov et al., 2013; Sun et al., 2014). Afterwards, we concatenate the SSWE features with the state-of-the-art hand-crafted features (Mohammad et al., 2013), and build the sentiment classifier with the benchmark dataset from SemEval 2013 (Nakov et al., 2013). To learn SSWE, we develop a tailored neural network, which incorporates the supervision from sentiment polarity of tweets in the hybrid loss function. We learn SSWE from tweets, leveraging massive tweets with emoticons as distantsupervised corpora without any manual annotations. We evaluate the deep learning system on the test set of Twitter Sentiment Analysis Track in SemEval 2014 2 . Our system (Coooolll) is ranked 2nd on the Twitter2014 test set, along with the SemEval 2013 participants o"
S14-2033,P14-2009,1,0.80825,"afted features. We develop a neural network with hybrid loss function 1 to learn SSWE, which encodes the sentiment information of tweets in the continuous representation of words. To obtain large-scale training corpora, we train SSWE from 10M tweets collected by positive and negative emoticons, without any manual annotation. Our system can be easily re-implemented with the publicly available sentiment-specific word embedding. 1 Introduction Twitter sentiment classification aims to classify the sentiment polarity of a tweet as positive, negative or neutral (Jiang et al., 2011; Hu et al., 2013; Dong et al., 2014). The majority of existing approaches follow Pang et al. (2002) and employ machine learning algorithms to build classifiers from tweets with manually annotated sentiment polarity. Under this direction, most studies focus on ∗ This work was partly done when the first author was visiting Microsoft Research. 1 This is one of the three sentiment-specific word embedding learning algorithms proposed in Tang et al. (2014). This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creat"
S14-2033,S13-2052,0,0.118118,"Kalchbrenner et al. (2014) model sentence representation with Dynamic Convolutional Neural Network. In this paper, we develop a deep learning system for Twitter sentiment classification. Firstly, we learn sentiment-specific word embedding (SSWE) (Tang et al., 2014), which encodes the sentiment information of text into the continuous representation of words (Mikolov et al., 2013; Sun et al., 2014). Afterwards, we concatenate the SSWE features with the state-of-the-art hand-crafted features (Mohammad et al., 2013), and build the sentiment classifier with the benchmark dataset from SemEval 2013 (Nakov et al., 2013). To learn SSWE, we develop a tailored neural network, which incorporates the supervision from sentiment polarity of tweets in the hybrid loss function. We learn SSWE from tweets, leveraging massive tweets with emoticons as distantsupervised corpora without any manual annotations. We evaluate the deep learning system on the test set of Twitter Sentiment Analysis Track in SemEval 2014 2 . Our system (Coooolll) is ranked 2nd on the Twitter2014 test set, along with the SemEval 2013 participants owning larger training data than us. The performance of only using SSWE as features is comparable to th"
S14-2033,W02-1011,0,0.0200027,"nction 1 to learn SSWE, which encodes the sentiment information of tweets in the continuous representation of words. To obtain large-scale training corpora, we train SSWE from 10M tweets collected by positive and negative emoticons, without any manual annotation. Our system can be easily re-implemented with the publicly available sentiment-specific word embedding. 1 Introduction Twitter sentiment classification aims to classify the sentiment polarity of a tweet as positive, negative or neutral (Jiang et al., 2011; Hu et al., 2013; Dong et al., 2014). The majority of existing approaches follow Pang et al. (2002) and employ machine learning algorithms to build classifiers from tweets with manually annotated sentiment polarity. Under this direction, most studies focus on ∗ This work was partly done when the first author was visiting Microsoft Research. 1 This is one of the three sentiment-specific word embedding learning algorithms proposed in Tang et al. (2014). This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/ 2 http://alt.qcri.org/semeval20"
S14-2033,P11-2008,0,0.291901,"Missing"
S14-2033,P14-1146,1,0.727439,"uru Wei‡ , Bing Qin† , Ting Liu† , Ming Zhou‡ Research Center for Social Computing and Information Retrieval Harbin Institute of Technology, China ‡ Microsoft Research, Beijing, China {dytang, qinb, tliu}@ir.hit.edu.cn {fuwei, mingzhou}@microsoft.com Abstract designing effective features to obtain better classification performance (Pang and Lee, 2008; Liu, 2012; Feldman, 2013). For example, Mohammad et al. (2013) implement diverse sentiment lexicons and a variety of hand-crafted features. To leverage massive tweets containing positive and negative emoticons for automatically feature learning, Tang et al. (2014) propose to learn sentiment-specific word embedding and Kalchbrenner et al. (2014) model sentence representation with Dynamic Convolutional Neural Network. In this paper, we develop a deep learning system for Twitter sentiment classification. Firstly, we learn sentiment-specific word embedding (SSWE) (Tang et al., 2014), which encodes the sentiment information of text into the continuous representation of words (Mikolov et al., 2013; Sun et al., 2014). Afterwards, we concatenate the SSWE features with the state-of-the-art hand-crafted features (Mohammad et al., 2013), and build the sentiment c"
S14-2033,P11-1016,1,0.797284,"res with the state-of-the-art hand-crafted features. We develop a neural network with hybrid loss function 1 to learn SSWE, which encodes the sentiment information of tweets in the continuous representation of words. To obtain large-scale training corpora, we train SSWE from 10M tweets collected by positive and negative emoticons, without any manual annotation. Our system can be easily re-implemented with the publicly available sentiment-specific word embedding. 1 Introduction Twitter sentiment classification aims to classify the sentiment polarity of a tweet as positive, negative or neutral (Jiang et al., 2011; Hu et al., 2013; Dong et al., 2014). The majority of existing approaches follow Pang et al. (2002) and employ machine learning algorithms to build classifiers from tweets with manually annotated sentiment polarity. Under this direction, most studies focus on ∗ This work was partly done when the first author was visiting Microsoft Research. 1 This is one of the three sentiment-specific word embedding learning algorithms proposed in Tang et al. (2014). This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the orga"
S14-2033,H05-1044,0,0.204982,"Missing"
S14-2033,P14-1062,0,0.00858127,"omputing and Information Retrieval Harbin Institute of Technology, China ‡ Microsoft Research, Beijing, China {dytang, qinb, tliu}@ir.hit.edu.cn {fuwei, mingzhou}@microsoft.com Abstract designing effective features to obtain better classification performance (Pang and Lee, 2008; Liu, 2012; Feldman, 2013). For example, Mohammad et al. (2013) implement diverse sentiment lexicons and a variety of hand-crafted features. To leverage massive tweets containing positive and negative emoticons for automatically feature learning, Tang et al. (2014) propose to learn sentiment-specific word embedding and Kalchbrenner et al. (2014) model sentence representation with Dynamic Convolutional Neural Network. In this paper, we develop a deep learning system for Twitter sentiment classification. Firstly, we learn sentiment-specific word embedding (SSWE) (Tang et al., 2014), which encodes the sentiment information of text into the continuous representation of words (Mikolov et al., 2013; Sun et al., 2014). Afterwards, we concatenate the SSWE features with the state-of-the-art hand-crafted features (Mohammad et al., 2013), and build the sentiment classifier with the benchmark dataset from SemEval 2013 (Nakov et al., 2013). To le"
S16-1002,L16-1465,1,0.768602,"Missing"
S16-1002,S15-2080,0,0.0503377,"Missing"
S16-1002,klinger-cimiano-2014-usage,0,0.0621363,"Missing"
S16-1002,P15-2128,0,0.0333833,"Missing"
S16-1002,S16-1003,0,0.0786167,"Missing"
S16-1002,S13-2052,0,0.0105895,"Missing"
S16-1002,piperidis-2012-meta,0,0.0160887,"Missing"
S16-1002,S14-2004,1,0.673256,"Missing"
S16-1002,S15-2082,1,0.813624,"Missing"
S16-1002,S14-2009,0,0.0111835,"Missing"
S16-1002,S15-2078,0,0.0105712,"Missing"
S16-1002,D13-1170,0,0.0173861,"Missing"
S16-1002,E12-2021,0,0.0937892,"Missing"
W08-2134,W05-0627,1,0.833091,"o the predicate (negative for being left to the predicate and positive for right), another feature is formed, namely “Bag of POS (Numbered)”. WIND5 BIGRAM (b3): 5 closest words from both left and right plus the predicate itself, in total 11 words form a “window”, within which bigrams are enumerated. The final optimized feature set for the task of predicate classification is (a1, a21, a23, a71, a72, a73, a74, a81, a82, a83, a84, a9, b11, b12, b22, b3, a71+a9). 3.4 Semantic Role Classification In our system, the identification and classification of semantic roles are achieved in a single stage (Liu et al., 2005) through one single classifier (actually two, one for noun predicates, and the other for verb predicates). Each word in a sentence is given probabilities to be each semantic role (including none of the these roles) for a predicate. Features introduced in addition to those of the previous subsections are the following: POS PATH (c11), REL PATH (c12): The “POS Path” feature consists of POS tags of the words along the path from a word to the predicate. Other than “Up” and “Down”, the “Left” and “Right” direction of the path is added. Similarly, the “Relation Path” feature consists of the relation"
W08-2134,C04-1197,0,0.0272563,"ost Inference. During the Predicate Identification stage we examine each word in a sentence to discover target predicates, including both noun predicates (from NomBank) and verb predicates (from PropBank). In the Predicate Classification stage, each predicate is assigned a certain sense number. For each predicate, the probabilities of a word in the sentence to be each semantic role are predicted in the Semantic Role Classification stage. Maximum entropy model is selected as our classifiers in these stages. Finally an ILP (Integer Linear Programming) based method is adopted for post inference (Punyakanok et al., 2004). 3.2 Predicate Identification The predicate identification is treated as a binary classification problem. Each word in a sentence is predicted to be a predicate or not to be. A set of features are extracted for each word, and an optimized subset of them are adopted in our final system. The following is a full list of the features: DEPREL (a1): Type of relation to the parent. WORD (a21), POS (a22), LEMMA (a23), HEAD (a31), HEAD POS (a32), HEAD LEMMA (a33): The forms, POS tags and lemmas of a word and it’s headword (parent) . FIRST WORD (a41), FIRST POS (a42), FIRST LEMMA (a43), LAST WORD (a51)"
W08-2134,W08-2121,0,0.0848569,"Missing"
W09-1207,W09-1201,0,0.118581,"Missing"
W09-1207,D08-1008,0,0.0936805,"Missing"
W09-1207,kawahara-etal-2002-construction,0,0.0239566,"ntropy classifier is implemented with Maximum Entropy Modeling Toolkit1 . The classifier parameters are tuned with the development data for different languages respectively. lp solve 5.52 is chosen as our ILP problem solver. 1 2 http://homepages.inf.ed.ac.uk/s0450736/maxent toolkit.html http://sourceforge.net/projects/lpsolve 5 Experiments 5.1 Experimental Setup We participate in the CoNLL 2009 shared task with all 7 languages: Catalan (Taul´e et al., 2008), Chinese (Palmer and Xue, 2009), Czech (Hajiˇc et al., 2006), English (Surdeanu et al., 2008), German (Burchardt et al., 2006), Japanese (Kawahara et al., 2002), and Spanish (Taul´e et al., 2008). Besides the closed challenge, we also submitted the open challenge results. Our open challenge strategy is very simple. We add the SRL development data of each language into their training data. The purpose is to examine the effect of the additional data, especially for out-of-domain (ood) data. Three machines (with 2.5GHz Xeon CPU and 16G memory) were used to train our models. During the peak time, Amazon’s EC2 (Elastic Compute Cloud)3 was used, too. Our system requires 15G memory at most and the longest training time is about 36 hours. During training the"
W09-1207,W02-1006,0,0.0162974,"dicate Classification The predicate classification is regarded as a supervised word sense disambiguation (WSD) task here. The task is divided into four steps: 1. Target words selection: predicates with multiple senses appearing in the training data are selected as target words. 2. Feature extraction: features in the context around these target words are extracted as shown in Table 4. The detailed explanation about these features can be found from (Che et al., 2008). 3. Classification: for each target word, a Support Vector Machine (SVM) classifier is used to classify its sense. As reported by Lee and Ng (2002) and Guo et al. (2007), SVM shows good performance on the WSD task. Here libsvm (Chang and Lin, 2001) is used. The linear kernel function is used and the trade off parameter C is 1. 4. Post processing: for each predicate in the test data which does not appear in the training data, its first sense in the frame files is used. 4 Semantic Role Labeling The semantic role labeling (SRL) can be divided into two separate stages: semantic role classification (SRC) and post inference (PI). During the SRC stage, a Maximum entropy (Berger et al., 1996) classifier is used to predict the probabilities of a"
W09-1207,P05-1013,0,0.0387666,"role labeling. 2 Syntactic Dependency Parsing We extend our CoNLL 2008 graph-based model (Che et al., 2008) in four ways: 1. We use bigram features to choose multiple possible syntactic labels for one arc, and decide the optimal label during decoding. 2. We extend the model with sibling features (McDonald, 2006). 3. We extend the model with grandchildren features. Rather than only using the left-most and rightmost grandchildren as Carreras (2007) and Johansson and Nugues (2008) did, we use all left and right grandchildren in our model. 4. We adopt the pseudo-projective approach introduced in (Nivre and Nilsson, 2005) to handle the non-projective languages including Czech, German and English. 49 For each arc, we firstly use unigram features to choose the K1 -best labels. The second parameter of f1lbl (·) indicates whether the node is the head of the arc, and the third parameter indicates the direction. L denotes the whole label set. Then we re-rank the labels by combining the bigram features, and choose K2 -best labels. During decoding, we only use the K2 labels chosen for each arc (K2 ¿ K1 < |L|). 2.2 High-order Model and Algorithm Following the Eisner (2000) algorithm, we use spans as the basic unit. A s"
W09-1207,C04-1197,0,0.029135,"tage anymore. For a predicate of each language, two classifiers (one for noun predicates, and the other for verb predicates) predict probabilities of each word in a sentence to be each semantic role (including virtual role “NULL”). The features used in this stage are listed in Table 4. The probability of each word to be a semantic role for a predicate is given by the SRC stage. The results generated by selecting the roles with the largest probabilities, however, do not satisfy some constrains. As we did in the last year’s system (Che et al., 2008), we use the ILP (Integer Linear Programming) (Punyakanok et al., 2004) to get the global optimization, which is satisfied with three constrains: C1: Each word should be labeled with one and only one label (including the virtual label “NULL”). C2: Roles with a small probability should never be labeled (except for the virtual role “NULL”). The threshold we use in our system is 0.3. C3: Statistics show that some roles (except for the virtual role “NULL”) usually appear once for a predicate. We impose a no-duplicate-roles constraint with a no-duplicate-roles list, which is constructed according to the times of semantic roles’ duplication for each single predicate. T"
W09-1207,W08-2121,0,0.165854,"Missing"
W09-1207,taule-etal-2008-ancora,0,0.0539562,"Missing"
W09-1207,burchardt-etal-2006-salsa,0,\N,Missing
W09-1207,S07-1034,1,\N,Missing
W09-1207,J96-1002,0,\N,Missing
W09-1207,W08-2134,1,\N,Missing
W09-1207,D07-1101,0,\N,Missing
W11-1921,N04-1001,0,\N,Missing
W11-1921,M95-1005,0,\N,Missing
W11-1921,H05-1004,0,\N,Missing
W11-1921,J01-4004,0,\N,Missing
