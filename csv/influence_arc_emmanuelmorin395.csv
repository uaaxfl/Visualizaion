2004.jeptalnrecital-long.13,W02-1402,0,0.0464137,"Missing"
2004.jeptalnrecital-long.13,C94-1084,1,0.703648,"Missing"
2004.jeptalnrecital-long.13,C02-1166,0,0.0777862,"Missing"
2004.jeptalnrecital-long.13,P99-1067,0,0.365691,"Missing"
2009.jeptalnrecital-long.10,P91-1022,0,0.477208,"Missing"
2009.jeptalnrecital-long.10,C02-2020,0,0.0405626,"Missing"
2009.jeptalnrecital-long.10,J93-1003,0,0.146284,"Missing"
2009.jeptalnrecital-long.10,W95-0114,0,0.593283,"Missing"
2009.jeptalnrecital-long.10,P97-1017,0,0.0435192,"Missing"
2009.jeptalnrecital-long.10,2005.jeptalnrecital-long.7,0,0.110646,"Missing"
2009.jeptalnrecital-long.10,P95-1050,0,0.447814,"Missing"
2009.jeptalnrecital-long.10,P99-1067,0,0.369372,"Missing"
2009.jeptalnrecital-long.6,I05-1062,1,0.939263,"Missing"
2009.jeptalnrecital-long.6,C02-1166,0,0.785233,"Missing"
2009.jeptalnrecital-long.6,J93-1003,0,0.375628,"Missing"
2009.jeptalnrecital-long.6,W95-0114,0,0.523774,"Missing"
2009.jeptalnrecital-long.6,W97-0119,0,0.38378,"Missing"
2009.jeptalnrecital-long.6,P07-1084,1,0.891428,"Missing"
2009.jeptalnrecital-long.6,P95-1050,0,0.642892,"Missing"
2009.jeptalnrecital-long.6,P99-1067,0,0.376765,"Missing"
2009.jeptalnrecital-long.6,C96-2098,0,0.140052,"Missing"
2009.mtsummit-posters.14,C02-2020,0,0.750124,"Missing"
2009.mtsummit-posters.14,claveau-2008-automatic,0,0.0239775,"detected transliteration to bilingual resources (see Section 4.1). 3.4.2 Identifying scientific compounds We extracted scientific compounds using a list of 606 medical suffixes and prefixes used in English2 . The process is quite simple: we compile regular expressions for every suffix and prefix and have them matched on the bilingual dictionaries used (see section 4.1). Words extracted are kept with their Japanese translation. Such pairs are then used as anchor points in the alignment process. This list, dedicated to the English language can easily be adapted to French (in accordance with the Claveau (2008) observation). We draw our inspiration from this work to write some simple conversion rules. For example, the -y suffix in English (as in psychology) corresponds to the -ie suffix in French (as in psychologie). After adapting rules to the French language, we performed the same extraction process than with English on the French dictionary, with the converted list of prefixes/suffixes. Some suffixes and prefixes are very productive (especially the a- prefix) and corresponding extracted terms are not necessarily built from this root. All suffixes and prefixes generating more than 1,000 pairs on b"
2009.mtsummit-posters.14,I05-1062,1,0.938445,"Missing"
2009.mtsummit-posters.14,J93-1003,0,0.489932,"is process. Our implementation consists of the following four steps: 1. Building Context-Vector For each lexical unit i, we collect all lexical units in its context and count the number of times these lexical units appear in a window of n words around i. We obtain, for each lexical unit i of the source and the target languages, a context-vector vi which collects the set of co-occurring units j associated with the number of times that j and i occur together. i ¬i j a = occ(i, j) c = occ(¬i, j) ¬j b = occ(i, ¬j) d = occ(¬i, ¬j) Table 1: Contingency table for terms i and j Log Likelihood, eq. 1 (Dunning, 1993), computed from a contingency table (see table 1). λ(i, j) = a log(a) + b log(b) + c log(c) + d log(d) +(a + b + c + d) log(a + b + c + d) −(a + b) log(a + b) − (a + c) log(a + c) −(b + d) log(b + d) − (c + d) log(c + d) (1) 2.2 Results of the direct approach 2. Normalisation of Context-Vector In order to identify specific words in the lexical context and to reduce word frequency artifacts, we normalise context-vectors using an association score. Context-vectors therefore record the association pattern of a word and its neighbours. 3. Translation of the vector Using a bilingual dictionary, we"
2009.mtsummit-posters.14,2005.jeptalnrecital-long.7,0,0.0277188,"e transliterations reflect specialised vocabulary used in document. Finally, Japanese transliteration are easy to identify, since they are written using a set of symbols mostly dedicated to foreign terms, the katakanas. Japanese transliteration are for the most adapted from English, but can be aligned with French term, since French and English share a large common vocabulary. For example, the Japanese term インスリン / i-n-su-ri-n can be aligned with English insulin and with French insuline. We also studied scientific compounds. They are words, in French and in English, built with specifics roots (Namer, 2005). (Claveau, 2007), studying automatic translation of medical vocabulary observe that biomedical terms are built on common Greek and Latine roots, and their derivations are consistent. These compounds are characteristic of a specialised vocabulary, especially in medical documents (Lovis et al., 1997; Namer and Zweigenbaum, 2004). Therefore, they seem to be relevant anchor points in the corpora we are using. Moreover, they can easily be identified from their morphology in French and English. 3.3 Improvement The main idea of this paper is to introduce depth in flat context-vectors, relying on sel"
2009.mtsummit-posters.14,P99-1067,0,0.96696,"arable corpora to show that those elements can efficiently be used to improve the quality of the alignment. 1 Introduction We are currently working on French- and EnglishJapanese term alignment from comparable corpora. Much work has been carried out on bilingual lexicon extraction, used to automatically update linguistic resources. This is especially interesting for specialised vocabulary and is needed by translators since regular bilingual dictionaries can not catch up with the growth of terminology. More specifically since the 90s, studies have focused on extraction from comparable corpora (Rapp, 1999; Fung, 1998). This is partly because there is a lack of parallel corpora, especially for language pairs not involving English. This holds even for language pairs such as French and Japanese, both of which have substantial number of speakers. In contrast, comparable corpora, defined as “sets of texts in different languages that are not translations of each other&quot; (Bowker and Pearson, 2002), are more readily available for wider range of language pairs. It is therefore natural to explore comparable corpora for bilingual term alignment. Kyo Kageura Graduate School of Education University of Tokyo"
2011.jeptalnrecital-long.13,C02-1166,1,0.855233,"Missing"
2011.jeptalnrecital-long.13,W97-0119,0,0.0741503,"Missing"
2011.jeptalnrecital-long.13,P98-1069,0,0.200369,"Missing"
2011.jeptalnrecital-long.13,W09-1117,0,0.0346531,"Missing"
2011.jeptalnrecital-long.13,P04-1067,1,0.88383,"Missing"
2011.jeptalnrecital-long.13,C10-1070,0,0.0270019,"Missing"
2011.jeptalnrecital-long.13,C10-1073,1,0.752074,"Missing"
2011.jeptalnrecital-long.13,P07-1084,1,0.870134,"Missing"
2011.jeptalnrecital-long.13,J03-1002,0,0.0041682,"Missing"
2011.jeptalnrecital-long.13,P99-1067,0,0.145921,"Missing"
2011.jeptalnrecital-long.13,E06-1029,0,0.0447777,"Missing"
2011.jeptalnrecital-long.13,P10-1011,0,0.0250748,"Missing"
2011.jeptalnrecital-long.13,N09-2031,0,0.0263672,"Missing"
2011.jeptalnrecital-long.19,I05-1062,1,0.831123,"Missing"
2011.jeptalnrecital-long.19,C02-1166,0,0.0740113,"Missing"
2011.jeptalnrecital-long.19,J93-1003,0,0.102225,"Missing"
2011.jeptalnrecital-long.19,P98-1069,0,0.230763,"Missing"
2011.jeptalnrecital-long.19,W97-0119,0,0.188053,"Missing"
2011.jeptalnrecital-long.19,C10-1070,0,0.0285164,"Missing"
2011.jeptalnrecital-long.19,2009.jeptalnrecital-long.6,1,0.798033,"Missing"
2011.jeptalnrecital-long.19,2007.mtsummit-papers.26,0,0.100096,"Missing"
2011.jeptalnrecital-long.19,P95-1050,0,0.21892,"Missing"
2011.jeptalnrecital-long.19,2009.mtsummit-posters.26,0,0.0719661,"Missing"
2012.amta-papers.5,C10-1073,0,0.254525,"kly face the problem of data scarcity: in order to extract high-quality lexicons, the corpus must contain text dealing with very specific subject domains and the target and source texts must be highly comparable. If one tries to increase the size of the corpus, one takes the risk of decreasing its quality by lowering its comparability or adding out-of-domain texts. Studies support the idea that the quality of the corpora is more important than its size. Morin et al. (2007) show that the discourse categorization of the documents increases the precision of the lexicon despite the data sparsity. Bo and Gaussier (2010) show that they improve the quality of a lexicon if they improve the comparability of the corpus by selecting a smaller - but more comparable - corpus from an initial set of documents. Consequently, one solution for increasing the number or translation pairs is to focus on identifying translation variants. This paper explores the feasibility of identifying ”fertile” translations in comparable corpora. In parallel texts processing, the notion of fertility has been defined by Brown et al. (1993). They defined the fertility of a source word e as the number of target words to which e is connected"
2012.amta-papers.5,J93-2003,0,0.0559399,"categorization of the documents increases the precision of the lexicon despite the data sparsity. Bo and Gaussier (2010) show that they improve the quality of a lexicon if they improve the comparability of the corpus by selecting a smaller - but more comparable - corpus from an initial set of documents. Consequently, one solution for increasing the number or translation pairs is to focus on identifying translation variants. This paper explores the feasibility of identifying ”fertile” translations in comparable corpora. In parallel texts processing, the notion of fertility has been defined by Brown et al. (1993). They defined the fertility of a source word e as the number of target words to which e is connected in a randomly selected alignment. Similarly, we call a fertile translation a translation pair in which the target term has more words than the source term. We propose to identify such translations with a method mixing morphological analysis and compositional translation : (i) the source term is decomposed into morphemes: postmenopausal is split into post- + menopause1 ; (ii) the morphemes are translated as bound morphemes or fully autonomous words: post- becomes post- or apr`es and menopause b"
2012.amta-papers.5,J96-2004,0,0.108904,"Missing"
2012.amta-papers.5,E09-1016,0,0.414699,"tiword term to multi-word term alignment and uses lexical words3 as atomic components : rate of evap3 as opposed to grammatical words: preposition, determiners, etc. oration is translated into French taux d’´evaporation by translating rate as taux and evaporation as e´ vaporation using dictionary lookup. Recomposition may be done by permutating the translated components (Morin and Daille, 2010) or with translation patterns (Baldwin and Tanaka, 2004). Sublexical compositional translation deals with single-word term translation. The atomic components are subparts of the source single-word term. Cartoni (2009) translates neologisms created by prefixation with a special formalism called Bilingual Lexeme Formation Rules. Atomic components are the prefix and the lexical base: Italian neologism anticonstituzionale ’anticonstitution’ is translated into French anticonstitution by translating the prefix anti- as anti- and the lexical base constituzionale as constitution. Weller et al. (2011) translate two types of single-word term. German single-word term formed by the concatenation of two neoclassical roots are decomposed into these two roots, then the roots are translated into target language roots and"
2012.amta-papers.5,I05-1062,1,0.801395,"bilingual lexicon extraction, compositional translation (CT ) consists in decomposing the source term into atomic components (D), translating these components into the target language (T ), recomposing the translated components into target terms (R) and finally filtering the generated translations with a selection function (S): CT (“ab”) = S(R(T (D(“ab”)))) = S(R(T ({a, b}))) = S(R({T (a) × T (b)})) = S(R({A , B})) = S({A , B}, {B , A}) = “BA” Related work Most of the research work in lexicon extraction from comparable corpora concentrates on samelength term alignment. To our knowledge, only Daille and Morin (2005) and Weller et al. (2011) tried to align terms of different lengths. Daille and Morin (2005) focus on the specific case of multiword terms whose meanings are non-compositional and tried to align these multi-word terms with either single-word terms or multi-word terms using a context-based approach2 .Weller et al. (2011) concentrate on aligning German N OUN -N OUN compounds to N OUN N OUN and N OUN P REP N OUN 1 We use the following notations for morphemes: trailing hyphen for prefixes (a-), leading hyphen for suffixes (-a), both for confixes (-a-) and no hyphen for autonomous morphemes (a). Mo"
2012.amta-papers.5,W97-0119,0,0.30839,"c case of multiword terms whose meanings are non-compositional and tried to align these multi-word terms with either single-word terms or multi-word terms using a context-based approach2 .Weller et al. (2011) concentrate on aligning German N OUN -N OUN compounds to N OUN N OUN and N OUN P REP N OUN 1 We use the following notations for morphemes: trailing hyphen for prefixes (a-), leading hyphen for suffixes (-a), both for confixes (-a-) and no hyphen for autonomous morphemes (a). Morpheme boundaries are represented by a plus sign (+). 2 Context-based methods were introduced by Rapp (1995) and Fung (1997). They consist in comparing the contexts in which the source and target terms occur. Their drawback is that they need the source and target terms to be very frequent. Principle of compositional translation where “ab” is a source term composed of a and b, “BA” is a target term composed of B and A and there exists a bilingual resource linking a to A and b to B. 2.2 Implementations of compositional translation Existing implementations differ on the kind of atomic components they use for translation. Lexical compositional translation (Grefenstette, 1999; Baldwin and Tanaka, 2004; Robitaille et al."
2012.amta-papers.5,W04-3208,0,0.0854358,"anslated texts tend to bear features like explication, simplification, normalization and leveling out. For instance, an English-French comparable corpus may contain the English term post-menopausal but not its “normalized” or “canonical” translation in French (post-m´enopausique). However, there might be some morphological or paraphrastic variants in the French texts like post-m´enopause ’post-menopause’ or apr`es la m´enopause ’after the menopause’. The solution that consists in increasing the size of the corpus in order to find more translation pairs or to extract parallel segments of text (Fung and Cheung, 2004; Rauf and Schwenk, 2009) is only possible when large amounts of texts are available. In the case of the extraction of domain-specific lexicons, we quickly face the problem of data scarcity: in order to extract high-quality lexicons, the corpus must contain text dealing with very specific subject domains and the target and source texts must be highly comparable. If one tries to increase the size of the corpus, one takes the risk of decreasing its quality by lowering its comparability or adding out-of-domain texts. Studies support the idea that the quality of the corpora is more important than"
2012.amta-papers.5,1999.tc-1.8,0,0.02692,"xt-based methods were introduced by Rapp (1995) and Fung (1997). They consist in comparing the contexts in which the source and target terms occur. Their drawback is that they need the source and target terms to be very frequent. Principle of compositional translation where “ab” is a source term composed of a and b, “BA” is a target term composed of B and A and there exists a bilingual resource linking a to A and b to B. 2.2 Implementations of compositional translation Existing implementations differ on the kind of atomic components they use for translation. Lexical compositional translation (Grefenstette, 1999; Baldwin and Tanaka, 2004; Robitaille et al., 2006; Morin and Daille, 2010) deals with multiword term to multi-word term alignment and uses lexical words3 as atomic components : rate of evap3 as opposed to grammatical words: preposition, determiners, etc. oration is translated into French taux d’´evaporation by translating rate as taux and evaporation as e´ vaporation using dictionary lookup. Recomposition may be done by permutating the translated components (Morin and Daille, 2010) or with translation patterns (Baldwin and Tanaka, 2004). Sublexical compositional translation deals with single"
2012.amta-papers.5,I11-1097,0,0.0405941,"was present in the target texts. The final test set for English-to-French experiments contains 1839 morphologically constructed source terms. The test set for English-to-German contains 1824 source terms. 4.3 Resources used in the translation step T Tables 2 and 3 show the size of the resources we used for translation. General language dictionary We used the general language dictionary which is part of the linguistic analysis suite X ELDA. Domain-specific dictionary We built this resource automatically by extracting pairs of cognates from the comparable corpora. We used the same technique as (Hauer and Kondrak, 2011): a SVM classifier trained on examples taken from online dictionaries6 . Morpheme translation table To our knowledge, there exists no publicly available morphology-based bilingual dictionary. Consequently, we asked trans6 http://www.dicts.info/uddl.php lators to create an ad hoc morpheme translation table for our experiment. This morpheme translation table links the English bound morphemes contained in the source terms to their French or German equivalents. The equivalents can be bound morphemes or lexical items. In order to handle the variation phenomena described in section 2.3, we used a di"
2012.amta-papers.5,P07-1084,1,0.915502,"Schwenk, 2009) is only possible when large amounts of texts are available. In the case of the extraction of domain-specific lexicons, we quickly face the problem of data scarcity: in order to extract high-quality lexicons, the corpus must contain text dealing with very specific subject domains and the target and source texts must be highly comparable. If one tries to increase the size of the corpus, one takes the risk of decreasing its quality by lowering its comparability or adding out-of-domain texts. Studies support the idea that the quality of the corpora is more important than its size. Morin et al. (2007) show that the discourse categorization of the documents increases the precision of the lexicon despite the data sparsity. Bo and Gaussier (2010) show that they improve the quality of a lexicon if they improve the comparability of the corpus by selecting a smaller - but more comparable - corpus from an initial set of documents. Consequently, one solution for increasing the number or translation pairs is to focus on identifying translation variants. This paper explores the feasibility of identifying ”fertile” translations in comparable corpora. In parallel texts processing, the notion of fertil"
2012.amta-papers.5,C00-2163,0,0.0365669,"n function uses the entries of the bound morphemes translation table (242 entries) and a list of 85k lexical items composed of the entries of the general language dictionary and English words extracted from the Leipzig Corpus (Quasthoff et al., 2006) which is a general language corpus. 5 5.1 texts of the corpus (Weller et al., 2011; Morin and Daille, 2010) or by using a search engine (Robitaille et al., 2006). Unlike alignment evaluation in parallel texts, there is no reference alignmens to which the selected translations can be compared and we cannot use standard evaluation metrics like AER (Och and Ney, 2000). It is also difficult to find reference lexicons in specific domains since the goal of the extraction process is to create such lexicons. Furthermore, we also wish to evaluate if the algorithm can identify non-canonical translations which, by definition, can not be found in a reference lexicon. Usually, the candidate translations are annotated manually as correct or incorrect by native speakers. Baldwin and Takana (2004) use two standards for evaluation: gold-standard, silver-standard. Gold-standard is the set of candidate translations which correspond to canonical, reference translations. Si"
2012.amta-papers.5,quasthoff-etal-2006-corpus,0,0.0808403,"xes EN→FR 38k→60k 6.7k→6.7k 242→729 50→134 185→574 7→21 EN→DE 38k→70k 6.4k→6.4k 242→761 50→166 185→563 7→32 Table 2: Nb. of entries in the multilingual resources Synonyms Morphol. EN→EN 5.1k→7.6k 5.9k→15k FR→FR 2.4k→3.2k 7.1k→18k DE→DE 4.2k→4.9k 7.4k→16k Table 3: Nb. of entries in the monolingual resources 4.4 Resources used in the decomposition step (D) The decomposition function uses the entries of the bound morphemes translation table (242 entries) and a list of 85k lexical items composed of the entries of the general language dictionary and English words extracted from the Leipzig Corpus (Quasthoff et al., 2006) which is a general language corpus. 5 5.1 texts of the corpus (Weller et al., 2011; Morin and Daille, 2010) or by using a search engine (Robitaille et al., 2006). Unlike alignment evaluation in parallel texts, there is no reference alignmens to which the selected translations can be compared and we cannot use standard evaluation metrics like AER (Och and Ney, 2000). It is also difficult to find reference lexicons in specific domains since the goal of the extraction process is to create such lexicons. Furthermore, we also wish to evaluate if the algorithm can identify non-canonical translation"
2012.amta-papers.5,P95-1050,0,0.383095,"s on the specific case of multiword terms whose meanings are non-compositional and tried to align these multi-word terms with either single-word terms or multi-word terms using a context-based approach2 .Weller et al. (2011) concentrate on aligning German N OUN -N OUN compounds to N OUN N OUN and N OUN P REP N OUN 1 We use the following notations for morphemes: trailing hyphen for prefixes (a-), leading hyphen for suffixes (-a), both for confixes (-a-) and no hyphen for autonomous morphemes (a). Morpheme boundaries are represented by a plus sign (+). 2 Context-based methods were introduced by Rapp (1995) and Fung (1997). They consist in comparing the contexts in which the source and target terms occur. Their drawback is that they need the source and target terms to be very frequent. Principle of compositional translation where “ab” is a source term composed of a and b, “BA” is a target term composed of B and A and there exists a bilingual resource linking a to A and b to B. 2.2 Implementations of compositional translation Existing implementations differ on the kind of atomic components they use for translation. Lexical compositional translation (Grefenstette, 1999; Baldwin and Tanaka, 2004; R"
2012.amta-papers.5,E09-1003,0,0.0594834,"bear features like explication, simplification, normalization and leveling out. For instance, an English-French comparable corpus may contain the English term post-menopausal but not its “normalized” or “canonical” translation in French (post-m´enopausique). However, there might be some morphological or paraphrastic variants in the French texts like post-m´enopause ’post-menopause’ or apr`es la m´enopause ’after the menopause’. The solution that consists in increasing the size of the corpus in order to find more translation pairs or to extract parallel segments of text (Fung and Cheung, 2004; Rauf and Schwenk, 2009) is only possible when large amounts of texts are available. In the case of the extraction of domain-specific lexicons, we quickly face the problem of data scarcity: in order to extract high-quality lexicons, the corpus must contain text dealing with very specific subject domains and the target and source texts must be highly comparable. If one tries to increase the size of the corpus, one takes the risk of decreasing its quality by lowering its comparability or adding out-of-domain texts. Studies support the idea that the quality of the corpora is more important than its size. Morin et al. (2"
2012.amta-papers.5,E06-1029,0,0.744304,") and Fung (1997). They consist in comparing the contexts in which the source and target terms occur. Their drawback is that they need the source and target terms to be very frequent. Principle of compositional translation where “ab” is a source term composed of a and b, “BA” is a target term composed of B and A and there exists a bilingual resource linking a to A and b to B. 2.2 Implementations of compositional translation Existing implementations differ on the kind of atomic components they use for translation. Lexical compositional translation (Grefenstette, 1999; Baldwin and Tanaka, 2004; Robitaille et al., 2006; Morin and Daille, 2010) deals with multiword term to multi-word term alignment and uses lexical words3 as atomic components : rate of evap3 as opposed to grammatical words: preposition, determiners, etc. oration is translated into French taux d’´evaporation by translating rate as taux and evaporation as e´ vaporation using dictionary lookup. Recomposition may be done by permutating the translated components (Morin and Daille, 2010) or with translation patterns (Baldwin and Tanaka, 2004). Sublexical compositional translation deals with single-word term translation. The atomic components are s"
2015.jeptalnrecital-court.16,S14-2149,0,0.0554541,"Missing"
2015.jeptalnrecital-court.16,C00-1044,0,0.193329,"Missing"
2015.jeptalnrecital-court.16,P10-1060,0,0.0647889,"Missing"
2015.jeptalnrecital-court.16,S14-2076,0,0.057384,"Missing"
2015.jeptalnrecital-court.16,2010.jeptalnrecital-court.26,0,0.0766736,"Missing"
2015.jeptalnrecital-court.16,P04-1035,0,0.0374245,"Missing"
2015.jeptalnrecital-court.17,saggion-2004-identifying,0,0.0948425,"Missing"
2015.jeptalnrecital-court.30,C04-1008,0,0.750028,"Missing"
2016.jeptalnrecital-long.14,P93-1002,0,0.662704,"Missing"
2016.jeptalnrecital-long.14,C02-2020,0,0.175148,"Missing"
2016.jeptalnrecital-long.14,P91-1017,0,0.676568,"Missing"
2016.jeptalnrecital-long.14,C02-1166,0,0.151418,"Missing"
2016.jeptalnrecital-long.14,J93-1003,0,0.190903,"Missing"
2016.jeptalnrecital-long.14,W95-0114,0,0.464512,"Missing"
2016.jeptalnrecital-long.14,W97-0119,0,0.372832,"Missing"
2016.jeptalnrecital-long.14,I13-1196,1,0.814261,"Missing"
2016.jeptalnrecital-long.14,W05-0809,0,0.108135,"Missing"
2016.jeptalnrecital-long.14,P07-1084,1,0.812567,"Missing"
2016.jeptalnrecital-long.14,P14-1121,1,0.851589,"Missing"
2016.jeptalnrecital-long.14,2001.mtsummit-papers.46,0,0.234148,"Missing"
2016.jeptalnrecital-long.14,P95-1050,0,0.521078,"Missing"
2016.jeptalnrecital-long.14,P99-1067,0,0.420831,"Missing"
2016.jeptalnrecital-long.14,I11-2003,1,0.893912,"Missing"
2016.jeptalnrecital-long.14,W99-0602,0,0.195673,"Missing"
2016.jeptalnrecital-long.3,W04-3240,0,0.15121,"Missing"
2016.jeptalnrecital-long.3,E12-1079,0,0.0552368,"Missing"
2016.jeptalnrecital-long.3,W13-4034,0,0.0386318,"Missing"
2016.jeptalnrecital-long.3,P05-2014,0,0.100989,"Missing"
2016.jeptalnrecital-long.3,D10-1084,0,0.0711291,"Missing"
2016.jeptalnrecital-long.3,Y12-1050,0,0.0419759,"Missing"
2016.jeptalnrecital-long.3,U06-1007,0,0.0530622,"Missing"
2016.jeptalnrecital-long.3,W15-4640,0,0.072811,"Missing"
2016.jeptalnrecital-long.3,D11-1069,0,0.0404232,"Missing"
2016.jeptalnrecital-long.3,W12-1616,0,0.0606554,"Missing"
2016.jeptalnrecital-long.3,W13-4017,0,0.0297467,"Missing"
2016.jeptalnrecital-poster.18,D10-1101,0,0.103036,"Missing"
2016.jeptalnrecital-poster.18,P10-1060,0,0.0489217,"Missing"
2016.jeptalnrecital-poster.18,2010.jeptalnrecital-court.26,0,0.124434,"Missing"
2016.jeptalnrecital-poster.18,S16-1002,0,0.0519231,"Missing"
2016.jeptalnrecital-poster.18,S15-2082,0,0.061116,"Missing"
2016.jeptalnrecital-poster.18,W03-1014,0,0.349202,"Missing"
2016.jeptalnrecital-poster.18,S15-2127,0,0.0159046,"es travaux plus récents en fouille d’opinion ciblée font usage de l’analyse en dépendances syntaxiques dans le but d’identifier les relations entre un sujet discuté et un marqueur d’opinion. Dans cette voie, Qiu et al. (2009) montrent qu’une sélection manuelle de règles d’extraction reposant sur l’analyse en dépendances permet d’atteindre des performances intéressantes à la fois en extraction d’opinion et en inférence de polarité. Jakob & Gurevych (2010) exploitent les dépendances syntaxiques comme traits d’un modèle markovien CRF (Conditional Random Field) dans le même but. Enfin, Liu et al. (2015) proposent une méthode syntaxique permettant de s’affranchir de la sélection manuelle de règles, et conservant des résultats intéressants. Un état de l’art détaillé des méthodes de fouille d’opinion ciblée peut être observé au 1. http://mpqa.cs.pitt.edu/lexicons/subj_lexicon/ 2. http://sentiwordnet.isti.cnr.it/ 3. http://emolex.u-grenoble3.fr/emoBase/ travers des campagnes d’évaluation S EM E VAL 2014 à 2016 (Pontiki et al., 2014, 2015, 2016). Les systèmes montrant les meilleurs résultats en identification des cibles d’opinion (San Vicente et al., 2015; Toh & Su, 2015) conservent cependant un"
2016.jeptalnrecital-poster.18,W02-1028,0,0.217512,"Missing"
2016.jeptalnrecital-poster.18,S15-2083,0,0.0334385,"Missing"
2016.jeptalnrecital-poster.18,H05-1044,0,0.21113,"Missing"
2018.jeptalnrecital-long.14,P17-1031,0,0.0617533,"Missing"
2018.jeptalnrecital-long.14,W14-4012,0,0.104688,"Missing"
2018.jeptalnrecital-long.14,D14-1179,0,0.0302477,"Missing"
2018.jeptalnrecital-long.14,N16-1055,0,0.0578219,"Missing"
2018.jeptalnrecital-long.14,P17-1184,0,0.0226671,"Missing"
2018.jeptalnrecital-long.2,D12-1050,0,0.535284,"Gaussier, 2010; Morin & Daille, 2012; Mikolov et al., 2013a; Xing et al., 2015; Artetxe et al., 2016; Hazem & Morin, 2016). Deux classes d’approches ont été développées selon la nature du terme à aligner. La première classe s’intéresse à l’alignement de mots et termes simples et repose sur des approches distributionnelles tandis que la seconde classe porte sur l’alignement de termes complexes et repose sur des approches compositionnelles. Peu de travaux se sont intéressés à proposer un cadre unifié permettant de réaliser l’alignement des termes simples et complexes en dehors de Delpech et al. (2012) pour l’alignement de termes simples vers des termes complexes. Notre objectif est de proposer un tel cadre unifié permettant l’alignement de termes de longueurs variables en domaine de spécialité. Nous proposons d’adapter l’approche compositionnelle étendue pour prendre en compte les termes simples et complexes. En outre, nous proposons d’améliorer l’approche standard pour l’alignement de termes simples qui est exploitée dans l’approche compositionnelle étendue. 2 Approches standard et compositionnelle Nous présentons dans cette section les approches existantes pour l’alignement de termes sim"
2018.jeptalnrecital-long.2,W03-1802,0,0.196917,"Missing"
2018.jeptalnrecital-long.2,P16-4003,0,0.0605024,"Missing"
2018.jeptalnrecital-long.2,C12-1046,1,0.900651,"Missing"
2018.jeptalnrecital-long.2,W95-0114,0,0.667362,"Missing"
2018.jeptalnrecital-long.2,1999.tc-1.8,0,0.0262001,"Missing"
2018.jeptalnrecital-long.2,C16-1321,1,0.892287,"Missing"
2018.jeptalnrecital-long.2,C10-1073,0,0.0729369,"Missing"
2018.jeptalnrecital-long.2,C12-1110,1,0.879758,"Missing"
2018.jeptalnrecital-long.2,D14-1162,0,0.0821718,"Missing"
2018.jeptalnrecital-long.2,P99-1067,0,0.447966,"Missing"
2018.jeptalnrecital-long.2,E06-1029,0,0.0905529,"Missing"
2018.jeptalnrecital-long.2,C02-1065,0,0.183649,"Missing"
2018.jeptalnrecital-long.2,N15-1104,0,0.0769135,"Missing"
2018.jeptalnrecital-long.9,Q17-1010,0,0.0429356,"Missing"
2018.jeptalnrecital-long.9,N16-1014,0,0.0908495,"Missing"
2018.jeptalnrecital-long.9,I17-1074,0,0.0465813,"Missing"
2018.jeptalnrecital-long.9,D16-1230,0,0.0610485,"Missing"
2018.jeptalnrecital-long.9,W02-0109,0,0.0145446,"Missing"
2018.jeptalnrecital-long.9,P17-1103,0,0.0453081,"Missing"
2018.jeptalnrecital-long.9,W15-4640,0,0.0508913,"Missing"
2018.jeptalnrecital-long.9,D14-1162,0,0.0806229,"Missing"
2018.jeptalnrecital-long.9,D17-1235,0,0.0465593,"Missing"
2018.jeptalnrecital-long.9,D13-1096,0,0.0523453,"Missing"
2018.jeptalnrecital-long.9,E17-1042,0,0.0634447,"Missing"
2018.jeptalnrecital-long.9,P17-1046,0,0.0418753,"Missing"
2019.jeptalnrecital-long.6,bechet-etal-2012-decoda,0,0.0382693,"Missing"
2019.jeptalnrecital-long.6,devillers-etal-2004-french,0,0.0594883,"Missing"
2019.jeptalnrecital-long.6,esteve-etal-2010-epac,1,0.527742,"Missing"
2019.jeptalnrecital-long.6,giraudel-etal-2012-repere,0,0.0191512,"Missing"
2019.jeptalnrecital-long.6,gravier-etal-2012-etape,0,0.0581911,"Missing"
2019.jeptalnrecital-long.6,W11-0411,0,0.0410345,"Missing"
2019.jeptalnrecital-long.6,N19-1119,0,0.0453567,"Missing"
2020.bucc-1.9,D19-1134,0,0.0115186,"y of the seed lexicons, and finally, Section 6 concludes our work. Cross-lingual word embeddings learning has triggered great attention in the recent years and several bilingual supervised (Mikolov et al., 2013; Xing et al., 2015; Artetxe et al., 2018a) and unsupervised (Artetxe et al., 2018b; Conneau et al., 2017) alignment methods have been proposed so far. Also, multilingual alignment approaches which consists in mapping several languages in one common space via a pivot language (Smith et al., 2017) or by training all language pairs simultaneously (Chen and Cardie, 2018; Wada et al., 2019; Taitelbaum et al., 2019b; Taitelbaum et al., 2019a; Alaux et al., 2018) are attracting a great attention. Among possible downstream applications of cross-lingual embedding models: Bilingual Lexicon Induction (BLI) which consists in the identification of translation pairs based on a comparable corpus. The BUCC shared task offers the first evaluation framework on BLI from comparable corpora. It covers six languages (English, French, German, Russian, Spanish and Chinese) and two corpora (Wikipedia and WaCKy). We describe in this paper our participation at the BLI shared task. We start by evaluating the cross-lingual wo"
2020.bucc-1.9,P19-1300,0,0.0111168,"iscusses the quality of the seed lexicons, and finally, Section 6 concludes our work. Cross-lingual word embeddings learning has triggered great attention in the recent years and several bilingual supervised (Mikolov et al., 2013; Xing et al., 2015; Artetxe et al., 2018a) and unsupervised (Artetxe et al., 2018b; Conneau et al., 2017) alignment methods have been proposed so far. Also, multilingual alignment approaches which consists in mapping several languages in one common space via a pivot language (Smith et al., 2017) or by training all language pairs simultaneously (Chen and Cardie, 2018; Wada et al., 2019; Taitelbaum et al., 2019b; Taitelbaum et al., 2019a; Alaux et al., 2018) are attracting a great attention. Among possible downstream applications of cross-lingual embedding models: Bilingual Lexicon Induction (BLI) which consists in the identification of translation pairs based on a comparable corpus. The BUCC shared task offers the first evaluation framework on BLI from comparable corpora. It covers six languages (English, French, German, Russian, Spanish and Chinese) and two corpora (Wikipedia and WaCKy). We describe in this paper our participation at the BLI shared task. We start by evalua"
2020.bucc-1.9,N15-1104,0,0.217484,"compared to individual strategies, except for English-Russian and Russian-English language pairs for which the concatenation approach was preferred. Keywords: Bilingual lexicon induction, Comparable corpora, Cognates, Word embeddings 1. Introduction sets, Section 3 presents the tested approaches and the chosen strategy. The results are given in Section 4, Section 5 discusses the quality of the seed lexicons, and finally, Section 6 concludes our work. Cross-lingual word embeddings learning has triggered great attention in the recent years and several bilingual supervised (Mikolov et al., 2013; Xing et al., 2015; Artetxe et al., 2018a) and unsupervised (Artetxe et al., 2018b; Conneau et al., 2017) alignment methods have been proposed so far. Also, multilingual alignment approaches which consists in mapping several languages in one common space via a pivot language (Smith et al., 2017) or by training all language pairs simultaneously (Chen and Cardie, 2018; Wada et al., 2019; Taitelbaum et al., 2019b; Taitelbaum et al., 2019a; Alaux et al., 2018) are attracting a great attention. Among possible downstream applications of cross-lingual embedding models: Bilingual Lexicon Induction (BLI) which consists"
2020.bucc-1.9,D18-1024,0,0.0172524,"Missing"
2020.bucc-1.9,P18-1073,0,0.115454,"ual strategies, except for English-Russian and Russian-English language pairs for which the concatenation approach was preferred. Keywords: Bilingual lexicon induction, Comparable corpora, Cognates, Word embeddings 1. Introduction sets, Section 3 presents the tested approaches and the chosen strategy. The results are given in Section 4, Section 5 discusses the quality of the seed lexicons, and finally, Section 6 concludes our work. Cross-lingual word embeddings learning has triggered great attention in the recent years and several bilingual supervised (Mikolov et al., 2013; Xing et al., 2015; Artetxe et al., 2018a) and unsupervised (Artetxe et al., 2018b; Conneau et al., 2017) alignment methods have been proposed so far. Also, multilingual alignment approaches which consists in mapping several languages in one common space via a pivot language (Smith et al., 2017) or by training all language pairs simultaneously (Chen and Cardie, 2018; Wada et al., 2019; Taitelbaum et al., 2019b; Taitelbaum et al., 2019a; Alaux et al., 2018) are attracting a great attention. Among possible downstream applications of cross-lingual embedding models: Bilingual Lexicon Induction (BLI) which consists in the identification"
2020.bucc-1.9,L18-1550,0,0.0175764,"portant questions about the evaluation process and suggest a careful handcrafted validation which will undoubtedly strengthen the BLI shared task. Table 4: Ratio of target words per source words for the validation lists for some language pair on different lists results than other language pairs (10 to almost 30 points). Unlike other pairs trained on WaCKy, this pair is the only one trained on Wikipedia, contradicting the idea that ”the WaCKy corpora seem somewhat better suited for the dictionary induction task than Wikipedia”. To verify this statement, we used pre-trained word embeddings from Grave et al. (2018) to check if the corpus was really the main problem. And actually, using the pre-trained embeddings on Wikipedia or Common Crawl led to much better results than the results obtained using the WaCKy corpora, reaching about the same F1-score as the English-Spanish language pair. Our final results for the shared task were reported from the mixed approach for all language pairs but the two with Russian, for which we only took the results from the concatenation approach. 6. 5. Seed Lexicon Analysis Conclusion We presented in this paper the participation of the TALN/LS2N team at the BUCC shared task"
2020.bucc-1.9,C18-1080,1,0.932267,"nsists in the identification of translation pairs based on a comparable corpus. The BUCC shared task offers the first evaluation framework on BLI from comparable corpora. It covers six languages (English, French, German, Russian, Spanish and Chinese) and two corpora (Wikipedia and WaCKy). We describe in this paper our participation at the BLI shared task. We start by evaluating the cross-lingual word embedding mapping approach (VecMap) (Artetxe et al., 2018a) using fastText embeddings. Then, we present an extension of VecMap approach that uses the concatenation of two mapped embedding models (Hazem and Morin, 2018). Finally, we present a cognates matching approach, merely an exact match string similarity. Based on the obtained results of the studied approaches, we derive our proposed system –Mix (Conc + Dist)– which combines the outputs of the embeddings concatenation and the cognates matching approaches. Overall, the obtained results on the validation data sets are in favor of our system for all language pairs except for English-Russian and Russian-English pairs, where the cognates matching approach obviously showed very weak results and for which the concatenation approach was preferred. In the follow"
2020.bucc-1.9,D14-1162,0,0.092491,"Missing"
2020.bucc-1.9,D19-1363,0,0.0115401,"y of the seed lexicons, and finally, Section 6 concludes our work. Cross-lingual word embeddings learning has triggered great attention in the recent years and several bilingual supervised (Mikolov et al., 2013; Xing et al., 2015; Artetxe et al., 2018a) and unsupervised (Artetxe et al., 2018b; Conneau et al., 2017) alignment methods have been proposed so far. Also, multilingual alignment approaches which consists in mapping several languages in one common space via a pivot language (Smith et al., 2017) or by training all language pairs simultaneously (Chen and Cardie, 2018; Wada et al., 2019; Taitelbaum et al., 2019b; Taitelbaum et al., 2019a; Alaux et al., 2018) are attracting a great attention. Among possible downstream applications of cross-lingual embedding models: Bilingual Lexicon Induction (BLI) which consists in the identification of translation pairs based on a comparable corpus. The BUCC shared task offers the first evaluation framework on BLI from comparable corpora. It covers six languages (English, French, German, Russian, Spanish and Chinese) and two corpora (Wikipedia and WaCKy). We describe in this paper our participation at the BLI shared task. We start by evaluating the cross-lingual wo"
2020.coling-main.527,D16-1250,0,0.0197432,". Using these approaches, words are embedded into a low-dimensional vector space. For instance, Mikolov et al. (2013) proposed an approach to learn a linear transformation from the source language into the target language. Faruqui and Dyer (2014) used the Canonical Correlation Analysis to project the embeddings in both languages into a shared vector space. Xing et al. (2015) proposed an orthogonal transformation based on normalized word vectors to improve the inconsistency among the objective function used to learn the word vectors and to learn the linear transformation matrix. More recently, Artetxe et al. (2016; 2018a) proposed and then improved an approach that generalizes previous works in order to preserve monolingual invariance using several meaningful and intuitive constraints (i.e. orthogonality, vector length normalization, mean centering, whitening, etc.). Finally, Conneau et al. (2017) and Artetxe et al. (2018b) proposed unsupervised mapping methods getting results close to supervised methods. A thorough comparison of cross-lingual word embeddings has been proposed by Ruder (2017). A comparison of the approaches of Mikolov et al. (2013) and Faruqui and Dyer (2014) with the standard bag of w"
2020.coling-main.527,P18-1073,0,0.0116683,"n both languages into a shared vector space. Xing et al. (2015) proposed an orthogonal transformation based on normalized word vectors to improve the inconsistency among the objective function used to learn the word vectors and to learn the linear transformation matrix. More recently, Artetxe et al. (2016; 2018a) proposed and then improved an approach that generalizes previous works in order to preserve monolingual invariance using several meaningful and intuitive constraints (i.e. orthogonality, vector length normalization, mean centering, whitening, etc.). Finally, Conneau et al. (2017) and Artetxe et al. (2018b) proposed unsupervised mapping methods getting results close to supervised methods. A thorough comparison of cross-lingual word embeddings has been proposed by Ruder (2017). A comparison of the approaches of Mikolov et al. (2013) and Faruqui and Dyer (2014) with the standard bag of words approach was carried out by Jakubina and Langlais (2017). The authors showed that embedding approaches perform well when the terms to be translated occur very frequently while the standard approach is slightly better when the terms are less frequent. On specialized domains, Hazem and Morin (2018) compared di"
2020.coling-main.527,D11-1033,0,0.03061,"arginal phenomenon in technical and scientific domains. In this sense, associating out-of-domain data with a specialized comparable corpus can be seen as a “heresy” or, as in the previous examples, an unfortunate way to introduce polysemy. In the context of Statistical Machine Translation (SMT), Wang et al. (2014) demonstrated that adding out-of-domain data to the training material of a system was detrimental when translating scientific and technical domains. Instead of using a data augmentation strategy, data selection is proposed to improve the quality of SMT systems (Moore and Lewis, 2010; Axelrod et al., 2011; Wang et al., 2014). The basic idea is that in-domain training data can be enriched with suitable sub-parts of out-of-domain data. Within this context, we apply two well-established data selection techniques often used in machine translation that is: Tf-Idf and cross entropy. We also propose for the first time to exploit BERT for data selection. We show that a subtle selection of the contexts of out-of-domain data allows to improve the representation of specialized domains, while preventing the introduction of polysemy induced by data augmentation. Overall, all the proposed techniques improve"
2020.coling-main.527,P13-2133,0,0.0168824,"l., 2020). In technical and scientific domains, comparable corpora suffer from their modest and limited size compared to general language corpora. This phenomenon is amplified by the difficulty to obtain many specialized documents in a language other than English (Morin and Hazem, 2014). This aspect, which is gradually changing with the deployment of open access data, is nevertheless a major difficulty. One way to overcome the limited size of specialized (in-domain) comparable corpora for bilingual lexicon induction (BLI) is to associate them with out-of-domain resources as lexical databases (Bouamor et al., 2013) or large general domain corpora (Hazem and Morin, 2018). For instance, by combining a specialized comparable corpus with a general domain corpus, methods based on distributional analysis are boosted (Hazem and Morin, 2018). General domain corpora greatly enhance the representation of specialized vocabulary by adding new contexts. The main drawbacks of this data augmentation approach is the introduction of polysemous information as well as the tremendous increase of computation time. Consider for instance, a French/English comparable corpus in the medical domain and the French terms os (bone)"
2020.coling-main.527,C02-2020,0,0.19169,"1998; Rapp, 1999), known as the standard bag of words approach, builds a context vector for each word of the source and the target languages, translates the source context vectors into the target language using a bilingual seed lexicon, and compares the translated context vectors to each target context vector using a similarity measure. While this approach gives interesting results for large general comparable corpora (Gaussier et al., 2004; Laroche and Langlais, 2010; Vuli´c et al., 2011), it is rather unsuitable for small specialized comparable corpora due to the context vectors sparsity. (Chiao and Zweigenbaum, 2002; Morin et al., 2007; Prochasson et al., 2009). More recent distributed approaches, based on deep neural network models (Mikolov et al., 2013), have come to renew traditional ones. Using these approaches, words are embedded into a low-dimensional vector space. For instance, Mikolov et al. (2013) proposed an approach to learn a linear transformation from the source language into the target language. Faruqui and Dyer (2014) used the Canonical Correlation Analysis to project the embeddings in both languages into a shared vector space. Xing et al. (2015) proposed an orthogonal transformation based"
2020.coling-main.527,N19-1423,0,0.101375,"1 represents the history of the word wi . Formally, HLMI,s (W ) represents the cross entropy of the sentence W given the language model LMI,s . The cross entropy is computed for every sentence of the out-of-domain corpus given the out-of-domain and in-domain language models. The source and target sentences are then evaluated by HLMI,b (WO ) − HLMO,b (WO ) (where b ∈ {s, t} refers to the side of the corpus) and ranked accordingly. The cross entropy is computed using the xenC tool (Rousseau, 2013). BERT is a supervised learning model that has proven to be efficient in many downstream NLP tasks (Devlin et al., 2019). It has been trained on the Masked Language Model (MLM) and Next Sentence Prediction (NSP) objectives (Devlin et al., 2019). Hence, it can be used whether for word representation or for sentence classification. In order to perform data selection, we use BERT as a binary classifier to predict if a given input sentence is in-domain or out-of-domain. The intuition behind this strategy is that BERT can learn shared features between in-domain sentences. For each positive (in-domain) training sentence, we randomly select a negative sentence from a general domain data set to keep a balanced training"
2020.coling-main.527,P19-2041,0,0.0527158,"Missing"
2020.coling-main.527,E14-1049,0,0.0298046,"ier et al., 2004; Laroche and Langlais, 2010; Vuli´c et al., 2011), it is rather unsuitable for small specialized comparable corpora due to the context vectors sparsity. (Chiao and Zweigenbaum, 2002; Morin et al., 2007; Prochasson et al., 2009). More recent distributed approaches, based on deep neural network models (Mikolov et al., 2013), have come to renew traditional ones. Using these approaches, words are embedded into a low-dimensional vector space. For instance, Mikolov et al. (2013) proposed an approach to learn a linear transformation from the source language into the target language. Faruqui and Dyer (2014) used the Canonical Correlation Analysis to project the embeddings in both languages into a shared vector space. Xing et al. (2015) proposed an orthogonal transformation based on normalized word vectors to improve the inconsistency among the objective function used to learn the word vectors and to learn the linear transformation matrix. More recently, Artetxe et al. (2016; 2018a) proposed and then improved an approach that generalizes previous works in order to preserve monolingual invariance using several meaningful and intuitive constraints (i.e. orthogonality, vector length normalization, m"
2020.coling-main.527,P04-1067,0,0.106425,"Missing"
2020.coling-main.527,C16-1321,1,0.818387,"a scientific portal vs crawled from the web). On the other hand, for the general corpora, W IKI is way bigger and diverse in terms of topics than JRC. Table 1 shows the size of each side of the corpora and their vocabulary. Corpus BC WE JRC W IKI English # tokens # types 525,934 14,800 311,898 15,344 64.2M 229,836 300M 3M French # tokens # types 521,262 11,746 656,178 15,799 70.3M 231,126 300M 3.1M Table 1: Content words’ size corpora. The bilingual terminology reference lists required to evaluate the quality of bilingual terminology induction from comparable corpora are the same as used in (Hazem and Morin, 2016) and was derived from the UMLS meta-thesaurus for the breast cancer domain and are provided with the corpora for wind energy domain (see footnote 2). Each word in the reference list appears at least 5 times in the specialized corpus. The reference list for the breast cancer is composed of a set of 248 French/English single word pairs and of 145 single word pairs for the wind energy domain. We are aware that the reference lists are small, but considering the fact that we are in a specialized domain, they remain representative of its vocabulary, and getting larger lists is difficult since the sp"
2020.coling-main.527,C18-1080,1,0.856764,"ble corpora suffer from their modest and limited size compared to general language corpora. This phenomenon is amplified by the difficulty to obtain many specialized documents in a language other than English (Morin and Hazem, 2014). This aspect, which is gradually changing with the deployment of open access data, is nevertheless a major difficulty. One way to overcome the limited size of specialized (in-domain) comparable corpora for bilingual lexicon induction (BLI) is to associate them with out-of-domain resources as lexical databases (Bouamor et al., 2013) or large general domain corpora (Hazem and Morin, 2018). For instance, by combining a specialized comparable corpus with a general domain corpus, methods based on distributional analysis are boosted (Hazem and Morin, 2018). General domain corpora greatly enhance the representation of specialized vocabulary by adding new contexts. The main drawbacks of this data augmentation approach is the introduction of polysemous information as well as the tremendous increase of computation time. Consider for instance, a French/English comparable corpus in the medical domain and the French terms os (bone) and sein (breast). When looking for additional contexts"
2020.coling-main.527,E17-2096,1,0.84798,"ed an approach that generalizes previous works in order to preserve monolingual invariance using several meaningful and intuitive constraints (i.e. orthogonality, vector length normalization, mean centering, whitening, etc.). Finally, Conneau et al. (2017) and Artetxe et al. (2018b) proposed unsupervised mapping methods getting results close to supervised methods. A thorough comparison of cross-lingual word embeddings has been proposed by Ruder (2017). A comparison of the approaches of Mikolov et al. (2013) and Faruqui and Dyer (2014) with the standard bag of words approach was carried out by Jakubina and Langlais (2017). The authors showed that embedding approaches perform well when the terms to be translated occur very frequently while the standard approach is slightly better when the terms are less frequent. On specialized domains, Hazem and Morin (2018) compared different approaches and observed that the one described by Bojanowski et al. (2016) (an enhanced variant of the skip-gram and C-BOW models) outperformed the others. Recently, Peters et al. (2018) proposed a deep contextualized word representation that improves the state-of-the-art across different challenging NLP tasks. This representation is use"
2020.coling-main.527,C10-1070,0,0.0199229,"taining a gain of about 4 points in MAP while decreasing computation time by a factor of 10. 2 Related Work The historical distributional approach (Fung, 1998; Rapp, 1999), known as the standard bag of words approach, builds a context vector for each word of the source and the target languages, translates the source context vectors into the target language using a bilingual seed lexicon, and compares the translated context vectors to each target context vector using a similarity measure. While this approach gives interesting results for large general comparable corpora (Gaussier et al., 2004; Laroche and Langlais, 2010; Vuli´c et al., 2011), it is rather unsuitable for small specialized comparable corpora due to the context vectors sparsity. (Chiao and Zweigenbaum, 2002; Morin et al., 2007; Prochasson et al., 2009). More recent distributed approaches, based on deep neural network models (Mikolov et al., 2013), have come to renew traditional ones. Using these approaches, words are embedded into a low-dimensional vector space. For instance, Mikolov et al. (2013) proposed an approach to learn a linear transformation from the source language into the target language. Faruqui and Dyer (2014) used the Canonical C"
2020.coling-main.527,C10-1073,0,0.0329296,"to automate the process of generating and extending dictionaries from bilingual corpora is a well known Natural Language Processing (NLP) application. Initially conducted on parallel data, this modus operandi has rapidly moved to using comparable corpora, mainly due to their availability and the fact that they are easier to acquire (Sharoff et al., 2013). Comparable corpora gather texts of the same domain, often over the same period without being in a translation relation. Journalistic or encyclopedic websites as well as web crawl data are usually popular for this purpose (Rapp, 1999; Li and Gaussier, 2010; Rapp et al., 2020). In technical and scientific domains, comparable corpora suffer from their modest and limited size compared to general language corpora. This phenomenon is amplified by the difficulty to obtain many specialized documents in a language other than English (Morin and Hazem, 2014). This aspect, which is gradually changing with the deployment of open access data, is nevertheless a major difficulty. One way to overcome the limited size of specialized (in-domain) comparable corpora for bilingual lexicon induction (BLI) is to associate them with out-of-domain resources as lexical"
2020.coling-main.527,P10-2041,0,0.336699,"often considered as a marginal phenomenon in technical and scientific domains. In this sense, associating out-of-domain data with a specialized comparable corpus can be seen as a “heresy” or, as in the previous examples, an unfortunate way to introduce polysemy. In the context of Statistical Machine Translation (SMT), Wang et al. (2014) demonstrated that adding out-of-domain data to the training material of a system was detrimental when translating scientific and technical domains. Instead of using a data augmentation strategy, data selection is proposed to improve the quality of SMT systems (Moore and Lewis, 2010; Axelrod et al., 2011; Wang et al., 2014). The basic idea is that in-domain training data can be enriched with suitable sub-parts of out-of-domain data. Within this context, we apply two well-established data selection techniques often used in machine translation that is: Tf-Idf and cross entropy. We also propose for the first time to exploit BERT for data selection. We show that a subtle selection of the contexts of out-of-domain data allows to improve the representation of specialized domains, while preventing the introduction of polysemy induced by data augmentation. Overall, all the propo"
2020.coling-main.527,P14-1121,1,0.796818,"and the fact that they are easier to acquire (Sharoff et al., 2013). Comparable corpora gather texts of the same domain, often over the same period without being in a translation relation. Journalistic or encyclopedic websites as well as web crawl data are usually popular for this purpose (Rapp, 1999; Li and Gaussier, 2010; Rapp et al., 2020). In technical and scientific domains, comparable corpora suffer from their modest and limited size compared to general language corpora. This phenomenon is amplified by the difficulty to obtain many specialized documents in a language other than English (Morin and Hazem, 2014). This aspect, which is gradually changing with the deployment of open access data, is nevertheless a major difficulty. One way to overcome the limited size of specialized (in-domain) comparable corpora for bilingual lexicon induction (BLI) is to associate them with out-of-domain resources as lexical databases (Bouamor et al., 2013) or large general domain corpora (Hazem and Morin, 2018). For instance, by combining a specialized comparable corpus with a general domain corpus, methods based on distributional analysis are boosted (Hazem and Morin, 2018). General domain corpora greatly enhance th"
2020.coling-main.527,P07-1084,1,0.566345,"the standard bag of words approach, builds a context vector for each word of the source and the target languages, translates the source context vectors into the target language using a bilingual seed lexicon, and compares the translated context vectors to each target context vector using a similarity measure. While this approach gives interesting results for large general comparable corpora (Gaussier et al., 2004; Laroche and Langlais, 2010; Vuli´c et al., 2011), it is rather unsuitable for small specialized comparable corpora due to the context vectors sparsity. (Chiao and Zweigenbaum, 2002; Morin et al., 2007; Prochasson et al., 2009). More recent distributed approaches, based on deep neural network models (Mikolov et al., 2013), have come to renew traditional ones. Using these approaches, words are embedded into a low-dimensional vector space. For instance, Mikolov et al. (2013) proposed an approach to learn a linear transformation from the source language into the target language. Faruqui and Dyer (2014) used the Canonical Correlation Analysis to project the embeddings in both languages into a shared vector space. Xing et al. (2015) proposed an orthogonal transformation based on normalized word"
2020.coling-main.527,N18-1202,0,0.00783104,". A comparison of the approaches of Mikolov et al. (2013) and Faruqui and Dyer (2014) with the standard bag of words approach was carried out by Jakubina and Langlais (2017). The authors showed that embedding approaches perform well when the terms to be translated occur very frequently while the standard approach is slightly better when the terms are less frequent. On specialized domains, Hazem and Morin (2018) compared different approaches and observed that the one described by Bojanowski et al. (2016) (an enhanced variant of the skip-gram and C-BOW models) outperformed the others. Recently, Peters et al. (2018) proposed a deep contextualized word representation that improves the state-of-the-art across different challenging NLP tasks. This representation is useful to model different types of syntactic and semantic information about words-in-context. It is thus possible from a general 6003 language corpus to have different embeddings for a given word, with each of these embeddings reflecting one of the word’s meanings. In specialized domains, it is widely accepted that words can have only one meaning even if their contexts may convey different meanings in a general language corpus. El Boukkouri et al"
2020.coling-main.527,2009.mtsummit-posters.14,1,0.689516,"words approach, builds a context vector for each word of the source and the target languages, translates the source context vectors into the target language using a bilingual seed lexicon, and compares the translated context vectors to each target context vector using a similarity measure. While this approach gives interesting results for large general comparable corpora (Gaussier et al., 2004; Laroche and Langlais, 2010; Vuli´c et al., 2011), it is rather unsuitable for small specialized comparable corpora due to the context vectors sparsity. (Chiao and Zweigenbaum, 2002; Morin et al., 2007; Prochasson et al., 2009). More recent distributed approaches, based on deep neural network models (Mikolov et al., 2013), have come to renew traditional ones. Using these approaches, words are embedded into a low-dimensional vector space. For instance, Mikolov et al. (2013) proposed an approach to learn a linear transformation from the source language into the target language. Faruqui and Dyer (2014) used the Canonical Correlation Analysis to project the embeddings in both languages into a shared vector space. Xing et al. (2015) proposed an orthogonal transformation based on normalized word vectors to improve the inc"
2020.coling-main.527,2020.bucc-1.2,0,0.0199533,"process of generating and extending dictionaries from bilingual corpora is a well known Natural Language Processing (NLP) application. Initially conducted on parallel data, this modus operandi has rapidly moved to using comparable corpora, mainly due to their availability and the fact that they are easier to acquire (Sharoff et al., 2013). Comparable corpora gather texts of the same domain, often over the same period without being in a translation relation. Journalistic or encyclopedic websites as well as web crawl data are usually popular for this purpose (Rapp, 1999; Li and Gaussier, 2010; Rapp et al., 2020). In technical and scientific domains, comparable corpora suffer from their modest and limited size compared to general language corpora. This phenomenon is amplified by the difficulty to obtain many specialized documents in a language other than English (Morin and Hazem, 2014). This aspect, which is gradually changing with the deployment of open access data, is nevertheless a major difficulty. One way to overcome the limited size of specialized (in-domain) comparable corpora for bilingual lexicon induction (BLI) is to associate them with out-of-domain resources as lexical databases (Bouamor e"
2020.coling-main.527,P99-1067,0,0.318478,"slation equivalents to automate the process of generating and extending dictionaries from bilingual corpora is a well known Natural Language Processing (NLP) application. Initially conducted on parallel data, this modus operandi has rapidly moved to using comparable corpora, mainly due to their availability and the fact that they are easier to acquire (Sharoff et al., 2013). Comparable corpora gather texts of the same domain, often over the same period without being in a translation relation. Journalistic or encyclopedic websites as well as web crawl data are usually popular for this purpose (Rapp, 1999; Li and Gaussier, 2010; Rapp et al., 2020). In technical and scientific domains, comparable corpora suffer from their modest and limited size compared to general language corpora. This phenomenon is amplified by the difficulty to obtain many specialized documents in a language other than English (Morin and Hazem, 2014). This aspect, which is gradually changing with the deployment of open access data, is nevertheless a major difficulty. One way to overcome the limited size of specialized (in-domain) comparable corpora for bilingual lexicon induction (BLI) is to associate them with out-of-domai"
2020.coling-main.527,tiedemann-2012-parallel,0,0.0292801,"itle or the keywords contain the term breast cancer in English and its translation in French. The Wind Energy corpus (WE) has been released within the TTC project2 and is composed of documents harvested from the Web using a focused crawler based on several keywords such as wind, energy, rotor in English and their translation in French. We considered two separate out-of-domain data sets in English and French: i) JRC acquis corpus (JRC) composed of legislative texts of the European Union3 (we used the French/English version at OPUS which is based on the paragraph-aligned corpus provided by JRC (Tiedemann, 2012)) and ii) a fraction of Wikipedia corpus (W IKI)4 . The used corpora differ in size and quality. On the one hand, for the specialized corpora, BC is of better quality than WE, due to their construction methods (crawled from a scientific portal vs crawled from the web). On the other hand, for the general corpora, W IKI is way bigger and diverse in terms of topics than JRC. Table 1 shows the size of each side of the corpora and their vocabulary. Corpus BC WE JRC W IKI English # tokens # types 525,934 14,800 311,898 15,344 64.2M 229,836 300M 3M French # tokens # types 521,262 11,746 656,178 15,79"
2020.coling-main.527,P11-2084,0,0.0836974,"Missing"
2020.coling-main.527,N15-1104,0,0.019821,"to the context vectors sparsity. (Chiao and Zweigenbaum, 2002; Morin et al., 2007; Prochasson et al., 2009). More recent distributed approaches, based on deep neural network models (Mikolov et al., 2013), have come to renew traditional ones. Using these approaches, words are embedded into a low-dimensional vector space. For instance, Mikolov et al. (2013) proposed an approach to learn a linear transformation from the source language into the target language. Faruqui and Dyer (2014) used the Canonical Correlation Analysis to project the embeddings in both languages into a shared vector space. Xing et al. (2015) proposed an orthogonal transformation based on normalized word vectors to improve the inconsistency among the objective function used to learn the word vectors and to learn the linear transformation matrix. More recently, Artetxe et al. (2016; 2018a) proposed and then improved an approach that generalizes previous works in order to preserve monolingual invariance using several meaningful and intuitive constraints (i.e. orthogonality, vector length normalization, mean centering, whitening, etc.). Finally, Conneau et al. (2017) and Artetxe et al. (2018b) proposed unsupervised mapping methods ge"
2020.jeptalnrecital-jep.8,galibert-etal-2014-etape,0,0.0354513,"Missing"
2020.jeptalnrecital-jep.8,gravier-etal-2012-etape,0,0.060583,"Missing"
2020.jeptalnrecital-jep.8,W11-0411,1,0.781368,"Missing"
2020.jeptalnrecital-jep.8,J98-4004,0,0.318247,"Missing"
2020.jeptalnrecital-jep.8,N16-1030,0,0.152007,"Missing"
2020.jeptalnrecital-jep.8,P10-1052,0,0.0882307,"Missing"
2020.jeptalnrecital-jep.8,P16-1101,0,0.0527671,"Missing"
2020.lrec-1.556,I11-1142,1,0.788645,"TAPE (Galibert et al., 2014). Since the ETAPE’s results publication in 2012, no new work were published, to the best of our knowledge, on named entity recognition from speech for Quaero-like treestructured French data. Tree-structured named entities can not be tackled as a simple sequence labeling task. At the time of the ETAPE campaign, state-of-the-art works focused on multiple processing steps before rebuilding a tree structure. Conditional Random Field (Lafferty et al., 2001) (CRF) are in the core of these previous sequence labeling approaches. Some approaches (Dinarelli and Rosset, 2012; Dinarelli and Rosset, 2011) used Probabilistic Context-Free Grammar (Johnson, 1998) (PCFG) in complement of CRF to implement a cascade model. CRF was trained on components information and PCFG was used to predict the whole entity tree. The ETAPE winning NER system (Raymond, 2013) only used CRF models with one model per base entity. Most of the typical approaches for named entity recognition from speech follows a two steps pipeline, with first an ASR system and then a NER system on automatic transcriptions produced by the ASR system. In this configuration, the NER component must deal with an imperfect transcription of sp"
2020.lrec-1.556,E12-1018,1,0.802446,"French evaluation campaign ETAPE (Galibert et al., 2014). Since the ETAPE’s results publication in 2012, no new work were published, to the best of our knowledge, on named entity recognition from speech for Quaero-like treestructured French data. Tree-structured named entities can not be tackled as a simple sequence labeling task. At the time of the ETAPE campaign, state-of-the-art works focused on multiple processing steps before rebuilding a tree structure. Conditional Random Field (Lafferty et al., 2001) (CRF) are in the core of these previous sequence labeling approaches. Some approaches (Dinarelli and Rosset, 2012; Dinarelli and Rosset, 2011) used Probabilistic Context-Free Grammar (Johnson, 1998) (PCFG) in complement of CRF to implement a cascade model. CRF was trained on components information and PCFG was used to predict the whole entity tree. The ETAPE winning NER system (Raymond, 2013) only used CRF models with one model per base entity. Most of the typical approaches for named entity recognition from speech follows a two steps pipeline, with first an ASR system and then a NER system on automatic transcriptions produced by the ASR system. In this configuration, the NER component must deal with an"
2020.lrec-1.556,galibert-etal-2014-etape,0,0.730244,"seeks to locate and classify named entity mentions in unstructured text into pre-defined categories (such as person names, organizations, locations, ...). Quaero project (Grouin et al., 2011) is at the initiative of an extended definition of named entity for French data. This extended version has a multilevel tree structure, where base entities are combined to define more complex ones. With the extended definition, named entity recognition consists in the detection, the classification and the decomposition of the entities. This new definition was used for the French evaluation campaign ETAPE (Galibert et al., 2014). Since the ETAPE’s results publication in 2012, no new work were published, to the best of our knowledge, on named entity recognition from speech for Quaero-like treestructured French data. Tree-structured named entities can not be tackled as a simple sequence labeling task. At the time of the ETAPE campaign, state-of-the-art works focused on multiple processing steps before rebuilding a tree structure. Conditional Random Field (Lafferty et al., 2001) (CRF) are in the core of these previous sequence labeling approaches. Some approaches (Dinarelli and Rosset, 2012; Dinarelli and Rosset, 2011)"
2020.lrec-1.556,giraudel-etal-2012-repere,0,0.0475205,"Missing"
2020.lrec-1.556,goryainova-etal-2014-morpho,1,0.845632,") and test (7 hours). These data have manual transcriptions and are fully manually annotated with named entities concepts. Our training data were augmented with the Quaero corpus (Grouin et al., 2011). This corpus is composed of data recorded from French radio and TV stations between 1998 and 2004. These data are made up of 100 hours of speech manually transcribed and fully annotated with named entities following the Quaero annotation guideline. 5.2. Automatic speech recognition In this study, we used several corpora (ESTER 1&2 (Galliano et al., 2009), REPERE (Giraudel et al., 2012) and VERA (Goryainova et al., 2014)) for a total of around 220 hours of speech. These data are used for the acoustic model training of the kaldi ASR system of the pipeline approach. The LM of this approach was trained using the speech transcripts augmented with several French newspapers (see section 4.2.3 in (Del´eglise et al., 2009)). For ASR parts, our pipeline system and our E2E system use the same dataset except for the speech of ETAPE train dataset which is used only with our E2E approach. 6. Experiments All our experiments are evaluated on the ETAPE test set with the Slot Error Rate (SER) metric (Makhoul et al., 1999) def"
2020.lrec-1.556,gravier-etal-2012-etape,0,0.0460721,"Missing"
2020.lrec-1.556,W11-0411,1,0.682334,"to do structured NER. Finally, we compare the performances of ETAPE’s systems (state-of-the-art systems in 2012) with the performances obtained using current technologies. The results show the interest of the E2E approach, which however remains below an updated pipeline approach. Keywords: Named Entity Recognition, Automatic Speech Recognition, Tree-structured Named Entity, End-to-End 1. Introduction Named entity recognition seeks to locate and classify named entity mentions in unstructured text into pre-defined categories (such as person names, organizations, locations, ...). Quaero project (Grouin et al., 2011) is at the initiative of an extended definition of named entity for French data. This extended version has a multilevel tree structure, where base entities are combined to define more complex ones. With the extended definition, named entity recognition consists in the detection, the classification and the decomposition of the entities. This new definition was used for the French evaluation campaign ETAPE (Galibert et al., 2014). Since the ETAPE’s results publication in 2012, no new work were published, to the best of our knowledge, on named entity recognition from speech for Quaero-like treest"
2020.lrec-1.556,J98-4004,0,0.294477,"n 2012, no new work were published, to the best of our knowledge, on named entity recognition from speech for Quaero-like treestructured French data. Tree-structured named entities can not be tackled as a simple sequence labeling task. At the time of the ETAPE campaign, state-of-the-art works focused on multiple processing steps before rebuilding a tree structure. Conditional Random Field (Lafferty et al., 2001) (CRF) are in the core of these previous sequence labeling approaches. Some approaches (Dinarelli and Rosset, 2012; Dinarelli and Rosset, 2011) used Probabilistic Context-Free Grammar (Johnson, 1998) (PCFG) in complement of CRF to implement a cascade model. CRF was trained on components information and PCFG was used to predict the whole entity tree. The ETAPE winning NER system (Raymond, 2013) only used CRF models with one model per base entity. Most of the typical approaches for named entity recognition from speech follows a two steps pipeline, with first an ASR system and then a NER system on automatic transcriptions produced by the ASR system. In this configuration, the NER component must deal with an imperfect transcription of speech. As a result, the quality of automatic transcriptio"
2020.lrec-1.556,N16-1030,0,0.0999424,"r named entity recognition from speech follows a two steps pipeline, with first an ASR system and then a NER system on automatic transcriptions produced by the ASR system. In this configuration, the NER component must deal with an imperfect transcription of speech. As a result, the quality of automatic transcriptions has a major impact on NER performances (Ben Jannet et al., 2015). In 2012, HMM-GMM implementations were still the stateof-the-art approaches for ASR technologies. Since this date, the great contribution of neural approaches for NER and ASR tasks were demonstrated. Recent studies (Lample et al., 2016; Ma and Hovy, 2016) improve the NER accuracy by using a combination of bidirectional Long Short-Term Memory (bLSTM) and CRF layers. Other studies (Tomashenko et al., 2016) are based on a combination of HMM and Deep Neural Network (DNN) to reach ASR state-of-the-art performances. Lately, some E2E approaches for Named Entity Recognition from speech have been proposed in (Ghannay et al., 2018). In this work, the E2E systems will learn an alignment between audio and manual transcription enriched with NE without tree-structure. Other works use End-to-End approach to map directly speech to intent i"
2020.lrec-1.556,P10-1052,0,0.0667031,"Missing"
2020.lrec-1.556,P16-1101,0,0.420815,"ition from speech follows a two steps pipeline, with first an ASR system and then a NER system on automatic transcriptions produced by the ASR system. In this configuration, the NER component must deal with an imperfect transcription of speech. As a result, the quality of automatic transcriptions has a major impact on NER performances (Ben Jannet et al., 2015). In 2012, HMM-GMM implementations were still the stateof-the-art approaches for ASR technologies. Since this date, the great contribution of neural approaches for NER and ASR tasks were demonstrated. Recent studies (Lample et al., 2016; Ma and Hovy, 2016) improve the NER accuracy by using a combination of bidirectional Long Short-Term Memory (bLSTM) and CRF layers. Other studies (Tomashenko et al., 2016) are based on a combination of HMM and Deep Neural Network (DNN) to reach ASR state-of-the-art performances. Lately, some E2E approaches for Named Entity Recognition from speech have been proposed in (Ghannay et al., 2018). In this work, the E2E systems will learn an alignment between audio and manual transcription enriched with NE without tree-structure. Other works use End-to-End approach to map directly speech to intent instead of map speech"
2020.lrec-1.556,E99-1023,0,0.156231,"de <loc.adm.town <name paris &gt; &gt; &gt;”. org.adm/loc.adm.town are Named Entities types with subtypes and kind/name are components. With the Quaero definition of named entity, NER consists in entity detection, classification and decomposition. Since this new definition is used for the French evaluation campaign ETAPE, the task in this study consists in Quaero named entity extraction from speech. 3. 3.1. problem by reducing all labels related to a word into a single one. Figure 1 illustrates an example of this concatenation. Pipelines systems 3-pass implementation Our NER systems use standard BIO2 (Sang and Veenstra, 1999) format. This standard consists of writing a column file with first the words column and then the labels column. There is one couple of word/label per line and two different sentences are separated by an empty line. The label of a word corresponds to the named entity concept in which the word is located. This label is prefixed by a ”B-” or an ”I-” depending on the position of the word in the concept. ”B-” (Begin) is used to prefixed the label of the first word and ”I-” (Inside) for all the others. ”O” (Outside) is the label used for words that are not inside a concept. Due to the structure of"
C12-1046,C10-1073,0,0.354473,"mainspecific lexicons, we quickly face the problem of data scarcity: in order to extract high-quality lexicons, the corpus must contain text dealing with very specific subject domains and the target and source texts must be highly comparable. If one tries to increase the size of the corpus, one takes the risk of decreasing its quality by adding out-of-domain texts. Studies support the idea that the quality of the corpora is more important than its size. Morin et al. (2007) show that the discourse categorization of the documents increases the precision of the lexicon despite the data sparsity. Bo & Gaussier (2010) show that they improve the quality of the extracted lexicon if they improve the comparability of the corpus by selecting a smaller – but more comparable – corpus from an initial set of documents. This paper proposes methods for ranking and extracting canonical translations as well as translation variants, with a special focus on the extraction of fertile translations. In parallel texts processing, the notion of fertility has been defined by Brown et al. (1993). They defined the fertility of a source word e as the number of target words to which e is connected in a randomly selected alignment."
C12-1046,J93-2003,0,0.0340751,"al. (2007) show that the discourse categorization of the documents increases the precision of the lexicon despite the data sparsity. Bo & Gaussier (2010) show that they improve the quality of the extracted lexicon if they improve the comparability of the corpus by selecting a smaller – but more comparable – corpus from an initial set of documents. This paper proposes methods for ranking and extracting canonical translations as well as translation variants, with a special focus on the extraction of fertile translations. In parallel texts processing, the notion of fertility has been defined by Brown et al. (1993). They defined the fertility of a source word e as the number of target words to which e is connected in a randomly selected alignment. Similarly, we call a fertile translation a translation pair in which the target term has more words than the source term. The identification of fertile translations is useful because (i) they frequentlty correspond to non-canonical translations, e.g. paraphrastic variants and (ii) they tend to correspond to vulgarized forms of technical terms (e.g. « cytotoxic » vs. « toxic to the cells ») which are useful when the translator translates lay science texts. Up t"
C12-1046,J96-2004,0,0.0158976,"t translation is a canonical translation like cytoprotection → Zellschutz (DE), protection des cellules 'protection of the cells' (FR). An acceptable translation is a variant of the canonical translation: cytoprotection → protéger les cellules 'protect the cells', cytoprotecteur 'cytoprotective'. A related translation is a translation which is only semantically related to the source term: insecure → ohne Sicherheit 'without safety'. All other translations are wrong translations. We computed inter-annotator agreement on a set of 100 randomly selected translations. We used the Kappa statistics (Carletta, 1996) and obtained a high agreement (0.77 for English to German translations and 0.71 for English to French). 4 4.1 Results Related work Generally, systems are compared using the TopN precision: the percentage of source terms with at least one exact translation among the TopN candidate translations. Compositional-translation methods tend to give better results when they are applied to general language texts rather than domain-specific texts. Indeed, it is easier to find translations of the components since they belong to the general language and large corpora are also easier to collect. Working wit"
C12-1046,E09-1016,0,0.233667,", no hyphen for autonomous morphemes (a) and a plus sign (+) for intra-word morpheme boundaries. The term confix is borrowed from Martinet (1979) and refers to neoclassical (Latin or Ancient Greek) roots. 747 taux d'évaporation by translating rate to taux and evaporation to évaporation using dictionary lookup. Recomposition may be done by permuting the translated components (Morin & Daille, 2010) or with translation patterns (Baldwin & Tanaka, 2004). Sublexical compositional translation deals with single-word term translation. The atomic components are subparts of the source single-word term. Cartoni (2009) translates neologisms created by prefixation with a formalism called Bilingual Lexeme Formation Rules. Atomic components are the prefix and the lexical base: Italian neologism ricostruire 'rebuild' is translated into French reconstruire by translating the prefix ri- to re- and the lexical base costruire as construire. Weller et al. (2011) translate two types of single-word term. German single-word terms formed by the concatenation of two neoclassical roots are decomposed into these two roots, then the roots are translated into target language roots and recomposed into an English or French sin"
C12-1046,R11-1048,0,0.0237131,"p to now, fertility has received little attention in the field of comparable corpora processing. To our knowledge, only Daille & Morin (2005) and Weller et al. (2011) tried to extract translation pairs of different lengths from comparable corpora. Daille & Morin (2005) focus on the specific case of multi-word terms whose meaning is not compositional and tried to align these multi-word terms with either single-word terms or multi-word terms using a context-based approach. Weller 746 et al. (2011) concentrate on translating noun compounds as noun phrases. Similar to the approach presented here, Claveau & Kijak (2011) use translation equivalences between morphemes to generate translations and can handle fertility. However it is not suited for comparable corpora since it requires domain-specific parallel data (in their case, a multilingual terminology) to learn alignment probabilities. Our method is based on compositional translation. We chose this approach because: (i) according to Namer & Baud (2007), compositional terms form a major part of the new terms found in technical and scientific domains, this is not restricted to the field of biomedicine as it is generally believed ; (ii) compositionality-based"
C12-1046,I05-1062,1,0.647465,"elected alignment. Similarly, we call a fertile translation a translation pair in which the target term has more words than the source term. The identification of fertile translations is useful because (i) they frequentlty correspond to non-canonical translations, e.g. paraphrastic variants and (ii) they tend to correspond to vulgarized forms of technical terms (e.g. « cytotoxic » vs. « toxic to the cells ») which are useful when the translator translates lay science texts. Up to now, fertility has received little attention in the field of comparable corpora processing. To our knowledge, only Daille & Morin (2005) and Weller et al. (2011) tried to extract translation pairs of different lengths from comparable corpora. Daille & Morin (2005) focus on the specific case of multi-word terms whose meaning is not compositional and tried to align these multi-word terms with either single-word terms or multi-word terms using a context-based approach. Weller 746 et al. (2011) concentrate on translating noun compounds as noun phrases. Similar to the approach presented here, Claveau & Kijak (2011) use translation equivalences between morphemes to generate translations and can handle fertility. However it is not su"
C12-1046,W11-4610,1,0.826635,"quence, one of the difficulties with comparable corpora is that the translation of a source term may not be present in its “normalized” or “canonical” form but rather in the form of a morphological or paraphrastic variant (e.g. postmenopausal translates to après la ménopause 'after the menopause' instead of postménopausique). Another limitation is that algorithms output, for each source term, a set of candidate translations instead of just one target term. This state of affairs makes it very challenging for translators to use lexicons extracted from comparable corpora in real-life situations (Delpech, 2011). The solution that consists in increasing the size of the corpus in order to find more translation pairs or to extract parallel segments of text (Fung & Cheung, 2004; Rauf & Schwenk, 2009) is only possible when large amounts of texts are available. In the case of the extraction of domainspecific lexicons, we quickly face the problem of data scarcity: in order to extract high-quality lexicons, the corpus must contain text dealing with very specific subject domains and the target and source texts must be highly comparable. If one tries to increase the size of the corpus, one takes the risk of d"
C12-1046,2012.amta-papers.5,1,0.309745,"ns of the four. Its output is a set of single or multi-word candidate translations. For instance, postoophorectomy may be translated to postovariectomie 'postoophorectomy' or après l'ovariectomie 'after the oophorectomy' or après l'ablation des ovaires 'after the removal of the ovaries'. Section 3.2 explains the algorithm for generating candidate translations. Section 3.3 describes different methods for ranking the candidate translations. 2.2 Generation algorithm The generation method is described in the algorithm 1. A detailed version of the algorithm can be found in the feasibility study of Delpech et al. (2012). Algorithm 1 Generate translations Require: source_term, target_corpus translations ← Ø for all {c1, … ci} in DECOMPOSE (source_term) do for all {e1, … ej} in CONCATENATE ({c1, … ci}) do for all {t1, … tk} in {TRANSLATE (e1) × … TRANSLATE (ej)} do if k ≠ j then continue for all{t1, … tk} in PERMUTATE ({t1, … tk}) do for all {w1, … wl} in CONCATENATE ({t1, … tk}) do for all match in MATCH ({w1, … wl}, target_corpus) do add match to translations return translations The DECOMPOSE function splits the source term into minimal components {c1, … ci} by matching substrings of the term with lists of p"
C12-1046,J93-1003,0,0.0205869,"rce phrase to the target phrase. From these alignments, we obtain P(y|x) with the following formula: ∑ P ( y∣x)= p (t∣ s) {a∈ A∣ poss =x , post =y } ∑ p (t∣s) {a∈ A∣ poss=x } The context similarity (CONT) corresponds to the method used for ranking translations in context-based approaches. For each source term and target term we build a context vector. This vector indicates the number of times the term co-occurs with each word of the corpus within a 2 http://www.temis.com 751 contextual window of 5 words around the term. The number of co-occurrences is normalized with the log-likelihood ratio (Dunning, 1993). Then, the vector of the source term is translated into the target language. Finally, the source vector and the target vector are compared: the most similar the vectors, the most likely the target and source terms are translations of each other. The similarity between source vector s and target vector t is computed with the weighted jaccard: ∑ WeightedJaccard ( s , t)= min( c( s , w ) , c(t , w )) w∈ s∩t ∑ max (c( s , w) , c(t , w ))+ w∈s ∩t ∑ w ∈ s ∖t c( s , w)+ ∑ c (t , w) w∈t∖ s where c(s, w), respectively c(t, w), is the normalized number of co-occurrences between the source, respectively"
C12-1046,W97-0119,0,0.310218,"n for hekurudhë). 1.2 Ranking and selection methods Generally, compositional translation generates several possible translations for one source term. One has to find a way to rank the translations from the most to the least reliable. Garera & Yarowsky (2008) tried two ranking methods: (i) a probability score P based on the number of different languages exhibiting the association between the literal gloss and the fluent translation ; (ii) the probability score P combined with the similarity of the source and target words' contexts using context-based methods like in the work of Rapp (1995) and Fung (1997). Robitaille et al. (2006) extract translation pairs from a corpus built by querying a search engine with a set of seed translation pairs. They select the candidate translations which are semantically related to the target seed terms. The semantic similarity measure is based on the number of hits containing the seed term and/or the candidate translation (Jaccard coefficient). Other works simply select the candidate translations which occur in the target corpus (Weller et al., 2001 ; Morin and Daille, 2010) or which are significantly attested on the Web (Cartoni, 2009). Only Baldwin and Takana"
C12-1046,W04-3208,0,0.0176812,"ather in the form of a morphological or paraphrastic variant (e.g. postmenopausal translates to après la ménopause 'after the menopause' instead of postménopausique). Another limitation is that algorithms output, for each source term, a set of candidate translations instead of just one target term. This state of affairs makes it very challenging for translators to use lexicons extracted from comparable corpora in real-life situations (Delpech, 2011). The solution that consists in increasing the size of the corpus in order to find more translation pairs or to extract parallel segments of text (Fung & Cheung, 2004; Rauf & Schwenk, 2009) is only possible when large amounts of texts are available. In the case of the extraction of domainspecific lexicons, we quickly face the problem of data scarcity: in order to extract high-quality lexicons, the corpus must contain text dealing with very specific subject domains and the target and source texts must be highly comparable. If one tries to increase the size of the corpus, one takes the risk of decreasing its quality by adding out-of-domain texts. Studies support the idea that the quality of the corpora is more important than its size. Morin et al. (2007) sho"
C12-1046,I08-1053,0,0.108481,"refix ri- to re- and the lexical base costruire as construire. Weller et al. (2011) translate two types of single-word term. German single-word terms formed by the concatenation of two neoclassical roots are decomposed into these two roots, then the roots are translated into target language roots and recomposed into an English or French single-word term, e.g. Kalori1metrie2 is translated as calori1metry2. German NOUN1+NOUN2 compounds are translated into French and English NOUN1 NOUN2 or NOUN1 PREP NOUN2 multi-word terms, e.g. Elektronen N1-mikroskopN2 is translated to electronN1 microscopeN2. Garera & Yarowsky (2008) translate various compound sequences (NOUN1+NOUN2, ADJ1+NOUN2 …). They generate an English literal gloss of the compounds with the compositional method (for instance, the English gloss for the Albanian word hekurudhë 'railway' is iron path). Then, they search for entries in Lx-to-English dictionaries where the entry in language Lx is a word-to-word translation of the English gloss (e.g. iron path matches the German entry Eisenbahn and the Italian entry ferrovia). The final candidate translations are the fluent English translations proposed by the bilingual dictionaries (e.g. Eisenbahn and fer"
C12-1046,1999.tc-1.8,0,0.148055,"5). Once the candidate translations have been generated, one generally ranks them and selects the TopN candidate translations. Generation methods are described in section 1.1. Ranking methods are described in section 2.3. 1.1 Generation methods Compositional translation consists in decomposing the source term into atomic components, translating these components into the target language and recomposing the translated components into target terms. Existing implementations differ on the kind of atomic components they use for translation. Lexical compositional translation (Baldwin & Tanaka, 2004; Grefenstette, 1999; Morin & Daille, 2009; Robitaille et al., 2006) deals with multi-word term to multi-word term alignment and uses lexical words as atomic components: rate of evaporation is translated into French as 1 We use the following notations: trailing hyphen for prefixes (a-), leading hyphen for suffixes (-a), both for confixes (a-), no hyphen for autonomous morphemes (a) and a plus sign (+) for intra-word morpheme boundaries. The term confix is borrowed from Martinet (1979) and refers to neoclassical (Latin or Ancient Greek) roots. 747 taux d'évaporation by translating rate to taux and evaporation to é"
C12-1046,I11-1097,0,0.0204218,"5k 451.75k 398.9k TABLE 1: Composition and size of corpora (nb. of words) 3.2 Resources for generation Tables 2 and 3 show the size of the resources we used for generation. General language dictionary: We used the dictionary which is part of the XELDA software. This dictionary was used for generating translations but also for computing the corpus comparability and for translating the context vectors for the context similarity measure (CONT score). Cognate dictionary: We built this resource automatically by extracting pairs of cognates from the comparable corpora. We used the same technique as Hauer & Kondrak (2011): a SVM classifier trained on examples taken from online dictionaries 6. Morpheme translation table: this resource was created manually by translators since there exists no publicly available morphology-based bilingual dictionary. This translation table links the English bound morphemes contained in the source terms to their French or German equivalents (which can be bound morphemes or lexical items). In order to handle the variation phenomena described in section 1.3, we used a dictionary of synonyms and lists of morphologically related words. The dictionary of synonyms is part of the XELDA s"
C12-1046,2008.amta-papers.11,0,0.0295586,"rately or in a combined manner. The frequency (FREQ) corresponds to the number of occurrences of the translation in the target corpus divided by the total number of words in the target corpus. The part-of-speech translation probability (POS) corresponds to P(y|x), the probability that a source term with part-of-speech x will be translated to a target term with part(s)-of-speech y, e.g. it is more probable that a NOUN is translated by another NOUN or by a NOUN PREP NOUN sequence rather than an ADVERB. The part-of-speech translation probabilities were acquired by running the software ANYMALIGN (Lardilleux, 2008) on the EMEA corpus (Tiedemann, 2009) which had been previously pos-tagged with the linguistic analyzer XELDA2. ANYMALIGN outputs a phrase translation table. Each line of the translation table corresponds to an alignment a = {lems, poss, lemt, post, p(s|t), p(t|s)} where poss is the parts-of-speech of the source phrase, post is the parts-ofspeech of the target phrase and pos(t|s) is the probability of translating the source phrase to the target phrase. From these alignments, we obtain P(y|x) with the following formula: ∑ P ( y∣x)= p (t∣ s) {a∈ A∣ poss =x , post =y } ∑ p (t∣s) {a∈ A∣ poss=x } T"
C12-1046,P07-1084,1,0.905648,"xt (Fung & Cheung, 2004; Rauf & Schwenk, 2009) is only possible when large amounts of texts are available. In the case of the extraction of domainspecific lexicons, we quickly face the problem of data scarcity: in order to extract high-quality lexicons, the corpus must contain text dealing with very specific subject domains and the target and source texts must be highly comparable. If one tries to increase the size of the corpus, one takes the risk of decreasing its quality by adding out-of-domain texts. Studies support the idea that the quality of the corpora is more important than its size. Morin et al. (2007) show that the discourse categorization of the documents increases the precision of the lexicon despite the data sparsity. Bo & Gaussier (2010) show that they improve the quality of the extracted lexicon if they improve the comparability of the corpus by selecting a smaller – but more comparable – corpus from an initial set of documents. This paper proposes methods for ranking and extracting canonical translations as well as translation variants, with a special focus on the extraction of fertile translations. In parallel texts processing, the notion of fertility has been defined by Brown et al"
C12-1046,quasthoff-etal-2006-corpus,0,0.0198487,"o handle the variation phenomena described in section 1.3, we used a dictionary of synonyms and lists of morphologically related words. The dictionary of synonyms is part of the XELDA software. Morphologically related words were collected by stemming the words of the comparable corpora and the entries of the bilingual dictionary with the algorithm of Porter (1980). The DECOMPOSE function uses the entries of the morpheme translation table (242 entries) and a list of 85k lexical items composed of the entries of the general language dictionary and English words extracted from the Leipzig Corpus (Quasthoff et al., 2006). 5 We used the measure defined by Bo & Gaussier (2010) which indicates, given a bilingual dictionary, the expectation of finding, for each word of the source corpus, its translation in the target corpus and vice-versa. 6 http://www.dicts.info/uddl.php 753 EN → FR EN → DE General language 38k → 60k 38k → 70k Domain specific 6.7k → 6.7k 6.4k → 6.4k Morphemes (TOTAL) 242 → 729 242 → 761 Prefixes 50 → 134 50 → 166 Confixes 185 → 574 185 → 563 Suffixes 7 → 21 7 → 32 TABLE 2: Nb. of entries in the multilingual resources EN → EN FR → FR DE → DE Synonyms 5.1k → 7.6k 2.4k → 3.2k 4.2k → 4.9k Morphologi"
C12-1046,P95-1050,0,0.400212,"ntial translation for hekurudhë). 1.2 Ranking and selection methods Generally, compositional translation generates several possible translations for one source term. One has to find a way to rank the translations from the most to the least reliable. Garera & Yarowsky (2008) tried two ranking methods: (i) a probability score P based on the number of different languages exhibiting the association between the literal gloss and the fluent translation ; (ii) the probability score P combined with the similarity of the source and target words' contexts using context-based methods like in the work of Rapp (1995) and Fung (1997). Robitaille et al. (2006) extract translation pairs from a corpus built by querying a search engine with a set of seed translation pairs. They select the candidate translations which are semantically related to the target seed terms. The semantic similarity measure is based on the number of hits containing the seed term and/or the candidate translation (Jaccard coefficient). Other works simply select the candidate translations which occur in the target corpus (Weller et al., 2001 ; Morin and Daille, 2010) or which are significantly attested on the Web (Cartoni, 2009). Only Bal"
C12-1046,E09-1003,0,0.0222349,"a morphological or paraphrastic variant (e.g. postmenopausal translates to après la ménopause 'after the menopause' instead of postménopausique). Another limitation is that algorithms output, for each source term, a set of candidate translations instead of just one target term. This state of affairs makes it very challenging for translators to use lexicons extracted from comparable corpora in real-life situations (Delpech, 2011). The solution that consists in increasing the size of the corpus in order to find more translation pairs or to extract parallel segments of text (Fung & Cheung, 2004; Rauf & Schwenk, 2009) is only possible when large amounts of texts are available. In the case of the extraction of domainspecific lexicons, we quickly face the problem of data scarcity: in order to extract high-quality lexicons, the corpus must contain text dealing with very specific subject domains and the target and source texts must be highly comparable. If one tries to increase the size of the corpus, one takes the risk of decreasing its quality by adding out-of-domain texts. Studies support the idea that the quality of the corpora is more important than its size. Morin et al. (2007) show that the discourse ca"
C12-1046,E06-1029,0,0.642367,"een generated, one generally ranks them and selects the TopN candidate translations. Generation methods are described in section 1.1. Ranking methods are described in section 2.3. 1.1 Generation methods Compositional translation consists in decomposing the source term into atomic components, translating these components into the target language and recomposing the translated components into target terms. Existing implementations differ on the kind of atomic components they use for translation. Lexical compositional translation (Baldwin & Tanaka, 2004; Grefenstette, 1999; Morin & Daille, 2009; Robitaille et al., 2006) deals with multi-word term to multi-word term alignment and uses lexical words as atomic components: rate of evaporation is translated into French as 1 We use the following notations: trailing hyphen for prefixes (a-), leading hyphen for suffixes (-a), both for confixes (a-), no hyphen for autonomous morphemes (a) and a plus sign (+) for intra-word morpheme boundaries. The term confix is borrowed from Martinet (1979) and refers to neoclassical (Latin or Ancient Greek) roots. 747 taux d'évaporation by translating rate to taux and evaporation to évaporation using dictionary lookup. Recompositio"
C12-1110,1999.tc-1.8,0,\N,Missing
C12-1110,J93-2003,0,\N,Missing
C12-1110,C02-2020,0,\N,Missing
C12-1110,C02-1065,0,\N,Missing
C12-1110,E06-1029,0,\N,Missing
C12-1110,W03-1803,0,\N,Missing
C12-1110,W04-0404,0,\N,Missing
C12-1110,P99-1067,0,\N,Missing
C12-1110,W02-0902,0,\N,Missing
C12-1110,P04-1067,0,\N,Missing
C12-1110,P06-1011,0,\N,Missing
C12-1110,C10-1070,0,\N,Missing
C12-1110,P07-1084,1,\N,Missing
C12-1110,I11-2003,1,\N,Missing
C16-1321,D11-1033,0,0.0245098,"11-17 2016. document aligned data at the topic level, we only deal with the standard approach and show at least that our approach improve drastically bilingual terminology extraction while adding well selected external data. The small size of specialized comparable corpora renders unreliable word co-occurrences which are the basis of the standard approach. In this paper, we propose to improve the reliability of word cooccurrences in specialized comparable corpora by adding general-domain data. This idea has already been successfully employed in machine translation task (Moore and Lewis, 2010; Axelrod et al., 2011; Wang et al., 2014, among others). The approach of using adapted external data, also known as data selection is often applied in Statistical Machine Translation (SMT) to improve the quality of the language and translation models, and hence, to increase the performance of SMT systems. If data selection has become a mainstream in SMT, it is still not the case in the task of bilingual lexicon extraction from specialized comparable corpora. The majority of the studies in this area support the principle that the quality of the comparable corpus is more important than its size and consequently, inc"
C16-1321,D13-1046,0,0.0678367,"968) and weighted Jaccard (Grefenstette, 1994) 3 Related Work In the past few years, several contributions have been proposed to improve each step of the standard approach. Prochasson et al. (2009) enhance the representativeness of the context vectors by strengthening the context words that happen to be transliterated and scientific compound words in the target 3402 language. Ismail and Manandhar (2010) also suggest that context vectors should be based on the most important contextually relevant words (in-domain terms), and thus propose a method for filtering the noise of the context vectors. Bouamor et al. (2013) propose an adaption of the standard approach that exploits Wikipedia to improve the context vector representation. From the context vector of a word to be translated, they build a vector of Wikipedia concepts using the ESA inverted index (Explicit Semantic Analysis). This vector of concepts is then translated into the target language. The candidate translations are found by projecting the translated vector of concepts using the ESA direct index onto the context vector of the target language. Prochasson and Fung (2011) propose to use a machine learning approach based both on the context-vector"
C16-1321,P13-2133,0,0.456183,"968) and weighted Jaccard (Grefenstette, 1994) 3 Related Work In the past few years, several contributions have been proposed to improve each step of the standard approach. Prochasson et al. (2009) enhance the representativeness of the context vectors by strengthening the context words that happen to be transliterated and scientific compound words in the target 3402 language. Ismail and Manandhar (2010) also suggest that context vectors should be based on the most important contextually relevant words (in-domain terms), and thus propose a method for filtering the noise of the context vectors. Bouamor et al. (2013) propose an adaption of the standard approach that exploits Wikipedia to improve the context vector representation. From the context vector of a word to be translated, they build a vector of Wikipedia concepts using the ESA inverted index (Explicit Semantic Analysis). This vector of concepts is then translated into the target language. The candidate translations are found by projecting the translated vector of concepts using the ESA direct index onto the context vector of the target language. Prochasson and Fung (2011) propose to use a machine learning approach based both on the context-vector"
C16-1321,C02-2020,0,0.774202,"parability. 5.2 Bilingual Dictionary The bilingual dictionary used in our experiments is the French/English dictionary ELRA-M00337 . This resource is a general language dictionary which contains around 244,000 entries. 5.3 Gold Standard To evaluate the quality of bilingual terminology extraction from comparable corpora, a bilingual terminology reference list that reflects the technical vocabulary of the comparable corpus is required. The 4 opus.lingfil.uu.se commoncrawl.org 6 www.ldc.upenn.edu 7 www.elra.info 5 3405 list is usually composed of more or less 100 single words: 95 single words in Chiao and Zweigenbaum (2002), 100 in Morin et al. (2010), 125 and 79 in Bouamor et al. (2013a). We build a reference list for each of the three comparable corpora using specialized glossaries available on the Web. For instance, the list is derived from the UMLS8 for the breast cancer corpus. Concerning wind energy, the list is provided with the corpus1 . In order to focus only on the vocabulary characteristic of the specialized corpus we remove technical terms that have a common meaning in the general domain such as analysis, factor, method, result, study, etc. Without this precaution, these terms would be mechanically b"
C16-1321,C12-1046,1,0.816226,"nslation (SMT) to improve the quality of the language and translation models, and hence, to increase the performance of SMT systems. If data selection has become a mainstream in SMT, it is still not the case in the task of bilingual lexicon extraction from specialized comparable corpora. The majority of the studies in this area support the principle that the quality of the comparable corpus is more important than its size and consequently, increasing the size of specialized comparable corpora by adding out-of-domain documents decreases the quality of bilingual lexicons (Li and Gaussier, 2010; Delpech et al., 2012). This statement remains true as long as the used data is not adapted to the domain. We propose two data selection techniques based on the combination of a specialized comparable corpus with external resources. Our hypothesis is that word co-occurrences learned from a general-domain corpus for general words (as opposed to the terms of the domain) improve the characterization of the specific vocabulary of the corpus (the terms of the domain). By enriching the general words representation in specialized comparable corpora, we improve their characterization and therefore improve the characterizat"
C16-1321,J93-1003,0,0.593415,"Missing"
C16-1321,W95-0114,0,0.831326,"the difficulty to obtain many specialized documents in a language other than English. For example, a single query on the Elsevier portal1 of documents containing in their title the term “breast cancer” returns 40,000 documents in English, where the same query returns 1,500 documents in French, 693 in Spanish and only 7 in German. The historical context-based approach dedicated to the task of bilingual lexicon extraction from comparable corpora, and also known as the standard approach, relies on the simple observation that a word and its translation tend to appear in the same lexical contexts (Fung, 1995; Rapp, 1999). In this approach, each word is described by its lexical contexts in both source and target languages, and words in translation relationship should have similar lexical contexts in both languages. To enhance bilingual lexicon induction, recent approaches use more sophisticated techniques such as topic models based on bilingual latent dirichlet allocation (BiLDA) (Vulic and Moens, 2013b; Vulic and Moens, 2013a) or bilingual word embeddings based on neural networks (Gouws et al., 2014; Chandar et al., 2014; Vulic and Moens, 2015; Vulic and Moens, 2016) (approaches respectively note"
C16-1321,P04-1067,0,0.690562,"Missing"
C16-1321,fiser-etal-2012-addressing,0,0.0208808,"co-occurrence counts in specialized comparable corpus more reliable by reestimating their probabilities. Morin and Hazem (2016) show the unfounded assumption of the balance in terms of quantity of data of the specialized comparable corpora and that the use of unbalanced corpora significantly improves the results of the standard approach. Other improvements to the standard approach have been proposed by introducing other paradigms. For instance, Gaussier et al. (2004) propose to apply Canonical Correlation Analysis (CCA) which is a bilingual extension of Latent Semantic Analysis (LSA) whereas Hazem and Morin (2012) propose to use Independent Component Analysis (ICA) which is basically an extension of the Principal Component Analysis (PCA). Vuli´c et al. (2011) also propose an extension of the Latent Dirichlet Allocation (LDA) taking into account bilinguality and called bilingual LDA (BiLDA), improvements of this latter can be found in (Vulic and Moens, 2013b; Vulic and Moens, 2013a). Gouws et al. (2014) and Chandar et al. (2014) use multilingual word embeddings based on sentence-aligned parallel data and/or translation dictionaries whereas Vuli´c and Moens. (2015; 2016) learn bilingual word embeddings f"
C16-1321,I13-1196,1,0.917301,"ector of Wikipedia concepts using the ESA inverted index (Explicit Semantic Analysis). This vector of concepts is then translated into the target language. The candidate translations are found by projecting the translated vector of concepts using the ESA direct index onto the context vector of the target language. Prochasson and Fung (2011) propose to use a machine learning approach based both on the context-vector similarity and the co-occurrence features to learn a model for rare words from one pair of languages and this model can be used to find translations from another pair of languages. Hazem and Morin (2013) study different word co-occurrence prediction models in order to make the observed co-occurrence counts in specialized comparable corpus more reliable by reestimating their probabilities. Morin and Hazem (2016) show the unfounded assumption of the balance in terms of quantity of data of the specialized comparable corpora and that the use of unbalanced corpora significantly improves the results of the standard approach. Other improvements to the standard approach have been proposed by introducing other paradigms. For instance, Gaussier et al. (2004) propose to apply Canonical Correlation Analy"
C16-1321,C10-2055,0,0.0205901,"context vector i to each context vector of the target language t through a similarity measure and rank the candidate translations according to this measure. The similarity measures employed are Cosine (Salton and Lesk, 1968) and weighted Jaccard (Grefenstette, 1994) 3 Related Work In the past few years, several contributions have been proposed to improve each step of the standard approach. Prochasson et al. (2009) enhance the representativeness of the context vectors by strengthening the context words that happen to be transliterated and scientific compound words in the target 3402 language. Ismail and Manandhar (2010) also suggest that context vectors should be based on the most important contextually relevant words (in-domain terms), and thus propose a method for filtering the noise of the context vectors. Bouamor et al. (2013) propose an adaption of the standard approach that exploits Wikipedia to improve the context vector representation. From the context vector of a word to be translated, they build a vector of Wikipedia concepts using the ESA inverted index (Explicit Semantic Analysis). This vector of concepts is then translated into the target language. The candidate translations are found by project"
C16-1321,C10-1073,0,0.378307,"ical Machine Translation (SMT) to improve the quality of the language and translation models, and hence, to increase the performance of SMT systems. If data selection has become a mainstream in SMT, it is still not the case in the task of bilingual lexicon extraction from specialized comparable corpora. The majority of the studies in this area support the principle that the quality of the comparable corpus is more important than its size and consequently, increasing the size of specialized comparable corpora by adding out-of-domain documents decreases the quality of bilingual lexicons (Li and Gaussier, 2010; Delpech et al., 2012). This statement remains true as long as the used data is not adapted to the domain. We propose two data selection techniques based on the combination of a specialized comparable corpus with external resources. Our hypothesis is that word co-occurrences learned from a general-domain corpus for general words (as opposed to the terms of the domain) improve the characterization of the specific vocabulary of the corpus (the terms of the domain). By enriching the general words representation in specialized comparable corpora, we improve their characterization and therefore im"
C16-1321,P10-2041,0,0.0673422,"Osaka, Japan, December 11-17 2016. document aligned data at the topic level, we only deal with the standard approach and show at least that our approach improve drastically bilingual terminology extraction while adding well selected external data. The small size of specialized comparable corpora renders unreliable word co-occurrences which are the basis of the standard approach. In this paper, we propose to improve the reliability of word cooccurrences in specialized comparable corpora by adding general-domain data. This idea has already been successfully employed in machine translation task (Moore and Lewis, 2010; Axelrod et al., 2011; Wang et al., 2014, among others). The approach of using adapted external data, also known as data selection is often applied in Statistical Machine Translation (SMT) to improve the quality of the language and translation models, and hence, to increase the performance of SMT systems. If data selection has become a mainstream in SMT, it is still not the case in the task of bilingual lexicon extraction from specialized comparable corpora. The majority of the studies in this area support the principle that the quality of the comparable corpus is more important than its size"
C16-1321,P11-1133,0,0.0935735,"erms), and thus propose a method for filtering the noise of the context vectors. Bouamor et al. (2013) propose an adaption of the standard approach that exploits Wikipedia to improve the context vector representation. From the context vector of a word to be translated, they build a vector of Wikipedia concepts using the ESA inverted index (Explicit Semantic Analysis). This vector of concepts is then translated into the target language. The candidate translations are found by projecting the translated vector of concepts using the ESA direct index onto the context vector of the target language. Prochasson and Fung (2011) propose to use a machine learning approach based both on the context-vector similarity and the co-occurrence features to learn a model for rare words from one pair of languages and this model can be used to find translations from another pair of languages. Hazem and Morin (2013) study different word co-occurrence prediction models in order to make the observed co-occurrence counts in specialized comparable corpus more reliable by reestimating their probabilities. Morin and Hazem (2016) show the unfounded assumption of the balance in terms of quantity of data of the specialized comparable corp"
C16-1321,2009.mtsummit-posters.14,1,0.909256,"atio (Evert, 2005). 2. Translate with a bilingual dictionary the context vector of a word to be translated from the source to the target language (i the translated context vector). 3. Compare the translated context vector i to each context vector of the target language t through a similarity measure and rank the candidate translations according to this measure. The similarity measures employed are Cosine (Salton and Lesk, 1968) and weighted Jaccard (Grefenstette, 1994) 3 Related Work In the past few years, several contributions have been proposed to improve each step of the standard approach. Prochasson et al. (2009) enhance the representativeness of the context vectors by strengthening the context words that happen to be transliterated and scientific compound words in the target 3402 language. Ismail and Manandhar (2010) also suggest that context vectors should be based on the most important contextually relevant words (in-domain terms), and thus propose a method for filtering the noise of the context vectors. Bouamor et al. (2013) propose an adaption of the standard approach that exploits Wikipedia to improve the context vector representation. From the context vector of a word to be translated, they bui"
C16-1321,2009.mtsummit-posters.15,0,0.039283,"tains about 21 languages. We used the French-English version 7 used for the WMT translation task3 JRC acquis corpus (JRC) is a collection of legislative European union texts4 . We used the FrenchEnglish aligned version at OPUS provided by JRC (Tiedemann, 2012). Common crawl corpus (CC) is a petabytes of data collected over 7 years of web crawling set of raw web page data and text extracts5 . Gigaword corpus (GW) is a set of monolingual newswire corpora provided by LDC6 . United nations corpus (UN) is a six language parallel text of the United Nations originally provided as translation memory (Rafalovitch and Dale, 2009). The French/English corpora were then normalized through the following linguistic pre-processing steps: tokenization, part-of-speech tagging, and lemmatization. Finally, the function words were removed and the words occurring less than twice in the French and in the English parts were discarded. Table 1 shows the size of the comparable corpora and also indicates the comparability degree in percentages (Comp.) between the French and the English parts of each comparable corpus. The comparability measure (Li and Gaussier, 2010) is based on the expectation of finding the translation for each word"
C16-1321,P99-1067,0,0.854117,"ty to obtain many specialized documents in a language other than English. For example, a single query on the Elsevier portal1 of documents containing in their title the term “breast cancer” returns 40,000 documents in English, where the same query returns 1,500 documents in French, 693 in Spanish and only 7 in German. The historical context-based approach dedicated to the task of bilingual lexicon extraction from comparable corpora, and also known as the standard approach, relies on the simple observation that a word and its translation tend to appear in the same lexical contexts (Fung, 1995; Rapp, 1999). In this approach, each word is described by its lexical contexts in both source and target languages, and words in translation relationship should have similar lexical contexts in both languages. To enhance bilingual lexicon induction, recent approaches use more sophisticated techniques such as topic models based on bilingual latent dirichlet allocation (BiLDA) (Vulic and Moens, 2013b; Vulic and Moens, 2013a) or bilingual word embeddings based on neural networks (Gouws et al., 2014; Chandar et al., 2014; Vulic and Moens, 2015; Vulic and Moens, 2016) (approaches respectively noted: Gouws, Cha"
C16-1321,tiedemann-2012-parallel,0,0.0532329,"11 88.52 87.90 85.30 86.13 85.56 84.73 Table 1: Characteristics of the specialized corpora and the external data. News commentary corpus (NC) is a twelve language parallel corpus of news commentaries provided by the WMT workshop for SMT4 . Europarl corpus (EP7) is a parallel corpus for SMT extracted from the proceedings of the European Parliament. It contains about 21 languages. We used the French-English version 7 used for the WMT translation task3 JRC acquis corpus (JRC) is a collection of legislative European union texts4 . We used the FrenchEnglish aligned version at OPUS provided by JRC (Tiedemann, 2012). Common crawl corpus (CC) is a petabytes of data collected over 7 years of web crawling set of raw web page data and text extracts5 . Gigaword corpus (GW) is a set of monolingual newswire corpora provided by LDC6 . United nations corpus (UN) is a six language parallel text of the United Nations originally provided as translation memory (Rafalovitch and Dale, 2009). The French/English corpora were then normalized through the following linguistic pre-processing steps: tokenization, part-of-speech tagging, and lemmatization. Finally, the function words were removed and the words occurring less t"
C16-1321,N13-1011,0,0.0572174,"k of bilingual lexicon extraction from comparable corpora, and also known as the standard approach, relies on the simple observation that a word and its translation tend to appear in the same lexical contexts (Fung, 1995; Rapp, 1999). In this approach, each word is described by its lexical contexts in both source and target languages, and words in translation relationship should have similar lexical contexts in both languages. To enhance bilingual lexicon induction, recent approaches use more sophisticated techniques such as topic models based on bilingual latent dirichlet allocation (BiLDA) (Vulic and Moens, 2013b; Vulic and Moens, 2013a) or bilingual word embeddings based on neural networks (Gouws et al., 2014; Chandar et al., 2014; Vulic and Moens, 2015; Vulic and Moens, 2016) (approaches respectively noted: Gouws, Chandar and BWESG+cos). All these approaches require at least sentence-aligned/document aligned parallel data (BiLDA, Gouws, Chandar) or non-parallel document-aligned data at the topic level (BWESG+cos). Since specialized comparable corpora are of small size, sentence-aligned (document aligned) parallel data are unavailable and nonparallel document-aligned data at the topic level can’t be"
C16-1321,D13-1168,0,0.25195,"k of bilingual lexicon extraction from comparable corpora, and also known as the standard approach, relies on the simple observation that a word and its translation tend to appear in the same lexical contexts (Fung, 1995; Rapp, 1999). In this approach, each word is described by its lexical contexts in both source and target languages, and words in translation relationship should have similar lexical contexts in both languages. To enhance bilingual lexicon induction, recent approaches use more sophisticated techniques such as topic models based on bilingual latent dirichlet allocation (BiLDA) (Vulic and Moens, 2013b; Vulic and Moens, 2013a) or bilingual word embeddings based on neural networks (Gouws et al., 2014; Chandar et al., 2014; Vulic and Moens, 2015; Vulic and Moens, 2016) (approaches respectively noted: Gouws, Chandar and BWESG+cos). All these approaches require at least sentence-aligned/document aligned parallel data (BiLDA, Gouws, Chandar) or non-parallel document-aligned data at the topic level (BWESG+cos). Since specialized comparable corpora are of small size, sentence-aligned (document aligned) parallel data are unavailable and nonparallel document-aligned data at the topic level can’t be"
C16-1321,P15-2118,0,0.0312426,"and its translation tend to appear in the same lexical contexts (Fung, 1995; Rapp, 1999). In this approach, each word is described by its lexical contexts in both source and target languages, and words in translation relationship should have similar lexical contexts in both languages. To enhance bilingual lexicon induction, recent approaches use more sophisticated techniques such as topic models based on bilingual latent dirichlet allocation (BiLDA) (Vulic and Moens, 2013b; Vulic and Moens, 2013a) or bilingual word embeddings based on neural networks (Gouws et al., 2014; Chandar et al., 2014; Vulic and Moens, 2015; Vulic and Moens, 2016) (approaches respectively noted: Gouws, Chandar and BWESG+cos). All these approaches require at least sentence-aligned/document aligned parallel data (BiLDA, Gouws, Chandar) or non-parallel document-aligned data at the topic level (BWESG+cos). Since specialized comparable corpora are of small size, sentence-aligned (document aligned) parallel data are unavailable and nonparallel document-aligned data at the topic level can’t be provided since specialized comparable corpora usually deal with one single topic. Based on the recent comparison in (Vulic and Moens, 2015; Vuli"
C16-1321,P11-2084,0,0.448124,"Missing"
C18-1080,D16-1250,0,0.190285,"each step of the standard approach (Gaussier et al., 2004; Gamallo, 2007; Ismail and Manandhar, 2010; Prochasson and Fung, 2011; Hazem and Morin, 2012; Bouamor et al., 2013, among others). With the advent of neural network techniques, Mikolov et al. (2013a) were the first to propose a method to learn a linear transformation from the source language into the target language to improve the task of lexicon extraction from bilingual corpora. Faruqui and Dyer (2014) introduced canonical correlation analysis (CCA) to project the embeddings in both languages to a shared vector space. More recently, Artetxe et al. (2016) presented an approach for learning bilingual mappings of word embeddings that preserves monolingual invariance using several meaningful and intuitive constraints related to other proposed methods (Faruqui and Dyer, 2014; Xing et al., 2015). Jakubina and Langlais (2017) made a careful comparison of the approaches of Mikolov et al. (2013a) and Faruqui and Dyer (2014) with the standard approach. They have clearly shown that the two previous approaches outperform the standard approach for very frequent terms to be translated (the number of occurrences is at least 250 from an English-Spanish compa"
C18-1080,P13-2133,0,0.603035,"ural network models (Bengio et al., 2003; Mikolov et al., 2013b). The historical standard approach builds a context vector for each word of the source and the target languages, translates the source context vectors to the target language using a bilingual seed lexicon, and compares the translated context vectors to each target context vector using a similarity measure. Different contributions have been proposed in the past few years to improve each step of the standard approach (Gaussier et al., 2004; Gamallo, 2007; Ismail and Manandhar, 2010; Prochasson and Fung, 2011; Hazem and Morin, 2012; Bouamor et al., 2013, among others). With the advent of neural network techniques, Mikolov et al. (2013a) were the first to propose a method to learn a linear transformation from the source language into the target language to improve the task of lexicon extraction from bilingual corpora. Faruqui and Dyer (2014) introduced canonical correlation analysis (CCA) to project the embeddings in both languages to a shared vector space. More recently, Artetxe et al. (2016) presented an approach for learning bilingual mappings of word embeddings that preserves monolingual invariance using several meaningful and intuitive c"
C18-1080,C02-2020,0,0.493512,"resource is a general language dictionary which contains only a few terms related to the medical and wind energy domains. 5.3 Gold Standard To evaluate the quality of bilingual terminology extraction from specialized comparable corpora, a bilingual terminology reference list is required. In the general domain, the reference list is randomly composed of a sub-part of the bilingual dictionary (Gaussier et al., 2004; Jakubina and Langlais, 2017). In the specialized domain, this list is usually composed of few words that reflect the terminology of the specialized comparable corpus. For instance, Chiao and Zweigenbaum (2002) used a list composed of 95 single words, Morin et al. (2007) used 100 single words and Bouamor et al. (2013) used 125 and 79 single words. For breast cancer, the lists are derived from the UMLS12 meta-thesaurus. Concerning wind energy, the lists are provided with the corpora (see footnote 5). Each word composing a pair of terms of a reference list appears at least 5 times in the comparable corpus. The bilingual terminology reference list is composed of 248 French/English single words for the Breast cancer corpus and 150 French/English single words for the Wind energy corpus. 6 Experiments and"
C18-1080,E14-1049,0,0.0759754,"nslated context vectors to each target context vector using a similarity measure. Different contributions have been proposed in the past few years to improve each step of the standard approach (Gaussier et al., 2004; Gamallo, 2007; Ismail and Manandhar, 2010; Prochasson and Fung, 2011; Hazem and Morin, 2012; Bouamor et al., 2013, among others). With the advent of neural network techniques, Mikolov et al. (2013a) were the first to propose a method to learn a linear transformation from the source language into the target language to improve the task of lexicon extraction from bilingual corpora. Faruqui and Dyer (2014) introduced canonical correlation analysis (CCA) to project the embeddings in both languages to a shared vector space. More recently, Artetxe et al. (2016) presented an approach for learning bilingual mappings of word embeddings that preserves monolingual invariance using several meaningful and intuitive constraints related to other proposed methods (Faruqui and Dyer, 2014; Xing et al., 2015). Jakubina and Langlais (2017) made a careful comparison of the approaches of Mikolov et al. (2013a) and Faruqui and Dyer (2014) with the standard approach. They have clearly shown that the two previous ap"
C18-1080,W97-0119,0,0.220071,"alized comparable corpora may be beneficial to bilingual terminology extraction task. The combination of external resources such as a general-domain comparable corpus with a specialized comparable corpus can be performed using word embedding models. We recently conducted a first attempt in Hazem and Morin (2017) and have shown under which conditions external resources introduced in the form of Skip-gram and CBOW models can be jointly used to improve the performance of bilingual terms extraction. However, our approach was not able to compete with the historical count-based projection approach (Fung and McKeown, 1997; Rapp, 1999). Our current work pursues this direction by contrasting different neural embedding models and by showing how to take advantage of their combination. More specifically, we show that the character-based Skip-gram and CBOW models (Bojanowski et al., 2016) drastically outperform other models including Skip-gram and CBOW. We also propose a new approach based on Ensemble models which combines specialized and general domain embeddings to obtain a unified Meta-Embedding model. Our approach shows significant improvements and obtains the best results on two specialized English/French compa"
C18-1080,2007.mtsummit-papers.26,0,0.0641027,", 1999). While in the latter, words are embedded into a low-dimensional vector space using neural network models (Bengio et al., 2003; Mikolov et al., 2013b). The historical standard approach builds a context vector for each word of the source and the target languages, translates the source context vectors to the target language using a bilingual seed lexicon, and compares the translated context vectors to each target context vector using a similarity measure. Different contributions have been proposed in the past few years to improve each step of the standard approach (Gaussier et al., 2004; Gamallo, 2007; Ismail and Manandhar, 2010; Prochasson and Fung, 2011; Hazem and Morin, 2012; Bouamor et al., 2013, among others). With the advent of neural network techniques, Mikolov et al. (2013a) were the first to propose a method to learn a linear transformation from the source language into the target language to improve the task of lexicon extraction from bilingual corpora. Faruqui and Dyer (2014) introduced canonical correlation analysis (CCA) to project the embeddings in both languages to a shared vector space. More recently, Artetxe et al. (2016) presented an approach for learning bilingual mappin"
C18-1080,W15-1513,0,0.0248905,"2013a) while acting at the word embedding level representation. Our idea is to enrich the word embedding representation of the source and target languages in order to improve the mapping matrix and so, bilingual terminology extraction from specialized comparable corpora. To do so, we present several ways to take advantage of word embedding models and ensemble approaches. The principle of ensemble approaches is to combine different models in order to capture the strengths of each individual model. The main combination techniques that have shown their effectiveness so far are vectors addition (Garten et al., 2015) and vectors concatenation (Garten et al., 2015; Yin and Sch¨utze, 2016). For vectors addition, given two embedding models, the procedure consists in applying a simple dimension-wise vectors addition2 . For vectors concatenation, given two embedding models of dimensions dim1 and dim2, the resulting concatenated embedding vector will be of size dim1+dim2. The vectors have to be normalized before concatenation. Usually L2 norm is applied3 . Yin and Sch¨utze (2016) performed a weighted concatenation of five embedding models. They also experienced the SVD (Singular Value Decomposition) on top of w"
C18-1080,P04-1067,0,0.543962,"Missing"
C18-1080,fiser-etal-2012-addressing,0,0.0243461,"l vector space using neural network models (Bengio et al., 2003; Mikolov et al., 2013b). The historical standard approach builds a context vector for each word of the source and the target languages, translates the source context vectors to the target language using a bilingual seed lexicon, and compares the translated context vectors to each target context vector using a similarity measure. Different contributions have been proposed in the past few years to improve each step of the standard approach (Gaussier et al., 2004; Gamallo, 2007; Ismail and Manandhar, 2010; Prochasson and Fung, 2011; Hazem and Morin, 2012; Bouamor et al., 2013, among others). With the advent of neural network techniques, Mikolov et al. (2013a) were the first to propose a method to learn a linear transformation from the source language into the target language to improve the task of lexicon extraction from bilingual corpora. Faruqui and Dyer (2014) introduced canonical correlation analysis (CCA) to project the embeddings in both languages to a shared vector space. More recently, Artetxe et al. (2016) presented an approach for learning bilingual mappings of word embeddings that preserves monolingual invariance using several mean"
C18-1080,I17-1069,1,0.82322,"nal Conference on Computational Linguistics, pages 937–949 Santa Fe, New Mexico, USA, August 20-26, 2018. According to Jakubina and Langlais (2017), word embeddings are more effective on large comparable corpora than on small comparable corpora. This statement lend support the idea that enriching small specialized comparable corpora may be beneficial to bilingual terminology extraction task. The combination of external resources such as a general-domain comparable corpus with a specialized comparable corpus can be performed using word embedding models. We recently conducted a first attempt in Hazem and Morin (2017) and have shown under which conditions external resources introduced in the form of Skip-gram and CBOW models can be jointly used to improve the performance of bilingual terms extraction. However, our approach was not able to compete with the historical count-based projection approach (Fung and McKeown, 1997; Rapp, 1999). Our current work pursues this direction by contrasting different neural embedding models and by showing how to take advantage of their combination. More specifically, we show that the character-based Skip-gram and CBOW models (Bojanowski et al., 2016) drastically outperform o"
C18-1080,P14-1006,0,0.0343235,"s direction by contrasting different neural embedding models and by showing how to take advantage of their combination. More specifically, we show that the character-based Skip-gram and CBOW models (Bojanowski et al., 2016) drastically outperform other models including Skip-gram and CBOW. We also propose a new approach based on Ensemble models which combines specialized and general domain embeddings to obtain a unified Meta-Embedding model. Our approach shows significant improvements and obtains the best results on two specialized English/French comparable corpora. 2 Related Work According to Hermann and Blunsom (2014), methods dealing with bilingual lexicon extraction from comparable corpora can be classified as distributional-based or distributed-based approaches. In the former, words are represented by their context vectors using a distributional count-based approach also known as the standard approach (Fung, 1998; Rapp, 1999). While in the latter, words are embedded into a low-dimensional vector space using neural network models (Bengio et al., 2003; Mikolov et al., 2013b). The historical standard approach builds a context vector for each word of the source and the target languages, translates the sourc"
C18-1080,C10-2055,0,0.0224508,"in the latter, words are embedded into a low-dimensional vector space using neural network models (Bengio et al., 2003; Mikolov et al., 2013b). The historical standard approach builds a context vector for each word of the source and the target languages, translates the source context vectors to the target language using a bilingual seed lexicon, and compares the translated context vectors to each target context vector using a similarity measure. Different contributions have been proposed in the past few years to improve each step of the standard approach (Gaussier et al., 2004; Gamallo, 2007; Ismail and Manandhar, 2010; Prochasson and Fung, 2011; Hazem and Morin, 2012; Bouamor et al., 2013, among others). With the advent of neural network techniques, Mikolov et al. (2013a) were the first to propose a method to learn a linear transformation from the source language into the target language to improve the task of lexicon extraction from bilingual corpora. Faruqui and Dyer (2014) introduced canonical correlation analysis (CCA) to project the embeddings in both languages to a shared vector space. More recently, Artetxe et al. (2016) presented an approach for learning bilingual mappings of word embeddings that p"
C18-1080,E17-2096,0,0.378099,"corpora of different types of discourse and gender (e.g. a corpus of popular science discourse supplementing a corpus of scientific discourse), corpora of general language or out-of-domain data. The main challenge is to know how to associate such resources with a comparable specialized corpus. This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http://creativecommons.org/licenses/by/4.0/. 937 Proceedings of the 27th International Conference on Computational Linguistics, pages 937–949 Santa Fe, New Mexico, USA, August 20-26, 2018. According to Jakubina and Langlais (2017), word embeddings are more effective on large comparable corpora than on small comparable corpora. This statement lend support the idea that enriching small specialized comparable corpora may be beneficial to bilingual terminology extraction task. The combination of external resources such as a general-domain comparable corpus with a specialized comparable corpus can be performed using word embedding models. We recently conducted a first attempt in Hazem and Morin (2017) and have shown under which conditions external resources introduced in the form of Skip-gram and CBOW models can be jointly"
C18-1080,P14-2050,0,0.0401562,"h word vectors with subwords information. It takes into account the internal structure of words which can be very useful for morphologically rich languages. It also incorporates character ngram embeddings where each word is represented by a bag-of character n-gram (Bojanowski et al., 2016). More precisely, it uses character embedding and word embedding models jointly performing the vector sum of both to form the final embedding representation of words. We refer to the character Skip-gram model by CharSG and the character CBOW model by CharCBOW. Other models such as the dependency-based model (Levy and Goldberg, 2014) and generalized-based model (Li et al., 2017) were assessed but not presented in this paper for sake of clarity and because of the very low results obtained on the specialized domains when compared to the above presented models. Several pre-trained embedding models are publicly available such as CBOW and Skip-gram models (Mikolov et al., 2013b), global word representation-based models (Pennington et al., 2014), character skip-gram-based models (Bojanowski et al., 2016), etc. If it is interesting to study the impact of pretrained embeddings on bilingual terminology extraction from comparable c"
C18-1080,D17-1257,0,0.0207493,"o account the internal structure of words which can be very useful for morphologically rich languages. It also incorporates character ngram embeddings where each word is represented by a bag-of character n-gram (Bojanowski et al., 2016). More precisely, it uses character embedding and word embedding models jointly performing the vector sum of both to form the final embedding representation of words. We refer to the character Skip-gram model by CharSG and the character CBOW model by CharCBOW. Other models such as the dependency-based model (Levy and Goldberg, 2014) and generalized-based model (Li et al., 2017) were assessed but not presented in this paper for sake of clarity and because of the very low results obtained on the specialized domains when compared to the above presented models. Several pre-trained embedding models are publicly available such as CBOW and Skip-gram models (Mikolov et al., 2013b), global word representation-based models (Pennington et al., 2014), character skip-gram-based models (Bojanowski et al., 2016), etc. If it is interesting to study the impact of pretrained embeddings on bilingual terminology extraction from comparable corpora. The major part of the above cited pre-"
C18-1080,N15-1142,0,0.0248719,"hat makes use of a global factorization model and local context window methods to represent words in a global vector space model (Pennington et al., 2014). This model directly captures the global statistics from the corpus based on co-occurrence word probabilities. Its training objective is to learn word vectors such that their dot product equals the log-probability of word’s co-occurrence. Glove has shown good results in word analogy, word similarity, and named entity recognition tasks. Structured Embeddings are two adaptations of CBOW and Skip-gram models that include ordering information1 (Ling et al., 2015). While word2vec is insensitive to word order, the structured embedding model includes position information in the context representation of words. Given the embedding of the center word w, the Skip-gram model for instance uses a single output matrix to predict every contextual word. In contrast, the structured Skip-gram adapts the model to the positioning of the surrounding words. It defines an output for each relative position to the center word. The adaptation of CBOW is the continuous window model where the input is the concatenation of the embeddings of context words. While in the standar"
C18-1080,P14-1121,1,0.899972,"rse, etc. without having a parallel source text-target text relationship, allow access to the original vocabulary without falling under the influence of the human translation. Compiling a large comparable corpus is easier, especially for general language (Talvensaari et al., 2007). In contrast, specialized comparable corpora are traditionally of modest size due to the difficulty to obtain many specialized documents in a language other than English. Specialized comparable corpora have a size of around one million words whereas general-domain comparable corpora can gather several million words (Morin and Hazem, 2014). One way to overcome the small size of specialized comparable corpora is to associate external resources. These resources may be close specialized corpora (e.g. a breast cancer corpus may benefit from contexts derived from a more general oncology corpus), corpora of different types of discourse and gender (e.g. a corpus of popular science discourse supplementing a corpus of scientific discourse), corpora of general language or out-of-domain data. The main challenge is to know how to associate such resources with a comparable specialized corpus. This work is licensed under a Creative Commons A"
C18-1080,P07-1084,1,0.829391,"erms related to the medical and wind energy domains. 5.3 Gold Standard To evaluate the quality of bilingual terminology extraction from specialized comparable corpora, a bilingual terminology reference list is required. In the general domain, the reference list is randomly composed of a sub-part of the bilingual dictionary (Gaussier et al., 2004; Jakubina and Langlais, 2017). In the specialized domain, this list is usually composed of few words that reflect the terminology of the specialized comparable corpus. For instance, Chiao and Zweigenbaum (2002) used a list composed of 95 single words, Morin et al. (2007) used 100 single words and Bouamor et al. (2013) used 125 and 79 single words. For breast cancer, the lists are derived from the UMLS12 meta-thesaurus. Concerning wind energy, the lists are provided with the corpora (see footnote 5). Each word composing a pair of terms of a reference list appears at least 5 times in the comparable corpus. The bilingual terminology reference list is composed of 248 French/English single words for the Breast cancer corpus and 150 French/English single words for the Wind energy corpus. 6 Experiments and Results We conducted two sets of experiments. The first one"
C18-1080,J03-1002,0,0.0174345,"of different word embedding models for bilingual terminology extraction from specialized comparable corpora. We emphasize how the character-based embedding model outperforms other models on the quality of the extracted bilingual lexicons. Further more, we propose a new efficient way to combine different embedding models learned from specialized and general-domain data sets. Our approach leads to higher performance than the best individual embedding model. 1 Introduction Bilingual lexicons are fundamental resources in multilingual natural language processing tasks such as machine translation (Och and Ney, 2003), cross-language information retrieval (Nie, 2010) or computerassisted translation (Delpech, 2014). Because a manual compilation of bilingual lexicons requires substantial human efforts, bilingual lexicons are automatically extracted from bilingual corpora. These corpora can be parallel or comparable data sets. Despite good results obtained when compiling bilingual lexicons from parallel corpora, the latter are scarce resources, especially for specialized and technical domains and for language pairs not involving English. In this context, comparable corpora are an interesting and practical alt"
C18-1080,D14-1162,0,0.0971268,"the representations of the middle word. If these models exhibit similar architectures, CBOW is faster and more suitable for large data sets while Skip-gram gives better word representations when monolingual data is small (Mikolov et al., 2013a). Glove takes advantage of the main benefits of count data while simultaneously capturing the meaningful linear substructures prevalent in prediction-based methods such as word2vec. It is a global log-linear regression model that makes use of a global factorization model and local context window methods to represent words in a global vector space model (Pennington et al., 2014). This model directly captures the global statistics from the corpus based on co-occurrence word probabilities. Its training objective is to learn word vectors such that their dot product equals the log-probability of word’s co-occurrence. Glove has shown good results in word analogy, word similarity, and named entity recognition tasks. Structured Embeddings are two adaptations of CBOW and Skip-gram models that include ordering information1 (Ling et al., 2015). While word2vec is insensitive to word order, the structured embedding model includes position information in the context representatio"
C18-1080,P11-1133,0,0.0189904,"edded into a low-dimensional vector space using neural network models (Bengio et al., 2003; Mikolov et al., 2013b). The historical standard approach builds a context vector for each word of the source and the target languages, translates the source context vectors to the target language using a bilingual seed lexicon, and compares the translated context vectors to each target context vector using a similarity measure. Different contributions have been proposed in the past few years to improve each step of the standard approach (Gaussier et al., 2004; Gamallo, 2007; Ismail and Manandhar, 2010; Prochasson and Fung, 2011; Hazem and Morin, 2012; Bouamor et al., 2013, among others). With the advent of neural network techniques, Mikolov et al. (2013a) were the first to propose a method to learn a linear transformation from the source language into the target language to improve the task of lexicon extraction from bilingual corpora. Faruqui and Dyer (2014) introduced canonical correlation analysis (CCA) to project the embeddings in both languages to a shared vector space. More recently, Artetxe et al. (2016) presented an approach for learning bilingual mappings of word embeddings that preserves monolingual invari"
C18-1080,P99-1067,0,0.39862,"a may be beneficial to bilingual terminology extraction task. The combination of external resources such as a general-domain comparable corpus with a specialized comparable corpus can be performed using word embedding models. We recently conducted a first attempt in Hazem and Morin (2017) and have shown under which conditions external resources introduced in the form of Skip-gram and CBOW models can be jointly used to improve the performance of bilingual terms extraction. However, our approach was not able to compete with the historical count-based projection approach (Fung and McKeown, 1997; Rapp, 1999). Our current work pursues this direction by contrasting different neural embedding models and by showing how to take advantage of their combination. More specifically, we show that the character-based Skip-gram and CBOW models (Bojanowski et al., 2016) drastically outperform other models including Skip-gram and CBOW. We also propose a new approach based on Ensemble models which combines specialized and general domain embeddings to obtain a unified Meta-Embedding model. Our approach shows significant improvements and obtains the best results on two specialized English/French comparable corpora"
C18-1080,tiedemann-2012-parallel,0,0.0157051,"itle or the keywords contain the term breast cancer in English and its translation in French. Wind energy corpus (WE) has been released in the TTC project5 . This corpus has been crawled from the Web using Babouk crawler (Groc, 2011) based on several keywords such as wind, energy, rotor in English and its translation in French. In addition, we use three corpora of general language as external resources: JRC acquis corpus (JRC) is a collection of legislative texts of the European Union6 . We used the French-English version at OPUS which is based on the paragraph-aligned corpus provided by JRC (Tiedemann, 2012). Common crawl corpus (CC) is an open repository of data collected over 7 years of web crawling sets of raw web page data and text extracts7 . 4 www.sciencedirect.com/ www.ttc-project.eu/index.php/releases-publications 6 opus.lingfil.uu.se/JRC-Acquis.php 7 commoncrawl.org 5 941 Wikipedia corpus (Wiki) The English wikipedia corpus8 is a dump which was released on 03-Feb-2018 and the French wikipedia corpus 9 was released on 02-Feb-2018. Even if JRC, CC are parallel corpora, we didn’t explicitly exploit their parallel relationship. We considered these external data sets as if they were comparabl"
C18-1080,P15-2118,0,0.0434677,"Missing"
C18-1080,N15-1104,0,0.115686,"et al. (2013a) were the first to propose a method to learn a linear transformation from the source language into the target language to improve the task of lexicon extraction from bilingual corpora. Faruqui and Dyer (2014) introduced canonical correlation analysis (CCA) to project the embeddings in both languages to a shared vector space. More recently, Artetxe et al. (2016) presented an approach for learning bilingual mappings of word embeddings that preserves monolingual invariance using several meaningful and intuitive constraints related to other proposed methods (Faruqui and Dyer, 2014; Xing et al., 2015). Jakubina and Langlais (2017) made a careful comparison of the approaches of Mikolov et al. (2013a) and Faruqui and Dyer (2014) with the standard approach. They have clearly shown that the two previous approaches outperform the standard approach for very frequent terms to be translated (the number of occurrences is at least 250 from an English-Spanish comparable corpus obtained from the 6th workshop on statistical machine translation gathering 2.55 giga words). On the other hand, when the terms are less frequent (the number of occurrences is less than 25 from a French/English comparable corpo"
C18-1080,P16-1128,0,0.0365979,"Missing"
C18-1125,Q17-1010,0,0.0267988,"Italian Comedy data: • RIMES (RM) stands for Recognition and Indexing of Handwritten Documents and Faxes) and is a French database developed to evaluate automatic systems that recognizes and indexes handwritten letters (Grosicki and El-Abed, 2011); • George Washington (GW) is an English database created from the George Washington Papers at the Library of Congress (Fischer et al., 2012); • Los Esposalles (ESP) is a Spanish database compiled from a marriage licence book collection from the 15t h and 17t h centuries (Fischer et al., 2012); • Wikipedia Fr data (Wiki), as used and distributed by (Bojanowski et al., 2017), provides all words whose frequency is greater than 5 in Wikipedia. From this dataset, we randomly selected 30 000 1478 words. To our knowledge, no other experiments dedicated to the construction of a decoder use the resources RM and ESP which are initially image resources. However, we found it interesting to observe the impact of trained models because ESP is mainly composed of named entities, and RM is a contemporary base with an administrative vocabulary. Therefore, they are far from our Italian Comedy data. 3.3 Representation Preprocessing of the Training Datasets In all the datasets, we"
C18-1125,P17-1031,0,0.0306437,"to all. However, digitization is not sufficient to make the documents usable: it is necessary to extract informations in order to index them. Researchers and historians in the humanities and in the social sciences need to be able to query them and find answers rapidly. Therefore, new projects involve the domains of language processing, document recognition, and information retrieval for historical studies. In the field of Natural Language Processing (NLP) for historical documents, the main challenge is currently to analyse and normalize the spelling of texts (Garrette and Alpert-Abrams, 2016; Bollmann et al., 2017) whereas challenges of Computer Vision for historical documents are more diverse: they involve segmentation into words, lines, or paragraphs, as well as keyword spotting, or handwriting recognition (HWR). In recent years, the number of competitions regarding historical documents has increased (Cloppet et al., 2016; Pratikakis et al., 2016; Sanchez et al., 2017). Such systems have to deal with the complexity of the task as well as the document medium, its level of deterioration, or even its written language. All of these can have a strong impact on the system efficiency. Lately, the trend in HW"
C18-1125,W14-4012,0,0.0201317,"Missing"
C18-1125,N16-1055,0,0.0144544,"heritage and making it accessible to all. However, digitization is not sufficient to make the documents usable: it is necessary to extract informations in order to index them. Researchers and historians in the humanities and in the social sciences need to be able to query them and find answers rapidly. Therefore, new projects involve the domains of language processing, document recognition, and information retrieval for historical studies. In the field of Natural Language Processing (NLP) for historical documents, the main challenge is currently to analyse and normalize the spelling of texts (Garrette and Alpert-Abrams, 2016; Bollmann et al., 2017) whereas challenges of Computer Vision for historical documents are more diverse: they involve segmentation into words, lines, or paragraphs, as well as keyword spotting, or handwriting recognition (HWR). In recent years, the number of competitions regarding historical documents has increased (Cloppet et al., 2016; Pratikakis et al., 2016; Sanchez et al., 2017). Such systems have to deal with the complexity of the task as well as the document medium, its level of deterioration, or even its written language. All of these can have a strong impact on the system efficiency."
C18-1125,L18-1069,1,0.858003,"Missing"
C18-1125,P17-1184,0,0.0291485,"erefore, they are far from our Italian Comedy data. 3.3 Representation Preprocessing of the Training Datasets In all the datasets, we replaced the diacritical marks by their simple form, such as {´e, e` } are e characters, and the typical long form of s from the 18th as a short form for the decoder input and also the target data. All words are case sensitive. For GIC only, text lines were split on whitespaces and punctuation marks. In contrast, ICR title lines were only split on whitespaces and we kept with the punctuations in the data. Letter-Ngrams As suggested by (Bengio and Heigold, 2014; Vania and Lopez, 2017), we use letterngrams as a pivot in our multimodal encoder-decoder. Initially, the authors selected the 50 000 most popular letter-ngrams to represent words. Adding [ and ] to symbolize the beginning and end of a word, respectively, we computed all the possible letter-ngrams with a maximum length of 3 on all the datasets. Therefore, we selected around 12,500 letter-ngrams. The large difference in the number of ngrams comes from the choice of keeping only letter-ngrams appearing in at least 2 different datasets: a joker was created to replace the non-selected letter-ngrams. With our example in"
C18-1242,D16-1250,0,0.442759,"tors are also called word embeddings (Mikolov et al., 2013b). In the case of bilingual word embedding, Mikolov et al. (2013a) propose a method to learn a linear transformation from the source language to the target language for the task of lexicon extraction from bilingual corpora. Much research has been focused on this area since then. Faruqui and Dyer (2014) introduce canonical correlation analysis (CCA) to project the embeddings in both languages to a shared vector space. Xing et al. (2015) propose the orthogonal transformation and the vector length normalization during the learning phase. Artetxe et al. (2016) generalize these works and explain the equivalence of different objective functions under orthogonality and different normalization procedures. They show that combining these models effectively improves the results for both monolingual and bilingual tasks. Finally Smith et al. (2017) point out that the mapping should be orthogonal in order to be self-consistent. Normalization, Mean Centering and Orthogonal Mapping The method of Artetxe et al. (2016)1 combines several related studies in this particular order: 1. Normalizing each word vector to unit length. 2. Dimension-wise mean centering for"
C18-1242,W04-0404,0,0.118325,"Missing"
C18-1242,D12-1050,0,0.015261,"ate terms are ranked according to their similarity with terms of the same length in the target language, and the final score for each possible translation is defined by the arithmetic or geometric mean of each similarity score. MWT Representation CMCBP does not, however, take the mapping of MWTs of variable lengths into account. For example, the English term “wind vane” can be translated as “girouette” in French and the English term “wind energy” by “Windenergie” in German. In order to take these cases into account, we propose to modify the representation of the MWTs, inspired by the works of Blacoe and Lapata (2012) in which the representation of a sentence is the sum of the distributional representations of each composing word: n vector(term) = 1 X vector(wi ) , where n is the term length n ||vector(wi )|| (5) i Notice that this is different from the mean vector introduced in the original work, here the mean vector is calculated from the normalized vectors because we want each component word to have the same impact rather than having the whole meaning influenced by the random vector length which could lead to some unpredictable bias. The MWT representation is then stored in a single vector, giving the a"
C18-1242,P13-2133,0,0.0236241,", 2014). Both use word vector representations. The vectors in the first approach are sparse, high dimensional and explicit (Levy and Goldberg, 2014). The vectors in the second one are dense, low dimensional and generalized. They both rely on the distributional hypothesis (Harris, 1968) which assumes that a word and its translation tend to appear in the same lexical contexts. 2.1 Context-Based Projection Approach The historical context-based projection approach, also known as the standard approach (SA) has been studied in a variety of works (Fung, 1995; Rapp, 1999; Chiao and Zweigenbaum, 2002; Bouamor et al., 2013; Hazem and Morin, 2016; Jakubina and Langlais, 2017). To implement this approach, we first build a co-occurrence matrix for the source and target languages, where each line represents a context vector in an n-word window. These vectors are then normalized using the Mutual Information (MI (Fano, 1961)) for instance. Then we get the word in the target language vector space by projecting each element in the context vector via a bilingual seed lexicon. Finally, the candidate translations are ranked by calculating the similarity of the projected context vector with all the context vectors into the"
C18-1242,C02-2020,0,0.140255,"emantics (Hermann and Blunsom, 2014). Both use word vector representations. The vectors in the first approach are sparse, high dimensional and explicit (Levy and Goldberg, 2014). The vectors in the second one are dense, low dimensional and generalized. They both rely on the distributional hypothesis (Harris, 1968) which assumes that a word and its translation tend to appear in the same lexical contexts. 2.1 Context-Based Projection Approach The historical context-based projection approach, also known as the standard approach (SA) has been studied in a variety of works (Fung, 1995; Rapp, 1999; Chiao and Zweigenbaum, 2002; Bouamor et al., 2013; Hazem and Morin, 2016; Jakubina and Langlais, 2017). To implement this approach, we first build a co-occurrence matrix for the source and target languages, where each line represents a context vector in an n-word window. These vectors are then normalized using the Mutual Information (MI (Fano, 1961)) for instance. Then we get the word in the target language vector space by projecting each element in the context vector via a bilingual seed lexicon. Finally, the candidate translations are ranked by calculating the similarity of the projected context vector with all the co"
C18-1242,C12-1046,1,0.750653,"t eˆ tre adapt´e en domaines de sp´ecialit´e. 1 Introduction Bilingual terminology extraction from comparable corpora has aroused a lot of attention since the 1990s (Fung, 1995; Rapp, 1999). Two classes of approach have been developed depending on the nature of the term to be aligned. The first class concerns the alignment of single-word terms using context-based or neural network approaches while the second attempts to align multi-word terms relying on compositional approaches. Few studies have focused on providing a unified framework for aligning single-word and multi-word terms, apart from Delpech et al. (2012) and Taslimipoor et al. (2016). The first requires some specific linguistic information like the morpheme translation table, which makes it difficult to employ for two languages from different linguistic families such as English and Chinese. The second incorporates word embeddings into their collocation alignment system but it is limited to several types of mapping that must match a set of pre-defined syntactic patterns. Our objective is to provide such a unified framework for aligning terms of variable length in specialized domains without specific linguistic knowledge of the source or target"
C18-1242,E14-1049,0,0.0424243,"co-occurrence counts are not impacted because their weight coefficient is always 1. 2.2 Neural Network-Based Approach The neural network based approach uses neural network models to obtain word representation in low dimensional and dense vectors. These word vectors are also called word embeddings (Mikolov et al., 2013b). In the case of bilingual word embedding, Mikolov et al. (2013a) propose a method to learn a linear transformation from the source language to the target language for the task of lexicon extraction from bilingual corpora. Much research has been focused on this area since then. Faruqui and Dyer (2014) introduce canonical correlation analysis (CCA) to project the embeddings in both languages to a shared vector space. Xing et al. (2015) propose the orthogonal transformation and the vector length normalization during the learning phase. Artetxe et al. (2016) generalize these works and explain the equivalence of different objective functions under orthogonality and different normalization procedures. They show that combining these models effectively improves the results for both monolingual and bilingual tasks. Finally Smith et al. (2017) point out that the mapping should be orthogonal in orde"
C18-1242,W95-0114,0,0.689794,"rables n’a pas e´ t´e beaucoup e´ tudi´ee. Dans ce travail, nous proposons un syst`eme unifi´e pour l’alignement des termes bilingues ind´ependamment de la longueur des termes. De plus nous introduisons e´ galement des am´eliorations aux approches bas´ees sur l’alignement de contexte et sur un r´eseau neuronal. Nos exp´eriences montrent l’efficacit´e de nos am´eliorations sur les travaux ant´erieurs, et le fait que le syst`eme peut eˆ tre adapt´e en domaines de sp´ecialit´e. 1 Introduction Bilingual terminology extraction from comparable corpora has aroused a lot of attention since the 1990s (Fung, 1995; Rapp, 1999). Two classes of approach have been developed depending on the nature of the term to be aligned. The first class concerns the alignment of single-word terms using context-based or neural network approaches while the second attempts to align multi-word terms relying on compositional approaches. Few studies have focused on providing a unified framework for aligning single-word and multi-word terms, apart from Delpech et al. (2012) and Taslimipoor et al. (2016). The first requires some specific linguistic information like the morpheme translation table, which makes it difficult to em"
C18-1242,1999.tc-1.8,0,0.00846032,"ding MWTs, we first describe the compositional approach and an adapted version which combines the traditional compositional approach and the context-based projection approach. We also introduce a unified representation for MWTs which enables the mapping of variable length terms. Finally we propose a new approach in this family which combines the traditional compositional approach and the neural network approach. To the best of our knowledge, this is the first time that word embedding vectors are used in a compositional approach. 2858 3.1 Compositional Approach The compositional approach (CA) (Grefenstette, 1999; Tanaka, 2002; Robitaille et al., 2006) is a simple and direct approach that consists in translating each element of an MWT via a dictionary and generating all possible combinations and permutations. The ranking of the candidates is done by their frequency in the target corpora. Compositional Approach with Context-Based Projection The main limitation of the traditional compositional approach is the inability to translate a term when one of its composing words is not in the dictionary. To solve this problem, Morin and Daille (2012) propose the Compositional Approach with Context-Based Projecti"
C18-1242,C16-1321,1,0.387591,"word embeddings into their collocation alignment system but it is limited to several types of mapping that must match a set of pre-defined syntactic patterns. Our objective is to provide such a unified framework for aligning terms of variable length in specialized domains without specific linguistic knowledge of the source or target language. Large size comparable corpora in specialized domains are not always available. Consequently, many data driven systems cannot learn from enough information. A possible solution to this problem is to associate external data such as general domain corpora (Hazem and Morin, 2016) to the specialized corpora. Our work adapts this method in order to improve the system performance. Besides the enrichment of the data, our work focuses on studying and improving different state-of-theart approaches both for single-word term and multi-word term alignments, which are finally unified into a single framework for alignment of terms of any length. We first describe the context-based projection This work is licenced under a Creative Commons Attribution 4.0 International Licence. creativecommons.org/licenses/by/4.0/ Licence details: http:// 2855 Proceedings of the 27th International"
C18-1242,I17-1069,1,0.892883,"Missing"
C18-1242,P14-1006,0,0.0191824,"oposed method is able to align variable length terms in a single process. 2 Single-Word Term Alignment In this section, we describe the two principal state-of-the-art approaches used for bilingual lexicon extraction from comparable corpora. Furthermore we introduce our enhancements in order to overcome various limitations of these approaches. The reason why we want to study these approaches for singleword terms (SWTs) is that they are fundamental low-level elements in our approaches for multi-word terms (MWTs). The two approaches for SWTs are known as distributional and distributed semantics (Hermann and Blunsom, 2014). Both use word vector representations. The vectors in the first approach are sparse, high dimensional and explicit (Levy and Goldberg, 2014). The vectors in the second one are dense, low dimensional and generalized. They both rely on the distributional hypothesis (Harris, 1968) which assumes that a word and its translation tend to appear in the same lexical contexts. 2.1 Context-Based Projection Approach The historical context-based projection approach, also known as the standard approach (SA) has been studied in a variety of works (Fung, 1995; Rapp, 1999; Chiao and Zweigenbaum, 2002; Bouamor"
C18-1242,E17-2096,0,0.0176243,". The vectors in the first approach are sparse, high dimensional and explicit (Levy and Goldberg, 2014). The vectors in the second one are dense, low dimensional and generalized. They both rely on the distributional hypothesis (Harris, 1968) which assumes that a word and its translation tend to appear in the same lexical contexts. 2.1 Context-Based Projection Approach The historical context-based projection approach, also known as the standard approach (SA) has been studied in a variety of works (Fung, 1995; Rapp, 1999; Chiao and Zweigenbaum, 2002; Bouamor et al., 2013; Hazem and Morin, 2016; Jakubina and Langlais, 2017). To implement this approach, we first build a co-occurrence matrix for the source and target languages, where each line represents a context vector in an n-word window. These vectors are then normalized using the Mutual Information (MI (Fano, 1961)) for instance. Then we get the word in the target language vector space by projecting each element in the context vector via a bilingual seed lexicon. Finally, the candidate translations are ranked by calculating the similarity of the projected context vector with all the context vectors into the target language. We use the Cosine similarity measur"
C18-1242,W14-1618,0,0.0255412,"ncipal state-of-the-art approaches used for bilingual lexicon extraction from comparable corpora. Furthermore we introduce our enhancements in order to overcome various limitations of these approaches. The reason why we want to study these approaches for singleword terms (SWTs) is that they are fundamental low-level elements in our approaches for multi-word terms (MWTs). The two approaches for SWTs are known as distributional and distributed semantics (Hermann and Blunsom, 2014). Both use word vector representations. The vectors in the first approach are sparse, high dimensional and explicit (Levy and Goldberg, 2014). The vectors in the second one are dense, low dimensional and generalized. They both rely on the distributional hypothesis (Harris, 1968) which assumes that a word and its translation tend to appear in the same lexical contexts. 2.1 Context-Based Projection Approach The historical context-based projection approach, also known as the standard approach (SA) has been studied in a variety of works (Fung, 1995; Rapp, 1999; Chiao and Zweigenbaum, 2002; Bouamor et al., 2013; Hazem and Morin, 2016; Jakubina and Langlais, 2017). To implement this approach, we first build a co-occurrence matrix for the"
C18-1242,C12-1110,1,0.890129,"uce two enhancements which improve our final results. Secondly, we cover the bilingual word embedding approach which depends on vector representations of words that can be learned on large text collections using different neural networks (Bengio et al., 2003; Mnih and Hinton, 2008; Collobert and Weston, 2008; Mikolov et al., 2013b), then we also propose an enhancement that re-normalizes the word embedding vector length in order to improve the results. After studying the approaches for single-word terms which are our fundamental components for processing multi-word terms, following the idea in Morin and Daille (2012), we propose a new system called Compositional Approach with Word Embedding Projection (CMWEP) to combine the advantages of the traditional compositional approach and the bilingual word embedding approach. Our final results show considerable improvements over the state-of-the-art approach for bilingual multi-word term extraction. Moreover, the proposed method is able to align variable length terms in a single process. 2 Single-Word Term Alignment In this section, we describe the two principal state-of-the-art approaches used for bilingual lexicon extraction from comparable corpora. Furthermore"
C18-1242,D14-1162,0,0.0794724,", λ ∈ [0, 1] (2) where w and c respectively denote the central word and the context word, g(c|w) the weight that is distributed to c as the context of w, ∆ is the distance between the two words and λ a hyper-parameter that determines the degree of penalization for distant word pairs. Note that λ = 0 is equivalent to a uniform distribution. Weighted Mutual Information Another limitation in the standard approach with MI is that MI overestimates low counts and underestimates high counts. In order to overcome this drawback, we propose the Weighted Mutual Information (WMI) inspired by the work of Pennington et al. (2014) where they introduce a function to smooth word co-occurrences. The original function is a weight function which prevents the overestimation of word co-occurrences. We also use it as a weight function for MI: W M I(w, c) = f (cooc(w, c)) × M I(w, c) ( (x/xmax )α , α = 3/4, xmax = 20, if x &lt; xmax f (x) = 1 otherwise (3) (4) We have kept the same value for the hyper-parameter α. Concerning the xmax , since our corpora size is much smaller than the one in their work, we decide to make it correspondingly smaller (20). By adding the weight function, the output value for low co-occurrence counts is"
C18-1242,P99-1067,0,0.738949,"as e´ t´e beaucoup e´ tudi´ee. Dans ce travail, nous proposons un syst`eme unifi´e pour l’alignement des termes bilingues ind´ependamment de la longueur des termes. De plus nous introduisons e´ galement des am´eliorations aux approches bas´ees sur l’alignement de contexte et sur un r´eseau neuronal. Nos exp´eriences montrent l’efficacit´e de nos am´eliorations sur les travaux ant´erieurs, et le fait que le syst`eme peut eˆ tre adapt´e en domaines de sp´ecialit´e. 1 Introduction Bilingual terminology extraction from comparable corpora has aroused a lot of attention since the 1990s (Fung, 1995; Rapp, 1999). Two classes of approach have been developed depending on the nature of the term to be aligned. The first class concerns the alignment of single-word terms using context-based or neural network approaches while the second attempts to align multi-word terms relying on compositional approaches. Few studies have focused on providing a unified framework for aligning single-word and multi-word terms, apart from Delpech et al. (2012) and Taslimipoor et al. (2016). The first requires some specific linguistic information like the morpheme translation table, which makes it difficult to employ for two"
C18-1242,E06-1029,0,0.0432515,"ompositional approach and an adapted version which combines the traditional compositional approach and the context-based projection approach. We also introduce a unified representation for MWTs which enables the mapping of variable length terms. Finally we propose a new approach in this family which combines the traditional compositional approach and the neural network approach. To the best of our knowledge, this is the first time that word embedding vectors are used in a compositional approach. 2858 3.1 Compositional Approach The compositional approach (CA) (Grefenstette, 1999; Tanaka, 2002; Robitaille et al., 2006) is a simple and direct approach that consists in translating each element of an MWT via a dictionary and generating all possible combinations and permutations. The ranking of the candidates is done by their frequency in the target corpora. Compositional Approach with Context-Based Projection The main limitation of the traditional compositional approach is the inability to translate a term when one of its composing words is not in the dictionary. To solve this problem, Morin and Daille (2012) propose the Compositional Approach with Context-Based Projection (CMCBP), where the objective is to co"
C18-1242,C02-1065,0,0.122914,"describe the compositional approach and an adapted version which combines the traditional compositional approach and the context-based projection approach. We also introduce a unified representation for MWTs which enables the mapping of variable length terms. Finally we propose a new approach in this family which combines the traditional compositional approach and the neural network approach. To the best of our knowledge, this is the first time that word embedding vectors are used in a compositional approach. 2858 3.1 Compositional Approach The compositional approach (CA) (Grefenstette, 1999; Tanaka, 2002; Robitaille et al., 2006) is a simple and direct approach that consists in translating each element of an MWT via a dictionary and generating all possible combinations and permutations. The ranking of the candidates is done by their frequency in the target corpora. Compositional Approach with Context-Based Projection The main limitation of the traditional compositional approach is the inability to translate a term when one of its composing words is not in the dictionary. To solve this problem, Morin and Daille (2012) propose the Compositional Approach with Context-Based Projection (CMCBP), wh"
C18-1242,N15-1104,0,0.026984,"d approach uses neural network models to obtain word representation in low dimensional and dense vectors. These word vectors are also called word embeddings (Mikolov et al., 2013b). In the case of bilingual word embedding, Mikolov et al. (2013a) propose a method to learn a linear transformation from the source language to the target language for the task of lexicon extraction from bilingual corpora. Much research has been focused on this area since then. Faruqui and Dyer (2014) introduce canonical correlation analysis (CCA) to project the embeddings in both languages to a shared vector space. Xing et al. (2015) propose the orthogonal transformation and the vector length normalization during the learning phase. Artetxe et al. (2016) generalize these works and explain the equivalence of different objective functions under orthogonality and different normalization procedures. They show that combining these models effectively improves the results for both monolingual and bilingual tasks. Finally Smith et al. (2017) point out that the mapping should be orthogonal in order to be self-consistent. Normalization, Mean Centering and Orthogonal Mapping The method of Artetxe et al. (2016)1 combines several rela"
daille-etal-2004-french,C02-1166,0,\N,Missing
daille-etal-2004-french,C02-1011,0,\N,Missing
daille-etal-2004-french,W02-1402,0,\N,Missing
daille-etal-2004-french,P99-1067,0,\N,Missing
daille-etal-2004-french,C94-1084,1,\N,Missing
F12-2011,W04-0404,0,0.0541818,"Missing"
F12-2011,C02-2020,0,0.049523,"Missing"
F12-2011,P04-1067,0,0.08128,"Missing"
F12-2011,1999.tc-1.8,0,0.0327037,"Missing"
F12-2011,P08-1088,0,0.0753205,"Missing"
F12-2011,C10-2055,0,0.025603,"Missing"
F12-2011,W02-0902,0,0.106214,"Missing"
F12-2011,C10-1070,0,0.0376199,"Missing"
F12-2011,C10-1073,0,0.0274471,"Missing"
F12-2011,P06-1011,0,0.0587709,"Missing"
F12-2011,P99-1067,0,0.0513396,"Missing"
F12-2011,E06-1029,0,0.0339711,"Missing"
F12-2011,P10-1011,0,0.0382427,"Missing"
F12-2011,C02-1065,0,0.0648446,"Missing"
F12-2011,W03-1803,0,0.0668664,"Missing"
F13-1018,C10-1013,0,0.0571207,"Missing"
F13-1018,C02-1166,0,0.101792,"Missing"
F13-1018,W95-0114,0,0.163721,"Missing"
F13-1018,P98-1069,0,0.229019,"Missing"
F13-1018,2007.mtsummit-papers.26,0,0.0985598,"Missing"
F13-1018,W09-1117,0,0.0459622,"Missing"
F13-1018,P04-1067,0,0.0670818,"Missing"
F13-1018,P08-1088,0,0.0472545,"Missing"
F13-1018,C10-1070,0,0.052701,"Missing"
F13-1018,C10-1073,0,0.0386019,"Missing"
F13-1018,2009.jeptalnrecital-long.6,1,0.844479,"Missing"
F13-1018,2004.jeptalnrecital-long.13,1,0.832715,"Missing"
F13-1018,2009.jeptalnrecital-long.10,1,0.85046,"Missing"
F13-1018,P95-1050,0,0.294849,"Missing"
F13-1018,P99-1067,0,0.10024,"Missing"
F13-1023,W04-0404,0,0.087688,"Missing"
F13-1023,E09-1016,0,0.0392405,"Missing"
F13-1023,C00-1032,1,0.71285,"Missing"
F13-1023,I11-1097,0,0.0312636,"Missing"
F13-1023,P95-1050,0,0.315621,"Missing"
F13-1023,I11-2003,1,0.900113,"Missing"
fourour-etal-2002-incremental,C96-1071,0,\N,Missing
fourour-etal-2002-incremental,E95-1004,0,\N,Missing
fourour-etal-2002-incremental,A97-1030,0,\N,Missing
hazem-morin-2012-adaptive,W97-0119,0,\N,Missing
hazem-morin-2012-adaptive,I05-1062,1,\N,Missing
hazem-morin-2012-adaptive,C02-1166,0,\N,Missing
hazem-morin-2012-adaptive,C02-2020,0,\N,Missing
hazem-morin-2012-adaptive,P98-1069,0,\N,Missing
hazem-morin-2012-adaptive,C98-1066,0,\N,Missing
hazem-morin-2012-adaptive,P99-1067,0,\N,Missing
hazem-morin-2012-adaptive,P04-1067,0,\N,Missing
hazem-morin-2012-adaptive,C10-1070,0,\N,Missing
hazem-morin-2012-adaptive,P07-1084,1,\N,Missing
I05-1062,C02-1011,0,0.0455052,"Missing"
I05-1062,W02-1402,0,0.0369952,"n single words and MWTs by exploiting the term contexts. After explaining the diﬃculties involved in aligning MWTs and specifying our approach, we show the adopted process for bilingual terminology extraction and the resources used in our experiments. Finally, we evaluate our approach and demonstrate its signiﬁcance, particularly in relation to non-compositional MWT alignment. 1 Introduction Traditional research into the automatic compilation of bilingual dictionaries from corpora exploits parallel texts, i.e. a text and its translation [17]. From sentenceto-sentence aligned corpora, symbolic [2], statistical [11], or combined [7] techniques are used for word and expression alignments. The use of parallel corpora raises two problems: – as a parallel corpus is a pair of translated texts, the vocabulary appearing in the translated text is highly inﬂuenced by the source text, especially for technical domains; – such corpora are diﬃcult to obtain for paired languages not involving English. New methods try to exploit comparable corpora: texts that are of the same text type and on the same subject without a source text-target text relationship. The main studies concentrate on ﬁnding in such"
I05-1062,C02-2020,0,0.632581,"Missing"
I05-1062,W03-1802,1,0.572123,"extraction, a word-to-word assumption being generally adopted. – When a MWT is translated into a MWT of the same length, the target sequence is not typically composed of the translation of its parts [13]. For example, the French term plantation ´energ´etique is translated into English as fuel plantation where fuel is not the translation of ´energ´etique. This property is referred to as “non-compositionality”. French-English Terminology Extraction from Comparable Corpora 709 – A MWT could appear in texts under diﬀerent forms reﬂecting either syntactic, morphological or semantic variations [12],[5]. Term variations should be taken into account in the translation process. For example, the French sequences am´enagement de la forˆet and am´enagement forestier refer to the same MWT and are both translated into the same English term: forest management. We propose tackling these three problems, fertility, non-compositionality, and variations, by using both linguistic and statistical methods. First, MWTs are identiﬁed in both the source and target language using a monolingual term extraction program. Second, a statistical alignment algorithm is used to link MWTs in the source language to singl"
I05-1062,C94-1084,1,0.554837,"ng the term contexts. After explaining the diﬃculties involved in aligning MWTs and specifying our approach, we show the adopted process for bilingual terminology extraction and the resources used in our experiments. Finally, we evaluate our approach and demonstrate its signiﬁcance, particularly in relation to non-compositional MWT alignment. 1 Introduction Traditional research into the automatic compilation of bilingual dictionaries from corpora exploits parallel texts, i.e. a text and its translation [17]. From sentenceto-sentence aligned corpora, symbolic [2], statistical [11], or combined [7] techniques are used for word and expression alignments. The use of parallel corpora raises two problems: – as a parallel corpus is a pair of translated texts, the vocabulary appearing in the translated text is highly inﬂuenced by the source text, especially for technical domains; – such corpora are diﬃcult to obtain for paired languages not involving English. New methods try to exploit comparable corpora: texts that are of the same text type and on the same subject without a source text-target text relationship. The main studies concentrate on ﬁnding in such corpora translation candidates for"
I05-1062,C02-1166,0,0.686416,"Missing"
I05-1062,P99-1067,0,0.816182,"Missing"
I08-1013,J97-3003,0,\N,Missing
I08-1013,W97-0119,0,\N,Missing
I08-1013,1999.tc-1.8,0,\N,Missing
I08-1013,C02-1166,0,\N,Missing
I08-1013,J93-2003,0,\N,Missing
I08-1013,C02-2020,0,\N,Missing
I08-1013,E06-1029,0,\N,Missing
I08-1013,W04-0404,0,\N,Missing
I08-1013,P99-1067,0,\N,Missing
I08-1013,P97-1063,0,\N,Missing
I08-1013,P07-1084,1,\N,Missing
I13-1046,P13-2133,0,0.0122308,"and align the best source and target sentences that contain the term and its translation candidates. We report results with two language pairs (French-English and FrenchGerman) using domain-specific comparable corpora. Our method significantly improves the top 1, top 5 and top 10 precisions of a domain-specific bilingual lexicon, and thus, provides a better useroriented results. 1 Introduction Comparable corpora have been the subject of interest for extracting bilingual lexicons by several researchers (Rapp, 1995; Fung and Mckeown, 1997; Rapp, 1999; Koehn and Knight, 2002; Morin et al., 2008; Bouamor et al., 2013, among others). Rapp (1995) was the first to suggest that if a word A co-occurs frequently with another word B in one language, then the translation of A and the translation of B should co-occur frequently in another language. Approaches emerging from (Rapp, 1995) make different assumptions to extract bilingual lexicon from comparable corpora. However, they are all based on the assumption that a translation pair shares some similar context in comparable corpora. We refer to such approaches that depend on co-occurrences of 401 International Joint Conference on Natural Language Processing, page"
I13-1046,P11-1140,0,0.0260098,"s technical. The results are also significantly better with the French-English language pair than with the French-German language pair. In fact, domainspecific corpora contain many terms that are compound nouns. In the German language, many compound nouns may be written as single units (e.g. German term “Produktionsstandort” is translated into French by “site de production”). Therefore, the distributional approach may consider such German terms as one word when computing co-occurrences. One way to overcome this problem would be to perform splitting before applying the distributional approach (Macherey et al., 2011). To analyze the results obtained by the distributional method in more depth, we measured the comparability of Wind Energy corpora for the different language pairs, using the comparability measure presented by Li et al. (2011). For the French-English corpora, we obtained a comparability value of 0.81. As for the French-German corpora, we obtained a comparability value of 0.70. This implies that our French-German corpora are less comparable than the French-English corpora, and partly justifies the reason behind obtaining worse results with the French-German pair using the distributional method."
I13-1046,C02-2020,0,0.0610457,"Missing"
I13-1046,W04-3208,0,0.174003,"ntains terms that are highly related to the “Breast Cancer” subject (e.g. chemotherapy, histological). Our assumption is that the best context (represented by sentences) can be extracted for a term as well as for its translation candidates, and that these extracted sentences can be aligned in order to re-rank the translation candidates of the term. After obtaining some candidate translations for Figure 1: Method to score a translation pair (source term and target term) 402 Parallel sentence (or fragment) extraction from comparable corpora has received the attention of a number of researchers (Fung and Cheung, 2004; Munteanu and Marcu, 2005; Munteanu and Marcu, 2006; Smith et al., 2010; Hunsicker et al., 2012, among others), to enrich parallel text used by statistical machine translation (SMT) systems. They conducted experiments with large corpora (mainly news stories) which were noisy parallel, comparable (contain topic alignments or articles published in similar circumstances), or very non-parallel (Fung and Cheung, 2004). Usually, these approaches perform document-level alignments before extracting parallel sentences. The domain-specific corpora we use contain few documents (ranging from 38 to 262 do"
I13-1046,J05-4003,0,0.590585,"ighly related to the “Breast Cancer” subject (e.g. chemotherapy, histological). Our assumption is that the best context (represented by sentences) can be extracted for a term as well as for its translation candidates, and that these extracted sentences can be aligned in order to re-rank the translation candidates of the term. After obtaining some candidate translations for Figure 1: Method to score a translation pair (source term and target term) 402 Parallel sentence (or fragment) extraction from comparable corpora has received the attention of a number of researchers (Fung and Cheung, 2004; Munteanu and Marcu, 2005; Munteanu and Marcu, 2006; Smith et al., 2010; Hunsicker et al., 2012, among others), to enrich parallel text used by statistical machine translation (SMT) systems. They conducted experiments with large corpora (mainly news stories) which were noisy parallel, comparable (contain topic alignments or articles published in similar circumstances), or very non-parallel (Fung and Cheung, 2004). Usually, these approaches perform document-level alignments before extracting parallel sentences. The domain-specific corpora we use contain few documents (ranging from 38 to 262 documents for each corpus) a"
I13-1046,P06-1011,0,0.0285608,"st Cancer” subject (e.g. chemotherapy, histological). Our assumption is that the best context (represented by sentences) can be extracted for a term as well as for its translation candidates, and that these extracted sentences can be aligned in order to re-rank the translation candidates of the term. After obtaining some candidate translations for Figure 1: Method to score a translation pair (source term and target term) 402 Parallel sentence (or fragment) extraction from comparable corpora has received the attention of a number of researchers (Fung and Cheung, 2004; Munteanu and Marcu, 2005; Munteanu and Marcu, 2006; Smith et al., 2010; Hunsicker et al., 2012, among others), to enrich parallel text used by statistical machine translation (SMT) systems. They conducted experiments with large corpora (mainly news stories) which were noisy parallel, comparable (contain topic alignments or articles published in similar circumstances), or very non-parallel (Fung and Cheung, 2004). Usually, these approaches perform document-level alignments before extracting parallel sentences. The domain-specific corpora we use contain few documents (ranging from 38 to 262 documents for each corpus) and no parallel sentences."
I13-1046,W97-0119,0,0.339849,"stani and B´eatrice Daille and Emmanuel Morin LINA UMR CNRS 6241 - University of Nantes 2 rue de la Houssini`ere, BP 92208 44322 Nantes, France {rima.harastani,beatrice.daille,emmanuel.morin}@univ-nantes.fr Abstract words to extract a bilingual lexicon by distributional approaches. Results obtained from distributional approaches vary according to many parameters. For example, one of the parameters that impacts the performance of distributional approaches is the way the context of a word is defined. Various approaches defined contexts differently: windows (Rapp, 1999), sentences or paragraphs (Fung and Mckeown, 1997), or by taking into consideration syntax dependencies based on POS tags (Gamallo, 2007). However, the most common way the context of a word is defined is by choosing words within windows centered around the word (Laroche and Langlais, 2010), usually of small sizes (e.g. a window of size 3 is used by Rapp (1999)). Domain-specific comparable corpora have been used for bilingual terminology extraction. These corpora are of modest sizes since large domainspecific corpora are not available for many domains (Morin et al., 2008). As a matter of fact, distributional approaches perform best with large"
I13-1046,2007.mtsummit-papers.26,0,0.79304,"Missing"
I13-1046,P95-1050,0,0.465207,"ra. For a source term and a list of translation candidates, we propose a method to identify and align the best source and target sentences that contain the term and its translation candidates. We report results with two language pairs (French-English and FrenchGerman) using domain-specific comparable corpora. Our method significantly improves the top 1, top 5 and top 10 precisions of a domain-specific bilingual lexicon, and thus, provides a better useroriented results. 1 Introduction Comparable corpora have been the subject of interest for extracting bilingual lexicons by several researchers (Rapp, 1995; Fung and Mckeown, 1997; Rapp, 1999; Koehn and Knight, 2002; Morin et al., 2008; Bouamor et al., 2013, among others). Rapp (1995) was the first to suggest that if a word A co-occurs frequently with another word B in one language, then the translation of A and the translation of B should co-occur frequently in another language. Approaches emerging from (Rapp, 1995) make different assumptions to extract bilingual lexicon from comparable corpora. However, they are all based on the assumption that a translation pair shares some similar context in comparable corpora. We refer to such approaches th"
I13-1046,P99-1067,0,0.632628,"ired from Comparable Corpora Rima Harastani and B´eatrice Daille and Emmanuel Morin LINA UMR CNRS 6241 - University of Nantes 2 rue de la Houssini`ere, BP 92208 44322 Nantes, France {rima.harastani,beatrice.daille,emmanuel.morin}@univ-nantes.fr Abstract words to extract a bilingual lexicon by distributional approaches. Results obtained from distributional approaches vary according to many parameters. For example, one of the parameters that impacts the performance of distributional approaches is the way the context of a word is defined. Various approaches defined contexts differently: windows (Rapp, 1999), sentences or paragraphs (Fung and Mckeown, 1997), or by taking into consideration syntax dependencies based on POS tags (Gamallo, 2007). However, the most common way the context of a word is defined is by choosing words within windows centered around the word (Laroche and Langlais, 2010), usually of small sizes (e.g. a window of size 3 is used by Rapp (1999)). Domain-specific comparable corpora have been used for bilingual terminology extraction. These corpora are of modest sizes since large domainspecific corpora are not available for many domains (Morin et al., 2008). As a matter of fact,"
I13-1046,2012.eamt-1.37,0,0.0675383,"gical). Our assumption is that the best context (represented by sentences) can be extracted for a term as well as for its translation candidates, and that these extracted sentences can be aligned in order to re-rank the translation candidates of the term. After obtaining some candidate translations for Figure 1: Method to score a translation pair (source term and target term) 402 Parallel sentence (or fragment) extraction from comparable corpora has received the attention of a number of researchers (Fung and Cheung, 2004; Munteanu and Marcu, 2005; Munteanu and Marcu, 2006; Smith et al., 2010; Hunsicker et al., 2012, among others), to enrich parallel text used by statistical machine translation (SMT) systems. They conducted experiments with large corpora (mainly news stories) which were noisy parallel, comparable (contain topic alignments or articles published in similar circumstances), or very non-parallel (Fung and Cheung, 2004). Usually, these approaches perform document-level alignments before extracting parallel sentences. The domain-specific corpora we use contain few documents (ranging from 38 to 262 documents for each corpus) and no parallel sentences. Furthermore, they are of modest size (about"
I13-1046,I11-2003,1,0.848816,"Stt ) = 1 − (6) |A| 3. Longest contiguous span: it is defined by (Munteanu and Marcu, 2005) as being the longest “pair of substrings in which the words in one substring are connected only to words in the other substring”. We assume that the length of a span must be greater than 2. The longest span is divided by the length of the smaller sentence, then: f3 (Sts , Stt ) = span(Sts , Stt ) min(|Sts |, |Stt |) 5 We first need to extract translations for a list of domain-specific terms in comparable corpora. In order to do this, we pre-process corpora and align terms with the free tool TermSuite5 (Rocheteau and Daille, 2011). The distributional method that is implemented in TermSuite is the one described in (Rapp, 1999). TermSuite provides a chosen number of translations for a term. Translations are ranked according to the scores provided by the distributional method. We try to enhance the top candidate translations of each reference source term by applying our re-ranking method. (7) 4. Number of connected bi-grams: this feature function is defined as the number of found connected bi-grams divided by the number of connected words in A, then: f4 (Sts , Stt ) = bi-grams(Sts , Stt ) |A| Evaluation (8) 5.1 The optima"
I13-1046,W02-0902,0,0.0327377,"candidates, we propose a method to identify and align the best source and target sentences that contain the term and its translation candidates. We report results with two language pairs (French-English and FrenchGerman) using domain-specific comparable corpora. Our method significantly improves the top 1, top 5 and top 10 precisions of a domain-specific bilingual lexicon, and thus, provides a better useroriented results. 1 Introduction Comparable corpora have been the subject of interest for extracting bilingual lexicons by several researchers (Rapp, 1995; Fung and Mckeown, 1997; Rapp, 1999; Koehn and Knight, 2002; Morin et al., 2008; Bouamor et al., 2013, among others). Rapp (1995) was the first to suggest that if a word A co-occurs frequently with another word B in one language, then the translation of A and the translation of B should co-occur frequently in another language. Approaches emerging from (Rapp, 1995) make different assumptions to extract bilingual lexicon from comparable corpora. However, they are all based on the assumption that a translation pair shares some similar context in comparable corpora. We refer to such approaches that depend on co-occurrences of 401 International Joint Confe"
I13-1046,N10-1063,0,0.0132274,"hemotherapy, histological). Our assumption is that the best context (represented by sentences) can be extracted for a term as well as for its translation candidates, and that these extracted sentences can be aligned in order to re-rank the translation candidates of the term. After obtaining some candidate translations for Figure 1: Method to score a translation pair (source term and target term) 402 Parallel sentence (or fragment) extraction from comparable corpora has received the attention of a number of researchers (Fung and Cheung, 2004; Munteanu and Marcu, 2005; Munteanu and Marcu, 2006; Smith et al., 2010; Hunsicker et al., 2012, among others), to enrich parallel text used by statistical machine translation (SMT) systems. They conducted experiments with large corpora (mainly news stories) which were noisy parallel, comparable (contain topic alignments or articles published in similar circumstances), or very non-parallel (Fung and Cheung, 2004). Usually, these approaches perform document-level alignments before extracting parallel sentences. The domain-specific corpora we use contain few documents (ranging from 38 to 262 documents for each corpus) and no parallel sentences. Furthermore, they ar"
I13-1046,C10-1070,0,0.0434659,"Missing"
I13-1046,P11-2083,0,\N,Missing
I13-1196,C02-1011,0,0.0385796,"Missing"
I13-1196,C02-2020,0,0.876242,"Missing"
I13-1196,I05-1062,1,0.841234,"nguage is computed using a similarity measure (cosine (Salton and Lesk, 1968), Jaccard (Grefenstette, 1994)...). Finally, the translation candidates are ranked according to their similarity score. Many variants of the Standard Approach have been proposed. They can differ in context representation (window-based, syntactic-based) (Morin et al., 2007; Gamallo, 2008), corpus characteristics (small, large, general or domain specific...)(Chiao and Zweigenbaum, 2002; D´ejean et al., 2002; Morin et al., 2007), type of words to translate (single word terms (SWTs) or multiword terms (MWTs))(Rapp, 1999; Daille and Morin, 2005), words frequency (less frequent, rare...)(Pekar et al., 2006), etc. There exist other approaches for bilingual lexicon extraction. D´ejean et al. (2002) introduce the Extended Approach to avoid the insufficient coverage of the bilingual dictionary required for the translation of source context vectors. A variation of the latter method based on centroid is proposed by Daille and Morin (2005). Haghighi et al. (2008) employ dimension reduction using canonical component analysis (CCA) and Rubino and Linares (2011) propose a multi-view approach based on linear discriminant analysis (LDA) among oth"
I13-1196,C02-1166,0,0.885949,"Missing"
I13-1196,J93-1003,0,0.644393,"ith similar meaning tend to occur in similar contexts, has been extended to the bilingual scenario (Fung, 1998; Rapp, 1999). Hence, using comparable corpora, a translation of a source word can be found by identifying a target word with the most similar context. A popular method often used as a baseline is the Standard Approach (Fung, 1998). It consists of using the bag-ofwords paradigm to represent words of source and target language by their context vector. After word contexts have been weighted using an association measure (the point-wise mutual information (Fano, 1961), the log-likelihood (Dunning, 1993), the discounted odds-ratio (Laroche and Langlais, 2010)), the similarity between a source word’s context vector and all the context vectors in the target language is computed using a similarity measure (cosine (Salton and Lesk, 1968), Jaccard (Grefenstette, 1994)...). Finally, the translation candidates are ranked according to their similarity score. Many variants of the Standard Approach have been proposed. They can differ in context representation (window-based, syntactic-based) (Morin et al., 2007; Gamallo, 2008), corpus characteristics (small, large, general or domain specific...)(Chiao a"
I13-1196,P07-2008,0,0.064878,"Missing"
I13-1196,W97-0119,0,0.627014,"tion of source context vectors. A variation of the latter method based on centroid is proposed by Daille and Morin (2005). Haghighi et al. (2008) employ dimension reduction using canonical component analysis (CCA) and Rubino and Linares (2011) propose a multi-view approach based on linear discriminant analysis (LDA) among others. 3 Standard Approach The Standard Approach is based on words cooccurrence vectors. The basic idea is to go through a corpus and to count the number of times n(c, t) each context word c occurs within a window of a certain size w around each target word t. According to (Fung and Mckeown, 1997; Fung, 1998; Rapp, 1999), the Standard Approach can be carried out as follows: For a source word to translate wis , we first build its context vector vwis . The vector vwis contains all the words that co-occur with wis within windows of n words. Let’s denote by coocc(wis , wjs ) the co-occurrence value of wis and a given word of its context wjs . The process of building context vectors is repeated for all the words of the target language. An association measure such as the point-wise mutual information (Fano, 1961), the log-likelihood (Dunning, 1993) or the discounted odds-ratio (Laroche and"
I13-1196,W95-0114,0,0.922129,"Missing"
I13-1196,P08-1088,0,0.025024,"ain specific...)(Chiao and Zweigenbaum, 2002; D´ejean et al., 2002; Morin et al., 2007), type of words to translate (single word terms (SWTs) or multiword terms (MWTs))(Rapp, 1999; Daille and Morin, 2005), words frequency (less frequent, rare...)(Pekar et al., 2006), etc. There exist other approaches for bilingual lexicon extraction. D´ejean et al. (2002) introduce the Extended Approach to avoid the insufficient coverage of the bilingual dictionary required for the translation of source context vectors. A variation of the latter method based on centroid is proposed by Daille and Morin (2005). Haghighi et al. (2008) employ dimension reduction using canonical component analysis (CCA) and Rubino and Linares (2011) propose a multi-view approach based on linear discriminant analysis (LDA) among others. 3 Standard Approach The Standard Approach is based on words cooccurrence vectors. The basic idea is to go through a corpus and to count the number of times n(c, t) each context word c occurs within a window of a certain size w around each target word t. According to (Fung and Mckeown, 1997; Fung, 1998; Rapp, 1999), the Standard Approach can be carried out as follows: For a source word to translate wis , we fir"
I13-1196,C10-1070,0,0.237652,"edicting models with the traditional Standard Approach (Fung, 1998) and show that once we have identified the best procedures, our method increases significantly the performance of extracting word translations from comparable corpora. 1 Introduction Using comparable corpora for bilingual lexicon extraction is becoming more and more a matter of interest, especially because of the easier availability of this kind of corpora comparing to parallel ones. Many researchers proposed a variety of approaches (Fung, 1995; Rapp, 1999; Chiao and Zweigenbaum, 2002; D´ejean et al., 2002; Morin et al., 2007; Laroche and Langlais, 2010, among others). While different improvements were achieved, the starting point remains words’co-occurrences as they represent the observable evidence that can be distilled from a corpus. Hence, frequency counts for word pairs often serve as a basis for distributional methods. The main assumption underlying bilingual lexicon extraction is: two words are more likely to be a translation of each other if they share the same lexical contexts (Fung, 1998). The most popular approach named, the Standard Approach (Fung and Mckeown, 1997; Rapp, 1999), makes use of this assumption to perform bilingual l"
I13-1196,P07-1084,1,0.807102,"compare different predicting models with the traditional Standard Approach (Fung, 1998) and show that once we have identified the best procedures, our method increases significantly the performance of extracting word translations from comparable corpora. 1 Introduction Using comparable corpora for bilingual lexicon extraction is becoming more and more a matter of interest, especially because of the easier availability of this kind of corpora comparing to parallel ones. Many researchers proposed a variety of approaches (Fung, 1995; Rapp, 1999; Chiao and Zweigenbaum, 2002; D´ejean et al., 2002; Morin et al., 2007; Laroche and Langlais, 2010, among others). While different improvements were achieved, the starting point remains words’co-occurrences as they represent the observable evidence that can be distilled from a corpus. Hence, frequency counts for word pairs often serve as a basis for distributional methods. The main assumption underlying bilingual lexicon extraction is: two words are more likely to be a translation of each other if they share the same lexical contexts (Fung, 1998). The most popular approach named, the Standard Approach (Fung and Mckeown, 1997; Rapp, 1999), makes use of this assum"
I13-1196,2009.mtsummit-posters.14,1,0.887361,"ect 3 http://www.nlm.nih.gov/research/umls 1396 5.4 Training Dataset Predicting models such as linear regression or the Good-Turing estimator need a large training corpus to estimate the adjusted co-occurrences. For that purpose, we chose a training corpus composed of two sets. A small set of 500,000 words and a large set of 10 million words. We selected the documents published in 1994 from newspapers Los Angeles Times/Le Monde. 6 Experiments and Results The baseline in our experiments is the Standard Approach (Fung, 1998) which is often used for comparison (Pekar et al., 2006; Gamallo, 2008; Prochasson and Morin, 2009), etc. In this section, we first give the parameters of the standard approach, than we present the results of the experiments conducted on the two corpora presented above: ’Breast cancer’ and ’Wind energy’. 6.1 Experimental Setup Using the Standard Approach, three major parameters need to be set: 1. The size of the window used to build the context vectors (Morin et al., 2007; Gamallo, 2008) 2. The association measure (the log-likelihood (Dunning, 1993), the point-wise mutual information (Fano, 1961), the discounted oddsratio (Laroche and Langlais, 2010)...) 3. The similarity measure (the weigh"
I13-1196,P99-1067,0,0.965104,"e building a predictive model of word co-occurrence counts. We compare different predicting models with the traditional Standard Approach (Fung, 1998) and show that once we have identified the best procedures, our method increases significantly the performance of extracting word translations from comparable corpora. 1 Introduction Using comparable corpora for bilingual lexicon extraction is becoming more and more a matter of interest, especially because of the easier availability of this kind of corpora comparing to parallel ones. Many researchers proposed a variety of approaches (Fung, 1995; Rapp, 1999; Chiao and Zweigenbaum, 2002; D´ejean et al., 2002; Morin et al., 2007; Laroche and Langlais, 2010, among others). While different improvements were achieved, the starting point remains words’co-occurrences as they represent the observable evidence that can be distilled from a corpus. Hence, frequency counts for word pairs often serve as a basis for distributional methods. The main assumption underlying bilingual lexicon extraction is: two words are more likely to be a translation of each other if they share the same lexical contexts (Fung, 1998). The most popular approach named, the Standard"
I17-1069,C02-2020,0,0.726673,"Missing"
I17-1069,E14-1049,0,0.103919,"over specialized and general domain data. min W n X kW xi − zi k2 (1) i=1 At prediction time, we can transfer the word embedding x for a word to be translated in the target language using the translation matrix such as z = W x. The candidate translations are obtained by ranking the closest target words to z according to a similarity measure such as the Cosine measure. Recently, Artetxe et al. (2016) presented an approach for learning bilingual mappings of word embeddings that preserves monolingual invariance using several meaningful and intuitive constraints related to other proposed methods (Faruqui and Dyer, 2014; Xing et al., 2015). These constraints are orthogonality, vectors length normalization for maximum cosine and mean centering for maximum covariance. Monolingual invariance tends to preserve the dot products after mapping, in order to avoid performance drop in monolingual tasks, while dimension-wise mean centering tends to insure that two randomly taken words would not be semantically related. This approach has shown meaningful improvements for both monolingual and bilingual tasks. Data Combination Using Neural Networks 3.1 Global Data Combination Using Neural Network Models This approach can"
I17-1069,W15-1513,0,0.207687,"e counts of specialized comparable corpora (Hazem and Morin, 2016). Our work is in this line and attempts to find out how a general-domain data can enrich a specialized comparable corpora to improve bilingual terminology extraction from specialized comparable corpora. Since bilingual word embeddings have recently provided efficient models for learning bilingual distributed representation of words from large general-domain data (Mikolov et al., 2013), we contrast different popular word embedding models for this task. In addition, we explore combinations of word embedding models as suggested by Garten et al. (2015) to improve distributed representations. We compare the results obtained with the state-of-the-art context-based projection approach. Our results show under which conditions the proposed model can compete with stateBilingual lexicon extraction from comparable corpora is constrained by the small amount of available data when dealing with specialized domains. This aspect penalizes the performance of distributionalbased approaches, which is closely related to the reliability of word’s cooccurrence counts extracted from comparable corpora. A solution to avoid this limitation is to associate extern"
I17-1069,P04-1067,0,0.402529,"Missing"
I17-1069,D16-1250,0,0.584267,"Projection Approach The historical context-based projection approach, known as the standard approach, has been studied by a number of researchers (Fung, 1998; Rapp, 1999; Chiao and Zweigenbaum, 2002; Morin et al., 2007; Prochasson and Fung, 2011; Bouamor et al., 2013; Morin and Hazem, 2016, among others). Its implementation can be carried out by applying the following steps: 2.2 Word Embedding Based Approach Bilingual word embeddings has become a source of great interest in recent times (Mikolov et al., 2013; Vuli´c and Moens, 2013; Zou et al., 2013; Chandar et al., 2014; Gouws et al., 2014; Artetxe et al., 2016, among others). Mikolov et al. (2013) was the first to propose a method to learn a linear transformation from the source language to the 1. For each word w of the source and the target languages, we build a context vector (resp. s and t for source and target languages) consisting in the measure of association of each word that appears in a short window of words 686 target language to improve the task of lexicon extraction from bilingual corpora. 3 During the training time of Mikolov’s method, for all {xi , zi }ni=1 bilingual word pairs of the seed lexicon, the word embedding xi ∈ Rd1 of word"
I17-1069,P13-2133,0,0.864718,"orpora are unreliable in specialized domain. This problem persists with other paradigms such as Canonical Correlation Analysis (CCA) (Gaussier et al., 2004), Independent Component Analysis (ICA) (Hazem and Morin, 2012) and Bilingual Latent Dirichlet Allocation (BiLDA) (Vuli´c et al., 2011). A solution to avoid this limitation and to increase the representativity of distributional representations is to associate external resources with the specialized comparable corpus. These resources can be lexical databases such as WordNet which allows the disambiguation of translations of polysemous words (Bouamor et al., 2013) or general-domain data to improve word cooccurrence counts of specialized comparable corpora (Hazem and Morin, 2016). Our work is in this line and attempts to find out how a general-domain data can enrich a specialized comparable corpora to improve bilingual terminology extraction from specialized comparable corpora. Since bilingual word embeddings have recently provided efficient models for learning bilingual distributed representation of words from large general-domain data (Mikolov et al., 2013), we contrast different popular word embedding models for this task. In addition, we explore com"
I17-1069,hazem-morin-2012-adaptive,1,0.819257,"manuel.morin}@univ-nantes.fr Abstract rable corpora are often of modest size (around 1 million words) due to the difficulty to obtain many specialized documents in a language other than English. Consequently, word co-occurrence counts of the historical context-based projection approach, known as the standard approach (Fung, 1995; Rapp, 1995), dedicated to bilingual lexicon extraction from comparable corpora are unreliable in specialized domain. This problem persists with other paradigms such as Canonical Correlation Analysis (CCA) (Gaussier et al., 2004), Independent Component Analysis (ICA) (Hazem and Morin, 2012) and Bilingual Latent Dirichlet Allocation (BiLDA) (Vuli´c et al., 2011). A solution to avoid this limitation and to increase the representativity of distributional representations is to associate external resources with the specialized comparable corpus. These resources can be lexical databases such as WordNet which allows the disambiguation of translations of polysemous words (Bouamor et al., 2013) or general-domain data to improve word cooccurrence counts of specialized comparable corpora (Hazem and Morin, 2016). Our work is in this line and attempts to find out how a general-domain data ca"
I17-1069,C16-1321,1,0.620478,"n Analysis (CCA) (Gaussier et al., 2004), Independent Component Analysis (ICA) (Hazem and Morin, 2012) and Bilingual Latent Dirichlet Allocation (BiLDA) (Vuli´c et al., 2011). A solution to avoid this limitation and to increase the representativity of distributional representations is to associate external resources with the specialized comparable corpus. These resources can be lexical databases such as WordNet which allows the disambiguation of translations of polysemous words (Bouamor et al., 2013) or general-domain data to improve word cooccurrence counts of specialized comparable corpora (Hazem and Morin, 2016). Our work is in this line and attempts to find out how a general-domain data can enrich a specialized comparable corpora to improve bilingual terminology extraction from specialized comparable corpora. Since bilingual word embeddings have recently provided efficient models for learning bilingual distributed representation of words from large general-domain data (Mikolov et al., 2013), we contrast different popular word embedding models for this task. In addition, we explore combinations of word embedding models as suggested by Garten et al. (2015) to improve distributed representations. We co"
I17-1069,P11-2084,0,0.0576393,"Missing"
I17-1069,P14-1006,0,0.0364936,"used to extract bilingual lexicons from comparable corpora. These approaches are both based on monolingual lexical context analysis and relies on the distributional hypothesis (Harris, 1968) which postulates that a word and its translation tend to appear in the same lexical contexts. This is the hypothesis that tends to be reduced to the famous sentence of the British linguist J. R. Firth (1957, p. 11) who said: “You shall know a word by the company it keeps.” even if the context was related to collocates. The two approaches are known as distributional and distributed semantics (according to Hermann and Blunsom (2014)). The first one is based on vector space models while the second one is based on neural language models. 2.1 Context-Based Projection Approach The historical context-based projection approach, known as the standard approach, has been studied by a number of researchers (Fung, 1998; Rapp, 1999; Chiao and Zweigenbaum, 2002; Morin et al., 2007; Prochasson and Fung, 2011; Bouamor et al., 2013; Morin and Hazem, 2016, among others). Its implementation can be carried out by applying the following steps: 2.2 Word Embedding Based Approach Bilingual word embeddings has become a source of great interest"
I17-1069,D13-1168,0,0.0434916,"Missing"
I17-1069,C10-1070,0,0.0231867,"s our conclusion. 2 2. For a word i to be translated, its context vector i is projected from the source to the target language by translating each element of its context vector thanks to a bilingual seed lexicon. 3. The translated context vector i is compared to each context vector t of the target language using a similarity measure such as Cosine or weighted Jaccard. The candidate translations are then ranked according to the scores of a given similarity measure. State-of-the-Art Approaches This approach is very sensitive to the choice of parameters. We invite readers to consult the study of Laroche and Langlais (2010) in which the influence of parameters such as the size of the context, the choice of the association and similarity measures have been examined. In order to improve the quality of bilingual terminology extraction from specialized comparable corpora, Hazem and Morin (2016) have proposed two ways to combine specialized comparable corpora with external resources. The hypothesis is that word co-occurrences learned from a large general-domain corpus for general words improve the characterisation of the specific vocabulary of the specialized corpus. The first adaptation called Global Standard Approa"
I17-1069,P15-2118,0,0.0643238,"Missing"
I17-1069,N15-1104,0,0.0388832,"eral domain data. min W n X kW xi − zi k2 (1) i=1 At prediction time, we can transfer the word embedding x for a word to be translated in the target language using the translation matrix such as z = W x. The candidate translations are obtained by ranking the closest target words to z according to a similarity measure such as the Cosine measure. Recently, Artetxe et al. (2016) presented an approach for learning bilingual mappings of word embeddings that preserves monolingual invariance using several meaningful and intuitive constraints related to other proposed methods (Faruqui and Dyer, 2014; Xing et al., 2015). These constraints are orthogonality, vectors length normalization for maximum cosine and mean centering for maximum covariance. Monolingual invariance tends to preserve the dot products after mapping, in order to avoid performance drop in monolingual tasks, while dimension-wise mean centering tends to insure that two randomly taken words would not be semantically related. This approach has shown meaningful improvements for both monolingual and bilingual tasks. Data Combination Using Neural Networks 3.1 Global Data Combination Using Neural Network Models This approach can be seen as similar t"
I17-1069,P07-1084,1,0.954959,"ntence of the British linguist J. R. Firth (1957, p. 11) who said: “You shall know a word by the company it keeps.” even if the context was related to collocates. The two approaches are known as distributional and distributed semantics (according to Hermann and Blunsom (2014)). The first one is based on vector space models while the second one is based on neural language models. 2.1 Context-Based Projection Approach The historical context-based projection approach, known as the standard approach, has been studied by a number of researchers (Fung, 1998; Rapp, 1999; Chiao and Zweigenbaum, 2002; Morin et al., 2007; Prochasson and Fung, 2011; Bouamor et al., 2013; Morin and Hazem, 2016, among others). Its implementation can be carried out by applying the following steps: 2.2 Word Embedding Based Approach Bilingual word embeddings has become a source of great interest in recent times (Mikolov et al., 2013; Vuli´c and Moens, 2013; Zou et al., 2013; Chandar et al., 2014; Gouws et al., 2014; Artetxe et al., 2016, among others). Mikolov et al. (2013) was the first to propose a method to learn a linear transformation from the source language to the 1. For each word w of the source and the target languages, we"
I17-1069,D13-1141,0,0.0202803,"nd one is based on neural language models. 2.1 Context-Based Projection Approach The historical context-based projection approach, known as the standard approach, has been studied by a number of researchers (Fung, 1998; Rapp, 1999; Chiao and Zweigenbaum, 2002; Morin et al., 2007; Prochasson and Fung, 2011; Bouamor et al., 2013; Morin and Hazem, 2016, among others). Its implementation can be carried out by applying the following steps: 2.2 Word Embedding Based Approach Bilingual word embeddings has become a source of great interest in recent times (Mikolov et al., 2013; Vuli´c and Moens, 2013; Zou et al., 2013; Chandar et al., 2014; Gouws et al., 2014; Artetxe et al., 2016, among others). Mikolov et al. (2013) was the first to propose a method to learn a linear transformation from the source language to the 1. For each word w of the source and the target languages, we build a context vector (resp. s and t for source and target languages) consisting in the measure of association of each word that appears in a short window of words 686 target language to improve the task of lexicon extraction from bilingual corpora. 3 During the training time of Mikolov’s method, for all {xi , zi }ni=1 bilingual word"
I17-1069,P11-1133,0,0.0879748,"h linguist J. R. Firth (1957, p. 11) who said: “You shall know a word by the company it keeps.” even if the context was related to collocates. The two approaches are known as distributional and distributed semantics (according to Hermann and Blunsom (2014)). The first one is based on vector space models while the second one is based on neural language models. 2.1 Context-Based Projection Approach The historical context-based projection approach, known as the standard approach, has been studied by a number of researchers (Fung, 1998; Rapp, 1999; Chiao and Zweigenbaum, 2002; Morin et al., 2007; Prochasson and Fung, 2011; Bouamor et al., 2013; Morin and Hazem, 2016, among others). Its implementation can be carried out by applying the following steps: 2.2 Word Embedding Based Approach Bilingual word embeddings has become a source of great interest in recent times (Mikolov et al., 2013; Vuli´c and Moens, 2013; Zou et al., 2013; Chandar et al., 2014; Gouws et al., 2014; Artetxe et al., 2016, among others). Mikolov et al. (2013) was the first to propose a method to learn a linear transformation from the source language to the 1. For each word w of the source and the target languages, we build a context vector (re"
I17-1069,P95-1050,0,0.339488,"reliability of word’s cooccurrence counts extracted from comparable corpora. A solution to avoid this limitation is to associate external resources with the comparable corpus. Since bilingual word embeddings have recently shown efficient models for learning bilingual distributed representation of words, we explore different word embedding models and show how a general-domain comparable corpus can enrich a specialized comparable corpus via neural networks. 1 Introduction Bilingual lexicon extraction from comparable corpora has shown substantial growth since the seminal work of Fung (1995) and Rapp (1995). Comparable corpora, which are comprised of texts sharing common features such as domain, genre, sampling period, etc. and without having a source text/target text relationship (McEnery and Xiao, 2007), are more abundant and reliable resources than parallel corpora. On the one hand, parallel corpora are difficult to obtain for language pairs not involving English. On the other hand, as parallel corpora are comprised of a pair of translated texts, the vocabulary appearing in the translated texts is highly influenced by the source texts. These problems are aggravated in specialized and technica"
I17-1069,P99-1067,0,0.532227,"that tends to be reduced to the famous sentence of the British linguist J. R. Firth (1957, p. 11) who said: “You shall know a word by the company it keeps.” even if the context was related to collocates. The two approaches are known as distributional and distributed semantics (according to Hermann and Blunsom (2014)). The first one is based on vector space models while the second one is based on neural language models. 2.1 Context-Based Projection Approach The historical context-based projection approach, known as the standard approach, has been studied by a number of researchers (Fung, 1998; Rapp, 1999; Chiao and Zweigenbaum, 2002; Morin et al., 2007; Prochasson and Fung, 2011; Bouamor et al., 2013; Morin and Hazem, 2016, among others). Its implementation can be carried out by applying the following steps: 2.2 Word Embedding Based Approach Bilingual word embeddings has become a source of great interest in recent times (Mikolov et al., 2013; Vuli´c and Moens, 2013; Zou et al., 2013; Chandar et al., 2014; Gouws et al., 2014; Artetxe et al., 2016, among others). Mikolov et al. (2013) was the first to propose a method to learn a linear transformation from the source language to the 1. For each"
L16-1661,C02-2020,0,0.0671227,"ikelihood (Dunning, 1993), the discounted odds-ratio (Laroche and Langlais, 2010)), the similarity between a source word’s context vector and all the context vectors in the target language is computed using a similarity measure (cosine (Salton and Lesk, 1968), Jaccard (Grefenstette, 1994)...). Finally, the translation candidates are ranked according to their similarity score. Many variants of the Standard Approach have been proposed. They can differ in context representation (window-based, syntactic-based) (Gamallo, 2008), corpus characteristics (small, large, general or domain spe´ cific...)(Chiao and Zweigenbaum, 2002; D´ejean and Eric Gaussier, 2002; Morin et al., 2007), type of words to translate (single word terms (SWTs) or multi-word terms (MWTs))(Rapp, 1999; Daille and Morin, 2005), words frequency (less frequent, rare...)(Pekar et al., 2006), etc. There exist other approaches for bilingual lexicon extraction. D´ejean et al. (2002) introduce the extended approach to avoid the insufficient coverage of the bilingual dictionary required for the translation of source context vectors. A variation of the latter method based on centroid is proposed by (Daille and Morin, 2005). Haghighi et al. (2008) employ d"
L16-1661,I05-1062,1,0.819286,"get language is computed using a similarity measure (cosine (Salton and Lesk, 1968), Jaccard (Grefenstette, 1994)...). Finally, the translation candidates are ranked according to their similarity score. Many variants of the Standard Approach have been proposed. They can differ in context representation (window-based, syntactic-based) (Gamallo, 2008), corpus characteristics (small, large, general or domain spe´ cific...)(Chiao and Zweigenbaum, 2002; D´ejean and Eric Gaussier, 2002; Morin et al., 2007), type of words to translate (single word terms (SWTs) or multi-word terms (MWTs))(Rapp, 1999; Daille and Morin, 2005), words frequency (less frequent, rare...)(Pekar et al., 2006), etc. There exist other approaches for bilingual lexicon extraction. D´ejean et al. (2002) introduce the extended approach to avoid the insufficient coverage of the bilingual dictionary required for the translation of source context vectors. A variation of the latter method based on centroid is proposed by (Daille and Morin, 2005). Haghighi et al. (2008) employ dimension reduction using canonical component analysis (CCA). The majority of the proposed approaches rely on context similarity. The starting point of context characterizat"
L16-1661,J93-1003,0,0.097408,"ngs tend to occur in similar contexts (Harris, 1954), has been extended to the bilingual scenario (Fung, 1998; Rapp, 1999). Hence, using comparable corpora, a translation of a source word can be found by identifying a target word with the most similar context. A popular method often used as a baseline is the Standard Approach (Fung, 1998). It consists of using the bag-of-words paradigm to represent words of source and target language by their context vector. After word contexts have been weighted using an association measure (the point-wise mutual information (Fano, 1961), the log-likelihood (Dunning, 1993), the discounted odds-ratio (Laroche and Langlais, 2010)), the similarity between a source word’s context vector and all the context vectors in the target language is computed using a similarity measure (cosine (Salton and Lesk, 1968), Jaccard (Grefenstette, 1994)...). Finally, the translation candidates are ranked according to their similarity score. Many variants of the Standard Approach have been proposed. They can differ in context representation (window-based, syntactic-based) (Gamallo, 2008), corpus characteristics (small, large, general or domain spe´ cific...)(Chiao and Zweigenbaum, 20"
L16-1661,C02-1166,0,0.141347,"Missing"
L16-1661,P04-1067,0,0.12927,"Missing"
L16-1661,P08-1088,0,0.0437467,"..)(Chiao and Zweigenbaum, 2002; D´ejean and Eric Gaussier, 2002; Morin et al., 2007), type of words to translate (single word terms (SWTs) or multi-word terms (MWTs))(Rapp, 1999; Daille and Morin, 2005), words frequency (less frequent, rare...)(Pekar et al., 2006), etc. There exist other approaches for bilingual lexicon extraction. D´ejean et al. (2002) introduce the extended approach to avoid the insufficient coverage of the bilingual dictionary required for the translation of source context vectors. A variation of the latter method based on centroid is proposed by (Daille and Morin, 2005). Haghighi et al. (2008) employ dimension reduction using canonical component analysis (CCA). The majority of the proposed approaches rely on context similarity. The starting point of context characterization is word co-occurrence statistics. It can provide a natural basis for semantic representation. Corpus-based word space models allow to go from distributional statistics to a geometric representation that induce the semantic representation of words from their patterns of co-occurrence in text. While literature suggest numerous techniques that could be used for that purpose, it is not obvious which is the best and"
L16-1661,C10-1070,0,0.145335,"is, 1954), has been extended to the bilingual scenario (Fung, 1998; Rapp, 1999). Hence, using comparable corpora, a translation of a source word can be found by identifying a target word with the most similar context. A popular method often used as a baseline is the Standard Approach (Fung, 1998). It consists of using the bag-of-words paradigm to represent words of source and target language by their context vector. After word contexts have been weighted using an association measure (the point-wise mutual information (Fano, 1961), the log-likelihood (Dunning, 1993), the discounted odds-ratio (Laroche and Langlais, 2010)), the similarity between a source word’s context vector and all the context vectors in the target language is computed using a similarity measure (cosine (Salton and Lesk, 1968), Jaccard (Grefenstette, 1994)...). Finally, the translation candidates are ranked according to their similarity score. Many variants of the Standard Approach have been proposed. They can differ in context representation (window-based, syntactic-based) (Gamallo, 2008), corpus characteristics (small, large, general or domain spe´ cific...)(Chiao and Zweigenbaum, 2002; D´ejean and Eric Gaussier, 2002; Morin et al., 2007)"
L16-1661,P07-1084,1,0.733431,"and Langlais, 2010)), the similarity between a source word’s context vector and all the context vectors in the target language is computed using a similarity measure (cosine (Salton and Lesk, 1968), Jaccard (Grefenstette, 1994)...). Finally, the translation candidates are ranked according to their similarity score. Many variants of the Standard Approach have been proposed. They can differ in context representation (window-based, syntactic-based) (Gamallo, 2008), corpus characteristics (small, large, general or domain spe´ cific...)(Chiao and Zweigenbaum, 2002; D´ejean and Eric Gaussier, 2002; Morin et al., 2007), type of words to translate (single word terms (SWTs) or multi-word terms (MWTs))(Rapp, 1999; Daille and Morin, 2005), words frequency (less frequent, rare...)(Pekar et al., 2006), etc. There exist other approaches for bilingual lexicon extraction. D´ejean et al. (2002) introduce the extended approach to avoid the insufficient coverage of the bilingual dictionary required for the translation of source context vectors. A variation of the latter method based on centroid is proposed by (Daille and Morin, 2005). Haghighi et al. (2008) employ dimension reduction using canonical component analysis"
L16-1661,2009.mtsummit-posters.14,1,0.702726,"f filtering, 321 French/English SWTs were extracted (from the UMLS4 meta-thesaurus.) for the breast cancer corpus, 150 pairs for the wind-energy corpus and 158 for the volcano corpus. 3.3. Evaluation Measure Three major parameters need to be set, namely the association measure, the similarity measure and the size of 2 www.elsevier.com ELRA dictionary has been done by Sciper in the Technolangue/Euradic project 4 http://www.nlm.nih.gov/research/umls 3 3.4. Baseline The baseline in our experiments is the standard approach (Fung, 1998) often used for comparison (Pekar et al., 2006; Gamallo, 2008; Prochasson and Morin, 2009), etc. 4. Experiments and Results We note that ’Top k’ means that the correct translation is present in the k first candidates of the list returned by a given method. We use also the mean average precision MAP (Manning and Schuze, 2008). 4.1. Experimental Setup The experiments have been carried out on three EnglishFrench comparable corpora. A specialized corpus of 1 million words from the medical domain within the sub-domain of ’breast cancer’2 , a specialized corpus from the domain of ’wind-energy’ of 600,000 words and a specialized corpus from the domain of geology within the sub-domain of V"
L16-1661,P99-1067,0,0.642571,"ce, we present in this paper a systematic exploration of the principal corpus-based word space models for bilingual terminology extraction from comparable corpora. We find that, once we have identified the best procedures, a very simple combination approach leads to significant improvements compared to individual models. Keywords: Comparable corpora, Bilingual lexicon extraction, word-space models 1. Introduction The distributional hypothesis which states that words with similar meanings tend to occur in similar contexts (Harris, 1954), has been extended to the bilingual scenario (Fung, 1998; Rapp, 1999). Hence, using comparable corpora, a translation of a source word can be found by identifying a target word with the most similar context. A popular method often used as a baseline is the Standard Approach (Fung, 1998). It consists of using the bag-of-words paradigm to represent words of source and target language by their context vector. After word contexts have been weighted using an association measure (the point-wise mutual information (Fano, 1961), the log-likelihood (Dunning, 1993), the discounted odds-ratio (Laroche and Langlais, 2010)), the similarity between a source word’s context ve"
L18-1069,L16-1155,0,0.0291872,"ying direct access to documents. Therefore, we added a condition on the workflow based on an arbitrary 448 towards reaching consensus by majority voting. Besides, it has been shown (Little et al., 2010) that an iterative updating process can be quite efficient to achieve the consensus in a transcription task. User engagement Online platforms can improve user engagement through rewards, challenges, community management with discussion forum, etc. Such simple feedback and motivational techniques have been proved useful when you face a small crowd of volunteers inherently interested in the task (Clematide et al., 2016). Figure 4: Distribution of answers (taking all activities together) per worker at the date of 2018-01-23. Only the workers that completed at least one task are considered. User profiling There exist state-of-the-art methods to insert fake tasks with known results to classify users (ELICE for Expert Label Injected Crowd Estimation) or learn from disagreement. Finally, an interesting perspective is related to the combination of CS results and supervised segmentation and transcription. This idea is described in the next section. 5. Line Transcription CS platforms requires a lot of workers to rea"
N13-1030,J08-4004,0,0.0440918,"Missing"
N13-1030,J05-3002,0,0.880595,"evaluation dataset made of 40 sets of related sentences along with reference compressions composed by humans. The rest of this paper is organized as follows. We first briefly review the previous work, followed by a description of the method we propose. Next, we give the details of the evaluation dataset we have constructed and present our experiments and results. Lastly, we conclude with a discussion and directions for further work. 2 Related work 2.1 Multi-sentence compression MSC have received much attention recently and many different approaches have been proposed. The pioneering work of (Barzilay and McKeown, 2005) introduced the framework used by many subsequent works: input sentences are represented by dependency trees, some words are aligned to merge the trees into a lattice, and the lattice is linearized using tree traversal to produce fusion sentences. (Filippova and Strube, 2008) cast MSC as an integer linear program, and show promising results for German. Later, (Elsner and Santhanam, 2011) proposed a supervised approach trained on examples of manually fused sentences. Previously described approaches require the use of a syntactic parser to control the grammaticality of the output. As an alternat"
N13-1030,P06-1048,0,0.0315939,"use of automatic methods for evaluating machine-generated text has gradually become the mainstream in Computational Linguistics. Well known examples are the ROUGE (Lin, 2004) and B LEU (Papineni et al., 2002) evaluation metrics used in the summarization and MT communities. These metrics assess the quality of a system output by computing its similarity to one or more human-generated references. Prior work in sentence compression use the F1 measure over grammatical relations to evaluate candidate compressions (Riezler et al., 2003). It was shown to correlate significantly with human judgments (Clarke and Lapata, 2006) and behave similarly to B LEU (Unno et al., 2006). However, this metric is not entirely reliable as it depends on parser accuracy and the type of dependency relations used (Napoles et al., 2011). In this work, the following evaluation measures are considered relevant: B LEU3 , ROUGE -1 (unigrams), ROUGE -2 (bigrams) and ROUGE - SU 4 (bigrams with skip distance up to 4 words)4 . ROUGE measures are computed using stopword removal and French stemming 5 . 4.3 Manual evaluation The quality of the generated compressions was assessed in an experiment with human raters. Two aspects were considered: g"
N13-1030,W11-1607,0,0.0134337,"h a discussion and directions for further work. 2 Related work 2.1 Multi-sentence compression MSC have received much attention recently and many different approaches have been proposed. The pioneering work of (Barzilay and McKeown, 2005) introduced the framework used by many subsequent works: input sentences are represented by dependency trees, some words are aligned to merge the trees into a lattice, and the lattice is linearized using tree traversal to produce fusion sentences. (Filippova and Strube, 2008) cast MSC as an integer linear program, and show promising results for German. Later, (Elsner and Santhanam, 2011) proposed a supervised approach trained on examples of manually fused sentences. Previously described approaches require the use of a syntactic parser to control the grammaticality of the output. As an alternative, several word graph-based approaches that only require a POS tagger were proposed. The key assumption is 299 that redundancy provides a reliable way of generating grammatical sentences. First, a directed word graph is constructed from the set of input sentences in which nodes represent unique words, defined as word and POS tuples, and edges express the original structure of sentences"
N13-1030,D08-1019,0,0.13079,"ils of the evaluation dataset we have constructed and present our experiments and results. Lastly, we conclude with a discussion and directions for further work. 2 Related work 2.1 Multi-sentence compression MSC have received much attention recently and many different approaches have been proposed. The pioneering work of (Barzilay and McKeown, 2005) introduced the framework used by many subsequent works: input sentences are represented by dependency trees, some words are aligned to merge the trees into a lattice, and the lattice is linearized using tree traversal to produce fusion sentences. (Filippova and Strube, 2008) cast MSC as an integer linear program, and show promising results for German. Later, (Elsner and Santhanam, 2011) proposed a supervised approach trained on examples of manually fused sentences. Previously described approaches require the use of a syntactic parser to control the grammaticality of the output. As an alternative, several word graph-based approaches that only require a POS tagger were proposed. The key assumption is 299 that redundancy provides a reliable way of generating grammatical sentences. First, a directed word graph is constructed from the set of input sentences in which n"
N13-1030,C10-1037,0,0.471327,"e summary (D’Avanzo and Magnini, 2005). In the same way, we hypothesize that keyphrases can be used to better generate sentences that convey the gist 298 Proceedings of NAACL-HLT 2013, pages 298–305, c Atlanta, Georgia, 9–14 June 2013. 2013 Association for Computational Linguistics of the set of related sentences. In this paper, we present a reranking method of N-best multi-sentence compressions based on keyphrase extraction and describe a series of experiments conducted on a manually constructed evaluation corpus. More precisely, the main contributions of our work are as follows: • We extend Filippova (2010)’s word graphbased MSC approach to produce wellpunctuated and more informative compressions. • We investigate the use of automatic Machine Translation (MT) and summarization evaluation metrics to evaluate MSC performance. • We introduce a French evaluation dataset made of 40 sets of related sentences along with reference compressions composed by humans. The rest of this paper is organized as follows. We first briefly review the previous work, followed by a description of the method we propose. Next, we give the details of the evaluation dataset we have constructed and present our experiments a"
N13-1030,C10-1039,0,0.113207,"that only require a POS tagger were proposed. The key assumption is 299 that redundancy provides a reliable way of generating grammatical sentences. First, a directed word graph is constructed from the set of input sentences in which nodes represent unique words, defined as word and POS tuples, and edges express the original structure of sentences (i.e. word ordering). Sentence compressions are obtained by finding commonly used paths in the graph. Word graphbased MSC approaches were used in different tasks, such as guided microblog summarization (Sharifi et al., 2010), opinion summarization (Ganesan et al., 2010) and newswire summarization (Filippova, 2010). 2.2 Keyphrase extraction Keyphrases are words that are representative of the main content of documents. Extracting keyphrases can benefit various Natural Language Processing tasks such as summarization, information retrieval and question-answering (Kim et al., 2010). Previous works fall into two categories: supervised and unsupervised methods. The idea behind supervised methods is to recast keyphrase extraction as a binary classification task. A model is trained using annotated data to determine whether a given phrase is a keyphrase or not (Frank"
N13-1030,W06-2606,0,0.0674369,"Missing"
N13-1030,A00-1043,0,0.0604216,"Missing"
N13-1030,S10-1004,0,0.0304693,"e original structure of sentences (i.e. word ordering). Sentence compressions are obtained by finding commonly used paths in the graph. Word graphbased MSC approaches were used in different tasks, such as guided microblog summarization (Sharifi et al., 2010), opinion summarization (Ganesan et al., 2010) and newswire summarization (Filippova, 2010). 2.2 Keyphrase extraction Keyphrases are words that are representative of the main content of documents. Extracting keyphrases can benefit various Natural Language Processing tasks such as summarization, information retrieval and question-answering (Kim et al., 2010). Previous works fall into two categories: supervised and unsupervised methods. The idea behind supervised methods is to recast keyphrase extraction as a binary classification task. A model is trained using annotated data to determine whether a given phrase is a keyphrase or not (Frank et al., 1999; Turney, 2000). Unsupervised approaches proposed so far have involved a number of techniques, including language modeling (Tomokiyo and Hurst, 2003), graph-based ranking (Mihalcea and Tarau, 2004; Wan and Xiao, 2008) and clustering (Liu et al., 2009). While supervised approaches have generally prove"
N13-1030,W04-1013,0,0.13211,"Missing"
N13-1030,D09-1027,0,0.0125769,"on, information retrieval and question-answering (Kim et al., 2010). Previous works fall into two categories: supervised and unsupervised methods. The idea behind supervised methods is to recast keyphrase extraction as a binary classification task. A model is trained using annotated data to determine whether a given phrase is a keyphrase or not (Frank et al., 1999; Turney, 2000). Unsupervised approaches proposed so far have involved a number of techniques, including language modeling (Tomokiyo and Hurst, 2003), graph-based ranking (Mihalcea and Tarau, 2004; Wan and Xiao, 2008) and clustering (Liu et al., 2009). While supervised approaches have generally proven more successful, the need for training data and the bias towards the domain on which they are trained remain two critical issues. 3 Method In this section, we first describe Filippova (2010)’s word graph-based MSC approach. Then, we present the keyphrase extraction approach we use and our method for reranking generated compressions. 3.1 Description of Filippova’s approach Let G = (V, E) be a directed graph with the set of vertices (nodes) V and a set of directed edges E, where E is a subset of V × V. Given a set of related sentences S = {s1 ,"
N13-1030,W11-1611,0,0.0272717,"Missing"
N13-1030,P02-1040,0,0.0857746,"Missing"
N13-1030,N03-1026,0,0.0174657,"Missing"
N13-1030,W03-1805,0,0.053469,"of documents. Extracting keyphrases can benefit various Natural Language Processing tasks such as summarization, information retrieval and question-answering (Kim et al., 2010). Previous works fall into two categories: supervised and unsupervised methods. The idea behind supervised methods is to recast keyphrase extraction as a binary classification task. A model is trained using annotated data to determine whether a given phrase is a keyphrase or not (Frank et al., 1999; Turney, 2000). Unsupervised approaches proposed so far have involved a number of techniques, including language modeling (Tomokiyo and Hurst, 2003), graph-based ranking (Mihalcea and Tarau, 2004; Wan and Xiao, 2008) and clustering (Liu et al., 2009). While supervised approaches have generally proven more successful, the need for training data and the bias towards the domain on which they are trained remain two critical issues. 3 Method In this section, we first describe Filippova (2010)’s word graph-based MSC approach. Then, we present the keyphrase extraction approach we use and our method for reranking generated compressions. 3.1 Description of Filippova’s approach Let G = (V, E) be a directed graph with the set of vertices (nodes) V a"
N13-1030,P06-2109,0,0.0224365,"ed text has gradually become the mainstream in Computational Linguistics. Well known examples are the ROUGE (Lin, 2004) and B LEU (Papineni et al., 2002) evaluation metrics used in the summarization and MT communities. These metrics assess the quality of a system output by computing its similarity to one or more human-generated references. Prior work in sentence compression use the F1 measure over grammatical relations to evaluate candidate compressions (Riezler et al., 2003). It was shown to correlate significantly with human judgments (Clarke and Lapata, 2006) and behave similarly to B LEU (Unno et al., 2006). However, this metric is not entirely reliable as it depends on parser accuracy and the type of dependency relations used (Napoles et al., 2011). In this work, the following evaluation measures are considered relevant: B LEU3 , ROUGE -1 (unigrams), ROUGE -2 (bigrams) and ROUGE - SU 4 (bigrams with skip distance up to 4 words)4 . ROUGE measures are computed using stopword removal and French stemming 5 . 4.3 Manual evaluation The quality of the generated compressions was assessed in an experiment with human raters. Two aspects were considered: grammaticality and informativity. Following previou"
N13-1030,C08-1122,0,0.0354415,"Processing tasks such as summarization, information retrieval and question-answering (Kim et al., 2010). Previous works fall into two categories: supervised and unsupervised methods. The idea behind supervised methods is to recast keyphrase extraction as a binary classification task. A model is trained using annotated data to determine whether a given phrase is a keyphrase or not (Frank et al., 1999; Turney, 2000). Unsupervised approaches proposed so far have involved a number of techniques, including language modeling (Tomokiyo and Hurst, 2003), graph-based ranking (Mihalcea and Tarau, 2004; Wan and Xiao, 2008) and clustering (Liu et al., 2009). While supervised approaches have generally proven more successful, the need for training data and the bias towards the domain on which they are trained remain two critical issues. 3 Method In this section, we first describe Filippova (2010)’s word graph-based MSC approach. Then, we present the keyphrase extraction approach we use and our method for reranking generated compressions. 3.1 Description of Filippova’s approach Let G = (V, E) be a directed graph with the set of vertices (nodes) V and a set of directed edges E, where E is a subset of V × V. Given a"
N13-1030,W04-3252,0,\N,Missing
N13-1030,N10-1100,0,\N,Missing
P07-1084,C02-1011,0,0.0343127,"Missing"
P07-1084,C02-2020,0,0.786859,"Missing"
P07-1084,C02-1166,0,0.638566,"Missing"
P07-1084,1999.tc-1.8,0,0.0479716,"exical units of the context vectors, which depends on the coverage of the bilingual dictionary vis-à-vis the corpus, is an important step of the direct approach: more elements of the context vector are translated more the context vector will be discrimating for selecting translations in the target language. If the bilingual dictionary provides several translations for a lexical unit, we consider all of them but weight the different translations by their frequency in the target language. If an MWT cannot be directly translated, we generate possible translations by using a compositional method (Grefenstette, 1999). For each element of the MWT found in the bilingual dictionary, we generate all the translated combinations identified by the term extraction program. For example, in the case of the MWT fatigue chronique (chronic fatigue), we have the fol, , lowing four translations for fatigue: , and the following two translations for chronique: , . Next, we generate all combinations of translated elements (See Table 1 7 ) and select those which refer to an existing MWT in the target language. Here, only one term has been identified by the Japanese terminology extraction program: . . In this approach, when"
P07-1084,P99-1067,0,0.865212,"ed and classified into two discourse categories: one contains only scientific documents and the other contains both scientific and popular science documents. We used a state-of-the-art multilingual terminology mining chain composed of two term extraction programs, one in each language, and an alignment program. The term extraction programs are publicly available and both extract multi-word terms that are more precise and specific to a particular scientific domain than single word terms. The alignment program makes use of the direct context-vector approach (Fung, 1998; Peters and Picchi, 1998; Rapp, 1999) slightly modified to handle both singleand multi-word terms. We evaluated the candidate translations of multi-word terms using a reference list compiled from publicly available resources. We found that taking discourse type into account resulted in candidate translations of a better quality even when the corpus size is reduced by half. Thus, even using a state-of-the-art alignment method wellknown as data greedy, we reached the conclusion that the quantity of data is not sufficient to obtain a terminological list of high quality and that a real comparability of corpora is required. 2 Multilin"
P07-1084,E06-1029,0,0.519904,"lowing four translations for fatigue: , and the following two translations for chronique: , . Next, we generate all combinations of translated elements (See Table 1 7 ) and select those which refer to an existing MWT in the target language. Here, only one term has been identified by the Japanese terminology extraction program: . . In this approach, when it is not possible to translate all parts of an MWT, or when the translated combinations are not identified by the term extraction program, the MWT is not taken into account in the translation process. This approach differs from that used by (Robitaille et al., 2006) for French/Japanese translation. They first decompose the French MWT into combinations of shorter multi-word units (MWU) elements. This approach makes the direct translation of a subpart of the MWT possible if it is present in the         7 the French word order is inverted to take into account the different constraints between French and Japanese. 667 fatigue          Table 1: Illustration of the compositional method. The underlined Japanese MWT actually exists.  bilingual dictionary. For an MWT of length , (Robitaille et al., 2006) produce all the combin"
P14-1121,P13-2133,0,0.733507,"complete study about the influence of these parameters on the quality of word alignment has been carried out by Laroche and Langlais (2010). The standard approach is used by most researchers so far (Rapp, 1995; Fung, 1998; Peters and Picchi, 1998; Rapp, 1999; Chiao and Zweigenbaum, 2002; D´ejean et al., 2002; Gaussier et al., 2004; Morin et al., 2007; Laroche and Langlais, 2010; Prochasson and Fung, 2011; 1285 References Tanaka and Iwasaki (1996) Fung and McKeown (1997) Rapp (1999) Chiao and Zweigenbaum (2002) D´ejean et al. (2002) Morin et al. (2007) Otero (2007) Ismail and Manandhar (2010) Bouamor et al. (2013) - Domain Newspaper Newspaper Newspaper Medical Medical Medical European Parliament European Parliament Financial Medical Languages EN/JP EN/JP GE/EN FR/EN GE/EN FR/JP SP/EN EN/SP FR/EN FR/EN Source/Target Sizes 30/33 million words 49/60 million bytes of data 135/163 million words 602,484/608,320 words 100,000/100,000 words 693,666/807,287 words 14/17 million words 500,000/500,000 sentences 402,486/756,840 words 396,524/524,805 words Table 2: Characteristics of the comparable corpora used for bilingual lexicon extraction Bouamor et al., 2013, among others) with the implicit hypothesis that com"
P14-1121,C02-2020,0,0.846181,"Missing"
P14-1121,C02-1166,0,0.696471,"Missing"
P14-1121,C04-1151,0,0.0832384,"Missing"
P14-1121,W97-0119,0,0.16547,"Missing"
P14-1121,W95-0114,0,0.598568,"carried out a study on the influence of unbalanced specialized comparable corpora on the quality of bilingual terminology extraction through different experiments. Moreover, we have introduced a regression model that boosts the observations of word cooccurrences used in the context-based projection method. Our results show that the use of unbalanced specialized comparable corpora induces a significant gain in the quality of extracted lexicons. 1 The bilingual lexicon extraction task from comparable corpora inherits this filiation. For instance, the historical context-based projection method (Fung, 1995; Rapp, 1995), known as the standard approach, dedicated to this task seems implicitly to lead to work with balanced comparable corpora in the same way as for parallel corpora (i.e. each part of the corpus is composed of the same amount of data). Introduction The bilingual lexicon extraction task from bilingual corpora was initially addressed by using parallel corpora (i.e. a corpus that contains source texts and their translation). However, despite good results in the compilation of bilingual lexicons, parallel corpora are scarce resources, especially for technical domains and for language pa"
P14-1121,P04-1067,0,0.463295,"Missing"
P14-1121,I13-1196,1,0.836171,"easure (for instance, this can be done by dividing each entry of a given context vector by the sum of its association scores). 2.2 Prediction Model Since comparable corpora are usually small in specialized domains (see Table 2), the discrimina1 We only found mention of this aspect in Diab and Finch (2000, p. 1501) “In principle, we do not have to have the same size corpora in order for the approach to work”. tive power of context vectors (i.e. the observations of word co-occurrences) is reduced. One way to deal with this problem is to re-estimate co-occurrence counts by a prediction function (Hazem and Morin, 2013). This consists in assigning to each observed co-occurrence count of a small comparable corpora, a new value learned beforehand from a large training corpus. In order to make co-occurrence counts more discriminant and in the same way as Hazem and Morin (2013), one strategy consists in addressing this problem through regression: given training corpora of small and large size (abundant in the general domain), we predict word cooccurrence counts in order to make them more reliable. We then apply the resulting regression function to each word co-occurrence count as a pre-processing step of the sta"
P14-1121,C10-2055,0,0.301609,"imilarity measures. The most complete study about the influence of these parameters on the quality of word alignment has been carried out by Laroche and Langlais (2010). The standard approach is used by most researchers so far (Rapp, 1995; Fung, 1998; Peters and Picchi, 1998; Rapp, 1999; Chiao and Zweigenbaum, 2002; D´ejean et al., 2002; Gaussier et al., 2004; Morin et al., 2007; Laroche and Langlais, 2010; Prochasson and Fung, 2011; 1285 References Tanaka and Iwasaki (1996) Fung and McKeown (1997) Rapp (1999) Chiao and Zweigenbaum (2002) D´ejean et al. (2002) Morin et al. (2007) Otero (2007) Ismail and Manandhar (2010) Bouamor et al. (2013) - Domain Newspaper Newspaper Newspaper Medical Medical Medical European Parliament European Parliament Financial Medical Languages EN/JP EN/JP GE/EN FR/EN GE/EN FR/JP SP/EN EN/SP FR/EN FR/EN Source/Target Sizes 30/33 million words 49/60 million bytes of data 135/163 million words 602,484/608,320 words 100,000/100,000 words 693,666/807,287 words 14/17 million words 500,000/500,000 sentences 402,486/756,840 words 396,524/524,805 words Table 2: Characteristics of the comparable corpora used for bilingual lexicon extraction Bouamor et al., 2013, among others) with the implic"
P14-1121,P11-1133,0,0.208858,"k t t t assoc√ ∑ 2 l k2 t assoct t assoct Cosinevvkl = √∑ (1) (2) This approach is sensitive to the choice of parameters such as the size of the context, the choice of the association and similarity measures. The most complete study about the influence of these parameters on the quality of word alignment has been carried out by Laroche and Langlais (2010). The standard approach is used by most researchers so far (Rapp, 1995; Fung, 1998; Peters and Picchi, 1998; Rapp, 1999; Chiao and Zweigenbaum, 2002; D´ejean et al., 2002; Gaussier et al., 2004; Morin et al., 2007; Laroche and Langlais, 2010; Prochasson and Fung, 2011; 1285 References Tanaka and Iwasaki (1996) Fung and McKeown (1997) Rapp (1999) Chiao and Zweigenbaum (2002) D´ejean et al. (2002) Morin et al. (2007) Otero (2007) Ismail and Manandhar (2010) Bouamor et al. (2013) - Domain Newspaper Newspaper Newspaper Medical Medical Medical European Parliament European Parliament Financial Medical Languages EN/JP EN/JP GE/EN FR/EN GE/EN FR/JP SP/EN EN/SP FR/EN FR/EN Source/Target Sizes 30/33 million words 49/60 million bytes of data 135/163 million words 602,484/608,320 words 100,000/100,000 words 693,666/807,287 words 14/17 million words 500,000/500,000 sen"
P14-1121,W02-0902,0,0.0465966,"that a word and its candidate translations share thematic similarities. Yu and Tsujii (2009) and Otero (2007) propose, for their part, to replace the window-based method by a syntax-based method in order to improve the representation of the lexical context. To improve the transfer context vectors step, and increase the number of elements of translated context vectors, Chiao and Zweigenbaum (2003) and Morin and Prochasson (2011) combine a standard general language dictionary with a specialized dictionary, whereas D´ejean et al. (2002) use the hierarchical properties of a specialized thesaurus. Koehn and Knight (2002) automatically induce the initial seed bilingual dictionary by using identical spelling features such as cognates and similar contexts. As regards the problem of words ambiguities, Bouamor et al. (2013) carried out word sense disambiguation process only in the target language whereas Gaussier et al. (2004) solve the problem through the source and target languages by using approaches based on CCA (Canonical Correlation Analysis) and multilingual PLSA (Probabilistic Latent Semantic Analysis). The rank of candidate translations can be improved by integrating different heuristics. For instance, Ch"
P14-1121,2009.mtsummit-posters.14,1,0.891893,"y ∈ R using one of the regression models presented below: yˆLin = β0 + β1 x yˆLogit = 1 1 + exp(−(β0 + β1 x)) yˆP olyn = β0 + β1 x + β2 x2 + ... + βn xn (3) (4) (5) where βi are the parameters to estimate. Let us denote by f the regression function and by cooc(wi , wj ) the co-occurrence count of the words wi and wj . The resulting predicted value of cooc(wi , wj ), noted cooc(w ˆ i , wj ) is given by the following equation: cooc(w ˆ i , wj ) = f (cooc(wi , wj )) (6) 2.3 Related Work In the past few years, several contributions have been proposed to improve each step of the standard approach. Prochasson et al. (2009) enhance the representativeness of the context vector by strengthening the context words that happen to be transliterated words and scientific compound words in the target language. Ismail and Manandhar (2010) also suggest that context vectors should be based on the most important contextually relevant words (indomain terms), and thus propose a method for filtering the noise of the context vectors. In another way, Rubino and Linar`es (2011) improve the context words based on the hypothesis that a word and its candidate translations share thematic similarities. Yu and Tsujii (2009) and Otero (2"
P14-1121,C10-1070,0,0.650654,"rd window approximates syntactic dependencies). In order to emphasize significant words in the context vector and to reduce word-frequency effects, the context vectors are normalized according to an association measure. Then, the translation is obtained by comparing the source context vector to each translation candidate vector after having translated each element of the source vector with a general dictionary. The implementation of the standard approach can be carried out by applying the following three steps (Rapp, 1999; Chiao and Zweigenbaum, 2002; D´ejean et al., 2002; Morin et al., 2007; Laroche and Langlais, 2010, among others): Computing context vectors We collect all the words in the context of each word i and count their occurrence frequency in a window of n words around i. For each word i of the source and the target languages, we obtain a context vector vi which gathers the set of co-occurrence words j associated with the number of times that j and i occur together cooc(i, j). In order to identify specific words in the lexical context and to reduce wordfrequency effects, we normalize context vectors using an association score such as Mutual Information, Log-likelihood, or the discounted log-odds"
P14-1121,P95-1050,0,0.448913,"a study on the influence of unbalanced specialized comparable corpora on the quality of bilingual terminology extraction through different experiments. Moreover, we have introduced a regression model that boosts the observations of word cooccurrences used in the context-based projection method. Our results show that the use of unbalanced specialized comparable corpora induces a significant gain in the quality of extracted lexicons. 1 The bilingual lexicon extraction task from comparable corpora inherits this filiation. For instance, the historical context-based projection method (Fung, 1995; Rapp, 1995), known as the standard approach, dedicated to this task seems implicitly to lead to work with balanced comparable corpora in the same way as for parallel corpora (i.e. each part of the corpus is composed of the same amount of data). Introduction The bilingual lexicon extraction task from bilingual corpora was initially addressed by using parallel corpora (i.e. a corpus that contains source texts and their translation). However, despite good results in the compilation of bilingual lexicons, parallel corpora are scarce resources, especially for technical domains and for language pairs not invol"
P14-1121,C10-1073,0,0.148459,"inguistic preprocessing steps: tokenisation, part-of-speech tagging, and lemmatisation. These steps were carried out using the TTC TermSuite4 that applies the same method to several languages including French and English. Finally, the function words were removed and the words occurring less than twice in the French part and in each English part were discarded. Table 3 shows the number of distinct words (# words) after these steps. It also indicates the comparability degree in percentage (comp.) between the French part and each English part of each comparable corpus. The comparability measure (Li and Gaussier, 2010) is based on the expectation of finding the translation for each word in the corpus and gives a good idea about how two corpora are comparable. We can notice that all the comparable corpora have a high degree of comparability with a better comparability of the breast cancer corpora as opposed to the diabetes corpora. In the remainder of this article, [breast cancer corpus i] for instance stands for the breast cancer comparable corpus composed of the unique French part and the English part i (i ∈ [1, 14]). 3.2 Bilingual Dictionary The bilingual dictionary used in our experiments is the French/E"
P14-1121,P99-1067,0,0.959406,"a word which occurs within the window of the word to be translated (e.g. a seven-word window approximates syntactic dependencies). In order to emphasize significant words in the context vector and to reduce word-frequency effects, the context vectors are normalized according to an association measure. Then, the translation is obtained by comparing the source context vector to each translation candidate vector after having translated each element of the source vector with a general dictionary. The implementation of the standard approach can be carried out by applying the following three steps (Rapp, 1999; Chiao and Zweigenbaum, 2002; D´ejean et al., 2002; Morin et al., 2007; Laroche and Langlais, 2010, among others): Computing context vectors We collect all the words in the context of each word i and count their occurrence frequency in a window of n words around i. For each word i of the source and the target languages, we obtain a context vector vi which gathers the set of co-occurrence words j associated with the number of times that j and i occur together cooc(i, j). In order to identify specific words in the lexical context and to reduce wordfrequency effects, we normalize context vectors"
P14-1121,C96-2098,0,0.212954,"n the compilation of bilingual lexicons, parallel corpora are scarce resources, especially for technical domains and for language pairs not involving English. For these reasons, research in bilingual lexicon extraction has focused on another kind of bilingual corpora comprised of texts sharing common features such as domain, genre, sampling period, etc. without having a source text/target text relationship (McEnery and Xiao, 2007). These corpora, well known now as comparable corpora, have also initially been introduced as non-parallel corpora (Fung, 1995; Rapp, 1995), and non-aligned corpora (Tanaka and Iwasaki, 1996). According to Fung and CheIn this paper we want to show that the assumption that comparable corpora should be balanced for bilingual lexicon extraction task is unfounded. Moreover, this assumption is prejudicial for specialized comparable corpora, especially when involving the English language for which many documents are available due the prevailing position of this language as a standard for international scientific publications. Within this context, our main contribution consists in a re-reading of the standard approach putting emphasis on the unfounded assumption of the balance of the spe"
P14-1121,W11-1205,1,0.889297,"vant words (indomain terms), and thus propose a method for filtering the noise of the context vectors. In another way, Rubino and Linar`es (2011) improve the context words based on the hypothesis that a word and its candidate translations share thematic similarities. Yu and Tsujii (2009) and Otero (2007) propose, for their part, to replace the window-based method by a syntax-based method in order to improve the representation of the lexical context. To improve the transfer context vectors step, and increase the number of elements of translated context vectors, Chiao and Zweigenbaum (2003) and Morin and Prochasson (2011) combine a standard general language dictionary with a specialized dictionary, whereas D´ejean et al. (2002) use the hierarchical properties of a specialized thesaurus. Koehn and Knight (2002) automatically induce the initial seed bilingual dictionary by using identical spelling features such as cognates and similar contexts. As regards the problem of words ambiguities, Bouamor et al. (2013) carried out word sense disambiguation process only in the target language whereas Gaussier et al. (2004) solve the problem through the source and target languages by using approaches based on CCA (Canonica"
P14-1121,N09-2031,0,0.0141912,"pproach. Prochasson et al. (2009) enhance the representativeness of the context vector by strengthening the context words that happen to be transliterated words and scientific compound words in the target language. Ismail and Manandhar (2010) also suggest that context vectors should be based on the most important contextually relevant words (indomain terms), and thus propose a method for filtering the noise of the context vectors. In another way, Rubino and Linar`es (2011) improve the context words based on the hypothesis that a word and its candidate translations share thematic similarities. Yu and Tsujii (2009) and Otero (2007) propose, for their part, to replace the window-based method by a syntax-based method in order to improve the representation of the lexical context. To improve the transfer context vectors step, and increase the number of elements of translated context vectors, Chiao and Zweigenbaum (2003) and Morin and Prochasson (2011) combine a standard general language dictionary with a specialized dictionary, whereas D´ejean et al. (2002) use the hierarchical properties of a specialized thesaurus. Koehn and Knight (2002) automatically induce the initial seed bilingual dictionary by using"
P14-1121,P07-1084,1,0.964614,"ted (e.g. a seven-word window approximates syntactic dependencies). In order to emphasize significant words in the context vector and to reduce word-frequency effects, the context vectors are normalized according to an association measure. Then, the translation is obtained by comparing the source context vector to each translation candidate vector after having translated each element of the source vector with a general dictionary. The implementation of the standard approach can be carried out by applying the following three steps (Rapp, 1999; Chiao and Zweigenbaum, 2002; D´ejean et al., 2002; Morin et al., 2007; Laroche and Langlais, 2010, among others): Computing context vectors We collect all the words in the context of each word i and count their occurrence frequency in a window of n words around i. For each word i of the source and the target languages, we obtain a context vector vi which gathers the set of co-occurrence words j associated with the number of times that j and i occur together cooc(i, j). In order to identify specific words in the lexical context and to reduce wordfrequency effects, we normalize context vectors using an association score such as Mutual Information, Log-likelihood,"
P14-1121,2007.mtsummit-papers.26,0,0.926132,"ciation and similarity measures. The most complete study about the influence of these parameters on the quality of word alignment has been carried out by Laroche and Langlais (2010). The standard approach is used by most researchers so far (Rapp, 1995; Fung, 1998; Peters and Picchi, 1998; Rapp, 1999; Chiao and Zweigenbaum, 2002; D´ejean et al., 2002; Gaussier et al., 2004; Morin et al., 2007; Laroche and Langlais, 2010; Prochasson and Fung, 2011; 1285 References Tanaka and Iwasaki (1996) Fung and McKeown (1997) Rapp (1999) Chiao and Zweigenbaum (2002) D´ejean et al. (2002) Morin et al. (2007) Otero (2007) Ismail and Manandhar (2010) Bouamor et al. (2013) - Domain Newspaper Newspaper Newspaper Medical Medical Medical European Parliament European Parliament Financial Medical Languages EN/JP EN/JP GE/EN FR/EN GE/EN FR/JP SP/EN EN/SP FR/EN FR/EN Source/Target Sizes 30/33 million words 49/60 million bytes of data 135/163 million words 602,484/608,320 words 100,000/100,000 words 693,666/807,287 words 14/17 million words 500,000/500,000 sentences 402,486/756,840 words 396,524/524,805 words Table 2: Characteristics of the comparable corpora used for bilingual lexicon extraction Bouamor et al., 2013, a"
P99-1050,E93-1011,0,0.0126581,"m links acquired through an information extraction procedure are projected on multi-word terms through the recognition of semantic variations. The quality of the projected links resulting from corpus-based acquisition is compared with projected links extracted from a technical thesaurus. 1 Motivation In the domain of corpus-based terminology, there are two m a i n topics of research: term acquisition--the discovery of candidate terms-and automatic thesaurus construction--the addition of semantic links to a term bank. Several studies have focused on automatic acquisition of terms from corpora (Bourigault, 1993; Justeson and Katz, 1995; Daille, 1996). The output of these tools is a list of unstructured multi-word terms. On the other hand, contributions to automatic construction of thesauri provide classes or links between single words. Classes are produced by clustering techniques based on similar word contexts (Schiitze, 1993) or similar distributional contexts (Grefenstette, 1994). Links result from automatic acquisition of relevant predicative or discursive patterns (Hearst, 1992; Basili et al., 1993; Riloff, 1993). Predicative patterns yield predicative relations such as cause or effect whereas"
P99-1050,P98-1082,0,0.0410576,"d composgs chimiques de la graine (chemical compounds of the seed). It relies on the morphological relation between the nouns composg (compound, .h4(N1,N)) and composition (composition, N1) and on the semantic relation (part/whole relation) between graine (seed, S(N2)) and fruit (fruit, N2). In addition to the morphological and semantic relations, the categories of the words in the semantic variant composdsN chimiquesA deprep laArt graineN satisfy the regular expression: the categories that are realized are underlined. Related Work Semantic normalization is presented as semantic variation in (Hamon et al., 1998) and consists in finding relations between multi-word terms based on semantic relations between single-word terms. Our approach differs from this preceding work in that we exploit domain specific corpusbased links instead of general purpose dictionary synonymy relationships. Another original contribution of our approach is that we exploit simultaneously morphological, syntactic, and semantic links in the detection of semantic variation in a single and cohesive framework. We thus cover a larger spectrum of linguistic phenomena: morpho-semantic variations such as contenu en isotope (isotopic con"
P99-1050,C92-2082,0,0.136937,"ion of semantic links to a term bank. Several studies have focused on automatic acquisition of terms from corpora (Bourigault, 1993; Justeson and Katz, 1995; Daille, 1996). The output of these tools is a list of unstructured multi-word terms. On the other hand, contributions to automatic construction of thesauri provide classes or links between single words. Classes are produced by clustering techniques based on similar word contexts (Schiitze, 1993) or similar distributional contexts (Grefenstette, 1994). Links result from automatic acquisition of relevant predicative or discursive patterns (Hearst, 1992; Basili et al., 1993; Riloff, 1993). Predicative patterns yield predicative relations such as cause or effect whereas discursive patterns yield non-predicative relations such as generic/specific or synonymy links. * The experiments presented in this paper were performed on [AGRO], a 1.3-million word French corpus of scientific abstracts in the agricultural domain. The termer used for multi-word term acquisition is A C A B I T (Daille, 1996). It has produced 15,875 multi-word terms composed of 4,194 single words. For expository purposes, some examples are taken from [MEDIC], a 1.56million word"
P99-1050,C98-1079,0,\N,Missing
P99-1050,P99-1044,0,\N,Missing
W09-3110,E06-2001,0,0.0327464,"rpora compilation, because translated resources are rare and there is a lack of resources when the languages involved do not include English. Furthermore, the amount of multilingual documents available on the Web ensures the possibility of automatically compiling them. Nevertheless, this task can not be summarized to a simple collection of documents sharing vocabulary. It is necessary to respect the common characteristics of texts in corpora, established before the compilation, according to the corpus finality (McEnery and Xiao, 2007). Many works are about compilation of corpora from the Web (Baroni and Kilgarriff, 2006) but none, in our knowledge, focuses on compilation of comparable corpora, which has to satisfy many constraints. We fix three comparability levels: domain, topic and type of discourse. Our goal is to automate recognition of these comparability levels in documents, in order to include them into a corpus. We work on Web documents on specialized scientific domains in French and Japanese languages. As document topics can be filtered with keywords in the Web search (Chakrabarti et al., 1999), we focus in this paper on automatic recognition of types of discourse that can be found in scientific docu"
W09-3110,C02-2020,0,0.0552411,"Missing"
W09-3110,C00-1032,1,0.710143,"(Quinlan, 1993), since both of them seem to be the most appropriate to our data (small corpora, binary classification, less than 100 features). Lexical Features Some of our lexical criteria are specific to the scientific documents, like bibliographies and bibliographic quotations, specialized vocabulary or the measurement units. To measure the terminological density (proportion of specialized vocabulary in the text) in French, we evaluate terms with stems of Greek-Latin (Namer and Baud, 2007) and suffix characters of relational adjectives that are particularly frequent in scientific domains (Daille, 2000). We listed about 50 stems such as inter-, auto- or nano-, and the 10 relational suffixes such such as -ique or -al. For Japanese, we listed prefix characteristics of names of disease or symptoms (先天性 (congenital), 遺伝性(hereditary), etc.). These stems can be found in both type of discourse, but not in the same proportions. Specialized terms are used in both type of discourse in different ways. For example, the term “ovarectomie” (ovarectomy) can be frequent in a scientific document and used once in a popular science documents to explain it and then replaced by “ablation des ovaires” (ovary abla"
W09-3110,C02-1166,0,0.0672515,"s of works, which induces different choices: • General language works, where texts of corpora usually share a domain and a period. Fung and Yee (1998) used a corpus composed of newspaper in English and Chinese on a specific period to extract words translations, using IR and NLP methods. Rapp (1999) used a English / German corpus, composed of documents coming from newspapers as well as scientific papers to study alignment methods and bilingual lexicon extraction from non-parallel corpora (which can be considered as comparable); • Specialized language works, where choice of criteria is various. Déjean et al. (2002) used a corpus composed of scientific abstracts from 56 Feature URL pattern Document’s format Meta tags Title tag Pages layout Pages background Images Links Paragraphs Item lists Number of sentences Typography Document’s length sions (structural, modal and lexical), whose combination characterizes scientific or popular science discourse. A specialized comparable corpus can be compiled from a single type of discourse document collection through several steps. Last part of this paper focuses on the automation of these steps using the IBM Unstructured Information Management Architecture (UIMA). 3"
W09-3110,P99-1067,0,0.0335771,"). The choice of the common characteristics, which define the content of corpora, affects the degree of comparability, notion used to quantify how two corpora can be comparable. The choice of these characteristics depends on the finality of the corpus. Among papers on comparable corpora, we distinguish two types of works, which induces different choices: • General language works, where texts of corpora usually share a domain and a period. Fung and Yee (1998) used a corpus composed of newspaper in English and Chinese on a specific period to extract words translations, using IR and NLP methods. Rapp (1999) used a English / German corpus, composed of documents coming from newspapers as well as scientific papers to study alignment methods and bilingual lexicon extraction from non-parallel corpora (which can be considered as comparable); • Specialized language works, where choice of criteria is various. Déjean et al. (2002) used a corpus composed of scientific abstracts from 56 Feature URL pattern Document’s format Meta tags Title tag Pages layout Pages background Images Links Paragraphs Item lists Number of sentences Typography Document’s length sions (structural, modal and lexical), whose combin"
W09-3110,P98-1069,0,0.0233603,"20). Comparability is ensured using characteristics which can refer to the text creation context (period, author...), or to the text itself (topic, genre...). The choice of the common characteristics, which define the content of corpora, affects the degree of comparability, notion used to quantify how two corpora can be comparable. The choice of these characteristics depends on the finality of the corpus. Among papers on comparable corpora, we distinguish two types of works, which induces different choices: • General language works, where texts of corpora usually share a domain and a period. Fung and Yee (1998) used a corpus composed of newspaper in English and Chinese on a specific period to extract words translations, using IR and NLP methods. Rapp (1999) used a English / German corpus, composed of documents coming from newspapers as well as scientific papers to study alignment methods and bilingual lexicon extraction from non-parallel corpora (which can be considered as comparable); • Specialized language works, where choice of criteria is various. Déjean et al. (2002) used a corpus composed of scientific abstracts from 56 Feature URL pattern Document’s format Meta tags Title tag Pages layout Pag"
W09-3110,nazar-etal-2008-suite,0,0.0301015,"used in a durable way must include this step. Documentation of the corpus includes information about the compilation (creator, date, method, resources, etc.) and information about the corpus documents. Text Encoding Initiative (TEI) standard has been created in order to conserve in an uniformed way this kind of information in a corpus 6 . A corpus quality highly depends on the first two steps. Moreover, these steps are directly linked to the creator use of the corpus. The first step must be realized by the user to create an relevant corpus. Although second step can be computerizable (Rogelio Nazar and Cabré, 2008), we choose to keep it manual in order to guarantee corpus quality. We decided to work on a system which realizes the last steps, i.e. normalization, annotation and documentation, starting from a collection of documents selected by a user. Our tool has been developed on Unstructured Information Management Architecture (UIMA) that has been created by IBM Research Division (Ferrucci and Lally, 2004). Unstructured data (texts, images, etc.) collections can be easily treated on this platform and many libraries are available. Our tool starts with a web documents or texts collection and is composed"
W09-3110,C98-1066,0,\N,Missing
W11-1205,E09-1003,0,0.176564,"s. This aspect can be a potential problem if too few corpus words are found in the bilingual dictionary (Chiao and Zweigenbaum, 2003; D´ejean et al., 2002). In this article, we want to show how this problem can be partially circumvented by combining a general bilingual dictionary with a specialized bilingual dictionary based on a parallel corpus extracted through mining of the comparable corpus. In the same way that recent works in Statistical Machine Translation (SMT) mines comparable corpora to discover parallel sentences (Resnik and Smith, 2003; Yang and Li, 2003; Munteanu and Marcu, 2005; Abdul-Rauf and Schwenk, 2009, among others), this work contributes to the bridging of the gap between comparable and parallel corpora by offering a framework for bilingual lexicon extraction from comparable corpus with the help of parallel corpusbased pairs of terms. The remainder of this article is organized as follows. In Section 2, we first present the method for bilingual lexicon extraction from comparable corpora enhanced with parallel corpora and the associated system architecture. We then quantify and analyse in Section 3 the performance improvement of our method on a medical comparable corpora when used to extrac"
W11-1205,C02-1011,0,0.0530489,"Missing"
W11-1205,W02-1402,0,0.0858549,"Missing"
W11-1205,C02-2020,0,0.784067,"Missing"
W11-1205,I05-1062,1,0.301475,"llel corpora. In this section, we then quantify and analyse the performance improvement of our method on a medical comparable corpus when used to extract specialized bilingual lexicon. 31 # SWs dict. 20,317 50,330 18,972 549 # SWs corpus 3,099 4,567 833 549 # TPE 1.8 2.8 1.6 1.0 In bilingual terminology extraction from specialized comparable corpora, the terminology reference list required to evaluate the performance of the alignment programs are often composed of 100 single-word terms (SWTs) (180 SWTs in (D´ejean and Gaussier, 2002), 95 SWTs in (Chiao and Zweigenbaum, 2002), and 100 SWTs in (Daille and Morin, 2005)). To build our reference list, we selected 400 French/English SWTs from the UMLS9 6 http://www.wiktionary.org/ http://www.elra.info/ 8 http://www.ncbi.nlm.nih.gov/mesh 9 http://www.nlm.nih.gov/research/umls 7 meta-thesaurus and the Grand dictionnaire terminologique10 . We kept only the French/English pair of SWTs which occur more than five times in each part of the comparable corpus. As a result of filtering, 122 French/English SWTs were extracted. 3.2 Experimental Results In order to evaluate the influence of the parallel corpus-based bilingual lexicon induced from the comparable corpus on t"
W11-1205,C94-1084,0,0.262933,"Missing"
W11-1205,C02-1166,0,0.369515,"Missing"
W11-1205,W04-3208,0,0.0114376,"characteristics of the documents harvested. These parallel sentences are then used to build a bilingual lexicon through a tool dedicated to bilingual lexicon extraction. Finally, this bilingual lexicon is used to perform the comparable corpus-based alignment. For a word to be translated, the output of the system is a ranked list of candidate translations. 2.1 Extracting Parallel Sentences from Comparable Corpora Parallel sentence extraction from comparable corpora has been studied by a number of researchers (Ma and Liberman, 1999; Chen and Nie, 2000; Resnik and Smith, 2003; Yang and Li, 2003; Fung and Cheung, 2004; Munteanu and Marcu, 2005; Abdul-Rauf and Schwenk, 2009, among others) and several systems have been developed such as BITS (Bilingual Internet Test Search) (Ma and Liberman, 1999), PTMiner (Parallel Text Miner) (Chen and Nie, 2000), and STRAND (Structural Translation Recognition for Acquiring Natural Data) (Resnik and Smith, 2003). Their work relies on the observation that a collection of texts in different languages composed independently and based on sharing common features such as content, domain, genre, register, sampling period, etc. contains probably some sentences with a source text-t"
W11-1205,C10-1070,0,0.299693,"Missing"
W11-1205,1999.mtsummit-1.79,0,0.0325907,"from the web, we first propose to extract parallel sentences based on the structural characteristics of the documents harvested. These parallel sentences are then used to build a bilingual lexicon through a tool dedicated to bilingual lexicon extraction. Finally, this bilingual lexicon is used to perform the comparable corpus-based alignment. For a word to be translated, the output of the system is a ranked list of candidate translations. 2.1 Extracting Parallel Sentences from Comparable Corpora Parallel sentence extraction from comparable corpora has been studied by a number of researchers (Ma and Liberman, 1999; Chen and Nie, 2000; Resnik and Smith, 2003; Yang and Li, 2003; Fung and Cheung, 2004; Munteanu and Marcu, 2005; Abdul-Rauf and Schwenk, 2009, among others) and several systems have been developed such as BITS (Bilingual Internet Test Search) (Ma and Liberman, 1999), PTMiner (Parallel Text Miner) (Chen and Nie, 2000), and STRAND (Structural Translation Recognition for Acquiring Natural Data) (Resnik and Smith, 2003). Their work relies on the observation that a collection of texts in different languages composed independently and based on sharing common features such as content, domain, genre,"
W11-1205,P07-1084,1,0.213835,"Missing"
W11-1205,J05-4003,0,0.127573,"onary vis-`a-vis the corpus. This aspect can be a potential problem if too few corpus words are found in the bilingual dictionary (Chiao and Zweigenbaum, 2003; D´ejean et al., 2002). In this article, we want to show how this problem can be partially circumvented by combining a general bilingual dictionary with a specialized bilingual dictionary based on a parallel corpus extracted through mining of the comparable corpus. In the same way that recent works in Statistical Machine Translation (SMT) mines comparable corpora to discover parallel sentences (Resnik and Smith, 2003; Yang and Li, 2003; Munteanu and Marcu, 2005; Abdul-Rauf and Schwenk, 2009, among others), this work contributes to the bridging of the gap between comparable and parallel corpora by offering a framework for bilingual lexicon extraction from comparable corpus with the help of parallel corpusbased pairs of terms. The remainder of this article is organized as follows. In Section 2, we first present the method for bilingual lexicon extraction from comparable corpora enhanced with parallel corpora and the associated system architecture. We then quantify and analyse in Section 3 the performance improvement of our method on a medical comparab"
W11-1205,P99-1067,0,0.752389,"ich occurs within the window of the word to be translated (for instance a seven-word window approximates syntactic dependencies). In order to emphasize significant words in the context vector and to reduce word-frequency effects, the context vectors are normalized according to association measures. Then, the translation is obtained by comparing the source context vector to each translation candidate vector after having translated each element of the source vector with a general dictionary. The implementation of this approach can be carried out by applying the four following steps (Fung, 1998; Rapp, 1999): 1. We collect all the lexical units in the context of each lexical unit i and count their occurrence frequency in a window of n words around i. For each lexical unit i of the source and the target languages, we obtain a context vector vi which gathers the set of co-occurrence units j associated with the number of times that j and i occur together occ(i, j). In order to identify specific words in the lexical context and to reduce word-frequency effects, we normalize context vectors using an association score such as Mutual Information (MI) or Log-likelihood, as shown in equations 1 and 2 and"
W11-1205,J03-3002,0,0.357509,"ends on the coverage of the bilingual dictionary vis-`a-vis the corpus. This aspect can be a potential problem if too few corpus words are found in the bilingual dictionary (Chiao and Zweigenbaum, 2003; D´ejean et al., 2002). In this article, we want to show how this problem can be partially circumvented by combining a general bilingual dictionary with a specialized bilingual dictionary based on a parallel corpus extracted through mining of the comparable corpus. In the same way that recent works in Statistical Machine Translation (SMT) mines comparable corpora to discover parallel sentences (Resnik and Smith, 2003; Yang and Li, 2003; Munteanu and Marcu, 2005; Abdul-Rauf and Schwenk, 2009, among others), this work contributes to the bridging of the gap between comparable and parallel corpora by offering a framework for bilingual lexicon extraction from comparable corpus with the help of parallel corpusbased pairs of terms. The remainder of this article is organized as follows. In Section 2, we first present the method for bilingual lexicon extraction from comparable corpora enhanced with parallel corpora and the associated system architecture. We then quantify and analyse in Section 3 the performance im"
W11-1206,C02-1011,0,0.057467,"Missing"
W11-1206,C02-2020,0,0.272078,"Missing"
W11-1206,I05-1062,1,0.959184,"r affinities show which words share the same environments. Words sharing second-order affinities need never appear together themselves, but their environments are similar (Grefenstette, 1994a, p. 280). Generally speaking, a bilingual dictionary is a bridge between two languages established by its entries. The extended approach is based on this observation and avoids explicit translation of vectors as shown in Figure 1. The implementation of this extended approach can be carried out in four steps where the first and last steps are identical to the standard approach (D´ejean and Gaussier, 2002; Daille and Morin, 2005): Reformulation in the target language For a lexical unit i to be translated, we identify the k-nearest lexical units (k nlu), among the dictionary entries corresponding to words in the source language, according to sim(i, s). Each nlu is translated via the bilingual dictionary, and the vector in 37 the target language, s, corresponding to the translation is selected. If the bilingual dictionary provides several translations for a given unit, s is given by the union of the vectors corresponding to the translations. It is worth noting that the context vectors are not translated directly, thus r"
W11-1206,C02-1166,0,0.231507,"Missing"
W11-1206,J93-1003,0,0.467787,"l dependencies). The implementation of this approach can be carried out by applying the following four steps (Rapp, 1995; Fung and McKeown, 1997): Context characterization All the lexical units in the context of each lexical unit i are collected, and their frequency in a window of n words around i extracted. For each lexical unit i of the source and the target languages, we obtain a context vector i where each entry, ij , of the vector is given by a function of the co-occurrences of units j and i. Usually, association measures such as the mutual information (Fano, 1961) or the log-likelihood (Dunning, 1993) are used to define vector entries. Vector transfer The lexical units of the context vector i are translated using a bilingual dictionary. Whenever the bilingual dictionary provides several translations for a lexical unit, all the entries are considered but weighted according to their frequency in the target language. Lexical units with no entry in the dictionary are discarded. Target language vector matching A similarity measure, sim(i, t), is used to score each lexical unit, t, in the target language with respect to the translated context vector, i. Usual measures of vector similarity includ"
W11-1206,P98-1069,0,0.804521,"ask. We then reinterpret the extended method, and motivate a novel model to reformulate this approach inspired by the metasearch engines in information retrieval. The empirical results show that performances of our model are always better than the baseline obtained with the extended approach and also competitive with the standard approach. 1 ˜ Saldarriaga Sebastian Pena 1100 rue Notre-Dame Ouest, Montr´eal, Qu´ebec, Canada H3C 1K3 spena@synchromedia.ca Introduction Bilingual lexicon extraction from comparable corpora has received considerable attention since the 1990s (Rapp, 1995; Fung, 1998; Fung and Lo, 1998; Peters and Picchi, 1998; Rapp, 1999; Chiao and Zweigenbaum, 2002a; D´ejean et al., 2002; Gaussier et al., 2004; Morin et al., 2007; Laroche and Langlais, 2010, among others). This attention has been motivated by the scarcity of parallel corpora, especially for countries with only one official language and for language pairs not involving English. Furthermore, as a parallel corpus is comprised of a pair of texts (a source text and a translated text), the vocabulary appearing in the translated text is highly influenced by the source text, especially in technical domains. Consequently, comparab"
W11-1206,W97-0119,0,0.123041,"of this observation consists in the identification of first-order affinities for each source and target language: First-order affinities de36 scribe what other words are likely to be found in the immediate vicinity of a given word (Grefenstette, 1994a, p. 279). These affinities can be represented by context vectors, and each vector element represents a word which occurs within the window of the word to be translated (for instance a seven-word window approximates syntactical dependencies). The implementation of this approach can be carried out by applying the following four steps (Rapp, 1995; Fung and McKeown, 1997): Context characterization All the lexical units in the context of each lexical unit i are collected, and their frequency in a window of n words around i extracted. For each lexical unit i of the source and the target languages, we obtain a context vector i where each entry, ij , of the vector is given by a function of the co-occurrences of units j and i. Usually, association measures such as the mutual information (Fano, 1961) or the log-likelihood (Dunning, 1993) are used to define vector entries. Vector transfer The lexical units of the context vector i are translated using a bilingual dict"
W11-1206,P04-1067,0,0.216071,"Missing"
W11-1206,C10-1070,0,0.183241,"Zweigenbaum, 2002a), and 100 SWTs in (Daille and Morin, 2005)). To build our reference list, we selected 400 French/English SWTs from the UMLS2 meta-thesaurus and the Grand dictionnaire terminologique3 . We kept only the French/English pair of SWTs which occur more than five times in each part of the comparable corpus. As a result of filtering, 122 French/English SWTs were extracted. 4.2 Three major parameters need to be set to the extended approach, namely the similarity measure, the association measure defining the entry vectors and the size of the window used to build the context vectors. Laroche and Langlais (2010) carried out a complete study about the influence of these parameters on the quality of bilingual alignment. As similarity measure, we chose to use the weighted jaccard index: P min (it , jt ) sim(i, j) = P t (5) t max (it , jt ) The entries of the context vectors were determined by the log-likelihood (Dunning, 1993), and we used a seven-word window since it approximates syntactic dependencies. Other combinations of parameters were assessed but the previous parameters turned out to give the best performance. 2 3 www.elsevier.com 39 Experimental Setup http://www.nlm.nih.gov/research/umls http:/"
W11-1206,P07-1084,1,0.90184,"Missing"
W11-1206,P95-1050,0,0.766389,"ally dedicated to this task. We then reinterpret the extended method, and motivate a novel model to reformulate this approach inspired by the metasearch engines in information retrieval. The empirical results show that performances of our model are always better than the baseline obtained with the extended approach and also competitive with the standard approach. 1 ˜ Saldarriaga Sebastian Pena 1100 rue Notre-Dame Ouest, Montr´eal, Qu´ebec, Canada H3C 1K3 spena@synchromedia.ca Introduction Bilingual lexicon extraction from comparable corpora has received considerable attention since the 1990s (Rapp, 1995; Fung, 1998; Fung and Lo, 1998; Peters and Picchi, 1998; Rapp, 1999; Chiao and Zweigenbaum, 2002a; D´ejean et al., 2002; Gaussier et al., 2004; Morin et al., 2007; Laroche and Langlais, 2010, among others). This attention has been motivated by the scarcity of parallel corpora, especially for countries with only one official language and for language pairs not involving English. Furthermore, as a parallel corpus is comprised of a pair of texts (a source text and a translated text), the vocabulary appearing in the translated text is highly influenced by the source text, especially in technical"
W11-1206,P99-1067,0,0.634338,"Missing"
W11-1206,C98-1066,0,\N,Missing
W13-2504,P91-1034,0,0.5039,"ss-based models provide an alternative to the independence assumption on the cooccurrence of given words w1 and w2 : the more frequent w2 is, the higher estimate of P (w2 |w1 ) will be, regardless of w1 . Introduction Cooccurrences play an important role in many corpus based approaches in the field of naturallanguage processing (Dagan et al., 1993). They represent the observable evidence that can be distilled from a corpus and are employed for a variety of applications such as machine translation (Brown et al., 1992), information retrieval (Maarek and Smadja, 1989), word sense disambiguation (Brown et al., 1991), etc. In bilingual lexicon extraction from comparable corpora, frequency counts for word pairs often serve as a basis for distributional methods, such as the standard approach (Fung, 1998) which compares the cooccurrence profile of a given source word, a 24 Proceedings of the 6th Workshop on Building and Using Comparable Corpora, pages 24–33, c Sofia, Bulgaria, August 8, 2013. 2013 Association for Computational Linguistics method are then described in Section 3. Section 4 describes the experimental setup and our resources. Section 5 presents the experiments and comments on several results. We"
W13-2504,J92-4003,0,0.42182,"Missing"
W13-2504,C02-2020,0,0.165679,"steps 2, 3 and 4 of the standard approach are unchanged). We chose not to study the prediction of unseen cooccurrences. The latter has been carried out successfully by (Pekar et al., 2006). We concentrate on the evaluation of smoothing techniques of known cooccurrences and their effect according to different association and similarity measures. The main idea for identifying translations of terms in comparable corpora relies on the distributional hypothesis 1 that has been extended to the bilingual scenario (Fung, 1998; Rapp, 1999). If many variants of the standard approach have been proposed (Chiao and Zweigenbaum, 2002; Herv´e D´ejean and Gaussier, 2002; Morin et al., 2007; Gamallo, 2008)[among others], they mainly differ in the way they implement each step and define its parameters. The standard approach can be carried out as follows: Step 1 For a source word to translate wis , we first build its context vector vwis . The vector vwis contains all the words that cooccur with wis within windows of n words. Lets denote by cooc(wis , wjs ) the cooccurrence value of wis and a given word of its context wjs . The process of building context vectors is repeated for all the words of the target language. Step 2 An a"
W13-2504,P93-1022,0,0.146892,"Goodman, 1999). Class-based models (Pereira et al., 1993) use classes of similar words to distinguish between unseen cooccurrences. The relationship between given words is modeled by analogy with other words that are in some sense similar to the given ones. Hence, class-based models provide an alternative to the independence assumption on the cooccurrence of given words w1 and w2 : the more frequent w2 is, the higher estimate of P (w2 |w1 ) will be, regardless of w1 . Introduction Cooccurrences play an important role in many corpus based approaches in the field of naturallanguage processing (Dagan et al., 1993). They represent the observable evidence that can be distilled from a corpus and are employed for a variety of applications such as machine translation (Brown et al., 1992), information retrieval (Maarek and Smadja, 1989), word sense disambiguation (Brown et al., 1991), etc. In bilingual lexicon extraction from comparable corpora, frequency counts for word pairs often serve as a basis for distributional methods, such as the standard approach (Fung, 1998) which compares the cooccurrence profile of a given source word, a 24 Proceedings of the 6th Workshop on Building and Using Comparable Corpora"
W13-2504,I05-1062,1,0.809718,"the ith term (always 1 in our case), and P (Rik ) is 0 if the reference translation is not found for the ith term or 1/r if it is (r is the rank of the reference translation in the translation candidates). Table 2: Dictionary coverage 4.3 Evaluation Measure Reference Lists In bilingual terminology extraction from specialized comparable corpora, the terminology reference list required to evaluate the performance of the alignment programs is often composed of 100 single-word terms (SWTs) (180 SWTs in (Herv´e D´ejean and Gaussier, 2002), 95 SWTs in (Chiao and Zweigenbaum, 2002), and 100 SWTs in (Daille and Morin, 2005)). To build our reference lists, we selected only the French/English pair of SWTs which occur more than five times in each part of the comparable corpus. As a result 4.5 Baseline The baseline in our experiments is the standard approach (Fung, 1998) without any smoothing of the data. The standard approach is often used for comparison (Pekar et al., 2006; Gamallo, 2008; Prochasson and Morin, 2009), etc. 4.6 Training Data Set Some smoothing techniques such as the GoodTuring estimators need a large training corpus to 3 ELRA dictionary has been created by Sciper in the Technolangue/Euradic project"
W13-2504,J93-1003,0,0.563583,"inly differ in the way they implement each step and define its parameters. The standard approach can be carried out as follows: Step 1 For a source word to translate wis , we first build its context vector vwis . The vector vwis contains all the words that cooccur with wis within windows of n words. Lets denote by cooc(wis , wjs ) the cooccurrence value of wis and a given word of its context wjs . The process of building context vectors is repeated for all the words of the target language. Step 2 An association measure such as the pointwise mutual information (Fano, 1961), the log-likelihood (Dunning, 1993) or the discounted odds-ratio (Laroche and Langlais, 2010) is used to score the strength of correlation between a word and all the words of its context vector. 4 Experimental Setup In order to evaluate the smoothing techniques, several resources and parameters are needed. We present hereafter the experiment data and the parameters of the standard approach. 4.1 Step 3 The context vector vwis is projected into t . Each word w s of v s the target language vw s wi j i is translated with the help of a bilingual dictionary D. If wjs is not present in D, wjs is discarded. Whenever the bilingual dicti"
W13-2504,C10-1070,0,0.064413,"ep and define its parameters. The standard approach can be carried out as follows: Step 1 For a source word to translate wis , we first build its context vector vwis . The vector vwis contains all the words that cooccur with wis within windows of n words. Lets denote by cooc(wis , wjs ) the cooccurrence value of wis and a given word of its context wjs . The process of building context vectors is repeated for all the words of the target language. Step 2 An association measure such as the pointwise mutual information (Fano, 1961), the log-likelihood (Dunning, 1993) or the discounted odds-ratio (Laroche and Langlais, 2010) is used to score the strength of correlation between a word and all the words of its context vector. 4 Experimental Setup In order to evaluate the smoothing techniques, several resources and parameters are needed. We present hereafter the experiment data and the parameters of the standard approach. 4.1 Step 3 The context vector vwis is projected into t . Each word w s of v s the target language vw s wi j i is translated with the help of a bilingual dictionary D. If wjs is not present in D, wjs is discarded. Whenever the bilingual dictionary provides several translations for a word, all the en"
W13-2504,P07-2008,0,0.179104,"resent a study of some widelyused smoothing algorithms for language n-gram modeling (Laplace, Good-Turing, Kneser-Ney...). Our main contribution is to investigate how the different smoothing techniques affect the performance of the standard approach (Fung, 1998) traditionally used for bilingual lexicon extraction. We show that using smoothing as a preprocessing step of the standard approach increases its performance significantly. 1 As has been known, words and other type-rich linguistic populations do not contain instances of all types in the population, even the largest samples (Zipf, 1949; Evert and Baroni, 2007). Therefore, the number and distribution of types in the available sample are not reliable estimators (Evert and Baroni, 2007), especially for small comparable corpora. The literature suggests two major approaches for solving the data sparseness problem: smoothing and class-based methods. Smoothing techniques (Good, 1953) are often used to better estimate probabilities when there is insufficient data to estimate probabilities accurately. They tend to make distributions more uniform, by adjusting low probabilities such as zero probabilities upward, and high probabilities downward. Generally, sm"
W13-2504,P04-1066,0,0.0457126,"Missing"
W13-2504,P07-1084,1,0.917396,"se not to study the prediction of unseen cooccurrences. The latter has been carried out successfully by (Pekar et al., 2006). We concentrate on the evaluation of smoothing techniques of known cooccurrences and their effect according to different association and similarity measures. The main idea for identifying translations of terms in comparable corpora relies on the distributional hypothesis 1 that has been extended to the bilingual scenario (Fung, 1998; Rapp, 1999). If many variants of the standard approach have been proposed (Chiao and Zweigenbaum, 2002; Herv´e D´ejean and Gaussier, 2002; Morin et al., 2007; Gamallo, 2008)[among others], they mainly differ in the way they implement each step and define its parameters. The standard approach can be carried out as follows: Step 1 For a source word to translate wis , we first build its context vector vwis . The vector vwis contains all the words that cooccur with wis within windows of n words. Lets denote by cooc(wis , wjs ) the cooccurrence value of wis and a given word of its context wjs . The process of building context vectors is repeated for all the words of the target language. Step 2 An association measure such as the pointwise mutual informa"
W13-2504,P93-1024,0,0.193297,"The literature suggests two major approaches for solving the data sparseness problem: smoothing and class-based methods. Smoothing techniques (Good, 1953) are often used to better estimate probabilities when there is insufficient data to estimate probabilities accurately. They tend to make distributions more uniform, by adjusting low probabilities such as zero probabilities upward, and high probabilities downward. Generally, smoothing methods not only prevent zero probabilities, but they also attempt to improve the accuracy of the model as a whole (Chen and Goodman, 1999). Class-based models (Pereira et al., 1993) use classes of similar words to distinguish between unseen cooccurrences. The relationship between given words is modeled by analogy with other words that are in some sense similar to the given ones. Hence, class-based models provide an alternative to the independence assumption on the cooccurrence of given words w1 and w2 : the more frequent w2 is, the higher estimate of P (w2 |w1 ) will be, regardless of w1 . Introduction Cooccurrences play an important role in many corpus based approaches in the field of naturallanguage processing (Dagan et al., 1993). They represent the observable evidenc"
W13-2504,2009.mtsummit-posters.14,1,0.839373,"Missing"
W13-2504,P99-1067,0,0.148079,"sed for calculating the association measure between wis and wjs and so on (steps 2, 3 and 4 of the standard approach are unchanged). We chose not to study the prediction of unseen cooccurrences. The latter has been carried out successfully by (Pekar et al., 2006). We concentrate on the evaluation of smoothing techniques of known cooccurrences and their effect according to different association and similarity measures. The main idea for identifying translations of terms in comparable corpora relies on the distributional hypothesis 1 that has been extended to the bilingual scenario (Fung, 1998; Rapp, 1999). If many variants of the standard approach have been proposed (Chiao and Zweigenbaum, 2002; Herv´e D´ejean and Gaussier, 2002; Morin et al., 2007; Gamallo, 2008)[among others], they mainly differ in the way they implement each step and define its parameters. The standard approach can be carried out as follows: Step 1 For a source word to translate wis , we first build its context vector vwis . The vector vwis contains all the words that cooccur with wis within windows of n words. Lets denote by cooc(wis , wjs ) the cooccurrence value of wis and a given word of its context wjs . The process of"
W15-3405,C02-2020,0,0.0609228,"t languages in particular. The experiments have shown that in some cases the quality of the extracted lexicon has been enhanced. 1 2 Bilingual Lexicon Extraction Initially designed for parallel corpora (Chen, 1993), and due to the scarcity of this kind of resources (Martin et al., 2005), bilingual lexicon extraction then tried to deal with comparable corpora instead (Fung, 1995; Rapp, 1995). An algorithm using comparable corpora is the standard method (Fung and McKeown, 1997) closely based on the notion of context vectors. Many implementations have been designed in order to do so (Rapp, 1999; Chiao and Zweigenbaum, 2002; Morin et al., 2010). A context vector w is, for a given word w, the representation of its contexts ct1 . . . cti and the number of occurrences found in the window of a corpus. In this approach, context vectors are calculated both in source and target languages corpora. They are also normalized according to association scores. Then, thanks to a seed dictionary, source context vectors are transferred into target language. The similarity between the translated context vector w for a given source word w to translate and all target context vectors t lead to the creation of a list of ranked candid"
W15-3405,P91-1017,0,0.544106,"in, 2013) or the use of unbalanced corpora (Morin and Hazem, 2014). Among them, and in the case of comparable corpora, we can denote that none looked into pivot-language approaches. Nevertheless, the idea of involving a pivot language for translation tasks is not recent. Bilingual lexicon extraction from parallel corpora has already been improved via the use of an intermediary language (Kwon et al., 2013; Seo et al., 2014; Kim et al., 2015), so does statistical translation (Simard, 1999; Och and Ney, 2001). Those works lay on the assumption that another language brings additional information (Dagan and Itai, 1991). 3 3.2 The second method based on pivot dictionaries consists in translating both source and target context vectors into pivot language. Thus, the operation of computing similarity occurs in the vectorial space of the pivot language. In order to do so, the context vector of a word in source language to translate is computed as it is usually done in the standard method. The second step is to transfer the source and target context vectors into the pivot language using source/pivot and target/pivot dictionaries. At this stage, we gather in the pivot language the translated source and all target"
W15-3405,W95-0114,0,0.257603,"nvestigated to which extent a third language could be interesting to bypass the original alignment. We have defined two original alignment approaches involving pivot languages and we have evaluated over four languages and two pivot languages in particular. The experiments have shown that in some cases the quality of the extracted lexicon has been enhanced. 1 2 Bilingual Lexicon Extraction Initially designed for parallel corpora (Chen, 1993), and due to the scarcity of this kind of resources (Martin et al., 2005), bilingual lexicon extraction then tried to deal with comparable corpora instead (Fung, 1995; Rapp, 1995). An algorithm using comparable corpora is the standard method (Fung and McKeown, 1997) closely based on the notion of context vectors. Many implementations have been designed in order to do so (Rapp, 1999; Chiao and Zweigenbaum, 2002; Morin et al., 2010). A context vector w is, for a given word w, the representation of its contexts ct1 . . . cti and the number of occurrences found in the window of a corpus. In this approach, context vectors are calculated both in source and target languages corpora. They are also normalized according to association scores. Then, thanks to a seed"
W15-3405,2001.mtsummit-papers.46,0,0.149772,"urus (D´ejean et al., 2002), implication of predictive methods for word co-occurrence counts (Hazem and Morin, 2013) or the use of unbalanced corpora (Morin and Hazem, 2014). Among them, and in the case of comparable corpora, we can denote that none looked into pivot-language approaches. Nevertheless, the idea of involving a pivot language for translation tasks is not recent. Bilingual lexicon extraction from parallel corpora has already been improved via the use of an intermediary language (Kwon et al., 2013; Seo et al., 2014; Kim et al., 2015), so does statistical translation (Simard, 1999; Och and Ney, 2001). Those works lay on the assumption that another language brings additional information (Dagan and Itai, 1991). 3 3.2 The second method based on pivot dictionaries consists in translating both source and target context vectors into pivot language. Thus, the operation of computing similarity occurs in the vectorial space of the pivot language. In order to do so, the context vector of a word in source language to translate is computed as it is usually done in the standard method. The second step is to transfer the source and target context vectors into the pivot language using source/pivot and t"
W15-3405,hazem-morin-2012-adaptive,1,0.851129,"in this field aims at improving the Introduction The main goal of this work is to investigate to which extent bilingual lexicon extraction using comparable corpora can be improved using a third language when dealing with poor resource language pairs. Indeed, the quality of the result of the extracted bilingual lexicon strongly depends on the quality of the resources, that is to say the corpora and a general language bilingual dictionary. In this study, we stress the key role of the potential high quality resources of the pivot language (Chiao and Zweigenbaum, 2004; Morin and Prochasson, 2011; Hazem and Morin, 2012). The idea of involving a third language is to benefit from the lexical information conveyed by the additional language. We also assume that in the case of not so usual language pairs the two comparable corpora are of medium quality, and the bilingual dictionary seems weak, due to the nonexistence of such a dictionary. We expect as a consequence a bad quality of the extracted lexicon. Nevertheless, we are highly confident that a language for which 32 Proceedings of the Eighth Workshop on Building and Using Comparable Corpora, pages 32–37, c Beijing, China, July 30, 2015. 2015 Association for C"
W15-3405,P95-1050,0,0.220061,"to which extent a third language could be interesting to bypass the original alignment. We have defined two original alignment approaches involving pivot languages and we have evaluated over four languages and two pivot languages in particular. The experiments have shown that in some cases the quality of the extracted lexicon has been enhanced. 1 2 Bilingual Lexicon Extraction Initially designed for parallel corpora (Chen, 1993), and due to the scarcity of this kind of resources (Martin et al., 2005), bilingual lexicon extraction then tried to deal with comparable corpora instead (Fung, 1995; Rapp, 1995). An algorithm using comparable corpora is the standard method (Fung and McKeown, 1997) closely based on the notion of context vectors. Many implementations have been designed in order to do so (Rapp, 1999; Chiao and Zweigenbaum, 2002; Morin et al., 2010). A context vector w is, for a given word w, the representation of its contexts ct1 . . . cti and the number of occurrences found in the window of a corpus. In this approach, context vectors are calculated both in source and target languages corpora. They are also normalized according to association scores. Then, thanks to a seed dictionary, s"
W15-3405,I13-1196,1,0.828741,"t vectors translated into target language. We can say that we transferred the context vectors via a pivot language. Finally, the last step of similarity computation stays unchanged: for one source word w for which we want to find the translation in target language, we compute the similarity between its context vector transferred successively w and all target context vectors t. This method is presented in Figure 1. quality of the extracted lexicon. For instance, we can cite the use of a bilingual thesaurus (D´ejean et al., 2002), implication of predictive methods for word co-occurrence counts (Hazem and Morin, 2013) or the use of unbalanced corpora (Morin and Hazem, 2014). Among them, and in the case of comparable corpora, we can denote that none looked into pivot-language approaches. Nevertheless, the idea of involving a pivot language for translation tasks is not recent. Bilingual lexicon extraction from parallel corpora has already been improved via the use of an intermediary language (Kwon et al., 2013; Seo et al., 2014; Kim et al., 2015), so does statistical translation (Simard, 1999; Och and Ney, 2001). Those works lay on the assumption that another language brings additional information (Dagan and"
W15-3405,P99-1067,0,0.236893,"and two pivot languages in particular. The experiments have shown that in some cases the quality of the extracted lexicon has been enhanced. 1 2 Bilingual Lexicon Extraction Initially designed for parallel corpora (Chen, 1993), and due to the scarcity of this kind of resources (Martin et al., 2005), bilingual lexicon extraction then tried to deal with comparable corpora instead (Fung, 1995; Rapp, 1995). An algorithm using comparable corpora is the standard method (Fung and McKeown, 1997) closely based on the notion of context vectors. Many implementations have been designed in order to do so (Rapp, 1999; Chiao and Zweigenbaum, 2002; Morin et al., 2010). A context vector w is, for a given word w, the representation of its contexts ct1 . . . cti and the number of occurrences found in the window of a corpus. In this approach, context vectors are calculated both in source and target languages corpora. They are also normalized according to association scores. Then, thanks to a seed dictionary, source context vectors are transferred into target language. The similarity between the translated context vector w for a given source word w to translate and all target context vectors t lead to the creati"
W15-3405,I11-2003,1,0.903509,"Missing"
W15-3405,W13-2502,0,0.0196461,"d in Figure 1. quality of the extracted lexicon. For instance, we can cite the use of a bilingual thesaurus (D´ejean et al., 2002), implication of predictive methods for word co-occurrence counts (Hazem and Morin, 2013) or the use of unbalanced corpora (Morin and Hazem, 2014). Among them, and in the case of comparable corpora, we can denote that none looked into pivot-language approaches. Nevertheless, the idea of involving a pivot language for translation tasks is not recent. Bilingual lexicon extraction from parallel corpora has already been improved via the use of an intermediary language (Kwon et al., 2013; Seo et al., 2014; Kim et al., 2015), so does statistical translation (Simard, 1999; Och and Ney, 2001). Those works lay on the assumption that another language brings additional information (Dagan and Itai, 1991). 3 3.2 The second method based on pivot dictionaries consists in translating both source and target context vectors into pivot language. Thus, the operation of computing similarity occurs in the vectorial space of the pivot language. In order to do so, the context vector of a word in source language to translate is computed as it is usually done in the standard method. The second st"
W15-3405,C10-1073,0,0.291438,"Missing"
W15-3405,W99-0602,0,0.118765,"ilingual thesaurus (D´ejean et al., 2002), implication of predictive methods for word co-occurrence counts (Hazem and Morin, 2013) or the use of unbalanced corpora (Morin and Hazem, 2014). Among them, and in the case of comparable corpora, we can denote that none looked into pivot-language approaches. Nevertheless, the idea of involving a pivot language for translation tasks is not recent. Bilingual lexicon extraction from parallel corpora has already been improved via the use of an intermediary language (Kwon et al., 2013; Seo et al., 2014; Kim et al., 2015), so does statistical translation (Simard, 1999; Och and Ney, 2001). Those works lay on the assumption that another language brings additional information (Dagan and Itai, 1991). 3 3.2 The second method based on pivot dictionaries consists in translating both source and target context vectors into pivot language. Thus, the operation of computing similarity occurs in the vectorial space of the pivot language. In order to do so, the context vector of a word in source language to translate is computed as it is usually done in the standard method. The second step is to transfer the source and target context vectors into the pivot language usin"
W15-3405,W05-0809,0,0.0257099,"ora demonstrated that more than two languages can be useful to improve the alignments. Our works have investigated to which extent a third language could be interesting to bypass the original alignment. We have defined two original alignment approaches involving pivot languages and we have evaluated over four languages and two pivot languages in particular. The experiments have shown that in some cases the quality of the extracted lexicon has been enhanced. 1 2 Bilingual Lexicon Extraction Initially designed for parallel corpora (Chen, 1993), and due to the scarcity of this kind of resources (Martin et al., 2005), bilingual lexicon extraction then tried to deal with comparable corpora instead (Fung, 1995; Rapp, 1995). An algorithm using comparable corpora is the standard method (Fung and McKeown, 1997) closely based on the notion of context vectors. Many implementations have been designed in order to do so (Rapp, 1999; Chiao and Zweigenbaum, 2002; Morin et al., 2010). A context vector w is, for a given word w, the representation of its contexts ct1 . . . cti and the number of occurrences found in the window of a corpus. In this approach, context vectors are calculated both in source and target languag"
W15-3405,P14-1121,1,0.76887,"t we transferred the context vectors via a pivot language. Finally, the last step of similarity computation stays unchanged: for one source word w for which we want to find the translation in target language, we compute the similarity between its context vector transferred successively w and all target context vectors t. This method is presented in Figure 1. quality of the extracted lexicon. For instance, we can cite the use of a bilingual thesaurus (D´ejean et al., 2002), implication of predictive methods for word co-occurrence counts (Hazem and Morin, 2013) or the use of unbalanced corpora (Morin and Hazem, 2014). Among them, and in the case of comparable corpora, we can denote that none looked into pivot-language approaches. Nevertheless, the idea of involving a pivot language for translation tasks is not recent. Bilingual lexicon extraction from parallel corpora has already been improved via the use of an intermediary language (Kwon et al., 2013; Seo et al., 2014; Kim et al., 2015), so does statistical translation (Simard, 1999; Och and Ney, 2001). Those works lay on the assumption that another language brings additional information (Dagan and Itai, 1991). 3 3.2 The second method based on pivot dict"
W15-3405,W11-1205,1,0.859115,"ed translation is. Research in this field aims at improving the Introduction The main goal of this work is to investigate to which extent bilingual lexicon extraction using comparable corpora can be improved using a third language when dealing with poor resource language pairs. Indeed, the quality of the result of the extracted bilingual lexicon strongly depends on the quality of the resources, that is to say the corpora and a general language bilingual dictionary. In this study, we stress the key role of the potential high quality resources of the pivot language (Chiao and Zweigenbaum, 2004; Morin and Prochasson, 2011; Hazem and Morin, 2012). The idea of involving a third language is to benefit from the lexical information conveyed by the additional language. We also assume that in the case of not so usual language pairs the two comparable corpora are of medium quality, and the bilingual dictionary seems weak, due to the nonexistence of such a dictionary. We expect as a consequence a bad quality of the extracted lexicon. Nevertheless, we are highly confident that a language for which 32 Proceedings of the Eighth Workshop on Building and Using Comparable Corpora, pages 32–37, c Beijing, China, July 30, 2015"
W15-3405,W97-0119,0,\N,Missing
W15-3405,C02-1166,0,\N,Missing
W15-3405,P93-1002,0,\N,Missing
W15-3413,E09-1003,0,0.0617287,"arallel corpora, that is, collections of documents that are mutual translations, are used in many natural language processing applications, particularly for statistical machine translation. Building such resources is however exceedingly expensive, requiring highly skilled annotators or professional translators (Preiss, 2012). Comparable corpora, that are sets of texts in two or more languages without being translations of each other, are often considered as a solution for the lack of parallel corpora, and many techniques have been proposed to extract parallel sentences (Munteanu et al., 2004; Abdul-Rauf and Schwenk, 2009; Smith et al., 2010), or mine word translations (Fung, 1995; Rapp, 1999; Chiao and Zweigenbaum, 2002; Morin et al., 2007; Vuli´c and Moens, 2012). Identifying comparable resources in a large amount of multilingual data remains a very challenging task. The purpose of the Building and Using Comparable Corpora (BUCC) 2015 shared task1 is to provide the first evaluation of existing approaches for identifying comparable resources. More precisely, given a large collection of Wikipedia pages in several languages, the task is to identify the most similar pages across languages. 1 Proposed Method webs"
W15-3413,C02-2020,0,0.0184779,"ral language processing applications, particularly for statistical machine translation. Building such resources is however exceedingly expensive, requiring highly skilled annotators or professional translators (Preiss, 2012). Comparable corpora, that are sets of texts in two or more languages without being translations of each other, are often considered as a solution for the lack of parallel corpora, and many techniques have been proposed to extract parallel sentences (Munteanu et al., 2004; Abdul-Rauf and Schwenk, 2009; Smith et al., 2010), or mine word translations (Fung, 1995; Rapp, 1999; Chiao and Zweigenbaum, 2002; Morin et al., 2007; Vuli´c and Moens, 2012). Identifying comparable resources in a large amount of multilingual data remains a very challenging task. The purpose of the Building and Using Comparable Corpora (BUCC) 2015 shared task1 is to provide the first evaluation of existing approaches for identifying comparable resources. More precisely, given a large collection of Wikipedia pages in several languages, the task is to identify the most similar pages across languages. 1 Proposed Method website major gaston links flutist marcel debost states sources college crunelle conservatoire principal"
W15-3413,N07-2008,0,0.198412,"{elizaveta.loginova,florian.boudin,emmanuel.morin}@univ-nantes.fr 2 Abstract In this paper, we describe the system that we developed for the BUCC 2015 shared track and show that a language agnostic approach can achieve promising results. This paper describes the LINA system for the BUCC 2015 shared track. Following (Enright and Kondrak, 2007), our system identify comparable documents by collecting counts of hapax words. We extend this method by filtering out document pairs sharing target documents using pigeonhole reasoning and cross-lingual information. 1 2 The method we propose is based on (Enright and Kondrak, 2007)’s approach to parallel document identification. Documents are treated as bags of words, in which only blank separated strings that are at least four characters long and that appear only once in the document (hapax words) are indexed. Given a document in language A, the document in language B that share the largest number of these words is considered as parallel. Although very simple, this approach was shown to perform very well in detecting parallel documents in Wikipedia (Patry and Langlais, 2011). The reason for this is that most hapax words are in practice proper nouns or numerical entitie"
W15-3413,W95-0114,0,0.101621,"s, are used in many natural language processing applications, particularly for statistical machine translation. Building such resources is however exceedingly expensive, requiring highly skilled annotators or professional translators (Preiss, 2012). Comparable corpora, that are sets of texts in two or more languages without being translations of each other, are often considered as a solution for the lack of parallel corpora, and many techniques have been proposed to extract parallel sentences (Munteanu et al., 2004; Abdul-Rauf and Schwenk, 2009; Smith et al., 2010), or mine word translations (Fung, 1995; Rapp, 1999; Chiao and Zweigenbaum, 2002; Morin et al., 2007; Vuli´c and Moens, 2012). Identifying comparable resources in a large amount of multilingual data remains a very challenging task. The purpose of the Building and Using Comparable Corpora (BUCC) 2015 shared task1 is to provide the first evaluation of existing approaches for identifying comparable resources. More precisely, given a large collection of Wikipedia pages in several languages, the task is to identify the most similar pages across languages. 1 Proposed Method website major gaston links flutist marcel debost states sources"
W15-3413,P07-1084,1,0.797261,"cations, particularly for statistical machine translation. Building such resources is however exceedingly expensive, requiring highly skilled annotators or professional translators (Preiss, 2012). Comparable corpora, that are sets of texts in two or more languages without being translations of each other, are often considered as a solution for the lack of parallel corpora, and many techniques have been proposed to extract parallel sentences (Munteanu et al., 2004; Abdul-Rauf and Schwenk, 2009; Smith et al., 2010), or mine word translations (Fung, 1995; Rapp, 1999; Chiao and Zweigenbaum, 2002; Morin et al., 2007; Vuli´c and Moens, 2012). Identifying comparable resources in a large amount of multilingual data remains a very challenging task. The purpose of the Building and Using Comparable Corpora (BUCC) 2015 shared task1 is to provide the first evaluation of existing approaches for identifying comparable resources. More precisely, given a large collection of Wikipedia pages in several languages, the task is to identify the most similar pages across languages. 1 Proposed Method website major gaston links flutist marcel debost states sources college crunelle conservatoire principal rampal united curren"
W15-3413,N04-1034,0,0.187111,"a pages. Introduction Parallel corpora, that is, collections of documents that are mutual translations, are used in many natural language processing applications, particularly for statistical machine translation. Building such resources is however exceedingly expensive, requiring highly skilled annotators or professional translators (Preiss, 2012). Comparable corpora, that are sets of texts in two or more languages without being translations of each other, are often considered as a solution for the lack of parallel corpora, and many techniques have been proposed to extract parallel sentences (Munteanu et al., 2004; Abdul-Rauf and Schwenk, 2009; Smith et al., 2010), or mine word translations (Fung, 1995; Rapp, 1999; Chiao and Zweigenbaum, 2002; Morin et al., 2007; Vuli´c and Moens, 2012). Identifying comparable resources in a large amount of multilingual data remains a very challenging task. The purpose of the Building and Using Comparable Corpora (BUCC) 2015 shared task1 is to provide the first evaluation of existing approaches for identifying comparable resources. More precisely, given a large collection of Wikipedia pages in several languages, the task is to identify the most similar pages across lan"
W15-3413,W11-1212,0,0.109518,"ing pigeonhole reasoning and cross-lingual information. 1 2 The method we propose is based on (Enright and Kondrak, 2007)’s approach to parallel document identification. Documents are treated as bags of words, in which only blank separated strings that are at least four characters long and that appear only once in the document (hapax words) are indexed. Given a document in language A, the document in language B that share the largest number of these words is considered as parallel. Although very simple, this approach was shown to perform very well in detecting parallel documents in Wikipedia (Patry and Langlais, 2011). The reason for this is that most hapax words are in practice proper nouns or numerical entities, which are often cognates. An example of hapax words extracted from a document is given in Table 1. We purposely keep urls and special characters, as these are useful clues for identifying translated Wikipedia pages. Introduction Parallel corpora, that is, collections of documents that are mutual translations, are used in many natural language processing applications, particularly for statistical machine translation. Building such resources is however exceedingly expensive, requiring highly skille"
W15-3413,N12-1065,0,0.0243744,"ords are in practice proper nouns or numerical entities, which are often cognates. An example of hapax words extracted from a document is given in Table 1. We purposely keep urls and special characters, as these are useful clues for identifying translated Wikipedia pages. Introduction Parallel corpora, that is, collections of documents that are mutual translations, are used in many natural language processing applications, particularly for statistical machine translation. Building such resources is however exceedingly expensive, requiring highly skilled annotators or professional translators (Preiss, 2012). Comparable corpora, that are sets of texts in two or more languages without being translations of each other, are often considered as a solution for the lack of parallel corpora, and many techniques have been proposed to extract parallel sentences (Munteanu et al., 2004; Abdul-Rauf and Schwenk, 2009; Smith et al., 2010), or mine word translations (Fung, 1995; Rapp, 1999; Chiao and Zweigenbaum, 2002; Morin et al., 2007; Vuli´c and Moens, 2012). Identifying comparable resources in a large amount of multilingual data remains a very challenging task. The purpose of the Building and Using Compara"
W15-3413,P99-1067,0,0.179393,"in many natural language processing applications, particularly for statistical machine translation. Building such resources is however exceedingly expensive, requiring highly skilled annotators or professional translators (Preiss, 2012). Comparable corpora, that are sets of texts in two or more languages without being translations of each other, are often considered as a solution for the lack of parallel corpora, and many techniques have been proposed to extract parallel sentences (Munteanu et al., 2004; Abdul-Rauf and Schwenk, 2009; Smith et al., 2010), or mine word translations (Fung, 1995; Rapp, 1999; Chiao and Zweigenbaum, 2002; Morin et al., 2007; Vuli´c and Moens, 2012). Identifying comparable resources in a large amount of multilingual data remains a very challenging task. The purpose of the Building and Using Comparable Corpora (BUCC) 2015 shared task1 is to provide the first evaluation of existing approaches for identifying comparable resources. More precisely, given a large collection of Wikipedia pages in several languages, the task is to identify the most similar pages across languages. 1 Proposed Method website major gaston links flutist marcel debost states sources college crun"
W15-3413,N10-1063,0,0.0848939,"ections of documents that are mutual translations, are used in many natural language processing applications, particularly for statistical machine translation. Building such resources is however exceedingly expensive, requiring highly skilled annotators or professional translators (Preiss, 2012). Comparable corpora, that are sets of texts in two or more languages without being translations of each other, are often considered as a solution for the lack of parallel corpora, and many techniques have been proposed to extract parallel sentences (Munteanu et al., 2004; Abdul-Rauf and Schwenk, 2009; Smith et al., 2010), or mine word translations (Fung, 1995; Rapp, 1999; Chiao and Zweigenbaum, 2002; Morin et al., 2007; Vuli´c and Moens, 2012). Identifying comparable resources in a large amount of multilingual data remains a very challenging task. The purpose of the Building and Using Comparable Corpora (BUCC) 2015 shared task1 is to provide the first evaluation of existing approaches for identifying comparable resources. More precisely, given a large collection of Wikipedia pages in several languages, the task is to identify the most similar pages across languages. 1 Proposed Method website major gaston link"
W15-3413,E12-1046,0,0.0407071,"Missing"
W17-4206,2015.jeptalnrecital-court.30,1,0.802127,"Missing"
W17-4206,C04-1008,0,0.0231536,"in LS2N, Universit´e de Nantes 2 Chemin de la Houssini`ere 44300 Nantes, France Pascale S´ebillot INSA Rennes, IRISA & INRIA Rennes 263 Avenue G´en´eral Leclerc 35042 Rennes, France Abstract to the relatively large accessibility and interest to both media professionals and general public, however mostly from the search angle. Typical search-based approaches consist in organizing datasets around clusters, in which similar or topically close news articles are grouped. The created clusters can be further processed to be displayed as threads (Ide et al., 2004), or according to temporal relations (Muller and Tannier, 2004). However, pitfalls appear when dealing with large timeframes, as the number of clusters to display becomes overwhelming. In this work, we rather focus on an exploration scenario without precise information need, where one has to get a comprehensive view on a topic in a limited amount of time, and for which the methods mentioned above are not suited. For this scenario, the usual approach consists in creating links between pairs of documents within the collection, allowing users to directly go from one news piece to another. By following links, the user is able to navigate the collection, choos"
