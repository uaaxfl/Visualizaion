2020.lrec-1.554,{H}edwig: A Named Entity Linker,2020,-1,-1,2,1,17774,marcus klang,Proceedings of the 12th Language Resources and Evaluation Conference,0,"Named entity linking is the task of identifying mentions of named things in text, such as {``}Barack Obama{''} or {``}New York{''}, and linking these mentions to unique identifiers. In this paper, we describe Hedwig, an end-to-end named entity linker, which uses a combination of word and character BILSTM models for mention detection, a Wikidata and Wikipedia-derived knowledge base with global information aggregated over nine language editions, and a PageRank algorithm for entity linking. We evaluated Hedwig on the TAC2017 dataset, consisting of news texts and discussion forums, and we obtained a final score of 59.9{\%} on CEAFmC+, an improvement over our previous generation linker Ugglan, and a trilingual entity link score of 71.9{\%}."
W19-6148,{D}ocria: Processing and Storing Linguistic Data with {W}ikipedia,2019,0,0,2,1,17774,marcus klang,Proceedings of the 22nd Nordic Conference on Computational Linguistics,0,"The availability of user-generated content has increased significantly over time. Wikipedia is one example of a corpora which spans a huge range of topics and is freely available. Storing and processing these corpora requires flexible documents models as they may contain malicious and incorrect data. Docria is a library which attempts to address this issue by providing a solution which can be used with small to large corpora, from laptops using Python interactively in a Jupyter notebook to clusters running map-reduce frameworks with optimized compiled code. Docria is available as open-source code."
L18-1540,"Linking, Searching, and Visualizing Entities in {W}ikipedia",2018,0,0,2,1,17774,marcus klang,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,"In this paper, we describe a new system to extract, index, search, and visualize entities in Wikipedia. To carry out the entity extraction, we designed a high-performance, multilingual, entity linker and we used a document model to store the resulting linguistic annotations. The entity linker, HEDWIG, extracts the mentions from text usinga string matching Engine and links them toentities with a combination of statistical rules and PageRank. The document model, Docforia (Klang and Nugues, 2017), consists of layers, where each layer is a sequence of ranges describing a specixefxacx81c annotation, here the entities. We evaluated HEDWIG with the TAC 2016 data and protocol (Ji and Nothman, 2016) and we reached the CEAFm scores of 70.0 on English, on 64.4 on Chinese, and 66.5 on Spanish. We applied the entity linker to the whole collection of English and Swedish articles of Wikipedia and we used Lucene to index the layers and a search module to interactively retrieve all the concordances of an entity in Wikipedia. The user can select and visualize the concordances in the articles or paragraphs. Contrary to classic text indexing, this system does not use strings to identify the entities but unique identixefxacx81ers from Wikidata (Less)"
W17-0206,Coreference Resolution for {S}wedish and {G}erman using Distant Supervision,2017,0,0,2,0,32164,alexander wallin,Proceedings of the 21st Nordic Conference on Computational Linguistics,0,"Coreference resolution is the identixefxacx81cation of phrases that refer to the same entity in a text. Current techniques to solve coreferences use machine-learning algorithms, which require large annotated data sets. Such annotated Resources are no tavailable for most languages today. In this paper, we describe a method for solving coreferences for Swedish and German using distant supervision that does not use manually annotated texts. We generate a weakly labelled training set using parallel corpora, English-Swedish and English-German, where we solve the coreference for English using CoreNLP and transfer it to Swedish and German using word alignments. To carry this out, we identify mentions from dependency graphs in both target languages using hand-written rules. Finally, we evaluate the end-to-end results using the evaluation script from the CoNLL 2012 shared task for which we obtain a score of 34.98 for Swedish and 13.16 for German and, respectively, 46.73 and 36.98 using gold mentions. (Less)"
W17-0211,A Multilingual Entity Linker Using {P}age{R}ank and Semantic Graphs,2017,0,4,2,0,32168,anton sodergren,Proceedings of the 21st Nordic Conference on Computational Linguistics,0,None
W17-0227,{D}ocforia: A Multilayer Document Model,2017,0,0,2,1,17774,marcus klang,Proceedings of the 21st Nordic Conference on Computational Linguistics,0,"In this paper, we describe Docforia, a multilayer document model and application programming interface (API) to store formatting, lexical, syntactic, and semantic annotations on Wikipedia and other kinds of text and visualize them. While Wikipedia has become a major NLP resource, its scale and heterogeneity makes it relatively difxefxacx81cult to do experimentations on the whole corpus. These experimentations are rendered even more complexas,to the best of our knowledge,there is no available tool to visualize easily the results of a processing pipeline. We designed Docforia so that it can store millions of documents and billions of tokens, annotated using different processing tools,that themselves use multiple formats, and compatible with cluster computing frameworks such as Hadoop or Spark. The annotation output, either partial or complete, can then be shared more easily. To validate Docforia, we processed six language versions of Wikipedia: English, French, German, Spanish, Russian, and Swedish, up to semantic role labeling, depending on the NLP tools available for a given language. We stored the results in our document model and we created a visualization tool to inspect the annotation results. (Less)"
W16-4410,Pairing {W}ikipedia Articles Across Languages,2016,10,0,2,1,17774,marcus klang,Proceedings of the Open Knowledge Base and Question Answering Workshop ({OKBQA} 2016),0,"Wikipedia has become a reference knowledge source for scores of NLP applications. One of its invaluable features lies in its multilingual nature, where articles on a same entity or concept can have from one to more than 200 different versions. The interlinking of language versions in Wikipedia has undergone a major renewal with the advent of Wikidata, a unified scheme to identify entities and their properties using unique numbers. However, as the interlinking is still manually carried out by thousands of editors across the globe, errors may creep in the assignment of entities. In this paper, we describe an optimization technique to match automatically language versions of articles, and hence entities, that is only based on bags of words and anchors. We created a dataset of all the articles on persons we extracted from Wikipedia in six languages: English, French, German, Russian, Spanish, and Swedish. We report a correct match of at least 94.3{\%} on each pair."
L16-1654,{WIKIPARQ}: A Tabulated {W}ikipedia Resource Using the Parquet Format,2016,13,3,2,1,17774,marcus klang,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"Wikipedia has become one of the most popular resources in natural language processing and it is used in quantities of applications. However, Wikipedia requires a substantial pre-processing step before it can be used. For instance, its set of nonstandardized annotations, referred to as the wiki markup, is language-dependent and needs specific parsers from language to language, for English, French, Italian, etc. In addition, the intricacies of the different Wikipedia resources: main article text, categories, wikidata, infoboxes, scattered into the article document or in different files make it difficult to have global view of this outstanding resource. In this paper, we describe WikiParq, a unified format based on the Parquet standard to tabulate and package the Wikipedia corpora. In combination with Spark, a map-reduce computing framework, and the SQL query language, WikiParq makes it much easier to write database queries to extract specific information or subcorpora from Wikipedia, such as all the first paragraphs of the articles in French, or all the articles on persons in Spanish, or all the articles on persons that have versions in French, English, and Spanish. WikiParq is available in six language versions and is potentially extendible to all the languages of Wikipedia. The WikiParq files are downloadable as tarball archives from this location: http://semantica.cs.lth.se/wikiparq/."
C16-2016,{L}angforia: Language Pipelines for Annotating Large Collections of Documents,2016,13,1,2,1,17774,marcus klang,"Proceedings of {COLING} 2016, the 26th International Conference on Computational Linguistics: System Demonstrations",0,"In this paper, we describe \textbf{Langforia}, a multilingual processing pipeline to annotate texts with multiple layers: formatting, parts of speech, named entities, dependencies, semantic roles, and entity links. Langforia works as a web service, where the server hosts the language processing components and the client, the input and result visualization. To annotate a text or a Wikipedia page, the user chooses an NLP pipeline and enters the text in the interface or selects the page URL. Once processed, the results are returned to the client, where the user can select the annotation layers s/he wants to visualize. We designed Langforia with a specific focus for Wikipedia, although it can process any type of text. Wikipedia has become an essential encyclopedic corpus used in many NLP projects. However, processing articles and visualizing the annotations are nontrivial tasks that require dealing with multiple markup variants, encodings issues, and tool incompatibilities across the language versions. This motivated the development of a new architecture. A demonstration of Langforia is available for six languages: English, French, German, Spanish, Russian, and Swedish at \url{http://vilde.cs.lth.se:9000/} as well as a web API: \url{http://vilde.cs.lth.se:9000/api}. Langforia is also provided as a standalone library and is compatible with cluster computing."
C16-1096,Multilingual Supervision of Semantic Annotation,2016,26,0,3,1,35730,peter exner,"Proceedings of {COLING} 2016, the 26th International Conference on Computational Linguistics: Technical Papers",0,"In this paper, we investigate the annotation projection of semantic units in a practical setting. Previous approaches have focused on using parallel corpora for semantic transfer. We evaluate an alternative approach using loosely parallel corpora that does not require the corpora to be exact translations of each other. We developed a method that transfers semantic annotations from one language to another using sentences aligned by entities, and we extended it to include alignments by entity-like linguistic units. We conducted our experiments on a large scale using the English, Swedish, and French language editions of Wikipedia. Our results show that the annotation projection using entities in combination with loosely parallel corpora provides a viable approach to extending previous attempts. In addition, it allows the generation of proposition banks upon which semantic parsers can be trained."
W15-2004,Extraction of lethal events from {W}ikipedia and a semantic repository,2015,8,1,2,0,36968,magnus norrby,Proceedings of the workshop on Semantic resources and semantic annotation for Natural Language Processing and the Digital Humanities at {NODALIDA} 2015,0,"This paper describes the extraction of information on lethal events from the Swedish version of Wikipedia. The information searched includes the personsxe2x80x99 cause of death, origin, and profession. We carried out the extraction using a processing pipeline of available tools for Swedish including a part-of-speech tagger, a dependency parser, and manually-written extraction rules. We also extracted structured semantic data from the Wikidata store that we combined with the information retrieved from Wikipedia. Eventually, we gathered a database of facts that covers both sources: Wikipedia and Wikidata."
S15-1029,A Distant Supervision Approach to Semantic Role Labeling,2015,27,3,3,1,35730,peter exner,Proceedings of the Fourth Joint Conference on Lexical and Computational Semantics,0,"Semanticrolelabelinghasbecomeakeymodule for many language processing applications such as question answering, information extraction, sentiment analysis, and machine translation. To build an unrestricted semantic role labeler, the xefxacx81rst step is to develop a comprehensive proposition bank. However, creating such a bank is a costly enterprise, which has only been achieved for a handful of languages. In this paper, we describe a technique to build proposition banks for new languages using distant supervision. Starting from PropBank inEnglishandlooselyparallelcorporasuchas versions of Wikipedia in different languages, we carried out a mapping of semantic propositions we extracted from English to syntactic structures in Swedish using named entities. We trained a semantic parser on the generated Swedishpropositionsandwereporttheresults we obtained. Using the CoNLL 2009 evaluation script, we could reach the scores of 52.25 for labeled propositions and 62.44 for the unlabeled ones. We believe our approach can be appliedtotrainsemanticrolelabelersforother resource-scarce languages."
K15-1019,Linking Entities Across Images and Text,2015,20,1,3,0,32479,rebecka weegar,Proceedings of the Nineteenth Conference on Computational Natural Language Learning,0,"This paper describes a set of methods to link entities across images and text. As a corpus, we used a data set of images, where each image is commented by a short caption and where the regions in the images are manually segmented and labeled with a category. We extracted the entity mentions from the captions and we computed a semantic similarity between the mentions and the region labels. We also measured the statistical associations between these mentions and the labels and we combined them with the semantic similarity to produce mappings in the form of pairs consisting of a region label and a caption entity. In a second step, we used the syntactic relationships between the mentions and the spatial relationships between the regions to rerank the lists of candidate mappings. To evaluate our methods, we annotated a test set of 200 images, where we manually linked the im- age regions to their corresponding mentions in the captions. Eventually, we could match objects in pictures to their correct mentions for nearly 89 percent of the segments, when such a matching exists. (Less)"
exner-nugues-2014-refractive,{REFRACTIVE}: An Open Source Tool to Extract Knowledge from Syntactic and Semantic Relations,2014,27,3,2,1,35730,peter exner,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"The extraction of semantic propositions has proven instrumental in applications like IBM Watson and in Google{'}s knowledge graph . One of the core components of IBM Watson is the PRISMATIC knowledge base consisting of one billion propositions extracted from the English version of Wikipedia and the New York Times. However, extracting the propositions from the English version of Wikipedia is a time-consuming process. In practice, this task requires multiple machines and a computation distribution involving a good deal of system technicalities. In this paper, we describe Refractive, an open-source tool to extract propositions from a parsed corpus based on the Hadoop variant of MapReduce. While the complete process consists of a parsing part and an extraction part, we focus here on the extraction from the parsed corpus and we hope this tool will help computational linguists speed up the development of applications."
W12-4505,Using Syntactic Dependencies to Solve Coreferences,2012,9,7,4,0,42148,marcus stamborg,Joint Conference on {EMNLP} and {C}o{NLL} - Shared Task,0,"This paper describes the structure of the LTH coreference solver used in the closed track of the CoNLL 2012 shared task (Pradhan et al., 2012). The solver core is a mention classifier that uses Soon et al. (2001)'s algorithm and features extracted from the dependency graphs of the sentences.n n This system builds on Bjorkelund and Nugues (2011)'s solver that we extended so that it can be applied to the three languages of the task: English, Chinese, and Arabic. We designed a new mention detection module that removes pleonastic pronouns, prunes constituents, and recovers mentions when they do not match exactly a noun phrase. We carefully redesigned the features so that they reflect more complex linguistic phenomena as well as discourse properties. Finally, we introduced a minimal cluster model grounded in the first mention of an entity.n n We optimized the feature sets for the three languages: We carried out an extensive evaluation of pairs of features and we complemented the single features with associations that improved the CoNLL score. We obtained the respective scores of 59.57, 56.62, and 48.25 on English, Chinese, and Arabic on the development set, 59.36, 56.85, and 49.43 on the test set, and the combined official score of 55.21."
exner-nugues-2012-constructing,Constructing Large Proposition Databases,2012,9,6,2,1,35730,peter exner,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"With the advent of massive online encyclopedic corpora such as Wikipedia, it has become possible to apply a systematic analysis to a wide range of documents covering a significant part of human knowledge. Using semantic parsers, it has become possible to extract such knowledge in the form of propositions (predicateâargument structures) and build large proposition databases from these documents. This paper describes the creation of multilingual proposition databases using generic semantic dependency parsing. Using Wikipedia, we extracted, processed, clustered, and evaluated a large number of propositions. We built an architecture to provide a complete pipeline dealing with the input of text, extraction of knowledge, storage, and presentation of the resulting propositions."
sundberg-etal-2012-visualizing,Visualizing Sentiment Analysis on a User Forum,2012,19,1,4,0,43009,rasmus sundberg,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"Sentiment analysis, or opinion mining, is the process of extracting sentiment from documents or sentences, where the expressed sentiment is typically categorized as positive, negative, or neutral. Many different techniques have been proposed. In this paper, we report the reimplementation of nine algorithms and their evaluation across four corpora to assess the sentiment at the sentence level. We extracted the named entities from each sentence and we associated them with the sentence sentiment. We built a graphical module based on the Qlikview software suite to visualize the sentiments attached to named entities mentioned in Internet forums and follow opinion changes over time."
W11-1905,Exploring Lexicalized Features for Coreference Resolution,2011,9,14,2,1,18856,anders bjorkelund,Proceedings of the Fifteenth Conference on Computational Natural Language Learning: Shared Task,0,"In this paper, we describe a coreference solver based on the extensive use of lexical features and features extracted from dependency graphs of the sentences. The solver uses Soon et al. (2001)'s classical resolution algorithm based on a pairwise classification of the mentions.n n We applied this solver to the closed track of the CoNLL 2011 shared task (Pradhan et al., 2011). We carried out a systematic optimization of the feature set using cross-validation that led us to retain 24 features. Using this set, we reached a MUC score of 58.61 on the test set of the shared task. We analyzed the impact of the features on the development set and we show the importance of lexicalization as well as of properties related to dependency links in coreference resolution."
C10-3009,A High-Performance Syntactic and Semantic Dependency Parser,2010,11,59,4,1,18856,anders bjorkelund,Coling 2010: Demonstrations,0,"This demonstration presents a high-performance syntactic and semantic dependency parser. The system consists of a pipeline of modules that carry out the to-kenization, lemmatization, part-of-speech tagging, dependency parsing, and semantic role labeling of a sentence. The system's two main components draw on improved versions of a state-of-the-art dependency parser (Bohnet, 2009) and semantic role labeler (Bjorkelund et al., 2009) developed independently by the authors.n n The system takes a sentence as input and produces a syntactic and semantic annotation using the CoNLL 2009 format. The processing time needed for a sentence typically ranges from 10 to 1000 milliseconds. The predicate--argument structures in the final output are visualized in the form of segments, which are more intuitive for a user."
C10-1093,Automatic Discovery of Feature Sets for Dependency Parsing,2010,12,8,2,0,46531,peter nilsson,Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010),0,"This paper describes a search procedure to discover optimal feature sets for dependency parsers. The search applies to the shift-reduce algorithm and the feature sets are extracted from the parser configuration. The initial feature is limited to the first word in the input queue. Then, the procedure uses a set of rules founded on the assumption that topological neighbors of significant features in the dependency graph may also have a significant contribution. The search can be fully automated and the level of greediness adjusted with the number of features examined at each iteration of the discovery procedure.n n Using our automated feature discovery on two corpora, the Swedish corpus in CoNLL-X and the English corpus in CoNLL 2008, and a single parser system, we could reach results comparable or better than the best scores reported in these evaluations. The CoNLL 2008 test set contains, in addition to a Wall Street Journal (WSJ) section, an out-of-domain sample from the Brown corpus. With sets of 15 features, we obtained a labeled attachment score of 84.21 for Swedish, 88.11 on the WSJ test set, and 81.33 on the Brown test set."
W09-4621,Text Categorization Using Predicate-Argument Structures,2009,22,9,3,0,46761,jacob persson,Proceedings of the 17th Nordic Conference of Computational Linguistics ({NODALIDA} 2009),0,"! Most text categorization methods use the vector space model in combination with a representation of documents based on bags of words. As its name indicates, bags of words ignore possible structures in the text and only take into account isolated, unrelated words. Although this limitation is widely acknowledged, most previous attempts to extend the bag-of-words model with more advanced approaches failed to produce conclusive improvements. We propose a novel method that extends the word-level representation to automatically extracted semantic and syntactic features. We investigated three extensions: word-sense information, subjectxe2x80x93verbxe2x80x93object triples, and rolesemantic predicatexe2x80x93argument tuples, all xefx83x9etting within the vector space model. We computed their contribution to the categorization results on the Reuters corpus of newswires (RCV1). We show that these three extensions, either taken individually or in combination, result in statistically signixefx83x9ecant improvements of the microaverageF 1 over a baseline using bags of words. We found that our best extended model that uses a combination of syntactic and semantic features reduces the error of the word-level baseline by up to 10 percent for the categories having more than 1,000 documents in the training corpus. ! Research done while at Lund University."
W09-3806,Predictive Text Entry using Syntax and Semantics,2009,26,4,3,0,46832,sebastian ganslandt,Proceedings of the 11th International Conference on Parsing Technologies ({IWPT}{'}09),0,"Most cellular telephones use numeric keypads, where texting is supported by dictionaries and frequency models. Given a key sequence, the entry system recognizes the matching words and proposes a rank-ordered list of candidates. The ranking quality is instrumental to an effective entry.n n This paper describes a new method to enhance entry that combines syntax and language models. We first investigate components to improve the ranking step: language models and semantic relatedness. We then introduce a novel syntactic model to capture the word context, optimize ranking, and then reduce the number of keystrokes per character (KSPC) needed to write a text. We finally combine this model with the other components and we discuss the results.n n We show that our syntax-based model reaches an error reduction in KSPC of 12.4% on a Swedish corpus over a baseline using word frequencies. We also show that bigrams are superior to all the other models. However, bigrams have a memory footprint that is unfit for most devices. Nonetheless, bigrams can be further improved by the addition of syntactic models with an error reduction that reaches 29.4%."
W09-1206,Multilingual Semantic Role Labeling,2009,10,136,3,1,18856,anders bjorkelund,Proceedings of the Thirteenth Conference on Computational Natural Language Learning ({C}o{NLL} 2009): Shared Task,0,"This paper describes our contribution to the semantic role labeling task (SRL-only) of the CoNLL-2009 shared task in the closed challenge (Hajic et al., 2009). Our system consists of a pipeline of independent, local classifiers that identify the predicate sense, the arguments of the predicates, and the argument labels. Using these local models, we carried out a beam search to generate a pool of candidates. We then reranked the candidates using a joint learning approach that combines the local models and proposition features.n n To address the multilingual nature of the data, we implemented a feature selection procedure that systematically explored the feature space, yielding significant gains over a standard set of features. Our system achieved the second best semantic score overall with an average labeled semantic F1 of 80.31. It obtained the best F1 score on the Chinese and German data and the second best one on English."
W08-2123,Dependency-based Syntactic{--}Semantic Analysis with {P}rop{B}ank and {N}om{B}ank,2008,12,131,2,0.909091,2644,richard johansson,{C}o{NLL} 2008: Proceedings of the Twelfth Conference on Computational Natural Language Learning,0,"This paper presents our contribution in the closed track of the 2008 CoNLL Shared Task (Surdeanu et al., 2008). To tackle the problem of joint syntactic--semantic analysis, the system relies on a syntactic and a semantic subcomponent. The syntactic model is a bottom-up projective parser using pseudo-projective transformations, and the semantic model uses global inference mechanisms on top of a pipeline of classifiers. The complete syntactic--semantic output is selected from a candidate pool generated by the subsystems.n n The system achieved the top score in the closed challenge: a labeled syntactic accuracy of 89.32%, a labeled semantic F1 of 81.65, and a labeled macro F1 of 85.49."
johansson-nugues-2008-comparing,Comparing Dependency and Constituent Syntax for Frame-semantic Analysis,2008,13,3,2,0.909091,2644,richard johansson,Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08),0,"We address the question of which syntactic representation is best suited for role-semantic analysis of English in the FrameNet paradigm. We compare systems based on dependencies and constituents, and a dependency syntax with a rich set of grammatical functions with one with a smaller set. Our experiments show that dependency-based and constituent-based analyzers give roughly equivalent performance, and that a richer set of functions has a positive influence on argument classification for verbs."
D08-1008,Dependency-based Semantic Role Labeling of {P}rop{B}ank,2008,30,78,2,0.909091,2644,richard johansson,Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing,0,"We present a PropBank semantic role labeling system for English that is integrated with a dependency parser. To tackle the problem of joint syntactic--semantic analysis, the system relies on a syntactic and a semantic subcomponent. The syntactic model is a projective parser using pseudo-projective transformations, and the semantic model uses global inference mechanisms on top of a pipeline of classifiers. The complete syntactic-semantic output is selected from a candidate pool generated by the subsystems.n n We evaluate the system on the CoNLL-2005 test sets using segment-based and dependency-based metrics. Using the segment-based CoNLL-2005 metric, our system achieves a near state-of-the-art F1 figure of 77.97 on the WSJBrown test set, or 78.84 if punctuation is treated consistently. Using a dependency-based metric, the F1 figure of our system is 84.29 on the test set from CoNLL-2008. Our system is the first dependency-based semantic role labeler for PropBank that rivals constituent-based systems in terms of performance."
C08-1050,The Effect of Syntactic Representation on Semantic Role Labeling,2008,30,56,2,0.909091,2644,richard johansson,Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008),0,"Almost all automatic semantic role labeling (SRL) systems rely on a preliminary parsing step that derives a syntactic structure from the sentence being analyzed. This makes the choice of syntactic representation an essential design decision. In this paper, we study the influence of syntactic representation on the performance of SRL systems. Specifically, we compare constituent-based and dependency-based representations for SRL of English in the FrameNet paradigm.n n Contrary to previous claims, our results demonstrate that the systems based on dependencies perform roughly as well as those based on constituents: For the argument classification task, dependency-based systems perform slightly higher on average, while the opposite holds for the argument identification task. This is remarkable because dependency parsers are still in their infancy while constituent parsing is more mature. Furthermore, the results show that dependency-based semantic role classifiers rely less on lexicalized features, which makes them more robust to domain changes and makes them learn more efficiently with respect to the amount of training data."
W07-2412,Evaluating Stages of Development in Second Language {F}rench: A Machine-Learning Approach,2007,11,6,2,1,48884,jonas granfeldt,Proceedings of the 16th Nordic Conference of Computational Linguistics ({NODALIDA} 2007),0,"This paper describes a system to define and evaluate development stages in second language French. The identification of such stages can be formulated as determining the frequency of some lexical and grammatical features in the learnersxe2x80x99 production and how they vary over time. The problems in this procedure are threefold: identify the relevant features, decide on cutoff points for the stages, and evaluate the degree of success of the model. The system addresses these three problems. It consists of a morphosyntactic analyzer called Direkt Profil and a machine-learning module connected to it. We first describe the usefulness and rationale behind its development. We then present the corpus we used to develop the analyzer. Finally, we present new and substantially improved results on training machine-learning classifiers compared to previous experiments (Granfeldt et al., 2006). We also introduce a method to select attributes in order to identify the most relevant grammatical features. (Less)"
W07-2416,Extended Constituent-to-Dependency Conversion for {E}nglish,2007,12,261,2,1,2644,richard johansson,Proceedings of the 16th Nordic Conference of Computational Linguistics ({NODALIDA} 2007),0,"We describe a new method to convert English constituent trees using the Penn Treebank annotation style into dependency trees. The new format was inspired by annotation practices used in other dependency treebanks with the intention to produce a better interface to further semantic processing than existing methods. In particular, we used a richer set of edge labels and introduced links to handle long-distance phenomena such as wh-movement and topicalization. The resulting trees generally have a more complex dependency structure. For example, 6% of the trees contain at least one nonprojective link, which is difficult for many parsing algorithms. As can be expected, the more complex structure and the enriched set of edge labels make the trees more difficult to predict, and we observed a decrease in parsing accuracy when applying two dependency parsers to the new corpus. However, the richer information contained in the new trees resulted in a 23% error reduction in a baseline FrameNet semantic role labeler that relied on dependency arc labels only. (Less)"
S07-1048,{LTH}: Semantic Structure Extraction using Nonprojective Dependency Trees,2007,14,74,2,1,2644,richard johansson,Proceedings of the Fourth International Workshop on Semantic Evaluations ({S}em{E}val-2007),0,"We describe our contribution to the SemEval task on Frame-Semantic Structure Extraction. Unlike most previous systems described in literature, ours is based on dependency syntax. We also describe a fully automatic method to add words to the FrameNet lexical database, which gives an improvement in the recall of frame detection."
D07-1123,Incremental Dependency Parsing Using Online Learning,2007,18,23,2,1,2644,richard johansson,Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning ({EMNLP}-{C}o{NLL}),0,"We describe an incremental parser that was trained to minimize cost over sentences rather than over individual parsing actions. This is an attempt to use the advantages of the two top-scoring systems in the CoNLL-X shared task. In the evaluation, we present the performance of the parser in the Multilingual task, as well as an evaluation of the contribution of bidirectional parsing and beam search to the parsing performance."
2007.jeptalnrecital-long.33,{\\'E}valuation des stades de d{\\'e}veloppement en fran{\\c{c}}ais langue {\\'e}trang{\\`e}re,2007,-1,-1,2,1,48884,jonas granfeldt,Actes de la 14{\\`e}me conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Articles longs,0,"Cet article d{\'e}crit un syst{\`e}me pour d{\'e}finir et {\'e}valuer les stades de d{\'e}veloppement en fran{\c{c}}ais langue {\'e}trang{\`e}re. L{'}{\'e}valuation de tels stades correspond {\`a} l{'}identification de la fr{\'e}quence de certains ph{\'e}nom{\`e}nes lexicaux et grammaticaux dans la production des apprenants et comment ces fr{\'e}quences changent en fonction du temps. Les probl{\`e}mes {\`a} r{\'e}soudre dans cette d{\'e}marche sont triples : identifier les attributs les plus r{\'e}v{\'e}lateurs, d{\'e}cider des points de s{\'e}paration entre les stades et {\'e}valuer le degr{\'e} d{'}efficacit{\'e} des attributs et de la classification dans son ensemble. Le syst{\`e}me traite ces trois probl{\`e}mes. Il se compose d{'}un analyseur morphosyntaxique, appel{\'e} Direkt Profil, auquel nous avons reli{\'e} un module d{'}apprentissage automatique. Dans cet article, nous d{\'e}crivons les id{\'e}es qui ont conduit au d{\'e}veloppement du syst{\`e}me et son int{\'e}r{\^e}t. Nous pr{\'e}sentons ensuite le corpus que nous avons utilis{\'e} pour d{\'e}velopper notre analyseur morphosyntaxique. Enfin, nous pr{\'e}sentons les r{\'e}sultats sensiblement am{\'e}lior{\'e}s des classificateurs compar{\'e} aux travaux pr{\'e}c{\'e}dents (Granfeldt et al., 2006). Nous pr{\'e}sentons {\'e}galement une m{\'e}thode de s{\'e}lection de param{\`e}tres afin d{'}identifier les attributs grammaticaux les plus appropri{\'e}s."
W06-2930,Investigating Multilingual Dependency Parsing,2006,15,29,2,1,2644,richard johansson,Proceedings of the Tenth Conference on Computational Natural Language Learning ({C}o{NLL}-X),0,"In this paper, we describe a system for the CoNLL-X shared task of multilingual dependency parsing. It uses a baseline Nivre's parser (Nivre, 2003) that first identifies the parse actions and then labels the dependency arcs. These two steps are implemented as SVM classifiers using LIBSVM. Features take into account the static context as well as relations dynamically built during parsing.n n We experimented two main additions to our implementation of Nivre's parser: N-best search and bidirectional parsing. We trained the parser in both left-right and right-left directions and we combined the results. To construct a single-head, rooted, and cycle-free tree, we applied the Chu-Liu/Edmonds optimization algorithm. We ran the same algorithm with the same parameters on all the languages."
P06-2057,A {F}rame{N}et-Based Semantic Role Labeler for {S}wedish,2006,16,32,2,1,2644,richard johansson,Proceedings of the {COLING}/{ACL} 2006 Main Conference Poster Sessions,0,"We present a FrameNet-based semantic role labeling system for Swedish text. As training data for the system, we used an annotated corpus that we produced by transferring FrameNet annotation from the English side to the Swedish side in a parallel corpus. In addition, we describe two frame element bracketing algorithms that are suitable when no robust constituent parsers are available.n n We evaluated the system on a part of the FrameNet example corpus that we translated manually, and obtained an accuracy score of 0.75 on the classification of presegmented frame elements, and precision and recall scores of 0.67 and 0.47 for the complete task."
berglund-etal-2006-extraction,Extraction of Temporal Information from Texts in {S}wedish,2006,16,2,3,1,50099,anders berglund,Proceedings of the Fifth International Conference on Language Resources and Evaluation ({LREC}{'}06),0,"This paper describes the implementation and evaluation of a generic component to extract temporal information from texts in Swedish. It proceeds in two steps. The first step extracts time expressions and events, and generates a feature vector for each element it identifies. Using the vectors, the second step determines the temporal relations, possibly none, between the extracted events and orders them in time. We used a machine learning approach to find the relations between events. To run the learning algorithm, we collected a corpus of road accident reports from newspapers websites that we manually annotated. It enabled us to train decision trees and to evaluate the performance of the algorithm."
johansson-nugues-2006-construction,Construction of a {F}rame{N}et Labeler for {S}wedish Text,2006,14,4,2,1,2644,richard johansson,Proceedings of the Fifth International Conference on Language Resources and Evaluation ({LREC}{'}06),0,"We describe the implementation of a FrameNet-based semantic role labeling system for Swedish text. To train the system, we used a semantically annotated corpus that was produced by projection across parallel corpora. As part of the system, we developed two frame element bracketing algorithms that are suitable when no robust constituent parsers are available. Apart from being the first such system for Swedish, this is, as far as we are aware, the first semantic role labeling system for a language for which no role-semantic annotated corpora are available. The estimated accuracy of classification of pre-segmented frame elements is 0.75, and the precision and recall measures for the complete task are 0.67 and 0.47, respectively."
granfeldt-etal-2006-cefle,{CEFLE} and Direkt Profil: a New Computer Learner Corpus in {F}rench {L}2 and a System for Grammatical Profiling,2006,10,8,2,1,48884,jonas granfeldt,Proceedings of the Fifth International Conference on Language Resources and Evaluation ({LREC}{'}06),0,"The importance of computer learner corpora for research in both second language acquisition and foreign language teaching is rapidly increasing. Computer learner corpora can provide us with data to describe the learnerÂs interlanguage system at different points of its development and they can be used to create pedagogical tools. In this paper, we first present a new computer learner corpus in French. We then describe an analyzer called Direkt Profil, that we have developed using this corpus. The system carries out a sentence analysis based on developmental sequences, i.e. local morphosyntactic phenomena linked to a development in the acquisition of French as a foreign language. We present a brief introduction to developmental sequences and some examples in French. In the final section, we introduce and evaluate a method to optimize the definition and detection of learner profiles using machine-learning techniques."
E06-2013,Automatic Annotation for All Semantic Layers in {F}rame{N}et,2006,8,7,2,1,2644,richard johansson,Demonstrations,0,"We describe a system for automatic annotation of English text in the FrameNet standard. In addition to the conventional annotation of frame elements and their semantic roles, we annotate additional semantic information such as support verbs and prepositions, aspectual markers, copular verbs, null arguments, and slot fillers. As far as we are aware, this is the first system that finds this information automatically."
E06-1049,A Machine Learning Approach to Extract Temporal Information from Texts in {S}wedish and Generate Animated 3{D} Scenes,2006,17,9,3,1,50099,anders berglund,11th Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,"Carsim is a program that automatically converts narratives into 3D scenes. Carsim considers authentic texts describing road accidents, generally collected from web sites of Swedish newspapers or transcribed from hand-written accounts by victims of accidents. One of the programxe2x80x99s key features is that it animates the generated scene to visualize events. To create a consistent animation, Carsim extracts the participants mentioned in a text and identifies what they do. In this paper, we focus on the extraction of temporal relations between actions. We first describe how we detect time expressions and events. We then present a machine learning technique to order the sequence of events identified in the narratives. We finally report the results we obtained. (Less)"
W05-0624,Sparse {B}ayesian Classification of Predicate Arguments,2005,8,12,2,1,2644,richard johansson,Proceedings of the Ninth Conference on Computational Natural Language Learning ({C}o{NLL}-2005),0,"We present an application of Sparse Bayesian Learning to the task of semantic role labeling, and we demonstrate that this method produces smaller classifiers than the popular Support Vector approach.n n We describe the classification strategy and the features used by the classifier. In particular, the contribution of six parse tree path features is investigated."
W05-0209,{D}irekt {P}rofil: A System for Evaluating Texts of Second Language Learners of {F}rench Based on Developmental Sequences,2005,4,11,2,1,48884,jonas granfeldt,Proceedings of the Second Workshop on Building Educational Applications Using {NLP},0,"Direkt Profil is an automatic analyzer of texts written in French as a second language. Its objective is to produce an evaluation of the developmental stage of students under the form of a grammatical learner profile. Direkt Profil carries out a sentence analysis based on developmental sequences, i.e. local morphosyntactic phenomena linked to a development in the acquisition of French.n n The paper presents the corpus that we use to develop the system and briefly, the developmental sequences. Then, it describes the annotation that we have defined, the parser, and the user interface. We conclude by the results obtained so far: on the test corpus the systems obtains a recall of 83% and a precision of 83%."
2005.jeptalnrecital-long.12,Direkt Profil : un syst{\\`e}me d{'}{\\'e}valuation de textes d{'}{\\'e}l{\\`e}ves de fran{\\c{c}}ais langue {\\'e}trang{\\`e}re fond{\\'e} sur les itin{\\'e}raires d{'}acquisition,2005,-1,-1,2,1,48884,jonas granfeldt,Actes de la 12{\\`e}me conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Articles longs,0,"Direkt Profil est un analyseur automatique de textes {\'e}crits en fran{\c{c}}ais comme langue {\'e}trang{\`e}re. Son but est de produire une {\'e}valuation du stade de langue des {\'e}l{\`e}ves sous la forme d{'}un profil d{'}apprenant. Direkt Profil r{\'e}alise une analyse des phrases fond{\'e}e sur des itin{\'e}raires d{'}acquisition, i.e. des ph{\'e}nom{\`e}nes morphosyntaxiques locaux li{\'e}s {\`a} un d{\'e}veloppement dans l{'}apprentissage du fran{\c{c}}ais. L{'}article pr{\'e}sente les corpus que nous traitons et d{'}une fa{\c{c}}on sommaire les itin{\'e}raires d{'}acquisition. Il d{\'e}crit ensuite l{'}annotation que nous avons d{\'e}finie, le moteur d{'}analyse syntaxique et l{'}interface utilisateur. Nous concluons par les r{\'e}sultats obtenus jusqu{'}ici : sur le corpus de test, le syst{\`e}me obtient un rappel de 83{\%} et une pr{\'e}cision de 83{\%}."
W04-0908,{C}arsim: A system to visualize written road accident reports as animated 3{D} scenes,2004,17,22,4,1,2644,richard johansson,Proceedings of the 2nd Workshop on Text Meaning and Interpretation,0,This paper describes a system to create animated 3D scenes of car accidents from reports written in Swedish. The system has been developed using news reports of varying size and complexity. The text-to-scene conversion process consists of two stages. An information extraction module creates a structured representation of the accident and a visual simulator generates and animates the scene.n n We first describe the overall structure of the text-to-scene conversion and the structure of the representation. We then explain the information extraction and visualization modules. We show snapshots of the car animation output and we conclude with the results we obtained.
dutoit-etal-2004-integral,"The Integral Dictionary: An Ontological Resource for the Semantic Web: Integration of {E}uro{W}ord{N}et, Balkanet, {TID}, and {SUMO}",2004,5,0,2,0.740741,50131,dominique dutoit,Proceedings of the Fourth International Conference on Language Resources and Evaluation ({LREC}{'}04),0,"The semantic organization of the web is one the major challenges for the future of the Internet. This important task may be based on the development of new approaches, taking the risk of reinventing the wheel, or may consider the previous efforts and successes, offering the opportunity to move research to market. This paper is a technical study that examines issues related to the latter possibility. We will first consider the structure of the Suggested Upper Merged Ontology (SUMO), which is a general proposal on the semantic web. We will then outline the challenges and possible strategies to integrate two existing ontologies, Wordnet for the English language and the Integral Dictionary for French (TID), to SUMO. Then, we will discuss the motivation of the mappings."
W03-2506,{HMS}: A Predictive Text Entry Method Using Bigrams,2003,8,32,3,0,52591,jon hasselgren,Proceedings of the 2003 {EACL} Workshop on Language Modeling for Text Entry Methods,0,"Due to the emergence of SMS messages, the significance of effective text entry on limited-size keyboards has increased. In this paper, we describe and discuss a new method to enter text more efficiently using a mobile telephone keyboard. This method, which we called HMS, predicts words from a sequence of keystrokes using a dictionary and a function combining bigram frequencies and word length.n n We implemented the HMS text entry method on a software-simulated mobile telephone keyboard and we compared it to a widely available commercial system. We trained the language model on a corpus of Swedish news and we evaluated the method. Although the training corpus does not reflect the language used in SMS messages, the results show a decrease by 7 to 13 percent in the number of keystrokes needed to enter a text. These figures are very encouraging even though the implementation can be optimized in several ways. The HMS text entry method can easily be transferred to other languages."
E03-2002,{C}ar{S}im: An Automatic 3{D} Text-to-Scene Conversion System Applied to Road Accident Reports,2003,9,23,4,0,52923,ola aakerberg,Demonstrations,0,CarSim is an automatic text-to-scene conversion system. It analyzes written descriptions of car accidents and synthesizes 3D scenes of them. The conversion process consists of two stages. An information extraction module creates a tabular description of the accident and a visual simulator generates and animates the scene.We implemented a first version of CarSim that considered a corpus of texts in French. We redesigned its linguistic modules and its interface and we applied it to texts in English from the National Transportation Safety Board in the United States.
dutoit-nugues-2002-algorithm,An Algorithm to Find Words from Definitions,2002,8,0,2,0.740741,50131,dominique dutoit,Proceedings of the Third International Conference on Language Resources and Evaluation ({LREC}{'}02),0,"This paper presents a system to find automatically words from a definition or a paraphrase. The system uses a lexical database of French words that is comparable in its size to WordNet and an algorithm that evaluates distances in the semantic graph between hypernyms and hyponyms of the words in the definition. The paper first outlines the structure of the lexical network on which the method is based. It then describes the algorithm. Finally, it concludes with examples of results we have obtained."
W01-1301,Generating A 3{D} Simulation Of A Car Accident From A Written Description In Natural Language: The {C}ar{S}im System,2001,9,48,4,0,53800,sylvain dupuy,Proceedings of the {ACL} 2001 Workshop on Temporal and Spatial Information Processing,0,"This paper describes a prototype system to visualize and animate 3D scenes from car accident reports, written in French. The problem of generating such a 3D simulation can be divided into two subtasks: the linguistic analysis and the virtual scene generation. As a means of communication between these two modules, we first designed a template formalism to represent a written accident report. The CARSIM system first processes written reports, gathers relevant information, and converts it into a formal description. Then, it creates the corresponding 3D scene and animates the vehicles."
