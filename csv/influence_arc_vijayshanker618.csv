1995.iwpt-1.30,J94-1004,0,0.104021,"Missing"
2000.iwpt-1.9,E99-1025,1,0.898278,"Missing"
2000.iwpt-1.9,P96-1025,0,0.138581,"Missing"
2000.iwpt-1.9,P97-1003,0,0.248647,"Missing"
2000.iwpt-1.9,J93-2004,0,0.0250126,"Missing"
2000.iwpt-1.9,C92-2065,0,0.12632,"Missing"
2000.iwpt-1.9,C88-2121,0,0.644335,"Missing"
2000.iwpt-1.9,1997.iwpt-1.22,0,0.416715,"Missing"
2021.bionlp-1.1,N19-1423,0,0.514117,"chieved the state of the art results in recent years (He et al., 2020; Chen et al., 2020). Despite its advancement, contrastive learning has not been well studied in biomedical natural language processing (BioNLP), especially for relation extraction (RE) tasks. One obstacle lies in the discrete characteristics of text data. Compared to computer vision, it is more challenging to design a general and efficient data augmentation method to construct positive pairs. Instead, there have been significant advances in the development of pretrained language models to facilitate downstream BioNLP tasks (Devlin et al., 2019; Radford et al., 2019; Peng et al., 2019). Therefore, leveraging contrastive learning in the large pre-trained language models to learn more general representation for RE tasks remains unexplored. To bridge this gap, this paper presents an innovative method of contrastive pre-training to improve the language model representation for biomedical relation extraction. As the main difference from the existing contrastive learning framework, we augment the datasets for RE tasks by randomly changing the words that do not affect the relation expression. Here, we hypothesize that the shortest dependen"
2021.bionlp-1.1,2020.emnlp-main.298,0,0.0298239,"From the disentangled representation’s perspective, it should encode as much information as possible. In this work, we will show contrastive learning can improve the invariant aspect of the representation. In the natural language processing (NLP) field, several works have utilized the contrastive learning technique. Fang et al. (2020) propose a pre-trained language representation model (CERT) using contrastive learning at the sentence level to benefit the language understanding tasks. Klein and Nabi (2020) employ contrastive self-supervised learning to solve the commonsense reasoning problem. Peng et al. (2020) propose a self-supervised pretraining framework for relation extraction to explore the encoded information for the textual context and entity type. Compared with the previous works, we employ different data augmentation techniques and utilize data from external knowledge bases in contrastive learning to improve the model for relation extraction tasks. Related Work Relation extraction is usually seen as a classification problem when the entity mentions are given in the text. Many different methods have been proposed to solve the relation extraction problem (Culotta and Sorensen, 2004; Sierra e"
2021.bionlp-1.1,2020.acl-main.671,0,0.0269919,"ncepts and the invariance to small and local changes are concerned in the abstraction and invariant property. From the disentangled representation’s perspective, it should encode as much information as possible. In this work, we will show contrastive learning can improve the invariant aspect of the representation. In the natural language processing (NLP) field, several works have utilized the contrastive learning technique. Fang et al. (2020) propose a pre-trained language representation model (CERT) using contrastive learning at the sentence level to benefit the language understanding tasks. Klein and Nabi (2020) employ contrastive self-supervised learning to solve the commonsense reasoning problem. Peng et al. (2020) propose a self-supervised pretraining framework for relation extraction to explore the encoded information for the textual context and entity type. Compared with the previous works, we employ different data augmentation techniques and utilize data from external knowledge bases in contrastive learning to improve the model for relation extraction tasks. Related Work Relation extraction is usually seen as a classification problem when the entity mentions are given in the text. Many differen"
2021.bionlp-1.1,W19-5006,1,0.887188,"ent years (He et al., 2020; Chen et al., 2020). Despite its advancement, contrastive learning has not been well studied in biomedical natural language processing (BioNLP), especially for relation extraction (RE) tasks. One obstacle lies in the discrete characteristics of text data. Compared to computer vision, it is more challenging to design a general and efficient data augmentation method to construct positive pairs. Instead, there have been significant advances in the development of pretrained language models to facilitate downstream BioNLP tasks (Devlin et al., 2019; Radford et al., 2019; Peng et al., 2019). Therefore, leveraging contrastive learning in the large pre-trained language models to learn more general representation for RE tasks remains unexplored. To bridge this gap, this paper presents an innovative method of contrastive pre-training to improve the language model representation for biomedical relation extraction. As the main difference from the existing contrastive learning framework, we augment the datasets for RE tasks by randomly changing the words that do not affect the relation expression. Here, we hypothesize that the shortest dependency path (SDP) between two entities (Bunesc"
2021.bionlp-1.1,N18-1202,0,0.0426206,"s, we employ different data augmentation techniques and utilize data from external knowledge bases in contrastive learning to improve the model for relation extraction tasks. Related Work Relation extraction is usually seen as a classification problem when the entity mentions are given in the text. Many different methods have been proposed to solve the relation extraction problem (Culotta and Sorensen, 2004; Sierra et al., 2008; Sahu and Anand, 2018; Zhang et al., 2019; Su et al., 2019). However, the language model methods redefine this field with their superior performance (Dai and Le, 2015; Peters et al., 2018; Devlin et al., 2019; Radford et al., 2019; Su and VijayShanker, 2020). Among all the language models, BERT (Devlin et al., 2019) –a language representation model based on bidirectional Transformer (Vaswani et al., 2017), attracts lots of attention in The history of contrastive representation learning can be traced back to (Hadsell et al., 2006), in which the authors explore the method of representation learning that similar inputs are mapped to nearby points in the representation space. Recently, with the development of data augmentation techniques, deep neural network architectures, contras"
2021.bionlp-1.1,D16-1011,0,0.21333,"s our method on three widely studied RE tasks in the biomedical domain: the chemical-protein interactions (ChemProt) (Krallinger et al., 2017), the drug-drug interactions (DDI) (Herrero-Zazo et al., 2013), and the protein-protein interactions (PPI) (Krallinger et al., 2008). The experimental results show that our method boosts the BERT model performance and achieves state-of-the-art results on all three tasks. Interest has also grown in designing interpretable BioNLP models that are both plausible (accurate) and rely on a specific part of the input (faithful rationales) (DeYoung et al., 2020; Lei et al., 2016). Here rationale is defined as the supporting evidence in the inputs for the model to make correct predictions. In this direction, we propose a new metric, ”prediction shift”, to measure the sensitivity degree to which the small changes (out of the SDP) of the inputs will make model change its predictions. We show that the contrastively pre-trained model is more robust than the original model, suggesting that our model is more likely to make predictions based on the rationales of the inputs. In sum, the contribution of this work is fourfold. (1) We propose a new method that utilizes contrastiv"
2021.bionlp-1.1,P09-1113,0,0.0848435,"n difference from the existing contrastive learning framework, we augment the datasets for RE tasks by randomly changing the words that do not affect the relation expression. Here, we hypothesize that the shortest dependency path (SDP) between two entities (Bunescu and Mooney, 2005) captures the required knowledge for the relation expression. We hence keep words on SDP fixed during the data augmentation. In addition, we utilize external knowledge bases to construct more data to make the learned representation generalize better, which is a method that is frequently used in distant supervision (Mintz et al., 2009; Peng et al., 2016). To verify the effectiveness of the proposed method, we use the transformer-based BERT model as a backbone (Devlin et al., 2019) and evaluate Contrastive learning has been used to learn a high-quality representation of the image in computer vision. However, contrastive learning is not widely utilized in natural language processing due to the lack of a general method of data augmentation for text data. In this work, we explore the method of employing contrastive learning to improve the text representation from the BERT model for relation extraction. The key knob of our fram"
2021.bionlp-1.1,D19-1670,0,0.0223156,"the sentence s with the relation type r, we keep e1 and e2 in the sentence and retain the relation expression between e1 and e2 in the augmented views. Specifically, we propose a data augmentation method utilizing the shortest dependency path (SDP) between the two entities in the text. We hypothesize that the shortest dependency path captures the required information to assert the relationship of the two entities (Bunescu and Mooney, 2005). Therefore we fix the shortest dependency path, and randomly change the other tokens in the text to generate the augmented data. This idea is inspired by (Wei and Zou, 2019), which Methodology The framework of contrastive learning Our goal is to learn a text representation by maximizing agreement between inputs from positive pairs via a contrastive loss in the latent space and the learned representation can then be used for relation extraction. Figure 1 shows our framework of contrastive learning. Given a sentence s = w1 , ...wn , we first produce two augmented views (a positive pair) v 0 = w1 , ..., wi0 , ...wn and v 00 = w1 ..., wj0 , ...wn (i 6= j) from s by applying text augmentation technique (Section 3.1.1). Our framework then uses one neural network to 3 O"
C86-1048,P85-1011,1,0.821545,"atural languages, were developed independently. TAG&apos;s deal with a set of elementary trees which are composed by means of an operation called a d j o i n i n g . HG&apos;s are like Context-free Grammars, except for the fact t h a t besides concatenation of strings, s t r i n g w r a p p i n g operations are permitted. TAG&apos;s were first introduced in 1975 by Joshi, Levy and Wakahashi [3]. Joshi [2] investigated some formal and linguistic properties of TAG&apos;s with local constraints. The formulation of local constraints was then modified and formal properties were investigated by Vijay-Shanker and Joshi [9]. The linguistic properties were studied in detail by Kroeh and Joshi [5]. HG&apos;s were first introduced by Pollard [6] in 1983 and their formal properties were investigated by Roach [7]. It was observed t h a t the two systems seemed to possess similar generative power and since they also appear to have the same closure properties [7,9] as well as similar parsing algorithn~ [6,9] a significant amount of indirect evidence existed to suggest t h a t they were formally equivalent. In the present paper, we will attempt to provide a characterization of the formal relationship between HG&apos;s and TAG&apos;s."
C86-1048,P86-1011,1,0.376533,"Missing"
C88-2147,P88-1032,0,\N,Missing
C88-2147,P85-1018,0,\N,Missing
C88-2147,P86-1038,0,\N,Missing
C88-2147,P84-1027,0,\N,Missing
C92-1034,P92-1010,1,0.778242,"fication. We allow for overwriting inherited attributes by assuming that the local specification has a higher precedence. Figure 2 shows a fragment of the hierarchy for verbs. The lexicon associates lexicaJ items with a set of classes. Entries specify relationships and properties of sets of nodes in trees which will be associated with the lexical items. The framework for describing the tree that will be associated with the lexicai item is very similar to unification based tree-adjoining grammar (Vijay-Shanker, 1992) in which the trees are described with partial descriptions of their topology (Rogers and Vijay-Shanker, 1992) using statements of domination Paoc. OFCOLING-92, NANTES,AUO. 23-28, 1992 different classes e.g. in NP-IOBJ or used in tile syntactic rides such as for Wll-movement. • linear precedence (LP) statements which define precedence an&apos;long nodes within the framework of I D / L P TAG proposed by Jo~hi (1987). give d~at~ ~t Figure 2: 1,¥agment of the Lexicon • anchm; anchor --- xspecifies that the node x is tile aalchor node of the tree being described. and linear precedence. We do not discuss tile description language in which these trees are stated. Instead, we will pictorially represent these part"
C92-1034,C88-2121,1,0.902782,"ssociated elementary trees of extended domain of locality. Furthermore, the lexical and syntactic rules can be used to derive new elementary trees from the default structures specified in the hierarchical lexicon. In the envisaged scheme, tbe use of a hierarchical lexicon and of lexical and syntactic rules for lexicalized tree-adjoining grammars will capture important linguistic generalizations and also allows for a space efficient representation of the grammar. This will allow for easy maintenance and facilitate updates to the grammar. 1 Motivations Lexicalized tree-adjoining grammar (LTAG) (Schabes et al., 1988; Schabes, 1990) is a tree-rewriting formalism used for specifying the syntax of natural languages. It combines elementary lexical trees with two operations, adjoining and substitution. In a LTAG, lexical itenm are associated with complex syntactic structures (in the form of trees) that define the various phrase structures they can participate in. LTAG allows for unique syntactic and semantic properties: • The domain of locality in LTAG is larger than for other formalisms, and • Most syntactic dependencies (such as filler-gap, verb-subject, verb-objects) and some semantic *The first author is"
C92-1034,C88-1002,0,0.0359946,"Missing"
C92-1034,W90-0203,0,0.0252095,"987) and Pollard and Sag (1987) use a hierarchical lexicon aud rules for implementing Headdriven Phrase Structure Grammars. Shieber (1986) proposed the use of default inheritance combined with templates and of transformation rules in the PATRII system for organizing a unification based grammar. Lexical redundancy rules have been used in LFG (Bresnan and Kaplan, 1983) to capture relations among lexicai items. Gazdar et al. (1985) proposed the use of meta-rules for expressing transformational relationships. There has been suggestions for compacting the size of a tree-adjoining grammar lexicons (Becker, 1990; ttabert, 1991). However, they only partially solve the problem since they fail to combine in a uniform way a compact representation of the lexicon and, at the same time, of their associated elementary trees. In this paper, we present a scheme to efficiently represent a LTAG and illustrate this scheme by examples. We examine the information that needs to be associated with the classes in our hierarchical organization in order that we can represent the elementary trees of a LTAG. Our main concern in this paper is the proposal for organizing a LTAG. In order to be concrete, we consider the repr"
C92-1034,C90-3045,1,0.844589,"Missing"
C92-1034,J92-4004,1,\N,Missing
C92-1034,W90-0102,1,\N,Missing
C98-2183,W95-0101,0,0.255952,"system replaces variables in the templates with appropriate values to generate rules. l~br example, the following template can be ~This condition is true only for the first utterance of a dialogue. 1151 instantiated with w = &quot; n o &quot; , X = S U G G E S T , and Y_=REJECT to produce the last rule in Figure 1. IF utterance lA contains the word w AND the tag on the utterance preceding ~l is X__ T H E N change u&apos;s tag to Y__ We have observed that TBL has a number of attractive characteristics for the task of computing dialogue acts. TBL has been effective on a similar 2 task, Part-of-Speech Tagging (Brill, 1995a). Also, TBL&apos;s rules are relatively intuitive, so a h u m a n (:an analyze tile rules to determine what the system has learned and perhaps develop a theory. TBL is very good at discarding irrelevant rules, because the effect of irrelevant rules on a training cortms is essentially random, resulting in low improvement scores. In addition, our implementation can accommodate a wide variety of different types of features, including set-valued features, features .that consider the context of surrounding utterances, and features that can take distant context into account. These and other attractive"
C98-2183,J86-3001,0,0.109164,"rtms is essentially random, resulting in low improvement scores. In addition, our implementation can accommodate a wide variety of different types of features, including set-valued features, features .that consider the context of surrounding utterances, and features that can take distant context into account. These and other attractive characteristics of TBL are discussed further in Samuel et al. (1998b). Dialogue Act Tagging To address a significant concern in machine learning, called the sparse d a t a problem, we nmst select an appropriate set of features. Researchers in discourse, such as Grosz and Sidner (1986), Lambert (1993), Hirschberg and Litman (1993), Chen (1995), Andernach (1996), Samuel (1996), and Chu-Carroll (1998) have suggested several features that might be relevant for the task of computing dialogue acts. Our system can consider the following features of an utterance: 1) tile cue phrases a in the utterance; 2) the word n-grams a in the utterance; 3) the dialogue act cues 3 in the utterance; 4) the entire utterance for one-, two-, or three-word utterances; 5) speaker information 4 for the utter2The part-of-speech tag of a word is dependent on the word&apos;s internal features and on the surr"
C98-2183,J93-3003,0,0.377499,"low improvement scores. In addition, our implementation can accommodate a wide variety of different types of features, including set-valued features, features .that consider the context of surrounding utterances, and features that can take distant context into account. These and other attractive characteristics of TBL are discussed further in Samuel et al. (1998b). Dialogue Act Tagging To address a significant concern in machine learning, called the sparse d a t a problem, we nmst select an appropriate set of features. Researchers in discourse, such as Grosz and Sidner (1986), Lambert (1993), Hirschberg and Litman (1993), Chen (1995), Andernach (1996), Samuel (1996), and Chu-Carroll (1998) have suggested several features that might be relevant for the task of computing dialogue acts. Our system can consider the following features of an utterance: 1) tile cue phrases a in the utterance; 2) the word n-grams a in the utterance; 3) the dialogue act cues 3 in the utterance; 4) the entire utterance for one-, two-, or three-word utterances; 5) speaker information 4 for the utter2The part-of-speech tag of a word is dependent on the word&apos;s internal features and on the surrounding words; similarly, the dialogue act of"
C98-2183,E95-1037,0,0.0140816,"ition, our implementation can accommodate a wide variety of different types of features, including set-valued features, features .that consider the context of surrounding utterances, and features that can take distant context into account. These and other attractive characteristics of TBL are discussed further in Samuel et al. (1998b). Dialogue Act Tagging To address a significant concern in machine learning, called the sparse d a t a problem, we nmst select an appropriate set of features. Researchers in discourse, such as Grosz and Sidner (1986), Lambert (1993), Hirschberg and Litman (1993), Chen (1995), Andernach (1996), Samuel (1996), and Chu-Carroll (1998) have suggested several features that might be relevant for the task of computing dialogue acts. Our system can consider the following features of an utterance: 1) tile cue phrases a in the utterance; 2) the word n-grams a in the utterance; 3) the dialogue act cues 3 in the utterance; 4) the entire utterance for one-, two-, or three-word utterances; 5) speaker information 4 for the utter2The part-of-speech tag of a word is dependent on the word&apos;s internal features and on the surrounding words; similarly, the dialogue act of an utterance"
C98-2183,P97-1011,0,0.038268,"Missing"
C98-2183,W97-0320,0,0.00990377,", to address limitations of TBL, we introduce a Monte Carlo strategy for training efficiently and a committee method for computing confidence measures. These ideas are combined in our working implementation, which labels held-out data as accurately as any other reported system for the dialogue act tagging task. Introduction Although machine learning approaches have achieved success in many areas of Natural Language Processing, researchers have only recently begun to investigate applying machine learning methods to discourse-level problems (Reithinger and Klesen, 1997; Di Eugenio et al., 1997; Wiebe et al., 1997; Andernach, 1996; Litman, 1994). An important task in discourse understanding is to interpret an utterance&apos;s dialogue act, which is a concise abstraction of a speaker&apos;s intention, such as SUGGEST and ACCEPT. Recognizing dialogue acts is critical for discourse-level understanding and can also be useful for other applications, such as resolving ambiguity in speech recognition. However, tomputing dialogue acts is a challenging task, because often a dialogue act cannot be directly inferred from a literal interpretation of an utterance. We have investigated applying Transformation-Based Learning ("
C98-2183,J95-4004,0,\N,Missing
D07-1118,C04-1080,0,0.0392532,"n biomedical text has been noted (Lease and Charniak, 2004, Smith et al, 2005). Smith et al. (2005) use this observation in the design of their POS tagger, MedPost, by building a Markov model with a lexicon containing the 10,000 most frequent words from Medline, and using annotated text from the Biomedical text for supervised training. There are many unsupervised approaches to POS tagging. We focus now on those that are most closely related to our work and contain ideas that have influenced this work. There have been many uses of EM training to build HMM taggers (Kupiec, 1992; Elworthy, 1994; Banko and Moore, 2004; Wang and Schuurmans, 2005). Banko and Moore (2004) achieved better accuracy by restricting the set of possible tags that are associated with words. By eliminating possibilities that may appear rarely with a word, they reduce the chances of unsupervised training spiraling along an unlikely path. We believe by using our approach we considerably reduce the set of tags to what is appropriate for each word. Further, we too remove any tag associated with low probability by kNN method. Usually these tags are noise introduced by some inappropriate exemplar. Wang and Schuurman (2005) suggest that EM"
D07-1118,A00-1031,0,0.180234,"erence to the new domain, we start from smoothed WSJ bigram probabilities. If we started from unsmoothed WSJ bigram probabilities, then EM would not allow us to account for transitions that are not observed in the WSJ corpus. For example, in scientific text, transition from RRB (the right round bracket) to VBG may be possible, while it does not occur in the WSJ corpus. Hence, we smooth the WSJ bigram probabilities with WSJ unigram probabilities. We compute smoothed initial bigram probabilities as P(t |t’) = λ PWSJ(t |t’) + (1-λ) PWSJ(t), where λ=0.9. We felt employing techniques suggested in (Brants, 2000) gave too high a preference for unigram probabilities. The initial emit probability is obtained from the domain text Text-Lex. The process is described in the next section. This information is derived purely from suffix and suffix distribution, or from orthographic information and does not account for the actual context of occurrences in the domain text. We take this suffix-based (and orthographic-based) emit probabilities as reasonable initial lexical probabilities. EM training will adjust them as necessary. We made one minor modification to the standard forward-backward EM algorithm. We damp"
D07-1118,J95-4004,0,0.316377,"Missing"
D07-1118,P00-1035,0,0.592092,"uffix information be used online during tagging, but also the presence or absence of morphologically related words can provide considerable information to pre-build a lexicon that associates possible tags with words. Consider the example of the word “broaden”. While the suffix “en” may be utilized to predict the likelihood of verbal tags (VB and VBP) for the word during tagging, if we were to build a lexicon offline, the existence of the words “broadened”, “broadening”, “broadens” and “broad” give further evidence to treat “broaden” as a verb. This type of information has been used before in (Cucerzan and Yarowsky, 2000). In the above example, the presence or absence of words with the suffix morphemes suggests POS tag information in two ways: 1) The presence of a suffix morpheme in a word suggests a POS tag or a small set of POS tags for the word. This is the type of information most taggers use to predict tags for unknown words during the tagging process; 2) The presence of the morpheme can also indicate possible tags for the words it attaches to. For example, the derivational morpheme “ment” indicates “government” is likely to be an NN and also that the word it attaches to, “govern” is likely to be a verb."
D07-1118,A92-1018,0,0.288634,"Missing"
D07-1118,A94-1009,0,0.225552,"new transitional probability be given by P(t |t’) = λ PNEW(t |t’) + (1-λ) POLD(t |t’) where POLD represents the transitional probability in the previous iteration and PNEW represents the probability by standard use of forward-backward algorithm. We use a damping factor of 0.5 for both transitional and emit probabilities. For the emit probabilities, this has the effect of moderating POS preferences derived from the training data and preserving words and POSes from the lexicon for use in the test set. Even with the damping factor, EM learning followed the pattern of ‘Early Maximum’ described by Elworthy (1994), where with good initial estimates EM learning only improves accuracy for a few iterations. For our EM training, we fixed iteration 2 as our ‘best’ EM trained model. 4 Development of the Lexicon and Initial Probabilities As noted earlier, we use a domain text, Text-Lex, to develop the initial lexical probabilities for the HMM. The essential process is as follows. Let a word w appear a sufficient number of times in Text-Lex (at least 5 times). We look in Text-Lex for related words in order to assign a feature vector with this word. Each feature is written as –x+y, where x and y represent suffi"
D07-1118,W04-3111,0,0.0827833,"Missing"
D07-1118,I05-1006,0,0.0588457,"ideas. Section 7 has some concluding remarks. 2 Basic Methodology Inadequate treatment of domain-specific vocabulary is often the primary cause in the degradation of performance when a tagger trained in one genre of text is ported to a new domain. The significance of out-of-vocabulary words has been noted in reduced accuracy of NLP components in the Biology 1103 Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational c Natural Language Learning, pp. 1103–1111, Prague, June 2007. 2007 Association for Computational Linguistics domain (e.g., Lease and Charniak, 2005; Smith et al. 2004). The handling of domain-specific vocabulary is the focus of our approach. It is quite common to use suffix information in the prediction of POS tags for occurrences of new words. However, its effectiveness may be limited in English, which is not a highly inflected language. However, even for English, we find that not only can suffix information be used online during tagging, but also the presence or absence of morphologically related words can provide considerable information to pre-build a lexicon that associates possible tags with words. Consider the example of the word"
D07-1118,J93-2004,0,0.0295483,"Missing"
E93-1045,P85-1011,0,0.208801,"Missing"
E93-1045,P88-1032,0,0.0936883,"Missing"
E93-1045,J93-4002,1,\N,Missing
E93-1045,P89-1018,0,\N,Missing
J01-1004,E91-1005,1,0.838818,"Missing"
J01-1004,1995.iwpt-1.6,1,0.81509,"Missing"
J01-1004,W98-0107,0,0.223773,"Missing"
J01-1004,E99-1029,1,0.906601,"Missing"
J01-1004,W00-2007,1,0.862891,"Missing"
J01-1004,P97-1003,0,0.100262,"Missing"
J01-1004,W98-0116,0,0.0543715,"Missing"
J01-1004,C96-1091,0,0.0632209,"Missing"
J01-1004,W98-0117,0,0.0362597,"Missing"
J01-1004,P95-1013,1,0.883932,"Missing"
J01-1004,W98-0124,0,0.0468347,"Missing"
J01-1004,P83-1020,0,0.179008,"Missing"
J01-1004,W98-0135,1,0.911043,"Missing"
J01-1004,P95-1021,1,0.931775,"Missing"
J01-1004,P92-1010,1,0.861998,"2, we give some formal definitions and in Section 3 discuss some of the formal properties of DSG. In Section 4, we present analyses in DSG for various linguistic constructions in several languages, and compare them to the corresponding LTAG analyses. In Section 5, we discuss the particular problem of modeling syntactic dependency. We conclude with a discussion of some related work and summary. 2. D e f i n i t i o n of D S G D-trees are the primitive elements of a DSG. D-trees are descriptions of trees, in particular, certain types of expressions in a tree description language such as that of Rogers and Vijay-Shanker (1992). In this section we define tree descriptions and substitution of tree descriptions (Section 2.1) and d-trees (Section 2.2) together with some associated terminology and the graphical representation (Section 2.3). We then define d-tree substitution grammars, along with derivations of d-tree substitution grammars (Section 2.4) and languages generated by these grammars (Section 2.5), and close with an informal discussion of path constraints (Section 2.6). 2.1 Tree D e s c r i p t i o n s and S u b s t i t u t i o n In the following, we are interested in a tree description language that provides"
J01-1004,J94-1004,0,0.260946,"Missing"
J01-1004,J92-4004,1,0.961676,"the tree into which adjunction occurs. In LTAG, the lexicalized elementary objects are defined in such a w a y that the structural relationships between the anchor and each of its dependents change during the course of a derivation through the operation of adjunction, as just illustrated. This approach is not the only possibility. An alternative would be to define the relationships between the nodes of the elementary objects in such a w a y that these relationships hold throughout the derivation, regardless of how the derivation proceeds. This perspective on the LTAG formalism was explored in Vijay-Shanker (1992) where, following the principles of d-theory parsing (Marcus, Hindle, and Fleck 1983), LTAG was seen as a system manipulating descriptions of trees rather than as a tree 2 The same analysis holds for wh-movement, but w e use topicalization as an example in order to avoid the superficial complication of the auxiliary n e e d e d in English questions. Sometimes, topicalized sentences s o u n d s o m e w h a t less natural than the corresponding wh-questions, which are always structurally equivalent. 88 Rambow, Vijay-Shanker, and Weir fl&apos;: NP I Peter S NP S D-Tree Substitution Grammars I I I S yo"
J01-1004,1995.iwpt-1.30,1,0.873684,"Missing"
J01-1004,P94-1036,1,\N,Missing
J90-1002,P84-1008,0,0.588208,"putationalLinguistics Volume 16, Number 1, March 1990 only a partial description of a feature structure. This property is precisely formulated in Section 2. Several researchers have expressed a need for extending this logic to include the operators of negation and implication. These two are related in that, in most logical systems, it is possible to use one to define the other (in the presence of a disjunction operator). In this paper, we shall concentrate on the problem of extending the logic to include negation, while also showing that it yields a satisfactory interpretation of implication. Karttunen (1984), for instance, provides examples of feature structures in which a negation operator might be useful. For instance, the most natural way to represent the number and person attributes of a verb such as sleep would be to say that it is not third person singular, rather than expressing it as a disjunction of the other possibilities. We express this agreement constraint by the following formula: agreement : -1 (person : third A number : singular) (1) Pereira (1987) provides the following example formula that expresses the semantic constraint that the subject and object of a clause cannot be corefe"
J90-1002,C88-1063,0,0.0523636,"classical semantics for negation in a RoundsKasper-like framework. While such approaches are appropriate under one view of feature structures, they are not satisfactory from the viewpoint of feature structures seen as partial descriptions. This is because the crucial property of monotonicity is lost, as can be seen from the following example: Example 1. A = [person:second] [person:second] B = [number : singularJ 4) = -q(person : second/~ number : singular) As can easily be seen, by the classical semantics, A ~ 4) and A E B, but B ~ 4). Computational Linguistics Volume 16, Number 1, March 1990 Kasper (1988a) discusses an interpretation of negation and implication in an implementation of Functional Unification G r a m m a r that is extended to include conditionals. Kasper&apos;s semantics is classical, but his unification procedure uses notions similar to those of three-valued logic. 3 Kasper also localized the effects of negation by disallowing path expressions within the scope of a negation. This restriction may not be linguistically warranted as can be seen from Pereira&apos;s formula example in Section 1. 3.2 INTUITIONISTICLOGIC Moshier and Rounds (1987) described an extension of the Rounds-Kasper log"
J90-1002,1993.eamt-1.1,0,0.0716862,"e an extension to the feature structure formalism. Various interpretations have been suggested that define a semantics for these operators (see Section 3), but none has gained universal acceptance. Pereira (1987) set forth certain properties that any such interpretation should satisfy. We suggested that three-valued logic provides us with a framework appropriate for defining the semantics of a feature description logic (which we will call FDL) that includes a negation operator (Dawar and Vijay-Shanker 1989). We also showed that the three-valued framework (based on Kleene&apos;s three-valued logic; Kleene 1952) is powerful enough to express most of the existing definitions of negation and implication. It is therefore possible to compare these different approaches. We also presented one particular three-valued interpretation for FDL, motivated by the approach to negation given by Karttunen (1984), that meets the conditions stated by Pereira. In the present work, we give an exposition of these results, and we also examine another three-valued interpretation for FDL, obtained by using a modified notion of the feature structures that serve as models. This new interpretation, while preserving the desirab"
J90-1002,P89-1003,1,\N,Missing
J90-1002,P84-1075,0,\N,Missing
J90-1002,P86-1038,0,\N,Missing
J90-1002,P88-1029,0,\N,Missing
J92-4004,C90-2029,0,0.280563,"trees, but the one obtained by equating the pairs of top and bottom quasi-nodes is not one of them. Obvi493 Computational Linguistics Volume 18, Number 4 ously this should be the case, since incompatible assertions about a pair of quasi-nodes indicates that they do refer to different nodes (and hence specify OA constraints). 2.8 Using One (Rather than Two) Feature Structure A question arises whether (as in standard CFG-based unification grammars) one could associate just one (rather than two) feature structure per node, i.e., whether it is necessary to consider pairs of quasi-nodes. In fact, Harbusch (1990) defined such a treatment of TAG where only one feature structure is associated with each node. One could argue that it may be inefficient (for instance, when implementing the formalism as defined here) to start with the pairs of quasi-nodes and then try to merge them eventually when possible. Strategies to improve processing may be considered particularly if we believe that, on an average, a relatively small proportion of potential sites will become actual targets of adjunctions during a derivation of a sentence. Then (to improve performance) we could specify that by defimlt the associated pa"
J92-4004,P89-1027,1,0.830848,"Missing"
J92-4004,C88-1060,0,0.10048,"indeed the appropriate structures to consider. For instance, no treatment of topicalization can justify the identification of the nodes referred to by s2 and s3. Thus, a pair of quasi-nodes is appropriate for their representation. As in the case of vpl and vp2 quasi-nodes in ~4, one can only claim that s2 quasi-node dominates s3 quasi-node (again, by domination, we also allow for the possibility that s2 and s3 can refer to the same node). It may be interesting to contrast this lack of information in ~5 (whether or not they refer to the same node) with the use of functional uncertainty in LFG (Kaplan and Maxwell 1988) to account for long-distance dependency. In order to consistently maintain the distinction between descriptions of trees with trees, while discussing the proposed interpretation of TAG we will use the terms 488 K. Vijay-Shanker Using Descriptions of Trees in a Tree Adjoining Grammar $ E~'6 S O/,/ : : ++ ['.,,=""&gt;Cl ,',[,....,= [...h,= [.,;<,&gt;| I I VP [head:<2&gt; [1 [h,.: <I~t •+ [h=a=<I&gt;[l ~* Figure 5 Associating feature structures with quasi-nodes. initial quasi-tree, auxiliary quasi-tree, quasi-root, and quasi-foot in place of initial tree, auxiliary tree, root node, and foot node, respectivel"
J92-4004,P86-1038,0,0.00799109,"w a y to specify an FTAG that will be convenient for our purposes here. In this section, we describe a logical formulation of the unification-based approach to TAGs. The p u r p o s e of providing a logical formulation of FTAG is so that we can find the denotation of an FTAG g r a m m a r (the set of structures generated) as well as contrast it with context-free grammar-based unification grammars. To define the denotation of an FTAG grammar, we will first describe h o w an FTAG g r a m m a r can be represented. This representation uses the logical formulation of feature structures as given by Kasper and Rounds (1986) and Johnson (1988) and is similar in approach to the logical formulation of Functional Unification G r a m m a r (FUG) given b y Rounds and Manaster-Ramer (1987). In the f r a m e w o r k of Rounds and Manaster-Ramer (1987), an FUG (or any contextfree g r a m m a r with associated unification equations as in, say PATR-II) can be represented by means of a set of equations, using the formulae of Kasper-Rounds to represent feature structures. For example, a context-free g r a m m a r rule S --* N P V P can be represented as s ::= C A T : S A 1 : np A 2 : vp. Here s, np, and vp are type variables"
J92-4004,P92-1010,1,0.81704,". However, in this case since the labels of these quasi-nodes are different, they cannot refer to the same node. Thus, in this case we have a path length that is greater than 0. • vp2 quasi-node immediately dominates the v2 quasi-node (i.e., path length=l). • Since Vl and v2 quasi-nodes are identified and since we insist on a tree structure, we have vpl and vp2 quasi-nodes in the domination relation. In fact vpl quasi-node must dominate vp2 quasi-node in the resulting structure by a path of length 0 or more (from the two observations above). Thus we get the structure given by c~2a as desired. Rogers and Vijay-Shanker (1992) describe a proof system that can be used to perform the type of reasoning involved in constructing the structure o~23as described above. In the manner described above we can build the default structure for every subcategorization frame. Such structures will be specified in any lexicalized TAG; the difference (in the envisaged specification method) is that we no longer precompile out 514 K. Vijay-Shanker Using Descriptions of Trees in a Tree Adjoining Grammar all possibilities (thus repeating the structure ~'12 in all structures associated with every type of verb). To complete the description"
J92-4004,P87-1013,0,0.281139,"ch to TAGs. The p u r p o s e of providing a logical formulation of FTAG is so that we can find the denotation of an FTAG g r a m m a r (the set of structures generated) as well as contrast it with context-free grammar-based unification grammars. To define the denotation of an FTAG grammar, we will first describe h o w an FTAG g r a m m a r can be represented. This representation uses the logical formulation of feature structures as given by Kasper and Rounds (1986) and Johnson (1988) and is similar in approach to the logical formulation of Functional Unification G r a m m a r (FUG) given b y Rounds and Manaster-Ramer (1987). In the f r a m e w o r k of Rounds and Manaster-Ramer (1987), an FUG (or any contextfree g r a m m a r with associated unification equations as in, say PATR-II) can be represented by means of a set of equations, using the formulae of Kasper-Rounds to represent feature structures. For example, a context-free g r a m m a r rule S --* N P V P can be represented as s ::= C A T : S A 1 : np A 2 : vp. Here s, np, and vp are type variables. The attributes 1 and 2 are used to indicate the first and second children respectively. Using standard techniques to derive fixed points from a set of recursive"
J92-4004,A92-1030,0,0.0153904,"Missing"
J92-4004,C88-2147,1,0.439099,"Missing"
J92-4004,C92-1034,1,0.853471,"Such structures will be specified in any lexicalized TAG; the difference (in the envisaged specification method) is that we no longer precompile out 514 K. Vijay-Shanker Using Descriptions of Trees in a Tree Adjoining Grammar all possibilities (thus repeating the structure ~'12 in all structures associated with every type of verb). To complete the description of the rest of the elementary quasi-trees one would have to use transformations, meta-rules, or lexical rules to specify the structures for passivization, wh-movement, topicalization, etc. Work along this direction is being carried out (Vijay-Shanker and Schabes 1992). 6. C o n c l u s i o n s In this paper, we have embedded TAG in the unification framework in a manner consistent with the constraint-based approach used in this framework. Starting from first principles and taking the localization of dependencies within the elementary structures of a TAG grammar as the only basic principle, we have argued that the objects manipulated by such a grammar are not trees but descriptions of well-formed syntactic structures. From D-Theory, we have adopted the use of domination relation and use of identifiers to refer to nodes while describing such structures. Quasi"
J92-4004,P83-1020,0,\N,Missing
J92-4004,C88-2121,0,\N,Missing
J93-4002,P87-1012,0,0.379616,"Missing"
J93-4002,P88-1031,0,0.0664247,"Missing"
J93-4002,P85-1011,0,0.629277,"to the formalisms listed as well as others that have similar characteristics (as outlined below) in their derivational process. Some of the main ideas underlying our scheme have been influenced by the observations that can be made about the constructions used in the proofs of the equivalence of these formalisms and Head Grammars (HG) (Vijay-Shanker 1987; Weir 1988; Vijay-Shanker and Weir 1993). There are similarities between the TAG and HG derivation processes and that of Context-Free Grammars (CFG). This is reflected in common features of the parsing algorithms for HG (Pollard 1984) and TAG (Vijay-Shanker and Joshi 1985) and the CKY algorithm for CFG (Kasami 1965; Younger 1967). In particular, what can happen at each step in a derivation can depend only on which of a finite set of &quot;states&quot; the derivation is in (for CFG these states can be considered to be the nonterminal symbols). This property, which we refer to as the context-freeness property, is important because it allows one to keep only a limited amount of context during the recognition process, * Department of Computer and InformationSciences,Universityof Delaware, Newark, DE 19716. E-mail: vijay@udel.edu. School of Cognitiveand ComputingSciences,Univ"
J93-4002,P88-1034,1,0.892476,"Missing"
J93-4002,H86-1020,0,\N,Missing
J93-4002,E93-1045,1,\N,Missing
J93-4002,P90-1001,1,\N,Missing
N09-2035,E06-1051,0,0.0407762,"Missing"
N09-2035,D07-1082,0,0.102519,"xamples relative to reducing slack error on negative (neg) train examples. The value of the ratio is crucial for balancing the precision recall tradeoff well. (Morik et al., 1999) showed that during PL, set# of neg training examples C+ = # of pos training examples is an effecting C − tive heuristic. Section 4 explores using this heuristic during AL and explains a modified heuristic that could work better during AL. (Ertekin et al., 2007) propose using the balancing of training data that occurs as a result of AL-SVM to handle imbalance and do not use any further measures to address imbalance. (Zhu and Hovy, 2007) used resampling to address imbalance and based the amount of resampling, which is the analog of our cost model, on the amount of imbalance in the current set of labeled train data, as PL approaches do. In contrast, the InitPA approach in Section 4 bases its cost models on overall (unlabeled) corpus imbalance rather than the amount of imbalance in the current set of labeled data. 3 Experimental Setup We use relation extraction (RE) and text classification (TC) datasets and SVMlight (Joachims, 1999) for training the SVMs. For RE, we use AImed, previously used to train protein interaction extrac"
N09-2035,W09-1107,1,\N,Missing
P86-1011,P86-1011,1,0.0608467,"ch they differ in their descriptive power. Abstract We examine the relationship between the two grammatical formalisms: Tree Adjoining Grammars and Head Grammars. We briefly investigate the weak equivalence of the two formalisms. We then turn to a discussion comparing the linguistic expressiveness of the two formalisms. 1.1 Definitions In this section, we shall briefly define the three formalisms: TAG&apos;s, HG&apos;s, and MHG&apos;s. 1 Introduction 1.1.1 Tree Adjoining G r a m m a r s Tree Adjoining Grammars differsfrom stringrewritingsystems such as Context Free Grammars in that they generate Recent work [9,3] has revealed a very close formal relationship between the grammatical formalisms of Tree Adjoining Grammars (TAG&apos;s) and Head Grammars (HG&apos;s). In this paper we examine whether they have the same power of linguistic description. TAG&apos;s were first introduced in 1975 by Joshi, Levy and Takahashi[1] and investigated further in [2,4,8]. H G &apos; s were first introduced by Pollard[5]. TAG&apos;s and HG&apos;s were introduced to capture certain structural properties of natural languages. These formalisms were developed independently and are notationally quite different. TAG&apos;s deal with a set of elementary trees co"
P86-1011,P85-1011,0,0.0616354,"initions In this section, we shall briefly define the three formalisms: TAG&apos;s, HG&apos;s, and MHG&apos;s. 1 Introduction 1.1.1 Tree Adjoining G r a m m a r s Tree Adjoining Grammars differsfrom stringrewritingsystems such as Context Free Grammars in that they generate Recent work [9,3] has revealed a very close formal relationship between the grammatical formalisms of Tree Adjoining Grammars (TAG&apos;s) and Head Grammars (HG&apos;s). In this paper we examine whether they have the same power of linguistic description. TAG&apos;s were first introduced in 1975 by Joshi, Levy and Takahashi[1] and investigated further in [2,4,8]. H G &apos; s were first introduced by Pollard[5]. TAG&apos;s and HG&apos;s were introduced to capture certain structural properties of natural languages. These formalisms were developed independently and are notationally quite different. TAG&apos;s deal with a set of elementary trees composed by means of an operation called a d j o i n i n g . HG&apos;s maintain the essential character of contextfree string rewriting rules, except for the fact t h a t besides concatenation of strings, string w r a p p i n g operations are permitted. Observations of similarities between properties of the two formalisms led us to stud"
P86-1011,C86-1048,1,0.399215,"ch they differ in their descriptive power. Abstract We examine the relationship between the two grammatical formalisms: Tree Adjoining Grammars and Head Grammars. We briefly investigate the weak equivalence of the two formalisms. We then turn to a discussion comparing the linguistic expressiveness of the two formalisms. 1.1 Definitions In this section, we shall briefly define the three formalisms: TAG&apos;s, HG&apos;s, and MHG&apos;s. 1 Introduction 1.1.1 Tree Adjoining G r a m m a r s Tree Adjoining Grammars differsfrom stringrewritingsystems such as Context Free Grammars in that they generate Recent work [9,3] has revealed a very close formal relationship between the grammatical formalisms of Tree Adjoining Grammars (TAG&apos;s) and Head Grammars (HG&apos;s). In this paper we examine whether they have the same power of linguistic description. TAG&apos;s were first introduced in 1975 by Joshi, Levy and Takahashi[1] and investigated further in [2,4,8]. H G &apos; s were first introduced by Pollard[5]. TAG&apos;s and HG&apos;s were introduced to capture certain structural properties of natural languages. These formalisms were developed independently and are notationally quite different. TAG&apos;s deal with a set of elementary trees co"
P87-1015,J84-3005,0,0.15258,"tions whose path sets have nested dependencies. Introduction Much of the study of grammatical systems in computational linguistics has been focused on the weak generative capacity of grammatical forma~sm- Little attention, however, has been paid to the structuraldescriptions that these formalisms can assign to strings, i.e. their strong generative capacity. This aspect of the formalism i s beth linguistically and computationally important. For example, Gazdar (1985) discusses the applicability of Indexed Grammars (IG&apos;s) to Natural Language in terms of the structural descriptions assigned; and Berwick (1984) discusses the strong generative capacity of Lexical-Functional Grammar CLFG) and Government and Bindings grammars (GB). The work of Thatcher (1973) and Rounds (1969) define formal systems that generate tree sets that are related to CFG&apos;s and IG&apos;s. We consider properties of the tree sets generated by CFG&apos;s, Tree Adjoining Grammars (TAG&apos;s), Head GrammarS (HG&apos;s), Categorial Grammars (CG&apos;s), and IG&apos;s. We examine both the complexity of the paths of trees in the tree sets, and the kinds of dependencies that the formalisms can impose between paths. These two properties of the tree sets are not only"
P87-1015,J88-4001,0,\N,Missing
P89-1003,P88-1029,0,0.357058,"Missing"
P89-1003,P86-1038,0,0.405872,"Missing"
P89-1003,P84-1008,0,\N,Missing
P89-1027,C88-2147,1,\N,Missing
P89-1027,C88-1060,0,\N,Missing
P89-1027,P88-1032,1,\N,Missing
P89-1027,P87-1015,1,\N,Missing
P89-1027,P88-1034,1,\N,Missing
P89-1027,P86-1038,0,\N,Missing
P90-1001,P89-1018,0,0.0613315,"Missing"
P90-1001,P87-1012,0,0.188195,"Missing"
P90-1001,W89-0217,0,0.0618666,"Missing"
P90-1001,P87-1011,0,0.330902,"Missing"
P90-1001,E89-1002,0,\N,Missing
P90-1001,P88-1031,0,\N,Missing
P90-1001,P88-1034,1,\N,Missing
P90-1035,P88-1032,1,0.828796,"e adjunctions between the above and below positions of the dot of interior nodes. STAa : .ao • By keeping track of the auxiliary trees being reduced, it is possible to output a parse instead of acceptance or an error. The parser recognizes the derived tree inside out: it extracts recursively the innermost auxiliary tree that has no adjunction performed in it. E F G 2.1 2.2 2.3 H I 3.1 3.2 Figure 3: Left to Right Tree Traversal 5 LR(0) Parsing Tables This section explain how to construct an LR(0) parsing table given a TAG. The construction is an extension of the one used for CFGs. Similarly to Schabes and Joshi (1988), we extend the notion of dotted rules to trees. We define the closure operations that correspond to adjunction. Then we explain how transitions between states are defined. We give in Figure 5 an example of a finite state automaton used to build the parsing table for a TAG (see Figure 5) generating a context-sensitive language. We first explain preliminary concepts (originally defined to construct an Earley-type parser for TAGs) that will be used by the algorithm. Dotted rules are extended to trees. Then we recall a tree traversal that the algorithm will mimic in order to scan the input from l"
P90-1035,J87-1004,0,0.0484334,"shown that they recognize exactly the set of languages recognized by deterministic push down automata. LR(k) parsers for CFGs have been proven useful for compilers as well as recently for natural language processing. For natural language processing, although LR(k) parsers are not powerful enough, *The first author is partially supported by Darpa grant N0014-85K0018, ARO grant DAAL03-89-C-003iPRI NSF grant-IRIS4-10413 A02. We are extremely grateful to Bernard Lang and David Weir for their valuable suggestions. 276 conflicts between multiple choices are solved by pseudoparallelism (Lang, 1974, Tomita, 1987). This gives rise to a class of powerful yet efficient parsers for natural languages. It is in this context that we study deterministic (LR(k)-style) parsing of TAGs. The set of Tree Adjoining Languages is a strict superset of the set of Context Free Languages (CFLs). For example, the cross serial dependency constmction in Dutch can be generated by a TAG. 1 Waiters (1970), R~v6sz (1971), Turnbull and Lee (1979) investigated deterministic parsing of the class of context-sensitive languages. However they used Turing machines which recognize languages much more powerful than Tree Adjoining Langua"
P92-1007,H86-1019,0,0.0156023,"operations necessary to build the string from its elementary components. The corresponding syntactic structure is then generated by doing Mumble-86 (McDonald & Pustejovsky, 1985; Meteer et al., 1987) is a sentence generator based on TAG that is able to take more than just the logical form representation into account. Mumble-86 is one of the foremost sentence generation systems and it (or its predecessors) has been used as the sentence generation components of a number of natural language generation projects (e.g., (McDonald, 1983; McCoy, 1989; Conklin & McDonald, 1982; Woolf& McDonald, 1984; Rubinoff, 1986)). After briefly describing the methodology in Mumble-86, we will point out some problematic aspects of its design. We will then describe our architecture which is based on interfacing TAG with a rich functional theory provided by functional systemic grammar (Halliday, 1970; Halliday, 1985; Fawcett, 1980; Hudson, 1981). 2 We pay particular attention to those aspects which distinguish our generator from Mumble-86. 2 Mumble-86 Mumble-86 generates from a specification of what is to be said in the form of an ""L-Spec"" 1This work is supported ill part by Grant #H133E80015 from the National hlstitute"
P92-1007,C88-2121,0,0.0886646,"Missing"
P92-1007,J90-1004,0,0.0650462,"Missing"
P92-1007,W90-0101,1,0.897423,"Missing"
P92-1007,P85-1012,0,\N,Missing
P92-1007,P82-1030,0,\N,Missing
P92-1010,P83-1020,0,0.547447,"Missing"
P92-1010,J92-4004,1,0.941001,"on between quasi-trees and trees. Further, for some inferences we will have the choice of making the inference or not. T h e choices we m a k e in defining the structure of the quasi-trees as a set of relationships will determine the structure of the sets of trees we can characterize with a single quasi-tree. Thus these choices will be driven by how much expressive power the application needs in describing these sets. Our guiding principle is to make the quasi-trees as tree-like as possible consistent with the needs of our applications. We discuss these considerations more fully in (Rogers &5 Vijay-Shanker, 1992). One inference we will not m a k e is as follows: from ""a dominates b"" infer either ""a equals b"" or, for some a&apos; and b&apos;, ""a dominates a&apos;, a&apos; is the parent of b&apos;, and b&apos; dominates b"". In structures that enforce this condition p a t h lengths cannot be left partially specified. As a result, the set of quasi-trees required to characterize s &apos; viewed as a set of trees, for instance, would be infinite. Similarly, we will not make the inference: for all a, b, either ""a is left-of b"", ""b is left-of a"", ""a dominates b"", or ""b dominates a"". In these structures the left-of relation is no longer partial"
P92-1010,C92-1034,1,\N,Missing
P95-1013,C88-2147,1,\N,Missing
P95-1013,J92-4004,1,\N,Missing
P95-1013,C88-2121,0,\N,Missing
P95-1013,P95-1021,1,\N,Missing
P95-1021,E91-1005,1,0.76567,"n Section 1.2, there are cases where satisfactory analyses cannot be obtained with adjunction. In particular, using adjunction in this way cannot handle cases in which parts of the clausal complement are required to be placed within the structure of the adjoined tree. The DTG operation of subsertion is designed to overcome this limitation. Subsertion can be viewed as a generalization of adjunction in which components of the clausal complement (the subserted structure) which are not substituted can be interspersed within the structure that is the site of the subsertion. Following earlier work (Becket et al., 1991; Vijay-Shanker, 1992), DTG provide a mechanism involving the use of domination links (d-edges) that ensure that parts of the subserted structure that are not substituted dominate those parts that are. Furthermore, there is a need to constrain the way in which the non-substituted components can be interspersed 3. This is done by either using appropriate feature constraints at nodes or by means of subsertion-insertion constraints (see Section 2). We end this section by briefly commenting on the other DTG operation of sister-adjunction. In TAG, modification is performed with adjunction of modifi"
P95-1021,J94-1004,0,0.271262,"hextraction in Kashmiri proceeds as in English, except that the wh-word ends up in sentence-second position, with a topic from the matrix clause in sentence-initial position. This is illustrated in (2a) for a simple clause and in (2b) for a complex clause. (2) a. rameshan kyaa dyutnay t s e RameshzRG whatNOM g a v e yOUDAT What did you give Ramesh? b. rameshan kyaal chu baasaan [ ki RameshzRG what is believeNperf that me kor ti] IZRG do What does Ramesh beheve that I did? claim SUBJ ~ "" ~ O M P seem he I COMP adore SUB~BJ Mary hotdog MOD ~ . ~ O D spicy small Figure 2: Dependency tree for (1) Schabes & Shieber (1994) solve the first problem 1For clarity, we depart from standard TAG notational practice and annotate nodes with lexemes and arcs with grammatical function: 152 Since the moved element does not appear in sentence-initial position, the TAG analysis of English wh-extraction of Kroch (1987; 1989) (in which the matrix clause is adjoined into the embedded clause) cannot be transferred, and in fact no linguistically plausible TAG analysis appears to be available. In the past, variants of TAG have been developed to extend the range of possible analyses. In Multi-Component TAG (MCTAG) (Joshi, 1987), tre"
P95-1021,J92-4004,1,0.799715,"s a separate functional (f-) structure, and dependency grammars (see e.g. Mel'~uk (1988)) use these notions as the principal basis for syntactic representation. We will follow the dependency literature in referring to complementation and modification as syntactic dependency. As observed by Rambow and Joshi (1992), for TAG, the importance of the dependency structure means that not only the derived phrase-structure tree is of interest, but also the operations by which we obtained it from elementary structures. This information is encoded in the derivation tree (Vijay-Shanker, 1987). However, as Vijay-Shanker (1992) observes, the TAG composition operations are not used uniformly: while substitution is used only to add a (nominal) complement, adjunction is used both for modification and (clausal) complementation. Clausal complementation could not be handled uniformly by substitution because of the existence of syntactic phenomena such as long-distance wh-movement in English. Furthermore, there is an inconsistency in the directionality of the operations used for complementation in TAG@: nominal complements are substituted into their governing verb's tree, while the governing verb's tree is adjoined into it"
P95-1021,P95-1013,1,\N,Missing
P95-1021,P94-1036,1,\N,Missing
P98-2188,W95-0101,0,0.0960781,"Missing"
P98-2188,J86-3001,0,0.0647169,"ddition, our implementation can accommodate a wide variety of different types of features, including set-valued features, features that consider the context of surrounding utterances, and features that can take distant context into account. These and other attractive characteristics of TBL are discussed further in Samuel et al. (1998b). 1This condition is true only for the first utterance of a dialogue. 1151 Dialogue Act Tagging To address a significant concern in machine learning, called the sparse data problem, we must select an appropriate set of features. Researchers in discourse, such as Grosz and Sidner (1986), Lambert (1993), Hirschberg and Litman (1993), Chen (1995), Andernach (1996), Samuel (1996), and Chu-Carroll (1998) have suggested several features that might be relevant for the task of computing dialogue acts. Our system can consider the following features of an utterance: 1) the cue phrases 3 in the utterance; 2) the word n-grams 3 in the utterance; 3) the dialogue act c u e s 3 in the utterance; 4) the entire utterance for one-, two-, or three-word utterances; 5) speaker information 4 for the utter2The part-of-speech tag of a word is dependent on the word&apos;s internal features and on the su"
P98-2188,J93-3003,0,0.384147,"te a wide variety of different types of features, including set-valued features, features that consider the context of surrounding utterances, and features that can take distant context into account. These and other attractive characteristics of TBL are discussed further in Samuel et al. (1998b). 1This condition is true only for the first utterance of a dialogue. 1151 Dialogue Act Tagging To address a significant concern in machine learning, called the sparse data problem, we must select an appropriate set of features. Researchers in discourse, such as Grosz and Sidner (1986), Lambert (1993), Hirschberg and Litman (1993), Chen (1995), Andernach (1996), Samuel (1996), and Chu-Carroll (1998) have suggested several features that might be relevant for the task of computing dialogue acts. Our system can consider the following features of an utterance: 1) the cue phrases 3 in the utterance; 2) the word n-grams 3 in the utterance; 3) the dialogue act c u e s 3 in the utterance; 4) the entire utterance for one-, two-, or three-word utterances; 5) speaker information 4 for the utter2The part-of-speech tag of a word is dependent on the word&apos;s internal features and on the surrounding words; similarly, the dialogue act o"
P98-2188,E95-1037,0,0.0127146,"types of features, including set-valued features, features that consider the context of surrounding utterances, and features that can take distant context into account. These and other attractive characteristics of TBL are discussed further in Samuel et al. (1998b). 1This condition is true only for the first utterance of a dialogue. 1151 Dialogue Act Tagging To address a significant concern in machine learning, called the sparse data problem, we must select an appropriate set of features. Researchers in discourse, such as Grosz and Sidner (1986), Lambert (1993), Hirschberg and Litman (1993), Chen (1995), Andernach (1996), Samuel (1996), and Chu-Carroll (1998) have suggested several features that might be relevant for the task of computing dialogue acts. Our system can consider the following features of an utterance: 1) the cue phrases 3 in the utterance; 2) the word n-grams 3 in the utterance; 3) the dialogue act c u e s 3 in the utterance; 4) the entire utterance for one-, two-, or three-word utterances; 5) speaker information 4 for the utter2The part-of-speech tag of a word is dependent on the word&apos;s internal features and on the surrounding words; similarly, the dialogue act of an utteranc"
P98-2188,P97-1011,0,0.0375113,"Missing"
P98-2188,W97-0320,0,0.00971113,", to address limitations of TBL, we introduce a Monte Carlo strategy for training efficiently and a committee method for computing confidence measures. These ideas are combined in our working implementation, which labels held-out data as accurately as any other reported system for the dialogue act tagging task. Introduction Although machine learning approaches have achieved success in many areas of Natural Language Processing, researchers have only recently begun to investigate applying machine learning methods to discourse-level problems (Reithinger and Klesen, 1997; Di Eugenio et al., 1997; Wiebe et al., 1997; Andernach, 1996; Litman, 1994). An important task in discourse understanding is to interpret an utterance&apos;s dialogue act, which is a concise abstraction of a speaker&apos;s intention, such as SUGGEST and ACCEPT. Recognizing dialogue acts is critical for discourse-level understanding and can also be useful for other applications, such as resolving ambiguity in speech recognition. However, computing dialogue acts is a challenging task, because often a dialogue act cannot be directly inferred from a literal interpretation of an utterance. We have investigated applying Transformation-Based Learning ("
P98-2188,J95-4004,0,\N,Missing
W03-1315,W99-0613,0,0.303221,"ames is not our main focus, we make use of some simple techniques to investigate different sources of information and verify our intuitions about their usefulness. 1 Introduction In this paper, we investigate the extent to which different sources of information contribute towards the task of classifying the type of biological entity a phrase might refer to. The classification task is an integral part of named entity extraction. For this reason, name classification has been studied in solving the named entity extraction task in the NLP and information extraction communities (see, for example, (Collins and Singer, 1999; Cucerzan and Yarowsky, 1999) and various approaches reported in the MUC conferences (MUC-6, 1995)). However, many of these approaches do not distinguish the detection of the names (i.e., identifying a sequence of characters and words in text as a name) from that of its classification as separate phases. Yet, we believe that we will gain from examining the two as separate tasks as the classification task, the focus of this work, is sufficiently distinct from the name identification task. More importantly, from the perspective of the current work, we hope to show that the sources of informatio"
W03-1315,W99-0612,0,0.067105,", we make use of some simple techniques to investigate different sources of information and verify our intuitions about their usefulness. 1 Introduction In this paper, we investigate the extent to which different sources of information contribute towards the task of classifying the type of biological entity a phrase might refer to. The classification task is an integral part of named entity extraction. For this reason, name classification has been studied in solving the named entity extraction task in the NLP and information extraction communities (see, for example, (Collins and Singer, 1999; Cucerzan and Yarowsky, 1999) and various approaches reported in the MUC conferences (MUC-6, 1995)). However, many of these approaches do not distinguish the detection of the names (i.e., identifying a sequence of characters and words in text as a name) from that of its classification as separate phases. Yet, we believe that we will gain from examining the two as separate tasks as the classification task, the focus of this work, is sufficiently distinct from the name identification task. More importantly, from the perspective of the current work, we hope to show that the sources of information that help in solving the two"
W03-1315,W02-0301,0,0.0359742,"nt role for this task. In fact, the coreference resolution method described in (Casta˜no et al., 2002) seeks to use such information by using the UMLS system1 and by applying type coercion. Finally, many information extraction methods are based on identifying or inducing patterns by which information (of the kind being extracted) is expressed in natural language text. If we can tag the text with occurrences of various types of names (or phrases that refer to biological entities) then better generalizations of patterns can be induced. There are at least two efforts (Narayanaswamy et al., 2003; Kazama et al., 2002) that consider the recognition of names of different classes of biomedical relevance. Work reported in (Pustejovsky et al., 2002; Casta˜no et al., 2002) also seeks to classify or find the sortal information of phrases that refer to biological entities. However, classification was not the primary focus of these papers and hence the details and accuracy of the classification methods are not described in much detail. Other related works include those of (Hatzivassiloglou et al., 2001; Liu et al., 2001) that use external or contextual clues to disambiguate ambiguous expressions. While these works"
W03-1315,W02-0302,0,0.0654709,"Missing"
W03-1315,P95-1026,0,0.0941205,"now turn our attention to looking at clues that are outside the name being classified. Using context has been widely used for WSD and has also been applied to name classification (for example, in (Collins and Singer, 1999; Cucerzan and Yarowsky, 1999)). This approach has also been adopted for the biomedical domain as illustrated in the work of (Hatzivassiloglou et al., 2001; Narayanaswamy et al., 2003; Casta˜no et al., 2002)2 . In the WSD work involving the use of context, we can find two approaches: one that uses few strong contextual evidences for disambiguation purposes, as exemplified by (Yarowsky, 1995); and the other that uses weaker evidences but considers a combination of a number of them, as exemplified by (Gale et al., 1992). We explore both the methods. In Section 4.4, we discuss our formulation and present a simple way of extracting contextual clues. 2 (Casta˜no et al., 2002) can be seen as using context in its type coercion rules. 3 Experimental Setup 3.1 Division of the corpus We divided the name-annotated GENIA corpus (consisting of 2000 abstracts) into two parts—1500 abstracts were used to derive all the clues: f-terms, suffixes, examples (for matching) and finally contextual feat"
W03-1601,1993.tmi-1.14,0,0.70528,"propriate realizations be produced. Di erent placement of argument realizations Sometimes di erent synonyms, like the verbs enjoy and please, place argument realizations di erently with respect to the head, as illustrated in (2a-2b). (2a) Amy enjoyed the interaction. (2b) The interaction pleased Amy. To handle this variety, a uniform generation methodology should not assume a xed mapping between thematic and syntactic roles but let each lexical item determine the placement of argument realizations. Generation systems that use such a xed mapping must override it for the divergent cases (e.g., (Dorr, 1993)). Words with overlapping meaning There are often cases of di erent words that realize different but overlapping semantic pieces. The easiest way to see this is in what has been termed incorporation, where a word not only realizes a predicate but also one or more of its arguments. Di erent words may incorporate di erent arguments or none at all, which may lead to paraphrases, as illustrated in (3a-3c). (3a) Charles ew across the ocean. (3b) Charles crossed the ocean by plane. (3c) Charles went across the ocean by plane. Notice that the verb y realizes not only going but also the mode of transp"
W03-1601,J97-2001,0,0.424341,"various ways using di erent lexical and syntactic means. These di erent realizations, called paraphrases, vary considerably in appropriateness based on pragmatic factors and communicative goals. If a generator is to come up with the most appropriate realization, it must be capable of generating all paraphrases that realize the input semantics. Even if it makes choices on pragmatic grounds during generation and produces a single realization, the ability to generate them all must still exist. Variety of lexical and grammatical forms of expression pose challenges to a generator ((Stede, 1999); (Elhadad et al., 1997); (Nicolov et al., 1995)). In this paper, we discuss the generation of single-sentence paraphrases realizing the same semantics in a uniform fashion using a simple sentence generation architecture. In order to handle the various ways of realizing meaning in a simple manner, we believe that the generation architecture should not be aware of the variety and not have any special mechanisms to handle the di erent types of realizations1 . Instead, we want all lexical and grammatical variety to follow automatically from the variety of the elementary building blocks of generation, lexico-grammatical"
W03-1601,W02-2121,1,0.626592,"domination of length zero or more where syntactic material (e.g., modi ers) may end up. critical for combining realizations (in step 3 of our algorithm in section 4.1). There are, however, advantages that our approach has. For one, we are not constrained by the isomorphism requirement in a Synchronous TAG derivation. Also, the DSG formalism that we use a ords 4.4 Using resources in our algorithm greater exibility, signi cant in our approach, as discussed later in this paper (and in more detail Step 1 of our algorithm requires matching the semantic side of a resource against the top of the in (Kozlowski, 2002b)). input and testing selectional restrictions. A se4.3 The grammatical formalism mantic side matches if it can be overlaid against Both step 3 of our algorithm (putting re- the input. Details of this process are given alizations together) and the needs of lexico- in (Kozlowski, 2002a). Selectional restrictions grammatical resources (the encapsulation of (type restrictions on arguments) are associated syntactic consequences such as the position with nodes on the semantic side of resources. of argument realizations) place signi cant de- In their evaluation, the appropriate knowledge mands on t"
W03-1601,J01-1004,1,0.808879,"restrictions on arguments) are associated syntactic consequences such as the position with nodes on the semantic side of resources. of argument realizations) place signi cant de- In their evaluation, the appropriate knowledge mands on the grammatical formalism to be used base instance is accessed and its type is tested. in the implementation of the architecture. One More details about using selectional restrictions grammatical formalism that is well-suited for in generation and in our architecture are given our purposes is the D-Tree Substitution Gram- in (Kozlowski et al., 2002). mars (DSG, (Rambow et al., 2001)), a variant Resources for enjoy and please which match of Tree-Adjoining Grammars (TAG). This for- the top of the input in Fig. 1 are shown in malism features an extended domain of locality Fig. 2. In doing the matching, the arguments and exibility in encapsulation of syntactic con- AMY and INTERACTION are uni ed with X and sequences, crucial in our architecture. Y. The dashed outlines around X and Y indicate Consider the elementary DSG structures on that the resource does not realize them. Our althe right-hand-side of the resources for enjoy gorithm calls for the independent recursive realan"
W03-1601,C90-3045,0,0.0397844,"sides contain the verbs enjoy and please in (Nicolov et al., 1995); (Stone and Doran, 1997)). the active voice con guration. The mappings include links between ENJOY and its realization as well as links between the unrealized agent (X) 4.1 Our algorithm Our generation algorithm is a simple, recursive, or theme (Y) and the subject or the complement. Our mapping between semantic and syntactic semantic-head-driven generation process, consistent with the approach described in section 2, constituents bears resemblance to the pairings in but one driven by the semantic input and the Synchronous TAG (Shieber and Schabes, 1990). Just like in Synchronous TAG, the mapping is lexico-grammatical resources. S S ENJOY EXPERIENCER X NP0 NP0 VP0 THEME VP0 NP0 VP0 VP1 VP1 Y V◆ enjoy S NP1 VP1 NP1 V◆ enjoy V◆ please NP1 S ENJOY EXPERIENCER X NP0 VP0 THEME Amy VP1 Y V◆ please NP1 Figure 2: Two di erent resources for ENJOY the interaction Figure 3: Combining argument realizations with the resources for enjoy and please arguments will be substituted. The positions of both the subject and the complement are encapsulated in these elementary structures. This allows the mapping between semantic and syntactic constituents to be de ne"
W03-1601,P97-1026,0,0.0328712,"dicate ENJOY source, generation can proceed no matter what and the thematic roles EXPERIENCER and THEME. choices are speci ed about these in each indi- The arguments lling those roles (which must be vidual resource. Our approach is fundamen- realized separately, as indicated by dashed outtally di erent from systems that reason directly lines) appear as variables X and Y which will be about syntax and build realizations by syntactic matched against actual arguments. The syntacrank ((Bateman, 1997), (Elhadad et al., 1997); tic sides contain the verbs enjoy and please in (Nicolov et al., 1995); (Stone and Doran, 1997)). the active voice con guration. The mappings include links between ENJOY and its realization as well as links between the unrealized agent (X) 4.1 Our algorithm Our generation algorithm is a simple, recursive, or theme (Y) and the subject or the complement. Our mapping between semantic and syntactic semantic-head-driven generation process, consistent with the approach described in section 2, constituents bears resemblance to the pairings in but one driven by the semantic input and the Synchronous TAG (Shieber and Schabes, 1990). Just like in Synchronous TAG, the mapping is lexico-grammatical"
W03-1601,W90-0102,0,\N,Missing
W06-3321,J95-4004,0,0.054869,"ent an experiment in the Biological domain where our POS tagger achieves results comparable to POS taggers specifically trained to this domain. Many machine-learning and statistical techniques employed for POS tagging train a model on an annotated corpus, such as the Penn Treebank (Marcus et al, 1993). Most state-of-the-art POS taggers use two main sources of information: 1) Information about neighboring tags, and 2) Information about the word itself. Methods using both sources of information for tagging are: Hidden Markov Modeling, Maximum Entropy modeling, and Transformation Based Learning (Brill, 1995). In moving to a new domain, performance can degrade dramatically because of the increase in the unknown word rate as well as domain-specific word use. We improve tagging performance by attacking these problems. Since our goal is to employ minimal manual effort or domain-specific knowledge, we consider only orthographic, inflectional and derivational information in deriving POS. We bypass the time, cost, resource, and content expert intensive approach of annotating a corpus for a new domain. K. Vijay-Shanker1 Methodology and Experiment The initial components in our POS tagging process are a le"
W06-3321,W04-3111,0,0.0591603,"Missing"
W06-3321,J93-2004,0,0.0408595,"sent a methodology for rapid adaptation of POS taggers to new domains. Our technique is unsupervised in that a manually annotated corpus for the new domain is not necessary. We use suffix information gathered from large amounts of raw text as well as orthographic information to increase the lexical coverage. We present an experiment in the Biological domain where our POS tagger achieves results comparable to POS taggers specifically trained to this domain. Many machine-learning and statistical techniques employed for POS tagging train a model on an annotated corpus, such as the Penn Treebank (Marcus et al, 1993). Most state-of-the-art POS taggers use two main sources of information: 1) Information about neighboring tags, and 2) Information about the word itself. Methods using both sources of information for tagging are: Hidden Markov Modeling, Maximum Entropy modeling, and Transformation Based Learning (Brill, 1995). In moving to a new domain, performance can degrade dramatically because of the increase in the unknown word rate as well as domain-specific word use. We improve tagging performance by attacking these problems. Since our goal is to employ minimal manual effort or domain-specific knowledge"
W06-3321,N01-1006,0,0.0548569,"Missing"
W07-1024,D07-1118,1,0.841637,"aw text and a generic POS annotated corpus to develop taggers for new domains without hand annotation or supervised training. We focus in particular on out-ofvocabulary words since they reduce accuracy (Lease and Charniak. 2005; Smith et al. 2005). There is substantial information in the derivational suffixes and few inflectional suffixes of English. We look at individual words and their suffixes along with the morphologically related words to build a domain specific lexicon containing POS tags and probabilities for each word. 2 Adaptation Methodology Our methodology is described in detail in Miller et al (2007) and summarized here: 1) Process generic POS annotated text to obtain state and lexical POS tag probabilities. 2) Obtain a frequency table of words from a large corpus of raw sub-domain text. 3) Construct a partial sub-domain lexicon matching relative frequencies of morphologically related words with words from the generic annotated text averaging POS probabilities of the k nearest neighbors. 4) Combine common generic words and orthographic word categories with the partial lexicon making the sub-domain lexicon. 5) Train a first order Hidden Markov Model (HMM) by Expectation Maximization (EM)."
W07-1024,J93-2004,0,\N,Missing
W07-1024,I05-1006,0,\N,Missing
W08-0620,E06-1051,0,0.0609645,"Missing"
W09-1107,J08-4004,0,0.0898426,"reement between two models would be to measure the percentage of points on which the models make the same predictions. However, experimental results on a separate development dataset show then that the cutoff agreement at which to stop is sensitive to the dataset being used. This is because different datasets have different levels of agreement that can be expected by chance and simple percent agreement doesn’t adjust for this. Measurement of agreement between human annotators has received significant attention and in that context, the drawbacks of using percent agreement have been recognized (Artstein and Poesio, 2008). Alternative metrics have been proposed that take chance agreement into account. In (Artstein and Poesio, 2008), a survey of several agreement metrics is presented. Most of the agreement metrics are of the form: agreement = Ao − Ae , 1 − Ae (1) where Ao = observed agreement, and Ae = agreement expected by chance. The different metrics differ in how they compute Ae . The Kappa statistic (Cohen, 1960) measures agreement expected by chance by modeling each coder (in our case model) with a separate distribution governing their likelihood of assigning a particular category. Formally, Kappa is defi"
W09-1107,N09-2035,1,0.648941,"ap in the level of aggressiveness available for stopping AL and supports providing users with control over stopping behavior. 85 stop point 1: stops too early; results in lower performing model 80 stop point 2: good place to stop stop point 3: stops too late; wastes around 30,000 human annotations 75 70 65 0 1 2 3 4 5 Number of Points for which Annotations Have Been Requested 6 7 4 x 10 Figure 1: Hypothetical Active Learning Curve with hypothetical stopping points. 1 Introduction The use of Active Learning (AL) to reduce NLP annotation costs has generated considerable interest recently (e.g. (Bloodgood and Vijay-Shanker, 2009; Baldridge and Osborne, 2008; Zhu et al., 2008a)). To realize the savings in annotation efforts that AL enables, we must have a mechanism for knowing when to stop the annotation process. Figure 1 is intended to motivate the value of stopping at the right time. The x-axis measures the number of human annotations that have been requested and ranges from 0 to 70,000. The y-axis measures ∗ This research was conducted while the first author was a PhD student at the University of Delaware. performance in terms of F-Measure. As can be seen from the figure, the issue is that if we stop too early whil"
W09-1107,C08-1059,0,0.0706752,"Missing"
W09-1107,D07-1082,0,0.0227722,"periments that margin exhaustion has a tendency to stop late. This is further confirmed in our experiments in Section 4. The confidence-based stopping criterion (hereafter, V2008) in (Vlachos, 2008) says to stop when model confidence consistently drops. As pointed out by (Vlachos, 2008), this stopping criterion is based on the assumption that the learner/feature representation is incapable of fully explaining all the examples. However, this assumption is often violated and then the performance of the method suffers (see Section 4). Two stopping criteria (max-conf and min-err) are reported in (Zhu and Hovy, 2007). The max-conf method indicates to stop when the confidence of the model on each unlabeled example exceeds a threshold. In the context of margin-based methods, maxconf boils down to be simply a generalization of the margin exhaustion method. Min-err, reported to be superior to max-conf, says to stop when the accuracy of the most recent model on the current batch of queried examples exceeds some threshold (they use 0.9). Zhu et al. (2008b) proposes the use of multicriteria-based stopping to handle setting the threshold for min-err. They refuse to stop and they raise the min-err threshold if the"
W09-1107,I08-1048,0,0.137233,"supports providing users with control over stopping behavior. 85 stop point 1: stops too early; results in lower performing model 80 stop point 2: good place to stop stop point 3: stops too late; wastes around 30,000 human annotations 75 70 65 0 1 2 3 4 5 Number of Points for which Annotations Have Been Requested 6 7 4 x 10 Figure 1: Hypothetical Active Learning Curve with hypothetical stopping points. 1 Introduction The use of Active Learning (AL) to reduce NLP annotation costs has generated considerable interest recently (e.g. (Bloodgood and Vijay-Shanker, 2009; Baldridge and Osborne, 2008; Zhu et al., 2008a)). To realize the savings in annotation efforts that AL enables, we must have a mechanism for knowing when to stop the annotation process. Figure 1 is intended to motivate the value of stopping at the right time. The x-axis measures the number of human annotations that have been requested and ranges from 0 to 70,000. The y-axis measures ∗ This research was conducted while the first author was a PhD student at the University of Delaware. performance in terms of F-Measure. As can be seen from the figure, the issue is that if we stop too early while useful generalizations are still being made,"
W09-1107,C08-1142,0,0.0668389,"don’t have to be labeled and stop when the predictions have stabilized. Some of the main advantages of the new method are that: it requires no additional labeled data, it’s widely applicable, it fills a need for a method which can aggressively save annotations, it has stable performance, and it provides users with control over how aggressively/conservatively to stop AL. Section 2 discusses related work. Section 3 explains our Stabilizing Predictions (SP) stopping criterion in detail. Section 4 evaluates the SP method and discusses results. Section 5 concludes. 2 Related Work Laws and Sch¨utze (2008) present stopping criteria based on the gradient of performance estimates and the gradient of confidence estimates. Their technique with gradient of performance estimates is only 40 applicable when probabilistic base learners are used. The gradient of confidence estimates method is more generally applicable (e.g., it can be applied with our experiments where we use SVMs as the base learner). This method, denoted by LS2008 in Tables and Figures, measures the rate of change of model confidence over a window of recent points and when the gradient falls below a threshold, AL is stopped. The margin"
W12-2420,C96-2183,0,0.25331,"Missing"
W12-2420,W09-1312,0,0.0303851,"bMed database, given a large training set for a specific topic. Goldberg et al. (2008) and Lu et al. (2009) describe in detail how they identified and ranked passages for the 2006 Trec Genomics Track (Hersh et al., 2006). Yeganova et al. (2011) present a method for ranking positively labeled data within large sets of data, and this method was applied by Neveol et al. (2011) to rank sentences containing deposition relationships between biological data and public repositories. Extraction of sentences describing gene functions has also been applied for creating gene summaries (Ling et al., 2007; Jin et al., 2009; Yang et al., 2009). However, these methods differ in that their goal is not to look for sentences containing specific terms and their relations with genes, but rather for sentences that fall into some predefined categories of sentences typically observed in gene summaries. Sentence simplification has been used to aid parsing (Chandrasekar et al., 1996; Jonnalagadda et al., 2009). Devlin and Tait (1998) and Carroll et al. (1998) use it to help people with aphasia. Siddharthan (2004) was concerned with cohesion and suggested some applications. The idea of using lexico-syntactic patterns to ide"
W12-2420,N09-2045,0,0.0196789,"11) to rank sentences containing deposition relationships between biological data and public repositories. Extraction of sentences describing gene functions has also been applied for creating gene summaries (Ling et al., 2007; Jin et al., 2009; Yang et al., 2009). However, these methods differ in that their goal is not to look for sentences containing specific terms and their relations with genes, but rather for sentences that fall into some predefined categories of sentences typically observed in gene summaries. Sentence simplification has been used to aid parsing (Chandrasekar et al., 1996; Jonnalagadda et al., 2009). Devlin and Tait (1998) and Carroll et al. (1998) use it to help people with aphasia. Siddharthan (2004) was concerned with cohesion and suggested some applications. The idea of using lexico-syntactic patterns to identify relation candidates has also been applied in the work of Banko et al. (2007), although their patterns are not used in the learning process. 5 Conclusion and Future Directions We have developed a system which aims to identify sentences that clearly and succinctly describe the relation between two entities. We used a set of preference judgements, as provided by biologists, to"
W12-2420,W11-0220,0,0.0178188,"ts. These judgments were generated automatically from search engine logs. Their learned rankings outperformed a static ranking function. Similar approaches in IR are those of Cohen et al. (1999) and Freund et al. (2003). Ranking of text passages and documents has been done previously in BioNLP for other purposes. Suomela and Andrade (2005) proposed a way to rank the entire PubMed database, given a large training set for a specific topic. Goldberg et al. (2008) and Lu et al. (2009) describe in detail how they identified and ranked passages for the 2006 Trec Genomics Track (Hersh et al., 2006). Yeganova et al. (2011) present a method for ranking positively labeled data within large sets of data, and this method was applied by Neveol et al. (2011) to rank sentences containing deposition relationships between biological data and public repositories. Extraction of sentences describing gene functions has also been applied for creating gene summaries (Ling et al., 2007; Jin et al., 2009; Yang et al., 2009). However, these methods differ in that their goal is not to look for sentences containing specific terms and their relations with genes, but rather for sentences that fall into some predefined categories of"
W17-2323,A00-2018,0,0.0418615,"ved from the document pools from where the training sets are sampled. Task PPI MIRGENE PLOC Documents 225 200 1,167 Figure 1: Example sentence for feature extraction. No. 1 2 3 4 5 6 Table 3: Features extracted from the example sentence. P1 and P2 represent the two protein mentions. 1: unlexicalized shortest dependency path; 2: e-walk features; 3: v-walks features; 4: three stem sequences, 5: number of edges on the shortest dependency path; 6: number of stems on the first stem sequence. For all the lexical terms, we use their stems produced by Porter’s stemmer (Porter, 1980). Charniak parser (Charniak, 2000; Charniak and Johnson, 2005) with the biomedical model (Mcclosky, 2010) is used to produce constituency parse for each sentence, which is converted to collapsed dependency parse using Stanford CoreNLP converter (Manning et al., 2014) with CCprocessed setting. We remove features that only appear once in the whole training set. Annotations (P/N) 1,000 / 4,611 464 / 775 125 / 1,783 4.2 4.1 Baselines The baseline is a LR model trained on the distantly labeled set without any filtering of noise. We also implement two previous methods for comparison. First, we train a LR model on the distantly labe"
W17-2323,P05-1022,0,0.0236434,"ument pools from where the training sets are sampled. Task PPI MIRGENE PLOC Documents 225 200 1,167 Figure 1: Example sentence for feature extraction. No. 1 2 3 4 5 6 Table 3: Features extracted from the example sentence. P1 and P2 represent the two protein mentions. 1: unlexicalized shortest dependency path; 2: e-walk features; 3: v-walks features; 4: three stem sequences, 5: number of edges on the shortest dependency path; 6: number of stems on the first stem sequence. For all the lexical terms, we use their stems produced by Porter’s stemmer (Porter, 1980). Charniak parser (Charniak, 2000; Charniak and Johnson, 2005) with the biomedical model (Mcclosky, 2010) is used to produce constituency parse for each sentence, which is converted to collapsed dependency parse using Stanford CoreNLP converter (Manning et al., 2014) with CCprocessed setting. We remove features that only appear once in the whole training set. Annotations (P/N) 1,000 / 4,611 464 / 775 125 / 1,783 4.2 4.1 Baselines The baseline is a LR model trained on the distantly labeled set without any filtering of noise. We also implement two previous methods for comparison. First, we train a LR model on the distantly labeled set filtered by a heurist"
W17-2323,P14-5010,0,0.00388636,"sentence. P1 and P2 represent the two protein mentions. 1: unlexicalized shortest dependency path; 2: e-walk features; 3: v-walks features; 4: three stem sequences, 5: number of edges on the shortest dependency path; 6: number of stems on the first stem sequence. For all the lexical terms, we use their stems produced by Porter’s stemmer (Porter, 1980). Charniak parser (Charniak, 2000; Charniak and Johnson, 2005) with the biomedical model (Mcclosky, 2010) is used to produce constituency parse for each sentence, which is converted to collapsed dependency parse using Stanford CoreNLP converter (Manning et al., 2014) with CCprocessed setting. We remove features that only appear once in the whole training set. Annotations (P/N) 1,000 / 4,611 464 / 775 125 / 1,783 4.2 4.1 Baselines The baseline is a LR model trained on the distantly labeled set without any filtering of noise. We also implement two previous methods for comparison. First, we train a LR model on the distantly labeled set filtered by a heuristic (DPFreq) proposed by Zheng and Blake (2015), which removes positively-labeled instances with a shortest dependency path that appear less than k times in the positive set. They hypothesize that rare depe"
W17-2323,N10-1004,0,0.0153982,"ask PPI MIRGENE PLOC Documents 225 200 1,167 Figure 1: Example sentence for feature extraction. No. 1 2 3 4 5 6 Table 3: Features extracted from the example sentence. P1 and P2 represent the two protein mentions. 1: unlexicalized shortest dependency path; 2: e-walk features; 3: v-walks features; 4: three stem sequences, 5: number of edges on the shortest dependency path; 6: number of stems on the first stem sequence. For all the lexical terms, we use their stems produced by Porter’s stemmer (Porter, 1980). Charniak parser (Charniak, 2000; Charniak and Johnson, 2005) with the biomedical model (Mcclosky, 2010) is used to produce constituency parse for each sentence, which is converted to collapsed dependency parse using Stanford CoreNLP converter (Manning et al., 2014) with CCprocessed setting. We remove features that only appear once in the whole training set. Annotations (P/N) 1,000 / 4,611 464 / 775 125 / 1,783 4.2 4.1 Baselines The baseline is a LR model trained on the distantly labeled set without any filtering of noise. We also implement two previous methods for comparison. First, we train a LR model on the distantly labeled set filtered by a heuristic (DPFreq) proposed by Zheng and Blake (20"
W17-2323,P11-1055,0,0.0460694,"ation extraction was first proposed by Craven and Kumlien (1999) to extract protein-localization relation. Mintz et al. (2009) used Freebase relations to annotate articles in Wikipedia and trained a logistic regression model to extract 102 different types of relations. Riedel et al. (2010) proposed to use multi-instance learning to tolerate noise in the positively-labeled data. They relaxed the original assumption in distant supervision that all the positively-labeled sentences of an entity pair express the relation of interest and instead, they assume that at least one of the sentences does. Hoffmann et al. (2011) and Surdeanu et al. (2012) continued to augment the multi-instance model with a multi-label classifier for each entity pair, to exploit correlations and conflicts among different relations to improve performance. In these approaches, researchers focus on developing models that can tolerate noise and improve extraction performance on entity pair level. However, it is important to note that the noise is not explicitly removed from the labeled data, and extraction on sentence level is not optimized directly. Focusing on explicitly reducing noise from the distantly-labeled training data, Intxaurr"
W17-2323,P09-1113,0,0.146527,"nt of Medline abstracts and PMC full-length articles can support applying distant supervision for many biomedical relation extraction tasks. However, the main drawback of distant supervision is that the created data can be very noisy, due to the guideless heuristic labeling process. Wrongly labeled instances exist in both positively and negatively-labeled data. For example, consider the two labeled sentences below for protein-protein interaction. 2 Related Work Distant supervision for relation extraction was first proposed by Craven and Kumlien (1999) to extract protein-localization relation. Mintz et al. (2009) used Freebase relations to annotate articles in Wikipedia and trained a logistic regression model to extract 102 different types of relations. Riedel et al. (2010) proposed to use multi-instance learning to tolerate noise in the positively-labeled data. They relaxed the original assumption in distant supervision that all the positively-labeled sentences of an entity pair express the relation of interest and instead, they assume that at least one of the sentences does. Hoffmann et al. (2011) and Surdeanu et al. (2012) continued to augment the multi-instance model with a multi-label classifier"
W17-2323,P12-1000,0,0.127984,"Missing"
W17-2323,P15-2045,0,0.0142064,"cal and syntactic information of the entity mention pairs. Next, high-confidence patterns can be discovered using the purified P , which can then be used to remove noise from N . Experiments on three tasks, extraction of protein-protein interaction, miRNA-gene regulation relation and proteinlocalization event, show that our methods can improve the F-score by 6, 10 and 14 points over the 185 tence for each task below. used pseudo-relevance feedback to discover highconfidence related entity pairs which do not exist in the database, and removed negatively-labeled instances of these entity pairs. Roller et al. (2015) tried to reduce noise in the negatively-labeled data by inferring new relations of a knowledge graph using a random-walk algorithm. Roth et al. (2013) gave a nice review of some of the above methods. Distant supervision has also been applied to extract biomedical relation. Zheng and Blake (2015) used a heuristic based on dependency path frequency to reduce noise in the positively-labeled data for extraction of protein-localization relations. Thomas et al. (2011) used a list of words which are frequently employed to indicate protein interaction to filter out noise for protein-protein interacti"
W17-2323,W15-3802,0,0.0163596,"e noise in the negatively-labeled data by inferring new relations of a knowledge graph using a random-walk algorithm. Roth et al. (2013) gave a nice review of some of the above methods. Distant supervision has also been applied to extract biomedical relation. Zheng and Blake (2015) used a heuristic based on dependency path frequency to reduce noise in the positively-labeled data for extraction of protein-localization relations. Thomas et al. (2011) used a list of words which are frequently employed to indicate protein interaction to filter out noise for protein-protein interaction extraction. Roller and Stevenson (2015) tried to combine existing hand-labeled data with distantly labeled data to improve the performance for drug-condition relations. Multi-instance learning was used by Roller et al. (2015) to extract two subsets of relations in UMLS database with reduced noise by a path ranking algorithm, and by Lamurias et al. (2017) to extract miRNA-gene relations. 3 3.1 • PPI: Interaction of Shc with Grb2 regulates association of Grb2 with mSOS. • MIRGENE: MicroRNA-223 regulates FOXO1 expression and cell proliferation. • PLOC: The cyclin G1 protein was localized in nucleus. 3.2 Training Data Construction To c"
W17-2323,S13-2056,0,0.0809993,"Missing"
W17-2323,P13-2117,0,0.0333922,"Missing"
W17-2323,D12-1042,0,0.199625,"proposed by Craven and Kumlien (1999) to extract protein-localization relation. Mintz et al. (2009) used Freebase relations to annotate articles in Wikipedia and trained a logistic regression model to extract 102 different types of relations. Riedel et al. (2010) proposed to use multi-instance learning to tolerate noise in the positively-labeled data. They relaxed the original assumption in distant supervision that all the positively-labeled sentences of an entity pair express the relation of interest and instead, they assume that at least one of the sentences does. Hoffmann et al. (2011) and Surdeanu et al. (2012) continued to augment the multi-instance model with a multi-label classifier for each entity pair, to exploit correlations and conflicts among different relations to improve performance. In these approaches, researchers focus on developing models that can tolerate noise and improve extraction performance on entity pair level. However, it is important to note that the noise is not explicitly removed from the labeled data, and extraction on sentence level is not optimized directly. Focusing on explicitly reducing noise from the distantly-labeled training data, Intxaurrondo et al. (2013) proposed"
W17-2323,P12-1076,0,0.0214242,"ove extraction performance on entity pair level. However, it is important to note that the noise is not explicitly removed from the labeled data, and extraction on sentence level is not optimized directly. Focusing on explicitly reducing noise from the distantly-labeled training data, Intxaurrondo et al. (2013) proposed three simple heuristics to remove noise from the positively-labeled data. They tried to filter out positively-labeled instances that appear too frequently or have a large distance from their cluster centroid, or positive entity pairs that have a low partial mutual information. Takamatsu et al. (2012) proposed a statistical model to estimate P (relation|pattern), and removed positively-labeled instances that match a low-probability pattern. Xu et al. (2013) • hMdm2, p53i: Ribosomal protein S3: A multi-functional protein that interacts with both p53 and MDM2 through its KH domain. • hLRAP35a, MYO18Ai: LRAP35a binds independently to MYO18A and MRCK. In the first sentence, although the protein pair hMdm2, p53i are interacting with each other according to IntAct, no explicit description in the sentence expresses such an interaction relation. It is labeled as a positive instance by the heuristi"
W17-2326,de-marneffe-etal-2014-universal,0,0.0258331,"Missing"
W17-2326,W07-1018,0,0.470318,"Missing"
W17-2326,P89-1020,0,0.501605,"Missing"
W17-2326,C08-1031,0,0.0320943,"Missing"
W17-2326,P14-5010,0,0.00635861,"Missing"
W17-2326,N10-1004,0,0.0541681,"Missing"
W17-2326,A00-2018,0,0.0420958,"Missing"
W17-2326,P09-2039,0,0.0276833,"Missing"
W90-0101,P83-1012,0,0.076382,"Missing"
W90-0101,C88-2121,0,0.184134,"Missing"
W98-0104,P97-1026,0,0.0570599,"Missing"
W98-0104,W98-1419,0,0.016817,"e all sources and goals to be present in the elementary tree. Only PPs whose meaning is implicit in the meaning of the verb itself are present in the elementary tree, whereas all other PPs are adjoined. This is in contrast with the analysis provided by Levin and Rappaport Hovav (1995) in which all sources and goals are treated as arguments as a result of a lexical rule that applies to verbs of motion. The goal of our work is to capture lexical semantic properties that we hope will be helpful in reducing the search space in parsing, as well as aid in generation (SPUD; see Stone and Doran 1997; Stone and Webber 1998) and machine translation (in the transfer of lexical semantic properties) (see Palmer, et al. (to appear)). We have examined several subclasses of motion verbs, and posited features to capture their semantic properties. These features not only allow us to place restrictions on the verbs to constrain possible derivations, but also allow us to account for regular sense extensions through the underspecification of certain features and by having modifiers introduce these features in the course of the derivation. The Conative Construction and Elementary Trees The other case to consider is the conat"
W98-0112,J94-1004,0,0.0563701,"onted wh-element and both the C&apos; and its trace. From this perspective, we can now understand why it was necessary in the framework of Vijay-Shanker ( 1992) to posit a domination relation between the two C&apos; nodes in a in Figure 1: as an indirect representation of (at least) the principle requiring that moved elements c-command their traces. This proposal allows us to explain many previously stipulated properties of TAG elementary trees and constraints on the adjunction operation. Consider, first of all, the structural differences between two classes of auxiliary treeg noted by Kroch (1989) and Schabes and Shieber (1994): complement auxHiary trees on the one hand and modifier or athematic auxiliaries on the other. Recall that modifier auxiliariea bave the distinctive property tbat their foot node is the sister of a modifying phrase and is the daughter of the root node. Following the principles in (1), it follows that the foot of a modifier auxiliary will c-command its XP sister, i.e„ the adjunction site, though not vice versa. In contrast, the foot node of a complement auxiliary must be the sister of some he.ad of which it is a comnlement. Thus. this foot node will both c-command ~d be c-comm~ded by its siste"
W98-0112,C88-2147,1,0.785562,"t. of Computer and Information Sciences University of Delaware vijay@cis.udel.edu The TAG adjunction operation operates by splitting a tree at one node, which we will call the adjunction site. In the resulting structure, the subtrees above and below the adjunction site are separated by, and connected with, the auxiliary tree used in the composition. As the adjunction site is thus split into two nodes, with a copy in each subtree, a natural way of formalizing the adjunction operation posits that each potential adjunction site is in fact represented by two distinct nodes. In the FTAG formalism (Vijay-Shanker, 1988) each potential adjunction site is associated with two feature structures, one for each copy. As an alternative to this operationally defined rewriting view of adjunction, Vijay-Shanker (1992) suggests that TAG derivations instead be viewed as a monotonic growth of structural assertions that characterize the structures being composed. This proposal rests crucially on the a.cisumption that the elementary trees are characterized in terms of a domination relation among nodes, and that each potential adjunction . site is represented by two nodes standing in a domination relation. Under th.is propo"
W98-0112,J92-4004,1,0.809762,"In the resulting structure, the subtrees above and below the adjunction site are separated by, and connected with, the auxiliary tree used in the composition. As the adjunction site is thus split into two nodes, with a copy in each subtree, a natural way of formalizing the adjunction operation posits that each potential adjunction site is in fact represented by two distinct nodes. In the FTAG formalism (Vijay-Shanker, 1988) each potential adjunction site is associated with two feature structures, one for each copy. As an alternative to this operationally defined rewriting view of adjunction, Vijay-Shanker (1992) suggests that TAG derivations instead be viewed as a monotonic growth of structural assertions that characterize the structures being composed. This proposal rests crucially on the a.cisumption that the elementary trees are characterized in terms of a domination relation among nodes, and that each potential adjunction . site is represented by two nodes standing in a domination relation. Under th.is proposal, the structures a and ß in Figure 1 would be used to derive long-distance wh-movement. To adjoin ß into a, the root and foot nodes of ß are identified with the two C1 nodes standing in a d"
W98-0135,P95-1021,1,0.733935,"s EDL. • The geometry of adjunction (GA). By this term, we mean the specific, mathcmatical defiuition of the adjunction operation in TAG and, especially, the shape of the resulting derived tree. Specifically, an auxiliary tree ß has a designated footnode; when ß is adjoined in a tree a at node v, it is inserted in its entirety into a. In the process, ß remains intact, but a is divided in two subtrees at node v, with ß now attached at v and the subtree formerly rooted in v now attached to the footnode of ß. However, the question arises how other treerewriting formalisms such as D-Tree Grammar (Rambow et al., 1995) can handle whmovement. Specifically, the question arises whether an equally elegant solution to the problem of wh-movement can be found. In this paper, we propose to study e:icactly which what features of the formal (mathematical) definition of TAG contribute to the correct analysis of whmovement (in English). We will mainly concentrate on TAG, but occasionally mention treelocal MC-TAG. • The factoring of recursion (FR). By definition, in an auxiliary tree ß, the footnode and the root node must have the same }abel, A. Furthermore, ß can only be adjoined at a node labeled A. We observe that th"
W98-0143,C96-1034,0,0.526327,"wh-movement, in many different trees creates redundancy, which poses a problem for grammar development and maintenance {VijayShanker and Schabes, 1992). To consistently implement a change in some general aspect of the design of the grammar, all the relevant trees currently must be inspected and edited. Vijay Shanker and Schabes suggested the use of hierarchical organization and of tree descriptions to specify substructures that would be present in several elementary trees of a grammar. Since then, in addition to ourselves, Becker, {Becker, 1994), Evans et al. {Evans et al., 1995), and Candito{Candito, 1996) have developed systerns for organizing trees of a TAG which could be used for developing and maintaining grammars. Our system is based on the ideas expressed in Vijay-Shanker and Schabes, (Vijay-Shanker and Schabes, 1992), to use partial-tree descriptions in specifying a grammar by separately defining pieces of tree structures to encode independent syntactic principles. Various individual specifications are then combined to form the elementary trees of the grammar. Our paper begins with a description of our grammar development system and the process by which it generates 180 the Penn English"
W98-0143,E89-1009,0,0.0214411,"-subject extraction block. 3 We have not yet attempted to extend our coverage to include punctuation, it-clefts, and a few idiosyncratic analyses that are included in the sixty trees we are not generating. . 182 tures, so that the head features will propagate from modifiee to modified node, while non-head features from the predicate as the head of the modifier will be passed to the modified node. 4 Comparison to Other Work Evans, Gazdar and Weir (Evans et al., 1995) also discuss a method for organizing the trees in a TAG hierarchically, using an existing lexical representational system, DATR (Evans and Gazdar, 1989). Since DATR can not capture directly dominance relation in the trees, these must be simulated by using feature equations. There are substantial similarities and significant differences in our approach and Candito&apos;s approach, which she applied primarily to French and Italian. Both systems have built upon the basic ideas expressed in (Vijay-Shanker and Schabes, 1992) for organizing trees hierarchically and the use of tree descriptions that encode substructures found in several trees. The main difference is how Candito uses her dimensions in generating the trees. Her system imposes explicit cond"
W98-0143,P95-1011,0,0.65657,"e of tree substructures, such as wh-movement, in many different trees creates redundancy, which poses a problem for grammar development and maintenance {VijayShanker and Schabes, 1992). To consistently implement a change in some general aspect of the design of the grammar, all the relevant trees currently must be inspected and edited. Vijay Shanker and Schabes suggested the use of hierarchical organization and of tree descriptions to specify substructures that would be present in several elementary trees of a grammar. Since then, in addition to ourselves, Becker, {Becker, 1994), Evans et al. {Evans et al., 1995), and Candito{Candito, 1996) have developed systerns for organizing trees of a TAG which could be used for developing and maintaining grammars. Our system is based on the ideas expressed in Vijay-Shanker and Schabes, (Vijay-Shanker and Schabes, 1992), to use partial-tree descriptions in specifying a grammar by separately defining pieces of tree structures to encode independent syntactic principles. Various individual specifications are then combined to form the elementary trees of the grammar. Our paper begins with a description of our grammar development system and the process by which it gen"
W98-0143,C92-1034,1,0.845874,"aspect of the design of the grammar, all the relevant trees currently must be inspected and edited. Vijay Shanker and Schabes suggested the use of hierarchical organization and of tree descriptions to specify substructures that would be present in several elementary trees of a grammar. Since then, in addition to ourselves, Becker, {Becker, 1994), Evans et al. {Evans et al., 1995), and Candito{Candito, 1996) have developed systerns for organizing trees of a TAG which could be used for developing and maintaining grammars. Our system is based on the ideas expressed in Vijay-Shanker and Schabes, (Vijay-Shanker and Schabes, 1992), to use partial-tree descriptions in specifying a grammar by separately defining pieces of tree structures to encode independent syntactic principles. Various individual specifications are then combined to form the elementary trees of the grammar. Our paper begins with a description of our grammar development system and the process by which it generates 180 the Penn English grammar as well as a Chinese TAG. We describe the significant properties of both grammars, pointing out the major differences between them, and the methods by which our system is informed about these language-specific prop"
