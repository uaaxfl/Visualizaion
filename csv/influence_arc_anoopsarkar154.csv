2012.amta-papers.16,P09-1088,0,0.134955,"del performs better than simple filtering approaches both in terms of BLEU and model size. Alternately, some of the recent works have employed Bayesian techniques for inducing SCFG. Blunsom et al. (2008) proposed a generative model for deriving a sentence pair through a series of terminal and ITG-style non-terminal rules and used Variational Bayes for learning the SCFG rules. Their goal of learning a SCFG is at variance with our objective of extracting a compact Hiero grammar. A non-parametric Bayesian model using a Gibbs sampler to reason over the space of derivations has also been proposed (Blunsom et al., 2009). Though the model specifically uses priors to bias the grammar to be small, they do not compare the resulting grammar size. Additionally, the model suffered from weaker reordering ability and involve an additional step of extracting the SCFG rules using Hiero rule extraction algorithm on the sampled hierarchical alignments. However both these approaches use small datasets that range between 33K-300K sentence pairs. In contrast, our experiments use large datasets having 1.1M and 1.7M sentence pairs respectively for Ar-En and En-Es, with 2.2M-2.7M thresholded phrase pairs. More recently Sankara"
2012.amta-papers.16,J07-2003,0,0.928443,"ting phrase pairs from the Hiero grammar. We use Variational Bayes (VB) for inference. The Bayesian model induces a compact Hiero grammar that has comparable performance to the original Hiero grammar in terms of the translation quality, and even improves on the full Hiero grammar when faced with a small amount of bilingual training data. On different datasets, the VB method achieves a significant reduction in the grammar size. We analyze the different extracted grammars and explain why the Bayesian model works better. 2 1 Introduction Hierarchical phrase-based statistical machine translation (Chiang, 2007) has been shown to perform competitively with phrase-based and syntax-based models in several language pairs. A major issue with hierarchical phrase-based translation has been the size of the trained translation model, which is typically several times larger than the phrase-based counterpart trained from the same dataset. This leads to over-generation, search errors and a slower decoder (de Gispert et al., 2010). In this paper we propose two alternative approaches to induce compact Hiero grammars. Similar to the original Hiero rule extraction (Chiang, 2007), we consider the phrase pairs that a"
2012.amta-papers.16,D09-1075,0,0.0243409,"., 2008) has further details. 5 Experiments Corpora. We use three language pairs in our experiments: Arabic-English and English-Spanish (large bilingual data conditions), and KoreanEnglish (small bilingual data condition). Table 1 summarizes the statistics for the bilingual corpora used in this paper. For the language model, we use English Gigaword corpus (v4) for the ArabicEnglish and Korean-English translation tasks, and the WMT10 training data together with the UN data for the English-Spanish translation task and use 5gram models for all language pairs. We used the University of Rochester (Chung and Gildea, 2009) corpus for our Korean-English experiments without changing the tuning or test set splits, so our results are directly comparable to theirs. We also used the same rule-based morphological analyzer5 as Chung and Gildea (2009) to segment the Korean side of the bitext. SMT Models. We use our in-house implementation of Hiero (Chiang, 2007) with the standard features such as forward and reverse translation probabilities and lexical weights, phrase and word penalties, glue penalty and language model feature. For each experiment, we use MERT (Och, 2003) to optimize the feature weights on a tuning set"
2012.amta-papers.16,J10-3008,0,0.0439681,"Missing"
2012.amta-papers.16,P06-1121,0,0.25849,"the rules in contrast to two non-terminals allowed by Hiero. However, we note that our model does capture reordering as well as discontiguous phrases (a key feature of Hiero). In terms of the reordering abilities, our model lies between the hierarchical phrasebased and phrase-based models. Our model allows the unaligned source words to be attached at all possible positions in the derivation tree. This results in multiple interpretations of the unaligned words reflecting through large number of derivations, which include wider and richer rule contexts. This is analogous to the method used in (Galley et al., 2006) for context-rich syntactic translation models and we hope this to be useful in the Hiero models as well. In contrast the original Hiero grammar extraction restricts the unaligned words to be attached only to the top most position and so it can participate in just a single derivation. To make VB inference practical, we need to efficiently enumerate all the derivations for a phrase pair such that they are consistent with the given word alignments. We use the factorization algorithm proposed by (Zhang et al., 2008) which encodes wordaligned phrase pairs as a compact alignment tree. (Zhang et al."
2012.amta-papers.16,E09-1044,0,0.155811,"Missing"
2012.amta-papers.16,D07-1103,0,0.0640563,"an the corresponding Hiero 1NT rules. Related Works Some earlier works have focussed on reducing the Hiero grammar size by eliminating rule redundancies in some form such as by discarding rules that can be obtained by monotonically composing the smaller rules (He et al., 2009) or by filtering the grammar, based on certain patterns of hierarchical rules in which the useful patterns were identified in a greedy fashion (Iglesias et al., 2009). Yang and Zheng (2009) applied the Fisher’s exact significance test for pruning the translation model, which has been earlier used for phrase-based models (Johnson et al., 2007). As we showed in our ArabicEnglish experiments, our Bayesian model performs better than simple filtering approaches both in terms of BLEU and model size. Alternately, some of the recent works have employed Bayesian techniques for inducing SCFG. Blunsom et al. (2008) proposed a generative model for deriving a sentence pair through a series of terminal and ITG-style non-terminal rules and used Variational Bayes for learning the SCFG rules. Their goal of learning a SCFG is at variance with our objective of extracting a compact Hiero grammar. A non-parametric Bayesian model using a Gibbs sampler"
2012.amta-papers.16,J04-4002,0,0.0787864,"with phrase-based and syntax-based models in several language pairs. A major issue with hierarchical phrase-based translation has been the size of the trained translation model, which is typically several times larger than the phrase-based counterpart trained from the same dataset. This leads to over-generation, search errors and a slower decoder (de Gispert et al., 2010). In this paper we propose two alternative approaches to induce compact Hiero grammars. Similar to the original Hiero rule extraction (Chiang, 2007), we consider the phrase pairs that are consistent with the word alignments (Och and Ney, 2004) as the starting point in this work. Our first approach learns a minimal grammar by solving a combinatorial optimization problem over a tripartite graph consisting of three types of nodes: phrase pairs, Anoop Sarkar Simon Fraser University Burnaby BC. Canada anoop@cs.sfu.ca Motivation Hierarchical phrase-based translation (Chiang, 2007) model uses a particular type of synchronous context-free grammar (SCFG) over the source and target languages. Unlike typical SCFGs, the rules are lexicalized on the right hand side with at least one aligned word pair in source and target and the grammar has one"
2012.amta-papers.16,P03-1021,0,0.00756982,"used the University of Rochester (Chung and Gildea, 2009) corpus for our Korean-English experiments without changing the tuning or test set splits, so our results are directly comparable to theirs. We also used the same rule-based morphological analyzer5 as Chung and Gildea (2009) to segment the Korean side of the bitext. SMT Models. We use our in-house implementation of Hiero (Chiang, 2007) with the standard features such as forward and reverse translation probabilities and lexical weights, phrase and word penalties, glue penalty and language model feature. For each experiment, we use MERT (Och, 2003) to optimize the feature weights on a tuning set, and evaluate using the corresponding optimal weights on the test set. To ensure robustness in Korean-English small data condition, we run MERT three times. The official NIST BLEU script6 is used for computing the case-insensitive BLEU scores. Evaluation. We compare our two translation grammar induction methods, based on variational Bayes (VB) and combinatorial optimization (Greedy), against the following grammars: • Original Hiero (2NT). The grammar as extracted by the original rule extraction algorithm (Chiang, 2007) with two non-terminals, •"
2012.amta-papers.16,W11-2167,1,0.943817,"xr of rule r. The base measure of a translation rule p0 (r) is the arithmetic mean of the two alignment scores above. p0 (r) ∝ (lfxr + lbxr )/2. Let lx be the geometric mean4 of the forward and backward alignment score over an initial phrase pair  1 2 x ∈ P, lx ∝ lfx lbx . We place a Beta(lx , 0.5) prior over the Bernoulli distribution that decides the derivation type zd and this is normalized by the sum of lexical weights from all phrase pairs. The Beta prior prefers to consolidate a phrase pair fragment (within a larger phrase-pair) having a higher lx as a single rule. This is similar to (Sankaran et al., 2011) and we discuss the differences between the two models in Section 6. 4.2 Variational Inference Variational inference (Ghahramani and Beal, 2000; Attias, 2000) is an approximation technique typically used in Bayesian settings. It is used for approximating an intractable posterior distribution p(Φ; θ) 3 If there are multiple alignments for xr (based on multiple initial phrase pairs), we take the union of these alignments as a. 4 The reason for picking arithmetic mean for p0 and geometric mean for lx is explained in (Sankaran et al., 2011). by finding a tractable variational distribution q(Φ; θ)"
2012.amta-papers.16,P09-2060,0,0.444988,"on variational Bayes (VB) and combinatorial optimization (Greedy), against the following grammars: • Original Hiero (2NT). The grammar as extracted by the original rule extraction algorithm (Chiang, 2007) with two non-terminals, • Original Hiero (1NT). A variant of the original 5 6 http://nlp.kookmin.ac.kr/HAM/eng/main-e.html ftp://jaguar.ncsl.nist.gov/mt/resources/mteval-v13.pl Lang Pair Ar-En En-Es Ko-En Dataset ISI Ar-En corpus WMT10: no UN URochester data Train/ Tune/ Test Grammar BLEU Model Size Speed (sent/min) 1.1 M/ 1982/ 987 1.7 M/ 5061/ 2489 59218/ 1118/ 1118 Original Hiero (2NT) - Yang and Zheng (2009) - Iglesias et al. (2009) filtered - Pruned (mincount 1.0) 33.11 32.84 32.52 31.68 4.82 4.70 3.59 2.24 3.62 3.73 4.99 5.57 Original Hiero (1NT) - Yang and Zheng (2009) - Iglesias et al. (2009) filtered - Pruned (mincount 1.0) 33.08 32.80 32.40 31.64 3.71 3.59 3.43 2.28 4.43 4.87 5.36 5.70 Greedy Approach 31.20 1.88 6.53 Variational Bayes - Pruned (mincount 1.0) - Pruned (mincount 1.5) 33.13 33.05 32.44 3.75 2.90 1.84 4.62 4.87 5.33 Table 1: Corpus Statistics in # of sentences rule extraction algorithm where the number of nonterminals is restricted to one7 . We compare the model size and BLEU s"
2012.amta-papers.16,D12-1089,0,0.0288271,"Missing"
2012.amta-papers.16,C08-1136,0,0.285114,"include wider and richer rule contexts. This is analogous to the method used in (Galley et al., 2006) for context-rich syntactic translation models and we hope this to be useful in the Hiero models as well. In contrast the original Hiero grammar extraction restricts the unaligned words to be attached only to the top most position and so it can participate in just a single derivation. To make VB inference practical, we need to efficiently enumerate all the derivations for a phrase pair such that they are consistent with the given word alignments. We use the factorization algorithm proposed by (Zhang et al., 2008) which encodes wordaligned phrase pairs as a compact alignment tree. (Zhang et al., 2008) has further details. 5 Experiments Corpora. We use three language pairs in our experiments: Arabic-English and English-Spanish (large bilingual data conditions), and KoreanEnglish (small bilingual data condition). Table 1 summarizes the statistics for the bilingual corpora used in this paper. For the language model, we use English Gigaword corpus (v4) for the ArabicEnglish and Korean-English translation tasks, and the WMT10 training data together with the UN data for the English-Spanish translation task a"
2012.amta-papers.16,C08-1144,0,0.0618312,"oy pruning based on fisher significance test (Yang and Zheng, 2009) to reduce the Hiero model. We also provide results for the pattern-based filtering (Iglesias et al., 2009) that filters the grammar extracted by the original rule extraction algorithm based on certain patterns that are found to be least useful in translation or in improving the quality. And finally, we apply a fixed count cut-off on the pseudo counts of the grammar rules and eliminate all rules having pseudo counts fewer than 1.0 (we call this parameter mincount). This is somewhat similar to the pruning of hierarchical rules (Zollmann et al., 2008) based on a threshold, except that here 7 The Hiero rule extraction algorithm can be trivially modified to limit to 1 NT grammar by replacing only one sub-phrase pair (in a larger phrase pair) with non-terminal X. Other rule extraction constraints are still applied. 8 Following Sankaran et al. (2011), we add the coverage phrase pairs (those with non-decomposable source-target alignments) without the threshold limit to avoid OOVs (in training). Table 2: Arabic-English (Threshold-3): Results. Model sizes is in millions. Boldface indicate the best setting of high BLEU, model size and decoding spe"
2014.amta-researchers.1,P11-1103,0,0.0502695,"Missing"
2014.amta-researchers.1,P09-1088,0,0.0395873,"Missing"
2014.amta-researchers.1,2012.eamt-1.42,0,0.0307714,"(a hierarchical representation of all the phrase pairs in linear time, which yields a set of minimal Hiero (SCFG) rules. They discuss that the method can be modified to extract all Hiero rules. But the algorithm is just applied as an analytical tool for aligned bilingual data. Syntax-based translation systems, tree-to-tree (Ding and Palmer, 2005), tree-to-string (Liu et al., 2006; Huang, 2006) and string-to-tree (Galley et al., 2006), extract sentence level rules, but they extract rules from parse trees (on source or target) rather word aligned sentence pairs which we discussed in this paper. Braune et al. (2012) extend Hiero by extracting an additional and separate set of rules for long-distance reorderings. They modify Hiero extractor based on some analysis on longdistance German-to-English movement and filtered them based on linguistic information. New rules are applied to long spans (11 to 50) but do not improve translation quality in terms of BLEU (in some case BLEU scores reduce by 0.4). However they show that their approach helps in terms of improving the reordering between source and target (using LRscore (Birch and Osborne, 2011) evaluation scores and some manual evaluation). 6 Conclusion We"
2014.amta-researchers.1,P05-1033,0,0.0731327,"eses((H, R), F) uses current hypothesis H and rule R to produce new hypotheses. The first best hypothesis, h0 along with its score h0c and corresponding cube (H, R) is placed in a priority queue heapQ (line 22 in Algorithm 1). Iteratively the K best hypotheses in the queue are popped (line 25) and for each hypothesis its neighbours in the cube are added to the priority queue (line 26). Decoding finishes when stack Sn has been filled. 3 Rule Extraction Hiero uses a synchronous context free grammar (SCFG), X → hγ, αi, where X is a nonterminal, γ and α are strings of terminals and non-terminals (Chiang, 2005, 2007). Unlike typical SCFGs, the rules are lexicalized on the right hand side with at least one aligned word pair in source and target. 4 As the length of rules is limited (at most MRL), we can ignore stacks with index less than i − MRL Al-Onaizan & Simard (Eds.) Proceedings of AMTA 2014, vol. 1: MT Researchers Vancouver, BC © The Authors 3 rules hypotheses 〈 ht , h s , hcov , hc 〉 G 1) X →〈 schuler X 1 / students X 1 〉 G 2) X →〈 X 1 heban X 2 /have X 1 X 2 〉 3 ) X →〈 X 1 noch nicht X 2 /not yet X 2 X 1 〉 4 ) X →〈 gemacht /done 〉 5 ) X →〈 ihre arbeit /their work 〉 6 ) X →〈 ./ . 〉 〈<s> , ⟦[0,"
2014.amta-researchers.1,J07-2003,0,0.86733,"ults in a more expressive grammar. LR-decoding can be used to decode with SCFGs with more than two non-terminals, but the CKY decoders used for Hiero systems cannot deal with such expressive grammars due to a blowup in computational complexity. In this paper we present a dynamic programming algorithm for GNF rule extraction which efficiently extracts sentence level SCFG rule sets with an arbitrary number of non-terminals. We analyze the performance of the obtained grammar for statistical machine translation on three language pairs. 1 Introduction Hierarchical phrase-based translation (Hiero) (Chiang, 2007) uses a lexicalized synchronous context-free grammar (SCFG) extracted from word and phrase alignments of a bitext. Decoding for Hiero is typically done with CKY-style decoding with time complexity O(n3 ) for source input with n words. Computing the language model score for each hypothesis within CKY decoding requires two histories, the left and the right edge of each span. This is due to the fact that the target side is built inside-out from sub-spans (Heafield et al., 2011, 2013). LR-decoding algorithms exist for phrase-based (Koehn, 2004; Galley and Manning, 2010) and syntax-based (Huang and"
2014.amta-researchers.1,D10-1053,0,0.0280203,"Missing"
2014.amta-researchers.1,P05-1067,0,0.0437776,"rules are used to obtain the word alignments rather than the SCFG rules for decoding. Unrestricted number of non-terminals makes the induced grammar unusable in CKY based decoders. Zhang et al. (2008) encode the word aligned sentence pair as a normalized decomposition tree (a hierarchical representation of all the phrase pairs in linear time, which yields a set of minimal Hiero (SCFG) rules. They discuss that the method can be modified to extract all Hiero rules. But the algorithm is just applied as an analytical tool for aligned bilingual data. Syntax-based translation systems, tree-to-tree (Ding and Palmer, 2005), tree-to-string (Liu et al., 2006; Huang, 2006) and string-to-tree (Galley et al., 2006), extract sentence level rules, but they extract rules from parse trees (on source or target) rather word aligned sentence pairs which we discussed in this paper. Braune et al. (2012) extend Hiero by extracting an additional and separate set of rules for long-distance reorderings. They modify Hiero extractor based on some analysis on longdistance German-to-English movement and filtered them based on linguistic information. New rules are applied to long spans (11 to 50) but do not improve translation qualit"
2014.amta-researchers.1,D12-1109,0,0.0163588,"icalized synchronous context-free grammar (SCFG) extracted from word and phrase alignments of a bitext. Decoding for Hiero is typically done with CKY-style decoding with time complexity O(n3 ) for source input with n words. Computing the language model score for each hypothesis within CKY decoding requires two histories, the left and the right edge of each span. This is due to the fact that the target side is built inside-out from sub-spans (Heafield et al., 2011, 2013). LR-decoding algorithms exist for phrase-based (Koehn, 2004; Galley and Manning, 2010) and syntax-based (Huang and Mi, 2010; Feng et al., 2012) models and also for hierarchical phrase-based models (Watanabe et al., 2006; Siahbani et al., 2013), which is our focus in this paper. Watanabe et al. (2006) was the first to propose a left-to-right (LR) decoding algorithm for Hiero (henceforth we refer to LR decoding for Hiero as LR-Hiero) which uses beam search and runs in O(n2 b) (in practice) where n is the length of source sentence and b is the size of beam (Huang and Mi, 2010). To simplify target generation, synchronous context-free grammar (SCFG) rules are constrained to be prefix-lexicalized on target side, aka Greibach Normal Form (G"
2014.amta-researchers.1,P06-1121,0,0.0458614,"tricted number of non-terminals makes the induced grammar unusable in CKY based decoders. Zhang et al. (2008) encode the word aligned sentence pair as a normalized decomposition tree (a hierarchical representation of all the phrase pairs in linear time, which yields a set of minimal Hiero (SCFG) rules. They discuss that the method can be modified to extract all Hiero rules. But the algorithm is just applied as an analytical tool for aligned bilingual data. Syntax-based translation systems, tree-to-tree (Ding and Palmer, 2005), tree-to-string (Liu et al., 2006; Huang, 2006) and string-to-tree (Galley et al., 2006), extract sentence level rules, but they extract rules from parse trees (on source or target) rather word aligned sentence pairs which we discussed in this paper. Braune et al. (2012) extend Hiero by extracting an additional and separate set of rules for long-distance reorderings. They modify Hiero extractor based on some analysis on longdistance German-to-English movement and filtered them based on linguistic information. New rules are applied to long spans (11 to 50) but do not improve translation quality in terms of BLEU (in some case BLEU scores reduce by 0.4). However they show that their"
2014.amta-researchers.1,N10-1140,0,0.0884392,"hical phrase-based translation (Hiero) (Chiang, 2007) uses a lexicalized synchronous context-free grammar (SCFG) extracted from word and phrase alignments of a bitext. Decoding for Hiero is typically done with CKY-style decoding with time complexity O(n3 ) for source input with n words. Computing the language model score for each hypothesis within CKY decoding requires two histories, the left and the right edge of each span. This is due to the fact that the target side is built inside-out from sub-spans (Heafield et al., 2011, 2013). LR-decoding algorithms exist for phrase-based (Koehn, 2004; Galley and Manning, 2010) and syntax-based (Huang and Mi, 2010; Feng et al., 2012) models and also for hierarchical phrase-based models (Watanabe et al., 2006; Siahbani et al., 2013), which is our focus in this paper. Watanabe et al. (2006) was the first to propose a left-to-right (LR) decoding algorithm for Hiero (henceforth we refer to LR decoding for Hiero as LR-Hiero) which uses beam search and runs in O(n2 b) (in practice) where n is the length of source sentence and b is the size of beam (Huang and Mi, 2010). To simplify target generation, synchronous context-free grammar (SCFG) rules are constrained to be prefi"
2014.amta-researchers.1,W11-2123,0,0.152614,"Missing"
2014.amta-researchers.1,2011.iwslt-evaluation.24,0,0.0132522,"rammar for statistical machine translation on three language pairs. 1 Introduction Hierarchical phrase-based translation (Hiero) (Chiang, 2007) uses a lexicalized synchronous context-free grammar (SCFG) extracted from word and phrase alignments of a bitext. Decoding for Hiero is typically done with CKY-style decoding with time complexity O(n3 ) for source input with n words. Computing the language model score for each hypothesis within CKY decoding requires two histories, the left and the right edge of each span. This is due to the fact that the target side is built inside-out from sub-spans (Heafield et al., 2011, 2013). LR-decoding algorithms exist for phrase-based (Koehn, 2004; Galley and Manning, 2010) and syntax-based (Huang and Mi, 2010; Feng et al., 2012) models and also for hierarchical phrase-based models (Watanabe et al., 2006; Siahbani et al., 2013), which is our focus in this paper. Watanabe et al. (2006) was the first to propose a left-to-right (LR) decoding algorithm for Hiero (henceforth we refer to LR decoding for Hiero as LR-Hiero) which uses beam search and runs in O(n2 b) (in practice) where n is the length of source sentence and b is the size of beam (Huang and Mi, 2010). To simplif"
2014.amta-researchers.1,N13-1116,0,0.023704,"Missing"
2014.amta-researchers.1,2006.amta-papers.8,0,0.0319864,"he SCFG rules for decoding. Unrestricted number of non-terminals makes the induced grammar unusable in CKY based decoders. Zhang et al. (2008) encode the word aligned sentence pair as a normalized decomposition tree (a hierarchical representation of all the phrase pairs in linear time, which yields a set of minimal Hiero (SCFG) rules. They discuss that the method can be modified to extract all Hiero rules. But the algorithm is just applied as an analytical tool for aligned bilingual data. Syntax-based translation systems, tree-to-tree (Ding and Palmer, 2005), tree-to-string (Liu et al., 2006; Huang, 2006) and string-to-tree (Galley et al., 2006), extract sentence level rules, but they extract rules from parse trees (on source or target) rather word aligned sentence pairs which we discussed in this paper. Braune et al. (2012) extend Hiero by extracting an additional and separate set of rules for long-distance reorderings. They modify Hiero extractor based on some analysis on longdistance German-to-English movement and filtered them based on linguistic information. New rules are applied to long spans (11 to 50) but do not improve translation quality in terms of BLEU (in some case BLEU scores red"
2014.amta-researchers.1,D10-1027,0,0.161318,"ng, 2007) uses a lexicalized synchronous context-free grammar (SCFG) extracted from word and phrase alignments of a bitext. Decoding for Hiero is typically done with CKY-style decoding with time complexity O(n3 ) for source input with n words. Computing the language model score for each hypothesis within CKY decoding requires two histories, the left and the right edge of each span. This is due to the fact that the target side is built inside-out from sub-spans (Heafield et al., 2011, 2013). LR-decoding algorithms exist for phrase-based (Koehn, 2004; Galley and Manning, 2010) and syntax-based (Huang and Mi, 2010; Feng et al., 2012) models and also for hierarchical phrase-based models (Watanabe et al., 2006; Siahbani et al., 2013), which is our focus in this paper. Watanabe et al. (2006) was the first to propose a left-to-right (LR) decoding algorithm for Hiero (henceforth we refer to LR decoding for Hiero as LR-Hiero) which uses beam search and runs in O(n2 b) (in practice) where n is the length of source sentence and b is the size of beam (Huang and Mi, 2010). To simplify target generation, synchronous context-free grammar (SCFG) rules are constrained to be prefix-lexicalized on target side, aka Gre"
2014.amta-researchers.1,koen-2004-pharaoh,0,0.0717549,"ction Hierarchical phrase-based translation (Hiero) (Chiang, 2007) uses a lexicalized synchronous context-free grammar (SCFG) extracted from word and phrase alignments of a bitext. Decoding for Hiero is typically done with CKY-style decoding with time complexity O(n3 ) for source input with n words. Computing the language model score for each hypothesis within CKY decoding requires two histories, the left and the right edge of each span. This is due to the fact that the target side is built inside-out from sub-spans (Heafield et al., 2011, 2013). LR-decoding algorithms exist for phrase-based (Koehn, 2004; Galley and Manning, 2010) and syntax-based (Huang and Mi, 2010; Feng et al., 2012) models and also for hierarchical phrase-based models (Watanabe et al., 2006; Siahbani et al., 2013), which is our focus in this paper. Watanabe et al. (2006) was the first to propose a left-to-right (LR) decoding algorithm for Hiero (henceforth we refer to LR decoding for Hiero as LR-Hiero) which uses beam search and runs in O(n2 b) (in practice) where n is the length of source sentence and b is the size of beam (Huang and Mi, 2010). To simplify target generation, synchronous context-free grammar (SCFG) rules"
2014.amta-researchers.1,P07-2045,0,0.00703859,"g with just a sentence-initial marker hsi and the list hs containing a span of the whole sentence, [0, n]. The hypotheses are stored in stacks S0 , . . . , Sn , where Sp contains hypotheses covering p source words just as in stack decoding for phrase-based SMT (Koehn et al., 2003). to convert an arbitrary SCFG to a weakly equivalent SCFG with rules constrained to be prefix-lexicalized on the target side. 2 Greibach Normal Form (GNF). Just the target side is prefix lexicalized (GNF form), not the synchronous grammar. 3 The future cost is precomputed in a way similar to the phrase-based models (Koehn et al., 2007) using only the terminal rules of the grammar. Al-Onaizan & Simard (Eds.) Proceedings of AMTA 2014, vol. 1: MT Researchers Vancouver, BC © The Authors 2 Algorithm 1 LR-Hiero Decoding with CP 1: Input sentence: f = f0 f1 . . . fn 2: F = FutureCost(f ) (Precompute future cost3 for spans) (Create empty initial stack) (Initial hypothesis 4-tuple) (Push initial hyp into first Stack) 3: S0 = {} 4: h0 = (hsi, [[0, n]], ∅, F[0,n] ) 5: Add h0 to S0 6: for i = 1, . . . , n do 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: cubeList = {} for p = max(i − MRL, 0), . . . , i − 1 do {G} = Grouped(Sp ) for g ∈ {G} d"
2014.amta-researchers.1,N03-1017,0,0.00654246,"artial hypothesis h is a 4-tuple (ht , hs , hcov , hc ): consisting of a translation prefix ht , a (LIFO-ordered) list hs of uncovered spans, source words coverage set hcov and the hypothesis cost hc which includes future cost and a score computed based on feature values (using a log-linear model). The initial hypothesis is a null string with just a sentence-initial marker hsi and the list hs containing a span of the whole sentence, [0, n]. The hypotheses are stored in stacks S0 , . . . , Sn , where Sp contains hypotheses covering p source words just as in stack decoding for phrase-based SMT (Koehn et al., 2003). to convert an arbitrary SCFG to a weakly equivalent SCFG with rules constrained to be prefix-lexicalized on the target side. 2 Greibach Normal Form (GNF). Just the target side is prefix lexicalized (GNF form), not the synchronous grammar. 3 The future cost is precomputed in a way similar to the phrase-based models (Koehn et al., 2007) using only the terminal rules of the grammar. Al-Onaizan & Simard (Eds.) Proceedings of AMTA 2014, vol. 1: MT Researchers Vancouver, BC © The Authors 2 Algorithm 1 LR-Hiero Decoding with CP 1: Input sentence: f = f0 f1 . . . fn 2: F = FutureCost(f ) (Precompute"
2014.amta-researchers.1,D12-1021,0,0.0186991,"(Eds.) Proceedings of AMTA 2014, vol. 1: MT Researchers Vancouver, BC © The Authors 10 number of language model calls since that directly corresponds to the number of hypotheses considered by the decoder, consequently the speed of decoder. Figure 5 shows the results in terms of average number of language model queries and times in milliseconds on a sample set of 50 sentences from test sets. 5 Related Work Many approaches have been developed to improve SCFG rules for Hiero. Some of the works have employed generative methods using Bayesian techniques to induce SCFG (Blunsom et al., 2008, 2009; Levenberg et al., 2012; Sankaran et al., 2012a) directly from bilingual data without word alignments. de Gispert et al. (2010) extract rules based on posterior distributions provided by the HMM word-to-word alignment model, rather than a single alignment which is used in original Hiero. Most of these approaches restrict the grammar to rules with one or at most two non-terminals to be able to use the grammar in decoding (Blunsom et al., 2008; de Gispert et al., 2010; Sankaran et al., 2012a). Recently Levenberg et al. (2012) propose an approach to learn grammars with unrestricted number of non-terminals but do not us"
2014.amta-researchers.1,P06-1077,0,0.038134,"ents rather than the SCFG rules for decoding. Unrestricted number of non-terminals makes the induced grammar unusable in CKY based decoders. Zhang et al. (2008) encode the word aligned sentence pair as a normalized decomposition tree (a hierarchical representation of all the phrase pairs in linear time, which yields a set of minimal Hiero (SCFG) rules. They discuss that the method can be modified to extract all Hiero rules. But the algorithm is just applied as an analytical tool for aligned bilingual data. Syntax-based translation systems, tree-to-tree (Ding and Palmer, 2005), tree-to-string (Liu et al., 2006; Huang, 2006) and string-to-tree (Galley et al., 2006), extract sentence level rules, but they extract rules from parse trees (on source or target) rather word aligned sentence pairs which we discussed in this paper. Braune et al. (2012) extend Hiero by extracting an additional and separate set of rules for long-distance reorderings. They modify Hiero extractor based on some analysis on longdistance German-to-English movement and filtered them based on linguistic information. New rules are applied to long spans (11 to 50) but do not improve translation quality in terms of BLEU (in some case B"
2014.amta-researchers.1,P03-1021,0,0.0139897,"Missing"
2014.amta-researchers.1,2012.amta-papers.16,1,0.913863,"orithm stops without considering subphrases at the left side of the unaligned word. To avoid this problem, unaligned words on the target side will be attached to the closest left phrase pair (if it exists) during computation of LRS. 4 Experiments To evaluate our rule extraction algorithm, we use it to extract the grammar for LR-Hiero on three language pairs: German-English (De-En), Czech-English (Cs-En) and Chinese-English (Zh-En). Table 1 shows the details of datasets. We use 2 baselines: (i) LR-Hiero in Python (we use the implementation described in (Siahbani and Sarkar, 2014)); (ii) Kriya (Sankaran et al., 2012b), an open-source implementation of Hiero in Python (available on https://github.com/sfu-natlang/Kriya) which performs comparably to other open-source Hiero systems. Both systems are in Python and use the same LM wrapper which allows us to make a fair comparison of LM calls and time differences in decoding. We use rule extraction of Kriya to extract Hiero (SCFG) and modify it to extract LR-Hiero (GNF). Both grammars use similar configuration and settings: rule arity 2, maximum source length 7, initial phrase pairs of length at most 10. We use our rule extraction algorithm to extract GNF rules"
2014.amta-researchers.1,D13-1110,1,0.733917,"bitext. Decoding for Hiero is typically done with CKY-style decoding with time complexity O(n3 ) for source input with n words. Computing the language model score for each hypothesis within CKY decoding requires two histories, the left and the right edge of each span. This is due to the fact that the target side is built inside-out from sub-spans (Heafield et al., 2011, 2013). LR-decoding algorithms exist for phrase-based (Koehn, 2004; Galley and Manning, 2010) and syntax-based (Huang and Mi, 2010; Feng et al., 2012) models and also for hierarchical phrase-based models (Watanabe et al., 2006; Siahbani et al., 2013), which is our focus in this paper. Watanabe et al. (2006) was the first to propose a left-to-right (LR) decoding algorithm for Hiero (henceforth we refer to LR decoding for Hiero as LR-Hiero) which uses beam search and runs in O(n2 b) (in practice) where n is the length of source sentence and b is the size of beam (Huang and Mi, 2010). To simplify target generation, synchronous context-free grammar (SCFG) rules are constrained to be prefix-lexicalized on target side, aka Greibach Normal Form (GNF). Throughout this paper we abuse the notation for simplicity and use the term GNF grammars for su"
2014.amta-researchers.1,D14-1028,1,0.413275,"j] for all i<j will be empty and the algorithm stops without considering subphrases at the left side of the unaligned word. To avoid this problem, unaligned words on the target side will be attached to the closest left phrase pair (if it exists) during computation of LRS. 4 Experiments To evaluate our rule extraction algorithm, we use it to extract the grammar for LR-Hiero on three language pairs: German-English (De-En), Czech-English (Cs-En) and Chinese-English (Zh-En). Table 1 shows the details of datasets. We use 2 baselines: (i) LR-Hiero in Python (we use the implementation described in (Siahbani and Sarkar, 2014)); (ii) Kriya (Sankaran et al., 2012b), an open-source implementation of Hiero in Python (available on https://github.com/sfu-natlang/Kriya) which performs comparably to other open-source Hiero systems. Both systems are in Python and use the same LM wrapper which allows us to make a fair comparison of LM calls and time differences in decoding. We use rule extraction of Kriya to extract Hiero (SCFG) and modify it to extract LR-Hiero (GNF). Both grammars use similar configuration and settings: rule arity 2, maximum source length 7, initial phrase pairs of length at most 10. We use our rule extra"
2014.amta-researchers.1,P06-1098,0,0.745374,"Missing"
2014.amta-researchers.1,C08-1136,0,0.154587,"Authors 4 extraction from a sentence pair that is linear in the output length (the number of GNF rules). e0 e1 e2 e3 e4 f0 f1 f2 f3 f4 e5 Figure 2: Example phrase pair with alignments. ([0,5],[0,4]) ([4,5],[3,4]) ([0,2],[0,2]) ([0,1],[0,1]) ([2,2],[2,2]) ([0,0],[0,0]) ([1,1],[1,1]) Figure 3: Decomposed alignment tree for the example alignment in Fig. 2. 3.2 Phrase Pair Extraction Unlike Hiero rule extraction, we do not limit the length of initial phrase pairs and extract rules from all phrase pairs (including whole sentence pairs) in the training data. A modified version of the algorithm by (Zhang et al., 2008) is used to efficiently extract phrase pairs. For a phrase pair with a given alignment as shown in Figure 2, Zhang et al. (2008) generalize the O(n + K) time algorithm for computing all K common intervals of two different permutations of length n. The contiguous blocks of the alignment are captured as the nodes in the alignment tree and the tree structure (for example, phrase pair in Figure 2 is shown in Figure 3). The italicized nodes form a left-branching chain in the alignment tree and the sub-spans of this chain also lead to alignment nodes that are not explicitly captured in the tree (Ple"
2014.amta-researchers.2,P09-1088,0,0.0202784,"lation models have been noticed earlier (DeNero and Klein, 2007). Apart from showing the incompatibility, they also propose an unsupervised HMM alignment model that soft constrains the alignments conditioned on the target sentences and the corresponding (automatically generated) parse trees. The main difference is that our approach seek to improve alignments of different granularities and not just the word-level alignments. Several other works have focussed on learning phrase alignments from synchronous derivations using non-lexicalized (Blunsom et al., 2008) or lexicalized (Hiero-style) ITG (Blunsom et al., 2009; Levenberg et al., 2012) rules and apply them for hierarchical phrase-based models. While these models extract ITG-style rules, they use them only for obtaining the alignment information. In other words, the extracted ITG-style rules are not directly used by a hierarchical translation decoder, which are in fact obtained from the alignments suggested by these rules. Thus the biggest drawback is that these models, strictly speaking, are alignment models and they use the heuristic rule extractor (Chiang, 2007) for learning the Hiero-style translation grammar. In contrast to these, Sankaran et al"
2014.amta-researchers.2,N10-1015,0,0.0980318,"eline. Based on the validation experiments involving two language pairs, our proposed iterative-cascade framework shows consistent gains over the traditional training pipeline for hierarchical translation. 1 Introduction Hierarchical phrase-based translation, similar to other statistical machine translation (SMT) models are trained in a series of steps that are disparate and often invoke heuristics. The training complexity as well as the modelling deficiencies in learning the translation rules using such multi-step, heuristic ridden pipeline have been documented in many previous publications (Burkett et al., 2010; DeNero and Klein, 2010; Saers et al., 2013a). Secondly the early steps in the training pipeline, are unaware of and are almost always at odds with, the final goal of training a translation model. As a specific example, the alignment models are trained early in the pipeline, isolated from the step that extracts translations and this could lead to sub-optimal alignments (DeNero and Klein, 2010). This is also true for the syntactic models that rely on word alignments to extract the translation rules that are consistent with those alignments (Galley et al., 2006; Chiang, 2007, inter alia). Consi"
2014.amta-researchers.2,D08-1092,0,0.015153,"speaking, are alignment models and they use the heuristic rule extractor (Chiang, 2007) for learning the Hiero-style translation grammar. In contrast to these, Sankaran et al. (2012, 2013) proposed a set of Bayesian models that directly learns the SCFG grammar. However these models only focus on the rule extraction part and rely on the heuristically extracted phrasal alignments. Instead our iterative-cascade framework simplifies the entire hierarchical translation training pipeline. On Joint models for PBMT: Several works have exploited word alignments to improving the performance of parsing (Burkett and Klein, 2008; Snyder et al., 2009) outside the machine translation setting. In the reverse direction syntactic parsing has been used to get better alignments (May and Knight, 2007; DeNero and Klein, 2007; Fossum et al., 2008) in the context of machine translations. Joint models for learning alignments and translation rules have been a fairly recent direction. A joint model using two syntactic parsers and combined with an ITG derivation to model alignments, enables the trees to diverge if required and otherwise encouraging the derivation to synchronize with the trees (Burkett et al., 2010). However it requ"
2014.amta-researchers.2,J07-2003,0,0.636302,"blications (Burkett et al., 2010; DeNero and Klein, 2010; Saers et al., 2013a). Secondly the early steps in the training pipeline, are unaware of and are almost always at odds with, the final goal of training a translation model. As a specific example, the alignment models are trained early in the pipeline, isolated from the step that extracts translations and this could lead to sub-optimal alignments (DeNero and Klein, 2010). This is also true for the syntactic models that rely on word alignments to extract the translation rules that are consistent with those alignments (Galley et al., 2006; Chiang, 2007, inter alia). Consider the example word-aligned phrase pair shown in Figure 1. The baseline Giza++ alignment incorrectly aligns the English the to the Chinese word 联合国 (united nations). While the aligner is not going to be perfect, the present serial pipeline does not allow the aligner to correct such mistakes or to adapt the alignments to yield better translation rules. Further the serial pipeline results in the propagation of the modelling deficiencies from the Al-Onaizan & Simard (Eds.) Proceedings of AMTA 2014, vol. 1: MT Researchers Vancouver, BC © The Authors 15 数 月 , months 联合国 , 难民 th"
2014.amta-researchers.2,D08-1033,0,0.172347,"his paper. We use the Bayesian hierarchical ITG alignment model (Neubig et al., 2011) for getting the phrasal alignments at the first step. For the phrase extraction step, we use the Bayesian model motivated by a lexical alignment prior employing Variational-Bayesian inference proposed by Sankaran et al. (2013), which operates on the extracted phrasal alignments in the earlier step. We now explain the two models briefly. 2.1 Alignments The joint model proposed by (Neubig et al., 2011) uses a phrasal-ITG based hierarchical model with a Pitman-Yor Process (PYP) prior. Unlike the earlier models (DeNero et al., 2008; Zhang et al., 2008) that extract minimal many-to-many phrase alignments, Neubig’s model extracts phrases of varying granularities. This is achieved by inverting the order to first generate the entire sentence/ phrase pair from a phrase distribution (Pt ) and then falls back to ITG derivation to divide the sentence/ phrase pair into shorter phrase-pairs (this effectively avoids the sparsity problem). Under this model each phrase pair gets some probability distribution Phier (he, f i : θx , θt ), where θx and θt are the parameters of symbol distribution and phrase table respectively. The phras"
2014.amta-researchers.2,P07-1003,0,0.17596,"gner Extractor Giza++ Heuristic Giza++ Var. Bayes Pialign Heuristic Pialign Var. Bayes Iterative-cascade (3 iters) B LEU 25.13 25.20 24.97 25.09 25.45 Table 3: Iterative-cascade framework: Arabic-English B LEU scores. For the iterative-cascade framework we ran Pialign and VB inferences independently for three runs and did a sample combination. The best B LEU score is in boldface. 4 Related work On models learning Alignments or Hiero-style grammar: The potential incompatibility between the word alignments and the translation rules for the syntactic translation models have been noticed earlier (DeNero and Klein, 2007). Apart from showing the incompatibility, they also propose an unsupervised HMM alignment model that soft constrains the alignments conditioned on the target sentences and the corresponding (automatically generated) parse trees. The main difference is that our approach seek to improve alignments of different granularities and not just the word-level alignments. Several other works have focussed on learning phrase alignments from synchronous derivations using non-lexicalized (Blunsom et al., 2008) or lexicalized (Hiero-style) ITG (Blunsom et al., 2009; Levenberg et al., 2012) rules and apply th"
2014.amta-researchers.2,P10-1147,0,0.0831426,"lidation experiments involving two language pairs, our proposed iterative-cascade framework shows consistent gains over the traditional training pipeline for hierarchical translation. 1 Introduction Hierarchical phrase-based translation, similar to other statistical machine translation (SMT) models are trained in a series of steps that are disparate and often invoke heuristics. The training complexity as well as the modelling deficiencies in learning the translation rules using such multi-step, heuristic ridden pipeline have been documented in many previous publications (Burkett et al., 2010; DeNero and Klein, 2010; Saers et al., 2013a). Secondly the early steps in the training pipeline, are unaware of and are almost always at odds with, the final goal of training a translation model. As a specific example, the alignment models are trained early in the pipeline, isolated from the step that extracts translations and this could lead to sub-optimal alignments (DeNero and Klein, 2010). This is also true for the syntactic models that rely on word alignments to extract the translation rules that are consistent with those alignments (Galley et al., 2006; Chiang, 2007, inter alia). Consider the example word-ali"
2014.amta-researchers.2,W08-0306,0,0.0141624,"models that directly learns the SCFG grammar. However these models only focus on the rule extraction part and rely on the heuristically extracted phrasal alignments. Instead our iterative-cascade framework simplifies the entire hierarchical translation training pipeline. On Joint models for PBMT: Several works have exploited word alignments to improving the performance of parsing (Burkett and Klein, 2008; Snyder et al., 2009) outside the machine translation setting. In the reverse direction syntactic parsing has been used to get better alignments (May and Knight, 2007; DeNero and Klein, 2007; Fossum et al., 2008) in the context of machine translations. Joint models for learning alignments and translation rules have been a fairly recent direction. A joint model using two syntactic parsers and combined with an ITG derivation to model alignments, enables the trees to diverge if required and otherwise encouraging the derivation to synchronize with the trees (Burkett et al., 2010). However it requires a parallel treebank and gold alignments to train on in addition to parsers for the source and target languages, thus Al-Onaizan & Simard (Eds.) Proceedings of AMTA 2014, vol. 1: MT Researchers Vancouver, BC ©"
2014.amta-researchers.2,P06-1121,0,0.0544672,"d in many previous publications (Burkett et al., 2010; DeNero and Klein, 2010; Saers et al., 2013a). Secondly the early steps in the training pipeline, are unaware of and are almost always at odds with, the final goal of training a translation model. As a specific example, the alignment models are trained early in the pipeline, isolated from the step that extracts translations and this could lead to sub-optimal alignments (DeNero and Klein, 2010). This is also true for the syntactic models that rely on word alignments to extract the translation rules that are consistent with those alignments (Galley et al., 2006; Chiang, 2007, inter alia). Consider the example word-aligned phrase pair shown in Figure 1. The baseline Giza++ alignment incorrectly aligns the English the to the Chinese word 联合国 (united nations). While the aligner is not going to be perfect, the present serial pipeline does not allow the aligner to correct such mistakes or to adapt the alignments to yield better translation rules. Further the serial pipeline results in the propagation of the modelling deficiencies from the Al-Onaizan & Simard (Eds.) Proceedings of AMTA 2014, vol. 1: MT Researchers Vancouver, BC © The Authors 15 数 月 , mont"
2014.amta-researchers.2,2010.iwslt-papers.11,0,0.0206859,"cade framework and extract SCFG rules in a separate phase. Iterative approach has been used for directly training the phrase translation models for a phrase-based system (Wuebker et al., 2010). This method employs force decoding the training set (based on the IBM word alignments) to obtain phrase segmentations. Phrase probabilities are then estimated using leaving-one-out technique, in order to avoid overfitting. Our present work on learning hierarchical translation model differs from this in obvious way; additionally we just use model aggregation as opposed to their iterative force decoding. Heger et al. (2010) extend the Wuebker’s work by combining the iteratively learned phase alignments with the heuristically learned hierarchical translation model for Hiero-style system. This approach is similar in spirit to our goal of learning a hierarchical translation model that is consistent throughout. However their approach does not learn the hierarchical SCFG rules through force alignments, but only combines the iteratively learned phrase table with hierarchical translation grammar extracted traditionally. Secondly our framework allow both alignments and SCFG rules to be improved iteratively unlike theirs"
2014.amta-researchers.2,N03-1017,0,0.0109763,"ammars. In both language pairs, baselines employing pialign perform marginally worse and the first iteration of iterative-cascade model in fact results in statistically significant B LEU reduction compared to phrase-based baseline of 8.23. However when we run our cascade framework for three iterations, we see consistent B LEU score improvements ranging between 0.2 and 0.65 as compared to other baselines in the table. One can also compare these scores to the phrase-based model for the sake of completeness. We consider two phrase-based models one using the regular heuristic training pipeline as Koehn et al. (2003) and the other using pialign. For pialign, we use the phrase table extracted by pialign and directly used it with Moses for tuning and decoding. Note that this baseline uses two additional features including span probability (see Neubig et al. (2011)) that are not used in the standard baseline or in the later models in the tables. The two phrase-based models obtained B LEU scores of 8.23 and 8.30 respectively and these are comparable to the performance of our iterative-cascade model. Now turning our attention to the Arabic-English language pair we again notice a very similar behaviour as we sa"
2014.amta-researchers.2,D12-1021,0,0.0686882,"n noticed earlier (DeNero and Klein, 2007). Apart from showing the incompatibility, they also propose an unsupervised HMM alignment model that soft constrains the alignments conditioned on the target sentences and the corresponding (automatically generated) parse trees. The main difference is that our approach seek to improve alignments of different granularities and not just the word-level alignments. Several other works have focussed on learning phrase alignments from synchronous derivations using non-lexicalized (Blunsom et al., 2008) or lexicalized (Hiero-style) ITG (Blunsom et al., 2009; Levenberg et al., 2012) rules and apply them for hierarchical phrase-based models. While these models extract ITG-style rules, they use them only for obtaining the alignment information. In other words, the extracted ITG-style rules are not directly used by a hierarchical translation decoder, which are in fact obtained from the alignments suggested by these rules. Thus the biggest drawback is that these models, strictly speaking, are alignment models and they use the heuristic rule extractor (Chiang, 2007) for learning the Hiero-style translation grammar. In contrast to these, Sankaran et al. (2012, 2013) proposed a"
2014.amta-researchers.2,D07-1038,0,0.0286074,"t al. (2012, 2013) proposed a set of Bayesian models that directly learns the SCFG grammar. However these models only focus on the rule extraction part and rely on the heuristically extracted phrasal alignments. Instead our iterative-cascade framework simplifies the entire hierarchical translation training pipeline. On Joint models for PBMT: Several works have exploited word alignments to improving the performance of parsing (Burkett and Klein, 2008; Snyder et al., 2009) outside the machine translation setting. In the reverse direction syntactic parsing has been used to get better alignments (May and Knight, 2007; DeNero and Klein, 2007; Fossum et al., 2008) in the context of machine translations. Joint models for learning alignments and translation rules have been a fairly recent direction. A joint model using two syntactic parsers and combined with an ITG derivation to model alignments, enables the trees to diverge if required and otherwise encouraging the derivation to synchronize with the trees (Burkett et al., 2010). However it requires a parallel treebank and gold alignments to train on in addition to parsers for the source and target languages, thus Al-Onaizan & Simard (Eds.) Proceedings of AMT"
2014.amta-researchers.2,P11-1064,0,0.152291,"i) generating phrase alignments of different granularities and ii) extracting SCFG rules that are consistent with the alignments. The two Al-Onaizan & Simard (Eds.) Proceedings of AMTA 2014, vol. 1: MT Researchers Vancouver, BC © The Authors 16 phases of the iterative-cascade framework are then repeated in an iterative setup. While we could possibly come up with a single model to achieve this, we intend to validate our framework in this work using a simpler approach. We do this by using existing Bayesian models for each step in this paper. We use the Bayesian hierarchical ITG alignment model (Neubig et al., 2011) for getting the phrasal alignments at the first step. For the phrase extraction step, we use the Bayesian model motivated by a lexical alignment prior employing Variational-Bayesian inference proposed by Sankaran et al. (2013), which operates on the extracted phrasal alignments in the earlier step. We now explain the two models briefly. 2.1 Alignments The joint model proposed by (Neubig et al., 2011) uses a phrasal-ITG based hierarchical model with a Pitman-Yor Process (PYP) prior. Unlike the earlier models (DeNero et al., 2008; Zhang et al., 2008) that extract minimal many-to-many phrase ali"
2014.amta-researchers.2,W13-0806,0,0.028115,"Missing"
2014.amta-researchers.2,W13-2810,0,0.125109,"olving two language pairs, our proposed iterative-cascade framework shows consistent gains over the traditional training pipeline for hierarchical translation. 1 Introduction Hierarchical phrase-based translation, similar to other statistical machine translation (SMT) models are trained in a series of steps that are disparate and often invoke heuristics. The training complexity as well as the modelling deficiencies in learning the translation rules using such multi-step, heuristic ridden pipeline have been documented in many previous publications (Burkett et al., 2010; DeNero and Klein, 2010; Saers et al., 2013a). Secondly the early steps in the training pipeline, are unaware of and are almost always at odds with, the final goal of training a translation model. As a specific example, the alignment models are trained early in the pipeline, isolated from the step that extracts translations and this could lead to sub-optimal alignments (DeNero and Klein, 2010). This is also true for the syntactic models that rely on word alignments to extract the translation rules that are consistent with those alignments (Galley et al., 2006; Chiang, 2007, inter alia). Consider the example word-aligned phrase pair sho"
2014.amta-researchers.2,2012.amta-papers.16,1,0.848986,"m et al., 2009; Levenberg et al., 2012) rules and apply them for hierarchical phrase-based models. While these models extract ITG-style rules, they use them only for obtaining the alignment information. In other words, the extracted ITG-style rules are not directly used by a hierarchical translation decoder, which are in fact obtained from the alignments suggested by these rules. Thus the biggest drawback is that these models, strictly speaking, are alignment models and they use the heuristic rule extractor (Chiang, 2007) for learning the Hiero-style translation grammar. In contrast to these, Sankaran et al. (2012, 2013) proposed a set of Bayesian models that directly learns the SCFG grammar. However these models only focus on the rule extraction part and rely on the heuristically extracted phrasal alignments. Instead our iterative-cascade framework simplifies the entire hierarchical translation training pipeline. On Joint models for PBMT: Several works have exploited word alignments to improving the performance of parsing (Burkett and Klein, 2008; Snyder et al., 2009) outside the machine translation setting. In the reverse direction syntactic parsing has been used to get better alignments (May and Kni"
2014.amta-researchers.2,I13-1050,1,0.895065,"BC © The Authors 16 phases of the iterative-cascade framework are then repeated in an iterative setup. While we could possibly come up with a single model to achieve this, we intend to validate our framework in this work using a simpler approach. We do this by using existing Bayesian models for each step in this paper. We use the Bayesian hierarchical ITG alignment model (Neubig et al., 2011) for getting the phrasal alignments at the first step. For the phrase extraction step, we use the Bayesian model motivated by a lexical alignment prior employing Variational-Bayesian inference proposed by Sankaran et al. (2013), which operates on the extracted phrasal alignments in the earlier step. We now explain the two models briefly. 2.1 Alignments The joint model proposed by (Neubig et al., 2011) uses a phrasal-ITG based hierarchical model with a Pitman-Yor Process (PYP) prior. Unlike the earlier models (DeNero et al., 2008; Zhang et al., 2008) that extract minimal many-to-many phrase alignments, Neubig’s model extracts phrases of varying granularities. This is achieved by inverting the order to first generate the entire sentence/ phrase pair from a phrase distribution (Pt ) and then falls back to ITG derivatio"
2014.amta-researchers.2,P09-1009,0,0.0271615,"odels and they use the heuristic rule extractor (Chiang, 2007) for learning the Hiero-style translation grammar. In contrast to these, Sankaran et al. (2012, 2013) proposed a set of Bayesian models that directly learns the SCFG grammar. However these models only focus on the rule extraction part and rely on the heuristically extracted phrasal alignments. Instead our iterative-cascade framework simplifies the entire hierarchical translation training pipeline. On Joint models for PBMT: Several works have exploited word alignments to improving the performance of parsing (Burkett and Klein, 2008; Snyder et al., 2009) outside the machine translation setting. In the reverse direction syntactic parsing has been used to get better alignments (May and Knight, 2007; DeNero and Klein, 2007; Fossum et al., 2008) in the context of machine translations. Joint models for learning alignments and translation rules have been a fairly recent direction. A joint model using two syntactic parsers and combined with an ITG derivation to model alignments, enables the trees to diverge if required and otherwise encouraging the derivation to synchronize with the trees (Burkett et al., 2010). However it requires a parallel treeba"
2014.amta-researchers.2,P10-1049,0,0.0198987,"he one-to-one alignments. In contrast, Neubig et al. (2011) proposed an unsupervised hierarchical ITG model for jointly learning the alignments and translations as we explained in section 2.1. The extracted translation rules are then directly used in a phrase-based decoder. While, their alignments are based on the ITG, it uses with a flat (phrase-based) model for translation. We extend their approach through our iterative-cascade framework and extract SCFG rules in a separate phase. Iterative approach has been used for directly training the phrase translation models for a phrase-based system (Wuebker et al., 2010). This method employs force decoding the training set (based on the IBM word alignments) to obtain phrase segmentations. Phrase probabilities are then estimated using leaving-one-out technique, in order to avoid overfitting. Our present work on learning hierarchical translation model differs from this in obvious way; additionally we just use model aggregation as opposed to their iterative force decoding. Heger et al. (2010) extend the Wuebker’s work by combining the iteratively learned phase alignments with the heuristically learned hierarchical translation model for Hiero-style system. This a"
2014.amta-researchers.2,P08-1012,0,0.0294575,"Bayesian hierarchical ITG alignment model (Neubig et al., 2011) for getting the phrasal alignments at the first step. For the phrase extraction step, we use the Bayesian model motivated by a lexical alignment prior employing Variational-Bayesian inference proposed by Sankaran et al. (2013), which operates on the extracted phrasal alignments in the earlier step. We now explain the two models briefly. 2.1 Alignments The joint model proposed by (Neubig et al., 2011) uses a phrasal-ITG based hierarchical model with a Pitman-Yor Process (PYP) prior. Unlike the earlier models (DeNero et al., 2008; Zhang et al., 2008) that extract minimal many-to-many phrase alignments, Neubig’s model extracts phrases of varying granularities. This is achieved by inverting the order to first generate the entire sentence/ phrase pair from a phrase distribution (Pt ) and then falls back to ITG derivation to divide the sentence/ phrase pair into shorter phrase-pairs (this effectively avoids the sparsity problem). Under this model each phrase pair gets some probability distribution Phier (he, f i : θx , θt ), where θx and θt are the parameters of symbol distribution and phrase table respectively. The phrase table parameters θt"
2014.amta-researchers.24,2008.iwslt-papers.1,0,0.043409,"Missing"
2014.amta-researchers.24,J93-2003,0,0.060346,"Missing"
2014.amta-researchers.24,P11-2031,0,0.0835679,"Missing"
2014.amta-researchers.24,P07-1092,0,0.084572,"Missing"
2014.amta-researchers.24,C10-1027,0,0.20687,"Missing"
2014.amta-researchers.24,W11-2123,0,0.11684,"Missing"
2014.amta-researchers.24,2012.amta-papers.8,0,0.0676802,"Missing"
2014.amta-researchers.24,P13-2073,0,0.0313429,"Missing"
2014.amta-researchers.24,I13-1167,0,0.0353583,"Missing"
2014.amta-researchers.24,P07-2045,0,0.00739136,"Missing"
2014.amta-researchers.24,W11-2164,0,0.0433827,"Missing"
2014.amta-researchers.24,J03-1002,0,0.0166427,"Missing"
2014.amta-researchers.24,E12-1055,0,0.024087,"Missing"
2020.aacl-srw.14,D18-1537,0,0.033896,"Missing"
2020.aacl-srw.14,P19-1284,0,0.0245031,"dels? This question has led to a wide variety of contemporary NLP research focusing on (a) different axes of interpretability including plausibility (or interchangeably human-interpretability) (Herman, 2017; Lage et al., 2019) and faithfulness (Lipton, 2018; Jacovi and Goldberg, 2020b), (b) interpretation of the neural model components (Belinkov et al., 2017; Dalvi et al., 2017; Vig and Belinkov, 2019), (c) explaining the decisions made by neural models to humans (using explanations, highlights, rationales, etc.) (Ribeiro et al., 2016; Li et al., 2016; Ding et al., 2017; Ghaeini et al., 2018; Bastings et al., 2019; Jain et al., 2020), and (d) evaluating different explanation methods from different perspectives • We propose a measure for quantifying faithfulness in NMT. • We introduce a novel learning objective based 93 Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing: Student Research Workshop, pages 93–100 c December 4 - 7, 2020. 2020 Association for Computational Linguistics on probability divergence that rewards faithful behavior and which can be included in the tr"
2020.aacl-srw.14,I17-1001,0,0.060751,"Missing"
2020.aacl-srw.14,2020.acl-main.386,0,0.0921281,"e’s law for the last century Figure 1: An example translation from Cs-En producing unfaithful attention weights. The model is generating the token century. In the left attention heatmap, the attention is on the word sto while the decoder generates century. However, in the right heatmap, sto is not attended to at all but century is still produced as the output. This is an example of unfaithful behavior. Yellow words are not attended. (Samek et al., 2016; Mohseni and Ragan, 2018; Poerner et al., 2018; Jain and Wallace, 2019; Serrano and Smith, 2019; Wiegreffe and Pinter, 2019; Li et al., 2020). Jacovi and Goldberg (2020b) emphasize distinguishing faithfulness from human-interpretability in interpretability research by providing several clarifications about the terminology used by researchers. They describe the following conditions on the evaluation of how well a research project tackles the notion of faithfulness: (1) Be explicit: provide a measurable evaluation of faithfulness, (2) Human judgements are not relevant because we are interested in model internals, (3) Do not match against gold labels (e.g. AER) because faithfulness of both correct and incorrect decisions made by the model are equally important,"
2020.aacl-srw.14,D14-1179,0,0.0544261,"Missing"
2020.aacl-srw.14,N19-1357,0,0.314311,"to let je to moorův zákon za posledních sto let it’s moore’s law for the last century it’s moore’s law for the last century Figure 1: An example translation from Cs-En producing unfaithful attention weights. The model is generating the token century. In the left attention heatmap, the attention is on the word sto while the decoder generates century. However, in the right heatmap, sto is not attended to at all but century is still produced as the output. This is an example of unfaithful behavior. Yellow words are not attended. (Samek et al., 2016; Mohseni and Ragan, 2018; Poerner et al., 2018; Jain and Wallace, 2019; Serrano and Smith, 2019; Wiegreffe and Pinter, 2019; Li et al., 2020). Jacovi and Goldberg (2020b) emphasize distinguishing faithfulness from human-interpretability in interpretability research by providing several clarifications about the terminology used by researchers. They describe the following conditions on the evaluation of how well a research project tackles the notion of faithfulness: (1) Be explicit: provide a measurable evaluation of faithfulness, (2) Human judgements are not relevant because we are interested in model internals, (3) Do not match against gold labels (e.g. AER) bec"
2020.aacl-srw.14,2020.acl-main.409,0,0.0104497,"led to a wide variety of contemporary NLP research focusing on (a) different axes of interpretability including plausibility (or interchangeably human-interpretability) (Herman, 2017; Lage et al., 2019) and faithfulness (Lipton, 2018; Jacovi and Goldberg, 2020b), (b) interpretation of the neural model components (Belinkov et al., 2017; Dalvi et al., 2017; Vig and Belinkov, 2019), (c) explaining the decisions made by neural models to humans (using explanations, highlights, rationales, etc.) (Ribeiro et al., 2016; Li et al., 2016; Ding et al., 2017; Ghaeini et al., 2018; Bastings et al., 2019; Jain et al., 2020), and (d) evaluating different explanation methods from different perspectives • We propose a measure for quantifying faithfulness in NMT. • We introduce a novel learning objective based 93 Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing: Student Research Workshop, pages 93–100 c December 4 - 7, 2020. 2020 Association for Computational Linguistics on probability divergence that rewards faithful behavior and which can be included in the training objective for"
2020.aacl-srw.14,W19-4828,0,0.0262159,"right it 's all online . it 's all here it 's all online . it 's all here it 's all online . src src ref ref base base ours ours sie drängten wasser aus dem land heraus und hinaus in den ﬂuss sie drängten wasser aus dem land heraus und hinaus in den ﬂuss they pushed water oﬀ the land and out into the river they pushed water oﬀ the land and out into the river they kept running water from the land and out in the river they kept running water from the land and out in the river they pushed water out of the country and out in the river . they pushed water out of the country and out in the river . Clark et al., 2019), evaluating attention as an interpretability approach has garnered a lot of interest. From the faithfulness perspective, (Jain and Wallace, 2019; Serrano and Smith, 2019) show that for instances in a data set there can be adversarial attention heatmaps that do not change the output of the text classifier. In other words, adversarial attention leads to no decision flip in each instance. They use this to claim that attention heatmaps are not to be trusted, or unfaithful. Wiegreffe and Pinter (2019) argue against per-instance modifications at test time for two reasons: 1) in classification tasks"
2020.aacl-srw.14,I17-1015,0,0.0222625,"ed with these criteria, we study faithfulness of attention in NMT, the extent to which it can reflect the true internal reasoning behind a prediction (Figure 1). We make the following contributions: Introduction Can we trust our neural models? This question has led to a wide variety of contemporary NLP research focusing on (a) different axes of interpretability including plausibility (or interchangeably human-interpretability) (Herman, 2017; Lage et al., 2019) and faithfulness (Lipton, 2018; Jacovi and Goldberg, 2020b), (b) interpretation of the neural model components (Belinkov et al., 2017; Dalvi et al., 2017; Vig and Belinkov, 2019), (c) explaining the decisions made by neural models to humans (using explanations, highlights, rationales, etc.) (Ribeiro et al., 2016; Li et al., 2016; Ding et al., 2017; Ghaeini et al., 2018; Bastings et al., 2019; Jain et al., 2020), and (d) evaluating different explanation methods from different perspectives • We propose a measure for quantifying faithfulness in NMT. • We introduce a novel learning objective based 93 Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Confe"
2020.aacl-srw.14,P17-4012,0,0.0307382,"model both right answers and right reasons instead of only the former. They achieve this by introducing a regularizing term that tends to shrink irrelevant gradients. In a similar spirit, we change our objective to account for the NMT model’s faithfulness as well as the cross-entropy score against the reference translations: F = Facc + λf aith Ff aith 0 + λuni log p(ˆ y |x, ICuni (x)) + (3) 4 Experimental Setup We use the Czech-English (Cs-En) dataset from IWSLT2016and the German-English (De-En) dataset from IWSLT2014.We used Moses (Koehn et al., 2007) to tokenize the dataset. We use OpenNMT (Klein et al., 2017) as our translation framework. We employ a 2 layer LSTM-based encoderdecoder (Sutskever et al., 2014; Cho et al., 2014) model with global attention (Luong et al., 2015). We use Adam (Kingma and Ba, 2014) for training our models and we set the learning rate to 0.001. Models are trained until convergence. The baseline model is trained using Eqn. (2) and we call it Fbaseline . We refer to the objective as Fall when Divergence-based Faithfulness Objective Consider a predictive model gθ in which an intermediate calculation is later employed to justify predictions: yˆ = arg max p(y|x) = arg max gθ ("
2020.aacl-srw.14,P17-1106,0,0.0628399,": Introduction Can we trust our neural models? This question has led to a wide variety of contemporary NLP research focusing on (a) different axes of interpretability including plausibility (or interchangeably human-interpretability) (Herman, 2017; Lage et al., 2019) and faithfulness (Lipton, 2018; Jacovi and Goldberg, 2020b), (b) interpretation of the neural model components (Belinkov et al., 2017; Dalvi et al., 2017; Vig and Belinkov, 2019), (c) explaining the decisions made by neural models to humans (using explanations, highlights, rationales, etc.) (Ribeiro et al., 2016; Li et al., 2016; Ding et al., 2017; Ghaeini et al., 2018; Bastings et al., 2019; Jain et al., 2020), and (d) evaluating different explanation methods from different perspectives • We propose a measure for quantifying faithfulness in NMT. • We introduce a novel learning objective based 93 Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing: Student Research Workshop, pages 93–100 c December 4 - 7, 2020. 2020 Association for Computational Linguistics on probability divergence that rewards faithful"
2020.aacl-srw.14,W04-3250,0,0.451743,"Missing"
2020.aacl-srw.14,P07-2045,0,0.0112355,"t al. (2017) change the loss function of their classifier to model both right answers and right reasons instead of only the former. They achieve this by introducing a regularizing term that tends to shrink irrelevant gradients. In a similar spirit, we change our objective to account for the NMT model’s faithfulness as well as the cross-entropy score against the reference translations: F = Facc + λf aith Ff aith 0 + λuni log p(ˆ y |x, ICuni (x)) + (3) 4 Experimental Setup We use the Czech-English (Cs-En) dataset from IWSLT2016and the German-English (De-En) dataset from IWSLT2014.We used Moses (Koehn et al., 2007) to tokenize the dataset. We use OpenNMT (Klein et al., 2017) as our translation framework. We employ a 2 layer LSTM-based encoderdecoder (Sutskever et al., 2014; Cho et al., 2014) model with global attention (Luong et al., 2015). We use Adam (Kingma and Ba, 2014) for training our models and we set the learning rate to 0.001. Models are trained until convergence. The baseline model is trained using Eqn. (2) and we call it Fbaseline . We refer to the objective as Fall when Divergence-based Faithfulness Objective Consider a predictive model gθ in which an intermediate calculation is later employ"
2020.aacl-srw.14,P18-1032,0,0.0228512,"Missing"
2020.aacl-srw.14,D17-2021,0,0.0145551,"on should change if the model is faithful (Jacovi and Goldberg, 2020b). A common stress test is the erasure test in which the most-relevant part of the input is removed (Arras et al., 2017). In the context of NMT, at decoding time step t the attention component assigns attention weights αt , attending to the source word at position mt = argmaxi αt [i] (or the k-best attended-to words in the source). These weights are often implicitly or explicitly regarded as an interpretation for the model’s prediction at the time step t (Tu et al., 2016; Mi et al., 2016; Liu et al., 2016; Wang et al., 2016; Lee et al., 2017; Ding et al., 2017; Ghaeini et al., 2018). The erasure stress test for evaluating faithfulness offered by αt is done by setting αt [mt ] to zero and observing whether or not the output changes. It is worth noting that erasure is only one of the possible stress tests for evaluating faithfulness. Passing more stress tests implies a more faithful model as it is resilient to more attacks or stress tests of its faithMany prior studies of attention (Jain and Wallace, 2019; Wiegreffe and Pinter, 2019) have used a binary measure: either attention is faithful or it is not. These studies typically are"
2020.aacl-srw.14,N16-3020,0,0.0261943,"1). We make the following contributions: Introduction Can we trust our neural models? This question has led to a wide variety of contemporary NLP research focusing on (a) different axes of interpretability including plausibility (or interchangeably human-interpretability) (Herman, 2017; Lage et al., 2019) and faithfulness (Lipton, 2018; Jacovi and Goldberg, 2020b), (b) interpretation of the neural model components (Belinkov et al., 2017; Dalvi et al., 2017; Vig and Belinkov, 2019), (c) explaining the decisions made by neural models to humans (using explanations, highlights, rationales, etc.) (Ribeiro et al., 2016; Li et al., 2016; Ding et al., 2017; Ghaeini et al., 2018; Bastings et al., 2019; Jain et al., 2020), and (d) evaluating different explanation methods from different perspectives • We propose a measure for quantifying faithfulness in NMT. • We introduce a novel learning objective based 93 Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing: Student Research Workshop, pages 93–100 c December 4 - 7, 2020. 2020 Association for Computational Linguistics on probabil"
2020.aacl-srw.14,2020.acl-main.35,0,0.0765781,"century it’s moore’s law for the last century Figure 1: An example translation from Cs-En producing unfaithful attention weights. The model is generating the token century. In the left attention heatmap, the attention is on the word sto while the decoder generates century. However, in the right heatmap, sto is not attended to at all but century is still produced as the output. This is an example of unfaithful behavior. Yellow words are not attended. (Samek et al., 2016; Mohseni and Ragan, 2018; Poerner et al., 2018; Jain and Wallace, 2019; Serrano and Smith, 2019; Wiegreffe and Pinter, 2019; Li et al., 2020). Jacovi and Goldberg (2020b) emphasize distinguishing faithfulness from human-interpretability in interpretability research by providing several clarifications about the terminology used by researchers. They describe the following conditions on the evaluation of how well a research project tackles the notion of faithfulness: (1) Be explicit: provide a measurable evaluation of faithfulness, (2) Human judgements are not relevant because we are interested in model internals, (3) Do not match against gold labels (e.g. AER) because faithfulness of both correct and incorrect decisions made by the m"
2020.aacl-srw.14,N16-1082,0,0.0311432,"ing contributions: Introduction Can we trust our neural models? This question has led to a wide variety of contemporary NLP research focusing on (a) different axes of interpretability including plausibility (or interchangeably human-interpretability) (Herman, 2017; Lage et al., 2019) and faithfulness (Lipton, 2018; Jacovi and Goldberg, 2020b), (b) interpretation of the neural model components (Belinkov et al., 2017; Dalvi et al., 2017; Vig and Belinkov, 2019), (c) explaining the decisions made by neural models to humans (using explanations, highlights, rationales, etc.) (Ribeiro et al., 2016; Li et al., 2016; Ding et al., 2017; Ghaeini et al., 2018; Bastings et al., 2019; Jain et al., 2020), and (d) evaluating different explanation methods from different perspectives • We propose a measure for quantifying faithfulness in NMT. • We introduce a novel learning objective based 93 Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing: Student Research Workshop, pages 93–100 c December 4 - 7, 2020. 2020 Association for Computational Linguistics on probability divergence th"
2020.aacl-srw.14,P19-1282,0,0.0713001,"n za posledních sto let it’s moore’s law for the last century it’s moore’s law for the last century Figure 1: An example translation from Cs-En producing unfaithful attention weights. The model is generating the token century. In the left attention heatmap, the attention is on the word sto while the decoder generates century. However, in the right heatmap, sto is not attended to at all but century is still produced as the output. This is an example of unfaithful behavior. Yellow words are not attended. (Samek et al., 2016; Mohseni and Ragan, 2018; Poerner et al., 2018; Jain and Wallace, 2019; Serrano and Smith, 2019; Wiegreffe and Pinter, 2019; Li et al., 2020). Jacovi and Goldberg (2020b) emphasize distinguishing faithfulness from human-interpretability in interpretability research by providing several clarifications about the terminology used by researchers. They describe the following conditions on the evaluation of how well a research project tackles the notion of faithfulness: (1) Be explicit: provide a measurable evaluation of faithfulness, (2) Human judgements are not relevant because we are interested in model internals, (3) Do not match against gold labels (e.g. AER) because faithfulness of both"
2020.aacl-srw.14,C16-1291,0,0.0180162,"in such a way that the model’s decision should change if the model is faithful (Jacovi and Goldberg, 2020b). A common stress test is the erasure test in which the most-relevant part of the input is removed (Arras et al., 2017). In the context of NMT, at decoding time step t the attention component assigns attention weights αt , attending to the source word at position mt = argmaxi αt [i] (or the k-best attended-to words in the source). These weights are often implicitly or explicitly regarded as an interpretation for the model’s prediction at the time step t (Tu et al., 2016; Mi et al., 2016; Liu et al., 2016; Wang et al., 2016; Lee et al., 2017; Ding et al., 2017; Ghaeini et al., 2018). The erasure stress test for evaluating faithfulness offered by αt is done by setting αt [mt ] to zero and observing whether or not the output changes. It is worth noting that erasure is only one of the possible stress tests for evaluating faithfulness. Passing more stress tests implies a more faithful model as it is resilient to more attacks or stress tests of its faithMany prior studies of attention (Jain and Wallace, 2019; Wiegreffe and Pinter, 2019) have used a binary measure: either attention is faithful or it"
2020.aacl-srw.14,D15-1166,0,0.0512684,"n a similar spirit, we change our objective to account for the NMT model’s faithfulness as well as the cross-entropy score against the reference translations: F = Facc + λf aith Ff aith 0 + λuni log p(ˆ y |x, ICuni (x)) + (3) 4 Experimental Setup We use the Czech-English (Cs-En) dataset from IWSLT2016and the German-English (De-En) dataset from IWSLT2014.We used Moses (Koehn et al., 2007) to tokenize the dataset. We use OpenNMT (Klein et al., 2017) as our translation framework. We employ a 2 layer LSTM-based encoderdecoder (Sutskever et al., 2014; Cho et al., 2014) model with global attention (Luong et al., 2015). We use Adam (Kingma and Ba, 2014) for training our models and we set the learning rate to 0.001. Models are trained until convergence. The baseline model is trained using Eqn. (2) and we call it Fbaseline . We refer to the objective as Fall when Divergence-based Faithfulness Objective Consider a predictive model gθ in which an intermediate calculation is later employed to justify predictions: yˆ = arg max p(y|x) = arg max gθ (x, IC(x), y) y (6) 0 λperm log p(ˆ y |x, ICperm (x)) 0 0 0 where ICzom , ICuni and ICperm are ZeroOutMax, Uniform and RandomPermute methods (see Sec. 2) to manipulate a"
2020.aacl-srw.14,Q17-1007,0,0.0374952,"Missing"
2020.aacl-srw.14,P16-1008,0,0.0192542,"rturb the model parameters chosen in such a way that the model’s decision should change if the model is faithful (Jacovi and Goldberg, 2020b). A common stress test is the erasure test in which the most-relevant part of the input is removed (Arras et al., 2017). In the context of NMT, at decoding time step t the attention component assigns attention weights αt , attending to the source word at position mt = argmaxi αt [i] (or the k-best attended-to words in the source). These weights are often implicitly or explicitly regarded as an interpretation for the model’s prediction at the time step t (Tu et al., 2016; Mi et al., 2016; Liu et al., 2016; Wang et al., 2016; Lee et al., 2017; Ding et al., 2017; Ghaeini et al., 2018). The erasure stress test for evaluating faithfulness offered by αt is done by setting αt [mt ] to zero and observing whether or not the output changes. It is worth noting that erasure is only one of the possible stress tests for evaluating faithfulness. Passing more stress tests implies a more faithful model as it is resilient to more attacks or stress tests of its faithMany prior studies of attention (Jain and Wallace, 2019; Wiegreffe and Pinter, 2019) have used a binary measure:"
2020.aacl-srw.14,D16-1249,0,0.0146501,"arameters chosen in such a way that the model’s decision should change if the model is faithful (Jacovi and Goldberg, 2020b). A common stress test is the erasure test in which the most-relevant part of the input is removed (Arras et al., 2017). In the context of NMT, at decoding time step t the attention component assigns attention weights αt , attending to the source word at position mt = argmaxi αt [i] (or the k-best attended-to words in the source). These weights are often implicitly or explicitly regarded as an interpretation for the model’s prediction at the time step t (Tu et al., 2016; Mi et al., 2016; Liu et al., 2016; Wang et al., 2016; Lee et al., 2017; Ding et al., 2017; Ghaeini et al., 2018). The erasure stress test for evaluating faithfulness offered by αt is done by setting αt [mt ] to zero and observing whether or not the output changes. It is worth noting that erasure is only one of the possible stress tests for evaluating faithfulness. Passing more stress tests implies a more faithful model as it is resilient to more attacks or stress tests of its faithMany prior studies of attention (Jain and Wallace, 2019; Wiegreffe and Pinter, 2019) have used a binary measure: either attention"
2020.aacl-srw.14,W19-4808,0,0.0217494,"ia, we study faithfulness of attention in NMT, the extent to which it can reflect the true internal reasoning behind a prediction (Figure 1). We make the following contributions: Introduction Can we trust our neural models? This question has led to a wide variety of contemporary NLP research focusing on (a) different axes of interpretability including plausibility (or interchangeably human-interpretability) (Herman, 2017; Lage et al., 2019) and faithfulness (Lipton, 2018; Jacovi and Goldberg, 2020b), (b) interpretation of the neural model components (Belinkov et al., 2017; Dalvi et al., 2017; Vig and Belinkov, 2019), (c) explaining the decisions made by neural models to humans (using explanations, highlights, rationales, etc.) (Ribeiro et al., 2016; Li et al., 2016; Ding et al., 2017; Ghaeini et al., 2018; Bastings et al., 2019; Jain et al., 2020), and (d) evaluating different explanation methods from different perspectives • We propose a measure for quantifying faithfulness in NMT. • We introduce a novel learning objective based 93 Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language"
2020.aacl-srw.14,D16-1058,0,0.0281286,"the model’s decision should change if the model is faithful (Jacovi and Goldberg, 2020b). A common stress test is the erasure test in which the most-relevant part of the input is removed (Arras et al., 2017). In the context of NMT, at decoding time step t the attention component assigns attention weights αt , attending to the source word at position mt = argmaxi αt [i] (or the k-best attended-to words in the source). These weights are often implicitly or explicitly regarded as an interpretation for the model’s prediction at the time step t (Tu et al., 2016; Mi et al., 2016; Liu et al., 2016; Wang et al., 2016; Lee et al., 2017; Ding et al., 2017; Ghaeini et al., 2018). The erasure stress test for evaluating faithfulness offered by αt is done by setting αt [mt ] to zero and observing whether or not the output changes. It is worth noting that erasure is only one of the possible stress tests for evaluating faithfulness. Passing more stress tests implies a more faithful model as it is resilient to more attacks or stress tests of its faithMany prior studies of attention (Jain and Wallace, 2019; Wiegreffe and Pinter, 2019) have used a binary measure: either attention is faithful or it is not. These stud"
2020.aacl-srw.14,D19-1002,0,0.224171,"t’s moore’s law for the last century it’s moore’s law for the last century Figure 1: An example translation from Cs-En producing unfaithful attention weights. The model is generating the token century. In the left attention heatmap, the attention is on the word sto while the decoder generates century. However, in the right heatmap, sto is not attended to at all but century is still produced as the output. This is an example of unfaithful behavior. Yellow words are not attended. (Samek et al., 2016; Mohseni and Ragan, 2018; Poerner et al., 2018; Jain and Wallace, 2019; Serrano and Smith, 2019; Wiegreffe and Pinter, 2019; Li et al., 2020). Jacovi and Goldberg (2020b) emphasize distinguishing faithfulness from human-interpretability in interpretability research by providing several clarifications about the terminology used by researchers. They describe the following conditions on the evaluation of how well a research project tackles the notion of faithfulness: (1) Be explicit: provide a measurable evaluation of faithfulness, (2) Human judgements are not relevant because we are interested in model internals, (3) Do not match against gold labels (e.g. AER) because faithfulness of both correct and incorrect decis"
2020.emnlp-main.644,D16-1263,0,0.0159333,"f the AST model, most of the other techniques for further improving the translation quality, such as data augmentation, which was examined in previous studies (McCarthy et al., 2020; Park et al., 2019) can also be applied. But we won’t study them in this paper. 5 Related Work The cascaded pipeline of transcribing speech signals and then translating them using an MT component (Ney, 1999; Cho et al., 2017) was for many years the standard design of speech translation systems (Inaguma et al., 2019). The idea of having an end-to-end structure for this task showed promising results in the works of (Adams et al., 2016; Duong et al., 2016; B´erard et al., 2016; Anastasopoulos et al., 2016; Anastasopoulos and Chiang, 2017; Bansal et al., 2017). After the success of (Weiss et al., 2017) in creating a powerful model for ST systems, more recent studies focused on exploring their power, and one of the main approaches to boost the performance of such models is to make use of available data from other tasks, such as ASR and NMT. (Weiss et al., 2017; Anastasopoulos and Chiang, 2018; Sperber et al., 2019) show that multitask learning can be effective and (Jia et al., 2019; Pino et al., 2019; Park et al., 2019; McCar"
2020.emnlp-main.644,D16-1133,0,0.0201149,"oving the translation quality, such as data augmentation, which was examined in previous studies (McCarthy et al., 2020; Park et al., 2019) can also be applied. But we won’t study them in this paper. 5 Related Work The cascaded pipeline of transcribing speech signals and then translating them using an MT component (Ney, 1999; Cho et al., 2017) was for many years the standard design of speech translation systems (Inaguma et al., 2019). The idea of having an end-to-end structure for this task showed promising results in the works of (Adams et al., 2016; Duong et al., 2016; B´erard et al., 2016; Anastasopoulos et al., 2016; Anastasopoulos and Chiang, 2017; Bansal et al., 2017). After the success of (Weiss et al., 2017) in creating a powerful model for ST systems, more recent studies focused on exploring their power, and one of the main approaches to boost the performance of such models is to make use of available data from other tasks, such as ASR and NMT. (Weiss et al., 2017; Anastasopoulos and Chiang, 2018; Sperber et al., 2019) show that multitask learning can be effective and (Jia et al., 2019; Pino et al., 2019; Park et al., 2019; McCarthy et al., 2020) investigate various data augmentation techniques. The"
2020.emnlp-main.644,1983.tc-1.13,0,0.184658,"Missing"
2020.emnlp-main.644,N19-1006,0,0.338005,"o compete with the cascaded model (Pino et al., 2019). Such models not only have lower inference latency, but they also do not suffer from the problem of errors that propagate from one component to the next. However, the scarcity of available resources is the main challenge in this task, and a variety of methods are proposed to address this problem. One of the most effective approaches to increase the performance of AST systems is to pretrain the encoder using an ASR model (Bansal et al., 2018). While pretraining the encoder by an ASR model even in different languages shows promising results (Bansal et al., 2019), using a pretrained MT decoder is not beneficial (Berard et al., 2018; Bansal et al., 2018) or slightly improve the result (Sperber et al., 2019) and even in some cases may worsen the results (Bahar et al., 2019). One explanation for this phenomenon is that the decoder works well only if its input comes from an encoder that it was trained with (Lample et al., 2018). To solve the problem of invariant encoder representations, we make use of an adversarial regularizer in our loss function to bring the output of the ASR encoder closer to the input of MT decoder. We show that this modification can"
2020.emnlp-main.644,E17-2076,0,0.0125242,"h was examined in previous studies (McCarthy et al., 2020; Park et al., 2019) can also be applied. But we won’t study them in this paper. 5 Related Work The cascaded pipeline of transcribing speech signals and then translating them using an MT component (Ney, 1999; Cho et al., 2017) was for many years the standard design of speech translation systems (Inaguma et al., 2019). The idea of having an end-to-end structure for this task showed promising results in the works of (Adams et al., 2016; Duong et al., 2016; B´erard et al., 2016; Anastasopoulos et al., 2016; Anastasopoulos and Chiang, 2017; Bansal et al., 2017). After the success of (Weiss et al., 2017) in creating a powerful model for ST systems, more recent studies focused on exploring their power, and one of the main approaches to boost the performance of such models is to make use of available data from other tasks, such as ASR and NMT. (Weiss et al., 2017; Anastasopoulos and Chiang, 2018; Sperber et al., 2019) show that multitask learning can be effective and (Jia et al., 2019; Pino et al., 2019; Park et al., 2019; McCarthy et al., 2020) investigate various data augmentation techniques. The impact of pretraining the encoder with ASR model is al"
2020.emnlp-main.644,N19-1202,0,0.201493,"Missing"
2020.emnlp-main.644,N16-1109,0,0.0854375,"t of the other techniques for further improving the translation quality, such as data augmentation, which was examined in previous studies (McCarthy et al., 2020; Park et al., 2019) can also be applied. But we won’t study them in this paper. 5 Related Work The cascaded pipeline of transcribing speech signals and then translating them using an MT component (Ney, 1999; Cho et al., 2017) was for many years the standard design of speech translation systems (Inaguma et al., 2019). The idea of having an end-to-end structure for this task showed promising results in the works of (Adams et al., 2016; Duong et al., 2016; B´erard et al., 2016; Anastasopoulos et al., 2016; Anastasopoulos and Chiang, 2017; Bansal et al., 2017). After the success of (Weiss et al., 2017) in creating a powerful model for ST systems, more recent studies focused on exploring their power, and one of the main approaches to boost the performance of such models is to make use of available data from other tasks, such as ASR and NMT. (Weiss et al., 2017; Anastasopoulos and Chiang, 2018; Sperber et al., 2019) show that multitask learning can be effective and (Jia et al., 2019; Pino et al., 2019; Park et al., 2019; McCarthy et al., 2020) in"
2020.emnlp-main.644,L18-1001,0,0.0612026,"ecome indistinguishable during training. Our discriminator consists of a three-layer feedforward network with 1024 hidden units, followed by a Leaky-ReLU activation function (Lample et al., 2018). 3 3.1 Experiments Dataset To evaluate our AST systems, we conducted our experiments on two datasets. For the EnglishGerman language pair, we use the MuST-C corpus (Di Gangi et al., 2019a), which consists of 408 hours of speech data aligned with 234K translated sentences. For the English-French language pair, we use the full training set of Translation 8015 Augmented Librispeech (Libri-Trans) corpus (Kocabiyikoglu et al., 2018) with 230 hours of speech aligned with 131K french sentences. We use LibriSpeech corpus (Panayotov et al., 2015) with 960h of English speeches in order to train our ASR system. Since the test and dev sets of Libri-Trans corpus is part of the ASR LibriSpeech dataset, we remove all utterances from ASR LibriSpeech that share the same (chapter-id, readerid) pairs with the test and dev sets in the LibriTrans corpus. For En-De MT training, we use the combination of TED and Opensubtitle2018 corpora 1 2 which contains more than 18M sentences pairs after filtering noisy pairs. The MT training of the En"
2020.emnlp-main.644,P02-1040,0,0.106002,"Missing"
2020.emnlp-main.644,W19-5202,0,0.0612502,"Missing"
2020.emnlp-main.644,Q19-1020,0,0.202586,"oblem of errors that propagate from one component to the next. However, the scarcity of available resources is the main challenge in this task, and a variety of methods are proposed to address this problem. One of the most effective approaches to increase the performance of AST systems is to pretrain the encoder using an ASR model (Bansal et al., 2018). While pretraining the encoder by an ASR model even in different languages shows promising results (Bansal et al., 2019), using a pretrained MT decoder is not beneficial (Berard et al., 2018; Bansal et al., 2018) or slightly improve the result (Sperber et al., 2019) and even in some cases may worsen the results (Bahar et al., 2019). One explanation for this phenomenon is that the decoder works well only if its input comes from an encoder that it was trained with (Lample et al., 2018). To solve the problem of invariant encoder representations, we make use of an adversarial regularizer in our loss function to bring the output of the ASR encoder closer to the input of MT decoder. We show that this modification can improve the BLEU score by +2.0 BLEU points. 2 2.1 Models End-to-End Speech Translation Similar to conventional MT models, the speech translation"
2021.eacl-main.241,2012.eamt-1.60,0,0.0249599,"Missing"
2021.eacl-main.241,W19-8624,0,0.017823,"ver sub-words p(yi |X, y0 , ..., yi−1 , yi+1 , ..., ymax len ) instead of guessing the next sub-word p(yi |X, y0 , ..., yi−1 ). This bidirectional context turns BERT into a provider of strong contextual sub-word embeddings in many languages. These massively overparameterized neural networks have revolutionized many different NLP tasks. Effective application of BERT in NMT has been studied in a number of contemporary research projects; Language Modeling, Named Entity Recognition, Question Answering, Natural Language Inference, Text Classification (Devlin et al., 2019), and Question Generation (Chan and Fan, 2019). We approach this problem from the novel perspective of extracting linguistic information encoded in BERT and applying such information in NMT. 2773 3 Linguistic Aspect Extraction from BERT Prediction Reconstructed Bert Embedding POS Classifier Since BERT contextual embeddings contain a variety of information (linguistic and non-linguistic), extraction of relevant information plays an important role in further improvement of the downstream tasks. In the rest of this section, we define aspect vectors as single-purpose dense vectors of extracted linguistic information from BERT, discuss how asp"
2021.eacl-main.241,D19-5611,0,0.212246,"T in the form of dense vectors and then use these vectors as linguistic “experts” that neural machine translation (NMT) models can consult during translation. But can syntax help improving NMT? Linzen et al. (2016); Kuncoro et al. (2018); Sundararaman et al. (2019) have reported that learning grammatical structure of sentences can lead to higher levels of performance in NLP models. In particular, Sennrich and Haddow (2016) show that augmenting NMT models with explicit linguistic annotations improves translation quality. BERT embeddings have been previously considered for improving NMT models. Clinchant et al. (2019) replace the encoder token embedding layer in a Transformer NMT model with BERT contextual embeddings. They also experiment with initializing all the encoder layers of the translation model with BERT parameters, in which case they report results on both freezing and fine-tuning the encoder parameters during training. In their experiments BERT embeddings can help with noisy inputs to the NMT model, but otherwise do not help improving NMT performance. Imamura and Sumita (2019) suggest that replacing the encoder layer with BERT embeddings and fine-tuning BERT while training the decoder leads to a"
2021.eacl-main.241,W14-3348,0,0.00764681,"input setting (Clinchant et al., 2019) which replaces the input embedding layer of the encoder module in transformer with a fully pre-trained BERT model. Appendix A.1.2 provides the configurations and 22 Average results of Equation 3 for all the tokens in the train set. For M30k and IWSLT data sets, we train two separate models, one using the aspect vectors trained on the source side of its own training data (indomain) and the other using the aspect vectors trained on the source side of WMT data (out-ofdomain). We use cased B LEU (evaluated with the standard mteval-v14.pl script) and METEOR (Denkowski and Lavie, 2014) to compare different models. Tables 4 and 7 show the results of evaluating the models trained with different mentioned settings. The evaluation results show that taking advantage of aspect vectors improves the accuracy of translating German to both English and French in M30k as well as German to English in IWSLT and WMT. Also, in majority of the cases WMT-trained aspect vectors have pushed the model to produce more accurate results since they contain more generalized information. Based on these results, we conjecture that aspect vectors trained on large outof-domain data can be helpful in low"
2021.eacl-main.241,N19-1423,0,0.189441,"BERT (Devlin et al., 2019) has been marginally useful in NMT because effective fine-tuning is difficult to obtain for NMT without making training brittle and unreliable. We augment NMT by extracting dense fine-tuned vector-based linguistic information from BERT instead of using point estimates. Experimental results show that our method of incorporating linguistic information helps NMT to generalize better in a variety of training contexts and is no more difficult to train than conventional Transformerbased NMT. 1 Introduction Probing studies into large contextual word embeddings such as BERT (Devlin et al., 2019) have shown that these deep multi-layer models essentially reconstruct the traditional NLP pipeline capturing syntax and semantics (Jawahar et al., 2019); information such as part-of-speech tags, constituents, dependencies, semantic roles, coreference resolution information (Tenney et al., 2019a,b) and subject-verb agreement information can be reconstructed from BERT embeddings (Goldberg, 2019). In this work, we wish to extract the relevant pieces of linguistic information related to various levels of syntax from BERT in the form of dense vectors and then use these vectors as linguistic “exper"
2021.eacl-main.241,J93-1004,0,0.180093,"Missing"
2021.eacl-main.241,D19-5603,0,0.0161128,"uistic annotations improves translation quality. BERT embeddings have been previously considered for improving NMT models. Clinchant et al. (2019) replace the encoder token embedding layer in a Transformer NMT model with BERT contextual embeddings. They also experiment with initializing all the encoder layers of the translation model with BERT parameters, in which case they report results on both freezing and fine-tuning the encoder parameters during training. In their experiments BERT embeddings can help with noisy inputs to the NMT model, but otherwise do not help improving NMT performance. Imamura and Sumita (2019) suggest that replacing the encoder layer with BERT embeddings and fine-tuning BERT while training the decoder leads to a catastrophic forgetting phenomenon where useful information in BERT is lost due to the magnitude and number of updates necessary for training the translation decoder and fine-tuning BERT. They present a two-step optimization regime in which the first step freezes the BERT parameters and trains only the decoder while the next step finetunes the encoder (BERT) and the decoder at the same time. Yang et al. (2020) also try to address the catastrophic forgetting phenomenon by th"
2021.eacl-main.241,P19-1356,0,0.0931799,"e and unreliable. We augment NMT by extracting dense fine-tuned vector-based linguistic information from BERT instead of using point estimates. Experimental results show that our method of incorporating linguistic information helps NMT to generalize better in a variety of training contexts and is no more difficult to train than conventional Transformerbased NMT. 1 Introduction Probing studies into large contextual word embeddings such as BERT (Devlin et al., 2019) have shown that these deep multi-layer models essentially reconstruct the traditional NLP pipeline capturing syntax and semantics (Jawahar et al., 2019); information such as part-of-speech tags, constituents, dependencies, semantic roles, coreference resolution information (Tenney et al., 2019a,b) and subject-verb agreement information can be reconstructed from BERT embeddings (Goldberg, 2019). In this work, we wish to extract the relevant pieces of linguistic information related to various levels of syntax from BERT in the form of dense vectors and then use these vectors as linguistic “experts” that neural machine translation (NMT) models can consult during translation. But can syntax help improving NMT? Linzen et al. (2016); Kuncoro et al."
2021.eacl-main.241,P18-1132,0,0.048875,"Missing"
2021.eacl-main.241,C04-1072,0,0.0265513,"Missing"
2021.eacl-main.241,Q16-1037,0,0.0254122,"and semantics (Jawahar et al., 2019); information such as part-of-speech tags, constituents, dependencies, semantic roles, coreference resolution information (Tenney et al., 2019a,b) and subject-verb agreement information can be reconstructed from BERT embeddings (Goldberg, 2019). In this work, we wish to extract the relevant pieces of linguistic information related to various levels of syntax from BERT in the form of dense vectors and then use these vectors as linguistic “experts” that neural machine translation (NMT) models can consult during translation. But can syntax help improving NMT? Linzen et al. (2016); Kuncoro et al. (2018); Sundararaman et al. (2019) have reported that learning grammatical structure of sentences can lead to higher levels of performance in NLP models. In particular, Sennrich and Haddow (2016) show that augmenting NMT models with explicit linguistic annotations improves translation quality. BERT embeddings have been previously considered for improving NMT models. Clinchant et al. (2019) replace the encoder token embedding layer in a Transformer NMT model with BERT contextual embeddings. They also experiment with initializing all the encoder layers of the translation model w"
2021.eacl-main.241,W18-6301,0,0.0670042,"Missing"
2021.eacl-main.241,N18-1202,0,0.0282948,"mbs Aspect Extractors BERT Positional Encodig Positional Encodig Input Embedding Linguistic Embedding Inputs Output Embedding Outputs Figure 2: Integration of Extracted Aspect Vectors into NMT. The right hand side part of this figure is taken from Vaswani et al. (2017). As another important point, a pre-trained BERT model has multiple encoder layers as well as an embedding layer. Choosing the proper layer which contains all of our desired aspects is not simply possible since different layers specialize in different linguistic aspects (Jawahar et al., 2019; Tenney et al., 2019a). Therefore, as Peters et al. (2018) suggest, we define BERT embedding vector E as a weighted sum of all BERT layers (of size `) using Equation 6 where α weights are learnable parameters and will be trained along with the other aspect extractor parameters. E= ` X αj EBERT j (6) j=0 3.3 Integrating Aspect Vectors into NMT Once the aspect vectors are created, we throw away the classifiers and the reconstruction layers and place the encoder part of our trained aspect extractor (the mapping from BERT contextual embeddings to aspect vectors) in an input integration module designed to augment the neural translation model input with as"
2021.eacl-main.241,E17-2025,0,0.0830575,"Missing"
2021.eacl-main.241,W18-2509,0,0.0524829,"Missing"
2021.eacl-main.241,W16-2209,0,0.0243885,"ment information can be reconstructed from BERT embeddings (Goldberg, 2019). In this work, we wish to extract the relevant pieces of linguistic information related to various levels of syntax from BERT in the form of dense vectors and then use these vectors as linguistic “experts” that neural machine translation (NMT) models can consult during translation. But can syntax help improving NMT? Linzen et al. (2016); Kuncoro et al. (2018); Sundararaman et al. (2019) have reported that learning grammatical structure of sentences can lead to higher levels of performance in NLP models. In particular, Sennrich and Haddow (2016) show that augmenting NMT models with explicit linguistic annotations improves translation quality. BERT embeddings have been previously considered for improving NMT models. Clinchant et al. (2019) replace the encoder token embedding layer in a Transformer NMT model with BERT contextual embeddings. They also experiment with initializing all the encoder layers of the translation model with BERT parameters, in which case they report results on both freezing and fine-tuning the encoder parameters during training. In their experiments BERT embeddings can help with noisy inputs to the NMT model, bu"
2021.eacl-main.241,P19-1452,0,0.117824,"Experimental results show that our method of incorporating linguistic information helps NMT to generalize better in a variety of training contexts and is no more difficult to train than conventional Transformerbased NMT. 1 Introduction Probing studies into large contextual word embeddings such as BERT (Devlin et al., 2019) have shown that these deep multi-layer models essentially reconstruct the traditional NLP pipeline capturing syntax and semantics (Jawahar et al., 2019); information such as part-of-speech tags, constituents, dependencies, semantic roles, coreference resolution information (Tenney et al., 2019a,b) and subject-verb agreement information can be reconstructed from BERT embeddings (Goldberg, 2019). In this work, we wish to extract the relevant pieces of linguistic information related to various levels of syntax from BERT in the form of dense vectors and then use these vectors as linguistic “experts” that neural machine translation (NMT) models can consult during translation. But can syntax help improving NMT? Linzen et al. (2016); Kuncoro et al. (2018); Sundararaman et al. (2019) have reported that learning grammatical structure of sentences can lead to higher levels of performance in"
2021.eacl-main.241,P16-1147,0,0.0341105,"Missing"
2021.eacl-main.243,P19-1284,0,0.134634,"dels? This question has led to a wide variety of contemporary NLP research focusing on (a) different axes of interpretability including plausibility (or interchangeably human-interpretability) (Herman, 2017; Lage et al., 2019) and faithfulness (Lipton, 2018; Jacovi and Goldberg, 2020b), (b) interpretation of the neural model components (Belinkov et al., 2017; Dalvi et al., 2017; Vig and Belinkov, 2019), (c) explaining the decisions made by neural models to humans (using explanations, highlights, rationales, etc.) (Ribeiro et al., 2016; Li et al., 2016; Ding et al., 2017; Ghaeini et al., 2018; Bastings et al., 2019; Jain et al., 2020), and (d) evaluating different explanation methods from different perspectives (Samek et al., 2016; Mohseni and Ragan, 2018; Poerner et al., 2018; Jain and Wallace, 2019; Serrano and Smith, 2019; Wiegreffe and Pinter, 2019; Li et al., 2020). All of these approaches make NLP neural models more trustworthy. In this work, we focus on faithfulness which intuitively provides the extent to which an explanation accurately represents the true reasoning behind a prediction. It is particularly important for NLP practitioners who wish to debug their neural models and improve them. Fau"
2021.eacl-main.243,I17-1001,0,0.0634982,"Missing"
2021.eacl-main.243,D14-1179,0,0.0620073,"Missing"
2021.eacl-main.243,W19-4828,0,0.0453756,"ce a more accurate translation. De-En Cs-En Cs-En Objective Fbaseline Fall Fbaseline Fall src ref base ours Model Fbaseline Fall Fent Fbaseline Fall Fent AvgEnt 0.69 0.84 0.35 0.89 1.0 0.43 AvgNormEnt 0.23 0.27 0.11 0.29 0.32 0.14 Table 4: Average entropy and average normalized entropy of the baseline, the proposed model (Fall ), and the model trained with attention entropy regularization. 6 Related Work Attention and Different Axes of Interpretability While several studies have focused on understanding the semantic notions captured by attention (Ghader and Monz, 2017; Vig and Belinkov, 2019; Clark et al., 2019), evaluating attention as an interpretability approach has garnered a lot of interest. From the faithfulness perspective, (Jain and Wallace, 2019; Serrano and Smith, 2019) show that for instances in a data set there can be adversarial attention heatmaps that do not change the output of the text classifier. In other words, adversarial attention leads to no decision flip in each instance. They use this to claim that attention heatmaps are not to be trusted, or unfaithful. Wiegreffe and Pinter (2019) argue against per-instance modifications at test time for two reasons: 1) in classification tasks"
2021.eacl-main.243,N16-1102,0,0.0197349,"mote sparsity in the attention. Regularizing Explanations Ross et al. (2017) augment the loss function of their classification model with an explanation objective to constrain input gradient explanations. Rieger et al. (2019) follow a similar spirit but they use contextual decomposition (Murdoch et al., 2018) to extract explanations offered by the model. Aligning attention (as explanation) with prior knowledge has also been extensively studied. This prior knowledge can include alignment data (Mi et al., 2016; Liu et al., 2016), human rationales (Zhong et al., 2019), or even structural biases (Cohn et al., 2016). Inherently Interpretable Neural Models Contrary to post-hoc explanation methods for interpreting a neural model, Stahlberg et al. (2018) show that the NMT model can be made self-explanatory by training it to produce the discrete decisions made by the model (from which the translations can be extracted later). In another work, (Lei et al., 2016; Bastings et al., 2019) propose models in which first a rationale is selected from the input and then is further used for prediction. 7 Conclusion We proposed a method for quantifying faithfulness of NMT models. To optimize faithfulness we have defined"
2021.eacl-main.243,I17-1015,0,0.0228112,"on the word sto while the decoder generates century. However, in the right heatmap, sto is not attended to at all but century is still produced as the output. This shows unfaithful behavior. Introduction How trustworthy are our neural models? This question has led to a wide variety of contemporary NLP research focusing on (a) different axes of interpretability including plausibility (or interchangeably human-interpretability) (Herman, 2017; Lage et al., 2019) and faithfulness (Lipton, 2018; Jacovi and Goldberg, 2020b), (b) interpretation of the neural model components (Belinkov et al., 2017; Dalvi et al., 2017; Vig and Belinkov, 2019), (c) explaining the decisions made by neural models to humans (using explanations, highlights, rationales, etc.) (Ribeiro et al., 2016; Li et al., 2016; Ding et al., 2017; Ghaeini et al., 2018; Bastings et al., 2019; Jain et al., 2020), and (d) evaluating different explanation methods from different perspectives (Samek et al., 2016; Mohseni and Ragan, 2018; Poerner et al., 2018; Jain and Wallace, 2019; Serrano and Smith, 2019; Wiegreffe and Pinter, 2019; Li et al., 2020). All of these approaches make NLP neural models more trustworthy. In this work, we focus on faithf"
2021.eacl-main.243,P17-1106,0,0.0600123,"duction How trustworthy are our neural models? This question has led to a wide variety of contemporary NLP research focusing on (a) different axes of interpretability including plausibility (or interchangeably human-interpretability) (Herman, 2017; Lage et al., 2019) and faithfulness (Lipton, 2018; Jacovi and Goldberg, 2020b), (b) interpretation of the neural model components (Belinkov et al., 2017; Dalvi et al., 2017; Vig and Belinkov, 2019), (c) explaining the decisions made by neural models to humans (using explanations, highlights, rationales, etc.) (Ribeiro et al., 2016; Li et al., 2016; Ding et al., 2017; Ghaeini et al., 2018; Bastings et al., 2019; Jain et al., 2020), and (d) evaluating different explanation methods from different perspectives (Samek et al., 2016; Mohseni and Ragan, 2018; Poerner et al., 2018; Jain and Wallace, 2019; Serrano and Smith, 2019; Wiegreffe and Pinter, 2019; Li et al., 2020). All of these approaches make NLP neural models more trustworthy. In this work, we focus on faithfulness which intuitively provides the extent to which an explanation accurately represents the true reasoning behind a prediction. It is particularly important for NLP practitioners who wish to de"
2021.eacl-main.243,I17-1004,0,0.0180195,"n the source side when needed in order to produce a more accurate translation. De-En Cs-En Cs-En Objective Fbaseline Fall Fbaseline Fall src ref base ours Model Fbaseline Fall Fent Fbaseline Fall Fent AvgEnt 0.69 0.84 0.35 0.89 1.0 0.43 AvgNormEnt 0.23 0.27 0.11 0.29 0.32 0.14 Table 4: Average entropy and average normalized entropy of the baseline, the proposed model (Fall ), and the model trained with attention entropy regularization. 6 Related Work Attention and Different Axes of Interpretability While several studies have focused on understanding the semantic notions captured by attention (Ghader and Monz, 2017; Vig and Belinkov, 2019; Clark et al., 2019), evaluating attention as an interpretability approach has garnered a lot of interest. From the faithfulness perspective, (Jain and Wallace, 2019; Serrano and Smith, 2019) show that for instances in a data set there can be adversarial attention heatmaps that do not change the output of the text classifier. In other words, adversarial attention leads to no decision flip in each instance. They use this to claim that attention heatmaps are not to be trusted, or unfaithful. Wiegreffe and Pinter (2019) argue against per-instance modifications at test tim"
2021.eacl-main.243,2020.acl-main.386,0,0.079469,"ntion weights produced during a Cs-En translation. Note in the left attention heatmap, the attention is on the word sto while the decoder generates century. However, in the right heatmap, sto is not attended to at all but century is still produced as the output. This shows unfaithful behavior. Introduction How trustworthy are our neural models? This question has led to a wide variety of contemporary NLP research focusing on (a) different axes of interpretability including plausibility (or interchangeably human-interpretability) (Herman, 2017; Lage et al., 2019) and faithfulness (Lipton, 2018; Jacovi and Goldberg, 2020b), (b) interpretation of the neural model components (Belinkov et al., 2017; Dalvi et al., 2017; Vig and Belinkov, 2019), (c) explaining the decisions made by neural models to humans (using explanations, highlights, rationales, etc.) (Ribeiro et al., 2016; Li et al., 2016; Ding et al., 2017; Ghaeini et al., 2018; Bastings et al., 2019; Jain et al., 2020), and (d) evaluating different explanation methods from different perspectives (Samek et al., 2016; Mohseni and Ragan, 2018; Poerner et al., 2018; Jain and Wallace, 2019; Serrano and Smith, 2019; Wiegreffe and Pinter, 2019; Li et al., 2020). A"
2021.eacl-main.243,N19-1357,0,0.23757,"ity) (Herman, 2017; Lage et al., 2019) and faithfulness (Lipton, 2018; Jacovi and Goldberg, 2020b), (b) interpretation of the neural model components (Belinkov et al., 2017; Dalvi et al., 2017; Vig and Belinkov, 2019), (c) explaining the decisions made by neural models to humans (using explanations, highlights, rationales, etc.) (Ribeiro et al., 2016; Li et al., 2016; Ding et al., 2017; Ghaeini et al., 2018; Bastings et al., 2019; Jain et al., 2020), and (d) evaluating different explanation methods from different perspectives (Samek et al., 2016; Mohseni and Ragan, 2018; Poerner et al., 2018; Jain and Wallace, 2019; Serrano and Smith, 2019; Wiegreffe and Pinter, 2019; Li et al., 2020). All of these approaches make NLP neural models more trustworthy. In this work, we focus on faithfulness which intuitively provides the extent to which an explanation accurately represents the true reasoning behind a prediction. It is particularly important for NLP practitioners who wish to debug their neural models and improve them. Faults of a neural model cannot be identified if the neural model does not provide a faithful and trustworthy description of what it is doing. However, the formal definition of faithfulness an"
2021.eacl-main.243,2020.acl-main.409,0,0.0108344,"led to a wide variety of contemporary NLP research focusing on (a) different axes of interpretability including plausibility (or interchangeably human-interpretability) (Herman, 2017; Lage et al., 2019) and faithfulness (Lipton, 2018; Jacovi and Goldberg, 2020b), (b) interpretation of the neural model components (Belinkov et al., 2017; Dalvi et al., 2017; Vig and Belinkov, 2019), (c) explaining the decisions made by neural models to humans (using explanations, highlights, rationales, etc.) (Ribeiro et al., 2016; Li et al., 2016; Ding et al., 2017; Ghaeini et al., 2018; Bastings et al., 2019; Jain et al., 2020), and (d) evaluating different explanation methods from different perspectives (Samek et al., 2016; Mohseni and Ragan, 2018; Poerner et al., 2018; Jain and Wallace, 2019; Serrano and Smith, 2019; Wiegreffe and Pinter, 2019; Li et al., 2020). All of these approaches make NLP neural models more trustworthy. In this work, we focus on faithfulness which intuitively provides the extent to which an explanation accurately represents the true reasoning behind a prediction. It is particularly important for NLP practitioners who wish to debug their neural models and improve them. Faults of a neural mode"
2021.eacl-main.243,P17-4012,0,0.0356229,"|Yˆi | X X i=1 j=1 Ent(αij ) (9) where entropy of attention weights is added to the cross-entropy loss (2) as a regularization term. 4 4.1 Experimental Setup Datasets We use the Czech-English (Cs-En) dataset from IWSLT20162 and the German-English (De-En) dataset from IWSLT20143 . For the Czech-English dataset we use dev2010, tst2010, tst2011, tst2012, and tst2013 as the test data. For the GermanEnglish dataset we use dev2010, tst2010, tst2011, dev2012, and tst2012 as the test data. We used Moses (Koehn et al., 2007) to tokenize the dataset. 4.2 Architecture and Hyperparameters We use OpenNMT (Klein et al., 2017) as our translation framework. We employ a 2 layer LSTMbased encoder-decoder (Sutskever et al., 2014; Cho et al., 2014) model with global attention (Luong et al., 2015). Dimension of the hidden states and the word embeddings for both source and target languages are set to 500. Vocabulary size for both the source and target language is set to 50000. We remove sentences with more than 50 tokens from the training data. We use Adam (Kingma and Ba, 2014) for training our models and we set the learning rate to 0.001. Models are trained until convergence. Our models have around 82M parameters. We opt"
2021.eacl-main.243,W04-3250,0,0.124512,"out in the river . src anstatt hunderte von kilometern entfernt im norden src instead sie drängten wasserof aus demaway landin heraus und hinaus in den ﬂuss ref of hundreds miles the north ref instead they pushed water oﬀ land andfrom out into river base of hundreds of the miles away norththe america base they kept running water from the land and out in the river ours instead of hundreds of miles away from north ours they pushed water out of the country and out in the river . src ref base ours Table 3: BLEU score of the baseline and the model trained with Fall . Pairwise bootstrap resampling (Koehn, 2004) resulted in a p-value < 0.01 which indicates the statistical significance of the observed difference. Improved BLEU scores for the faithful model can be due to two reasons: 1) the faithfulness objective can be seen as a regularization term which prevents the model from relying too much on the target-side context and the implicit language model in the decoder, which results in increased contribution of attention on the decoder and reducing some bias in the model. 2) penalizing the model for the lack of connection between justification and prediction forces the model to learn better translation"
2021.eacl-main.243,P07-2045,0,0.0111543,"mized. We used attention entropy regularization (Zhang et al., 2018): 2794 Fent = Facc + λent |S ||Yˆi | X X i=1 j=1 Ent(αij ) (9) where entropy of attention weights is added to the cross-entropy loss (2) as a regularization term. 4 4.1 Experimental Setup Datasets We use the Czech-English (Cs-En) dataset from IWSLT20162 and the German-English (De-En) dataset from IWSLT20143 . For the Czech-English dataset we use dev2010, tst2010, tst2011, tst2012, and tst2013 as the test data. For the GermanEnglish dataset we use dev2010, tst2010, tst2011, dev2012, and tst2012 as the test data. We used Moses (Koehn et al., 2007) to tokenize the dataset. 4.2 Architecture and Hyperparameters We use OpenNMT (Klein et al., 2017) as our translation framework. We employ a 2 layer LSTMbased encoder-decoder (Sutskever et al., 2014; Cho et al., 2014) model with global attention (Luong et al., 2015). Dimension of the hidden states and the word embeddings for both source and target languages are set to 500. Vocabulary size for both the source and target language is set to 50000. We remove sentences with more than 50 tokens from the training data. We use Adam (Kingma and Ba, 2014) for training our models and we set the learning"
2021.eacl-main.243,D17-2021,0,0.0172889,"on should change if the model is faithful (Jacovi and Goldberg, 2020b). A common stress test is the erasure test in which the most-relevant part of the input is removed (Arras et al., 2017). In the context of NMT, at decoding time step t the attention component assigns attention weights αt , attending to the source word at position mt = argmaxi αt [i] (or the k-best attended-to words in the source). These weights are often implicitly or explicitly regarded as an interpretation for the model’s prediction at the time step t (Tu et al., 2016; Mi et al., 2016; Liu et al., 2016; Wang et al., 2016; Lee et al., 2017; Ding et al., 2017; Ghaeini et al., 2018). The erasure stress test for evaluating faithfulness offered by αt is done by setting αt [mt ] to zero and observing whether or not the output changes. It is worth noting that erasure is only one of the possible stress tests for evaluating faithfulness. Passing • We provide empirical evidence that we can improve faithfulness in an NMT model. Our 1 Our code is available at https://github.com/ sfu-natlang/attention_regularization Contributions We seek to improve faithfulness of NMT models. To this end, we make the following contributions in this work: •"
2021.eacl-main.243,D16-1011,0,0.0652149,"Missing"
2021.eacl-main.243,2020.acl-main.35,0,0.077297,"and Goldberg, 2020b), (b) interpretation of the neural model components (Belinkov et al., 2017; Dalvi et al., 2017; Vig and Belinkov, 2019), (c) explaining the decisions made by neural models to humans (using explanations, highlights, rationales, etc.) (Ribeiro et al., 2016; Li et al., 2016; Ding et al., 2017; Ghaeini et al., 2018; Bastings et al., 2019; Jain et al., 2020), and (d) evaluating different explanation methods from different perspectives (Samek et al., 2016; Mohseni and Ragan, 2018; Poerner et al., 2018; Jain and Wallace, 2019; Serrano and Smith, 2019; Wiegreffe and Pinter, 2019; Li et al., 2020). All of these approaches make NLP neural models more trustworthy. In this work, we focus on faithfulness which intuitively provides the extent to which an explanation accurately represents the true reasoning behind a prediction. It is particularly important for NLP practitioners who wish to debug their neural models and improve them. Faults of a neural model cannot be identified if the neural model does not provide a faithful and trustworthy description of what it is doing. However, the formal definition of faithfulness and the proper approach to its evaluation are still contended in the lite"
2021.eacl-main.243,N16-1082,0,0.116936,"l behavior. Introduction How trustworthy are our neural models? This question has led to a wide variety of contemporary NLP research focusing on (a) different axes of interpretability including plausibility (or interchangeably human-interpretability) (Herman, 2017; Lage et al., 2019) and faithfulness (Lipton, 2018; Jacovi and Goldberg, 2020b), (b) interpretation of the neural model components (Belinkov et al., 2017; Dalvi et al., 2017; Vig and Belinkov, 2019), (c) explaining the decisions made by neural models to humans (using explanations, highlights, rationales, etc.) (Ribeiro et al., 2016; Li et al., 2016; Ding et al., 2017; Ghaeini et al., 2018; Bastings et al., 2019; Jain et al., 2020), and (d) evaluating different explanation methods from different perspectives (Samek et al., 2016; Mohseni and Ragan, 2018; Poerner et al., 2018; Jain and Wallace, 2019; Serrano and Smith, 2019; Wiegreffe and Pinter, 2019; Li et al., 2020). All of these approaches make NLP neural models more trustworthy. In this work, we focus on faithfulness which intuitively provides the extent to which an explanation accurately represents the true reasoning behind a prediction. It is particularly important for NLP practitio"
2021.eacl-main.243,C16-1291,0,0.119552,"in such a way that the model’s decision should change if the model is faithful (Jacovi and Goldberg, 2020b). A common stress test is the erasure test in which the most-relevant part of the input is removed (Arras et al., 2017). In the context of NMT, at decoding time step t the attention component assigns attention weights αt , attending to the source word at position mt = argmaxi αt [i] (or the k-best attended-to words in the source). These weights are often implicitly or explicitly regarded as an interpretation for the model’s prediction at the time step t (Tu et al., 2016; Mi et al., 2016; Liu et al., 2016; Wang et al., 2016; Lee et al., 2017; Ding et al., 2017; Ghaeini et al., 2018). The erasure stress test for evaluating faithfulness offered by αt is done by setting αt [mt ] to zero and observing whether or not the output changes. It is worth noting that erasure is only one of the possible stress tests for evaluating faithfulness. Passing • We provide empirical evidence that we can improve faithfulness in an NMT model. Our 1 Our code is available at https://github.com/ sfu-natlang/attention_regularization Contributions We seek to improve faithfulness of NMT models. To this end, we make the fo"
2021.eacl-main.243,D15-1166,0,0.0605801,"e use the Czech-English (Cs-En) dataset from IWSLT20162 and the German-English (De-En) dataset from IWSLT20143 . For the Czech-English dataset we use dev2010, tst2010, tst2011, tst2012, and tst2013 as the test data. For the GermanEnglish dataset we use dev2010, tst2010, tst2011, dev2012, and tst2012 as the test data. We used Moses (Koehn et al., 2007) to tokenize the dataset. 4.2 Architecture and Hyperparameters We use OpenNMT (Klein et al., 2017) as our translation framework. We employ a 2 layer LSTMbased encoder-decoder (Sutskever et al., 2014; Cho et al., 2014) model with global attention (Luong et al., 2015). Dimension of the hidden states and the word embeddings for both source and target languages are set to 500. Vocabulary size for both the source and target language is set to 50000. We remove sentences with more than 50 tokens from the training data. We use Adam (Kingma and Ba, 2014) for training our models and we set the learning rate to 0.001. Models are trained until convergence. Our models have around 82M parameters. We optimize the hyperparameters of our models using the validation set. The baseline model is trained using Eqn. (2) and we call it Fbaseline . λent in Eq. (9) is set to 0.04"
2021.eacl-main.243,P18-2059,0,0.0410422,"Missing"
2021.eacl-main.243,D16-1249,0,0.0824367,"arameters chosen in such a way that the model’s decision should change if the model is faithful (Jacovi and Goldberg, 2020b). A common stress test is the erasure test in which the most-relevant part of the input is removed (Arras et al., 2017). In the context of NMT, at decoding time step t the attention component assigns attention weights αt , attending to the source word at position mt = argmaxi αt [i] (or the k-best attended-to words in the source). These weights are often implicitly or explicitly regarded as an interpretation for the model’s prediction at the time step t (Tu et al., 2016; Mi et al., 2016; Liu et al., 2016; Wang et al., 2016; Lee et al., 2017; Ding et al., 2017; Ghaeini et al., 2018). The erasure stress test for evaluating faithfulness offered by αt is done by setting αt [mt ] to zero and observing whether or not the output changes. It is worth noting that erasure is only one of the possible stress tests for evaluating faithfulness. Passing • We provide empirical evidence that we can improve faithfulness in an NMT model. Our 1 Our code is available at https://github.com/ sfu-natlang/attention_regularization Contributions We seek to improve faithfulness of NMT models. To this e"
2021.eacl-main.243,D19-5624,1,0.905973,"ts implies a more faithful model as it is properly reacting to more adversaries by changing its decision. In this paper we consider three intuitive stress test cases: ZeroOutMax: (Arras et al., 2017): Here we remove attention from the most important token according to the attention weights by setting αt [mt ] = 0. where S is the training data and X and Y are source sentence and the correct translation respectively. This training objective does not explicitly model the interpretability aspects (e.g. faithfulness) of the network and it remains unoptimized during training. Stress tests Uniform: (Moradi et al., 2019): In this stress test all attention weights are set to be equal, αt = 1~ m 1, where m is the length of the source sentence. This is to confuse the model about which part of the input is the most important one. RandomPermute: (Jain and Wallace, 2019): In this stress test we randomly permute attention weights several times until a change in the model output is observed. We ensure that mt , the most important token according to attention, is always changed. We set αt0 = random permute(αt ) such that argmaxi αt0 [i] 6= mt Many prior studies of attention (Jain and Wallace, 2019; Wiegreffe and Pinte"
2021.eacl-main.243,petrov-etal-2012-universal,0,0.0136648,"Missing"
2021.eacl-main.243,P18-1032,0,0.025695,"Missing"
2021.eacl-main.243,2020.acl-main.432,0,0.356675,"ng faithfulness and addressing evaluation of faithfulness separately from plausibility respectively. Subramanian et al. (2020) have investigated the concept of faithfulness in neural modular networks (NMN) which are employed for modeling compositionality. They question the faithfulness of the structure of the network modules describing the true abstract reasoning of the model. Similar to us, they attempt to quantify faithfulness and improve upon it. However their contributions like training with an auxilary atomic-task supervision for improved faithfulness are specific to the context of NMNs. Pruthi et al. (2020) demonstrate that it is possible to train a model that produces a deceptive attention mask, questioning the use of attention weights as explanation from the fairness and accountability perspective. Alvarez-Melis and Jaakkola (2018) investigate the interpretability methods from the robustness perspective. They attempt to quantify robustness and show that current interpretability methods cannot be considered as robust. Sparsity For Improved Interpretability This line of work suggests making attention sparser so that the most contributing input word is more distinguishable over other input words."
2021.eacl-main.243,N16-3020,0,0.0308642,". This shows unfaithful behavior. Introduction How trustworthy are our neural models? This question has led to a wide variety of contemporary NLP research focusing on (a) different axes of interpretability including plausibility (or interchangeably human-interpretability) (Herman, 2017; Lage et al., 2019) and faithfulness (Lipton, 2018; Jacovi and Goldberg, 2020b), (b) interpretation of the neural model components (Belinkov et al., 2017; Dalvi et al., 2017; Vig and Belinkov, 2019), (c) explaining the decisions made by neural models to humans (using explanations, highlights, rationales, etc.) (Ribeiro et al., 2016; Li et al., 2016; Ding et al., 2017; Ghaeini et al., 2018; Bastings et al., 2019; Jain et al., 2020), and (d) evaluating different explanation methods from different perspectives (Samek et al., 2016; Mohseni and Ragan, 2018; Poerner et al., 2018; Jain and Wallace, 2019; Serrano and Smith, 2019; Wiegreffe and Pinter, 2019; Li et al., 2020). All of these approaches make NLP neural models more trustworthy. In this work, we focus on faithfulness which intuitively provides the extent to which an explanation accurately represents the true reasoning behind a prediction. It is particularly important"
2021.eacl-main.243,P19-1282,0,0.179598,"et al., 2019) and faithfulness (Lipton, 2018; Jacovi and Goldberg, 2020b), (b) interpretation of the neural model components (Belinkov et al., 2017; Dalvi et al., 2017; Vig and Belinkov, 2019), (c) explaining the decisions made by neural models to humans (using explanations, highlights, rationales, etc.) (Ribeiro et al., 2016; Li et al., 2016; Ding et al., 2017; Ghaeini et al., 2018; Bastings et al., 2019; Jain et al., 2020), and (d) evaluating different explanation methods from different perspectives (Samek et al., 2016; Mohseni and Ragan, 2018; Poerner et al., 2018; Jain and Wallace, 2019; Serrano and Smith, 2019; Wiegreffe and Pinter, 2019; Li et al., 2020). All of these approaches make NLP neural models more trustworthy. In this work, we focus on faithfulness which intuitively provides the extent to which an explanation accurately represents the true reasoning behind a prediction. It is particularly important for NLP practitioners who wish to debug their neural models and improve them. Faults of a neural model cannot be identified if the neural model does not provide a faithful and trustworthy description of what it is doing. However, the formal definition of faithfulness and the proper approach to"
2021.eacl-main.243,W18-5420,0,0.0156855,"with an explanation objective to constrain input gradient explanations. Rieger et al. (2019) follow a similar spirit but they use contextual decomposition (Murdoch et al., 2018) to extract explanations offered by the model. Aligning attention (as explanation) with prior knowledge has also been extensively studied. This prior knowledge can include alignment data (Mi et al., 2016; Liu et al., 2016), human rationales (Zhong et al., 2019), or even structural biases (Cohn et al., 2016). Inherently Interpretable Neural Models Contrary to post-hoc explanation methods for interpreting a neural model, Stahlberg et al. (2018) show that the NMT model can be made self-explanatory by training it to produce the discrete decisions made by the model (from which the translations can be extracted later). In another work, (Lei et al., 2016; Bastings et al., 2019) propose models in which first a rationale is selected from the input and then is further used for prediction. 7 Conclusion We proposed a method for quantifying faithfulness of NMT models. To optimize faithfulness we have defined a novel objective function that rewards faithful behavior through probability divergence. We also show that the additional constraint in"
2021.eacl-main.243,2020.acl-main.495,0,0.0214138,"focus on the internal state of the NMT model rather than proxy models. They use human references, e.g. AER, for evaluating fidelity. As discussed earlier, evaluation of faithfulness cannot involved human judgements or reference data. It is possible that our faithful NMT models are also better at fidelity, but that is an open question. While prior works have mostly failed to explicitly distinguish faithfulness from plausibility in their arguments, Jacovi and Goldberg (2020a,b) focus on formalizing faithfulness and addressing evaluation of faithfulness separately from plausibility respectively. Subramanian et al. (2020) have investigated the concept of faithfulness in neural modular networks (NMN) which are employed for modeling compositionality. They question the faithfulness of the structure of the network modules describing the true abstract reasoning of the model. Similar to us, they attempt to quantify faithfulness and improve upon it. However their contributions like training with an auxilary atomic-task supervision for improved faithfulness are specific to the context of NMNs. Pruthi et al. (2020) demonstrate that it is possible to train a model that produces a deceptive attention mask, questioning th"
2021.eacl-main.243,Q17-1007,0,0.0412668,"Missing"
2021.eacl-main.243,P16-1008,0,0.0208733,"rturb the model parameters chosen in such a way that the model’s decision should change if the model is faithful (Jacovi and Goldberg, 2020b). A common stress test is the erasure test in which the most-relevant part of the input is removed (Arras et al., 2017). In the context of NMT, at decoding time step t the attention component assigns attention weights αt , attending to the source word at position mt = argmaxi αt [i] (or the k-best attended-to words in the source). These weights are often implicitly or explicitly regarded as an interpretation for the model’s prediction at the time step t (Tu et al., 2016; Mi et al., 2016; Liu et al., 2016; Wang et al., 2016; Lee et al., 2017; Ding et al., 2017; Ghaeini et al., 2018). The erasure stress test for evaluating faithfulness offered by αt is done by setting αt [mt ] to zero and observing whether or not the output changes. It is worth noting that erasure is only one of the possible stress tests for evaluating faithfulness. Passing • We provide empirical evidence that we can improve faithfulness in an NMT model. Our 1 Our code is available at https://github.com/ sfu-natlang/attention_regularization Contributions We seek to improve faithfulness of NMT"
2021.eacl-main.243,W19-4808,0,0.110737,"le the decoder generates century. However, in the right heatmap, sto is not attended to at all but century is still produced as the output. This shows unfaithful behavior. Introduction How trustworthy are our neural models? This question has led to a wide variety of contemporary NLP research focusing on (a) different axes of interpretability including plausibility (or interchangeably human-interpretability) (Herman, 2017; Lage et al., 2019) and faithfulness (Lipton, 2018; Jacovi and Goldberg, 2020b), (b) interpretation of the neural model components (Belinkov et al., 2017; Dalvi et al., 2017; Vig and Belinkov, 2019), (c) explaining the decisions made by neural models to humans (using explanations, highlights, rationales, etc.) (Ribeiro et al., 2016; Li et al., 2016; Ding et al., 2017; Ghaeini et al., 2018; Bastings et al., 2019; Jain et al., 2020), and (d) evaluating different explanation methods from different perspectives (Samek et al., 2016; Mohseni and Ragan, 2018; Poerner et al., 2018; Jain and Wallace, 2019; Serrano and Smith, 2019; Wiegreffe and Pinter, 2019; Li et al., 2020). All of these approaches make NLP neural models more trustworthy. In this work, we focus on faithfulness which intuitively"
2021.eacl-main.243,D16-1058,0,0.0296082,"the model’s decision should change if the model is faithful (Jacovi and Goldberg, 2020b). A common stress test is the erasure test in which the most-relevant part of the input is removed (Arras et al., 2017). In the context of NMT, at decoding time step t the attention component assigns attention weights αt , attending to the source word at position mt = argmaxi αt [i] (or the k-best attended-to words in the source). These weights are often implicitly or explicitly regarded as an interpretation for the model’s prediction at the time step t (Tu et al., 2016; Mi et al., 2016; Liu et al., 2016; Wang et al., 2016; Lee et al., 2017; Ding et al., 2017; Ghaeini et al., 2018). The erasure stress test for evaluating faithfulness offered by αt is done by setting αt [mt ] to zero and observing whether or not the output changes. It is worth noting that erasure is only one of the possible stress tests for evaluating faithfulness. Passing • We provide empirical evidence that we can improve faithfulness in an NMT model. Our 1 Our code is available at https://github.com/ sfu-natlang/attention_regularization Contributions We seek to improve faithfulness of NMT models. To this end, we make the following contributio"
2021.eacl-main.243,D19-1002,0,0.241937,"ulness (Lipton, 2018; Jacovi and Goldberg, 2020b), (b) interpretation of the neural model components (Belinkov et al., 2017; Dalvi et al., 2017; Vig and Belinkov, 2019), (c) explaining the decisions made by neural models to humans (using explanations, highlights, rationales, etc.) (Ribeiro et al., 2016; Li et al., 2016; Ding et al., 2017; Ghaeini et al., 2018; Bastings et al., 2019; Jain et al., 2020), and (d) evaluating different explanation methods from different perspectives (Samek et al., 2016; Mohseni and Ragan, 2018; Poerner et al., 2018; Jain and Wallace, 2019; Serrano and Smith, 2019; Wiegreffe and Pinter, 2019; Li et al., 2020). All of these approaches make NLP neural models more trustworthy. In this work, we focus on faithfulness which intuitively provides the extent to which an explanation accurately represents the true reasoning behind a prediction. It is particularly important for NLP practitioners who wish to debug their neural models and improve them. Faults of a neural model cannot be identified if the neural model does not provide a faithful and trustworthy description of what it is doing. However, the formal definition of faithfulness and the proper approach to its evaluation are still con"
2021.emnlp-main.130,D18-1337,1,0.821107,"a) our model compares partial translations with full-sentence translations of the same I NTERPRETER, which leads to having an oracle that finds notably more effective policies without any unnecessary hyper-parameters. (2) we are using a completely different architecture (3) we explore the performance of our oracle using translation components other than offline translation models. Satija and Pineau (2016); Gu et al. (2017) propose to use a reinforcement learning algorithm to train the Agent. Their proposed agent observes the pretrained offline translation model to learn when to READ or WRITE. Alinejad et al. (2018) improve 6 Conclusion their model by introducing a new predict token in order to have a better estimate of the proper action We present a novel idea for generating optimal segfor the next time step. ments in simultaneous translation by comparing Monotonic attention mechanism (Raffel et al., the output of a simultaneous system for translation 2017; Chiu and Raffel, 2018) proposes to use at- with an offline translation model. This provides 1741 us with oracle action sequences that we can use to train an Agent used to produce read and write actions for simultaneous translation. Our experimental r"
2021.emnlp-main.130,P19-1126,0,0.0699316,"Missing"
2021.emnlp-main.130,2021.eacl-main.233,0,0.446764,"al in input to permit a WRITE action; a signal that might come from a parsing feature (Grissom II et al., 2014), a stochastic classifier (Gu et al., 2017), or the attention module of the translation model (Arivazhagan et al., 2019; Ma et al., 2020). Recently, imitation learning has been considered to train adaptive policies. This thread of research focuses on designing supervised oracle agents that can compute the optimal sequence of READ/WRITE actions; a sequence that leads the model to produce the most similar translation to that of an offline translation model1 . Zheng et al. (2019a,b) and Arthur et al. (2021) use pairs of (input, reference) sentences to train the oracle agent. In this work, we create such an oracle agent using a fully trained neural machine translation model. We do not use reference translations to improve our agent and the translation model is not fine-tuned based on the performance of the agent. In spite of that, we are successful at lowering the latency of simultaneous translation below all previous methods across many different language pairs while retaining a competitive translation quality. Our results also demonstrate the applicability of our policy in augmenting existing s"
2021.emnlp-main.130,2014.iwslt-evaluation.1,0,0.0144092,"s of the prediction space with certain Agent mistakes we augment our training data of action sequences with additional examples that are introduce distortions in the action sequence (Arthur et al., 2021). For each input sentence X we generate its oracle action sequence A and the set of observations O = {o1 , . . . , oT } with ot = (xi , yj , ηh ) to be used for training the Agent. Then we randomly choose a time step t and check the training example (ot , at ) to see if it has the following conditions: • t∈ / {1, T }. • xi 6= &lt;/s>. • yj 6= &lt;/s>. 3 3.1 Experimental Setup Dataset We use IWSLT14 (Cettolo et al., 2014) and WMT153 German to English and IWSLT15 (Luong et al., 2015) Vietnamese to English translation tasks to examine the effectiveness of our approach. Following Elbayad et al. (2020),we tokenize and lower-case the German to English data and BPE (Sennrich et al., 2016) sub-word tokenize both sides. For IWSLT14 data, we choose 10K separate BPE merge operations resulting in approximately 8.8K German and 6.6K English sub-word types. We train our models on 160K sentences and keep 7K of the train data as the validation set. We test our models on a concatenation of dev2010 and tst2010 to tst2013 (a tot"
2021.emnlp-main.130,N18-2079,0,0.0206796,"multaneous translation by introducing a new training objective, which is easier to train than previous attempts at training the agent using reinforcement learning techniques for this task. Our experimental results show that our novel training method for action generation produces much higher quality translations while minimizing the average lag in simultaneous translation. 1 Introduction chooses among two possible actions of READ (waiting to receive more input) and WRITE (producing an output based on the available input inventory). The segmentation agents can either use a fixedlatency policy (Dalvi et al., 2018; Ma et al., 2019a) or use an adaptive policy (Grissom II et al., 2014). In the former, the agent waits for receiving a certain number of input tokens and performs pairs of one WRITE and one READ afterward. In the latter, the agent looks for a specific signal in input to permit a WRITE action; a signal that might come from a parsing feature (Grissom II et al., 2014), a stochastic classifier (Gu et al., 2017), or the attention module of the translation model (Arivazhagan et al., 2019; Ma et al., 2020). Recently, imitation learning has been considered to train adaptive policies. This thread of r"
2021.emnlp-main.130,N13-1073,0,0.0460106,"ty is considerably more accurate. The performance of the eval-wait-5 policy improves when we replace the offline I NTERPRETER with the multi-path model. However, the multi-path model does not outperform eval-wait-5 combined with our oracle policy. The delay of our oracle policy decreases when we change the underlying translation component to a multi-path translation model so using a base translation model trained to handle shorter segments can be combined with an agent trained on our reference actions to improve the average lagging. components. Only using alignments extracted from fast-align (Dyer et al., 2013) on an offline translation component gives us very low BLEU scores. The oracle in (Zheng et al., 2019a) is the closest model to our work. Unlike our policy, they compare each word in partial translations to the target words to find the optimal action sequence. The numbers in Table 1 correspond to the closest results we could get by searching for the optimal hyperparameters. Our oracle policy outperforms their oracle policy in both language pairs. 4.2 Trained Agent Performance Figure 3 shows the results of our trained policy in comparison with the policy trained with multi-path method on DE ↔ E"
2021.emnlp-main.130,D14-1140,0,0.0493431,"Missing"
2021.emnlp-main.130,E17-1099,0,0.218153,"ible actions of READ (waiting to receive more input) and WRITE (producing an output based on the available input inventory). The segmentation agents can either use a fixedlatency policy (Dalvi et al., 2018; Ma et al., 2019a) or use an adaptive policy (Grissom II et al., 2014). In the former, the agent waits for receiving a certain number of input tokens and performs pairs of one WRITE and one READ afterward. In the latter, the agent looks for a specific signal in input to permit a WRITE action; a signal that might come from a parsing feature (Grissom II et al., 2014), a stochastic classifier (Gu et al., 2017), or the attention module of the translation model (Arivazhagan et al., 2019; Ma et al., 2020). Recently, imitation learning has been considered to train adaptive policies. This thread of research focuses on designing supervised oracle agents that can compute the optimal sequence of READ/WRITE actions; a sequence that leads the model to produce the most similar translation to that of an offline translation model1 . Zheng et al. (2019a,b) and Arthur et al. (2021) use pairs of (input, reference) sentences to train the oracle agent. In this work, we create such an oracle agent using a fully train"
2021.emnlp-main.130,2015.iwslt-evaluation.11,0,0.065839,"Missing"
2021.emnlp-main.130,P14-2090,0,0.0279415,"e applicability of our policy in augmenting existing simultaneous neural translation approaches and improving their Simultaneous Machine Translation focuses on the real-time translation of the stream of utterances in the source language to the target language. The essence of simultaneous translation imposes a trade-off between translation quality and the delay in the delivery of the translated utterances. Finding the optimal segments in the input stream is one important task to balance the delay and the translation quality. Segmentation based on the input sentence structure (Ryu et al., 2006; Oda et al., 2014; Shavarani et al., 2015) has been explored in previous work. The other approach to reach the optimal segmen1 tation strategy (policy) is to define the segmentaA model that waits to receive all the input stream tokens and then starts to translate. Such a model can provide more tion problem in reinforcement learning framework accurate translations in comparison to simultaneous translation (Satija and Pineau, 2016; Gu et al., 2017; Aline- model that performs the task using partial and incomplete jad et al., 2018) in which the segmentation agent information. 1734 Proceedings of the 2021 Conferenc"
2021.emnlp-main.130,N19-4009,0,0.0242085,"al., 2019a) 31.18 4.50 20.71 3.31 Table 1: Comparison between different oracles on IWSLT14 DE → EN and IWSLT15 VI → EN datasets. ble) while trying to balance the loss in the translation quality as measured by BLEU score. Among the two measures, BLEU normally gets lower in production settings, as the data is not as clean and well prepared as the benchmark data. On the other hand, improving average lagging is a more reliable method to improve the user experience (in simultaneous translation). 3.3 Model Configuration We use fairly standard Transformer-based NMT system as implemented in Fairseq (Ott et al., 2019) for all of our experiments6 . We augment the implementation to perform simultaneous translation and we incorporate our Agent trained to produce read and write actions with the base NMT system and decoder. Our I NTERPRETER can be additionally configured to replicate the model proposed in (Elbayad et al., 2020). Our agent consists of 4 unidirectional LSTM layers (Hochreiter and Schmidhuber, 1997) with 512 units in each layer. We use a history of the last 3 previous action tokens (i.e. h = 3). Each embedding and linear layer generates a vector of dimension 512. 4 Results and Analysis We compare"
2021.emnlp-main.130,P02-1040,0,0.109406,"If all of the conditions are true, then we swap the action at from READ to WRITE or vice versa, and we generate a new oracle action sequence for the rest of the sentence. The observation ot is updated according to the newly generated oracle. An example is presented and discussed in the appendix. Beam search vs. Greedy decoding Following 3.2 Evaluation previous work, to boost the simultaneous nature We evaluate the translation quality of the translated of the model, we use greedy decoding while persentences using tokenized word-level BLEU score forming the simultaneous decoding in the I NTER (Papineni et al., 2002)5 . We use Average Lagging PRETER . (AL) (Ma et al., 2019b) to measure the decoding The simultaneous decoder (aka the I NTER latency for our models. AL measures the average PRETER ) can use beam search to get more acnumber of words we are lagging behind a policy curate results at expense of the translation speed. that produces words at a rate proportional to the However, this modification does not substantially ratio between target and source lengths, with no change the comparison with baseline methods, so delay. we leave this for future work. Our goal in this paper is to minimize the decodSin"
2021.emnlp-main.130,D19-1137,0,0.218612,"t looks for a specific signal in input to permit a WRITE action; a signal that might come from a parsing feature (Grissom II et al., 2014), a stochastic classifier (Gu et al., 2017), or the attention module of the translation model (Arivazhagan et al., 2019; Ma et al., 2020). Recently, imitation learning has been considered to train adaptive policies. This thread of research focuses on designing supervised oracle agents that can compute the optimal sequence of READ/WRITE actions; a sequence that leads the model to produce the most similar translation to that of an offline translation model1 . Zheng et al. (2019a,b) and Arthur et al. (2021) use pairs of (input, reference) sentences to train the oracle agent. In this work, we create such an oracle agent using a fully trained neural machine translation model. We do not use reference translations to improve our agent and the translation model is not fine-tuned based on the performance of the agent. In spite of that, we are successful at lowering the latency of simultaneous translation below all previous methods across many different language pairs while retaining a competitive translation quality. Our results also demonstrate the applicability of our po"
2021.emnlp-main.130,P19-1582,0,0.236228,"t looks for a specific signal in input to permit a WRITE action; a signal that might come from a parsing feature (Grissom II et al., 2014), a stochastic classifier (Gu et al., 2017), or the attention module of the translation model (Arivazhagan et al., 2019; Ma et al., 2020). Recently, imitation learning has been considered to train adaptive policies. This thread of research focuses on designing supervised oracle agents that can compute the optimal sequence of READ/WRITE actions; a sequence that leads the model to produce the most similar translation to that of an offline translation model1 . Zheng et al. (2019a,b) and Arthur et al. (2021) use pairs of (input, reference) sentences to train the oracle agent. In this work, we create such an oracle agent using a fully trained neural machine translation model. We do not use reference translations to improve our agent and the translation model is not fine-tuned based on the performance of the agent. In spite of that, we are successful at lowering the latency of simultaneous translation below all previous methods across many different language pairs while retaining a competitive translation quality. Our results also demonstrate the applicability of our po"
2021.emnlp-main.130,P06-2088,0,0.0894898,"lso demonstrate the applicability of our policy in augmenting existing simultaneous neural translation approaches and improving their Simultaneous Machine Translation focuses on the real-time translation of the stream of utterances in the source language to the target language. The essence of simultaneous translation imposes a trade-off between translation quality and the delay in the delivery of the translated utterances. Finding the optimal segments in the input stream is one important task to balance the delay and the translation quality. Segmentation based on the input sentence structure (Ryu et al., 2006; Oda et al., 2014; Shavarani et al., 2015) has been explored in previous work. The other approach to reach the optimal segmen1 tation strategy (policy) is to define the segmentaA model that waits to receive all the input stream tokens and then starts to translate. Such a model can provide more tion problem in reinforcement learning framework accurate translations in comparison to simultaneous translation (Satija and Pineau, 2016; Gu et al., 2017; Aline- model that performs the task using partial and incomplete jad et al., 2018) in which the segmentation agent information. 1734 Proceedings of"
2021.emnlp-main.130,P16-1162,0,0.030562,"ence A and the set of observations O = {o1 , . . . , oT } with ot = (xi , yj , ηh ) to be used for training the Agent. Then we randomly choose a time step t and check the training example (ot , at ) to see if it has the following conditions: • t∈ / {1, T }. • xi 6= &lt;/s>. • yj 6= &lt;/s>. 3 3.1 Experimental Setup Dataset We use IWSLT14 (Cettolo et al., 2014) and WMT153 German to English and IWSLT15 (Luong et al., 2015) Vietnamese to English translation tasks to examine the effectiveness of our approach. Following Elbayad et al. (2020),we tokenize and lower-case the German to English data and BPE (Sennrich et al., 2016) sub-word tokenize both sides. For IWSLT14 data, we choose 10K separate BPE merge operations resulting in approximately 8.8K German and 6.6K English sub-word types. We train our models on 160K sentences and keep 7K of the train data as the validation set. We test our models on a concatenation of dev2010 and tst2010 to tst2013 (a total of 6750 sentence pairs). For WMT15, we choose BPE merge operations such that we achieve a joint BPE vocabulary of size 32K types. We will randomly choose 20 percent of the sentence pairs for training the Agent4 . We will also use the same subset for distorting th"
2021.emnlp-main.130,2015.iwslt-papers.14,1,0.597666,"our policy in augmenting existing simultaneous neural translation approaches and improving their Simultaneous Machine Translation focuses on the real-time translation of the stream of utterances in the source language to the target language. The essence of simultaneous translation imposes a trade-off between translation quality and the delay in the delivery of the translated utterances. Finding the optimal segments in the input stream is one important task to balance the delay and the translation quality. Segmentation based on the input sentence structure (Ryu et al., 2006; Oda et al., 2014; Shavarani et al., 2015) has been explored in previous work. The other approach to reach the optimal segmen1 tation strategy (policy) is to define the segmentaA model that waits to receive all the input stream tokens and then starts to translate. Such a model can provide more tion problem in reinforcement learning framework accurate translations in comparison to simultaneous translation (Satija and Pineau, 2016; Gu et al., 2017; Aline- model that performs the task using partial and incomplete jad et al., 2018) in which the segmentation agent information. 1734 Proceedings of the 2021 Conference on Empirical Methods in"
2021.findings-acl.362,Q17-1010,0,0.051027,"gn names seq. of sign names seq. of sign names seq. of sign names seq. of sign names seq. of sign names Embedding Sizes3 16, 32, 64, 128, 256 16, 32, 64, 128, 256 16, 32, 64, 128, 256 16, 32, 64, 128, 256 16, 32, 64, 128, 256 64 Other Parameters window size: 15 window size: 15 window size: 15 window size: 15 window size: 15 hidden dimension: 64 lm.image+text 64 lm.image seq. of sign names and images seq. of sign images image recognition individual sign image 64 hidden dimension: 64 image size: 64×64 hidden dimension: 64 image size: 64×64 image size: 64×64 64 Description Pennington et al. 2014 Bojanowski et al. 2017 Bojanowski et al. 2017 Mikolov et al. 2013a Mikolov et al. 2013a Figure 1, blue (image embedding) omitted. Figure 1. Figure 1, red (text embedding) omitted. Figure 1, blue (image embedding) only. Table 1: List of models considered in this work. This model uses the blue image embedding component from Figure 1 to produce a representation of an input image; a dense layer predicts the name of the sign from this embedding. This model only sees signs in isolation, meaning it will not learn from distributional information. If a result holds for the multimodal LM but not for this image recognition mo"
2021.findings-acl.362,W19-2516,1,0.936875,"ees signs in isolation, meaning it will not learn from distributional information. If a result holds for the multimodal LM but not for this image recognition model, this implies that the result arises from contextual information in the text, and not simply from visual resemblances between signs. We also train CBoW and skipgram models with FastText4 (Bojanowski et al., 2017) and word2vec (Mikolov et al., 2013a), as well as GloVe embeddings (Pennington et al., 2014). Table 1 summarizes all of the models used in this work and important hyperparameters. We train these models on the PE corpus from Born et al. (2019), which is a cleaned version of texts originally published by the Cuneiform Digital Library Initiative (CDLI). This contains digitized transliterations from 1399 tablets comprising 11013 lines in total, or 33778 tokens. 7508 tokens represent broken or unreadable signs, and another 11364 represent numerals, leaving only 14906 non-numerical tokens. 1107 tokens (comprising nearly half the sign types in our cleaned data) are labeled as CGs. We treat each entry of a tablet as a single input sentence for training LMs, and set aside 500 lines as a validation set. Prior to training, we replace all sig"
2021.findings-acl.362,P16-1187,0,0.110213,"pax |M362+M600|. Although these signs are orthographically compositional, it is not known whether they are also semantically compositional. Similar constructions exist in proto-cuneiform (PC), including “containers” with signs inscribed to indicate specific products (Wagensonner, 2015). Some PC compounds survive into later cuneiform, and sometimes have idiomatic meanings, e.g. cuneiform GU7 “eat”, a combination of “head” and “bowl”. Chinese characters likewise exhibit varying degrees of visual and semantic compositionality (Sproat, 2006). Past work (Mikolov et al., 2013b; Salehi et al., 2015; Cordeiro et al., 2016) suggests that embedding models capture semantic compositionality in noun compounds and multiword expressions. Often, these models assign a compound a representation which is similar to the sum of the representations of the words in the compound. Thus we predict that if CGs are semantically compositional, their embeddings will be additively compositional at a higher rate than expected by chance. Their embeddings may also exhibit other signs of internal structure, such as the ability to model proportional analogy between CGs with shared components: red, is a standard embedding layer which repla"
2021.findings-acl.362,2020.conll-1.29,0,0.0287101,"ble 3 shows examples of signs which appear to be compositional in the image LM but not the image recognition model. These are signs for which contextual information plays a deciding role in making them appear semantically compositional, and which may therefore be of interest to analyze in future work. M153 M175 M327 M362 M157 M175 M218 + M106 + M286 + M348 + M244 + M288 + M153 + M388 ≈ |M153+M106| ≈ |M175+M286| ≈ |M327+M348| ≈ |M362+M244| ≈ |M157+M288| ≈ |M175+M153| ≈ |M218+M388| To assess the contribution of a sign to the CGs it occurs in, we consider the pairing consistency score (PCS) from Fournier et al. (2020). This metric measures whether the offsets between pairs of words are more parallel than expected by chance. If a sign s always contributes the same meaning to the CGs in which it occurs, then the offset between the pair of signs (t, |t + s|) is expected to be roughly parallel to the offset between the pair (u, |u + s|) for most choices of t and u. If CGs containing s have idiomatic meanings (so the contribution of s is not consistent), the offsets between such pairs are not likely to be parallel. Thus PCS serves as a proxy for compositionality, and allows us to investigate the impact of indiv"
2021.findings-acl.362,E17-1006,0,0.0171279,"rwise irrelevant to our results. By contrast, Kambhatla et al. (2018) actually sample text from a neural language model to help estimate the quality of a proposed decipherment. Future work could similarly sample from a language model as a means of counteracting the small size of the PE corpus; this should be done with caution, however, given the difficulty of evaluating whether the sampled text is fluent. Salehi et al. (2015) and Cordeiro et al. (2016) demonstrate that English word embeddings tend to be additively compositional and can capture human intuitions about semantic compositionality. Hartung et al. (2017) investigate other methods for decomposing word embeddings. Sproat (2006) discusses a variety of writing systems and the degrees to which they employ phonetic versus semantic information. The discussion is largely taxonomic and addresses subtle nuances between scripts which are already well-understood. In this way it demonstrates the wide range of variation observed between scripts, and by extension the range of possibilities which should be considered when analyzing an undeciphered script such as PE. 6 Conclusion Interpreting what a word embedding model has learned typically involves a compar"
2021.findings-acl.362,D18-1102,1,0.833189,"re sophisticated embedding models and performs a more detailed investigation of the embedding space. Luo et al. (2019) perform automated decipherment of Ugaritic. Their technique finds alignments between orthographic representations of phonetic information, and thus is not easily applicable to ideographic scripts. It also requires multilingual data, and cannot extract information from a script with no known surviving relatives. Our work exploits the embedding space learned by a neural language model, but the actual task of language modeling is otherwise irrelevant to our results. By contrast, Kambhatla et al. (2018) actually sample text from a neural language model to help estimate the quality of a proposed decipherment. Future work could similarly sample from a language model as a means of counteracting the small size of the PE corpus; this should be done with caution, however, given the difficulty of evaluating whether the sampled text is fluent. Salehi et al. (2015) and Cordeiro et al. (2016) demonstrate that English word embeddings tend to be additively compositional and can capture human intuitions about semantic compositionality. Hartung et al. (2017) investigate other methods for decomposing word"
2021.findings-acl.362,P17-1188,0,0.0163954,"dal architecture is finding a balance between contextual and visual information as intended. 5 Related Work Sun et al. (2019) introduce “character-enhanced” embeddings of Chinese words. Their architecture roughly parallels our own, but requires a deeper CNN due to the visual complexity of Chinese characters. We train with a full context language modeling objective whereas they use a sampling scheme similar to word2vec. They use character-level information to improve word embeddings, where we exclusively learn character embeddings. Our application of this architecture to decipherment is novel. Liu et al. (2017) explicitly learn compositional embeddings for Chinese characters. They use su4143 8 Full figures are available in our supplemental data. pervised data to help identify when two visuallydistinct signs use the same radical (as in 水 and 池). In our data, it is not known which signs are truly related to one another, thus we refrain from giving the model explicit information about compositionality. Yin et al. (2019) segment and transcribe undeciphered scripts based on visual similarities between glyphs. Although their transcription error rate is high, they still achieve partial decipherments with n"
2021.findings-acl.362,P19-1303,0,0.0161318,"ription error rate is high, they still achieve partial decipherments with no human intervention. Dencker et al. (2020) perform OCR-style sign detection on images of Sumerian cuneiform tablets, recognizing signs which may be written very differently across the corpus. Their task benefits from the existence of supervised Sumerian training data. Born et al. (2019) train topic models on PE texts and cluster PE signs in a simple mutual information-based embedding model. The present work considers more sophisticated embedding models and performs a more detailed investigation of the embedding space. Luo et al. (2019) perform automated decipherment of Ugaritic. Their technique finds alignments between orthographic representations of phonetic information, and thus is not easily applicable to ideographic scripts. It also requires multilingual data, and cannot extract information from a script with no known surviving relatives. Our work exploits the embedding space learned by a neural language model, but the actual task of language modeling is otherwise irrelevant to our results. By contrast, Kambhatla et al. (2018) actually sample text from a neural language model to help estimate the quality of a proposed d"
2021.findings-acl.362,D14-1162,0,0.0927468,"Missing"
2021.findings-acl.362,N15-1099,0,0.0183446,"Missing"
2021.findings-acl.362,N19-1277,0,0.329796,"isolation, meaning it will not learn from distributional information. If a result holds for the multimodal LM but not for this image recognition model, this implies that the result arises from contextual information in the text, and not simply from visual resemblances between signs. We also train CBoW and skipgram models with FastText4 (Bojanowski et al., 2017) and word2vec (Mikolov et al., 2013a), as well as GloVe embeddings (Pennington et al., 2014). Table 1 summarizes all of the models used in this work and important hyperparameters. We train these models on the PE corpus from Born et al. (2019), which is a cleaned version of texts originally published by the Cuneiform Digital Library Initiative (CDLI). This contains digitized transliterations from 1399 tablets comprising 11013 lines in total, or 33778 tokens. 7508 tokens represent broken or unreadable signs, and another 11364 represent numerals, leaving only 14906 non-numerical tokens. 1107 tokens (comprising nearly half the sign types in our cleaned data) are labeled as CGs. We treat each entry of a tablet as a single input sentence for training LMs, and set aside 500 lines as a validation set. Prior to training, we replace all sig"
C00-2100,P91-1027,0,0.26691,"nd Minnen, 1998; Carroll and Rooth, 1998) give several reasons why subcategorization information is important for a natural language parser. Machinereadable dictionaries are not comprehensive enough to provide this lexical information (Manning, 1993; Briscoe and Carroll, 1997). Furthermore, such dictionaries are available only for very few languages. We need some general method for the automatic extraction of subcategorization information from text corpora. Several techniques and results have been reported on learning subcategorization frames (SFs) from text corpora (Webster and Marcus, 1989; Brent, 1991; Brent, 1993; Brent, 1994; Ushioda et al., 1993; Manning, 1993; Ersan and Charniak, 1996; Briscoe and Carroll, 1997; Carroll and Minnen, 1998; Carroll and Rooth, 1998). All of this work  This work was done during the second author’s visit to the University of Pennsylvania. We would like to thank Prof. Aravind Joshi, David Chiang, Mark Dras and the anonymous reviewers for their comments. The first author’s work is partially supported by NSF Grant SBR 8920230. Many tools used in this work are the results of project No. VS96151 of the Ministry of Education of the Czech Republic. The data (PDT)"
C00-2100,J93-2002,0,0.634068,"98; Carroll and Rooth, 1998) give several reasons why subcategorization information is important for a natural language parser. Machinereadable dictionaries are not comprehensive enough to provide this lexical information (Manning, 1993; Briscoe and Carroll, 1997). Furthermore, such dictionaries are available only for very few languages. We need some general method for the automatic extraction of subcategorization information from text corpora. Several techniques and results have been reported on learning subcategorization frames (SFs) from text corpora (Webster and Marcus, 1989; Brent, 1991; Brent, 1993; Brent, 1994; Ushioda et al., 1993; Manning, 1993; Ersan and Charniak, 1996; Briscoe and Carroll, 1997; Carroll and Minnen, 1998; Carroll and Rooth, 1998). All of this work  This work was done during the second author’s visit to the University of Pennsylvania. We would like to thank Prof. Aravind Joshi, David Chiang, Mark Dras and the anonymous reviewers for their comments. The first author’s work is partially supported by NSF Grant SBR 8920230. Many tools used in this work are the results of project No. VS96151 of the Ministry of Education of the Czech Republic. The data (PDT) is thanks to"
C00-2100,A97-1052,0,0.73027,"erved frame types) was 450. 5 Comparison with related work Preliminary work on SF extraction from corpora was done by (Brent, 1991; Brent, 1993; Brent, 1994) and (Webster and Marcus, 1989; Ushioda et al., 1993). Brent (Brent, 1993; Brent, 1994) uses the standard method of testing miscue probabilities for filtering frames observed with a verb. (Brent, 1994) presents a method for estimating p!f . Brent applied his method to a small number of verbs and associated SF types. (Manning, 1993) applies Brent’s method to parsed data and obtains a subcategorization dictionary for a larger set of verbs. (Briscoe and Carroll, 1997; Carroll and Minnen, 1998) differs from earlier work in that a substantially larger set of SF types are considered; (Carroll and Rooth, 1998) use an EM algorithm to learn subcategorization as a result of learning rule probabilities, and, in turn, to improve parsing accuracy by applying the verb SFs obtained. (Basili and Vindigni, 1998) use a conceptual clustering algorithm for acquiring subcategorization frames for Italian. They establish a partial order on partially overlapping OFs (similar to our OF subsets) which is then used to suggest a potential SF. A complete comparison of all the prev"
C00-2100,W98-1114,0,0.0563471,"5 Comparison with related work Preliminary work on SF extraction from corpora was done by (Brent, 1991; Brent, 1993; Brent, 1994) and (Webster and Marcus, 1989; Ushioda et al., 1993). Brent (Brent, 1993; Brent, 1994) uses the standard method of testing miscue probabilities for filtering frames observed with a verb. (Brent, 1994) presents a method for estimating p!f . Brent applied his method to a small number of verbs and associated SF types. (Manning, 1993) applies Brent’s method to parsed data and obtains a subcategorization dictionary for a larger set of verbs. (Briscoe and Carroll, 1997; Carroll and Minnen, 1998) differs from earlier work in that a substantially larger set of SF types are considered; (Carroll and Rooth, 1998) use an EM algorithm to learn subcategorization as a result of learning rule probabilities, and, in turn, to improve parsing accuracy by applying the verb SFs obtained. (Basili and Vindigni, 1998) use a conceptual clustering algorithm for acquiring subcategorization frames for Italian. They establish a partial order on partially overlapping OFs (similar to our OF subsets) which is then used to suggest a potential SF. A complete comparison of all the previous approaches with the cu"
C00-2100,W98-1505,0,0.111682,"; Brent, 1994) and (Webster and Marcus, 1989; Ushioda et al., 1993). Brent (Brent, 1993; Brent, 1994) uses the standard method of testing miscue probabilities for filtering frames observed with a verb. (Brent, 1994) presents a method for estimating p!f . Brent applied his method to a small number of verbs and associated SF types. (Manning, 1993) applies Brent’s method to parsed data and obtains a subcategorization dictionary for a larger set of verbs. (Briscoe and Carroll, 1997; Carroll and Minnen, 1998) differs from earlier work in that a substantially larger set of SF types are considered; (Carroll and Rooth, 1998) use an EM algorithm to learn subcategorization as a result of learning rule probabilities, and, in turn, to improve parsing accuracy by applying the verb SFs obtained. (Basili and Vindigni, 1998) use a conceptual clustering algorithm for acquiring subcategorization frames for Italian. They establish a partial order on partially overlapping OFs (similar to our OF subsets) which is then used to suggest a potential SF. A complete comparison of all the previous approaches with the current work is given in Table 2. While these approaches differ in size and quality of training data, number of SF ty"
C00-2100,J93-1003,0,0.0847181,"Missing"
C00-2100,P98-1080,0,0.0317281,"Missing"
C00-2100,W99-0632,0,0.0619095,"Missing"
C00-2100,P99-1051,0,0.107523,"Missing"
C00-2100,E99-1007,0,0.0818818,"Missing"
C00-2100,W99-0503,0,0.0676496,"Missing"
C00-2100,W93-0109,0,0.841214,"8) give several reasons why subcategorization information is important for a natural language parser. Machinereadable dictionaries are not comprehensive enough to provide this lexical information (Manning, 1993; Briscoe and Carroll, 1997). Furthermore, such dictionaries are available only for very few languages. We need some general method for the automatic extraction of subcategorization information from text corpora. Several techniques and results have been reported on learning subcategorization frames (SFs) from text corpora (Webster and Marcus, 1989; Brent, 1991; Brent, 1993; Brent, 1994; Ushioda et al., 1993; Manning, 1993; Ersan and Charniak, 1996; Briscoe and Carroll, 1997; Carroll and Minnen, 1998; Carroll and Rooth, 1998). All of this work  This work was done during the second author’s visit to the University of Pennsylvania. We would like to thank Prof. Aravind Joshi, David Chiang, Mark Dras and the anonymous reviewers for their comments. The first author’s work is partially supported by NSF Grant SBR 8920230. Many tools used in this work are the results of project No. VS96151 of the Ministry of Education of the Czech Republic. The data (PDT) is thanks to grant No. 405/96/K214 of the Grant"
C00-2100,P89-1022,0,0.102663,"ns by a parser. (Carroll and Minnen, 1998; Carroll and Rooth, 1998) give several reasons why subcategorization information is important for a natural language parser. Machinereadable dictionaries are not comprehensive enough to provide this lexical information (Manning, 1993; Briscoe and Carroll, 1997). Furthermore, such dictionaries are available only for very few languages. We need some general method for the automatic extraction of subcategorization information from text corpora. Several techniques and results have been reported on learning subcategorization frames (SFs) from text corpora (Webster and Marcus, 1989; Brent, 1991; Brent, 1993; Brent, 1994; Ushioda et al., 1993; Manning, 1993; Ersan and Charniak, 1996; Briscoe and Carroll, 1997; Carroll and Minnen, 1998; Carroll and Rooth, 1998). All of this work  This work was done during the second author’s visit to the University of Pennsylvania. We would like to thank Prof. Aravind Joshi, David Chiang, Mark Dras and the anonymous reviewers for their comments. The first author’s work is partially supported by NSF Grant SBR 8920230. Many tools used in this work are the results of project No. VS96151 of the Ministry of Education of the Czech Republic. Th"
C00-2100,H91-1067,0,\N,Missing
C00-2100,C98-1077,0,\N,Missing
C00-2100,P93-1032,0,\N,Missing
C02-1040,A97-1052,0,0.0615904,"find the verb’s subcategorization frame (SF). For this paper, we are interested in whether the verb takes an intransitive SF or a transitive SF. In general, the problem of identifying subcategorization frames is to distinguish between arguments and adjuncts among the constituents modifying a verb. For example, in “John saw Mary yesterday at the station”, only “John” and “Mary” are required arguments while the other constituents are optional (adjuncts).3 The problem of SF identification using statistical methods has had a rich discussion in the literature (Ushioda et al., 1993; Manning, 1993; Briscoe and Carroll, 1997; Brent, 1994) (also see the refences cited in (Sarkar and Zeman, 2000)). In this paper, we use the method of hypothesis testing to discover the SF for a given verb (Brent, 1994). Along with the techniques given in these papers, (Sarkar and Zeman, 2000; Korhonen et al., 2000) also discuss other methods for hypothesis testing such the use of the t-score statistic and the likelihood ratio test. After experimenting with all three of these methods we selected the likelihood ratio test because it performed with higher accuracy on a small set of hand-annotated instances. We use the determination of"
C02-1040,J93-1003,0,0.0366903,". For fur3 There is some controversy as to the correct subcategorization of a given verb and linguists often disagree as to what is the right set of SFs for a given verb. A machine learning approach such as the one followed in this paper sidesteps this issue altogether, since it is left to the algorithm to learn what is an appropriate SF for a verb. The stance taken in this paper is that the efficacy of SF learning is evaluated on some domain, as is done here on learning verb alternations. ther background on this method of hypothesis testing the reader is referred to (Bickel and Doksum, 1977; Dunning, 1993). 3.1 Likelihood ratio test Let us take the hypothesis that the distribution of an observed frame f in the training data is independent of the distribution of a verb v. We can phrase this hypothesis as p( f |v) = p( f |!v) = p( f ), that is distribution of a frame f given that a verb v is present is the same as the distribution of f given that v is not present (written as !v). We use the log likelihood test statistic (Bickel and Doksum, 1977, 209) as a measure to discover particular frames and verbs that are highly associated in the training data. k1 n1 k2 n2 = = = = c( f, v) c(v) = c( f, v) +"
C02-1040,C02-1132,0,0.0957304,"Missing"
C02-1040,W00-1325,0,0.0140779,"ts modifying a verb. For example, in “John saw Mary yesterday at the station”, only “John” and “Mary” are required arguments while the other constituents are optional (adjuncts).3 The problem of SF identification using statistical methods has had a rich discussion in the literature (Ushioda et al., 1993; Manning, 1993; Briscoe and Carroll, 1997; Brent, 1994) (also see the refences cited in (Sarkar and Zeman, 2000)). In this paper, we use the method of hypothesis testing to discover the SF for a given verb (Brent, 1994). Along with the techniques given in these papers, (Sarkar and Zeman, 2000; Korhonen et al., 2000) also discuss other methods for hypothesis testing such the use of the t-score statistic and the likelihood ratio test. After experimenting with all three of these methods we selected the likelihood ratio test because it performed with higher accuracy on a small set of hand-annotated instances. We use the determination of the verb’s SF as an input to our argument structure classifier (see Section 4). The method works as follows: for each verb, we need to associate a score to the hypothesis that a particular set of dependents of the verb are arguments of that verb. In other words, we need to as"
C02-1040,W99-0632,0,0.539848,"nd Zeman, 2000) is used to extract SFs in English. The extracted SFs are evaluated by classifying verbs into verb alternation classes. 1 Introduction The classification of verbs based on their underlying thematic structure involves distinguishing verbs that take the same number and category of arguments but assign different thematic roles to these arguments. This is often termed as the classification of verb diathesis roles or the lexical semantics of predicates in natural language (see (Levin, 1993; McCarthy and Korhonen, 1998; Stevenson and Merlo, 1999; Stevenson et al., 1999; Lapata, 1999; Lapata and Brew, 1999; Schulte im Walde, 2000)). Following the method described in (Merlo and Stevenson, 2001; Stevenson and Merlo, 1999; Stevenson et al., 1999), we exploit the distributions of some selected features from the local context of a verb but we differ from these previous studies in the use of minimally annotated data to construct our classifier. The data we use is only passed through a part-ofspeech tagger and a chunker which is used to identify base phrasal categories such as noun-phrase and verb-phrase chunks to identify potential arguments of each verb. Lexical knowledge acquisition plays an import"
C02-1040,P99-1051,0,0.0190205,"zech (Sarkar and Zeman, 2000) is used to extract SFs in English. The extracted SFs are evaluated by classifying verbs into verb alternation classes. 1 Introduction The classification of verbs based on their underlying thematic structure involves distinguishing verbs that take the same number and category of arguments but assign different thematic roles to these arguments. This is often termed as the classification of verb diathesis roles or the lexical semantics of predicates in natural language (see (Levin, 1993; McCarthy and Korhonen, 1998; Stevenson and Merlo, 1999; Stevenson et al., 1999; Lapata, 1999; Lapata and Brew, 1999; Schulte im Walde, 2000)). Following the method described in (Merlo and Stevenson, 2001; Stevenson and Merlo, 1999; Stevenson et al., 1999), we exploit the distributions of some selected features from the local context of a verb but we differ from these previous studies in the use of minimally annotated data to construct our classifier. The data we use is only passed through a part-ofspeech tagger and a chunker which is used to identify base phrasal categories such as noun-phrase and verb-phrase chunks to identify potential arguments of each verb. Lexical knowledge acqu"
C02-1040,P98-2247,0,0.0214997,"s that a subcategorization frame (SF) learning algorithm previously applied to Czech (Sarkar and Zeman, 2000) is used to extract SFs in English. The extracted SFs are evaluated by classifying verbs into verb alternation classes. 1 Introduction The classification of verbs based on their underlying thematic structure involves distinguishing verbs that take the same number and category of arguments but assign different thematic roles to these arguments. This is often termed as the classification of verb diathesis roles or the lexical semantics of predicates in natural language (see (Levin, 1993; McCarthy and Korhonen, 1998; Stevenson and Merlo, 1999; Stevenson et al., 1999; Lapata, 1999; Lapata and Brew, 1999; Schulte im Walde, 2000)). Following the method described in (Merlo and Stevenson, 2001; Stevenson and Merlo, 1999; Stevenson et al., 1999), we exploit the distributions of some selected features from the local context of a verb but we differ from these previous studies in the use of minimally annotated data to construct our classifier. The data we use is only passed through a part-ofspeech tagger and a chunker which is used to identify base phrasal categories such as noun-phrase and verb-phrase chunks to"
C02-1040,J01-3003,0,0.504465,". Consider the following verbs, each occuring with intransitive and transitive SFs1 . Unergative (1) a. The horse raced past the barn. b. The jockey raced the horse past the barn. Unaccusative (2) a. The butter melted in the pan. b. The cook melted the butter in the pan. ∗ This research was supported in part by NSF grant SBR-8920230. Thanks to Paola Merlo, Dan Gildea, David Chiang, Aravind Joshi and the anonymous reviewers for their comments. Also thanks to Virginie Nanta for an earlier collaboration with the first author on an unsupervised version of this work. 1 The examples are taken from (Merlo and Stevenson, 2001). See (Levin, 1993) for more information. The particular categorization that we use here is motivated in (Stevenson and Merlo, 1997) Object-Drop (3) a. The boy washed. b. The boy washed the hall. Each of the verbs above occurs with both the intransitive and transitive SFs. However, the verbs differ in their underlying argument structure. Each verb assigns a different role to their arguments in the two subcategorization possibilities. For each verb above, the following lists the roles assigned to each of the noun phrase arguments in the SFs permitted for the verb. This information can be used f"
C02-1040,C00-2100,1,0.853576,"NPagent washed TRAN: NPagent washed NPtheme Our task is to identify the transitive and intransitive usage of a particular verb as being related via this notion of argument structure. This is called the argument structure classification of the verb. In the remainder of this paper we will look at the problem of placing verbs into such classes automatically. Our results in this paper serve as a replication and extension of the results in (Merlo and Stevenson, 2001). Our main contribution in this paper is to show that a subcategorization frame (SF) learning algorithm previously applied to Czech (Sarkar and Zeman, 2000) can be applied to English and evaluated by classifying verbs into verb alternation classes. We perform this task using only tagged and chunked data as input to our subcategorization frame learning stage. Our result can be compared to previous work (Merlo and Stevenson, 2001) which did not use SF learning but used a 65M word WSJ corpus which was tagged as well as automatically parsed with a Treebank trained statistical parser. It is important to note that (Merlo and Stevenson, 2001) extract some features using the tagged information (in fact, those features that we use SF learning to extract)"
C02-1040,C00-2108,0,0.0663593,"SFs in English. The extracted SFs are evaluated by classifying verbs into verb alternation classes. 1 Introduction The classification of verbs based on their underlying thematic structure involves distinguishing verbs that take the same number and category of arguments but assign different thematic roles to these arguments. This is often termed as the classification of verb diathesis roles or the lexical semantics of predicates in natural language (see (Levin, 1993; McCarthy and Korhonen, 1998; Stevenson and Merlo, 1999; Stevenson et al., 1999; Lapata, 1999; Lapata and Brew, 1999; Schulte im Walde, 2000)). Following the method described in (Merlo and Stevenson, 2001; Stevenson and Merlo, 1999; Stevenson et al., 1999), we exploit the distributions of some selected features from the local context of a verb but we differ from these previous studies in the use of minimally annotated data to construct our classifier. The data we use is only passed through a part-ofspeech tagger and a chunker which is used to identify base phrasal categories such as noun-phrase and verb-phrase chunks to identify potential arguments of each verb. Lexical knowledge acquisition plays an important role in corpus-based"
C02-1040,E99-1007,0,0.299046,"ame (SF) learning algorithm previously applied to Czech (Sarkar and Zeman, 2000) is used to extract SFs in English. The extracted SFs are evaluated by classifying verbs into verb alternation classes. 1 Introduction The classification of verbs based on their underlying thematic structure involves distinguishing verbs that take the same number and category of arguments but assign different thematic roles to these arguments. This is often termed as the classification of verb diathesis roles or the lexical semantics of predicates in natural language (see (Levin, 1993; McCarthy and Korhonen, 1998; Stevenson and Merlo, 1999; Stevenson et al., 1999; Lapata, 1999; Lapata and Brew, 1999; Schulte im Walde, 2000)). Following the method described in (Merlo and Stevenson, 2001; Stevenson and Merlo, 1999; Stevenson et al., 1999), we exploit the distributions of some selected features from the local context of a verb but we differ from these previous studies in the use of minimally annotated data to construct our classifier. The data we use is only passed through a part-ofspeech tagger and a chunker which is used to identify base phrasal categories such as noun-phrase and verb-phrase chunks to identify potential argument"
C02-1040,W99-0503,0,0.0620409,"previously applied to Czech (Sarkar and Zeman, 2000) is used to extract SFs in English. The extracted SFs are evaluated by classifying verbs into verb alternation classes. 1 Introduction The classification of verbs based on their underlying thematic structure involves distinguishing verbs that take the same number and category of arguments but assign different thematic roles to these arguments. This is often termed as the classification of verb diathesis roles or the lexical semantics of predicates in natural language (see (Levin, 1993; McCarthy and Korhonen, 1998; Stevenson and Merlo, 1999; Stevenson et al., 1999; Lapata, 1999; Lapata and Brew, 1999; Schulte im Walde, 2000)). Following the method described in (Merlo and Stevenson, 2001; Stevenson and Merlo, 1999; Stevenson et al., 1999), we exploit the distributions of some selected features from the local context of a verb but we differ from these previous studies in the use of minimally annotated data to construct our classifier. The data we use is only passed through a part-ofspeech tagger and a chunker which is used to identify base phrasal categories such as noun-phrase and verb-phrase chunks to identify potential arguments of each verb. Lexical"
C02-1040,W93-0109,0,0.061087,"argument structure of the verb is to find the verb’s subcategorization frame (SF). For this paper, we are interested in whether the verb takes an intransitive SF or a transitive SF. In general, the problem of identifying subcategorization frames is to distinguish between arguments and adjuncts among the constituents modifying a verb. For example, in “John saw Mary yesterday at the station”, only “John” and “Mary” are required arguments while the other constituents are optional (adjuncts).3 The problem of SF identification using statistical methods has had a rich discussion in the literature (Ushioda et al., 1993; Manning, 1993; Briscoe and Carroll, 1997; Brent, 1994) (also see the refences cited in (Sarkar and Zeman, 2000)). In this paper, we use the method of hypothesis testing to discover the SF for a given verb (Brent, 1994). Along with the techniques given in these papers, (Sarkar and Zeman, 2000; Korhonen et al., 2000) also discuss other methods for hypothesis testing such the use of the t-score statistic and the likelihood ratio test. After experimenting with all three of these methods we selected the likelihood ratio test because it performed with higher accuracy on a small set of hand-annotat"
C02-1040,W96-0213,0,\N,Missing
C02-1040,P93-1032,0,\N,Missing
C02-1040,C98-2242,0,\N,Missing
C08-1039,P07-1036,0,0.0437843,"Missing"
C08-1039,N04-1042,0,0.0400746,"ror on Citation Test (300L5000U) EMfreez Error on Citation Test (300L5000U) λ2 0.18 λ 0.21 Viterbi Decoding SMS Decoding Viterbi Decoding SMS Decoding 0.2 0.17 Error (per position) Error (per position) 0.19 0.16 0.15 0.14 0.18 0.17 0.16 0.15 0.14 0.13 λMLE 0.12 0 0.1 0.2 0.3 0.4 0.5 λ 0.6 0.7 0.8 0.9 λMLE 0.13 0.12 1 0 (a) 0.1 0.2 0.3 0.4 0.5 λ 0.6 0.7 0.8 0.9 1 (b) Figure 2: EMλ error rates while increasing the allocation from 0 to 1 by the step size 0.025. to segment the document into fields, and to label each field. In our experiments we use the bibliographic citation dataset described in (Peng and McCallum, 2004) (see Fig. 1 for an example of the input and expected label output for this task). This dataset has 500 annotated citations with 13 fields; 5000 unannotated citations were added to it later by (Grenager et al., 2005). The annotated data is split into a 300-document training set, a 100-document development (dev) set, and a 100document test set7 . We use a first order HMM with the size of hidden states equal to the number of fields (equal to 13). We freeze the transition probabilities to what has been observed in the labeled data and only learn the emission probabilities. The transition probabil"
C08-1039,J94-2001,0,\N,Missing
C08-1039,P05-1046,0,\N,Missing
C96-2103,P88-1032,1,\N,Missing
C96-2103,H91-1035,1,\N,Missing
C98-2152,J91-3004,0,0.347155,"Missing"
C98-2152,C92-2066,0,0.107344,"Missing"
C98-2152,C88-1075,0,0.099961,"Missing"
C98-2152,J95-2002,0,0.426521,"hnology, which is sponsored by NWO (Dutch Organization for Scientific Research). The second and third authors were partially supported by NSF grant SBR8920230 and ARO grant DAAH0404-94G-0426. The authors wish to thank Aravind Joshi for his support in this research. 953 Giorgio Satta Dip. di Elettr. e Inf. Univ. di Padova via Gradenigo 6/A, 35131 Padova, Italy satta~dei, unipd, it ural language would improve performance of such language models, some researchers tried to use stochastic context-free grammars (CFGs) to produce language models (Wright and Wrigley, 1989; Jelinek and Lafferty, 1991; Stolcke, 1995). The probability model used for a stochastic grammar was ~we~* Pr(at...aT~w). However, language models that are based on trigram probability models out-perform stochastic CFGs. The common wisdom about this failure of CFGs is that trigram models are lexicalized models while CFGs are ,lot. q~'ee Adjoining Grammars (TAGs) are important in this respect since they are easily lexicalized while capturing the constituent structure of language. More importantly, TAGs allow greater linguistic expressiveness. The trees associated with words can be used to encode argument and adjunct relations in various"
C98-2152,W89-0211,0,0.0378846,"and by the Priority Programme Language and Speech Technology, which is sponsored by NWO (Dutch Organization for Scientific Research). The second and third authors were partially supported by NSF grant SBR8920230 and ARO grant DAAH0404-94G-0426. The authors wish to thank Aravind Joshi for his support in this research. 953 Giorgio Satta Dip. di Elettr. e Inf. Univ. di Padova via Gradenigo 6/A, 35131 Padova, Italy satta~dei, unipd, it ural language would improve performance of such language models, some researchers tried to use stochastic context-free grammars (CFGs) to produce language models (Wright and Wrigley, 1989; Jelinek and Lafferty, 1991; Stolcke, 1995). The probability model used for a stochastic grammar was ~we~* Pr(at...aT~w). However, language models that are based on trigram probability models out-perform stochastic CFGs. The common wisdom about this failure of CFGs is that trigram models are lexicalized models while CFGs are ,lot. q~'ee Adjoining Grammars (TAGs) are important in this respect since they are easily lexicalized while capturing the constituent structure of language. More importantly, TAGs allow greater linguistic expressiveness. The trees associated with words can be used to enco"
C98-2185,C92-2066,0,0.0926339,"ewers for their valuable comments. 1164 From the literature on probabilistic contextfree grammars (CFGs) we know precisely the conditions which ensure that (1) is true for a given CFG. This paper derives the conditions under which a given probabilistic TAG can be shown to be consistent. TAGs are important in the modelling of natural language since they can be easily lexicalized; moreover the trees associated with words can be used to encode argument and adjunct relations in various syntactic environments. This paper assumes some familiarity with the TAG formalism. (Joshi, 1988) and (Joshi and Schabes, 1992) are good introductions to the formalism and its linguistic relevance. TAGs have been shown to have relations with both phrasestructure grammars and dependency grammars (Rambow and Joshi, 1995) and can handle (non-projective) long distance dependencies. Consistency of probabilistic TAGs has practical significance for the following reasons: • The conditions derived here can be used to ensure that probability models that use TAGs can be checked for deficiency. • Existing EM based estinmtion algorithms for probabilistic TAGs assume that the property of consistency holds (Schabes, 1992). EM based"
D07-1062,P05-1022,0,0.153171,"Missing"
D07-1062,W03-1006,0,0.268469,"Missing"
D07-1062,2000.iwpt-1.9,0,0.111401,"Missing"
D07-1062,P00-1058,0,0.0557747,"ication and classification accuracy in SRL. 2 Using LTAG-based Features in SRL We assume some familiarity with Lexicalized TreeAdjoining Grammar (LTAG); (Joshi and Schabes, 1997) is a good introduction to this formalism. A LTAG is defined to be a set of lexicalized elementary trees (etree for short), of which there are two types, initial trees and auxiliary trees. Typically etrees can be composed through two operations into parse trees, substitution and adjunction. We use sister adjunction which is commonly used in LTAG statistical parsers to deal with the relatively flat Penn Treebank trees (Chiang, 2000). The tree produced by composing the etrees is the derived/parse tree and the tree that records the history of composition is the derivation tree. A reasonable way to define SRL features is to provide a strictly local dependency (i.e. within a single etree) between predicate and argument. There have been many different proposals on how to maintain syntactic locality (Xia, 1999; Chen and VijayShanker, 2000) and SRL locality (Chen and Rambow, 2003; Shen and Joshi, 2005) when extracting LTAG etrees from a Treebank. These proposed methods are exemplified by the derivation tree γ1 in Fig. 1. Howeve"
D07-1062,W03-1008,0,0.100169,"has relied on various syntactic representations of input sentences, such as syntactic chunks (Hacioglu et al., 2004) and full syntactic parses (Gildea and Jurafsky, 2002). In contrast with features from shallow parsing, previous work (Gildea and Palmer, 2002; Punyakanok et al., 2005b) has shown the necessity of full syntactic parsing for SRL. In order to generalize the path feature (see Table 1 in Section 3) which is probably the most salient (while being the most data sparse) feature for SRL, previous work has extracted features from other syntactic representations, such as CCG derivations (Gildea and Hockenmaier, 2003) and dependency trees (Hacioglu, 2004) or integrated features from different parsers (Pradhan et al., 2005b). To avoid explicit feature engineering on trees, (Moschitti, 2004) used convolution kernels on selective portions of syntactic trees. In this paper, we also compare our work with tree kernel based methods. Most SRL systems exploit syntactic trees as the main source of features. We would like to take this one step further and show that using LTAG deriva590 Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational c Natural Language Lea"
D07-1062,J02-3001,0,0.609955,"[A0seller Ports of Call Inc.] reached agreements to [Vverb sell] [A1thing its remaining seven aircraft] [A2buyer to buyers that weren’t disclosed] . is an example of SRL annotation from the PropBank corpus (Palmer et al., 2005), where the subscripted information maps the semantic roles A0, A1, A2 to arguments for the predicate sell as defined in the PropBank Frame Scheme. For SRL, high accuracy has been achieved by: (i) proposing new types of features (see Table 1 in Section 3 for previously proposed features), (ii) modeling the predicate frameset by capturing dependencies between arguments (Gildea and Jurafsky, 2002; Pradhan et al., 2004; Toutanova et al., 2005; Punyakanok et al., 2005a), (iii) dealing with incorrect parser output by using more than one parser (Pradhan et al., 2005b). Our work in this paper falls into category (i). We propose several novel features based on Lexicalized Tree Adjoining Grammar (LTAG) derivation trees in order to improve SRL performance. To show the usefulness of these features, we provide an experimental study comparing LTAG-based features with the standard set of features and kernel methods used in state-of-the-art SRL systems. The LTAG formalism provides an extended doma"
D07-1062,P02-1031,0,0.0564995,"ased features with the standard set of features and kernel methods used in state-of-the-art SRL systems. The LTAG formalism provides an extended domain of locality in which to specify predicate-argument relationships and also provides the notion of a derivation tree. These two properties of LTAG make it well suited to address the SRL task. SRL feature extraction has relied on various syntactic representations of input sentences, such as syntactic chunks (Hacioglu et al., 2004) and full syntactic parses (Gildea and Jurafsky, 2002). In contrast with features from shallow parsing, previous work (Gildea and Palmer, 2002; Punyakanok et al., 2005b) has shown the necessity of full syntactic parsing for SRL. In order to generalize the path feature (see Table 1 in Section 3) which is probably the most salient (while being the most data sparse) feature for SRL, previous work has extracted features from other syntactic representations, such as CCG derivations (Gildea and Hockenmaier, 2003) and dependency trees (Hacioglu, 2004) or integrated features from different parsers (Pradhan et al., 2005b). To avoid explicit feature engineering on trees, (Moschitti, 2004) used convolution kernels on selective portions of synt"
D07-1062,W04-2416,0,0.0262026,"vation trees in order to improve SRL performance. To show the usefulness of these features, we provide an experimental study comparing LTAG-based features with the standard set of features and kernel methods used in state-of-the-art SRL systems. The LTAG formalism provides an extended domain of locality in which to specify predicate-argument relationships and also provides the notion of a derivation tree. These two properties of LTAG make it well suited to address the SRL task. SRL feature extraction has relied on various syntactic representations of input sentences, such as syntactic chunks (Hacioglu et al., 2004) and full syntactic parses (Gildea and Jurafsky, 2002). In contrast with features from shallow parsing, previous work (Gildea and Palmer, 2002; Punyakanok et al., 2005b) has shown the necessity of full syntactic parsing for SRL. In order to generalize the path feature (see Table 1 in Section 3) which is probably the most salient (while being the most data sparse) feature for SRL, previous work has extracted features from other syntactic representations, such as CCG derivations (Gildea and Hockenmaier, 2003) and dependency trees (Hacioglu, 2004) or integrated features from different parsers (Pr"
D07-1062,C04-1186,0,0.0452413,"input sentences, such as syntactic chunks (Hacioglu et al., 2004) and full syntactic parses (Gildea and Jurafsky, 2002). In contrast with features from shallow parsing, previous work (Gildea and Palmer, 2002; Punyakanok et al., 2005b) has shown the necessity of full syntactic parsing for SRL. In order to generalize the path feature (see Table 1 in Section 3) which is probably the most salient (while being the most data sparse) feature for SRL, previous work has extracted features from other syntactic representations, such as CCG derivations (Gildea and Hockenmaier, 2003) and dependency trees (Hacioglu, 2004) or integrated features from different parsers (Pradhan et al., 2005b). To avoid explicit feature engineering on trees, (Moschitti, 2004) used convolution kernels on selective portions of syntactic trees. In this paper, we also compare our work with tree kernel based methods. Most SRL systems exploit syntactic trees as the main source of features. We would like to take this one step further and show that using LTAG deriva590 Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational c Natural Language Learning, pp. 590–599, Prague, June 2007."
D07-1062,W06-1518,1,0.869297,"Missing"
D07-1062,P05-1012,0,0.0446552,"rovided by the CoNLL 2005 shared task. We did this for three reasons: (i) our results are directly comparable to those who have used the Charniak parses distributed with the CoNLL 2005 data-set; (ii) we avoid the possibility of a better parser identifying a larger number of argument constituents and thus leading to better results, which is orthogonal to the discriminative power of our proposed LTAG-based features; and (iii) the quality of LTAG derivation trees depends indirectly on the quality of head dependencies recovered by the parser and it is a well-known folklore result (see Table 3 in (McDonald et al., 2005)) that applying the head-percolation heuristics on parser output produces better dependencies when compared to dependencies directly recovered by the parser (whether the parser is an LTAG parser or a lexicalized PCFG parser). 2.1.1 Decompositions of Parse Trees LTAG-based Features We defined 5 LTAG feature categories: predicate etree-related features (P for short), argument etreerelated features (A), subcategorization-related features (S), topological relation-related features (R), intermediate etree-related features (I). Since we consider up to 6 intermediate etrees between the predicate and"
D07-1062,P04-1043,0,0.0812328,"h features from shallow parsing, previous work (Gildea and Palmer, 2002; Punyakanok et al., 2005b) has shown the necessity of full syntactic parsing for SRL. In order to generalize the path feature (see Table 1 in Section 3) which is probably the most salient (while being the most data sparse) feature for SRL, previous work has extracted features from other syntactic representations, such as CCG derivations (Gildea and Hockenmaier, 2003) and dependency trees (Hacioglu, 2004) or integrated features from different parsers (Pradhan et al., 2005b). To avoid explicit feature engineering on trees, (Moschitti, 2004) used convolution kernels on selective portions of syntactic trees. In this paper, we also compare our work with tree kernel based methods. Most SRL systems exploit syntactic trees as the main source of features. We would like to take this one step further and show that using LTAG deriva590 Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational c Natural Language Learning, pp. 590–599, Prague, June 2007. 2007 Association for Computational Linguistics α1 : S S NP VP MD(will) VP VP V(join) γ1 : α1 (join) NP β1 (will) MD(will) VP∗ V(join) PP"
D07-1062,J05-1004,0,0.167509,"Missing"
D07-1062,N04-1030,0,0.367247,"c.] reached agreements to [Vverb sell] [A1thing its remaining seven aircraft] [A2buyer to buyers that weren’t disclosed] . is an example of SRL annotation from the PropBank corpus (Palmer et al., 2005), where the subscripted information maps the semantic roles A0, A1, A2 to arguments for the predicate sell as defined in the PropBank Frame Scheme. For SRL, high accuracy has been achieved by: (i) proposing new types of features (see Table 1 in Section 3 for previously proposed features), (ii) modeling the predicate frameset by capturing dependencies between arguments (Gildea and Jurafsky, 2002; Pradhan et al., 2004; Toutanova et al., 2005; Punyakanok et al., 2005a), (iii) dealing with incorrect parser output by using more than one parser (Pradhan et al., 2005b). Our work in this paper falls into category (i). We propose several novel features based on Lexicalized Tree Adjoining Grammar (LTAG) derivation trees in order to improve SRL performance. To show the usefulness of these features, we provide an experimental study comparing LTAG-based features with the standard set of features and kernel methods used in state-of-the-art SRL systems. The LTAG formalism provides an extended domain of locality in whic"
D07-1062,W05-0634,0,0.368208,"otation from the PropBank corpus (Palmer et al., 2005), where the subscripted information maps the semantic roles A0, A1, A2 to arguments for the predicate sell as defined in the PropBank Frame Scheme. For SRL, high accuracy has been achieved by: (i) proposing new types of features (see Table 1 in Section 3 for previously proposed features), (ii) modeling the predicate frameset by capturing dependencies between arguments (Gildea and Jurafsky, 2002; Pradhan et al., 2004; Toutanova et al., 2005; Punyakanok et al., 2005a), (iii) dealing with incorrect parser output by using more than one parser (Pradhan et al., 2005b). Our work in this paper falls into category (i). We propose several novel features based on Lexicalized Tree Adjoining Grammar (LTAG) derivation trees in order to improve SRL performance. To show the usefulness of these features, we provide an experimental study comparing LTAG-based features with the standard set of features and kernel methods used in state-of-the-art SRL systems. The LTAG formalism provides an extended domain of locality in which to specify predicate-argument relationships and also provides the notion of a derivation tree. These two properties of LTAG make it well suited t"
D07-1062,P05-1072,0,0.654432,"otation from the PropBank corpus (Palmer et al., 2005), where the subscripted information maps the semantic roles A0, A1, A2 to arguments for the predicate sell as defined in the PropBank Frame Scheme. For SRL, high accuracy has been achieved by: (i) proposing new types of features (see Table 1 in Section 3 for previously proposed features), (ii) modeling the predicate frameset by capturing dependencies between arguments (Gildea and Jurafsky, 2002; Pradhan et al., 2004; Toutanova et al., 2005; Punyakanok et al., 2005a), (iii) dealing with incorrect parser output by using more than one parser (Pradhan et al., 2005b). Our work in this paper falls into category (i). We propose several novel features based on Lexicalized Tree Adjoining Grammar (LTAG) derivation trees in order to improve SRL performance. To show the usefulness of these features, we provide an experimental study comparing LTAG-based features with the standard set of features and kernel methods used in state-of-the-art SRL systems. The LTAG formalism provides an extended domain of locality in which to specify predicate-argument relationships and also provides the notion of a derivation tree. These two properties of LTAG make it well suited t"
D07-1062,W05-0625,0,0.047692,"g its remaining seven aircraft] [A2buyer to buyers that weren’t disclosed] . is an example of SRL annotation from the PropBank corpus (Palmer et al., 2005), where the subscripted information maps the semantic roles A0, A1, A2 to arguments for the predicate sell as defined in the PropBank Frame Scheme. For SRL, high accuracy has been achieved by: (i) proposing new types of features (see Table 1 in Section 3 for previously proposed features), (ii) modeling the predicate frameset by capturing dependencies between arguments (Gildea and Jurafsky, 2002; Pradhan et al., 2004; Toutanova et al., 2005; Punyakanok et al., 2005a), (iii) dealing with incorrect parser output by using more than one parser (Pradhan et al., 2005b). Our work in this paper falls into category (i). We propose several novel features based on Lexicalized Tree Adjoining Grammar (LTAG) derivation trees in order to improve SRL performance. To show the usefulness of these features, we provide an experimental study comparing LTAG-based features with the standard set of features and kernel methods used in state-of-the-art SRL systems. The LTAG formalism provides an extended domain of locality in which to specify predicate-argument relationships and"
D07-1062,W05-0639,0,0.0284457,"g its remaining seven aircraft] [A2buyer to buyers that weren’t disclosed] . is an example of SRL annotation from the PropBank corpus (Palmer et al., 2005), where the subscripted information maps the semantic roles A0, A1, A2 to arguments for the predicate sell as defined in the PropBank Frame Scheme. For SRL, high accuracy has been achieved by: (i) proposing new types of features (see Table 1 in Section 3 for previously proposed features), (ii) modeling the predicate frameset by capturing dependencies between arguments (Gildea and Jurafsky, 2002; Pradhan et al., 2004; Toutanova et al., 2005; Punyakanok et al., 2005a), (iii) dealing with incorrect parser output by using more than one parser (Pradhan et al., 2005b). Our work in this paper falls into category (i). We propose several novel features based on Lexicalized Tree Adjoining Grammar (LTAG) derivation trees in order to improve SRL performance. To show the usefulness of these features, we provide an experimental study comparing LTAG-based features with the standard set of features and kernel methods used in state-of-the-art SRL systems. The LTAG formalism provides an extended domain of locality in which to specify predicate-argument relationships and"
D07-1062,W03-1012,1,0.702325,"Missing"
D07-1062,P03-1002,0,0.0872322,"nt as well as the distance characterize the topological relations among the relevant etrees. In particular, the attachment point and distance features can explicitly capture important information hidden in the standard path feature. The intermediate tree related features can give richer contextual information between predicate tree and argument trees. We added the subcat index feature to be complementary to the sub-cat and syntactic frame features in the standard feature set. 3 Standard Feature Set Our standard feature set is a combination of features proposed by (Gildea and Jurafsky, 2002), (Surdeanu et al., 2003; Pradhan et al., 2004; Pradhan et al., 2005b) and (Xue and Palmer, 2004). All features listed in Table 1 are used for argument classification in our baseline system; and features with asterisk are not used for argument identification2 . We compare this baseline SRL system with a system that includes a combination of these features with the LTAG-based features. Our baseline uses all features that have been used in the state-of-the-art SRL systems and as our experimental results show, these standard features do indeed obtain state-of-the-art 2 This is a standard idea in the SRL literature: remo"
D07-1062,P05-1073,0,0.13504,"Missing"
D07-1062,W04-3212,0,0.389833,"he relevant etrees. In particular, the attachment point and distance features can explicitly capture important information hidden in the standard path feature. The intermediate tree related features can give richer contextual information between predicate tree and argument trees. We added the subcat index feature to be complementary to the sub-cat and syntactic frame features in the standard feature set. 3 Standard Feature Set Our standard feature set is a combination of features proposed by (Gildea and Jurafsky, 2002), (Surdeanu et al., 2003; Pradhan et al., 2004; Pradhan et al., 2005b) and (Xue and Palmer, 2004). All features listed in Table 1 are used for argument classification in our baseline system; and features with asterisk are not used for argument identification2 . We compare this baseline SRL system with a system that includes a combination of these features with the LTAG-based features. Our baseline uses all features that have been used in the state-of-the-art SRL systems and as our experimental results show, these standard features do indeed obtain state-of-the-art 2 This is a standard idea in the SRL literature: removing features more useful for classification, e.g. named entity features,"
D07-1062,C00-2137,0,0.105818,"Missing"
D07-1062,A00-2018,0,\N,Missing
D13-1110,W12-3102,0,0.0685887,"Missing"
D13-1110,P05-1033,0,0.268489,"Missing"
D13-1110,J07-2003,0,0.734101,"operties, we show that the original LR-Hiero decoding proposed by (Watanabe et al., 2006b) does not perform to the same level of the standard CKY Hiero with cube pruning (see Table 3). In addition, the current LR decoding algorithm does not obtain BLEU scores comparable to phrase-based or CKYbased Hiero models for different language pairs (see Table 4). In this paper we propose modifications to the LR decoding algorithm that addresses these limitations and provides, for the first time, a true alternative to the standard CKY Hiero algorithm that uses left-to-right decoding. Introduction Hiero (Chiang, 2007) models translation using a lexicalized synchronous context-free grammar (SCFG) extracted from word aligned bitexts. Typically, CKY-style decoding is used for Hiero with time complexity O(n3 ) for source input with n words. Scoring the target language output using a language model within CKY-style decoding requires two histories per hypothesis, one on the left edge of each span and one on the right, due to the fact that the target side is not generated in left to right order, but rather built bottom-up from sub-spans. This leads to complex problems in efficient language model integration and r"
D13-1110,P11-2031,0,0.0511244,"nges in different tuning runs in our experiments. We find a gain of about 1 BLEU point when we add a single distortion feature d and a further gain of 0.3 BLEU (not shown due to lack of space) when we split the distortion feature for the two rule types (dp and dg ). The last line in part two of Table 4 shows a consistent gain of 1.6 BLEU over the LRHiero baseline for both language pairs. It shows that LR-Hiero maintains the BLEU scores obtained by “phrase-based” and “CKY Hiero-GNF”. We performed statistical significance tests using two different tools: Moses bootstrap resampling and MultEval (Clark et al., 2011). The difference between “LR-Hiero+CP+reordering feat” and three baselines: “phrase-based”, “CKY HieroGNF”, “LR-Hiero+reordering feat” are not statistically significant even for p-value of 0.1 for both tools. To investigate the impact of proposed reordering features with other decoder or models. We add these features to both Hiero and Hiero-GNF7 . The last part of Table 4 shows the performance CKY decoder 7 Feature rhi is defined for SCFG rules and cannot be adopted to phrase-based translation systems; and Moses uses distortion feature therefore we omit Moses from this experiment. 1097 with di"
D13-1110,P81-1022,0,0.235145,"t side of Hiero rules and conclude that source discontinuous spans are always more useful than discontinuities on the target side with experiments on four language pairs (zh-en, fren, de-en and es-en). As we shall also see in our experimental results (see Table 4) we can get close to the BLEU scores obtained using the full set of Hiero rules by using only target lexicalized rules in our LR decoder. 2.2 LR-Hiero Decoding LR-Hiero decoding uses a top-down depth-first search, which strictly grows the hypotheses in target surface ordering. Search on the source side follows an Earley-style search (Earley, 1970), the dot jumps around on the source side of the rules based on the order of nonterminals on the target side. This search is integrated with beam search or cube pruning to efficiently find the k-best translations. Several important details about the algorithm of LR-Hiero decoding are implicit and unexplained in (Watanabe et al., 2006b). In this section we describe the LR-Hiero decoding algorithm in more detail than the original description in (Watanabe et al., 1091 Algorithm 1: LR-Hiero Decoding 1: Input sentence: f = f0 f1 . . . fn 2: F = FutureCost(f ) (Precompute future cost for spans) 3: f"
D13-1110,N10-1140,0,0.0972203,"och nicht gemacht X2 , X2 not yet done X1 i. allow reordering as well. X → hf¯X1 , e¯X1 i X → hX1 f¯, e¯X1 i X → hX1 f¯X2 , e¯X1 X2 i (3) X → hX1 f¯X2 , e¯X2 X1 i It might appear that the restriction that target-side rules be GNF is a severe restriction on the coverage of possible hypotheses compared to the full set of rules permitted by the Hiero extraction heuristic. However there is some evidence in the literature that discontinuous spans on the source side in translation rules is a lot more useful than discontinuous spans in the target side (which is disallowed in the GNF). For instance, (Galley and Manning, 2010) do an extensive study of discontinuous spans on source and target side and show that source side discontinuous spans are very useful but removing discontinuous spans on the target side only lowers the BLEU score by 0.2 points (using the Joshua SMT system on Chinese-English). Removing discontinuous spans means that the target side rules have the form: uX, Xu, XuX, XXu, or uXX of which we disallow Xu, XuX, XXu. Zhang and Zong (2012) also conduct a study on discontinuous spans on source and target side of Hiero rules and conclude that source discontinuous spans are always more useful than discon"
D13-1110,2011.iwslt-evaluation.24,0,0.190675,"zed synchronous context-free grammar (SCFG) extracted from word aligned bitexts. Typically, CKY-style decoding is used for Hiero with time complexity O(n3 ) for source input with n words. Scoring the target language output using a language model within CKY-style decoding requires two histories per hypothesis, one on the left edge of each span and one on the right, due to the fact that the target side is not generated in left to right order, but rather built bottom-up from sub-spans. This leads to complex problems in efficient language model integration and requires state reduction techniques (Heafield et al., 2011; Heafield et al., 2013). The size of a Hiero SCFG grammar is typically larger than phrase-based models extracted We introduce a new extended version of the LR decoding algorithm presented in (Watanabe et al., 2006b) which is demonstrably more efficient than the CKY Hiero algorithm. We measure the efficiency of the LR Hiero decoder in a way that is independent of the choice of system and programming language by measuring the number of language model queries. Although more efficient, the new LR decoding algorithm suffered from lower BLEU scores compared to CKY Hiero. Our analysis of left to rig"
D13-1110,P07-1019,0,0.0519278,"The Algorithm 2 shows the pseudocode for LR-decoding using cube pruning. The structure of stacks and hypotheses and computing the future cost is similar to Algorithm 1 (lines 1-5). To fill stack Si , it iterates over previous stacks (line 8 in Algorithm 2) 4 . All hypotheses in each stack Sp (covering p words on the source-side) are first partitioned into a set of groups, {G}, based on their first uncovered span (line 9) 5 . Each group g is a 4 As the length of rules are limited (at most MRL), we can ignore stacks with index less than i − MRL 5 The beam search decoder in Phrase-based system (Huang and Chiang, 2007; Koehn et al., 2007; Sankaran et al., 2010) 1093 2-tuple (gspan , ghyps ), where ghyps is a list of hypotheses which share the same first uncovered span gspan . Rules matching the span gspan are obtained from routine GetSpanRules, which are then grouped based on unique source side rules (i.e. each Rs contains rules that share the same source side s but have different target sides). Each ghyps and possible Rs 6 create a cube which is added to cubeList. In LR-Hiero, each hypothesis is developed with only one uncovered span, therefore each cube always has just two dimensions: (1) hypotheses with"
D13-1110,D10-1027,0,0.203395,"s uncovered by current hypothesis together with the hypothesis cost. The future cost is precomputed (line 2 Algorithm 1) in a way similar to the phrase-based models (Koehn et al., 2007) using only the terminal rules of the grammar. The ComputeCost method (line 22 in Algorithm 1) uses the usual log-linear model and scores a hypothesis based on its different feature scores g(h0 ) and the future cost of the yet to be covered spans (F¬h0cov ). Time complexity of left to right Hiero decoding with beam search is O(n2 b) in practice where n is the length of source sentence and b is the size of beam (Huang and Mi, 2010). 2.3 LR-Hiero Decoding with Cube Pruning The Algorithm 1 presented earlier does an exhaustive search as it generates all possible partial translations for a given stack that are reachable from the hypotheses in previous stacks. However only a few of these hypotheses are retained, while majority of them are pruned away. The cube pruning technique (Chiang, 2007) avoids the wasteful generation of poor hypotheses that are likely to be pruned away by efficiently restricting the generation to only high scoring partial translations. We modify the cube pruning for LR-decoding that takes into account"
D13-1110,N03-1017,0,0.0267236,". Decoding the example in Figure 1(b) is explained using a walk-through shown in Figure 2. Each partial hypothesis h is a 4-tuple (ht , hs , hcov , hc ): consisting of a translation prefix ht , a (LIFO-ordered) list hs of uncovered spans, source words coverage set hcov and the hypothesis cost hc . The initial hypothesis is a null string with just a sentence-initial marker hsi and the list hs containing a span of the whole sentence, [0, n]. The hypotheses are stored in stacks S0 , . . . , Sn , where each stack corresponds to a coverage vector of same size, covering same number of source words (Koehn et al., 2003). At the beginning of beam search the initial hyrules G 1) X →〈 schuler X 1 / students X 1 〉 G 2) X →〈 X 1 heban X 2 / have X 1 X 2 〉 3 ) X →〈 X 1 noch nicht X 2 /not yet X 2 X 1 〉 4 ) X →〈 gemacht /done 〉 5 ) X →〈 ihre arbeit / their work 〉 6 ) X →〈 ./. 〉 source side coverage hypothesis  X ⟦ schuler ihre arbeit noch nicht gemacht haben .⟧ 1 schuler  X 1 ⟦ihre arbeit noch nicht gemacht haben .⟧ schuler  X 21 ⟦ihre arbeit noch nicht gemacht ⟧ haben X 22 ⟦.⟧ 3 3 &lt;s> 2 schuler X 1 ⟦ihre arbeit ⟧ noch nicht  X 2 ⟦ gemacht⟧ haben X 2 ⟦.⟧ 3 2 schuler  X 1 ⟦ihre arbeit ⟧ noch nicht gemacht haben"
D13-1110,P07-2045,0,0.0550689,"er 1092 considers the second hypothesis and pops the span [1, 8]. It then matches the rule (#2) and pushes the spans [1, 6] and [7, 8] into the list hs in the reverse order of their appearance in the target-side rule. At each step the new hypothesis is added to the decoder stack Sl depending on the number of covered words in the new hypothesis (line 13 in Algorithm 1). For pruning we use an estimate of the future cost3 of the spans uncovered by current hypothesis together with the hypothesis cost. The future cost is precomputed (line 2 Algorithm 1) in a way similar to the phrase-based models (Koehn et al., 2007) using only the terminal rules of the grammar. The ComputeCost method (line 22 in Algorithm 1) uses the usual log-linear model and scores a hypothesis based on its different feature scores g(h0 ) and the future cost of the yet to be covered spans (F¬h0cov ). Time complexity of left to right Hiero decoding with beam search is O(n2 b) in practice where n is the length of source sentence and b is the size of beam (Huang and Mi, 2010). 2.3 LR-Hiero Decoding with Cube Pruning The Algorithm 1 presented earlier does an exhaustive search as it generates all possible partial translations for a given st"
D13-1110,D07-1104,0,0.0234525,"der state using Earley dot notation (superscripts show rule#) (c) Hypotheses pane showing translation prefix and ordered list of yet-to-be-covered spans. pothesis h0 is added to the decoder stack S0 (line 6 in Algoorithm 1). Hypotheses in each decoder stack are expanded iteratively, generating new hypotheses, which are added to the latter stacks corresponding to the number of source words covered. In each step it pops from the LIFO list hs , the span [u, v] of the next hypothesis h to be processed. All rules that match the entire span [u, v] are then obtained efficiently via pattern matching (Lopez, 2007). GetSpanRules addresses possible ambiguities in matched rules to the given span [u, v]. For example, given a rule r, with source side rs : hX1 the X2 i and source phrase p : hok, the more the betteri. There is ambiguity in matching r to p. GetSpanRules returns a distinct matched rule for each possible matching. The GrowHypothesis routine creates a new candidate by expanding given hypothesis h using rule r and computes the complete hypothesis score including language model score. Since the target-side rules are in GNF, the translation prefix of the new hypothesis is obtained by simply concaten"
D13-1110,H91-1036,0,0.441135,"096 Time Efficiency Comparison To evaluate the performance of LR-Hiero decoding with cube pruning (LR-Hiero+CP), we compare it with three baselines: (i) CKY Hiero, (ii) CKY Hiero-GNF, and (iii) LR-Hiero (without cube pruning) with two different beam size 500 and 1000. When it comes to instrument timing results, there are lots of system level details that we wish to abstract away from, and focus only on the number of “edges” processed by the decoder. In comparison of parsing algorithms, the common practice is to measure the number of edges processed by different algorithms for the same reason (Moore and Dowding, 1991). By analogy to parsing algorithm comparisons, we compare the different decoding algorithms with respect to the number of calls made to the language model (LM) since that directly corresponds to the number of hypotheses considered by the decoder. A decoder is more time efficient if it can consider fewer translation hypotheses while maintaining the same BLEU score. All of the baselines use the same wrapper to query the language model, and we have instrumented the wrapper to count the statistics we need and thus we can say this is a fair comparison. For this experiment we use a sample set of 50"
D13-1110,P13-1156,0,0.0492858,"r models. We add these features to both Hiero and Hiero-GNF7 . The last part of Table 4 shows the performance CKY decoder 7 Feature rhi is defined for SCFG rules and cannot be adopted to phrase-based translation systems; and Moses uses distortion feature therefore we omit Moses from this experiment. 1097 with different models (full Hiero and GNF) with the new reordering features in terms of BLEU score. The results show that these features are helpful in both models. Although, they do not make a big difference in Hiero with full model, they can alleviate the lack of non-GNF rules in Hiero-GNF. Nguyen and Vogel (2013) integrate traditional phrase-based features: distortion and lexicalized reordering into Hiero as well. They show that such features can be useful to boost the translation quality of CKY Hiero with the full rule set. Nguyen and Vogel (2013) compute the distortion feature in a different way, only applicable to CKY. The distortion for each cell is computed after the translation for nonterminal sub-spans is complete. In LR-decoding, we compute distortion for rules even though we are yet to translate some of the sub-spans. Thus our approach computes the distortion incrementally for the untranslate"
D13-1110,P03-1021,0,0.0170171,"ificantly equal BLEU scores when compared with Moses (Koehn et al., 2007) for several language pairs (Razmara et al., 2012; Callison-Burch et al., 2012). • Hiero-GNF: where we use Hiero decoder with the restricted LR-Hiero grammar (GNF rules). • LR-Hiero: our implementation of LR-Hiero (Watanabe et al., 2006b) in Python. • phrase-based: Moses (Koehn et al., 2007) • LR-Hiero+CP: LR-Hiero decoding with cube pruning. We use a 5-gram LM trained on the Gigaword corpus and use KenLM (Heafield, 2011) for LM scoring during decoding. We tune weights by minimizing BLEU loss on the dev set through MERT (Och, 2003) and report BLEU scores on the test set. We use comparable pop limits in each of the decoders: 1000 for Moses and LR-Hiero and 500 with cube pruning for CKY Hiero and LR-Hiero+CP. Other extraction and decoder settings such as maximum phrase length, etc. were identical across settings so that the results are comparable. Table 2 shows how the LR-Hiero grammar is much smaller than CKY-based Hiero. 1096 Time Efficiency Comparison To evaluate the performance of LR-Hiero decoding with cube pruning (LR-Hiero+CP), we compare it with three baselines: (i) CKY Hiero, (ii) CKY Hiero-GNF, and (iii) LR-Hier"
D13-1110,W12-3145,1,0.854484,"7.61 1,303.2 / 4.2 7,231.62 / 20.33 5,858.74 / 18.23 83,518.63 / 328.11 42,783.12 / 192.23 1,697.7 / 5.67 Table 3: Comparing average number and time of language model queries. 4.2 Table 2: Model sizes (millions of rules). We do not count glue rules for LR-Hiero which are created at runtime as needed. • Hiero: we used Kriya, our open-source implementation of Hiero in Python, which performs comparably to other open-source Hiero systems (Sankaran et al., 2012). Kriya can obtain statistically significantly equal BLEU scores when compared with Moses (Koehn et al., 2007) for several language pairs (Razmara et al., 2012; Callison-Burch et al., 2012). • Hiero-GNF: where we use Hiero decoder with the restricted LR-Hiero grammar (GNF rules). • LR-Hiero: our implementation of LR-Hiero (Watanabe et al., 2006b) in Python. • phrase-based: Moses (Koehn et al., 2007) • LR-Hiero+CP: LR-Hiero decoding with cube pruning. We use a 5-gram LM trained on the Gigaword corpus and use KenLM (Heafield, 2011) for LM scoring during decoding. We tune weights by minimizing BLEU loss on the dev set through MERT (Och, 2003) and report BLEU scores on the test set. We use comparable pop limits in each of the decoders: 1000 for Moses an"
D13-1110,W10-1733,1,0.893633,"-decoding using cube pruning. The structure of stacks and hypotheses and computing the future cost is similar to Algorithm 1 (lines 1-5). To fill stack Si , it iterates over previous stacks (line 8 in Algorithm 2) 4 . All hypotheses in each stack Sp (covering p words on the source-side) are first partitioned into a set of groups, {G}, based on their first uncovered span (line 9) 5 . Each group g is a 4 As the length of rules are limited (at most MRL), we can ignore stacks with index less than i − MRL 5 The beam search decoder in Phrase-based system (Huang and Chiang, 2007; Koehn et al., 2007; Sankaran et al., 2010) 1093 2-tuple (gspan , ghyps ), where ghyps is a list of hypotheses which share the same first uncovered span gspan . Rules matching the span gspan are obtained from routine GetSpanRules, which are then grouped based on unique source side rules (i.e. each Rs contains rules that share the same source side s but have different target sides). Each ghyps and possible Rs 6 create a cube which is added to cubeList. In LR-Hiero, each hypothesis is developed with only one uncovered span, therefore each cube always has just two dimensions: (1) hypotheses with the same number of covered words and simila"
D13-1110,2006.iwslt-evaluation.14,0,0.0652911,"aran et al., 2012). Abstract Left-to-right (LR) decoding (Watanabe et al., 2006b) is a promising decoding algorithm for hierarchical phrase-based translation (Hiero). It generates the target sentence by extending the hypotheses only on the right edge. LR decoding has complexity O(n2 b) for input of n words and beam size b, compared to O(n3 ) for the CKY algorithm. It requires a single language model (LM) history for each target hypothesis rather than two LM histories per hypothesis as in CKY. In this paper we present an augmented LR decoding algorithm that builds on the original algorithm in (Watanabe et al., 2006b). Unlike that algorithm, using experiments over multiple language pairs we show two new results: our LR decoding algorithm provides demonstrably more efficient decoding than CKY Hiero, four times faster; and by introducing new distortion and reordering features for LR decoding, it maintains the same translation quality (as in BLEU scores) obtained phrase-based and CKY Hiero with the same translation model. 1 In contrast, the LR-decoding algorithm could avoid these shortcomings such as faster time complexity, reduction in the grammar size and the simplified left-to-right language model scorin"
D13-1110,P06-1098,0,0.0868599,"aran et al., 2012). Abstract Left-to-right (LR) decoding (Watanabe et al., 2006b) is a promising decoding algorithm for hierarchical phrase-based translation (Hiero). It generates the target sentence by extending the hypotheses only on the right edge. LR decoding has complexity O(n2 b) for input of n words and beam size b, compared to O(n3 ) for the CKY algorithm. It requires a single language model (LM) history for each target hypothesis rather than two LM histories per hypothesis as in CKY. In this paper we present an augmented LR decoding algorithm that builds on the original algorithm in (Watanabe et al., 2006b). Unlike that algorithm, using experiments over multiple language pairs we show two new results: our LR decoding algorithm provides demonstrably more efficient decoding than CKY Hiero, four times faster; and by introducing new distortion and reordering features for LR decoding, it maintains the same translation quality (as in BLEU scores) obtained phrase-based and CKY Hiero with the same translation model. 1 In contrast, the LR-decoding algorithm could avoid these shortcomings such as faster time complexity, reduction in the grammar size and the simplified left-to-right language model scorin"
D13-1110,N13-1116,0,\N,Missing
D13-1110,W11-2123,0,\N,Missing
D14-1028,N03-1017,0,0.0094552,"iated to span [4, i]. A worked out example of how the decoder works is shown in Figure 2. Each partial hypothesis h is a 4-tuple (ht , hs , hcov , hc ): consisting of a translation prefix ht , a (LIFO-ordered) list hs of uncovered spans, source words coverage set hcov and the hypothesis cost hc . The initial hypothesis is a null string with just a sentence-initial marker hsi and the list hs containing a span of the whole sentence, [0, n]. The hypotheses are stored in stacks S0 , . . . , Sn , where Sp contains hypotheses covering p source words just like in stack decoding for phrase-based SMT (Koehn et al., 2003). To fill stack Si we consider hypotheses in each stack Sp 2 , which are first partitioned into a set of groups {G}, based on their first uncovered span (line 9). Each group g is a 2-tuple (gspan , ghyps ), where ghyps is a list of hypotheses which share the same first uncovered span gspan . Rules matching the span gspan are obtained from routine GetSpanRules. Each ghyps and possible Rs create a cube which is added to cubeList. The Merge routine gets the best hypotheses from all cubes (see Fig.1). Hypotheses (rows) and columns (rules) are sorted based on their scores. GetBestHypotheses((H, R),"
D14-1028,J07-2003,0,0.915255,"coding algorithm for hierarchical phrase-based translation (Hiero) that visits input spans in arbitrary order producing the output translation in left to right order. This leads to far fewer language model calls, but while LR decoding is more efficient than CKY decoding, it is unable to capture some hierarchical phrase alignments reachable using CKY decoding and suffers from lower translation quality as a result. This paper introduces two improvements to LR decoding that make it comparable in translation quality to CKY-based Hiero. 1 Introduction Hierarchical phrase-based translation (Hiero) (Chiang, 2007) uses a lexicalized synchronous context-free grammar (SCFG) extracted from word and phrase alignments of a bitext. Decoding for Hiero is typically done with CKY-style decoding with time complexity O(n3 ) for source input with n words. Computing the language model score for each hypothesis within CKY decoding requires two histories, the left and the right edge of each span, due to the fact that the target side is built inside-out from sub-spans (Heafield et al., 2011; Heafield et al., 2013). LR-decoding algorithms exist for phrasebased (Koehn, 2004; Galley and Manning, 2010) and syntax-based (H"
D14-1028,P07-2045,0,0.0108012,"dot jumps around on the source side of the rules based on the order of nonterminals on the target side. This search is integrated with beam search or cube pruning to find the k-best translations. Algorithm 1 shows the pseudocode for LRHiero decoding with cube pruning (Chiang, 2007) (CP). LR-Hiero with CP was introduced in (Siahbani et al., 2013). In this pseudocode, we have introduced the notion of queue diversity (explained below). However to understand our change we need to understand the algorithm in more detail. 1 The future cost is precomputed in a way similar to the phrase-based models (Koehn et al., 2007) using only the terminal rules of the grammar. 2 As the length of rules are limited (at most MRL), we can ignore stacks with index less than i − MRL 222 hypotheses rules hsi[0, 15] G 1)hT aiguo shi X1 /T hailand X1 i hsi Thailand [2,15] G 2)hyao X1 /wants X1 i hsiThailand wants [3,15] G 3)hliyong X1 /to utilize X1 i hsiThailand wants to utilize [4,15] 4)hzhe bi qian X1 /this money X1 i hsiThailand wants to utilize this money [7,15] 5)hX1 zhuru geng duo X2 /to inject more X2 X1 i hsiThailand wants to utilize this money to inject more [12,15][7,9] 6)hliudong X1 /circulating X1 i hsiThailand want"
D14-1028,P11-2031,0,0.110351,"Missing"
D14-1028,koen-2004-pharaoh,0,0.148502,"erarchical phrase-based translation (Hiero) (Chiang, 2007) uses a lexicalized synchronous context-free grammar (SCFG) extracted from word and phrase alignments of a bitext. Decoding for Hiero is typically done with CKY-style decoding with time complexity O(n3 ) for source input with n words. Computing the language model score for each hypothesis within CKY decoding requires two histories, the left and the right edge of each span, due to the fact that the target side is built inside-out from sub-spans (Heafield et al., 2011; Heafield et al., 2013). LR-decoding algorithms exist for phrasebased (Koehn, 2004; Galley and Manning, 2010) and syntax-based (Huang and Mi, 2010; Feng et al., 2012) models and also for hierarchical phrasebased models (Watanabe et al., 2006; Siahbani et al., 2013), which is our focus in this paper. Watanabe et al. (2006) first proposed left-toright (LR) decoding for Hiero (LR-Hiero henceforth) which uses beam search and runs in O(n2 b) in practice where n is the length of source sentence and b is the size of beam (Huang and Mi, 2010). To simplify target generation, SCFG rules are con2 LR Decoding with Queue Diversity LR-Hiero uses a constrained lexicalized SCFG which we ca"
D14-1028,P81-1022,0,0.405903,"new hyp in queue) 24: hypList = {} 25: while |heapQ |> 0 and |hypList |< K do 26: (h0c , h0 , [H, R]) = pop(heapQ) (pop the best hypothesis) 27: push(heapQ, GetN eighbours([H, R]) (Push neighbours to queue) 28: Add h0 to hypList 29: return hypList the target string is generated from left to right. The rules are obtained from a word and phrase aligned bitext using the rule extraction algorithm in (Watanabe et al., 2006). LR-Hiero decoding uses a top-down depth-first search, which strictly grows the hypotheses in target surface ordering. Search on the source side follows an Earley-style search (Earley, 1970), the dot jumps around on the source side of the rules based on the order of nonterminals on the target side. This search is integrated with beam search or cube pruning to find the k-best translations. Algorithm 1 shows the pseudocode for LRHiero decoding with cube pruning (Chiang, 2007) (CP). LR-Hiero with CP was introduced in (Siahbani et al., 2013). In this pseudocode, we have introduced the notion of queue diversity (explained below). However to understand our change we need to understand the algorithm in more detail. 1 The future cost is precomputed in a way similar to the phrase-based mo"
D14-1028,D12-1109,0,0.217361,"synchronous context-free grammar (SCFG) extracted from word and phrase alignments of a bitext. Decoding for Hiero is typically done with CKY-style decoding with time complexity O(n3 ) for source input with n words. Computing the language model score for each hypothesis within CKY decoding requires two histories, the left and the right edge of each span, due to the fact that the target side is built inside-out from sub-spans (Heafield et al., 2011; Heafield et al., 2013). LR-decoding algorithms exist for phrasebased (Koehn, 2004; Galley and Manning, 2010) and syntax-based (Huang and Mi, 2010; Feng et al., 2012) models and also for hierarchical phrasebased models (Watanabe et al., 2006; Siahbani et al., 2013), which is our focus in this paper. Watanabe et al. (2006) first proposed left-toright (LR) decoding for Hiero (LR-Hiero henceforth) which uses beam search and runs in O(n2 b) in practice where n is the length of source sentence and b is the size of beam (Huang and Mi, 2010). To simplify target generation, SCFG rules are con2 LR Decoding with Queue Diversity LR-Hiero uses a constrained lexicalized SCFG which we call a GNF grammar: X → hγ, ¯b βi where γ is a string of non-terminal and terminal sym"
D14-1028,P03-1021,0,0.0165187,"age pairs (Table 1): GermanEnglish (De-En), Czech-English (Cs-En) and Chinese-English (Zh-En). 3 In rule type (c) Xn will be in β and Xm will be in γ. For the sake of simplicity, in rule type (b) we can merge Xn and Xr as they are in the same order on both source and target side. We extend the LR-Hiero decoder to handle such cases by making the GNF grammar more expressive. Rules are partitioned to three types based on 4 224 Model Hiero LR-Hiero We use a 5-gram LM trained on the Gigaword corpus and use KenLM (Heafield, 2011). We tune weights by minimizing BLEU loss on the dev set through MERT (Och, 2003) and report BLEU scores on the test set. Pop limit for Hiero and LRHiero+CP is 500 and beam size LR-Hiero is 500. Other extraction and decoder settings such as maximum phrase length, etc. were identical across settings. To make the results comparable we use the same feature set for all baselines, Hiero as well (including new features proposed by (Siahbani et al., 2013)). We use 3 baselines: (i) our implementation of (Watanabe et al., 2006): LR-Hiero with beam search (LR-Hiero) and (ii) LR-Hiero with cube pruning (Siahbani et al., 2013): (LR-Hiero+CP); and (iii) Kriya, an open-source implementa"
D14-1028,N09-2036,0,0.0187009,"tching match X2 to all subspans [12,13], [12,14] and [12,15], corresponding to 3 hypotheses. hypotheses in the queue are popped (line 26) and for each hypothesis its neighbours in the cube are added to the priority queue (line 27). Decoding finishes when stack Sn has been filled. sity at queue level, before generating K best hypothesis, such that the GetBestHypotheses routine generates d best hypotheses from each cube and all these hypotheses are pushed to the priority queue (line 22-23). We fill each stack differently from CKY-Hiero and so queue diversity is different from lazy cube pruning (Pust and Knight, 2009) or cube growing (Huang and Chiang, 2007; Vilar and Ney, 2009; Xu and Koehn, 2012). The language model (LM) score violates the hypotheses generation assumption of CP and can cause search errors. In Figure 1, the topmost and leftmost entry of the right cube has a score worse than many hypotheses in the left cube due to the LM score. This means the right cube has hypotheses that are ignored. This type of search error hurts LR-Hiero more than CKYHiero, due to the fact that hypotheses scores in LR-Hiero rely on a future cost, while CKY-Hiero uses the inside score for each hypothesis. To solve this"
D14-1028,N10-1140,0,0.118049,"rase-based translation (Hiero) (Chiang, 2007) uses a lexicalized synchronous context-free grammar (SCFG) extracted from word and phrase alignments of a bitext. Decoding for Hiero is typically done with CKY-style decoding with time complexity O(n3 ) for source input with n words. Computing the language model score for each hypothesis within CKY decoding requires two histories, the left and the right edge of each span, due to the fact that the target side is built inside-out from sub-spans (Heafield et al., 2011; Heafield et al., 2013). LR-decoding algorithms exist for phrasebased (Koehn, 2004; Galley and Manning, 2010) and syntax-based (Huang and Mi, 2010; Feng et al., 2012) models and also for hierarchical phrasebased models (Watanabe et al., 2006; Siahbani et al., 2013), which is our focus in this paper. Watanabe et al. (2006) first proposed left-toright (LR) decoding for Hiero (LR-Hiero henceforth) which uses beam search and runs in O(n2 b) in practice where n is the length of source sentence and b is the size of beam (Huang and Mi, 2010). To simplify target generation, SCFG rules are con2 LR Decoding with Queue Diversity LR-Hiero uses a constrained lexicalized SCFG which we call a GNF grammar: X → hγ, ¯"
D14-1028,2011.iwslt-evaluation.24,0,0.159108,"decoding that make it comparable in translation quality to CKY-based Hiero. 1 Introduction Hierarchical phrase-based translation (Hiero) (Chiang, 2007) uses a lexicalized synchronous context-free grammar (SCFG) extracted from word and phrase alignments of a bitext. Decoding for Hiero is typically done with CKY-style decoding with time complexity O(n3 ) for source input with n words. Computing the language model score for each hypothesis within CKY decoding requires two histories, the left and the right edge of each span, due to the fact that the target side is built inside-out from sub-spans (Heafield et al., 2011; Heafield et al., 2013). LR-decoding algorithms exist for phrasebased (Koehn, 2004; Galley and Manning, 2010) and syntax-based (Huang and Mi, 2010; Feng et al., 2012) models and also for hierarchical phrasebased models (Watanabe et al., 2006; Siahbani et al., 2013), which is our focus in this paper. Watanabe et al. (2006) first proposed left-toright (LR) decoding for Hiero (LR-Hiero henceforth) which uses beam search and runs in O(n2 b) in practice where n is the length of source sentence and b is the size of beam (Huang and Mi, 2010). To simplify target generation, SCFG rules are con2 LR Dec"
D14-1028,D13-1110,1,0.644201,"Siahbani and Anoop Sarkar School of Computing Science Simon Fraser University Burnaby BC. Canada msiahban,anoop@cs.sfu.ca Abstract strained to be prefix-lexicalized on target side aka Griebach Normal Form (GNF). Throughout this paper we abuse the notation for simplicity and use the term GNF grammars for such SCFGs. This constraint drastically reduces the size of grammar for LR-Hiero in comparison to Hiero grammar (Siahbani et al., 2013). However, the original LR-Hiero decoding algorithm does not perform well in comparison to current state-of-the-art Hiero and phrase-based translation systems. Siahbani et al. (2013) propose an augmented version of LR decoding to address some limitations in the original LR-Hiero algorithm in terms of translation quality and time efficiency. Although, LR-Hiero performs much faster than Hiero in decoding and obtains BLEU scores comparable to phrase-based translation system on some language pairs, there is still a notable gap between CKY-Hiero and LR-Hiero (Siahbani et al., 2013). We show in this paper using instructive examples that CKY-Hiero can capture some complex phrasal re-orderings that are observed in language pairs such as Chinese-English that LR-Hiero cannot (c.f."
D14-1028,N13-1116,0,0.115894,"omparable in translation quality to CKY-based Hiero. 1 Introduction Hierarchical phrase-based translation (Hiero) (Chiang, 2007) uses a lexicalized synchronous context-free grammar (SCFG) extracted from word and phrase alignments of a bitext. Decoding for Hiero is typically done with CKY-style decoding with time complexity O(n3 ) for source input with n words. Computing the language model score for each hypothesis within CKY decoding requires two histories, the left and the right edge of each span, due to the fact that the target side is built inside-out from sub-spans (Heafield et al., 2011; Heafield et al., 2013). LR-decoding algorithms exist for phrasebased (Koehn, 2004; Galley and Manning, 2010) and syntax-based (Huang and Mi, 2010; Feng et al., 2012) models and also for hierarchical phrasebased models (Watanabe et al., 2006; Siahbani et al., 2013), which is our focus in this paper. Watanabe et al. (2006) first proposed left-toright (LR) decoding for Hiero (LR-Hiero henceforth) which uses beam search and runs in O(n2 b) in practice where n is the length of source sentence and b is the size of beam (Huang and Mi, 2010). To simplify target generation, SCFG rules are con2 LR Decoding with Queue Diversi"
D14-1028,2009.eamt-1.33,0,0.014096,"rresponding to 3 hypotheses. hypotheses in the queue are popped (line 26) and for each hypothesis its neighbours in the cube are added to the priority queue (line 27). Decoding finishes when stack Sn has been filled. sity at queue level, before generating K best hypothesis, such that the GetBestHypotheses routine generates d best hypotheses from each cube and all these hypotheses are pushed to the priority queue (line 22-23). We fill each stack differently from CKY-Hiero and so queue diversity is different from lazy cube pruning (Pust and Knight, 2009) or cube growing (Huang and Chiang, 2007; Vilar and Ney, 2009; Xu and Koehn, 2012). The language model (LM) score violates the hypotheses generation assumption of CP and can cause search errors. In Figure 1, the topmost and leftmost entry of the right cube has a score worse than many hypotheses in the left cube due to the LM score. This means the right cube has hypotheses that are ignored. This type of search error hurts LR-Hiero more than CKYHiero, due to the fact that hypotheses scores in LR-Hiero rely on a future cost, while CKY-Hiero uses the inside score for each hypothesis. To solve this issue for LR-Hiero we introduce the notion of queue diversit"
D14-1028,W11-2123,0,0.0924729,"cannot. 4 Experiments We evaluate our modifications to LR-Hiero decoder on three language pairs (Table 1): GermanEnglish (De-En), Czech-English (Cs-En) and Chinese-English (Zh-En). 3 In rule type (c) Xn will be in β and Xm will be in γ. For the sake of simplicity, in rule type (b) we can merge Xn and Xr as they are in the same order on both source and target side. We extend the LR-Hiero decoder to handle such cases by making the GNF grammar more expressive. Rules are partitioned to three types based on 4 224 Model Hiero LR-Hiero We use a 5-gram LM trained on the Gigaword corpus and use KenLM (Heafield, 2011). We tune weights by minimizing BLEU loss on the dev set through MERT (Och, 2003) and report BLEU scores on the test set. Pop limit for Hiero and LRHiero+CP is 500 and beam size LR-Hiero is 500. Other extraction and decoder settings such as maximum phrase length, etc. were identical across settings. To make the results comparable we use the same feature set for all baselines, Hiero as well (including new features proposed by (Siahbani et al., 2013)). We use 3 baselines: (i) our implementation of (Watanabe et al., 2006): LR-Hiero with beam search (LR-Hiero) and (ii) LR-Hiero with cube pruning ("
D14-1028,P06-1098,0,0.194824,"such as Chinese-English that LR-Hiero cannot (c.f. Sec.3). We introduce two improvements to LR decoding of GNF grammars: (1) We add queue diversity to the cube pruning algorithm for LR-Hiero, and (2) We extend the LR-Hiero decoder to capture all the hierarchical phrasal alignments that are reachable in CKY-Hiero (restricted to using GNF grammars). We evaluate our modifications on three language pairs and show that LR-Hiero can reach the translation scores comparable to CKY-Hiero in two language pairs, and reduce the gap between Hiero and LR-Hiero on the third one. Left-to-right (LR) decoding (Watanabe et al., 2006) is promising decoding algorithm for hierarchical phrase-based translation (Hiero) that visits input spans in arbitrary order producing the output translation in left to right order. This leads to far fewer language model calls, but while LR decoding is more efficient than CKY decoding, it is unable to capture some hierarchical phrase alignments reachable using CKY decoding and suffers from lower translation quality as a result. This paper introduces two improvements to LR decoding that make it comparable in translation quality to CKY-based Hiero. 1 Introduction Hierarchical phrase-based trans"
D14-1028,P07-1019,0,0.0200601,"[12,14] and [12,15], corresponding to 3 hypotheses. hypotheses in the queue are popped (line 26) and for each hypothesis its neighbours in the cube are added to the priority queue (line 27). Decoding finishes when stack Sn has been filled. sity at queue level, before generating K best hypothesis, such that the GetBestHypotheses routine generates d best hypotheses from each cube and all these hypotheses are pushed to the priority queue (line 22-23). We fill each stack differently from CKY-Hiero and so queue diversity is different from lazy cube pruning (Pust and Knight, 2009) or cube growing (Huang and Chiang, 2007; Vilar and Ney, 2009; Xu and Koehn, 2012). The language model (LM) score violates the hypotheses generation assumption of CP and can cause search errors. In Figure 1, the topmost and leftmost entry of the right cube has a score worse than many hypotheses in the left cube due to the LM score. This means the right cube has hypotheses that are ignored. This type of search error hurts LR-Hiero more than CKYHiero, due to the fact that hypotheses scores in LR-Hiero rely on a future cost, while CKY-Hiero uses the inside score for each hypothesis. To solve this issue for LR-Hiero we introduce the not"
D14-1028,D10-1027,0,0.41287,") uses a lexicalized synchronous context-free grammar (SCFG) extracted from word and phrase alignments of a bitext. Decoding for Hiero is typically done with CKY-style decoding with time complexity O(n3 ) for source input with n words. Computing the language model score for each hypothesis within CKY decoding requires two histories, the left and the right edge of each span, due to the fact that the target side is built inside-out from sub-spans (Heafield et al., 2011; Heafield et al., 2013). LR-decoding algorithms exist for phrasebased (Koehn, 2004; Galley and Manning, 2010) and syntax-based (Huang and Mi, 2010; Feng et al., 2012) models and also for hierarchical phrasebased models (Watanabe et al., 2006; Siahbani et al., 2013), which is our focus in this paper. Watanabe et al. (2006) first proposed left-toright (LR) decoding for Hiero (LR-Hiero henceforth) which uses beam search and runs in O(n2 b) in practice where n is the length of source sentence and b is the size of beam (Huang and Mi, 2010). To simplify target generation, SCFG rules are con2 LR Decoding with Queue Diversity LR-Hiero uses a constrained lexicalized SCFG which we call a GNF grammar: X → hγ, ¯b βi where γ is a string of non-termi"
D15-1163,N09-1014,0,0.0274817,"ve model to domain adaptation based on canonical correlation analysis Haghighi et al. (2008). However, they use artificially created monolingual corpora very related to the same domain as test data. Irvine and Callison-Burch (2014a) generate a large, noisy phrase table by composing unigram translations which are obtained by a supervised method (Irvine and Callison-Burch, 2013). Comparable monolingual data is used to re-score and filter the phrase table. Zhang and Zong (2013) use a large manually generated lexicon for domain adaptation. In contrast to these methods, our method is unsupervised. Alexandrescu and Kirchhoff (2009) use a graph-based semi-supervised model determine similarities between sentences, then use it to rerank the n-best translation hypothesis. Liu et al. (2012) extend this model to derive some features to be used during decoding. These approaches are orthogonal to our approach. Saluja et al. (2014) use Structured Label Propagation (Liu et al., 2012) in two parallel graphs constructed on source and target paraphrases. In their case the graph construction is extremely expensive. Leveraging a morphological analyzer, they reach significant improvement on Arabic. We can not directly compare our resul"
D15-1163,W14-4001,0,0.0178193,"rase table (Zou et al., 2013; Zhang et al., 2014; Gao et al., 2014). Mikolov et al. (2013a) learn a global linear projection from source to target using representation of frequent words on both sides. This model can be used to generate translations for new words, but a large amounts of bilingual data is required to create such a model. (Mikolov et al., 2013b) also uses bilingual data to project new translation rules. Zhao et al. (2015) extend Mikolov’s model to learn one local linear projection for each phrase. Their model reaches comparable results to Saluja et al. (2014) while works faster. Alkhouli et al. (2014) use neural network phrase representation for paraphrasing OOVs and find translation for them using a phrase-table created from limited parallel data. Our experimental settings is different from the approaches in (Alkhouli et al., 2014; Mikolov et al., 2013a; Mikolov et al., 2013b). 7 Conclusion and Future work In future work, we would like to include translations for infrequent phrases which are not OOVs. We would like to explore new propagation methods that can directly use confidence estimates and control propagation based on label sparsity. We also would like to expand this work for morpho"
D15-1163,D11-1108,0,0.0495082,"Missing"
D15-1163,P14-1066,0,0.0240478,"logical variants of OOVs in test data. We do not use the dev/test data to augment the phrase table. Using comparable corpora to extract parallel sentences and phrases (Munteanu and Marcu, 2006; Smith et al., 2010; Tamura et al., 2012) are orthogonal to the approach we discuss here. Bilingual and multilingual word and phrase representation using neural networks have been applied to machine translation (Zou et al., 2013; Mikolov et al., 2013a; Zhang et al., 2014). However, most of these methods focus on frequent words or an available bilingual phrase table (Zou et al., 2013; Zhang et al., 2014; Gao et al., 2014). Mikolov et al. (2013a) learn a global linear projection from source to target using representation of frequent words on both sides. This model can be used to generate translations for new words, but a large amounts of bilingual data is required to create such a model. (Mikolov et al., 2013b) also uses bilingual data to project new translation rules. Zhao et al. (2015) extend Mikolov’s model to learn one local linear projection for each phrase. Their model reaches comparable results to Saluja et al. (2014) while works faster. Alkhouli et al. (2014) use neural network phrase representation for"
D15-1163,P05-1074,0,0.190597,"s (Marton et al., 2009; Daum´e and Jagarlamudi, 2011; Irvine et al., 2013; Irvine and Callison-Burch, 2014a). Graph-based semisupervised methods extend this approach and propagate translation candidates across a graph with phrasal nodes connected via weighted paraphrase relationships (Razmara et al., 2013; Saluja et al., 2014; Zhao et al., 2015). Saluja et al. (2014) extend paraphrases for SMT from the words to phrases, which we also do in this work. Bilingual pivoting uses parallel data instead of context vectors for paraphrase extraction (Mann and Yarowsky, 2001; Schafer and Yarowsky, 2002; Bannard and Callison-Burch, 2005; CallisonBurch et al., 2006; Zhao et al., 2008; CallisonBurch, 2008). Ganitkevitch and Callison-Burch (2014) published a large-scale multilingual Paraphrase Database (PPDB) http://paraphrase. org which includes lexical, phrasal, and syntactic paraphrases (available for 22 languages with up to 170 million paraphrases each). To our knowledge, this paper is the first comprehensive study of the use of PPDB for statistical machine translation model training. Our framework has three stages: 1) a novel graph construction approach for PPDB paraphrases linked 1379 Proceedings of the 2015 Conference on"
D15-1163,W09-1117,0,0.0480354,"Missing"
D15-1163,2008.iwslt-papers.2,0,0.0172704,"MT system on 685K sentence pairs (randomly selected from LDC2007T08 and LDC2008T09) and use NIST OpenMT 2012 for dev and test data. Arabic side of 1M sentences of LDC2007T08 and LDC2008T09 is used to extract unigram paraphrases for DP. Table 7 shows that PPDB (large; with phrases) resulted in +1.53 BLEU score improvement over DP which only slightly improved over baseline. 6 Related Work Sentence level paraphrasing has been used for generating alternative reference translations (Madnani et al., 2007; Kauchak and Barzilay, 2006), or augmenting the training data with sentential para1386 phrases (Bond et al., 2008; Nakov, 2008; Mirkin et al., 2009). Phrase level paraphrasing was done using crowdsourcing (Resnik et al., 2010) or by using paraphrases in lattice decoding (Onishi et al., 2010; Du et al., 2010). Daum´e and Jagarlamudi (2011) apply a generative model to domain adaptation based on canonical correlation analysis Haghighi et al. (2008). However, they use artificially created monolingual corpora very related to the same domain as test data. Irvine and Callison-Burch (2014a) generate a large, noisy phrase table by composing unigram translations which are obtained by a supervised method (Irvine an"
D15-1163,P08-2015,0,0.0277183,"n two parallel graphs constructed on source and target paraphrases. In their case the graph construction is extremely expensive. Leveraging a morphological analyzer, they reach significant improvement on Arabic. We can not directly compare our results to (Saluja et al., 2014) because they exploit several external resources such as a morphological analyzer and also had different sizes of training and test. In experiments (Sec. 5) we obtained comparable BLEU score improvement on Arabic-English by using bilingual pivoting only on source phrases. (Saluja et al., 2014) also use methods similar to (Habash, 2008) that expand the phrase table with spelling and morphological variants of OOVs in test data. We do not use the dev/test data to augment the phrase table. Using comparable corpora to extract parallel sentences and phrases (Munteanu and Marcu, 2006; Smith et al., 2010; Tamura et al., 2012) are orthogonal to the approach we discuss here. Bilingual and multilingual word and phrase representation using neural networks have been applied to machine translation (Zou et al., 2013; Mikolov et al., 2013a; Zhang et al., 2014). However, most of these methods focus on frequent words or an available bilingua"
D15-1163,N06-1003,0,0.020249,"Missing"
D15-1163,P08-1088,0,0.0253371,"which only slightly improved over baseline. 6 Related Work Sentence level paraphrasing has been used for generating alternative reference translations (Madnani et al., 2007; Kauchak and Barzilay, 2006), or augmenting the training data with sentential para1386 phrases (Bond et al., 2008; Nakov, 2008; Mirkin et al., 2009). Phrase level paraphrasing was done using crowdsourcing (Resnik et al., 2010) or by using paraphrases in lattice decoding (Onishi et al., 2010; Du et al., 2010). Daum´e and Jagarlamudi (2011) apply a generative model to domain adaptation based on canonical correlation analysis Haghighi et al. (2008). However, they use artificially created monolingual corpora very related to the same domain as test data. Irvine and Callison-Burch (2014a) generate a large, noisy phrase table by composing unigram translations which are obtained by a supervised method (Irvine and Callison-Burch, 2013). Comparable monolingual data is used to re-score and filter the phrase table. Zhang and Zong (2013) use a large manually generated lexicon for domain adaptation. In contrast to these methods, our method is unsupervised. Alexandrescu and Kirchhoff (2009) use a graph-based semi-supervised model determine similari"
D15-1163,D08-1021,0,0.0981528,"Missing"
D15-1163,W11-2123,0,0.0525525,"et al., 2007) as SMT pipeline, GIZA++ (Och and Ney, 2003) for word alignment and MERT (Och, 2003) for tuning. We add our own feature to the SMT log-linear model as described in Sec. 3.3. 1 http://www.cdec-decoder.org EgivenFCoherent, SampleCountF, CountEF, MaxLexFgivenE, MaxLexEgivenF, IsSingletonF, IsSingletonEF 1384 2 Experiments Case 1 Case 2 - Med. Case 2 - Sci. Case 3 OOV type/token 1830 / 2163 2294 / 4190 5272 / 14121 1543 / 1895 Rules added 7.0K 7.8K 10.4K 8.1K Table 2: Statistics of settings in Sec. 5. Last column shows how many rules added in the phrase table integration step. KenLM (Heafield, 2011) is used to train a 5gram language model on English Gigaword (V5: LDC2011T07). For scalable graph propagation we use the Junto framework3 . We use maximum phrase length 10. For our experiments we use the Hadoop distributed computing framework executed on a cluster with 12 nodes (each node has 8 cores and 16GB of RAM). Each graph propagation iteration takes about 3 minutes. For French, we apply a simple heuristic to detect named entities: words that are capitalized in the original dev/test set that do not appear at the beginning of a sentence are named entities. Based on eyeballing the results,"
D15-1163,P11-2031,0,0.02307,"nd test sentences exist inside the training data and all the OOVs are known (Fully observers cannot avoid model and search errors). 5.3 Fr-En Baseline + Lexical OOV + Phrasal OOV Fully observed i=1 Recall = |{gold list} ∩ {candidate list}| |{gold list}| (6) Table 5 compares DP and PPDB in terms of BLEU, MRR and Recall. It indicates that PPDB (large size) outperforms DP in both intrinsic and extrinsic evaluation measures. Although tripartite graph did not improve the results for DP, it results in statistically significantly better BLEU score for PPDB in comparison to DP (evaluated by MultEval (Clark et al., 2011)). Thus we use tripartite graph in the rest of experiments. The last row in the table shows the result of combining DP and PPDB by multiplying the normalized scores of both paraphrase lists. This setting is included for three reasons: 1) we exploit the small data size to explore different choices in our approach such as, e.g. choosing bipartite versus tripartite graph structures; 2) 4 Junto : https://github.com/parthatalukdar/junto 1385 http://www.statmt.org/wpt05/mt-shared-task/ OOV proc´ed´es PPDB NNs processus quantique quantiques DP NNs m´ethodes outils mat´eriaux - mlzm ADTr mlzmA Referen"
D15-1163,P11-2071,0,0.0493518,"Missing"
D15-1163,D10-1041,0,0.0161381,"to extract unigram paraphrases for DP. Table 7 shows that PPDB (large; with phrases) resulted in +1.53 BLEU score improvement over DP which only slightly improved over baseline. 6 Related Work Sentence level paraphrasing has been used for generating alternative reference translations (Madnani et al., 2007; Kauchak and Barzilay, 2006), or augmenting the training data with sentential para1386 phrases (Bond et al., 2008; Nakov, 2008; Mirkin et al., 2009). Phrase level paraphrasing was done using crowdsourcing (Resnik et al., 2010) or by using paraphrases in lattice decoding (Onishi et al., 2010; Du et al., 2010). Daum´e and Jagarlamudi (2011) apply a generative model to domain adaptation based on canonical correlation analysis Haghighi et al. (2008). However, they use artificially created monolingual corpora very related to the same domain as test data. Irvine and Callison-Burch (2014a) generate a large, noisy phrase table by composing unigram translations which are obtained by a supervised method (Irvine and Callison-Burch, 2013). Comparable monolingual data is used to re-score and filter the phrase table. Zhang and Zong (2013) use a large manually generated lexicon for domain adaptation. In contras"
D15-1163,N13-1056,0,0.0142925,"al., 2008; Nakov, 2008; Mirkin et al., 2009). Phrase level paraphrasing was done using crowdsourcing (Resnik et al., 2010) or by using paraphrases in lattice decoding (Onishi et al., 2010; Du et al., 2010). Daum´e and Jagarlamudi (2011) apply a generative model to domain adaptation based on canonical correlation analysis Haghighi et al. (2008). However, they use artificially created monolingual corpora very related to the same domain as test data. Irvine and Callison-Burch (2014a) generate a large, noisy phrase table by composing unigram translations which are obtained by a supervised method (Irvine and Callison-Burch, 2013). Comparable monolingual data is used to re-score and filter the phrase table. Zhang and Zong (2013) use a large manually generated lexicon for domain adaptation. In contrast to these methods, our method is unsupervised. Alexandrescu and Kirchhoff (2009) use a graph-based semi-supervised model determine similarities between sentences, then use it to rerank the n-best translation hypothesis. Liu et al. (2012) extend this model to derive some features to be used during decoding. These approaches are orthogonal to our approach. Saluja et al. (2014) use Structured Label Propagation (Liu et al., 20"
D15-1163,W14-1617,0,0.0211747,"eference translations (Madnani et al., 2007; Kauchak and Barzilay, 2006), or augmenting the training data with sentential para1386 phrases (Bond et al., 2008; Nakov, 2008; Mirkin et al., 2009). Phrase level paraphrasing was done using crowdsourcing (Resnik et al., 2010) or by using paraphrases in lattice decoding (Onishi et al., 2010; Du et al., 2010). Daum´e and Jagarlamudi (2011) apply a generative model to domain adaptation based on canonical correlation analysis Haghighi et al. (2008). However, they use artificially created monolingual corpora very related to the same domain as test data. Irvine and Callison-Burch (2014a) generate a large, noisy phrase table by composing unigram translations which are obtained by a supervised method (Irvine and Callison-Burch, 2013). Comparable monolingual data is used to re-score and filter the phrase table. Zhang and Zong (2013) use a large manually generated lexicon for domain adaptation. In contrast to these methods, our method is unsupervised. Alexandrescu and Kirchhoff (2009) use a graph-based semi-supervised model determine similarities between sentences, then use it to rerank the n-best translation hypothesis. Liu et al. (2012) extend this model to derive some featur"
D15-1163,W14-3357,0,0.0120225,"eference translations (Madnani et al., 2007; Kauchak and Barzilay, 2006), or augmenting the training data with sentential para1386 phrases (Bond et al., 2008; Nakov, 2008; Mirkin et al., 2009). Phrase level paraphrasing was done using crowdsourcing (Resnik et al., 2010) or by using paraphrases in lattice decoding (Onishi et al., 2010; Du et al., 2010). Daum´e and Jagarlamudi (2011) apply a generative model to domain adaptation based on canonical correlation analysis Haghighi et al. (2008). However, they use artificially created monolingual corpora very related to the same domain as test data. Irvine and Callison-Burch (2014a) generate a large, noisy phrase table by composing unigram translations which are obtained by a supervised method (Irvine and Callison-Burch, 2013). Comparable monolingual data is used to re-score and filter the phrase table. Zhang and Zong (2013) use a large manually generated lexicon for domain adaptation. In contrast to these methods, our method is unsupervised. Alexandrescu and Kirchhoff (2009) use a graph-based semi-supervised model determine similarities between sentences, then use it to rerank the n-best translation hypothesis. Liu et al. (2012) extend this model to derive some featur"
D15-1163,D13-1109,0,0.165743,"graph structure. They pre-structure the graph into bipartite graphs (only connections between phrases with known translation and OOV phrases) and tripartite graphs (connections can also go from a known phrasal node to an OOV phrasal node through one node that is a paraphrase of both but does not have translations, i.e. it is an unlabeled node). In these pre-structured graphs there are no connections between nodes of the same type (known, OOV or unlabeled). We apply this method in our low resource setting experiments (Sec. 5.3) to compare our bipartite and tripartite results to Razmara et al. (2013). In the rest of the experiments we use the tripartite approach since it outperforms the bipartite approach. 4.2.2 Graph random walks Our goal is to limit the number of hops in the propagation of translation candidates preferring closely connected and highly probable edge weights. Optimization for the Modified Adsorption (MAD) objective function in Sec. 3.2 can be viewed as a controlled random walk (Talukdar et al., 2008; Talukdar and Crammer, 2009). This is formalized as three actions: inject, continue and abandon with corresponding pre-defined probabilities Pinj , Pcont and Pabnd respectivel"
D15-1163,N06-1058,0,0.0654842,"due to pivoting over English. We choose Arabic-English task for this experiment. We train the SMT system on 685K sentence pairs (randomly selected from LDC2007T08 and LDC2008T09) and use NIST OpenMT 2012 for dev and test data. Arabic side of 1M sentences of LDC2007T08 and LDC2008T09 is used to extract unigram paraphrases for DP. Table 7 shows that PPDB (large; with phrases) resulted in +1.53 BLEU score improvement over DP which only slightly improved over baseline. 6 Related Work Sentence level paraphrasing has been used for generating alternative reference translations (Madnani et al., 2007; Kauchak and Barzilay, 2006), or augmenting the training data with sentential para1386 phrases (Bond et al., 2008; Nakov, 2008; Mirkin et al., 2009). Phrase level paraphrasing was done using crowdsourcing (Resnik et al., 2010) or by using paraphrases in lattice decoding (Onishi et al., 2010; Du et al., 2010). Daum´e and Jagarlamudi (2011) apply a generative model to domain adaptation based on canonical correlation analysis Haghighi et al. (2008). However, they use artificially created monolingual corpora very related to the same domain as test data. Irvine and Callison-Burch (2014a) generate a large, noisy phrase table b"
D15-1163,W02-0902,0,0.120296,"Missing"
D15-1163,N03-1017,0,0.0163188,"Missing"
D15-1163,P10-4002,0,0.0118955,"We varied the number of iterations from 1 to 10 on a held-out dev set and found that 5 iterations was optimal. 5 Evaluation We first show the effect of OOVs on translation quality, then evaluate our approach in three different SMT settings: low resource SMT, domain shift, and morphologically complex languages. In each case, we compare results of using paraphrases extracted by Distributional Profile (DP) and PPDB in an end-to-end SMT system. Important: no subset of the test data sentences are used in the bilingual corpora for paraphrase extraction process. 5.1 Experimental Setup We use CDEC1 (Dyer et al., 2010) as an endto-end SMT pipeline with its standard features2 . fast align (Dyer et al., 2013) is used for word alignment, and weights are tuned by minimizing BLEU loss on the dev set using MIRA (Crammer and Singer, 2003). This setup is used for most of our experiments: oracle (Sec. 5.2), domain adaptation (Sec. 5.4) and morphologically complex languages (Sec. 5.5). But as we wish to fairly compare our approach with Razmara et al. (2013) on low resource setting, we follow their setup in Sec. 5.3: Moses (Koehn et al., 2007) as SMT pipeline, GIZA++ (Och and Ney, 2003) for word alignment and MERT (Oc"
D15-1163,P07-2045,0,0.00575877,"pora for paraphrase extraction process. 5.1 Experimental Setup We use CDEC1 (Dyer et al., 2010) as an endto-end SMT pipeline with its standard features2 . fast align (Dyer et al., 2013) is used for word alignment, and weights are tuned by minimizing BLEU loss on the dev set using MIRA (Crammer and Singer, 2003). This setup is used for most of our experiments: oracle (Sec. 5.2), domain adaptation (Sec. 5.4) and morphologically complex languages (Sec. 5.5). But as we wish to fairly compare our approach with Razmara et al. (2013) on low resource setting, we follow their setup in Sec. 5.3: Moses (Koehn et al., 2007) as SMT pipeline, GIZA++ (Och and Ney, 2003) for word alignment and MERT (Och, 2003) for tuning. We add our own feature to the SMT log-linear model as described in Sec. 3.3. 1 http://www.cdec-decoder.org EgivenFCoherent, SampleCountF, CountEF, MaxLexFgivenE, MaxLexEgivenF, IsSingletonF, IsSingletonEF 1384 2 Experiments Case 1 Case 2 - Med. Case 2 - Sci. Case 3 OOV type/token 1830 / 2163 2294 / 4190 5272 / 14121 1543 / 1895 Rules added 7.0K 7.8K 10.4K 8.1K Table 2: Statistics of settings in Sec. 5. Last column shows how many rules added in the phrase table integration step. KenLM (Heafield, 201"
D15-1163,2005.mtsummit-papers.11,0,0.0673822,"for OOVs are given by concatenating the test data to training and running a word aligner. |O| Case 1: Limited Parallel Data In this experiment we use a setup similar to (Razmara et al., 2013). To have fair comparison, 3 Dev 27.90 28.10 28.50 46.88 Table 4: The impact of translating OOVs. 1 X 1 MRR = for O = {OOVs} (5) |O| ranki Impact of OOVs: Oracle experiment This oracle experiment shows that translation of OOVs beyond named entities, dates, etc. is potentially very useful in improving output translation. We trained a SMT system on 10K French-English sentences from the Europarl corpus(v7) (Koehn, 2005). WMT 2011 and WMT 2012 are used as dev and test data respectively. Table 4 shows the results in terms of BLEU on dev and test. The first row is baseline which simply copies OOVs to output. The second and third rows show the result of augmenting phrase-table by adding translations for single-word OOVs and phrases containing OOVs. The last row shows the oracle result where dev and test sentences exist inside the training data and all the OOVs are known (Fully observers cannot avoid model and search errors). 5.3 Fr-En Baseline + Lexical OOV + Phrasal OOV Fully observed i=1 Recall = |{gold list}"
D15-1163,ganitkevitch-callison-burch-2014-multilingual,0,0.0857059,"gation to transfer translation candidates in a way that is sensitive to SMT concerns. In our experiments (Sec. 5) we compare our approach with the state-of-the-art in three different settings in SMT: 1) when faced with limited amount of parallel training data; 2) a domain shift between training and test data; and 3) handling a morphologically complex source language. In each case, we show that our PPDB-based approach outperforms the distributional profile approach. 2 Paraphrase Extraction Our goal is to produce translations for OOV phrases by exploiting paraphrases from the multilingual PPDB (Ganitkevitch and Callison-Burch, 2014) by using graph propagation. Since our approach relies on phrase-level paraphrases we compare with the current state of the art approaches that use monolingual data and distributional profiles to construct paraphrases and use graph propagation (Razmara et al., 2013; Saluja et al., 2014). 2.1 Paraphrases from Distributional Profiles A distributional profile (DP) of a word or phrase was first proposed in (Rapp, 1995) for SMT. Given a word f , its distributional profile is: DP (f ) = {hA(f, wi )i |wi ∈ V } V is the vocabulary and the surrounding words wi are taken from a monolingual corpus using"
D15-1163,P98-2127,0,0.408636,"l profile is: DP (f ) = {hA(f, wi )i |wi ∈ V } V is the vocabulary and the surrounding words wi are taken from a monolingual corpus using a fixed window size. We use a window size of 4 words based on the experiments in (Razmara et al., 2013). DPs need an association measure A(·, ·) to compute distances between potential paraphrases. A comparison of different association measures appears in (Marton et al., 2009; Razmara et al., 2013; Saluja et al., 2014) and our preliminary experiments validated the choice of the same association measure as in these papers, namely Pointwise Mutual Information (Lin, 1998) (PMI). For each potential context word wi : A(f, wi ) = log2 P (f, wi ) P (f )P (wi ) (1) To evaluate the similarity between two phrases we use cosine similarity. The cosine coefficient of two phrases f1 and f2 is: S(f1 , f2 ) = cos(DP (f1 ), DP (f2 )) = P wi ∈V A(f1 , wi )A(f2 , wi ) qP qP 2 2 A(f , w ) 1 i wi ∈V wi ∈V A(f2 , wi ) (2) where V is the vocabulary. Note that in Eqn. (2) wi ’s are the words that appear in the context of f1 or f2 , otherwise the PMI values would be zero. Considering all possible candidate paraphrases is very expensive. Thus, we use the heuristic applied in previou"
D15-1163,P12-1032,0,0.0176709,"as test data. Irvine and Callison-Burch (2014a) generate a large, noisy phrase table by composing unigram translations which are obtained by a supervised method (Irvine and Callison-Burch, 2013). Comparable monolingual data is used to re-score and filter the phrase table. Zhang and Zong (2013) use a large manually generated lexicon for domain adaptation. In contrast to these methods, our method is unsupervised. Alexandrescu and Kirchhoff (2009) use a graph-based semi-supervised model determine similarities between sentences, then use it to rerank the n-best translation hypothesis. Liu et al. (2012) extend this model to derive some features to be used during decoding. These approaches are orthogonal to our approach. Saluja et al. (2014) use Structured Label Propagation (Liu et al., 2012) in two parallel graphs constructed on source and target paraphrases. In their case the graph construction is extremely expensive. Leveraging a morphological analyzer, they reach significant improvement on Arabic. We can not directly compare our results to (Saluja et al., 2014) because they exploit several external resources such as a morphological analyzer and also had different sizes of training and tes"
D15-1163,W07-0716,0,0.02395,"Even more so in PPDB due to pivoting over English. We choose Arabic-English task for this experiment. We train the SMT system on 685K sentence pairs (randomly selected from LDC2007T08 and LDC2008T09) and use NIST OpenMT 2012 for dev and test data. Arabic side of 1M sentences of LDC2007T08 and LDC2008T09 is used to extract unigram paraphrases for DP. Table 7 shows that PPDB (large; with phrases) resulted in +1.53 BLEU score improvement over DP which only slightly improved over baseline. 6 Related Work Sentence level paraphrasing has been used for generating alternative reference translations (Madnani et al., 2007; Kauchak and Barzilay, 2006), or augmenting the training data with sentential para1386 phrases (Bond et al., 2008; Nakov, 2008; Mirkin et al., 2009). Phrase level paraphrasing was done using crowdsourcing (Resnik et al., 2010) or by using paraphrases in lattice decoding (Onishi et al., 2010; Du et al., 2010). Daum´e and Jagarlamudi (2011) apply a generative model to domain adaptation based on canonical correlation analysis Haghighi et al. (2008). However, they use artificially created monolingual corpora very related to the same domain as test data. Irvine and Callison-Burch (2014a) generate"
D15-1163,N01-1020,0,0.139703,"Missing"
D15-1163,D09-1040,0,0.0939786,"that handling such OOVs correctly does improve translation scores. In this paper, we build on the following research: Bilingual lexicon induction is the task of learning translations of words from monolingual data in source and target languages (Schafer and Yarowsky, 2002; Koehn and Knight, 2002; Haghighi et al., 2008). The distributional profile (DP) approach uses context vectors to link words as potential paraphrases to translation candidates (Rapp, 1995; Koehn and Knight, 2002; Haghighi et al., 2008; Garera et al., 2009). DPs have been used in SMT to assign translation candidates to OOVs (Marton et al., 2009; Daum´e and Jagarlamudi, 2011; Irvine et al., 2013; Irvine and Callison-Burch, 2014a). Graph-based semisupervised methods extend this approach and propagate translation candidates across a graph with phrasal nodes connected via weighted paraphrase relationships (Razmara et al., 2013; Saluja et al., 2014; Zhao et al., 2015). Saluja et al. (2014) extend paraphrases for SMT from the words to phrases, which we also do in this work. Bilingual pivoting uses parallel data instead of context vectors for paraphrase extraction (Mann and Yarowsky, 2001; Schafer and Yarowsky, 2002; Bannard and Callison-B"
D15-1163,P09-1089,0,0.0184808,"on based on canonical correlation analysis Haghighi et al. (2008). However, they use artificially created monolingual corpora very related to the same domain as test data. Irvine and Callison-Burch (2014a) generate a large, noisy phrase table by composing unigram translations which are obtained by a supervised method (Irvine and Callison-Burch, 2013). Comparable monolingual data is used to re-score and filter the phrase table. Zhang and Zong (2013) use a large manually generated lexicon for domain adaptation. In contrast to these methods, our method is unsupervised. Alexandrescu and Kirchhoff (2009) use a graph-based semi-supervised model determine similarities between sentences, then use it to rerank the n-best translation hypothesis. Liu et al. (2012) extend this model to derive some features to be used during decoding. These approaches are orthogonal to our approach. Saluja et al. (2014) use Structured Label Propagation (Liu et al., 2012) in two parallel graphs constructed on source and target paraphrases. In their case the graph construction is extremely expensive. Leveraging a morphological analyzer, they reach significant improvement on Arabic. We can not directly compare our resul"
D15-1163,E12-1017,0,0.0350509,"Missing"
D15-1163,P06-1011,0,0.0356607,"pare our results to (Saluja et al., 2014) because they exploit several external resources such as a morphological analyzer and also had different sizes of training and test. In experiments (Sec. 5) we obtained comparable BLEU score improvement on Arabic-English by using bilingual pivoting only on source phrases. (Saluja et al., 2014) also use methods similar to (Habash, 2008) that expand the phrase table with spelling and morphological variants of OOVs in test data. We do not use the dev/test data to augment the phrase table. Using comparable corpora to extract parallel sentences and phrases (Munteanu and Marcu, 2006; Smith et al., 2010; Tamura et al., 2012) are orthogonal to the approach we discuss here. Bilingual and multilingual word and phrase representation using neural networks have been applied to machine translation (Zou et al., 2013; Mikolov et al., 2013a; Zhang et al., 2014). However, most of these methods focus on frequent words or an available bilingual phrase table (Zou et al., 2013; Zhang et al., 2014; Gao et al., 2014). Mikolov et al. (2013a) learn a global linear projection from source to target using representation of frequent words on both sides. This model can be used to generate transl"
D15-1163,J03-1002,0,0.00452835,"xperimental Setup We use CDEC1 (Dyer et al., 2010) as an endto-end SMT pipeline with its standard features2 . fast align (Dyer et al., 2013) is used for word alignment, and weights are tuned by minimizing BLEU loss on the dev set using MIRA (Crammer and Singer, 2003). This setup is used for most of our experiments: oracle (Sec. 5.2), domain adaptation (Sec. 5.4) and morphologically complex languages (Sec. 5.5). But as we wish to fairly compare our approach with Razmara et al. (2013) on low resource setting, we follow their setup in Sec. 5.3: Moses (Koehn et al., 2007) as SMT pipeline, GIZA++ (Och and Ney, 2003) for word alignment and MERT (Och, 2003) for tuning. We add our own feature to the SMT log-linear model as described in Sec. 3.3. 1 http://www.cdec-decoder.org EgivenFCoherent, SampleCountF, CountEF, MaxLexFgivenE, MaxLexEgivenF, IsSingletonF, IsSingletonEF 1384 2 Experiments Case 1 Case 2 - Med. Case 2 - Sci. Case 3 OOV type/token 1830 / 2163 2294 / 4190 5272 / 14121 1543 / 1895 Rules added 7.0K 7.8K 10.4K 8.1K Table 2: Statistics of settings in Sec. 5. Last column shows how many rules added in the phrase table integration step. KenLM (Heafield, 2011) is used to train a 5gram language model o"
D15-1163,P03-1021,0,0.00706647,"0) as an endto-end SMT pipeline with its standard features2 . fast align (Dyer et al., 2013) is used for word alignment, and weights are tuned by minimizing BLEU loss on the dev set using MIRA (Crammer and Singer, 2003). This setup is used for most of our experiments: oracle (Sec. 5.2), domain adaptation (Sec. 5.4) and morphologically complex languages (Sec. 5.5). But as we wish to fairly compare our approach with Razmara et al. (2013) on low resource setting, we follow their setup in Sec. 5.3: Moses (Koehn et al., 2007) as SMT pipeline, GIZA++ (Och and Ney, 2003) for word alignment and MERT (Och, 2003) for tuning. We add our own feature to the SMT log-linear model as described in Sec. 3.3. 1 http://www.cdec-decoder.org EgivenFCoherent, SampleCountF, CountEF, MaxLexFgivenE, MaxLexEgivenF, IsSingletonF, IsSingletonEF 1384 2 Experiments Case 1 Case 2 - Med. Case 2 - Sci. Case 3 OOV type/token 1830 / 2163 2294 / 4190 5272 / 14121 1543 / 1895 Rules added 7.0K 7.8K 10.4K 8.1K Table 2: Statistics of settings in Sec. 5. Last column shows how many rules added in the phrase table integration step. KenLM (Heafield, 2011) is used to train a 5gram language model on English Gigaword (V5: LDC2011T07). For"
D15-1163,P10-2001,0,0.0179567,"d LDC2008T09 is used to extract unigram paraphrases for DP. Table 7 shows that PPDB (large; with phrases) resulted in +1.53 BLEU score improvement over DP which only slightly improved over baseline. 6 Related Work Sentence level paraphrasing has been used for generating alternative reference translations (Madnani et al., 2007; Kauchak and Barzilay, 2006), or augmenting the training data with sentential para1386 phrases (Bond et al., 2008; Nakov, 2008; Mirkin et al., 2009). Phrase level paraphrasing was done using crowdsourcing (Resnik et al., 2010) or by using paraphrases in lattice decoding (Onishi et al., 2010; Du et al., 2010). Daum´e and Jagarlamudi (2011) apply a generative model to domain adaptation based on canonical correlation analysis Haghighi et al. (2008). However, they use artificially created monolingual corpora very related to the same domain as test data. Irvine and Callison-Burch (2014a) generate a large, noisy phrase table by composing unigram translations which are obtained by a supervised method (Irvine and Callison-Burch, 2013). Comparable monolingual data is used to re-score and filter the phrase table. Zhang and Zong (2013) use a large manually generated lexicon for domain adap"
D15-1163,P95-1050,0,0.328645,"nd remove the named entities, dates, etc. in the source and focus on the use of paraphrases to help translate the remaining OOVs. In Sec. 5.2 we show that handling such OOVs correctly does improve translation scores. In this paper, we build on the following research: Bilingual lexicon induction is the task of learning translations of words from monolingual data in source and target languages (Schafer and Yarowsky, 2002; Koehn and Knight, 2002; Haghighi et al., 2008). The distributional profile (DP) approach uses context vectors to link words as potential paraphrases to translation candidates (Rapp, 1995; Koehn and Knight, 2002; Haghighi et al., 2008; Garera et al., 2009). DPs have been used in SMT to assign translation candidates to OOVs (Marton et al., 2009; Daum´e and Jagarlamudi, 2011; Irvine et al., 2013; Irvine and Callison-Burch, 2014a). Graph-based semisupervised methods extend this approach and propagate translation candidates across a graph with phrasal nodes connected via weighted paraphrase relationships (Razmara et al., 2013; Saluja et al., 2014; Zhao et al., 2015). Saluja et al. (2014) extend paraphrases for SMT from the words to phrases, which we also do in this work. Bilingual"
D15-1163,P13-1109,1,0.131791,"d Knight, 2002; Haghighi et al., 2008). The distributional profile (DP) approach uses context vectors to link words as potential paraphrases to translation candidates (Rapp, 1995; Koehn and Knight, 2002; Haghighi et al., 2008; Garera et al., 2009). DPs have been used in SMT to assign translation candidates to OOVs (Marton et al., 2009; Daum´e and Jagarlamudi, 2011; Irvine et al., 2013; Irvine and Callison-Burch, 2014a). Graph-based semisupervised methods extend this approach and propagate translation candidates across a graph with phrasal nodes connected via weighted paraphrase relationships (Razmara et al., 2013; Saluja et al., 2014; Zhao et al., 2015). Saluja et al. (2014) extend paraphrases for SMT from the words to phrases, which we also do in this work. Bilingual pivoting uses parallel data instead of context vectors for paraphrase extraction (Mann and Yarowsky, 2001; Schafer and Yarowsky, 2002; Bannard and Callison-Burch, 2005; CallisonBurch et al., 2006; Zhao et al., 2008; CallisonBurch, 2008). Ganitkevitch and Callison-Burch (2014) published a large-scale multilingual Paraphrase Database (PPDB) http://paraphrase. org which includes lexical, phrasal, and syntactic paraphrases (available for 22"
D15-1163,D10-1013,0,0.0438155,"Missing"
D15-1163,P14-1064,0,0.250513,"hi et al., 2008). The distributional profile (DP) approach uses context vectors to link words as potential paraphrases to translation candidates (Rapp, 1995; Koehn and Knight, 2002; Haghighi et al., 2008; Garera et al., 2009). DPs have been used in SMT to assign translation candidates to OOVs (Marton et al., 2009; Daum´e and Jagarlamudi, 2011; Irvine et al., 2013; Irvine and Callison-Burch, 2014a). Graph-based semisupervised methods extend this approach and propagate translation candidates across a graph with phrasal nodes connected via weighted paraphrase relationships (Razmara et al., 2013; Saluja et al., 2014; Zhao et al., 2015). Saluja et al. (2014) extend paraphrases for SMT from the words to phrases, which we also do in this work. Bilingual pivoting uses parallel data instead of context vectors for paraphrase extraction (Mann and Yarowsky, 2001; Schafer and Yarowsky, 2002; Bannard and Callison-Burch, 2005; CallisonBurch et al., 2006; Zhao et al., 2008; CallisonBurch, 2008). Ganitkevitch and Callison-Burch (2014) published a large-scale multilingual Paraphrase Database (PPDB) http://paraphrase. org which includes lexical, phrasal, and syntactic paraphrases (available for 22 languages with up to"
D15-1163,W02-2026,0,0.0942043,"Missing"
D15-1163,N10-1063,0,0.0250646,"a et al., 2014) because they exploit several external resources such as a morphological analyzer and also had different sizes of training and test. In experiments (Sec. 5) we obtained comparable BLEU score improvement on Arabic-English by using bilingual pivoting only on source phrases. (Saluja et al., 2014) also use methods similar to (Habash, 2008) that expand the phrase table with spelling and morphological variants of OOVs in test data. We do not use the dev/test data to augment the phrase table. Using comparable corpora to extract parallel sentences and phrases (Munteanu and Marcu, 2006; Smith et al., 2010; Tamura et al., 2012) are orthogonal to the approach we discuss here. Bilingual and multilingual word and phrase representation using neural networks have been applied to machine translation (Zou et al., 2013; Mikolov et al., 2013a; Zhang et al., 2014). However, most of these methods focus on frequent words or an available bilingual phrase table (Zou et al., 2013; Zhang et al., 2014; Gao et al., 2014). Mikolov et al. (2013a) learn a global linear projection from source to target using representation of frequent words on both sides. This model can be used to generate translations for new words"
D15-1163,D08-1061,0,0.0905751,"Missing"
D15-1163,D12-1003,0,0.0210162,"use they exploit several external resources such as a morphological analyzer and also had different sizes of training and test. In experiments (Sec. 5) we obtained comparable BLEU score improvement on Arabic-English by using bilingual pivoting only on source phrases. (Saluja et al., 2014) also use methods similar to (Habash, 2008) that expand the phrase table with spelling and morphological variants of OOVs in test data. We do not use the dev/test data to augment the phrase table. Using comparable corpora to extract parallel sentences and phrases (Munteanu and Marcu, 2006; Smith et al., 2010; Tamura et al., 2012) are orthogonal to the approach we discuss here. Bilingual and multilingual word and phrase representation using neural networks have been applied to machine translation (Zou et al., 2013; Mikolov et al., 2013a; Zhang et al., 2014). However, most of these methods focus on frequent words or an available bilingual phrase table (Zou et al., 2013; Zhang et al., 2014; Gao et al., 2014). Mikolov et al. (2013a) learn a global linear projection from source to target using representation of frequent words on both sides. This model can be used to generate translations for new words, but a large amounts"
D15-1163,P13-1140,0,0.0155909,"al., 2010) or by using paraphrases in lattice decoding (Onishi et al., 2010; Du et al., 2010). Daum´e and Jagarlamudi (2011) apply a generative model to domain adaptation based on canonical correlation analysis Haghighi et al. (2008). However, they use artificially created monolingual corpora very related to the same domain as test data. Irvine and Callison-Burch (2014a) generate a large, noisy phrase table by composing unigram translations which are obtained by a supervised method (Irvine and Callison-Burch, 2013). Comparable monolingual data is used to re-score and filter the phrase table. Zhang and Zong (2013) use a large manually generated lexicon for domain adaptation. In contrast to these methods, our method is unsupervised. Alexandrescu and Kirchhoff (2009) use a graph-based semi-supervised model determine similarities between sentences, then use it to rerank the n-best translation hypothesis. Liu et al. (2012) extend this model to derive some features to be used during decoding. These approaches are orthogonal to our approach. Saluja et al. (2014) use Structured Label Propagation (Liu et al., 2012) in two parallel graphs constructed on source and target paraphrases. In their case the graph con"
D15-1163,P14-1011,0,0.0303965,"al pivoting only on source phrases. (Saluja et al., 2014) also use methods similar to (Habash, 2008) that expand the phrase table with spelling and morphological variants of OOVs in test data. We do not use the dev/test data to augment the phrase table. Using comparable corpora to extract parallel sentences and phrases (Munteanu and Marcu, 2006; Smith et al., 2010; Tamura et al., 2012) are orthogonal to the approach we discuss here. Bilingual and multilingual word and phrase representation using neural networks have been applied to machine translation (Zou et al., 2013; Mikolov et al., 2013a; Zhang et al., 2014). However, most of these methods focus on frequent words or an available bilingual phrase table (Zou et al., 2013; Zhang et al., 2014; Gao et al., 2014). Mikolov et al. (2013a) learn a global linear projection from source to target using representation of frequent words on both sides. This model can be used to generate translations for new words, but a large amounts of bilingual data is required to create such a model. (Mikolov et al., 2013b) also uses bilingual data to project new translation rules. Zhao et al. (2015) extend Mikolov’s model to learn one local linear projection for each phrase"
D15-1163,P08-1089,0,0.168755,"t al., 2013; Irvine and Callison-Burch, 2014a). Graph-based semisupervised methods extend this approach and propagate translation candidates across a graph with phrasal nodes connected via weighted paraphrase relationships (Razmara et al., 2013; Saluja et al., 2014; Zhao et al., 2015). Saluja et al. (2014) extend paraphrases for SMT from the words to phrases, which we also do in this work. Bilingual pivoting uses parallel data instead of context vectors for paraphrase extraction (Mann and Yarowsky, 2001; Schafer and Yarowsky, 2002; Bannard and Callison-Burch, 2005; CallisonBurch et al., 2006; Zhao et al., 2008; CallisonBurch, 2008). Ganitkevitch and Callison-Burch (2014) published a large-scale multilingual Paraphrase Database (PPDB) http://paraphrase. org which includes lexical, phrasal, and syntactic paraphrases (available for 22 languages with up to 170 million paraphrases each). To our knowledge, this paper is the first comprehensive study of the use of PPDB for statistical machine translation model training. Our framework has three stages: 1) a novel graph construction approach for PPDB paraphrases linked 1379 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processi"
D15-1163,N15-1176,0,0.0316203,"machine translation (Zou et al., 2013; Mikolov et al., 2013a; Zhang et al., 2014). However, most of these methods focus on frequent words or an available bilingual phrase table (Zou et al., 2013; Zhang et al., 2014; Gao et al., 2014). Mikolov et al. (2013a) learn a global linear projection from source to target using representation of frequent words on both sides. This model can be used to generate translations for new words, but a large amounts of bilingual data is required to create such a model. (Mikolov et al., 2013b) also uses bilingual data to project new translation rules. Zhao et al. (2015) extend Mikolov’s model to learn one local linear projection for each phrase. Their model reaches comparable results to Saluja et al. (2014) while works faster. Alkhouli et al. (2014) use neural network phrase representation for paraphrasing OOVs and find translation for them using a phrase-table created from limited parallel data. Our experimental settings is different from the approaches in (Alkhouli et al., 2014; Mikolov et al., 2013a; Mikolov et al., 2013b). 7 Conclusion and Future work In future work, we would like to include translations for infrequent phrases which are not OOVs. We woul"
D15-1163,D13-1141,0,0.030308,"vement on Arabic-English by using bilingual pivoting only on source phrases. (Saluja et al., 2014) also use methods similar to (Habash, 2008) that expand the phrase table with spelling and morphological variants of OOVs in test data. We do not use the dev/test data to augment the phrase table. Using comparable corpora to extract parallel sentences and phrases (Munteanu and Marcu, 2006; Smith et al., 2010; Tamura et al., 2012) are orthogonal to the approach we discuss here. Bilingual and multilingual word and phrase representation using neural networks have been applied to machine translation (Zou et al., 2013; Mikolov et al., 2013a; Zhang et al., 2014). However, most of these methods focus on frequent words or an available bilingual phrase table (Zou et al., 2013; Zhang et al., 2014; Gao et al., 2014). Mikolov et al. (2013a) learn a global linear projection from source to target using representation of frequent words on both sides. This model can be used to generate translations for new words, but a large amounts of bilingual data is required to create such a model. (Mikolov et al., 2013b) also uses bilingual data to project new translation rules. Zhao et al. (2015) extend Mikolov’s model to learn"
D15-1163,C98-2122,0,\N,Missing
D15-1163,2010.amta-workshop.3,0,\N,Missing
D15-1163,D08-1076,0,\N,Missing
D15-1163,N13-1073,0,\N,Missing
D18-1037,P17-2021,0,0.784306,"low scores if important words like verbs are missing (Chiang, 2005; Zollmann and Venugopal, 2006; Galley et al., 2006). To this end, there has been a push to incorporate some syntax into NMT models: Sennrich and Haddow (2016) incorporate POS tags and dependency information from the source side of a translation pair in NMT models. Stahlberg et al. (2016) use source language syntax to guide the decoder of an NMT system to follow hierarchical structures (Hiero) rules (Chiang, 2005). Eriguchi et al. (2016) and Bastings et al. (2017) use tree-structured encoders to exploit source language syntax. Aharoni and Goldberg (2017) take the approach of serialising the parse trees to use in a sequential decoder. Eriguchi et al. (2017) propose an NMT+RNNG model, which explores the possibilities of using dependency syntax trees from the target language using StackLSTMs (Dyer et al., 2015, 2016) to aid a sequential decoder. These approaches showed promising improvements in translation quality but all the models in previous work, even the model in Eriguchi et al. (2017) which uses RNNG, are bottom-up tree structured decoders. In contrast, we use a top-down tree-structured model called DRNN (Doubly-Recurrent Neural Networks)"
D18-1037,D17-1209,0,0.0715156,"Missing"
D18-1037,P05-1033,0,0.174528,"mon Fraser University, Burnaby, BC V5A 1S6, Canada {jeticg, sshavara, anoop}@sfu.ca Abstract using tree structures which get very low scores if important words like verbs are missing (Chiang, 2005; Zollmann and Venugopal, 2006; Galley et al., 2006). To this end, there has been a push to incorporate some syntax into NMT models: Sennrich and Haddow (2016) incorporate POS tags and dependency information from the source side of a translation pair in NMT models. Stahlberg et al. (2016) use source language syntax to guide the decoder of an NMT system to follow hierarchical structures (Hiero) rules (Chiang, 2005). Eriguchi et al. (2016) and Bastings et al. (2017) use tree-structured encoders to exploit source language syntax. Aharoni and Goldberg (2017) take the approach of serialising the parse trees to use in a sequential decoder. Eriguchi et al. (2017) propose an NMT+RNNG model, which explores the possibilities of using dependency syntax trees from the target language using StackLSTMs (Dyer et al., 2015, 2016) to aid a sequential decoder. These approaches showed promising improvements in translation quality but all the models in previous work, even the model in Eriguchi et al. (2017) which uses RNN"
D18-1037,D13-1176,0,0.0596295,"l network (Seq2DRNN+SynC). We compare our NMT model with sequential and state of the art syntax-based NMT models and show that our model produces more fluent translations with better reordering. Since our model is capable of doing translation and constituency parsing at the same time we also compare our parsing accuracy against other neural parsing models. 1 Introduction Neural machine translation (NMT) models were initially proposed as extensions of sequential neural language models (Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2015) or convolutions over n-grams in the decoder (Kalchbrenner and Blunsom, 2013). Early methods for discriminative training of machine translation models showed that the loss functions for translation were not sensitive to the production of certain important words such as verbs, without which the output sentence might be uninterpretable by humans. A good solution was to penalise such bad outputs 401 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 401–413 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics 2.2 We perform extensive experiments comparing our model against other state"
D18-1037,D16-1257,0,0.0261524,"Reference parse trees obtained using Stanford Parser. parsing (Klein and Manning, 2003b). The presence of SynC in the decoder influences 408 our proposed models can outperform a strong sequence to sequence NMT baseline and several rival models and do parsing competitively. In the future we hope to incorporate source side syntax into the model. We plan to explore the applications of SynC in NMT with more structured attention mechanisms, and potentially a hybrid phrase-based NMT systems with SynC, in which the model can benefit from SynC to be more extensible when handling larger lexicons. els. Choe and Charniak (2016) showed that a neural parsing problem shares similarity to neural language modelling problem, which forms a building block of an NMT system. We can then make the assumption that structural syntactic information utilised in neural parsing models should be able to aid NMT, which is shown to be true here. Zhang et al. (2016) proposed TreeLSTM which is another structured neural decoder. TreeLSTM is not only structurally more complicated but also uses external classifiers. Dong and Lapata (2016) also proposed a sequence-to-tree (Seq2Tree) model for question answering. Both of these models are not d"
D18-1037,P16-1004,0,0.046566,"nC, in which the model can benefit from SynC to be more extensible when handling larger lexicons. els. Choe and Charniak (2016) showed that a neural parsing problem shares similarity to neural language modelling problem, which forms a building block of an NMT system. We can then make the assumption that structural syntactic information utilised in neural parsing models should be able to aid NMT, which is shown to be true here. Zhang et al. (2016) proposed TreeLSTM which is another structured neural decoder. TreeLSTM is not only structurally more complicated but also uses external classifiers. Dong and Lapata (2016) also proposed a sequence-to-tree (Seq2Tree) model for question answering. Both of these models are not designed for NMT and lack a language model. While operate from top-tobottom like Seq2DRNN(+SynC), TreeLSTM and Seq2Tree produce components that lack sequential continuity which we have shown to be nonnegligible for language generation. Aharoni and Goldberg (2017), Wu et al. (2017), and Eriguchi et al. (2017) experimented with NMT models that utilise target side structural syntax. Aharoni and Goldberg (2017) treated constituency trees as sequential strings (linearised-tree) and trained a Seq2"
D18-1037,P03-1054,0,0.528062,"s the representation of previous words as a sequence y<j ; 2. Ancestral: treating the ancestors of the current node as a sequence (from root to the immediate parent), the representation of that sequence: pk (∀k, zj ∈ pk ); 3 Experiments 3.1 3. Fraternal: treating the previous siblings of the current node as well as the previous siblings of its parent node and so on as a sequence, the representation of that sequence: pk (∀k, precedes(pk , zj )). Model Training Experiments in this paper utilise constituency trees on the target side, these trees are obtained by using the Stanford Lexical Parser (Klein and Manning, 2003a) which we chose for its speed and accuracy prior to training. This procedure of pre-parsing data is not required at test time, our NMT system would take a sentence as input and produces the translation in target language along with its constituency tree as output. We use the German-English dataset from IWSLT2017 1 for our experiments, and tst20102015 as the test set (Table 1). To compare with other decoders that utilise target-side syntactic information, we also evaluate on three more datasets from News Commentary v8 using newstest2016 as testset (Table 2). We replace all rarely occurring wo"
D18-1037,P15-1033,0,0.0680246,"mation from the source side of a translation pair in NMT models. Stahlberg et al. (2016) use source language syntax to guide the decoder of an NMT system to follow hierarchical structures (Hiero) rules (Chiang, 2005). Eriguchi et al. (2016) and Bastings et al. (2017) use tree-structured encoders to exploit source language syntax. Aharoni and Goldberg (2017) take the approach of serialising the parse trees to use in a sequential decoder. Eriguchi et al. (2017) propose an NMT+RNNG model, which explores the possibilities of using dependency syntax trees from the target language using StackLSTMs (Dyer et al., 2015, 2016) to aid a sequential decoder. These approaches showed promising improvements in translation quality but all the models in previous work, even the model in Eriguchi et al. (2017) which uses RNNG, are bottom-up tree structured decoders. In contrast, we use a top-down tree-structured model called DRNN (Doubly-Recurrent Neural Networks) first proposed by Alvarez-Melis and Jaakkola (2017) to model structural syntactic information for NMT. We call our novel NMT model Seq2DRNN, using DRNNs as a treestructured decoder combined with a sequential encoder and a novel syntax-aware attention model."
D18-1037,N16-1024,0,0.471353,"l Network The Doubly-Recurrent Neural Network model (Alvarez-Melis and Jaakkola, 2017) takes a vector representation as input and generates a tree. Alvarez-Melis and Jaakkola (2017) show that the DRNN model can effectively reconstruct trees but they do not use DRNNs within a full-scale NMT system. We also use DRNNs for phrasestructure (aka constituency) tree structures rather than dependency trees as in previous work. DRNN decoding proceeds top-down; the generation of nodes at depth d depend solely on the state of nodes at depth < d. Unlike previous work in tree-structured decoding for NMT by Dyer et al. (2016) and Eriguchi et al. (2017), the output sentence generation is not done in sequence, where the target word yj is generated after all y<j are generated. DRNN first predicts the structure of the sentence and then expands each component to predict words. When generating yj , information regarding the structure of words from 1 to j − 1 and j + 1 to m can be used to aid prediction of yj . A DRNN consists of two recurrent neural network units, which separately process ancestral and → − → − −−→ h i = RNNenc ( h i−1 , Wx (xi )) ← − ← − ←−− (1) h i = RNNenc ( h i+1 , Wx (xi )) → − ← − enc hi = [ h i ;"
D18-1037,P16-1078,0,0.0355505,"ersity, Burnaby, BC V5A 1S6, Canada {jeticg, sshavara, anoop}@sfu.ca Abstract using tree structures which get very low scores if important words like verbs are missing (Chiang, 2005; Zollmann and Venugopal, 2006; Galley et al., 2006). To this end, there has been a push to incorporate some syntax into NMT models: Sennrich and Haddow (2016) incorporate POS tags and dependency information from the source side of a translation pair in NMT models. Stahlberg et al. (2016) use source language syntax to guide the decoder of an NMT system to follow hierarchical structures (Hiero) rules (Chiang, 2005). Eriguchi et al. (2016) and Bastings et al. (2017) use tree-structured encoders to exploit source language syntax. Aharoni and Goldberg (2017) take the approach of serialising the parse trees to use in a sequential decoder. Eriguchi et al. (2017) propose an NMT+RNNG model, which explores the possibilities of using dependency syntax trees from the target language using StackLSTMs (Dyer et al., 2015, 2016) to aid a sequential decoder. These approaches showed promising improvements in translation quality but all the models in previous work, even the model in Eriguchi et al. (2017) which uses RNNG, are bottom-up tree st"
D18-1037,D15-1166,0,0.174047,"Missing"
D18-1037,P17-2012,0,0.42523,"al., 2006). To this end, there has been a push to incorporate some syntax into NMT models: Sennrich and Haddow (2016) incorporate POS tags and dependency information from the source side of a translation pair in NMT models. Stahlberg et al. (2016) use source language syntax to guide the decoder of an NMT system to follow hierarchical structures (Hiero) rules (Chiang, 2005). Eriguchi et al. (2016) and Bastings et al. (2017) use tree-structured encoders to exploit source language syntax. Aharoni and Goldberg (2017) take the approach of serialising the parse trees to use in a sequential decoder. Eriguchi et al. (2017) propose an NMT+RNNG model, which explores the possibilities of using dependency syntax trees from the target language using StackLSTMs (Dyer et al., 2015, 2016) to aid a sequential decoder. These approaches showed promising improvements in translation quality but all the models in previous work, even the model in Eriguchi et al. (2017) which uses RNNG, are bottom-up tree structured decoders. In contrast, we use a top-down tree-structured model called DRNN (Doubly-Recurrent Neural Networks) first proposed by Alvarez-Melis and Jaakkola (2017) to model structural syntactic information for NMT. W"
D18-1037,P06-1121,0,0.118853,"Missing"
D18-1037,P02-1040,0,0.102668,"kept. 3.2 Modelling details Results Attention Module We visualise the attention weights of our Seq2DRNN+SynC model. Attention §2.2.3 computes a context vector for each node in the tree (a weighted sum of the source side vector representations). For the translation pair in Figure 2, we show the attention weight of each pair of word and node (Equation 9) in Figure 5. The addition of syntax nodes in the output enables the attention model to be used more effectively and is also valuable for visual inspection of syntactic nodes in the output mapping to the input. Table 3 and Table 4 has the BLEU (Papineni et al., 2002) and RIBES (Isozaki et al., 2010) scores. In our IWSLT2017 tests, both Seq2DRNN and Seq2DRNN+SynC produce better results than the Seq2Seq baseline model in terms of BLEU scores, while Seq2DRNN+SynC also produces better RIBES scores indicating better reordering of phrases in the output. The Seq2DRNN+SynC model performs better than the Seq2DRNN model. Both Seq2Seq and Seq2DRNN+SynC are able to produce results with lower perplexities 406 Dataset Seq2Seq Str2Tree NMT+RNNG Seq2DRNN Seq2DRNN+SynC DE-EN BLEU RIBES 16.61 73.8 16.13 — 16.41 75.0 16.90 75.1 17.21 75.8 CS-EN BLEU RIBES 11.22 69.6 11.65 —"
D18-1037,D10-1092,0,0.0145775,"ts Attention Module We visualise the attention weights of our Seq2DRNN+SynC model. Attention §2.2.3 computes a context vector for each node in the tree (a weighted sum of the source side vector representations). For the translation pair in Figure 2, we show the attention weight of each pair of word and node (Equation 9) in Figure 5. The addition of syntax nodes in the output enables the attention model to be used more effectively and is also valuable for visual inspection of syntactic nodes in the output mapping to the input. Table 3 and Table 4 has the BLEU (Papineni et al., 2002) and RIBES (Isozaki et al., 2010) scores. In our IWSLT2017 tests, both Seq2DRNN and Seq2DRNN+SynC produce better results than the Seq2Seq baseline model in terms of BLEU scores, while Seq2DRNN+SynC also produces better RIBES scores indicating better reordering of phrases in the output. The Seq2DRNN+SynC model performs better than the Seq2DRNN model. Both Seq2Seq and Seq2DRNN+SynC are able to produce results with lower perplexities 406 Dataset Seq2Seq Str2Tree NMT+RNNG Seq2DRNN Seq2DRNN+SynC DE-EN BLEU RIBES 16.61 73.8 16.13 — 16.41 75.0 16.90 75.1 17.21 75.8 CS-EN BLEU RIBES 11.22 69.6 11.65 — 12.06 70.4 11.84 67.3 12.11 70.3"
D18-1037,W16-2209,0,0.106245,"Missing"
D18-1037,P16-2049,0,0.0270191,"Connections for Neural Machine Translation and Parsing ¯ Hassan S. Shavarani, Anoop Sarkar Jetic Gu, TASC 9404, 8888 University Drive, Simon Fraser University, Burnaby, BC V5A 1S6, Canada {jeticg, sshavara, anoop}@sfu.ca Abstract using tree structures which get very low scores if important words like verbs are missing (Chiang, 2005; Zollmann and Venugopal, 2006; Galley et al., 2006). To this end, there has been a push to incorporate some syntax into NMT models: Sennrich and Haddow (2016) incorporate POS tags and dependency information from the source side of a translation pair in NMT models. Stahlberg et al. (2016) use source language syntax to guide the decoder of an NMT system to follow hierarchical structures (Hiero) rules (Chiang, 2005). Eriguchi et al. (2016) and Bastings et al. (2017) use tree-structured encoders to exploit source language syntax. Aharoni and Goldberg (2017) take the approach of serialising the parse trees to use in a sequential decoder. Eriguchi et al. (2017) propose an NMT+RNNG model, which explores the possibilities of using dependency syntax trees from the target language using StackLSTMs (Dyer et al., 2015, 2016) to aid a sequential decoder. These approaches showed promising"
D18-1037,P15-1150,0,0.0851234,"Parser. parse tree construction: the Seq2DRNN+SynC F1 score is comparable but lower than Seq2DRNN. are of non-ensemble models. Seq2DRNN Seq2DRNN+SynC Labelled Rec. F1 91.69 91.66 90.22 90.48 Table 9: IWSLT Translation result constituency labelled Table 7: Parser scores. Numbers from (Vinyals et al., 2015) Prec. 96.87 96.43 Prec. 91.63 90.73 Unlabelled Rec. F1 96.93 96.90 95.89 96.16 4 Related Work Recent research shows that modelling syntax is useful for various neural NLP tasks. Dyer et al. (2015, 2016); Vinyals et al. (2015); Luong et al. (2016) have works on language modelling and parsing, Tai et al. (2015) on semantic analysis, and Zhang et al. (2016) on sentence completion, etc. Eriguchi et al. (2017) showed that NMT model can benefit from neural syntactical parsing modTable 8: IWSLT Translation result constituency unlabelled scores. Reference parse trees obtained using Stanford Parser. parsing (Klein and Manning, 2003b). The presence of SynC in the decoder influences 408 our proposed models can outperform a strong sequence to sequence NMT baseline and several rival models and do parsing competitively. In the future we hope to incorporate source side syntax into the model. We plan to explore t"
D18-1037,P17-1065,0,0.243235,"id NMT, which is shown to be true here. Zhang et al. (2016) proposed TreeLSTM which is another structured neural decoder. TreeLSTM is not only structurally more complicated but also uses external classifiers. Dong and Lapata (2016) also proposed a sequence-to-tree (Seq2Tree) model for question answering. Both of these models are not designed for NMT and lack a language model. While operate from top-tobottom like Seq2DRNN(+SynC), TreeLSTM and Seq2Tree produce components that lack sequential continuity which we have shown to be nonnegligible for language generation. Aharoni and Goldberg (2017), Wu et al. (2017), and Eriguchi et al. (2017) experimented with NMT models that utilise target side structural syntax. Aharoni and Goldberg (2017) treated constituency trees as sequential strings (linearised-tree) and trained a Seq2Seq model to produce such sequences. Wu et al. (2017) proposed SD-NMT, which models dependency syntax trees by adding a shift-reduce neural parser to a standard RNN decoder. Eriguchi et al. (2017) in addition to Wu et al. (2017)’s work, proposed NMT+RNNG which uses a modified RNNG generator (Dyer et al., 2016) to process dependency instead of constituency information as originally p"
D18-1037,W06-3119,0,0.174512,"Missing"
D18-1102,D13-1087,0,0.72375,"size of 10M, our system outperforms the state of the art (Nuhn et al., 2014) on this task. Related Work Automatic decipherment for substitution ciphers started with dictionary attacks (Hart, 1994; Jakobsen, 1995; Olson, 2007). Ravi and Knight (2008) frame the decipherment problem as an integer linear programming (ILP) problem. Knight et al. (2006) use an HMM-based EM algorithm for solving a variety of decipherment problems. Ravi and Knight (2011) extend the HMM-based EM approach with a Bayesian approach, and report the 872 Acknowledgments first automatic decipherment of the Zodiac-408 cipher. Berg-Kirkpatrick and Klein (2013) show that a large number of random restarts can help the EM approach.Corlett and Penn (2010) presented an efficient A* search algorithm to solve letter substitution ciphers. Nuhn et al. (2013) produce better results in faster time compared to ILP and EM-based decipherment methods by employing a higher order language model and an iterative beam search algorithm. Nuhn et al. (2014) present various improvements to the beam search algorithm in Nuhn et al. (2013) including improved rest cost estimation and an optimized strategy for ordering decipherment of the cipher symbols. Hauer et al. (2014) p"
D18-1102,P10-1106,0,0.573969,"iversity Burnaby, BC , Canada {nkambhat,amansour,anoop}@sfu.ca Abstract 2 We use the notation from Nuhn et al. (2013). Ciphertext f1N = f1 ..fi ..fN and plaintext eN 1 = e1 ..ei ..eN consist of vocabularies fi ∈ Vf and ei ∈ Ve respectively. The beginning tokens in the ciphertext (f0 ) and plaintext (e0 ) are set to “$” denoting the beginning of a sentence. The substitutions are represented by a function φ : Vf → Ve such that 1:1 substitutions are bijective while homophonic substitutions are general. A cipher function φ which does not have every φ(f ) fixed is called a partial cipher function (Corlett and Penn, 2010). The number of f s that are fixed in φ is given by its cardinality. φ0 is called an extension of φ, if f is fixed in φ0 such that δ(φ0 (f ), φ(f )) yields true ∀f ∈ Vf which are already fixed in φ where δ is Kronecker delta. Decipherment is then the task of finding the φ for which the probability of the deciphered text is maximized. φˆ = arg max p(φ(f1 )...φ(fN )) (1) Decipherment of homophonic substitution ciphers using language models (LMs) is a wellstudied task in NLP. Previous work in this topic scores short local spans of possible plaintext decipherments using n-gram LMs. The most widely"
D18-1102,W16-6003,0,0.0692643,"Missing"
D18-1102,C14-1218,0,0.566421,"significantly better error rates with much smaller beam sizes. 1 Decipherment Model Introduction φ Breaking substitution ciphers recovers the plaintext from a ciphertext that uses a 1:1 or homophonic cipher key. Previous work using pretrained language models (LMs) for decipherment use n-gram LMs (Ravi and Knight, 2011; Nuhn et al., 2013). Some methods use the ExpectationMaximization (EM) algorithm (Knight et al., 2006) while most state-of-the-art approaches for decipherment of 1:1 and homophonic substitution ciphers use beam search and rely on the clever use of n-gram LMs (Nuhn et al., 2014; Hauer et al., 2014). Neural LMs globally score the entire candidate plaintext sequence (Mikolov et al., 2010). However, using a neural LM for decipherment is not trivial because scoring the entire candidate partially deciphered plaintext is computationally challenging. We solve both of these problems in this paper and provide an improved beam search based decipherment algorithm for homophonic ciphers that exploits pre-trained neural LMs for the first time. where p(.) is the language model (LM). Finding this argmax is solved using a beam search algorithm (Nuhn et al., 2013) which incrementally finds the most like"
D18-1102,P06-2065,0,0.193949,"ts the prediction power of a neural LM. We compare against the state of the art n-gram based methods on many different decipherment tasks. On challenging ciphers such as the Beale cipher we provide significantly better error rates with much smaller beam sizes. 1 Decipherment Model Introduction φ Breaking substitution ciphers recovers the plaintext from a ciphertext that uses a 1:1 or homophonic cipher key. Previous work using pretrained language models (LMs) for decipherment use n-gram LMs (Ravi and Knight, 2011; Nuhn et al., 2013). Some methods use the ExpectationMaximization (EM) algorithm (Knight et al., 2006) while most state-of-the-art approaches for decipherment of 1:1 and homophonic substitution ciphers use beam search and rely on the clever use of n-gram LMs (Nuhn et al., 2014; Hauer et al., 2014). Neural LMs globally score the entire candidate plaintext sequence (Mikolov et al., 2010). However, using a neural LM for decipherment is not trivial because scoring the entire candidate partially deciphered plaintext is computationally challenging. We solve both of these problems in this paper and provide an improved beam search based decipherment algorithm for homophonic ciphers that exploits pre-t"
D18-1102,P13-1154,0,0.778556,"0 is called an extension of φ, if f is fixed in φ0 such that δ(φ0 (f ), φ(f )) yields true ∀f ∈ Vf which are already fixed in φ where δ is Kronecker delta. Decipherment is then the task of finding the φ for which the probability of the deciphered text is maximized. φˆ = arg max p(φ(f1 )...φ(fN )) (1) Decipherment of homophonic substitution ciphers using language models (LMs) is a wellstudied task in NLP. Previous work in this topic scores short local spans of possible plaintext decipherments using n-gram LMs. The most widely used technique is the use of beam search with n-gram LMs proposed by Nuhn et al. (2013). We propose a beam search algorithm that scores the entire candidate plaintext at each step of the decipherment using a neural LM. We augment beam search with a novel rest cost estimation that exploits the prediction power of a neural LM. We compare against the state of the art n-gram based methods on many different decipherment tasks. On challenging ciphers such as the Beale cipher we provide significantly better error rates with much smaller beam sizes. 1 Decipherment Model Introduction φ Breaking substitution ciphers recovers the plaintext from a ciphertext that uses a 1:1 or homophonic ci"
D18-1102,D14-1184,0,0.763812,"e cipher we provide significantly better error rates with much smaller beam sizes. 1 Decipherment Model Introduction φ Breaking substitution ciphers recovers the plaintext from a ciphertext that uses a 1:1 or homophonic cipher key. Previous work using pretrained language models (LMs) for decipherment use n-gram LMs (Ravi and Knight, 2011; Nuhn et al., 2013). Some methods use the ExpectationMaximization (EM) algorithm (Knight et al., 2006) while most state-of-the-art approaches for decipherment of 1:1 and homophonic substitution ciphers use beam search and rely on the clever use of n-gram LMs (Nuhn et al., 2014; Hauer et al., 2014). Neural LMs globally score the entire candidate plaintext sequence (Mikolov et al., 2010). However, using a neural LM for decipherment is not trivial because scoring the entire candidate partially deciphered plaintext is computationally challenging. We solve both of these problems in this paper and provide an improved beam search based decipherment algorithm for homophonic ciphers that exploits pre-trained neural LMs for the first time. where p(.) is the language model (LM). Finding this argmax is solved using a beam search algorithm (Nuhn et al., 2013) which incrementall"
D18-1102,D08-1085,0,0.571643,"e Pt. 2 408 763 Unique Symbols 54 180 Obs/ symbol 7.55 4.23 sequence. In all the experiments we use a character NLM trained on English Gigaword corpus augmented with a short corpus of plaintext letters of about 2000 words authored by the Zodiac killer2 . Table 1: Homophonic ciphers used in our experiments. 4.1 pled plaintext symbols from the NLM. Since more terms participate in the rest cost estimation with global context, we use the plaintext LM to provide us with a better rest cost in the beam search. 3.3 In this experiment we use a synthetic 1:1 letter substitution cipher dataset following Ravi and Knight (2008), Nuhn et al. (2013) and Hauer et al. (2014). The text is from English Wikipedia articles about history3 , preprocessed by stripping the text of all images, tables, then lower-casing all characters, and removing all non-alphabetic and non-space characters. We create 50 cryptograms for each length 16, 32, 64, 128 and 256 using a random Caesar-cipher 1:1 substitution. Frequency Matching Heuristic Alignment by frequency similarity (Yarowsky and Wicentowski, 2000) assumes that two forms belong to the same lemma when their relative frequency fits the expected distribution. We use this heuristic to"
D18-1102,P11-1025,0,0.134938,"the decipherment using a neural LM. We augment beam search with a novel rest cost estimation that exploits the prediction power of a neural LM. We compare against the state of the art n-gram based methods on many different decipherment tasks. On challenging ciphers such as the Beale cipher we provide significantly better error rates with much smaller beam sizes. 1 Decipherment Model Introduction φ Breaking substitution ciphers recovers the plaintext from a ciphertext that uses a 1:1 or homophonic cipher key. Previous work using pretrained language models (LMs) for decipherment use n-gram LMs (Ravi and Knight, 2011; Nuhn et al., 2013). Some methods use the ExpectationMaximization (EM) algorithm (Knight et al., 2006) while most state-of-the-art approaches for decipherment of 1:1 and homophonic substitution ciphers use beam search and rely on the clever use of n-gram LMs (Nuhn et al., 2014; Hauer et al., 2014). Neural LMs globally score the entire candidate plaintext sequence (Mikolov et al., 2010). However, using a neural LM for decipherment is not trivial because scoring the entire candidate partially deciphered plaintext is computationally challenging. We solve both of these problems in this paper and"
D18-1102,P00-1027,0,0.0417378,"rovide us with a better rest cost in the beam search. 3.3 In this experiment we use a synthetic 1:1 letter substitution cipher dataset following Ravi and Knight (2008), Nuhn et al. (2013) and Hauer et al. (2014). The text is from English Wikipedia articles about history3 , preprocessed by stripping the text of all images, tables, then lower-casing all characters, and removing all non-alphabetic and non-space characters. We create 50 cryptograms for each length 16, 32, 64, 128 and 256 using a random Caesar-cipher 1:1 substitution. Frequency Matching Heuristic Alignment by frequency similarity (Yarowsky and Wicentowski, 2000) assumes that two forms belong to the same lemma when their relative frequency fits the expected distribution. We use this heuristic to augment the score estimation (SCORE):   ν(f ) 0 FMH(φ ) = log f ∈ Vf , e ∈ Ve ν(e) (3) ν(f ) is the percentage relative frequency of the ciphertext symbol f , while ν(e) is the percentage relative frequency of the plaintext token e in the plaintext language model. The closer this value to 0, the more likely it is that f is mapped to e. Thus given a φ with the SCORE(φ), the extension φ0 (Algo. 1) is scored as: SCORE(φ0 ) = SCORE(φ) + NEW(φ0 ) − FMH(φ0 ) (4) w"
D18-1337,N12-1048,0,0.295262,"agent which is not able to trade-off quality with delay. It always prefers to read more words from the input and this approach does not work well in practice. (Satija and Pineau, 2016) introduced a trainable agent which they trained using Deep Q networks (Mnih et al., 2015). We modified the SNMT trainable agent in (Gu et al., 2017) and added a new non-trivial PREDICT action to the agent. We compare to their model and show better results in delay and quality. 5 Related work 6 Early work in SNMT was done in speech, where the incoming signals were segmented based on acoustic or statistical cues (Bangalore et al., 2012; F¨ugen et al., 2007). (Sridhar et al., 2013; Matusov We introduce a new prediction action in a trainable agent for simultaneous neural machine translation. With prediction, the agent can be informed about future time steps in the input stream. Compared to a very strong baseline our results show that prediction can lower delay and improve the 2 See Figure 6 in supplementary materials for more numerical results. 3025 Conclusion translation quality, especially for longer sentences and translating from an SOV (subject-object-verb) language (DE) to an SVO language (EN). Acknowledgments We would l"
D18-1337,W14-3346,0,0.0121502,"each time step, the AGENT decides whether to READ, WRITE or PREDICT for the following time steps. R P W Figure 2: Action transition graph. R, P, and W stands for READ, PREDICT and WRITE actions respectively. ter a series of WRITE, the AGENT cannot choose to PREDICT, and after a sequence of PREDICTs, READ is not an option. Reward Function: The total reward at any time step is calculated as the cumulative sum of rewards for actions at each preceding step. All the evaluation metrics have been modified to be computed for every time step. Quality: We use a modified smoothed version of BLEU score (Chen and Cherry, 2014) multiplied by Brevity Penalty (Lin and Och, 2004) for evaluating the impact of each action on translation quality. At each point in time, the reward for translation quality is: ( BLEU(t) t&lt;T rtQ = BLEU(W, W ⇤ ) t = T The BLEU(t) is the difference between BLEU score of the translated sentence at the previous time step and the current time step; BLEU(t) = BLEU(W t , W ⇤ ) BLEU(W t 1 , W ⇤ ); where W t is the prefix of the translated sentence at time t. Delay: The Delay reward is used to motivate the AGENT to minimize delay. We use Average Proportion (AP) (Cho and Esipova, 2016) for this purpose"
D18-1337,D14-1140,0,0.125158,"Missing"
D18-1337,E17-1099,0,0.582651,"ing reinforcement learning with a novel reward function. Our agent with prediction has better translation quality and less delay compared to an agent-based simultaneous translation system without prediction. 1 Introduction One of the next significant challenges in machine translation research is to make translation ubiquitous using real-time translation. Simultaneous machine translation aims to address this issue by interleaving reading the input with writing the output translation. Current Simultaneous Neural Machine Translation (SNMT) systems (Satija and Pineau, 2016; Cho and Esipova, 2016; Gu et al., 2017) use an AGENT to control an incremental encoder-decoder (or sequence to sequence) NMT model. Each READ adds more information to the encoder RNN, and each WRITE produces more output using the decoder RNN. In this paper, we propose adding a new action to the AGENT: a PREDICT action that predicts what words might appear in the input stream. Prediction was previously proposed in simultaneous statistical machine translation (Grissom II et al., 2014) but has not been studied in the context of Neural Machine Translation (NMT). In SNMT systems, prediction of future words augments the encoder-decoder m"
D18-1337,P16-1162,0,0.0816098,"m (Gu et al., 2017; Williams, 1992) which searches hP for i the maximum in T J(✓) = E⇡✓ using the gradient: t=1 rt ⇡✓ (s, a)] where r✓ J(✓) = E [r log ⇡ (a|s)Q ⇡✓ ✓ ✓ PT Q = t=1 rt . The gradient for a sentence is the cumulative sum of gradients at each time step. We pre-train the E NVIRONMENT on full sentences using log-loss log p(y|x). 4 Experiments We train and evaluate our model on EnglishGerman (EN-DE) in both directions. We use WMT 2015 for training and Newstest 2013 for validation and testing. All sentences have been tokenized and the words are segmented using byte pair encoding (BPE) (Sennrich et al., 2016). 100 80 Percentage needed when translating each word. Given the source words X and translated words W , AP can be computed as: X 1 d(X, W ) = s(t) |X||W |t ( 0 t&lt;T D rt = d(X, W ) t = T Where s(t) denotes the number of source words 32.11 24.51 23.93 35.23 33.98 40.26 42.09 14.69 22.94 23.87 34.57 34.96 42.49 41.18 20 25 38.13 60 34.1 Read Write Predict ↵=1 = 0.5 = 0.5 40 20 0 33.79 0 5 47.18 10 15 Iteration (x1000) Quality: 17.54 Delay: 0.74 Figure 3: Action distribution for English to German translation in the first 25000 iterations. The numbers in each bar are the average action percentage"
D18-1337,N13-1023,0,0.294983,"th delay. It always prefers to read more words from the input and this approach does not work well in practice. (Satija and Pineau, 2016) introduced a trainable agent which they trained using Deep Q networks (Mnih et al., 2015). We modified the SNMT trainable agent in (Gu et al., 2017) and added a new non-trivial PREDICT action to the agent. We compare to their model and show better results in delay and quality. 5 Related work 6 Early work in SNMT was done in speech, where the incoming signals were segmented based on acoustic or statistical cues (Bangalore et al., 2012; F¨ugen et al., 2007). (Sridhar et al., 2013; Matusov We introduce a new prediction action in a trainable agent for simultaneous neural machine translation. With prediction, the agent can be informed about future time steps in the input stream. Compared to a very strong baseline our results show that prediction can lower delay and improve the 2 See Figure 6 in supplementary materials for more numerical results. 3025 Conclusion translation quality, especially for longer sentences and translating from an SOV (subject-object-verb) language (DE) to an SVO language (EN). Acknowledgments We would like to thank the anonymous reviewers for thei"
D18-1337,P04-1077,0,0.0205316,"ITE or PREDICT for the following time steps. R P W Figure 2: Action transition graph. R, P, and W stands for READ, PREDICT and WRITE actions respectively. ter a series of WRITE, the AGENT cannot choose to PREDICT, and after a sequence of PREDICTs, READ is not an option. Reward Function: The total reward at any time step is calculated as the cumulative sum of rewards for actions at each preceding step. All the evaluation metrics have been modified to be computed for every time step. Quality: We use a modified smoothed version of BLEU score (Chen and Cherry, 2014) multiplied by Brevity Penalty (Lin and Och, 2004) for evaluating the impact of each action on translation quality. At each point in time, the reward for translation quality is: ( BLEU(t) t&lt;T rtQ = BLEU(W, W ⇤ ) t = T The BLEU(t) is the difference between BLEU score of the translated sentence at the previous time step and the current time step; BLEU(t) = BLEU(W t , W ⇤ ) BLEU(W t 1 , W ⇤ ); where W t is the prefix of the translated sentence at time t. Delay: The Delay reward is used to motivate the AGENT to minimize delay. We use Average Proportion (AP) (Cho and Esipova, 2016) for this purpose, which is the average number of source words 3023"
D19-5624,W16-1601,0,0.0692997,"Missing"
D19-5624,W18-2803,0,0.0236087,"Missing"
D19-5624,D14-1179,0,0.0846615,"Missing"
D19-5624,P16-1000,0,0.204629,"Missing"
D19-5624,W19-1902,0,0.0205006,"igate how a particular concept is represented in the network. Analyzing and interpreting the attention mechanism in NLP (Koehn and Knowles, 2017; Ghader and Monz, 2017; Tang and Nivre, 2018; Clark et al., 2019; Vig and Belinkov, 2019) is another direction that has drawn major interest. Although attention weights have been implicitly or explicitly used to explain a model’s decisions, the reliability of this approach is not proven. Several attempts have been made to investigate the reliability of this approach for explaining a models’ decision in NLP (Serrano and Smith, 2019; Baan et al., 2019; Jain et al., 2019; Jain and Wallace, 2019), and also in information retrieval (Jain and Madhyastha, 2019). Our work was inspired by Jain and Wallace (2019). However, in this work we have focused on similar issues in neural machine translation which is has different challenges compared to text classification in terms of objective and architecture. Moreover, our paper studies the effect of different counterfactual attention methods. 7 categories such as parts of speech (POS) or out-ofvocabulary (OOV) words. Another logical investigation for future would be to address interpretability of copy mechanism in NMT (Gu"
D19-5624,N19-1357,0,0.222078,"nt words (Sec 4). Function words (e.g., a, the, is) have little lexical meaning in contrast to content words and thus we are curious whether explanatory power of the attention mechanism differs for generation of these two groups of words. model makes the same decision again and outputs “accessible”. This example shows that using attention weights to reason about the model’s predictions can be misleading, as these two heatmaps convey different explanations. There are relatively few previous studies on investigating the power of attention mechanism in rationalizing a model’s predictions in NLP (Jain and Wallace, 2019; Serrano and Smith, 2019) and they all target text classification tasks where attention is over the input document. Their findings do not generalize easily to NMT due to the difference in how the decoder works in the translation task which produces a sequence rather than a class label. NMT is a sequence-to-sequence (Seq2Seq) task, which is different from text classification. Also, the size of the output space is quite limited in text classification, whereas in NMT, it is equal to the vocabulary size of the target language that can be very large. Furthermore, different neural architectures (e."
D19-5624,W19-5201,0,0.0310296,"Missing"
D19-5624,P17-1106,0,0.0385962,"y the previous findings suggesting special hidden units keep track of translation length (Shi et al., 2016). As a result, EOS token is generated upon receiving signal from these units rather than using attention. This indicates that attention weights are highly unreliable for explaining the generation of EOS tokens. This is worth noting because early generation of the EOS token is often a major reason of the undertranslation problem in NMT (Kuang et al., 2018). Thus, attention weights should not be used to debug early generation of EOS, and that some other underlying influence in the network (Ding et al., 2017) might be responsible for the model’s decision in this case. 5.4 Last encoder hidden state is a poor representation of the source context in the attentional model The last encoder state in a non-attentional model is passed to the decoder as the representation of the source context (Cho et al., 2014) although the accuracy of this representation has been shown to degrade as sentence length increases (Bahdanau et al., 2014). We experiment with the LastEncoderState method to investigate how well the last encoder state can be representative of the source context in the attentional setting, and if e"
D19-5624,J82-2005,0,0.573898,"Missing"
D19-5624,D18-1407,0,0.0520402,"Missing"
D19-5624,P07-2045,0,0.00621909,"e Uniform or ZeroOutMax methods. To maximize the chance of finding a counterfactual attention, for each output token, we try out all the proposed methods to check if we can find a counterfactual attention (row 4). As evident from Table 2, this approach greatly increases the chance of finding a counterfactual attention. Note that as previously stated, these percentages are a lowerbound for the true percentage. In this work we use the German-English dataset from IWSLT20143 . We concatenate dev2010, dev2012, tst2010, tst2011 and tst2012 to be used as the test data. Data is tokenized using Moses (Koehn et al., 2007). 4.2 Model Details OpenNMT (Klein et al., 2017) is used for our Seq2Seq implementation. We use Long ShortTerm Memory (LSTM) as RNN units. Each LSTM unit has 2 layers, and the dimension size for LSTM units and word embeddings is set to 500. The model is trained using Adam trainer with learning rate 0.001 for 50000 steps using early stopping. Vocabulary size for both the source and target language is set to 50000. Sentences longer that 50 tokens are pruned. 5 Method RandomPermute Uniform ZeroOutMax Aggregate(1+2+3) ZeroOut LastEncoderState OnlyMax KeepMaxUniformOthers Table 2: Percentage of the"
D19-5624,I17-1004,0,0.0258232,"ds with a counterfactual attention indicating unreliability of using attention weights as explanation. • The generation of function words relies more on the target context, whereas the generation of content words relies more on the source context. This results in a higher likelihood of 227 output (Li et al., 2016b). While these methods focus on explaining a model’s decision, Shi et al. (2016); K´ad´ar et al. (2017); Calvillo and Crocker (2018) investigate how a particular concept is represented in the network. Analyzing and interpreting the attention mechanism in NLP (Koehn and Knowles, 2017; Ghader and Monz, 2017; Tang and Nivre, 2018; Clark et al., 2019; Vig and Belinkov, 2019) is another direction that has drawn major interest. Although attention weights have been implicitly or explicitly used to explain a model’s decisions, the reliability of this approach is not proven. Several attempts have been made to investigate the reliability of this approach for explaining a models’ decision in NLP (Serrano and Smith, 2019; Baan et al., 2019; Jain et al., 2019; Jain and Wallace, 2019), and also in information retrieval (Jain and Madhyastha, 2019). Our work was inspired by Jain and Wallace (2019). However, i"
D19-5624,D18-1537,0,0.0314328,"Missing"
D19-5624,W17-3204,0,0.0647147,"Missing"
D19-5624,P18-1164,0,0.0561794,"Missing"
D19-5624,P16-1154,0,0.0961857,"Missing"
D19-5624,N16-1082,0,0.0672156,"Being able to interpret the deficiencies of a model is also crucial to further improve upon it. This requires an explainable understanding of the internals of the model, including how certain concepts are being modeled or represented. Therefore developing methods to interpret and understand neural models is an important research goal. Visualizing and interpreting neural models has been extensively studied in computer vision (Simonyan et al., 2013; Bach et al., 2015; Zeiler and Fergus, 2014; Montavon et al., 2017), and more recently in natural language processing (NLP) (Karpathy et al., 2015; Li et al., 2016a; Strobelt et al., 2017, 2018). Recently, the integration of attention mechanism (Bahdanau et al., 2014) with an NMT sequence to sequence model (Sutskever et al., 2014) has led to significant improvements in translation quality especially for longer sentence lengths. The attention mechanism provides a weighted average over the information from the source encodings to be used at each translation step. These weights are often regarded as a measure of importance, and are implicitly or explicitly used as an explanation for the model’s decision. However it has not yet been established to what exte"
D19-5624,Q17-1007,0,0.0845469,"Missing"
D19-5624,D15-1166,0,0.0624497,"Missing"
D19-5624,W19-4808,0,0.0371785,"ing attention weights as explanation. • The generation of function words relies more on the target context, whereas the generation of content words relies more on the source context. This results in a higher likelihood of 227 output (Li et al., 2016b). While these methods focus on explaining a model’s decision, Shi et al. (2016); K´ad´ar et al. (2017); Calvillo and Crocker (2018) investigate how a particular concept is represented in the network. Analyzing and interpreting the attention mechanism in NLP (Koehn and Knowles, 2017; Ghader and Monz, 2017; Tang and Nivre, 2018; Clark et al., 2019; Vig and Belinkov, 2019) is another direction that has drawn major interest. Although attention weights have been implicitly or explicitly used to explain a model’s decisions, the reliability of this approach is not proven. Several attempts have been made to investigate the reliability of this approach for explaining a models’ decision in NLP (Serrano and Smith, 2019; Baan et al., 2019; Jain et al., 2019; Jain and Wallace, 2019), and also in information retrieval (Jain and Madhyastha, 2019). Our work was inspired by Jain and Wallace (2019). However, in this work we have focused on similar issues in neural machine tra"
D19-5624,P19-1282,0,0.139481,"on words (e.g., a, the, is) have little lexical meaning in contrast to content words and thus we are curious whether explanatory power of the attention mechanism differs for generation of these two groups of words. model makes the same decision again and outputs “accessible”. This example shows that using attention weights to reason about the model’s predictions can be misleading, as these two heatmaps convey different explanations. There are relatively few previous studies on investigating the power of attention mechanism in rationalizing a model’s predictions in NLP (Jain and Wallace, 2019; Serrano and Smith, 2019) and they all target text classification tasks where attention is over the input document. Their findings do not generalize easily to NMT due to the difference in how the decoder works in the translation task which produces a sequence rather than a class label. NMT is a sequence-to-sequence (Seq2Seq) task, which is different from text classification. Also, the size of the output space is quite limited in text classification, whereas in NMT, it is equal to the vocabulary size of the target language that can be very large. Furthermore, different neural architectures (e.g., presence of an encoder"
D19-5624,D16-1248,0,0.0522332,"Missing"
D19-5624,J17-4003,0,\N,Missing
D19-5624,P17-4012,0,\N,Missing
E03-1008,H91-1060,0,0.0320313,"Missing"
E03-1008,W99-0613,0,0.0609689,"Missing"
E03-1008,W01-0521,0,0.161063,"Missing"
E03-1008,J93-2004,0,0.0278308,"Missing"
E03-1008,W01-0501,0,0.185139,"Missing"
E03-1008,N01-1023,1,0.616825,"Missing"
E03-1008,P95-1026,0,0.323358,"Missing"
E03-1008,J03-4003,0,\N,Missing
E03-1008,P02-1046,0,\N,Missing
E06-1042,J98-1002,0,0.0383962,"as forced to eat his spinach” vs. “he was forced to eat his words”) or our knowledge of the world (“the sponge absorbed the water” vs. “the company absorbed the loss”). Nonliteral is then anything that is “not literal”, including most tropes, such as metaphors, idioms, as well phrasal verbs and other anomalous expressions that cannot really be seen as literal. In terms of metonymy, TroFi may cluster a verb used in a metonymic expression such as “I read Keats” as nonliteral, but we make no strong claims about this. 3.1 The Data The TroFi algorithm requires a target set (called original set in (Karov & Edelman, 1998)) – the set of sentences containing the verbs to be classified into literal or nonliteral – and the seed sets: the literal feedback set and the nonliteral feedback set. These sets contain feature lists consisting of the stemmed nouns and verbs in a sentence, with target or seed words and frequent words removed. The frequent word list (374 words) consists of the 332 most frequent words in the British National Corpus plus contractions, single letters, and numbers from 0-10. The target set is built using the ’88-’89 Wall Street Journal Corpus (WSJ) tagged using the (Ratnaparkhi, 1996) tagger and"
E06-1042,J04-1002,0,0.471713,"tin, 1990; Martin, 1992) – must be largely hand-coded and generally work well on an enumerable set of metaphors or in limited domains. Dictionarybased systems use existing machine-readable dictionaries and path lengths between words as one of their primary sources for metaphor processing information (e.g. (Dolan, 1995)). Corpus-based systems primarily extract or learn the necessary metaphor-processing information from large corpora, thus avoiding the need for manual annotation or metaphor-map construction. Examples of such systems can be found in (Murata et. al., 2000; Nissim & Markert, 2003; Mason, 2004). The work on supervised metonymy resolution by Nissim & Markert and the work on conceptual metaphors by Mason come closest to what we are trying to do with TroFi. Nissim & Markert (2003) approach metonymy resolution with machine learning methods, “which [exploit] the similarity between examples of conventional metonymy” ((Nissim & Markert, 2003), p. 56). They see metonymy resolution as a classification problem between the literal use of a word and a number of pre-defined metonymy types. They use similarities between possibly metonymic words (PMWs) and known metonymies as well as context simil"
E06-1042,P03-1008,0,0.018677,"s (e.g. (Fass, 1997; Martin, 1990; Martin, 1992) – must be largely hand-coded and generally work well on an enumerable set of metaphors or in limited domains. Dictionarybased systems use existing machine-readable dictionaries and path lengths between words as one of their primary sources for metaphor processing information (e.g. (Dolan, 1995)). Corpus-based systems primarily extract or learn the necessary metaphor-processing information from large corpora, thus avoiding the need for manual annotation or metaphor-map construction. Examples of such systems can be found in (Murata et. al., 2000; Nissim & Markert, 2003; Mason, 2004). The work on supervised metonymy resolution by Nissim & Markert and the work on conceptual metaphors by Mason come closest to what we are trying to do with TroFi. Nissim & Markert (2003) approach metonymy resolution with machine learning methods, “which [exploit] the similarity between examples of conventional metonymy” ((Nissim & Markert, 2003), p. 56). They see metonymy resolution as a classification problem between the literal use of a word and a number of pre-defined metonymy types. They use similarities between possibly metonymic words (PMWs) and known metonymies as well as"
E06-1042,J76-2003,0,0.452192,"the first one. In order to do this, it is crucial to know that “hit” is being used nonliterally in the first sentence. Ideally, we would like to look at TroFi as a first step towards an unsupervised, scalable, widely applicable approach to nonliteral language processing that works on real-world data from any domain in any language. 329 2 Previous Work The foundations of TroFi lie in a rich collection of metaphor and metonymy processing systems: everything from hand-coded rule-based systems to statistical systems trained on large corpora. Rule-based systems – some using a type of interlingua (Russell, 1976); others using complicated networks and hierarchies often referred to as metaphor maps (e.g. (Fass, 1997; Martin, 1990; Martin, 1992) – must be largely hand-coded and generally work well on an enumerable set of metaphors or in limited domains. Dictionarybased systems use existing machine-readable dictionaries and path lengths between words as one of their primary sources for metaphor processing information (e.g. (Dolan, 1995)). Corpus-based systems primarily extract or learn the necessary metaphor-processing information from large corpora, thus avoiding the need for manual annotation or metaph"
E06-1042,J99-2004,0,\N,Missing
E06-1042,markert-nissim-2002-towards,0,\N,Missing
E06-1042,C90-3001,0,\N,Missing
E06-1042,E89-1001,0,\N,Missing
E06-1042,C86-1059,0,\N,Missing
E06-1042,P94-1038,0,\N,Missing
E06-1042,C90-3045,0,\N,Missing
E06-1042,P95-1026,0,\N,Missing
E06-1042,P97-1008,0,\N,Missing
E06-1042,P97-1048,0,\N,Missing
E06-1042,N03-1003,0,\N,Missing
E06-1042,1993.eamt-1.6,0,\N,Missing
E06-1042,W90-0102,0,\N,Missing
E06-1042,P99-1005,0,\N,Missing
E17-2097,W12-3102,0,0.087474,"Missing"
E17-2097,N03-1017,0,0.0257823,"side aka Greibach Normal Form (GNF). Throughout this paper we abuse the notation for simplicity and use the term GNF grammars for such SCFGs. This leads to a single language model (LM) history for each hypothesis and speeds up decoding significantly, up to four times faster (Siahbani et al., 2013). Introduction Phrase-based machine translation handles reordering between source and target languages by visiting phrases in the source in arbitrary order while generating the target from left to right. A distortion penalty is used to penalize deviation from the monotone translation (no reordering) (Koehn et al., 2003; Och and Ney, 2004). Identical distortion penalties for different types of phrases ignore the fact that certain phrases (with certain words) were more likely to reorder than others. State-of-the-art phrase based translation systems address this issue by applying a lexicalized reordering model (LRM) (Tillmann, 2004; Koehn et al., 2007; Galley and Manning, 2008; Galley and Manning, 2010) which uses word aligned data to score phrase pair reordering. These models distinguish three orientations with respect to the previously translated phrase: monotone (M), swap (S), The Hiero translation model ha"
E17-2097,C14-1108,0,0.021741,"n the bottom. significantly improves translation quality in LRHiero and improves Hiero results to a lesser extent. Nguyen and Vogel (2013) integrate phrase-based distortion and lexicalized reordering features with CKY-based Hiero decoder which significantly improve the translation quality. In their approach, each partial hypothesis during decoding is mapped into a sequence of phrase-pairs then the distortion and reordering features are computed similar to phrase-based MT. They use a LRM trained for phrase-based MT (Galley and Manning, 2010) which applies some restrictions on the Hiero rules. (Cao et al., 2014; Huck et al., 2013) propose different approaches to directly train LRM for Hiero rules. However, these approaches are designed for CKY-decoding and cannot be directly used or adapted for LR-Hiero decoding which uses an Earley-style parsing algorithm. The crucial difference is the nature of bottom-up versus left to right decisions for lexicalized reordering and generating the translation in left-to-right manner. In this paper, we introduce a novel shift-reduce algorithm to learn a lexicalized reordering model (LRM) for LR-Hiero. We show that augmenting LR-Hiero with an LRM improves translation"
E17-2097,P07-2045,0,0.0553305,"anslation handles reordering between source and target languages by visiting phrases in the source in arbitrary order while generating the target from left to right. A distortion penalty is used to penalize deviation from the monotone translation (no reordering) (Koehn et al., 2003; Och and Ney, 2004). Identical distortion penalties for different types of phrases ignore the fact that certain phrases (with certain words) were more likely to reorder than others. State-of-the-art phrase based translation systems address this issue by applying a lexicalized reordering model (LRM) (Tillmann, 2004; Koehn et al., 2007; Galley and Manning, 2008; Galley and Manning, 2010) which uses word aligned data to score phrase pair reordering. These models distinguish three orientations with respect to the previously translated phrase: monotone (M), swap (S), The Hiero translation model handles reordering very differently from a phrase-based model, through weighted translation rules (SCFGs) determined by non-terminal mappings. The rule X → hne X1 pas, do not X1 i indicates the translation of the phrase between ne and pas will be after the English phrase do not. However, reordering features can also be added to the Hier"
E17-2097,N13-1003,0,0.0171589,": An example showing that the shift-reduce algorithm can capture local reorderings like: the right of life and was deprived. D. We consider phrase pairs of any length to compute orientation. Note that although phrase pairs extracted from the rules that can be discontinuous (on source), just continuous source phrases in each sentence pair are used to compute orientation (previously translated phrases). Once orientation counts for rules (phrase-pairs obtained form rules) are collected from the bitext, the probability model P (o|f¯, e¯) is estimated using recursive MAP smoothing as discussed in (Cherry, 2013). Training We compute P (o|f¯, e¯), which is the probability of an orientation given phrase pair of a rule, r.p = hf¯, e¯i, on word-aligned data using relative frequency. We assume that phrase e¯ spans the word range s . . . t in the target sentence and the phrase f¯ spans the range u . . . v in the source sentence. For a given phrase pair hf¯, e¯i, we set o = M if there is a phrase pair,hf¯0 , e¯0 i, where its target side, e¯0 , appears just before the target side of the given phrase, e¯, or s = t0 + 1, and its source side, f¯0 , also appears just before f¯, or u = v 0 + 1. Orientation is S i"
E17-2097,J07-2003,0,0.0708399,"sing decoding algorithm for Hiero (LR-Hiero) that visits input spans in arbitrary order and produces the translation in left to right (LR) order which leads to far fewer language model calls and leads to a considerable speedup in decoding. We introduce a novel shift-reduce algorithm to LR-Hiero to decode with our lexicalized reordering model (LRM) and show that it improves translation quality for Czech-English, Chinese-English and German-English. 1 Anoop Sarkar School of Computing Science Simon Fraser University Burnaby BC, Canada anoop@cs.sfu.ca Hierarchical phrase-based translation (Hiero) (Chiang, 2007) uses hierarchical phrases for translations represented as lexicalized synchronous context-free grammar (SCFG). Non-terminals in the SCFG rules correspond to gaps in phrases which are recursively filled by other rules (phrases). The SCFG rules are extracted from word and phrase alignments of a bitext. Hiero uses CKY-style decoding which parses the source sentence with time complexity O(n3 ) and synchronously generates the target sentence (translation). Watanabe et al. (2006) proposed a left-to-right (LR) decoding algorithm for Hiero (LR-Hiero) which follows the Earley (Earley, 1970) algorithm"
E17-2097,P13-1156,0,0.0215933,"source sentence. We only define the left-to-right case here; the right-to-left case (f¯ai+1 ) is symmetrical. The probability of an orientation given a phrase pair hf¯, e¯i can be estimated using relative frequency: condition . 4 )⟨ 状况 /condition⟩ 5 )⟨ ./.⟩ Figure 2: A word-aligned Chinese-English sentence pair on the top (from devset data used in experiments.) The sourcetarget phrase pairs created by removing the non-terminals from the rules used in decoding (Fig. 1) are shown on the bottom. significantly improves translation quality in LRHiero and improves Hiero results to a lesser extent. Nguyen and Vogel (2013) integrate phrase-based distortion and lexicalized reordering features with CKY-based Hiero decoder which significantly improve the translation quality. In their approach, each partial hypothesis during decoding is mapped into a sequence of phrase-pairs then the distortion and reordering features are computed similar to phrase-based MT. They use a LRM trained for phrase-based MT (Galley and Manning, 2010) which applies some restrictions on the Hiero rules. (Cao et al., 2014; Huck et al., 2013) propose different approaches to directly train LRM for Hiero rules. However, these approaches are des"
E17-2097,P11-2031,0,0.0358086,"o. The GNF rules are obtained from word and phrase aligned bitext using the rule extraction algorithm proposed by (Siahbani and Sarkar, 2014a). Table 3 compares the performance of different translation systems in terms of translation quality (BLEU). In all language pairs the proposed lexicalized reordering model improves the translation quality of LR-Hiero. These observations are comparable to the effect of LRM in phrase-based translation system. In Cs-En, LRM gets the best results and it significantly improves the the LR-Hiero results for De-En and Zh-En (p-value&lt;0.05, evaluated by MultEval (Clark et al., 2011)). To compare our approach to Nguyen and Vogel (2013), we adopt their algorithm to LR-Hiero and use the same LRM trained for GNF rules (marked as NVLRM in Table 3). Unsurprisingly this approach could not improve the translation quality in LRHiero. This approach computes the LRM for all candidate translation of each span after obtainAcknowledgments The authors wish to thank the anonymous reviewers for their helpful comments. The research was also partially supported by the Natural Sciences and Engineering Research Council of Canada (NSERC RGPIN 262313 and RGPAS 446348) to the second author. Ref"
E17-2097,J04-4002,0,0.120369,"rmal Form (GNF). Throughout this paper we abuse the notation for simplicity and use the term GNF grammars for such SCFGs. This leads to a single language model (LM) history for each hypothesis and speeds up decoding significantly, up to four times faster (Siahbani et al., 2013). Introduction Phrase-based machine translation handles reordering between source and target languages by visiting phrases in the source in arbitrary order while generating the target from left to right. A distortion penalty is used to penalize deviation from the monotone translation (no reordering) (Koehn et al., 2003; Och and Ney, 2004). Identical distortion penalties for different types of phrases ignore the fact that certain phrases (with certain words) were more likely to reorder than others. State-of-the-art phrase based translation systems address this issue by applying a lexicalized reordering model (LRM) (Tillmann, 2004; Koehn et al., 2007; Galley and Manning, 2008; Galley and Manning, 2010) which uses word aligned data to score phrase pair reordering. These models distinguish three orientations with respect to the previously translated phrase: monotone (M), swap (S), The Hiero translation model handles reordering ver"
E17-2097,P81-1022,0,0.554307,"n (Hiero) (Chiang, 2007) uses hierarchical phrases for translations represented as lexicalized synchronous context-free grammar (SCFG). Non-terminals in the SCFG rules correspond to gaps in phrases which are recursively filled by other rules (phrases). The SCFG rules are extracted from word and phrase alignments of a bitext. Hiero uses CKY-style decoding which parses the source sentence with time complexity O(n3 ) and synchronously generates the target sentence (translation). Watanabe et al. (2006) proposed a left-to-right (LR) decoding algorithm for Hiero (LR-Hiero) which follows the Earley (Earley, 1970) algorithm to parse the source sentence and synchronously generate the translation in a left-to-right manner. This algorithm is combined with beam search and has time complexity O(n2 b) where n is the length of source sentence and b is the size of beam (Huang and Mi, 2010). LR-Hiero constrains the SCFG rules to be prefix-lexicalized on the target side aka Greibach Normal Form (GNF). Throughout this paper we abuse the notation for simplicity and use the term GNF grammars for such SCFGs. This leads to a single language model (LM) history for each hypothesis and speeds up decoding significantly,"
E17-2097,D08-1089,0,0.0392255,"ppears just before the target side of the given phrase, e¯, or s = t0 + 1, and its source side, f¯0 , also appears just before f¯, or u = v 0 + 1. Orientation is S if there is a phrase pair, hf¯0 , e¯0 i, where e¯0 appears just before e¯, or s = t0 + 1, and f¯0 appears just after f¯, or v = u0 −1. Otherwise orientation is 2.2 Decoding Phrase-based LRM uses local information to determine orientation for a new phrase pair, hf¯ai , e¯i i, during decoding (Koehn et al., 2007; Tillmann, 2004). For left-to-right order, f¯ai is compared to the previously translated phrase f¯ai−1 . Galley and Manning (2008) introduce the hierarchical phrase 614 Figure 3 illustrates the application of shiftreduce approach to compute orientation for initial decoding steps of a Chinese-English sentence pair shown in Figure 4. We show source words in the rules with the corresponding index in the source sentence. S and ri .f¯ for the initial hypothesis are set to −1, corresponding to the start of sentence symbol, making it easy to compute the correct orientation for spans at the beginning of the input (with index 0). reordering model (HRM) which increases the consistency of orientation assignments. In HRM, the emphas"
E17-2097,P03-1021,0,0.0233517,"he source sentence. It makes LRM in LR-Hiero comparable to HRM in phrase-based MT. However, we cannot rely on the full translation history like HRM, since translation model is a SCFG grammar encoding reordering information. Experiments We evaluate lexicalized reordering model for LRHiero on three language pairs: German-English (De-En), Czech-English (Cs-En) and ChineseEnglish (Zh-En). Table 1 shows the corpus statistics for all language. We train a 5-gram LM on the Gigaword corpus using KenLM (Heafield, 2011). The weights in the log-linear model are tuned by minimizing BLEU loss through MERT (Och, 2003) on the dev set for each language pair and then report BLEU scores on the test set. Pop limit for Hiero and LR-Hiero is 500 and beam size for Moses is 1000. Other extraction and decoder settings such as maximum phrase length, etc. are identical across different settings. We use 3 baselines in our experiments: We employ a shift-reduce approach to find a compact representation of the recent translated source spans which is also represented by a stack, S, for each hypothesis. However, S always contains just one source span (which might be discontiguous), unlike HRM which maintains all previously"
E17-2097,N10-1140,0,0.0881072,"d target languages by visiting phrases in the source in arbitrary order while generating the target from left to right. A distortion penalty is used to penalize deviation from the monotone translation (no reordering) (Koehn et al., 2003; Och and Ney, 2004). Identical distortion penalties for different types of phrases ignore the fact that certain phrases (with certain words) were more likely to reorder than others. State-of-the-art phrase based translation systems address this issue by applying a lexicalized reordering model (LRM) (Tillmann, 2004; Koehn et al., 2007; Galley and Manning, 2008; Galley and Manning, 2010) which uses word aligned data to score phrase pair reordering. These models distinguish three orientations with respect to the previously translated phrase: monotone (M), swap (S), The Hiero translation model handles reordering very differently from a phrase-based model, through weighted translation rules (SCFGs) determined by non-terminal mappings. The rule X → hne X1 pas, do not X1 i indicates the translation of the phrase between ne and pas will be after the English phrase do not. However, reordering features can also be added to the Hiero log-linear translation model. Siahbani et al. (2013"
E17-2097,W12-3145,1,0.841452,"ri−1 .f¯. In Figure 3, rules #5,#4 are monotone, while both are covered by the current span in S. Since the stack always contains one span, this algorithm runs in O(1). Therefore, only a limited number of comparisons is used to update S and compute orientation. Unlike HRM which needs to maintain a sequence of contiguous spans in the stack and runs in linear time. • Hiero: we use our in-house implementation of Hiero, Kriya, in Python (Sankaran et al., 2012). Kriya can obtain statistically significantly equal BLEU scores when compared with Moses (Koehn et al., 2007) for several language pairs (Razmara et al., 2012; Callison-Burch et al., 2012). • phrase-based: Moses (Koehn et al., 2007) with and without lexicalized reordering features. • LR-Hiero: LR-Hiero decoding with cube pruning and queue diversity of 10 (Siahbani and Sarkar, 2014b). To make the results comparable we use the standard SMT features for log-linear model in translation systems. relative-frequency translation probabilities p(f |e) and p(e|f ), lexical translation probabilities pl (f |e) and pl (e|f ), a language model probability, word count, phrase count and distortion. In addition, two distortion features proposed 615 Corpus Cs-En Eur"
E17-2097,W11-2123,0,0.0495741,"lexicalized glue rules (Watanabe et al., 2006), non-terminals can be matched to very long spans on the source sentence. It makes LRM in LR-Hiero comparable to HRM in phrase-based MT. However, we cannot rely on the full translation history like HRM, since translation model is a SCFG grammar encoding reordering information. Experiments We evaluate lexicalized reordering model for LRHiero on three language pairs: German-English (De-En), Czech-English (Cs-En) and ChineseEnglish (Zh-En). Table 1 shows the corpus statistics for all language. We train a 5-gram LM on the Gigaword corpus using KenLM (Heafield, 2011). The weights in the log-linear model are tuned by minimizing BLEU loss through MERT (Och, 2003) on the dev set for each language pair and then report BLEU scores on the test set. Pop limit for Hiero and LR-Hiero is 500 and beam size for Moses is 1000. Other extraction and decoder settings such as maximum phrase length, etc. are identical across different settings. We use 3 baselines in our experiments: We employ a shift-reduce approach to find a compact representation of the recent translated source spans which is also represented by a stack, S, for each hypothesis. However, S always contains"
E17-2097,2014.amta-researchers.1,1,0.89521,"used to update S and compute orientation. Unlike HRM which needs to maintain a sequence of contiguous spans in the stack and runs in linear time. • Hiero: we use our in-house implementation of Hiero, Kriya, in Python (Sankaran et al., 2012). Kriya can obtain statistically significantly equal BLEU scores when compared with Moses (Koehn et al., 2007) for several language pairs (Razmara et al., 2012; Callison-Burch et al., 2012). • phrase-based: Moses (Koehn et al., 2007) with and without lexicalized reordering features. • LR-Hiero: LR-Hiero decoding with cube pruning and queue diversity of 10 (Siahbani and Sarkar, 2014b). To make the results comparable we use the standard SMT features for log-linear model in translation systems. relative-frequency translation probabilities p(f |e) and p(e|f ), lexical translation probabilities pl (f |e) and pl (e|f ), a language model probability, word count, phrase count and distortion. In addition, two distortion features proposed 615 Corpus Cs-En Europarl.v7; CzEng.v0.9; News commentary(nc) 2008,2009,2011 De-En Europarl.v7; WMT2006 Zh-En HK + GALE ph1; MTC 1,3,4 Train/Dev/Test 7.95M/3000/3003 1.5M/2000/2000 2.3M/1928/919 Table 1: Corpus statistics in number of sentences."
E17-2097,D10-1027,0,0.0208888,"are extracted from word and phrase alignments of a bitext. Hiero uses CKY-style decoding which parses the source sentence with time complexity O(n3 ) and synchronously generates the target sentence (translation). Watanabe et al. (2006) proposed a left-to-right (LR) decoding algorithm for Hiero (LR-Hiero) which follows the Earley (Earley, 1970) algorithm to parse the source sentence and synchronously generate the translation in a left-to-right manner. This algorithm is combined with beam search and has time complexity O(n2 b) where n is the length of source sentence and b is the size of beam (Huang and Mi, 2010). LR-Hiero constrains the SCFG rules to be prefix-lexicalized on the target side aka Greibach Normal Form (GNF). Throughout this paper we abuse the notation for simplicity and use the term GNF grammars for such SCFGs. This leads to a single language model (LM) history for each hypothesis and speeds up decoding significantly, up to four times faster (Siahbani et al., 2013). Introduction Phrase-based machine translation handles reordering between source and target languages by visiting phrases in the source in arbitrary order while generating the target from left to right. A distortion penalty i"
E17-2097,D14-1028,1,0.798836,"used to update S and compute orientation. Unlike HRM which needs to maintain a sequence of contiguous spans in the stack and runs in linear time. • Hiero: we use our in-house implementation of Hiero, Kriya, in Python (Sankaran et al., 2012). Kriya can obtain statistically significantly equal BLEU scores when compared with Moses (Koehn et al., 2007) for several language pairs (Razmara et al., 2012; Callison-Burch et al., 2012). • phrase-based: Moses (Koehn et al., 2007) with and without lexicalized reordering features. • LR-Hiero: LR-Hiero decoding with cube pruning and queue diversity of 10 (Siahbani and Sarkar, 2014b). To make the results comparable we use the standard SMT features for log-linear model in translation systems. relative-frequency translation probabilities p(f |e) and p(e|f ), lexical translation probabilities pl (f |e) and pl (e|f ), a language model probability, word count, phrase count and distortion. In addition, two distortion features proposed 615 Corpus Cs-En Europarl.v7; CzEng.v0.9; News commentary(nc) 2008,2009,2011 De-En Europarl.v7; WMT2006 Zh-En HK + GALE ph1; MTC 1,3,4 Train/Dev/Test 7.95M/3000/3003 1.5M/2000/2000 2.3M/1928/919 Table 1: Corpus statistics in number of sentences."
E17-2097,W13-2258,0,0.0179724,"ificantly improves translation quality in LRHiero and improves Hiero results to a lesser extent. Nguyen and Vogel (2013) integrate phrase-based distortion and lexicalized reordering features with CKY-based Hiero decoder which significantly improve the translation quality. In their approach, each partial hypothesis during decoding is mapped into a sequence of phrase-pairs then the distortion and reordering features are computed similar to phrase-based MT. They use a LRM trained for phrase-based MT (Galley and Manning, 2010) which applies some restrictions on the Hiero rules. (Cao et al., 2014; Huck et al., 2013) propose different approaches to directly train LRM for Hiero rules. However, these approaches are designed for CKY-decoding and cannot be directly used or adapted for LR-Hiero decoding which uses an Earley-style parsing algorithm. The crucial difference is the nature of bottom-up versus left to right decisions for lexicalized reordering and generating the translation in left-to-right manner. In this paper, we introduce a novel shift-reduce algorithm to learn a lexicalized reordering model (LRM) for LR-Hiero. We show that augmenting LR-Hiero with an LRM improves translation quality for Czech-E"
E17-2097,D13-1110,1,0.699016,"ource sentence and synchronously generate the translation in a left-to-right manner. This algorithm is combined with beam search and has time complexity O(n2 b) where n is the length of source sentence and b is the size of beam (Huang and Mi, 2010). LR-Hiero constrains the SCFG rules to be prefix-lexicalized on the target side aka Greibach Normal Form (GNF). Throughout this paper we abuse the notation for simplicity and use the term GNF grammars for such SCFGs. This leads to a single language model (LM) history for each hypothesis and speeds up decoding significantly, up to four times faster (Siahbani et al., 2013). Introduction Phrase-based machine translation handles reordering between source and target languages by visiting phrases in the source in arbitrary order while generating the target from left to right. A distortion penalty is used to penalize deviation from the monotone translation (no reordering) (Koehn et al., 2003; Och and Ney, 2004). Identical distortion penalties for different types of phrases ignore the fact that certain phrases (with certain words) were more likely to reorder than others. State-of-the-art phrase based translation systems address this issue by applying a lexicalized re"
E17-2097,N04-4026,0,0.396765,"based machine translation handles reordering between source and target languages by visiting phrases in the source in arbitrary order while generating the target from left to right. A distortion penalty is used to penalize deviation from the monotone translation (no reordering) (Koehn et al., 2003; Och and Ney, 2004). Identical distortion penalties for different types of phrases ignore the fact that certain phrases (with certain words) were more likely to reorder than others. State-of-the-art phrase based translation systems address this issue by applying a lexicalized reordering model (LRM) (Tillmann, 2004; Koehn et al., 2007; Galley and Manning, 2008; Galley and Manning, 2010) which uses word aligned data to score phrase pair reordering. These models distinguish three orientations with respect to the previously translated phrase: monotone (M), swap (S), The Hiero translation model handles reordering very differently from a phrase-based model, through weighted translation rules (SCFGs) determined by non-terminal mappings. The rule X → hne X1 pas, do not X1 i indicates the translation of the phrase between ne and pas will be after the English phrase do not. However, reordering features can also"
E17-2097,P06-1098,0,0.125405,"ol of Computing Science Simon Fraser University Burnaby BC, Canada anoop@cs.sfu.ca Hierarchical phrase-based translation (Hiero) (Chiang, 2007) uses hierarchical phrases for translations represented as lexicalized synchronous context-free grammar (SCFG). Non-terminals in the SCFG rules correspond to gaps in phrases which are recursively filled by other rules (phrases). The SCFG rules are extracted from word and phrase alignments of a bitext. Hiero uses CKY-style decoding which parses the source sentence with time complexity O(n3 ) and synchronously generates the target sentence (translation). Watanabe et al. (2006) proposed a left-to-right (LR) decoding algorithm for Hiero (LR-Hiero) which follows the Earley (Earley, 1970) algorithm to parse the source sentence and synchronously generate the translation in a left-to-right manner. This algorithm is combined with beam search and has time complexity O(n2 b) where n is the length of source sentence and b is the size of beam (Huang and Mi, 2010). LR-Hiero constrains the SCFG rules to be prefix-lexicalized on the target side aka Greibach Normal Form (GNF). Throughout this paper we abuse the notation for simplicity and use the term GNF grammars for such SCFGs."
I08-4025,W02-1001,0,0.0706473,"CRF tagger could be improved with the use of other tagsets. However, this does not affect our comparative experiments in this paper. 144 Inputs: Training Data h(x1 , y1 ), . . . , (xm , ym )i Initialization: Set w = 0 Algorithm: for t = 1, . . . , T do for i = 1, . . . , m do 0 Calculate yi , where 0 yi = argmax Φ(y) · w y∈N-best Candidates 0 if yi 6= y b then 0 w = w + Φ(y b ) − Φ(yi ) end if end for end for Figure 2: Training using a perceptron algorithm over N-best candidates. perceptron. However, due to the computational issues with the voted perceptron, the averaged perceptron algorithm (Collins, 2002a) is used instead. Rather than using w, we use the averaged weight parameter γ over the m training examples for future predictions on unseen data: X 1 γ= wi,t mT i=1..m,t=1..T In calculating γ, an accumulating parameter vector σ i,t is maintained and updated using P wi,tfor each training example; therefore, σ i,t = w . After i,t the last iteration, σ /mT produces the final parameter vector γ. When the number of features is large, it is time consuming to calculate the total parameter σ i,t for each training example. To reduce the time complexity, we adapted the lazy update proposed in (Collins"
I08-4025,D07-1033,0,0.0222854,"l features) Significance (p-value) ≤ 1.19e-12 ≤ 4.43e-69 ≤ 3.55e-88 ≤ 2.17e-18 Table 3: F-scores on the Fourth SIGHAN Bakeoff Corpora was too large. By lowering the weight from 100 to 4, we obtains an F-score of 0.9354, which is significantly better than the baseline CRF tagger. The significance values in Table 3 were produced using the McNemar’s Test (Gillick, 1989)5 . All our results are significantly better. 4 Related Work Re-ranking over N-best lists has been applied to so many tasks in natural language that it is not possible to list them all here. Closest to our approach is the work in (Kazama and Torisawa, 2007). They proposed a margin perceptron approach for named entity recognition with non-local features on an Nbest list. In contrast to their approach, in our system, global features examine the entire sentence instead of partial phrases. For word segmentation, (Wang and Shi, 2006) implemented a re-ranking method with POS tagging features. In their approach, character-based CRF model produces the Nbest list for each test sentence. The Penn Chinese TreeBank is used to train a POS tagger, which is used in re-ranking. However the POS tags are used as local and not global features. Note that we would n"
I08-4025,W04-3230,0,0.10699,"Missing"
I08-4025,W96-0213,0,0.25052,"Missing"
I08-4025,P07-1106,0,0.0248957,"ing w, we use the averaged weight parameter γ over the m training examples for future predictions on unseen data: X 1 γ= wi,t mT i=1..m,t=1..T In calculating γ, an accumulating parameter vector σ i,t is maintained and updated using P wi,tfor each training example; therefore, σ i,t = w . After i,t the last iteration, σ /mT produces the final parameter vector γ. When the number of features is large, it is time consuming to calculate the total parameter σ i,t for each training example. To reduce the time complexity, we adapted the lazy update proposed in (Collins, 2002b), which was also used in (Zhang and Clark, 2007). After processing each training sentence, not all dimensions of σ i,t are updated. Instead, an update vector τ is used to store the exact location (i, t) where each dimension of the averaged parameter vector was last updated, and only those dimensions corresponding to features appearing in the current sentence are updated. While for the last example in the last iteration, each dimension of τ is updated, no matter whether the candidate output is correct. 2.2 Feature Templates The feature templates used in our system include both local features and global features. For local features, we consid"
I13-1029,2008.iwslt-papers.1,0,0.0253921,"d German as the source and target languages and English as the only pivot language. They showed that phrase-table triangulation is superior to the MT system cascades but both of them did not outperform the direct src  tgt system. The phrase-table triangulation approach with multiple pivot languages has been also investigated in several work (Cohn and Lapata, 2007; Wu and Wang, 2007). These triangulated phrasetables are combined together using linear and loglinear mixture models. They also successfully combined the mixed phrase-table with a src-tgt phrase-table to achieve a higher BLEU score. Bertoldi et al. (2008) formulated phrase triangulation in the decoder where they also consider the phrase-segmentation model between src-pvt and the reordering model between src-tgt. Beside machine translation, the use of pivot languages has found applications in other NLP areas. Gollins and Sanderson (2001) used a similar idea in cross-lingual information retrieval where query terms were translated through multiple pivot languages to the target language and the translations are combined to reduce the error. Pivot languages have also been successfully used in inducing translation lexicons (Mann and Yarowsky, 2001)"
I13-1029,N06-1003,0,0.0343456,"n model between src-pvt and the reordering model between src-tgt. Beside machine translation, the use of pivot languages has found applications in other NLP areas. Gollins and Sanderson (2001) used a similar idea in cross-lingual information retrieval where query terms were translated through multiple pivot languages to the target language and the translations are combined to reduce the error. Pivot languages have also been successfully used in inducing translation lexicons (Mann and Yarowsky, 2001) as well as word alignments for resourcepoor languages (Kumar et al., 2007; Wang et al., 2006). Callison-Burch et al. (2006) used pivot languages to extract paraphrases for unknown words. 3 Baselines In this paper, we compare our approach with two baselines. A simple baseline is the direct system between the source and target languages which is trained on the same amount of parallel data as the triangulated ones. In addition, we implemented a phrase-table triangulation method (Cohn and Lapata, 2007; Wu and Wang, 2007; Utiyama and Isahara, 2007). This approach presents a probabilistic formulation for triangulation by marginalizing out the pivot phrases, and factorizing using the chain rule: X p(¯ e |f¯) = p(¯ e, ¯i"
I13-1029,P05-1033,0,0.0612917,"right languages as pivot languages. Thus, we needed to run experiments on a large number of language pairs, and for each language pair we wanted to work with many pivot languages. To this end, we created small sub-corpora from Europarl by sampling 10,000 sentence pairs and conducted our experiments on them. As we will show, using larger data than this would result in prohibitively large triangulated phrase tables. Table 2 shows the number of words on both sides of used language pairs in our corpora. The ensemble decoder is built on top of an inhouse implementation of a Hiero-style MT system (Chiang, 2005) called Kriya (Sankaran et al., 2012). This Hiero decoder obtains BLEU 1 256 http://www.statmt.org/wpt05/mt-shared-task/ de tgt → src↓ pivots de en fr it direct mixture wmax wsum switch es es 15.94 – 13.45 14.90 20.70 22.30 21.32 21.42 21.80 fr 13.62 13.43 – 11.67 17.37 18.28 18.22 17.98 17.70 de – 14.50 12.48 13.69 16.30 17.75 17.34 16.79 16.53 en 18.84 – 22.81 23.14 28.11 28.99 29.23 28.79 29.16 fr 23.28 18.55 – 23.44 29.83 29.47 30.54 30.12 29.68 tgt → src↓ de es fr it direct mixture wmax wsum switch pivots pivots en es fr it direct mixture wmax wsum switch en – 14.47 14.39 14.14 21.94 21.8"
I13-1029,P07-1092,0,0.503977,"n the source F, pivot I and target E languages respectively and T is a set representing a phrase table. Utiyama and Isahara (2007) also experimented with phrase-table triangulation. They compared both triangulation approaches when using Spanish, French and German as the source and target languages and English as the only pivot language. They showed that phrase-table triangulation is superior to the MT system cascades but both of them did not outperform the direct src  tgt system. The phrase-table triangulation approach with multiple pivot languages has been also investigated in several work (Cohn and Lapata, 2007; Wu and Wang, 2007). These triangulated phrasetables are combined together using linear and loglinear mixture models. They also successfully combined the mixed phrase-table with a src-tgt phrase-table to achieve a higher BLEU score. Bertoldi et al. (2008) formulated phrase triangulation in the decoder where they also consider the phrase-segmentation model between src-pvt and the reordering model between src-tgt. Beside machine translation, the use of pivot languages has found applications in other NLP areas. Gollins and Sanderson (2001) used a similar idea in cross-lingual information retriev"
I13-1029,W11-2107,0,0.0214377,"re of the direct system, mixture models and wmax on all 12 systems. On average the wmax method obtains 0.33 BLEU points higher than the mixture models. direct 6.3 23.82 mixture 24.27 ensemble (wmax) 22.5 and the results are summarized in Figure 4. As the figure illustrates, our ensemble decoding approach with wmax outperforms the mixture models in 11 of 12 language pairs based on Meteor scores. 24.6 23 23.5 24 24.5 BLEU Figure 3: The average BLEU scores of the direct system, mixture models and wmax ensemble triangulation approach over all 12 language pairs. We also computed the Meteor scores (Denkowski and Lavie, 2011) for all systems Phrase table coverage Figure 2 shows the phrase-table coverage of the test set for different language pairs. The coverage is defined as the percentage of unigrams in the source side of the test set for which the corresponding phrase-table has translations for. The first set of bars shows the coverage of the direct systems and the second one shows that of the combined triangulated systems for three pivot languages. Finally, the last set of bars indicate the coverage when the direct phrase-table is combined with the triangulated ones. In all language pairs, the combined triangul"
I13-1029,2005.mtsummit-papers.11,0,0.024641,"ompleted, these weights comprise the final tuned weights. Thus, the total number of ensemble evaluations reduces from O(n4 ) to O(3n). In addition to this significant complexity reduction, this method enables parallelism in tuning, since the three individual tuning branches can now be run independently. The final tuned weights are not necessarily a local optima and one can run further optimization steps around this point to get to even better solutions which should lead to higher BLEU scores. 6 Experiments & Results 6.1 Experimental Setup For our experiments, we used the Europarl corpus (v7) (Koehn, 2005) for training sets and ACL/WMT 20051 data for dev/test sets (2k sentence pairs) following Cohn and Lapata (2007). Our goal in this paper was to understand how multiple languages can help in triangulation, the improvement in coverage of the unseen data due to triangulation, and the importance of choosing the right languages as pivot languages. Thus, we needed to run experiments on a large number of language pairs, and for each language pair we wanted to work with many pivot languages. To this end, we created small sub-corpora from Europarl by sampling 10,000 sentence pairs and conducted our exp"
I13-1029,J10-4005,0,0.0290244,"Section 6.2). In linear mixture models, each feature in the mixture phrase-table is computed as a linear interpolation of corresponding features in the component phrase-tables using a weight vector ~λ. X p(¯ e |f¯) = λi pi (¯ e |f¯) i p(f¯ |e¯) = X λi pi (f¯ |e¯) i ∀ λi > 1 X λi = 1 i Following Cohn and Lapata (2007), we combined triangulated phrase-tables with uniform weights into a single phrase table and then interpolated it with the phrase-table of the direct system. 254 4 Ensemble Decoding for each cell, the model that has the highest weighted best-rule score wins: SMT log-linear models (Koehn, 2010) find the most likely target language output e given the source language input f using a vector of feature functions φ:   p(e|f ) ∝ exp w · φ Ensemble decoding combines several models dynamically at the decoding time. The scores are combined for each partial hypothesis using a user-defined mixture operation over component models.   p(e|f ) ∝ exp w1 · φ1 w2 · φ2 . . . Razmara et al. (2012) successfully applied ensemble decoding to domain adaptation in SMT and showed that it performed better than approaches that pre-compute linear mixtures of different models. Several mixture operations were"
I13-1029,D07-1005,0,0.0831137,"hey also consider the phrase-segmentation model between src-pvt and the reordering model between src-tgt. Beside machine translation, the use of pivot languages has found applications in other NLP areas. Gollins and Sanderson (2001) used a similar idea in cross-lingual information retrieval where query terms were translated through multiple pivot languages to the target language and the translations are combined to reduce the error. Pivot languages have also been successfully used in inducing translation lexicons (Mann and Yarowsky, 2001) as well as word alignments for resourcepoor languages (Kumar et al., 2007; Wang et al., 2006). Callison-Burch et al. (2006) used pivot languages to extract paraphrases for unknown words. 3 Baselines In this paper, we compare our approach with two baselines. A simple baseline is the direct system between the source and target languages which is trained on the same amount of parallel data as the triangulated ones. In addition, we implemented a phrase-table triangulation method (Cohn and Lapata, 2007; Wu and Wang, 2007; Utiyama and Isahara, 2007). This approach presents a probabilistic formulation for triangulation by marginalizing out the pivot phrases, and factorizi"
I13-1029,N01-1020,0,0.0579197,"e. Bertoldi et al. (2008) formulated phrase triangulation in the decoder where they also consider the phrase-segmentation model between src-pvt and the reordering model between src-tgt. Beside machine translation, the use of pivot languages has found applications in other NLP areas. Gollins and Sanderson (2001) used a similar idea in cross-lingual information retrieval where query terms were translated through multiple pivot languages to the target language and the translations are combined to reduce the error. Pivot languages have also been successfully used in inducing translation lexicons (Mann and Yarowsky, 2001) as well as word alignments for resourcepoor languages (Kumar et al., 2007; Wang et al., 2006). Callison-Burch et al. (2006) used pivot languages to extract paraphrases for unknown words. 3 Baselines In this paper, we compare our approach with two baselines. A simple baseline is the direct system between the source and target languages which is trained on the same amount of parallel data as the triangulated ones. In addition, we implemented a phrase-table triangulation method (Cohn and Lapata, 2007; Wu and Wang, 2007; Utiyama and Isahara, 2007). This approach presents a probabilistic formulati"
I13-1029,P00-1056,0,0.298216,"r corpora. L1 - L2 L1 tokens (K) L2 tokens (K) de - en de - es de - fr de - it en - es en - fr en - it es - fr es - it fr - it 232 232 231 245 250 251 260 262 274 272 249 263 259 253 264 262 251 261 252 251 Table 2: Number of tokens in each language pair in the training data. scores equal to or better than the state-of-the-art in phrase-based and hierarchical phrase-based translation over a wide variety of language pairs and data sets. It uses the following standard features: forward and backward relative-frequency and lexical TM probabilities; LM; word, phrase and glue-rules penalty. GIZA++ (Och and Ney, 2000) has been used for word alignment with phrase length limit of 10. In both systems, feature weights were optimized using MERT (Och, 2003). We used the target sides of the Europarl corpus (2M sentences) to build 5-gram language models and smooth them using the Kneser-Ney method. We used SRILM (Stolcke, 2002) as the language model toolkit. 6.2 Results Table 1 shows the BLEU scores when using two languages from {fr, en, es, de} as source and target, and the other two languages plus it as intermediate languages. The first group of numbers are BLEU scores for triangulated systems through the specifi"
I13-1029,P03-1021,0,0.0465535,"Missing"
I13-1029,P12-1099,1,0.803127,"single phrase table and then interpolated it with the phrase-table of the direct system. 254 4 Ensemble Decoding for each cell, the model that has the highest weighted best-rule score wins: SMT log-linear models (Koehn, 2010) find the most likely target language output e given the source language input f using a vector of feature functions φ:   p(e|f ) ∝ exp w · φ Ensemble decoding combines several models dynamically at the decoding time. The scores are combined for each partial hypothesis using a user-defined mixture operation over component models.   p(e|f ) ∝ exp w1 · φ1 w2 · φ2 . . . Razmara et al. (2012) successfully applied ensemble decoding to domain adaptation in SMT and showed that it performed better than approaches that pre-compute linear mixtures of different models. Several mixture operations were proposed, allowing the user to encode belief about the relative strengths of the component models. These mixture operations receive two or more probabilities and return the mixture probability p(¯ e |f¯) for each rule f¯ → e¯ used in the decoder. Different options for these operations are: M X λm exp wm · φm e The probability of each phrase-pair (¯ e, f¯) is then: M X p(¯ e |f¯) = δ(f¯, m) p"
I13-1029,C88-2125,0,0.513393,"riginal src  tgt system is a good idea. When combining multiple systems, the upper bound on the number of OOVs is the minimum among all OOVs in the different triangulations. These OOV rates provide useful hints, among other clues, as to which pivot 252 International Joint Conference on Natural Language Processing, pages 252–260, Nagoya, Japan, 14-18 October 2013. Related Work Use of pivot languages in machine translation dates back to the early days of machine translation. Boitet (1988) discusses the choice of pivot languages, natural or artificial (e.g. interlingua), in machine translation. Schubert (1988) argues that a proper choice for an intermediate language for high-quality machine translation is a natural language due to the inherent lack of expressiveness in artificial languages. Previous work in applying pivot languages in machine translation can be categorized into these divisions: 2.1 System Cascades In this approach, a src  pvt translation system translates the source input into the pivot language and a second pvt  tgt system takes the output of the previous system and translates it into the target language. Utiyama and Isahara (2007) use this es 25 77 4370 35 de 01 2 3339 32 langu"
I13-1029,N07-1061,0,0.541619,"or artificial (e.g. interlingua), in machine translation. Schubert (1988) argues that a proper choice for an intermediate language for high-quality machine translation is a natural language due to the inherent lack of expressiveness in artificial languages. Previous work in applying pivot languages in machine translation can be categorized into these divisions: 2.1 System Cascades In this approach, a src  pvt translation system translates the source input into the pivot language and a second pvt  tgt system takes the output of the previous system and translates it into the target language. Utiyama and Isahara (2007) use this es 25 77 4370 35 de 01 2 3339 32 languages will be more useful. In Figure 1, we can expect Italian (it) to help more than Spanish (es) and both to help more than German (de) in translation from French (fr) to English (en), which we confirmed in our experimental results (Table 1). In addition to providing translations for otherwise untranslatable phrases, triangulation can find new translations for current phrases. The conditional distributions used for the translation model have been estimated on small amounts of data and hence are not robust due to data sparseness. Using triangulati"
I13-1029,P06-2112,0,0.143478,"e phrase-segmentation model between src-pvt and the reordering model between src-tgt. Beside machine translation, the use of pivot languages has found applications in other NLP areas. Gollins and Sanderson (2001) used a similar idea in cross-lingual information retrieval where query terms were translated through multiple pivot languages to the target language and the translations are combined to reduce the error. Pivot languages have also been successfully used in inducing translation lexicons (Mann and Yarowsky, 2001) as well as word alignments for resourcepoor languages (Kumar et al., 2007; Wang et al., 2006). Callison-Burch et al. (2006) used pivot languages to extract paraphrases for unknown words. 3 Baselines In this paper, we compare our approach with two baselines. A simple baseline is the direct system between the source and target languages which is trained on the same amount of parallel data as the triangulated ones. In addition, we implemented a phrase-table triangulation method (Cohn and Lapata, 2007; Wu and Wang, 2007; Utiyama and Isahara, 2007). This approach presents a probabilistic formulation for triangulation by marginalizing out the pivot phrases, and factorizing using the chain r"
I13-1029,P07-1108,0,0.101847,"and target E languages respectively and T is a set representing a phrase table. Utiyama and Isahara (2007) also experimented with phrase-table triangulation. They compared both triangulation approaches when using Spanish, French and German as the source and target languages and English as the only pivot language. They showed that phrase-table triangulation is superior to the MT system cascades but both of them did not outperform the direct src  tgt system. The phrase-table triangulation approach with multiple pivot languages has been also investigated in several work (Cohn and Lapata, 2007; Wu and Wang, 2007). These triangulated phrasetables are combined together using linear and loglinear mixture models. They also successfully combined the mixed phrase-table with a src-tgt phrase-table to achieve a higher BLEU score. Bertoldi et al. (2008) formulated phrase triangulation in the decoder where they also consider the phrase-segmentation model between src-pvt and the reordering model between src-tgt. Beside machine translation, the use of pivot languages has found applications in other NLP areas. Gollins and Sanderson (2001) used a similar idea in cross-lingual information retrieval where query terms"
I13-1029,D08-1076,0,\N,Missing
I13-1050,J04-4002,0,0.0803904,"Canada anoop@cs.sfu.ca X → hβp Xk βs , γp Xk γs i (1) Here βp (βs ) refers to any prefix (suffix) of β that precedes (follows) fij . Note that the non-terminals are co-indexed with a unique index so that they are rewritten simultaneously. 数 月 months Introduction , 联合国 , 难民 the 专员 公署 unhcr Figure 1: Chinese-English phrase-pair with alignments Hierarchical phrase-based translation (Hiero) as described in (Chiang, 2005; Chiang, 2007) uses a synchronous context-free grammar (SCFG) derived from heuristically extracted phrase pairs obtained by symmetrizing bidirectional many-tomany word alignments (Och and Ney, 2004). The phrase-pairs are constrained by the source-target alignments such that all the alignment links from the source (target) words are connected to the target (source) words within the phrase. Given a word-aligned sentence pair hf1J , eI1 , Ai, where A indicate the alignments, the source-target se0 quence pair hfij , eji0 i can be a phrase-pair iff the following alignment constraint is satisfied. As a concrete example, consider the word aligned Chinese-English phrase pair shown in Figure 1. Notice that the phrase 联合国 (united nations) is incorrectly aligned to English determiner the, even thou"
I13-1050,W99-0604,0,0.143945,"tistics and then communicates the statistics to a central aggregator reduce node. Distributed inference for Expectation Maximization algorithm was studied in (Wolfe et al., 2008). They used three different topologies in 4 Experiments Training Corpus URochester data ISI Ar-En corpus HK + GALE ph-1 Train/ Tune/ Test 59218/ 1118/ 1118 1.1 M/ 1982/ 987 2.3 M/ 1928/ 919 Table 1: Corpus Statistics in # of sentences We follow the standard MT practice and use GIZA++ (Och and Ney, 2003) for word aligning the parallel corpus. We then use the heuristic step that symmetrizes the bidirectional alignments (Och et al., 1999) to extract the initial phrase-pairs up to a certain length, consistent with the word alignments. Finally we employ our proposed Variational-Bayes training to learn rules for 5 We simulate the Map-Reduce style of computation using a regular high-performance cluster using a mounted filesystem rather than a Hadoop cluster with a distributed filesystem. In our experiments, we set the number of iterations to 10. 441 Model Baseline Variational-Bayes Ko-En 7.18 7.68 Ar-En 37.82 37.76 Cn-En 28.58 28.40 a simple threshold pruning strategy on the grammar learned from our proposed model. Table 3 shows t"
I13-1050,P09-1088,0,0.0771872,"BLEU BLEU Model size (in Millions) Terminal 23.3 27.9 38.4 27.2 27 26.8 VB-‐Pr (1.0) Figure 6: Cn-En: Model sizes and B LEU for different grammars. The pruned models are identified by the suffix ’Pr’, whose mincount is shown in the brackets. The y-axis on the left marks the model sizes and that on the right denotes B LEU. The numbers in the stacked bars denote the # of rules (in millions) for the corresponding rule type. 444 Most of the research on learning Hiero SCFG rules has been focussed on inducing phrasal alignments between source and target using Bayesian models (Blunsom et al., 2008; Blunsom et al., 2009; Levenberg et al., 2012; Cohn and Haffari, 2013). Broadly speaking, these generative approaches learn a posterior over parallel tree structures on the sentence pairs. While these methods extract hierarchical rules, they do not conform to Hierostyle rules. Consequently the hierarchical rules are used only for learning an alignment model and cannot be used directly in the Hiero decoder. Instead, these approaches employ the standard Hiero heuristics to extract rules to be used by the decoder from the alignments predicted by their model. In this sense, these are similar to Bayesian models for lea"
I13-1050,N10-1050,0,0.0475371,"Missing"
I13-1050,P05-1033,0,0.0912779,"bic and Chinese into English demonstrate that they are able to exceed or retain the performance of baseline hierarchical phrase-based models. 1 Anoop Sarkar Simon Fraser University Burnaby BC. Canada anoop@cs.sfu.ca X → hβp Xk βs , γp Xk γs i (1) Here βp (βs ) refers to any prefix (suffix) of β that precedes (follows) fij . Note that the non-terminals are co-indexed with a unique index so that they are rewritten simultaneously. 数 月 months Introduction , 联合国 , 难民 the 专员 公署 unhcr Figure 1: Chinese-English phrase-pair with alignments Hierarchical phrase-based translation (Hiero) as described in (Chiang, 2005; Chiang, 2007) uses a synchronous context-free grammar (SCFG) derived from heuristically extracted phrase pairs obtained by symmetrizing bidirectional many-tomany word alignments (Och and Ney, 2004). The phrase-pairs are constrained by the source-target alignments such that all the alignment links from the source (target) words are connected to the target (source) words within the phrase. Given a word-aligned sentence pair hf1J , eI1 , Ai, where A indicate the alignments, the source-target se0 quence pair hfij , eji0 i can be a phrase-pair iff the following alignment constraint is satisfied."
I13-1050,W11-2167,1,0.850112,"Rochester KoreanEnglish dataset consisting of almost 60K sentence pairs for the small data setting. For moderate and large datasets we use Arabic-English (ISI parallel corpus) and Chinese-English (Hong Kong parallel text and GALE phase-1) corpora. We use the MTC dataset having 4 references for tuning and testing for our Chinese-English experiments. The statistics of the corpora used in our experiments are summarized in Table 1. We run inference for a fixed number of iterations4 and use the grammar along with their posterior counts from the last iteration for the translation table. Following (Sankaran et al., 2011), we use the shift-reduce style algorithm to efficiently encode the word aligned phrase-pair as a normalized decomposition tree (Zhang et al., 2008). The possible derivations (that are consistent with the word alignments) could then be enumerated by simply traversing every node in the decomposition tree and replacing its span by a non-terminal X. 3.1 Lang. Ko-En Ar-En Cn-En Distributing Inference While the above training procedure works well for smaller datasets, it does not scale well for the realistic MT datasets (which have millions of sentence pairs) due to greater memory and time requirem"
I13-1050,J07-2003,0,0.869994,"y co-indexed non-terminals and rewriting the replaced source-target word sequences as separate rules. Consider a rule X → hβ, γi, where β and γ are sequences of terminals and non-terminals. Now, given another rule 0 0 X → hfij , eji0 i, such that fij and eji0 are contained fully within β and γ as sub-phrases, the larger rule could be rewritten to create a new rule. We present a Variational-Bayes model for learning rules for the Hierarchical phrasebased model directly from the phrasal alignments. Our model is an alternative to heuristic rule extraction in hierarchical phrase-based translation (Chiang, 2007), which uniformly distributes the probability mass to the extracted rules locally. In contrast, in our approach the probability assigned to a rule is globally determined by its contribution towards all phrase pairs and results in a sparser rule set. We also propose a distributed framework for efficiently running inference for realistic MT corpora. Our experiments translating Korean, Arabic and Chinese into English demonstrate that they are able to exceed or retain the performance of baseline hierarchical phrase-based models. 1 Anoop Sarkar Simon Fraser University Burnaby BC. Canada anoop@cs.sf"
I13-1050,2012.amta-papers.16,1,0.721088,"Missing"
I13-1050,P13-1077,1,0.834378,".3 27.9 38.4 27.2 27 26.8 VB-‐Pr (1.0) Figure 6: Cn-En: Model sizes and B LEU for different grammars. The pruned models are identified by the suffix ’Pr’, whose mincount is shown in the brackets. The y-axis on the left marks the model sizes and that on the right denotes B LEU. The numbers in the stacked bars denote the # of rules (in millions) for the corresponding rule type. 444 Most of the research on learning Hiero SCFG rules has been focussed on inducing phrasal alignments between source and target using Bayesian models (Blunsom et al., 2008; Blunsom et al., 2009; Levenberg et al., 2012; Cohn and Haffari, 2013). Broadly speaking, these generative approaches learn a posterior over parallel tree structures on the sentence pairs. While these methods extract hierarchical rules, they do not conform to Hierostyle rules. Consequently the hierarchical rules are used only for learning an alignment model and cannot be used directly in the Hiero decoder. Instead, these approaches employ the standard Hiero heuristics to extract rules to be used by the decoder from the alignments predicted by their model. In this sense, these are similar to Bayesian models for learning alignments using stochastic Inversion trans"
I13-1050,P06-1121,0,0.191541,"Missing"
I13-1050,P09-1037,0,0.0302037,"Missing"
I13-1050,E09-1044,0,0.0434264,"Missing"
I13-1050,J97-3002,0,0.266872,"Missing"
I13-1050,D07-1103,0,0.0891106,"Missing"
I13-1050,P09-2060,0,0.0460073,"Missing"
I13-1050,D12-1021,0,0.1033,"in Millions) Terminal 23.3 27.9 38.4 27.2 27 26.8 VB-‐Pr (1.0) Figure 6: Cn-En: Model sizes and B LEU for different grammars. The pruned models are identified by the suffix ’Pr’, whose mincount is shown in the brackets. The y-axis on the left marks the model sizes and that on the right denotes B LEU. The numbers in the stacked bars denote the # of rules (in millions) for the corresponding rule type. 444 Most of the research on learning Hiero SCFG rules has been focussed on inducing phrasal alignments between source and target using Bayesian models (Blunsom et al., 2008; Blunsom et al., 2009; Levenberg et al., 2012; Cohn and Haffari, 2013). Broadly speaking, these generative approaches learn a posterior over parallel tree structures on the sentence pairs. While these methods extract hierarchical rules, they do not conform to Hierostyle rules. Consequently the hierarchical rules are used only for learning an alignment model and cannot be used directly in the Hiero decoder. Instead, these approaches employ the standard Hiero heuristics to extract rules to be used by the decoder from the alignments predicted by their model. In this sense, these are similar to Bayesian models for learning alignments using s"
I13-1050,C08-1136,0,0.0202451,"ish (ISI parallel corpus) and Chinese-English (Hong Kong parallel text and GALE phase-1) corpora. We use the MTC dataset having 4 references for tuning and testing for our Chinese-English experiments. The statistics of the corpora used in our experiments are summarized in Table 1. We run inference for a fixed number of iterations4 and use the grammar along with their posterior counts from the last iteration for the translation table. Following (Sankaran et al., 2011), we use the shift-reduce style algorithm to efficiently encode the word aligned phrase-pair as a normalized decomposition tree (Zhang et al., 2008). The possible derivations (that are consistent with the word alignments) could then be enumerated by simply traversing every node in the decomposition tree and replacing its span by a non-terminal X. 3.1 Lang. Ko-En Ar-En Cn-En Distributing Inference While the above training procedure works well for smaller datasets, it does not scale well for the realistic MT datasets (which have millions of sentence pairs) due to greater memory and time requirements. To address this shortcoming, we distribute the training using a Map-Reduce style framework, where each node works on the local dataset in comp"
I13-1050,C08-1144,0,0.0496567,"Missing"
I13-1050,J03-1002,0,0.0106887,"we distribute the training using a Map-Reduce style framework, where each node works on the local dataset in computing the required statistics and then communicates the statistics to a central aggregator reduce node. Distributed inference for Expectation Maximization algorithm was studied in (Wolfe et al., 2008). They used three different topologies in 4 Experiments Training Corpus URochester data ISI Ar-En corpus HK + GALE ph-1 Train/ Tune/ Test 59218/ 1118/ 1118 1.1 M/ 1982/ 987 2.3 M/ 1928/ 919 Table 1: Corpus Statistics in # of sentences We follow the standard MT practice and use GIZA++ (Och and Ney, 2003) for word aligning the parallel corpus. We then use the heuristic step that symmetrizes the bidirectional alignments (Och et al., 1999) to extract the initial phrase-pairs up to a certain length, consistent with the word alignments. Finally we employ our proposed Variational-Bayes training to learn rules for 5 We simulate the Map-Reduce style of computation using a regular high-performance cluster using a mounted filesystem rather than a Hadoop cluster with a distributed filesystem. In our experiments, we set the number of iterations to 10. 441 Model Baseline Variational-Bayes Ko-En 7.18 7.68"
I13-1149,N09-1034,0,0.0158231,"ltiple views significantly outperforms batch learning for latent representations with a single view on a grammaticality prediction task. 1 Introduction Natural language data is implicitly richly structured, and making use of that structure can be valuable in a wide variety of NLP tasks. However, finding these latent structures is a complex task of its own right. Early work used a twophase pipeline process, in which the output of a structure prediction algorithm (e.g. a noun phrase finder) acts as fixed input features to train a classifier for a different task (e.g. grammaticality prediction). Chang et al. (2009), Das and Smith (2009), Goldwasser and Roth (2008), and Mccallum and Bellare (2005) have shown that this approach can propagate error from the structured prediction to the task-specific classifier. Recent work has combined unsupervised learning of (latent) structure prediction with a supervised learning approach for the task. Work in this vein has focused on jointly ∗ This research was partially supported by an NSERC, Canada (RGPIN: 264905) grant and a Google Faculty Award to the third author. learning the latent structures together with the task-specific classifier (Cherry and Quirk, 2008; Ch"
I13-1149,N10-1066,0,0.348119,"9), Das and Smith (2009), Goldwasser and Roth (2008), and Mccallum and Bellare (2005) have shown that this approach can propagate error from the structured prediction to the task-specific classifier. Recent work has combined unsupervised learning of (latent) structure prediction with a supervised learning approach for the task. Work in this vein has focused on jointly ∗ This research was partially supported by an NSERC, Canada (RGPIN: 264905) grant and a Google Faculty Award to the third author. learning the latent structures together with the task-specific classifier (Cherry and Quirk, 2008; Chang et al., 2010). Chang et al. (2010) in particular introduce a framework for solving classification problems using constraints over latent structures, referred to as Learning over Constrained Latent Representations (LCLR). We extend this framework for discriminative joint learning over latent structures to a novel online algorithm. Our algorithm learns the latent structures in an unsupervised manner, but it can be initialized with the model weights from a supervised learner for the latent task trained on some (other) annotated data. This can be seen as a form of domain adaptation from the supervised latent s"
I13-1149,2003.mtsummit-papers.6,0,0.0171447,"Related and Future Work As discussed, our work is most similar to Chang et al. (2010). We expand upon their framework by developing an efficient online algorithm and exploring learning over multiple views on latent representations. In terms of the task, max-margin LMs for speech recognition focus on the word prediction task (Gao et al., 2005; Roark et al., 2007; Singh-Miller and Collins, 2007). This focus is also shared by other syntactic LMs (Chelba and Jelinek, 1998; Xu et al., 2002; Schwartz et al., 2011; Charniak, 2001) which use syntax but rely on supervised data to train their parsers. Charniak et al. (2003) and Shen et al. (2010) use parsing based LMs for machine translation which are not whole-sentence models and they also rely on supervised parsers. Our focus is on using unsupervised latent variables (optionally initialized from supervised data) and training whole-sentence discriminative LMs. Our chunker model is related to the semi-Markov model in Okanohara and Tsujii (2007), but ours can take advantage of latent structures. Our work is related to Cherry and Quirk (2008) but differs in ways previously described. In future work, we plan to apply our algorithms to a wider range of tasks, and we"
I13-1149,P01-1017,0,0.0481116,"e algorithm requires fewer updates total in training compared to the batch version. 5 Related and Future Work As discussed, our work is most similar to Chang et al. (2010). We expand upon their framework by developing an efficient online algorithm and exploring learning over multiple views on latent representations. In terms of the task, max-margin LMs for speech recognition focus on the word prediction task (Gao et al., 2005; Roark et al., 2007; Singh-Miller and Collins, 2007). This focus is also shared by other syntactic LMs (Chelba and Jelinek, 1998; Xu et al., 2002; Schwartz et al., 2011; Charniak, 2001) which use syntax but rely on supervised data to train their parsers. Charniak et al. (2003) and Shen et al. (2010) use parsing based LMs for machine translation which are not whole-sentence models and they also rely on supervised parsers. Our focus is on using unsupervised latent variables (optionally initialized from supervised data) and training whole-sentence discriminative LMs. Our chunker model is related to the semi-Markov model in Okanohara and Tsujii (2007), but ours can take advantage of latent structures. Our work is related to Cherry and Quirk (2008) but differs in ways previously"
I13-1149,P98-1035,0,0.0609245,"ve than the online versions on test-set accuracy, and 2) the online algorithm requires fewer updates total in training compared to the batch version. 5 Related and Future Work As discussed, our work is most similar to Chang et al. (2010). We expand upon their framework by developing an efficient online algorithm and exploring learning over multiple views on latent representations. In terms of the task, max-margin LMs for speech recognition focus on the word prediction task (Gao et al., 2005; Roark et al., 2007; Singh-Miller and Collins, 2007). This focus is also shared by other syntactic LMs (Chelba and Jelinek, 1998; Xu et al., 2002; Schwartz et al., 2011; Charniak, 2001) which use syntax but rely on supervised data to train their parsers. Charniak et al. (2003) and Shen et al. (2010) use parsing based LMs for machine translation which are not whole-sentence models and they also rely on supervised parsers. Our focus is on using unsupervised latent variables (optionally initialized from supervised data) and training whole-sentence discriminative LMs. Our chunker model is related to the semi-Markov model in Okanohara and Tsujii (2007), but ours can take advantage of latent structures. Our work is related t"
I13-1149,2008.amta-papers.4,0,0.0746418,"tion). Chang et al. (2009), Das and Smith (2009), Goldwasser and Roth (2008), and Mccallum and Bellare (2005) have shown that this approach can propagate error from the structured prediction to the task-specific classifier. Recent work has combined unsupervised learning of (latent) structure prediction with a supervised learning approach for the task. Work in this vein has focused on jointly ∗ This research was partially supported by an NSERC, Canada (RGPIN: 264905) grant and a Google Faculty Award to the third author. learning the latent structures together with the task-specific classifier (Cherry and Quirk, 2008; Chang et al., 2010). Chang et al. (2010) in particular introduce a framework for solving classification problems using constraints over latent structures, referred to as Learning over Constrained Latent Representations (LCLR). We extend this framework for discriminative joint learning over latent structures to a novel online algorithm. Our algorithm learns the latent structures in an unsupervised manner, but it can be initialized with the model weights from a supervised learner for the latent task trained on some (other) annotated data. This can be seen as a form of domain adaptation from th"
I13-1149,P09-1053,0,0.012151,"antly outperforms batch learning for latent representations with a single view on a grammaticality prediction task. 1 Introduction Natural language data is implicitly richly structured, and making use of that structure can be valuable in a wide variety of NLP tasks. However, finding these latent structures is a complex task of its own right. Early work used a twophase pipeline process, in which the output of a structure prediction algorithm (e.g. a noun phrase finder) acts as fixed input features to train a classifier for a different task (e.g. grammaticality prediction). Chang et al. (2009), Das and Smith (2009), Goldwasser and Roth (2008), and Mccallum and Bellare (2005) have shown that this approach can propagate error from the structured prediction to the task-specific classifier. Recent work has combined unsupervised learning of (latent) structure prediction with a supervised learning approach for the task. Work in this vein has focused on jointly ∗ This research was partially supported by an NSERC, Canada (RGPIN: 264905) grant and a Google Faculty Award to the third author. learning the latent structures together with the task-specific classifier (Cherry and Quirk, 2008; Chang et al., 2010). Cha"
I13-1149,P07-1010,0,0.165787,"viable trigrams, but a human could easily judge them to be ungrammatical. However, if a language model used latent information like a shallow syntactic parse, it could also recognize the lack of grammaticality. Discriminative models can take into account arbitrary features of data, and thus may be able to avoid the shortcomings of n-gram LMs in judging the grammaticality of text. In the case of language modeling, however, there is no obvious choice of categories between which the model should discriminate. Cherry and Quirk (2008) show that by following the pseudo-negative examples approach of Okanohara and Tsujii (2007), they can build a syntactic discriminative LM that learns to distinguish between samples from a corpus generated by human speakers (positives) and samples generated by an n-gram model (negatives). Our approach is similar to Cherry and Quirk (2008), but they use probabilistic context-free grammar (PCFG) parses as latent structure, use a latent SVM as the learning model (we use latent passive-aggressive (PA) learning), and they handle negative examples differently. Instead of PCFG parsing, we use a chunking representation of sentence structure, which can be seen as a shallow parse, in which eac"
I13-1149,E99-1023,0,0.0494232,", however, updates for all negative examples are performed at once and all are re-decoded until no new structures are found for any single negative example. 3.4 Multiple Views on Latent Representations Shen and Sarkar (2005) find that using multiple chunking representations is advantageous for the chunking task. Moreover, they demonstrate that the careful selection of latent structure can yield more helpful features for a task-specific classifier. We thus perform inference separately to generate distinct latent structures for each of their five chunking representations (which are mostly from (Sang and Veenstra, 1999)) at line 5 of Alg. 1; at line 6 we evaluate the dot product of the weight vector with the features from the combined outputs of the different views. Each of the views use a different representation of the chunk structures, which we will only briefly describe due to space limitations; for more detailed information, please see Shen and Sarkar (2005). Each representation uses a set of tags to label each token in a sentence as belonging to a nonoverlapping chunk type. We refer to the chunking IOB1 O I I O I I B O I O O O I I B I O IOB2 O B I O B I B O B O O O B I B I O IOE1 O I I O I E I O I O O"
I13-1149,P11-1063,0,0.0128535,"uracy, and 2) the online algorithm requires fewer updates total in training compared to the batch version. 5 Related and Future Work As discussed, our work is most similar to Chang et al. (2010). We expand upon their framework by developing an efficient online algorithm and exploring learning over multiple views on latent representations. In terms of the task, max-margin LMs for speech recognition focus on the word prediction task (Gao et al., 2005; Roark et al., 2007; Singh-Miller and Collins, 2007). This focus is also shared by other syntactic LMs (Chelba and Jelinek, 1998; Xu et al., 2002; Schwartz et al., 2011; Charniak, 2001) which use syntax but rely on supervised data to train their parsers. Charniak et al. (2003) and Shen et al. (2010) use parsing based LMs for machine translation which are not whole-sentence models and they also rely on supervised parsers. Our focus is on using unsupervised latent variables (optionally initialized from supervised data) and training whole-sentence discriminative LMs. Our chunker model is related to the semi-Markov model in Okanohara and Tsujii (2007), but ours can take advantage of latent structures. Our work is related to Cherry and Quirk (2008) but differs in"
I13-1149,J10-4005,0,0.0141816,"discussed, our work is most similar to Chang et al. (2010). We expand upon their framework by developing an efficient online algorithm and exploring learning over multiple views on latent representations. In terms of the task, max-margin LMs for speech recognition focus on the word prediction task (Gao et al., 2005; Roark et al., 2007; Singh-Miller and Collins, 2007). This focus is also shared by other syntactic LMs (Chelba and Jelinek, 1998; Xu et al., 2002; Schwartz et al., 2011; Charniak, 2001) which use syntax but rely on supervised data to train their parsers. Charniak et al. (2003) and Shen et al. (2010) use parsing based LMs for machine translation which are not whole-sentence models and they also rely on supervised parsers. Our focus is on using unsupervised latent variables (optionally initialized from supervised data) and training whole-sentence discriminative LMs. Our chunker model is related to the semi-Markov model in Okanohara and Tsujii (2007), but ours can take advantage of latent structures. Our work is related to Cherry and Quirk (2008) but differs in ways previously described. In future work, we plan to apply our algorithms to a wider range of tasks, and we will present an analys"
I13-1149,P02-1025,0,0.034071,"Missing"
I13-1149,H05-1027,0,0.0265575,"haviour of the online algorithm due to lack of space, but our findings were 1) that the batch models were slower to improve than the online versions on test-set accuracy, and 2) the online algorithm requires fewer updates total in training compared to the batch version. 5 Related and Future Work As discussed, our work is most similar to Chang et al. (2010). We expand upon their framework by developing an efficient online algorithm and exploring learning over multiple views on latent representations. In terms of the task, max-margin LMs for speech recognition focus on the word prediction task (Gao et al., 2005; Roark et al., 2007; Singh-Miller and Collins, 2007). This focus is also shared by other syntactic LMs (Chelba and Jelinek, 1998; Xu et al., 2002; Schwartz et al., 2011; Charniak, 2001) which use syntax but rely on supervised data to train their parsers. Charniak et al. (2003) and Shen et al. (2010) use parsing based LMs for machine translation which are not whole-sentence models and they also rely on supervised parsers. Our focus is on using unsupervised latent variables (optionally initialized from supervised data) and training whole-sentence discriminative LMs. Our chunker model is related"
I13-1149,D08-1037,0,0.0186575,"h learning for latent representations with a single view on a grammaticality prediction task. 1 Introduction Natural language data is implicitly richly structured, and making use of that structure can be valuable in a wide variety of NLP tasks. However, finding these latent structures is a complex task of its own right. Early work used a twophase pipeline process, in which the output of a structure prediction algorithm (e.g. a noun phrase finder) acts as fixed input features to train a classifier for a different task (e.g. grammaticality prediction). Chang et al. (2009), Das and Smith (2009), Goldwasser and Roth (2008), and Mccallum and Bellare (2005) have shown that this approach can propagate error from the structured prediction to the task-specific classifier. Recent work has combined unsupervised learning of (latent) structure prediction with a supervised learning approach for the task. Work in this vein has focused on jointly ∗ This research was partially supported by an NSERC, Canada (RGPIN: 264905) grant and a Google Faculty Award to the third author. learning the latent structures together with the task-specific classifier (Cherry and Quirk, 2008; Chang et al., 2010). Chang et al. (2010) in particul"
I13-1149,C98-1035,0,\N,Missing
J02-3005,C88-2121,0,0.157817,"Missing"
J11-4010,N01-1016,0,0.0245869,"al with limited coverage of the grammar by performing insertions, deletions, or substitutions on the input string. This makes a lot of sense in programming language parsers, but for natural languages it makes little sense to transform the input because the grammar has poor coverage. It is trivial to add (weighted) glue rules that accept any input string, or a ﬁnite-state acceptor of strings can be used as a back-off grammar to improve coverage. Speech repair and other such cases are typically handled using appropriate augmentations of the underlying grammar combined with grammar-driven edits (Charniak and Johnson 2001). ´ Despite this, error-repair is a good use-case for parsing schemas. Gomez Rodr´ıguez can show that some existing error-repair parsers are in fact provably correct, and also a generic recipe can be given that converts any given parser schema into an error-repair parser schema. This is an instructive use of parsing schema transformations, because it is easy to show that the changes preserve correctness. Parsing schemas for dependency parsers. This third part of the book has the potential to be the most popular. There is increased interest in multilingual dependency parsing, and there are a la"
J11-4010,J07-2003,0,0.0202781,"sing (Goodman 1998, 1999) is not mentioned. The use of probabilities or weights is generally ignored in this book, even though it enables interesting methods for speeding up parsers such as coarse to ﬁne parsing (Goodman 1997) or generalized A∗ search for parsing (Pauls and Klein 2009). While there is more than enough content in this book, it does not cover the use of parsing schemas in machine translation. In particular, formal properties of schemas might make it easier to describe and implement the integration of language models into parsing algorithms for synchronous context-free grammars (Chiang 2007). Schemas might have much to offer with respect to proving correctness in machine translation decoders. The potential reader for this book is likely to be a parsing enthusiast curious about the power of schemas to represent parsing algorithms succinctly and to prove them correct. They might also be interested in showing relationships between their novel parsing schemas and other well-known parsers, or showing how extensions to existing parsers are well justiﬁed. Dependency parsing enthusiasts who want to wrap their head around the many different parsing algorithms out there might also be inter"
J11-4010,H05-1036,0,0.0267397,". 2005), there is a step that eliminates cycles in the graph. This step is not constructive and therefore the MST parser cannot be represented as a schema. Parsing schemas are generally grammar-driven and often parsers are written without any ﬁnite underlying grammar, which makes tree building harder to describe concisely. The discussion of related work touches on the use of Prolog for parsing schemas (Shieber, Schabes, and Pereira 1995), Datalog for specifying parsers (McAllester 2002; Liu and Stoller 2003), the DyALog system (Villemonte de la Clergerie 2005), and Dyna (Eisner, Goldlust, and Smith 2005). It is true that Dyna is quite powerful because it is a full general-purpose declarative programming language, but for that reason it offers an attractive alternative to parsing schemas. On the other hand, schemas do allow formal reasoning about parsers that may be more ﬁne-grained than is possible in Dyna. Surprisingly, work on semiring parsing (Goodman 1998, 1999) is not mentioned. The use of probabilities or weights is generally ignored in this book, even though it enables interesting methods for speeding up parsers such as coarse to ﬁne parsing (Goodman 1997) or generalized A∗ search for"
J11-4010,W97-0302,0,0.025578,"and Dyna (Eisner, Goldlust, and Smith 2005). It is true that Dyna is quite powerful because it is a full general-purpose declarative programming language, but for that reason it offers an attractive alternative to parsing schemas. On the other hand, schemas do allow formal reasoning about parsers that may be more ﬁne-grained than is possible in Dyna. Surprisingly, work on semiring parsing (Goodman 1998, 1999) is not mentioned. The use of probabilities or weights is generally ignored in this book, even though it enables interesting methods for speeding up parsers such as coarse to ﬁne parsing (Goodman 1997) or generalized A∗ search for parsing (Pauls and Klein 2009). While there is more than enough content in this book, it does not cover the use of parsing schemas in machine translation. In particular, formal properties of schemas might make it easier to describe and implement the integration of language models into parsing algorithms for synchronous context-free grammars (Chiang 2007). Schemas might have much to offer with respect to proving correctness in machine translation decoders. The potential reader for this book is likely to be a parsing enthusiast curious about the power of schemas to"
J11-4010,J99-4004,0,0.101526,"Missing"
J11-4010,D10-1027,0,0.0439587,"Missing"
J11-4010,P10-1110,0,0.06108,"Missing"
J11-4010,H05-1066,0,0.13674,"Missing"
K19-1002,N09-2047,0,0.140364,") (4) S is the top-K set of supertags for each word in the input sequence. The hyperparameters αi can be tuned. However we found in our experiments that the results were not very sensitive to the values, and the uniform distribution over all the tasks performed the best. The model and decoding step for our multi-task model is shown in Fig. 2. We also experiment with a commonly used multi-task model where some or all of the components are shared between the different (unlike our approach).. 5 Dataset We use the dataset that has been widely used by previous work in supertagging and TAG parsing (Bangalore et al., 2009; Chung et al., 2016; Friedman et al., 2017; Kasai et al., 2017, 2018). We use the grammar and the TAG-annotated WSJ Penn Tree Bank extracted by Chen et al. (2006). As in previous work, we use Sections 01-22 as the training set, Section 00 as the dev set, and Section 23 as the test set. The training, dev, and test sets comprise 39832, 1921, and 2415 sentences; 950028, 46451, 56683 tokens, respectively. The TAG-annotated version of Penn treebank (Chen and Shankar, 2001) includes 4727 distinct supertags (including an unknown supertag) and the grammar file of all supertags is downloaded from http"
K19-1002,J07-3004,0,0.0599545,"e combines these supertags into a parse tree. Supertagging is a task that learns a sequence prediction task from this annotated data and is able to then assign the most likely sequence of supertags to an input sequence of words (Bangalore and Joshi, 1999). Once the right supertag is assigned then parsing is a much easier task and may not even be needed for many applications where information about syntax is needed but a full parse is unnecessary. Supertagging has been shown to be useful for both Tree Adjoining Grammar (TAG) (Bangalore and Joshi, 1999) and combinatory categorial grammar (CCG) (Hockenmaier and Steedman, 2007) parsing. In this paper we aim to improve the state-of-the-art for the task of learning a TAG supertagger from an annotated treebank (Kasai 12 Proceedings of the 23rd Conference on Computational Natural Language Learning, pages 12–21 c Hong Kong, China, November 3-4, 2019. 2019 Association for Computational Linguistics tion node which can be expanded by a tree rooted in the same label, e.g. t3 rooted in NP substitutes into the NP↓ node in t46. The ∗ symbol on the leaf node of a tree t represents an adjunction node (also called a footnode) and signifies that t can be inserted into an internal n"
K19-1002,J99-2004,0,0.426363,"om grammar formalisms that are more expressive than context-free grammars for phrase structure trees or dependency trees. In Tree Adjoining Grammar (TAG), the supertags are tree fragments that can express various syntactic facts such as transitive verb, wh- extraction, relative clauses, appositive clauses, light verbs, prepositional phrase attachment and many other syntactic phenomena. In combinatory categorial grammar (CCG) the supertags are types and their type-raised variants which also capture similar syntactic phenomena as in TAG supertags. Supertagging can be viewed as “almost parsing” (Bangalore and Joshi, 1999) and can provide the benefits of syntactic parsing without a full parser. In this paper we focus on the TAG supertagging task, however, our proposed methods can likely be used to improve CCG supertagging as well. Supertagging is a relatively simple linear time sequence prediction task similar to part of speech tagging. Supertagging can be useful in many applications such as machine translation, grammatical error detection, disfluency prediction, and many others while being a much simpler task than full parsing. In addition, for both TAG and CCG, supertagging is an essential first step to parsi"
K19-1002,E17-2026,0,0.0486046,"Missing"
K19-1002,D17-1180,0,0.0674705,"quence. The hyperparameters αi can be tuned. However we found in our experiments that the results were not very sensitive to the values, and the uniform distribution over all the tasks performed the best. The model and decoding step for our multi-task model is shown in Fig. 2. We also experiment with a commonly used multi-task model where some or all of the components are shared between the different (unlike our approach).. 5 Dataset We use the dataset that has been widely used by previous work in supertagging and TAG parsing (Bangalore et al., 2009; Chung et al., 2016; Friedman et al., 2017; Kasai et al., 2017, 2018). We use the grammar and the TAG-annotated WSJ Penn Tree Bank extracted by Chen et al. (2006). As in previous work, we use Sections 01-22 as the training set, Section 00 as the dev set, and Section 23 as the test set. The training, dev, and test sets comprise 39832, 1921, and 2415 sentences; 950028, 46451, 56683 tokens, respectively. The TAG-annotated version of Penn treebank (Chen and Shankar, 2001) includes 4727 distinct supertags (including an unknown supertag) and the grammar file of all supertags is downloaded from http://mica.lif.univ-mrs.fr/. There are 69 auxiliary tree TYPEs, 40"
K19-1002,N18-1107,0,0.0901807,"or both TAG and CCG, supertagging is an essential first step to parsing so any improvements in supertag prediction will benefit parsing as well. For all these reasons, in this paper we focus on the supertagging task. TAG and CCG can be parsed using graph-parsing methods in O(n3 ) but the complexity of unrestricted parsing for both formalisms is O(n6 ) which is prohibitive on real-world data. Neural linear-time transition based parsers are still not accurate enough to compete with the state-of-the-art supertagging models or parsers that use supertagging as the initial step (Chung et al., 2016; Kasai et al., 2018). An example of the supertagging task for Tree Adjoining Grammars (TAGs) is shown in Fig. 1. The ↓ symbol on a leaf node represents a substitu3 Baseline Supertagging Model For our baseline supertagging model we use the state-of-the-art model that currently has the highest accuracy on the Penn treebank dataset (Kasai et al., 2018). For the supertagging model the main contribution of Kasai et al. (2018) was two-fold: the first was to add a character CNN for modeling word embeddings using subword features, and the second was to add highway connections to add more layers to a standard bidirectiona"
K19-1002,Q16-1026,0,0.0272113,"and backward units each. Dropout layers (Gal and Ghahramani, 2016; Srivastava et al., 2014) are inserted between the input and BiLSTM layer, between BiLSTM layers, and between recurrent time steps. The dropout rate used was 0.5. We used 2-3 BiLSTM layers. Kasai et al. (2018) provide some reasons why > 3 layers do not provide any additional accuracy even with highway connections. (Kasai et al., 2018) we use two components in the word embedding: • a 30-dimensional character level embedding vector computed using a char-CNN which captures the morphological information (Santos and Zadrozny, 2014; Chiu and Nichols, 2016; Ma and Hovy, 2016; Kasai et al., 2018). Each character is encoded as a 30-dimensional vector, and then we apply 30 convolutional filters with a window size of 5. This produces a 30-dimensional character embedding. • a 100/200/300 size word embedding which is initialized using GloVe (Pennington et al., 2014). For words that do not appear in GloVe, we randomly initialized the word embedding. 3.3 We concatenate hidden vectors from both directions of the last layer of BiLSTM and pass it into a multilayer perceptron (MLP). In practice a single layer perceptron performs just as well in this task."
K19-1002,N19-1341,0,0.0409212,"Missing"
K19-1002,N16-1026,0,0.0392804,"Missing"
K19-1002,P19-1441,0,0.0557523,"Missing"
K19-1002,D16-1181,0,0.0170305,"odel and this work had the state-of-the-art accuracy before our paper on the Penn treebank dataset. Friedman et al. (2017) investigated a recursive treebased vector representation of TAG supertags, but while their model can learn useful facts about supertags, about how one can be related to another, there was no performance improvement as a result of their model on the supertagging task. Xu et al. (2015) uses RNN for the CCG supertagging task, Lewis et al. (2016) adopted the LSTM structure into this task, while Vaswani et al. (2016) also introduced another variation of Bi-LSTM into this task. Xu (2016) then proposed an attention-based Bi-LSTM supertagging model. t2: NP NP N NP∗ A NP∗ N NP∗ t81: t27: t81: S S NP0 ↓ VP Most Helpful Task NP0 ↓ S VP NP0 ↓ VP V NP1 ↓ V HEAD V SKETCH t38: NP t3: t3: NP NP NP∗ NP N N N t3: TYPE, SKETCH t3: NP NP t18: N N t132: N ROOT, SKETCH t132: S S t20: S PRN Punct S∗ PRN Punct S∗ Punct S∗ SPINE, SKETCH Table 5: Some examples of how the deconstructing of base models correct the prediction made by the supertagging model. 8 Conclusion In this paper we have introduced a novel multitask framework for the TAG supertagging task. The approach involved a"
K19-1002,P15-2041,0,0.0197022,"ags and suffix embeddings as inputs, then Kasai et al. (2018) further extends the BiLSTM model with highway connection as well as character CNN as input, and jointly train the supertagging model with parsing model and this work had the state-of-the-art accuracy before our paper on the Penn treebank dataset. Friedman et al. (2017) investigated a recursive treebased vector representation of TAG supertags, but while their model can learn useful facts about supertags, about how one can be related to another, there was no performance improvement as a result of their model on the supertagging task. Xu et al. (2015) uses RNN for the CCG supertagging task, Lewis et al. (2016) adopted the LSTM structure into this task, while Vaswani et al. (2016) also introduced another variation of Bi-LSTM into this task. Xu (2016) then proposed an attention-based Bi-LSTM supertagging model. t2: NP NP N NP∗ A NP∗ N NP∗ t81: t27: t81: S S NP0 ↓ VP Most Helpful Task NP0 ↓ S VP NP0 ↓ VP V NP1 ↓ V HEAD V SKETCH t38: NP t3: t3: NP NP NP∗ NP N N N t3: TYPE, SKETCH t3: NP NP t18: N N t132: N ROOT, SKETCH t132: S S t20: S PRN Punct S∗ PRN Punct S∗ Punct S∗ SPINE, SKETCH Table 5: Some examples of how the deconstruct"
K19-1002,P16-1101,0,0.0194205,". Dropout layers (Gal and Ghahramani, 2016; Srivastava et al., 2014) are inserted between the input and BiLSTM layer, between BiLSTM layers, and between recurrent time steps. The dropout rate used was 0.5. We used 2-3 BiLSTM layers. Kasai et al. (2018) provide some reasons why > 3 layers do not provide any additional accuracy even with highway connections. (Kasai et al., 2018) we use two components in the word embedding: • a 30-dimensional character level embedding vector computed using a char-CNN which captures the morphological information (Santos and Zadrozny, 2014; Chiu and Nichols, 2016; Ma and Hovy, 2016; Kasai et al., 2018). Each character is encoded as a 30-dimensional vector, and then we apply 30 convolutional filters with a window size of 5. This produces a 30-dimensional character embedding. • a 100/200/300 size word embedding which is initialized using GloVe (Pennington et al., 2014). For words that do not appear in GloVe, we randomly initialized the word embedding. 3.3 We concatenate hidden vectors from both directions of the last layer of BiLSTM and pass it into a multilayer perceptron (MLP). In practice a single layer perceptron performs just as well in this task. The number of input"
K19-1002,J93-2004,0,0.0679641,"Missing"
K19-1002,D14-1162,0,0.0809819,"Missing"
K19-1002,N18-1202,0,0.0173642,". Each of these probabilities are defined as a sequence prediction task over the auxiliary tasks using the functions defined in Section 4.1. Figure 2: The prediction procedure of combining models trained on separate tasks The usual criticism of a fair comparison between multi-task and single-task learning is that the multi-task setting simply uses more labeled data instances (typically with different data sources) and as a result a fair comparison between a multi-task and a single-task setting should involve large pre-trained models trained using a language modelling objective (such as ELMO (Peters et al., 2018) or BERT (Devlin et al., 2018)). In our case, because we re-use the same training set for multi-task learning, we have made sure our experimental settings exactly match the previous best state-of-the-art method for supertagging (Kasai et al., 2018) and we use the same pre-trained word embeddings to ensure a fair comparison. We train six different neural sequence prediction models independently on the supertagging task, root node prediction (ROOT), head node prediction (HEAD), tree type prediction (TYPE), tree sketch prediction (SKETCH) and tree spine prediction (SPINE) tasks. For each task, we"
K19-1002,N16-1027,0,0.0151682,"as character CNN as input, and jointly train the supertagging model with parsing model and this work had the state-of-the-art accuracy before our paper on the Penn treebank dataset. Friedman et al. (2017) investigated a recursive treebased vector representation of TAG supertags, but while their model can learn useful facts about supertags, about how one can be related to another, there was no performance improvement as a result of their model on the supertagging task. Xu et al. (2015) uses RNN for the CCG supertagging task, Lewis et al. (2016) adopted the LSTM structure into this task, while Vaswani et al. (2016) also introduced another variation of Bi-LSTM into this task. Xu (2016) then proposed an attention-based Bi-LSTM supertagging model. t2: NP NP N NP∗ A NP∗ N NP∗ t81: t27: t81: S S NP0 ↓ VP Most Helpful Task NP0 ↓ S VP NP0 ↓ VP V NP1 ↓ V HEAD V SKETCH t38: NP t3: t3: NP NP NP∗ NP N N N t3: TYPE, SKETCH t3: NP NP t18: N N t132: N ROOT, SKETCH t132: S S t20: S PRN Punct S∗ PRN Punct S∗ Punct S∗ SPINE, SKETCH Table 5: Some examples of how the deconstructing of base models correct the prediction made by the supertagging model. 8 Conclusion In this paper we have introduced a novel mul"
M95-1015,A88-1019,0,0.107606,"sed on the above criteria. Also, the tokenizer is responsible for maintaining the mapping between these tw o tokenizations so that the output of tools which use different tokenization schemes can be combined . Part-of-Speech Tagging Several components of the MUC coreference system, such as the noun phrase detector, require part-ofspeech (POS) tags for all of the words in an article . We combined the output of the following three POS tagger s using a simple voting scheme : Eric Brill&apos;s Rule Based Tagger version 1 .14 [2], the XTAG tagger, which is an implementation of Ken Church&apos;s PARTS tagger [4] and Adwait Ratnaparkhi&apos;s Maximum Entropy Tagger [11] . Each of these taggers uses the Penn Treebank tagset [8] . These three taggers, which were trained on the Penn Treebank Wall Street Journal corpus, tag pre-tokenized text . The tag actually used by the MUC system is determined by a majority voting scheme, in which a tag is chose n as the ""winner"" if at least two of the taggers postulate it . In the rare event that all three taggers disagree, the system uses the tag assigned by the maximum entropy tagger . In most cases, the majority voting scheme eliminates error s that are esoteric to a s"
M95-1015,W95-0103,0,0.0276076,"Missing"
M95-1015,C92-3145,0,0.0311847,"ome transformations to the set learned from the treebank . These transformations generalized o n learned ones . For instance, rules were learned which involved days of the week, but due to sparsity of training data , they were learned only for a subset of the seven days of the week . We manually added the missing cases . We did no t independently measure the performance of their tool using this modified rule set, but may do so in the future . 181 Knowledge Source s We experimented with various knowledge sources during system development, including WordNet [9], th e XTAG morphological analyzer [6], Roget&apos;s publicly available 1911 thesaurus, the Collins dictionary, a version of the American Heritage dictionary for which the University of Pennsylvania has a site license and the Gazetteer . Onl y WordNet, the XTAG morphological analyzer and the Gazetteer were used in the final system . We extracted a geographic name database from a publicly available version of the Gazetteer which we downloaded from the Center for Lexical Research. This database contains names of continents, islands, island groups , countries, provinces, cities and airports . This information is used when performing type"
M95-1015,J94-4002,0,0.196275,"ues . The difference in weighting between the two is currently based on intuition, though corpus methods migh t yield a more exact estimate of how much weight to give the female reading based on how often such words are actually used to refer to women . Pleonastic It Detectio n It is often used anaphorically in Wall Street Journal Text . Nonetheless, identifying instances of pleonastic it, which do not corefer, is still significant . The system identifies these instances of it by scanning tagged text and applying partly syntactic and partly lexical tests . Most of these tests are described in [7], but some additional test s were added to increase coverage . The fifteen rules used to detect pleonastic it are shown below in table 4 . Part of speech tags follow words and a slash, and are specified using the Penn Treebank tagset . Disjunctions are indicated using a vertical bar, (I), and optional elements are surrounded by brackets, ([]) . S abbreviates sentence ; NP mean s noun phrase ; and VP stands for verb phrase . We abbreviate CA for comparative adjectives, such as larger or smaller; SA for superlatives, such as greatest or largest ; MA for modal adjectives, such as necessary or unc"
M95-1015,J93-2004,0,0.0288585,"andard tasks . The first is the alteration of headline word capitalization . The Wall Street Journal adheres to standard conventions fo r capitalization of words in headlines, but since capitalization is an important cue for coreference resolution, w e attempted to eliminate capitalization which resulted solely from these conventions . Headline words which were capitalized in the body of the text anywhere other than sentence-initial position remained capitalized, as did thos e which were frequently capitalized other than in sentence-initial position in the Treebank Wall Street Journal corpu s [8] . All other uppercase words were converted to lowercase . The second non-standard task addressed by the tokenizer is the extraction of date information . The datelin e field is parsed to determine when each article was written . This information is later used to posit coreference between words or phrases such as today, tomorrow, this week, this year, and dates, such as November 20, 1995 . The third non-standard component determines whether &apos;s or &apos; is a genitive marker or part of a company name . When it is actually part of a company name, it does not indicate possession of the following noun"
M95-1015,W95-0107,0,0.0211053,"iting parsers and preprocessing utilities which allowed various pre existing tools to communicate with one another and produce output which could be used by other tools further in th e processing pipeline . Thus, we were freed to spend time developing the task-specific components of the system an d performing data analysis . Although no time was spent developing tools particularly for the MUC task prior t o January, many hours went into developing some of the off-the-shelf components we used, such as Eric Brills partof-speech tagger [2] and Lance Ramshaw and Mitch Marcus&apos; Noun Phrase Detector [10] . We estimate the tota l number of hours spent on the project itself to be roughly 1800, distributed among the eight graduate students wh o worked on the project. The vast majority of these hours were contributed between the end of July and competitio n week in early October . Table 1 shows the performance of our system when simple formatting errors, which hurt performance o n two of the 30 test files, were corrected . Table 2 contains our official system performance figures . Table 3 contain s system performance when optional elements were treated as if required . This set of scores is prese"
M95-1015,A88-1000,0,\N,Missing
N01-1023,H91-1060,0,0.043965,"Missing"
N01-1023,P98-1035,0,0.00829831,"t the mismatch between minimizing error rate and iteratively increasing the likelihood of the corpus. Other approaches have tried to move away from phrase structural representations into dependency style parsing (Lafferty et al., 1992; Fong and Wu, 1996). However, there are still inherent computational limitations due to the vast search space (see (Pietra et al., 1994) for discussion). None of these approaches can even be realistically compared to supervised parsers that are trained and tested on the kind of representations and the complexity of sentences that are found in the Penn Treebank. (Chelba and Jelinek, 1998) combine unlabeled and labeled data for parsing with a view towards language modeling applications. The goal in their work is not to get the right bracketing or dependencies but to reduce the word error rate in a speech recognizer. Our approach is closely related to previous CoTraining methods (Yarowsky, 1995; Blum and Mitchell, 1998; Goldman and Zhou, 2000; Collins and Singer, 1999). (Yarowsky, 1995) first introduced an iterative method for increasing a small set of seed data used to disambiguate dual word senses by exploiting the constraint that in a segment of discourse only one sense of a"
N01-1023,W99-0613,0,0.482612,"to train a statistical parser. We use a CoTraining method (Yarowsky, 1995; Blum and Mitchell,  I would like to thank Aravind Joshi, Mitch Marcus, Mark Liberman, B. Srinivas, David Chiang and the anonymous reviewers for helpful comments on this work. This work was partially supported by NSF Grant SBR8920230, ARO Grant DAAH0404-94-G-0426, and DARPA Grant N66001-00-1-8915. 1998; Goldman and Zhou, 2000) that has been used previously to train classifiers in applications like word-sense disambiguation (Yarowsky, 1995), document classification (Blum and Mitchell, 1998) and named-entity recognition (Collins and Singer, 1999) and apply this method to the more complex domain of statistical parsing. 2 Unsupervised techniques in language processing While machine learning techniques that exploit annotated data have been very successful in attacking problems in NLP, there are still some aspects which are considered to be open issues:    Adapting to new domains: training on one domain, testing (using) on another. Higher performance when using limited amounts of annotated data. Separating structural (robust) aspects of the problem from lexical (sparse) ones to improve performance on unseen data. In the particular doma"
N01-1023,A92-1018,0,0.0537414,"Missing"
N01-1023,A94-1009,0,0.109517,"Missing"
N01-1023,W00-1306,0,0.0785903,"e conditional independence between the features used by the two models that is exploited in co-training. Also, EM has been used successfully in text classification in combination of labeled and unlabeled data (see (Nigam et al., 1999)). In our experiments, unlike (Blum and Mitchell, 1998) we do not balance the label priors when picking new labeled examples for addition to the training data. One way to incorporate this into our algorithm would be to incorporate some form of sample selection (or active learning) into the selection of examples that are considered as labeled with high confidence (Hwa, 2000). 8 Conclusion In this paper, we proposed a new approach for training a statistical parser that combines labeled with unlabeled data. It uses a Co-Training method where a pair of models attempt to increase their agreement on labeling the data. The algorithm takes as input a small corpus of 9695 sentences (234467 word tokens) of bracketed data, a large pool of unlabeled text and a tag dictionary of lexicalized structures for each word in this training set (based on the LTAG formalism). The algorithm presented iteratively labels the unlabeled data set with parse trees. We then train a statistica"
N01-1023,W95-0102,0,0.0444678,"Missing"
N01-1023,J93-2004,0,0.0317705,"as input a small corpus (9695 sentences) annotated with parse trees, a dictionary of possible lexicalized structures for each word in the training set and a large pool of unlabeled text. The algorithm iteratively labels the entire data set with parse trees. Using empirical results based on parsing the Wall Street Journal corpus we show that training a statistical parser on the combined labeled and unlabeled data strongly outperforms training only on the labeled data. 1 Introduction The current crop of statistical parsers share a similar training methodology. They train from the Penn Treebank (Marcus et al., 1993); a collection of 40,000 sentences that are labeled with corrected parse trees (approximately a million word tokens). In this paper, we explore methods for statistical parsing that can be used to combine small amounts of labeled data with unlimited amounts of unlabeled data. In the experiment reported here, we use 9695 sentences of bracketed data (234467 word tokens). Such methods are attractive for the following reasons:    Bracketing sentences is an expensive process. A parser that can be trained on a small amount of labeled data will reduce this annotation cost. Creating statistical pars"
N01-1023,J94-2001,0,0.185076,"Missing"
N01-1023,C92-2065,0,0.0364691,"yproduct of this kind of representation we obtain more than the phrase structure of each sentence. We also produce a more embellished parse in which phenomena such as predicate-argument structure, subcategorization and movement are given a probabilisS tic treatment. Pierre Pierre_Vinken will NP NP join the_board VP join Vinken NP as a_nonexecutive_director S NP Pierre VP Vinken join NP Figure 3: A derivation indicating all the attachments between trees that have occurred during the parse of the sentence. 2.1 The Generative Model A stochastic LTAG derivation proceeds as follows (Schabes, 1992; Resnik, 1992). An initial tree is selected with probability Pinit and other trees selected by words in the sentence are combined using the operations of substitution and adjoining. These operations are explained below with examples. Each of these operations is performed with probability Pattach. For each  that can be valid start of a derivation: XP  init ( ) NP VP VP will join NP VP S XP attach 0 (;  !  ) = 1 Pattach(;  ! NA) + XP 0 attach (;  !  0) = 1 Pattach here is the probability that  0 rewrites an internal node  in tree  or that no adjoining (NA) occurs at node  in  . The additional"
N01-1023,C92-2066,0,0.00723319,"Penn Treebank into this representation. NP Pierre VP Vinken will VP S NP NP VP join the NP board VP VP NP PP as NP a non−executive director NP(Vinken) ! Pierre Vinken VP(join) ! will VP(join) VP(join) ! join NP(board) PP(as) ::: However, such a recursive decomposition of structure does not allow a simple notion of a tag dictionary. We solve this problem by decomposing the structure in an approach that is different from that shown above which uses context-free rules. The approach uses the notion of tree rewriting as defined in the Lexicalized Tree Adjoining Grammar (LTAG) formalism (Joshi and Schabes, 1992)1 which re1 This is a lexicalized version of Tree Adjoining Grammar (Joshi et al., 1975; Joshi, 1985). Figure 2: Parsing as tree classification and attachment. Combining the trees together by rewriting nodes as trees (explained in Section 2.1) gives us the parse tree in Figure 1. A history of the bi-lexical dependencies that define the probability model used to construct the parse is shown in Figure 3. This history is called the derivation tree. In addition, as a byproduct of this kind of representation we obtain more than the phrase structure of each sentence. We also produce a more embellish"
N01-1023,W00-1307,0,0.0114739,"Missing"
N01-1023,P95-1026,0,0.805242,"ng reasons:    Bracketing sentences is an expensive process. A parser that can be trained on a small amount of labeled data will reduce this annotation cost. Creating statistical parsers for novel domains and new languages will become easier. Combining labeled data with unlabeled data allows exploration of unsupervised methods which can now be tested using evaluations compatible with supervised statistical parsing. In this paper we introduce a new approach that combines unlabeled data with a small amount of labeled (bracketed) data to train a statistical parser. We use a CoTraining method (Yarowsky, 1995; Blum and Mitchell,  I would like to thank Aravind Joshi, Mitch Marcus, Mark Liberman, B. Srinivas, David Chiang and the anonymous reviewers for helpful comments on this work. This work was partially supported by NSF Grant SBR8920230, ARO Grant DAAH0404-94-G-0426, and DARPA Grant N66001-00-1-8915. 1998; Goldman and Zhou, 2000) that has been used previously to train classifiers in applications like word-sense disambiguation (Yarowsky, 1995), document classification (Blum and Mitchell, 1998) and named-entity recognition (Collins and Singer, 1999) and apply this method to the more complex domai"
N01-1023,C98-1035,0,\N,Missing
N03-1031,A00-2018,0,0.0290607,"s are iteratively re-trained on each other’s output; and a semi-supervised approach, corrected co-training, in which a human corrects each parser’s output before adding it to the training data. The selection of labeled training examples is an integral part of both frameworks. We propose several selection methods based on the criteria of minimizing errors in the data and maximizing training utility. We show that incorporating the utility criterion into the selection method results in better parsers for both frameworks. 1 Introduction Current state-of-the-art statistical parsers (Collins, 1999; Charniak, 2000) are trained on large annotated corpora such as the Penn Treebank (Marcus et al., 1993). However, the production of such corpora is expensive and labor-intensive. Given this bottleneck, there is considerable interest in (partially) automating the annotation process. To overcome this bottleneck, two approaches from machine learning have been applied to training parsers. One is sample selection (Thompson et al., 1999; Hwa, 2000; Tang et al., 2002), a variant of active learning (Cohn et al., 1994), which tries to identify a small set of unlabeled sentences with high training utility for the human"
N03-1031,P96-1042,0,0.012772,"Missing"
N03-1031,N01-1023,1,0.558581,"e. Given this bottleneck, there is considerable interest in (partially) automating the annotation process. To overcome this bottleneck, two approaches from machine learning have been applied to training parsers. One is sample selection (Thompson et al., 1999; Hwa, 2000; Tang et al., 2002), a variant of active learning (Cohn et al., 1994), which tries to identify a small set of unlabeled sentences with high training utility for the human to label1 . Sentences with high training utility are those most likely to improve the parser. The other approach, and the focus of this paper, is co-training (Sarkar, 2001), a mostlyunsupervised algorithm that replaces the human by having two (or more) parsers label training examples for each other. The goal is for both parsers to improve by bootstrapping off each other’s strengths. Because the parsers may label examples incorrectly, only a subset of their output, chosen by some selection mechanism, is used in order to minimize errors. The choice of selection method significantly affects the quality of the resulting parsers. We investigate a novel approach of selecting training examples for co-training parsers by incorporating the idea of maximizing training uti"
N03-1031,E03-1008,1,0.765705,"Missing"
N03-1031,W00-1306,1,0.314843,"ility criterion into the selection method results in better parsers for both frameworks. 1 Introduction Current state-of-the-art statistical parsers (Collins, 1999; Charniak, 2000) are trained on large annotated corpora such as the Penn Treebank (Marcus et al., 1993). However, the production of such corpora is expensive and labor-intensive. Given this bottleneck, there is considerable interest in (partially) automating the annotation process. To overcome this bottleneck, two approaches from machine learning have been applied to training parsers. One is sample selection (Thompson et al., 1999; Hwa, 2000; Tang et al., 2002), a variant of active learning (Cohn et al., 1994), which tries to identify a small set of unlabeled sentences with high training utility for the human to label1 . Sentences with high training utility are those most likely to improve the parser. The other approach, and the focus of this paper, is co-training (Sarkar, 2001), a mostlyunsupervised algorithm that replaces the human by having two (or more) parsers label training examples for each other. The goal is for both parsers to improve by bootstrapping off each other’s strengths. Because the parsers may label examples inc"
N03-1031,P02-1016,0,0.00757357,"rion into the selection method results in better parsers for both frameworks. 1 Introduction Current state-of-the-art statistical parsers (Collins, 1999; Charniak, 2000) are trained on large annotated corpora such as the Penn Treebank (Marcus et al., 1993). However, the production of such corpora is expensive and labor-intensive. Given this bottleneck, there is considerable interest in (partially) automating the annotation process. To overcome this bottleneck, two approaches from machine learning have been applied to training parsers. One is sample selection (Thompson et al., 1999; Hwa, 2000; Tang et al., 2002), a variant of active learning (Cohn et al., 1994), which tries to identify a small set of unlabeled sentences with high training utility for the human to label1 . Sentences with high training utility are those most likely to improve the parser. The other approach, and the focus of this paper, is co-training (Sarkar, 2001), a mostlyunsupervised algorithm that replaces the human by having two (or more) parsers label training examples for each other. The goal is for both parsers to improve by bootstrapping off each other’s strengths. Because the parsers may label examples incorrectly, only a sub"
N03-1031,J93-2004,0,0.0255646,"h, corrected co-training, in which a human corrects each parser’s output before adding it to the training data. The selection of labeled training examples is an integral part of both frameworks. We propose several selection methods based on the criteria of minimizing errors in the data and maximizing training utility. We show that incorporating the utility criterion into the selection method results in better parsers for both frameworks. 1 Introduction Current state-of-the-art statistical parsers (Collins, 1999; Charniak, 2000) are trained on large annotated corpora such as the Penn Treebank (Marcus et al., 1993). However, the production of such corpora is expensive and labor-intensive. Given this bottleneck, there is considerable interest in (partially) automating the annotation process. To overcome this bottleneck, two approaches from machine learning have been applied to training parsers. One is sample selection (Thompson et al., 1999; Hwa, 2000; Tang et al., 2002), a variant of active learning (Cohn et al., 1994), which tries to identify a small set of unlabeled sentences with high training utility for the human to label1 . Sentences with high training utility are those most likely to improve the"
N03-1031,P00-1016,0,0.0133509,"Missing"
N03-1031,W01-0501,0,0.0173865,"have different goals, their selection methods focus on different criteria: co-training typically favors selecting accurately labeled examples, while sample selection typically favors selecting examples with high training utility, which often are not sentences that the parsers already label accurately. In this work, we investigate selection methods for co-training that explore the trade-off between maximizing training utility and minimizing errors. Empirical studies were conducted to compare selection methods under both co-training and a semi-supervised framework called corrected co-training (Pierce and Cardie, 2001), in which the selected examples are manually checked and corrected before being added to the 1 In the context of training parsers, a labeled example is a sentence with its parse tree. Throughout this paper, we use the term “label” and “parse” interchangeably. training data. For co-training, we show that the benefit of selecting examples with high training utility can offset the additional errors they contain. For corrected co-training, we show that selecting examples with high training utility reduces the number of sentences the human annotator has to check. For both frameworks, we show that"
N03-1031,J03-4003,0,\N,Missing
N03-1031,P02-1046,0,\N,Missing
N04-1021,J00-1004,0,0.0540744,"Missing"
N04-1021,J93-2003,0,0.0210066,"e different types of features, including features based on syntactic analyses of the source and target sentences, which we hope will address the grammaticality of the translations, as well as lower-level features. As we work on n-best lists, we can easily use global sentence-level features. We begin by describing our baseline system and the n-best rescoring framework within which we conducted our experiments. We then present a selection of new features, progressing from word-level features to those based Anoop Sarkar Simon Fraser U. As an alternative to the often used source-channel approach (Brown et al., 1993), we directly model the posterior probability P r(eI1 |f1J ) (Och and Ney, 2002) using a log-linear combination of feature functions. In this framework, we have a set of M feature functions hm (eI1 , f1J ), m = 1, . . . , M . For each feature function, there exists a model parameter λm , m = 1, . . . , M . The direct translation probability is given by: PM exp[ m=1 λm hm (eI1 , f1J )] I J P r(e1 |f1 ) = P (2) PM J 0I e0I exp[ m=1 λm hm (e 1 , f1 )] 1 We obtain the following decision rule: eˆI1 = argmax eI1 M nX o λm hm (eI1 , f1J ) (3) m=1 The standard criterion for training such a log-linear"
N04-1021,P03-1011,1,0.218398,"of words in tree fragment and k for maximum height of with some probability, to transform one tree into another. tree fragment. We proceed from left to right in the ChiHowever, when training the model, trees for both the nese sentence and incrementally grow a pair of subtrees, source and target languages are provided, in our case one subtree in Chinese and the other in English, such that from the Chinese and English parsers. each word in the Chinese subtree is aligned to a word in We began with the tree-to-tree alignment model prethe English subtree. We grow this pair of subtrees unsented by Gildea (2003). The model was extended to hantil we can no longer grow either subtree without violatdle dependency trees, and to make use of the word-level ing the two parameter values n and k. Note that these alignments produced by the baseline MT system. The aligned subtree pairs have properties similar to alignment probability assigned by the tree-to-tree alignment model, templates. They can rearrange in complex ways between given the word-level alignment with which the candidate source and target. Figure 2 shows how subtree-pairs for translation was generated, was used as a feature in our parameters n ="
N04-1021,P03-1021,1,0.129951,"nsisting of S sentence pairs {(fs , es ) : s = 1, . . . , S}. However, this does not guarantee optimal performance on the metric of translation quality by which our system will ultimately be evaluated. For this reason, we optimize the parameters directly against the BLEU metric on held-out data. This is a more difficult optimization problem, as the search space is no longer convex. Figure 1: Example segmentation of Chinese sentence and its English translation into alignment templates. However, certain properties of the BLEU metric can be exploited to speed up search, as described in detail by Och (2003). We use this method of optimizing feature weights throughout this paper. 2.1 Baseline MT System: Alignment Templates Our baseline MT system is the alignment template system described in detail by Och, Tillmann, and Ney (1999) and Och and Ney (2004). In the following, we give a short description of this baseline model. The probability model of the alignment template system for translating a sentence can be thought of in distinct stages. First, the source sentence words f1J are grouped to phrases f˜1K . For each phrase f˜ an alignment template z is chosen and the sequence of chosen alignment te"
N04-1021,P02-1038,1,0.203592,"the source and target sentences, which we hope will address the grammaticality of the translations, as well as lower-level features. As we work on n-best lists, we can easily use global sentence-level features. We begin by describing our baseline system and the n-best rescoring framework within which we conducted our experiments. We then present a selection of new features, progressing from word-level features to those based Anoop Sarkar Simon Fraser U. As an alternative to the often used source-channel approach (Brown et al., 1993), we directly model the posterior probability P r(eI1 |f1J ) (Och and Ney, 2002) using a log-linear combination of feature functions. In this framework, we have a set of M feature functions hm (eI1 , f1J ), m = 1, . . . , M . For each feature function, there exists a model parameter λm , m = 1, . . . , M . The direct translation probability is given by: PM exp[ m=1 λm hm (eI1 , f1J )] I J P r(e1 |f1 ) = P (2) PM J 0I e0I exp[ m=1 λm hm (e 1 , f1 )] 1 We obtain the following decision rule: eˆI1 = argmax eI1 M nX o λm hm (eI1 , f1J ) (3) m=1 The standard criterion for training such a log-linear model is to maximize the probability of the parallel training corpus consisting"
N04-1021,J04-4002,1,0.260908,"ters directly against the BLEU metric on held-out data. This is a more difficult optimization problem, as the search space is no longer convex. Figure 1: Example segmentation of Chinese sentence and its English translation into alignment templates. However, certain properties of the BLEU metric can be exploited to speed up search, as described in detail by Och (2003). We use this method of optimizing feature weights throughout this paper. 2.1 Baseline MT System: Alignment Templates Our baseline MT system is the alignment template system described in detail by Och, Tillmann, and Ney (1999) and Och and Ney (2004). In the following, we give a short description of this baseline model. The probability model of the alignment template system for translating a sentence can be thought of in distinct stages. First, the source sentence words f1J are grouped to phrases f˜1K . For each phrase f˜ an alignment template z is chosen and the sequence of chosen alignment templates is reordered (according to π1K ). Then, every phrase f˜ produces its translation e˜ (using the corresponding alignment template z). Finally, the sequence of phrases e˜K 1 constitutes the sequence of words eI1 . Our baseline system incorporat"
N04-1021,W99-0604,1,0.236444,"Missing"
N04-1021,W03-1002,0,0.0153043,"of the AT touches the upper right corner of the previous AT and the first word in the current AT immediately follows the last word in the previous AT. The total probability is the product over all alignment templates i, either P (ATi is right-continuous) or 1 − P (ATi is right-continuous). In both models, the probabilities P have been estimated from the full training data (train). 5 Shallow Syntactic Feature Functions By shallow syntax, we mean the output of the part-ofspeech tagger and chunkers. We hope that such features can combine the strengths of tag- and chunk-based translation systems (Schafer and Yarowsky, 2003) with our baseline system. 5.1 Projected POS Language Model This feature uses Chinese POS tag sequences as surrogates for Chinese words to model movement. Chinese words are too sparse to model movement, but an attempt to model movement using Chinese POS may be more successful. We hope that this feature will compensate for a weak model of word movement in the baseline system. Chinese POS sequences are projected to English using the word alignment. Relative positions are indicated for each Chinese tag. The feature function was also tried without the relative positions: CD +0 M +1 NN +3 NN -1 NN"
N04-1021,C96-2141,0,0.254737,"Missing"
N04-1021,P98-2230,0,0.0760244,"Missing"
N04-1021,P01-1067,1,0.0551165,"ical human translations. One reason for that is that the MT output uses fewer unseen words and typically more frequent words which lead to a higher language model probability. We also performed experiments to balance this effect by dividing the parser probability by the word unigram probability and using this ’normalized parser probability’ as a feature function, but also this did not yield improvements. 6.2 Tree-to-String Alignment A tree-to-string model is one of several syntaxbased translation models used. The model is a conditional probability p(f |T (e)). Here, we used a model defined by Yamada and Knight (2001) and Yamada and Knight (2002). Internally, the model performs three types of operations on each node of a parse tree. First, it reorders the child nodes, such as changing VP → VB NP PP into VP → NP PP VB. Second, it inserts an optional word at each node. Third, it translates the leaf English words into Chinese words. These operations are stochastic and their probabilities are assumed to depend only on the node, and are independent of other operations on the node, or other nodes. The probability of each operation is automatically obtained by a training algorithm, using about 780,000 English par"
N04-1021,C98-2225,0,\N,Missing
N04-1023,J90-2002,0,0.65721,"a baseline statistical machine translation system, a ranked best list of candidate translations in the target language. We introduce two novel perceptroninspired reranking algorithms that improve on the quality of machine translation over the baseline system based on evaluation using the BLEU metric. We provide experimental results on the NIST 2003 Chinese-English large data track evaluation. We also provide theoretical analysis of our algorithms and experiments that verify that our algorithms provide state-of-theart performance in machine translation. 1 Introduction The noisy-channel model (Brown et al., 1990) has been the foundation for statistical machine translation (SMT) for over ten years. Recently so-called reranking techniques, such as maximum entropy models (Och and Ney, 2002) and gradient methods (Och, 2003), have been applied to machine translation (MT), and have provided significant improvements. In this paper, we introduce two novel machine learning algorithms specialized for the MT task. Discriminative reranking algorithms have also contributed to improvements in natural language parsing and tagging performance. Discriminative reranking algorithms used for these applications include Pe"
N04-1023,P02-1034,0,0.064415,"Missing"
N04-1023,P03-1011,0,0.0173227,"thin a template were used to handle phrase to phrase translation. However, phrase level alignment cannot handle long distance reordering effectively. Parse trees have also been used in alignment models. Wu (1997) introduced constraints on alignments using a probabilistic synchronous context-free grammar restricted to Chomskynormal form. (Wu, 1997) was an implicit or selforganizing syntax model as it did not use a Treebank. Yamada and Knight (2001) used a statistical parser trained using a Treebank in the source language to produce parse trees and proposed a tree to string model for alignment. Gildea (2003) proposed a tree to tree alignment model using output from a statistical parser in both source and target languages. The translation model involved tree alignments in which subtree cloning was used to handle cases of reordering that were not possible in earlier tree-based alignment models. 1.2 Discriminative Models for MT Och and Ney (2002) proposed a framework for MT based on direct translation, using the conditional model  estimated using a maximum entropy model. A small number of feature functions defined on the source and target sentence were used to rerank the translations genera"
N04-1023,P02-1038,1,0.3858,"thms that improve on the quality of machine translation over the baseline system based on evaluation using the BLEU metric. We provide experimental results on the NIST 2003 Chinese-English large data track evaluation. We also provide theoretical analysis of our algorithms and experiments that verify that our algorithms provide state-of-theart performance in machine translation. 1 Introduction The noisy-channel model (Brown et al., 1990) has been the foundation for statistical machine translation (SMT) for over ten years. Recently so-called reranking techniques, such as maximum entropy models (Och and Ney, 2002) and gradient methods (Och, 2003), have been applied to machine translation (MT), and have provided significant improvements. In this paper, we introduce two novel machine learning algorithms specialized for the MT task. Discriminative reranking algorithms have also contributed to improvements in natural language parsing and tagging performance. Discriminative reranking algorithms used for these applications include Perceptron, Boosting and Support Vector Machines (SVMs). In the machine learning community, some novel discriminative ranking (also called ordinal regression) algorithms have been"
N04-1023,P98-2162,1,0.686374,"ke word context into account. This means unlikely alignments are being considered while training the model and this also results in additional decoding complexity. Several MT models were proposed as extensions of the IBM models which used this intuition to add additional linguistic constraints to decrease the decoding perplexity and increase the translation quality. Wang and Waibel (1998) proposed an SMT model based on phrase-based alignments. Since their translation model reordered phrases directly, it achieved higher accuracy for translation between languages with different word orders. In (Och and Weber, 1998; Och et al., 1999), a two-level alignment model was employed to utilize shallow phrase structures: alignment between templates was used to handle phrase reordering, and word alignments within a template were used to handle phrase to phrase translation. However, phrase level alignment cannot handle long distance reordering effectively. Parse trees have also been used in alignment models. Wu (1997) introduced constraints on alignments using a probabilistic synchronous context-free grammar restricted to Chomskynormal form. (Wu, 1997) was an implicit or selforganizing syntax model as it did not u"
N04-1023,W99-0604,1,0.666456,"account. This means unlikely alignments are being considered while training the model and this also results in additional decoding complexity. Several MT models were proposed as extensions of the IBM models which used this intuition to add additional linguistic constraints to decrease the decoding perplexity and increase the translation quality. Wang and Waibel (1998) proposed an SMT model based on phrase-based alignments. Since their translation model reordered phrases directly, it achieved higher accuracy for translation between languages with different word orders. In (Och and Weber, 1998; Och et al., 1999), a two-level alignment model was employed to utilize shallow phrase structures: alignment between templates was used to handle phrase reordering, and word alignments within a template were used to handle phrase to phrase translation. However, phrase level alignment cannot handle long distance reordering effectively. Parse trees have also been used in alignment models. Wu (1997) introduced constraints on alignments using a probabilistic synchronous context-free grammar restricted to Chomskynormal form. (Wu, 1997) was an implicit or selforganizing syntax model as it did not use a Treebank. Yama"
N04-1023,P03-1021,1,0.0734557,"ne translation over the baseline system based on evaluation using the BLEU metric. We provide experimental results on the NIST 2003 Chinese-English large data track evaluation. We also provide theoretical analysis of our algorithms and experiments that verify that our algorithms provide state-of-theart performance in machine translation. 1 Introduction The noisy-channel model (Brown et al., 1990) has been the foundation for statistical machine translation (SMT) for over ten years. Recently so-called reranking techniques, such as maximum entropy models (Och and Ney, 2002) and gradient methods (Och, 2003), have been applied to machine translation (MT), and have provided significant improvements. In this paper, we introduce two novel machine learning algorithms specialized for the MT task. Discriminative reranking algorithms have also contributed to improvements in natural language parsing and tagging performance. Discriminative reranking algorithms used for these applications include Perceptron, Boosting and Support Vector Machines (SVMs). In the machine learning community, some novel discriminative ranking (also called ordinal regression) algorithms have been proposed in recent years. Based o"
N04-1023,2001.mtsummit-papers.68,0,0.0624691,"Missing"
N04-1023,W03-0402,1,0.817649,"2 - . 3.6  from .- , but not for the case of Large Margin Classifiers There are quite a few linear classifiers1 that can separate samples with large margin, such as SVMs (Vapnik, 1998), Boosting (Schapire et al., 1997), Winnow (Zhang, 2000) and Perceptron (Krauth and Mezard, 1987). The performance of SVMs is superior to other linear classifiers because of their ability to margin maximization. However, SVMs are extremely slow in training since they need to solve a quadratic programming search. For example, SVMs even cannot be used to train on the whole Penn Treebank in parse reranking (Shen and Joshi, 2003). Taking this into account, we use perceptron-like algorithms, since the perceptron algorithm is fast in training which allow us to do experiments on real-world data. Its large margin version is able to provide relatively good results in general. 3.7 Pairwise Samples In previous work on the PRank algorithm, ranks are defined on the entire training and test data. Thus we can define boundaries between consecutive ranks on the entire data. But in MT reranking, ranks are defined over every single source sentence. For example, in our data set, the rank of a translation is only the rank among all th"
N04-1023,P98-2221,0,0.0109325,"lity over candidate translations and the translation model   which is a generative conditional probability of the source sentence given a candidate translation  . The lexicon of the single-word based IBM models does not take word context into account. This means unlikely alignments are being considered while training the model and this also results in additional decoding complexity. Several MT models were proposed as extensions of the IBM models which used this intuition to add additional linguistic constraints to decrease the decoding perplexity and increase the translation quality. Wang and Waibel (1998) proposed an SMT model based on phrase-based alignments. Since their translation model reordered phrases directly, it achieved higher accuracy for translation between languages with different word orders. In (Och and Weber, 1998; Och et al., 1999), a two-level alignment model was employed to utilize shallow phrase structures: alignment between templates was used to handle phrase reordering, and word alignments within a template were used to handle phrase to phrase translation. However, phrase level alignment cannot handle long distance reordering effectively. Parse trees have also been used in"
N04-1023,J97-3002,0,0.0127999,"del based on phrase-based alignments. Since their translation model reordered phrases directly, it achieved higher accuracy for translation between languages with different word orders. In (Och and Weber, 1998; Och et al., 1999), a two-level alignment model was employed to utilize shallow phrase structures: alignment between templates was used to handle phrase reordering, and word alignments within a template were used to handle phrase to phrase translation. However, phrase level alignment cannot handle long distance reordering effectively. Parse trees have also been used in alignment models. Wu (1997) introduced constraints on alignments using a probabilistic synchronous context-free grammar restricted to Chomskynormal form. (Wu, 1997) was an implicit or selforganizing syntax model as it did not use a Treebank. Yamada and Knight (2001) used a statistical parser trained using a Treebank in the source language to produce parse trees and proposed a tree to string model for alignment. Gildea (2003) proposed a tree to tree alignment model using output from a statistical parser in both source and target languages. The translation model involved tree alignments in which subtree cloning was used t"
N04-1023,P01-1067,0,0.0765215,"999), a two-level alignment model was employed to utilize shallow phrase structures: alignment between templates was used to handle phrase reordering, and word alignments within a template were used to handle phrase to phrase translation. However, phrase level alignment cannot handle long distance reordering effectively. Parse trees have also been used in alignment models. Wu (1997) introduced constraints on alignments using a probabilistic synchronous context-free grammar restricted to Chomskynormal form. (Wu, 1997) was an implicit or selforganizing syntax model as it did not use a Treebank. Yamada and Knight (2001) used a statistical parser trained using a Treebank in the source language to produce parse trees and proposed a tree to string model for alignment. Gildea (2003) proposed a tree to tree alignment model using output from a statistical parser in both source and target languages. The translation model involved tree alignments in which subtree cloning was used to handle cases of reordering that were not possible in earlier tree-based alignment models. 1.2 Discriminative Models for MT Och and Ney (2002) proposed a framework for MT based on direct translation, using the conditional model"
N04-1023,A00-2018,0,\N,Missing
N04-1023,J93-2003,0,\N,Missing
N04-1023,W03-1012,1,\N,Missing
N04-1023,P02-1040,0,\N,Missing
N04-1023,J05-1003,0,\N,Missing
N04-1023,C98-2157,1,\N,Missing
N04-1023,C98-2216,0,\N,Missing
N07-2025,H05-1091,0,0.0402508,"proposed an approach that ∗ This research was partially supported by NSERC, Canada. solely considers the shallow semantic features extracted from sentences. For relation extraction in the newswire domain, syntactic features have been used in a generative model (Miller et al., 2000) and in a discriminative log-linear model (Kambhatla, 2004). In comparison, we use a much larger set of syntactic features extracted from parse trees, many of which have been shown useful in SRL task. Kernel-based methods have also been used for relation extraction (Zelenko et al., 2003; Culotta and Sorensen, 2004; Bunescu and Mooney, 2005) on various syntactic representations, such as dependency trees or constituencybased parse trees. In contrast, we explore a much wider variety of syntactic features in this work. To benefit from both views, a composite kernel (Zhang et al., 2006) integrates the flat features from entities and structured features from parse trees. In our work, we also combine a linear kernel with a tree kernel for improved performance. 2 SRL Features for Information Extraction Fig. 2 shows one example illustrating the ternary relation we are identifying. In this example, “Exoenzyme S” is a PROTEIN name, “extrac"
N07-2025,E06-1051,0,0.0201353,"bout where a PROTEIN is located in an ORGANISM, giving a valuable clue to the biological function of the PROTEIN and helping to identify suitable drug, vaccine and diagnostic targets. Fig. 1 illustrates possible locations of proteins in Gram+ and Gram− bacteria. Previous work in biomedical relation extraction task (Sekimizu et al., 1998; Blaschke et al., 1999; Feldman et al., 2002) suggested the use of predicate-argument structure by taking verbs as the center of the relation – in contrast, in this paper we directly link protein named entities (NEs) to their locations; in other related work, (Claudio et al., 2006) proposed an approach that ∗ This research was partially supported by NSERC, Canada. solely considers the shallow semantic features extracted from sentences. For relation extraction in the newswire domain, syntactic features have been used in a generative model (Miller et al., 2000) and in a discriminative log-linear model (Kambhatla, 2004). In comparison, we use a much larger set of syntactic features extracted from parse trees, many of which have been shown useful in SRL task. Kernel-based methods have also been used for relation extraction (Zelenko et al., 2003; Culotta and Sorensen, 2004;"
N07-2025,P04-1054,0,0.0993168,"work, (Claudio et al., 2006) proposed an approach that ∗ This research was partially supported by NSERC, Canada. solely considers the shallow semantic features extracted from sentences. For relation extraction in the newswire domain, syntactic features have been used in a generative model (Miller et al., 2000) and in a discriminative log-linear model (Kambhatla, 2004). In comparison, we use a much larger set of syntactic features extracted from parse trees, many of which have been shown useful in SRL task. Kernel-based methods have also been used for relation extraction (Zelenko et al., 2003; Culotta and Sorensen, 2004; Bunescu and Mooney, 2005) on various syntactic representations, such as dependency trees or constituencybased parse trees. In contrast, we explore a much wider variety of syntactic features in this work. To benefit from both views, a composite kernel (Zhang et al., 2006) integrates the flat features from entities and structured features from parse trees. In our work, we also combine a linear kernel with a tree kernel for improved performance. 2 SRL Features for Information Extraction Fig. 2 shows one example illustrating the ternary relation we are identifying. In this example, “Exoenzyme S”"
N07-2025,P04-3022,0,0.0138759,"e et al., 1999; Feldman et al., 2002) suggested the use of predicate-argument structure by taking verbs as the center of the relation – in contrast, in this paper we directly link protein named entities (NEs) to their locations; in other related work, (Claudio et al., 2006) proposed an approach that ∗ This research was partially supported by NSERC, Canada. solely considers the shallow semantic features extracted from sentences. For relation extraction in the newswire domain, syntactic features have been used in a generative model (Miller et al., 2000) and in a discriminative log-linear model (Kambhatla, 2004). In comparison, we use a much larger set of syntactic features extracted from parse trees, many of which have been shown useful in SRL task. Kernel-based methods have also been used for relation extraction (Zelenko et al., 2003; Culotta and Sorensen, 2004; Bunescu and Mooney, 2005) on various syntactic representations, such as dependency trees or constituencybased parse trees. In contrast, we explore a much wider variety of syntactic features in this work. To benefit from both views, a composite kernel (Zhang et al., 2006) integrates the flat features from entities and structured features fro"
N07-2025,A00-2030,0,0.166719,"edical relation extraction task (Sekimizu et al., 1998; Blaschke et al., 1999; Feldman et al., 2002) suggested the use of predicate-argument structure by taking verbs as the center of the relation – in contrast, in this paper we directly link protein named entities (NEs) to their locations; in other related work, (Claudio et al., 2006) proposed an approach that ∗ This research was partially supported by NSERC, Canada. solely considers the shallow semantic features extracted from sentences. For relation extraction in the newswire domain, syntactic features have been used in a generative model (Miller et al., 2000) and in a discriminative log-linear model (Kambhatla, 2004). In comparison, we use a much larger set of syntactic features extracted from parse trees, many of which have been shown useful in SRL task. Kernel-based methods have also been used for relation extraction (Zelenko et al., 2003; Culotta and Sorensen, 2004; Bunescu and Mooney, 2005) on various syntactic representations, such as dependency trees or constituencybased parse trees. In contrast, we explore a much wider variety of syntactic features in this work. To benefit from both views, a composite kernel (Zhang et al., 2006) integrates"
N07-2025,P04-1043,0,0.0811505,"th default linear kernel to feature vectors and Moschetti’s SVM-light-TK-1.22 with the default tree kernel. The models are: Baseline1 is a purely word-based system, where the features consist of the unigrams and bigrams between the PROTEIN name and the ORGANISM/LOCATION names inclusively, where the stopwords are selectively eliminated. Baseline2 is a naive approach that assumes that any example containing PROTEIN, LOCATION names has the PL relation. The same assumption is made for PO and POL relations. PAK system uses predicate-argument structure kernel (PAK) based method. PAK was defined in (Moschitti, 2004) and only considers the path from the predicate to the target argument, which in our setting is the path from the PROTEIN to the ORGANISM or LOCATION names. SRL is an SRL system which is adapted to use our new feature set. A default linear kernel is applied with SVM learning. TRK system is similar to PAK system except that the input is an entire parse tree instead of a PAK path. TRK+SRL combines full parse trees and manually extracted features and uses the kernel combination. 1 http://svmlight.joachims.org/ http://ai-nlp.info.uniroma2.it/moschitti/TK1.2software/Tree-Kernel.htm 2 Method PL PO P"
N07-2025,N07-2041,1,0.830826,", ORGANISM and LOCATION). The Syntactic Annotator parses the sentences and inserts the head information to the parse trees by using the Magerman/Collins head percolation rules. The main component of the system is our SRL-based relation extraction module, where we first manually extract features along the path from the PROTEIN name to the ORGANISM/LOCATION name and then train a binary SVM classifier for the binary relation extraction. Finally, we fuse the extracted binary relations into a ternary relation. In contrast with our discriminative model, a statistical parsing based generative model (Shi et al., 2007) has been proposed for a related task on this data set where the NEs and their relations are extracted together and used to identify which NEs are relevant in a particular sentence. Since our final goal is to facilitate the biologists to generate the annotated corpus, in future • each word and its Part-of-Speech (POS) tag of PRO name • head word (hw) and its POS of PRO name • subcategorization that records the immediate structure that expands from PRO name. Non-PRO daughters will be eliminated • POS of parent node of PRO name • hw and its POS of the parent node of PRO name • each word and its"
N07-2025,P06-1104,0,0.0197885,"ive model (Miller et al., 2000) and in a discriminative log-linear model (Kambhatla, 2004). In comparison, we use a much larger set of syntactic features extracted from parse trees, many of which have been shown useful in SRL task. Kernel-based methods have also been used for relation extraction (Zelenko et al., 2003; Culotta and Sorensen, 2004; Bunescu and Mooney, 2005) on various syntactic representations, such as dependency trees or constituencybased parse trees. In contrast, we explore a much wider variety of syntactic features in this work. To benefit from both views, a composite kernel (Zhang et al., 2006) integrates the flat features from entities and structured features from parse trees. In our work, we also combine a linear kernel with a tree kernel for improved performance. 2 SRL Features for Information Extraction Fig. 2 shows one example illustrating the ternary relation we are identifying. In this example, “Exoenzyme S” is a PROTEIN name, “extracellular” a LOCATION name and “Pseudomonas aeruginosa” an ORGANISM name. Our task is to identify if there exists a ”PROTEIN-ORGANISM-LOCATION” relation among these three NEs. To simplify the problem, we first reduce the POL 97 Proceedings of NAACL"
N07-2041,W04-3224,0,0.0163281,"ate relations) in the sentence by only recognizing relevant NEs. Each input sentence is assumed to have at least one BPL relation. Nine of 10 monoclonal antibodies mapped within the carboxyterminal region of [PROTEIN OprF] that is homologous to the [ORGANISM Escherichia coli] [LOCATION outer membrane] protein [PROTEIN OmpA]. 3 PO_PTR/PP Statistical Syntactic and Semantic Parser Similar to the approach in (Miller et al., 2000) and (Kulick et al., 2004), our parser integrates both syntactic and semantic annotations into a single annotation as shown in Figure 2. A lexicalized statistical parser (Bikel, 2004) is applied to the parsing task. The parse tree is decorated with two types of semantic annotations: 1) Annotations on relevant PROTEIN, BACTERIUM and LOCATION NEs. Tags are PROTEIN R, BACTERIUM R and LOCATION R respectively. 2) Annotations on paths between relevant NEs. The lower-most node that spans both NEs is tagged as LNK and all nodes along the path to the NEs are tagged as PTR. Binary relations are apparently much easier to represent on the parse tree, therefore we split the BPL ternary relation into two binary relations: BP (BACTERIUM and PROTEIN) and PL (PROTEIN and LOCATION). After c"
N07-2041,W04-3111,0,0.0258734,"are two PROTEIN NEs in the sentence below but only one, OmpA, is relevant. Our system aims to identify the correct BPL relation among all possible BPL tuples (candidate relations) in the sentence by only recognizing relevant NEs. Each input sentence is assumed to have at least one BPL relation. Nine of 10 monoclonal antibodies mapped within the carboxyterminal region of [PROTEIN OprF] that is homologous to the [ORGANISM Escherichia coli] [LOCATION outer membrane] protein [PROTEIN OmpA]. 3 PO_PTR/PP Statistical Syntactic and Semantic Parser Similar to the approach in (Miller et al., 2000) and (Kulick et al., 2004), our parser integrates both syntactic and semantic annotations into a single annotation as shown in Figure 2. A lexicalized statistical parser (Bikel, 2004) is applied to the parsing task. The parse tree is decorated with two types of semantic annotations: 1) Annotations on relevant PROTEIN, BACTERIUM and LOCATION NEs. Tags are PROTEIN R, BACTERIUM R and LOCATION R respectively. 2) Annotations on paths between relevant NEs. The lower-most node that spans both NEs is tagged as LNK and all nodes along the path to the NEs are tagged as PTR. Binary relations are apparently much easier to represen"
N07-2041,N07-2025,1,0.887915,"Missing"
N07-2041,A00-2030,0,0.155937,"RGANISM) NEs, e.g., there are two PROTEIN NEs in the sentence below but only one, OmpA, is relevant. Our system aims to identify the correct BPL relation among all possible BPL tuples (candidate relations) in the sentence by only recognizing relevant NEs. Each input sentence is assumed to have at least one BPL relation. Nine of 10 monoclonal antibodies mapped within the carboxyterminal region of [PROTEIN OprF] that is homologous to the [ORGANISM Escherichia coli] [LOCATION outer membrane] protein [PROTEIN OmpA]. 3 PO_PTR/PP Statistical Syntactic and Semantic Parser Similar to the approach in (Miller et al., 2000) and (Kulick et al., 2004), our parser integrates both syntactic and semantic annotations into a single annotation as shown in Figure 2. A lexicalized statistical parser (Bikel, 2004) is applied to the parsing task. The parse tree is decorated with two types of semantic annotations: 1) Annotations on relevant PROTEIN, BACTERIUM and LOCATION NEs. Tags are PROTEIN R, BACTERIUM R and LOCATION R respectively. 2) Annotations on paths between relevant NEs. The lower-most node that spans both NEs is tagged as LNK and all nodes along the path to the NEs are tagged as PTR. Binary relations are apparent"
N07-2041,P05-1022,0,\N,Missing
N09-1047,2005.iwslt-1.7,0,0.244638,"Missing"
N09-1047,W07-0737,0,0.119679,"Missing"
N09-1047,P03-1021,0,0.00557207,"test 2K in-dom L 11K in-dom U 20K See Sec. 4.2 Bangla in-dom dev 450 in-dom test 1K Hansards Fr out-dom L 5K Table 1: Specification of different data sets we will use in experiments. The target language is English in the bilingual sets, and the source languages are either French (Fr), German (Ge), Spanish (Sp), or Bangla. model which assigns a penalty based on the number of source words which are skipped when generating a new target phrase, and (d) a word penalty. These different models are combined log-linearly. Their weights are optimized w.r.t. BLEU score using the algorithm described in (Och, 2003). This is done on a development corpus which we will call dev1 in this paper. The weight vectors in n-gram and similarity methods are set to (.15, .2, .3, .35) to emphasize longer n-grams. We set α = β = .35 for HAS, and use the 100-best list of translations when identifying candidate phrases while setting the maximum phrase length to 10. We set ǫ = .5 to smooth probabilities when computing scores based on translation units. 4.1 Simulated Low Density Language Pairs We use three language pairs (French-English, German-English, Spanish-English) to compare all of the proposed sentence selection st"
N09-1047,P08-1098,0,0.0227829,"e to this method (or its computationally demanding generalization in which instead of a single sentence, several sets of sentences of size k are selected and ranked) we use a hill climbing search on the surface of dev2’s BLEU score. For a fixed value of the weight vector, dev1 sentences are ranked and then the top-k output is selected and the amount of improvement the retrained SMT system gives on dev2’s BLEU score is measured. Starting from a random initial value for αk ’s, we improve one dimension at a time and traverse the discrete grid 2 To see how different rankings can be combined, see (Reichart et al., 2008) which proposes this for multi-task AL. 3 Here the retrained SMT model is the one learned by adding a particular sentence from dev1 into L. 418 placed on the values of the weight vector. Starting with a coarse grid, we make it finer when we get stuck in local optima during hill climbing. 3.5 Hierarchical Adaptive Sampling (HAS) (Dasgupta and Hsu, 2008) propose a technique for sample selection that, under certain settings, is guaranteed to be no worse than random sampling. Their method exploits the cluster structure (if there is any) in the unlabeled data. Ideally, querying the label of only on"
N09-1047,W08-0305,0,0.117908,"Missing"
N09-1047,J07-1003,0,0.015363,"Missing"
N09-1047,P07-1004,1,0.944538,"orithm 1 AL-SMT 1: Given bilingual corpus L, and monolingual corpus U . 2: MF →E = train(L, ∅) 3: for t = 1, 2, ... do 4: U + = translate(U, MF →E ) 5: Select k sentence pairs from U + , and ask a human for their true translations. 6: Remove the k sentences from U , and add the k sentence pairs (translated by human) to L 7: MF →E = train(L, U + ) 8: Monitor the performance on the test set T 9: end for Phrase tables from U + will get a 0 score in minimum error rate training if they are not useful, so our method is more general. Also, this method has been shown empirically to be more effective (Ueffing et al., 2007b) than (1) using the weighted combination of the two phrase tables from L and U + , or (2) combining the two sets of data and training from the bitext L ∪ U + . The setup in Algorithm 1 helps us to investigate how to maximally take advantage of human effort (for sentence translation) when learning an SMT model from the available data, that includes bilingual and monolingual text. 3 Sentence Selection Strategies Our sentence selection strategies can be divided into two categories: (1) those which are independent of the target language and just look into the source language, and (2) those which"
N09-1047,W07-0724,0,\N,Missing
N12-1060,P09-1088,0,0.0574024,"Missing"
N12-1060,P05-1033,0,0.0656876,"l., 2009) reordering glue rule (§ 3) and a monotonic X-glue rule (§ 4). Our experiments show the resulting Shallow-n decoding is comparable in translation quality to full Hiero-style decoding while at the same time being considerably faster. All the experiments in this paper were done using Kriya (Sankaran et al., 2012) hierarchical phrasebased system which also supports decoding with Shallow-n grammars. We extended Kriya to additionally support reordering glue rules as well. Shallow-n grammars (de Gispert et al., 2010) were introduced to reduce over-generation in the Hiero translation model (Chiang, 2005) resulting in much faster decoding and restricting reordering to a desired level for specific language pairs. However, Shallow-n grammars require parameters which cannot be directly optimized using minimum error-rate tuning by the decoder. This paper introduces some novel improvements to the translation model for Shallow-n grammars. We introduce two rules: a BITG-style reordering glue rule and a simpler monotonic concatenation rule. We use separate features for the new rules in our loglinear model allowing the decoder to directly optimize the feature weights. We show this formulation of Shallo"
N12-1060,J07-2003,0,0.320953,"r introduces some novel improvements to the translation model for Shallow-n grammars. We introduce two rules: a BITG-style reordering glue rule and a simpler monotonic concatenation rule. We use separate features for the new rules in our loglinear model allowing the decoder to directly optimize the feature weights. We show this formulation of Shallow-n hierarchical phrasebased translation is comparable in translation quality to full Hiero-style decoding (without shallow rules) while at the same time being considerably faster. 1 Introduction Hierarchical phrase-based translation (Chiang, 2005; Chiang, 2007) extends the highly lexicalized models from phrase-based translation systems in order to model lexicalized reordering and discontiguous phrases. However, a major drawback in this approach, when compared to phrase-based systems, is the total number of rules that are learnt are several orders of magnitude larger than standard phrase tables, which leads to over-generation and search errors and contribute to much longer decoding times. Several approaches have been proposed to address these issues: from filtering the extracted synchronous grammar (Zollmann et al., 2008; He et al., 2009; Iglesias et"
N12-1060,J10-3008,0,0.0649877,"Missing"
N12-1060,E09-1044,0,0.0429841,"Missing"
N12-1060,P02-1040,0,0.0830908,"arts 1 and 3 (1928 sentences) and MTC part 4 (919 sentences) respectively. We used the usual pre-processing pipeline and an additional segmentation step for the Chinese side of the bitext using the LDC segmenter2 . Our log-linear model uses the standard features conditional (p(e|f ) and p(f |e)) and lexical (pl (e|f ) and pl (f |e)) probabilities, phrase (pp ) and word (wp ) penalties, language model and regular glue penalty (mg ) apart from two additional features for R−glue (rg ) and X−glue (xg ). Table 2 shows the BLEU scores and decoding time for the MTC test-set. We provide the IBM BLEU (Papineni et al., 2002) scores for the Shallown grammars for order: n = 1, 2, 3 and compare it to the full-Hiero baseline. Finally, we experiment with two variants of the S glue rules, i) a restricted version where the glue rules combine only X at level N , (column ’Glue: X N ’ in table), ii) more free variant where they are allowed to use any X freely (column ’Glue: X’ in table). As it can be seen, the unrestricted glue rules variant (column ’Glue: X’) consistently outperforms the glue rules restricted to the top-level non-terminal X N , achieving a maximum BLEU score of 26.24, which is about 1.4 BLEU points higher"
N12-1060,W09-3804,0,0.021845,"pproaches for learning minimal grammars (Blunsom et al., 2008; Blunsom et al., 2009; Sankaran et al., 2011). The idea of Shallow-n grammars (de Gispert et al., 2010) takes an orthogonal direction for controlling the over-generation and search space in Hiero decoder by restricting the degree of nesting allowed for Hierarchical rules. We propose an novel statistical model for Shallow-n grammars which does not require additional non-terminals for monotonic re-ordering and also eliminates hand-tuned parameters and instead introduces an automatically tunable alternative. We introduce a BITG-style (Saers et al., 2009) reordering glue rule (§ 3) and a monotonic X-glue rule (§ 4). Our experiments show the resulting Shallow-n decoding is comparable in translation quality to full Hiero-style decoding while at the same time being considerably faster. All the experiments in this paper were done using Kriya (Sankaran et al., 2012) hierarchical phrasebased system which also supports decoding with Shallow-n grammars. We extended Kriya to additionally support reordering glue rules as well. Shallow-n grammars (de Gispert et al., 2010) were introduced to reduce over-generation in the Hiero translation model (Chiang, 2"
N12-1060,W11-2167,1,0.835296,"Missing"
N12-1060,C08-1144,0,0.0190696,"rase-based translation (Chiang, 2005; Chiang, 2007) extends the highly lexicalized models from phrase-based translation systems in order to model lexicalized reordering and discontiguous phrases. However, a major drawback in this approach, when compared to phrase-based systems, is the total number of rules that are learnt are several orders of magnitude larger than standard phrase tables, which leads to over-generation and search errors and contribute to much longer decoding times. Several approaches have been proposed to address these issues: from filtering the extracted synchronous grammar (Zollmann et al., 2008; He et al., 2009; Iglesias et al., 2009) to alternative 2 Shallow-n Grammars Formally a Shallow-n grammar G is defined as a 5tuple: G = (N, T, R, Rg , S), such that T is a set of finite terminals and N a set of finite non-terminals {X 0 , . . . , X N }. Rg refers to the glue rules that rewrite the start symbol S: S → &lt;X, X&gt; (1) S → &lt;SX, SX&gt; (2) R is the set of finite production rules in G and has two types, viz. hierarchical (3) and terminal (4). The hierarchical rules at each level n are additionally conditioned to have at least one X n−1 non-terminal 533 2012 Conference of the North America"
N12-1060,2010.iwslt-keynotes.2,0,\N,Missing
N13-1115,D11-1031,0,0.0571522,"Missing"
N13-1115,aziz-etal-2012-pet,0,0.0328302,"Missing"
N13-1115,N12-1062,0,0.181062,"imizing only one. 1 Introduction Tuning algorithms are used to find the weights for a statistical machine translation (MT) model by minimizing error with respect to a single MT evaluation metric. The tuning process improves the performance of an SMT system as measured by this metric; with BLEU (Papineni et al., 2002) being the most popular choice. Minimum error-rate training (MERT) (Och, 2003) was the first approach in MT to directly optimize an evaluation metric. Several alternatives now exist: MIRA (Watanabe et al., 2007; Chiang et al., 2008), PRO (Hopkins and May, 2011), linear regression (Bazrafshan et al., 2012) and ORO (Watanabe, 2012) among others. However these approaches optimize towards the best score as reported by a single evaluation metric. MT system developers typically use BLEU and Kevin Duh Nara Institute of Science & Technology Ikoma, Nara. JAPAN kevinduh@is.naist.jp ignore all the other metrics. This is done despite the fact that other metrics model wide-ranging aspects of translation: from measuring the translation edit rate (TER) in matching a translation output to a human reference (Snover et al., 2006), to capturing lexical choices in translation as in METEOR (Lavie and Denkowski, 20"
N13-1115,W11-2103,0,0.0418762,"hing a translation output to a human reference (Snover et al., 2006), to capturing lexical choices in translation as in METEOR (Lavie and Denkowski, 2009) to modelling semantic similarity through textual entailment (Pad´o et al., 2009) to RIBES, an evaluation metric that pays attention to long-distance reordering (Isozaki et al., 2010). While some of these metrics such as TER, METEOR are gaining prominence, BLEU enjoys the status of being the de facto standard tuning metric as it is often claimed and sometimes observed that optimizing with BLEU produces better translations than other metrics (Callison-Burch et al., 2011). The gains obtained by the MT system tuned on a particular metric do not improve performance as measured under other metrics (Cer et al., 2010), suggesting that over-fitting to a specific metric might happen without improvements in translation quality. In this paper we propose a new tuning framework for jointly optimizing multiple evaluation metrics. Pareto-optimality is a natural way to think about multi-metric optimization and multi-metric optimization (MMO) was recently explored using the notion of Pareto optimality in the Pareto-based Multi-objective Optimization (PMO) approach (Duh et al"
N13-1115,N10-1080,0,0.492006,"to modelling semantic similarity through textual entailment (Pad´o et al., 2009) to RIBES, an evaluation metric that pays attention to long-distance reordering (Isozaki et al., 2010). While some of these metrics such as TER, METEOR are gaining prominence, BLEU enjoys the status of being the de facto standard tuning metric as it is often claimed and sometimes observed that optimizing with BLEU produces better translations than other metrics (Callison-Burch et al., 2011). The gains obtained by the MT system tuned on a particular metric do not improve performance as measured under other metrics (Cer et al., 2010), suggesting that over-fitting to a specific metric might happen without improvements in translation quality. In this paper we propose a new tuning framework for jointly optimizing multiple evaluation metrics. Pareto-optimality is a natural way to think about multi-metric optimization and multi-metric optimization (MMO) was recently explored using the notion of Pareto optimality in the Pareto-based Multi-objective Optimization (PMO) approach (Duh et al., 2012). PMO provides several equivalent solutions (parameter weights) having different trade-offs between the different MT metrics. In (Duh et"
N13-1115,P12-1098,0,0.0365651,"Missing"
N13-1115,N12-1047,0,0.069524,"Missing"
N13-1115,D08-1024,0,0.0752244,"oduce better MT output, optimizing multiple metrics is better than optimizing only one. 1 Introduction Tuning algorithms are used to find the weights for a statistical machine translation (MT) model by minimizing error with respect to a single MT evaluation metric. The tuning process improves the performance of an SMT system as measured by this metric; with BLEU (Papineni et al., 2002) being the most popular choice. Minimum error-rate training (MERT) (Och, 2003) was the first approach in MT to directly optimize an evaluation metric. Several alternatives now exist: MIRA (Watanabe et al., 2007; Chiang et al., 2008), PRO (Hopkins and May, 2011), linear regression (Bazrafshan et al., 2012) and ORO (Watanabe, 2012) among others. However these approaches optimize towards the best score as reported by a single evaluation metric. MT system developers typically use BLEU and Kevin Duh Nara Institute of Science & Technology Ikoma, Nara. JAPAN kevinduh@is.naist.jp ignore all the other metrics. This is done despite the fact that other metrics model wide-ranging aspects of translation: from measuring the translation edit rate (TER) in matching a translation output to a human reference (Snover et al., 2006), to capt"
N13-1115,J07-2003,0,0.0354992,"on that prefers the locally best component. These are simpler to implement and also performed competitively in their domain adaptation experiments. Unless explicitly noted otherwise, the results presented in Section 6 are based on linear mixture operation log-wsum, which empirically performed better than the log-wmax for ensemble tuning. 6 Experiments We evaluate the different methods on ArabicEnglish translation in single as well as multiple references scenario. Corpus statistics are shown in Table 1. For all the experiments in this paper, we use Kriya, our in-house Hierarchical phrasebased (Chiang, 2007) (Hiero) system, and integrated the required changes for ensemble decoding. Kriya performs comparably to the state of the art in phrase-based and hierarchical phrase-based translation over a wide variety of language pairs and data sets (Sankaran et al., 2012). We use PRO (Hopkins and May, 2011) for optimizing the feature weights and PMO-PRO (Duh et al., 2012) for optimizing meta weights, wherever applicable. In both cases, we use SVMRank (Joachims, 2006) as the optimizer. We used the default parameter settings for different MT tuning metrics. For METEOR, we tried both METEOR-tune and METEOR-ht"
N13-1115,W11-2107,0,0.0297673,"was marginally better in METEOR3 and RIBES scores. We observed the margin of loss in BLEU and TER to outweigh the gains in METEOR and RIBES and we chose METEOR-hter setting for both optimization and evaluation of all our experiments. 6.1 Evaluation on Tuning Set Unlike conventional tuning methods, PMO (Duh et al., 2012) was originally evaluated on the tuning set to avoid potential mismatch with the test set. In order to ensure robustness of evaluation, they redecode the devset using the optimal weights from the last tuning iteration and report the scores on 13 This behaviour was also noted by Denkowski and Lavie (2011) in their analysis of Urdu-English system for tunable metrics task in WMT11. 952 best candidates. Corpus ISI corpus Training size 1.1 M Tuning/ test set 1664/ 1313 (MTA) 1982/ 987 (ISI) Table 1: Corpus Statistics (# of sentences) for Arabic-English. MTA (4-refs) and ISI (1-ref). We follow the same strategy and compare our PMO-ensemble approach with PMO-PRO (denoted P) and a linear combination4 (denoted L) baseline. Similar to Duh et al. (2012), we use five different BLEU:RIBES weight settings, viz. (0.0, 1.0), (0.3, 0.7), (0.5, 0.5), (0.7, 0.3) and (1.0, 0.0), marked L1 through L5 or P1 throug"
N13-1115,N12-1059,0,0.0551629,"Missing"
N13-1115,P12-1001,1,0.900409,"l., 2011). The gains obtained by the MT system tuned on a particular metric do not improve performance as measured under other metrics (Cer et al., 2010), suggesting that over-fitting to a specific metric might happen without improvements in translation quality. In this paper we propose a new tuning framework for jointly optimizing multiple evaluation metrics. Pareto-optimality is a natural way to think about multi-metric optimization and multi-metric optimization (MMO) was recently explored using the notion of Pareto optimality in the Pareto-based Multi-objective Optimization (PMO) approach (Duh et al., 2012). PMO provides several equivalent solutions (parameter weights) having different trade-offs between the different MT metrics. In (Duh et al., 2012) the choice of which option to use rests with the MT system developer and in that sense their approach is an a posteriori method to specify the preference (Marler and Arora, 2004). In contrast to this, our tuning framework provides a principled way of using the Pareto optimal options using ensemble decoding (Razmara et al., 2012). We also introduce a novel method of ensemble tuning for jointly tuning multiple MT evaluation metrics and further combin"
N13-1115,W09-0426,0,0.329084,"Missing"
N13-1115,N12-1023,0,0.115129,"Missing"
N13-1115,D11-1138,0,0.0566372,"Missing"
N13-1115,P12-1031,0,0.119299,"Missing"
N13-1115,2009.mtsummit-posters.8,0,0.106993,"Missing"
N13-1115,D11-1125,0,0.214609,"imizing multiple metrics is better than optimizing only one. 1 Introduction Tuning algorithms are used to find the weights for a statistical machine translation (MT) model by minimizing error with respect to a single MT evaluation metric. The tuning process improves the performance of an SMT system as measured by this metric; with BLEU (Papineni et al., 2002) being the most popular choice. Minimum error-rate training (MERT) (Och, 2003) was the first approach in MT to directly optimize an evaluation metric. Several alternatives now exist: MIRA (Watanabe et al., 2007; Chiang et al., 2008), PRO (Hopkins and May, 2011), linear regression (Bazrafshan et al., 2012) and ORO (Watanabe, 2012) among others. However these approaches optimize towards the best score as reported by a single evaluation metric. MT system developers typically use BLEU and Kevin Duh Nara Institute of Science & Technology Ikoma, Nara. JAPAN kevinduh@is.naist.jp ignore all the other metrics. This is done despite the fact that other metrics model wide-ranging aspects of translation: from measuring the translation edit rate (TER) in matching a translation output to a human reference (Snover et al., 2006), to capturing lexical choices in tran"
N13-1115,D11-1035,0,0.119177,"Missing"
N13-1115,mauser-etal-2008-automatic,0,0.0487964,"Missing"
N13-1115,P03-1021,0,0.0645367,"is contrasts the traditional tuning where gains are usually limited to a single metric. Our human evaluation results confirm that in order to produce better MT output, optimizing multiple metrics is better than optimizing only one. 1 Introduction Tuning algorithms are used to find the weights for a statistical machine translation (MT) model by minimizing error with respect to a single MT evaluation metric. The tuning process improves the performance of an SMT system as measured by this metric; with BLEU (Papineni et al., 2002) being the most popular choice. Minimum error-rate training (MERT) (Och, 2003) was the first approach in MT to directly optimize an evaluation metric. Several alternatives now exist: MIRA (Watanabe et al., 2007; Chiang et al., 2008), PRO (Hopkins and May, 2011), linear regression (Bazrafshan et al., 2012) and ORO (Watanabe, 2012) among others. However these approaches optimize towards the best score as reported by a single evaluation metric. MT system developers typically use BLEU and Kevin Duh Nara Institute of Science & Technology Ikoma, Nara. JAPAN kevinduh@is.naist.jp ignore all the other metrics. This is done despite the fact that other metrics model wide-ranging a"
N13-1115,P02-1040,0,0.102374,"ross several metrics (BLEU, RIBES), without any significant reduction in other metrics. This contrasts the traditional tuning where gains are usually limited to a single metric. Our human evaluation results confirm that in order to produce better MT output, optimizing multiple metrics is better than optimizing only one. 1 Introduction Tuning algorithms are used to find the weights for a statistical machine translation (MT) model by minimizing error with respect to a single MT evaluation metric. The tuning process improves the performance of an SMT system as measured by this metric; with BLEU (Papineni et al., 2002) being the most popular choice. Minimum error-rate training (MERT) (Och, 2003) was the first approach in MT to directly optimize an evaluation metric. Several alternatives now exist: MIRA (Watanabe et al., 2007; Chiang et al., 2008), PRO (Hopkins and May, 2011), linear regression (Bazrafshan et al., 2012) and ORO (Watanabe, 2012) among others. However these approaches optimize towards the best score as reported by a single evaluation metric. MT system developers typically use BLEU and Kevin Duh Nara Institute of Science & Technology Ikoma, Nara. JAPAN kevinduh@is.naist.jp ignore all the other"
N13-1115,P12-1099,1,0.916893,"was recently explored using the notion of Pareto optimality in the Pareto-based Multi-objective Optimization (PMO) approach (Duh et al., 2012). PMO provides several equivalent solutions (parameter weights) having different trade-offs between the different MT metrics. In (Duh et al., 2012) the choice of which option to use rests with the MT system developer and in that sense their approach is an a posteriori method to specify the preference (Marler and Arora, 2004). In contrast to this, our tuning framework provides a principled way of using the Pareto optimal options using ensemble decoding (Razmara et al., 2012). We also introduce a novel method of ensemble tuning for jointly tuning multiple MT evaluation metrics and further combine this with the PMO ap947 Proceedings of NAACL-HLT 2013, pages 947–957, c Atlanta, Georgia, 9–14 June 2013. 2013 Association for Computational Linguistics proach (Duh et al., 2012). We also introduce three other approaches for multi-metric tuning and compare their performance to the ensemble tuning. Our experiments yield the highest metric scores across many different metrics (that are being optimized), something that has not been possible until now. Our ensemble tuning met"
N13-1115,P12-1002,0,0.121243,"Missing"
N13-1115,2006.amta-papers.25,0,0.571545,"., 2007; Chiang et al., 2008), PRO (Hopkins and May, 2011), linear regression (Bazrafshan et al., 2012) and ORO (Watanabe, 2012) among others. However these approaches optimize towards the best score as reported by a single evaluation metric. MT system developers typically use BLEU and Kevin Duh Nara Institute of Science & Technology Ikoma, Nara. JAPAN kevinduh@is.naist.jp ignore all the other metrics. This is done despite the fact that other metrics model wide-ranging aspects of translation: from measuring the translation edit rate (TER) in matching a translation output to a human reference (Snover et al., 2006), to capturing lexical choices in translation as in METEOR (Lavie and Denkowski, 2009) to modelling semantic similarity through textual entailment (Pad´o et al., 2009) to RIBES, an evaluation metric that pays attention to long-distance reordering (Isozaki et al., 2010). While some of these metrics such as TER, METEOR are gaining prominence, BLEU enjoys the status of being the de facto standard tuning metric as it is often claimed and sometimes observed that optimizing with BLEU produces better translations than other metrics (Callison-Burch et al., 2011). The gains obtained by the MT system tu"
N13-1115,D11-1117,0,0.167625,", 2012). We also introduce three other approaches for multi-metric tuning and compare their performance to the ensemble tuning. Our experiments yield the highest metric scores across many different metrics (that are being optimized), something that has not been possible until now. Our ensemble tuning method over multiple metrics produced superior translations than single metric tuning as measured by a post-editing task. HTER (Snover et al., 2006) scores in our human evaluation confirm that multi-metric optimization can lead to better MT output. 2 Related Work In grammar induction and parsing (Spitkovsky et al., 2011; Hall et al., 2011; Auli and Lopez, 2011) have proposed multi-objective methods based on roundrobin iteration of single objective optimizations. Research in SMT parameter tuning has seen a surge of interest recently, including online/batch learning (Watanabe, 2012; Cherry and Foster, 2012), large-scale training (Simianer et al., 2012; He and Deng, 2012), and new discriminative objectives (Gimpel and Smith, 2012; Zheng et al., 2012; Bazrafshan et al., 2012). However, few works have investigated the multi-metric tuning problem in depth. Linear combination of BLEU and TER is reported in (Zaidan,"
N13-1115,D07-1080,0,0.0904751,"irm that in order to produce better MT output, optimizing multiple metrics is better than optimizing only one. 1 Introduction Tuning algorithms are used to find the weights for a statistical machine translation (MT) model by minimizing error with respect to a single MT evaluation metric. The tuning process improves the performance of an SMT system as measured by this metric; with BLEU (Papineni et al., 2002) being the most popular choice. Minimum error-rate training (MERT) (Och, 2003) was the first approach in MT to directly optimize an evaluation metric. Several alternatives now exist: MIRA (Watanabe et al., 2007; Chiang et al., 2008), PRO (Hopkins and May, 2011), linear regression (Bazrafshan et al., 2012) and ORO (Watanabe, 2012) among others. However these approaches optimize towards the best score as reported by a single evaluation metric. MT system developers typically use BLEU and Kevin Duh Nara Institute of Science & Technology Ikoma, Nara. JAPAN kevinduh@is.naist.jp ignore all the other metrics. This is done despite the fact that other metrics model wide-ranging aspects of translation: from measuring the translation edit rate (TER) in matching a translation output to a human reference (Snover"
N13-1115,N12-1026,0,0.381172,"uning algorithms are used to find the weights for a statistical machine translation (MT) model by minimizing error with respect to a single MT evaluation metric. The tuning process improves the performance of an SMT system as measured by this metric; with BLEU (Papineni et al., 2002) being the most popular choice. Minimum error-rate training (MERT) (Och, 2003) was the first approach in MT to directly optimize an evaluation metric. Several alternatives now exist: MIRA (Watanabe et al., 2007; Chiang et al., 2008), PRO (Hopkins and May, 2011), linear regression (Bazrafshan et al., 2012) and ORO (Watanabe, 2012) among others. However these approaches optimize towards the best score as reported by a single evaluation metric. MT system developers typically use BLEU and Kevin Duh Nara Institute of Science & Technology Ikoma, Nara. JAPAN kevinduh@is.naist.jp ignore all the other metrics. This is done despite the fact that other metrics model wide-ranging aspects of translation: from measuring the translation edit rate (TER) in matching a translation output to a human reference (Snover et al., 2006), to capturing lexical choices in translation as in METEOR (Lavie and Denkowski, 2009) to modelling semantic"
N13-1115,D10-1092,1,\N,Missing
N13-1115,2012.eamt-1.31,0,\N,Missing
N13-1115,2012.tc-1.5,0,\N,Missing
P07-1004,J93-2003,0,0.0190284,"eights are optimized w.r.t. BLEU score using the algorithm described in (Och, 2003). This is done on a development corpus which we will call dev1 in this paper. The search algorithm implemented in the decoder is a dynamic-programming beam-search algorithm. After the main decoding step, rescoring with additional models is performed. The system generates a 5,000-best list of alternative translations for each source sentence. These lists are rescored with the following models: (a) the different models used in the decoder which are described above, (b) two different features based on IBM Model 1 (Brown et al., 1993), (c) posterior probabilities for words, phrases, n-grams, and sentence length (Zens and Ney, 2006; Ueffing and Ney, 2007), all calculated over the N best list and using the sentence probabilities which the baseline system assigns to the translation hypotheses. The weights of these additional models and of the decoder models are again optimized to maximize BLEU score. This is performed on a second development corpus, dev2. 3 The Framework 3.1 The Algorithm Our transductive learning algorithm, Algorithm 1, is inspired by the Yarowsky algorithm (Yarowsky, 1995; Abney, 2004). The algorithm works"
P07-1004,P04-1023,0,0.0375155,"Missing"
P07-1004,2002.tmi-tutorials.2,0,0.0466643,"3.2 Table 5: Translation quality using an additional phrase table trained on monolingual Chinese news data. Selection step using threshold on confidence scores. NIST Chinese–English. word alignment. Experiments showed that putting a large weight on the model trained on labeled data performs best. Along similar lines, (Fraser and Marcu, 2006) combine a generative model of word alignment with a log-linear discriminative model trained on a small set of hand aligned sentences. The word alignments are used to train a standard phrasebased SMT system, resulting in increased translation quality . In (Callison-Burch, 2002) co-training is applied to MT. This approach requires several source languages which are sentence-aligned with each other and all translate into the same target language. One language pair creates data for another language pair and can be naturally used in a (Blum and Mitchell, 1998)-style co-training algorithm. Experiments on the EuroParl corpus show a decrease in WER. However, the selection algorithm applied there is actually supervised because it takes the reference translation into account. Moreover, when the algorithm is run long enough, large amounts of co-trained data injected too much"
P07-1004,P06-1097,0,0.0197031,"Missing"
P07-1004,niessen-etal-2000-evaluation,0,0.120084,"Missing"
P07-1004,P03-1021,0,0.0835163,"ployed by the decoder are: (a) one or several phrase table(s), which model the translation direction p(s |t), (b) one or several n-gram language model(s) trained with the SRILM toolkit (Stolcke, 2002); in the experiments reported here, we used 4-gram models on the NIST data, and a trigram model on EuroParl, (c) a distortion model which assigns a penalty based on the number of source words which are skipped when generating a new target phrase, and (d) a word penalty. These different models are combined loglinearly. Their weights are optimized w.r.t. BLEU score using the algorithm described in (Och, 2003). This is done on a development corpus which we will call dev1 in this paper. The search algorithm implemented in the decoder is a dynamic-programming beam-search algorithm. After the main decoding step, rescoring with additional models is performed. The system generates a 5,000-best list of alternative translations for each source sentence. These lists are rescored with the following models: (a) the different models used in the decoder which are described above, (b) two different features based on IBM Model 1 (Brown et al., 1993), (c) posterior probabilities for words, phrases, n-grams, and s"
P07-1004,P02-1040,0,0.121563,"Missing"
P07-1004,J07-1003,1,0.66008,"us which we will call dev1 in this paper. The search algorithm implemented in the decoder is a dynamic-programming beam-search algorithm. After the main decoding step, rescoring with additional models is performed. The system generates a 5,000-best list of alternative translations for each source sentence. These lists are rescored with the following models: (a) the different models used in the decoder which are described above, (b) two different features based on IBM Model 1 (Brown et al., 1993), (c) posterior probabilities for words, phrases, n-grams, and sentence length (Zens and Ney, 2006; Ueffing and Ney, 2007), all calculated over the N best list and using the sentence probabilities which the baseline system assigns to the translation hypotheses. The weights of these additional models and of the decoder models are again optimized to maximize BLEU score. This is performed on a second development corpus, dev2. 3 The Framework 3.1 The Algorithm Our transductive learning algorithm, Algorithm 1, is inspired by the Yarowsky algorithm (Yarowsky, 1995; Abney, 2004). The algorithm works as follows: First, the translation model is estimated based on the sentence pairs in the bilingual training data L. Then,"
P07-1004,2006.iwslt-papers.3,1,0.922512,"r scores 11: Xi := Xi ∪ {(tn , s, π (i) (tn |s))N n=1 } 12: end for 13: Scoring step: Si := Score(Xi ) // Assign a score to sentence pairs (t, s) from X. 14: Selection step: Ti := Select(Xi , Si ) // Choose a subset of good sentence pairs (t, s) from X. 15: i := i + 1. 16: until i &gt; R of the model on labeled and unlabeled data which can be very expensive if L is very large (as on the Chinese–English data set). This additional phrase table is small and specific to the development or test set it is trained on. It overlaps with the original phrase tables, but also contains many new phrase pairs (Ueffing, 2006). Mixture Model: Another alternative for Estimate is to create a mixture model of the phrase table probabilities with new phrase table probabilities p(s |t) = λ · Lp (s |t) + (1 − λ) · Tp (s |t) (2) where Lp and Tp are phrase table probabilities estimated on L and T , respectively. In cases where new phrase pairs are learned from T , they get added into the merged phrase table. 3.3 The Scoring Function In Algorithm 1, the Score function assigns a score to each translation hypothesis t. We used the following scoring functions in our experiments: Length-normalized Score: Each translated sentence"
P07-1004,P95-1026,0,0.188406,"features based on IBM Model 1 (Brown et al., 1993), (c) posterior probabilities for words, phrases, n-grams, and sentence length (Zens and Ney, 2006; Ueffing and Ney, 2007), all calculated over the N best list and using the sentence probabilities which the baseline system assigns to the translation hypotheses. The weights of these additional models and of the decoder models are again optimized to maximize BLEU score. This is performed on a second development corpus, dev2. 3 The Framework 3.1 The Algorithm Our transductive learning algorithm, Algorithm 1, is inspired by the Yarowsky algorithm (Yarowsky, 1995; Abney, 2004). The algorithm works as follows: First, the translation model is estimated based on the sentence pairs in the bilingual training data L. Then, a set of source language sentences, U , is translated based on the current model. A subset of good translations and their sources, Ti , is selected in each 26 iteration and added to the training data. These selected sentence pairs are replaced in each iteration, and only the original bilingual training data, L, is kept fixed throughout the algorithm. The process of generating sentence pairs, selecting a subset of good sentence pairs, and"
P07-1004,W06-3110,0,0.021885,"n a development corpus which we will call dev1 in this paper. The search algorithm implemented in the decoder is a dynamic-programming beam-search algorithm. After the main decoding step, rescoring with additional models is performed. The system generates a 5,000-best list of alternative translations for each source sentence. These lists are rescored with the following models: (a) the different models used in the decoder which are described above, (b) two different features based on IBM Model 1 (Brown et al., 1993), (c) posterior probabilities for words, phrases, n-grams, and sentence length (Zens and Ney, 2006; Ueffing and Ney, 2007), all calculated over the N best list and using the sentence probabilities which the baseline system assigns to the translation hypotheses. The weights of these additional models and of the decoder models are again optimized to maximize BLEU score. This is performed on a second development corpus, dev2. 3 The Framework 3.1 The Algorithm Our transductive learning algorithm, Algorithm 1, is inspired by the Yarowsky algorithm (Yarowsky, 1995; Abney, 2004). The algorithm works as follows: First, the translation model is estimated based on the sentence pairs in the bilingual"
P07-1004,C04-1046,1,\N,Missing
P07-1004,W07-0724,1,\N,Missing
P07-1004,J04-3004,0,\N,Missing
P09-1021,N07-1029,0,0.0521419,"ild multiple MT systems from multiple source languages to the new target language, each MT system can be seen as a different ‘view’ on the desired output translation. Thus, we can train our multiple MT systems using either self-training or co-training (Blum and Mitchell, 1998). In selftraining each MT system is re-trained using human labeled data plus its own noisy translation output on the unlabeled data. In co-training each MT system is re-trained using human labeled data plus noisy translation output from the other MT systems in the ensemble. We use consensus translations (He et al., 2008; Rosti et al., 2007; Matusov et al., 2006) as an effective method for co-training between multiple MT systems. This paper makes the following contributions: • We provide a new framework for multilingual MT, in which we build multiple MT systems and add a new language to an existing multilingual parallel corpus. The multilingual setStatistical machine translation (SMT) models require bilingual corpora for training, and these corpora are often multilingual with parallel text in multiple languages simultaneously. We introduce an active learning task of adding a new language to an existing multilingual set of parall"
P09-1021,P07-1004,1,0.0888127,"f highly informative sentences from U, and add those sentences together with their human-provided translations to L. This process is continued iteratively until a certain level of translation quality is met (we use the BLEU score, WER and PER) (Papineni et al., 2002). In the baseline, against which we compare our sentence selection methods, the sentences are chosen randomly. When (re-)training the models, two phrase tables are learned for each SMT model: one from the labeled data L and the other one from pseudolabeled data U+ (which we call the main and auxiliary phrase tables respectively). (Ueffing et al., 2007; Haffari et al., 2009) show that treating U+ as a source for a new feature function in a loglinear model for SMT (Och and Ney, 2004) allows us to maximally take advantage of unlabeled data by finding a weight for this feature using minimum error-rate training (MERT) (Och, 2003). Since each entry in U+ has multiple translations, there are two options when building the auxiliary table for a particular language pair (F d , E): (i) to use the corresponding translation ed of the source language in a self-training setting, or (ii) to use the consensus translation among all the translation candidate"
P09-1021,W03-0310,0,0.0653416,"see Fig. 5). Enhancing the estimated distributions to capture this power law behavior would improve the quality of the proposed sentence selection methods. (Reichart et al., 2008) introduces multi-task active learning where unlabeled data require annotations for multiple tasks, e.g. they consider namedentities and parse trees, and showed that multiple tasks helps selection compared to individual tasks. Our setting is different in that the target language is the same across multiple MT tasks, which we exploit to use consensus translations and cotraining to improve active learning performance. (Callison-Burch and Osborne, 2003b; CallisonBurch and Osborne, 2003a) provide a co-training approach to MT, where one language pair creates data for another language pair. In contrast, our co-training approach uses consensus translations and our setting for active learning is very different from their semi-supervised setting. A Ph.D. proposal by Chris Callison-Burch (Callison-burch, 2003) lays out the promise of AL for SMT and proposes some algorithms. However, the lack of experimental results means that performance and feasibility of those methods cannot be compared to ours. While we use consensus translations (He et al., 20"
P09-1021,P07-1092,0,0.0556948,"∗ KL(Preg k Preg ) 4.37 4.17 4.38 ∗ KL(Preg k unif ) 5.37 5.21 5.80 ∗ KL(Poov k Poov ) 3.04 4.58 4.73 ∗ KL(Poov k unif ) 3.41 4.75 4.99 Table 2: For regular/OOV phrases, the KL-divergence between the true distribution (P ∗ ) and the estimated (P ) or uniform (unif ) distributions are shown, where: ∗ P (x) KL(P ∗ k P ) := x P ∗ (x) log PP (x) . Analysis The basis for our proposed methods has been the popularity of regular/OOV phrases in U and their data is very noisy and future work should omit this pair. 4 Choice of Germanic and Romance for our experimental setting is inspired by results in (Cohn and Lapata, 2007) 187 Regular Phrases in U between the true and uniform distributions, in all three language pairs. Since uniform distribution conveys no information, this is evidence that there is some information encoded in the estimated distribution about the true distribution. However we noticed that the true distributions of regular/OOV phrases exhibit Zipfian (power law) behavior5 which is not well captured by the estimated distributions (see Fig. 5). Enhancing the estimated distributions to capture this power law behavior would improve the quality of the proposed sentence selection methods. (Reichart et"
P09-1021,C02-1117,0,0.0346595,"Missing"
P09-1021,N09-1047,1,0.822574,"entences from U, and add those sentences together with their human-provided translations to L. This process is continued iteratively until a certain level of translation quality is met (we use the BLEU score, WER and PER) (Papineni et al., 2002). In the baseline, against which we compare our sentence selection methods, the sentences are chosen randomly. When (re-)training the models, two phrase tables are learned for each SMT model: one from the labeled data L and the other one from pseudolabeled data U+ (which we call the main and auxiliary phrase tables respectively). (Ueffing et al., 2007; Haffari et al., 2009) show that treating U+ as a source for a new feature function in a loglinear model for SMT (Och and Ney, 2004) allows us to maximally take advantage of unlabeled data by finding a weight for this feature using minimum error-rate training (MERT) (Och, 2003). Since each entry in U+ has multiple translations, there are two options when building the auxiliary table for a particular language pair (F d , E): (i) to use the corresponding translation ed of the source language in a self-training setting, or (ii) to use the consensus translation among all the translation candidates (e1 , .., eD ) in a c"
P09-1021,D08-1011,0,0.0328607,"tions. When we build multiple MT systems from multiple source languages to the new target language, each MT system can be seen as a different ‘view’ on the desired output translation. Thus, we can train our multiple MT systems using either self-training or co-training (Blum and Mitchell, 1998). In selftraining each MT system is re-trained using human labeled data plus its own noisy translation output on the unlabeled data. In co-training each MT system is re-trained using human labeled data plus noisy translation output from the other MT systems in the ensemble. We use consensus translations (He et al., 2008; Rosti et al., 2007; Matusov et al., 2006) as an effective method for co-training between multiple MT systems. This paper makes the following contributions: • We provide a new framework for multilingual MT, in which we build multiple MT systems and add a new language to an existing multilingual parallel corpus. The multilingual setStatistical machine translation (SMT) models require bilingual corpora for training, and these corpora are often multilingual with parallel text in multiple languages simultaneously. We introduce an active learning task of adding a new language to an existing multil"
P09-1021,2005.mtsummit-papers.11,0,0.0110887,"ials responsible for generating phrases in each of the labeled and unlabeled data sets. To generate a phrase, we first toss a coin and depending on the outcome we either generate the phrase from the multinomial associated with reguoov lar phrases θ reg U or potential phrases θ U : 5 Sentence Scoring The sentence score is a linear combination of two terms: one coming from regular phrases and the other from OOV phrases: Experiments X λ P (x|θ U ) log reg P (x|θ L ) |Xs | reg x∈Xs + 1−λ |Xsoov | X X x∈Xsoov h∈Hx y∈Yx Corpora. We pre-processed the EuroParl corpus (http://www.statmt.org/europarl) (Koehn, 2005) and built a multilingual parallel corpus with 653,513 sentences, excluding the Q4/2000 portion of the data (2000-10 to 2000-12) which is reserved as the test set. We subsampled 5,000 sentences as the labeled data L and 20,000 sentences as U for the pool of untranslated sentences (while hiding the English part). The test set consists of 2,000 multi-language sentences and comes from the multilingual parallel corpus built from Q4/2000 portion of the data. Consensus Finding. Let T be the union of the nbest lists of translations for a particular sentence. The consensus translation tc is where θ U"
P09-1021,E06-1005,0,0.120986,"ems from multiple source languages to the new target language, each MT system can be seen as a different ‘view’ on the desired output translation. Thus, we can train our multiple MT systems using either self-training or co-training (Blum and Mitchell, 1998). In selftraining each MT system is re-trained using human labeled data plus its own noisy translation output on the unlabeled data. In co-training each MT system is re-trained using human labeled data plus noisy translation output from the other MT systems in the ensemble. We use consensus translations (He et al., 2008; Rosti et al., 2007; Matusov et al., 2006) as an effective method for co-training between multiple MT systems. This paper makes the following contributions: • We provide a new framework for multilingual MT, in which we build multiple MT systems and add a new language to an existing multilingual parallel corpus. The multilingual setStatistical machine translation (SMT) models require bilingual corpora for training, and these corpora are often multilingual with parallel text in multiple languages simultaneously. We introduce an active learning task of adding a new language to an existing multilingual set of parallel text and constructin"
P09-1021,J04-4002,0,0.00429109,"ntinued iteratively until a certain level of translation quality is met (we use the BLEU score, WER and PER) (Papineni et al., 2002). In the baseline, against which we compare our sentence selection methods, the sentences are chosen randomly. When (re-)training the models, two phrase tables are learned for each SMT model: one from the labeled data L and the other one from pseudolabeled data U+ (which we call the main and auxiliary phrase tables respectively). (Ueffing et al., 2007; Haffari et al., 2009) show that treating U+ as a source for a new feature function in a loglinear model for SMT (Och and Ney, 2004) allows us to maximally take advantage of unlabeled data by finding a weight for this feature using minimum error-rate training (MERT) (Och, 2003). Since each entry in U+ has multiple translations, there are two options when building the auxiliary table for a particular language pair (F d , E): (i) to use the corresponding translation ed of the source language in a self-training setting, or (ii) to use the consensus translation among all the translation candidates (e1 , .., eD ) in a co-training setting (sharing information between multiple SMT models). A whole range of methods exist in the li"
P09-1021,P03-1021,0,0.0118839,"inst which we compare our sentence selection methods, the sentences are chosen randomly. When (re-)training the models, two phrase tables are learned for each SMT model: one from the labeled data L and the other one from pseudolabeled data U+ (which we call the main and auxiliary phrase tables respectively). (Ueffing et al., 2007; Haffari et al., 2009) show that treating U+ as a source for a new feature function in a loglinear model for SMT (Och and Ney, 2004) allows us to maximally take advantage of unlabeled data by finding a weight for this feature using minimum error-rate training (MERT) (Och, 2003). Since each entry in U+ has multiple translations, there are two options when building the auxiliary table for a particular language pair (F d , E): (i) to use the corresponding translation ed of the source language in a self-training setting, or (ii) to use the consensus translation among all the translation candidates (e1 , .., eD ) in a co-training setting (sharing information between multiple SMT models). A whole range of methods exist in the literature for combining the output translations of multiple MT systems for a single language pair, operating either at the sentence, phrase, or wor"
P09-1021,P02-1040,0,0.105082,"tial MT systems {MF d →E }D d=1 on the multilingual corpus L, and use them to translate all monolingual sentences in U. We denote sentences in U together with their multiple translations by U+ (line 4 of Algorithm 1). Then we retrain the SMT systems on L ∪ U+ and use the resulting model to decode the test set. Afterwards, we select and remove a subset of highly informative sentences from U, and add those sentences together with their human-provided translations to L. This process is continued iteratively until a certain level of translation quality is met (we use the BLEU score, WER and PER) (Papineni et al., 2002). In the baseline, against which we compare our sentence selection methods, the sentences are chosen randomly. When (re-)training the models, two phrase tables are learned for each SMT model: one from the labeled data L and the other one from pseudolabeled data U+ (which we call the main and auxiliary phrase tables respectively). (Ueffing et al., 2007; Haffari et al., 2009) show that treating U+ as a source for a new feature function in a loglinear model for SMT (Och and Ney, 2004) allows us to maximally take advantage of unlabeled data by finding a weight for this feature using minimum error-"
P09-1021,P08-1098,0,0.383296,"pean Parliament (EuroParl) and U.N. proceedings. In this paper, we consider how to use active learning (AL) in order to add a new language to such a multilingual parallel corpus and at the same time we construct an MT system from each language in the original corpus into this new target language. We introduce a novel combined measure of translation quality for multiple target language outputs (the same content from multiple source languages). The multilingual setting provides new opportunities for AL over and above a single language pair. This setting is similar to the multi-task AL scenario (Reichart et al., 2008). In our case, the multiple tasks are individual machine translation tasks for several language pairs. The nature of the translation processes vary from any of the source ∗ Thanks to James Peltier for systems support for our experiments. This research was partially supported by NSERC, Canada (RGPIN: 264905) and an IBM Faculty Award. 181 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 181–189, c Suntec, Singapore, 2-7 August 2009. 2009 ACL and AFNLP overload the term entry to denote a tuple in L or in U (it should be clear from the context). For a single"
P09-1021,W07-0724,0,\N,Missing
P11-1004,P08-1087,0,0.238648,"sup L-match system, even though the word choices lead to a lower BLEU score overall when compared to Unsup L-match. 4 Related Work The work on morphology in MT can be grouped into three categories, factored models, segmented translation, and morphology generation. Factored models (Koehn and Hoang, 2007) factor the phrase translation probabilities over additional information annotated to each word, allowing for text to be represented on multiple levels of analysis. We discussed the drawbacks of factored models for our task in Section 2.1. While (Koehn and Hoang, 2007; Yang and Kirchhoff, 2006; Avramidis and Koehn, 2008) obtain improvements using factored models for translation into English, German, Spanish, and Czech, these models may be less useful for capturing long-distance dependencies in languages with much more complex morphological systems such as Finnish. In our experiments factored models did worse than the baseline. Segmented translation performs morphological analysis on the morphologically complex text for use in the translation model (Brown et al., 1993; Goldwater and McClosky, 2005; de Gispert and Mari˜ no, 2008). This method unpacks complex forms into simpler, more frequently occurring compone"
P11-1004,J93-2003,0,0.0123913,"ysis. We discussed the drawbacks of factored models for our task in Section 2.1. While (Koehn and Hoang, 2007; Yang and Kirchhoff, 2006; Avramidis and Koehn, 2008) obtain improvements using factored models for translation into English, German, Spanish, and Czech, these models may be less useful for capturing long-distance dependencies in languages with much more complex morphological systems such as Finnish. In our experiments factored models did worse than the baseline. Segmented translation performs morphological analysis on the morphologically complex text for use in the translation model (Brown et al., 1993; Goldwater and McClosky, 2005; de Gispert and Mari˜ no, 2008). This method unpacks complex forms into simpler, more frequently occurring components, and may also increase the symmetry of the lexically realized content be(1) Input: ‘the charter we are to approve today both strengthens and gives visible shape to the common fundamental rights and values our community is to be based upon.’ a. Reference: perusoikeuskirja , jonka t¨ an¨ a¨ an aiomme hyv¨ aksy¨ a , sek¨ a vahvistaa ett¨ a selvent¨ a¨ a (selvent¨ aa ¨/VERB/ACT/INF/SG/LAT-clarify) niit¨ a (ne/PRONOUN/PL/PAR-them) yhteisi¨ a perusoikeu"
P11-1004,W08-0336,0,0.0338745,"Missing"
P11-1004,P05-1066,0,0.0600523,"Missing"
P11-1004,H05-1085,0,0.100818,"he drawbacks of factored models for our task in Section 2.1. While (Koehn and Hoang, 2007; Yang and Kirchhoff, 2006; Avramidis and Koehn, 2008) obtain improvements using factored models for translation into English, German, Spanish, and Czech, these models may be less useful for capturing long-distance dependencies in languages with much more complex morphological systems such as Finnish. In our experiments factored models did worse than the baseline. Segmented translation performs morphological analysis on the morphologically complex text for use in the translation model (Brown et al., 1993; Goldwater and McClosky, 2005; de Gispert and Mari˜ no, 2008). This method unpacks complex forms into simpler, more frequently occurring components, and may also increase the symmetry of the lexically realized content be(1) Input: ‘the charter we are to approve today both strengthens and gives visible shape to the common fundamental rights and values our community is to be based upon.’ a. Reference: perusoikeuskirja , jonka t¨ an¨ a¨ an aiomme hyv¨ aksy¨ a , sek¨ a vahvistaa ett¨ a selvent¨ a¨ a (selvent¨ aa ¨/VERB/ACT/INF/SG/LAT-clarify) niit¨ a (ne/PRONOUN/PL/PAR-them) yhteisi¨ a perusoikeuksia ja arvoja , joiden on olt"
P11-1004,D07-1091,0,0.524654,"guistic analysis of the output. Our proposed approaches are significantly better than the state of the art, achieving the highest reported BLEU scores on the English-Finnish Europarl version 3 data-set. Our linguistic analysis shows that our models have fewer morpho-syntactic errors compared to the word-based baseline. 2 Models 2.1 Baseline Models We set up three baseline models for comparison in this work. The first is a basic wordbased model (called Baseline in the results); we trained this on the original unsegmented version of the text. Our second baseline is a factored translation model (Koehn and Hoang, 2007) (called Factored), which used as factors the word, “stem”1 and suffix. These are derived from the same unsupervised segmentation model used in other experiments. The results (Table 3) show that a factored model was unable to match the scores of a simple wordbased baseline. We hypothesize that this may be an inherently difficult representational form for a language with the degree of morphological complexity found in Finnish. Because the morphology generation must be precomputed, for languages with a high degree of morphological complexity, the combinatorial explosion makes it unmanageable to"
P11-1004,P07-2045,0,0.006726,"all but the last suffix collapsed and called the “stem”. Total Morph Hanging Morph Training Set 64,106,047 30,837,615 10,906,406 Test Set 21,938 5,191 296 Table 1: Morpheme occurences in the phrase table and in translation. any word type that contained a match from the most frequent suffix set, looking for the longest matching suffix character string. We call this method Unsup L-match. After the segmentation, word-internal morpheme boundary markers were inserted into the segmented text to be used to reconstruct the surface forms in the MT output. We then trained the Moses phrase-based system (Koehn et al., 2007) on the segmented and marked text. After decoding, it was a simple matter to join together all adjacent morphemes with word-internal boundary markers to reconstruct the surface forms. Figure 1(a) gives the full model overview for all the variants of the segmented translation model (supervised/unsupervised; with and without the Unsup L-match procedure). Table 1 shows how morphemes are being used in the MT system. Of the phrases that included segmentations (‘Morph’ in Table 1), roughly a third were ‘productive’, i.e. had a hanging morpheme (with a form such as stem+) that could be joined to a su"
P11-1004,2005.mtsummit-papers.11,0,0.0370492,"in our experiments, unsupervised morphology always outperforms the use of a hand-built morphological analyzer. Rather than focusing on a few linguistically motivated aspects of Finnish morphological behaviour, we develop techniques for handling morphological complexity in general. We chose Finnish as our target language for this work, because it exemplifies many of the problems morphologically complex languages present for SMT. Among all the languages in the Europarl data-set, Finnish is the most difficult language to translate from and into, as was demonstrated in the MT Summit shared task (Koehn, 2005). Another reason is the current lack of knowledge about how to apply SMT successfully to agglutinative languages like Turkish or Finnish. Our main contributions are: 1) the introduction of the notion of segmented translation where we explicitly allow phrase pairs that can end with a dangling morpheme, which can connect with other morphemes as part of the translation process, and 2) the use of a fully segmented translation model in combination with a post-processing morpheme prediction system, using unsupervised morphology induction. Both of these approaches beat the state of the art on the Eng"
P11-1004,D10-1015,0,0.208008,"lation: ‘and in this context, we should value the last measures as the Commissioner Fischler, that his experience in Morocco has on the 26th and high-level meetings took place, including with the king.’ Figure 3: Morphological fluency analysis (see Section 3.1). tween source and target. In a somewhat orthogonal approach to ours, (Ma et al., 2007) use alignment of a parallel text to pack together adjacent segments in the alignment output, which are then fed back to the word aligner to bootstrap an improved alignment, which is then used in the translation model. We compared our results against (Luong et al., 2010) in Table 3 since their results are directly comparable to ours. They use a segmented phrase table and language model along with the word-based versions in the decoder and in tuning a Finnish target. Their approach requires segmented phrases 39 to match word boundaries, eliminating morphologically productive phrases. In their work a segmented language model can score a translation, but cannot insert morphology that does not show source-side reflexes. In order to perform a similar experiment that still allowed for morphologically productive phrases, we tried training a segmented translation mod"
P11-1004,P07-1039,0,0.042039,"Missing"
P11-1004,P07-1017,0,0.359088,"s major vocabulary reduction in the translation model, and allows the use of morphologically targeted features for modeling inflection. A possible disadvantage of this approach is that in this model there is no opportunity to con34 sider the morphology in translation since it is removed prior to training the translation model. Morphology generation models can use a variety of bilingual and contextual information to capture dependencies between morphemes, often more long-distance than what is possible using n-gram language models over morphemes in the segmented model. Similar to previous work (Minkov et al., 2007; Toutanova et al., 2008), we model morphology generation as a sequence learning problem. Unlike previous work, we use unsupervised morphology induction and use automatically generated suffix classes as tags. The first phase of our morphology prediction model is to train a MT system that produces morphologically simplified word forms in the target language. The output word forms are complex stems (a stem and some suffixes) but still missing some important suffix morphemes. In the second phase, the output of the MT decoder is then tagged with a sequence of abstract suffix tags. In particular, t"
P11-1004,P02-1040,0,0.101926,"with other morphemes as part of the translation process, and 2) the use of a fully segmented translation model in combination with a post-processing morpheme prediction system, using unsupervised morphology induction. Both of these approaches beat the state of the art on the English-Finnish translation task. Morphology can express both content and function categories, and our experiments show that it is important to use morphology both within the translation model (for morphology with content) and outside it (for morphology contributing to fluency). Automatic evaluation measures for MT, BLEU (Papineni et al., 2002), WER (Word Error Rate) and PER (Position Independent Word Error Rate) use the word as the basic unit rather than morphemes. In a word comProceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 32–42, c Portland, Oregon, June 19-24, 2011. 2011 Association for Computational Linguistics prised of multiple morphemes, getting even a single morpheme wrong means the entire word is wrong. In addition to standard MT evaluation measures, we perform a detailed linguistic analysis of the output. Our proposed approaches are significantly better than the state of the"
P11-1004,popovic-ney-2004-towards,0,0.267206,"Missing"
P11-1004,P09-1090,0,0.0125672,"Missing"
P11-1004,P06-1122,0,0.0295151,"Missing"
P11-1004,P08-1059,0,0.357928,"duction in the translation model, and allows the use of morphologically targeted features for modeling inflection. A possible disadvantage of this approach is that in this model there is no opportunity to con34 sider the morphology in translation since it is removed prior to training the translation model. Morphology generation models can use a variety of bilingual and contextual information to capture dependencies between morphemes, often more long-distance than what is possible using n-gram language models over morphemes in the segmented model. Similar to previous work (Minkov et al., 2007; Toutanova et al., 2008), we model morphology generation as a sequence learning problem. Unlike previous work, we use unsupervised morphology induction and use automatically generated suffix classes as tags. The first phase of our morphology prediction model is to train a MT system that produces morphologically simplified word forms in the target language. The output word forms are complex stems (a stem and some suffixes) but still missing some important suffix morphemes. In the second phase, the output of the MT decoder is then tagged with a sequence of abstract suffix tags. In particular, the output of the MT decod"
P11-1004,E06-1006,0,0.0105811,"e segmented translation Unsup L-match system, even though the word choices lead to a lower BLEU score overall when compared to Unsup L-match. 4 Related Work The work on morphology in MT can be grouped into three categories, factored models, segmented translation, and morphology generation. Factored models (Koehn and Hoang, 2007) factor the phrase translation probabilities over additional information annotated to each word, allowing for text to be represented on multiple levels of analysis. We discussed the drawbacks of factored models for our task in Section 2.1. While (Koehn and Hoang, 2007; Yang and Kirchhoff, 2006; Avramidis and Koehn, 2008) obtain improvements using factored models for translation into English, German, Spanish, and Czech, these models may be less useful for capturing long-distance dependencies in languages with much more complex morphological systems such as Finnish. In our experiments factored models did worse than the baseline. Segmented translation performs morphological analysis on the morphologically complex text for use in the translation model (Brown et al., 1993; Goldwater and McClosky, 2005; de Gispert and Mari˜ no, 2008). This method unpacks complex forms into simpler, more"
P11-1004,P08-2015,0,\N,Missing
P11-2125,N09-2066,0,0.0108452,"Missing"
P11-2125,J92-4003,0,0.626577,"inative dependency parsing was provided in (Koo et al., 2008) which involved clustering the labeled and unlabeled data and then each word in the dependency treebank was assigned a cluster identifier. These identifiers were used to augment the feature representation of the edge-factored or secondorder features, and this extended feature set was used to discriminatively train a dependency parser. The use of clusters leads to the question of how to integrate various types of clusters (possibly from different clustering algorithms) in discriminative dependency parsing. Clusters obtained from the (Brown et al., 1992) clustering algorithm are typically viewed as “semantic”, e.g. one cluster might contain plan, letter, request, memo, . . . while another may contain people, customers, employees, students, . . .. Another clustering view that is more “syntactic” in nature comes from the use of statesplitting in PCFGs. For instance, we could extract a syntactic cluster loss, time, profit, earnings, performance, rating, . . .: all head words of noun phrases corresponding to cluster of direct objects of 710 Marzieh Razavi and Anoop Sarkar School of Computing Science Simon Fraser University Vancouver, Canada {mraz"
P11-2125,D07-1101,0,0.00496803,"pendency Parsing A dependency tree represents the syntactic structure of a sentence with a directed graph (Figure 1), where nodes correspond to the words, and arcs indicate head-modifier pairs (Mel’ˇcuk, 1987). Graph-based dependency parsing searches for the highest-scoring tree according to a part-factored scoring function. In the first-order parsing models, the parts are individual head-modifier arcs in the dependency tree (McDonald et al., 2005). In the higher-order models, the parts consist of arcs together with some context, e.g. the parent or the sister arcs (McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010). With a linear scoring function, the parse for a sentence s X is: PARSE(s) = arg max w · f (s, r) (1) t∈T (s) r∈t where T (s) is the space of dependency trees for s, and f (s, r) is the feature vector for the part r which is linearly combined using the model parameter w to give the part score. The above arg max search for non-projective dependency parsing is accomProceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 710–714, c Portland, Oregon, June 19-24, 2011. 2011 Association for Computational Linguistics root For"
P11-2125,W02-1001,0,0.0100834,"m the split non-terminals from the Berkeley parser output. The first row under the words are the split POS tags (Syn-Low), the second row are the split bracketing tags (Syn-High), and the third row is the first 4 bits (to save space in this figure) of the (Brown et al., 1992) clusters. plished using minimum spanning tree algorithms (West, 2001) or approximate inference algorithms (Smith and Eisner, 2008; Koo et al., 2010). The (Eisner, 1996) algorithm is typically used for projective parsing. The model parameters are trained using a discriminative learning algorithm, e.g. averaged perceptron (Collins, 2002) or MIRA (Crammer and Singer, 2003). In this paper, we work with both first-order and second-order models, we train the models using MIRA, and we use the (Eisner, 1996) algorithm for inference. The baseline features capture information about the lexical items and their part of speech (POS) tags (as defined in (McDonald et al., 2005)). In this work, following (Koo et al., 2008), we use word cluster identifiers as the source of an additional set of features. The reader is directed to (Koo et al., 2008) for the list of cluster-based feature templates. The clusters inject long distance syntactic o"
P11-2125,C96-1058,0,0.108579,"NP-18 1010 access NN-13 NP-24 0011 to TO-0 TO-0 0011 American JJ-31 JJ-31 0110 markets NNS-25 NP-9 1011 Figure 1: Dependency tree with cluster identifiers obtained from the split non-terminals from the Berkeley parser output. The first row under the words are the split POS tags (Syn-Low), the second row are the split bracketing tags (Syn-High), and the third row is the first 4 bits (to save space in this figure) of the (Brown et al., 1992) clusters. plished using minimum spanning tree algorithms (West, 2001) or approximate inference algorithms (Smith and Eisner, 2008; Koo et al., 2010). The (Eisner, 1996) algorithm is typically used for projective parsing. The model parameters are trained using a discriminative learning algorithm, e.g. averaged perceptron (Collins, 2002) or MIRA (Crammer and Singer, 2003). In this paper, we work with both first-order and second-order models, we train the models using MIRA, and we use the (Eisner, 1996) algorithm for inference. The baseline features capture information about the lexical items and their part of speech (POS) tags (as defined in (McDonald et al., 2005)). In this work, following (Koo et al., 2008), we use word cluster identifiers as the source of a"
P11-2125,D07-1097,0,0.0881868,"s it is Syn-High, and for Pronouns it is Syn-Low that is the best. But the ensemble always does the best in every grammatical category. Fig. 2(b) shows the F-score of the different models for various dependency lengths, where the length of a dependency from word wi to word wj is equal to |i − j|. We see that different models are experts on different lengths (Syn-Low on 8, Syn-High on 9), while the ensemble model can always combine their expertise and do better at each length. 6 Comparison to Related Work Several ensemble models have been proposed for dependency parsing (Sagae and Lavie, 2006; Hall et al., 2007; Nivre and McDonald, 2008; Attardi and Dell’Orletta, 2009; Surdeanu and Manning, 2010). Essentially, all of these approaches combine different dependency parsing systems, i.e. transitionbased and graph-based. Although graph-based models are globally trained and can use exact inference algorithms, their features are defined over a limited history of parsing decisions. Since transitionbased parsing models have the opposite characteristics, the idea is to combine these two types of models to exploit their complementary strengths. The base parsing models are either independently trained (Sagae an"
P11-2125,P10-1001,0,0.00584032,"A dependency tree represents the syntactic structure of a sentence with a directed graph (Figure 1), where nodes correspond to the words, and arcs indicate head-modifier pairs (Mel’ˇcuk, 1987). Graph-based dependency parsing searches for the highest-scoring tree according to a part-factored scoring function. In the first-order parsing models, the parts are individual head-modifier arcs in the dependency tree (McDonald et al., 2005). In the higher-order models, the parts consist of arcs together with some context, e.g. the parent or the sister arcs (McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010). With a linear scoring function, the parse for a sentence s X is: PARSE(s) = arg max w · f (s, r) (1) t∈T (s) r∈t where T (s) is the space of dependency trees for s, and f (s, r) is the feature vector for the part r which is linearly combined using the model parameter w to give the part score. The above arg max search for non-projective dependency parsing is accomProceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 710–714, c Portland, Oregon, June 19-24, 2011. 2011 Association for Computational Linguistics root For IN-1 PP-2 0111 , ,-0 ,-"
P11-2125,P08-1068,0,0.701852,"rithms (Smith and Eisner, 2008; Koo et al., 2010). The (Eisner, 1996) algorithm is typically used for projective parsing. The model parameters are trained using a discriminative learning algorithm, e.g. averaged perceptron (Collins, 2002) or MIRA (Crammer and Singer, 2003). In this paper, we work with both first-order and second-order models, we train the models using MIRA, and we use the (Eisner, 1996) algorithm for inference. The baseline features capture information about the lexical items and their part of speech (POS) tags (as defined in (McDonald et al., 2005)). In this work, following (Koo et al., 2008), we use word cluster identifiers as the source of an additional set of features. The reader is directed to (Koo et al., 2008) for the list of cluster-based feature templates. The clusters inject long distance syntactic or semantic information into the model (in contrast with the use of POS tags in the baseline) and help alleviate the sparse data problem for complex features that include n-grams. 3 The Ensemble Model A word can have different syntactic or semantic cluster representations, each of which may lead to a different parsing model. We use ensemble learning (Dietterich, 2002) in order"
P11-2125,D10-1125,0,0.00600379,"-1 S-14 0101 trend NN-23 NP-18 1010 access NN-13 NP-24 0011 to TO-0 TO-0 0011 American JJ-31 JJ-31 0110 markets NNS-25 NP-9 1011 Figure 1: Dependency tree with cluster identifiers obtained from the split non-terminals from the Berkeley parser output. The first row under the words are the split POS tags (Syn-Low), the second row are the split bracketing tags (Syn-High), and the third row is the first 4 bits (to save space in this figure) of the (Brown et al., 1992) clusters. plished using minimum spanning tree algorithms (West, 2001) or approximate inference algorithms (Smith and Eisner, 2008; Koo et al., 2010). The (Eisner, 1996) algorithm is typically used for projective parsing. The model parameters are trained using a discriminative learning algorithm, e.g. averaged perceptron (Collins, 2002) or MIRA (Crammer and Singer, 2003). In this paper, we work with both first-order and second-order models, we train the models using MIRA, and we use the (Eisner, 1996) algorithm for inference. The baseline features capture information about the lexical items and their part of speech (POS) tags (as defined in (McDonald et al., 2005)). In this work, following (Koo et al., 2008), we use word cluster identifier"
P11-2125,D07-1013,0,0.011974,"understand the contribution of each model to the ensemble, we take a closer look at the parsing errors for each model and the ensemble. For each dependent to head depen1 code.google.com/p/berkeleyparser Sentences of the Penn Treebank were excluded from the text used for the clustering. 3 people.csail.mit.edu/maestro/papers/bllip-clusters.gz 4 Terry Koo was kind enough to share the source code for the (Koo et al., 2008) paper with us, and we plan to incorporate all the features in our future work. 2 713 dency, Fig. 2(a) shows the error rate for each dependent grouped by a coarse POS tag (c.f. (McDonald and Nivre, 2007)). For most POS categories, the Brown cluster model is the best individual model, but for Adjectives it is Syn-High, and for Pronouns it is Syn-Low that is the best. But the ensemble always does the best in every grammatical category. Fig. 2(b) shows the F-score of the different models for various dependency lengths, where the length of a dependency from word wi to word wj is equal to |i − j|. We see that different models are experts on different lengths (Syn-Low on 8, Syn-High on 9), while the ensemble model can always combine their expertise and do better at each length. 6 Comparison to Rela"
P11-2125,E06-1011,0,0.0170693,"ross all our test sets. 2 Dependency Parsing A dependency tree represents the syntactic structure of a sentence with a directed graph (Figure 1), where nodes correspond to the words, and arcs indicate head-modifier pairs (Mel’ˇcuk, 1987). Graph-based dependency parsing searches for the highest-scoring tree according to a part-factored scoring function. In the first-order parsing models, the parts are individual head-modifier arcs in the dependency tree (McDonald et al., 2005). In the higher-order models, the parts consist of arcs together with some context, e.g. the parent or the sister arcs (McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010). With a linear scoring function, the parse for a sentence s X is: PARSE(s) = arg max w · f (s, r) (1) t∈T (s) r∈t where T (s) is the space of dependency trees for s, and f (s, r) is the feature vector for the part r which is linearly combined using the model parameter w to give the part score. The above arg max search for non-projective dependency parsing is accomProceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 710–714, c Portland, Oregon, June 19-24, 2011. 2011 Association for Computational Ling"
P11-2125,P05-1012,0,0.262097,"orithms (West, 2001) or approximate inference algorithms (Smith and Eisner, 2008; Koo et al., 2010). The (Eisner, 1996) algorithm is typically used for projective parsing. The model parameters are trained using a discriminative learning algorithm, e.g. averaged perceptron (Collins, 2002) or MIRA (Crammer and Singer, 2003). In this paper, we work with both first-order and second-order models, we train the models using MIRA, and we use the (Eisner, 1996) algorithm for inference. The baseline features capture information about the lexical items and their part of speech (POS) tags (as defined in (McDonald et al., 2005)). In this work, following (Koo et al., 2008), we use word cluster identifiers as the source of an additional set of features. The reader is directed to (Koo et al., 2008) for the list of cluster-based feature templates. The clusters inject long distance syntactic or semantic information into the model (in contrast with the use of POS tags in the baseline) and help alleviate the sparse data problem for complex features that include n-grams. 3 The Ensemble Model A word can have different syntactic or semantic cluster representations, each of which may lead to a different parsing model. We use e"
P11-2125,P08-1108,0,0.0872987,"nd for Pronouns it is Syn-Low that is the best. But the ensemble always does the best in every grammatical category. Fig. 2(b) shows the F-score of the different models for various dependency lengths, where the length of a dependency from word wi to word wj is equal to |i − j|. We see that different models are experts on different lengths (Syn-Low on 8, Syn-High on 9), while the ensemble model can always combine their expertise and do better at each length. 6 Comparison to Related Work Several ensemble models have been proposed for dependency parsing (Sagae and Lavie, 2006; Hall et al., 2007; Nivre and McDonald, 2008; Attardi and Dell’Orletta, 2009; Surdeanu and Manning, 2010). Essentially, all of these approaches combine different dependency parsing systems, i.e. transitionbased and graph-based. Although graph-based models are globally trained and can use exact inference algorithms, their features are defined over a limited history of parsing decisions. Since transitionbased parsing models have the opposite characteristics, the idea is to combine these two types of models to exploit their complementary strengths. The base parsing models are either independently trained (Sagae and Lavie, 2006; Hall et al."
P11-2125,P06-1055,0,0.00347755,"eems more scalable though, since we can incrementally add a large number of clustering algorithms into the ensemble. 4 Syntactic and Semantic Clustering In our ensemble model we use three different clustering methods to obtain three types of word representations that can help alleviate sparse data in a dependency parser. Our first word representation is exactly the same as the one used in (Koo et al., 2008) where words are clustered using the Brown algorithm (Brown et al., 1992). Our two other clusterings are extracted from the split non-terminals obtained from the PCFG-based Berkeley parser (Petrov et al., 2006). Split non-terminals from the Berkeley parser output are converted into cluster identifiers in two different ways: 1) the split POS tags for each word are used as an alternate word representation. We call this representation Syn-Low, and 2) head percolation rules are used to label each non-terminal in the parse such that each non-terminal has a unique daughter labeled as head. Each word is assigned a cluster identifier which is defined as the parent split non-terminal of that word if it is not marked as head, else if the parent is marked as head we recursively check its parent until we reach"
P11-2125,N10-1003,0,0.0111963,"Missing"
P11-2125,W96-0213,0,0.273964,"ig. 1. If we group all the head-words in the training data that project up to split non-terminal NP-24 then we get a cluster: loss, time, profit, earnings, performance, rating, . . . which are head words of the noun phrases that appear as direct object of verbs like improve. 5 Experimental Results ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! opment set (Section 22), and test sets (Sections 0, 1, 23, and 24). All our experimental settings match previous work (Yamada and Matsumoto, 2003; McDonald et al., 2005; Koo et al., 2008). POS tags for the development and test data were assigned by MXPOST (Ratnaparkhi, 1996), where the tagger was trained on the entire training corpus. To generate part of speech tags for the training data, we used 20way jackknifing, i.e. we tagged each fold with the tagger trained on the other 19 folds. We set model weights αk in Eqn (2) to one for all experiments. The experiments were done on the English Penn Treebank, using standard head-percolation rules (Yamada and Matsumoto, 2003) to convert the phrase structure into dependency trees. We split the Syntactic State-Splitting The sentence-specific Treebank into a training set (Sections 2-21), a devel- word clusters are derived f"
P11-2125,N06-2033,0,0.14961,"odel, but for Adjectives it is Syn-High, and for Pronouns it is Syn-Low that is the best. But the ensemble always does the best in every grammatical category. Fig. 2(b) shows the F-score of the different models for various dependency lengths, where the length of a dependency from word wi to word wj is equal to |i − j|. We see that different models are experts on different lengths (Syn-Low on 8, Syn-High on 9), while the ensemble model can always combine their expertise and do better at each length. 6 Comparison to Related Work Several ensemble models have been proposed for dependency parsing (Sagae and Lavie, 2006; Hall et al., 2007; Nivre and McDonald, 2008; Attardi and Dell’Orletta, 2009; Surdeanu and Manning, 2010). Essentially, all of these approaches combine different dependency parsing systems, i.e. transitionbased and graph-based. Although graph-based models are globally trained and can use exact inference algorithms, their features are defined over a limited history of parsing decisions. Since transitionbased parsing models have the opposite characteristics, the idea is to combine these two types of models to exploit their complementary strengths. The base parsing models are either independentl"
P11-2125,D08-1016,0,0.0113783,"DT-15 1101 improves VBZ-1 S-14 0101 trend NN-23 NP-18 1010 access NN-13 NP-24 0011 to TO-0 TO-0 0011 American JJ-31 JJ-31 0110 markets NNS-25 NP-9 1011 Figure 1: Dependency tree with cluster identifiers obtained from the split non-terminals from the Berkeley parser output. The first row under the words are the split POS tags (Syn-Low), the second row are the split bracketing tags (Syn-High), and the third row is the first 4 bits (to save space in this figure) of the (Brown et al., 1992) clusters. plished using minimum spanning tree algorithms (West, 2001) or approximate inference algorithms (Smith and Eisner, 2008; Koo et al., 2010). The (Eisner, 1996) algorithm is typically used for projective parsing. The model parameters are trained using a discriminative learning algorithm, e.g. averaged perceptron (Collins, 2002) or MIRA (Crammer and Singer, 2003). In this paper, we work with both first-order and second-order models, we train the models using MIRA, and we use the (Eisner, 1996) algorithm for inference. The baseline features capture information about the lexical items and their part of speech (POS) tags (as defined in (McDonald et al., 2005)). In this work, following (Koo et al., 2008), we use word"
P11-2125,N10-1091,0,0.148352,"semble always does the best in every grammatical category. Fig. 2(b) shows the F-score of the different models for various dependency lengths, where the length of a dependency from word wi to word wj is equal to |i − j|. We see that different models are experts on different lengths (Syn-Low on 8, Syn-High on 9), while the ensemble model can always combine their expertise and do better at each length. 6 Comparison to Related Work Several ensemble models have been proposed for dependency parsing (Sagae and Lavie, 2006; Hall et al., 2007; Nivre and McDonald, 2008; Attardi and Dell’Orletta, 2009; Surdeanu and Manning, 2010). Essentially, all of these approaches combine different dependency parsing systems, i.e. transitionbased and graph-based. Although graph-based models are globally trained and can use exact inference algorithms, their features are defined over a limited history of parsing decisions. Since transitionbased parsing models have the opposite characteristics, the idea is to combine these two types of models to exploit their complementary strengths. The base parsing models are either independently trained (Sagae and Lavie, 2006; Hall et al., 2007; Attardi and Dell’Orletta, 2009; Surdeanu and Manning,"
P11-2125,W03-3023,0,0.0305711,")))length. ! ! ! ! 0.80 ! For the Berkeley parser output shown above, the resulting word representations and dependency tree is shown in Fig. 1. If we group all the head-words in the training data that project up to split non-terminal NP-24 then we get a cluster: loss, time, profit, earnings, performance, rating, . . . which are head words of the noun phrases that appear as direct object of verbs like improve. 5 Experimental Results ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! ! opment set (Section 22), and test sets (Sections 0, 1, 23, and 24). All our experimental settings match previous work (Yamada and Matsumoto, 2003; McDonald et al., 2005; Koo et al., 2008). POS tags for the development and test data were assigned by MXPOST (Ratnaparkhi, 1996), where the tagger was trained on the entire training corpus. To generate part of speech tags for the training data, we used 20way jackknifing, i.e. we tagged each fold with the tagger trained on the other 19 folds. We set model weights αk in Eqn (2) to one for all experiments. The experiments were done on the English Penn Treebank, using standard head-percolation rules (Yamada and Matsumoto, 2003) to convert the phrase structure into dependency trees. We split the"
P12-1065,J04-3004,0,0.657401,"o derive the labelled points. We refer to this setting as bootstrapping. In contrast, typical semi-supervised learning deals with a large number of labelled points, and a domain adaptation task with unlabelled points from the new domain. The two dominant discriminative learning methods for bootstrapping are self-training (Scudder, 1965) and co-training (Blum and Mitchell, 1998). In this paper we focus on a self-training style bootstrapping algorithm, the Yarowsky algorithm (Yarowsky, 1995). Variants of this algorithm have been formalized as optimizing an objective function in previous work by Abney (2004) and Haffari and Sarkar (2007), but it is not clear that any perform as well as the Yarowsky algorithm itself. We take advantage of this formalization and introduce a novel algorithm called Yarowsky-prop which builds on the algorithms of Yarowsky (1995) and Subramanya et al. (2010). It is theoretically ∗ This research was partially supported by an NSERC, Canada (RGPIN: 264905) grant. We would like to thank Gholamreza Haffari and the anonymous reviewers for their comments. We particularly thank Michael Collins, Jason Eisner, and Damianos Karakos for the data we used in our experiments. denotes"
P12-1065,W99-0613,0,0.16052,"and the non-transductive version is closely related to Abney (2004) c.f. Haffari and Sarkar (2007).1 3 Existing algorithms 3.1 Yarowsky A decision list (DL) is a (ordered) list of featurelabel pairs (rules) which is produced by assigning a score to each rule and sorting on this score. It chooses a label for an example from the first rule whose feature is a feature of the example. For a DL the prediction distribution is defined by πx (j) ∝ maxf ∈Fx θf j . The basic Yarowsky algorithm is shown in algorithm 1. Note that at any point some training examples may be left unlabelled by Y (t) . We use Collins and Singer (1999) for our exact specification of Yarowsky.2 It uses DL rule scores θf j ∝ |Λf j |+  |Λf |+ L (1) where  is a smoothing constant. When constructing a DL it keeps only the rules with (pre-normalized) score over a threshold ζ. In our implementation we add the seed rules to each subsequent DL.3 1 Large-scale information extraction, e.g. (Hearst, 1992), Snowball (Agichtein and Gravano, 2000), AutoSlog (Riloff and Shepherd, 1997), and Junto (Talukdar, 2010) among others, also have similarities to our approach. We focus on the formal analysis of the Yarowsky algorithm by Abney (2004). 2 It is simil"
P12-1065,P97-1003,0,0.0114139,"Exchange loc. California loc. New-York loc. court-in loc. Company-of loc. .. . Figure 2: A DL from iteration 5 of Yarowsky on the named entity task. Scores are pre-normalized values from the expression on the left side of (1), not θf j values. Context features are indicated by italics; all others are spelling features. Specific feature types are omitted. Seed rules are indicated by bold ranks. kindly provided by the respective authors. The task of Collins and Singer (1999) is named entity classification on data from New York Times text.7 The data set was pre-processed by a statistical parser (Collins, 1997) and all noun phrases that are potential named entities were extracted from the parse tree. Each noun phrase is to be labelled as a person, organization, or location. The parse tree provides the surrounding context as context features such as the words in prepositional phrase and relative clause modifiers, etc., and the actual words in the noun phrase provide the spelling features. The test data additionally contains some noise examples which are not in the three named entity categories. We use the seed rules the authors provide, which are the first seven items in figure 2. For DL-CoTrain, we"
P12-1065,H05-1050,0,0.0230633,". The parse tree provides the surrounding context as context features such as the words in prepositional phrase and relative clause modifiers, etc., and the actual words in the noun phrase provide the spelling features. The test data additionally contains some noise examples which are not in the three named entity categories. We use the seed rules the authors provide, which are the first seven items in figure 2. For DL-CoTrain, we use their two views: one view is the spelling features, and the other is the context features. Figure 2 shows a DL from Yarowsky training on this task. The tasks of Eisner and Karakos (2005) are word sense disambiguation on several English words which have two senses corresponding to two different words in French. Data was extracted from the Canadian Hansards, using the English side to produce training and test data and the French side to produce the gold labelling. Features are the original and lemmatized words immediately adja7 6.1 Tasks and data For evaluation we use the tasks of Collins and Singer (1999) and Eisner and Karakos (2005), with data 624 Score 0.999900 0.999900 0.999900 0.999900 0.999900 0.999900 0.999900 0.999976 0.999957 0.999952 0.999947 0.999946 0.975154 We rem"
P12-1065,P06-1111,0,0.0232021,"form distribution. In the bootstrapping setting the learner is given an initial partial labelling Y (0) where only a few examples are (0) labelled (i.e. Yx = ⊥ for most x). Abney (2004) defines three probability distributions in his analysis of bootstrapping: θf j is the parameter for feature f with label j, taken to be normalized so that θf is a distribution over labels. φx is the labelling distribution representing the current Y ; it is a point distribution for labelled examples and uniform for unlabelled examples. πx is the prediction distribution over labels for example x. The approach of Haghighi and Klein (2006b) and Haghighi and Klein (2006a) also uses a small set of 620 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 620–628, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics Algorithm 1: The basic Yarowsky algorithm. 3.2 Yarowsky-cautious (0) Require: training data X and a seed DL θ 1: apply θ (0) to X produce a labelling Y (0) 2: for iteration t to maximum or convergence do 3: train a new DL θ on Y (t) 4: apply θ to X, to produce Y (t+1) 5: end for seed rules but uses them to inject features into a joint model"
P12-1065,N06-1041,0,0.0557266,"form distribution. In the bootstrapping setting the learner is given an initial partial labelling Y (0) where only a few examples are (0) labelled (i.e. Yx = ⊥ for most x). Abney (2004) defines three probability distributions in his analysis of bootstrapping: θf j is the parameter for feature f with label j, taken to be normalized so that θf is a distribution over labels. φx is the labelling distribution representing the current Y ; it is a point distribution for labelled examples and uniform for unlabelled examples. πx is the prediction distribution over labels for example x. The approach of Haghighi and Klein (2006b) and Haghighi and Klein (2006a) also uses a small set of 620 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 620–628, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics Algorithm 1: The basic Yarowsky algorithm. 3.2 Yarowsky-cautious (0) Require: training data X and a seed DL θ 1: apply θ (0) to X produce a labelling Y (0) 2: for iteration t to maximum or convergence do 3: train a new DL θ on Y (t) 4: apply θ to X, to produce Y (t+1) 5: end for seed rules but uses them to inject features into a joint model"
P12-1065,C92-2082,0,0.0406769,"ure of the example. For a DL the prediction distribution is defined by πx (j) ∝ maxf ∈Fx θf j . The basic Yarowsky algorithm is shown in algorithm 1. Note that at any point some training examples may be left unlabelled by Y (t) . We use Collins and Singer (1999) for our exact specification of Yarowsky.2 It uses DL rule scores θf j ∝ |Λf j |+  |Λf |+ L (1) where  is a smoothing constant. When constructing a DL it keeps only the rules with (pre-normalized) score over a threshold ζ. In our implementation we add the seed rules to each subsequent DL.3 1 Large-scale information extraction, e.g. (Hearst, 1992), Snowball (Agichtein and Gravano, 2000), AutoSlog (Riloff and Shepherd, 1997), and Junto (Talukdar, 2010) among others, also have similarities to our approach. We focus on the formal analysis of the Yarowsky algorithm by Abney (2004). 2 It is similar to that of Yarowsky (1995) but is better specified and omits word sense disambiguation optimizations. The general algorithm in Yarowsky (1995) is self-training with any kind of underlying supervised classifier, but we follow the convention of using Yarowsky to refer to the DL algorithm. 3 This is not clearly specified in Collins and Singer (1999)"
P12-1065,W97-0313,0,0.179081,"ed by πx (j) ∝ maxf ∈Fx θf j . The basic Yarowsky algorithm is shown in algorithm 1. Note that at any point some training examples may be left unlabelled by Y (t) . We use Collins and Singer (1999) for our exact specification of Yarowsky.2 It uses DL rule scores θf j ∝ |Λf j |+  |Λf |+ L (1) where  is a smoothing constant. When constructing a DL it keeps only the rules with (pre-normalized) score over a threshold ζ. In our implementation we add the seed rules to each subsequent DL.3 1 Large-scale information extraction, e.g. (Hearst, 1992), Snowball (Agichtein and Gravano, 2000), AutoSlog (Riloff and Shepherd, 1997), and Junto (Talukdar, 2010) among others, also have similarities to our approach. We focus on the formal analysis of the Yarowsky algorithm by Abney (2004). 2 It is similar to that of Yarowsky (1995) but is better specified and omits word sense disambiguation optimizations. The general algorithm in Yarowsky (1995) is self-training with any kind of underlying supervised classifier, but we follow the convention of using Yarowsky to refer to the DL algorithm. 3 This is not clearly specified in Collins and Singer (1999), 621 Collins and Singer (1999) also introduce a variant algorithm Yarowsky-ca"
P12-1065,D10-1017,0,0.072016,"learning methods for bootstrapping are self-training (Scudder, 1965) and co-training (Blum and Mitchell, 1998). In this paper we focus on a self-training style bootstrapping algorithm, the Yarowsky algorithm (Yarowsky, 1995). Variants of this algorithm have been formalized as optimizing an objective function in previous work by Abney (2004) and Haffari and Sarkar (2007), but it is not clear that any perform as well as the Yarowsky algorithm itself. We take advantage of this formalization and introduce a novel algorithm called Yarowsky-prop which builds on the algorithms of Yarowsky (1995) and Subramanya et al. (2010). It is theoretically ∗ This research was partially supported by an NSERC, Canada (RGPIN: 264905) grant. We would like to thank Gholamreza Haffari and the anonymous reviewers for their comments. We particularly thank Michael Collins, Jason Eisner, and Damianos Karakos for the data we used in our experiments. denotes an example denote features denote labels set of training examples set of features for example x current labelling of X current label for example x value of Yx for unlabelled examples number of labels (not including ⊥) set of currently labelled examples set of currently unlabelled e"
P12-1065,P95-1026,0,0.909024,"ng, in that the labelled and unlabelled data points are from the same domain and only a small set of seed rules is used to derive the labelled points. We refer to this setting as bootstrapping. In contrast, typical semi-supervised learning deals with a large number of labelled points, and a domain adaptation task with unlabelled points from the new domain. The two dominant discriminative learning methods for bootstrapping are self-training (Scudder, 1965) and co-training (Blum and Mitchell, 1998). In this paper we focus on a self-training style bootstrapping algorithm, the Yarowsky algorithm (Yarowsky, 1995). Variants of this algorithm have been formalized as optimizing an objective function in previous work by Abney (2004) and Haffari and Sarkar (2007), but it is not clear that any perform as well as the Yarowsky algorithm itself. We take advantage of this formalization and introduce a novel algorithm called Yarowsky-prop which builds on the algorithms of Yarowsky (1995) and Subramanya et al. (2010). It is theoretically ∗ This research was partially supported by an NSERC, Canada (RGPIN: 264905) grant. We would like to thank Gholamreza Haffari and the anonymous reviewers for their comments. We pa"
P12-1065,C08-1135,0,\N,Missing
P12-1065,J94-2001,0,\N,Missing
P12-1065,P94-1013,0,\N,Missing
P12-1065,P02-1046,0,\N,Missing
P12-1099,W09-0432,0,0.123035,"tabolic diseases . IN naglazyme treatment should be supervis´e by a doctor the with in the management of patients with mps vi or other hereditary metabolic disease . OUT naglazyme ’s treatment must be supervised by a doctor with the experience of the care of patients with mps vi. or another disease hereditary metabolic . ENSEMBLE naglazyme treatment should be supervised by a physician experienced in the management of patients with mps vi or other hereditary metabolic disease . Figure 2: Examples illustrating how this method is able to use expertise of both out-of-domain and in-domain systems. Bertoldi and Federico, 2009). In this approach, a system is trained on the parallel OUT and IN data and it is used to translate the monolingual IN data set. Iteratively, most confident sentence pairs are selected and added to the training corpus on which a new system is trained. 5.2 System Combination Tackling the model adaptation problem using system combination approaches has been experimented in various work (Koehn and Schroeder, 2007; Hildebrand and Vogel, 2009). Among these approaches are sentence-based, phrase-based and word-based output combination methods. In a similar approach, Koehn and Schroeder (2007) use a f"
P12-1099,D08-1024,0,0.0137368,"ith the other two state-of-the-art SMT systems. Secondly, our combining method uses the union option, but instead of preserving the features of all phrase-tables, it only combines their scores using various mixture operations. This enables us to experiment with a number of different operations as opposed to sticking to only one combination method. Finally, by avoiding increasing the number of features we can add as many translation models as we need without serious performance drop. In addition, MERT would not be an appropriate optimizer when the number of features increases a certain amount (Chiang et al., 2008). Our approach differs from the model combination approach of DeNero et al. (2010), a generalization of consensus or minimum Bayes risk decoding where the search space consists of those of multiple systems, in that model combination uses forest of derivations of all component models to do the combination. In other words, it requires all component models to fully decode each sentence, compute n-gram expectations from each component model and calculate posterior probabilities over translation derivations. While, in our approach we only use partial hypotheses from component models and the derivat"
P12-1099,P05-1033,0,0.540776,"se a novel system combination approach called ensemble decoding in order to combine two or more translation models with the goal of constructing a system that outperforms all the component models. The strength of this system combination method is that the systems are combined in the decoder. This enables the decoder to pick the best hypotheses for each span of the input. The main applications of ensemble models are domain adaptation, domain mixing and system combination. We have modified Kriya (Sankaran et al., 2012), an in-house implementation of hierarchical phrase-based translation system (Chiang, 2005), to implement ensemble decoding using multiple translation models. We compare the results of ensemble decoding with a number of baselines for domain adaptation. In addition to the basic approach of concatenation of in-domain and out-of-domain data, we also trained a log-linear mixture model (Foster and Kuhn, 2007) 940 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 940–949, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics as well as the linear mixture model of (Foster et al., 2010) for conditional phrase-p"
P12-1099,W07-0722,0,0.0287266,"es. Foster et al. (2010) propose a similar method for machine translation that uses features to capture degrees of generality. Particularly, they include the output from an SVM classifier that uses the intersection between IN and OUT as positive examples. Unlike previous work on instance weighting in machine translation, they use phraselevel instances instead of sentences. A large body of work uses interpolation techniques to create a single TM/LM from interpolating a number of LMs/TMs. Two famous examples of such methods are linear mixtures and log-linear mixtures (Koehn and Schroeder, 2007; Civera and Juan, 2007; Foster and Kuhn, 2007) which were used as baselines and discussed in Section 2. Other methods include using self-training techniques to exploit monolingual in-domain data (Ueffing et al., 2007; SOURCE REF IN OUT ENSEMBLE SOURCE REF am´enorrh´ee , menstruations irr´eguli`eres amenorrhoea , irregular menstruation amenorrhoea , menstruations irr´eguli`eres am´enorrh´ee , irregular menstruation amenorrhoea , irregular menstruation le traitement par naglazyme doit eˆ tre supervis´e par un m´edecin ayant l’ exp´erience de la prise en charge des patients atteints de mps vi ou d’ une autre maladie m"
P12-1099,P11-2031,0,0.0183841,"OUT LOGLIN LINMIX PBS 31.84 24.08 31.75 32.21 33.81 Hiero 33.69 25.32 33.76 – 35.57 Table 2: The results of various baselines implemented in a phrase-based (PBS) and a Hiero SMT on EMEA. Table 3 shows the results of ensemble decoding with different mixture operations and model weight settings. Each mixture operation has been evaluated on the test-set by setting the component weights uniformly (denoted by uniform) and by tuning the weights using C ONDOR (denoted by tuned) on a held-out set. The tuned scores (3rd column in Table 3) are averages of three runs with different initial points as in Clark et al. (2011). We also reported the BLEU scores when we applied the span-wise normalization heuristic. All of these mixture operations were able to significantly improve over the concatenation baseline. In particular, Switching:Max could gain up to 2.2 BLEU points over the concatenation baseline and 0.39 BLEU points over the best performing baseline (i.e. linear mixture model implemented in Hiero) which is statistically significant based on Clark et al. (2011) (p = 0.02). Prod when using with uniform weights gets the Mixture Operation W MAX W SUM S WITCHING :M AX S WITCHING :S UM P ROD Uniform 35.39 35.35"
P12-1099,eck-etal-2004-language,0,0.209087,"on Majid Razmara1 1 George Foster2 Baskaran Sankaran1 Anoop Sarkar1 Simon Fraser University, 8888 University Dr., Burnaby, BC, Canada {razmara,baskaran,anoop}@sfu.ca 2 National Research Council Canada, 283 Alexandre-Tach´e Blvd, Gatineau, QC, Canada george.foster@nrc.gc.ca Abstract translation model adaptation, because various measures such as perplexity of adapted language models can be easily computed on data in the target domain. As a result, language model adaptation has been well studied in various work (Clarkson and Robinson, 1997; Seymore and Rosenfeld, 1997; Bacchiani and Roark, 2003; Eck et al., 2004) both for speech recognition and for machine translation. It is also easier to obtain monolingual data in the target domain, compared to bilingual data which is required for translation model adaptation. In this paper, we focused on adapting only the translation model by fixing a language model for all the experiments. We expect domain adaptation for machine translation can be improved further by combining orthogonal techniques for translation model adaptation combined with language model adaptation. Statistical machine translation is often faced with the problem of combining training data fro"
P12-1099,W07-0717,1,0.427919,"he decoder to pick the best hypotheses for each span of the input. The main applications of ensemble models are domain adaptation, domain mixing and system combination. We have modified Kriya (Sankaran et al., 2012), an in-house implementation of hierarchical phrase-based translation system (Chiang, 2005), to implement ensemble decoding using multiple translation models. We compare the results of ensemble decoding with a number of baselines for domain adaptation. In addition to the basic approach of concatenation of in-domain and out-of-domain data, we also trained a log-linear mixture model (Foster and Kuhn, 2007) 940 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 940–949, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics as well as the linear mixture model of (Foster et al., 2010) for conditional phrase-pair probabilities over IN and OUT. Furthermore, within the framework of ensemble decoding, we study and evaluate various methods for combining translation tables. 2 Baselines The natural baseline for model adaption is to concatenate the IN and OUT data into a single parallel corpus and train a model on it. In addit"
P12-1099,D10-1044,1,0.742619,"Missing"
P12-1099,W09-0406,0,0.0440285,"r other hereditary metabolic disease . Figure 2: Examples illustrating how this method is able to use expertise of both out-of-domain and in-domain systems. Bertoldi and Federico, 2009). In this approach, a system is trained on the parallel OUT and IN data and it is used to translate the monolingual IN data set. Iteratively, most confident sentence pairs are selected and added to the training corpus on which a new system is trained. 5.2 System Combination Tackling the model adaptation problem using system combination approaches has been experimented in various work (Koehn and Schroeder, 2007; Hildebrand and Vogel, 2009). Among these approaches are sentence-based, phrase-based and word-based output combination methods. In a similar approach, Koehn and Schroeder (2007) use a feature of the factored translation model framework in Moses SMT system (Koehn and Schroeder, 2007) to use multiple alternative decoding paths. Two decoding paths, one for each translation table (IN and OUT), were used during decoding. The weights are set with minimum error rate training (Och, 2003). Our work is closely related to Koehn and Schroeder (2007) but uses a different approach to deal with multiple translation tables. The Moses S"
P12-1099,2005.eamt-1.19,0,0.0283502,"T models to construct a better translation than both of them. In the first example, there are two OOVs one for each of the IN and OUT models. Our approach is able to resolve the OOV issues by taking advantage of the other model’s presence. Similarly, the second example shows how ensemble decoding improves lexical choices as well as word re-orderings. 945 5 5.1 Related Work Domain Adaptation Early approaches to domain adaptation involved information retrieval techniques where sentence pairs related to the target domain were retrieved from the training corpus using IR methods (Eck et al., 2004; Hildebrand et al., 2005). Foster et al. (2010), however, uses a different approach to select related sentences from OUT. They use language model perplexities from IN to select relavant sentences from OUT. These sentences are used to enrich the IN training set. Other domain adaptation methods involve techniques that distinguish between general and domainspecific examples (Daum´e and Marcu, 2006). Jiang and Zhai (2007) introduce a general instance weighting framework for model adaptation. This approach tries to penalize misleading training instances from OUT and assign more weight to IN-like instances than OUT instance"
P12-1099,P07-1034,0,0.0251815,"approaches to domain adaptation involved information retrieval techniques where sentence pairs related to the target domain were retrieved from the training corpus using IR methods (Eck et al., 2004; Hildebrand et al., 2005). Foster et al. (2010), however, uses a different approach to select related sentences from OUT. They use language model perplexities from IN to select relavant sentences from OUT. These sentences are used to enrich the IN training set. Other domain adaptation methods involve techniques that distinguish between general and domainspecific examples (Daum´e and Marcu, 2006). Jiang and Zhai (2007) introduce a general instance weighting framework for model adaptation. This approach tries to penalize misleading training instances from OUT and assign more weight to IN-like instances than OUT instances. Foster et al. (2010) propose a similar method for machine translation that uses features to capture degrees of generality. Particularly, they include the output from an SVM classifier that uses the intersection between IN and OUT as positive examples. Unlike previous work on instance weighting in machine translation, they use phraselevel instances instead of sentences. A large body of work"
P12-1099,W07-0733,0,0.780622,"instances than OUT instances. Foster et al. (2010) propose a similar method for machine translation that uses features to capture degrees of generality. Particularly, they include the output from an SVM classifier that uses the intersection between IN and OUT as positive examples. Unlike previous work on instance weighting in machine translation, they use phraselevel instances instead of sentences. A large body of work uses interpolation techniques to create a single TM/LM from interpolating a number of LMs/TMs. Two famous examples of such methods are linear mixtures and log-linear mixtures (Koehn and Schroeder, 2007; Civera and Juan, 2007; Foster and Kuhn, 2007) which were used as baselines and discussed in Section 2. Other methods include using self-training techniques to exploit monolingual in-domain data (Ueffing et al., 2007; SOURCE REF IN OUT ENSEMBLE SOURCE REF am´enorrh´ee , menstruations irr´eguli`eres amenorrhoea , irregular menstruation amenorrhoea , menstruations irr´eguli`eres am´enorrh´ee , irregular menstruation amenorrhoea , irregular menstruation le traitement par naglazyme doit eˆ tre supervis´e par un m´edecin ayant l’ exp´erience de la prise en charge des patients atteints de mps vi ou"
P12-1099,N03-1017,0,0.049812,"¯ e|f¯) and pm (f¯|¯ e). Thus, for 2 component models (from IN and OUT training corpora), there are 4 ∗ 2 = 8 TM weights to tune. Whenever a phrase pair does not appear in a component phrase table, we set the corresponding pm (¯ e|f¯) to a small epsilon value. 2.2 Linear Mixture Linear TM mixtures are of the form: p(¯ e|f¯) = M X λm pm (¯ e|f¯) m Our technique for setting λm is similar to that outlined in Foster et al. (2010). We first extract a joint phrase-pair distribution p˜(¯ e, f¯) from the development set using standard techniques (HMM word alignment with grow-diag-and symmeterization (Koehn et al., 2003)). We then find the set ˆ that minimize the cross-entropy of the of weights λ mixture p(¯ e|f¯) with respect to p˜(¯ e, f¯): 941 ˆ = argmax λ λ X p˜(¯ e, f¯) log e¯,f¯ M X λm pm (¯ e|f¯) m For efficiency and stability, we use the EM algoˆ rather than L-BFGS as in (Foster et rithm to find λ, al., 2010). Whenever a phrase pair does not appear in a component phrase table, we set the corresponding pm (¯ e|f¯) to 0; pairs in p˜(¯ e, f¯) that do not appear in at least one component table are discarded. We learn separate linear mixtures for relative-frequency and lexical estimates for both p(¯ e|f¯)"
P12-1099,P09-1065,0,0.0178941,"ed by the ensemble model. A major difference is that in the model combination approach the component search spaces are conjoined and they are not intermingled as opposed to our approach where these search spaces are intermixed on spans. This enables us to generate new sentences that cannot be generated by component models. Furthermore, various combination methods can be explored in our approach. Finally, main techniques used in this work are orthogonal to our approach such as Minimum Bayes Risk decoding, using n-gram features and tuning using MERT. Finally, our work is most similar to that of Liu et al. (2009) where max-derivation and maxtranslation decoding have been used. Maxderivation finds a derivation with highest score and max-translation finds the highest scoring translation by summing the score of all derivations with the same yield. The combination can be done in two levels: translation-level and derivation-level. Their derivation-level max-translation decoding is similar to our ensemble decoding with wsum as the mixture operation. We did not restrict ourself to this particular mixture operation and experimented with a 947 number of different mixing techniques and as Table 3 shows we could"
P12-1099,P00-1056,0,0.0614986,"e corpus was word-aligned using both HMM and IBM2 models, and the phrase table was the union of phrases extracted from these separate alignments, with a length limit of 7. It was filtered to retain the top 20 translations for each source phrase using the TM part of the current loglinear model. For ensemble decoding, we modified an in-house implementation of hierarchical phrase-based system, Kriya (Sankaran et al., 2012) which uses the same features mentioned in (Chiang, 2005): forward and backward relative-frequency and lexical TM probabilities; LM; word, phrase and glue-rules penalty. GIZA++(Och and Ney, 2000) has been used for word alignment with phrase length limit of 7. In both systems, feature weights were optimized using MERT (Och, 2003) and with a 5-gram lan1 2 guage model and Kneser-Ney smoothing was used in all the experiments. We used SRILM (Stolcke, 2002) as the langugage model toolkit. Fixing the language model allows us to compare various translation model combination techniques. www.statmt.org/europarl Please contact the authors to access the data-sets. 944 Results Table 2 shows the results of the baselines. The first group are the baseline results on the phrase-based system discussed"
P12-1099,P03-1021,0,0.148516,"baseline for model adaption is to concatenate the IN and OUT data into a single parallel corpus and train a model on it. In addition to this baseline, we have experimented with two more sophisticated baselines which are based on mixture techniques. 2.1 Log-Linear Mixture Log-linear translation model (TM) mixtures are of the form: X  M ¯ ¯ p(¯ e|f ) ∝ exp λm log pm (¯ e|f ) m where m ranges over IN and OUT, pm (¯ e|f¯) is an estimate from a component phrase table, and each λm is a weight in the top-level log-linear model, set so as to maximize dev-set BLEU using minimum error rate training (Och, 2003). We learn separate weights for relative-frequency and lexical estimates for both pm (¯ e|f¯) and pm (f¯|¯ e). Thus, for 2 component models (from IN and OUT training corpora), there are 4 ∗ 2 = 8 TM weights to tune. Whenever a phrase pair does not appear in a component phrase table, we set the corresponding pm (¯ e|f¯) to a small epsilon value. 2.2 Linear Mixture Linear TM mixtures are of the form: p(¯ e|f¯) = M X λm pm (¯ e|f¯) m Our technique for setting λm is similar to that outlined in Foster et al. (2010). We first extract a joint phrase-pair distribution p˜(¯ e, f¯) from the development"
P12-1099,W05-0822,1,0.808324,"1 Experimental Setup 4.2 We carried out translation experiments using the European Medicines Agency (EMEA) corpus (Tiedemann, 2009) as IN, and the Europarl (EP) corpus1 as OUT, for French to English translation. The dev and test sets were randomly chosen from the EMEA corpus.2 The details of datasets used are summarized in Table 1. Dataset Sents EMEA Europarl Dev Test 11770 1.3M 1533 1522 Words French English 168K 144K 40M 37M 29K 25K 29K 25K Table 1: Training, dev and test sets for EMEA. For the mixture baselines, we used a standard one-pass phrase-based system (Koehn et al., 2003), Portage (Sadat et al., 2005), with the following 7 features: relative-frequency and lexical translation model (TM) probabilities in both directions; worddisplacement distortion model; language model (LM) and word count. The corpus was word-aligned using both HMM and IBM2 models, and the phrase table was the union of phrases extracted from these separate alignments, with a length limit of 7. It was filtered to retain the top 20 translations for each source phrase using the TM part of the current loglinear model. For ensemble decoding, we modified an in-house implementation of hierarchical phrase-based system, Kriya (Sanka"
P12-1099,P05-1003,0,0.0331143,"f¯) ∝ exp M X λm (wm · φm )  In Section 4.2, we compare the BLEU scores of different mixture operations on a French-English experimental setup. 3.2 Normalization m Product models have been used in combining LMs and TMs in SMT as well as some other NLP tasks such as ensemble parsing (Petrov, 2010). Each of these mixture operations has a specific property that makes it work in specific domain adaptation or system combination scenarios. For instance, LOPs may not be optimal for domain adaptation in the setting where there are two or more models trained on heterogeneous corpora. As discussed in (Smith et al., 2005), LOPs work best when all the models accuracies are high and close to each other with some degree of diversity. LOPs give veto power to any of the component models and this perfectly works for settings such as the one in (Petrov, 2010) where a number of parsers are trained by changing the randomization seeds but having the same base parser and using the same training set. They noticed that parsers trained using different randomization seeds have high accuracies but there are some diversities among them and they used product models for their advantage to get an even better parser. We assume tha"
P12-1099,P07-1004,1,0.757959,"es the intersection between IN and OUT as positive examples. Unlike previous work on instance weighting in machine translation, they use phraselevel instances instead of sentences. A large body of work uses interpolation techniques to create a single TM/LM from interpolating a number of LMs/TMs. Two famous examples of such methods are linear mixtures and log-linear mixtures (Koehn and Schroeder, 2007; Civera and Juan, 2007; Foster and Kuhn, 2007) which were used as baselines and discussed in Section 2. Other methods include using self-training techniques to exploit monolingual in-domain data (Ueffing et al., 2007; SOURCE REF IN OUT ENSEMBLE SOURCE REF am´enorrh´ee , menstruations irr´eguli`eres amenorrhoea , irregular menstruation amenorrhoea , menstruations irr´eguli`eres am´enorrh´ee , irregular menstruation amenorrhoea , irregular menstruation le traitement par naglazyme doit eˆ tre supervis´e par un m´edecin ayant l’ exp´erience de la prise en charge des patients atteints de mps vi ou d’ une autre maladie m´etabolique h´er´editaire . naglazyme treatment should be supervised by a physician experienced in the management of patients with mps vi or other inherited metabolic diseases . IN naglazyme tre"
P12-1099,N10-1141,0,\N,Missing
P12-1099,N10-1003,0,\N,Missing
P12-1099,D08-1076,0,\N,Missing
P13-1109,N09-1014,0,0.0734438,"enerative models based on canonical correlation analysis to extract translation lexicons for non-parallel corpora by learning a matching between source and target lexicons. Using monolingual features to represent words, feature vectors are projected from source and target words into a canonical space to find the appropriate matching between them. Their method relies on context features which need a seed lexicon and orthographic features which only works for phylogenetically related languages. Graph-based semi-supervised methods have been shown to be useful for domain adaptation in MT as well. Alexandrescu and Kirchhoff (2009) applied a graph-based method to determine similarities between sentences and use these similarities to promote similar translations for similar sentences. They used a graph-based semi-supervised model to re-rank the n-best translation hypothesis. Liu et al. (2012) extended Alexandrescu’s model to use translation consensus among similar sentences in bilingual training data by developing a new structured label propagation method. They derived some features to use during decoding process that has been shown useful in improving translation quality. Our graph propagation method connects monolingua"
P13-1109,N06-1003,0,0.762665,"was partially supported by an NSERC, Canada (RGPIN: 264905) grant. The third author was supported by an early career research award from Monash University to visit Simon Fraser University. re-order the words in the target language (Zhang et al., 2012). Increasing the size of the parallel data can reduce the number of oovs. However, there will always be some words or phrases that are new to the system and finding ways to translate such words or phrases will be beneficial to the system. Researchers have applied a number of approaches to tackle this problem. Some approaches use pivot languages (Callison-Burch et al., 2006) while others use lexicon-induction-based approaches from source language monolingual corpora (Koehn and Knight, 2002; Garera et al., 2009; Marton et al., 2009). Pivot language techniques tackle this problem by taking advantage of available parallel data between the source language and a third language. Using a pivot language, oovs are translated into a third language and back into the source language and thereby paraphrases to those oov words are extracted (Callison-Burch et al., 2006). For each oov, the system can be augmented by aggregating the translations of all its paraphrases and assign"
P13-1109,P11-2071,0,0.518554,"Missing"
P13-1109,J93-1003,0,0.034648,"Missing"
P13-1109,R11-1018,0,0.0412122,"1999; Fung and Yee, 1998), or dependency relations (Garera et al., 2009). Laws et al. (2010) used linguistic analysis in the form of graph-based models instead of a vector space. But all of these researches used an available seed lexicon as the basic source of similarity between source and target languages unlike our method which just needs a monolingual corpus of source language which is freely available for many languages and a small bilingual corpora. Some methods tried to alleviate the lack of seed lexicon by using orthographic similarity to extract a seed lexicon (Koehn and Knight, 2002; Fiser and Ljubesic, 2011). But it is not a practical solution in case of unrelated languages. Haghighi et al. (2008) and Daum´e and Jagarlamudi (2011) proposed generative models based on canonical correlation analysis to extract translation lexicons for non-parallel corpora by learning a matching between source and target lexicons. Using monolingual features to represent words, feature vectors are projected from source and target words into a canonical space to find the appropriate matching between them. Their method relies on context features which need a seed lexicon and orthographic features which only works for ph"
P13-1109,P98-1069,0,0.917184,"k paraphrases that have translations in the phrase-table are used to assign translations and scores to each oov word by marginalizing translations over paraphrases: p(t|o) = X p(t|s)p(s|o) s where t is a phrase on the target side, o is the oov word or phrase, and s is a paraphrase of o. p(s|o) is estimated using a similarity measure over DPs and p(t|s) is coming from the phrase-table. We reimplemented this collocational approach for finding translations for oovs and used it as a baseline system. Alternative ways of modeling and comparing distributional profiles have been proposed (Rapp, 1999; Fung and Yee, 1998; Terra and Clarke, 2003; Garera et al., 2009; Marton et al., 2009). We review some of them here and compare their performance in Section 4.3. 2.2 Association Measures Given a word u, its distributional profile DP (u) is constructed by counting surrounding words (in a fixed window size) in a monolingual corpus. 1106 DP (u) = {hA(u, wi )i |wi ∈ V } The counts can be collected in positional3 (Rapp, 1999) or non-positional way (count all the word occurrences within the sliding window). A(·, ·) is an association measure and can simply be defined as co-occurrence counts within sliding windows. Stro"
P13-1109,W09-1117,0,0.79905,"iversity to visit Simon Fraser University. re-order the words in the target language (Zhang et al., 2012). Increasing the size of the parallel data can reduce the number of oovs. However, there will always be some words or phrases that are new to the system and finding ways to translate such words or phrases will be beneficial to the system. Researchers have applied a number of approaches to tackle this problem. Some approaches use pivot languages (Callison-Burch et al., 2006) while others use lexicon-induction-based approaches from source language monolingual corpora (Koehn and Knight, 2002; Garera et al., 2009; Marton et al., 2009). Pivot language techniques tackle this problem by taking advantage of available parallel data between the source language and a third language. Using a pivot language, oovs are translated into a third language and back into the source language and thereby paraphrases to those oov words are extracted (Callison-Burch et al., 2006). For each oov, the system can be augmented by aggregating the translations of all its paraphrases and assign them to the oov. However, these methods require parallel corpora between the source language and one or multiple pivot languages. Another"
P13-1109,D12-1098,0,0.025325,"eds to be created. The number of possible edges can easily explode in size as there can be as many as O(n2 ) edges where n is the number of nodes. A common practice to control the number of edges is to connect each node to at most k other nodes (k-nearest neighbor). However, finding the top-k nearest nodes to each node requires considering its similarity to all the other nodes which requires O(n2 ) computations and since n is usually very large, doing such is practically intractable. Therefore, researchers usually resort to an approximate k-NN algorithms such as locality-sensitive hashing (?; Goyal et al., 2012). Fortunately, since we use context words as cues for relating their meaning and since the similarity measures are defined based on these cues, the number of neighbors we need to consider for each node is reduced by several orders of magnitude. We incorporate an inverted-index-style data structure which indicates what nodes are neighbors based on each context word. Therefore, the set of neighbors of a node consists of union of all the neighbors bridged by each context word in the DP of the node. However, the number of neighbors to be considered for each node even after this drastic reduction i"
P13-1109,P08-2015,0,0.0438362,"taking advantage of available parallel data between the source language and a third language. Using a pivot language, oovs are translated into a third language and back into the source language and thereby paraphrases to those oov words are extracted (Callison-Burch et al., 2006). For each oov, the system can be augmented by aggregating the translations of all its paraphrases and assign them to the oov. However, these methods require parallel corpora between the source language and one or multiple pivot languages. Another line of work exploits spelling and morphological variants of oov words. Habash (2008) presents techniques for online handling of oov words for Arabic to English such as spelling expansion and morphological expansion. Huang et al. (2011) proposes a method to combine sublexical/constituent translations of an oov word or phrase to generate its translations. Several researchers have applied lexiconinduction methods to create a bilingual lexicon for those oovs. Marton et al. (2009) use a monolingual text on the source side to find paraphrases to oov words for which the translations are available. The translations for these paraphrases are 1105 Proceedings of the 51st Annual Meeting"
P13-1109,P08-1088,0,0.644087,"ble is to be tuned along with the language model on the dev set, and run on the test set. BLEU (Papineni et al., 2002) is still the de facto evaluation metric for machine translation and we use that to measure the quality of our proposed approaches for MT. candiate list particularly specific only particular should and especially support agreement approval accession will approve endorses Table 7: Two examples of oov translations found by our method. 5 Related work There has been a long line of research on learning translation pairs from non-parallel corpora (Rapp, 1995; Koehn and Knight, 2002; Haghighi et al., 2008; Garera et al., 2009; Marton et al., 2009; Laws et al., 2010). Most have focused on extracting a translation lexicon by mining monolingual resources of data to find clues, using probabilistic methods to map words, or by exploiting the cross-language evidence of closely related languages. Most of them evaluated only highfrequency words of specific types (nouns or content words) (Rapp, 1995; Koehn and Knight, 2002; Haghighi et al., 2008; Garera et al., 2009; Laws et al., 2010) In contrast, we do not consider any constraint on our test data and our data includes many low frequency words. It has"
P13-1109,W11-2123,0,0.00533283,"taset Europarl EMEA Dev types tokens 1893 2229 2325 4317 Test types tokens 1830 2163 2294 4190 Table 2: number of oovs in dev and test sets for Europarl and EMEA systems. For the end-to-end MT pipeline, we used Moses (Koehn et al., 2007) with these standard features: relative-frequency and lexical translation model (TM) probabilities in both directions; distortion model; language model (LM) and word count. Word alignment is done using GIZA++ (Och and Ney, 2003). We used distortion limit of 6 and max-phrase-length of 10 in all the experiments. For the language model, we used the KenLM toolkit (Heafield, 2011) to create a 5-gram language model on the target side of the Europarl corpus (v7) with approximately 54M tokens with Kneser-Ney smoothing. 4.1.1 Phrase-table Integration Once the translations and their probabilities for each oov are extracted, they are added to the 8 http://www.statmt.org/wpt05/mt-shared-task/ phrase-table that is induced from the parallel text. The probability for new entries are added as a new feature in the log-linear framework to be tuned along with other features. The value of this newly introduced feature for original entries in the phrase-table is set to 1. Similarly, t"
P13-1109,W02-0902,0,0.963888,"rch award from Monash University to visit Simon Fraser University. re-order the words in the target language (Zhang et al., 2012). Increasing the size of the parallel data can reduce the number of oovs. However, there will always be some words or phrases that are new to the system and finding ways to translate such words or phrases will be beneficial to the system. Researchers have applied a number of approaches to tackle this problem. Some approaches use pivot languages (Callison-Burch et al., 2006) while others use lexicon-induction-based approaches from source language monolingual corpora (Koehn and Knight, 2002; Garera et al., 2009; Marton et al., 2009). Pivot language techniques tackle this problem by taking advantage of available parallel data between the source language and a third language. Using a pivot language, oovs are translated into a third language and back into the source language and thereby paraphrases to those oov words are extracted (Callison-Burch et al., 2006). For each oov, the system can be augmented by aggregating the translations of all its paraphrases and assign them to the oov. However, these methods require parallel corpora between the source language and one or multiple piv"
P13-1109,P07-2045,0,0.00763609,"rom the parallel data. From the oovs, we exclude numbers as well as named entities. We apply a simple heuristic to detect named entities: basically words that are capitalized in the original dev/test set that do not appear at the beginning of a sentence are named entities. Table 2 shows the number of oov types and tokens for Europarl and EMEA systems in both dev and test sets. Dataset Europarl EMEA Dev types tokens 1893 2229 2325 4317 Test types tokens 1830 2163 2294 4190 Table 2: number of oovs in dev and test sets for Europarl and EMEA systems. For the end-to-end MT pipeline, we used Moses (Koehn et al., 2007) with these standard features: relative-frequency and lexical translation model (TM) probabilities in both directions; distortion model; language model (LM) and word count. Word alignment is done using GIZA++ (Och and Ney, 2003). We used distortion limit of 6 and max-phrase-length of 10 in all the experiments. For the language model, we used the KenLM toolkit (Heafield, 2011) to create a 5-gram language model on the target side of the Europarl corpus (v7) with approximately 54M tokens with Kneser-Ney smoothing. 4.1.1 Phrase-table Integration Once the translations and their probabilities for ea"
P13-1109,2005.mtsummit-papers.11,0,0.032002,"Missing"
P13-1109,C10-2070,0,0.235868,"Missing"
P13-1109,P98-2127,0,0.273455,"way (count all the word occurrences within the sliding window). A(·, ·) is an association measure and can simply be defined as co-occurrence counts within sliding windows. Stronger association measures can also be used such as: Conditional probability: the probability for the occurrence of each word in DP given the occurrence of u: CP(u, wi ) = P (wi |u) (Sch¨utze and Pedersen, 1997) Pointwise Mutual Information: this measure is a transformation of the independence assumption into a ratio. Positive values indicate that words co-occur more than what we expect under the independence assumption (Lin, 1998): PMI(u, wi ) = log2 P (u, wi ) P (u)P (wi ) L(P (wi |u); p) ∗ L(P (wi |¬u); p) L(P (wi |u); p1 ) ∗ L(P (wi |¬u); p2 ) where L is likelihood function under the assumption that word counts in text have binomial distributions. The numerator represents the likelihood of the hypothesis that u and wi are independent (P (wi |u) = P (wi |¬u) = p) and the denominator represents the likelihood of the hypothesis that u and wi are dependent (P (wi |u) 6= P (wi |¬u) , P (wi |u) = p1 , P (wi |¬u) = p2 )4 . Chi-square test: is a statistical hypothesis testing method to evaluate independence of two categoric"
P13-1109,P12-1032,0,0.101525,"to a canonical space to find the appropriate matching between them. Their method relies on context features which need a seed lexicon and orthographic features which only works for phylogenetically related languages. Graph-based semi-supervised methods have been shown to be useful for domain adaptation in MT as well. Alexandrescu and Kirchhoff (2009) applied a graph-based method to determine similarities between sentences and use these similarities to promote similar translations for similar sentences. They used a graph-based semi-supervised model to re-rank the n-best translation hypothesis. Liu et al. (2012) extended Alexandrescu’s model to use translation consensus among similar sentences in bilingual training data by developing a new structured label propagation method. They derived some features to use during decoding process that has been shown useful in improving translation quality. Our graph propagation method connects monolingual source phrases with oovs to obtain translation and so is a very different use of graph propagation from these previous works. Recently label propagation has been used for lexicon induction (Tamura et al., 2012). They used a graph based on context similarity as we"
P13-1109,N01-1020,0,0.230157,"s to map words, or by exploiting the cross-language evidence of closely related languages. Most of them evaluated only highfrequency words of specific types (nouns or content words) (Rapp, 1995; Koehn and Knight, 2002; Haghighi et al., 2008; Garera et al., 2009; Laws et al., 2010) In contrast, we do not consider any constraint on our test data and our data includes many low frequency words. It has been shown that translation of high-frequency words is easier than low frequency words (Tamura et al., 2012). Some methods have used a third language(s) as pivot or bridge to find translation pairs (Mann and Yarowsky, 2001; Schafer and Yarowsky, 2002; Callison-Burch et al., 2006). 1112 Corpus Europarl EMEA System Baseline Our approach Baseline Our approach MRR – 5.9 – 3.6 Recall – 12.6 – 7.4 Dev Bleu 28.53 28.76 20.05 20.54 Test Bleu 28.97 29.40* 20.34 20.80* * Statistically significant with p < 0.02 using the bootstrap resampling significance test (in Moses). Table 6: Bleu scores for different domains with or without using oov translations. Context similarity has been used effectively in bilingual lexicon induction (Rapp, 1995; Koehn and Knight, 2002; Haghighi et al., 2008; Garera et al., 2009; Marton et al.,"
P13-1109,D09-1040,0,0.248917,"on Fraser University. re-order the words in the target language (Zhang et al., 2012). Increasing the size of the parallel data can reduce the number of oovs. However, there will always be some words or phrases that are new to the system and finding ways to translate such words or phrases will be beneficial to the system. Researchers have applied a number of approaches to tackle this problem. Some approaches use pivot languages (Callison-Burch et al., 2006) while others use lexicon-induction-based approaches from source language monolingual corpora (Koehn and Knight, 2002; Garera et al., 2009; Marton et al., 2009). Pivot language techniques tackle this problem by taking advantage of available parallel data between the source language and a third language. Using a pivot language, oovs are translated into a third language and back into the source language and thereby paraphrases to those oov words are extracted (Callison-Burch et al., 2006). For each oov, the system can be augmented by aggregating the translations of all its paraphrases and assign them to the oov. However, these methods require parallel corpora between the source language and one or multiple pivot languages. Another line of work exploits"
P13-1109,J03-1002,0,0.00334896,"beginning of a sentence are named entities. Table 2 shows the number of oov types and tokens for Europarl and EMEA systems in both dev and test sets. Dataset Europarl EMEA Dev types tokens 1893 2229 2325 4317 Test types tokens 1830 2163 2294 4190 Table 2: number of oovs in dev and test sets for Europarl and EMEA systems. For the end-to-end MT pipeline, we used Moses (Koehn et al., 2007) with these standard features: relative-frequency and lexical translation model (TM) probabilities in both directions; distortion model; language model (LM) and word count. Word alignment is done using GIZA++ (Och and Ney, 2003). We used distortion limit of 6 and max-phrase-length of 10 in all the experiments. For the language model, we used the KenLM toolkit (Heafield, 2011) to create a 5-gram language model on the target side of the Europarl corpus (v7) with approximately 54M tokens with Kneser-Ney smoothing. 4.1.1 Phrase-table Integration Once the translations and their probabilities for each oov are extracted, they are added to the 8 http://www.statmt.org/wpt05/mt-shared-task/ phrase-table that is induced from the parallel text. The probability for new entries are added as a new feature in the log-linear framewor"
P13-1109,P03-1021,0,0.0175584,"Missing"
P13-1109,P02-1040,0,0.0883445,"nder intrinsic and extrinsic evaluation metrics. 1 Introduction Out-of-vocabulary (oov) words or phrases still remain a challenge in statistical machine translation. SMT systems usually copy unknown words verbatim to the target language output. Although this is helpful in translating a small fraction of oovs such as named entities for languages with same writing systems, it harms the translation in other types of oovs and distant language pairs. In general, copied-over oovs are a hindrance to fluent, high quality translation, and we can see evidence of this in automatic measures such as BLEU (Papineni et al., 2002) and also in human evaluation scores such as HTER. The problem becomes more severe when only a limited amount of parallel text is available for training or when the training and test data are from different domains. Even noisy translation of oovs can aid the language model to better ∗ This research was partially supported by an NSERC, Canada (RGPIN: 264905) grant. The third author was supported by an early career research award from Monash University to visit Simon Fraser University. re-order the words in the target language (Zhang et al., 2012). Increasing the size of the parallel data can re"
P13-1109,W09-3209,0,0.0184289,"v with different extent for different labeled nodes. The second term (2) enforces the smoothness of the labeling according to the graph structure and edge weights. The last term (3) regularizes the soft labeling for a vertex v to match a priori label Rv , e.g. for high-degree unlabeled nodes (hubs in the graph) we may believe that the neighbors are not going to produce reliable label and hence the probability of undefined label ⊥ should be higher. The optimization problem can be solved with an efficient iterative algorithm which is parallelized in a MapReduce framework (Talukdar et al., 2008; Rao and Yarowsky, 2009). We used the Junto label propagation toolkit (Talukdar and Crammer, 2009) for label propagation. 3.2 Efficient Graph Construction Graph-based approaches can easily become computationally very expensive as the number of nodes grow. In our case, we use phrases in the monolingual text as graph vertices. These phrases are n-grams up to a certain value, which can result in millions of nodes. For each node a distributional profile (DP) needs to be created. The number of possible edges can easily explode in size as there can be as many as O(n2 ) edges where n is the number of nodes. A common practic"
P13-1109,D08-1061,0,0.20752,"Missing"
P13-1109,D12-1003,0,0.379855,"racting a translation lexicon by mining monolingual resources of data to find clues, using probabilistic methods to map words, or by exploiting the cross-language evidence of closely related languages. Most of them evaluated only highfrequency words of specific types (nouns or content words) (Rapp, 1995; Koehn and Knight, 2002; Haghighi et al., 2008; Garera et al., 2009; Laws et al., 2010) In contrast, we do not consider any constraint on our test data and our data includes many low frequency words. It has been shown that translation of high-frequency words is easier than low frequency words (Tamura et al., 2012). Some methods have used a third language(s) as pivot or bridge to find translation pairs (Mann and Yarowsky, 2001; Schafer and Yarowsky, 2002; Callison-Burch et al., 2006). 1112 Corpus Europarl EMEA System Baseline Our approach Baseline Our approach MRR – 5.9 – 3.6 Recall – 12.6 – 7.4 Dev Bleu 28.53 28.76 20.05 20.54 Test Bleu 28.97 29.40* 20.34 20.80* * Statistically significant with p < 0.02 using the bootstrap resampling significance test (in Moses). Table 6: Bleu scores for different domains with or without using oov translations. Context similarity has been used effectively in bilingual"
P13-1109,N03-1032,0,0.0251207,"ave translations in the phrase-table are used to assign translations and scores to each oov word by marginalizing translations over paraphrases: p(t|o) = X p(t|s)p(s|o) s where t is a phrase on the target side, o is the oov word or phrase, and s is a paraphrase of o. p(s|o) is estimated using a similarity measure over DPs and p(t|s) is coming from the phrase-table. We reimplemented this collocational approach for finding translations for oovs and used it as a baseline system. Alternative ways of modeling and comparing distributional profiles have been proposed (Rapp, 1999; Fung and Yee, 1998; Terra and Clarke, 2003; Garera et al., 2009; Marton et al., 2009). We review some of them here and compare their performance in Section 4.3. 2.2 Association Measures Given a word u, its distributional profile DP (u) is constructed by counting surrounding words (in a fixed window size) in a monolingual corpus. 1106 DP (u) = {hA(u, wi )i |wi ∈ V } The counts can be collected in positional3 (Rapp, 1999) or non-positional way (count all the word occurrences within the sliding window). A(·, ·) is an association measure and can simply be defined as co-occurrence counts within sliding windows. Stronger association measure"
P13-1109,P95-1050,0,0.887422,"with target-side translations and their feature values. A graph propagation algorithm is then used to propagate translations from labeled nodes to unlabeled nodes (phrases appearing only in the monolingual text and oovs). This provides a general purpose approach to handle several types of oovs, including morphological variants, spelling variants and synonyms2 . Constructing such a huge graph and propagating messages through it pose severe computational challenges. Throughout the paper, we will see how these challenges are dealt with using scalable algorithms. 2 Collocational Lexicon Induction Rapp (1995) introduced the notion of a distributional profile in bilingual lexicon induction from monolingual data. A distributional profile (DP) of a word or phrase type is a co-occurrence vector created by combining all co-occurrence vectors of the tokens of that phrase type. Each distributional profile can be seen as a point in a |V |-dimensional space where V is the vocabulary where each word type represents a unique axis. Points (i.e. phrase types) that are close to one another in this highdimensional space can represent paraphrases. This approach has also been used in machine translation to find in"
P13-1109,P99-1067,0,0.51044,"2). The top-k paraphrases that have translations in the phrase-table are used to assign translations and scores to each oov word by marginalizing translations over paraphrases: p(t|o) = X p(t|s)p(s|o) s where t is a phrase on the target side, o is the oov word or phrase, and s is a paraphrase of o. p(s|o) is estimated using a similarity measure over DPs and p(t|s) is coming from the phrase-table. We reimplemented this collocational approach for finding translations for oovs and used it as a baseline system. Alternative ways of modeling and comparing distributional profiles have been proposed (Rapp, 1999; Fung and Yee, 1998; Terra and Clarke, 2003; Garera et al., 2009; Marton et al., 2009). We review some of them here and compare their performance in Section 4.3. 2.2 Association Measures Given a word u, its distributional profile DP (u) is constructed by counting surrounding words (in a fixed window size) in a monolingual corpus. 1106 DP (u) = {hA(u, wi )i |wi ∈ V } The counts can be collected in positional3 (Rapp, 1999) or non-positional way (count all the word occurrences within the sliding window). A(·, ·) is an association measure and can simply be defined as co-occurrence counts within s"
P13-1109,W02-2026,0,0.377279,"loiting the cross-language evidence of closely related languages. Most of them evaluated only highfrequency words of specific types (nouns or content words) (Rapp, 1995; Koehn and Knight, 2002; Haghighi et al., 2008; Garera et al., 2009; Laws et al., 2010) In contrast, we do not consider any constraint on our test data and our data includes many low frequency words. It has been shown that translation of high-frequency words is easier than low frequency words (Tamura et al., 2012). Some methods have used a third language(s) as pivot or bridge to find translation pairs (Mann and Yarowsky, 2001; Schafer and Yarowsky, 2002; Callison-Burch et al., 2006). 1112 Corpus Europarl EMEA System Baseline Our approach Baseline Our approach MRR – 5.9 – 3.6 Recall – 12.6 – 7.4 Dev Bleu 28.53 28.76 20.05 20.54 Test Bleu 28.97 29.40* 20.34 20.80* * Statistically significant with p < 0.02 using the bootstrap resampling significance test (in Moses). Table 6: Bleu scores for different domains with or without using oov translations. Context similarity has been used effectively in bilingual lexicon induction (Rapp, 1995; Koehn and Knight, 2002; Haghighi et al., 2008; Garera et al., 2009; Marton et al., 2009; Laws et al., 2010). It"
P13-1109,C98-1066,0,\N,Missing
P13-1109,P05-1077,0,\N,Missing
P13-1109,C98-2122,0,\N,Missing
P13-1109,D08-1076,0,\N,Missing
P13-2060,P05-1033,0,0.688663,"ent models. These mixture operations receive two or more probabilities and return the mixture probability p(¯ e |f¯) for each rule e¯, f¯ used in the decoder. Different options for these operations are: p(¯ e |f¯) ∝ Train size m  4 Experiments & Results We experimented with two language pairs: French to English and Spanish to English on the Europarl corpus (v7) (Koehn, 2005) and used ACL/WMT 2005 2 data for dev and test sets. For the base models, we used an in-house implementation of hierarchical phrase-based systems, Kriya (Sankaran et al., 2012), which uses the same features mentioned in (Chiang, 2005): forward and backward relative-frequency and lexical TM probabilities; LM; word, phrase and gluerules penalty. GIZA++ (Och and Ney, 2003) has been used for word alignment with phrase length limit of 10. Feature weights were optimized using MERT (Och, 2003). We built a 5-gram language model on the English side of Europarl and used the Kneser-Ney smoothing method and SRILM (Stolcke, 2002) as the language model toolkit.  λm (wm · φm ) • Model Switching (Switch): Each cell in the CKY chart is populated only by rules from one of the models and the other models’ rules are discarded. Each component"
P13-2060,D09-1114,0,0.254398,"Missing"
P13-2060,C10-1035,0,0.0455825,"Missing"
P13-2060,P03-1021,0,0.281095,"2011; Xiao et al., 2010; Lagarda and Casacuberta, 2008). In this paper, we propose a method to adopt stacking (Wolpert, 1992), an ensemble learning technique, to SMT. We manipulate the full set of training data, creating k disjoint sets of held-out and held-in data sets as in k-fold cross-validation and build a model on each partition. This creates a diverse ensemble of statistical machine translation models where each member of the ensemble has different feature function values for the SMT log-linear model (Koehn, 2010). The weights of model are then tuned using minimum error rate training (Och, 2003) on the held-out fold to provide k weak models. We then create a strong Ensemble Learning Methods Two well-known instances of general framework of ensemble learning are bagging and boosting. Bagging (Breiman, 1996a) (bootstrap aggregating) takes a number of samples with replacement from a training set. The generated sample set may have 0, 1 or more instances of each original training instance. This procedure is repeated a number of times and the base learner is applied to each sample to produce a weak learner. These models are aggregated by doing a uniform voting for classification or averagin"
P13-2060,D08-1017,0,0.179884,"wo language pairs and three different sizes of training data show significant improvements of up to 4 BLEU points over a conventionally trained SMT model. 1 Introduction 2 Ensemble-based methods have been widely used in machine learning with the aim of reducing the instability of classifiers and regressors and/or increase their bias. The idea behind ensemble learning is to combine multiple models, weak learners, in an attempt to produce a strong model with less error. It has also been successfully applied to a wide variety of tasks in NLP (Tomeh et al., 2010; Surdeanu and Manning, 2010; F. T. Martins et al., 2008; Sang, 2002) and recently has attracted attention in the statistical machine translation community in various work (Xiao et al., 2013; Song et al., 2011; Xiao et al., 2010; Lagarda and Casacuberta, 2008). In this paper, we propose a method to adopt stacking (Wolpert, 1992), an ensemble learning technique, to SMT. We manipulate the full set of training data, creating k disjoint sets of held-out and held-in data sets as in k-fold cross-validation and build a model on each partition. This creates a diverse ensemble of statistical machine translation models where each member of the ensemble has d"
P13-2060,P12-1099,1,0.920553,"the k-fold crossvalidation technique. A diverse ensemble of weak systems is created by learning a model on each k − 1 fold and tuning the statistical machine translation log-linear weights on the remaining fold. However, instead of learning a model on the output of base models as in (Wolpert, 1992), we combine hypotheses from the base models in the decoder with uniform weights. For the base learner, we use Kriya (Sankaran et al., 2012), an in-house hierarchical phrase-based machine translation system, to produce multiple weak models. These models are combined together using Ensemble Decoding (Razmara et al., 2012) to produce a strong model in the decoder. This method is briefly explained in next section. 3.1 Ensemble Decoding SMT Log-linear models (Koehn, 2010) find the most likely target language output e given the source language input f using a vector of feature functions φ: 1 335   p(e|f ) ∝ exp w · φ http://www.netflixprize.com/ Ensemble decoding combines several models dynamically at decoding time. The scores are combined for each partial hypothesis using a user-defined mixture operation ⊗ over component models.   p(e|f ) ∝ exp w1 · φ1 ⊗ w2 · φ2 ⊗ . . . m λm exp wm · φm M X m 67K 365K 3M 58K"
P13-2060,W07-0717,0,0.0465742,"Missing"
P13-2060,P06-1121,0,0.0475484,"scores. However, doing such will generally lead to higher variance among base learners. Figure 1 shows the BLEU score of each of the base models resulted from a 20-fold partitioning of the devset along with the strong models’ BLEU scores. As the figure shows, the strong models are generally superior to the base models whose mean is represented as a horizontal line. 4.2 5 Related Work Xiao et al. (2013) have applied both boosting and bagging on three different statistical machine translation engines: phrase-based (Koehn et al., 2003), hierarchical phrase-based (Chiang, 2005) and syntax-based (Galley et al., 2006) and showed SMT can benefit from these methods as well. Duan et al. (2009) creates an ensemble of models by using feature subspace method in the machine learning literature (Ho, 1998). Each member of the ensemble is built by removing one nonLM feature in the log-linear framework or varying the order of language model. Finally they use a sentence-level system combination on the outputs of the base models to pick the best system for each Training on train+dev When we have some training data, we can use the cross-validation-style partitioning to create k splits. We then train a system on k − 1 fo"
P13-2060,2011.mtsummit-papers.33,0,0.0837879,"Missing"
P13-2060,N03-1017,0,0.0252494,"pairs. As the table shows, increasing the number of folds results in higher BLEU scores. However, doing such will generally lead to higher variance among base learners. Figure 1 shows the BLEU score of each of the base models resulted from a 20-fold partitioning of the devset along with the strong models’ BLEU scores. As the figure shows, the strong models are generally superior to the base models whose mean is represented as a horizontal line. 4.2 5 Related Work Xiao et al. (2013) have applied both boosting and bagging on three different statistical machine translation engines: phrase-based (Koehn et al., 2003), hierarchical phrase-based (Chiang, 2005) and syntax-based (Galley et al., 2006) and showed SMT can benefit from these methods as well. Duan et al. (2009) creates an ensemble of models by using feature subspace method in the machine learning literature (Ho, 1998). Each member of the ensemble is built by removing one nonLM feature in the log-linear framework or varying the order of language model. Finally they use a sentence-level system combination on the outputs of the base models to pick the best system for each Training on train+dev When we have some training data, we can use the cross-val"
P13-2060,2005.mtsummit-papers.11,0,0.0257167,"it performed better than approaches that pre-compute linear mixtures of different models (Razmara et al., 2012). Several mixture operations were proposed, allowing the user to encode belief about the relative strengths of the component models. These mixture operations receive two or more probabilities and return the mixture probability p(¯ e |f¯) for each rule e¯, f¯ used in the decoder. Different options for these operations are: p(¯ e |f¯) ∝ Train size m  4 Experiments & Results We experimented with two language pairs: French to English and Spanish to English on the Europarl corpus (v7) (Koehn, 2005) and used ACL/WMT 2005 2 data for dev and test sets. For the base models, we used an in-house implementation of hierarchical phrase-based systems, Kriya (Sankaran et al., 2012), which uses the same features mentioned in (Chiang, 2005): forward and backward relative-frequency and lexical TM probabilities; LM; word, phrase and gluerules penalty. GIZA++ (Och and Ney, 2003) has been used for word alignment with phrase length limit of 10. Feature weights were optimized using MERT (Och, 2003). We built a 5-gram language model on the English side of Europarl and used the Kneser-Ney smoothing method a"
P13-2060,2010.amta-papers.18,0,0.0286784,"the weak models on-the-fly. Experimental results on two language pairs and three different sizes of training data show significant improvements of up to 4 BLEU points over a conventionally trained SMT model. 1 Introduction 2 Ensemble-based methods have been widely used in machine learning with the aim of reducing the instability of classifiers and regressors and/or increase their bias. The idea behind ensemble learning is to combine multiple models, weak learners, in an attempt to produce a strong model with less error. It has also been successfully applied to a wide variety of tasks in NLP (Tomeh et al., 2010; Surdeanu and Manning, 2010; F. T. Martins et al., 2008; Sang, 2002) and recently has attracted attention in the statistical machine translation community in various work (Xiao et al., 2013; Song et al., 2011; Xiao et al., 2010; Lagarda and Casacuberta, 2008). In this paper, we propose a method to adopt stacking (Wolpert, 1992), an ensemble learning technique, to SMT. We manipulate the full set of training data, creating k disjoint sets of held-out and held-in data sets as in k-fold cross-validation and build a model on each partition. This creates a diverse ensemble of statistical machine tr"
P13-2060,J10-4005,0,0.184821,"stical machine translation community in various work (Xiao et al., 2013; Song et al., 2011; Xiao et al., 2010; Lagarda and Casacuberta, 2008). In this paper, we propose a method to adopt stacking (Wolpert, 1992), an ensemble learning technique, to SMT. We manipulate the full set of training data, creating k disjoint sets of held-out and held-in data sets as in k-fold cross-validation and build a model on each partition. This creates a diverse ensemble of statistical machine translation models where each member of the ensemble has different feature function values for the SMT log-linear model (Koehn, 2010). The weights of model are then tuned using minimum error rate training (Och, 2003) on the held-out fold to provide k weak models. We then create a strong Ensemble Learning Methods Two well-known instances of general framework of ensemble learning are bagging and boosting. Bagging (Breiman, 1996a) (bootstrap aggregating) takes a number of samples with replacement from a training set. The generated sample set may have 0, 1 or more instances of each original training instance. This procedure is repeated a number of times and the base learner is applied to each sample to produce a weak learner. T"
P13-2060,2008.eamt-1.14,0,0.58782,"have been widely used in machine learning with the aim of reducing the instability of classifiers and regressors and/or increase their bias. The idea behind ensemble learning is to combine multiple models, weak learners, in an attempt to produce a strong model with less error. It has also been successfully applied to a wide variety of tasks in NLP (Tomeh et al., 2010; Surdeanu and Manning, 2010; F. T. Martins et al., 2008; Sang, 2002) and recently has attracted attention in the statistical machine translation community in various work (Xiao et al., 2013; Song et al., 2011; Xiao et al., 2010; Lagarda and Casacuberta, 2008). In this paper, we propose a method to adopt stacking (Wolpert, 1992), an ensemble learning technique, to SMT. We manipulate the full set of training data, creating k disjoint sets of held-out and held-in data sets as in k-fold cross-validation and build a model on each partition. This creates a diverse ensemble of statistical machine translation models where each member of the ensemble has different feature function values for the SMT log-linear model (Koehn, 2010). The weights of model are then tuned using minimum error rate training (Och, 2003) on the held-out fold to provide k weak models"
P13-2060,P10-1076,0,0.0741592,"emble-based methods have been widely used in machine learning with the aim of reducing the instability of classifiers and regressors and/or increase their bias. The idea behind ensemble learning is to combine multiple models, weak learners, in an attempt to produce a strong model with less error. It has also been successfully applied to a wide variety of tasks in NLP (Tomeh et al., 2010; Surdeanu and Manning, 2010; F. T. Martins et al., 2008; Sang, 2002) and recently has attracted attention in the statistical machine translation community in various work (Xiao et al., 2013; Song et al., 2011; Xiao et al., 2010; Lagarda and Casacuberta, 2008). In this paper, we propose a method to adopt stacking (Wolpert, 1992), an ensemble learning technique, to SMT. We manipulate the full set of training data, creating k disjoint sets of held-out and held-in data sets as in k-fold cross-validation and build a model on each partition. This creates a diverse ensemble of statistical machine translation models where each member of the ensemble has different feature function values for the SMT log-linear model (Koehn, 2010). The weights of model are then tuned using minimum error rate training (Och, 2003) on the held-o"
P13-2060,P08-1108,0,0.0300347,"been used in SMT (Xiao et al., 2013; Xiao et al., 2010; Lagarda ∗ This research was partially supported by an NSERC, Canada (RGPIN: 264905) grant and a Google Faculty Award to the second author. 334 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 334–339, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics Algorithm 1: Stacking for SMT Stacking (aka blending) has been used in the system that won the Netflix Prize1 , which used a multi-level stacking algorithm. Stacking has been actively used in statistical parsing: Nivre and McDonald (2008) integrated two models for dependency parsing by letting one model learn from features generated by the other; F. T. Martins et al. (2008) further formalized the stacking algorithm and improved on Nivre and McDonald (2008); Surdeanu and Manning (2010) includes a detailed analysis of ensemble models for statistical parsing: i) the diversity of base parsers is more important than the complexity of the models; ii) unweighted voting performs as well as weighted voting; and iii) ensemble models that combine at decoding time significantly outperform models that combine multiple models at training ti"
P13-2060,J03-1002,0,0.00710432,", f¯ used in the decoder. Different options for these operations are: p(¯ e |f¯) ∝ Train size m  4 Experiments & Results We experimented with two language pairs: French to English and Spanish to English on the Europarl corpus (v7) (Koehn, 2005) and used ACL/WMT 2005 2 data for dev and test sets. For the base models, we used an in-house implementation of hierarchical phrase-based systems, Kriya (Sankaran et al., 2012), which uses the same features mentioned in (Chiang, 2005): forward and backward relative-frequency and lexical TM probabilities; LM; word, phrase and gluerules penalty. GIZA++ (Och and Ney, 2003) has been used for word alignment with phrase length limit of 10. Feature weights were optimized using MERT (Och, 2003). We built a 5-gram language model on the English side of Europarl and used the Kneser-Ney smoothing method and SRILM (Stolcke, 2002) as the language model toolkit.  λm (wm · φm ) • Model Switching (Switch): Each cell in the CKY chart is populated only by rules from one of the models and the other models’ rules are discarded. Each component model is considered as an expert on different spans of the source. A binary indicator function δ(f¯, m) picks a component model for each"
P13-2060,N10-1091,0,\N,Missing
P13-2060,D08-1076,0,\N,Missing
P18-1107,J76-4004,0,0.606533,"Missing"
P18-1107,C92-1057,0,0.656323,"CFG in the interest of reducing the rank of the transformed grammar. 7 Related Work Our work continues the study of TAGs and lexicalization (e.g. Joshi et al. 1975; Schabes and Waters 1993). Schabes and Waters (1995) show that TAG can strongly lexicalize CFG, whereas CFG only weakly lexicalizes itself; we show a similar result for SCFGs. Kuhlmann and Satta (2012) show that TAG is not closed under strong lexicalization, and Maletti and Engelfriet (2012) show how to strongly lexicalize TAG using simple context-free tree grammars (CFTGs). Other extensions of GNF to new grammar formalisms include Dymetman (1992) for definite clause grammars, Fernau and Stiebe (2002) for CF valence grammars, and Engelfriet et al. (2017) for multiple CFTGs. Although multiple CFTG subsumes SCFG (and STAG), Engelfriet et al.’s result appears to guarantee only that some side of every synchronous production will be lexicalized, whereas our result guarantees that it is always the target side that will be prefix lexicalized. Lexicalization of synchronous grammars was addressed by Zhang and Gildea (2005), but they consider lexicalization rather than prefix lexicalization, and they only consider SCFGs of rank 2. They motivate"
P18-1107,W13-0808,0,0.398518,"calized, because the prefix of every production is a lexical item. GNF has a variety of theoretical and practical applications, including for example the proofs of the famous theorems due to Shamir and Chomsky-Sch¨utzenberger (Shamir, 1967; Chomsky and Sch¨utzenberger, 1963; Autebert et al., 1997). Other applications of prefix lexicalization include proving coverage of parsing algorithms (Gray and Harrison, 1972) and decidability of equivalence problems (Christensen et al., 1995). By using prefix lexicalized synchronous context-free grammars (SCFGs), Watanabe et al. (2006) and Siahbani et al. (2013) obtain asymptotic and empirical speed improvements on a machine translation task. Using a prefix lexicalized grammar ensures that target sentences can be generated from left to right, which allows the use of beam search to constrain their decoder’s search space as it performs a left-to-right traversal of translation hypotheses. To achieve these results, new grammars had to be heuristically constrained to include only prefix lexicalized productions, as there is at present no way to automatically convert an existing SCFG to a prefix lexicalized form. This work investigates the formal properties"
P18-1107,J12-3006,0,0.0238245,"to handle more reorderings. Note that, since applying our transformation may double the rank of a grammar, this method may prove prohibitively slow. This highlights the need for future work to examine the generative power of rank-k PL-RSTAG relative to rankk SCFG in the interest of reducing the rank of the transformed grammar. 7 Related Work Our work continues the study of TAGs and lexicalization (e.g. Joshi et al. 1975; Schabes and Waters 1993). Schabes and Waters (1995) show that TAG can strongly lexicalize CFG, whereas CFG only weakly lexicalizes itself; we show a similar result for SCFGs. Kuhlmann and Satta (2012) show that TAG is not closed under strong lexicalization, and Maletti and Engelfriet (2012) show how to strongly lexicalize TAG using simple context-free tree grammars (CFTGs). Other extensions of GNF to new grammar formalisms include Dymetman (1992) for definite clause grammars, Fernau and Stiebe (2002) for CF valence grammars, and Engelfriet et al. (2017) for multiple CFTGs. Although multiple CFTG subsumes SCFG (and STAG), Engelfriet et al.’s result appears to guarantee only that some side of every synchronous production will be lexicalized, whereas our result guarantees that it is always th"
P18-1107,P12-1053,0,0.0201238,"rank of a grammar, this method may prove prohibitively slow. This highlights the need for future work to examine the generative power of rank-k PL-RSTAG relative to rankk SCFG in the interest of reducing the rank of the transformed grammar. 7 Related Work Our work continues the study of TAGs and lexicalization (e.g. Joshi et al. 1975; Schabes and Waters 1993). Schabes and Waters (1995) show that TAG can strongly lexicalize CFG, whereas CFG only weakly lexicalizes itself; we show a similar result for SCFGs. Kuhlmann and Satta (2012) show that TAG is not closed under strong lexicalization, and Maletti and Engelfriet (2012) show how to strongly lexicalize TAG using simple context-free tree grammars (CFTGs). Other extensions of GNF to new grammar formalisms include Dymetman (1992) for definite clause grammars, Fernau and Stiebe (2002) for CF valence grammars, and Engelfriet et al. (2017) for multiple CFTGs. Although multiple CFTG subsumes SCFG (and STAG), Engelfriet et al.’s result appears to guarantee only that some side of every synchronous production will be lexicalized, whereas our result guarantees that it is always the target side that will be prefix lexicalized. Lexicalization of synchronous grammars was a"
P18-1107,P08-1069,0,0.0959921,"Missing"
P18-1107,P94-1022,0,0.723196,"ee rooted in SXA is prefix lexicalized. Thus the trees created in step 4 are all prefix lexicalized variants of non-lexicalized tree pairs; steps 5–6 then remove the non-lexicalized trees from the grammar.  Figure 5 gives an example of this transformation applied to a small grammar. Note how A nodes at the left edge of the target trees end up rewritten as SAA nodes, as per step 4 of the transformation. 5 Complexity & Formal Properties Our conversion generates a subset of the class of prefix lexicalized STAGs in regular form, which we abbreviate to PL-RSTAG (regular form for TAG is defined in Rogers 1994). This section discusses some formal properties of PL-RSTAG. Generative Capacity PL-RSTAG is weakly equivalent to the class of ε-free, chain-free SCFGs: this follows immediately from the proof that our transformation does not change the language generated by the input SCFG. Note that every TAG in regular form generates a context-free language (Rogers, 1994). Alignments and Reordering PL-RSTAG generates the same set of reorderings (alignments) as SCFG. Observe that our transformation does not cause nonterminals which were linked in the original grammar to become unlinked, as noted for example i"
P18-1107,J95-4002,0,0.703207,"decod1167 ing; unlike a heuristically induced PL-SCFG, the transformed PL-RSTAG will generate the same language as the original SCFG which is known to handle more reorderings. Note that, since applying our transformation may double the rank of a grammar, this method may prove prohibitively slow. This highlights the need for future work to examine the generative power of rank-k PL-RSTAG relative to rankk SCFG in the interest of reducing the rank of the transformed grammar. 7 Related Work Our work continues the study of TAGs and lexicalization (e.g. Joshi et al. 1975; Schabes and Waters 1993). Schabes and Waters (1995) show that TAG can strongly lexicalize CFG, whereas CFG only weakly lexicalizes itself; we show a similar result for SCFGs. Kuhlmann and Satta (2012) show that TAG is not closed under strong lexicalization, and Maletti and Engelfriet (2012) show how to strongly lexicalize TAG using simple context-free tree grammars (CFTGs). Other extensions of GNF to new grammar formalisms include Dymetman (1992) for definite clause grammars, Fernau and Stiebe (2002) for CF valence grammars, and Engelfriet et al. (2017) for multiple CFTGs. Although multiple CFTG subsumes SCFG (and STAG), Engelfriet et al.’s re"
P18-1107,P06-1098,0,0.222234,"A grammar in GNF is said to be prefix lexicalized, because the prefix of every production is a lexical item. GNF has a variety of theoretical and practical applications, including for example the proofs of the famous theorems due to Shamir and Chomsky-Sch¨utzenberger (Shamir, 1967; Chomsky and Sch¨utzenberger, 1963; Autebert et al., 1997). Other applications of prefix lexicalization include proving coverage of parsing algorithms (Gray and Harrison, 1972) and decidability of equivalence problems (Christensen et al., 1995). By using prefix lexicalized synchronous context-free grammars (SCFGs), Watanabe et al. (2006) and Siahbani et al. (2013) obtain asymptotic and empirical speed improvements on a machine translation task. Using a prefix lexicalized grammar ensures that target sentences can be generated from left to right, which allows the use of beam search to constrain their decoder’s search space as it performs a left-to-right traversal of translation hypotheses. To achieve these results, new grammars had to be heuristically constrained to include only prefix lexicalized productions, as there is at present no way to automatically convert an existing SCFG to a prefix lexicalized form. This work investi"
P18-1107,J97-3002,0,0.812353,"formation has rank at most 2k. To see this, observe that the construction of the intermediate grammars increases the rank by at most 1 (see Figure 3(b)). When a prefix lexicalized tree is substituted at the left edge of a non-lexicalized tree, the link on the substitution site will be consumed, but up to k + 1 new links will be introduced by the substituting tree, so that the final tree will have rank at most 2k. In the general case, rank-k STAG is more powerful than rank-k SCFG; for example, a rank-4 SCFG is required to generate the reordering in hS → A 1 B 2 C 3 D 4 , S → C 3 A 1 D 4 B 2 i (Wu, 1997), but this reordering is captured by the 6 Although we consume one link whenever we substitute a prefix lexicalized tree at the left edge of an unlexicalized tree, that link can still be remembered and used to reconstruct the reorderings which occurred between the two sentences. following rank-3 STAG: S  X  S , A↓ 1 C↓ X2 C↓ A↓ 3 2 3 X  X↓ 1 X  , B↓ 1 X∗ D↓ 2 D↓ 2 B↓ 1 For this reason, we speculate that it is possible to further transform the grammars produced by our lexicalization in order to reduce their rank, but the details of this transformation remain as future work. This potentially"
P18-1107,P05-1059,0,0.066156,"strongly lexicalize TAG using simple context-free tree grammars (CFTGs). Other extensions of GNF to new grammar formalisms include Dymetman (1992) for definite clause grammars, Fernau and Stiebe (2002) for CF valence grammars, and Engelfriet et al. (2017) for multiple CFTGs. Although multiple CFTG subsumes SCFG (and STAG), Engelfriet et al.’s result appears to guarantee only that some side of every synchronous production will be lexicalized, whereas our result guarantees that it is always the target side that will be prefix lexicalized. Lexicalization of synchronous grammars was addressed by Zhang and Gildea (2005), but they consider lexicalization rather than prefix lexicalization, and they only consider SCFGs of rank 2. They motivate their results using a word alignment task, which may be another possible application for our lexicalization. Analogous to our closure result, Aho and Ullman (1969) prove that SCFG does not admit a normal form with bounded rank like Chomsky normal form. Blum and Koch (1999) use intermediate grammars like our GXA s to transform a CFG to GNF. Another GNF transformation (Rosenkrantz, 1967) is used by Schabes and Waters (1995) to define Tree Insertion Grammars (which are also"
P18-1107,D13-1110,1,0.933102,"o be prefix lexicalized, because the prefix of every production is a lexical item. GNF has a variety of theoretical and practical applications, including for example the proofs of the famous theorems due to Shamir and Chomsky-Sch¨utzenberger (Shamir, 1967; Chomsky and Sch¨utzenberger, 1963; Autebert et al., 1997). Other applications of prefix lexicalization include proving coverage of parsing algorithms (Gray and Harrison, 1972) and decidability of equivalence problems (Christensen et al., 1995). By using prefix lexicalized synchronous context-free grammars (SCFGs), Watanabe et al. (2006) and Siahbani et al. (2013) obtain asymptotic and empirical speed improvements on a machine translation task. Using a prefix lexicalized grammar ensures that target sentences can be generated from left to right, which allows the use of beam search to constrain their decoder’s search space as it performs a left-to-right traversal of translation hypotheses. To achieve these results, new grammars had to be heuristically constrained to include only prefix lexicalized productions, as there is at present no way to automatically convert an existing SCFG to a prefix lexicalized form. This work investigates the formal properties"
P18-1107,2014.amta-researchers.1,1,0.943502,"6 Although we consume one link whenever we substitute a prefix lexicalized tree at the left edge of an unlexicalized tree, that link can still be remembered and used to reconstruct the reorderings which occurred between the two sentences. following rank-3 STAG: S  X  S , A↓ 1 C↓ X2 C↓ A↓ 3 2 3 X  X↓ 1 X  , B↓ 1 X∗ D↓ 2 D↓ 2 B↓ 1 For this reason, we speculate that it is possible to further transform the grammars produced by our lexicalization in order to reduce their rank, but the details of this transformation remain as future work. This potentially poses a solution to an issue raised by Siahbani and Sarkar (2014b). On a Chinese-English translation task, they find that sentences like (15) involve reorderings which cannot be captured by a rank-2 prefix lexicalized SCFG: (15) T¯a bˇuch¯ong shu¯o , li´anh´e zh`engfˇu m`uqi´an zhu`angku`ang wˇend`ıng ... He added that the coalition government is now in stable condition ... If rank-k PL-RSTAG is more powerful than rank-k 1166 SCFG, using a PL-RSTAG here would permit capturing more reorderings without using grammars of higher rank. its maximum at x = 0.5. |G| 15k 15k 15k 15k 15k 15k 15k 15k 15k Parse Complexity Because the grammar produced is in regular for"
P18-1107,D14-1028,1,0.907815,"Missing"
P18-1107,P13-1030,0,0.0254874,"r lexicalization. Analogous to our closure result, Aho and Ullman (1969) prove that SCFG does not admit a normal form with bounded rank like Chomsky normal form. Blum and Koch (1999) use intermediate grammars like our GXA s to transform a CFG to GNF. Another GNF transformation (Rosenkrantz, 1967) is used by Schabes and Waters (1995) to define Tree Insertion Grammars (which are also weakly equivalent to CFG). We rely on Rogers (1994) for the claim that our transformed grammars generate context-free languages despite allowing wrapping adjunction; an alternative proof could employ the results of Swanson et al. (2013), who develop their own context-free TAG variant known as osTAG. Kaeshammer (2013) introduces the class of synchronous linear context-free rewriting systems to model reorderings which cannot be captured by a rank-2 SCFG. In the event that rank-k PL-RSTAG is more powerful than rank-k SCFG, our work can be seen as an alternative approach to the same problem. Finally, Nesson et al. (2008) present an algorithm for reducing the rank of an STAG on-the-fly during parsing; this presents a promising avenue for proving a smaller upper bound on the rank increase caused by our transformation. 8 Conclusion"
P93-1047,P84-1038,0,0.0217353,"ing phonological rules to coexist. The computational complexity of the extension is the same as Kimmo&apos;s two-level model. (i) LR: tulai+Ota SR: tolaiOtta (adj. who resembles [something]) In this example, the consonant insertion at the morpheme boundary is consistent with Tamil phonology, but the realization of u as o in the environment of t u follows a morphology that originates in Sanskrit and which causes inconsistency when used as a general rule in Tamil. The following example illustrates how regular Tamil phonology works. INTRODUCTION Kimmo Koskenniemi&apos;s two-level model (Koskenniemi, 1983, Koskenniemi, 1984) uses finite-state transducers to implement phonological rules. This paper presents the experience of attempting a twolevel phonology for certain Indian languages; the problems faced in this attempt and their resolution. The languages we consider are Tamil and Hindi. For the languages considered we want to show that practical descriptions of their morphology can be achieved by a simple generalization of the two-level model. Although the basic two-level model has been generalized in this paper, the extensions do not affect the complexity or the basic tenets of the two-level model. (2) LR: kudi+"
P93-1047,W83-0114,0,\N,Missing
P96-1056,P90-1035,0,0.0150089,"parse tables built so far. 1 LR Parser of Figure 1: Recognition of adjunction in a BEPDA. Generation Tree Adjoining Grammars (TAGs) are tree rewriting systems which combine trees with the single operation of adjoining. (Schabes and VijayShanker, 1990) describes the construction of an LR parsing algorithm for TAGs 1. Parser generation here is taken to be the construction of LR(0) tables (i.e., without any lookahead) for a particular TAG z. The moves made by the parser can be explained by an automaton which is weakly equivalent to TAGs called Bottom-Up Embedded Pushdown A u t o m a t a (BEPDA) (Schabes and Vijay-Shanker, 1990) 3. Storage in a BEPDA is a sequence of stacks, *This work is partially supported by NSF grant NSFSTC SBR 8920230 ARPA grant N00014-94 and ARO grant DAAH04-94-G0426. Thanks to Breck Baldwin, Dania Egedi, Jason Eisner, B. Srinivas and the three anonymous reviewers for their valuable comments. 1Familiarity with TAGs and their parsing techniques is assumed throughout the paper, see (Schabes and Joshi, 1991) for an introduction. We assume that our definition of TAG does not have the substitution operation. See (Aho et al., 1986) for details on LR parsing. 2The algorithm described here can be exten"
P96-1056,E91-1006,0,0.0369204,"ad) for a particular TAG z. The moves made by the parser can be explained by an automaton which is weakly equivalent to TAGs called Bottom-Up Embedded Pushdown A u t o m a t a (BEPDA) (Schabes and Vijay-Shanker, 1990) 3. Storage in a BEPDA is a sequence of stacks, *This work is partially supported by NSF grant NSFSTC SBR 8920230 ARPA grant N00014-94 and ARO grant DAAH04-94-G0426. Thanks to Breck Baldwin, Dania Egedi, Jason Eisner, B. Srinivas and the three anonymous reviewers for their valuable comments. 1Familiarity with TAGs and their parsing techniques is assumed throughout the paper, see (Schabes and Joshi, 1991) for an introduction. We assume that our definition of TAG does not have the substitution operation. See (Aho et al., 1986) for details on LR parsing. 2The algorithm described here can be extended to use SLR(1) tables (Schabes and Vijay-Shanker, 1990). SNote that the LR(0) tables considered here are deterministic and hence correspond to a subset of the TALs. Techniques developed in (Tomita, 1986) can be used to resolve nondeterminism in the parser. 375 The LR parser (of (Schabes and Vijay-Shanker, 1990)) uses a parsing table and a sequence of stacks (Fig. 1) to parse the input. The parsing tab"
P98-2157,J91-3004,0,0.178971,"amme Language and Speech Technology, which is sponsored by NWO (Dutch Organization for Scientific Research). The second and third authors were partially supported by NSF grant SBR8920230 and ARO grant DAAH0404-94G-0426. The authors wish to thank Aravind Joshi for his support in this research. 953 Giorgio Satta Dip. di Elettr. e Inf. Univ. di Padova via Gradenigo 6/A, 35131 Padova, Italy satta@dei, unipd, it ural language would improve performance of such language models, some researchers tried to use stochastic context-free grammars (CFGs) to produce language models (Wright and Wrigley, 1989; Jelinek and Lafferty, 1991; Stolcke, 1995). The probability model used for a stochastic grammar was ~we~* P r ( a l . . - a n w ) . However, language models that are based on trigram probability models out-perform stochastic CFGs. The common wisdom about this failure of CFGs is that trigram models are lexicalized models while CFGs are not. Tree Adjoining Grammars (TAGs) are important in this respect since they are easily lexicalized while capturing the constituent structure of language. More importantly, TAGs allow greater linguistic expressiveness. The trees associated with words can be used to encode argument and adj"
P98-2157,C92-2066,0,0.0422418,"s that are based on trigram probability models out-perform stochastic CFGs. The common wisdom about this failure of CFGs is that trigram models are lexicalized models while CFGs are not. Tree Adjoining Grammars (TAGs) are important in this respect since they are easily lexicalized while capturing the constituent structure of language. More importantly, TAGs allow greater linguistic expressiveness. The trees associated with words can be used to encode argument and adjunct relations in various syntactic environments. This paper assumes some familiarity with the TAG formalism. (Joshi, 1988) and (Joshi and Schabes, 1992) are good introductions to the formalism and its linguistic relevance. TAGs have been shown to have relations with both phrase-structure grammars and dependency grammars (Rambow and Joshi, 1995), which is relevant because recent work on structured language models (Chelba et al., 1997) have used dependency grammars to exploit their lexicalization. We use stochastic TAGs as such a structured language model in contrast with earlier work where TAGs have been exploited in a class-based n-gram language model (Srinivas, 1996). This paper derives an algorithm to compute prefix probabilities ~we~* P r"
P98-2157,C88-1075,0,0.0391071,"s for stochastic TAGs. flff/~2 n 4.1 C General equations E The prefix probability is given by: Figure 2: Wrapping of auxiliary trees when computing the prefix probability To derive a method for the computation of prefix probabilities, we give some simple recursive equations. Each equation decomposes an item into other items in all possible ways, in the sense that it expresses the probability of that item as a function of the probabilities of items associated with equal or smaller portions of the input. In specifying the equations, we exploit techniques used in the parsing of incomplete input (Lang, 1988). This allows us to compute the prefix probability as a by-product of computing the inside probability. 955 Pr(al...anw) wEE* = ~ P([t,O,n,-,-]), fEZ where P is a function over items recursively defined as follows: P([t,i,j, fl,f2]) = P([Rt, i,j, fl,f2]); P([t~N,i,j,-,-]) = P([a,i,k,-,-]) k(i < k < j) . P([N,k,j,-,-]), if a ¢ e A -~dft(aN); P([t~N, i, j, fl, f2]) = Z P([a,i,k,-,-])-P([N,k,j, k(i < k < fl) if ~ ¢ ¢ A dft(g); (1) (2) (3) fl,f2]), P([aN, i, j, fl, f2]) = (4) P([a, i, k, fl, f2]). P([N, k, j, -, -]), k(f2 <_ k <_j) if # c^ ((i',j') = (i,j))A = (fl, f2)v ((f~ = f~ = iV f{ = f~ = j"
P98-2157,J95-2002,0,0.358178,"hnology, which is sponsored by NWO (Dutch Organization for Scientific Research). The second and third authors were partially supported by NSF grant SBR8920230 and ARO grant DAAH0404-94G-0426. The authors wish to thank Aravind Joshi for his support in this research. 953 Giorgio Satta Dip. di Elettr. e Inf. Univ. di Padova via Gradenigo 6/A, 35131 Padova, Italy satta@dei, unipd, it ural language would improve performance of such language models, some researchers tried to use stochastic context-free grammars (CFGs) to produce language models (Wright and Wrigley, 1989; Jelinek and Lafferty, 1991; Stolcke, 1995). The probability model used for a stochastic grammar was ~we~* P r ( a l . . - a n w ) . However, language models that are based on trigram probability models out-perform stochastic CFGs. The common wisdom about this failure of CFGs is that trigram models are lexicalized models while CFGs are not. Tree Adjoining Grammars (TAGs) are important in this respect since they are easily lexicalized while capturing the constituent structure of language. More importantly, TAGs allow greater linguistic expressiveness. The trees associated with words can be used to encode argument and adjunct relations i"
P98-2157,W89-0211,0,0.0328774,"and by the Priority Programme Language and Speech Technology, which is sponsored by NWO (Dutch Organization for Scientific Research). The second and third authors were partially supported by NSF grant SBR8920230 and ARO grant DAAH0404-94G-0426. The authors wish to thank Aravind Joshi for his support in this research. 953 Giorgio Satta Dip. di Elettr. e Inf. Univ. di Padova via Gradenigo 6/A, 35131 Padova, Italy satta@dei, unipd, it ural language would improve performance of such language models, some researchers tried to use stochastic context-free grammars (CFGs) to produce language models (Wright and Wrigley, 1989; Jelinek and Lafferty, 1991; Stolcke, 1995). The probability model used for a stochastic grammar was ~we~* P r ( a l . . - a n w ) . However, language models that are based on trigram probability models out-perform stochastic CFGs. The common wisdom about this failure of CFGs is that trigram models are lexicalized models while CFGs are not. Tree Adjoining Grammars (TAGs) are important in this respect since they are easily lexicalized while capturing the constituent structure of language. More importantly, TAGs allow greater linguistic expressiveness. The trees associated with words can be use"
P98-2190,C92-2066,0,0.120449,"ewers for their valuable comments. 1164 From the literature on probabilistic contextfree grammars (CFGs) we know precisely the conditions which ensure that (1) is true for a given CFG. This paper derives the conditions under which a given probabilistic TAG can be shown to be consistent. TAGs are important in the modelling of natural language since they can be easily lexicalized; moreover the trees associated with words can be used to encode argument and adjunct relations in various syntactic environments. This paper assumes some familiarity with the TAG formalism. (Joshi, 1988) and (Joshi and Schabes, 1992) are good introductions to the formalism and its linguistic relevance. TAGs have been shown to have relations with b o t h phrasestructure grammars and dependency grammars (Rambow and Joshi, 1995) and can handle (non-projective) long distance dependencies. Consistency of probabilistic TAGs has practical significance for the following reasons: • The conditions derived here can be used to ensure that probability models that use TAGs can be checked for deficiency. • Existing EM based estimation algorithms for probabilistic TAGs assume that the property of consistency holds (Schabes, 1992). EM bas"
Q17-1035,N10-1083,0,0.0657553,"Missing"
Q17-1035,P06-1009,0,0.149179,"attention, are also an important component in neural machine translation (NMT) systems. The classic generative model approach to word alignment is based on IBM models 1-5 (Brown et al., 1993) and the HMM model (Vogel et al., 1996; Och and Ney, 2000a). These traditional models use unsupervised algorithms to learn alignments, relying on a large amount of parallel training data without hand annotated alignments. Supervised algorithms for word alignment have become more widespread with the availability of manually annotated word-aligned data and have shown promising results (Taskar et al., 2005; Blunsom and Cohn, 2006; Moore et al., 2006; Liang et al., 2006). Manually word-aligned data are valuable resources for SMT research, but they are costly to create and are only available for a handful of language pairs. Semisupervised methods for word alignment combine Unsupervised or semi-supervised probabilistic word alignment models do not play a central role in neural machine translation (NMT) (Bahdanau et al., 2015; Sutskever et al., 2014; Luong et al., 2015; Chung et al., 2016). However, attention models, which are crucial for high-quality NMT, have been augmented with ideas from statistical word alignment (Lu"
Q17-1035,J93-2003,0,0.0939877,"propose novel semi-supervised learning algorithms for this task. We also solve a sub-task of predicting the alignment type given an aligned word pair. In our experimental results, the generative models we introduce to model alignment types significantly outperform the models without alignment types. 1 Introduction Word alignment is an essential component in a statistical machine translation (SMT) system. Soft alignments, or attention, are also an important component in neural machine translation (NMT) systems. The classic generative model approach to word alignment is based on IBM models 1-5 (Brown et al., 1993) and the HMM model (Vogel et al., 1996; Och and Ney, 2000a). These traditional models use unsupervised algorithms to learn alignments, relying on a large amount of parallel training data without hand annotated alignments. Supervised algorithms for word alignment have become more widespread with the availability of manually annotated word-aligned data and have shown promising results (Taskar et al., 2005; Blunsom and Cohn, 2006; Moore et al., 2006; Liang et al., 2006). Manually word-aligned data are valuable resources for SMT research, but they are costly to create and are only available for a"
Q17-1035,P04-1023,0,0.0610546,"ted alignment type. From the confusion SEM FUN GIS GIF MDE PDE CDE COI TIN SEM 11374 196 2790 16 0 1 0 91 22 FUN 136 8172 31 118 0 0 5 2 12 GIS 2002 21 3312 26 1 1 0 48 11 GIF 10 16 18 772 0 2 0 0 3 MDE 0 2 2 0 293 40 0 0 0 PDE 0 0 1 0 26 55 0 0 0 CDE 0 0 2 0 2 0 79 0 0 COI 14 1 8 0 0 0 0 38 0 TIN 3 1 0 2 0 0 0 0 5 Table 11: Confusion matrix of the HMM+Type+Gen model on the LDC test data. The vertical axis represents the actual alignment type and the horizontal axis represents the predicted alignment type. 7 Related Work There has been several studies on semi-supervised word alignment models. Callison-Burch et al. (2004) improve alignment and translation quality by interpolating hand-annotated, word-aligned data and automatic sentence-aligned data. They showed 4 We should note that these incorrectly predicted alignments are only kept out of the confusion matrix. All alignments, correct or incorrect, are included in all the results we show in the other tables. 511 意大利人 和 西班牙人 以 33 天 和 31 天 分列 第二 位 和 第三 位 。 1 3 2∗ 3 3 3 1∗ 1∗ 3 3 1 3 3 1 1 3 italians and spaniards took 2nd and 3rd place with 33 days and 31 days , respectively . (a) 什么时候 能 跟 胡 锦涛 见面 ? matrix, we found that our model works well in predicting SEM,"
Q17-1035,P16-1160,0,0.0311258,"ore widespread with the availability of manually annotated word-aligned data and have shown promising results (Taskar et al., 2005; Blunsom and Cohn, 2006; Moore et al., 2006; Liang et al., 2006). Manually word-aligned data are valuable resources for SMT research, but they are costly to create and are only available for a handful of language pairs. Semisupervised methods for word alignment combine Unsupervised or semi-supervised probabilistic word alignment models do not play a central role in neural machine translation (NMT) (Bahdanau et al., 2015; Sutskever et al., 2014; Luong et al., 2015; Chung et al., 2016). However, attention models, which are crucial for high-quality NMT, have been augmented with ideas from statistical word alignment (Luong et al., 2015; Cohn et al., 2016). Other than machine translation, word alignments are also important in the best performing models for NLP tasks. They play a central role in learning paraphrases in a source language by doing round-trips from source to target and back using word alignments (Ganitkevitch et al., 2013). Aligments also form the basis for learning multi-lingual word embeddings (Faruqui and Dyer, 2014; Lu et al., 2015) and in the projection of sy"
Q17-1035,P11-2031,0,0.0108352,"sentences of MTC part 4 (LDC2006T04). We trained models in each translation direction and then symmetrized the produced alignments using the grow-diag-final heuristic (Och and Ney, 2003). We used Moses (Koehn et al., 2007) with standard features, and tuned the weights with MERT (Och, 2003). An English 5-gram language model is trained using KenLM (Heafield, 2011) on the Gigaword corpus (Parker et al., 2011). We give a comparison between HMM+Type+Gen model, our baseline HMM, GIZA++ HMM and standard GIZA++ (as used by Moses) in Table 10. We report the BLEU scores and TER computed using MultEval (Clark et al., 2011). The generative model improves over GIZA++ HMM by 1.0 BLEU points. It also im510 Discussion Figure 2 shows the performance of baseline HMM and HMM+Type+Gen model for two word alignment examples extracted from the test data, where squares indicate the gold standard alignments. Numbers in the circles show the IDs of the predicted tags by the HMM+Type+Gen model, where ID of each tag is defined in Table 1. The incorrectly predicted tags are shown with the ∗ symbol. In both examples, the HMM+Type+Gen model identifies difficult alignments over long distances better than the baseline HMM. For exampl"
Q17-1035,N16-1102,0,0.0117533,"06; Liang et al., 2006). Manually word-aligned data are valuable resources for SMT research, but they are costly to create and are only available for a handful of language pairs. Semisupervised methods for word alignment combine Unsupervised or semi-supervised probabilistic word alignment models do not play a central role in neural machine translation (NMT) (Bahdanau et al., 2015; Sutskever et al., 2014; Luong et al., 2015; Chung et al., 2016). However, attention models, which are crucial for high-quality NMT, have been augmented with ideas from statistical word alignment (Luong et al., 2015; Cohn et al., 2016). Other than machine translation, word alignments are also important in the best performing models for NLP tasks. They play a central role in learning paraphrases in a source language by doing round-trips from source to target and back using word alignments (Ganitkevitch et al., 2013). Aligments also form the basis for learning multi-lingual word embeddings (Faruqui and Dyer, 2014; Lu et al., 2015) and in the projection of syntactic and semantic annotations from one language to another (Hwa et al., 2005; McDonald et al., 2011). Therefore, there is still a prominent role for word alignment in N"
Q17-1035,P10-1147,0,0.0183133,"r’s output as features. This feedback loop iteratively improves the quality of both aligners. Niehues and Vogel (2008) propose a discriminative model that directly models the alignment matrix. Although the discriminative model provides the flexibility to use manually word-aligned data to tune its weights, it still relies on the model parameters of IBM models and alignment links from GIZA++ as features. Gao et al. (2010a) present a semi-supervised algorithm that extends IBM Model 4 by using partial manual alignments. Partial alignments are fixed and treated as constraints into the EM training. DeNero and Klein (2010) present a supervised model for extracting phrase pairs under a discriminative model by using word alignments. They consider two types of alignment links, sure and possible, that are extracted from the manually word-aligned data. Possible alignment links dictate which phrase pairs can be extracted from a sentence pair. Among the unsupervised methods, (Toutanova et al., 2002) utilizes additional source of information apart from the parallel sentences. Part-of-speech tags of the words in the sentence pair are incorporated as a linguistic constraint on the HMM-based word alignment. The part-of-sp"
Q17-1035,P07-1001,0,0.029918,"016; Revision batch: 3/17; Published 11/2017. c 2017 Association for Computational Linguistics. Distributed under a CC-BY 4.0 license. 所以 一定 要 好好 照顾 自己 GIS FUN so you must FUN SEM be sure 。 GIF FUN SEM to take really good care of yourself . Figure 1: An alignment between a Chinese sentence and its translation in English which is enriched with alignment types . SEM (semantic), FUN (function), GIF (grammatically inferred function) and GIS (grammatically inferred semantic) are tags of the links. by designing linguistically-motivated features (Ittycheriah and Roukos, 2005; Blunsom and Cohn, 2006; Deng and Gao, 2007; Berg-Kirkpatrick et al., 2010; Dyer et al., 2011). These models provide evidence that additional constraints can help in modelling word alignments in a log-linear model where word based features can be augmented with morphological, syntactic or semantic features. For example, such a model might learn that function words in one language tend to be aligned to function words in the other language. In this paper, we propose a novel task which is the joint prediction of word alignment and alignment types for a given sentence pair in a parallel corpus. We present how to enhance the alignment model"
Q17-1035,P11-1042,0,0.0199902,"7 Association for Computational Linguistics. Distributed under a CC-BY 4.0 license. 所以 一定 要 好好 照顾 自己 GIS FUN so you must FUN SEM be sure 。 GIF FUN SEM to take really good care of yourself . Figure 1: An alignment between a Chinese sentence and its translation in English which is enriched with alignment types . SEM (semantic), FUN (function), GIF (grammatically inferred function) and GIS (grammatically inferred semantic) are tags of the links. by designing linguistically-motivated features (Ittycheriah and Roukos, 2005; Blunsom and Cohn, 2006; Deng and Gao, 2007; Berg-Kirkpatrick et al., 2010; Dyer et al., 2011). These models provide evidence that additional constraints can help in modelling word alignments in a log-linear model where word based features can be augmented with morphological, syntactic or semantic features. For example, such a model might learn that function words in one language tend to be aligned to function words in the other language. In this paper, we propose a novel task which is the joint prediction of word alignment and alignment types for a given sentence pair in a parallel corpus. We present how to enhance the alignment model with alignment types. The primary contribution of"
Q17-1035,E14-1049,0,0.0246322,"015; Sutskever et al., 2014; Luong et al., 2015; Chung et al., 2016). However, attention models, which are crucial for high-quality NMT, have been augmented with ideas from statistical word alignment (Luong et al., 2015; Cohn et al., 2016). Other than machine translation, word alignments are also important in the best performing models for NLP tasks. They play a central role in learning paraphrases in a source language by doing round-trips from source to target and back using word alignments (Ganitkevitch et al., 2013). Aligments also form the basis for learning multi-lingual word embeddings (Faruqui and Dyer, 2014; Lu et al., 2015) and in the projection of syntactic and semantic annotations from one language to another (Hwa et al., 2005; McDonald et al., 2011). Therefore, there is still a prominent role for word alignment in NLP; research into improvements in word alignment is a worthy goal. Adding additional information such as part-ofspeech tags and syntactic parse information has yielded some improvements in word alignment quality. Toutanova et al. (2002) incorporated the partof-speech (POS) tags of the words in the sentence pair as a constraint on HMM-based word alignment. Additional constraints ha"
Q17-1035,P06-1097,0,0.0491435,"ws the alignment type predictions for the word pairs that were correctly aligned by HMM+Type+Gen.4 3 3 when will [ he ] meet 1 3 with hu 1 jintao 1 3 ? (b) Figure 2: A comparison between the performance of baseline HMM and HMM+Type+Gen model for two test sentences; #: HMM+Type+Gen, 4: HMM and 2: gold alignment. Numbers in the circles show the IDs of the predicted alignment types by the HMM+Type+Gen model, where ids are given in Table 1. The incorrectly predicted alignment types are shown with the ∗ symbol. that a much higher weight should be assigned to the model trained on word-aligned data. Fraser and Marcu (2006) propose a semi-supervised training approach to word alignment, based on IBM Model 4, that alternates the EM step which is applied on a large training corpus with a discriminative error training step on a small hand-annotated subcorpus. The alignment problem is viewed as a search problem over a log-linear space with features (submodels) coming from the IBM Model 4. In the proposed algorithm, discriminative training controls the contribution of sub-models while an EM-like procedure is used to estimate the sub-model parameters. Unlike previous approaches (Och and Ney, 2003; Fraser and Marcu, 200"
Q17-1035,D07-1006,0,0.0306254,"opose a semi-supervised training approach to word alignment, based on IBM Model 4, that alternates the EM step which is applied on a large training corpus with a discriminative error training step on a small hand-annotated subcorpus. The alignment problem is viewed as a search problem over a log-linear space with features (submodels) coming from the IBM Model 4. In the proposed algorithm, discriminative training controls the contribution of sub-models while an EM-like procedure is used to estimate the sub-model parameters. Unlike previous approaches (Och and Ney, 2003; Fraser and Marcu, 2006; Fraser and Marcu, 2007) that use discriminative methods to tune the weights of generative models, Gao et al. (2010b) proposes a semi-supervised word alignment technique that integrates discriminative and generative methods. They propose to use a discriminative word aligner to produce high precision partial alignments that can serve as constraints for the EM algorithm. The discriminative word aligner uses the generative aligner’s output as features. This feedback loop iteratively improves the quality of both aligners. Niehues and Vogel (2008) propose a discriminative model that directly models the alignment matrix. A"
Q17-1035,N13-1092,0,0.0794403,"Missing"
Q17-1035,W10-1701,0,0.0194278,"the EM step which is applied on a large training corpus with a discriminative error training step on a small hand-annotated subcorpus. The alignment problem is viewed as a search problem over a log-linear space with features (submodels) coming from the IBM Model 4. In the proposed algorithm, discriminative training controls the contribution of sub-models while an EM-like procedure is used to estimate the sub-model parameters. Unlike previous approaches (Och and Ney, 2003; Fraser and Marcu, 2006; Fraser and Marcu, 2007) that use discriminative methods to tune the weights of generative models, Gao et al. (2010b) proposes a semi-supervised word alignment technique that integrates discriminative and generative methods. They propose to use a discriminative word aligner to produce high precision partial alignments that can serve as constraints for the EM algorithm. The discriminative word aligner uses the generative aligner’s output as features. This feedback loop iteratively improves the quality of both aligners. Niehues and Vogel (2008) propose a discriminative model that directly models the alignment matrix. Although the discriminative model provides the flexibility to use manually word-aligned data"
Q17-1035,C10-1040,0,0.0259695,"the EM step which is applied on a large training corpus with a discriminative error training step on a small hand-annotated subcorpus. The alignment problem is viewed as a search problem over a log-linear space with features (submodels) coming from the IBM Model 4. In the proposed algorithm, discriminative training controls the contribution of sub-models while an EM-like procedure is used to estimate the sub-model parameters. Unlike previous approaches (Och and Ney, 2003; Fraser and Marcu, 2006; Fraser and Marcu, 2007) that use discriminative methods to tune the weights of generative models, Gao et al. (2010b) proposes a semi-supervised word alignment technique that integrates discriminative and generative methods. They propose to use a discriminative word aligner to produce high precision partial alignments that can serve as constraints for the EM algorithm. The discriminative word aligner uses the generative aligner’s output as features. This feedback loop iteratively improves the quality of both aligners. Niehues and Vogel (2008) propose a discriminative model that directly models the alignment matrix. Although the discriminative model provides the flexibility to use manually word-aligned data"
Q17-1035,W11-2123,0,0.00918787,".4 Machine Translation Experiment To see whether the improvement in F1-score by our generative model also improves the BLEU score, we aligned the 20K LDC data and 1 million sentences of the HK Hansards data using the augmented model and tested on 919 sentences of MTC part 4 (LDC2006T04). We trained models in each translation direction and then symmetrized the produced alignments using the grow-diag-final heuristic (Och and Ney, 2003). We used Moses (Koehn et al., 2007) with standard features, and tuned the weights with MERT (Och, 2003). An English 5-gram language model is trained using KenLM (Heafield, 2011) on the Gigaword corpus (Parker et al., 2011). We give a comparison between HMM+Type+Gen model, our baseline HMM, GIZA++ HMM and standard GIZA++ (as used by Moses) in Table 10. We report the BLEU scores and TER computed using MultEval (Clark et al., 2011). The generative model improves over GIZA++ HMM by 1.0 BLEU points. It also im510 Discussion Figure 2 shows the performance of baseline HMM and HMM+Type+Gen model for two word alignment examples extracted from the test data, where squares indicate the gold standard alignments. Numbers in the circles show the IDs of the predicted tags by the HM"
Q17-1035,H05-1012,0,0.0435468,". Action Editor: Philipp Koehn. Submission batch: 10/2016; Revision batch: 3/17; Published 11/2017. c 2017 Association for Computational Linguistics. Distributed under a CC-BY 4.0 license. 所以 一定 要 好好 照顾 自己 GIS FUN so you must FUN SEM be sure 。 GIF FUN SEM to take really good care of yourself . Figure 1: An alignment between a Chinese sentence and its translation in English which is enriched with alignment types . SEM (semantic), FUN (function), GIF (grammatically inferred function) and GIS (grammatically inferred semantic) are tags of the links. by designing linguistically-motivated features (Ittycheriah and Roukos, 2005; Blunsom and Cohn, 2006; Deng and Gao, 2007; Berg-Kirkpatrick et al., 2010; Dyer et al., 2011). These models provide evidence that additional constraints can help in modelling word alignments in a log-linear model where word based features can be augmented with morphological, syntactic or semantic features. For example, such a model might learn that function words in one language tend to be aligned to function words in the other language. In this paper, we propose a novel task which is the joint prediction of word alignment and alignment types for a given sentence pair in a parallel corpus. W"
Q17-1035,P07-2045,0,0.00502124,"ions. The generative models significantly outperform their baseline and discriminative counterparts (p-value &lt; 0.0001). 5.3 BLEU 23.4 23.2 23.5 24.4 Machine Translation Experiment To see whether the improvement in F1-score by our generative model also improves the BLEU score, we aligned the 20K LDC data and 1 million sentences of the HK Hansards data using the augmented model and tested on 919 sentences of MTC part 4 (LDC2006T04). We trained models in each translation direction and then symmetrized the produced alignments using the grow-diag-final heuristic (Och and Ney, 2003). We used Moses (Koehn et al., 2007) with standard features, and tuned the weights with MERT (Och, 2003). An English 5-gram language model is trained using KenLM (Heafield, 2011) on the Gigaword corpus (Parker et al., 2011). We give a comparison between HMM+Type+Gen model, our baseline HMM, GIZA++ HMM and standard GIZA++ (as used by Moses) in Table 10. We report the BLEU scores and TER computed using MultEval (Clark et al., 2011). The generative model improves over GIZA++ HMM by 1.0 BLEU points. It also im510 Discussion Figure 2 shows the performance of baseline HMM and HMM+Type+Gen model for two word alignment examples extracte"
Q17-1035,N10-1062,0,0.0248634,"apable of predicting the alignment types and hence are not included in this table. However, it is interesting to compute word alignments using our baselines and then apply the logistic regression classifier on the alignments to get the corresponding alignment types. In Table 5, MODEL→Disc denotes this pipelined version of MODEL. The only difference between MODEL+Type+Disc and MODEL→Disc is in the 3 GIZA++ does not allow the user to run it as a classifier (a model that is trained on the training data and can be tested on new data). Initially, we performed incremental training with inc-giza-pp (Levenberg et al., 2010). Since the performance was very poor, we used GIZA++ in our experiments by appending the test data to the training data (even though our models did not see the test data) and reported the result of Viterbi output from the trained GIZA++ model on the combined data. WA Task: Train(20K+1M) + Test(2K) Model Prec. Rec. F1-score IBM1 49.7 39.6 44.1 IBM1+Type+Disc 50.5 40.2 44.8 IBM1+Type+Gen 59.5 47.4 52.8 HMM 67.7 48.8 56.7 HMM+Type+Disc 66.1 50.7 57.4 HMM+Type+Gen 73.1 58.2 64.8 GIZA++ 60.0 47.0 52.7 WA+Type Task: Train(20K+1M) + Test(2K) Model Prec. Rec. F1-score IBM1+Type+Disc 42.9 36.6 39.5 IB"
Q17-1035,N06-1014,0,0.0539827,"in neural machine translation (NMT) systems. The classic generative model approach to word alignment is based on IBM models 1-5 (Brown et al., 1993) and the HMM model (Vogel et al., 1996; Och and Ney, 2000a). These traditional models use unsupervised algorithms to learn alignments, relying on a large amount of parallel training data without hand annotated alignments. Supervised algorithms for word alignment have become more widespread with the availability of manually annotated word-aligned data and have shown promising results (Taskar et al., 2005; Blunsom and Cohn, 2006; Moore et al., 2006; Liang et al., 2006). Manually word-aligned data are valuable resources for SMT research, but they are costly to create and are only available for a handful of language pairs. Semisupervised methods for word alignment combine Unsupervised or semi-supervised probabilistic word alignment models do not play a central role in neural machine translation (NMT) (Bahdanau et al., 2015; Sutskever et al., 2014; Luong et al., 2015; Chung et al., 2016). However, attention models, which are crucial for high-quality NMT, have been augmented with ideas from statistical word alignment (Luong et al., 2015; Cohn et al., 2016). Oth"
Q17-1035,N15-1028,0,0.020308,"014; Luong et al., 2015; Chung et al., 2016). However, attention models, which are crucial for high-quality NMT, have been augmented with ideas from statistical word alignment (Luong et al., 2015; Cohn et al., 2016). Other than machine translation, word alignments are also important in the best performing models for NLP tasks. They play a central role in learning paraphrases in a source language by doing round-trips from source to target and back using word alignments (Ganitkevitch et al., 2013). Aligments also form the basis for learning multi-lingual word embeddings (Faruqui and Dyer, 2014; Lu et al., 2015) and in the projection of syntactic and semantic annotations from one language to another (Hwa et al., 2005; McDonald et al., 2011). Therefore, there is still a prominent role for word alignment in NLP; research into improvements in word alignment is a worthy goal. Adding additional information such as part-ofspeech tags and syntactic parse information has yielded some improvements in word alignment quality. Toutanova et al. (2002) incorporated the partof-speech (POS) tags of the words in the sentence pair as a constraint on HMM-based word alignment. Additional constraints have also been injec"
Q17-1035,D15-1166,0,0.0358476,"Missing"
Q17-1035,D11-1006,0,0.0267965,"Missing"
Q17-1035,P06-1065,0,0.0357943,"important component in neural machine translation (NMT) systems. The classic generative model approach to word alignment is based on IBM models 1-5 (Brown et al., 1993) and the HMM model (Vogel et al., 1996; Och and Ney, 2000a). These traditional models use unsupervised algorithms to learn alignments, relying on a large amount of parallel training data without hand annotated alignments. Supervised algorithms for word alignment have become more widespread with the availability of manually annotated word-aligned data and have shown promising results (Taskar et al., 2005; Blunsom and Cohn, 2006; Moore et al., 2006; Liang et al., 2006). Manually word-aligned data are valuable resources for SMT research, but they are costly to create and are only available for a handful of language pairs. Semisupervised methods for word alignment combine Unsupervised or semi-supervised probabilistic word alignment models do not play a central role in neural machine translation (NMT) (Bahdanau et al., 2015; Sutskever et al., 2014; Luong et al., 2015; Chung et al., 2016). However, attention models, which are crucial for high-quality NMT, have been augmented with ideas from statistical word alignment (Luong et al., 2015; Co"
Q17-1035,W08-0303,0,0.0246681,"ters. Unlike previous approaches (Och and Ney, 2003; Fraser and Marcu, 2006; Fraser and Marcu, 2007) that use discriminative methods to tune the weights of generative models, Gao et al. (2010b) proposes a semi-supervised word alignment technique that integrates discriminative and generative methods. They propose to use a discriminative word aligner to produce high precision partial alignments that can serve as constraints for the EM algorithm. The discriminative word aligner uses the generative aligner’s output as features. This feedback loop iteratively improves the quality of both aligners. Niehues and Vogel (2008) propose a discriminative model that directly models the alignment matrix. Although the discriminative model provides the flexibility to use manually word-aligned data to tune its weights, it still relies on the model parameters of IBM models and alignment links from GIZA++ as features. Gao et al. (2010a) present a semi-supervised algorithm that extends IBM Model 4 by using partial manual alignments. Partial alignments are fixed and treated as constraints into the EM training. DeNero and Klein (2010) present a supervised model for extracting phrase pairs under a discriminative model by using w"
Q17-1035,C00-2163,0,0.623476,"task. We also solve a sub-task of predicting the alignment type given an aligned word pair. In our experimental results, the generative models we introduce to model alignment types significantly outperform the models without alignment types. 1 Introduction Word alignment is an essential component in a statistical machine translation (SMT) system. Soft alignments, or attention, are also an important component in neural machine translation (NMT) systems. The classic generative model approach to word alignment is based on IBM models 1-5 (Brown et al., 1993) and the HMM model (Vogel et al., 1996; Och and Ney, 2000a). These traditional models use unsupervised algorithms to learn alignments, relying on a large amount of parallel training data without hand annotated alignments. Supervised algorithms for word alignment have become more widespread with the availability of manually annotated word-aligned data and have shown promising results (Taskar et al., 2005; Blunsom and Cohn, 2006; Moore et al., 2006; Liang et al., 2006). Manually word-aligned data are valuable resources for SMT research, but they are costly to create and are only available for a handful of language pairs. Semisupervised methods for wor"
Q17-1035,P00-1056,0,0.872583,"task. We also solve a sub-task of predicting the alignment type given an aligned word pair. In our experimental results, the generative models we introduce to model alignment types significantly outperform the models without alignment types. 1 Introduction Word alignment is an essential component in a statistical machine translation (SMT) system. Soft alignments, or attention, are also an important component in neural machine translation (NMT) systems. The classic generative model approach to word alignment is based on IBM models 1-5 (Brown et al., 1993) and the HMM model (Vogel et al., 1996; Och and Ney, 2000a). These traditional models use unsupervised algorithms to learn alignments, relying on a large amount of parallel training data without hand annotated alignments. Supervised algorithms for word alignment have become more widespread with the availability of manually annotated word-aligned data and have shown promising results (Taskar et al., 2005; Blunsom and Cohn, 2006; Moore et al., 2006; Liang et al., 2006). Manually word-aligned data are valuable resources for SMT research, but they are costly to create and are only available for a handful of language pairs. Semisupervised methods for wor"
Q17-1035,J03-1002,0,0.163539,"tasks: (1) The traditional word alignment task and (2) The joint prediction of word alignment and alignment types task. The second task is harder as the model has to predict both word alignment and alignment types correctly. Moreover, as the baseline IBM Model 1 and the baseline HMM cannot predict the alignment types, we can only make a comparison between our generative and discriminative models for the second task. We initialized the translation probabilities of Model 1 uniformly over the word pairs that occur together in the same sentence pair. We built an HMM similar to the one proposed by Och and Ney (2003). This model is referred to as HMM in this paper. HMM was initialized with uniform transition probabilities and Model 1 translation probabilities. Model 1 was trained for 5 iterations; it is followed by 5 iterations of HMM. To handle unseen data when the model is applied to the test data, smoothing has been used. We smooth translation probability p(f |e) by backingoff to a uniform probability 1/|V |where |V |is the source vocabulary size. For smoothing alignment type probabilities p(h|f, e), we used the following linear interpolation: p∗ (h|f, e) = λ1 p(h|f, e) + λ2 p(h|tf , te ) + λ3 p(h) (22"
Q17-1035,P03-1021,0,0.0108361,"iminative counterparts (p-value &lt; 0.0001). 5.3 BLEU 23.4 23.2 23.5 24.4 Machine Translation Experiment To see whether the improvement in F1-score by our generative model also improves the BLEU score, we aligned the 20K LDC data and 1 million sentences of the HK Hansards data using the augmented model and tested on 919 sentences of MTC part 4 (LDC2006T04). We trained models in each translation direction and then symmetrized the produced alignments using the grow-diag-final heuristic (Och and Ney, 2003). We used Moses (Koehn et al., 2007) with standard features, and tuned the weights with MERT (Och, 2003). An English 5-gram language model is trained using KenLM (Heafield, 2011) on the Gigaword corpus (Parker et al., 2011). We give a comparison between HMM+Type+Gen model, our baseline HMM, GIZA++ HMM and standard GIZA++ (as used by Moses) in Table 10. We report the BLEU scores and TER computed using MultEval (Clark et al., 2011). The generative model improves over GIZA++ HMM by 1.0 BLEU points. It also im510 Discussion Figure 2 shows the performance of baseline HMM and HMM+Type+Gen model for two word alignment examples extracted from the test data, where squares indicate the gold standard align"
Q17-1035,H05-1010,0,0.127422,"Missing"
Q17-1035,W02-1012,0,0.195562,"Missing"
Q17-1035,N03-1033,0,0.060701,"lignment Type Prediction Given Alignments For the alignment type prediction task given an aligned word pair, we have examined three simple maximum likelihood classifiers, as well as the logistic regression classifier with the features shown in Table 2. We have trained all these classifiers on the parallel Chinese-English 20K LDC data which is annotated with gold alignment and alignment types. To obtain the word pairs, we have extracted the word pairs from the parallel sentences with the gold alignment. To get the part-of-speech tags, we annotated the 20K LDC data with the Stanford POS tagger (Toutanova et al., 2003). We ignored the gold alignment if the Chinese side of the gold alignment is not contiguous; i.e., it cannot form one Chinese word. This usually happens in the many-to-one 1 Catalog numbers: LDC2012T16, LDC2012T20, LDC2012T24, LDC2013T05, LDC2013T23 and LDC2014T25. 2 All our codes for the baselines and the proposed models are available at https://github.com/sfu-natlang/ align-type-tacl2017-code. Model ML word-based ML tag-based ML word-tag Logistic regression and many-to-many alignments. There were only a small number of these discontiguous alignments as mentioned in the LDC catalog entry for"
Q17-1035,C96-2141,0,0.865585,"algorithms for this task. We also solve a sub-task of predicting the alignment type given an aligned word pair. In our experimental results, the generative models we introduce to model alignment types significantly outperform the models without alignment types. 1 Introduction Word alignment is an essential component in a statistical machine translation (SMT) system. Soft alignments, or attention, are also an important component in neural machine translation (NMT) systems. The classic generative model approach to word alignment is based on IBM models 1-5 (Brown et al., 1993) and the HMM model (Vogel et al., 1996; Och and Ney, 2000a). These traditional models use unsupervised algorithms to learn alignments, relying on a large amount of parallel training data without hand annotated alignments. Supervised algorithms for word alignment have become more widespread with the availability of manually annotated word-aligned data and have shown promising results (Taskar et al., 2005; Blunsom and Cohn, 2006; Moore et al., 2006; Liang et al., 2006). Manually word-aligned data are valuable resources for SMT research, but they are costly to create and are only available for a handful of language pairs. Semisupervi"
Q17-1035,D14-1173,0,0.0164863,"e. TIN (Translated Incorrectly) and NTR (Not translated) types are designed to handle the various errors that occur in the translation process, such as incorrect translation and no translation. MTA (Meta word) was designed to handle special characters that usually appear in the context of web pages. Sub-categorizing different types of word alignments is likely to result in better word alignments. The alignment types provided by the LDC as annotations on each word alignment link have never been used (as far as we are aware) in order to improve word alignment. A subset of this data was used in (Wang et al., 2014) to refine word segmentation for machine translation but they ignore the alignment link types in their experiments. 3 follows: J Y 1 P r(f, a|e) = p(fj |eaj ) (I + 1)J In the Hidden Markov alignment model, we assume a first order dependence for the alignments aj . The HMM-based model has the following form: P r(f, a|e) = j=1 X p(aj |aj−1 , I) · p(fj |eaj ) p(i|i0 , I) = PI c(i − i0 ) i00 =1 c(i 00 − i0 ) 0 p(i + I|i + I, I) = p0 · δ(i, i ) P r(f, a|e) (1) In IBM Model 1, the alignment model is decomposed into the product of translation probabilities as 503 (4) where at each EM iteration c(i −"
Q17-1035,C00-2137,0,0.0401621,"int prediction of word alignment and alignment types task, and (2) word alignment models followed by the discriminative classifier to predict alignment types. proves over the standard GIZA++ by 1.2 BLEU points. HMM+Type+Gen significantly outperforms GIZA++ HMM (p-value=0.00036) and GIZA++ IBM4 (p-value=0.0004) evaluated by MultEval. 6 The results for the joint prediction task are shown in Table 9. This confirms our success in improving the performance of all the methods, compared to the results in Table 5. Statistical significance tests were performed using the approximate randomization test (Yeh, 2000) with 10,000 iterations. The generative models significantly outperform their baseline and discriminative counterparts (p-value &lt; 0.0001). 5.3 BLEU 23.4 23.2 23.5 24.4 Machine Translation Experiment To see whether the improvement in F1-score by our generative model also improves the BLEU score, we aligned the 20K LDC data and 1 million sentences of the HK Hansards data using the augmented model and tested on 919 sentences of MTC part 4 (LDC2006T04). We trained models in each translation direction and then symmetrized the produced alignments using the grow-diag-final heuristic (Och and Ney, 200"
W00-1605,E99-1025,0,0.298469,"Missing"
W00-1605,J93-2004,0,0.0251099,"Missing"
W00-1605,W00-2027,1,0.634896,"ation forest for each sentence which stores, in compact form, all derivations for each sentence. 2 1 0 2 4 6 8 10 12 Sentence length 14 16 18 20 Figure 3: Parse times plotted against sentence length. Coefficient of determination: . (x-axis: Sentence length; y-axis: log(time in seconds)) Since we can easily determine the number of trees selected by a sentence before we start parsing, we can use this number to predict the number of edges that will be proposed by a parser when parsing this sentence, allowing us to better handle difficult cases before parsing. 1 2 Some of these results appear in (Sarkar, 2000). In this section we present some additional data on the previous results and also the results of some new experiments that do not appear in the earlier work. Note that the precise number of edges proposed by the parser and other common indicators of complexity can be obtained only while or after parsing. We are interested in predicting parsing complexity. 39 10 the parser was reduced: 926 sentences (out of the 2250) did not get any parse. This was because some crucial tree was missing in the -best output. The results are graphed in Figure 6. The total number of derivations for all sentences w"
W00-1605,1997.iwpt-1.22,0,0.0335395,"ture for each word in the sentence. This eliminates lexical syntactic ambiguity but does not eliminate attachment ambiguity for the parser. The graph comparing the parsing times is shown in Figure 5. As the comparison shows, the elimination of lexical ambiguity leads to a drastic increase in parsing efficiency. The total time taken to parse all sentences went from 548K seconds to 31.2 seconds. Figure 5 shows us that a model which disambiguates syntactic lexical ambiguity can potentially be extremely useful in terms of parsing efficiency. Thus disambiguation of tree assignment or SuperTagging (Srinivas, 1997) of a sentence before parsing it might be a way of improving parsing efficiency. This gives us a way to reduce the parsing complexity for precisely the sentences which were problematic: the ones which selected too many trees. To test whether parsing times are reduced after SuperTagging we conducted an experiment in which the output of an -best SuperTagger was taken as input to the parser. In our experiment we set to be .3 The time taken to parse the same set of sentences was again dramatically reduced (the total time taken was 21K seconds). However, the disadvantage of this method was that the"
W00-1605,A00-2022,0,\N,Missing
W00-2027,E91-1006,0,0.0799951,"Missing"
W00-2027,W89-0206,0,\N,Missing
W02-2207,1997.iwpt-1.6,0,0.0115576,"In our parser, we allow multiple adjunctions at a node and also we exploit TIG style probabilities and . This was done so that the output easily convertible to the earlier dependency style parser that was used in the project (with which we compare performance in our evaluation). There are many other probability measures that can be used with TAG and its variants. One can easily go beyond the bi-lexical probabilities that have been the main focus in this chapter to probabilities that invoke greater amounts of structural or lexical context. (Carroll and Weir, 1997), for example, gives some additional probability models one might consider useful when using TAGs. An example output from the statistical parser is shown in Figure 6. In the parser (and in the Lextract output), each elementary tree is anchored by exactly one lexical item. The gloss and translation for the example in Figure 6 is given in the following example: (4) Motun hochwul tayho-nun mayil 24 si-ey pakkwui-key every call sign everyday 24 hour-at switch-AuxConnect toy-ciyo. be-Decl ‘Every call sign is switched at midnight everyday.’ Index Word 0 1 2 3 4 5 6 7 8       + 24  +  +"
W02-2207,P00-1058,0,0.0644114,"Missing"
W02-2207,han-etal-2000-handling,1,0.88394,"Missing"
W02-2207,J93-2004,0,0.0262084,"Missing"
W02-2207,C92-2065,0,0.0349575,"r derivation tree for each sentence. The statistical parsing model is then trained using these derivation trees. 4.1. Probability Models The statistical parser uses three probabilistic models: one model for picking a tree as the start of a derivation; and two models for the probability of one tree substituting or adjoining into another tree. Each of these models can be trained directly using maximum likelihood estimation from the Lextract output. The probabilistic models of substitution and adjunction provide a natural domain to describe the dependencies between pairs of words in a sentence. (Resnik, 1992) provided some early motivation for a stochastic version of Tree Adjoining Grammars and gave a formal definition of stochastic TAG. Simultaneously, (Schabes, 1992) also provided an identical stochastic version of TAG and also extended the Inside-Outside algorithm for CFGs (Lari and Young, 1990) to stochastic TAGs. (Schabes, 1992) also performed experiments to show that a stochastic TAG can be learnt from the ATIS corpus. A stochastic LTAG derivation proceeds as follows (Schabes, 1992; Resnik, 1992). An initial tree is selected   and adjunctions are performed with probability and subsequent"
W02-2207,C92-2066,0,0.0227933,"es three probabilistic models: one model for picking a tree as the start of a derivation; and two models for the probability of one tree substituting or adjoining into another tree. Each of these models can be trained directly using maximum likelihood estimation from the Lextract output. The probabilistic models of substitution and adjunction provide a natural domain to describe the dependencies between pairs of words in a sentence. (Resnik, 1992) provided some early motivation for a stochastic version of Tree Adjoining Grammars and gave a formal definition of stochastic TAG. Simultaneously, (Schabes, 1992) also provided an identical stochastic version of TAG and also extended the Inside-Outside algorithm for CFGs (Lari and Young, 1990) to stochastic TAGs. (Schabes, 1992) also performed experiments to show that a stochastic TAG can be learnt from the ATIS corpus. A stochastic LTAG derivation proceeds as follows (Schabes, 1992; Resnik, 1992). An initial tree is selected   and adjunctions are performed with probability and subsequent substitutions are performed with probability  with probability . For each  that can be valid start of a derivation:     Each subsequent substitution or"
W02-2207,J94-1004,0,0.0229683,"max  BEST   *+ ,- arg max *+     *+  arg max  *+     (3) The particular definition of a stochastic TAG is by no means the only way of defining a probabilistic grammar formalism with TAG. There have been some variants from the standard model that have been published since the original stochastic TAG papers. 54 Proceedings of TAG+6 For example, the restriction of one adjunction per node could be dropped and a new variant of standard TAG can be defined which permits arbitrary number of modifications per node. This variant was first introduced by (Schabes and Shieber, 1992; Schabes and Shieber, 1994). Tree Insertion Grammar (Schabes and Waters, 1995) is a variant of TAG where the adjoining operation is restricted in a certain way and this restricted operation is named insertion. TIGs are weakly equivalent to CFGs but they can produce structural descriptions that are not obtainable by any CFG. A stochastic version of insertion (Schabes and Waters, 1996) was defined in the context of Tree Insertion Grammar (TIG). In this model, multiple trees can be adjoined to the left and to the right of each node with the following probabilities:      NA    NA"
W02-2207,J95-4002,0,0.0154181,"arg max  *+     (3) The particular definition of a stochastic TAG is by no means the only way of defining a probabilistic grammar formalism with TAG. There have been some variants from the standard model that have been published since the original stochastic TAG papers. 54 Proceedings of TAG+6 For example, the restriction of one adjunction per node could be dropped and a new variant of standard TAG can be defined which permits arbitrary number of modifications per node. This variant was first introduced by (Schabes and Shieber, 1992; Schabes and Shieber, 1994). Tree Insertion Grammar (Schabes and Waters, 1995) is a variant of TAG where the adjoining operation is restricted in a certain way and this restricted operation is named insertion. TIGs are weakly equivalent to CFGs but they can produce structural descriptions that are not obtainable by any CFG. A stochastic version of insertion (Schabes and Waters, 1996) was defined in the context of Tree Insertion Grammar (TIG). In this model, multiple trees can be adjoined to the left and to the right of each node with the following probabilities:      NA    NA                      In our parser, we allow"
W02-2207,W00-1307,0,0.114334,"ological tagger/analyzer that we use in the parser. A detailed discussion about the parser is presented in section 4. This section also presents the method we used to combine the morphological information into the statistical LTAG parser. We also provide the experimental evaluation of the statistical parser on unseen test data in section 4. 2. Automatically Extracted LTAG Grammar for Korean In this section we describe the Penn Korean TreeBank and the nature of the extracted LTAG grammar from this TreeBank. 2.1. Korean TreeBank The LTAG grammar we use in the parser is extracted using LexTract (Xia et al., 2000) from the Penn Korean TreeBank. The derivation trees obtained by using LexTract on the Treebank are used to train the statistical parser. The TreeBank has 54,366 words and 5,078 sentences. The annotation consists of a phrase structure analysis for each sentence, with head/phrase level tags as well as function tags (e.g., -SBJ, -OBJ) and empty category tags for traces (*T*) and dropped arguments (*pro*). Each word is morphologically analyzed, where the lemma and the inflections are identified. The lemma is tagged with a part-of-speech (POS) tag (e.g., NNC: noun, NPN: pronoun, VV: verb, VX: auxi"
W02-2207,1997.iwpt-1.25,0,0.122631,"Missing"
W02-2207,W00-1208,1,\N,Missing
W03-1012,P93-1005,0,0.0143757,"s, thus making these constraints strictly local. For example, in the derivation tree of Examples 1, α1 (join) and α2 (V inken) are directly connected whether there is an auxiliary tree β 2 (will) or not. We will show how this property affects our redefined tree kernel later in this paper. In our experiments in this paper, we only use LTAG grammars where each elementary tree is lexicalized by exactly one word (terminal symbol) on the frontier. 3 Parse Reranking In recent years, reranking techniques have been successfully used in statistical parsers to rerank the output of history-based models (Black et al., 1993). In this paper, we will use the LTAG based features to improve the performance of reranking. Our motivations for using LTAG based features for reranking are the following: • Unlike the generative model, it is trivial to incorporate features of various kinds in a reranking setting. Furthermore the nature of reranking makes it possible to use global features, VP α1 (join)hi α2 (Vinken)h00i β2 (will)h01i α3 (board)h011i β1 (Pierre)h0i β3 (the)h0i will β4 (as)h01i .. . α4 (director)h011i • Several hand-crafted and arbitrary features have been exploited in the statistical parsing task, especially"
W03-1012,E03-1005,0,0.00617033,"nel over sequences. However, to use all possible n-gram features typically introduces too many noisy features, which can result in lower accuracy. One way to solve this problem is to use a kernel function that is tailored for particular NLP applications, such as the tree kernel (Collins and Duffy, 2001) for statistical parsing. In addition to n-gram features, more complex high-level features are often exploited to obtain higher accuracy, especially when discriminative models are used for statistical parsing. For example, all possible sub-trees can be used as features (Collins and Duffy, 2002; Bod, 2003). However, most of the sub-trees are linguistically meaningless, and are a source of noisy features thus limiting efficiency and accuracy. An alternative to the use of arbitrary sets of sub-trees is to use the set of elementary trees as defined in Lexicalized Tree Adjoining Grammar (LTAG) (Joshi and Schabes, 1997). LTAG based features not only allow a more limited and a linguistically more valid set of features over sub-trees, they also provide the use of features that use discontinuous sub-trees which are outside the scope of previous tree kernel definitions using arbitrary sub-trees. In this"
W03-1012,2000.iwpt-1.9,0,0.0585427,"s are excised and recorded into the derivation tree as cases of sister adjunction. Each sub-tree excised is recursively analyzed with this method, split up into elementary trees and then recorded into the derivation tree. The output of our algorithm for the input parse tree in Fig. 6 is shown in Fig. 2 and Fig. 3. Our algorithm is similar to the derivation tree extraction explained in (Chiang, 2000), except we extract our LTAG from n-best sets of parse trees, while in (Chiang, 2000) the LTAG is extracted from the Penn Treebank.3 For other techniques for LTAG grammar extraction see (Xia, 2001; Chen and Vijay-Shanker, 2000). 4.3 Using Derivation Trees In this paper, we have described two models to employ derivation trees. Model 1 uses tree kernels on derivation trees. In order to make the tree kernel more lexicalized, we extend the original definition of the tree kernel, which we will describe below. Model 2 abstracts features from derivation trees and uses them with a linear kernel. In Model 1, we combine the SVM results of the tree kernel on derivation trees with the SVM results given by a linear kernel based on features on the derived trees. 3 Also note that the path from the root node to the foot node in aux"
W03-1012,P00-1058,0,0.168692,"alized Tree Adjoining Grammar (more details can be found in (Joshi and Schabes, 1997)). In LTAG, each word is associated with a set of elementary trees. Each elementary tree represents a possible tree structure for the word. There are two kinds of elementary trees, initial trees and auxiliary trees. Elementary trees can be combined through two operations, substitution and adjunction. Substitution is used to attach an initial tree, and adjunction is used to attach an auxiliary tree. In addition to adjunction, we also use sister adjunction as defined in the LTAG statistical parser described in (Chiang, 2000). 1 The tree resulting from the combination of elementary trees is is called a derived tree. The tree that records the history of how a derived tree is built from the elementary trees is called a derivation tree. 2 We illustrate the LTAG formalism using an example. Example 1: Pierre Vinken will join the board as a non-executive director. The derived tree for Example 1 is shown in Fig. 1 (we omit the POS tags associated with each word to save space), and Fig. 2 shows the elementary trees for each word in the sentence. Fig. 3 is the derivation tree (the history of tree combinations). One of 1 Ad"
W03-1012,P02-1034,0,0.611474,"e use of a polynomial kernel over sequences. However, to use all possible n-gram features typically introduces too many noisy features, which can result in lower accuracy. One way to solve this problem is to use a kernel function that is tailored for particular NLP applications, such as the tree kernel (Collins and Duffy, 2001) for statistical parsing. In addition to n-gram features, more complex high-level features are often exploited to obtain higher accuracy, especially when discriminative models are used for statistical parsing. For example, all possible sub-trees can be used as features (Collins and Duffy, 2002; Bod, 2003). However, most of the sub-trees are linguistically meaningless, and are a source of noisy features thus limiting efficiency and accuracy. An alternative to the use of arbitrary sets of sub-trees is to use the set of elementary trees as defined in Lexicalized Tree Adjoining Grammar (LTAG) (Joshi and Schabes, 1997). LTAG based features not only allow a more limited and a linguistically more valid set of features over sub-trees, they also provide the use of features that use discontinuous sub-trees which are outside the scope of previous tree kernel definitions using arbitrary sub-tr"
W03-1012,W01-1802,0,0.0833267,"Missing"
W03-1012,W03-0402,1,0.84097,"is to use the set of elementary trees as defined in Lexicalized Tree Adjoining Grammar (LTAG) (Joshi and Schabes, 1997). LTAG based features not only allow a more limited and a linguistically more valid set of features over sub-trees, they also provide the use of features that use discontinuous sub-trees which are outside the scope of previous tree kernel definitions using arbitrary sub-trees. In this paper, we use the LTAG based features in the parse reranking problem (Collins, 2000; Collins and Duffy, 2002). We use the Support Vector Machine (SVM) (Vapnik, 1999) based algorithm proposed in (Shen and Joshi, 2003) as the reranker in this paper. We apply the tree kernel to derivation trees of LTAG, and extract features from derivation trees. Both the tree kernel and the linear kernel on the richer feature set are used. Our experiments show that the use of tree kernel on derivation trees makes the notion of a tree kernel more powerful and more applicable. S NP VP Pierre Vinken will VP VP join PP NP as the board a non-executive director Figure 1: Derived tree (parse tree) for Example 1. β1 : NP Pierre α2 : NP Vinken α1 : S β2 : VP will VP∗ NP↓ 2 Lexicalized Tree Adjoining Grammar In this section, we give"
W03-1012,J03-4003,0,\N,Missing
W06-0118,C92-1019,0,0.0928601,"Participating in the third SIGHAN Chinese Word Segmentation Bakeoff in 2006, our system is tested on the closed track of CityU, MSRA and UPUC corpora. The sections below provide a detailed description of the system and our experimental results. 2 Forward Maximum Post−processing Result Figure 1: Outline of the segmentation process 2.1 Forward Maximum Matching The maximum matching algorithm is a greedy segmentation approach. It proceeds through the sentence, mapping the longest word at each point with an entry in the dictionary. In our system, the well-known forward maximum matching algorithm (Chen and Liu, 1992) is implemented. System Description In our segmentation system, a hybrid strategy is applied (Figure 1): First, forward maximum matching (Chen and Liu, 1992), which is a dictionary-based method, is used to generate a segmentation result. Also, the CRF model using maximum subword-based tagging (Zhang et al., 2006) and the CRF model using minimum subword-based tagging, both of which are statistical methods, are used individually to solve the The maximum matching approach is simple and efficient, and it results in high in-vocabulary accuracy; However, the small size of the dictionary, which is ob"
W06-0118,2005.iwslt-1.18,0,0.0660163,"Missing"
W06-0118,I05-3028,0,0.0611784,"Missing"
W06-0118,N06-2049,0,0.0556006,"Missing"
W06-1518,W03-1006,0,0.413583,"n this paper we use the CoNLL 2005 shared SRL task data (Carreras and M`arquez, 2005) which provides a standard train/test split, a standard method for training and testing on various problematic cases involving coordination. However, in some cases, the CoNLL 2005 data is not ideal for the use of LTAG-based features as some “deep” information cannot be recovered due to the fact that trace information and other empty categories like PRO are removed entirely from the training data. As a result some of the features that undo longdistance movement via trace information in the TreeBank as used in (Chen and Rambow, 2003) cannot be exploited in our model. Our results are shown in Table 1. Note that we test on the gold standard parse trees because we want to compare a model using features from the derived parse trees to the model using the LTAG derivation trees. 5 Related Work In the community of SRL researchers (cf. (Gildea and Jurafsky, 2002; Punyakanok, Roth and Yih, 2005; Pradhan et al, 2005; Toutanova et al., 2005)), the focus has been on two different aspects of the SRL task: (a) finding appropriate features, and (b) resolving the parsing accuracy problem by combining multiple parsers/predictions. Systems"
W06-1518,J02-3001,0,0.816839,"re this model with a model that converts the syntactic parse tree into a Lexicalized Tree-Adjoining Grammar (LTAG) derivation tree and uses features extracted from the elementary trees and the LTAG derivation tree. In each model the features of that model are used in a discriminative model for semantic role labeling. The model is a simple decision list 127 Proceedings of the 8th International Workshop on Tree Adjoining Grammar and Related Formalisms, pages 127–132, c Sydney, July 2006. 2006 Association for Computational Linguistics feature functions that are based on the syntactic parse tree (Gildea and Jurafsky, 2002), we explore the use of LTAG-based features in a simple discriminative decision-list learner. 3 root node in an LTAG derivation tree is a spinal elementary tree and the derivation tree provides the path from the predicate to the constituent in question. Figure 2 shows the resulting elementary tree after decomposition of the pruned tree. For each of the elementary trees we consider their labeling in the derivation tree to be their semantic role labels from the training data. Figure 3 is the derivation tree for the entire pruned tree. Note that the LTAG-based decomposition of the parse tree allo"
W06-1518,W05-0625,0,0.139441,"Missing"
W06-1518,W04-3212,0,\N,Missing
W06-1518,P05-1073,0,\N,Missing
W06-1518,W05-0634,0,\N,Missing
W06-1518,J05-1004,0,\N,Missing
W07-0104,J98-1002,0,0.563572,"strictions for the literal usage of verbs and the non-standard set which we assume will identify the nonliteral usage. Our identification model for literal vs. nonliteral usage of verbs is described in detail in a previous publication (Birke & Sarkar, 2006). Here we provide a brief description of the model so that the use of this model in our proposed active learning approach can be explained. Since we are attempting to reduce the problem of literal/nonliteral recognition to one of word-sense disambiguation, we use an existing similarity-based word-sense disambiguation algorithm developed by (Karov & Edelman, 1998), henceforth KE. The KE algorithm is based on the principle of attraction: similarities are calculated between sentences containing the word we wish to disambiguate (the target word) and collections of seed sentences (feedback sets). It requires a target set – the set of sentences containing the verbs to be classified into literal or nonliteral – and the seed sets: the literal feedback set and the nonliteral feedback set. A target set sentence is considered to be attracted to the feedback set containing the sentence to which it shows the highest similarity. Two sentences are similar if they co"
W07-0104,J04-1002,0,0.115515,", 1997; Martin, 1990; Martin, 1992) – must be largely hand-coded and generally work well on an enumerable set of metaphors or in limited domains. Dictionary-based systems use existing machine-readable dictionaries and path lengths between words as one of their primary sources for metaphor processing information (e.g. (Dolan, 1995)). Corpus-based systems primarily extract or learn the necessary metaphor-processing information from large corpora, thus avoiding the need for manual annotation or metaphor-map construction. Examples of such systems are (Murata et. al., 2000; Nissim & Markert, 2003; Mason, 2004). Nissim & Markert (2003) approach metonymy resolution with machine learning methods, “which [exploit] the similarity between examples of conventional metonymy” ((Nissim & Markert, 2003), p. 56). They see metonymy resolution as a classification problem between the literal use of a word and a number of pre-defined metonymy types. They use similarities between possibly metonymic words (PMWs) and known metonymies as well as context similarities to classify the PMWs. Mason (2004) presents CorMet, “a corpus-based system for discovering metaphorical mappings between concepts” ((Mason, 2004), p. 23)."
W07-0104,P03-1008,0,0.0609346,"etaphor maps (e.g. (Fass, 1997; Martin, 1990; Martin, 1992) – must be largely hand-coded and generally work well on an enumerable set of metaphors or in limited domains. Dictionary-based systems use existing machine-readable dictionaries and path lengths between words as one of their primary sources for metaphor processing information (e.g. (Dolan, 1995)). Corpus-based systems primarily extract or learn the necessary metaphor-processing information from large corpora, thus avoiding the need for manual annotation or metaphor-map construction. Examples of such systems are (Murata et. al., 2000; Nissim & Markert, 2003; Mason, 2004). Nissim & Markert (2003) approach metonymy resolution with machine learning methods, “which [exploit] the similarity between examples of conventional metonymy” ((Nissim & Markert, 2003), p. 56). They see metonymy resolution as a classification problem between the literal use of a word and a number of pre-defined metonymy types. They use similarities between possibly metonymic words (PMWs) and known metonymies as well as context similarities to classify the PMWs. Mason (2004) presents CorMet, “a corpus-based system for discovering metaphorical mappings between concepts” ((Mason,"
W07-0104,J76-2003,0,0.805391,"methods obtain when used in bootstrapping methods (e.g. (Fujii et. al., 1998)). Other machine learning approaches that derive from optimal experiment design are not appropriate in our case because we do not yet have a strong predictive (or generative) model of the literal/nonliteral distinction. Our machine learning model only does identification of verb usage as literal or nonliteral but it can be seen as a first step towards the use of machine learning for more sophisticated metaphor and metonymy processing tasks on larger text corpora. Rule-based systems – some using a type of interlingua (Russell, 1976); others using complicated networks and hierarchies often referred to as metaphor maps (e.g. (Fass, 1997; Martin, 1990; Martin, 1992) – must be largely hand-coded and generally work well on an enumerable set of metaphors or in limited domains. Dictionary-based systems use existing machine-readable dictionaries and path lengths between words as one of their primary sources for metaphor processing information (e.g. (Dolan, 1995)). Corpus-based systems primarily extract or learn the necessary metaphor-processing information from large corpora, thus avoiding the need for manual annotation or metap"
W07-0104,J99-2004,0,\N,Missing
W07-0104,J98-4002,0,\N,Missing
W07-0104,W96-0213,0,\N,Missing
W07-0104,P96-1042,0,\N,Missing
W09-2601,D08-1052,0,0.0466648,"Missing"
W09-2601,boxwell-white-2008-projecting,0,0.0346175,"Missing"
W09-2601,P03-1002,0,0.0856666,"Missing"
W09-2601,W03-1006,0,0.0590579,"Missing"
W09-2601,Y01-1001,0,0.088889,"Missing"
W09-2601,W04-3212,0,0.0221908,"2. 2 XP-NNP is a normalized e-tree form used in (Shen et al., 2008) for efficiency and to avoid the problem of sparse data over too many e-trees. 3 it can either be predicate e-tree or argument e-tree. For example, for pattern P←A, the A(rgument) e-tree is the host spine. 5 NEG}, c = 0.1, j = 2 for {A1, A2, A4, AMEXT} and c = 0.1 and j = 4 for the rest. For comparison, we also built up a standard 3stage phrase-structure based SRL system, where exactly the same data set5 is used from 2004 February release of the Propbank. SVM-light with linear kernel is used to train on a standard feature set (Xue and Palmer, 2004). The Charniak and Johnson parser (2006) is used to produce the automatic parses. Note that this phrase-structure based SRL system is state-of-the-art and we have included all the features proposed in the literature that use phrase-structure trees. This system obtains a higher SRL accuracy which can be improved only by using global inference and other ways (such as using multiple parsers) to improve the accuracy on automatic parses. position and order. Features from intermediate predicate e-tree same features as predicate e-tree features. Features from spine node of intermediate predicate e-tr"
W09-2601,W03-1008,0,0.509287,"Missing"
W09-2601,J02-3001,0,0.311097,"Missing"
W09-2601,P02-1031,0,0.296415,"Missing"
W09-2601,C04-1186,0,0.6156,"Missing"
W09-2601,D07-1062,1,0.817282,"Missing"
W09-2601,J05-1004,0,0.139751,"tic role played by the different child nodes. So far, we can see that in contrast with traditional LTAG where arguments refer to obligatory constituents only, subcategorization frames and argument-adjunct distinction are underspecified in LTAG-spinal. Since argument-adjunct disambiguation is one of the major challenges faced by LTAG treebank construction, LTAG-spinal works around this issue by leaving the disambiguation task for further deep processing, such as semantic role labeling. LTAG-spinal is weakly equivalent to traditional LTAG with adjunction constraints1 (Shen, 2006). The Propbank (Palmer et al., 2005) is an annotated corpus of verb subcategorization and alternations which was created by adding a layer of predicate-argument annotation over the phrase structure trees in the Penn Treebank. The LTAGspinal Treebank is extracted from the Penn Treebank by exploiting Propbank annotation. Specifically, as described in (Shen et al., 2008), a Penn Treebank syntax tree is taken as an LTAG-spinal derived tree; then information from the Penn Treebank and Propbank is merged using tree transformations. For instance, LTAG predicate coordination and instances of adjunction are recognized using Propbank anno"
W09-2601,N04-1030,0,0.0636838,"Missing"
W09-2601,P05-1072,0,0.357053,"Missing"
W09-2601,J94-1004,0,0.104403,"ee: a spinal initial tree is composed of a lexical spine from the root to the anchor, and nothing else; a spinal auxiliary tree is composed of a lexical spine and a recursive spine from the root to the foot node. For example, in Figure 1 (from (Shen et al., 2008)), the lexical spine for the auxiliary tree is B1 , .., Bi , .., Bn , the recursive spine is B1 , .., Bi , .., B1∗ . Two operations attachment and adjunction are defined in LTAG-spinal where adjunction is the same as adjunction in the traditional LTAG; attachment stems from sister adjunction as defined in Tree Insertion Grammar (TIG) (Schabes and Shieber, 1994), which corresponds to the case where the root of an initial tree is taken as a child of another spinal e-tree. The two operations are applied to LTAG-spinal e-tree pairs resulting in an LTAG derivation tree which is similar to a dependency tree (see Figure 2). In Figure 2, e-tree anchored with continue is the only auxiliary tree; all other e-trees are initial trees. The arrow is directed from parent to child, with the type of operation labeled on the arc. The operation types are: att denotes attachment operation; adj denotes adjunction operation. The sibling nodes may have differ1 null adjunc"
W09-2601,H05-1102,0,0.109974,"ample of SRL annotation from the PropBank corpus (Palmer et al., 2005), where the subscripted information maps the semantic roles A0, A1 and A2 to arguments for the predicate sell as defined in the PropBank Frame Scheme. The availability of annotated corpora like PropBank and FrameNet (Fillmore et al., 2001) have provided rapid development of research into SRL (Gildea and Jurafsky, 2002; Gildea and Palmer, 2002; Surdeanu et al., 2003; Chen and Rambow, 2003; Gildea and Hockenmaier, 2003; The LTAG-spinal formalism was initially proposed for automatic treebank extraction and statistical parsing (Shen and Joshi, 2005). However, its Propbank-guided treebank extraction process further strengthens the connection between the LTAG-spinal and semantic role labeling. In this paper, we present an SRL system that was built to 1 Proceedings of the 2009 Workshop on Grammar Engineering Across Frameworks, ACL-IJCNLP 2009, pages 1–9, c Suntec, Singapore, 6 August 2009. 2009 ACL and AFNLP explore the utility of this new formalism, its Treebank and the output of its statistical parser. Experiments show that our LTAG-spinal based SRL system achieves very high precision on both goldstandard and automatic parses, and signifi"
W09-2601,W08-2121,0,\N,Missing
W09-2601,W05-0620,0,\N,Missing
W10-1733,W02-1020,0,0.0366282,"two cases. Clearly, this is because of the delay in pruning the tortoises until the race course limit. Even with such significantly large number of hypotheses being retained for every bin, DP results in improved speed (over re-decoding from scratch) and better performance by avoiding search errors (compared to the incremental decoder that does not use DP). 4.3 knowledge. They may be simply re-translating the input each time using a fast decoder or re-using prior decoder states as we do here. Intereactive translation using text prediction strategies have been studied well (Foster et al., 1997; Foster et al., 2002; Och et al., 2003). They all attempt to interactively help the human user in the postediting process, by suggesting completion of the word/phrase based on the user accepted prefix and the source sentece. Incremental feedback is part of Caitra (Koehn, 2009) an interactive tool for human-aided MT and works on a similar setting to interactive MT. In Caitra, the source text is pre-translated first and during the interactions it dynamically generates user suggestions. Our incremental decoder work differs from these text prediction based approaches, in the sense that the input text is not available"
W10-1733,P03-1021,0,0.0487581,"es of the top-hypotheses in each hyper-edge that is not represented in cube pruning and based on a threshold (which is obtained using a development test set), we selectively choose few hyper-edge bundles and generate a small number (typically 1-3) of hypotheses from each of them and flag them as tortoises. 4 Evaluation and Discussion The evaluation was performed using our own implementation of the beam-search decoding algorithms. The architecture of our system is similar to Moses, which we also use for training and for minimum error rate training (MERT) of the loglinear model for translation (Och, 2003; Koehn et al., 2007). Our features include 7 standard phrasebased features: 4 translation model features, i.e. p(f |e), p(e|f ), plex (f |e) and plex (e|f ), where e and f are target and source phrases respectively; features for phrase penalty, word penalty and language model, and we do not include the reordering feature. We used Giza++ and Moses respectively for aligning the sentences and training the system. The decoder was written in Java and includes cube pruning (Huang and Chiang, 2007) and lazier cube pruning (Pust and Knight, 2009) functionalities as part of the decoder. Our decoder su"
W10-1733,P07-1019,0,0.0414299,"s, which we also use for training and for minimum error rate training (MERT) of the loglinear model for translation (Och, 2003; Koehn et al., 2007). Our features include 7 standard phrasebased features: 4 translation model features, i.e. p(f |e), p(e|f ), plex (f |e) and plex (e|f ), where e and f are target and source phrases respectively; features for phrase penalty, word penalty and language model, and we do not include the reordering feature. We used Giza++ and Moses respectively for aligning the sentences and training the system. The decoder was written in Java and includes cube pruning (Huang and Chiang, 2007) and lazier cube pruning (Pust and Knight, 2009) functionalities as part of the decoder. Our decoder supports both regular beam search (similar to Moses) and incremental decoding. In our experiments we experimented various approaches for storing partial decoder states including memcache and transactional persistence using JDBM but found that the serialization and deserialization of decoder objects directly into and from the memory to work better in terms of speed and memory requirements. The partial object is retrieved and deserialized from the memory when required by the incremental decoder."
W10-1733,P02-1040,0,0.0961715,"T. To benchmark our Java decoder, we compare it with Moses by running it in regular beam search mode. The Moses systems were also optimized separately on the WMT07 devsets. Apart from comparing our decoder with Moses in regular beam search, we also compared the incremental decoding with regular regular beam using our decoder. To make it comparable with incremental decoding, we used the regular beam search to re-decode the sentence fragments for every additional word in the input sentence. We measured the following parameters in our empirical analysis: translation quality (as measured by BLEU (Papineni et al., 2002) and TER (Snover et al., 2006)), search errors and translation speed. Finally, we also measured the effect of different race course limits on BLEU and decoding speed for incremental decoding. 4.1 We further test effectiveness of delayed pruning (DP) in incremental decoding by comparing it to the case where we turn off the DP. For incremental decoding, we set the beam size and the race course limit (for DP) to be 3. Additionally, we used a threshold of −2.0 (in log-scale) for normalized LM in the delay phase of DP, which was obtained by testing on a separate development test set. We would like"
W10-1733,P07-2045,0,0.0457765,"t the user input and translation happen sequentially without any interaction between the two phases. In this paper we study decoding for SMT with the constraint that translations are to be generated incrementally for every word typed in by the user. Such a translation service can be used for language learning, where the user is fluent in the target language and experiments with many different source language sentences interactively, or in real-time translation environments such as speechspeech translation or translation during interactive chats. We use a phrase-based decoder similar to Moses (Koehn et al., 2007) and propose novel modifications in the decoding algorithm to tackle incremental decoding. Our system maintains a 2.1 Incremental Decoding Beam Search for Phrase-based SMT In this section we review the usual beam search decoder for phrase-based MT because we present our modifications for incremental decoding using the same notation. Beam search decoding for phrasebased SMT (Koehn, 2004) begins by collecting the translation options from the phrase table for all possible phrases of a given input sentence and precomputes the future cost for all possible contiguous sequences in the sentence. The p"
W10-1733,N09-2036,0,0.0918189,"phrases Pnew , whereas the newer hypotheses are processed as in regular beam-search. Algorithm 3 PreProcess subroutine 1: Input: partial sentence Sp of length ls 2: Retrieve partial decoder object for Sp−1 3: Identify possible Pnew (subject to MaxPhrLen) 4: Recompute fc for all spans in 1...ls 5: for every Pnew in local phrase table do 6: Load translation options to table 7: for every Pold in local phrase table do 8: Update fc with the recomputed cost 2.4 Lazier Cube Pruning We have adapted the pervasive lazy algorithm (or ’lazier cube pruning’) proposed originally for Hiero-style systems by (Pust and Knight, 2009) for our phrase-based system. This step corresponds to the lines 5 − 9 of algorithm 2 and allows us to only generate as many hypotheses as specified by the configurable parameters, beam size and beam threshold. Figure 1 illustrates the process of lazier cube pruning for a single bin. At the highest level it uses a priority queue, which is populated by the different hyper-edges or surfaces3 , each corresponding to a pair of hypotheses that are being merged to create a new hypothesis. New hypotheses are generated iteratively, such that the hypothesis with the highest score is chosen in each iter"
W10-1733,2006.amta-papers.25,0,0.0534038,", we compare it with Moses by running it in regular beam search mode. The Moses systems were also optimized separately on the WMT07 devsets. Apart from comparing our decoder with Moses in regular beam search, we also compared the incremental decoding with regular regular beam using our decoder. To make it comparable with incremental decoding, we used the regular beam search to re-decode the sentence fragments for every additional word in the input sentence. We measured the following parameters in our empirical analysis: translation quality (as measured by BLEU (Papineni et al., 2002) and TER (Snover et al., 2006)), search errors and translation speed. Finally, we also measured the effect of different race course limits on BLEU and decoding speed for incremental decoding. 4.1 We further test effectiveness of delayed pruning (DP) in incremental decoding by comparing it to the case where we turn off the DP. For incremental decoding, we set the beam size and the race course limit (for DP) to be 3. Additionally, we used a threshold of −2.0 (in log-scale) for normalized LM in the delay phase of DP, which was obtained by testing on a separate development test set. We would like to highlight two observations"
W10-1733,koen-2004-pharaoh,0,0.0502965,"source language sentences interactively, or in real-time translation environments such as speechspeech translation or translation during interactive chats. We use a phrase-based decoder similar to Moses (Koehn et al., 2007) and propose novel modifications in the decoding algorithm to tackle incremental decoding. Our system maintains a 2.1 Incremental Decoding Beam Search for Phrase-based SMT In this section we review the usual beam search decoder for phrase-based MT because we present our modifications for incremental decoding using the same notation. Beam search decoding for phrasebased SMT (Koehn, 2004) begins by collecting the translation options from the phrase table for all possible phrases of a given input sentence and precomputes the future cost for all possible contiguous sequences in the sentence. The pseudo-code for the usual beam-search decoding algorithm is illustrated in Algorithm 1. The decoder creates n bins for storing hypotheses grouped by the number of source words covered. Starting from a null hypothesis in bin 0, the decoder iterates through bins 1 though n filling them with new hypotheses by extending the entries in the earlier bins. A hypothesis contains the target words"
W10-1733,P09-4005,0,0.0202714,"formance by avoiding search errors (compared to the incremental decoder that does not use DP). 4.3 knowledge. They may be simply re-translating the input each time using a fast decoder or re-using prior decoder states as we do here. Intereactive translation using text prediction strategies have been studied well (Foster et al., 1997; Foster et al., 2002; Och et al., 2003). They all attempt to interactively help the human user in the postediting process, by suggesting completion of the word/phrase based on the user accepted prefix and the source sentece. Incremental feedback is part of Caitra (Koehn, 2009) an interactive tool for human-aided MT and works on a similar setting to interactive MT. In Caitra, the source text is pre-translated first and during the interactions it dynamically generates user suggestions. Our incremental decoder work differs from these text prediction based approaches, in the sense that the input text is not available to the decoder beforehand and the decoding is being done dynamically for every source word as opposed to generating suggestions dynamically for completing target sentece. 6 We presented a modified beam search algorithm for an efficient incremental decoder"
W10-1733,E03-1032,0,0.0158291,"his is because of the delay in pruning the tortoises until the race course limit. Even with such significantly large number of hypotheses being retained for every bin, DP results in improved speed (over re-decoding from scratch) and better performance by avoiding search errors (compared to the incremental decoder that does not use DP). 4.3 knowledge. They may be simply re-translating the input each time using a fast decoder or re-using prior decoder states as we do here. Intereactive translation using text prediction strategies have been studied well (Foster et al., 1997; Foster et al., 2002; Och et al., 2003). They all attempt to interactively help the human user in the postediting process, by suggesting completion of the word/phrase based on the user accepted prefix and the source sentece. Incremental feedback is part of Caitra (Koehn, 2009) an interactive tool for human-aided MT and works on a similar setting to interactive MT. In Caitra, the source text is pre-translated first and during the interactions it dynamically generates user suggestions. Our incremental decoder work differs from these text prediction based approaches, in the sense that the input text is not available to the decoder bef"
W11-2167,P09-1088,0,0.583313,"e probability p(e|f ) has to be shared by all the rules having the same source side string f , leading to fragmentation and resulting in many rules having very poor probability. Approaches to improve the inference (the induction of the SCFG rules from the bitext) typically follows two streams. One focusses on filtering the extracted hierarchical rules either by removing redundancy (He et al., 2009) or by filtering rules based on certain patterns (Iglesias et al., 2009), while the other stream is concerned about alternative approaches for learning the synchronous grammar (Blunsom et al., 2008; Blunsom et al., 2009; de Gispert et al., 2010). This paper falls under the latter category and we use a non-parametric Bayesian approach for rule extraction for Hiero-style systems. Our objective in this paper is to provide a principled 533 Proceedings of the 6th Workshop on Statistical Machine Translation, pages 533–541, c Edinburgh, Scotland, UK, July 30–31, 2011. 2011 Association for Computational Linguistics rule extraction method using a Bayesian framework that can extract the minimal SCFG rules without reducing the BLEU score. 2 Motivation and Related Work The large number of rules in Hiero-style systems le"
W11-2167,J07-2003,0,0.908814,"high quality rules leading to improved generalization and the automatic identification of commonly re-used rules. We show that our Bayesian model is able to extract minimal set of hierarchical phrase rules without impacting the translation quality as measured by the BLEU score. 1 While most of the research in Hiero-style systems is focused on the improving the decoder, and in particular the link to the language model, comparatively few papers have considered the inference of the probabilistic SCFG from the word alignments. A majority of the systems employ the classic ruleextraction algorithm (Chiang, 2007) which extracts rules by replacing possible sub-spans (permitted by the word alignments) with a non-terminal and then using relative frequencies to estimate the probabilistic synchronous context-free grammar. One of the issues in building Hiero-style systems is in managing the size of the synchronous grammar. The original approach extracts a larger number of rules when compared to a phrase-based system on the same data leading to practical issues in terms of memory requirements and decoding speed. Introduction Hierarchical phrase-based (Hiero) machine translation (Chiang, 2007) has attracted s"
W11-2167,N09-1062,0,0.0607756,"l maximize the likelihood of producing the entire set of observed phrase pairs). Using Bayes’ rule, the posterior over the derivations r given the phrase pairs Rp can be written as: P (r|Rp ) ∝ P (Rp |r)P (r) (1) where P (Rp |r) is equal to one when the sequence of rules r and phrase-pairs Rp are consistent, i.e. r can be partitioned into derivations to compose the set of phrase-pairs such that the derivations respect the given word alignments; otherwise P (Rp |r) is zero. The overall structure of the model is analogous to the Bayesian model for inducing Tree Substitution Grammars proposed by Cohn et al. (2009). Note that, our model extracts hierarchical rules for the word-aligned phrase pairs and not for the sentences. Similar to the other Hiero-style systems, we use two types of rules: terminal and hierarchical rules. For each phrase-pair, our model either generates a terminal rule by not segmenting the phrase-pair, or decides to segment the phrase-pair and extract some rules. Though it is possible to segment phrase-pairs by two (or more) non-overlapping spans, we propose a simpler model in this paper and restrict the hierarchical rules to contain only one non-terminal (unlike the case of classic"
W11-2167,D10-1053,0,0.0591137,"Missing"
W11-2167,D08-1033,0,0.0400314,"e 1: An example phrase-pair with Viterbi alignments X → (Eighth and Ninth X1 for the financial year, octavo y noveno X1 para el ejercicio) X → (European Development Funds, Fondos Europeos de Desarrollo) X → (Eighth and Ninth X1 , octavo y noveno X1 ) X → (European Development Funds for the financial year, Fondos Europeos de Desarrollo para el ejercicio) Figure 2: Two possible derivations of the phrase-pair in Figure 1 where a is the set of alignments in the given subspan; if the sub-span has multiple Viterbi alignments from different phrase-pairs, we consider the union of all such alignments. DeNero et al. (2008) use a similar prior- geometric mean of the forward and reverse IBM-1 alignments. However, we use the product of geometric means of the forward and reverse alignment scores. We also experimented with the arithmetic mean of the lexical alignment probabilities. The lexical prior lx in the first step can be defined similarly. We found the particular combination of, ‘arithmetic mean’ for the lexical prior lx (in the first step) and ‘geometric mean’ for the base distribution P0 (in the second step) to work better, as we discuss later in Section 5. Assuming the heuristically extracted phrase pairs t"
W11-2167,P07-1019,0,0.0149304,"nslation model and the language model was trained with the same set and an additional 2 million sentences from the UN data, using SRILM toolkit with Knesser-Ney discounting. We tuned the feature weights on the WMT-10 devset using MERT (Och, 2003) and evaluate on the test set by computing lower-cased BLEU score (Papineni et al., 2002) using the WMT-10 standard evaluation script. We use Kriya – an in-house implementation of hierarchical phrase-based translation written predominantly in Python. Kriya supports the entire translation pipeline of SCFG rule extraction and decoding with cube pruning (Huang and Chiang, 2007) and LM integration (Chiang, 2007). We use the 7 features (4 translation model features, extracted rules penalty, word penalty and language model) as is typical in Hiero-style systems. For tuning the feature weights, we have adapted the MERT implementation in Moses1 for use with Kriya as the decoder. We started by training and evaluating the two baseline systems using i) two non-terminals and ii) one non-terminal, which were trained using the conventional heuristic extraction approach. For the baseline with one non-terminal, we modified the heuristic rule extraction algorithm appropriately2 ."
W11-2167,E09-1044,0,0.587594,"Missing"
W11-2167,W02-1018,0,0.0804535,"Missing"
W11-2167,P03-1021,0,0.0416857,",[3,4]) ([2,2],[2,2]) ([0,0],[0,0]) ([1,1],[1,1]) Figure 4: Decomposed alignment tree for the example alignment in Fig. 3. 5 Experiments We use the English-Spanish data from WMT-10 shared task for the experiments to evaluate the effectiveness of our Bayesian rule extraction approach. We used the entire shared task training set except the UN data for training translation model and the language model was trained with the same set and an additional 2 million sentences from the UN data, using SRILM toolkit with Knesser-Ney discounting. We tuned the feature weights on the WMT-10 devset using MERT (Och, 2003) and evaluate on the test set by computing lower-cased BLEU score (Papineni et al., 2002) using the WMT-10 standard evaluation script. We use Kriya – an in-house implementation of hierarchical phrase-based translation written predominantly in Python. Kriya supports the entire translation pipeline of SCFG rule extraction and decoding with cube pruning (Huang and Chiang, 2007) and LM integration (Chiang, 2007). We use the 7 features (4 translation model features, extracted rules penalty, word penalty and language model) as is typical in Hiero-style systems. For tuning the feature weights, we hav"
W11-2167,P02-1040,0,0.0828271,"ent tree for the example alignment in Fig. 3. 5 Experiments We use the English-Spanish data from WMT-10 shared task for the experiments to evaluate the effectiveness of our Bayesian rule extraction approach. We used the entire shared task training set except the UN data for training translation model and the language model was trained with the same set and an additional 2 million sentences from the UN data, using SRILM toolkit with Knesser-Ney discounting. We tuned the feature weights on the WMT-10 devset using MERT (Och, 2003) and evaluate on the test set by computing lower-cased BLEU score (Papineni et al., 2002) using the WMT-10 standard evaluation script. We use Kriya – an in-house implementation of hierarchical phrase-based translation written predominantly in Python. Kriya supports the entire translation pipeline of SCFG rule extraction and decoding with cube pruning (Huang and Chiang, 2007) and LM integration (Chiang, 2007). We use the 7 features (4 translation model features, extracted rules penalty, word penalty and language model) as is typical in Hiero-style systems. For tuning the feature weights, we have adapted the MERT implementation in Moses1 for use with Kriya as the decoder. We started"
W11-2167,C08-1136,0,0.171982,"unts averaged by the number of thin iterations become our translation model. In our model, a sample for a given phrase pair corresponds either to its terminal derivation or two rules in a hierarchical derivation. The model samples a derivation from the space of derivations that are consistent with the word alignments. In order to achieve this, we need an efficient way to enumerate the derivations for a phrase pair such that they are consistent with the alignments. We use the linear time algorithm to maximally decompose a wordaligned phrase pair, so as to encode it as a compact alignment tree (Zhang et al., 2008). e0 e1 e2 e3 e4 f0 f1 f2 f3 f4 e5 Figure 3: Example phrase pair with alignments. For a phrase-pair with a given alignment as shown in Figure 3, Zhang et al. (2008) generalize the O(n+ K) time algorithm for computing all K common intervals of two different permutations of length n. The contiguous blocks of the alignment are captured as the nodes in the alignment tree and the tree structure for the example phrase pair in Figure 3 is shown in Figure 4. The italicized nodes form a leftbranching chain in the alignment tree and the subspans of this chain also lead to alignment nodes that are not ex"
W11-2167,C08-1144,0,0.205434,"om an aligned bitext. The synchronous context-free grammar links non-terminals in source and target languages. Decoding in such systems employ a modified CKYparser that is integrated with a language model. The primary advantage of Hiero-style systems lie in their unsupervised model of syntax for translation: allowing long-distance reordering and capturing certain syntactic constructions, particularly those that involve discontiguous phrases. It has been demonstrated to be a successful framework with comparable performance with other statistical frameworks and suitable for large-scale corpora (Zollmann et al., 2008). However, one of the Extremely large Hiero phrase tables may also lead to statistical issues, where the probability mass has to be shared by more rules: the probability p(e|f ) has to be shared by all the rules having the same source side string f , leading to fragmentation and resulting in many rules having very poor probability. Approaches to improve the inference (the induction of the SCFG rules from the bitext) typically follows two streams. One focusses on filtering the extracted hierarchical rules either by removing redundancy (He et al., 2009) or by filtering rules based on certain pat"
W12-3145,J07-2003,0,0.323815,"Missing"
W12-3145,P11-1004,1,0.83664,"mprove the robustness and applicability of our ensemble approach for different datasets and language pairs. 359 PROD Weights uniform uniform uniform uniform uniform optimized Base 27.67 27.72 27.96 27.98 27.99 28.25 Norm. 27.94 27.95 26.21 27.98 28.09 28.11 Table 2: Applying ensemble decoding with different mixture operations on the Test-11 dataset. Best performing setting is shown in Boldface. 3 3.1 English-Czech System Morpheme Segmented Model For English-Czech, we additionally experimented using morphologically segmented versions of the Czech side of the parallel data, since previous work (Clifton and Sarkar, 2011) has shown that segmentation of morphologically rich languages can aid translation. To derive the segmentation, we built an unsupervised morphological segmentation model using the Morfessor toolkit (Creutz and Lagus, 2007). Morfessor uses minimum description length criteria to train a HMM-based segmentation model. Varying the perplexity threshold in Morfessor does not segment more word types, but rather oversegments the same word types. We hand tuned the model parameters over training data size and perplexity; these control the granularity and coverage of the segmentations. Specifically, we tr"
W12-3145,W11-2123,0,0.0462496,"thesis fragment, it finds the best position for the fragment in the final sentence and uses the corresponding score. Specifically, we compute three different scores corresponding to the three states where the fragment can end up in the final sentence, viz. sentence initial, middle and final and choose the best score. Thus given a fragment tf consisting of a sequence of target tokens, we compute LM scores for (i) &lt;s&gt; tf , (ii) tf and (iii) tf &lt;/s&gt; and use the best score (only) for pruning.2 While this increases the number of LM queries, we exploit the language model state information in KenLM (Heafield, 2011) to optimize the queries by saving the scores for the unchanged states. Our earlier experiments showed significant reduction in search errors due to this approach, in addition to a small but consistent increase in BLEU score (Sankaran et al., 2012). 2 French-English System In addition to the baseline system, we also trained separate systems for News and Non-News genres for applying ensemble decoding (Razmara et al., 2012). The news genre system was trained only using the news-commentary corpus (about 137K sen1 Alternately systems add sentence boundary markers (&lt;s&gt; and &lt;/s&gt;) to the training dat"
W12-3145,P07-2045,0,0.00869682,"hod improved the BLEU score by 0.4 points over the baseline in newstest-2011. For English-Czech, we segmented the Czech side of the corpora and trained two different segmented models in addition to our baseline system. 1 Baseline Systems Our shared task submissions are trained in the hierarchical phrase-based model (Chiang, 2007) framework. Specifically, we use Kriya (Sankaran et al., 2012) - our in-house Hiero-style system for training and decoding. We now briefly explain the baseline systems in French-English and English-Czech language pairs. We use GIZA++ for word alignments and the Moses (Koehn et al., 2007) phrase-extractor for extracting the initial phrases. The translation models are trained using the rule extraction module in Kriya. In both cases, we pre-processed the training data by running it through the usual pre-processing pipeline of tokenization and lowercasing. For French-English baseline system, we trained a simplified hierarchical phrase-based model where the right-hand side can have at most one nonterminal (denoted as 1NT) instead of the usual two non-terminal (2NT) model. In our earlier experiments we found the 1NT model to perform comparably to the 2NT model for close language pa"
W12-3145,P03-1021,0,0.169795,"ed for French-English use a simpler Hiero translation Method Baseline Hiero News data Non-news data Ensemble P ROD Devset 26.03 24.02 26.09 25.66 Test-11 27.63 26.47 27.87 28.25 Test-12 28.15 26.27 28.15 28.09 Mix. Operation WMAX WSUM SWITCH M AX SWITCH S UM PROD Table 1: French-English BLEU scores. Best performing setting is shown in Boldface. model having at most one non-terminal (1NT) on the right-hand side. We use 7567 sentence pairs from news-tests 2008 through 2010 for tuning and use news-test 2011 for testing in addition to the 2012 test data. The feature weights were tuned using MERT (Och, 2003) and we report the devset (IBM) BLEU scores and the testset BLEU scores computed using the official evaluation script (mteval-v11b.pl). The results for the French-English experiments are reported in Table 1. We note that both baseline Hiero model and the model trained from the nonnews genre get comparable BLEU scores. The news genre model however gets a lesser BLEU score and this is to be expected due to the very small training data available for this genre. Table 2 shows the results of applying various mixture operations on the devset and testset, both in normalized (denoted by Norm.) and un-"
W12-3145,P12-1099,1,0.863463,"tf , (ii) tf and (iii) tf &lt;/s&gt; and use the best score (only) for pruning.2 While this increases the number of LM queries, we exploit the language model state information in KenLM (Heafield, 2011) to optimize the queries by saving the scores for the unchanged states. Our earlier experiments showed significant reduction in search errors due to this approach, in addition to a small but consistent increase in BLEU score (Sankaran et al., 2012). 2 French-English System In addition to the baseline system, we also trained separate systems for News and Non-News genres for applying ensemble decoding (Razmara et al., 2012). The news genre system was trained only using the news-commentary corpus (about 137K sen1 Alternately systems add sentence boundary markers (&lt;s&gt; and &lt;/s&gt;) to the training data so that they are explicitly present in the translation and language models. While this can speed up the decoding as the cube pruning is more aggressive, it also limits the applicability of rules having the boundary contexts. 2 This ensures the the LM score estimates are never underestimated for pruning. We retain the LM score for fragment (case ii) for estimating the score for the full candidate sentence later. 357 tenc"
W16-2904,N09-1014,0,0.0683716,"Missing"
W16-2904,P06-1027,0,0.0331244,"; Tamura et al., 2012; Talukdar et al., 2008; Das and Petrov, 2011), we integrated the graph based semi-supervised algorithm of Subramanya et al. (2010) and adapted their approach to improve on the results from BANNER. We show that our approach achieves a statistically significant improvement in terms of F-measure on the BioCreative II dataset for gene mention tagging. Semi-supervised learning for gene mention tagging is not without precedent. There has been several semi-supervised approaches for the gene mention task and they have always been more successful than fully supervised approaches (Jiao et al., 2006; Ando, 2007; Campos et al., 2013; Munkhdalai et al., 2015). Ando (2007) used a semi-supervised approach, Alternative Structure Optimization or ASO, in the BioCreative II gene mention shared task along with other extensions, such as using a lexicon or combining several classifiers. ASO ranked first among all competitors in the shared task competition 2007. Ando reported usage of unlabeled data as the most useful part of his system improving the F-measure of the baseline by 2.09 points where the complete (winning) system had a total improvement of 3.23 points over the baseline CRF (Ando, 2007)."
W16-2904,P14-1064,0,0.0149443,"emi-supervised Gene Mention Tagging Golnar Sheikhshab1,2 , Elizabeth Starks2 , Aly Karsan2 , Anoop Sarkar1 , Inanc Birol1,2 gsheikhs@sfu.ca,{lstarks, akarsan}@bcgsc.ca,anoop@sfu.ca,ibirol@bcgsc.ca 1 2 School of Computing Science, Simon Fraser University, Burnaby, BC, Canada Canada’s Michael Smith Genome Sciences Centre, British Columbia Cancer Agency, Vancouver, BC, Canada Abstract Inspired by the success of graph-based semisupervised learning methods in other NLP tasks (Subramanya et al., 2010; Zhu et al., 2003; Subramanya and Bilmes, 2009; Alexandrescu and Kirchhoff, 2009; Liu et al., 2012; Saluja et al., 2014; Tamura et al., 2012; Talukdar et al., 2008; Das and Petrov, 2011), we integrated the graph based semi-supervised algorithm of Subramanya et al. (2010) and adapted their approach to improve on the results from BANNER. We show that our approach achieves a statistically significant improvement in terms of F-measure on the BioCreative II dataset for gene mention tagging. Semi-supervised learning for gene mention tagging is not without precedent. There has been several semi-supervised approaches for the gene mention task and they have always been more successful than fully supervised approaches ("
W16-2904,N03-1028,0,0.088213,"sure over BANNER, a widely used biomedical NER system. We note that our tool is transductive and modular in nature, and can be integrated with other CRF-based supervised NER tools. 1 Introduction Detecting biomedical named entities such as genes and proteins is one of the first steps in many natural language processing systems that analyze biomedical text. Finding relations between entities, and expanding knowledge bases are examples of research that highly depend on the accuracy of gene and protein mention tagging. Named entity recognition is typically modelled as a sequence tagging problem (Sha and Pereira, 2003). One of the most commonly used models for sequence tagging is a Conditional Random Field (CRF) (Lafferty et al., 2001; Sha and Pereira, 2003). Many popular and best performing biomedical named entity recognition systems, such as BANNER (Leaman et al., 2008), Gimli (Campos et al., 2013) and BANNER-CHEMDNER (Munkhdalai et al., 2015) use CRF as their core machine learning model built on the MALLET toolkit (McCallum, 2002). 27 Proceedings of the 15th Workshop on Biomedical Natural Language Processing, pages 27–35, c Berlin, Germany, August 12, 2016. 2016 Association for Computational Linguistics"
W16-2904,D10-1017,0,0.339288,"s, akarsan}@bcgsc.ca,anoop@sfu.ca,ibirol@bcgsc.ca 1 2 School of Computing Science, Simon Fraser University, Burnaby, BC, Canada Canada’s Michael Smith Genome Sciences Centre, British Columbia Cancer Agency, Vancouver, BC, Canada Abstract Inspired by the success of graph-based semisupervised learning methods in other NLP tasks (Subramanya et al., 2010; Zhu et al., 2003; Subramanya and Bilmes, 2009; Alexandrescu and Kirchhoff, 2009; Liu et al., 2012; Saluja et al., 2014; Tamura et al., 2012; Talukdar et al., 2008; Das and Petrov, 2011), we integrated the graph based semi-supervised algorithm of Subramanya et al. (2010) and adapted their approach to improve on the results from BANNER. We show that our approach achieves a statistically significant improvement in terms of F-measure on the BioCreative II dataset for gene mention tagging. Semi-supervised learning for gene mention tagging is not without precedent. There has been several semi-supervised approaches for the gene mention task and they have always been more successful than fully supervised approaches (Jiao et al., 2006; Ando, 2007; Campos et al., 2013; Munkhdalai et al., 2015). Ando (2007) used a semi-supervised approach, Alternative Structure Optimiz"
W16-2904,D08-1061,0,0.0933228,"Missing"
W16-2904,D12-1003,0,0.0273552,"ention Tagging Golnar Sheikhshab1,2 , Elizabeth Starks2 , Aly Karsan2 , Anoop Sarkar1 , Inanc Birol1,2 gsheikhs@sfu.ca,{lstarks, akarsan}@bcgsc.ca,anoop@sfu.ca,ibirol@bcgsc.ca 1 2 School of Computing Science, Simon Fraser University, Burnaby, BC, Canada Canada’s Michael Smith Genome Sciences Centre, British Columbia Cancer Agency, Vancouver, BC, Canada Abstract Inspired by the success of graph-based semisupervised learning methods in other NLP tasks (Subramanya et al., 2010; Zhu et al., 2003; Subramanya and Bilmes, 2009; Alexandrescu and Kirchhoff, 2009; Liu et al., 2012; Saluja et al., 2014; Tamura et al., 2012; Talukdar et al., 2008; Das and Petrov, 2011), we integrated the graph based semi-supervised algorithm of Subramanya et al. (2010) and adapted their approach to improve on the results from BANNER. We show that our approach achieves a statistically significant improvement in terms of F-measure on the BioCreative II dataset for gene mention tagging. Semi-supervised learning for gene mention tagging is not without precedent. There has been several semi-supervised approaches for the gene mention task and they have always been more successful than fully supervised approaches (Jiao et al., 2006; An"
W16-2904,P12-1032,0,0.0601445,"Missing"
W16-2904,J92-4003,0,\N,Missing
W16-2904,P11-1061,0,\N,Missing
W17-6205,W04-3315,0,0.0298182,"oshi (1996) within the TAG literature. Sarkar and Joshi (1996) posit that the shared argument is located in the canonical position within each conjunct, and propose an operation, the Conjoin Operation, that applies across elementary trees. This operation identifies and merges the shared argument when two elementary trees combine via coordination, yielding a derived tree in which an argument is multiply dominated by two verbal projections. The Conjoin Operation analysis has been used and extended often in TAG-based linguistic research, including the semantics of clausal coordination and scope (Banik, 2004; Han et al., 2008; Storoshenko and Frank, 2012), and the syntax of Right-Node-Raising (Han et al., 2010). According to the multiple dominance analysis, as the shared argument is in a dominance relation 2 Argument Sharing via the Conjoin Operation Sarkar and Joshi (1996) utilize elementary trees with contraction sets and coordinating auxiliary trees. The elementary trees necessary to derive (1) are given in Figure 1.1 In each of (αlikes{DP } ) 1 We follow Frank’s (2002) Condition on Elementary Tree Minimality (CETM), and adopt the DP Hypothesis and the VP-internal Subject Hypothesis in definin"
W17-6205,C96-2103,1,0.615355,"same meaning. But this is not always the case (Sabbagh, 2007). For instance, while (9) means that the same student read every paper and summarized every book, (10) can mean different students read every paper and summarized every book. (9) A student read every paper and summarized every book. (10) (11) A student read every paper and a student summarized every book. This takes us to the multiple dominance analysis, first proposed by McCawley (1982), that postulates that a shared argument is multiply dominated by elements from multiple conjuncts. A version of this approach has been developed in Sarkar and Joshi (1996) within the TAG literature. Sarkar and Joshi (1996) posit that the shared argument is located in the canonical position within each conjunct, and propose an operation, the Conjoin Operation, that applies across elementary trees. This operation identifies and merges the shared argument when two elementary trees combine via coordination, yielding a derived tree in which an argument is multiply dominated by two verbal projections. The Conjoin Operation analysis has been used and extended often in TAG-based linguistic research, including the semantics of clausal coordination and scope (Banik, 2004"
W17-6205,J94-1004,0,0.243698,"the subject argument variable, and a t auxiliary tree for the scope component of the subject argument. (αtakes{DPi ,DP } ) is an initial predicative tree with an empty DP position for the subject and a DP substitution site for the object, and is paired with (α0 takes{DPi ,DP } ), which has e substitution sites for the subject and the object argument variables. Note that the t node of (α0 takes{DPi ,DP } ) has multiple links, 1 and 2 , for the scope components of the subject and the object DPs. This indicates that the two scope component trees will multiply-adjoin to the t node, as defined in Schabes and Shieber (1994), and predicts scope ambiguity, as the order in which the two trees adjoin is not specified. 5 Conclusion and Future Work We have outlined a Synchronous TAG analysis of clausal coordination with shared arguments that does not rely on the Conjoin Operation, utilizing only the standard TAG operations, substitution and adjoining. Therefore, we do not require modified parsing algorithms to handle the Conjoin Operation, unrooted trees, or tree nodes with multiple parents as in Sarkar and Joshi (1996). In our analysis, the shared argument is present syntactically only in one conjunct in which it app"
W17-6205,W12-4627,0,0.0369214,"f (1), an example of clausal coordination with a shared object argument, we propose elementary tree pairs in (βlikes{DP } ) and (β 0 likes{DP } ) in Figure 2. (βlikes{DP } ) is an auxiliary TP tree that introduces a coordinator and adjoins to another TP it coordinates with. The object argument of this auxiliary tree is null, directly reflecting the fact that it is absent in the 2 For the sake of simplicity, we include only the links that are relevant for the current discussion. 3 Semantic elementary trees in which λ-operators abstract over argument variables have been proposed and utilized in Frank and Storoshenko (2012) to handle many difficult cases of quantifier scope within tree-local MC-TAG. trees. Elementary trees such as (βand hates{DP } ) are in accordance with CETM, as coordinators are functional heads. 45 αlikes{DP } : βand hates{DP } : TP T’ DPi ↓ γ1: TP TP TP TP* Conj VP T V’ DP DP likes αSue: DP D D Sue Pete Kim Sue DP V ti αPete: DP V’ DP D D hates and T’ DPi VP T αKim: DP V ti T’ DPi ↓ and TP Conj TP D VP T DP V’ ti V T’ DPi VP T Kim likes V’ DP ti V DP hates D Pete Figure 1: Elementary trees and derived tree for Sue likes and Kim hates Pete with the Conjoin Operation   αhates{DP} : TPoa 3 DP"
W17-6205,C90-3045,0,0.806532,"ect argument. (α0 wh hates{DPj } ) is a 47  Conj TP T’ DPi D T γ 0 2: t TP and DPi VP  T’ T he, ti VP he, ti , Sue DP ti V’ DP V DP hates D Pete ti V’ λx e ∧ Sue he, ti t λx t V DP e he, ti e he, ti likes D Pete λy.hates(x, y) Kim λy.likes(x, y) Kim Figure 7: Derived trees for Sue hates Pete and likes Kim corresponding semantics tree with a λ-abstrated object argument. Here, we abstract away from the full semantics of wh-questions and simply represent the predicate-argument structure. In representing the semantics of who, we follow the tree-local multi-component treatment of quantification (Shieber and Schabes, 1990; Nesson and Shieber, 2006) and implement a generalized quantifier analysis to adopt the model of Han et al. (2008). We thus propose that the semantics of who has two components: (α0 who) is a variable and substitutes into the argument position e linked with 2 in (α0 wh hates{DPj } ), and (β 0 who) represents the scope and adjoins onto t again linked with 2 in (α0 wh hates{DPj } ). The coordinating auxiliary tree pairs (βlikes{DP } ) and (β 0 likes{DP } ) depicted in Figure 2 will each adjoin onto the TP node in (αwh hates{DPj } ) and the he, ti node in (α0 wh hates{DPj } ), both linked with 3"
W17-6205,W12-4602,0,0.0297189,"rature. Sarkar and Joshi (1996) posit that the shared argument is located in the canonical position within each conjunct, and propose an operation, the Conjoin Operation, that applies across elementary trees. This operation identifies and merges the shared argument when two elementary trees combine via coordination, yielding a derived tree in which an argument is multiply dominated by two verbal projections. The Conjoin Operation analysis has been used and extended often in TAG-based linguistic research, including the semantics of clausal coordination and scope (Banik, 2004; Han et al., 2008; Storoshenko and Frank, 2012), and the syntax of Right-Node-Raising (Han et al., 2010). According to the multiple dominance analysis, as the shared argument is in a dominance relation 2 Argument Sharing via the Conjoin Operation Sarkar and Joshi (1996) utilize elementary trees with contraction sets and coordinating auxiliary trees. The elementary trees necessary to derive (1) are given in Figure 1.1 In each of (αlikes{DP } ) 1 We follow Frank’s (2002) Condition on Elementary Tree Minimality (CETM), and adopt the DP Hypothesis and the VP-internal Subject Hypothesis in defining our elementary 44 and (βand hates{DP } ), the"
W17-6205,W08-2305,1,0.469716,"ithin the TAG literature. Sarkar and Joshi (1996) posit that the shared argument is located in the canonical position within each conjunct, and propose an operation, the Conjoin Operation, that applies across elementary trees. This operation identifies and merges the shared argument when two elementary trees combine via coordination, yielding a derived tree in which an argument is multiply dominated by two verbal projections. The Conjoin Operation analysis has been used and extended often in TAG-based linguistic research, including the semantics of clausal coordination and scope (Banik, 2004; Han et al., 2008; Storoshenko and Frank, 2012), and the syntax of Right-Node-Raising (Han et al., 2010). According to the multiple dominance analysis, as the shared argument is in a dominance relation 2 Argument Sharing via the Conjoin Operation Sarkar and Joshi (1996) utilize elementary trees with contraction sets and coordinating auxiliary trees. The elementary trees necessary to derive (1) are given in Figure 1.1 In each of (αlikes{DP } ) 1 We follow Frank’s (2002) Condition on Elementary Tree Minimality (CETM), and adopt the DP Hypothesis and the VP-internal Subject Hypothesis in defining our elementary 4"
W17-6205,J92-4004,0,0.684259,"Missing"
W17-6205,W10-4402,1,0.670026,"cated in the canonical position within each conjunct, and propose an operation, the Conjoin Operation, that applies across elementary trees. This operation identifies and merges the shared argument when two elementary trees combine via coordination, yielding a derived tree in which an argument is multiply dominated by two verbal projections. The Conjoin Operation analysis has been used and extended often in TAG-based linguistic research, including the semantics of clausal coordination and scope (Banik, 2004; Han et al., 2008; Storoshenko and Frank, 2012), and the syntax of Right-Node-Raising (Han et al., 2010). According to the multiple dominance analysis, as the shared argument is in a dominance relation 2 Argument Sharing via the Conjoin Operation Sarkar and Joshi (1996) utilize elementary trees with contraction sets and coordinating auxiliary trees. The elementary trees necessary to derive (1) are given in Figure 1.1 In each of (αlikes{DP } ) 1 We follow Frank’s (2002) Condition on Elementary Tree Minimality (CETM), and adopt the DP Hypothesis and the VP-internal Subject Hypothesis in defining our elementary 44 and (βand hates{DP } ), the object DP node is in the contraction set, notated as a su"
W17-6205,W07-0402,0,0.0626638,"Missing"
W17-6205,W90-0102,0,\N,Missing
W17-6205,W04-3316,0,\N,Missing
W18-1815,N12-1048,0,0.559646,"Missing"
W18-1815,P81-1022,0,0.702099,"Missing"
W18-1815,D15-1128,0,0.0133097,"speech stream is segmented and translated incrementally to reduce the latency. There are two approaches for simultaneous translation task: sentence segmentation and incremental decoding, also called stream decoding. In incremental decoding, incoming words are fed into the decoder one-by-one, and the decoder updates its internal state. The decoder is responsible to decide when to begin the translation process and when to output the translation. Incremental decoding algorithms have been proposed for phrase-based (Kolss et al., 2008; Sankaran et al., 2010) translation, hierarchical phrase-based (Finch et al., 2015) and syntax-based (Oda et al., 2015) translation systems. Real-world speech translation systems estimate the sentence boundaries using punctuation insertion methods (Rangarajan Sridhar et al., 2013). As a result, recent work in simultaneous machine translation assume the input is already segmented into sentences, and focus on splitting the sentences into shorter subsequences of words (segments). This approach is called sentence segmentation. As soon as a segment is recognized, it is given to a decoder to generate and output the translation for that segment. Proceedings of AMTA 2018, vol. 1: MT"
W18-1815,D14-1140,0,0.580832,"Missing"
W18-1815,E17-1099,0,0.35894,"Missing"
W18-1815,D15-1006,0,0.037326,"Missing"
W18-1815,W11-2123,0,0.0340924,"tem in terms of translation quality and latency and compare it with different baselines. 4.1 System Setup We use the parallel text provided as training data of IWSLT 2013 and about one million sentence pairs of Europarl (v7), to train the translation system. We use development set 2010 and 2012 and test set 2010 of IWSLT shared task as development set to tune the translation system (LR-Hiero) and test set of IWSLT 2013 is used as the test set to evaluate the simultaneous translation system. We use a 5-gram LM trained on the monolingual German data provided by WMT 2013 shared task using KenLM (Heaﬁeld, 2011). In LR-Hiero, we set pop limit 500, maximum source rule length 7 and at most 2 non-terminals. The standard feature set of LR-Hiero (Siahbani et al., 2013) is used in a discriminative log-linear model. The weights in the log-linear model are tuned by minimizing BLEU loss through MERT (Och, 2003) on the dev set for each language pair. In these experiments, we use the reference transcript of the utterance for dev and test sets. LR-Hiero is trained once and used in all experiments. We use Stanford POS-Tagger (Toutanova et al., 2003) to obtain the POS tags to extract features for the segmentation"
W18-1815,P07-2045,0,0.0171061,"ng results and fast to be computed. In addition to POS tags we also propose to use two features created based on reordering. We compare four different sets of features including the basic features (set1) to train the segmentation model: • Part of Speech tags: The ﬁrst group uses POS tags of the candidate segment as features. We considered the last three POS tags in a segment and also bigrams and trigrams of the POS tags for each segment (set2). In addition to these features we consider POS bigram surrounding the segment boundary (set3). • Reordering Features: The lexicalized reordering model (Koehn et al., 2007) of phrase-based translation system determines the orientation of phrases with respect to the previous phrase, monotone (M), Proceedings of AMTA 2018, vol. 1: MT Research Track Boston, March 17 - 21, 2018 |Page 158 Set1: Set2: Set3: Set4: Word, Position, Length + POS tags + Cross POS tag + Reordering “engineers”, 9, 5 [NNS],[CC-NNS],[NN-CC-NNS] [NNS-IN] 0.8904, 0.6 Table 1: Feature sets and an example (for segment “from our scientist and engineers” in the English sentence in Figure 1). swap (S) and discontinuous (D). We expect the segments to be monotonically ordered. For each segment, we deﬁn"
W18-1815,N03-1017,0,0.23647,"phrases) s = s1 . . . sK : sk = jk−1 , jk ∀k = 1 . . . K, j0 = 1 (1) To restrict the reorderings inside the segments, we should extract segments where ajk−1 < ajk for k = 1 . . . K. This segmentation results in a phrase alignment for the sentence pair f, e called monotonic phrase alignment. Figure 1 shows word alignment matrix and monotonic phrase alignment for an EnglishGerman sentence pair. Monotonic phrase alignment for a sentence pair can be found in linear time, given the word alignment. Experimentally, it has been shown that translation quality improves signiﬁcantly with longer phrases (Koehn et al., 2003). Therefore to avoid too many short segments which could lead to wordto-word translation, the segmentation algorithm is given a constraint based on a constant μ and segments of length less than μ are disallowed1 . Usually a word alignment model is trained over a parallel corpus containing the parallel data of the 1μ is usually set to 4 (Yarmohammadi et al., 2013; Siahbani et al., 2014). Proceedings of AMTA 2018, vol. 1: MT Research Track Boston, March 17 - 21, 2018 |Page 156 translation task and parallel corpus C. It provides us the oracle word alignment for C which can be used to extract labe"
W18-1815,C04-1072,0,0.0920594,"llel corpus2 . Given a parallel corpus C = F, E and an expected number of segments, K, the translation-based segmentation heuristic ﬁrst extracts all features over the corpus along with their frequencies, c1 . . . cm . The translation-based segmentation heuristic tries to ﬁnd a feature set s containing l(≤ m) features according to l the least harmful segmentation criterion for the translation accuracy where i=1 csi = K (features of set s appear in K points of source corpus which result in K segments). Oda et al. (2014) deﬁne translation accuracy as the summation of sentence-level BLEU score (Lin and Och, 2004) of the translations of segmented sentences. The feature set is initialized as empty (s = {}), then the best feature (adding it to the corpus causes the least translation loss) is greedily chosen and added to the feature set. Once a feature has been chosen, all the points exhibiting that feature are segmented at the same time. This approach requires running the translation system for each possible feature in each iteration which takes a long time. To overcome this issue, they propose dynamic programming (DP) and call their approach Greedy-DP (GDP)3 . However, this approach does not consider th"
W18-1815,W02-1018,0,0.119124,"yond just decoding cues. Hiero models encode the translation correspondences in hierarchical phrases, unlike the phrase-based models that use contiguous translation phrases. The notion of hierarchy allows the Hiero models to capture long-distance reordering between source and target languages unlike phrase-based models. Additionally they also model discontiguous translations, e.g. translating the English word not as ne pas in French (with an appropriate verb form inserted between ne and pas). These properties make Hiero models more appropriate for some language pairs than phrase-based models (Marcu and Wong, 2002; Och and Ney, 2002, 2004). Hiero uses a lexicalized synchronous context-free grammar (SCFG) extracted from word and phrase alignments of a bitext. Typically, Hiero uses a CKY-style decoding algorithm with time complexity O(n3 ) where the source input has n words. Previous translation services proposed for real-time translation environments, are mainly phrasebased (F¨ugen et al., 2007; Sankaran et al., 2010; Bangalore et al., 2012; Yarmohammadi et al., 2013; Oda et al., 2014). Since a phrase-based decoder generates translations in a left-to-right manner, it is more suited than the CKY based de"
W18-1815,P03-1021,0,0.128933,"test set 2010 of IWSLT shared task as development set to tune the translation system (LR-Hiero) and test set of IWSLT 2013 is used as the test set to evaluate the simultaneous translation system. We use a 5-gram LM trained on the monolingual German data provided by WMT 2013 shared task using KenLM (Heaﬁeld, 2011). In LR-Hiero, we set pop limit 500, maximum source rule length 7 and at most 2 non-terminals. The standard feature set of LR-Hiero (Siahbani et al., 2013) is used in a discriminative log-linear model. The weights in the log-linear model are tuned by minimizing BLEU loss through MERT (Och, 2003) on the dev set for each language pair. In these experiments, we use the reference transcript of the utterance for dev and test sets. LR-Hiero is trained once and used in all experiments. We use Stanford POS-Tagger (Toutanova et al., 2003) to obtain the POS tags to extract features for the segmentation model 6 . 4.2 Evaluating the Segmentation Model In Section 2 we discussed two heuristics: translation-based and alignment-based, to provide training data for segmentation model. We conduct some experiments to compare different feature sets for these heuristics. We use Dev 2010 and 2012 and Test"
W18-1815,P02-1038,0,0.22963,"s. Hiero models encode the translation correspondences in hierarchical phrases, unlike the phrase-based models that use contiguous translation phrases. The notion of hierarchy allows the Hiero models to capture long-distance reordering between source and target languages unlike phrase-based models. Additionally they also model discontiguous translations, e.g. translating the English word not as ne pas in French (with an appropriate verb form inserted between ne and pas). These properties make Hiero models more appropriate for some language pairs than phrase-based models (Marcu and Wong, 2002; Och and Ney, 2002, 2004). Hiero uses a lexicalized synchronous context-free grammar (SCFG) extracted from word and phrase alignments of a bitext. Typically, Hiero uses a CKY-style decoding algorithm with time complexity O(n3 ) where the source input has n words. Previous translation services proposed for real-time translation environments, are mainly phrasebased (F¨ugen et al., 2007; Sankaran et al., 2010; Bangalore et al., 2012; Yarmohammadi et al., 2013; Oda et al., 2014). Since a phrase-based decoder generates translations in a left-to-right manner, it is more suited than the CKY based decoding which requir"
W18-1815,J04-4002,0,0.36305,"Missing"
W18-1815,P14-2090,0,0.366631,", Abbotsford, V2S 7M7, Canada Hassan S. Shavarani Ashkan Alinejad Anoop Sarkar sshavara@sfu.ca aalineja@sfu.ca anoop@sfu.ca School of Computing Science, Simon Fraser University, Burnaby, V5A 1S6, Canada Abstract Previous simultaneous translation approaches either use a separate segmentation step followed by a machine translation decoder or rely on the decoder to segment and translate without training the segmenter to minimize delay or increase translation quality. We integrate a segmentation model and an incremental decoding algorithm to create an automatic simultaneous translation framework. Oda et al. (2014) propose a method to provide annotated data for sentence segmentation. This work uses this data to train a segmentation model that is integrated with a novel simultaneous translation decoding algorithm. We show that this approach is more accurate than previously proposed segmentation models when integrated with a translation decoder. Our results on the speech translation of TED talks from English to German show that our system can achieve translation quality close to the ofﬂine translation system while at the same time minimizing the delay in producing the translations incrementally. Our appro"
W18-1815,P15-1020,0,0.105279,"ted incrementally to reduce the latency. There are two approaches for simultaneous translation task: sentence segmentation and incremental decoding, also called stream decoding. In incremental decoding, incoming words are fed into the decoder one-by-one, and the decoder updates its internal state. The decoder is responsible to decide when to begin the translation process and when to output the translation. Incremental decoding algorithms have been proposed for phrase-based (Kolss et al., 2008; Sankaran et al., 2010) translation, hierarchical phrase-based (Finch et al., 2015) and syntax-based (Oda et al., 2015) translation systems. Real-world speech translation systems estimate the sentence boundaries using punctuation insertion methods (Rangarajan Sridhar et al., 2013). As a result, recent work in simultaneous machine translation assume the input is already segmented into sentences, and focus on splitting the sentences into shorter subsequences of words (segments). This approach is called sentence segmentation. As soon as a segment is recognized, it is given to a decoder to generate and output the translation for that segment. Proceedings of AMTA 2018, vol. 1: MT Research Track Boston, March 17 - 2"
W18-1815,P02-1040,0,0.112407,"revious works latency was simply deﬁned as the number of segments divided by the total translation time (Oda et al., 2014; Shavarani et al., 2015). We extend the Pareto optimality approach by modifying the deﬁnition of both objective functions: translation accuracy and latency. 2.2.1 Translation Accuracy Our primarily experiments show that using the sentence-level BLEU to measure the translation accuracy in GDP (and Pareto Optimality approach) tends to oversegment some sentences in the corpus and leave the other sentences untouched. To overcome this issue, we propose to use corpus-level BLEU (Papineni et al., 2002) to measure translation accuracy. The corpus-level BLEU gives a general view over the corpus 2 The feature set which results in the best segmentation strategy (a set of segmentation points which gives us the best translation for the given parallel corpus). 3 Please refer to (Oda et al., 2014) for more details Proceedings of AMTA 2018, vol. 1: MT Research Track Boston, March 17 - 21, 2018 |Page 157 therefore it alleviate the tendency to localize the segmentation. 2.2.2 Latency In simultaneous translation, the translation process starts before receiving the end of sentence, and the evaluation ob"
W18-1815,N13-1023,0,0.0797364,"ed stream decoding. In incremental decoding, incoming words are fed into the decoder one-by-one, and the decoder updates its internal state. The decoder is responsible to decide when to begin the translation process and when to output the translation. Incremental decoding algorithms have been proposed for phrase-based (Kolss et al., 2008; Sankaran et al., 2010) translation, hierarchical phrase-based (Finch et al., 2015) and syntax-based (Oda et al., 2015) translation systems. Real-world speech translation systems estimate the sentence boundaries using punctuation insertion methods (Rangarajan Sridhar et al., 2013). As a result, recent work in simultaneous machine translation assume the input is already segmented into sentences, and focus on splitting the sentences into shorter subsequences of words (segments). This approach is called sentence segmentation. As soon as a segment is recognized, it is given to a decoder to generate and output the translation for that segment. Proceedings of AMTA 2018, vol. 1: MT Research Track Boston, March 17 - 21, 2018 |Page 154 Different methods have been proposed for sentence segmentation. Some use prosodic boundaries for segmentation (F¨ugen et al., 2007; Bangalore et"
W18-1815,W10-1733,1,0.91052,"tency. 1 Introduction In simultaneous translation the incoming speech stream is segmented and translated incrementally to reduce the latency. There are two approaches for simultaneous translation task: sentence segmentation and incremental decoding, also called stream decoding. In incremental decoding, incoming words are fed into the decoder one-by-one, and the decoder updates its internal state. The decoder is responsible to decide when to begin the translation process and when to output the translation. Incremental decoding algorithms have been proposed for phrase-based (Kolss et al., 2008; Sankaran et al., 2010) translation, hierarchical phrase-based (Finch et al., 2015) and syntax-based (Oda et al., 2015) translation systems. Real-world speech translation systems estimate the sentence boundaries using punctuation insertion methods (Rangarajan Sridhar et al., 2013). As a result, recent work in simultaneous machine translation assume the input is already segmented into sentences, and focus on splitting the sentences into shorter subsequences of words (segments). This approach is called sentence segmentation. As soon as a segment is recognized, it is given to a decoder to generate and output the transl"
W18-1815,2015.iwslt-papers.14,1,0.818271,"egmentation. Some use prosodic boundaries for segmentation (F¨ugen et al., 2007; Bangalore et al., 2012), while others use classiﬁcation models. For example Rangarajan Sridhar et al. (2013) train a classiﬁer to predict punctuation marks. The other approaches rely on the reordering probabilities of phrases to predict the segment boundaries (Fujita et al., 2013; Yarmohammadi et al., 2013; Siahbani et al., 2014). Oda et al. (2014) propose a method to provide annotated data for sentence segmentation which can be used in training a segmentation model. This method which later have been extended by (Shavarani et al., 2015) aims to ﬁnd the best segmentation strategy for a given set of sentence which optimizes the translation accuracy. But the obtained annotated data has never been used in an end-to-end simultaneous translation system. In this work, we focus on sentence segmentation approach for simultaneous translation. We model the segmentation task as a classiﬁcation problem and investigate different methods to provide annotated data for training the segmentation model (Section 2). We modify Oda et al. (2014) approach by propose a new formula to compute the latency and use Pareto-optimality for ﬁnding good seg"
W18-1815,D13-1110,1,0.726718,"lly. Although hierarchical phrase-based (Hiero) translation system usually performs comparable to or better than conventional phrase-based systems, they use CKY based decoding algorithm which requires the entire input sentence to generate the translation. While phrasebased decoders generate translation in a left-to-right manner and it makes phrase-based systems more suitable for simultaneous translation than Hiero. We use LR-Hiero for simultaneous translation which uses hierarchical phrase-based translation models while generates the translation in left-to-right manner (Watanabe et al., 2006; Siahbani et al., 2013). We modify LR-Hiero decoder and combine it with the segmentation model to incrementally translate the input sentence (stream of words). We evaluate our simultaneous translation system on the speech translation of TED talks on EnglishGerman. The experimental results show that our system can achieve translation quality close to ofﬂine SMT system while generate the output translation words around twenty times faster. We also compare our simultaneous translation system to neural machine translation (NMT) simultaneous translation systems. Our system outperforms the state of the art NMT-based simul"
W18-1815,N03-1033,0,0.114386,"the monolingual German data provided by WMT 2013 shared task using KenLM (Heaﬁeld, 2011). In LR-Hiero, we set pop limit 500, maximum source rule length 7 and at most 2 non-terminals. The standard feature set of LR-Hiero (Siahbani et al., 2013) is used in a discriminative log-linear model. The weights in the log-linear model are tuned by minimizing BLEU loss through MERT (Och, 2003) on the dev set for each language pair. In these experiments, we use the reference transcript of the utterance for dev and test sets. LR-Hiero is trained once and used in all experiments. We use Stanford POS-Tagger (Toutanova et al., 2003) to obtain the POS tags to extract features for the segmentation model 6 . 4.2 Evaluating the Segmentation Model In Section 2 we discussed two heuristics: translation-based and alignment-based, to provide training data for segmentation model. We conduct some experiments to compare different feature sets for these heuristics. We use Dev 2010 and 2012 and Test 2010 from IWSLT to provide the training date for the segmentation model. Table 2 shows the statistics of data used in our experiments. To evaluate segment translation quality, we use corpus level BLEU (Papineni et al., 2002). To compute th"
W18-1815,P06-1098,0,0.0305738,"input segment individually. Although hierarchical phrase-based (Hiero) translation system usually performs comparable to or better than conventional phrase-based systems, they use CKY based decoding algorithm which requires the entire input sentence to generate the translation. While phrasebased decoders generate translation in a left-to-right manner and it makes phrase-based systems more suitable for simultaneous translation than Hiero. We use LR-Hiero for simultaneous translation which uses hierarchical phrase-based translation models while generates the translation in left-to-right manner (Watanabe et al., 2006; Siahbani et al., 2013). We modify LR-Hiero decoder and combine it with the segmentation model to incrementally translate the input sentence (stream of words). We evaluate our simultaneous translation system on the speech translation of TED talks on EnglishGerman. The experimental results show that our system can achieve translation quality close to ofﬂine SMT system while generate the output translation words around twenty times faster. We also compare our simultaneous translation system to neural machine translation (NMT) simultaneous translation systems. Our system outperforms the state of"
W18-1815,I13-1141,0,0.297388,"s recognized, it is given to a decoder to generate and output the translation for that segment. Proceedings of AMTA 2018, vol. 1: MT Research Track Boston, March 17 - 21, 2018 |Page 154 Different methods have been proposed for sentence segmentation. Some use prosodic boundaries for segmentation (F¨ugen et al., 2007; Bangalore et al., 2012), while others use classiﬁcation models. For example Rangarajan Sridhar et al. (2013) train a classiﬁer to predict punctuation marks. The other approaches rely on the reordering probabilities of phrases to predict the segment boundaries (Fujita et al., 2013; Yarmohammadi et al., 2013; Siahbani et al., 2014). Oda et al. (2014) propose a method to provide annotated data for sentence segmentation which can be used in training a segmentation model. This method which later have been extended by (Shavarani et al., 2015) aims to ﬁnd the best segmentation strategy for a given set of sentence which optimizes the translation accuracy. But the obtained annotated data has never been used in an end-to-end simultaneous translation system. In this work, we focus on sentence segmentation approach for simultaneous translation. We model the segmentation task as a classiﬁcation problem and"
W18-5119,D13-1087,0,0.0220152,"parately and finally use our classifier to determine the number of offensive instances that were correctly classified in decrypted text. The objective here is to measure how well either of the techniques is able to recover the originally intended message from the encrypted version of the message, simulating a real online user’s behaviour of disguising an offensive text to beat a rule-based filter. In other words, we aim at decreasing the gap between the classification accuracies on a test set and its encrypted form. Decipherment HMM: For the HMM based decipherment, we run 100 random restarts (Berg-Kirkpatrick and Klein, 2013), running the EM algorithm to convergence 100 times, to find the initialization that leads to the local optima with highest likelihood. Spelling Correction: We use settings for the noisy channel model based spelling correction as stated in Norvig (2009): pspell error = 0.05, and λ =1. The maximum edit distance limit is set to 3, and the error model is trained on the real chat messages we collected. The error model trained on pairs of misspelled words and the correctly spelled English words from the Linux system dictionary. 4.4 Evaluation We evaluate our approach in terms of the classifier accu"
W18-5119,W17-3013,0,0.0605405,"Missing"
W18-5119,W17-3006,0,0.0316072,"ine learning based classifiers. Nobata et al. (2016) unified predefined linguistic elements and word embeddings to train a regression model. Su et al. (2017) presented a system to detect and rephrase profane words written in Chinese. samghabadi2017detecting Recently, deep learning methods have been employed for abusive language detection. Zhang et al. (2018) presented an algorithm for detecting hate speech using a combination of Convolutional Neural Networks (CNN) and Gated Recurrent Unit (GRU). Gamb¨ack and Sikdar (2017) used four CNNs to classify tweets to one of four predefined categories. Park and Fung (2017) adopted a two-step approach with two classifiers. The first step performs classification on abusive language and the second step classifies a text into types of sexist and racist abusive language given that the language is abusive. Three CNN-based models have been used for classification: CharCNN, WordCNN and HybridCNN. Badjatiya et al. (2017) used an LSTM model with features extracted by character n-grams for hate speech detection. As malicious users can change their ways of transforming text to avoid being filtered, an approach such as ours which takes into account the adversarial relations"
W18-5119,P11-1002,0,0.0588965,"Missing"
W18-5119,P06-2065,0,0.253368,"e actual intended content as the plaintext. Then we apply decipherment to recover more recognizable plaintext from the ciphertext and apply filters on the plaintext. Conceivably these users may create very complex unbreakable ciphers which cannot be deciphered by our system, but in such cases the ciphers are likely to also be unreadable by other humans who are the intended audience. We do not see many examples of this in our real-world chat messages. We use Expectation Maximization (Dempster et al., 1977) and Hidden Markov Models (HMM, Rabiner (1989)) for our unsupervised decipherment method (Knight et al., 2006). We use an efficient beam search decoding algorithm to decipher ciphertext into the most likely plaintext. We also compare against supervised noisy channel models Automated filters are commonly used by online services to stop users from sending ageinappropriate, bullying messages, or asking others to expose personal information. Previous work has focused on rules or classifiers to detect and filter offensive messages, but these are vulnerable to cleverly disguised plaintext and unseen expressions especially in an adversarial setting where the users can repeatedly try to bypass the filter. In"
W18-5119,W99-0906,0,0.181965,"training can recover most of the words in our test 156 NSERC RGPIN-2018-06437 and RGPAS-2018522574 and a Department of National Defence (DND) and NSERC grant DGDND-2018-00025 to the third author. We would also like to thank Ken Dwyer and Michael Harris from the Two Hat Security Company who collected and organized the real-world chat data for us. Thanks also to the Two Hat CEO, Chris Priebe, who supported this research and also provided an internship to the first author where he worked on spelling correction for chats. Previous work on decipherment are often based on noisy-channel frameworks. Knight and Yamada (1999) proposed to use EM algorithm to estimate the mapping distribution over the sound to characters, then generated the plaintext using Viterbi algorithm (Forney Jr, 1973). The learning objective is to maximize the probability of the mapping ciphertext phoneme-tokens to plaintext characters. Further, Knight et al. (2006) studied using EM for unsupervised learning of the substitution maps for 1:1 letter substitution ciphers. Ravi and Knight (2011) proposed to regard foreign language as ciphertext and English as plaintext, converting a translation problem into one of word substitution decipherment."
W18-5119,C08-1056,0,0.0900928,"Missing"
W18-5119,W17-3003,0,0.0545251,"erns and applied multi-level classification for flame detection. Chen et al. (2012) introduced a Lexical Syntactic Feature (LSF) architecture to detect offensive content and identify potential offensive users in social media. Kansara and Shekokar (2015) proposed a framework that detects bullying texts and images in using feature extraction and classifiers. Djuric et al. (2015) leveraged word embedding representations (Mikolov et al., 2013) to improve machine learning based classifiers. Nobata et al. (2016) unified predefined linguistic elements and word embeddings to train a regression model. Su et al. (2017) presented a system to detect and rephrase profane words written in Chinese. samghabadi2017detecting Recently, deep learning methods have been employed for abusive language detection. Zhang et al. (2018) presented an algorithm for detecting hate speech using a combination of Convolutional Neural Networks (CNN) and Gated Recurrent Unit (GRU). Gamb¨ack and Sikdar (2017) used four CNNs to classify tweets to one of four predefined categories. Park and Fung (2017) adopted a two-step approach with two classifiers. The first step performs classification on abusive language and the second step classif"
W18-5119,2005.mtsummit-papers.11,0,0.0142853,"l b 2: Randomly initialize the s(c |e) substitution table and normalize 3: for i = 1 . . . d do 4: preprocess the i-th cipher text sequence by removing repeated characters and lower case the text 5: insert NULL based on techniques in Sec. 2.1 6: if i 6= 1 then 7: initialize s(c |e) using the (i − 1)th trained table s(c |e) 8: apply Forward-Backward algorithm to learn parameters s(c |e) 3 3.1 in a balanced training set of 155,251 tokens. 3.2 Other data Language Model: For the character models we used a combination of Wiktionary and the European Parliament Proceedings Parallel Corpus 1996-2011 (Koehn, 2005). 100K English sentences were sampled from the German-English EuroParl Corpus to give us a 2.7M token English plaintext corpus. Spelling Correction: We use the English Gigaword corpus (Graff et al., 2003) to train the language model for the noisy channel spelling correction module. The preprocessed corpus has 7.4M lowercase English word tokens. For spelling correction, the Linux system dictionary with 479,829 English words in lower case is used. We used 3,393,741 pairs of human disguised words and original plain text words from the rule-based filtering system to train the error model. Data 3.3"
W18-5119,W12-2103,0,0.050706,"using rules or classifiers. We provide experimental results on three different datasets and show that decipherment is an effective tool for this task. 1 Introduction Under-aged social media users and users of chat rooms associated with software like video games are routinely exposed to offensive language including sexting, profanities, age-inappropriate languages, cyber-bullying, and requests for personal identifying information. A common approach is to have a filter to block such messages. Filters are either rule-based (Razavi et al., 2010) or machine learning classifiers (Yin et al., 2009; Warner and Hirschberg, 2012; Williams and Burnap, 2015). However, users wishing to bypass such filters can subtly transform messages in novel ways which can be hard to detect. Since malicious users and spammers can change their attacks to avoid being filtered, an approach to offensive text detection that takes into account this adversarial relationship is what can deal with real-world abusive language detection better. Techniques like spelling correction have 149 Proceedings of the Second Workshop on Abusive Language Online (ALW2), pages 149–159 c Brussels, Belgium, October 31, 2018. 2018 Association for Computational L"
W18-5119,P13-1154,0,0.0158029,"kov Model (Knight et al., 2006) with cipher characters c as observations and plaintext characters p as the hidden states of the HMM. We train this using unsupervised learning using the Forward-Backward algorithm (Rabiner, 1989). We propose our own initialization algorithm (Algorithm 1), based on the assumption that the previous trained table can help us to reach better local optima with fewer iterations compared to uniform initialization with random perturbations. With the learned posterior distribution and language model score we use beam search to obtain the best plaintext (Forney Jr, 1973; Nuhn et al., 2013). Beam search combines breadth-first search and child pruning reducing the search space for each partial hypothesis of the decipherment. I> (c(snd ), c(tnd )) d∈{L,R} j=1 (1) 150 Algorithm 1 Initialization with previous trained table 1: Given a set of cipher text sentences with size of d, a plain text with vocabulary size v and plain text trigram model b 2: Randomly initialize the s(c |e) substitution table and normalize 3: for i = 1 . . . d do 4: preprocess the i-th cipher text sequence by removing repeated characters and lower case the text 5: insert NULL based on techniques in Sec. 2.1 6: i"
W18-5618,P14-5010,0,0.00325706,"rom a super-domain of ’blood cells’ and ’transcription factors’. The documents are annotated for protein, DNA, RNA, cell line, and cell type entity classes. them for training ELMo. This dataset is described in detail in section 4.1. We report results on two benchmark datasets, which we describe in sections 4.2 and 4.3. 4.1 ELMo Training Set We downloaded the text files of a subset of PMC documents that are available at ftp://ftp.ncbi.nlm.nih.gov/pub/pmc in May 2018, and picked 3960 full-text documents that had a Medical Subject Heading (Mesh) term ’cancer’. We ran StanfordNLP/CoreNLP toolkit (Manning et al., 2014) on these documents for sentence splitting and tokenization. Tokens of each sentence were joined with space character in between to form the sentences in the training set. This dataset contains about 21 million tokens, and is substantially smaller than the One Billion Word Benchmark (Chelba et al., 2014) that Peters et al. (2018) used for training ELMo but contains in-domain text that is more likely to benefit the biomedical text analysis of interest in this paper. 4.2 5 Table 1 shows the leading results in the literature (top four rows) in comparison with our results (bottom three rows) on BC"
W18-5618,D14-1162,0,0.0870716,"d word representation, and showed how it can be used in existing task-specific deep neural networks. The method improves the state of the art over a variety of NLP tasks such as question answering, word sense disambiguation, sentiment analysis, and named entity recognition. The developers of the tool also provide an ELMo model pre-trained on the Billion-word Language Model (LM) dataset (Chelba et al., 2014) as an off-the2 ELMo ELMo (Peters et al., 2018) is a system that produces context-aware embeddings for word tokens. Similar to traditional context-independent word embeddings such as GloVe (Pennington et al., 2014) and Word2Vec (Mikolov et al., 2013), ELMo representations can be used as input to a neural network for downstream tasks. Though, ELMo is different from the traditional word embeddings in that it gives the representation of the 160 Proceedings of the 9th International Workshop on Health Text Mining and Information Analysis (LOUHI 2018), pages 160–164 c Brussels, Belgium, October 31, 2018. 2018 Association for Computational Linguistics Figure 1: ELMo is a bidirectional LSTM for language modelling where the next or precedent tokens are predicted from the softmax layers over forward and backward"
W18-5618,N18-1202,0,0.670548,"mprovement can be achieved even when the in-domain training dataset is smaller than the Billion-word LM data. 4. The resulting model achieves the highest precision/recall/F1 scores so far on BioCreative II Gene mention detection shared task (BC2GM). Introduction We explain ELMo and AllenNER, the named entity recognizer we used, in sections 2 and section 3. Then, we describe our datasets in section 4, and we move on to report the results in section 5. Last decade witnessed substantial improvements in machine learning methods and their application to natural language processing tasks. Recently, Peters et al. (2018) introduced ELMo (Embeddings from Language Models), a system for deep contextualized word representation, and showed how it can be used in existing task-specific deep neural networks. The method improves the state of the art over a variety of NLP tasks such as question answering, word sense disambiguation, sentiment analysis, and named entity recognition. The developers of the tool also provide an ELMo model pre-trained on the Billion-word Language Model (LM) dataset (Chelba et al., 2014) as an off-the2 ELMo ELMo (Peters et al., 2018) is a system that produces context-aware embeddings for word"
W18-5618,C16-1030,0,0.142802,"gs to AllenNLP (Gardner et al., 2017) for NER tasks. AllenNLP uses a bidirectional two-layer LSTM-CRF (Lample et al., 2016) to perform NER as a sequence tagging task. Each word is tagged with an output that marks if it is at the beginning (B), in the middle (I), at the end (E or L), or outside (O) of an entity type. One-word entities are also marked (as S or U). For example B-Gene and I-Gene stand for beginning and inside of a Gene, whereas B-DNA and E-DNA stand for beginning and ending of a DNA entity type. AllenNLP embeds the input words using a Convolutional Neural Network over characters. Rei et al. (2016) showed that word embeddings from character compositions outperform lookup embeddings such as word2vec, when used for named entity recognition. AllenNLP combines the layers in ELMo Model using learned task-specific weights, concatenates the result for each token to context-independent word embeddings, and feed the concatenation into the LSTM-CRF as illustrated in Figure 2. 4 Datasets We collected a focused domain-specific subset of PubMed Central (PMC) documents, and used 161 Figure 2: Architecture of LSTM-CRF (Lample et al., 2016) with ELMo. Traditional word embeddings and ELMo representation"
W18-5618,W04-1213,0,0.744911,"Missing"
W19-2516,Q16-1006,0,0.0144584,"tup typically followed by human archaeological-decipherment experts (Robinson, 2009), have been useful in several real world tasks. Snyder et al. (2010) propose an automatic decipherment technique that further improves existing methods by incorporating cognate identification and lexicon induction. When applied to Ugaritic, the model is able to correctly map 29 of 30 letters to their Hebrew counterparts. Reddy and Knight (2011) study the Voynich manuscript for its linguistic properties, and show that the letter sequences are generally more predictable than in natural languages. Following this, Hauer and Kondrak (2016) treat the text in the Voynich manuscript as anagrammed substitution ciphers, and their experiments suggest, arguably, that Hebrew is the language of the document. Hierarchical clustering has previously been used by Knight et al. (2011) to aid in the decipherment of the Copiale cipher, where it was able to identify meaningful groups such as word boundary markers as well as signs which correspond to the same 10 7 Conclusions We have shown that methods from computational linguistics can offer valuable insights into the proto-Elamite script, and can substantially improve the toolkit available to"
W19-2516,W11-1202,0,0.0235919,"ncorporating cognate identification and lexicon induction. When applied to Ugaritic, the model is able to correctly map 29 of 30 letters to their Hebrew counterparts. Reddy and Knight (2011) study the Voynich manuscript for its linguistic properties, and show that the letter sequences are generally more predictable than in natural languages. Following this, Hauer and Kondrak (2016) treat the text in the Voynich manuscript as anagrammed substitution ciphers, and their experiments suggest, arguably, that Hebrew is the language of the document. Hierarchical clustering has previously been used by Knight et al. (2011) to aid in the decipherment of the Copiale cipher, where it was able to identify meaningful groups such as word boundary markers as well as signs which correspond to the same 10 7 Conclusions We have shown that methods from computational linguistics can offer valuable insights into the proto-Elamite script, and can substantially improve the toolkit available to the PE specialist. Hierarchical sign clustering replicates previous work by rediscovering meaningful groups of signs, and suggests avenues for future work by revealing similarities between yet-undeciphered signs. Analysis of n-gram freq"
W19-2516,P06-2065,0,0.0479756,"in signs could appear in sign-strings. Dahl (2002) was the first to use basic computer-assisted data sorting to present information on sign frequencies, and Englund (2004:129–138) concluded his discussion of “the state of decipherment” by suggesting that the newly transliterated corpus would benefit from more intensive study of sign ordering phenomena. Apart from the use of Rapidminer10 to perform simple data sorting in Kelley 2018, no publications have yet described any effort to apply computational approaches to the dataset. Computational approaches to decipherment (Knight and Yamada, 1999; Knight et al., 2006), which resemble the setup typically followed by human archaeological-decipherment experts (Robinson, 2009), have been useful in several real world tasks. Snyder et al. (2010) propose an automatic decipherment technique that further improves existing methods by incorporating cognate identification and lexicon induction. When applied to Ugaritic, the model is able to correctly map 29 of 30 letters to their Hebrew counterparts. Reddy and Knight (2011) study the Voynich manuscript for its linguistic properties, and show that the letter sequences are generally more predictable than in natural lang"
W19-2516,W99-0906,0,0.34232,"positions in which certain signs could appear in sign-strings. Dahl (2002) was the first to use basic computer-assisted data sorting to present information on sign frequencies, and Englund (2004:129–138) concluded his discussion of “the state of decipherment” by suggesting that the newly transliterated corpus would benefit from more intensive study of sign ordering phenomena. Apart from the use of Rapidminer10 to perform simple data sorting in Kelley 2018, no publications have yet described any effort to apply computational approaches to the dataset. Computational approaches to decipherment (Knight and Yamada, 1999; Knight et al., 2006), which resemble the setup typically followed by human archaeological-decipherment experts (Robinson, 2009), have been useful in several real world tasks. Snyder et al. (2010) propose an automatic decipherment technique that further improves existing methods by incorporating cognate identification and lexicon induction. When applied to Ugaritic, the model is able to correctly map 29 of 30 letters to their Hebrew counterparts. Reddy and Knight (2011) study the Voynich manuscript for its linguistic properties, and show that the letter sequences are generally more predictabl"
W19-2516,W17-2202,0,0.029176,"etations of the topics serve only to highlight the amount of potentially fruitful analysis that still remains to be done. It also remains to see what topics arise when sign variants are collapsed together: preliminary results suggest that topics resembling our topic 6 and topic 10 are still found, but new topics also appear which have no clear correlates in the model discussed in this paper. 6 plaintext symbol. Homburg and Chiarcos (2016) report preliminary results on automatic word segmentation for Akkadian cuneiform using rule-based, dictionary based, and data-driven statistical techniques. Pagé-Perron et al. (2017) furnish an analysis of Sumerian text including morphology, parts-ofspeech (POS) tagging, syntactic parsing, and machine translation using a parallel corpus. Although Sumerian and Akkadian are both geographically and chronologically close to PE, these corpora are very large (e.g. 1.5 million lines for Sumerian), and are presented in word level transliterations rather than sign-by-sign transcriptions. This makes most of these techniques inapplicable to PE. Our study is more similar in spirit to Reddy and Knight (2011), as the Voynich manuscript and PE are both undeciphered and resource-poor, ma"
W19-2516,W11-1511,0,0.0869903,"using rule-based, dictionary based, and data-driven statistical techniques. Pagé-Perron et al. (2017) furnish an analysis of Sumerian text including morphology, parts-ofspeech (POS) tagging, syntactic parsing, and machine translation using a parallel corpus. Although Sumerian and Akkadian are both geographically and chronologically close to PE, these corpora are very large (e.g. 1.5 million lines for Sumerian), and are presented in word level transliterations rather than sign-by-sign transcriptions. This makes most of these techniques inapplicable to PE. Our study is more similar in spirit to Reddy and Knight (2011), as the Voynich manuscript and PE are both undeciphered and resource-poor, making analysis especially difficult. Related Work Meriggi (1971:173–174) conducted manual graphotactic analysis of PE (and later linear Elamite) texts, for example by noting the positions in which certain signs could appear in sign-strings. Dahl (2002) was the first to use basic computer-assisted data sorting to present information on sign frequencies, and Englund (2004:129–138) concluded his discussion of “the state of decipherment” by suggesting that the newly transliterated corpus would benefit from more intensive"
W19-2516,W14-3110,0,0.0931738,"Missing"
W19-2516,P10-1107,0,0.0272536,"138) concluded his discussion of “the state of decipherment” by suggesting that the newly transliterated corpus would benefit from more intensive study of sign ordering phenomena. Apart from the use of Rapidminer10 to perform simple data sorting in Kelley 2018, no publications have yet described any effort to apply computational approaches to the dataset. Computational approaches to decipherment (Knight and Yamada, 1999; Knight et al., 2006), which resemble the setup typically followed by human archaeological-decipherment experts (Robinson, 2009), have been useful in several real world tasks. Snyder et al. (2010) propose an automatic decipherment technique that further improves existing methods by incorporating cognate identification and lexicon induction. When applied to Ugaritic, the model is able to correctly map 29 of 30 letters to their Hebrew counterparts. Reddy and Knight (2011) study the Voynich manuscript for its linguistic properties, and show that the letter sequences are generally more predictable than in natural languages. Following this, Hauer and Kondrak (2016) treat the text in the Voynich manuscript as anagrammed substitution ciphers, and their experiments suggest, arguably, that Hebr"
W97-1505,A88-1019,0,0.155472,"Missing"
W97-1505,W94-0104,0,0.0419199,"n performed along either of the hierarchy dimensions. The expansion could be done by general principles (add all trees of a certain subcat frame if any are present), or could be done based on performance of the sub-grammar on held-out training data. Most domains have a rich terminological vocabulary, which if not taken into account can cause prohibitive ambiguity in parsing and interpretation. Identifying and demarcating domain specific terminology is helpful for all of these approaches, since the terms can then be treated as single tokens. This can been done either manually or automatically (Daille, 1994; Jacquemin and Royaut, 1994). Once the sub-grammar has been finalized, strategies for recovering from failure to parse should be developed. One simple strategy is to fall back to the large/whole grammar. A more sophisticated strategy would be to back off using a lexical hierarchy in the same way it was used for generalizing from the training set. hierarchy. Without the description hierarchy, there would be no need to reconcile these differences, since they would be entirely independent pieces of a flat grammar. 3 Tailoring X T A G to t h e W e a t h e r Domain While it is certainly interestin"
W97-1505,C94-1024,1,0.834435,"there is training data (usually unannotated) available; that default mechanisms will be adequate for handling over-specialization (since we know training data will not perfectly reflect the genre) and that the smaller grammar combined with defaults will still be more efficient than the large grammar. Based on these assumptions, the first choice is whether to do full parsing at all in the final application. If the domain contains a large number of fragments, it might be preferable to use a partial parsing approach, in which case development of a sub-grammar will be less crucial. Supertagging (Joshi and Srinivas, 1994) is one such approach; once the supertagger is trained for the domain, it could be used in place of the full parser. If, however, it is determined that full parsing is practicable for the domain, there are still a number of considerations in deriving the sub-grammar. In the ideal situation, there would already be a corrected parsed corpus (treebank), which can be used for crafting a sub-grammar for the domain. This is exceptionally unlikely, and in the more common case, training data will have to be constructed, either manually or automatically. In a lexicalized grammar like LTAG, this turns o"
W97-1505,C92-3145,0,0.0309251,"and more portable implementation of the X-interface to the grammar and all of the supporting tools in CLISP, which is freely available. We also present a methodology for specializing our grammar to a particular domain, and give some results on this effort. 1 1.1 Development of XTAG and Current Status 1.2 Current status of XTAG Working with and developing a large grammar is a challenging process, and the importance of having good visualization tools cannot be over-emphasized. Currently the XTAG system has X-windows based tools for viewing and updating the morphological and syntactic databases (Karp et al., 1992; Egedi and Martin, 1994), and a sophisticated parsing and grammar development interface. This interface includes a tree editor, the ability to vary parameters H i s t o r y of X T A G The XTAG project has been ongoing at Penn in some form or another since 1988. It began with a toy grammar run on LISP machines, and currently has a large English grammar, small grammars in several other languages, a sophisticated X-windows based grammar development environment and numerous satellite tools. Approximately 35 people have worked extensively on the system, and at least that many have worked more peri"
W97-1505,C96-2120,0,0.0494734,"Missing"
W97-1505,C96-1034,0,\N,Missing
W98-0130,J91-3004,0,0.0372569,"n.edu Giorgio Satta Dip. di Elettronica e Informatica Universita di Padova satta©dei.unipd.it Abstract vVe show how prefix probabilities can be computed for stochastic linear indexed grammars (SLIGs). Our results apply as weil to stochastic tree-adjoining grammars (STAGs), due to their equivalence to SLIGs. 1 Introd uction Thc problcm of computing prefix probabilities for stochastic context-free languages is defined as follows. Given a word sequence ai ···an over some alphabet E, which we call the input prefix, we must compute quantity LweE• Pr(a1 · · ·anw). This problem has been discussed in [1, 4] with the main motivation of applications in speech recognition, where we are given some word sequence a1 • • • an-li and must hypothesize the next word an. The main idea leading to the solution of this problem is that all parts of context-free derivations that are potentially of unbounded size are captured into a set of equations that can be solved &quot;off-line&quot;, i.e., before a specific prefix is considered. This is possible because the involved deriva.tions do not depend on the given prefix. Once these equations have been solved, the results are stored. When computing the prefix probability for"
W98-0130,C98-2152,1,0.731264,"input symbols. The probabilities of the former subderivations can be computed off-line, and the results are combined with subderivations of the latter kind during computation of the prefix probability for a given string. The distinction between the two kinds of subderivations requires a certain notational system that is difficult to define for tree-adjoining grammars. We will therefore concentrate on stochastic linear indexed grammars instead, relying on their equivalence to STAGs [3]. The solution proposed in the present paper is an alternative to a different approach by the same authors in [2]. In that publication, a set of equations is transformed in order to distinguish off-line and on-line computations. 2 Computation of prefix probabilities We refer the reader to [2] for the definition ofLIG. In what follows, we use a,ß, ... to denote strings of nonterminals associated with empty stacks of indices, x,y,v,w,z, ... to denote strings of terminal symbols, and a to denote a terminal symbol. Without loss of generality we require that rules are of the form A[17 oo] - a B[17&apos; oo] ß with 11111&apos;1 = 1, or of the form A[] - z, where lzl :::; 1. As usual, - is extended to a binary relation b"
W98-0130,C92-2066,0,0.0278857,"ded size and are -* 116 independent on actual input, and parts that are always of bounded length and do depend on input symbols. The probabilities of the former subderivations can be computed off-line, and the results are combined with subderivations of the latter kind during computation of the prefix probability for a given string. The distinction between the two kinds of subderivations requires a certain notational system that is difficult to define for tree-adjoining grammars. We will therefore concentrate on stochastic linear indexed grammars instead, relying on their equivalence to STAGs [3]. The solution proposed in the present paper is an alternative to a different approach by the same authors in [2]. In that publication, a set of equations is transformed in order to distinguish off-line and on-line computations. 2 Computation of prefix probabilities We refer the reader to [2] for the definition ofLIG. In what follows, we use a,ß, ... to denote strings of nonterminals associated with empty stacks of indices, x,y,v,w,z, ... to denote strings of terminal symbols, and a to denote a terminal symbol. Without loss of generality we require that rules are of the form A[17 oo] - a B[17&apos;"
W98-0130,J95-2002,0,0.0273054,"n.edu Giorgio Satta Dip. di Elettronica e Informatica Universita di Padova satta©dei.unipd.it Abstract vVe show how prefix probabilities can be computed for stochastic linear indexed grammars (SLIGs). Our results apply as weil to stochastic tree-adjoining grammars (STAGs), due to their equivalence to SLIGs. 1 Introd uction Thc problcm of computing prefix probabilities for stochastic context-free languages is defined as follows. Given a word sequence ai ···an over some alphabet E, which we call the input prefix, we must compute quantity LweE• Pr(a1 · · ·anw). This problem has been discussed in [1, 4] with the main motivation of applications in speech recognition, where we are given some word sequence a1 • • • an-li and must hypothesize the next word an. The main idea leading to the solution of this problem is that all parts of context-free derivations that are potentially of unbounded size are captured into a set of equations that can be solved &quot;off-line&quot;, i.e., before a specific prefix is considered. This is possible because the involved deriva.tions do not depend on the given prefix. Once these equations have been solved, the results are stored. When computing the prefix probability for"
zeman-sarkar-2000-learning,W99-0503,0,\N,Missing
zeman-sarkar-2000-learning,W98-1505,0,\N,Missing
zeman-sarkar-2000-learning,W97-0318,0,\N,Missing
zeman-sarkar-2000-learning,W98-1114,0,\N,Missing
zeman-sarkar-2000-learning,H91-1067,0,\N,Missing
zeman-sarkar-2000-learning,E99-1007,0,\N,Missing
zeman-sarkar-2000-learning,A97-1052,0,\N,Missing
zeman-sarkar-2000-learning,J93-2002,0,\N,Missing
zeman-sarkar-2000-learning,P98-1080,0,\N,Missing
zeman-sarkar-2000-learning,C98-1077,0,\N,Missing
zeman-sarkar-2000-learning,P93-1032,0,\N,Missing
zeman-sarkar-2000-learning,P91-1027,0,\N,Missing
zeman-sarkar-2000-learning,P99-1051,0,\N,Missing
zeman-sarkar-2000-learning,W99-0632,0,\N,Missing
zeman-sarkar-2000-learning,C96-1004,0,\N,Missing
